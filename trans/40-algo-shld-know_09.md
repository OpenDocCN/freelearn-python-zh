# 传统监督学习算法

在本章中，我们将重点介绍监督式机器学习算法，这是现代算法中最重要的类型之一。监督式机器学习算法的显著特征是使用带标签的数据来训练模型。在本书中，监督式机器学习算法分为两章。在本章中，我们将介绍所有传统的监督式机器学习算法，不包括神经网络。下一章将全面介绍使用神经网络实现监督式机器学习算法。事实上，在这一领域有如此多的持续发展，神经网络是一个值得在本书中单独章节讨论的综合性主题。

因此，这一章是关于监督式机器学习算法的两个部分中的第一部分。首先，我们将介绍监督式机器学习的基本概念。接下来，我们将介绍两种监督式机器模型——分类器和回归器。为了展示分类器的能力，我们将首先提出一个真实世界的问题作为挑战。然后，我们将介绍六种不同的分类算法，用于解决这个问题。然后，我们将专注于回归算法，首先提出一个类似的问题，以便为回归器解决问题。接下来，我们将介绍三种回归算法，并使用它们来解决问题。最后，我们将比较结果，以帮助我们总结本章介绍的概念。

本章的总体目标是让您了解不同类型的监督式机器学习技术，并了解对于某些类别的问题，最佳的监督式机器学习技术是什么。

本章讨论了以下概念：

+   理解监督式机器学习

+   理解分类算法

+   评估分类器性能的方法

+   理解回归算法

+   评估回归算法性能的方法

让我们从理解监督式机器学习背后的基本概念开始。

# 理解监督式机器学习

机器学习专注于使用数据驱动的方法来创建可以帮助我们做出决策的自主系统，无论是否有人类监督。为了创建这些自主系统，机器学习使用一组算法和方法来发现和制定数据中可重复的模式。在机器学习中最流行和强大的方法之一是监督式机器学习方法。在监督式机器学习中，算法被给定一组输入，称为**特征**，以及它们对应的输出，称为**目标** **变量**。使用给定的数据集，监督式机器学习算法用于训练一个捕捉特征和目标变量之间复杂关系的模型，该关系由数学公式表示。这个训练好的模型是用于预测的基本工具。

通过训练模型，通过生成未知特征集的目标变量来进行预测。

在监督学习中从现有数据中学习的能力类似于人脑从经验中学习的能力。监督学习中的这种学习能力使用了人脑的一个属性，是将决策能力和智能引入机器的基本途径。

让我们考虑一个例子，我们想要使用监督式机器学习技术训练一个模型，可以将一组电子邮件分类为合法邮件（称为**合法**）和不需要的邮件（称为**垃圾邮件**）。首先，为了开始，我们需要过去的例子，这样机器才能学习应该将什么样的电子邮件内容分类为垃圾邮件。这种基于内容的文本数据学习任务是一个复杂的过程，可以通过监督式机器学习算法之一来实现。在这个例子中，可以用来训练模型的一些监督式机器学习算法包括决策树和朴素贝叶斯分类器，我们将在本章后面讨论。

# 制定监督式机器学习

在深入研究监督式机器学习算法的细节之前，让我们定义一些基本的监督式机器学习术语：

| **术语** | **解释** |
| --- | --- |
| 目标变量 | 目标变量是我们希望模型预测的变量。在监督式机器学习模型中只能有一个目标变量。 |
| 标签 | 如果我们想要预测的目标变量是一个类别变量，那么它被称为标签。 |
| 特征 | 用于预测标签的一组输入变量称为特征。 |
| 特征工程 | 将特征转换为所选监督式机器学习算法准备的过程称为特征工程。 |
| 特征向量 | 在将输入提供给监督式机器学习算法之前，所有特征都被组合在一个称为特征向量的数据结构中。 |
| 历史数据 | 用于制定目标变量和特征之间关系的过去数据称为历史数据。历史数据带有示例。 |
| 训练/测试数据 | 历史数据与示例被分成两部分——一个更大的数据集称为训练数据，一个较小的数据集称为测试数据。 |
| 模型 | 目标变量和特征之间关系的最佳捕捉模式的数学表达。 |
| 训练 | 使用训练数据创建模型。 |
| 测试 | 使用测试数据评估训练模型的质量。 |
| 预测 | 使用模型预测目标变量。 |

经过训练的监督式机器学习模型能够通过估计特征来预测目标变量。

让我们介绍一下本章中将使用的符号，讨论机器学习技术：

| **变量** | **含义** |
| --- | --- |
| *y* | 实际标签 |
| *ý* | 预测标签 |
| *d* | 总示例数量 |
| *b* | 训练示例的数量 |
| *c* | 测试示例的数量 |

现在，让我们看看一些这些术语如何在实际中被制定。

正如我们讨论的，特征向量被定义为一个包含所有特征的数据结构。

如果特征的数量是*n*，训练示例的数量是*b*，那么`X_train`表示训练特征向量。每个示例都是特征向量中的一行。

对于训练数据集，特征向量由`X_train`表示。如果训练数据集中有*b*个示例，那么`X_train`将有*b*行。如果训练数据集中有*n*个变量，那么它将有*n*列。因此，训练数据集将具有*n* x *b*的维度，如下图所示：

![](img/83067a86-5ff3-4854-a16f-5415111cc00d.png)

现在，让我们假设有*b*个训练示例和*c*个测试示例。一个特定的训练示例由(*X*, *y*)表示。

我们使用上标来指示训练集中的每个训练示例。

因此，我们的标记数据集由 D = {X^((1)),y^((1))), (X^((2)),y^((2))), ..... , (X^((d)),y^((d)))}表示。

我们将其分为两部分——D[train]和 D[test]。

因此，我们的训练集可以用 D[train] = {X^((1)),y^((1))), (X^((2)),y^((2))), ..... , (X^((b)),y^((b)))}来表示。

训练模型的目标是对于训练集中的任何第 i 个示例，目标值的预测值应尽可能接近示例中的实际值。换句话说，![](img/191e0803-78b6-4df4-b1a4-54ac88b95d3f.png)。

因此，我们的测试集可以用 D[test] = {X^((1)),y^((1))), (X^((2)),y^((2))), ..... , (X^((c)),y^((c)))}来表示。

目标变量的值由向量*Y*表示：

Y = {y^((1)), y^((2)), ....., y^((m))}

# 理解启用条件

监督式机器学习是基于算法使用示例来训练模型的能力。监督式机器学习算法需要满足一定的启用条件才能执行。这些启用条件如下：

+   **足够的示例**：监督式机器学习算法需要足够的示例来训练模型。

+   **历史数据中的模式**：用于训练模型的示例需要具有其中的模式。我们感兴趣事件的发生可能性应取决于模式、趋势和事件的组合。如果没有这些，我们处理的是无法用于训练模型的随机数据。

+   **有效的假设**：当我们使用示例训练监督式机器学习模型时，我们期望适用于示例的假设在未来也是有效的。让我们看一个实际的例子。如果我们想要为政府训练一个可以预测学生是否会获得签证的机器学习模型，那么理解是在模型用于预测时，法律和政策不会发生变化。如果在训练模型后实施了新的政策或法律，可能需要重新训练模型以纳入这些新信息。

# 区分分类器和回归器

在机器学习模型中，目标变量可以是类别变量或连续变量。目标变量的类型决定了我们拥有的监督式机器学习模型的类型。基本上，我们有两种类型的监督式机器学习模型：

+   **分类器**：如果目标变量是类别变量，则机器学习模型称为分类器。分类器可用于回答以下类型的业务问题：

+   这种异常组织生长是否是恶性肿瘤？

+   根据当前的天气条件，明天会下雨吗？

+   基于特定申请人的资料，他们的抵押贷款申请是否应该被批准？

+   **回归器**：如果目标变量是连续变量，我们训练一个回归器。回归器可用于回答以下类型的业务问题：

+   根据当前的天气条件，明天会下多少雨？

+   具有给定特征的特定房屋的价格将是多少？

让我们更详细地看看分类器和回归器。

# 理解分类算法

在监督式机器学习中，如果目标变量是类别变量，则模型被归类为分类器：

+   目标变量称为**标签**。

+   历史数据称为**标记数据**。

+   需要预测标签的生产数据称为**未标记数据**。

使用训练模型准确标记未标记数据的能力是分类算法的真正力量。分类器预测未标记数据的标签以回答特定的业务问题。

在我们介绍分类算法的细节之前，让我们首先提出一个业务问题，作为分类器的挑战。然后我们将使用六种不同的算法来回答相同的挑战，这将帮助我们比较它们的方法、途径和性能。

# 提出分类器挑战

我们将首先提出一个常见的问题，我们将使用它作为测试六种不同分类算法的挑战。这个常见的问题在本章中被称为分类器挑战。使用所有六种分类器来解决同一个问题将帮助我们以两种方式：

+   所有输入变量都需要被处理和组装成一个复杂的数据结构，称为特征向量。使用相同的特征向量可以帮助我们避免为所有六个算法重复数据准备。

+   我们可以通过使用相同的特征向量作为输入来比较各种算法的性能。

分类器挑战是关于预测一个人购买的可能性。在零售行业，可以帮助最大化销售的一件事是更好地了解客户的行为。这可以通过分析历史数据中发现的模式来实现。让我们先阐述问题。

# 问题陈述

根据历史数据，我们能否训练一个二元分类器，可以预测特定用户最终是否会购买产品？

首先，让我们探索可用于解决这个问题的历史标记数据集：

x € ℜ^b, y € {0,1}

对于特定示例，当*y* = 1 时，我们称之为正类，当*y* = 0 时，我们称之为负类。

尽管正类和负类的级别可以任意选择，但定义正类为感兴趣的事件是一个好的做法。如果我们试图为银行标记欺诈交易，那么正类（即*y* = 1）应该是欺诈交易，而不是相反。

现在，让我们来看一下以下内容：

+   实际标签，用*y*表示

+   预测的标签，用*y`*表示

请注意，对于我们的分类器挑战，示例中找到的标签的实际值由*y*表示。如果在我们的示例中，有人购买了一个物品，我们说*y* = 1。预测值由*y`*表示。输入特征向量*x*的维度为 4。我们想确定用户在给定特定输入时购买的概率是多少。

因此，我们希望确定在给定特征向量*x*的特定值时*y* = 1 的概率。从数学上讲，我们可以表示如下：

![](img/64b062ae-9b4e-4cb4-b2f0-608d0ba569e0.png)

现在，让我们看看如何处理和组装特征向量*x*中的不同输入变量。在下一节中，将更详细地讨论使用处理管道组装*x*的不同部分的方法。

# 使用数据处理管道进行特征工程

为了选择一个特定的机器学习算法的数据准备被称为**特征工程**，它是机器学习生命周期的一个关键部分。特征工程在不同的阶段或阶段进行。用于处理数据的多阶段处理代码被统称为**数据管道**。在可能的情况下使用标准处理步骤制作数据管道，使其可重用并减少训练模型所需的工作量。通过使用更多经过测试的软件模块，代码的质量也得到了提高。

让我们为分类器挑战设计一个可重用的处理管道。如前所述，我们将准备数据一次，然后将其用于所有分类器。

# 导入数据

这个问题的历史数据存储在一个名为`dataset`的文件中，格式为`.csv`。我们将使用 pandas 的`pd.read_csv`函数将数据导入为数据框：

```py
dataset = pd.read_csv('Social_Network_Ads.csv')
```

# 特征选择

选择与我们想要解决的问题相关的特征的过程称为**特征选择**。这是特征工程的一个重要部分。

一旦文件被导入，我们删除`User ID`列，该列用于识别一个人，并且在训练模型时应该被排除：

```py
dataset = dataset.drop(columns=['User ID'])
```

现在让我们预览数据集：

```py
dataset.head(5)
```

数据集如下：

![](img/b0a76952-07bb-4025-9614-1e7e6d683bf0.png)

现在，让我们看看如何进一步处理输入数据集。

# 独热编码

许多机器学习算法要求所有特征都是连续变量。这意味着如果一些特征是类别变量，我们需要找到一种策略将它们转换为连续变量。独热编码是执行这种转换的最有效方式之一。对于这个特定的问题，我们唯一的类别变量是`Gender`。让我们使用独热编码将其转换为连续变量：

```py
enc = sklearn.preprocessing.OneHotEncoder()
enc.fit(dataset.iloc[:,[0]])
onehotlabels = enc.transform(dataset.iloc[:,[0]]).toarray()
genders = pd.DataFrame({'Female': onehotlabels[:, 0], 'Male': onehotlabels[:, 1]})
result = pd.concat([genders,dataset.iloc[:,1:]], axis=1, sort=False)
result.head(5)
```

一旦转换完成，让我们再次查看数据集：

![](img/6a3f7425-0e6d-444f-914c-7c12b15ada54.png)

请注意，为了将变量从类别变量转换为连续变量，独热编码已将`Gender`转换为两个单独的列——`Male`和`Female`。

# 指定特征和标签

让我们指定特征和标签。我们将使用`y`来代表标签，`X`代表特征集：

```py
y=result['Purchased']
X=result.drop(columns=['Purchased'])
```

`X`代表特征向量，包含我们需要用来训练模型的所有输入变量。

# 将数据集分为测试和训练部分

现在，让我们使用`sklearn.model_selection import train_test_split`将训练数据集分为 25%的测试部分和 75%的训练部分：

```py
#from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)
```

这已经创建了以下四个数据结构：

+   `X_train`：包含训练数据特征的数据结构

+   `X_test`：包含训练测试特征的数据结构

+   `y_train`：包含训练数据集中标签值的向量

+   `y_test`：包含测试数据集中标签值的向量

# 缩放特征

对于许多机器学习算法，将变量从`0`到`1`进行缩放是一个好的做法。这也被称为**特征归一化**。让我们应用缩放转换来实现这一点：

```py
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
```

在我们缩放数据之后，它准备好作为输入用于我们将在后续部分中介绍的不同分类器。

# 评估分类器

模型训练完成后，我们需要评估其性能。为此，我们将使用以下过程：

1.  我们将标签数据集分为两部分——训练部分和测试部分。我们将使用测试部分来评估训练好的模型。

1.  我们将使用测试部分的特征来为每一行生成标签。这是我们的预测标签集。

1.  我们将比较预测标签集与实际标签以评估模型。

除非我们试图解决的问题非常琐碎，否则在评估模型时会有一些错误分类。我们如何解释这些错误分类以确定模型的质量取决于我们选择使用的性能指标。

一旦我们有了实际标签集和预测标签集，就可以使用一系列性能指标来评估模型。用于量化模型的最佳指标将取决于我们想要解决的业务问题的要求，以及训练数据集的特征。

# 混淆矩阵

混淆矩阵用于总结对分类器的评估结果。二元分类器的混淆矩阵如下所示：

![](img/ca9d79e4-5e30-4bf9-b81f-d0aee047b206.png)如果我们正在训练的分类器的标签有两个级别，则称为**二元分类器**。监督机器学习的第一个关键用例，特别是二元分类器，是在第一次世界大战期间用于区分飞机和飞行鸟。

分类可以分为以下四类：

+   **真正例**（**TP**）：正确分类的正分类

+   **真负例**（**TN**）：正确分类的负分类

+   **假正例**（**FP**）：实际上是负分类的正分类

+   **假阴性**（**FN**）：实际上是积极的负面分类

让我们看看如何使用这四个类别来创建各种性能指标。

# 性能指标

性能指标用于量化训练模型的性能。基于此，让我们定义以下四个指标：

| **指标** | **公式** |
| --- | --- |
| 准确率 | ![](img/6732024e-647e-40b8-8965-e0e10573db8d.png) |
| 召回率 | ![](img/52d52b33-c954-4bc6-ac31-325849e8a6e7.png) |
| 精度 | ![](img/985538cb-41f1-4fcf-93a4-8da6a3fecf13.png) |
| F1 分数 | ![](img/e50cf3cc-6542-4e86-82dc-39e65afcba92.png) |

准确率是所有预测中正确分类的比例。在计算准确率时，我们不区分 TP 和 TN。通过准确率评估模型是直接的，但在某些情况下，它不起作用。

让我们看看我们需要更多的东西来量化模型的性能的情况。其中之一是当我们使用模型来预测罕见事件时，比如以下的例子：

+   一个用于预测银行交易数据库中欺诈交易的模型

+   一个用于预测飞机发动机零部件机械故障可能性的模型

在这两个例子中，我们试图预测罕见事件。在这种情况下，比准确率更重要的是召回率和精度。让我们逐个来看：

+   **召回率**：这计算了命中率。在前面的例子中，它是模型成功标记的欺诈文件占所有欺诈文件的比例。如果在我们的测试数据集中有 100 万笔交易，其中有 100 笔被确认为欺诈交易，模型能够识别出 78 笔。在这种情况下，召回率值将是 78/100。

+   **精度**：精度衡量了模型标记的交易中实际上是坏的交易有多少。我们不是专注于模型未能标记的坏交易，而是想确定模型标记的坏交易有多精确。

请注意，F1 分数将召回率和精度结合在一起。如果一个模型的精度和召回率都是完美的，那么它的 F1 分数将是完美的。高 F1 分数意味着我们训练了一个高质量的模型，具有高召回率和精度。

# 理解过拟合

如果一个机器学习模型在开发环境中表现出色，但在生产环境中明显下降，我们说这个模型是过拟合的。这意味着训练模型过于密切地遵循训练数据集。这表明模型创建的规则中有太多细节。模型方差和偏差之间的权衡最能捕捉到这个概念。让我们逐个来看这些概念。

# 偏差

任何机器学习模型都是基于某些假设进行训练的。一般来说，这些假设是对一些真实世界现象的简化近似。这些假设简化了特征和特征特性之间的实际关系，并使模型更容易训练。更多的假设意味着更多的偏差。因此，在训练模型时，更简化的假设=高偏差，更符合实际现象的现实假设=低偏差。

在线性回归中，忽略了特征的非线性，并将它们近似为线性变量。因此，线性回归模型天生容易表现出高偏差。

# 方差

方差量化了模型在使用不同数据集训练时对目标变量的估计准确性。它量化了我们的模型的数学公式是否是底层模式的良好概括。

基于特定情景和情况的特定过拟合规则=高方差，而基于广泛情景和情况的泛化规则=低方差。

我们在机器学习中的目标是训练表现出低偏差和低方差的模型。实现这一目标并不总是容易的，通常会让数据科学家夜不能寐。

# 偏差-方差权衡

在训练特定的机器学习模型时，很难确定训练模型所包含的规则的正确泛化级别。为了找到正确的泛化级别而进行的挣扎被称为偏差-方差权衡。

请注意，更简化的假设=更泛化=低方差=高方差。

偏差和方差之间的权衡是由算法的选择、数据的特征和各种超参数决定的。根据您尝试解决的具体问题的要求，重要的是在偏差和方差之间取得正确的折衷。

# 指定分类器的阶段

一旦标记的数据准备好，分类器的开发包括训练、评估和部署。在以下图表中，CRISP-DM（数据挖掘的跨行业标准流程）生命周期展示了实施分类器的这三个阶段（CRISP-DM 生命周期在第五章*，图形算法中有更详细的解释）

![](img/0d74d428-cad1-4af4-bf70-f6eecb3aba00.png)

在实施分类器的前两个阶段——测试和训练阶段，我们使用标记的数据。标记的数据被分成两个分区——一个更大的分区称为训练数据，一个更小的分区称为测试数据。使用随机抽样技术将输入的标记数据分成训练和测试分区，以确保两个分区都包含一致的模式。请注意，如前图所示，首先是训练阶段，使用训练数据来训练模型。训练阶段结束后，使用测试数据评估训练模型。不同的性能指标用于量化训练模型的性能。评估模型后，我们有模型部署阶段，其中训练好的模型被部署并用于推理，通过标记未标记的数据解决现实世界的问题。

现在，让我们看一些分类算法。

我们将在接下来的部分中看到以下分类算法：

+   决策树算法

+   XGBoost 算法

+   随机森林算法

+   逻辑回归算法

+   支持向量机（SVM）算法

+   朴素贝叶斯算法

让我们从决策树算法开始。

# 决策树分类算法

决策树基于递归分区方法（分而治之），生成一组规则，可用于预测标签。它从根节点开始，分成多个分支。内部节点表示对某个属性的测试，测试的结果由分支到下一级表示。决策树以包含决策的叶节点结束。当分区不再改善结果时，过程停止。

# 理解决策树分类算法

决策树分类的显著特点是生成可解释的层次规则，用于在运行时预测标签。该算法具有递归性质。创建这些规则层次涉及以下步骤：

1.  **找到最重要的特征**：在所有特征中，算法确定了最能区分训练数据集中数据点的特征。计算基于信息增益或基尼不纯度等指标。

1.  **分叉**：使用最重要的特征，算法创建一个标准，用于将训练数据集分成两个分支：

+   通过满足标准的数据点

+   未通过标准的数据点

1.  **检查叶节点**：如果任何结果分支大多包含一个类的标签，则该分支被确定为最终分支，形成一个叶节点。

1.  **检查停止条件并重复**：如果未满足提供的停止条件，则算法将返回到*步骤 1*进行下一次迭代。否则，模型被标记为已训练，并且结果决策树的每个最低级节点都被标记为叶节点。停止条件可以简单地定义为迭代次数，或者可以使用默认的停止条件，即一旦每个叶节点达到一定的同质性水平，算法就会停止。

决策树算法可以用以下图解释：

![](img/53f88b67-009e-48f6-9389-b487fe7c0388.png)

在上图中，根节点包含一堆圆圈和十字。该算法创建了一个标准，试图将圆圈与十字分开。在每个级别，决策树创建数据的分区，预期从第 1 级开始越来越同质。完美的分类器只包含只包含圆圈或十字的叶节点。由于训练数据集固有的随机性，训练完美的分类器通常很困难。

# 使用决策树分类算法进行分类器挑战

现在，让我们使用决策树分类算法来解决我们之前定义的常见问题，预测客户最终是否购买产品：

1.  首先，让我们实例化决策树分类算法，并使用我们为分类器准备的训练部分数据来训练模型：

```py
classifier = sklearn.tree.DecisionTreeClassifier(criterion = 'entropy', random_state = 100, max_depth=2)
classifier.fit(X_train, y_train)
```

1.  现在，让我们使用我们训练好的模型来预测我们标记数据的测试部分的标签。让我们生成一个可以总结我们训练好的模型性能的混淆矩阵：

```py
import sklearn.metrics as metrics
y_pred = classifier.predict(X_test)
cm = metrics.confusion_matrix(y_test, y_pred)
cm
```

这给出了以下输出：

![](img/7588760f-69c4-4470-ade7-85255d5e67f7.png)

1.  现在，让我们通过使用决策树分类算法来计算所创建分类器的`准确率`、`召回率`和`精确度`值：

```py
accuracy= metrics.accuracy_score(y_test,y_pred)
recall = metrics.recall_score(y_test,y_pred)
precision = metrics.precision_score(y_test,y_pred)
print(accuracy,recall,precision)
```

1.  运行上述代码将产生以下输出：

![](img/a7a7e80c-e144-4e7c-8c1b-98cc1cec5fef.png)

性能指标帮助我们比较不同的训练建模技术。

# 决策树分类器的优势和劣势

在本节中，让我们看看使用决策树分类算法的优势和劣势。

# 优势

以下是决策树分类器的优势：

+   使用决策树算法创建的模型的规则可被人类解释。这样的模型被称为**白盒模型**。白盒模型是在需要追踪决策的细节和原因时的必要条件。这种透明性在我们想要防止偏见和保护脆弱社区的应用中至关重要。例如，在政府和保险行业的关键用例中，通常需要白盒模型。

+   决策树分类器旨在从离散问题空间中提取信息。这意味着大多数特征都是类别变量，因此使用决策树来训练模型是一个不错的选择。

# 劣势

以下是决策树分类器的弱点：

+   如果决策树分类器生成的树太深，规则会捕捉太多细节，导致过拟合的模型。在使用决策树算法时，我们需要意识到决策树容易过拟合，因此我们需要及时修剪树以防止这种情况。

+   决策树分类器的一个弱点是它们无法捕捉规则中的非线性关系。

# 用例

在本节中，让我们看看决策树算法用于哪些用例。

# 分类记录

决策树分类器可用于对数据点进行分类，例如以下示例：

+   **抵押贷款申请**：训练一个二元分类器，以确定申请人是否可能违约。

+   **客户细分**：将客户分类为高价值、中价值和低价值客户，以便为每个类别定制营销策略。

+   **医学诊断**：训练一个分类器，可以对良性或恶性生长进行分类。

+   **治疗效果分析**：训练一个分类器，可以标记对特定治疗产生积极反应的患者。

# 特征选择

决策树分类算法选择一小部分特征来创建规则。当特征数量很大时，可以使用该特征选择来选择另一个机器学习算法的特征。

# 理解集成方法

集成是一种机器学习方法，通过使用不同的参数创建多个略有不同的模型，然后将它们组合成一个聚合模型。为了创建有效的集成，我们需要找到我们的聚合标准，以生成最终模型。让我们看看一些集成算法。

# 使用 XGBoost 算法实现梯度提升

XGBoost 于 2014 年创建，基于梯度提升原理。它已成为最受欢迎的集成分类算法之一。它生成一堆相互关联的树，并使用梯度下降来最小化残差误差。这使其非常适合分布式基础设施，如 Apache Spark，或云计算，如 Google Cloud 或**亚马逊网络服务（AWS）**。

现在让我们看看如何使用 XGBoost 算法实现梯度提升：

1.  首先，我们将实例化 XGBClassfier 分类器，并使用数据的训练部分来训练模型：

![](img/27a8fe49-3854-4c12-84f2-e71c0f64e430.png)

1.  然后，我们将基于新训练的模型生成预测：

```py
y_pred = classifier.predict(X_test)
cm = metrics.confusion_matrix(y_test, y_pred)
cm
```

产生以下输出：

![](img/3d7d4afa-c3ae-4ae3-a2d2-7ab69b67dcdd.png)

1.  最后，我们将量化模型的性能：

```py
accuracy= metrics.accuracy_score(y_test,y_pred)
recall = metrics.recall_score(y_test,y_pred)
precision = metrics.precision_score(y_test,y_pred)
print(accuracy,recall,precision)
```

这给我们以下输出：

![](img/beaac243-597f-4f5d-b1ec-022f89f01584.png)

接下来，让我们看看随机森林算法。

# 使用随机森林算法

随机森林是一种集成方法，通过组合多个决策树来减少偏差和方差。

# 训练随机森林算法

在训练中，该算法从训练数据中获取*N*个样本，并创建我们整体数据的*m*个子集。这些子集是通过随机选择输入数据的一些行和列来创建的。该算法构建*m*个独立的决策树。这些分类树由`C[1]`到`C[m]`表示。

# 使用随机森林进行预测

模型训练完成后，可以用于标记新数据。每个个体树生成一个标签。最终预测由这些个体预测的投票决定，如下所示：

![](img/89d3a5b6-24cc-4fbd-b922-90865ca6b739.png)

请注意，在上图中，训练了*m*棵树，表示为`C[1]`到`C[m]`。即树 = {C[1],..,C[m]}

每棵树生成一个由一组表示的预测：

个体预测 = P= {P[1],..., P[m]}

最终预测由`P[f]`表示。它由个体预测的大多数决定。`mode`函数可用于找到多数决定（`mode`是最常重复且处于多数的数字）。个体预测和最终预测如下所示：

P[f] = mode (P)

# 区分随机森林算法和集成提升

随机森林算法生成的每棵树都是完全独立的。它不知道集成中其他树的任何细节。这使它与其他技术有所不同，如集成增强。

# 使用随机森林算法进行分类器挑战

让我们实例化随机森林算法，并使用它来训练我们的模型使用训练数据。

这里有两个关键的超参数：

+   `n_estimators`

+   `max_depth`

`n_estimators`超参数控制构建多少个独立的决策树，`max_depth`超参数控制每个独立决策树可以有多深。

换句话说，决策树可以不断分裂，直到它有一个节点代表训练集中的每个给定示例。通过设置`max_depth`，我们限制了它可以进行多少级别的分裂。这控制了模型的复杂性，并确定了它与训练数据的拟合程度。如果我们参考以下输出，`n_estimators`控制了随机森林模型的宽度，`max_depth`控制了模型的深度：

![](img/d4ee0995-7c13-4508-bf5c-5fa23929a1be.png)

一旦随机森林模型训练好了，让我们用它进行预测：

```py
y_pred = classifier.predict(X_test)
cm = metrics.confusion_matrix(y_test, y_pred)
cm
```

它的输出是：

![](img/374d8a18-c24f-44c3-a475-e19c54bc4329.png)

现在，让我们量化我们的模型有多好：

```py
accuracy= metrics.accuracy_score(y_test,y_pred)
recall = metrics.recall_score(y_test,y_pred)
precision = metrics.precision_score(y_test,y_pred)
print(accuracy,recall,precision)
```

我们将观察以下输出：

![](img/ac3d4ce9-a599-4064-a93f-4742874af7b4.png)

接下来，让我们来看看逻辑回归。

# 逻辑回归

逻辑回归是一种用于二元分类的分类算法。它使用逻辑函数来制定输入特征和目标变量之间的交互。它是用于建模二元因变量的最简单的分类技术之一。

# 假设

逻辑回归假设以下内容：

+   训练数据集没有缺失值。

+   标签是一个二进制类别变量。

+   标签是有序的，换句话说，是一个具有有序值的分类变量。

+   所有特征或输入变量彼此独立。

# 建立关系

对于逻辑回归，预测值计算如下：

![](img/57472f0a-fed2-46a3-8f85-82a58abe001c.png)

假设![](img/57ba1e44-a466-4007-99ec-5e7c03a7d299.png)。

所以现在：

![](img/5a9b351d-1683-44cf-8324-c8897ee20680.png)

上述关系可以用图形表示如下：

![](img/c4c1e51c-a610-4fb4-8c21-ba38fed82d20.png)

注意，如果*z*很大，σ (*z*)将等于`1`。如果*z*非常小或非常负，σ (*z*)将等于`0`。因此，逻辑回归的目标是找到*W*和*j*的正确值。

逻辑回归是根据用于制定它的函数命名的，称为**逻辑**或**Sigmoid 函数**。

# 损失和成本函数

`loss`函数定义了我们想要量化训练数据中特定示例的错误的方式。`cost`函数定义了我们想要最小化整个训练数据集中的错误的方式。因此，`loss`函数用于训练数据集中的一个示例，`cost`函数用于量化实际值和预测值的整体偏差。它取决于*w*和*h*的选择。

逻辑回归中使用的`loss`函数如下：

*Loss (ý^((i)), y^((i))) = - (y^((i))log ý^((i))+(1-y^((i)) ) log (1-ý^((i)))*

注意当*y^((i))  = 1, Loss(ý^((i)), y^((i))**) = - logý^((i))*.最小化损失将导致ý^((i))的值很大。作为 Sigmoid 函数，最大值将是`1`。

如果*y^((i)) = 0, Loss (ý^((i)), y^((i))) = - log (1-ý^((i))**)*。最小化损失将导致*ý^((i))*尽可能小，即`0`。

逻辑回归的成本函数如下：

![](img/41a6378a-1b02-4912-8fcd-0279b7e7fc45.png)

# 何时使用逻辑回归

逻辑回归在二元分类器方面表现出色。当数据量很大但数据质量不佳时，逻辑回归效果不佳。它可以捕捉不太复杂的关系。虽然它通常不会产生最佳性能，但它确实为起步设定了一个很好的基准。

# 使用逻辑回归算法进行分类器挑战

在本节中，我们将看到如何使用逻辑回归算法进行分类器挑战：

1.  首先，让我们实例化一个逻辑回归模型，并使用训练数据对其进行训练：

```py
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train, y_train)
```

1.  让我们预测`test`数据的值并创建一个混淆矩阵：

```py

y_pred = classifier.predict(X_test)
cm = metrics.confusion_matrix(y_test, y_pred)
cm
```

运行上述代码后，我们得到以下输出：

![](img/e6d66de7-957c-43d0-83a6-bc2b2f334b6d.png)

1.  现在，让我们看看性能指标：

```py
accuracy= metrics.accuracy_score(y_test,y_pred)
recall = metrics.recall_score(y_test,y_pred)
precision = metrics.precision_score(y_test,y_pred)
print(accuracy,recall,precision)
```

1.  运行上述代码后，我们得到以下输出：

![](img/31737825-b84d-423e-8416-c6885f175eb1.png)

接下来，让我们看看**SVM**。

# SVM 算法

现在，让我们看看 SVM。SVM 是一种找到最大化两个类之间间隔的最优超平面的分类器。在 SVM 中，我们的优化目标是最大化间隔。间隔被定义为分隔超平面（决策边界）与最靠近该超平面的训练样本之间的距离，称为**支持向量**。因此，让我们从一个只有两个维度*X1*和*X2*的非常基本的例子开始。我们希望有一条线将圆圈与十字分开。如下图所示：

![](img/9830448e-e681-400a-9017-57888b5df6da.png)

我们画了两条线，都完美地将十字与圆圈分开。然而，必须有一个最佳线或决策边界，使我们有最佳机会正确分类大多数额外的例子。一个合理的选择可能是一条均匀分布在这两个类之间的线，为每个类提供一点缓冲，如下所示：

![](img/710cb98c-c09e-42bb-a173-1e392b0f8d5b.png)

现在，让我们看看如何使用 SVM 来训练我们挑战的分类器。

# 使用 SVM 算法进行分类器挑战

1.  首先，让我们实例化 SVM 分类器，然后使用标记数据的训练部分对其进行训练。`kernel`超参数确定应用于输入数据的转换类型，以使其线性可分。

```py
from sklearn.svm import SVC
classifier = SVC(kernel = 'linear', random_state = 0)
classifier.fit(X_train, y_train)
```

1.  训练完成后，让我们生成一些预测并查看混淆矩阵：

```py
y_pred = classifier.predict(X_test)
cm = metrics.confusion_matrix(y_test, y_pred)
cm
```

1.  观察以下输出：

![](img/9e0d3b4b-2ba9-48ee-8999-692c32d334b5.png)

1.  现在，让我们来看看各种性能指标：

```py
accuracy= metrics.accuracy_score(y_test,y_pred)
recall = metrics.recall_score(y_test,y_pred)
precision = metrics.precision_score(y_test,y_pred)
print(accuracy,recall,precision)
```

运行上述代码后，我们得到以下值作为输出：

![](img/64c827cb-bf53-453d-96f1-cb1d34c01db9.png)

# 理解朴素贝叶斯算法

基于概率论，朴素贝叶斯是最简单的分类算法之一。如果使用正确，它可以得出准确的预测。朴素贝叶斯算法之所以被如此命名有两个原因：

+   它基于一个天真的假设，即特征和输入变量之间是独立的。

+   它基于贝叶斯定理。

该算法试图基于先前属性/实例的概率对实例进行分类，假设属性完全独立。

有三种类型的事件：

+   **独立**事件不会影响另一个事件发生的概率（例如，收到一封电子邮件提供免费参加科技活动的机会*和*公司进行重新组织）。

+   **依赖**事件会影响另一个事件发生的概率；也就是说，它们在某种程度上是相关的（例如，你准时参加会议的概率可能会受到航空公司员工罢工或航班不准时的影响）。

+   **互斥**事件不能同时发生（例如，单次掷骰子得到三和六的概率为 0——这两个结果是互斥的）。

# 贝叶斯定理

贝叶斯定理用于计算两个独立事件*A*和*B*之间的条件概率。事件*A*和*B*发生的概率由 P（*A*）和 P（*B*）表示。条件概率由 P（*B*|*A*）表示，这是事件*A*发生的条件概率，假设事件*B*已经发生：

![](img/c3857335-8ad7-48cb-bc9d-d0a1a132aebf.png)

# 计算概率

朴素贝叶斯基于概率基本原理。单个事件发生的概率（观察概率）是通过将事件发生的次数除以可能导致该事件发生的总进程次数来计算的。例如，呼叫中心每天接到 100 多个支持电话，一个月内有 50 次。您想知道基于以前的响应时间，呼叫在 3 分钟内得到响应的概率。如果呼叫中心在 27 次匹配这个时间记录，那么 100 次呼叫在 3 分钟内得到响应的观察概率如下：

* P（3 分钟内 100 个支持电话）=（27/50）= 0.54（54％）*

根据过去的 50 次记录，100 次呼叫大约有一半的时间可以在 3 分钟内得到响应。

# AND 事件的乘法规则

要计算两个或更多事件同时发生的概率，请考虑事件是独立还是相关的。如果它们是独立的，则使用简单的乘法规则：

* P（结果 1 和结果 2）= P（结果 1）* P（结果 2）*

例如，要计算收到免费参加技术活动的电子邮件的概率*和*工作场所发生重新组织的概率，将使用这个简单的乘法规则。这两个事件是独立的，因为其中一个发生并不影响另一个发生的机会

如果收到技术活动的电子邮件的概率为 31％，并且员工重新组织的概率为 82％，则同时发生的概率如下计算：

P（电子邮件和重新组织）= P（电子邮件）* P（重新组织）=（0.31）*（0.82）= 0.2542（25％）

# 一般乘法规则

如果两个或更多事件是相关的，则使用一般乘法规则。这个公式实际上在独立和相关事件的情况下都是有效的：

* P（结果 1 和结果 2）= P（结果 1）* P（结果 2 | 结果 1）*

请注意，`P（结果 2 | 结果 1）`指的是`结果 1`已经发生的情况下`结果 2`发生的条件概率。该公式包含了事件之间的依赖关系。如果事件是独立的，那么条件概率是无关紧要的，因为一个结果不会影响另一个发生的机会，`P（结果 2 | 结果 1）`就是`P（结果 2）`。请注意，在这种情况下，该公式变成了简单的乘法规则。

# OR 事件的加法规则

在计算两个事件中的一个或另一个发生的概率（互斥）时，使用以下简单的加法规则：

* P（结果 1 或结果 2）= P（结果 1）+ P（结果 2）*

例如，掷出 6 或 3 的概率是多少？要回答这个问题，首先注意到两个结果不能同时发生。掷出 6 的概率是（1/6），掷出 3 的概率也是如此：

* P（6 或 3）=（1/6）+（1/6）= 0.33（33％）*

如果事件不是互斥的并且可以同时发生，请使用以下一般加法公式，这在互斥和非互斥的情况下都是有效的：

* P（结果 1 或结果 2）= P（结果 1）+ P（结果 2）P（结果 1 和结果 2）*

# 使用朴素贝叶斯算法进行分类器挑战

现在，让我们使用朴素贝叶斯算法来解决分类器挑战：

1.  首先，我们导入`GaussianNB（）`函数并用它来训练模型：

```py
from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)
```

1.  现在，让我们使用训练好的模型来预测结果。我们将用它来预测我们的测试分区`X_test`的标签：

```py
Predicting the Test set results
y_pred = classifier.predict(X_test)
cm = metrics.confusion_matrix(y_test, y_pred)
cm
```

1.  现在，让我们打印混淆矩阵：

![](img/61ad873a-a502-41be-8d3b-433d21072c96.png)

1.  现在，让我们打印性能矩阵来量化我们训练模型的质量：

```py
accuracy= metrics.accuracy_score(y_test,y_pred)
recall = metrics.recall_score(y_test,y_pred)
precision = metrics.precision_score(y_test,y_pred)
print(accuracy,recall,precision)
```

这将产生以下输出：

![](img/7cc3ecd6-99b2-426d-9e47-d434e60e3d13.png)

# 对于分类算法，获胜者是...

让我们看一下我们提出的各种算法的性能指标。这在下表中总结如下：

| **算法** | **准确度** | **召回率** | **精确度** |
| --- | --- | --- | --- |
| 决策树 | 0.94 | 0.93 | 0.88 |
| XGBoost | 0.93 | 0.90 | 0.87 |
| 随机森林 | 0.93 | 0.90 | 0.87 |
| 逻辑回归 | 0.91 | 0.81 | 0.89 |
| 支持向量机 | 0.89 | 0.71 | 0.92 |
| 朴素贝叶斯 | 0.92 | 0.81 | 0.92 |

从前面的表中可以看出，决策树分类器在准确性和召回率方面表现最佳。如果我们寻求精确度，那么支持向量机和朴素贝叶斯之间存在平局，因此任何一个都适用于我们。

# 了解回归算法

监督机器学习模型使用回归算法之一，如果目标变量是连续变量。在这种情况下，机器学习模型被称为回归器。

在本节中，我们将介绍各种可用于训练监督机器学习回归模型的算法，或者简单地说，回归器。在我们深入了解算法的细节之前，让我们首先为这些算法创建一个挑战，以测试它们的性能、能力和有效性。

# 呈现回归器挑战

与分类算法使用的方法类似，我们将首先提出一个问题，作为所有回归算法的挑战来解决。我们将把这个共同的问题称为回归器挑战。然后，我们将使用三种不同的回归算法来解决这个挑战。使用一个共同的挑战来测试不同的回归算法有两个好处：

+   我们可以准备一次数据，然后在所有三个回归算法上使用准备好的数据。

+   我们可以以有意义的方式比较三种回归算法的性能，因为我们将使用它们来解决同一个问题。

让我们看一下挑战的问题陈述。

# 回归器挑战的问题陈述

预测不同车辆的里程数在当今是很重要的。高效的车辆对环境有益，也具有成本效益。里程数可以根据发动机功率和车辆特性来估算。让我们为回归器创建一个挑战，训练一个能够根据车辆特性预测车辆的**每加仑英里数**（**MPG**）的模型。

让我们看看我们将用来训练回归器的历史数据集。

# 探索历史数据集

以下是我们拥有的历史数据集数据的特征：

| **名称** | **类型** | **描述** |
| --- | --- | --- |
| `名称` | 类别 | 标识特定车辆 |
| `CYLINDERS` | 连续 | 气缸数量（4 至 8 之间） |
| `DISPLACEMENT` | 连续 | 发动机排量（立方英寸） |
| `HORSEPOWER` | 连续 | 发动机马力 |
| `ACCELERATION` | 连续 | 从 0 到 60 英里/小时的加速时间（秒） |

这个问题的目标变量是一个连续变量，`MPG`，它指定了每辆车的英里数。

让我们首先为这个问题设计数据处理管道。

# 使用数据处理管道进行特征工程

让我们看看如何设计一个可重复使用的处理管道来解决回归器挑战。如前所述，我们将一次准备数据，然后在所有回归算法中使用它。让我们按照以下步骤进行：

1.  我们首先导入数据集，如下所示：

```py
dataset = pd.read_csv('auto.csv')
```

1.  现在让我们预览数据集：

```py
dataset.head(5)
```

数据集将如下所示：

![](img/c62f1a01-274e-4d93-b005-cd688ca4d630.png)

1.  现在，让我们继续进行特征选择。让我们删除 `NAME` 列，因为它只是一个用于汽车的标识符。用于识别数据集中行的列对于训练模型是不相关的。让我们删除这一列：

```py
dataset=dataset.drop(columns=['NAME'])
```

1.  让我们转换所有的输入变量并填充所有的空值：

```py
dataset=dataset.drop(columns=['NAME'])
dataset= dataset.apply(pd.to_numeric, errors='coerce')
dataset.fillna(0, inplace=True)
```

填充提高了数据的质量，并准备好用于训练模型。现在，让我们看最后一步：

1.  让我们将数据分成测试和训练分区：

```py
from sklearn.model_selection import train_test_split
#from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)
```

这创建了以下四个数据结构：

+   `X_train`：包含训练数据的特征的数据结构

+   `X_test`：包含训练测试的特征的数据结构

+   `y_train`：包含训练数据集中标签的值的向量

+   `y_test`：包含测试数据集中标签的值的向量

现在，让我们使用准备好的数据在三个不同的回归器上，以便比较它们的性能。

# 线性回归

在所有监督学习技术中，线性回归算法是最容易理解的。我们首先看一下简单线性回归，然后将概念扩展到多元线性回归。

# 简单线性回归

在其最简单的形式中，线性回归阐述了单个连续自变量和单个连续自变量之间的关系。回归用于显示因变量（显示在 *y* 轴上）的变化程度可以归因于解释变量（显示在 *x* 轴上）的变化程度。它可以表示如下：

![](img/a1ca60f3-867b-410e-b01f-f8f00c083695.png)

这个公式可以解释如下：

+   *y* 是因变量。

+   *X* 是自变量。

+   ![](img/4dc02d52-61ac-4dc5-ade3-3868a299cd92.png) 是斜率，表示每增加一个 *X*，线就上升多少。

+   *α* 是截距，表示 *X* = 0 时 *y* 的值。

单个连续因变量和单个连续自变量之间关系的一些例子如下：

+   一个人的体重和他们的卡路里摄入量

+   特定社区房屋价格和面积

+   空气中的湿度和下雨的可能性

对于线性回归，输入（自变量）和目标（因变量）变量都必须是数值型的。最佳关系是通过最小化每个点到通过所有点的线的垂直距离的平方和来找到的。假设预测变量和目标变量之间是线性关系。例如，投入研发的资金越多，销售额就越高。

让我们看一个具体的例子。让我们尝试阐明特定产品的营销支出和销售之间的关系。它们被发现直接相关。营销支出和销售在二维图上绘制，并显示为蓝色的钻石。这种关系最好通过绘制一条直线来近似，如下图所示：

![](img/4308ca96-fec7-4de9-96f0-f700a67e5f77.png)

一旦画出线性线，我们就可以看到营销支出和销售之间的数学关系。

# 评估回归器

我们画的线性线是因变量和自变量之间关系的近似值。即使最佳线也会与实际值有一些偏差，如下所示：

![](img/71d38f55-1568-4a86-ba02-4681a035c983.png)

评估线性回归模型性能的一种典型方法是使用**均方根误差**（**RMSE**）。这通过数学计算训练模型产生的误差的标准偏差。对于训练数据集中的某个示例，`loss` 函数计算如下：

损失（ý^((i)), y^((i))) = 1/2(ý^((i)-) y^((i)))²

这导致以下`cost`函数，最小化训练集中所有示例的损失：

![](img/25cd8116-2626-4182-a7c5-fb22681dd6b4.png)

让我们尝试解释 RMSE。如果我们的示例模型的 RMSE 为$50，这意味着大约 68.2%的预测值将在真实值（即*α*）的$50 范围内。这也意味着 95%的预测值将在实际值的$100（即 2*α*）范围内。最后，99.7%的预测值将在实际值的$150 范围内。

# 多元回归

事实上，大多数现实世界的分析都有多个自变量。多元回归是简单线性回归的扩展。关键区别在于额外的预测变量有额外的 beta 系数。在训练模型时，目标是找到最小化线性方程误差的 beta 系数。让我们尝试数学上阐述因变量和一组自变量（特征）之间的关系。

与简单线性方程类似，因变量*y*被量化为截距项的总和，加上*β*系数乘以每个*i*特征的*x*值：

y = α + β  [1]  x  [1]  + β  [2]  x 2 +...+ β  [i]  x  [i]  + ε

误差用*ε*表示，表明预测并不完美。

*β*系数允许每个特征对*y*的值有单独的估计影响，因为*y*每增加一个单位的*x*[*i*]，*y*的变化量为*β  [i]*。此外，截距（*α*）表示当独立变量都为 0 时*y*的期望值。

请注意，前述方程中的所有变量都可以用一堆向量表示。目标和预测变量现在是带有行的向量，而回归系数*β*和误差*ε*也是向量。

# 使用线性回归算法进行回归挑战

现在，让我们使用数据集的训练部分来训练模型：

1.  让我们从导入线性回归包开始：

```py
from sklearn.linear_model import LinearRegression
```

1.  然后，让我们实例化线性回归模型，并使用训练数据集对其进行训练：

```py
regressor = LinearRegression()
regressor.fit(X_train, y_train)
```

1.  现在，让我们使用数据集的测试部分来预测结果：

```py
y_pred = regressor.predict(X_test)
from sklearn.metrics import mean_squared_error
from math import sqrt
sqrt(mean_squared_error(y_test, y_pred))
```

1.  运行上述代码生成的输出将生成以下内容：

![](img/10b80cc4-475a-4fc7-9c53-ab9cad02c6e7.png)

如前一节所讨论的，RMSE 是误差的标准差。它表明 68.2%的预测值将在目标变量值的`4.36`范围内。

# 何时使用线性回归？

线性回归用于解决许多现实世界的问题，包括以下内容：

+   销售预测

+   预测最佳产品价格

+   量化事件和响应之间的因果关系，例如临床药物试验、工程安全测试或市场研究

+   识别可用于预测未来行为的模式，给定已知条件，例如预测保险索赔、自然灾害损失、选举结果和犯罪率

# 线性回归的弱点

线性回归的弱点如下：

+   它只适用于数值特征。

+   分类数据需要进行预处理。

+   它无法很好地处理缺失数据。

+   它对数据做出假设。

# 回归树算法

回归树算法类似于分类树算法，只是目标变量是连续变量，而不是类别变量。

# 使用回归树算法进行回归挑战

在本节中，我们将看到如何使用回归树算法进行回归挑战：

1.  首先，我们使用回归树算法训练模型：

![](img/2e89f7e5-0fd7-4b78-82ee-d60ba4e2b15d.png)

1.  一旦回归树模型训练完成，我们就可以使用训练好的模型来预测值：

```py
y_pred = regressor.predict(X_test)
```

1.  然后，我们计算 RMSE 来量化模型的性能：

```py
from sklearn.metrics import mean_squared_error
from math import sqrt
sqrt(mean_squared_error(y_test, y_pred))
```

我们得到以下输出：

![](img/c5716e34-aa3b-4b39-b5b2-88b8ab4419c0.png)

# 梯度提升回归算法

现在，让我们来看看梯度提升回归算法。它使用一组决策树来更好地表达数据中的潜在模式。

# 使用梯度提升回归算法来解决回归问题

在这一部分，我们将看到如何使用梯度提升回归算法来解决回归问题：

1.  首先，我们使用梯度提升回归算法来训练模型：

![](img/c59155c9-6344-4272-b33e-826d814ed78f.png)

1.  一旦梯度回归算法模型被训练，我们就可以用它来预测数值：

```py
y_pred = regressor.predict(X_test)
```

1.  最后，我们计算 RMSE 来量化模型的性能：

```py
from sklearn.metrics import mean_squared_error
from math import sqrt
sqrt(mean_squared_error(y_test, y_pred))
```

1.  运行这个将给我们输出值，如下所示：

![](img/bd5c2be4-d453-4b08-90be-dbe55b9e1dc9.png)

# 对于回归算法，获胜者是...

让我们来看看我们在相同数据和完全相同用例上使用的三种回归算法的表现：

| **算法** | **RMSE** |
| --- | --- |
| 线性回归 | 4.36214129677179 |
| 回归树 | 5.2771702288377 |
| 梯度提升回归 | 4.034836373089085 |

从所有回归算法的表现来看，很明显梯度提升回归的表现最好，因为它具有最低的 RMSE。其次是线性回归。对于这个问题，回归树算法的表现最差。

# 实际例子 - 如何预测天气

让我们看看如何使用本章中开发的概念来预测天气。假设我们想根据一年内针对特定城市收集的数据来预测明天是否会下雨。

用于训练该模型的数据在名为`weather.csv`的 CSV 文件中：

1.  让我们将数据导入为一个 pandas 数据框：

```py
import numpy as np 
import pandas as pd
df = pd.read_csv("weather.csv")
```

1.  让我们来看看数据框的列：

![](img/cc8c37ff-f4b0-42ab-8a02-2d6901bbd040.png)

1.  接下来，让我们来看一下`weather.csv`数据的前 13 列的标题：

![](img/5c5f2025-72f7-413f-b7a2-42b3fcfa249b.png)

1.  现在，让我们来看一下`weather.csv`数据的最后 10 列：

![](img/825cb777-bb57-4caf-ab05-15d8ab625241.png)

1.  让我们用`x`来代表输入特征。我们将在特征列表中删除`Date`字段，因为在预测的情境下它没有用处。我们还将删除`RainTomorrow`标签：

```py
x = df.drop(['Date','RainTomorrow'],axis=1)
```

1.  让我们用`y`来代表标签：

```py
y = df['RainTomorrow']
```

1.  现在，让我们将数据分成`train_test_split`：

```py
from sklearn.model_selection import train_test_split
train_x , train_y ,test_x , test_y = train_test_split(x,y , test_size = 0.2,random_state = 2)
```

1.  由于标签是一个二元变量，我们正在训练一个分类器。因此，在这里逻辑回归将是一个不错的选择。首先，让我们实例化逻辑回归模型：

```py
model = LogisticRegression()
```

1.  现在，我们可以使用`train_x`和`test_x`来训练模型：

```py
model.fit(train_x , test_x)
```

1.  一旦模型被训练，让我们用它进行预测：

```py
predict = model.predict(train_y)
```

1.  现在，让我们找出我们训练模型的准确性：

![](img/baa59ee9-e705-4a2c-9824-b92d37b0b333.png)

现在，这个二元分类器可以用来预测明天是否会下雨。

# 摘要

在本章中，我们首先了解了监督式机器学习的基础知识。然后，我们更详细地了解了各种分类算法。接下来，我们研究了评估分类器性能的不同方法，并研究了各种回归算法。我们还研究了用于评估我们研究的算法性能的不同方法。

在下一章中，我们将研究神经网络和深度学习算法。我们将研究训练神经网络所使用的方法，还将研究用于评估和部署神经网络的各种工具和框架。
