# 自然语言处理算法

本章介绍了**自然语言处理**（**NLP**）的算法。本章从理论到实践逐步进行。它将首先介绍NLP的基础知识，然后介绍基本算法。然后，它将研究最流行的神经网络之一，该网络被广泛用于设计和实施文本数据的重要用例的解决方案。最后，我们将研究NLP的局限性，最后学习如何使用NLP来训练一个可以预测电影评论极性的机器学习模型。

本章将包括以下部分：

+   介绍NLP

+   基于词袋（BoW）的NLP

+   词嵌入介绍

+   使用递归神经网络进行NLP

+   使用NLP进行情感分析

+   案例研究：电影评论情感分析

通过本章结束时，您将了解用于NLP的基本技术。您应该能够理解NLP如何用于解决一些有趣的现实世界问题。

让我们从基本概念开始。

# 介绍NLP

NLP用于研究形式化和规范化计算机与人类（自然）语言之间的交互。NLP是一个综合性的学科，涉及使用计算机语言学算法和人机交互技术和方法来处理复杂的非结构化数据。NLP可以用于各种情况，包括以下情况：

+   **主题识别**：发现文本存储库中的主题，并根据发现的主题对存储库中的文档进行分类

+   **情感分析**：根据文本中包含的积极或消极情感对文本进行分类

+   **机器翻译**：将文本从一种口头人类语言翻译成另一种口头人类语言

+   **文本转语音**：将口头语言转换为文本

+   **主观解释**：智能地解释问题并利用可用信息回答问题

+   **实体识别**：从文本中识别实体（如人、地点或物品）

+   **假新闻检测**：根据内容标记假新闻

让我们首先看一些在讨论NLP时使用的术语。

# 理解NLP术语

NLP是一个综合性的学科。在围绕某一领域的文献中，我们会观察到，有时会使用不同的术语来指定相同的事物。我们将从一些与NLP相关的基本术语开始。让我们从规范化开始，这是一种基本的NLP处理，通常在输入数据上执行。

# 规范化

规范化是对输入文本数据进行的处理，以提高其在训练机器学习模型的情况下的质量。规范化通常包括以下处理步骤：

+   将所有文本转换为大写或小写

+   去除标点符号

+   去除数字

请注意，尽管通常需要前面的处理步骤，但实际的处理步骤取决于我们想要解决的问题。它们会因用例而异，例如，如果文本中的数字代表了在我们尝试解决的问题的情境中可能具有一些价值的东西，那么我们在规范化阶段可能就不需要从文本中去除数字。

# 语料库

我们用来解决问题的输入文档组称为**语料库**。语料库充当NLP问题的输入数据。

# 标记化

当我们使用NLP时，第一项工作是将文本分成一个标记列表。这个过程称为**标记化**。由于目标的不同，生成的标记的粒度也会有所不同，例如，每个标记可以包括以下内容：

+   一个词

+   一组单词的组合

+   一个句子

+   一个段落

# 命名实体识别

在NLP中，有许多用例需要从非结构化数据中识别特定的单词和数字，这些单词和数字属于预定义的类别，如电话号码、邮政编码、姓名、地点或国家。这用于为非结构化数据提供结构。这个过程称为**命名实体识别**（**NER**）。

# 停用词

在单词级别的标记化之后，我们得到了文本中使用的单词列表。其中一些单词是常见单词，预计几乎会出现在每个文档中。这些单词不会为它们出现在的文档提供任何额外的见解。这些单词被称为**停用词**。它们通常在数据处理阶段被移除。一些停用词的例子是*was*、*we*和*the*。

# 情感分析

情感分析，或者称为意见挖掘，是从文本中提取正面或负面情感的过程。

# 词干提取和词形还原

在文本数据中，大多数单词可能以稍微不同的形式存在。将每个单词减少到其原始形式或词干所属的词族中称为**词干提取**。它用于根据它们的相似含义对单词进行分组，以减少需要分析的单词总数。基本上，词干提取减少了问题的整体条件性。

例如，{use, used, using, uses} => use。

英语词干提取的最常见算法是波特算法。

词干提取是一个粗糙的过程，可能会导致词尾被截断。这可能导致拼写错误的单词。对于许多用例来说，每个单词只是我们问题空间中的一个级别的标识符，拼写错误的单词并不重要。如果需要正确拼写的单词，那么应该使用词形还原而不是词干提取。

算法缺乏常识。对于人类大脑来说，将类似的单词视为相同是很简单的。对于算法，我们必须引导它并提供分组标准。

从根本上讲，有三种不同的NLP实现方法。这三种技术在复杂性方面有所不同，如下所示：

+   基于词袋模型（Bo**W**-based）的NLP

+   传统的NLP分类器

+   使用深度学习进行自然语言处理

# NLTK

**自然语言工具包**（**NLTK**）是Python中处理NLP任务最广泛使用的包。NLTK是用于NLP的最古老和最流行的Python库之一。NLTK非常好，因为它基本上为构建任何NLP流程提供了一个起点，它为您提供了基本工具，然后您可以将它们链接在一起以实现您的目标，而不是从头开始构建所有这些工具。许多工具都打包到了NLTK中，在下一节中，我们将下载该包并探索其中的一些工具。

让我们来看看基于词袋模型的NLP。

# 基于词袋模型的NLP

将输入文本表示为一组标记的过程称为**基于词袋模型的处理**。使用词袋模型的缺点是我们丢弃了大部分语法和标记化，这有时会导致丢失单词的上下文。在词袋模型的方法中，我们首先量化要分析的每个文档中每个单词的重要性。

从根本上讲，有三种不同的方法来量化每个文档中单词的重要性：

+   **二进制**：如果单词出现在文本中，则特征的值为1，否则为0。

+   **计数**：特征将以单词在文本中出现的次数作为其值，否则为0。

+   **词项频率/逆文档频率**：特征的值将是单个文档中单词的独特程度与整个文档语料库中单词的独特程度的比率。显然，对于常见单词，如the、in等（称为停用词），**词项频率-逆文档频率**（**TF-IDF**）得分将很低。对于更独特的单词，例如领域特定术语，得分将更高。

请注意，通过使用词袋模型，我们丢失了信息——即文本中单词的顺序。这通常有效，但可能会导致准确性降低。

让我们看一个具体的例子。我们将训练一个模型，可以将餐厅的评论分类为负面或正面。输入文件是一个结构化文件，其中评论将被分类为正面或负面。

为此，让我们首先处理输入数据。

处理步骤在下图中定义：

![](assets/70cf5585-8206-45a8-9b86-8aa31f914b37.png)

让我们通过以下步骤实现这个处理流程：

1.  首先，让我们导入我们需要的包：

```py
import numpy as np
import pandas as pd
```

1.  然后我们从`CSV`文件中导入数据集：

![](assets/99fd1e80-976c-43b6-bf61-3e878651d157.png)

1.  接下来，我们清理数据：

```py
# Cleaning the texts
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
corpus = []
for i in range(0, 1000):
    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])
    review = review.lower()
    review = review.split()
    ps = PorterStemmer()
    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
    review = ' '.join(review)
    corpus.append(review)
```

1.  现在让我们定义特征（用`y`表示）和标签（用`X`表示）：

```py
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features = 1500)
X = cv.fit_transform(corpus).toarray()
y = dataset.iloc[:, 1].values
```

1.  让我们将数据分成测试数据和训练数据：

```py
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)
```

1.  对于训练模型，我们使用朴素贝叶斯算法：

```py
from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)
```

1.  让我们预测测试集的结果：

```py
y_pred = classifier.predict(X_test)
```

1.  混淆矩阵如下所示：

![](assets/d3c55c12-09ac-460b-a3c2-915340b89103.png)

通过观察混淆矩阵，我们可以估计误分类情况。

# 词嵌入简介

在前面的部分中，我们学习了如何使用词袋模型作为输入文本数据的抽象来执行NLP。NLP的一个主要进展是我们能够以密集向量的形式创建单词的有意义的数值表示能力。这种技术称为词嵌入。Yoshua Bengio首次在他的论文《神经概率语言模型》中引入了这个术语。NLP问题中的每个词都可以被视为一个分类对象。将每个词映射到表示为向量的数字列表称为词嵌入。换句话说，用于将单词转换为实数的方法称为词嵌入。嵌入的一个区别特征是它使用密集向量，而不是使用传统方法使用稀疏矩阵向量。

使用词袋模型进行NLP存在两个基本问题：

+   **语义上下文的丢失**：当我们对数据进行标记化时，它的上下文就丢失了。一个词可能根据它在句子中的使用位置有不同的含义；当解释复杂的人类表达时，比如幽默或讽刺，这变得更加重要。

+   **稀疏输入**：当我们进行标记化时，每个单词都成为一个特征。正如我们在前面的例子中看到的，每个单词都是一个特征。这导致了稀疏的数据结构。

# 一个词的邻域

如何向算法呈现文本数据（特别是单词或词元）的关键见解来自语言学。在词嵌入中，我们关注每个词的邻域，并用它来确定其含义和重要性。一个词的邻域是围绕特定词的一组词。一个词的上下文是由它的邻域决定的。

请注意，在词袋模型中，一个词失去了它的上下文，因为它的上下文来自它所在的邻域。

# 词嵌入的特性

良好的词嵌入具有以下四个特性：

+   **它们是密集的**：实际上，嵌入本质上是因子模型。因此，嵌入向量的每个组件代表一个（潜在）特征的数量。通常我们不知道该特征代表什么；但是，我们将有非常少的（如果有的话）零值，这将导致稀疏输入。

+   **它们是低维的**：嵌入具有预定义的维度（作为超参数选择）。我们之前看到，在BoW表示中，我们需要为每个单词输入|*V*|，因此输入的总大小为|*V*| * *n*，其中*n*是我们用作输入的单词数。使用单词嵌入，我们的输入大小将是*d* * *n*，其中*d*通常在50到300之间。考虑到大型文本语料库通常远大于300个单词，这意味着我们在输入大小上有很大的节省，我们看到这可能导致更小的数据实例总数的更高准确性。

+   **它们嵌入领域语义**：这个属性可能是最令人惊讶的，但也是最有用的。当正确训练时，嵌入会学习关于其领域的含义。

+   **易于泛化**：最后，网络嵌入能够捕捉到一般的抽象模式——例如，我们可以对（嵌入的）猫、鹿、狗等进行训练，模型将理解我们指的是动物。请注意，模型从未接受过对羊的训练，但模型仍然会正确分类它。通过使用嵌入，我们可以期望得到正确的答案。

现在让我们探讨一下，我们如何使用RNN进行自然语言处理。

# 使用RNN进行NLP

RNN是一个具有反馈的传统前馈网络。对RNN的一种简单思考方式是，它是一个带有状态的神经网络。RNN可用于任何类型的数据，用于生成和预测各种数据序列。训练RNN模型是关于构建这些数据序列。RNN可用于文本数据，因为句子只是单词序列。当我们将RNN用于NLP时，我们可以用它来进行以下操作：

+   在输入时预测下一个单词

+   生成新的文本，遵循文本中已经使用的风格：

![](assets/8aba997b-e588-4c5d-8307-d33b6ad5c15b.png)

还记得导致它们正确预测的单词组合吗？RNN的学习过程是基于语料库中的文本。它们通过减少预测的下一个单词和实际的下一个单词之间的错误来进行训练。

# 使用NLP进行情感分析

本节介绍的方法是基于对分类高速流推文的使用情况。手头的任务是提取关于所选主题的推文中嵌入的情绪。情感分类实时量化每条推文中的极性，然后聚合所有推文的总情感，以捕捉关于所选主题的整体情感。为了应对Twitter流数据的内容和行为带来的挑战，并有效地执行实时分析，我们使用NLP使用训练过的分类器。然后将训练过的分类器插入Twitter流中，以确定每条推文的极性（积极、消极或中性），然后聚合并确定关于某一主题的所有推文的整体极性。让我们一步一步地看看这是如何完成的。

首先，我们必须训练分类器。为了训练分类器，我们需要一个已经准备好的数据集，其中包含有历史的Twitter数据，并且遵循实时数据的模式和趋势。因此，我们使用了来自网站[www.sentiment140.com](http://www.sentiment140.com/)的数据集，该数据集带有一个人工标记的语料库（基于该分析的大量文本集合），其中包含超过160万条推文。该数据集中的推文已经被标记为三种极性之一：零表示负面，两表示中性，四表示正面。除了推文文本之外，语料库还提供了推文ID、日期、标志和推文用户。现在让我们看看在*训练*分类器之前对实时推文执行的每个操作：

1.  首先将推文分割成称为标记的单词（标记化）。

1.  标记化的输出创建了一个BoW，其中包含文本中的单个单词。

1.  这些推文进一步通过去除数字、标点和停用词（停用词去除）进行过滤。停用词是非常常见的词，如*is*、*am*、*are*和*the*。由于它们没有额外的信息，这些词被移除。

1.  此外，非字母字符，如*#**@*和数字，使用模式匹配进行删除，因为它们在情感分析的情况下没有相关性。正则表达式用于仅匹配字母字符，其余字符将被忽略。这有助于减少Twitter流的混乱。

1.  先前阶段的结果被用于词干处理阶段。在这个阶段，派生词被减少到它们的词根-例如，像*fish*这样的词与*fishing*和*fishes*具有相同的词根。为此，我们使用标准NLP库，它提供各种算法，如Porter词干处理。

1.  一旦数据被处理，它被转换成一个称为**术语文档矩阵**（**TDM**）的结构。TDM表示过滤后语料库中每个词的术语和频率。

1.  从TDM中，推文到达训练过的分类器（因为它经过训练，可以处理推文），它计算每个词的**情感极性重要性**（**SPI**），这是一个从-5到+5的数字。正负号指定了该特定词所代表的情绪类型，其大小表示情感的强度。这意味着推文可以被分类为正面或负面（参考下图）。一旦我们计算了个别推文的极性，我们将它们的总体SPI相加，以找到来源的聚合情感-例如，总体极性大于一表示我们观察时间内推文的聚合情感是积极的。

为了获取实时原始推文，我们使用Scala库*Twitter4J*，这是一个提供实时Twitter流API包的Java库。该API要求用户在Twitter上注册开发者帐户并填写一些认证参数。该API允许您获取随机推文或使用选择的关键词过滤推文。我们使用过滤器来检索与我们选择的关键词相关的推文。

总体架构如下图所示：

![](assets/3dc16321-97c8-4e5e-b815-3584e3e61f97.png)

情感分析有各种应用。它可以用来分类客户的反馈。政府可以利用社交媒体极性分析来找到他们政策的有效性。它还可以量化各种广告活动的成功。

在接下来的部分，我们将学习如何实际应用情感分析来预测电影评论的情感。

# 案例研究：电影评论情感分析

让我们使用NLP进行电影评论情感分析。为此，我们将使用一些开放的电影评论数据，可在[http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)上找到：

1.  首先，我们将导入包含电影评论的数据集：

```py
import numpy as np
import pandas as pd
```

1.  现在，让我们加载电影数据并打印前几行以观察其结构。

```py
df=pd.read_csv("moviereviews.tsv",sep='\t')
df.head()
```

![](assets/d613c48f-67c9-44c0-97b0-db32f368deae.png)

请注意数据集有`2000`条电影评论。其中一半是负面的，一半是正面的。

1.  现在，让我们开始准备数据集以训练模型。首先，让我们删除数据中的任何缺失值

```py
df.dropna(inplace=True)
```

1.  现在我们需要移除空格。空格不是空的，但需要被移除。为此，我们需要遍历输入`DataFrame`中的每一行。我们将使用`.itertuples()`来访问每个字段：

```py
blanks=[] 

for i,lb,rv in df.itertuples():
    if rv.isspace():
        blanks.append(i)
df.drop(blanks,inplace=True)      
```

请注意，我们已经使用`i`，`lb`和`rv`来索引、标签和评论列。

让我们将数据分割成测试和训练数据集：

1.  第一步是指定特征和标签，然后将数据分割成训练集和测试集：

```py
from sklearn.model_selection import train_test_split

X = df['review']
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
```

现在我们有测试和训练数据集。

1.  现在让我们将数据集分成训练集和测试集：

```py
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

# Naïve Bayes:
text_clf_nb = Pipeline([('tfidf', TfidfVectorizer()),
                     ('clf', MultinomialNB()),
])
```

请注意，我们正在使用`tfidf`来量化集合中数据点的重要性。

接下来，让我们使用朴素贝叶斯算法来训练模型，然后测试训练好的模型。

让我们按照以下步骤来训练模型：

1.  现在让我们使用我们创建的测试和训练数据集来训练模型：

```py
text_clf_nb.fit(X_train, y_train)
```

1.  让我们运行预测并分析结果：

```py
# Form a prediction set
predictions = text_clf_nb.predict(X_test)
```

让我们通过打印混淆矩阵来查看模型的性能。我们还将查看*精确度*、*召回率*、*F1分数*和*准确度*。

![](assets/774aa10e-3479-403d-a3a5-949c7179ab32.png)

这些性能指标为我们提供了预测质量的度量。准确率为0.78，现在我们已经成功训练了一个可以预测特定电影评论类型的模型。

# 摘要

在本章中，我们讨论了与自然语言处理相关的算法。首先，我们研究了与自然语言处理相关的术语。接下来，我们研究了实施自然语言处理策略的BoW方法。然后，我们研究了词嵌入的概念以及在自然语言处理中使用神经网络。最后，我们看了一个实际的例子，我们在这一章中使用了开发的概念来根据电影评论的文本来预测情感。通过学习本章内容，用户应该能够将自然语言处理用于文本分类和情感分析。

在下一章中，我们将研究推荐引擎。我们将研究不同类型的推荐引擎以及它们如何用于解决一些现实世界的问题。
