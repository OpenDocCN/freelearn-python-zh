# 数据算法

本章讨论了数据中心算法的三个方面：存储、流式处理和压缩。本章首先简要概述了数据中心算法，然后讨论了可以用于数据存储的各种策略。接下来，描述了如何将算法应用于流式数据，然后讨论了压缩数据的不同方法。最后，我们将学习如何使用本章中开发的概念来使用最先进的传感器网络监测高速公路上行驶车辆的速度。

通过本章的学习，您应该能够理解设计各种数据中心算法所涉及的概念和权衡。

本章讨论了以下概念：

+   数据分类

+   数据存储算法

+   如何使用算法来压缩数据

+   如何使用算法来流式处理数据

让我们首先介绍基本概念。

# 数据算法简介

无论我们是否意识到，我们生活在一个大数据时代。只需了解一下不断产生的数据量，看一下谷歌在2019年发布的一些数字就可以了。众所周知，Google Photos是谷歌创建的存储照片的多媒体库。2019年，平均每天有12亿张照片和视频上传到Google Photos。此外，每天平均有400小时的视频（相当于1 PB的数据）上传到YouTube。我们可以肯定地说，正在产生的数据量简直是爆炸式增长。

当前对数据驱动算法的兴趣是因为数据包含有价值的信息和模式。如果以正确的方式使用，数据可以成为政策制定、营销、治理和趋势分析的基础。

由于处理数据的算法变得越来越重要，有明显的原因。设计能够处理数据的算法是一个活跃的研究领域。毫无疑问，探索最佳利用数据以提供一些可量化的好处是全世界各种组织、企业和政府的关注焦点。但原始形式的数据很少有用。要从原始数据中挖掘信息，需要对其进行处理、准备和分析。

为此，我们首先需要将其存储在某个地方。高效存储数据的方法变得越来越重要。请注意，由于单节点系统的物理存储限制，大数据只能存储在由高速通信链路连接的多个节点组成的分布式存储中。因此，对于学习数据算法来说，首先看不同的数据存储算法是有意义的。

首先，让我们将数据分类为各种类别。

# 数据分类

让我们来看看在设计数据算法的背景下如何对数据进行分类。正如在[第2章](04672393-683c-406b-8dd1-4dab5b5d9c4f.xhtml)中讨论的那样，*算法中使用的数据结构*，量化数据的容量、多样性和速度可以用来对其进行分类。这种分类可以成为设计数据算法的基础，用于其存储和处理。

让我们在数据算法的背景下逐个查看这些特征：

+   **容量** 量化了需要在算法中存储和处理的数据量。随着容量的增加，任务变得数据密集，并需要足够的资源来存储、缓存和处理数据。大数据是一个模糊定义的术语，用来描述无法由单个节点处理的大量数据。

+   **速度**定义了新数据生成的速率。通常，高速数据被称为“热数据”或“热流”，低速数据被称为“冷流”或简单地称为“冷数据”。在许多应用中，数据将是热流和冷流的混合，首先需要准备并合并到一个表中，然后才能与算法一起使用。

+   **多样性**指的是需要将不同类型的结构化和非结构化数据合并到一个表中，然后才能被算法使用。

下一节将帮助我们理解涉及的权衡，并在设计存储算法时提出各种设计选择。

# 数据存储算法介绍

可靠和高效的数据存储库是分布式系统的核心。如果这个数据存储库是为分析而创建的，那么它也被称为数据湖。数据存储库将来自不同领域的数据汇集到一个地方。让我们首先了解分布式存储库中与数据存储相关的不同问题。

# 了解数据存储策略

在数字计算的最初几年，设计数据存储库的常规方式是使用单节点架构。随着数据集的不断增大，数据的分布式存储现在已经成为主流。在分布式环境中存储数据的正确策略取决于数据的类型、预期的使用模式以及其非功能性需求。为了进一步分析分布式数据存储的需求，让我们从**一致性可用性分区容忍（CAP）**定理开始，这为我们提供了制定分布系统数据存储策略的基础。

# 介绍CAP定理

1998年，Eric Brewer提出了一个定理，后来被称为CAP定理。它突出了设计分布式存储系统涉及的各种权衡。

为了理解CAP定理，首先让我们定义分布式存储系统的以下三个特性：一致性、可用性和分区容忍。CAP实际上是由这三个特性组成的首字母缩写：

+   **一致性**（简称C）：分布式存储由各种节点组成。这些节点中的任何一个都可以用于读取、写入或更新数据存储库中的记录。一致性保证在某个时间*t[1]*，无论我们使用哪个节点来读取数据，我们都会得到相同的结果。每个*读*操作要么返回跨分布式存储库一致的最新数据，要么给出错误消息。

+   **可用性**（简称A）：可用性保证分布式存储系统中的任何节点都能立即处理请求，无论是否具有一致性。

+   **分区容忍**（简称P）：在分布式系统中，多个节点通过通信网络连接。分区容忍保证在一小部分节点（一个或多个）之间的通信失败的情况下，系统仍然可以正常运行。请注意，为了保证分区容忍，数据需要在足够数量的节点上复制。

使用这三种特性，CAP定理仔细总结了分布系统的架构和设计中涉及的权衡。具体来说，CAP定理规定，在存储系统中，我们只能拥有以下两种特性中的两种：一致性或C，可用性或A，以及分区容忍性或P。

这在以下图表中显示：

![](assets/aed77c3c-5f64-4ccd-9799-36306f2bb941.png)

CAP定理也意味着我们可以有三种类型的分布式存储系统：

+   CA系统（实现一致性-可用性）

+   AP系统（实现可用性-分区容忍）

+   CP系统（实现一致性-分区容忍）

让我们依次来看看它们。

# CA系统

传统的单节点系统是CA系统。这是因为如果我们没有分布式系统，那么我们就不需要担心分区容忍性。在这种情况下，我们可以拥有既有一致性又有可用性的系统，即CA系统。

传统的单节点数据库，如Oracle或MySQL，都是CA系统的例子。

# AP系统

AP系统是为可用性调整的分布式存储系统。设计为高度响应的系统，它们可以牺牲一致性，以适应高速数据。这意味着这些是设计为立即处理用户请求的分布式存储系统。典型的用户请求是读取或写入快速变化的数据。典型的AP系统用于实时监控系统，如传感器网络。

高速分布式数据库，如Cassandra，是AP系统的良好例子。

让我们看看AP系统可以在哪些地方使用。如果加拿大交通部想要通过在渥太华一条高速公路上安装的传感器网络监控交通情况，建议使用AP系统来实现分布式数据存储。

# CP系统

CP系统具有一致性和分区容忍性。这意味着这些是调整为在读取过程可以获取值之前保证一致性的分布式存储系统。

CP系统的典型用例是当我们想要以JSON格式存储文档文件时。像MongoDB这样的文档数据存储系统是为分布式环境中的一致性而调整的CP系统。

分布式数据存储越来越成为现代IT基础设施中最重要的部分。分布式数据存储应该根据数据的特性和我们想要解决的问题的要求进行精心设计。将数据存储分类为CA、AP和CP系统有助于我们理解在设计数据存储系统时涉及的各种权衡。

现在，让我们来看看流数据算法。

# 呈现流数据算法

数据可以被分类为有界或无界。有界数据是静态数据，通常通过批处理过程处理。流式处理基本上是对无界数据进行数据处理。让我们看一个例子。假设我们正在分析银行的欺诈交易。如果我们想要查找7天前的欺诈交易，我们必须查看静态数据；这是一个批处理的例子。

另一方面，如果我们想要实时检测欺诈，那就是流式处理的一个例子。因此，流数据算法是处理数据流的算法。其基本思想是将输入数据流分成批次，然后由处理节点处理。流算法需要具有容错能力，并且应该能够处理数据的传入速度。随着对实时趋势分析的需求增加，对流处理的需求也在这些天增加。请注意，为了使流处理工作，数据必须快速处理，而在设计算法时，这一点必须始终牢记在心。

# 流应用

流数据及其有意义的利用有许多应用。一些应用如下：

+   欺诈检测

+   系统监控

+   智能订单路由

+   实时仪表板

+   高速公路上的交通传感器

+   信用卡交易

+   用户在多用户在线游戏中移动

现在，让我们看看如何使用Python实现流处理。

# 呈现数据压缩算法

数据压缩算法参与了减小数据大小的过程。

在这一章中，我们将深入研究一种名为无损压缩算法的特定数据压缩算法。

# 无损压缩算法

这些算法能够以一种可以在解压缩时不丢失信息的方式压缩数据。当重要的是在解压缩后检索到确切的原始文件时，它们被使用。无损压缩算法的典型用途如下：

+   压缩文件

+   压缩和打包源代码和可执行文件

+   将大量小文件转换为少量大文件

# 了解无损压缩的基本技术

数据压缩是基于这样一个原则，即大多数数据使用的位数比其熵所指示的最佳位数多。回想一下，熵是一个用来指定数据所携带信息的术语。这意味着可以有更优化的位表示相同信息。探索和制定更有效的位表示成为设计压缩算法的基础。无损数据压缩利用这种冗余来压缩数据而不丢失任何信息。在80年代后期，Ziv和Lempel提出了基于字典的数据压缩技术，可以用于实现无损数据压缩。由于其速度和良好的压缩率，这些技术一炮而红。这些技术被用于创建流行的基于Unix的*compress*工具。此外，普遍存在的`gif`图像格式使用了这些压缩技术，因为它们可以用较少的位数表示相同的信息，节省了空间和通信带宽。这些技术后来成为开发`zip`实用程序及其变体的基础。调制解调器中使用的压缩标准V.44也是基于它。

现在让我们逐一查看即将到来的部分中的技术。

# Huffman编码

Huffman编码是压缩数据的最古老方法之一，它基于创建Huffman树，用于对数据进行编码和解码。Huffman编码可以通过利用某些数据（例如字母表的某些字符）在数据流中更频繁地出现这一事实，以更紧凑的形式表示数据内容。通过使用不同长度的编码（对于最常见的字符较短，对于最不常见的字符较长），数据占用的空间更少。

现在，让我们学习一些与Huffman编码相关的术语：

+   **编码：**在数据的上下文中，编码表示将数据从一种形式表示为另一种形式的方法。我们希望结果形式简洁。

+   **码字：**编码形式中的特定字符称为码字。

+   **固定长度编码：**每个编码字符，即码字，使用相同数量的位。

+   **可变长度编码：**码字可以使用不同数量的位。

+   **代码评估：**这是每个码字的预期位数。

+   **前缀自由码：**这意味着没有码字是任何其他码字的前缀。

+   **解码：**这意味着可变长度编码必须不受任何前缀的限制。

要理解最后两个术语，您首先需要查看此表：

| **字符** | **频率** | **固定长度编码** | **可变长度编码** |
| --- | --- | --- | --- |
| L | .45 | 000 | 0 |
| M | .13 | 001 | 101 |
| N | .12 | 010 | 100 |
| X | .16 | 011 | 111 |
| Y | .09 | 100 | 1101 |
| Z | .05 | 101 | 1100 |

现在，我们可以推断以下内容：

+   **固定长度编码：**该表的固定长度编码为3。

+   **可变长度编码：**该表的可变长度编码为*45(1) + .13(3) + .12(3) + .16(3) + .09(4) + .05(4) = 2.24*。

以下图表显示了从上面的例子创建的Huffman树：

![](assets/87919c43-e9b2-4df9-8d42-adf6b96c29d2.png)

请注意，Huffman编码是将数据转换为Huffman树以实现压缩。解码或解压缩将数据恢复到原始格式。

# 一个实际的例子- Twitter实时情感分析

据说Twitter每秒有近7000条关于各种话题的推文。让我们尝试构建一个情感分析器，可以实时捕捉来自不同新闻来源的新闻的情绪。我们将从导入所需的包开始：

1.  导入所需的包：

```py
import tweepy,json,time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()
```

请注意，我们使用以下两个包：

1.  **VADER**情感分析，代表**Valence Aware Dictionary and Sentiment Reasoner**。这是一种流行的基于规则的情感分析工具，专为社交媒体开发。如果您以前从未使用过它，那么您首先需要运行以下命令：

```py
pip install vaderSentiment
```

1.  `Tweepy`，这是一个用于访问Twitter的基于Python的API。同样，如果您以前从未使用过它，您需要首先运行这个：

```py
pip install Tweepy
```

1.  下一步有点棘手。您需要向Twitter发出请求，创建一个开发者帐户，以获取对推文的实时流的访问权限。一旦您获得API密钥，您可以用以下变量表示它们：

```py
twitter_access_token = <your_twitter_access_token>
twitter_access_token_secret = <your_twitter_access_token_secret>
twitter_consumer_key = <your_consumer_key>
twitter_consumer_secret = <your_twitter_consumer_secret>
```

1.  然后，让我们配置`Tweepy` API的身份验证。为此，我们需要提供先前创建的变量：

```py
auth = tweepy.OAuthHandler(twitter_consumer_key, twitter_consumer_secret)
auth.set_access_token(twitter_access_token, twitter_access_token_secret)
api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
```

1.  现在是有趣的部分。我们将选择我们想要监控情感分析的新闻来源的Twitter句柄。在这个例子中，我们选择了以下新闻来源：

```py
news_sources = ("@BBC", "@ctvnews", "@CNN","@FoxNews", "@dawn_com")
```

1.  现在，让我们创建主循环。这个循环将从一个名为`array_sentiments`的空数组开始，用于保存情感。然后，我们将循环遍历所有五个新闻来源，并收集每个100条推文。然后，对于每条推文，我们将计算其极性：

![](assets/0270c5ec-21f7-4aa3-b229-4544795df1c4.png)

1.  现在，让我们创建一个图表，显示来自这些个别新闻来源的新闻的极性：

![](assets/85779482-ee6d-485a-b4f2-d432095bffba.png)

请注意，每个新闻来源都用不同的颜色表示。

1.  现在，让我们看一下摘要统计信息：

![](assets/8f9fb7d1-bc46-4e29-8c1e-3aa6d7390809.png)

上述数字总结了情感的趋势。例如，BBC的情感被发现是最积极的，而加拿大新闻频道CTVnews似乎带有最消极的情绪。

# 摘要

在本章中，我们研究了以数据为中心的算法的设计。我们关注了数据为中心算法的三个方面：存储、压缩和流式处理。

我们研究了数据特征如何决定数据存储设计。我们研究了两种不同类型的数据压缩算法。然后，我们研究了一个实际示例，说明了如何使用数据流算法来计算文本数据流中的单词计数。

在下一章中，我们将研究加密算法。我们将学习如何利用这些算法的力量来保护交换和存储的消息。
