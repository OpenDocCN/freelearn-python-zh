- en: InterPlanetary - A Brave New File System
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 星际文件系统-一个崭新的文件系统
- en: In this chapter, we are going to learn about the **InterPlanetary File System**
    (**IPFS**). The IPFS is not actually part of the blockchain technology; instead,
    it complements it. IPFS with blockchain is a match made in heaven. As you have
    learned in previous chapters, storage in a blockchain is expensive. Usually, people
    save links to files in a blockchain and save the actual files in normal storage,
    such as cloud storage. But this strategy suffers the fate of centralization. IPFS
    offers blockchain developers a way to avoid this.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习**星际文件系统**（**IPFS**）。IPFS实际上并不是区块链技术的一部分；相反，它是对其的补充。IPFS与区块链是天作之合。正如你在之前的章节中学到的，区块链中的存储是昂贵的。通常，人们在区块链中保存文件的链接，并将实际文件保存在普通存储中，比如云存储。但这种策略遭受了中心化的命运。IPFS为区块链开发者提供了一种避免这种情况的方法。
- en: 'In this chapter, you are going to learn about the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习以下内容：
- en: The motivation behind IPFS
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IPFS背后的动机
- en: Merkle DAG
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merkle DAG
- en: Peer-to-peer networking
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点对点网络
- en: The motivation behind IPFS
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IPFS背后的动机
- en: IPFS is not a normal filesystem, such as `fat32`, `ntfs`, or `ext3`. It is more
    similar to Dropbox. It is a cross-device filesystem. You can save a file in this
    filesystem and people around the world can access it as easily as if the file
    were on their own computer. If Ethereum can be thought of as the world's singleton
    operating system, IPFS can be considered as the world's singleton storage!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: IPFS不是一个普通的文件系统，比如`fat32`，`ntfs`或`ext3`。它更类似于Dropbox。它是一个跨设备的文件系统。你可以把文件保存在这个文件系统中，全世界的人都可以像在自己的电脑上一样轻松地访问它。如果以太坊可以被认为是世界上唯一的操作系统，那么IPFS可以被认为是世界上唯一的存储！
- en: 'The slogan of the IPFS website is *IPFS is the Distributed Web*. IPFS tries
    to replace, or at least supplement, HTTP. The HTTP protocol has served us for
    a long time, over 20 years, but it is not considered sufficient for upcoming challenges,
    such as increasing bandwidth demands or redundancy of files. HTTP uses a client-server
    model. You can only choose one of these two roles: either to be a server or a
    client.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: IPFS网站的口号是*IPFS是分布式网络*。IPFS试图取代，或者至少补充HTTP。HTTP协议已经服务了我们很长时间，超过20年，但它被认为不足以应对即将出现的挑战，比如带宽需求增加或文件冗余。HTTP使用客户端-服务器模型。你只能选择这两种角色中的一种：要么是服务器，要么是客户端。
- en: 'There are a couple of problems with this architecture:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构存在一些问题：
- en: The first problem is that to pick up the server role, we have to have sufficient
    resources. If not, if the server is flooded with a lot of requests, it could go
    down rapidly. The resources required to handle one million requests per minute
    is out of reach for many common people.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个问题是，要担任服务器角色，我们必须有足够的资源。如果没有，如果服务器被大量请求淹没，它可能会迅速崩溃。处理每分钟一百万个请求所需的资源对许多普通人来说是不可及的。
- en: The second problem is that the server-and-client architecture is not efficient
    in some situations. Imagine that you are sat beside a grandma in a park and both
    of you are watching the same video of a cute panda from the same URL (something
    like [https://example.com/cute_panda.mp4](https://example.com/cute_panda.mp4)).
    Let's say that the size of this video is 20 MB. This means the server must send
    a 20 MB file twice to two different locations, even though these two different
    locations are located closely together with a proximity of one meter. In other
    words, the server uses 40 MB of bandwidth. Imagine, however, if you could pull
    the file not from the server, but from the grandma who sits beside you (in this
    case, let's assume grandma has watched the cute panda video two minutes before
    you). Wouldn't this be more efficient?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个问题是，服务器和客户端的架构在某些情况下并不高效。想象一下，你坐在公园里的一位老奶奶旁边，你们两个都在从同一个URL观看一只可爱熊猫的视频（类似[https://example.com/cute_panda.mp4](https://example.com/cute_panda.mp4)）。假设这个视频的大小是20MB。这意味着服务器必须两次发送一个20MB的文件到两个不同的位置，即使这两个不同的位置距离很近，相隔一米。换句话说，服务器使用了40MB的带宽。然而，想象一下，如果你可以不是从服务器，而是从坐在你旁边的老奶奶那里获取文件（在这种情况下，假设老奶奶比你早两分钟观看了这个可爱熊猫的视频）。这不是更高效吗？
- en: Juan Benet was inspired to build IPFS in late 2013\. Back then, he was working
    with knowledge tools, a term that refers to software that can be used to efficiently
    gather knowledge from papers. Let's say, for example, that a scientist reads a
    lot of papers. It would be better if that scientist could get this knowledge faster. Benet
    came across the problem that datasets required too much effort to distribute.
    There was no easy way to handle the versioning of datasets. He looked at various
    tools, such as Git and BitTorrent, and wondered if they could be combined to solve
    this problem. As a result, IPFS was born. BitTorrent inspired IPFS with regard
    to distributing files and finding files among the nodes. Git inspired IPFS with
    regard to keeping the integrity of files and converting saved files into storage.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Juan Benet在2013年末受到启发建立了IPFS。当时，他正在研究知识工具，这个术语指的是可以有效地从论文中获取知识的软件。比如，一个科学家读了很多论文。如果科学家能更快地获取这些知识就更好了。Benet遇到的问题是数据集分发需要太多的工作。没有简单的方法来处理数据集的版本控制。他看了各种工具，比如Git和BitTorrent，并想知道它们是否可以结合起来解决这个问题。结果，IPFS诞生了。BitTorrent启发了IPFS在节点之间分发文件和查找文件。Git启发了IPFS保持文件的完整性和将保存的文件转换为存储。
- en: IPFS is a peer-to-peer hypermedia protocol that makes the web faster, safer,
    and more open. The goal of IPFS is pragmatic and idealistic. Besides saving bandwidth,
    another of its aims is to increase the longevity of a file. Keeping a file in
    a server for a very long time (such as a decade) requires a huge amount of resources.
    The reason why we might want a file to stay alive is usually because it has some
    kind of economic benefit for the owner of the server; for example, it could be
    monetized with ads if it is a blog post. If not, there is a possibility that the
    file will be destroyed by the owner of the storage server. This happened when
    Geocities was shut down.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: IPFS是一个点对点的超媒体协议，使网络更快、更安全、更开放。IPFS的目标是务实的和理想化的。除了节省带宽，它的另一个目标是增加文件的寿命。在服务器上保存文件很长时间（比如十年）需要大量的资源。我们希望文件保持存活的原因通常是因为它对服务器所有者有某种经济利益；例如，如果是博客文章，可以通过广告实现货币化。如果不是，文件有可能会被存储服务器的所有者销毁。这就是当Geocities关闭时发生的情况。
- en: Geocities was a website that allowed people to create their own personal website.
    It was similar to [wordpress.com](http://wordpress.com) and [medium.com](http://medium.com).
    Some owners of servers would keep files alive even without ads, like Wikipedia,
    which works thanks to donations. Other than that, however, the files are not so
    lucky.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Geocities是一个允许人们创建自己个人网站的网站。它类似于[wordpress.com](http://wordpress.com)和[medium.com](http://medium.com)。一些服务器所有者会保持文件存活，即使没有广告，就像维基百科一样，它靠捐赠维持运转。除此之外，文件就没有那么幸运了。
- en: The other goals of IPFS are more idealistic and involved democratizing how we
    provide content. Right now, content is heavily centralized. We usually go to just
    a few websites, such as Facebook, Instagram, Reddit, Medium, Netflix, Amazon,
    Google, Wikipedia, and so on. This oligopoly of information hinders innovation
    on the internet because information is controlled literally by a few companies.
    Apart from Wikipedia, most, if not all, companies are beholden to rich shareholders.
    This situation is in stark contrast to 10 years ago, when the internet was considered
    a great equalizer of wealth and information, similar to printing press technology.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: IPFS的其他目标更加理想化，涉及到如何使我们提供内容的方式更加民主化。目前，内容非常集中。我们通常只会去一些网站，比如Facebook、Instagram、Reddit、Medium、Netflix、Amazon、Google、Wikipedia等等。这种信息的寡头垄断阻碍了互联网的创新，因为信息实际上是由少数几家公司控制的。除了维基百科，大多数，如果不是全部，公司都受到富有的股东的约束。这种情况与10年前形成了鲜明对比，当时互联网被认为是财富和信息的伟大均等化者，类似于印刷技术。
- en: The other disadvantage of this heavy centralization is that the information
    that's provided is susceptible to censorship. For example, Google is a company
    based in Mountain View, California, and is therefore subject to US law. Most people
    who have the power to make decisions (senior executives and C-levels) are American
    and therefore have an American bias in their perception of the world. Things that
    are fine in most countries in Europe could be censored in the name of American
    morals. This could include content that is disliked by the state because it is
    considered blasphemous or dangerous. The founder of the IPFS project likened this
    situation to the case of burning books that were considered dangerous by the state
    or powerful institutions. One of the goals of the IPFS project was to increase
    the resistance of documents to censorship. IPFS makes it easier for people to
    mirror and serve dangerous documents. We'll discuss how IPFS achieves this goal
    in a later section of this chapter.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这种重度集中化的另一个缺点是提供的信息容易受到审查。例如，Google是一家位于加利福尼亚州山景城的公司，因此受美国法律约束。大多数有权做决定的人（高级管理人员和C级别人员）都是美国人，因此在他们对世界的看法中存在美国偏见。在欧洲大多数国家都是合法的事情，在美国可能因为被认为是亵渎或危险而被审查。IPFS项目的创始人将这种情况比作被国家或强大机构认为危险而被焚烧的书籍。IPFS项目的一个目标是增加文件对审查的抵抗力。IPFS使人们更容易地镜像和提供危险文件。我们将在本章的后面讨论IPFS如何实现这一目标。
- en: The final goal of IPFS, which is more pragmatic, concerns our fragile internet
    infrastructure, which is composed of computer networks and core routers connected
    by fiber-optic cables. If the connecting cable is damaged accidentally or deliberately,
    a block or area could go offline. In 2011, a woman with a shovel damaged the cable
    that brought internet to Armenia when she was digging looking for metal to sell.
    The IPFS project does not solve this problem completely, but it can mitigate the
    damage to some extent.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: IPFS的最终目标更加务实，涉及到我们脆弱的互联网基础设施，由计算机网络和由光纤电缆连接的核心路由器组成。如果连接的电缆意外或故意受损，一个区块或区域可能会离线。2011年，一名女子用铁锹损坏了为亚美尼亚带来互联网的电缆，当时她在挖掘寻找金属出售。IPFS项目并不能完全解决这个问题，但它可以在一定程度上减轻损害。
- en: You can find the incident about the woman and her shovel here: [https://web.archive.org/web/20141225063937/http://www.wsj.com/articles/SB10001424052748704630004576249013084603344.](https://web.archive.org/web/20141225063937/http://www.wsj.com/articles/SB10001424052748704630004576249013084603344)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到关于那位女士和她的铁锹的事件：[https://web.archive.org/web/20141225063937/http://www.wsj.com/articles/SB10001424052748704630004576249013084603344.](https://web.archive.org/web/20141225063937/http://www.wsj.com/articles/SB10001424052748704630004576249013084603344)
- en: Merkle DAG
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Merkle DAG
- en: If you have learned about the internals of Git, Merkle **Directed Acyclic Graph** (**DAG**)
    shouldn't be too foreign. As a version control system software, Git is required
    to keep many versions of a file and distribute them easily to other people. It
    also needs to be able to check the integrity of the file very quickly.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经了解了Git的内部工作原理，Merkle有向无环图（DAG）就不会太陌生。作为一个版本控制系统软件，Git需要保留文件的许多版本并轻松地将它们分发给其他人。它还需要能够快速检查文件的完整性。
- en: 'There are two words that make up Merkle DAG: Merkle and DAG. Let''s discuss
    Merkle first. Actually, the full word of Merkle in this context is Merkle tree.
    A Merkle tree is a fast way to check whether partial data has been tampered with
    or not.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Merkle DAG由两个词组成：Merkle和DAG。让我们先讨论Merkle。实际上，在这个上下文中，Merkle的完整词是Merkle树。 Merkle树是一种快速检查部分数据是否被篡改的方法。
- en: Merkle tree
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Merkle树
- en: 'Let''s take a look at an example of a Merkle tree in order to understand it.
    Let''s say you have eight pieces of data. In this case, we will use the names
    of animals for our data, but in Bitcoin, which uses a Merkle tree, the pieces
    of data are usually transactions. Back to Merkle trees: put the data in order,
    so in this case, cat is the first piece of data, dog is the second, ant is the
    third, and so on:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个Merkle树的例子以便理解它。假设您有八个数据。在这种情况下，我们将使用动物的名称作为我们的数据，但在使用Merkle树的比特币中，数据通常是交易。回到Merkle树：按顺序放置数据，所以在这种情况下，猫是第一条数据，狗是第二条，蚂蚁是第三条，依此类推：
- en: '![](assets/405fb6e7-eda7-4523-b59e-6bbe3c647838.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/405fb6e7-eda7-4523-b59e-6bbe3c647838.png)'
- en: We take the hash of each piece of data, in this case, cat, dog, ant, and so
    on. For this demonstration, we use the hash function SHA256\. Because of limited
    space, we have truncated the full hash result in the diagram. For now, we will
    order the data from left to right, so the hash of the "cat" string is `Data 1`,
    the hash of the "dog" string is `Data 2`, the hash of the "ant" string is `Data
    3`, and so on.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获取每个数据的哈希，例如猫，狗，蚂蚁等。在这个演示中，我们使用哈希函数SHA256。由于空间有限，我们在图中截断了完整的哈希结果。现在，我们将数据从左到右排序，所以“猫”字符串的哈希是`Data
    1`，“狗”字符串的哈希是`Data 2`，“蚂蚁”字符串的哈希是`Data 3`，依此类推。
- en: Here's come the interesting part. For `Data 1` and `Data 2`, we combine the
    hash and hash the result. Combining the hash means concatenating it. Do this for
    `Data 3` and `Data 4`, `Data 5` and `Data 6`, `Data 7` and `Data 8` as well.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是有趣的部分。对于`Data 1`和`Data 2`，我们合并哈希并对结果进行哈希。合并哈希意味着连接它。对`Data 3`和`Data 4`，`Data
    5`和`Data 6`，`Data 7`和`Data 8`也是如此。
- en: This might remind you of a knockout competition. We are now in the semi-final
    phase. We now have `Hash 1` (from `Data 1` and `Data 2`), `Hash 2` (from `Data
    3` and `Data 4`), `Hash 3` (from `Data 5` and `Data 6`), and `Hash 4` (from `Data
    7` and `Data 8`).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能让您想起淘汰赛。我们现在进入半决赛阶段。我们现在有`Hash 1`（来自`Data 1`和`Data 2`），`Hash 2`（来自`Data 3`和`Data
    4`），`Hash 3`（来自`Data 5`和`Data 6`），以及`Hash 4`（来自`Data 7`和`Data 8`）。
- en: We then concatenate `Hash 1` and `Hash 2`, hash the result, and name this `Hash
    5`. We then do the same thing for `Hash 3` and `Hash 4`. Name the result `Hash
    6`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们连接`Hash 1`和`Hash 2`，对结果进行哈希，并将其命名为`Hash 5`。然后我们对`Hash 3`和`Hash 4`做同样的事情。将结果命名为`Hash
    6`。
- en: We are now in the final phase. Combine `Hash 5` and `Hash 6`, then hash the
    result. The result is the `Root Hash`. This `Root Hash` can guarantee the integrity
    of all the pieces of data (from `Data 1` to `Data 8`). If you change any of the
    data, `Root Hash` would be different.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在进入最后阶段。合并`Hash 5`和`Hash 6`，然后对结果进行哈希。结果就是`Root Hash`。这个`Root Hash`可以保证所有数据的完整性（从`Data
    1`到`Data 8`）。如果更改任何数据，`Root Hash`将会不同。
- en: You may be asking why we don't just concatenate all the data (from `Data 1`
    to `Data 8`) from the beginning and then hash the result. It turns out, however,
    that Merkle trees has some benefits over just concatenating all the data together
    and then hashing it (this technique is called a **hash list**, and it is used
    in some situations). One of the benefits is that it is easier and cheaper to check
    the integrity of the partial data when we use a Merkel tree.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会问为什么我们不只是从一开始就连接所有数据（从`Data 1`到`Data 8`）然后对结果进行哈希。然而，事实证明，Merkle树比仅仅连接所有数据然后对其进行哈希（这种技术称为**哈希列表**，在某些情况下使用）具有一些优点。其中一个好处是，当我们使用Merkel树时，检查部分数据的完整性更容易和更便宜。
- en: 'In a Merkle tree, to check the integrity of `Data 5`, you only need to download
    `Data 5`, `Data 6`, `Hash 4`, `Hash 5`, and the `Root Hash`, as shown in the following
    diagram. You don''t need to download all the data:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在Merkle树中，要检查`Data 5`的完整性，您只需要下载`Data 5`，`Data 6`，`Hash 4`，`Hash 5`和`Root Hash`，如下图所示。您不需要下载所有数据：
- en: '![](assets/1dc3424f-2b27-496c-9b53-600829a14d09.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/1dc3424f-2b27-496c-9b53-600829a14d09.png)'
- en: If you use a naive approach, you need to download all the hashes of the data
    (`Data 1` to `Data 8`) and the `Root Hash`. In this example, we only have eight
    pieces of data. Imagine if we had 100 and you had to download the entire dataset.
    Merkle trees makes this process more efficient because we don't need to download
    the full set of data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用天真的方法，您需要下载所有数据的哈希值（`Data 1`到`Data 8`）和`Root Hash`。在这个例子中，我们只有八个数据。想象一下，如果我们有100个数据，你需要下载整个数据集。Merkle树使这个过程更有效，因为我们不需要下载完整的数据集。
- en: If we had an odd number of nodes, such as seven, the general rule (the one that
    Bitcoin implements) is to clone the last node, so `Data 8` is a copy of `Data
    7`. You could use another rule, however; I have seen an implementation of a Merkle
    tree in which a single piece of data (`Data 7` in our example) is simply promoted
    to the top. In this case, `Hash 4` is just `Data 7`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有奇数个节点，比如七个，一般规则（比特币实现的规则）是克隆最后一个节点，所以`Data 8`是`Data 7`的副本。然而，您也可以使用另一条规则；我见过Merkle树的一种实现，其中单个数据（在我们的例子中是`Data
    7`）被简单地提升到顶部。在这种情况下，`Hash 4`就是`Data 7`。
- en: This is what Bitcoin does when people use Simplified Payment Verification. With
    a mobile app, downloading the full node is difficult. In order to send Bitcoin
    transactions, the user downloads only the important parts of the node instead
    of the full node. Merkle tree enables this process.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是当人们使用简化支付验证时比特币所做的事情。使用移动应用程序下载完整节点是困难的。为了发送比特币交易，用户只下载节点的重要部分而不是完整节点。Merkle树使这一过程成为可能。
- en: In the next section, we will move on to learn about DAGs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将继续学习DAGs。
- en: Directive Acrylic Graphs (DAGs)
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指令丙烯酸图（DAGs）
- en: '**Directive Acrylic Graphs** (**DAGs**), as its name suggests, are graphs in
    which each vertex (or node) can have edges pointing to other vertexes, as shown
    in the following diagram:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**指令丙烯酸图**（**DAGs**），顾名思义，是图，其中每个顶点（或节点）都可以有指向其他顶点的边，如下图所示：'
- en: '![](assets/d0bcac5f-5616-4cbb-a1c8-b580ce87596d.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/d0bcac5f-5616-4cbb-a1c8-b580ce87596d.png)'
- en: 'The direction of the arrow does not matter, as long as you make it consistent:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 箭头的方向并不重要，只要保持一致即可：
- en: '![](assets/510e335c-e929-47e0-9d13-cd718836c316.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/510e335c-e929-47e0-9d13-cd718836c316.png)'
- en: 'The rule is that these edges should not make a cycle. In the following figure,
    we can see that vertexes A, C, and D make a cycle, which is against the rules
    of a DAG:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 规则是这些边不应该构成一个循环。在下图中，我们可以看到顶点A、C和D构成一个循环，这违反了DAG的规则：
- en: '![](assets/52c1b720-a13c-45d3-bcd0-109dfaea1cfd.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/52c1b720-a13c-45d3-bcd0-109dfaea1cfd.png)'
- en: Now, if you combine a Merkle tree and DAG, you get a Merkle DAG. This is the
    data structure that is used by Git and IPFS.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你结合Merkle树和DAG，你会得到一个Merkle DAG。这是Git和IPFS使用的数据结构。
- en: In a Merkle tree, only the leaf nodes hold data. In a Merkle DAG, however, any
    node could hold the data. In a Merkle tree, the tree has to be balanced, but there
    is no such limitation in a Merkle DAG.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在Merkle树中，只有叶节点保存数据。然而，在Merkle DAG中，任何节点都可以保存数据。在Merkle树中，树必须是平衡的，但在Merkle DAG中没有这样的限制。
- en: Before we jump into Merkle DAGs, let's learn about content addressing, because
    Merkle DAGs are dependent on this feature.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入Merkle DAG之前，让我们先了解内容寻址，因为Merkle DAG依赖于这个特性。
- en: Content addressing
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内容寻址
- en: In a linked list, you chain together nodes (or blocks) with a pointer. A pointer
    is a data type that points to memory. For example, let's say we have two nodes,
    node A and node B. Node A is the head and node B is the tail. The structure of
    the node has two important components. The first component is the data component
    where you store the data. In Git, this data could be the content of the file.
    The second component is a link to another node. In a linked list, this is the
    pointer to a node's address.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在链表中，你用指针将节点（或块）链接在一起。指针是指向内存的数据类型。例如，假设我们有两个节点，节点A和节点B。节点A是头，节点B是尾。节点的结构有两个重要的组成部分。第一个组成部分是数据组成部分，你在其中存储数据。在Git中，这个数据可以是文件的内容。第二个组成部分是指向另一个节点的链接。在链表中，这是指向节点地址的指针。
- en: But with content addressing, instead of just a pointer, we also add the hash
    of the target (in this case, node B). You may recognize this concept; this is
    exactly what happens in blockchain. A Merkle DAG, however, is not a linked list
    that spans linearly in one straight line. A Merkle DAG is a tree that can have
    branches.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，使用内容寻址时，我们不仅仅添加一个指针，还会添加目标的哈希值（在这种情况下是节点B）。你可能会认出这个概念；这正是区块链中发生的事情。然而，Merkle
    DAG不是一个线性延伸的链表。Merkle DAG是一个可以有分支的树。
- en: 'This is a linked list. It is used in a blockchain''s data structure:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个链表。它用于区块链的数据结构：
- en: '![](assets/dc737437-77bb-4f46-b437-6a5f01b6993b.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dc737437-77bb-4f46-b437-6a5f01b6993b.png)'
- en: 'Now, consider this case. We have three nodes: nodes A1 and A2 are both heads
    that point to node B. Instead of putting the pointers on node A1 and node A2,
    we put the pointers on node B. Node B now has two pointers. Node B hashes nodes
    A1 and A2, then concatenates both hashes before hashing the result again. In this
    way, node B can keep the integrity of the content of node A1 and node A2\. If
    somebody changes the content of node A1 or the content of node A2, the hash kept
    by node B would be invalid:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑这种情况。我们有三个节点：节点A1和A2都是指向节点B的头。我们不是把指针放在节点A1和节点A2上，而是把指针放在节点B上。节点B现在有两个指针。节点B对节点A1和节点A2进行哈希，然后连接两个哈希再次进行哈希。这样，节点B可以保持节点A1和节点A2的内容的完整性。如果有人改变了节点A1或节点A2的内容，节点B保存的哈希将无效：
- en: '![](assets/afdb0bee-6c71-4ad7-bf53-7259f5221866.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/afdb0bee-6c71-4ad7-bf53-7259f5221866.png)'
- en: 'IPFS is different to HTTP in terms of how it fetches a document. HTTP uses
    links, which work like pointers. For example, let''s say we have the following
    link: [https://example.com/cute_panda.png](https://example.com/cute_panda.png).
    This uses a location to fetch a document called `cute_panda.png`. Only one provider
    could serve this document, which is `example.com`. IPFS, however, does not use
    a URL link. Instead, it uses a hash link, such as `ipfs://QmYeAiiK1UfB8MGLRefok1N7vBTyX8hGPuMXZ4Xq1DPyt7`.
    When you access this hash link, the IPFS sofware will find the document that,
    when hashed, will give you the same hash output. Because hashing is a one-way
    function, IPFS must have some other information to locate the document. Basically,
    it broadcasts the request to nodes that are nearby the document that has this
    hash output. If the nearby nodes don''t have these files, they forward the requests
    to their nearby nodes. This peer-finding request is quite complex. IPFS uses S/Kademlia
    Distributed Hash Tables, which we will discuss in a later section of this chapter.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: IPFS在获取文档方面与HTTP不同。HTTP使用链接，它们像指针一样工作。例如，假设我们有以下链接：[https://example.com/cute_panda.png](https://example.com/cute_panda.png)。这使用一个位置来获取名为`cute_panda.png`的文档。只有一个提供者可以提供这个文档，那就是`example.com`。然而，IPFS不使用URL链接。相反，它使用哈希链接，比如`ipfs://QmYeAiiK1UfB8MGLRefok1N7vBTyX8hGPuMXZ4Xq1DPyt7`。当你访问这个哈希链接时，IPFS软件将找到文档，当哈希后，将给出相同的哈希输出。因为哈希是一个单向函数，IPFS必须有一些其他信息来定位文档。基本上，它会将请求广播到附近具有这个哈希输出的文档的节点。如果附近的节点没有这些文件，它们会将请求转发给它们附近的节点。这种对等查找请求非常复杂。IPFS使用S/Kademlia分布式哈希表，我们将在本章的后面部分讨论。
- en: The interesting thing is that when you use content addressing, there may be
    multiple providers that can serve this document. In the case of the `cute_panda.png`
    document, there could be more than four nodes that can serve this document. We
    can pick the nearest node to make the download process more efficient. This property
    also makes censorship much more difficult. In the case of  HTTP, an actor could
    ban the server [https://example.com](https://example.com). In the case of IPFS,
    however, anyone could launch a new node and serve the document. Right now, IPFS
    is transparent, perhaps too much so. The node that requests the document can see
    the IP address of the node serving the document, and vice versa. The actor could
    ban the IP address to forbid this document being spread. The development to make
    IPFS work with Tor, software that allows users to browse websites anonymously, however,
    is still in its early days.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，当你使用内容寻址时，可能会有多个提供者可以提供这个文档。在`cute_panda.png`文档的情况下，可能有四个以上的节点可以提供这个文档。我们可以选择最近的节点来使下载过程更有效率。这个特性也使得审查变得更加困难。在HTTP的情况下，一个行为者可以禁止服务器[https://example.com](https://example.com)。然而，在IPFS的情况下，任何人都可以启动一个新节点并提供文档。现在，IPFS是透明的，也许太过透明。请求文档的节点可以看到提供文档的节点的IP地址，反之亦然。行为者可以禁止IP地址以阻止这个文档的传播。然而，使IPFS与Tor一起工作的开发，Tor是一种允许用户匿名浏览网站的软件，仍处于早期阶段。
- en: If you download a document from [https://example.com/cute_panda.png](https://example.com/cute_panda.png),
    the document that you get at that moment may be different to the document that
    your friend downloaded from the same URL yesterday. It could be that the admin
    of the server changed the document before you downloaded it today.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从[https://example.com/cute_panda.png](https://example.com/cute_panda.png)下载一个文档，那么你在那一刻得到的文档可能与你的朋友昨天从同样的URL下载的文档不同。可能是服务器的管理员在你今天下载之前改变了文档。
- en: With the content addressing system, however, the document that you get from
    the IPFS hash link, `ipfs://QmYeAiiK1UfB8MGLRefok1N7vBTyX8hGPuMXZ4Xq1DPyt7`, will
    always be the same, no matter when or where you download it. This hash link guarantees
    that nobody can tamper with the document. If you change the document and upload
    it to IPFS, the IPFS URL or hash would be different.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通过内容寻址系统，你从IPFS哈希链接`ipfs://QmYeAiiK1UfB8MGLRefok1N7vBTyX8hGPuMXZ4Xq1DPyt7`获取的文档，无论何时何地下载，都是相同的。这个哈希链接保证了没有人可以篡改文档。如果你改变文档并上传到IPFS，IPFS的URL或哈希将会不同。
- en: We can create a simple Python script to illustrate this case. Create a directory
    called `ipfs_tutorial`. Create three sample files in this directory. The first
    sample file is `hello.txt`, which has the content `I am a good boy.\n`. The second
    sample file is `hello2.txt`, which has the content `I am a good girl.\n`. The
    third sample file is `hello3.txt`, which has the content `I am a good horse.\n`.
    The fourth sample file is `hello4.txt`, which has the content `I am a good girl.\n`.
    The fact that the second and fourth files have the same content is deliberate.
    You can create different files, if you wish, but make sure that at least two of
    them have the same content.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一个简单的Python脚本来说明这种情况。在这个目录中创建一个名为`ipfs_tutorial`的目录。在这个目录中创建三个示例文件。第一个示例文件是`hello.txt`，内容是`I
    am a good boy.\n`。第二个示例文件是`hello2.txt`，内容是`I am a good girl.\n`。第三个示例文件是`hello3.txt`，内容是`I
    am a good horse.\n`。第四个示例文件是`hello4.txt`，内容是`I am a good girl.\n`。第二个和第四个文件具有相同的内容是故意的。如果你愿意，你可以创建不同的文件，但请确保其中至少有两个文件具有相同的内容。
- en: 'Create a Python script as shown in the following code block and name it `create_hash_from_content.py`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个Python脚本，如下面的代码块所示，并将其命名为`create_hash_from_content.py`：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This script lists all files in the same directory that have a name that starts
    with `hello`. You can modify this part if your sample files don't start with `hello`.
    The long hash is the hash of the content of `hello2.txt`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本列出了同一目录中所有以`hello`开头的文件。如果你的示例文件不是以`hello`开头的，你可以修改这部分。长哈希是`hello2.txt`内容的哈希值。
- en: 'When you run the script, you will get the following result:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行这个脚本时，你会得到以下结果：
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see, there are four files, but the final output is three, not four.
    This is because there are three files with unique content, not four. This is how
    content addressing works. It does not care about the filename, it only cares about
    the content. It doesn't matter whether the file is called `hello1.txt` or `hello2.txt`
    or `hello4.txt`, it only matters that the content, `I am a good girl.\n`, is the
    same. Technically speaking, this is a **white lie**; there is a situation when
    IPFS must consider the filename and cannot ignore it. I'll explain the truth of
    this matter later in this chapter.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，有四个文件，但最终输出是三个，而不是四个。这是因为有三个文件具有独特的内容，而不是四个。这就是内容寻址的工作原理。它不关心文件名，只关心内容。文件名是`hello1.txt`还是`hello2.txt`或`hello4.txt`都无关紧要，重要的是内容`I
    am a good girl.\n`是相同的。从技术上讲，这是一个“善意的谎言”；有一种情况下IPFS必须考虑文件名，不能忽视它。我将在本章后面解释这个问题的真相。
- en: What we have seen in the preceding example is normal hashing. There is no Markle
    DAG or even Merkle tree. Let's now create a more complicated scene with a big
    file. Hashing a big file is not efficient. Usually, we split the file into multiple
    smaller pieces of the same size. For example, a 900 KB file would turn into four
    files. The first, second, and third files would have a size of 250 KB. The fourth
    file would have a size of 150 KB. Then, we hash each smaller file and combine
    it with a Merkle tree.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们看到的是正常的哈希。没有Markle DAG甚至Merkle树。现在让我们创建一个更复杂的场景，有一个大文件。对大文件进行哈希不是有效的。通常，我们将文件分割成多个相同大小的小块。例如，一个900KB的文件会变成四个文件。第一、第二和第三个文件的大小为250KB。第四个文件的大小为150KB。然后，我们对每个较小的文件进行哈希，并与Merkle树结合。
- en: For illustration purposes, we won't use a large file, but we will make some
    imaginary limitations. We don't want to hash content that spans more than one
    line. If the text file has four lines, we would split them into four smaller files.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明目的，我们不会使用一个大文件，但我们会设置一些虚构的限制。我们不希望对跨越多行的内容进行哈希。如果文本文件有四行，我们会将它们分成四个更小的文件。
- en: 'Inside your project directory, create a file called `hello_big.txt` and enter
    the following lines:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目目录中，创建一个名为`hello_big.txt`的文件，并输入以下内容：
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Before we create a script to hash this big file, let's create a very simple
    Merkle tree library and name it `merkle_tree.py`. Refer to the GitLab link for
    the complete code file: [https://gitlab.com/arjunaskykok/hands-on-blockchain-for-python-developers/tree/master/chapter_10](https://gitlab.com/arjunaskykok/hands-on-blockchain-for-python-developers/tree/master/chapter_10).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建一个脚本来对这个大文件进行哈希之前，让我们创建一个非常简单的Merkle树库，并将其命名为`merkle_tree.py`。有关完整代码文件，请参考GitLab链接：[https://gitlab.com/arjunaskykok/hands-on-blockchain-for-python-developers/tree/master/chapter_10](https://gitlab.com/arjunaskykok/hands-on-blockchain-for-python-developers/tree/master/chapter_10)。
- en: 'Let''s discuss this Merkle tree library, starting from its initialization:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从初始化开始讨论这个Merkle树库：
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We make sure there are at least four nodes. If not, we might as well use the
    hash list technique. The `leaf_nodes` are original data nodes. They are string
    lists, such as `['cat', 'dog', 'unicorn', 'elephant']`. The `hash_nodes` are the
    hash list of the data nodes, such as `[hash of 'cat', hash of 'dog', hash of 'unicorn',
    hash of 'elephant']` or `['77af778...', 'cd6357e...', 'c6cb50e...', 'cd08c4c...']`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 确保至少有四个节点。如果没有，我们可以使用哈希列表技术。`leaf_nodes`是原始数据节点。它们是字符串列表，比如`['cat', 'dog', 'unicorn',
    'elephant']`。`hash_nodes`是数据节点的哈希列表，比如`[hash of 'cat', hash of 'dog', hash of
    'unicorn', hash of 'elephant']`或`['77af778...', 'cd6357e...', 'c6cb50e...', 'cd08c4c...']`。
- en: 'We use the `_hash_list()` method to hash list the data if there are less than
    four nodes. We concatenate all the pieces of data before hashing them:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果节点少于四个，我们使用`_hash_list()`方法对数据进行哈希列表。我们在对它们进行哈希之前将所有数据拼接起来：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the `_turn_leaf_nodes_to_hash_nodes()` method, we fill the `hash_nodes` based
    on the `leaf_nodes`. This is one-to-one mapping:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在`_turn_leaf_nodes_to_hash_nodes()`方法中，我们根据`leaf_nodes`填充`hash_nodes`。这是一对一的映射：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In the `_hash()` method, we wrap the `sha256` hashing function. This is to
    make the customization of the class easier, since we may want to use a different
    hashing function:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在`_hash()`方法中，我们包装了`sha256`哈希函数。这是为了使类的定制更容易，因为我们可能想使用不同的哈希函数：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following code block shows how we can get the root nodes from the hash
    nodes:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块显示了如何从哈希节点中获取根节点：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here, we are carrying out multiple iterations on hash nodes. It jumps two steps
    on each iteration. For each iteration, it works on two nodes. It concatenates
    the hash of these two nodes, then hashes the result. The resulting hash is the
    parent of these two nodes. This parent becomes part of the hash nodes that will
    be iterated over again. This parent, along with its neighbor, will be concatenated
    again before being hashed, and so on. If there is an odd number of hash nodes,
    the last node will be concatenated with itself before being hashed. If there is
    only one parent, we return the hash of that, which is the **root hash**:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们对哈希节点进行多次迭代。每次迭代都会跳过两步。对于每次迭代，它都会处理两个节点。它连接这两个节点的哈希，然后对结果进行哈希。得到的哈希是这两个节点的父节点。这个父节点成为将要再次迭代的哈希节点的一部分。这个父节点和它的邻居将再次连接并进行哈希，依此类推。如果哈希节点数为奇数，最后一个节点将在进行哈希之前与自身连接。如果只有一个父节点，我们返回该节点的哈希，即**根哈希**：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `_convert_parent_from_two_nodes()` method allows us to get the parent hash
    from the two child nodes. We concatenate the two nodes and hash them. If the second
    node is `None`, meaning there is an odd number of nodes or we are processing the
    last node, we just concatenate the node with itself before hashing it.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`_convert_parent_from_two_nodes()`方法允许我们从两个子节点获取父哈希。我们将这两个节点连接起来并对它们进行哈希。如果第二个节点是`None`，表示节点数为奇数或者我们正在处理最后一个节点，我们只需将节点与自身连接再进行哈希。'
- en: 'Now that the Merkle tree library is ready, we will create a Python script to
    hash the `hello_big.txt` file and name it `hash_big_file.py`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在Merkle树库已经准备好了，我们将创建一个Python脚本来对`hello_big.txt`文件进行哈希，并将其命名为`hash_big_file.py`：
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If you execute this Python script, you will get the following output:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果执行这个Python脚本，你将得到以下输出：
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If the file is big, you would not hash it directly, because this could cause
    you to run out of memory. Instead, you split the file. Here, we split the text
    file based on the new lines. If you handle the binary file, you read the file
    chunk by chunk and save that chunk into a smaller file. Of course, before feeding
    them into a Merkle tree, you need to serialize the binary data into the text data.
    Once you have done that, you can feed the pieces of data into a Merkle tree. You
    get the root hash, which will protect the integrity of the original file. If you
    alter a single bit in a piece of data, the root hash would be different.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果文件很大，你不会直接对其进行哈希，因为这可能会导致内存耗尽。相反，你会将文件分割。在这里，我们根据换行符分割文本文件。如果处理二进制文件，你会逐块读取文件并将该块保存到一个较小的文件中。当然，在将它们输入Merkle树之前，你需要将二进制数据序列化为文本数据。一旦完成了这一步，你就可以将数据片段输入Merkle树。你会得到根哈希，它将保护原始文件的完整性。如果在数据片段中更改了一个位，根哈希将会不同。
- en: The Merkle DAG data structure
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Merkle DAG数据结构
- en: We have used content addressing to handle a file. If the file is big, we can
    split it and get the root hash with a Merkle tree. In this case, we only care
    about the content of the file; we don't even save its name.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用内容寻址来处理文件。如果文件很大，我们可以对其进行分割，并使用Merkle树获取根哈希。在这种情况下，我们只关心文件的内容；我们甚至不保存它的名称。
- en: 'There is a situation, however, where the name of the file does matter. For
    example, let''s say that you want to save a file directory that contains 100 images
    of cute pandas. The names of the files in this case don''t matter; what we care
    about is the content, the pictures of the cute pandas! If this is a directory
    of a programming project, however, the names of the files do matter. If one Python
    file tries to import another Python library that is contained in a different file,
    we have to keep the name of the file. Let''s say that we have a Python file called `main.py` that
    has the following content:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一种情况，文件名确实很重要。例如，假设您想保存一个包含100张可爱熊猫图片的文件目录。在这种情况下，文件的名称并不重要；我们关心的是内容，即可爱熊猫的图片！然而，如果这是一个编程项目的目录，文件的名称就很重要。如果一个Python文件尝试导入另一个包含在不同文件中的Python库，我们必须保留文件的名称。假设我们有一个名为`main.py`的Python文件，其内容如下：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `main.py` file is dependent on another file in the same directory called `secret_algorithm.py`.
    It is not just the content of the `secret_algorithm.py` file that matters, but
    also its name. If the filename changes, `main.py` will not be able to import the
    library.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`main.py`文件依赖于同一目录中的另一个名为`secret_algorithm.py`的文件。重要的不仅是`secret_algorithm.py`文件的内容，还有它的名称。如果文件名更改，`main.py`将无法导入该库。'
- en: In order to save the content and the filename, we need to use a Merkle DAG data
    structure. As mentioned before, one of the differences between a Merkle DAG and
    a Merkle tree is that any node in a Merkle DAG can hold data, not just a leaf
    node, as is the case in a Merkle tree.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保存内容和文件名，我们需要使用Merkle DAG数据结构。如前所述，Merkle DAG和Merkle树之间的一个区别是，Merkle DAG中的任何节点都可以保存数据，而不仅仅是叶节点，这是Merkle树中的情况。
- en: 'Let''s create a sample directory that contains sample files and a nested directory
    that also contains files:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个包含示例文件和一个嵌套目录的示例目录：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then, create a Python script to explain this new data structure. Create a file
    called `merkle_dag.py` in your project directory. Refer to the GitLab link for
    the complete code file: [https://gitlab.com/arjunaskykok/hands-on-blockchain-for-python-developers/tree/master/chapter_10](https://gitlab.com/arjunaskykok/hands-on-blockchain-for-python-developers/tree/master/chapter_10).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，创建一个Python脚本来解释这种新的数据结构。在项目目录中创建一个名为`merkle_dag.py`的文件。有关完整代码文件，请参考GitLab链接：[https://gitlab.com/arjunaskykok/hands-on-blockchain-for-python-developers/tree/master/chapter_10](https://gitlab.com/arjunaskykok/hands-on-blockchain-for-python-developers/tree/master/chapter_10)。
- en: 'Let''s discuss the `MerkleDAGNode` class, starting from its initialization
    method:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论`MerkleDAGNode`类，从其初始化方法开始：
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `_init_()` method accepts a file path as an argument. This could be a path
    to a file or a directory. We make an assumption that this is a valid path and
    not a symbolic link.  `self.pointers` will be explained later on in the section
    with the `_iterate_directory_contents()` method. `self.dirtype` is used to differentiate
    between the directory or the file. `self.filename` is used to hold the name of
    the file or the name of the directory.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`_init_()`方法接受文件路径作为参数。这可以是文件或目录的路径。我们假设这是一个有效的路径，而不是一个符号链接。`self.pointers`将在`_iterate_directory_contents()`方法的部分中解释。`self.dirtype`用于区分目录或文件。`self.filename`用于保存文件或目录的名称。'
- en: If the argument is the path to the file (not the directory), we read the content
    into `self.content`. For demonstration purposes, we assume the content of the
    file is small and we don't try to split the files like we did before. Then, we
    calculate the hash based on the filename and the content.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果参数是文件的路径（而不是目录），我们将内容读入`self.content`。为了演示目的，我们假设文件的内容很小，我们不尝试像之前那样拆分文件。然后，我们根据文件名和内容计算哈希值。
- en: 'If the argument is the path to the directory, the content would be an array
    of `MerkleDAGNode` objects of the inner files inside that directory. To calculate
    the hash, we use a Merkle tree to get the root hash of its children. However,
    we need to concatenate this with the name of the directory before hashing it again:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果参数是目录的路径，则内容将是该目录内部文件的`MerkleDAGNode`对象数组。为了计算哈希值，我们使用Merkle树来获取其子节点的根哈希。但是，我们需要在再次对其进行哈希之前将其与目录的名称连接起来：
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`_hash()` is a wrapper method of the `sha256` hashing function.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`_hash()`是`sha256`哈希函数的包装方法。'
- en: 'The `_iterate_directory_contents()` method is used to iterate over the inner
    children of the directory. We convert every file or directory inside this directory
    to a `MerkleDAGNode` object. The `self.pointers` object is used to make it easier
    to access the `MerkleDAGNode` based on the filename. Basically, it is like a recursive
    function, especially when we hit a directory:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`_iterate_directory_contents()`方法用于迭代目录的内部子项。我们将该目录内的每个文件或目录转换为`MerkleDAGNode`对象。`self.pointers`对象用于更轻松地根据文件名访问`MerkleDAGNode`。基本上，这就像一个递归函数，特别是当我们遇到一个目录时：'
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `_repr_()` method is used to make it easier to print objects for debugging:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`_repr_()`方法用于更轻松地打印对象以进行调试：'
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `_eq_()` method is needed so that we can compare the `MerkleDAGNode` object
    with other `MerkleDAGNode` objects. This is useful during the testing process:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`_eq_()`方法是必需的，以便我们可以将`MerkleDAGNode`对象与其他`MerkleDAGNode`对象进行比较。这在测试过程中非常有用：'
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s create a `hash_directory.py` file to demonstrate the power of this data
    structure:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个`hash_directory.py`文件来演示这种数据结构的强大功能：
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You would get the following result if you execute the script:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果执行脚本，您将获得以下结果：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The output is the schema of the Merkle DAG node.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是Merkle DAG节点的模式。
- en: 'This is how Git keeps the files. Our implementation is just for education purposes
    and would not be fit for production purposes. In the real world, you should have
    many optimizations. One of the optimizations that you could implement is using
    a reference for the data, just like Git. If there are two different files that
    have the same content (but different filenames), the content would be saved just
    once. The other optimization is that Git uses compression. The following diagram
    illustrates the concept of Git, where we have two files, **file B** and **file
    D***.* These both have the same content, **content xxx**.**File B **is saved just
    once in **directory A***.* **File D** is saved at **directory C** with **file
    E***,* which has a different content, **content yyy**. **Directory C** is also saved
    in **directory A**. But the content of **File B** and **File D**, which is **content
    xxx**, is saved only once:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是Git保存文件的方式。我们的实现只是为了教育目的，不适合生产目的。在现实世界中，你应该有很多优化。你可以实现的一个优化是使用数据的引用，就像Git一样。如果有两个不同的文件具有相同的内容（但文件名不同），则内容只保存一次。另一个优化是Git使用压缩。下图说明了Git的概念，我们有两个文件**文件B**和**文件D**。这两个文件都有相同的内容**内容xxx**。**文件B**只保存一次在**目录A**中。**文件D**保存在**目录C**中，有一个不同的内容**内容yyy**。**目录C**也保存在**目录A**中。但**文件B**和**文件D**的内容**内容xxx**只保存一次：
- en: '![](assets/46c4396a-6399-4c6b-be3c-9cc48bf2509e.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/46c4396a-6399-4c6b-be3c-9cc48bf2509e.png)'
- en: 'Now that we know how to save a directory of files with Merkle DAG, what if
    we want to change the content of the file? Should we abandon this Merkle DAG node
    and create a totally new node? A more efficient way to solve this problem would
    be to use a versioning system. A file could have version 1, version 2, version
    3, and so on. The easiest way to implement versioning is to use a linked list,
    as illustrated in the following diagram:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何使用Merkle DAG保存文件目录，如果我们想要更改文件的内容怎么办？我们应该放弃这个Merkle DAG节点并创建一个全新的节点吗？解决这个问题的更有效的方法是使用一个版本控制系统。一个文件可以有版本1、版本2、版本3等。实现版本控制的最简单方法是使用链表，如下图所示：
- en: '![](assets/d99ffb30-fb8b-497a-8b8d-1e4415225d2f.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/d99ffb30-fb8b-497a-8b8d-1e4415225d2f.png)'
- en: Peer-to-peer networking
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 点对点网络
- en: We understand how to save files in IPFS. The key is the **hash**. The value
    is the name of the file or directory and the content of the file or directory.
    If we were building a centralized system, our story would be finished. We would
    just need to add a few other things to create a piece of software to save files
    and search them based on the hash. This software would be similar to a database,
    such as SQLite or LevelDB. IPFS is neither of those; it is a peer-to-peer filesystem
    that is like a database but spread all over the place. In other words, it is a
    distributed hash table.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解了如何在IPFS中保存文件。关键是**哈希**。值是文件或目录的名称和文件或目录的内容。如果我们正在构建一个集中式系统，我们的故事就结束了。我们只需要添加一些其他东西来创建一个保存文件并根据哈希搜索文件的软件。这个软件类似于数据库，比如SQLite或LevelDB。IPFS都不是这些；它是一个像数据库一样分散在各处的点对点文件系统。换句话说，它是一个分布式哈希表。
- en: IPFS uses S/Kademlia, an extended version of Kademlia, as a distributed hash
    table. Before we discuss Kademlia, let's discuss its predecessor.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: IPFS使用S/Kademlia，Kademlia的扩展版本，作为分布式哈希表。在讨论Kademlia之前，让我们讨论它的前身。
- en: 'First, imagine a hash table, which is like a dictionary in Python, as shown
    in the following table:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，想象一个哈希表，就像Python中的字典，如下表所示：
- en: '| **Key** | **Value** |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| **键** | **值** |'
- en: '| 2 | Cat |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 猫 |'
- en: '| 5 | Unicorn |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 独角兽 |'
- en: '| 9 | Elephant |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 大象 |'
- en: '| 11 | Horse |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 马 |'
- en: '| 4 | Rhino |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 犀牛 |'
- en: '| 101 | Blue Parrot |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 101 | 蓝鹦鹉 |'
- en: '| 33 | Dragon |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 33 | 龙 |'
- en: In IPFS, the key is the hash, not a number. But for demonstration purposes,
    let's make it a simple integer. The value is just a simple name of animal, not
    the content of the file or the content of the files inside a directory.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在IPFS中，关键是哈希，而不是数字。但为了演示目的，让我们把它变成一个简单的整数。值只是动物的简单名称，而不是文件的内容或目录中文件的内容。
- en: Now, imagine you have four nodes. A node could be a computer that is located
    in a different continent to the rest of the nodes.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象你有四个节点。一个节点可以是位于不同大陆的计算机。
- en: 'Let''s define which node holds which keys:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义哪个节点持有哪些键：
- en: '| **Node** | **Keys** |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| **节点** | **键** |'
- en: '| A | 2, 9, 11 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| A | 2, 9, 11 |'
- en: '| B | 5 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| B | 5 |'
- en: '| C | 4, 33 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| C | 4, 33 |'
- en: '| D | 101 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| D | 101 |'
- en: You keep this table in a central server. One of the nodes will be the central
    node. This means that if someone wants to access key five, they have to ask the
    central server before receiving the answer, node B. After that, the request can
    be directed to node B. Node B would return "Unicorn" to the data requester.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 你把这个表保存在一个中央服务器上。其中一个节点将是中央节点。这意味着如果有人想要访问键五，他们必须在收到答案之前向中央服务器询问，节点B。之后，请求可以被定向到节点B。节点B会向数据请求者返回“独角兽”。
- en: This method is very efficient; no time is wasted. Napster, the peer-to-peer
    music sharing system, uses this approach. The drawback is that the central server
    is a single point of failure. An adversary (someone who does not like this information
    being spread; in the case of Napster, this could be a big music labels) could
    attack the central server.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法非常高效；没有时间被浪费。Napster，点对点音乐共享系统，使用了这种方法。缺点是中央服务器是单点故障。对手（不喜欢这些信息传播的人；在Napster的情况下，这可能是大音乐公司）可能会攻击中央服务器。
- en: One solution would be to ask all nodes about which node holds the key instead
    of keeping this information in the central node. This is what Gnutella does. This
    setup is resilient to censorship and attacks from adversaries but it makes life
    hard for nodes and people who request the data. The node must work hard when receiving
    many requests. This setup is called **flooding**. It is suitable for Bitcoin,
    but not for IPFS.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一个解决方案是询问所有节点关于哪个节点保存了密钥，而不是将这些信息保存在中央节点中。这就是Gnutella所做的。这种设置对审查和对手的攻击是有抵抗力的，但对于请求数据的节点和人来说会增加难度。当接收到许多请求时，节点必须努力工作。这种设置称为**泛洪**。它适用于比特币，但不适用于IPFS。
- en: This is why the distributed hash table technology was created. There are a couple
    of distributed hash table algorithms, one of which is Kademlia. This algorithm
    was created by Petar Maymounkov and David Mazières in 2002\. It was later used
    by the eDonkey file sharing platform.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是分布式哈希表技术被创建的原因。有几种分布式哈希表算法，其中之一是Kademlia。这个算法是由Petar Maymounkov和David Mazières于2002年创建的。后来被eDonkey文件共享平台使用。
- en: The notion of closeness of data and nodes
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据和节点的接近概念
- en: In a distributed hash table, we don't put the data in every node. We put the
    data in certain nodes according to the notion of closeness. We want to put the
    data in nearby nodes. This means that we have the concept of distance not just
    between nodes, but also between the data and the nodes.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式哈希表中，我们不会将数据放在每个节点上。我们根据接近的概念将数据放在某些节点上。我们希望将数据放在附近的节点上。这意味着我们不仅在节点之间有距离的概念，而且在数据和节点之间也有距离的概念。
- en: 'Imagine that every node launched or created in this distributed hash table
    is given an ID between 1 and 1000\. Every node ID is unique, so there can be a
    maximum of 1,000 nodes. There are likely to be more than 1,000 nodes in a real-world
    setting, but this will work as an example. Let''s say that we have 10 nodes:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，在这个分布式哈希表中启动或创建的每个节点都被赋予1到1000之间的ID。每个节点ID都是唯一的，因此最多可以有1,000个节点。在现实世界中可能会有超过1,000个节点，但这将作为一个例子。假设我们有10个节点：
- en: '| **Node ID** |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| **节点ID** |'
- en: '| 5 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 5 |'
- en: '| 13 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 13 |'
- en: '| 45 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 45 |'
- en: '| 48 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 48 |'
- en: '| 53 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 53 |'
- en: '| 60 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 60 |'
- en: '| 102 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 102 |'
- en: '| 120 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 120 |'
- en: '| 160 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 160 |'
- en: '| 220 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 220 |'
- en: 'We also have some data. To make it simple, the data in this case is just some
    strings:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一些数据。为了简单起见，这种情况下的数据只是一些字符串：
- en: '| **Data** |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| **数据** |'
- en: '| Unicorn |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 独角兽 |'
- en: '| Pegasus |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 飞马 |'
- en: '| Cat |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 猫 |'
- en: '| Donkey |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 驴 |'
- en: '| Horse |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 马 |'
- en: 'To be able to say whether this data is close to or far from certain nodes,
    we need to convert this data into a number between 1 and 1000\. In the real world,
    you could hash the data. But for our practical demonstration, we will just allocate
    a random number:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够判断这些数据与某些节点的接近程度或远离程度，我们需要将这些数据转换为1到1000之间的数字。在现实世界中，您可以对数据进行哈希。但是对于我们的实际演示，我们将只分配一个随机数：
- en: '| **Key** | **Data** |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| **密钥** | **数据** |'
- en: '| 54 | Unicorn |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 54 | 独角兽 |'
- en: '| 2 | Pegasus |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 飞马 |'
- en: '| 100 | Cat |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 猫 |'
- en: '| 900 | Donkey |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 900 | 驴 |'
- en: '| 255 | Horse |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 255 | 马 |'
- en: If we want to store the Unicorn data in the four nearest nodes (four is just
    a configuration number), this can be done as follows. First, you check the key,
    which is 54\. Then, we want to get the nearest four nodes to 54\. If you check
    the node ID list, the nearest four nodes are 45, 48, 53, and 60\. So, we store
    the Unicorn data in these four nodes. If we want to store the Cat data, the nearest
    neighbors from its key, 100, are 53, 60, 102, and 120, so we store the Cat data
    in these four nodes.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要将独角兽数据存储在最近的四个节点中（四只是一个配置数字），可以按以下方式进行。首先，您检查密钥，即54。然后，我们想要获取最接近54的四个节点。如果您检查节点ID列表，最近的四个节点是45、48、53和60。因此，我们将独角兽数据存储在这四个节点中。如果我们想要存储猫数据，从其密钥100的最近邻居是53、60、102和120，因此我们将猫数据存储在这四个节点中。
- en: We treat data as a node when calculating the distance. This is how we look up
    data in a distributed hash table. The data and the nodes share the same space.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算距离时，我们将数据视为节点。这就是我们在分布式哈希表中查找数据的方式。数据和节点共享相同的空间。
- en: XOR distance
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XOR距离
- en: However, in Kademlia, we don't measure distance by decimal subtraction. To make
    it clear, decimal subtraction is just normal subtraction. The distance between
    45 and 50 is 5\. The distance between 53 and 63 is 10.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在Kademlia中，我们不是通过十进制减法来测量距离的。为了清楚起见，十进制减法只是普通的减法。45和50之间的距离是5。53和63之间的距离是10。
- en: 'In Kademlia, measuring distance is done by XOR distance. The XOR distance between
    3 and 6 is 5, not 3\. Here''s how to count it:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kademlia中，通过XOR距离来测量距离。3和6之间的XOR距离是5，而不是3。下面是如何计算它的方法：
- en: The binary version of 3 is 011\. The binary version of 6 is 110\. What I mean
    by binary version is the number in base 2\. XOR means *exclusive or*. Using the
    XOR operation, 1 XOR 0 is 1, 1 XOR 1 is 0, 0 XOR 0 is 0, and 0 XOR 1 is 1\. If
    two operands are same, the result is 0\. If two operands are different, the result
    is 1.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 3的二进制版本是011。6的二进制版本是110。我所说的二进制版本是指二进制中的数字。XOR表示“异或”。使用XOR运算，1 XOR 0是1，1 XOR
    1是0，0 XOR 0是0，0 XOR 1是1。如果两个操作数相同，则结果为0。如果两个操作数不同，则结果为1。
- en: '[PRE20]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 101 is the binary version of 5.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 101是5的二进制版本。
- en: The XOR distance has a few useful properties that prompted the author of the
    Kademlia paper to choose the XOR distance to measure the distance between the
    nodes.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: XOR距离具有一些有用的特性，促使Kademlia论文的作者选择XOR距离来衡量节点之间的距离。
- en: 'The first property is that the XOR distance of a node to itself is 0\. The
    closest node to a node with an ID of 5 is another node with an ID 5, or itself.
    The binary version of 5 is 0101:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个特性是节点与自身的XOR距离为0。与ID为5的节点最接近的节点是另一个ID为5的节点，或者是它自己。5的二进制版本是0101：
- en: '[PRE21]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The 0 distance is only possible if we measure the distance between a node and
    itself.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当我们测量节点与自身之间的距离时，才可能出现0距离。
- en: 'The second property is that the distance between different nodes is symmetrical.
    The XOR distance between 4 and 8 is same as the XOR distance between 8 and 4\.
    The binary version of 4 is 0100 and the binary version of 8 is 1000\. So, if we
    calculate the distance between them using their binary value, we get the same
    value. The XOR distance between 4 and 8 is as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个属性是不同节点之间的距离是对称的。4和8之间的异或距离与8和4之间的异或距离相同。4的二进制版本是0100，8的二进制版本是1000。所以，如果我们使用它们的二进制值计算它们之间的距离，我们会得到相同的值。4和8之间的异或距离如下：
- en: '[PRE22]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The XOR distance between 8 and 4 is as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 8和4之间的异或距离如下：
- en: '[PRE23]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: If you are used to working with decimal subtraction distances, this will be
    intuitive to you.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你习惯使用十进制减法距离，这对你来说会很直观。
- en: The last useful property is that the distance between node X and node Z is less
    than or equal to the distance between node X and node Y plus the distance between
    node Y and Z. This last property is important because a node in a Kademlia distributed
    hash table does not save all the other nodes' addresses. It only saves some nodes'
    addresses. But a node can reach another node through intermediate nodes. Node
    X knows the address of node Y, but does not know the address of node Z. Node Y
    does know the address of node Z. Node X can query the neighbor nodes of node Y
    from node Y. Then, node X can reach node Z knowing that the distance to node Z
    is less than or equal to the distance of node X and node Y added to the distance
    of node Y and node Z.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个有用的属性是节点X和节点Z之间的距离小于或等于节点X和节点Y之间的距离加上节点Y和节点Z之间的距离。这个属性很重要，因为Kademlia分布式哈希表中的节点不保存所有其他节点的地址。它只保存一些节点的地址。但是一个节点可以通过中间节点到达另一个节点。节点X知道节点Y的地址，但不知道节点Z的地址。节点Y知道节点Z的地址。节点X可以从节点Y查询节点Y的邻居节点。然后，节点X可以到达节点Z，知道到节点Z的距离小于或等于节点X和节点Y的距离加上节点Y和节点Z的距离。
- en: If this property were not true, the longer node X searches for a node, the further
    the distance a particular node will be, which is not what we wanted. But with
    this property, the addresses of neighbor nodes from other nodes may be smaller
    than, if not the same as, the combined distances.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个属性不成立，节点X搜索节点的时间越长，特定节点之间的距离就越远，这不是我们想要的。但是有了这个属性，其他节点的邻居节点的地址可能比组合距离小，甚至相同。
- en: 'When you think about using XOR distance, you should think that the more prefixes
    shared by two numbers, the shorter the distance between those two numbers. For
    example, if the numbers share three common prefixes, such as five and four, the
    distance is one:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当你考虑使用异或距离时，你应该想到两个数字共享的前缀越多，这两个数字之间的距离就越短。例如，如果数字共享三个公共前缀，比如五和四，距离就是一：
- en: '[PRE24]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Likewise, for numbers 14 and 15, the distance is also 1:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，对于数字14和15，距离也是1：
- en: '[PRE25]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'But, if the bit differences are on the left side, such as is the case for 5
    and 13, the distance might be large, in this case eight:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果位的差异在左边，比如5和13的情况，距离可能很大，在这种情况下是8：
- en: '[PRE26]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The XOR distance between 4 and 5 is 1 but the XOR distance between 5 and 6
    is 3\. This is counter-intuitive if you are accustomed to decimal subtraction
    distances. To make this concept easier to explain, let''s create a binary tree
    that is composed of numbers from 1 to 15:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 4和5之间的异或距离是1，但5和6之间的异或距离是3。如果你习惯十进制减法距离，这是违反直觉的。为了更容易解释这个概念，让我们创建一个由1到15的数字组成的二叉树：
- en: '![](assets/8c80866f-e3cb-4e74-848d-350b3decd707.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/8c80866f-e3cb-4e74-848d-350b3decd707.png)'
- en: Look at this tree carefully. The XOR distance between 4 and 5 is 1, but the
    XOR distance between 5 and 6 is 3\. If you look at the picture, 4 and 5 are under
    an immediate branch, whereas 5 and 6 is under a larger branch, which implies a
    larger distance. The immediate branch corresponds to the bit on the right. The
    parent branch of that immediate branch corresponds to the second-most right bit.
    The top branch corresponds to the bit on the left. So, if the number is separated
    by a top branch, the distance is at least 8\. The binary version of 8 is 1000.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细看这棵树。4和5之间的异或距离是1，但5和6之间的异或距离是3。如果你看图片，4和5在一个直接的分支下，而5和6在一个更大的分支下，这意味着更大的距离。直接的分支对应右边的位。直接分支的父分支对应第二右边的位。顶部分支对应左边的位。所以，如果数字被顶部分支分开，距离至少是8。8的二进制版本是1000。
- en: This is just for understanding purposes; it is not a rigorous mathematical definition.
    If you look at the journey from 5 to 11 and 5 to 13, you should get roughly the
    same distance, but this is not the case. The XOR distance of 5 and 13 is 8 but
    the XOR distance of 5 and 11 is 14.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是为了理解目的；这不是严格的数学定义。如果你看从5到11和5到13的路径，你应该得到大致相同的距离，但事实并非如此。5和13的异或距离是8，但5和11的异或距离是14。
- en: 'In Python, you can XOR two numbers with the `^` operator:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，你可以使用`^`运算符对两个数字进行异或运算：
- en: '[PRE27]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You can turn any decimal number to its binary version using the `bin` function:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`bin`函数将任何十进制数转换为它的二进制版本：
- en: '[PRE28]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then, if you want to convert the binary number back to a decimal number, use
    the `int` function:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，如果你想将二进制数转换回十进制数，使用`int`函数：
- en: '[PRE29]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The second argument of the `int` function indicates which base the first argument
    is. Binary is base 2.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`函数的第二个参数表示第一个参数的基数。二进制是基数2。'
- en: Buckets
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 桶
- en: Now that we have gone through XOR distances, we will take a look at how a node
    saves other nodes' addresses. A node does not save all other nodes in a distributed
    hash table. The number of nodes a node can save depends on the number of bits
    in a node and the *k* configuration number. Let's discuss these one by one.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了异或距离，我们将看一下节点如何保存其他节点的地址。一个节点在分布式哈希表中不保存所有其他节点。一个节点可以保存的节点数量取决于节点中的位数和*k*配置数字。让我们逐一讨论这些。
- en: 'Remember the tree picture we saw previously? It has 16 leaves. Now imagine
    the smallest tree:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们之前看到的树图片吗？它有16个叶子。现在想象最小的树：
- en: '![](assets/6b875b7f-a32b-4794-9b1a-00f29338e452.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/6b875b7f-a32b-4794-9b1a-00f29338e452.png)'
- en: 'It has two leaves. Let''s double the tree:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 它有两片叶子。让我们再加倍这棵树：
- en: '![](assets/2db92237-ac9d-4f48-bda5-a2b63d59e967.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/2db92237-ac9d-4f48-bda5-a2b63d59e967.png)'
- en: 'The tree now has four leaves. Let''s double it again:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 树现在有四片叶子。让我们再次加倍：
- en: '![](assets/b73a1d5c-48ab-43bb-a493-6076e78744e1.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/b73a1d5c-48ab-43bb-a493-6076e78744e1.png)'
- en: The tree now has eight leaves. If you double it again, you would have a tree
    like our previous tree, which has 16 leaves.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 树现在有八片叶子。如果再次加倍，你会得到一棵像我们之前的树一样有16片叶子的树。
- en: The progression we can see is 2, 4, 8, 16\. If we continue the journey, the
    numbers would be 32, 64, 128, and so on. This can be written as 2^(1,) 2^(2,)
    2^(3,) 2^(4,) 2^(5 )... 2^n.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到的进展是2，4，8，16。如果我们继续这个过程，数字将是32，64，128，等等。这可以写成2^(1,) 2^(2,) 2^(3,) 2^(4,)
    2^(5 )... 2^n。
- en: Let's focus on a tree with 16 leaves. When we represent the leaf number, we
    must use a 4-bit binary number, such as 0001 or 0101, because the biggest number
    is 15, or 1111\. If we use a tree with 64 leaves, we must use a 6-bit number,
    such as 000001, 010101 because the biggest possible number is 63 or 111111\. The
    bigger the bit number, the larger the amount of nodes a node must save in its
    address book.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们专注于一棵有16片叶子的树。当我们表示叶子编号时，我们必须使用一个4位的二进制数，比如0001或者0101，因为最大的数字是15，或者1111。如果我们使用一棵有64片叶子的树，我们必须使用一个6位的数字，比如000001，010101，因为最大可能的数字是63或者111111。位数越大，节点在其地址簿中保存的节点数量就越多。
- en: Then, we have the *k* configuration number. *k* decides the maximum amount of
    nodes a node can save in a bucket. The number of buckets is the same as the number
    of bits used in a distributed hash table. In a tree with 16 leaves, the number
    of buckets is 4\. In a tree with 64 leaves, the number of buckets is 6\. Each
    bucket corresponds to a bit. Let's say we have a tree with 16 leaves, so each
    number has 4 bits, such as 0101 or 1100\. This means the node has four buckets.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有*k*配置数字。*k*决定了一个节点在一个桶中可以保存的最大节点数量。桶的数量与分布式哈希表中使用的位数相同。在有16片叶子的树中，桶的数量是4。在有64片叶子的树中，桶的数量是6。每个桶对应一个位。假设我们有一棵有16片叶子的树，所以每个数字有4位，比如0101或者1100。这意味着节点有四个桶。
- en: The first bucket corresponds to the first bit from the left. The second bucket
    corresponds to the second bit from the left. The third bucket corresponds to the
    third bit from the left. The fourth bucket corresponds to the fourth bit from
    the left.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个桶对应于从左边开始的第一个位。第二个桶对应于从左边开始的第二个位。第三个桶对应于从左边开始的第三个位。第四个桶对应于从左边开始的第四个位。
- en: Let's take a look at example of a node with ID 3 in a tree with 16 leaves. For
    now, we assume we have 16 nodes in a tree that has 16 leaves. In the real world,
    the tree would be sparse and a lot of branches would be empty.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个在有16片叶子的树中ID为3的节点的例子。现在，我们假设在有16片叶子的树中有16个节点。在现实世界中，树会是稀疏的，很多分支会是空的。
- en: In the paper that describes Kademlia, the authors used 160 buckets or a 160-bit
    address. The number of leaves in this tree is vast. For comparison, 2^(78) is
    the number of atoms in visible universe. The *k* configuration number is chosen
    as 20 in this paper, so a node can have a maximum of 3,200 nodes in its address
    book.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述Kademlia的论文中，作者使用了160个桶或者160位地址。这棵树中的叶子数量是巨大的。作为对比，2^(78)是可见宇宙中的原子数量。在这篇论文中，*k*配置数字选择为20，因此一个节点的地址簿中最多可以有3,200个节点。
- en: For this example, let's say that the *k* number is 2\. This means for every
    bucket, the node saves two other nodes. The first bucket, which corresponds to
    the first bit, corresponds to the other half of the tree, where the node does
    not reside. We have eight nodes in this half of the tree but we can only save
    two of them because the *k* number is 2\. Let's choose nodes 11 and 14 for this
    bucket. How we choose which nodes go in which buckets will be described later.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，假设*k*数字是2。这意味着对于每个桶，节点保存另外两个节点。第一个桶对应于第一个位，对应于树的另一半，节点不驻留在这里。我们在树的这一半有八个节点，但是我们只能保存其中的两个，因为*k*数字是2。让我们选择节点11和14放入这个桶。如何选择哪些节点放入哪些桶将在后面描述。
- en: Then, let's divide the half of the tree where the node resides, so we have two
    branches. The first branch consists of a node with ID 0, a node with ID 1, a node
    with ID 2, and a node with ID 3\. The second branch consists of a node with ID
    4, a node with ID 5, a node with ID 6, and a node with ID 7\. This second branch
    is the second bucket. There are four nodes in this branch, but we can only save
    two nodes. Let's choose the node with ID 4 and the node with ID 5.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们把包含节点的一半的树分成两个分支。第一个分支包括ID为0的节点，ID为1的节点，ID为2的节点和ID为3的节点。第二个分支包括ID为4的节点，ID为5的节点，ID为6的节点和ID为7的节点。第二个分支是第二个桶。这个分支有四个节点，但我们只能保存两个节点。让我们选择ID为4的节点和ID为5的节点。
- en: Then, let's divide the branch where our node (the node with ID 3) resides so
    we have two small branches. The first small branch consists of a node with ID
    0 and a node with ID 1\. The second small branch consists of a node with ID 2
    and a node with ID 3\. So the third bucket is the first small branch. There are
    only two nodes, a node with ID 0 and node with ID 1, so we save both.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们把包含我们的节点（ID为3的节点）的分支分成两个小分支。第一个小分支包括ID为0的节点和ID为1的节点。第二个小分支包括ID为2的节点和ID为3的节点。所以第三个桶是第一个小分支。这里只有两个节点，ID为0的节点和ID为1的节点，所以我们都保存。
- en: Finally, let's divide the the small branch where our node (the node with ID
    3) resides so we have two tiny branches. The first branch consists of a node with
    ID 2 and the second branch consists of a node with ID 3\. The fourth bucket, or
    the last bucket, would be the branch that consists of node 3.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们把包含我们的节点（ID为3的节点）的小分支分成两个小分支。第一个小分支包括ID为2的节点，第二个小分支包括ID为3的节点。第四个桶，或者最后一个桶，将是包含节点3的分支。
- en: 'We save this one node because it is less than the *k* configuration number:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只保存了一个节点，因为它小于*k*配置数字：
- en: '![](assets/7d329036-5aeb-4c97-a021-9e0881104884.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/7d329036-5aeb-4c97-a021-9e0881104884.png)'
- en: 'The following diagram shows the full four buckets. Each bucket is half of the
    branch in which the source node does not reside. The bucket configuration of different
    nodes are different. The node with ID 11 could have a bucket configuration that
    looks as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了完整的四个桶。每个桶都是源节点不在的分支的一半。不同节点的桶配置是不同的。ID为11的节点可能有以下桶配置：
- en: '![](assets/2b9a9034-ab6e-4ea9-9c39-581024b84b36.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/2b9a9034-ab6e-4ea9-9c39-581024b84b36.png)'
- en: Let's take a look at an example of how a certain node could find another node
    that does not reside in its address book. Imagine the *k* configuration number
    is 1\. The source node is the node with the ID 3 in a tree with 16 leaves. For
    the first bucket (the largest branch that consists of the nodes from ID 8 to ID
    15), the node with ID 3 saves the node with ID 10\. But the node with ID 3 wants
    to find the node with ID 13\. The node with ID 3 contacts the node with ID 10
    with a request, "Can you help me find the node with ID 13?". The node with ID
    10 has saved the node with ID 14 in its corresponding bucket (the branch that
    consists of nodes with IDs 12, 13, 14, and 15). The node with ID 10 gives the
    node with ID 14 to the node with ID 3\. The node with ID 3 asks the same question
    to the node with ID 14, "Can you help me find the node with ID 13?". The node
    with ID 14 does not have it, but it has the node with ID 12 in its bucket (the
    branch that consists of the node with ID 12 and the node with ID 13). The node
    with ID 14 gives the node with ID 12 to the node with ID 3\. The node with ID
    3 asks the same question again to the node with ID 12\. This time, the node with
    ID 12 can give the destination node or the node with ID 13 to the node with ID
    3\. A happy ending!
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子，说明一个特定节点如何找到不在其地址簿中的另一个节点。假设*k*配置号为1。源节点是树中具有16个叶子的ID为3的节点。对于第一个桶（由ID为8到ID为15的节点组成的最大分支），ID为3的节点保存了ID为10的节点。但是ID为3的节点想要找到ID为13的节点。ID为3的节点向ID为10的节点发送请求：“你能帮我找到ID为13的节点吗？”ID为10的节点在其相应的桶中保存了ID为14的节点（由ID为12、13、14和15的节点组成的分支）。ID为10的节点将ID为14的节点交给ID为3的节点。ID为3的节点向ID为14的节点提出同样的问题：“你能帮我找到ID为13的节点吗？”ID为14的节点没有，但它在其桶中保存了ID为12的节点（由ID为12和ID为13的节点组成的分支）。ID为14的节点将ID为12的节点交给ID为3的节点。ID为3的节点再次向ID为12的节点提出同样的问题。这次，ID为12的节点可以将目标节点或ID为13的节点交给ID为3的节点。大团圆！
- en: 'The following diagram shows the nodes:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了节点：
- en: '![](assets/cfbe47f1-27c2-428f-811e-d7b9c40d0bc6.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/cfbe47f1-27c2-428f-811e-d7b9c40d0bc6.png)'
- en: Did you notice how many times the node ID 3 must repeat the request? Four times.
    If this number sounds familiar, that is because this tree has 16 leaves, which
    is 2^(4.) In computer science, the worst case scenario of the amount of hopping
    required before getting to the destination is 2 log *n* + *c*. *n* is how many
    leaves the tree has and *c* is constant number.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 你注意到ID为3的节点必须重复请求多少次了吗？四次。如果这个数字听起来很熟悉，那是因为这棵树有16个叶子，即2^(4.)在计算机科学中，到达目的地所需的跳数的最坏情况是2
    log *n* + *c*。*n*是树有多少叶子，*c*是常数。
- en: 'The tree you have just seen has full nodes; there are no empty leaves or empty
    branches. In the real world, however, there are empty branches and empty leaves.
    Imagine that you have a tree with 1,024 (2^(10)) leaves and the *k* number is
    3\. You launch the first node with the ID 0\. This node will be the source node.
    We will see the tree from the lens of the node with ID 0:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚看到的树有满的节点；没有空叶子或空分支。然而，在现实世界中，有空分支和空叶子。想象一下，你有一棵具有1,024（2^(10)）个叶子的树，*k*号为3。你启动ID为0的第一个节点。这个节点将是源节点。我们将从ID为0的节点的角度看树：
- en: '![](assets/0f92b192-ba20-438a-938f-75da307a0aab.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/0f92b192-ba20-438a-938f-75da307a0aab.png)'
- en: 'Then, you launch the node with ID 800:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，启动ID为800的节点：
- en: '![](assets/32cc83c0-7e33-4d6f-b7e2-53896d14288a.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/32cc83c0-7e33-4d6f-b7e2-53896d14288a.png)'
- en: 'The tree will be split into two buckets. Then, you launch the node with ID
    900 and the node with ID 754:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 树将被分成两个桶。然后，启动ID为900的节点和ID为754的节点：
- en: '![](assets/481a5c67-8515-47b9-bb3c-6723620403ce.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/481a5c67-8515-47b9-bb3c-6723620403ce.png)'
- en: What if we add another node to the bucket? Let's launch the node with ID 1011\.
    The node with ID 0 will ping the least recently used node, which is the node with
    ID 800, to see if it is still alive. If it is, it will check the other nodes.
    If the node with ID 754 is not alive, then this node will be replaced with the
    node with ID 1011\. If all the nodes are still alive, then the node with ID 1011
    will be rejected from the bucket. The reason for this is to avoid new nodes swamping
    the system. We assume that the nodes with longer uptimes are trustworthy and we
    prefer these nodes to new nodes. Let's say we reject the node with ID 1011.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们向桶中添加另一个节点会怎样？让我们启动ID为1011的节点。ID为0的节点将ping最近未使用的节点，即ID为800的节点，以查看其是否仍然存活。如果是，它将检查其他节点。如果ID为754的节点不存活，那么此节点将被ID为1011的节点替换。如果所有节点仍然存活，则ID为1011的节点将被拒绝进入桶。这样做的原因是为了避免新节点淹没系统。我们假设运行时间更长的节点是值得信赖的，我们更喜欢这些节点而不是新节点。假设我们拒绝ID为1011的节点。
- en: 'First, we launch the node with ID 490\. Then, we split the branch where the
    node with ID 0 resides:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们启动ID为490的节点。然后，我们分割ID为0的节点所在的分支：
- en: '![](assets/f415e136-998e-4aec-a5b3-5028bc47442a.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/f415e136-998e-4aec-a5b3-5028bc47442a.png)'
- en: 'Now, let''s add the node with ID 230:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们添加ID为230的节点：
- en: '![](assets/e8f5df06-7ab4-4ea4-a2f7-bf275c745490.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/e8f5df06-7ab4-4ea4-a2f7-bf275c745490.png)'
- en: 'Let''s add the node with ID 60:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们添加ID为60的节点：
- en: '![](assets/9179cbd8-eff2-4dc0-b73f-ca7f4a29ade6.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/9179cbd8-eff2-4dc0-b73f-ca7f4a29ade6.png)'
- en: '...and so on. Every time we add a node in a branch where the source node resides,
    it will split the bucket into two until it reaches the lowest level. If we add
    a node in other branch on which the source node does not live, we add nodes until
    we reach the *k* number.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '...等等。每次我们在源节点所在的分支中添加一个节点，它都会将桶分成两部分，直到达到最低级别。如果我们在源节点不在的其他分支中添加一个节点，我们会一直添加节点，直到达到*k*号。'
- en: You now have a basic understanding of how Kademlia works. This is not, however,
    the whole story. If a node is inserted, a node needs to tell the older nodes of
    its existence. That node also needs to get the contacts from the old node. I mentioned
    that the branch is split when a node is inserted to a branch on which the source
    node resides, but there is a case where the branch is split even when the source
    node does not reside there. This happens because a node is required to keep all
    valid nodes in a branch that has at least *k* nodes if that means the branch in
    which the source node does not reside has to be split.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经基本了解了Kademlia的工作原理。然而，这还不是全部。如果插入一个节点，节点需要告诉旧节点它的存在。该节点还需要从旧节点获取联系人。我提到，当节点插入到源节点所在的分支时，分支会分裂，但即使源节点不驻留在那里，也有分支分裂的情况。这是因为要求节点在至少有*k*个节点的分支中保留所有有效节点，即使这意味着源节点不驻留的分支也必须分裂。
- en: There are other important aspects of Kademlia other than routing algorithms.
    A node is required to republish the key and the value (the data) every hour. This
    is to anticipate the old nodes leaving and the new nodes joining the system. These
    nodes are nearer, so they are more suited to keep the data. There is also an accelerated
    lookup algorithm so that we can use fewer steps when a node is looking for another
    node.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 除了路由算法之外，Kademlia还有其他重要方面。节点需要每小时重新发布密钥和值（数据），以预期旧节点离开和新节点加入系统。这些节点更接近，因此更适合保存数据。还有一种加速查找算法，这样当一个节点正在寻找另一个节点时，我们可以使用更少的步骤。
- en: You can refer to the Kademlia paper for the full specification. [https://pdos.csail.mit.edu/~petar/papers/maymounkov-kademlia-lncs.pdf](https://pdos.csail.mit.edu/~petar/papers/maymounkov-kademlia-lncs.pdf).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考Kademlia论文获取完整的规范。[https://pdos.csail.mit.edu/~petar/papers/maymounkov-kademlia-lncs.pdf](https://pdos.csail.mit.edu/~petar/papers/maymounkov-kademlia-lncs.pdf)。
- en: IPFS uses S/Kademlia, an extended version of Kademlia. It differs from the original
    Kademlia algorithm in that S/Kademlia has some security requirements. Not all
    nodes join the Kademlia distributed hash table with a noble purpose. So, in S/Kademlia,
    to generate the ID of a node, it requires the node to generate a cryptography
    key pair, so it is very difficult to tamper with the communication between the
    nodes. Other requirements include the fact that proof-of-work (like in Bitcoin
    and Ethereum) is used before a node is able to generate its ID. There is also
    some adjustment in the routing algorithm to make sure a node can communicate with
    other nodes in the midst of adversaries, such as nodes that spam the network.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: IPFS使用S/Kademlia，这是Kademlia的扩展版本。它与原始的Kademlia算法不同之处在于S/Kademlia有一些安全要求。并非所有节点加入Kademlia分布式哈希表都是出于崇高的目的。因此，在S/Kademlia中，为了生成节点的ID，需要节点生成一个密码学密钥对，这样很难篡改节点之间的通信。其他要求包括在节点能够生成其ID之前使用工作证明（就像比特币和以太坊中一样）。还有一些路由算法的调整，以确保节点在对手中间能够与其他节点通信，比如那些在网络中发送垃圾信息的节点。
- en: Summary
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have studied IPFS. We started by looking at the motivations
    of the IPFS project and its history. Although IPFS is not a part of the blockchain
    technology, it is similar to blockchain because it complements blockchain technology.
    We then learned about the data structure of the content that we saved in the IPFS
    filesystem. This data structure is Merkle **Directed Acyclic Graph** (**DAG**),
    which is based on the Merkle tree. We created simple Merkle tree and Merkle DAG libraries
    to understand the uniqueness of these data structures. Merkle trees provide an
    easy way to check the integrity of partial data, while Merkle DAGs are used when
    we want to save a directory with files and we want to keep the filenames. Then,
    we learned about the peer-to-peer networking aspect of a Kademlia distributed
    hash table. The distance between nodes is based on the XOR distance. The nodes
    also are kept in buckets, which corresponds to bit addressing. Finally, we showed
    how a node can find others nodes by hopping through the buckets.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了IPFS。我们首先研究了IPFS项目的动机和历史。虽然IPFS不是区块链技术的一部分，但它类似于区块链，因为它是区块链技术的补充。然后，我们了解了IPFS文件系统中保存的内容的数据结构。这个数据结构是基于Merkle树的Merkle有向无环图（DAG）。我们创建了简单的Merkle树和Merkle
    DAG库，以了解这些数据结构的独特性。Merkle树提供了一种简单的方法来检查部分数据的完整性，而Merkle DAG在我们想要保存一个带有文件的目录并且想要保留文件名时使用。然后，我们了解了Kademlia分布式哈希表的点对点网络方面。节点之间的距离是基于XOR距离。节点也被保存在对应于位寻址的桶中。最后，我们展示了节点如何通过跳转桶来找到其他节点。
- en: In the next chapter, we are going to use the IPFS software and interact with
    it programmatically.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用IPFS软件并以编程方式与其交互。
