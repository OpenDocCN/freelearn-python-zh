- en: Chapter 12. The Multiprocessing and Threading Modules
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章。多进程和线程模块
- en: When we eliminate complex, shared state and design around non-strict processing,
    we can leverage parallelism to improve performance. In this chapter, we'll look
    at the multiprocessing and multithreading techniques that are available to us.
    Python library packages become particularly helpful when applied to algorithms
    that permit lazy evaluation.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们消除复杂的共享状态并设计非严格处理时，我们可以利用并行性来提高性能。在本章中，我们将研究可用于我们的多进程和多线程技术。Python库包在应用于允许惰性评估的算法时尤其有帮助。
- en: The central idea here is to distribute a functional program across several threads
    within a process or across several processes. If we've created a sensible functional
    design, we don't have complex interactions among application components; we have
    functions that accept argument values and produce results. This is an ideal structure
    for a process or a thread.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的核心思想是在一个进程内或跨多个进程中分发一个函数式程序。如果我们创建了一个合理的函数式设计，我们就不会有应用程序组件之间的复杂交互；我们有接受参数值并产生结果的函数。这是进程或线程的理想结构。
- en: We'll focus on the `multiprocessing` and `concurrent.futures` modules. These
    modules allow a number of parallel execution techniques.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于“多进程”和“concurrent.futures”模块。这些模块允许多种并行执行技术。
- en: We'll also focus on process-level parallelism instead of multithreading. The
    idea behind process parallelism allows us to ignore Python's **Global Interpreter
    Lock** (**GIL**) and achieve outstanding performance.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将专注于进程级并行而不是多线程。进程并行的理念使我们能够忽略Python的全局解释器锁（GIL），实现出色的性能。
- en: For more information on Python's GIL, see [https://docs.python.org/3.3/c-api/init.html#thread-state-and-the-global-interpreter-lock](https://docs.python.org/3.3/c-api/init.html#thread-state-and-the-global-interpreter-lock).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Python的GIL的更多信息，请参阅[https://docs.python.org/3.3/c-api/init.html#thread-state-and-the-global-interpreter-lock](https://docs.python.org/3.3/c-api/init.html#thread-state-and-the-global-interpreter-lock)。
- en: We won't emphasize features of the `threading` module. This is often used for
    parallel processing. If we have done our functional programming design well, any
    issues that stem from multithreaded write access should be minimized. However,
    the presence of the GIL means that multithreaded applications in **CPython** suffer
    from some small limitations. As waiting for I/O doesn't involve the GIL, it's
    possible that some I/O bound programs might have unusually good performance.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会强调“线程”模块的特性。这经常用于并行处理。如果我们的函数式编程设计得当，那么由多线程写访问引起的任何问题都应该被最小化。然而，GIL的存在意味着在CPython中，多线程应用程序会受到一些小限制的影响。由于等待I/O不涉及GIL，一些I/O绑定的程序可能具有异常良好的性能。
- en: The most effective parallel processing occurs where there are no dependencies
    among the tasks being performed. With some careful design, we can approach parallel
    programming as an ideal processing technique. The biggest difficulty in developing
    parallel programs is coordinating updates to shared resources.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最有效的并行处理发生在正在执行的任务之间没有依赖关系的情况下。通过一些精心设计，我们可以将并行编程视为一种理想的处理技术。开发并行程序的最大困难在于协调对共享资源的更新。
- en: When following functional design patterns and avoiding stateful programs, we
    can also minimize concurrent updates to shared objects. If we can design software
    where lazy, non-strict evaluation is central, we can also design software where
    concurrent evaluation is possible.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在遵循函数式设计模式并避免有状态的程序时，我们还可以最小化对共享对象的并发更新。如果我们能够设计出中心是惰性、非严格评估的软件，我们也可以设计出可以进行并发评估的软件。
- en: Programs will always have some strict dependencies where ordering of operations
    matters. In the `2*(3+a)` expression, the `(3+a)` subexpression must be evaluated
    first. However, when working with a collection, we often have situations where
    the processing order among items in the collection doesn't matter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 程序总是会有一些严格的依赖关系，其中操作的顺序很重要。在“2*(3+a)”表达式中，“(3+a)”子表达式必须首先进行评估。然而，在处理集合时，我们经常遇到集合中项目的处理顺序并不重要的情况。
- en: 'Consider the following two examples:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下两个例子：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Both of these commands have the same result even though the items are evaluated
    in the reverse order.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管项目以相反的顺序进行评估，但这两个命令都会产生相同的结果。
- en: 'Indeed, even this following command snippet has the same result:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，即使是以下命令片段也会产生相同的结果：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The evaluation order is random. As the evaluation of each item is independent,
    the order of evaluation doesn't matter. This is the case with many algorithms
    that permit non-strict evaluation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 评估顺序是随机的。由于每个项目的评估是独立的，评估顺序并不重要。许多允许非严格评估的算法都是如此。
- en: What concurrency really means
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并发真正意味着什么
- en: In a small computer, with a single processor and a single core, all evaluations
    are serialized only through the core of the processor. The operating system will
    interleave multiple processes and multiple threads through clever time-slicing
    arrangements.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在一台小型计算机上，只有一个处理器和一个核心，所有评估都是通过处理器的核心进行串行化的。操作系统将通过巧妙的时间切片安排交错执行多个进程和多个线程。
- en: On a computer with multiple CPUs or multiple cores in a single CPU, there can
    be some actual concurrent processing of CPU instructions. All other concurrency
    is simulated through time slicing at the OS level. A Mac OS X laptop can have
    200 concurrent processes that share the CPU; this is far more processes than the
    number of available cores. From this, we can see that the OS time slicing is responsible
    for most of the apparently concurrent behavior.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有多个CPU或单个CPU中的多个核心的计算机上，可以对CPU指令进行一些实际的并发处理。所有其他并发都是通过操作系统级别的时间切片模拟的。Mac OS
    X笔记本电脑可以有200个共享CPU的并发进程；这比可用核心数多得多。由此可见，操作系统的时间切片负责大部分表面上的并发行为。
- en: The boundary conditions
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 边界条件
- en: Let's consider a hypothetical algorithm which has ![The boundary conditions](graphics/B03652_12_01.jpg).
    Assume that there is an inner loop that involves 1,000 bytes of Python code. When
    processing 10,000 objects, we're executing 100 billion Python operations. This
    is the essential processing budget. We can try to allocate as many processes and
    threads as we feel might be helpful, but the processing budget can't change.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个假设的算法，其中有![边界条件](graphics/B03652_12_01.jpg)。假设有一个涉及1000字节Python代码的内部循环。在处理10000个对象时，我们执行了1000亿次Python操作。这是基本的处理预算。我们可以尝试分配尽可能多的进程和线程，但处理预算是不能改变的。
- en: The individual CPython bytecode doesn't have a simple execution timing. However,
    a long-term average on a Mac OS X laptop shows that we can expect about 60 MB
    of code to be executed per second. This means that our 100 billion bytecode operation
    will take about 1,666 seconds, or 28 minutes.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 单个CPython字节码没有简单的执行时间。然而，在Mac OS X笔记本上的长期平均值显示，我们可以预期每秒执行大约60MB的代码。这意味着我们的1000亿字节码操作将需要大约1666秒，或28分钟。
- en: 'If we have a dual processor, four-core computer, then we might cut the elapsed
    time to 25 percent of the original total: 7 minutes. This presumes that we can
    partition the work into four (or more) independent OS processes.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一台双处理器、四核的计算机，那么我们可能将经过时间缩短到原始总时间的25%：7分钟。这假设我们可以将工作分成四个（或更多）独立的操作系统进程。
- en: The important consideration here is that our budget of 100 billion bytecodes
    can't be changed. Parallelism won't magically reduce the workload. It can only
    change the schedule to, perhaps, reduce the elapsed time.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的重要考虑因素是我们的1000亿字节码的预算是不能改变的。并行性不会神奇地减少工作量。它只能改变时间表，也许可以减少经过时间。
- en: Switching to a better algorithm which is ![The boundary conditions](graphics/B03652_12_02.jpg)
    can reduce the workload to 132 MB of operations. At 60 MBps, this workload is
    considerably smaller. Parallelism won't have the kind of dramatic improvements
    that algorithm change will have.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 切换到一个更好的算法可以将工作量减少到132MB的操作。以60MBps的速度，这个工作量要小得多。并行性不会像算法改变那样带来戏剧性的改进。
- en: Sharing resources with process or threads
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与进程或线程共享资源
- en: The OS assures that there is little or no interaction between processes. For
    two processes to interact, some common OS resource must be explicitly shared.
    This can be a common file, a specific shared memory object, or a semaphore with
    a shared state between the processes. Processes are inherently independent, interaction
    is exceptional.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统确保进程之间几乎没有交互。要使两个进程交互，必须显式共享一些公共的操作系统资源。这可以是一个共享文件，一个特定的共享内存对象，或者是进程之间共享状态的信号量。进程本质上是独立的，交互是例外。
- en: Multiple threads, on the other hand, are part of a single process; all threads
    of a process share OS resources. We can make an exception to get some thread-local
    memory that can be freely written without interference from other threads. Outside
    thread-local memory, operations that write to memory can set the internal state
    of the process in a potentially unpredictable order. Explicit locking must be
    used to avoid problems with these stateful updates. As noted previously, the overall
    sequence of instruction executions is rarely, strictly speaking, concurrent. The
    instructions from concurrent threads and processes are generally interleaved in
    an unpredictable order. With threading comes the possibility of destructive updates
    to shared variables and the need for careful locking. With parallel processing
    come the overheads of OS-level process scheduling.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，多个线程是单个进程的一部分；进程的所有线程共享操作系统资源。我们可以例外地获得一些线程本地内存，可以自由写入而不受其他线程干扰。除了线程本地内存，写入内存的操作可能以潜在的不可预测顺序设置进程的内部状态。必须使用显式锁定来避免这些有状态更新的问题。正如之前所指出的，指令执行的整体顺序很少是严格并发的。并发线程和进程的指令通常以不可预测的顺序交错执行。使用线程会带来对共享变量的破坏性更新的可能性，需要仔细的锁定。并行处理会带来操作系统级进程调度的开销。
- en: Indeed, even at the hardware level, there are some complex memory write situations.
    For more information on issues in memory writes, visit [http://en.wikipedia.org/wiki/Memory_disambiguation](http://en.wikipedia.org/wiki/Memory_disambiguation).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，即使在硬件级别，也存在一些复杂的内存写入情况。有关内存写入问题的更多信息，请访问[http://en.wikipedia.org/wiki/Memory_disambiguation](http://en.wikipedia.org/wiki/Memory_disambiguation)。
- en: The existence of concurrent object updates is what raises havoc with trying
    to design multithreaded applications. Locking is one way to avoid concurrent writes
    to shared objects. Avoiding shared objects is another viable design technique.
    This is more applicable to functional programming.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 并发对象更新的存在是设计多线程应用程序时所面临的困难。锁定是避免对共享对象进行并发写入的一种方法。避免共享对象是另一种可行的设计技术。这更适用于函数式编程。
- en: In CPython, the GIL is used to assure that OS thread scheduling will not interfere
    with updates to Python data structures. In effect, the GIL changes the granularity
    of scheduling from machine instructions to Python virtual machine operations.
    Without the GIL, it's possible that an internal data structure might be corrupted
    by the interleaved interaction of competing threads.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPython中，GIL用于确保操作系统线程调度不会干扰对Python数据结构的更新。实际上，GIL将调度的粒度从机器指令改变为Python虚拟机操作。没有GIL，内部数据结构可能会被竞争线程的交错交互所破坏。
- en: Where benefits will accrue
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利益将会产生的地方
- en: A program that does a great deal of calculation and relatively little I/O will
    not see much benefit from concurrent processing. If a calculation has a budget
    of 28 minutes of computation, then interleaving the operations in different ways
    won't have very much impact. Switching from strict to non-strict evaluation of
    100 billion bytecodes won't shrink the elapsed execution time.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一个进行大量计算而相对较少I/O的程序不会从并发处理中获得太多好处。如果一个计算有28分钟的计算时间，那么以不同的方式交错操作不会产生太大影响。从严格到非严格评估1000亿个字节码不会缩短经过的执行时间。
- en: However, if a calculation involves a great deal of I/O, then interleaving CPU
    processing and I/O requests can have an impact on performance. Ideally, we'd like
    to do our computations on some pieces of data while waiting for the OS to complete
    input of the next pieces of data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果一个计算涉及大量I/O，那么交错CPU处理和I/O请求可能会影响性能。理想情况下，我们希望在等待操作系统完成下一批数据输入时对一些数据进行计算。
- en: 'We have two approaches to interleaving computation and I/O. They are as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两种交错计算和I/O的方法。它们如下：
- en: We can try to interleave I/O and calculation for the entire problem as a whole.
    We might create a pipeline of processing with read, compute, and write as operations.
    The idea is to have individual data objects flowing through the pipe from one
    stage to the next. Each stage can operate in parallel.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以尝试将I/O和计算整体问题交错进行。我们可以创建一个包含读取、计算和写入操作的处理流水线。这个想法是让单独的数据对象从一个阶段流向下一个阶段。每个阶段可以并行操作。
- en: We can decompose the problem into separate, independent pieces that can be processed
    from the beginning to the end in parallel.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将问题分解成可以并行处理的独立部分，从头到尾进行处理。
- en: The differences between these approaches aren't crisp; there is a blurry middle
    region that's not clearly one or the other. For example, multiple parallel pipelines
    are a hybrid mixture of both designs. There are some formalisms that make it somewhat
    easier to design concurrent programs. The **Communicating Sequential Processes**
    (**CSP**) paradigm can help design message-passing applications. Packages such
    as `pycsp` can be used to add CSP formalisms to Python.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法之间的差异并不明显；有一个模糊的中间区域，不太清楚是哪一个。例如，多个并行流水线是两种设计的混合体。有一些形式化方法可以更容易地设计并发程序。**通信顺序进程**（**CSP**）范式可以帮助设计消息传递应用程序。像`pycsp`这样的包可以用来向Python添加CSP形式化方法。
- en: I/O-intensive programs often benefit from concurrent processing. The idea is
    to interleave I/O and processing. CPU-intensive programs rarely benefit from attempting
    concurrent processing.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: I/O密集型程序通常受益于并发处理。这个想法是交错I/O和处理。CPU密集型程序很少受益于尝试并发处理。
- en: Using multiprocessing pools and tasks
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用多处理池和任务
- en: To make non-strict evaluation available in a larger context, the `multiprocessing`
    package introduces the concept of a `Pool` object. We can create a `Pool` object
    of concurrent worker processes, assign tasks to them, and expect the tasks to
    be executed concurrently. As noted previously, this creation does not actually
    mean simultaneous creation of `Pool` objects. It means that the order is difficult
    to predict because we've allowed OS scheduling to interleave execution of multiple
    processes. For some applications, this permits more work to be done in less elapsed
    time.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在更大的上下文中使用非严格评估，`multiprocessing`包引入了`Pool`对象的概念。我们可以创建一个并发工作进程的`Pool`对象，将任务分配给它们，并期望任务并发执行。正如之前所述，这个创建并不实际意味着同时创建`Pool`对象。这意味着顺序很难预测，因为我们允许操作系统调度交错执行多个进程。对于一些应用程序，这允许在更少的经过时间内完成更多的工作。
- en: To make the most use of this capability, we need to decompose our application
    into components for which non-strict concurrent execution is beneficial. We'd
    like to define discrete tasks that can be processed in an indefinite order.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分利用这一能力，我们需要将应用程序分解成组件，对于这些组件，非严格并发执行是有益的。我们希望定义可以以不确定顺序处理的离散任务。
- en: An application that gathers data from the Internet via web scraping is often
    optimized through parallel processing. We can create a `Pool` object of several
    identical website scrapers. The tasks are URLs to be analyzed by the pooled processes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过网络抓取从互联网收集数据的应用程序通常通过并行处理进行优化。我们可以创建几个相同的网站抓取器的`Pool`对象。任务是由池化进程分析的URL。
- en: An application that analyzes multiple logfiles is also a good candidate for
    parallelization. We can create a `Pool` object of analytical processes. We can
    assign each logfile to an analyzer; this allows reading and analysis to proceed
    in parallel among the various workers in the `Pool` object. Each individual worker
    will involve serialized I/O and computation. However, one worker can be analyzing
    the computation while other workers are waiting for I/O to complete.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 分析多个日志文件的应用程序也是并行化的一个很好的候选。我们可以创建一个分析进程的`Pool`对象。我们可以将每个日志文件分配给一个分析器；这允许在`Pool`对象的各个工作进程之间并行进行读取和分析。每个单独的工作进程将涉及串行I/O和计算。然而，一个工作进程可以在其他工作进程等待I/O完成时分析计算。
- en: Processing many large files
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理许多大文件
- en: 'Here is an example of a multiprocessing application. We''ll scrape **Common
    Log Format** (**CLF**) lines in web logfiles. This is the generally used format
    for an access log. The lines tend to be long, but look like the following when
    wrapped to the book''s margins:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个多处理应用程序的例子。我们将在网络日志文件中抓取**通用日志格式**（**CLF**）行。这是访问日志的通用格式。这些行往往很长，但在书的边距处包装时看起来像下面这样：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We often have large numbers of large files that we'd like to analyze. The presence
    of many independent files means that concurrency will have some benefit for our
    scraping process.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常有大量大文件需要分析。许多独立文件的存在意味着并发对我们的抓取过程有一些好处。
- en: 'We''ll decompose the analysis into two broad areas of functionality. The first
    phase of any processing is the essential parsing of the logfiles to gather the
    relevant pieces of information. We''ll decompose this into four stages. They are
    as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分解分析为两个广泛的功能领域。任何处理的第一阶段都是解析日志文件以收集相关信息的基本阶段。我们将这分解为四个阶段。它们如下：
- en: All the lines from multiple source logfiles are read.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取来自多个源日志文件的所有行。
- en: Then, create simple namedtuples from the lines of log entries in a collection
    of files.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，从文件集合中的日志条目的行创建简单的命名元组。
- en: The details of more complex fields such as dates and URLs are parsed.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更复杂字段的细节，如日期和URL，被解析。
- en: Uninteresting paths from the logs are rejected; we can also think of this as
    passing only the interesting paths.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 日志中的无趣路径被拒绝；我们也可以认为这是只传递有趣的路径。
- en: Once past the parsing phase, we can perform a large number of analyses. For
    our purposes in demonstrating the `multiprocessing` module, we'll look at a simple
    analysis to count occurrences of specific paths.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦过了解析阶段，我们就可以执行大量的分析。为了演示`multiprocessing`模块，我们将进行一个简单的分析，计算特定路径的出现次数。
- en: The first portion, reading from source files, involves the most input processing.
    The Python use of file iterators will translate into lower-level OS requests for
    buffering of data. Each OS request means that the process must wait for the data
    to become available.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 从源文件中读取的第一部分涉及最多的输入处理。Python对文件迭代器的使用将转换为更低级别的OS请求来缓冲数据。每个OS请求意味着进程必须等待数据变得可用。
- en: Clearly, we want to interleave the other operations so that they are not waiting
    for I/O to complete. We can interleave operations along a spectrum from individual
    rows to whole files. We'll look at interleaving whole files first, as this is
    relatively simple to implement.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们希望交错进行其他操作，以便它们不必等待I/O完成。我们可以沿着从单个行到整个文件的光谱交错操作。我们将首先查看交错整个文件，因为这相对简单实现。
- en: 'The functional design for parsing Apache CLF files can look as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 解析Apache CLF文件的功能设计可以如下所示：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We've decomposed the larger parsing problem into a number of functions that
    will handle each portion of the parsing problem. The `local_gzip()` function reads
    rows from locally-cached GZIP files. The `access_iter()` function creates a simple
    `namedtuple` object for each row in the access log. The `access_detail_iter()`
    function will expand on some of the more difficult to parse fields. Finally, the
    `path_filter()` function will discard some paths and file extensions that aren't
    of much analytical value.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将更大的解析问题分解为将处理解析问题的各部分的多个函数。`local_gzip()`函数从本地缓存的GZIP文件中读取行。`access_iter()`函数为访问日志中的每一行创建一个简单的`namedtuple`对象。`access_detail_iter()`函数将扩展一些更难解析的字段。最后，`path_filter()`函数将丢弃一些分析价值不高的路径和文件扩展名。
- en: Parsing log files – gathering the rows
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解析日志文件-收集行
- en: 'Here is the first stage in parsing a large number of files: reading each file
    and producing a simple sequence of lines. As the logfiles are saved in the `.gzip`
    format, we need to open each file with the `gzip.open()` function instead of the
    `io.open()` function or the `__builtins__.open()` function.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是解析大量文件的第一阶段：读取每个文件并生成一系列简单的行。由于日志文件以`.gzip`格式保存，我们需要使用`gzip.open()`函数而不是`io.open()`函数或`__builtins__.open()`函数来打开每个文件。
- en: 'The `local_gzip()` function reads lines from locally cached files, as shown
    in the following command snippet:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`local_gzip()`函数从本地缓存的文件中读取行，如下命令片段所示：'
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The preceding function iterates through all files. For each file, the yielded
    value is a generator function that will iterate through all lines within that
    file. We've encapsulated several things, including wildcard file matching, the
    details of opening a logfile compressed with the `.gzip` format, and breaking
    a file into a sequence of lines without any trailing `\n` characters.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的函数遍历所有文件。对于每个文件，生成的值是一个生成器函数，它将遍历该文件中的所有行。我们封装了一些东西，包括通配符文件匹配、打开以`.gzip`格式压缩的日志文件的细节，以及将文件分解为一系列不带任何尾随`\n`字符的行。
- en: The essential design pattern here is to yield values that are generator expressions
    for each file. The preceding function can be restated as a function and a mapping
    that applies that function to each file.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的基本设计模式是产生每个文件的生成器表达式的值。前面的函数可以重新表述为一个函数和一个将该函数应用于每个文件的映射。
- en: 'There are several other ways to produce similar output. For example, here is
    an alternative version of the inner `for` loop in the preceding example. The `line_iter()`
    function will also emit lines of a given file:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他几种方法可以产生类似的输出。例如，以下是前面示例中内部`for`循环的另一种替代版本。`line_iter()`函数还将发出给定文件的行：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `line_iter()` function applies the `gzip.open()` function and some line
    cleanup. We can use a mapping to apply the `line_iter()` function to all files
    that match a pattern as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`line_iter()`函数应用`gzip.open()`函数和一些行清理。我们可以使用映射将`line_iter()`函数应用于符合模式的所有文件，如下所示：'
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: While this alternative mapping is succinct, it has the disadvantage of leaving
    open file objects lying around waiting to be properly garbage-collected when there
    are no more references. When processing a large number of files, this seems like
    a needless bit of overhead. For this reason, we'll focus on the `local_gzip()`
    function shown previously.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种替代映射很简洁，但它的缺点是在没有更多引用时，会留下等待被正确垃圾回收的打开文件对象。处理大量文件时，这似乎是一种不必要的开销。因此，我们将专注于先前显示的`local_gzip()`函数。
- en: The previous alternative mapping has the distinct advantage of fitting well
    with the way the `multiprocessing` module works. We can create a worker pool and
    map tasks (such as file reading) to the pool of processes. If we do this, we can
    read these files in parallel; the open file objects will be part of separate processes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的替代映射具有与“多进程”模块配合良好的明显优势。我们可以创建一个工作进程池，并将任务（如文件读取）映射到进程池中。如果这样做，我们可以并行读取这些文件；打开的文件对象将成为单独的进程的一部分。
- en: An extension to this design will include a second function to transfer files
    from the web host using FTP. As the files are collected from the web server, they
    can be analyzed using the `local_gzip()` function.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对这种设计的扩展将包括第二个函数，用于使用FTP从Web主机传输文件。当从Web服务器收集文件时，可以使用`local_gzip()`函数对其进行分析。
- en: The results of the `local_gzip()` function are used by the `access_iter()` function
    to create namedtuples for each row in the source file that describes a file access.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`local_gzip()`函数的结果被`access_iter()`函数使用，为源文件中描述文件访问的每一行创建命名元组。'
- en: Parsing log lines into namedtuples
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将日志行解析为命名元组
- en: Once we have access to all of the lines of each logfile, we can extract details
    of the access that's described. We'll use a regular expression to decompose the
    line. From there, we can build a `namedtuple` object.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们可以访问每个日志文件的所有行，我们就可以提取描述的访问的详细信息。我们将使用正则表达式来分解行。从那里，我们可以构建一个`namedtuple`对象。
- en: 'Here is a regular expression to parse lines in a CLF file:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是解析CLF文件中行的正则表达式：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can use this regular expression to break each row into a dictionary of nine
    individual data elements. The use of `[]`and `"` to delimit complex fields such
    as the `time`, `request`, `referrer`, and `user_agent` parameters are handled
    gracefully by the namedtuple pattern.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个正则表达式将每一行分解为九个单独的数据元素的字典。使用`[]`和`"`来界定复杂字段（如`time`、`request`、`referrer`和`user_agent`参数）的方式由命名元组模式优雅地处理。
- en: 'Each individual access can be summarized as a `namedtuple()` function as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单独的访问可以总结为一个`namedtuple()`函数，如下所示：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We've taken pains to assure that the `namedtuple` function's fields match the
    regular expression group names in the `(?P<name>)` constructs for each portion
    of the record. By making sure the names match, we can very easily transform the
    parsed dictionary into a tuple for further processing.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经费心确保`namedtuple`函数的字段与`(?P<name>)`构造中每条记录的正则表达式组名匹配。通过确保名称匹配，我们可以非常容易地将解析的字典转换为元组以进行进一步处理。
- en: 'Here is the `access_iter()` function that requires each file to be represented
    as an iterator over the lines of the file:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`access_iter()`函数，它要求每个文件都表示为文件行的迭代器：
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output from the `local_gzip()` function is a sequence of sequences. The
    outer sequence consists of individual logfiles. For each file, there is an iterable
    sequence of lines. If the line matches the given pattern, it's a file access of
    some kind. We can create an `Access` namedtuple from the `match` dictionary.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`local_gzip()`函数的输出是一个序列的序列。外部序列由单独的日志文件组成。对于每个文件，都有一个可迭代的行序列。如果行与给定模式匹配，它就是某种文件访问。我们可以从`match`字典中创建一个`Access`命名元组。'
- en: The essential design pattern here is to build a static object from the results
    of a parsing function. In this case, the parsing function is a regular expression
    matcher.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的基本设计模式是从解析函数的结果构建静态对象。在这种情况下，解析函数是一个正则表达式匹配器。
- en: 'There are some alternative ways to do this. For example, we can revise the
    use of the `map()` function as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些替代方法可以做到这一点。例如，我们可以修改`map()`函数的使用如下：
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The preceding alternative function embodies just the essential parse and builds
    an `Access` object processing. It will either return an `Access` or a `None` object.
    This differs from the version above that also filters items that don't match the
    regular expression.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的替代函数仅包含基本的解析和构建`Access`对象的处理。它将返回一个`Access`或`None`对象。这与上面的版本不同，后者还过滤了不匹配正则表达式的项目。
- en: 'Here is how we can use this function to flatten logfiles into a single stream
    of the `Access` objects:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们如何使用此函数将日志文件展平为`Access`对象的单个流：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This shows how we can transform the output from the `local_gzip()` function
    into a sequence of the `Access` instances. In this case, we apply the `access_builder()`
    function to the nested iterator of iterable structure that results from reading
    a collection of files.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了我们如何将`local_gzip()`函数的输出转换为`Access`实例的序列。在这种情况下，我们将`access_builder()`函数应用于从读取文件集合中产生的嵌套迭代器的可迭代结构。
- en: Our point here is to show that we have a number of functional styles for parsing
    files. In [Chapter 4](ch04.html "Chapter 4. Working with Collections"), *Working
    with Collections* we showed very simple parsing. Here, we're performing more complex
    parsing, using a variety of techniques.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的重点在于展示我们有许多解析文件的功能样式。在[第4章](ch04.html "第4章。与集合一起工作")中，*与集合一起工作*，我们展示了非常简单的解析。在这里，我们正在执行更复杂的解析，使用各种技术。
- en: Parsing additional fields of an Access object
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解析访问对象的其他字段
- en: The initial `Access` object created previously doesn't decompose some inner
    elements in the nine fields that comprise an access log line. We'll parse those
    items separately from the overall decomposition into high-level fields. It keeps
    the regular expressions for parsing somewhat simpler if we break this down into
    separate parsing operations.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 先前创建的初始`Access`对象并没有分解组成访问日志行的九个字段中的一些内部元素。我们将这些项目分别从整体分解成高级字段。如果我们将这个分解成单独的解析操作，可以使解析正则表达式变得更简单。
- en: 'The resulting object is a `namedtuple` object that will wrap the original `Access`
    tuple. It will have some additional fields for the details parsed separately:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 结果对象是一个`namedtuple`对象，它将包装原始的`Access`元组。它将具有一些额外的字段，用于单独解析的细节：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `access` attribute is the original `Access` object. The `time` attribute
    is the parsed `access.time` string. The `method`, `url`, and `protocol` attributes
    come from decomposing the `access.request` field. The `referrer` attribute is
    a parsed URL. The `agent` attribute can also be broken down into fine-grained
    fields. Here are the fields that comprise agent details:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: “access”属性是原始的“Access”对象。“time”属性是解析的“access.time”字符串。“method”、“url”和“protocol”属性来自分解“access.request”字段。“referrer”属性是解析的URL。“agent”属性也可以分解为细粒度字段。以下是组成代理详情的字段：
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: These fields reflect the most common syntax for agent descriptions. There is
    considerable variation in this area, but this particular subset of values seems
    to be reasonably common.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这些字段反映了代理描述的最常见语法。在这个领域有相当大的变化，但这个特定的值子集似乎是相当常见的。
- en: 'We''ll combine three detailed parser functions into a single overall parsing
    function. Here is the first part with the various detail parsers:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将三个详细的解析器函数合并成一个整体解析函数。这是第一部分，包括各种详细解析器：
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We've written three parsers for the HTTP request, the time stamp, and the user
    agent information. The request is usually a three-word string such as `GET /some/path
    HTTP/1.1`. The `parse_request()` function extracts these three space-separated
    values. In the unlikely event that the path has spaces in it, we'll extract the
    first word and the last word as the method and protocol; all the remaining words
    are part of the path.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经为HTTP请求、时间戳和用户代理信息编写了三个解析器。请求通常是一个包含三个单词的字符串，例如“GET /some/path HTTP/1.1”。
    “parse_request（）”函数提取这三个以空格分隔的值。如果路径中有空格，我们将提取第一个单词和最后一个单词作为方法和协议；其余所有单词都是路径的一部分。
- en: Time parsing is delegated to the `datetime` module. We've simply provided the
    proper format in the `parse_time()` function.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 时间解析委托给“datetime”模块。我们在“parse_time（）”函数中提供了正确的格式。
- en: Parsing the user agent is challenging. There are many variations; we've chosen
    a common one for the `parse_agent()` function. If the user agent matches the given
    regular expression, we'll have the attributes of an `AgentDetails` namedtuple.
    If the user agent information doesn't match the regular expression, we'll simply
    use the `None` value instead.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 解析用户代理是具有挑战性的。有许多变化；我们为“parse_agent（）”函数选择了一个常见的变体。如果用户代理与给定的正则表达式匹配，我们将拥有“AgentDetails”命名元组的属性。如果用户代理信息不匹配正则表达式，我们将简单地使用“None”值。
- en: 'We''ll use these three parsers to build `AccessDetails` instances from the
    given `Access` objects. The main body of the `access_detail_iter()` function looks
    as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这三个解析器从给定的“访问”对象构建“AccessDetails”实例。 “access_detail_iter（）”函数的主体如下：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We've used a similar design pattern to the previous `access_iter()` function.
    A new object is built from the results of parsing some input object. The new `AccessDetails`
    object will wrap the previous `Access` object. This technique allows us to use
    immutable objects, yet still contain more refined information.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用了与之前的“access_iter（）”函数类似的设计模式。从解析某个输入对象的结果构建了一个新对象。新的“AccessDetails”对象将包装先前的“Access”对象。这种技术允许我们使用不可变对象，但仍然包含更精细的信息。
- en: 'This function is essentially a mapping from an `Access` object to an `AccessDetails`
    object. We can imagine changing the design to use `map()` as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数本质上是从“Access”对象到“AccessDetails”对象的映射。我们可以想象改变设计以使用“map（）”如下：
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We've changed the construction of the `AccessDetails` object to be a function
    that returns a single value. We can map that function to the iterable input stream
    of the `Access` objects. This also fits nicely with the way the `multiprocessing`
    module works.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经更改了“AccessDetails”对象的构造方式，使其成为返回单个值的函数。我们可以将该函数映射到“Access”对象的可迭代输入流。这也与“multiprocessing”模块的工作方式非常匹配。
- en: In an object-oriented programming environment, these additional parsers might
    be method functions or properties of a class definition. The advantage of this
    design is that items aren't parsed unless they're needed. This particular functional
    design parses everything, assuming that it's going to be used.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在面向对象的编程环境中，这些额外的解析器可能是类定义的方法函数或属性。这种设计的优点是，除非需要，否则不会解析项目。这种特定的功能设计解析了一切，假设它将被使用。
- en: A different function design might rely on the three parser functions to extract
    and parse the various elements from a given `Access` object as needed. Rather
    than using the `details.time` attribute, we'd use the `parse_time(access.time)`
    parameter. The syntax is longer, but the attribute is only parsed as needed.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的函数设计可能依赖于三个解析器函数，根据需要从给定的“Access”对象中提取和解析各个元素。我们将使用“parse_time（access.time）”参数，而不是使用“details.time”属性。语法更长，但只有在需要时才解析属性。
- en: Filtering the access details
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过滤访问细节
- en: We'll look at several filters for the `AccessDetails` objects. The first is
    a collection of filters that reject a lot of overhead files that are rarely interesting.
    The second filter will be part of the analysis functions, which we'll look at
    later.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看“AccessDetails”对象的几个过滤器。第一个是一组过滤器，拒绝了许多很少有趣的开销文件。第二个过滤器将成为分析函数的一部分，我们稍后会看到。
- en: 'The `path_filter()` function is a combination of three functions:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: “path_filter（）”函数是三个函数的组合：
- en: Exclude empty paths.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 排除空路径。
- en: Exclude some specific filenames.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 排除一些特定的文件名。
- en: Exclude files that have a given extension.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 排除具有特定扩展名的文件。
- en: 'An optimized version of the `path_filter()` function looks as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: “path_filter（）”函数的优化版本如下：
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: For each individual `AccessDetails` object, we'll apply three filter tests.
    If the path is essentially empty or the part includes one of the excluded names
    or the path's final name has an excluded extension, the item is quietly ignored.
    If the path doesn't match any of these criteria, it's potentially interesting
    and is part of the results yielded by the `path_filter()` function.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个单独的`AccessDetails`对象，我们将应用三个过滤测试。如果路径基本为空，或者部分包括被排除的名称之一，或者路径的最终名称具有被排除的扩展名，该项目将被静默地忽略。如果路径不符合这些标准之一，它可能是有趣的，并且是`path_filter()`函数产生的结果的一部分。
- en: This is an optimization because all of the tests are applied using an imperative
    style `for` loop body.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个优化，因为所有的测试都是使用命令式风格的`for`循环体应用的。
- en: 'The design started with each test as a separate first-class filter-style function.
    For example, we might have a function like the following to handle empty paths:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 设计始于每个测试作为一个单独的一流过滤器风格函数。例如，我们可能有一个处理空路径的函数如下：
- en: '[PRE18]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This function simply assures that the path contains a name. We can use the
    `filter()` function as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数只是确保路径包含一个名称。我们可以使用`filter()`函数如下：
- en: '[PRE19]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can write similar tests for the `non_excluded_names()` and `non_excluded_ext()`
    functions. The entire sequence of `filter()` functions will look as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为`non_excluded_names()`和`non_excluded_ext()`函数编写类似的测试。整个`filter()`函数序列将如下所示：
- en: '[PRE20]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This applies each `filter()` function to the results of the previous `filter()`
    function. The empty paths are rejected; from this subset, the excluded names and
    the excluded extensions are rejected. We can also state the preceding example
    as a series of assignment statements as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这将每个`filter()`函数应用于前一个`filter()`函数的结果。空路径将被拒绝；从这个子集中，被排除的名称和被排除的扩展名也将被拒绝。我们也可以将前面的示例陈述为一系列赋值语句如下：
- en: '[PRE21]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This version has the advantage of being slightly easier to expand when we add
    new filter criteria.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个版本的优点是在添加新的过滤条件时稍微更容易扩展。
- en: Note
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The use of generator functions (such as the `filter()` function) means that
    we aren't creating large intermediate objects. Each of the intermediate variables,
    `ne`, `nx_name`, and `nx_ext`, are proper lazy generator functions; no processing
    is done until the data is consumed by a client process.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用生成器函数（如`filter()`函数）意味着我们不会创建大型的中间对象。每个中间变量`ne`、`nx_name`和`nx_ext`都是适当的惰性生成器函数；直到数据被客户端进程消耗之前，都不会进行处理。
- en: While elegant, this suffers from a small inefficiency because each function
    will need to parse the path in the `AccessDetails` object. In order to make this
    more efficient, we will need to wrap a `path.split('/')` function with the `lru_cache`
    attribute.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然优雅，但这会导致一些小的低效，因为每个函数都需要解析`AccessDetails`对象中的路径。为了使这更有效，我们需要使用`lru_cache`属性包装`path.split('/')`函数。
- en: Analyzing the access details
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析访问细节
- en: We'll look at two analysis functions we can use to filter and analyze the individual
    `AccessDetails` objects. The first function, a `filter()` function, will pass
    only specific paths. The second function will summarize the occurrences of each
    distinct path.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看看两个分析函数，我们可以用来过滤和分析单个`AccessDetails`对象。第一个函数，一个`filter()`函数，将只传递特定的路径。第二个函数将总结每个不同路径的出现次数。
- en: 'We''ll define the `filter()` function as a small function and combine this
    with the built-in `filter()` function to apply the function to the details. Here
    is the composite `filter()` function:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`filter()`函数定义为一个小函数，并将其与内置的`filter()`函数结合起来，将该函数应用于细节。这是复合`filter()`函数：
- en: '[PRE22]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We've defined a rule, the `book_in_path()` attribute, that we'll apply to each
    `AccessDetails` object. If the path is not empty and the first-level attribute
    of the path is `book`, then we're interested in these objects. All other `AccessDetails`
    objects can be quietly rejected.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个规则，即`book_in_path()`属性，我们将应用于每个`AccessDetails`对象。如果路径不为空，并且路径的第一级属性是`book`，那么我们对这些对象感兴趣。所有其他`AccessDetails`对象可以被静默地拒绝。
- en: 'Here is the final reduction that we''re interested in:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们感兴趣的最终减少：
- en: '[PRE23]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This function will produce a `Counter()` object that shows the frequency of
    each path in an `AccessDetails` object. In order to focus on a particular set
    of paths, we'll use the `reduce_total(book_filter(details))` method. This provides
    a summary of only items that are passed by the given filter.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将产生一个`Counter()`对象，显示了`AccessDetails`对象中每个路径的频率。为了专注于特定的路径集，我们将使用`reduce_total(book_filter(details))`方法。这提供了一个仅显示通过给定过滤器的项目的摘要。
- en: The complete analysis process
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完整的分析过程
- en: 'Here is the composite `analysis()` function that digests a collection of logfiles:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这是消化日志文件集合的复合`analysis()`函数：
- en: '[PRE24]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The preceding command snippet will work with a single filename or file pattern.
    It applies a standard set of parsing functions, `path_filter()`, `access_detail_iter()`,
    `access_iter()`, and `local_gzip()`, to a filename or file pattern and returns
    an iterable sequence of the `AccessDetails` objects. It then applies our analytical
    filter and reduction to that sequence of the `AccessDetails` objects. The result
    is a `Counter` object that shows the frequency of access for certain paths.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令片段将适用于单个文件名或文件模式。它将一组标准的解析函数`path_filter()`、`access_detail_iter()`、`access_iter()`和`local_gzip()`应用于文件名或文件模式，并返回`AccessDetails`对象的可迭代序列。然后，它将我们的分析过滤器和减少器应用于`AccessDetails`对象的这个序列。结果是一个`Counter`对象，显示了某些路径的访问频率。
- en: A specific collection of saved `.gzip` format logfiles totals about 51 MB. Processing
    the files serially with this function takes over 140 seconds. Can we do better
    using concurrent processing?
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一组特定的保存为`.gzip`格式的日志文件总共约51MB。使用这个函数串行处理文件需要超过140秒。我们能否使用并发处理做得更好？
- en: Using a multiprocessing pool for concurrent processing
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用多进程池进行并发处理
- en: One elegant way to make use of the `multiprocessing` module is to create a processing
    `Pool` object and assign work to the various processes in that pool. We will use
    the OS to interleave execution among the various processes. If each of the processes
    has a mixture of I/O and computation, we should be able to assure that our processor
    is very busy. When processes are waiting for I/O to complete, other processes
    can do their computation. When an I/O completes, a process will be ready to run
    and can compete with others for processing time.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`multiprocessing`模块的一个优雅的方法是创建一个处理`Pool`对象，并将工作分配给该池中的各个进程。我们将使用操作系统在各个进程之间交错执行。如果每个进程都有I/O和计算的混合，我们应该能够确保我们的处理器非常忙碌。当进程等待I/O完成时，其他进程可以进行计算。当I/O完成时，一个进程将准备好运行，并且可以与其他进程竞争处理时间。
- en: 'The recipe for mapping work to a separate process looks as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 将工作映射到单独的进程的方法如下：
- en: '[PRE25]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We've created a `Pool` object with four separate processes and assigned this
    `Pool` object to the `workers` variable. We've then mapped a function, `analysis`,
    to an iterable queue of work to be done, using the pool of processes. Each process
    in the `workers` pool will be assigned items from the iterable queue. In this
    case, the queue is the result of the `glob.glob(pattern)` attribute, which is
    a sequence of file names.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个具有四个独立进程的`Pool`对象，并将该`Pool`对象分配给`workers`变量。然后，我们将一个名为`analysis`的函数映射到要执行的工作的可迭代队列上，使用进程池。`workers`池中的每个进程将被分配来自可迭代队列的项目。在这种情况下，队列是`glob.glob(pattern)`属性的结果，它是文件名的序列。
- en: As the `analysis()` function returns a result, the parent process that created
    the `Pool` object can collect those results. This allows us to create several
    concurrently-built `Counter` objects and merge them into a single, composite result.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`analysis()`函数返回一个结果，创建`Pool`对象的父进程可以收集这些结果。这使我们能够创建几个并发构建的`Counter`对象，并将它们合并成一个单一的复合结果。
- en: If we start *p* processes in the pool, our overall application will include
    *p+1* processes. There will be one parent process and *p* children. This often
    works out well because the parent process will have little to do after the subprocess
    pools are started. Generally, the workers will be assigned to separate CPUs (or
    cores) and the parent will share a CPU with one of the children in the `Pool`
    object.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在池中启动*p*个进程，我们的整个应用程序将包括*p+1*个进程。将有一个父进程和*p*个子进程。这通常效果很好，因为在子进程池启动后，父进程将几乎没有什么要做。通常情况下，工作进程将被分配到单独的CPU（或核心），而父进程将与`Pool`对象中的一个子进程共享一个CPU。
- en: Note
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The ordinary Linux parent/child process rules apply to the subprocesses created
    by this module. If the parent crashes without properly collecting final status
    from the child processes, then "zombie" processes can be left running. For this
    reason, a process `Pool` object is a context manager. When we use a pool via the
    `with` statement, at the end of the context, the children are properly terminated.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由该模块创建的子进程遵循普通的Linux父/子进程规则。如果父进程在没有正确收集子进程的最终状态的情况下崩溃，那么可能会留下“僵尸”进程在运行。因此，进程`Pool`对象是一个上下文管理器。当我们通过`with`语句使用进程池时，在上下文结束时，子进程会被正确终止。
- en: By default, a `Pool` object will have a number of workers based on the value
    of the `multiprocessing.cpu_count()` function. This number is often optimal, and
    simply using the `with multiprocessing.Pool() as workers:` attribute might be
    sufficient.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`Pool`对象将具有基于`multiprocessing.cpu_count()`函数值的工作进程数。这个数字通常是最佳的，只需使用`with
    multiprocessing.Pool() as workers:`属性可能就足够了。
- en: In some cases, it can help to have more workers than CPUs. This might be true
    when each worker has I/O-intensive processing. Having many worker processes waiting
    for I/O to complete can improve the elapsed running time of an application.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，有时比CPU更多的工作进程可能会有所帮助。当每个工作进程都有I/O密集型处理时，这可能是真的。有许多工作进程等待I/O完成可以改善应用程序的运行时间。
- en: If a given `Pool` object has *p* workers, this mapping can cut the processing
    time to almost ![Using a multiprocessing pool for concurrent processing](graphics/B03652_12_03.jpg)
    of the time required to process all of the logs serially. Pragmatically, there
    is some overhead involved with communication between the parent and child processes
    in the `Pool` object. Therefore, a four-core processor might only cut the processing
    time in half.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果给定的`Pool`对象有*p*个工作进程，这种映射可以将处理时间减少到几乎处理所有日志的时间的![使用多进程池进行并发处理](graphics/B03652_12_03.jpg)。实际上，在`Pool`对象中父进程和子进程之间的通信涉及一些开销。因此，一个四核处理器可能只能将处理时间减少一半。
- en: 'The multiprocessing `Pool` object has four map-like methods to allocate work
    to a pool: `map()`, `imap()`, `imap_unordered()`, and `starmap()`. Each of these
    is a variation on the common theme of mapping a function to a pool of processes.
    They differ in the details of allocating work and collecting results.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 多进程`Pool`对象有四种类似map的方法来分配工作给进程池：`map()`、`imap()`、`imap_unordered()`和`starmap()`。每个方法都是将函数映射到进程池的变体。它们在分配工作和收集结果的细节上有所不同。
- en: The `map(function, iterable)` method allocates items from the iterable to each
    worker in the pool. The finished results are collected in the order they were
    allocated to the `Pool` object so that order is preserved.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`map(function, iterable)`方法将可迭代对象中的项目分配给池中的每个工作进程。完成的结果按照它们分配给`Pool`对象的顺序进行收集，以保持顺序。'
- en: The `imap(function, iterable)` method is described as "lazier" than map. By
    default, it sends each individual item from the iterable to the next available
    worker. This might involve more communication overhead. For this reason, a chunk
    size larger than 1 is suggested.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`imap(function, iterable)` 方法被描述为比 map 方法“更懒”。默认情况下，它会将可迭代对象中的每个单独项目发送给下一个可用的工作进程。这可能涉及更多的通信开销。因此建议使用大于1的块大小。'
- en: The `imap_unordered(function, iterable)` method is similar to the `imap()` method,
    but the order of the results is not preserved. Allowing the mapping to be processed
    out of order means that, as each process finishes, the results are collected.
    Otherwise, the results must be collected in order.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`imap_unordered(function, iterable)`方法类似于`imap()`方法，但结果的顺序不被保留。允许映射无序处理意味着每个进程完成时结果都被收集。否则，结果必须按顺序收集。'
- en: The `starmap(function, iterable)` method is similar to the `itertools.starmap()`
    function. Each item in the iterable must be a tuple; the tuple is passed to the
    function using the `*` modifier so that each value of the tuple becomes a positional
    argument value. In effect, it's performing `function(*iterable[0])`, `function(*iterable[1])`,
    and so on.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`starmap(function, iterable)`方法类似于`itertools.starmap()`函数。可迭代对象中的每个项目必须是一个元组；使用`*`修饰符将元组传递给函数，以便元组的每个值成为位置参数值。实际上，它执行`function(*iterable[0])`，`function(*iterable[1])`等等。'
- en: 'Here is one of the variations on the preceding mapping theme:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述映射主题的一个变体：
- en: '[PRE26]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We've created a `Counter()` function that we'll use to consolidate the results
    from each worker in the pool. We created a pool of subprocesses based on the number
    of available CPUs and used the `Pool` object as a context manager. We then mapped
    our `analysis()` function to each file in our file-matching pattern. The resulting
    `Counter` objects from the `analysis()` function are combined into a single resulting
    counter.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个`Counter()`函数，用于整合池中每个工作进程的结果。我们根据可用CPU的数量创建了一个子进程池，并使用`Pool`对象作为上下文管理器。然后我们将我们的`analysis()`函数映射到我们文件匹配模式中的每个文件上。来自`analysis()`函数的结果`Counter`对象被合并成一个单一的计数器。
- en: This takes about 68 seconds. The time to analyze the logs was cut in half using
    several concurrent processes.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这大约需要68秒。使用多个并发进程，分析日志的时间减少了一半。
- en: We've created a two-tiered map-reduce process with the `multiprocessing` module's
    `Pool.map()` function. The first tier was the `analysis()` function, which performed
    a map-reduce on a single logfile. We then consolidated these reductions in a higher-level
    reduce operation.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`multiprocessing`模块的`Pool.map()`函数创建了一个两层的map-reduce过程。第一层是`analysis()`函数，它对单个日志文件执行了map-reduce。然后我们在更高级别的reduce操作中
    consolide 这些减少。
- en: Using apply() to make a single request
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用apply()来发出单个请求
- en: 'In addition to the `map()` function''s variants, a pool also has an `apply(function,
    *args, **kw)` method that we can use to pass one value to the worker pool. We
    can see that the `map()` method is really just a `for` loop wrapped around the
    `apply()` method, we can, for example, use the following command:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`map()`函数的变体外，池还有一个`apply(function, *args, **kw)`方法，我们可以使用它来将一个值传递给工作池。我们可以看到`map()`方法实际上只是一个包装在`apply()`方法周围的`for`循环，例如，我们可以使用以下命令：
- en: '[PRE27]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: It's not clear, for our purposes, that this is a significant improvement. Almost
    everything we need to do can be expressed as a `map()` function.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的来说，这并不明显是一个重大的改进。我们几乎可以把所有需要做的事情都表达为一个`map()`函数。
- en: Using map_async(), starmap_async(), and apply_async()
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用map_async()，starmap_async()和apply_async()
- en: The behavior of the `map()`, `starmap()`, and `apply()` functions is to allocate
    work to a subprocess in the `Pool` object and then collect the response from the
    subprocess when that response is ready. This can cause the child to wait for the
    parent to gather the results. The `_async()` function's variations do not wait
    for the child to finish. These functions return an object that can be queried
    to get the individual results from the child processes.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`map()`，`starmap()`和`apply()`函数的行为是将工作分配给`Pool`对象中的子进程，然后在子进程准备好响应时收集响应。这可能导致子进程等待父进程收集结果。`_async()`函数的变体不会等待子进程完成。这些函数返回一个对象，可以查询该对象以获取子进程的单个结果。'
- en: 'The following is a variation using the `map_async()` method:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用`map_async()`方法的变体：
- en: '[PRE28]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We've created a `Counter()` function that we'll use to consolidate the results
    from each worker in the pool. We created a pool of subprocesses based on the number
    of available CPUs and used this `Pool` object as a context manager. We then mapped
    our `analysis()` function to each file in our file-matching pattern. The response
    from the `map_async()` function is a `MapResult` object; we can query this for
    results and overall status of the pool of workers. In this case, we used the `get()`
    method to get the sequence of the `Counter` objects.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个`Counter()`函数，用于整合池中每个工作进程的结果。我们根据可用CPU的数量创建了一个子进程池，并将这个`Pool`对象用作上下文管理器。然后我们将我们的`analysis()`函数映射到我们文件匹配模式中的每个文件上。`map_async()`函数的响应是一个`MapResult`对象；我们可以查询这个对象以获取池中工作进程的结果和整体状态。在这种情况下，我们使用`get()`方法获取`Counter`对象的序列。
- en: The resulting `Counter` objects from the `analysis()` function are combined
    into a single resulting `Counter` object. This aggregate gives us an overall summary
    of a number of logfiles. This processing is not any faster than the previous example.
    The use of the `map_async()` function allows the parent process to do additional
    work while waiting for the children to finish.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 来自`analysis()`函数的结果`Counter`对象被合并成一个单一的`Counter`对象。这个聚合给我们提供了多个日志文件的总体摘要。这个处理并没有比之前的例子更快。使用`map_async()`函数允许父进程在等待子进程完成时做额外的工作。
- en: More complex multiprocessing architectures
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更复杂的多进程架构
- en: The `multiprocessing` package supports a wide variety of architectures. We can
    easily create multiprocessing structures that span multiple servers and provide
    formal authentication techniques to create a necessary level of security. We can
    pass objects from process to process using queues and pipes. We can share memory
    between processes. We can also share lower-level locks between processes as a
    way to synchronize access to shared resources such as files.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`multiprocessing`包支持各种各样的架构。我们可以轻松创建跨多个服务器的多进程结构，并提供正式的身份验证技术，以创建必要的安全级别。我们可以使用队列和管道在进程之间传递对象。我们可以在进程之间共享内存。我们还可以在进程之间共享较低级别的锁，以同步对共享资源（如文件）的访问。'
- en: Most of these architectures involve explicitly managing state among several
    working processes. Using locks and shared memory, in particular, are imperative
    in nature and don't fit well with a functional programming approach.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些架构都涉及显式管理多个工作进程之间的状态。特别是使用锁和共享内存，这是必要的，但与函数式编程方法不太匹配。
- en: We can, with some care, treat queues and pipes in a functional manner. Our objective
    is to decompose a design into producer and consumer functions. A producer can
    create objects and insert them into a queue. A consumer will take objects out
    of a queue and process them, perhaps putting intermediate results into another
    queue. This creates a network of concurrent processors and the workload is distributed
    among these various processes. Using the `pycsp` package can simplify the queue-based
    exchange of messages among processes. For more information, visit [https://pypi.python.org/pypi/pycsp](https://pypi.python.org/pypi/pycsp).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一些小心处理，以函数式方式处理队列和管道。我们的目标是将设计分解为生产者和消费者函数。生产者可以创建对象并将它们插入队列。消费者将从队列中取出对象并处理它们，可能将中间结果放入另一个队列。这样就创建了一个并发处理器网络，工作负载分布在这些不同的进程之间。使用`pycsp`包可以简化进程之间基于队列的消息交换。欲了解更多信息，请访问[https://pypi.python.org/pypi/pycsp](https://pypi.python.org/pypi/pycsp)。
- en: This design technique has some advantages when designing a complex application
    server. The various subprocesses can exist for the entire life of the server,
    handling individual requests concurrently.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计复杂的应用服务器时，这种设计技术有一些优势。各个子进程可以存在于服务器的整个生命周期中，同时处理各个请求。
- en: Using the concurrent.futures module
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`concurrent.futures`模块
- en: In addition to the `multiprocessing` package, we can also make use of the `concurrent.futures`
    module. This also provides a way to map data to a concurrent pool of threads or
    processes. The module API is relatively simple and similar in many ways to the
    `multiprocessing.Pool()` function's interface.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`multiprocessing`包，我们还可以使用`concurrent.futures`模块。这也提供了一种将数据映射到并发线程或进程池的方法。模块API相对简单，并且在许多方面类似于`multiprocessing.Pool()`函数的接口。
- en: 'Here is an example to show just how similar they are:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例，展示它们有多相似：
- en: '[PRE29]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The most significant change between the preceding example and previous examples
    is that we're using an instance of the `concurrent.futures.ProcessPoolExecutor`
    object instead of the `multiprocessing.Pool` method. The essential design pattern
    is to map the `analysis()` function to the list of filenames using the pool of
    available workers. The resulting `Counter` objects are consolidated to create
    a final result.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 前面示例和之前的示例之间最显著的变化是，我们使用了`concurrent.futures.ProcessPoolExecutor`对象的实例，而不是`multiprocessing.Pool`方法。基本的设计模式是使用可用工作进程池将`analysis()`函数映射到文件名列表。生成的`Counter`对象被合并以创建最终结果。
- en: The performance of the `concurrent.futures` module is nearly identical to the
    `multiprocessing` module.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`concurrent.futures`模块的性能几乎与`multiprocessing`模块相同。'
- en: Using concurrent.futures thread pools
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`concurrent.futures`线程池
- en: The `concurrent.futures` module offers a second kind of executor that we can
    use in our applications. Instead of creating a `concurrent.futures.ProcessPoolExecutor`
    object, we can use the `ThreadPoolExecutor` object. This will create a pool of
    threads within a single process.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`concurrent.futures`模块提供了第二种我们可以在应用程序中使用的执行器。我们可以使用`concurrent.futures.ProcessPoolExecutor`对象，也可以使用`ThreadPoolExecutor`对象。这将在单个进程中创建一个线程池。'
- en: The syntax is otherwise identical to using a `ProcessPoolExecutor` object. The
    performance, however, is remarkably different. The logfile processing is dominated
    by I/O. All of the threads in a process share the same OS scheduling constraints.
    Due to this, the overall performance of multithreaded logfile analysis is about
    the same as processing the logfiles serially.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 语法与使用`ProcessPoolExecutor`对象完全相同。然而，性能却有显著不同。日志文件处理受I/O控制。一个进程中的所有线程共享相同的操作系统调度约束。因此，多线程日志文件分析的整体性能与串行处理日志文件的性能大致相同。
- en: 'Using sample logfiles and a small four-core laptop running Mac OS X, these
    are the kinds of results that indicate the difference between threads that share
    I/O resources and processes:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 使用示例日志文件和运行Mac OS X的小型四核笔记本电脑，以下是表明共享I/O资源的线程和进程之间差异的结果类型：
- en: Using the `concurrent.futures` thread pool, the elapsed time was 168 seconds
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`concurrent.futures`线程池，经过的时间是168秒
- en: Using a process pool, the elapsed time was 68 seconds
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用进程池，经过的时间是68秒
- en: In both cases, the `Pool` object's size was 4\. It's not clear which kind of
    applications benefit from a multithreading approach. In general, multiprocessing
    seems to be best for Python applications.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，`Pool`对象的大小都是4。目前尚不清楚哪种应用程序受益于多线程方法。一般来说，多进程似乎对Python应用程序最有利。
- en: Using the threading and queue modules
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用线程和队列模块
- en: The Python `threading` package involves a number of constructs helpful for building
    imperative applications. This module is not focused on writing functional applications.
    We can make use of thread-safe queues in the `queue` module to pass objects from
    thread to thread.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Python的`threading`包涉及一些有助于构建命令式应用程序的构造。这个模块不专注于编写函数式应用程序。我们可以利用`queue`模块中的线程安全队列，在线程之间传递对象。
- en: The `threading` module doesn't have a simple way to distribute work to various
    threads. The API isn't ideally suited to functional programming.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`threading`模块没有一种简单的方法来将工作分配给各个线程。API并不理想地适用于函数式编程。'
- en: As with the more primitive features of the `multiprocessing` module, we can
    try to conceal the stateful and imperative nature of locks and queues. It seems
    easier, however, to make use of the `ThreadPoolExecutor` method in the `concurrent.futures`
    module. The `ProcessPoolExecutor.map()` method provides us with a very pleasant
    interface to concurrent processing of the elements of a collection.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 与`multiprocessing`模块的更原始特性一样，我们可以尝试隐藏锁和队列的有状态和命令性本质。然而，似乎更容易利用`concurrent.futures`模块中的`ThreadPoolExecutor`方法。`ProcessPoolExecutor.map（）`方法为我们提供了一个非常愉快的界面，用于并发处理集合的元素。
- en: The use of the `map()` function primitive to allocate work seems to fit nicely
    with our functional programming expectations. For this reason, it's best to focus
    on the `concurrent.futures` module as the most accessible way to write concurrent
    functional programs.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`map（）`函数原语来分配工作似乎与我们的函数式编程期望很好地契合。因此，最好专注于`concurrent.futures`模块作为编写并发函数程序的最可访问的方式。
- en: Designing concurrent processing
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计并发处理
- en: 'From a functional programming perspective, we''ve seen three ways to use the
    `map()` function concept applied to data items concurrently. We can use any one
    of the following:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 从函数式编程的角度来看，我们已经看到了三种并发应用`map（）`函数概念的方法。我们可以使用以下任何一种：
- en: '`multiprocessing.Pool`'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`multiprocessing.Pool`'
- en: '`concurrent.futures.ProcessPoolExecutor`'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`concurrent.futures.ProcessPoolExecutor`'
- en: '`concurrent.futures.ThreadPoolExecutor`'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`concurrent.futures.ThreadPoolExecutor`'
- en: These are almost identical in the way we interact with them; all three have
    a `map()` method that applies a function to items of an iterable collection. This
    fits elegantly with other functional programming techniques. The performance is
    different because of the nature of concurrent threads versus concurrent processes.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 它们在与它们交互的方式上几乎是相同的；所有三个都有一个`map（）`方法，它将一个函数应用于可迭代集合的项。这与其他函数式编程技术非常优雅地契合。性能有所不同，因为并发线程与并发进程的性质不同。
- en: 'As we stepped through the design, our log analysis application decomposed into
    two overall areas:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们逐步设计时，我们的日志分析应用程序分解为两个整体领域：
- en: 'The lower-level parsing: This is generic parsing that will be used by almost
    any log analysis application'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析的下层：这是通用解析，几乎可以被任何日志分析应用程序使用
- en: 'The higher-level analysis application: This is more specific filtering and
    reduction focused on our application needs'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高级别的分析应用程序：这更具体的过滤和减少专注于我们的应用需求
- en: 'The lower-level parsing can be decomposed into four stages:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 下层解析可以分解为四个阶段：
- en: Reading all the lines from multiple source logfiles. This was the `local_gzip()`
    mapping from file name to a sequence of lines.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从多个源日志文件中读取所有行。这是从文件名到行序列的`local_gzip（）`映射。
- en: Creating simple namedtuples from the lines of log entries in a collection of
    files. This was the `access_iter()` mapping from text lines to Access objects.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文件集合中的日志条目的行创建简单的命名元组。这是从文本行到Access对象的`access_iter（）`映射。
- en: Parsing the details of more complex fields such as dates and URLs. This was
    the `access_detail_iter()` mapping from `Access` objects to `AccessDetails` objects.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析更复杂字段的细节，如日期和URL。这是从`Access`对象到`AccessDetails`对象的`access_detail_iter（）`映射。
- en: Rejecting uninteresting paths from the logs. We can also think of this as passing
    only the interesting paths. This was more of a filter than a map operation. This
    was a collection of filters bundled into the `path_filter()` function.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从日志中拒绝不感兴趣的路径。我们也可以认为这只传递有趣的路径。这更像是一个过滤器而不是一个映射操作。这是捆绑到`path_filter（）`函数中的一系列过滤器。
- en: We defined an overall `analysis()` function that parsed and analyzed a given
    logfile. It applied the higher-level filter and reduction to the results of the
    lower-level parsing. It can also work with a wild-card collection of files.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个总体的`analysis（）`函数，它解析和分析给定的日志文件。它将更高级别的过滤和减少应用于下层解析的结果。它也可以处理通配符文件集合。
- en: 'Given the number of mappings involved, we can see several ways to decompose
    this problem into work that can be mapped to into a pool of threads or processes.
    Here are some of the mappings we can consider as design alternatives:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到涉及的映射数量，我们可以看到将这个问题分解为可以映射到线程或进程池中的工作的几种方法。以下是一些我们可以考虑的设计替代方案：
- en: Map the `analysis()` function to individual files. We use this as a consistent
    example throughout this chapter.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`analysis（）`函数映射到单个文件。我们在本章中始终使用这个作为一个一致的例子。
- en: Refactor the `local_gzip()` function out of the overall `analysis()` function.
    We can now map the revised `analysis()` function to the results of the `local_gzip()`
    function.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`local_gzip（）`函数重构为总体`analysis（）`函数之外。现在我们可以将修订后的`analysis（）`函数映射到`local_gzip（）`函数的结果。
- en: Refactor the `access_iter(local_gzip(pattern))` function out of the overall
    `analysis()` function. We can map this revised `analysis()` function against the
    iterable sequence of the `Access` objects.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`access_iter（local_gzip（pattern））`函数重构为总体`analysis（）`函数之外。我们可以将这个修订后的`analysis（）`函数映射到`Access`对象的可迭代序列。
- en: Refactor the `access_detail_iter(access-iter(local_gzip(pattern)))` function
    into a separate iterable. We will then map the `path_filter()` function and the
    higher-level filter and reduction against the iterable sequence of the `AccessDetail`
    objects.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`access_detail_iter（access-iter（local_gzip（pattern）））`函数重构为一个单独的可迭代对象。然后我们将对`AccessDetail`对象的可迭代序列进行`path_filter（）`函数和更高级别的过滤和减少映射。
- en: We can also refactor the lower-level parsing into a function that is separate
    from the higher-level analysis. We can map the analysis filter and reduction against
    the output from the lower-level parsing.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以将下层解析重构为与更高级别分析分开的函数。我们可以将分析过滤器和减少映射到下层解析的输出。
- en: All of these are relatively simple restructurings of the example application.
    The benefit of using functional programming techniques is that each part of the
    overall process can be defined as a mapping. This makes it practical to consider
    different architectures to locate an optimal design.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都是对示例应用程序相对简单的重组。使用函数式编程技术的好处在于整个过程的每个部分都可以定义为一个映射。这使得考虑不同的架构来找到最佳设计变得实际可行。
- en: In this case, however, we need to distribute the I/O processing to as many CPUs
    or cores as we have available. Most of these potential refactorings will perform
    all of the I/O in the parent process; these will only distribute the computations
    to multiple concurrent processes with little resulting benefit. Then, we want
    to focus on the mappings, as these distribute the I/O to as many cores as possible.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们需要将I/O处理分配到尽可能多的CPU或核心。大多数潜在的重构将在父进程中执行所有I/O；这些重构只会将计算分配给多个并发进程，但效益很小。然后，我们希望专注于映射，因为这些可以将I/O分配到尽可能多的核心。
- en: It's often important to minimize the amount of data being passed from process
    to process. In this example, we provided just short filename strings to each worker
    process. The resulting `Counter` object was considerably smaller than the 10 MB
    of compressed detail data in each logfile. We can further reduce the size of each
    `Counter` object by eliminating items that occur only once; or we can limit our
    application to only the 20 most popular items.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化从一个进程传递到另一个进程的数据量通常很重要。在这个例子中，我们只向每个工作进程提供了短文件名字符串。结果的`Counter`对象比每个日志文件中10MB压缩详细数据要小得多。我们可以通过消除仅出现一次的项目来进一步减少每个`Counter`对象的大小；或者我们可以将我们的应用程序限制为仅使用最受欢迎的20个项目。
- en: The fact that we can reorganize the design of this application freely doesn't
    mean we should reorganize the design. We can run a few benchmarking experiments
    to confirm our suspicion that logfile parsing is dominated by the time required
    to read the files.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以自由重新组织这个应用程序的设计，并不意味着我们应该重新组织设计。我们可以运行一些基准实验来确认我们的怀疑，即日志文件解析主要受到读取文件所需的时间的影响。
- en: Summary
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we''ve looked at two ways to support concurrent processing
    of multiple pieces of data:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经看到了支持多个数据并发处理的两种方法：
- en: 'The `multiprocessing` module: Specifically, the `Pool` class and the various
    kinds of mappings available to a pool of workers.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`multiprocessing`模块：具体来说，`Pool`类和可用于工作池的各种映射。'
- en: 'The `concurrent.futures` module: Specifically the `ProcessPoolExecutor` and
    `ThreadPoolExecutor` class. These classes also support a mapping that will distribute
    work among workers that are threads or processes.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`concurrent.futures`模块：具体来说，`ProcessPoolExecutor`和`ThreadPoolExecutor`类。这些类还支持一种映射，可以在线程或进程之间分配工作。'
- en: We've also noted some alternatives that don't seem to fit well with functional
    programming. There are numerous other features of the `multiprocessing` module,
    but they're not a good fit with functional design. Similarly, the `threading`
    and `queue` modules can be used to build multithreaded applications, but the features
    aren't a good fit with functional programs.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还注意到了一些似乎不太适合函数式编程的替代方案。`multiprocessing`模块还有许多其他特性，但它们与函数式设计不太匹配。同样，`threading`和`queue`模块可以用于构建多线程应用，但这些特性与函数式程序不太匹配。
- en: In the next chapter, we'll look at the `operator` module. This can be used to
    simplify some kinds of algorithms. We can use a built-in operator function instead
    of defining a lambda form. We'll also look at some techniques to design flexible
    decision making and allow expressions to be evaluated in a non-strict order.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍`operator`模块。这可以用来简化某些类型的算法。我们可以使用内置的操作函数，而不是定义lambda形式。我们还将探讨一些灵活决策设计的技巧，并允许表达式以非严格顺序进行评估。
