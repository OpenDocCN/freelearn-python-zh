- en: Chapter 12. Performance – Tracking and Reducing Your Memory and CPU Usage
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章。性能-跟踪和减少内存和CPU使用
- en: 'Before we talk about performance, there is a quote by *Donald Knuth* you need
    to consider first:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们谈论性能之前，有一句*Donald Knuth*的话您需要首先考虑：
- en: '*"The real problem is that programmers have spent far too much time worrying
    about efficiency in the wrong places and at the wrong times; premature optimization
    is the root of all evil (or at least most of it) in programming."*'
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “真正的问题在于程序员花了太多时间在错误的地方和错误的时间上担心效率；过早的优化是编程中所有邪恶的根源（或至少是大部分）。”
- en: Note
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Donald Knuth is often called the father of algorithm analysis. His book series,
    *The Art of Computer Programming*, can be considered the Bible of all fundamental
    algorithms.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Donald Knuth经常被称为算法分析之父。他的书系*计算机编程艺术*可以被认为是所有基本算法的圣经。
- en: As long as you pick the correct data structures with the right algorithms, performance
    should not be something to worry about. That does not mean you should ignore performance
    entirely, but just make sure you pick the right battles and optimize only when
    it is actually needed. Micro/premature optimizations can definitely be fun, but
    only very rarely useful.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 只要您选择了正确的数据结构和正确的算法，性能就不应该成为一个值得担忧的问题。这并不意味着您应该完全忽略性能，而是确保您选择正确的战斗，并且只在实际需要时进行优化。微观/过早的优化肯定很有趣，但很少有用。
- en: We have seen the performance characteristics of many data structures in [Chapter
    2](ch02.html "Chapter 2. Pythonic Syntax, Common Pitfalls, and Style Guide"),
    *Pythonic Syntax, Common Pitfalls, and Style Guide*, already, so we won't discuss
    that, but we will show you how performance can be measured and how problems can
    be detected. There are cases where micro optimizations make a difference, but
    you won't know until you measure the performance.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第2章](ch02.html "第2章。Pythonic Syntax, Common Pitfalls, and Style Guide")中看到了许多数据结构的性能特征，*Pythonic
    Syntax, Common Pitfalls, and Style Guide*，所以我们不会讨论那个，但我们会向您展示如何测量性能以及如何检测问题。有些情况下，微观优化会产生影响，但在测量性能之前，您不会知道。
- en: 'Within this chapter, we will cover:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: Profiling CPU usage
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析CPU使用情况
- en: Profiling memory usage
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析内存使用
- en: Learning how to correctly compare performance metrics
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何正确比较性能指标
- en: Optimizing performance
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化性能
- en: Finding and fixing memory leaks
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找和修复内存泄漏
- en: What is performance?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是性能？
- en: 'Performance is a very broad term. It has many different meanings and in many
    cases it is defined incorrectly. You have probably heard statements similar to
    "Language X is faster than Python". However, that statement is inherently wrong.
    Python is neither fast nor slow; Python is a programming language and a language
    has no performance metrics whatsoever. If one were to say that the CPython interpreter
    is faster or slower than interpreter Y for language X, that would be possible.
    The performance characteristics of code can vary greatly between different interpreters.
    Just take a look at this small test:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 性能是一个非常广泛的术语。它有许多不同的含义，在许多情况下被错误地定义。您可能听过类似于“语言X比Python快”的说法。然而，这种说法本质上是错误的。Python既不快也不慢；Python是一种编程语言，语言根本没有性能指标。如果有人说CPython解释器对于语言X比解释器Y快或慢，那是可能的。代码的性能特征在不同的解释器之间可能有很大的差异。只需看看这个小测试：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Three different interpreters with all vastly different performance! All are
    Python but the interpreters obviously vary. Looking at this benchmark, you might
    be tempted to drop the CPython interpreter completely and only use Pypy. The danger
    with benchmarks such as these is that they rarely offer any meaningful results.
    For this limited example, the Pypy interpreter was about four times faster than
    the CPython3 interpreter, but that has no relevance whatsoever for the general
    case. The only conclusion that can safely be drawn here is that this specific
    version of the Pypy interpreter is more than four times faster than this specific
    version of CPython3 for this exact test. For any other test and interpreter version
    the results could be vastly different.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 三种不同的解释器，性能差异巨大！所有都是Python，但解释器显然不同。看到这个基准测试，您可能会想要完全放弃CPython解释器，只使用Pypy。这类基准测试的危险在于它们很少提供任何有意义的结果。对于这个有限的例子，Pypy解释器比CPython3解释器快大约四倍，但这与一般情况毫无关系。唯一可以安全得出的结论是，这个特定版本的Pypy解释器比这个特定版本的CPython3快四倍以上，对于任何其他测试和解释器版本，结果可能大不相同。
- en: Timeit – comparing code snippet performance
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Timeit-比较代码片段的性能
- en: 'Before we can start improving performance, we need a reliable method to measure
    it. Python has a really nice module (`timeit`) with the specific purpose of measuring
    execution times of bits of code. It executes a bit of code many times to make
    sure there is as little variation as possible and to make the measurement fairly
    clean. It''s very useful if you want to compare a few code snippets. Following
    are example executions:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始改进性能之前，我们需要一种可靠的方法来衡量它。Python有一个非常好的模块（`timeit`），专门用于测量代码片段的执行时间。它多次执行一小段代码，以确保变化尽可能小，并使测量相对干净。如果您想比较几个代码片段，这非常有用。以下是示例执行：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'These few examples demonstrate the performance difference between `list.insert`,
    `list.append`, a list comprehension, and the `list` function. But more importantly,
    it demonstrates how to use the `timeit` command. Naturally, the command can be
    used with regular scripts as well, but the `timeit` module only accepts statements
    as strings to execute which is a bit of an annoyance. Luckily, you can easily
    work around that by wrapping your code in a function and just timing that function:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子展示了`list.insert`、`list.append`、列表推导和`list`函数之间的性能差异。但更重要的是，它演示了如何使用`timeit`命令。当然，该命令也可以用于常规脚本，但`timeit`模块只接受要执行的语句作为字符串，这有点麻烦。幸运的是，您可以通过将代码包装在一个函数中并计时该函数来轻松解决这个问题：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When executing this, you will get something along the following lines:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此操作时，您将得到以下内容：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you may have noticed, this script is still a bit basic. While the regular
    version keeps trying until it reaches 0.2 seconds or more, this script just has
    a fixed number of executions. Unfortunately, the `timeit` module wasn't entirely
    written with re-use in mind, so besides calling `timeit.main()` from your script
    there is not much you can do to re-use that logic.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，这个脚本仍然有点基础。而常规版本会一直尝试，直到达到0.2秒或更多，这个脚本只有固定数量的执行。不幸的是，`timeit`模块并没有完全考虑重用，所以除了从脚本中调用`timeit.main()`之外，你几乎没有办法重用这个逻辑。
- en: 'Personally, I recommend using IPython instead, as it makes measurements much
    easier:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 个人建议使用IPython，因为它可以更轻松地进行测量：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this case, IPython automatically takes care of the string wrapping and passing
    of `globals()`. Still, this is all very limited and useful only for comparing
    multiple methods of doing the same thing. When it comes to full Python applications,
    there are more methods available.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，IPython会自动处理字符串包装和`globals()`的传递。不过，这一切都非常有限，只适用于比较多种执行相同操作的方法。在完整的Python应用程序中，有更多的方法可用。
- en: Tip
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: To view the source of both IPython functions and regular modules, entering `object??`
    in the IPython shell returns the source. In this case just enter `timeit??` to
    view the `timeit` IPython function definition.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看IPython函数和常规模块的源代码，可以在IPython shell中输入`object??`来返回源代码。在这种情况下，只需输入`timeit??`来查看`timeit`
    IPython函数的定义。
- en: 'The easiest way you can implement the `%timeit` function yourself is to simply
    call `timeit.main`:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以自己实现`%timeit`函数的最简单方法就是简单地调用`timeit.main`：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The internals of the `timeit` module are nothing special. A basic version can
    be implemented with just an `eval` and a `time.perf_counter` (the highest resolution
    timer available in Python) combination:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`timeit`模块的内部并没有什么特别之处。一个基本版本可以只用`eval`和`time.perf_counter`（Python中可用的最高分辨率计时器）的组合来实现：'
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The actual `timeit` code is a bit more advanced in terms of checking the input
    but this example roughly shows how the `timeit.repeat` function can be implemented.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`timeit`代码实际上在检查输入方面更加先进，但这个例子大致展示了`timeit.repeat`函数如何实现。'
- en: 'To register your own function in IPython, you need to use some IPython magic.
    Note that the magic is not a pun. The IPython module that takes care of commands
    such as these is actually called `magic`. To demonstrate:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要在IPython中注册自己的函数，需要使用一些IPython魔术。请注意，这个魔术并不是双关语。负责这些命令的IPython模块实际上被称为`magic`。为了演示：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: To learn more about custom magic in IPython, take a look at the IPython documentation
    at [https://ipython.org/ipython-doc/3/config/custommagics.html](https://ipython.org/ipython-doc/3/config/custommagics.html).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于IPython中自定义魔术的信息，请查看IPython文档[https://ipython.org/ipython-doc/3/config/custommagics.html](https://ipython.org/ipython-doc/3/config/custommagics.html)。
- en: cProfile – finding the slowest components
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: cProfile – 查找最慢的组件
- en: The `profile` module makes it easily possible to analyze the relative CPU cycles
    used in a script/application. Be very careful not to compare these with the results
    from the `timeit` module. While the `timeit` module tries as best as possible
    to give an accurate benchmark of the absolute amount of time it takes to execute
    a code snippet, the `profile` module is only useful for relative results. The
    reason is that the profiling code itself incurs such a slowdown that the results
    are not comparable with non-profiled code. There is a way to make it a bit more
    accurate however, but more about that later.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`profile`模块使得分析脚本/应用程序中使用的相对CPU周期变得非常容易。但一定要小心，不要将其与`timeit`模块的结果进行比较。虽然`timeit`模块尽可能准确地提供执行代码片段所需的绝对时间的基准，但`profile`模块只对相对结果有用。原因是，分析代码本身会导致减速，因此结果与非分析代码不可比。然而，有一种方法可以使其更加准确，但稍后再详细介绍。'
- en: Note
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Within this section we will be talking about the `profile` module but in the
    examples we will actually use the `cProfile` module. The `cProfile` module is
    a high-performance emulation of the pure Python `profile` module.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将讨论`profile`模块，但在示例中我们实际上将使用`cProfile`模块。`cProfile`模块是纯Python`profile`模块的高性能仿真。
- en: First profiling run
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 首次分析运行
- en: 'Let''s profile our Fibonacci function from [Chapter 5](ch05.html "Chapter 5. Decorators
    – Enabling Code Reuse by Decorating"), *Decorators– Enabling Code Reuse by Decorating*,
    both with and without the cache function. First, the code:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从[第5章](ch05.html "第5章。装饰器-通过装饰实现代码重用")中的Fibonacci函数进行分析，*装饰器-通过装饰实现代码重用*，分别使用缓存函数和不使用缓存函数。首先，代码：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For readabilities sake, all `cProfile` statistics will be stripped of the `percall`
    and `cumtime` columns in all `cProfile` outputs. These columns are irrelevant
    for the purposes of these examples.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可读性，所有`cProfile`统计数据将在所有`cProfile`输出中剥离`percall`和`cumtime`列。这些列对于这些示例的目的来说是无关紧要的。
- en: 'First we''ll execute the function without cache:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们将不使用缓存来执行函数：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'That''s quite a lot of calls, isn''t it? Apparently, we called the `test_fibonacci`
    function nearly 3 million times. That is where the profiling modules provide a
    lot of insight. Let''s analyze the metrics a bit further:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是相当多的调用，不是吗？显然，我们调用了`test_fibonacci`函数将近300万次。这就是分析模块提供了很多见解的地方。让我们进一步分析一下指标：
- en: '**Ncalls**: The number of calls that were made to the function'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ncalls**：对函数进行的调用次数'
- en: '**Tottime**: The total time spent in seconds within this function with all
    sub-functions excluded'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tottime**：在此函数内部花费的总时间（以秒为单位），不包括所有子函数'
- en: Percall, `tottime / ncalls`
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Percall，`tottime / ncalls`
- en: '**Cumtime**: The total time spent within this function, including sub-functions'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cumtime**：在此函数内部花费的总时间，包括子函数'
- en: Percall, `cumtime / ncalls`
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Percall，`cumtime / ncalls`
- en: 'Which is the most useful depends on your use case. It''s quite simple to change
    the sort order using the `-s` parameter within the default output. But now let''s
    see what the result is with the cached version. Once again, with stripped output:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个对你的用例最有用取决于情况。使用默认输出中的`-s`参数可以很容易地改变排序顺序。但现在让我们看看缓存版本的结果。再次，只有简化的输出：
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This time we see a `tottime` of `0.000` because it's just too fast to measure.
    But also, while the `fibonacci_cached` function is still the most executed function,
    it's only being executed 31 times instead of 3 million.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们看到`tottime`是`0.000`，因为它太快了，无法测量。但是，虽然`fibonacci_cached`函数仍然是执行次数最多的函数，但它只执行了31次，而不是300万次。
- en: Calibrating your profiler
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 校准你的性能分析器
- en: 'To illustrate the difference between `profile` and `cProfile,` let''s try the
    uncached run again with the `profile` module instead. Just a heads up, this is
    much slower so don''t be surprised if it stalls a little:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明`profile`和`cProfile`之间的差异，让我们再次尝试使用`profile`模块进行未缓存的运行。提醒一下，这会慢得多，所以如果它有点停滞，不要感到惊讶：
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Huge difference, isn''t it? Now the code is nearly 10 times slower and the
    only difference is using the pure Python `profile` module instead of the `cProfile`
    module. This does indicate a big problem with the `profile` module. The overhead
    from the module itself is great enough to skew the results, which means we should
    account for that offset. That''s what the `Profile.calibrate()` function takes
    care of, as it calculates the bias incurred by the profile module. To calculate
    the bias, we can use the following script:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 巨大的差异，不是吗？现在代码几乎慢了10倍，唯一的区别是使用了纯Python的`profile`模块而不是`cProfile`模块。这确实表明了`profile`模块存在很大的问题。模块本身的开销足以扭曲结果，这意味着我们应该考虑这种偏差。这就是`Profile.calibrate()`函数要处理的问题，它计算了profile模块所产生的偏差。为了计算偏差，我们可以使用以下脚本：
- en: '[PRE12]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The numbers will vary slightly but you should be able to get a fair estimate
    of the bias using this code. If the numbers still vary a lot, you can increase
    the trials from `100000` to something even larger. This type of calibration only
    works for the profile module, but if you are looking for more accurate results
    and the `cProfile` module does not work for you due to inheritance or not being
    supported on your platform, you can use this code to set your bias globally and
    get more accurate results:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 数字会略有不同，但是你应该能够使用这段代码得到一个公平的偏差估计。如果数字仍然有很大的变化，你可以将试验次数从`100000`增加到更大的值。这种校准只适用于profile模块，但是如果你想要更准确的结果，而`cProfile`模块由于继承或者不受你的平台支持而无法使用，你可以使用这段代码来全局设置你的偏差并获得更准确的结果：
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'For a specific `Profile` instance:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定的`Profile`实例：
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Note that in general a smaller bias is better to use than a large one, because
    a large bias could cause very strange results. In some cases you will even get
    negative timings. Let''s give it a try for our Fibonacci code:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，较小的偏差比较大的偏差更好，因为较大的偏差可能会导致非常奇怪的结果。在某些情况下，甚至会得到负的时间。让我们试试我们的斐波那契代码：
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'While running it, it indeed appears that I''ve used a bias that''s too large:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行时，确实出现了我使用了一个太大的偏差：
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Still, it shows how the code can be used properly. You can even incorporate
    the bias calculation within the script using a snippet like this:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，它展示了代码如何正确使用。你甚至可以在脚本中使用类似这样的片段来整合偏差计算：
- en: '[PRE17]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Selective profiling using decorators
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用装饰器进行选择性性能分析
- en: 'Calculating simple timings is easy enough using decorators, but profiling is
    also important. Both are useful but serve different goals. Let''s look at both
    the options:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用装饰器计算简单的时间很容易，但是性能分析也很重要。两者都很有用，但是目标不同。让我们看看两种选择：
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The code is simple enough, just a basic timer and profiler printing some default
    statistics. Which functions best for you depends on your use-case of course, but
    they definitely both have their uses. The added advantage of this selective profiling
    is that the output is more limited which helps with readability:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 代码足够简单，只是一个基本的计时器和性能分析器打印一些默认的统计数据。哪种对你来说更好取决于你的用例，但它们肯定都有用。这种选择性性能分析的额外优势是输出更有限，有助于可读性：
- en: '[PRE19]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As you can see, the profiler still makes the code about twice as slow, but it's
    definitely usable.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，性能分析器仍然使代码变慢了大约两倍，但它确实可用。
- en: Using profile statistics
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用性能分析统计
- en: 'To get some more intricate profiling results, we will profile the `pystone`
    script. The `pystone` script is an internal Python performance test which benchmarks
    the Python interpreter fairly thoroughly. First, let''s create the statistics
    using this script:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更复杂的性能分析结果，我们将对`pystone`脚本进行性能分析。`pystone`脚本是一个内部的Python性能测试，它相当彻底地对Python解释器进行基准测试。首先，让我们使用这个脚本创建统计数据：
- en: '[PRE20]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'When executing the script, you should get something like this:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行脚本时，你应该会得到类似这样的结果：
- en: '[PRE21]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'After running the script, you should have a `pystone.profile` file containing
    the profiling results. These results can be And the `pystone.profile` file which
    contains all of the profiling statistics. These statistics can be viewed through
    the `pstats` module which is bundled with Python:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本后，你应该会得到一个包含性能分析结果的`pystone.profile`文件。这些结果可以通过Python捆绑的`pstats`模块查看：
- en: '[PRE22]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In some cases, it can be interesting to combine the results from multiple measurements.
    That is possible by specifying multiple files or by using `stats.add(*filenames)`.
    But first, let''s look at the regular output:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，将多次测量的结果结合起来可能是有趣的。可以通过指定多个文件或使用`stats.add(*filenames)`来实现。但首先，让我们看看常规输出：
- en: '[PRE23]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Obviously, the parameters can easily be modified to change the sort order and
    the number of output lines. But that is not the only possibility of the statistics.
    There are quite a few packages around which can parse these results and visualize
    them. One option is RunSnakeRun, which although useful does not run on Python
    3 currently. Also, we have QCacheGrind, a very nice visualizer for profile statistics
    but which requires some manual compiling to get running or some searching for
    binaries of course.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，参数可以很容易地修改以改变排序顺序和输出行数。但这并不是统计数据的唯一可能性。有很多包可以解析这些结果并可视化它们。其中一个选择是RunSnakeRun，虽然有用，但目前不支持Python
    3。此外，我们还有QCacheGrind，一个非常好的性能分析统计可视化工具，但需要一些手动编译才能运行，或者当然也可以搜索到二进制文件。
- en: 'Let''s look at the output from QCacheGrind. In the case of Windows, the QCacheGrindWin
    package provides a binary, whereas within Linux it is most likely available through
    your package manager, and with OS X you can try `brew install qcachegrind --with-graphviz`.
    But there is one more package you will require: the `pyprof2calltree` package.
    It transforms the `profile` output into a format that QCacheGrind understands.
    So, after a simple `pip install pyprof2calltree,` we can now convert the `profile`
    file into a `callgrind` file:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看QCacheGrind的输出。在Windows的情况下，QCacheGrindWin包提供了一个二进制文件，而在Linux中，它很可能通过您的软件包管理器提供，在OS
    X中，您可以尝试`brew install qcachegrind --with-graphviz`。但是还有一个包你需要：`pyprof2calltree`包。它将`profile`输出转换为QCacheGrind理解的格式。因此，在简单的`pip
    install pyprof2calltree`之后，我们现在可以将`profile`文件转换为`callgrind`文件：
- en: '[PRE24]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This results in running of the `QCacheGrind` application. After switching to
    the appropriate tabs, you should see something like the following image:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致运行`QCacheGrind`应用程序。切换到适当的标签后，您应该看到类似以下图像的东西：
- en: '![Using profile statistics](images/4711_12_01.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![使用profile统计信息](images/4711_12_01.jpg)'
- en: For a simple script such as this, pretty much all output works. However, with
    full applications, a tool such as QCacheGrind is invaluable. Looking at the output
    generated by QCacheGrind, it is immediately obvious which process took the most
    time. The structure at the top right shows bigger rectangles if the amount of
    time taken was greater, which is a very useful visualization of the chunks of
    CPU time that were used. The list at the left is very similar to `cProfile` and
    therefore nothing new. The tree at the bottom right can be very valuable or very
    useless as it is in this case. It shows you the percentage of CPU time taken in
    a function and more importantly, the relationship of that function with the other
    functions.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这样一个简单的脚本，几乎所有的输出都有效。然而，对于完整的应用程序，像QCacheGrind这样的工具是非常宝贵的。查看QCacheGrind生成的输出，立即就能看出哪个进程花费了最多的时间。右上角的结构显示了更大的矩形，如果花费的时间更长，这是对CPU时间块的非常有用的可视化。左边的列表与`cProfile`非常相似，因此没有什么新鲜的。右下角的树可能非常有价值，也可能非常无用，就像在这种情况下一样。它显示了函数中所占CPU时间的百分比，更重要的是，该函数与其他函数的关系。
- en: Because these tools scale depending on the input the results are useful for
    just about any application. Whether a function takes 100 milliseconds or 100 minutes
    makes no difference, the output will show a clear overview of the slow parts,
    which is what we will try to fix.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这些工具根据输入进行缩放，所以结果对于几乎任何应用程序都是有用的。无论一个函数需要100毫秒还是100分钟，都没有关系，输出将显示慢的部分的清晰概述，这是我们要尝试修复的问题。
- en: Line profiler
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 行分析器
- en: '`line_profiler` is actually not a package that''s bundled with Python, but
    it''s far too useful to ignore. While the regular `profile` module profiles all
    (sub)functions within a certain block, `line_profiler` allows for profiling line
    per line within a function. The Fibonacci function is not best suited here, but
    we can use a prime number generator instead. But first, install `line_profiler`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`line_profiler`实际上不是Python捆绑的包，但它太有用了，不能忽视。虽然常规的`profile`模块对某个块内的所有（子）函数进行分析，但`line_profiler`允许在函数内逐行进行分析。斐波那契函数在这里并不是最合适的，但我们可以使用一个素数生成器。但首先，安装`line_profiler`：'
- en: '[PRE25]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now that we have installed the `line_profiler` module (and with that the `kernprof`
    command), let''s test `line_profiler`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了`line_profiler`模块（以及`kernprof`命令），让我们测试`line_profiler`：
- en: '[PRE26]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You might be wondering where the `profile` decorator is coming from. It originates
    from the `line_profiler` module, which is why we have to run the script with the
    `kernprof` command:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道`profile`装饰器是从哪里来的。它来自`line_profiler`模块，这就是为什么我们必须用`kernprof`命令运行脚本的原因：
- en: '[PRE27]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As the command says, the results have been written to the `test_primes.py.lprof`
    file. So let''s look at the output of that with the `Time` column skipped for
    readability:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 正如命令所说，结果已经写入了`test_primes.py.lprof`文件。因此，为了便于阅读，让我们查看该文件的输出，跳过`Time`列：
- en: '[PRE28]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Wonderful output, isn't it? It makes it trivial to find the slow part within
    a bit of code. Within this code, the slowness is obviously originating from the
    loop, but within other code it might not be that clear.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒的输出，不是吗？这使得在一小段代码中找到慢的部分变得微不足道。在这段代码中，慢显然是来自循环，但在其他代码中可能不那么清楚。
- en: Note
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: This module can be added as an IPython extension as well, which enables the
    `%lprun` command within IPython. To load the extension, the `load_ext` command
    can be used from the IPython shell `%load_ext line_profiler`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模块也可以作为IPython扩展添加，这样就可以在IPython中使用`%lprun`命令。要加载扩展，可以在IPython shell中使用`load_ext`命令`%load_ext
    line_profiler`。
- en: Improving performance
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高性能
- en: Much can be said about performance optimization, but truthfully, if you have
    read the entire book up to this point, you know most of the Python-specific techniques
    to write fast code. The most important factor in application performance will
    always be the choice of algorithms, and by extension, the data structures. Searching
    for an item within `list` is almost always a worse idea than searching for an
    item in `dict` or `set`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 关于性能优化可以说很多，但实际上，如果你已经读完整本书，你就会知道大部分编写快速代码的Python特定技术。应用程序性能中最重要的因素始终是算法的选择，以及数据结构。在`list`中搜索项目几乎总是比在`dict`或`set`中搜索项目更糟糕的想法。
- en: Using the right algorithm
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用正确的算法
- en: 'Within any application, the right choice of algorithm is by far the most important
    performance characteristic, which is why I am repeating it to illustrate the results
    of a bad choice:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何应用程序中，正确选择的算法是远远最重要的性能特征，这就是为什么我重复强调它以说明错误选择的结果：
- en: '[PRE29]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Checking whether an item is within a `list` is an `O(n)` operation and checking
    whether an item is within a `dict` is an `O(1)` operation. A huge difference when
    `n=1000000` obviously, in this simple test we can see that for 1 million items
    it's 500 times faster.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 检查一个项目是否在`list`中是一个`O(n)`操作，而检查一个项目是否在`dict`中是一个`O(1)`操作。当`n=1000000`时有很大的差异，显然，在这个简单的测试中，对于100万个项目，它快了500倍。
- en: All other performance tips combined together might make your code twice as fast,
    but using the right algorithm for the job can cause a much greater improvement.
    Using an algorithm that takes `O(n)` time instead of `O(n^2)` time will make your
    code `1000` times faster for `n=1000,` and with a larger `n` the difference only
    grows further.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 所有其他性能提示结合在一起可能使您的代码速度加快一倍，但使用正确的算法可能会带来更大的改进。使用需要`O(n)`时间的算法而不是`O(n^2)`时间将使您的代码对于`n=1000`快`1000`倍，对于更大的`n`，差异只会进一步增加。
- en: Global interpreter lock
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全局解释器锁
- en: One of the most obscure components of the CPython interpreter is the **global
    interpreter lock** (**GIL**), a **mutual exclusion lock** (**mutex**) required
    to prevent memory corruption. The Python memory manager is not thread-safe and
    that is why the GIL is needed. Without the GIL, multiple threads might alter memory
    at the same time, causing all sorts of unexpected and potentially dangerous results.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: CPython解释器最隐晦的组件之一是全局解释器锁（GIL），这是为了防止内存损坏而需要的互斥锁。Python内存管理器不是线程安全的，这就是为什么需要GIL。没有GIL，多个线程可能同时更改内存，导致各种意外和潜在的危险结果。
- en: So what is the impact of the GIL in a real-life application? Within single-threaded
    applications it makes no difference whatsoever and is actually an extremely fast
    method for memory consistency. Within multithreaded applications however, it can
    slow your application down a bit, because only a single thread can access the
    GIL at a time. So if your code has to access the GIL a lot, it might benefit from
    some restructuring.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 那么GIL在现实应用中的影响是什么？在单线程应用程序中，它根本没有任何影响，实际上是一种非常快速的内存一致性方法。然而，在多线程应用程序中，它可能会稍微减慢应用程序的速度，因为一次只有一个线程可以访问GIL。因此，如果您的代码需要频繁访问GIL，可能会受益于一些重构。
- en: 'Luckily, Python offers a few other options for parallel processing: the `asyncio`
    module that we saw earlier and the `multiprocessing` library that we will see
    in [Chapter 13](ch13.html "Chapter 13. Multiprocessing – When a Single CPU Core
    Is Not Enough"), *Multiprocessing – When a Single CPU Core Is Not Enough*.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Python提供了一些其他并行处理选项：我们之前看到的`asyncio`模块和我们将在[第13章](ch13.html "第13章。多进程-当单个CPU核心不够用")中看到的`multiprocessing`库，*多进程-当单个CPU核心不够用*。
- en: Try versus if
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 尝试与if
- en: In many languages a `try/except` type of block incurs quite a performance hit,
    but within Python this is not the case. It's not that an `if` statement is heavy,
    but if you expect your `try/except` to succeed most of the time and only fail
    in rare cases, it's definitely a valid alternative. As always though, focus on
    readability and conveying the purpose of the code. If the intention of the code
    is clearer using an `if` statement, use the `if` statement. If `try/except` conveys
    the intention in a better way, use that.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多语言中，`try/except`类型的块会带来相当大的性能损失，但在Python中并非如此。并不是说`if`语句很重，但如果您期望您的`try/except`大部分时间都成功，只在罕见情况下失败，那绝对是一个有效的替代方案。不过，始终专注于可读性和传达代码目的。如果使用`if`语句更清晰表达代码的意图，就使用`if`语句。如果`try/except`以更好的方式传达意图，就使用它。
- en: Lists versus generators
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 列表与生成器
- en: Evaluating code lazily using generators is almost always a better idea than
    calculating the entire dataset. The most important rule of performance optimization
    is probably that you shouldn't calculate anything you're not going to use. If
    you're not sure that you are going to need it, don't calculate it.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用生成器懒惰地评估代码几乎总是比计算整个数据集更好的主意。性能优化的最重要规则可能是，不要计算任何您不打算使用的东西。如果您不确定自己是否需要它，请不要计算它。
- en: Don't forget that you can easily chain multiple generators, so everything is
    calculated only when it's actually needed. Do be careful that this won't result
    in recalculation though; `itertools.tee` is generally a better idea than recalculating
    your results completely.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记您可以轻松地链接多个生成器，因此只有在实际需要时才计算所有内容。但要小心，这不会导致重新计算；`itertools.tee`通常比完全重新计算结果更好。
- en: String concatenation
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 字符串连接
- en: You might have seen benchmarks saying that using `+=` is much slower than joining
    strings. At one point this made quite a lot of difference indeed. With Python
    3 however, most of the differences have vanished.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经看到过一些基准测试，表明使用`+=`比连接字符串要慢得多。在某个时候，这确实产生了很大的差异。然而，使用Python 3后，大部分差异已经消失。
- en: '[PRE30]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: There are still some differences of course, but they are so small that I recommend
    to simply ignore them and choose the most readable option instead.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当然仍然存在一些差异，但它们非常小，建议忽略它们，选择最可读的选项。
- en: Addition versus generators
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加法与生成器
- en: As is the case with string concatenation, once a significant difference now
    too small to mention.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 与字符串连接一样，一度显著差异现在已经微乎其微。
- en: '[PRE31]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: What does help though is letting Python handle everything internally using native
    functions, as can be seen in the last example.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有助于让Python使用本机函数来处理一切，就像在最后一个示例中所看到的那样。
- en: Map versus generators and list comprehensions
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 映射与生成器和列表推导
- en: 'Once again, readability counts more than performance. There are a few cases
    where `map` is faster than list comprehensions and generators, but only if the
    `map` function can use a predefined function. As soon as you need to whip out
    `lambda,` it''s actually slower. Not that it matters much, since readability should
    be key anyhow, use generators or list comprehensions instead of `map`:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，可读性比性能更重要。有一些情况下，`map`比列表推导和生成器更快，但只有当`map`函数可以使用预定义函数时才是如此。一旦您需要使用`lambda`，实际上速度就会变慢。不过这并不重要，因为无论如何，可读性应该是关键，使用生成器或列表推导而不是`map`：
- en: '[PRE32]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As you can see, the list comprehension is obviously quite a bit faster than
    the generator. In many cases I would still recommend the generator over the list
    comprehension though, if only because of the memory usage and the potential laziness.
    If for some reason you are only going to use the first 10 items, you're still
    wasting a lot of resources by calculating the full list of items.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，列表推导式显然比生成器快得多。然而，在许多情况下，我仍然会推荐使用生成器而不是列表推导式，仅仅是因为内存使用和潜在的惰性。如果由于某种原因你只打算使用前10个项目，那么通过计算完整的项目列表，你仍然会浪费大量资源。
- en: Caching
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓存
- en: We have already covered the `functools.lru_cache` decorator in [Chapter 5](ch05.html
    "Chapter 5. Decorators – Enabling Code Reuse by Decorating"), *Decorators – Enabling
    Code Reuse by Decorating* but the importance should not be underestimated. Regardless
    of how fast and smart your code is, not having to calculate results is always
    better and that's what caching does. Depending on your use case, there are many
    options available. Within a simple script, `functools.lru_cache` is a very good
    contender, but between multiple executions of an application, the `cPickle` module
    can be a life saver as well.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第5章](ch05.html "第5章。装饰器-通过装饰实现代码重用")中介绍了`functools.lru_cache`装饰器，*装饰器-通过装饰实现代码重用*，但它的重要性不容小觑。无论你的代码有多快多聪明，不必计算结果总是更好的，这就是缓存的作用。根据你的用例，有许多选项可用。在一个简单的脚本中，`functools.lru_cache`是一个很好的选择，但在多次执行应用程序时，`cPickle`模块也可以拯救生命。
- en: If multiple servers are involved, I recommend taking a look at **Redis**. The
    Redis server is a single threaded in-memory server which is extremely fast and
    has many useful data structures available. If you see articles or tutorials about
    improving performance using Memcached, simply replace Memcached with Redis everywhere.
    Redis is superior to Memcached in every way and in its most basic form the API
    is compatible.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果涉及多个服务器，我建议看看**Redis**。Redis服务器是一个单线程的内存服务器，非常快速，并且有许多有用的数据结构可用。如果你看到关于使用Memcached提高性能的文章或教程，只需在所有地方用Redis替换Memcached。Redis在各个方面都优于Memcached，在其最基本的形式下，API是兼容的。
- en: Lazy imports
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 延迟导入
- en: A common problem in application load times is that everything is loaded immediately
    at the start of the program, while with many applications this is actually not
    needed and certain parts of the application only require loading when they are
    actually used. To facilitate this, one can occasionally move the imports inside
    of functions so they can be loaded on demand.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序加载时间的一个常见问题是，在程序启动时立即加载所有内容，而实际上许多应用程序并不需要这样做，应用程序的某些部分只在实际使用时才需要加载。为了方便起见，可以偶尔将导入移动到函数内部，以便按需加载。
- en: 'While it''s a valid strategy in some cases, I don''t generally recommend it
    for two reasons:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在某些情况下这是一个有效的策略，但我通常不推荐它，原因有两个：
- en: It makes your code less clear; having all imports in the same style at the top
    of the file improves readability.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它使你的代码不够清晰；在文件顶部以相同的风格放置所有导入可以提高可读性。
- en: It doesn't make the code faster as it just moves the load time to a different
    part.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它并没有使代码更快，因为它只是将加载时间移到不同的部分。
- en: Using optimized libraries
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用优化库
- en: 'This is actually a very broad tip, but useful nonetheless. If there''s a highly
    optimized library which suits your purpose, you most likely won''t be able to
    beat its performance without a significant amount of effort. Libraries such as
    `numpy`, `pandas`, `scipy`, and `sklearn` are highly optimized for performance
    and their native operations can be incredibly fast. If they suit your purpose,
    be sure to give them a try. Just to illustrate how fast `numpy` can be compared
    to plain Python, refer to the following:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是一个非常广泛的提示，但仍然很有用。如果有一个非常优化的库适合你的目的，你很可能无法在没有大量努力的情况下击败它的性能。像`numpy`、`pandas`、`scipy`和`sklearn`这样的库都经过了高度优化，它们的本机操作可能非常快。如果它们适合你的目的，请务必尝试一下。只是为了说明`numpy`相对于纯Python有多快，请参考以下内容：
- en: '[PRE33]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The `numpy` code does exactly the same as the Python code, except that it uses
    `numpy` arrays instead of Python lists. This little difference has made the code
    more than 25 times faster.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`numpy`代码与Python代码完全相同，只是它使用`numpy`数组而不是Python列表。这个小差异使代码快了25倍以上。'
- en: Just-in-time compiling
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 即时编译
- en: '**Just-in-time** (**JIT**) compiling is a method of dynamically compiling (parts)
    an application during runtime. Because there is much more information available
    at runtime, this can have a huge effect and make your application much faster.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**即时**（**JIT**）编译是一种在运行时动态编译（部分）应用程序的方法。因为在运行时有更多的信息可用，这可能会产生巨大的影响，并使你的应用程序运行得更快。'
- en: The `numba` package provides selective JIT compiling for you, allowing you to
    mark the functions that are JIT compiler compatible. Essentially, if your functions
    follow the functional programming paradigm of basing the calculations only on
    the input, then it will most likely work with the JIT compiler.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`numba`包为你提供了选择性的JIT编译，允许你标记与JIT编译器兼容的函数。基本上，如果你的函数遵循基于输入的函数式编程范式，那么它很可能会与JIT编译器一起工作。'
- en: 'Basic example of how the `numba` JIT compiler can be used:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`numba` JIT编译器的基本示例：'
- en: '[PRE34]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The use cases for these are limited, but if you are using `numpy` or pandas
    you will most likely benefit from `numba`.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这些用例有限，但如果你使用`numpy`或pandas，你很可能会从`numba`中受益。
- en: Another very interesting fact to note is that `numba` supports not only CPU
    optimized execution but GPU as well. This means that for certain operations you
    can use the fast processor in your video card to process the results.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常有趣的事实是，`numba`不仅支持CPU优化执行，还支持GPU。这意味着对于某些操作，你可以使用视频卡中的快速处理器来处理结果。
- en: Converting parts of your code to C
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将代码的部分转换为C
- en: We will see more about this in [Chapter 14](ch14.html "Chapter 14. Extensions
    in C/C++, System Calls, and C/C++ Libraries"), *Extensions in C/C++, System Calls,
    and C/C++ Libraries*, but if high performance is really required, then a native
    C function can help quite a lot. This doesn't even have to be that difficult.
    The Cython module makes it trivial to write parts of your code with performance
    very close to native C code.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第14章](ch14.html "第14章。C/C++扩展，系统调用和C/C++库")中看到更多关于这个问题的内容，但是如果真的需要高性能，那么本地C函数可以帮助很多。这甚至不必那么困难。Cython模块使得用性能非常接近本地C代码编写代码的过程变得非常简单。
- en: 'Following is an example from the Cython manual to approximate the value of
    pi:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Cython手册中用于近似π值的示例：
- en: '[PRE35]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: While there are some small differences such as `cdef` instead of `def` and type
    definitions for the values and parameters, the code is largely the same as regular
    Python would be, but certainly much faster.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有一些小的差异，比如使用`cdef`代替`def`以及对值和参数进行类型定义，但代码基本上与常规Python代码相同，但速度肯定要快得多。
- en: Memory usage
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存使用
- en: So far we have simply looked at the execution times and ignored the memory usage
    of the scripts. In many cases, the execution times are the most important, but
    memory usage should not be ignored. In almost all cases, CPU and memory are traded;
    a code either uses a lot of CPU or a lot of memory, which means that both do matter
    a lot.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只看了执行时间，忽略了脚本的内存使用。在许多情况下，执行时间是最重要的，但内存使用也不容忽视。在几乎所有情况下，CPU和内存是相互交换的；代码要么使用大量CPU，要么使用大量内存，这意味着两者都非常重要。
- en: Tracemalloc
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Tracemalloc
- en: 'Monitoring memory usage used to be something that was only possible through
    external Python modules such as **Dowser** or **Heapy**. While those modules still
    work, they are largely obsolete now because of the `tracemalloc` module. Let''s
    give the `tracemalloc` module a try to see how easy memory usage monitoring is
    nowadays:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 监视内存使用曾经是只能通过外部Python模块（如**Dowser**或**Heapy**）实现的事情。虽然这些模块仍然有效，但由于`tracemalloc`模块的出现，它们现在基本上已经过时了。让我们尝试一下`tracemalloc`模块，看看现在监视内存使用有多容易：
- en: '[PRE36]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This results in:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是：
- en: '[PRE37]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: You can easily see how every part of the code allocated memory and where it
    might be wasted. While it might still be unclear which part was actually causing
    the memory usage, there are options for that as well, as we will see in the following
    sections.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以很容易地看到代码的每个部分分配了内存以及可能浪费了多少。虽然可能仍然不清楚哪一部分实际上导致了内存使用，但我们将在接下来的部分中看到有关此问题的解决方案。
- en: Memory profiler
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存分析器
- en: 'The `memory_profiler` module is very similar to `line_profiler` discussed earlier,
    but for memory usage instead. Installing it is as easy as `pip install memory_profiler`,
    but the optional `pip install psutil` is also highly recommended (and required
    in the case of Windows) as it increases your performance by a large amount. To
    test `line_profiler,` we will use the following script:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`memory_profiler`模块与前面讨论的`line_profiler`非常相似，但用于内存使用。安装它就像`pip install memory_profiler`一样容易，但是强烈建议（在Windows的情况下是必需的）安装可选的`pip
    install psutil`，因为它可以大大提高性能。为了测试`line_profiler`，我们将使用以下脚本：'
- en: '[PRE38]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Note that we actually import the `memory_profiler` here although that is not
    strictly required. It can also be executed through `python3 -m memory_profiler
    your_scripts.py`:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管这里实际上导入了`memory_profiler`，但这并不是严格要求的。它也可以通过`python3 -m memory_profiler
    your_scripts.py`来执行：
- en: '[PRE39]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Even though everything runs as expected, you might be wondering about the varying
    amounts of memory used by the lines of code here. Why does `a` take `3.5 MiB`
    and `b` only `3.2 MiB`? This is caused by the Python memory allocation code; it
    reserves memory in larger blocks, which is subdivided and reused internally. Another
    problem is that `memory_profiler` takes snapshots internally, which results in
    memory being attributed to the wrong variables in some cases. The variations should
    be small enough to not make a large difference in the end, but some changes are
    to be expected.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一切都按预期运行，但您可能会对这里代码行使用的内存量不同而感到困惑。为什么`a`占用`3.5 MiB`，而`b`只占用`3.2 MiB`？这是由Python内存分配代码引起的；它以较大的块保留内存，然后在内部进行细分和重复使用。另一个问题是`memory_profiler`在内部进行快照，这导致在某些情况下将内存归因于错误的变量。这些变化应该足够小，以至于最终不会产生很大的差异，但是应该预期会有一些变化。
- en: Note
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: This module can be added as an IPython extension as well, which enables the
    `%mprun` command within IPython. To load the extension, the `load_ext` command
    can be used from the IPython shell `%load_ext memory_profiler`. Another very useful
    command is `%memit` which is the memory equivalent of the `%timeit` command.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模块也可以作为IPython扩展添加，这样就可以在IPython中使用`%mprun`命令。要加载扩展，可以从IPython shell使用`%load_ext
    memory_profiler`命令。另一个非常有用的命令是`%memit`，它是`%timeit`命令的内存等价命令。
- en: Memory leaks
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存泄漏
- en: 'The usage of these modules will generally be limited to the search for memory
    leaks. Especially the `tracemalloc` module has a few features to make that fairly
    easy. The Python memory management system is fairly simple; it just has a simple
    reference counter to see if an object is used. While this works great in most
    cases, it can easily introduce memory leaks when circular references are involved.
    The basic premise of a memory leak with leak detection code looks like this:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模块的使用通常会受到对内存泄漏的搜索的限制。特别是`tracemalloc`模块具有一些功能，使得这一过程相当容易。Python内存管理系统相当简单；它只是有一个简单的引用计数器来查看对象是否被使用。虽然在大多数情况下这很有效，但当涉及到循环引用时，它很容易引入内存泄漏。带有泄漏检测代码的内存泄漏的基本前提如下：
- en: '[PRE40]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let''s see how bad this code is actually leaking:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这段代码实际上有多糟糕的内存泄漏：
- en: '[PRE41]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: In absolute memory usage the increase is not even that great, but it is definitely
    leaking a little. The first leak is negligible; at the last iteration we see an
    increase of 28 bytes which is next to nothing. The second leak however leaks a
    lot and peaks at a 18.3 megabyte increase. Those are memory leaks, the Python
    garbage collector (`gc`) is smart enough to clean circular references eventually
    but it won't clean them until a certain limit is reached. More about that soon.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在绝对内存使用上，增加并不是很大，但肯定有一点泄漏。第一个泄漏是微不足道的；在最后一次迭代中，我们看到增加了28字节，几乎可以忽略不计。然而第二个泄漏泄漏了很多，并在增加了18.3兆字节。这些都是内存泄漏，Python垃圾收集器（`gc`）足够聪明，最终会清除循环引用，但在达到一定限制之前不会清除它们。很快就会了解更多。
- en: 'Whenever you want to have a circular reference that does not cause memory leaks,
    the `weakref` module is available. It creates reference which don''t count towards
    the object reference count. Before we look at the `weakref` module, let''s take
    a look at the object references themselves through the eyes of the Python garbage
    collector (`gc`):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 每当你想要有一个不会导致内存泄漏的循环引用时，`weakref`模块是可用的。它创建的引用不计入对象引用计数。在我们看`weakref`模块之前，让我们通过Python垃圾收集器（`gc`）的眼睛来看一下对象引用本身：
- en: '[PRE42]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'So let''s have a look at the output:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看输出：
- en: '[PRE43]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: As we can see here, until we manually call the garbage collector, the `Eggs`
    objects will stay in the memory. Even after explicitly deleting the objects. So
    does this mean you are always required to manually call `gc.collect()` to remove
    these references? Luckily that's not needed, as the Python garbage collector will
    automatically collect once the thresholds have been reached. By default, the thresholds
    for the Python garbage collector are set to `700, 10, 10` for the three generations
    of collected objects. The collector keeps track of all the memory allocations
    and deallocations in Python, and as soon as the number of allocations minus the
    number of deallocations reaches 700, the object is either removed if it's not
    referenced anymore or it is moved to the next generation if it still has a reference.
    The same is repeated for generation 2 and 3, albeit with the lower thresholds
    of 10.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在这里看到的，直到我们手动调用垃圾收集器之前，`Eggs`对象将一直留在内存中。即使明确删除了对象。那么这是否意味着你总是需要手动调用`gc.collect()`来删除这些引用？幸运的是，不需要，因为Python垃圾收集器在达到阈值后会自动进行收集。默认情况下，Python垃圾收集器的阈值设置为三代被收集对象的`700,
    10, 10`。收集器跟踪Python中所有的内存分配和释放，一旦分配次数减去释放次数达到700，如果对象不再被引用，它就会被移除，如果它仍然有引用，它就会被移动到下一代。对于第2和第3代，重复相同的操作，尽管阈值较低为10。
- en: 'This begs the question: where and when is it useful to manually call the garbage
    collector? Since the Python memory allocator reuses blocks of memory and only
    rarely releases it, for long running scripts the garbage collector can be very
    useful. That''s exactly where I recommend its usage: long running scripts in memory-strapped
    environments and specifically, right before you allocate a large amount of memory.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了一个问题：在哪里和何时手动调用垃圾收集器是有用的？由于Python内存分配器重用内存块并且很少释放它，对于长时间运行的脚本，垃圾收集器可能非常有用。这正是我推荐使用它的地方：在内存紧张的环境中长时间运行的脚本，特别是在分配大量内存之前。
- en: 'More to the point however, the `gc` module can help you a lot when looking
    for memory leaks as well. The `tracemalloc` module can show you the parts that
    take the most memory in bytes but the `gc` module can help you find the most defined
    objects. Just be careful with setting the garbage collector debug settings such
    as `gc.set_debug(gc.DEBUG_LEAK)`; it returns a large amount of output even if
    you don''t reserve any memory yourself. Revisiting our `Spam` and `Eggs` script
    from earlier, let''s see where and how the memory is being used using the garbage
    collection module:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，`gc`模块在寻找内存泄漏时也可以帮助你很多。`tracemalloc`模块可以显示占用最多内存的部分，但`gc`模块可以帮助你找到最多定义的对象。只是要小心设置垃圾收集器的调试设置，比如`gc.set_debug(gc.DEBUG_LEAK)`；即使你自己没有保留任何内存，它也会返回大量输出。重新审视我们之前的`Spam`和`Eggs`脚本，让我们看看垃圾收集模块是如何使用内存的：
- en: '[PRE44]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output is probably close to what you were already expecting:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 输出可能与你已经预期的非常接近：
- en: '[PRE45]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The large amount of `dict` objects is because of the internal state of the
    classes, but beyond that we simply see the `Eggs` objects just as we would expect.
    The `Spam` objects were properly removed by the garbage collector because they
    and all of the references were just removed. The `Eggs` objects couldn''t be removed
    because of the circular references. Now we will repeat the same example using
    the `weakref` module to see if it makes a difference:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 大量的`dict`对象是因为类的内部状态，但除此之外，我们只看到了`Eggs`对象，就像我们所期望的那样。`Spam`对象被垃圾收集器正确地移除了，因为它们和所有引用都被移除了。`Eggs`对象无法被移除，因为存在循环引用。现在我们将使用`weakref`模块重复相同的示例，看看它是否有所不同：
- en: '[PRE46]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now let''s see what remained this time:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看这次剩下了什么：
- en: '[PRE47]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Nothing besides some standard built-in Python objects, which is exactly what
    we had hoped for. Be careful with weak references though, as they can easily blow
    up in your face if the referenced object has disappeared:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 除了一些标准内置的Python对象，这正是我们所希望的。但要小心弱引用，因为如果被引用的对象消失了，它们很容易爆炸：
- en: '[PRE48]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This results in one working reference and a dead one:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致一个有效的引用和一个无效的引用：
- en: '[PRE49]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Reducing memory usage
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 减少内存使用
- en: In general, memory usage probably won't be your biggest problem in Python, but
    it can still be useful to know what you can do to reduce memory usage. When trying
    to reduce memory usage, it's important to understand how Python allocates memory.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，在Python中，内存使用可能不是你最大的问题，但了解如何减少内存使用仍然是有用的。在尝试减少内存使用时，重要的是要了解Python如何分配内存。
- en: 'There are four concepts which you need to know about within the Python memory
    manager:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Python内存管理器中有四个你需要了解的概念：
- en: First we have the heap. The heap is the collection of all Python managed memory.
    Note that this is separate from the regular heap and mixing the two could result
    in corrupt memory and crashes.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先是堆。堆是所有Python管理内存的集合。请注意，这与常规堆是分开的，混合两者可能导致内存损坏和崩溃。
- en: Second are the arenas. These are the chunks that Python requests from the system.
    These chunks have a fixed size of 256 KiB each and they are the objects that make
    up the heap.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次是arena。这些是Python从系统请求的块。这些块每个固定大小为256 KiB，它们是构成堆的对象。
- en: Third we have the pools. These are the chunks of memory that make up the arenas.
    These chunks are 4 KiB each. Since the pools and arenas have fixed sizes, they
    are simple arrays.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三，我们有pools。这些是构成arena的内存块。这些块每个大小为4 KiB。由于pools和arenas具有固定大小，它们是简单的数组。
- en: Fourth and last, we have the blocks. The Python objects get stored within these
    and every block has a specific format depending on the data type. Since an integer
    takes up more space than a character, for efficiency a different block size is
    used.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四，最后，我们有blocks。Python对象存储在这些blocks中，每个block的格式取决于数据类型。由于整数占用的空间比字符多，为了效率使用了不同的块大小。
- en: Now that we know how the memory is allocated, we can also understand how it
    can be returned to the operating system. Whenever an arena is completely empty,
    it can and will be freed. To increase the likelihood of this happening, some heuristics
    are used to maximize the usage of fuller arenas.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了内存是如何分配的，我们也可以理解它如何被返回给操作系统。每当一个arena完全为空时，它可以并且将被释放。为了增加这种情况发生的可能性，一些启发式方法被用来最大化更满的arenas的使用。
- en: Note
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: It is important to note that the regular heap and Python heap are maintained
    separately as mixing them can result in corruption and/or crashing of applications.
    Unless you write your own extensions, you will probably never have to worry about
    manual memory allocation though.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，常规堆和Python堆是分开维护的，混合它们可能导致应用程序的损坏和/或崩溃。除非你编写自己的扩展，否则你可能永远不必担心手动内存分配。
- en: Generators versus lists
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成器与列表
- en: The most important tip is to use generators whenever possible. Python 3 has
    come a long way in replacing lists with generators already, but it really pays
    off to keep that in mind as it saves not only memory, but CPU as well when not
    all of that memory needs to be kept at the same time.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的提示是尽可能使用生成器。Python 3已经在很大程度上用生成器替换了列表，但是牢记这一点确实很值得，因为它不仅节省了内存，而且在不需要一次性保留所有内存时也节省了CPU。
- en: 'To illustrate the difference:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明区别：
- en: '[PRE50]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The `range()` generator takes such little memory that it doesn't even register,
    whereas the list of numbers takes `38.6 MiB`.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '`range()` 生成器所占用的内存非常小，甚至不值一提，而数字列表占用了 `38.6 MiB`。'
- en: Recreating collections versus removing items
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重新创建集合与删除项目
- en: 'One very important detail about collections in Python is that many of them
    can only grow; they won''t just shrink by themselves. To illustrate:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Python中集合的一个非常重要的细节是，其中许多集合只能增长；它们不会自行收缩。为了说明：
- en: '[PRE51]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This is one of the most common memory usage mistakes made with lists and dictionaries.
    Besides recreating the objects, there is, of course, also the option of using
    generators instead so the memory is never allocated at all.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用列表和字典时最常见的内存使用错误之一。除了重新创建对象，当然还有使用生成器的选项，这样内存根本就不会被分配。
- en: Using slots
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用slots
- en: If you've used Python for a long time you may have seen the `__slots__` feature
    of classes. It allows you to specify which fields you want to store in a class
    and it skips all the others by not implementing `instance.__dict__`. While this
    method does save a little bit of memory in your class definitions, I recommend
    against its usage as there are several downsides to using it. The most important
    one is that they make inheritance non-obvious (adding `__slots__` to a subclassed
    class that doesn't have `__slots__` has no effect). It also makes it impossible
    to modify class attributes on the fly and breaks `weakref` by default. And lastly,
    classes with slots cannot be pickled without defining a `__getstate__` function.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你长时间使用Python，你可能已经见过类的 `__slots__` 功能。它允许你指定你想在类中存储哪些字段，并通过不实现 `instance.__dict__`
    跳过所有其他字段。虽然这种方法确实在类定义中节省了一点内存，但我不建议使用它，因为使用它有几个缺点。最重要的一个是它使继承变得不明显（给没有 `__slots__`
    的子类添加 `__slots__` 没有效果）。它还使得不可能在运行时修改类属性，并且默认情况下破坏了 `weakref`。最后，具有slots的类不能在没有定义
    `__getstate__` 函数的情况下被pickle。
- en: 'For completeness however, here''s a demonstration of the slots feature and
    the difference in memory usage:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了完整起见，这里演示了slots功能和内存使用的差异：
- en: '[PRE52]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'And the memory usage:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 和内存使用情况：
- en: '[PRE53]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: You might argue that this is not a fair comparison, since they both store a
    lot of data which skews the results. And you would indeed be right, because the
    "bare" comparison storing only `index` and nothing else gives `2 MiB` versus `4.5
    MiB`. But let's be honest, if you're not going to store data, then what's the
    point in creating class instances? That's why I recommend against the usage of
    `__slots__` and instead recommend the usage of tuples or `collections.namedtuple`
    if memory is that important. There is one more structure that's even more memory
    efficient, the `array` module. It stores the data in pretty much a bare memory
    array. Note that this is generally slower than lists and much less convenient
    to use.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为这不是一个公平的比较，因为它们都存储了大量数据，这扭曲了结果。你的确是对的，因为“裸”比较只存储 `index` 而不存储其他内容，结果是
    `2 MiB` 对比 `4.5 MiB`。但让我们诚实一点，如果你不打算存储数据，那么创建类实例有什么意义呢？这就是为什么我建议不要使用 `__slots__`，而是建议使用元组或
    `collections.namedtuple`，如果内存很重要的话。还有一种更节省内存的结构，那就是 `array` 模块。它几乎是以裸内存数组的方式存储数据。请注意，这通常比列表慢得多，使用起来也不那么方便。
- en: Performance monitoring
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能监控
- en: So far we have seen how to measure and improve both CPU and memory performance,
    but there is one part we have completely skipped over. Performance changes due
    to external factors such as growing amounts of data are very hard to predict.
    In real life applications, bottlenecks aren't constant. They change all the time
    and code that was once extremely fast might bog down as soon as more load is applied.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了如何测量和改进CPU和内存性能，但有一个部分我们完全忽略了。由于数据量的增长等外部因素导致的性能变化是非常难以预测的。在现实生活中的应用程序中，瓶颈并不是恒定的。它们一直在变化，曾经非常快的代码一旦增加了更多的负载就可能变慢。
- en: Because of that I recommend implementing a monitoring solution that tracks the
    performance of anything and everything over time. The big problem with performance
    monitoring is that you can't know what will slow down in the future and what the
    cause is going to be. I've even had websites slow down because of Memcached and
    Redis calls. These are memory only caching servers that respond well within a
    millisecond which makes slowdowns highly unlikely, until you do over a 100 cache
    calls and the latency towards the cache server increases from 0.1 milliseconds
    to 2 milliseconds, and all of a sudden those 100 calls take 200 milliseconds instead
    of 10 milliseconds. Even though 200 milliseconds still sounds like very little,
    if your total page load time is generally below 100 milliseconds, that is all
    of a sudden an enormous increase and definitely noticeable.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我建议实施一个监控解决方案，随时间跟踪任何事物的性能。性能监控的一个大问题是，您无法知道未来会出现什么减速以及原因是什么。我甚至曾经因为Memcached和Redis调用而导致网站减速。这些都是仅用于缓存的内存服务器，响应时间在毫秒内，这使得减速变得极不可能，直到您进行了100次以上的缓存调用，缓存服务器的延迟从0.1毫秒增加到2毫秒，突然之间这100次调用需要200毫秒而不是10毫秒。即使200毫秒听起来仍然很少，但如果您的总页面加载时间通常低于100毫秒，那么这突然间就是一个巨大的增加，而且肯定是显而易见的。
- en: To monitor performance and to be able to track changes over time and find the
    responsible components, I am personally a big fan of the Statsd statistic collection
    server together with the Graphite interface. Even though usability is a bit lacking,
    the result is a graphing interface which you can dynamically query to analyze
    when, where, and how your performance changed. To be able to use these you will
    have to send the metrics from your application towards the Statsd server. To do
    just that, I have written the Python-Statsd ([https://pypi.python.org/pypi/python-statsd](https://pypi.python.org/pypi/python-statsd))
    and Django-Statsd ([https://pypi.python.org/pypi/django-statsd](https://pypi.python.org/pypi/django-statsd))
    packages. These packages allow you to monitor your application from beginning
    to end, and in the case of Django you will be able to monitor your performance
    per application or view and within those see all of the components, such as the
    database, template, and caching layers. This way, you know exactly what is causing
    the slowdowns in your website (or application).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 为了监控性能并能够随时间跟踪变化并找到负责的组件，我个人非常喜欢Statsd统计收集服务器以及Graphite接口。尽管可用性有点欠缺，但结果是一个图形界面，您可以动态查询以分析性能何时、何地以及如何改变。为了能够使用这些，您将需要将应用程序的指标发送到Statsd服务器。为了做到这一点，我编写了Python-Statsd（[https://pypi.python.org/pypi/python-statsd](https://pypi.python.org/pypi/python-statsd)）和Django-Statsd（[https://pypi.python.org/pypi/django-statsd](https://pypi.python.org/pypi/django-statsd)）包。这些包允许您从头到尾监控您的应用程序，在Django的情况下，您将能够监控每个应用程序或视图的性能，并在其中查看所有组件，例如数据库、模板和缓存层。这样，您就可以准确地知道是什么导致了网站（或应用程序）的减速。
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: When it comes to performance, there is no holy grail, no single thing you can
    do to ensure peak performance in all cases. This shouldn't worry you however,
    as in most cases you will never need to tune the performance and if you do, a
    single tweak could probably fix your problem. You should be able to find performance
    problems and memory leaks in your code now which is what matters most, so just
    try to contain yourself and only tweak when it's actually needed.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在性能方面，没有圣杯，也没有一种方法可以确保在所有情况下都实现最佳性能。然而，这不应该让您担心，因为在大多数情况下，您永远不需要调整性能，如果需要，一个小调整可能就能解决您的问题。现在您应该能够找到代码中的性能问题和内存泄漏，这才是最重要的，所以请尽量克制自己，只有在真正需要时才进行调整。
- en: 'The most important outtakes from this chapter are:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 本章最重要的要点是：
- en: Test before you invest any effort. Making some functions faster seems like a
    great achievement but it is only rarely needed.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在投入任何努力之前进行测试。使一些函数更快似乎是一个很大的成就，但实际上很少需要。
- en: Choosing the correct data structure/algorithm is much more effective than any
    other performance optimization.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择正确的数据结构/算法比任何其他性能优化更有效。
- en: Circular references drain the memory until the garbage collector starts cleaning.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环引用会耗尽内存，直到垃圾收集器开始清理。
- en: Slots are not worth the effort.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 插槽不值得付出努力。
- en: The next chapter will discuss multiprocessing, a library which makes it trivial
    to employ multiple processors for your scripts. If you can't squeeze any more
    performance out of your script, multiprocessing might be your answer, as every
    (remote?) CPU core can make your script faster.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将讨论多进程，这是一个使您的脚本能够轻松利用多个处理器的库。如果您无法从脚本中挤出更多性能，多进程可能是您的答案，因为每个（远程？）CPU核心都可以使您的脚本更快。
