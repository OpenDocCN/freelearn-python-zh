- en: Chapter 11
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 11 章
- en: 'Project 3.7: Interim Data Persistence'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 项目 3.7：临时数据持久化
- en: 'Our goal is to create files of clean, converted data we can then use for further
    analysis. To an extent, the goal of creating a file of clean data has been a part
    of all of the previous chapters. We’ve avoided looking deeply at the interim results
    of acquisition and cleaning. This chapter formalizes some of the processing that
    was quietly assumed in those earlier chapters. In this chapter, we’ll look more
    closely at two topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是创建干净、转换后的数据文件，然后我们可以使用这些文件进行进一步的分析。在某种程度上，创建干净数据文件的目标是所有前几章的一部分。我们避免深入查看获取和清洗的中间结果。在这一章中，我们将正式化一些在早期章节中被默默假设的处理过程。在这一章中，我们将更仔细地探讨两个主题：
- en: File formats and data persistence
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件格式和数据持久化
- en: The architecture of applications
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序的架构
- en: 11.1 Description
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 描述
- en: In the previous chapters, particularly those starting with [*Chapter** 9*](ch013.xhtml#x1-2080009),
    [*Project 3.1:* *Data Cleaning Base Application*](ch013.xhtml#x1-2080009), the
    question of ”persistence” was dealt with casually. The previous chapters all wrote
    the cleaned samples into a file in ND JSON format. This saved delving into the
    alternatives and the various choices available. It’s time to review the previous
    projects and consider the choice of file format for persistence.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，尤其是从[*第 9 章*](ch013.xhtml#x1-2080009)、[*项目 3.1：数据清洗基础应用*](ch013.xhtml#x1-2080009)开始的章节，"持久化"问题被随意处理。前面的章节都将清洗后的样本写入
    ND JSON 格式的文件。这避免了深入研究替代方案和各种可用的选择。是时候回顾以前的项目并考虑持久化所选择的文件格式了。
- en: What’s important is the overall flow of data from acquisition to analysis. The
    conceptual flow of data is shown in [*Figure 11.1*](#11.1).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是从获取到分析的数据整体流程。数据的概念流程在[*图 11.1*](#11.1)中展示。
- en: '![Figure 11.1: Data Analysis Pipeline ](img/file51.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.1：数据分析流程](img/file51.jpg)'
- en: 'Figure 11.1: Data Analysis Pipeline'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1：数据分析流程
- en: This differs from the diagram shown in [*Chapter** 2*](ch006.xhtml#x1-470002),
    [*Overview of the Projects*](ch006.xhtml#x1-470002), where the stages were not
    quite as well defined. Some experience with acquiring and cleaning data helps
    to clarify the considerations around saving and working with data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这与[*第 2 章*](ch006.xhtml#x1-470002)、[*项目概述*](ch006.xhtml#x1-470002)中显示的图表不同，那里的阶段定义并不那么明确。一些获取和清洗数据的经验有助于阐明关于保存和使用数据的考虑。
- en: 'The diagram shows a few of the many choices for persisting interim data. A
    more complete list of format choices includes the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示了持久化临时数据的选择之一。更完整的格式选择列表包括以下内容：
- en: CSV
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSV
- en: TOML
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TOML
- en: JSON
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSON
- en: Pickle
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pickle
- en: A SQL database
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQL 数据库
- en: YAML
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YAML
- en: There are others, but this list contains formats that enjoy a direct implementation
    in Python. Note that YAML is popular but isn’t a built-in feature of the Python
    standard library. Additional formats include protocol buffers ( [https://protobuf.dev](https://protobuf.dev))
    and Parquet ( [https://parquet.apache.org](https://parquet.apache.org)). These
    two formats require a bit more work to define the structure before serializing
    and deserializing Python data; we’ll leave them out of this discussion.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有其他格式，但这个列表包含了在 Python 中有直接实现的格式。请注意，YAML 很受欢迎，但不是 Python 标准库的内置功能。其他格式包括协议缓冲区（[https://protobuf.dev](https://protobuf.dev)）和
    Parquet（[https://parquet.apache.org](https://parquet.apache.org)）。这两种格式在序列化和反序列化
    Python 数据之前需要更多的工作来定义结构；我们将它们排除在这个讨论之外。
- en: The CSV format has two disadvantages. The most notable of these problems is
    the representation of all data types as simple strings. This means any type of
    conversion information must be offered in metadata outside the CSV file. The **Pydantic**
    package provides the needed metadata in the form of a class definition, making
    this format tolerable. The secondary problem is the lack of a deeper structure
    to the data. This forces the files to have a flat sequence of primitive attributes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: CSV 格式有两个缺点。其中最明显的问题是所有数据类型都以简单字符串的形式表示。这意味着任何类型的转换信息都必须在 CSV 文件之外的元数据中提供。**Pydantic**
    包通过类定义的形式提供所需的元数据，使得这种格式可以容忍。次要问题是数据缺乏更深层次的结构。这迫使文件具有扁平的原始属性序列。
- en: The JSON format doesn’t—directly—serialize datetime or timedelta objects. To
    make this work reliably, additional metadata is required to deserialize these
    types from supported JSON values like text or numbers. This missing feature is
    provided by the **Pydantic** package and works elegantly. A `datetime.datetime`
    object will serialize as a string, and the type information in the class definition
    is used to properly parse the string. Similarly, a `datetime.timedelta` is serialized
    as a float number but converted — correctly — into a `datetime.timedelta` based
    on the type information in the class definition.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: JSON格式并不直接序列化datetime或timedelta对象。为了使这一过程可靠，需要额外的元数据来从支持的JSON值（如文本或数字）反序列化这些类型。这个缺失的功能由**Pydantic**包提供，并且工作得非常优雅。`datetime.datetime`对象将序列化为字符串，并且类定义中的类型信息被用来正确解析这个字符串。同样，`datetime.timedelta`被序列化为浮点数，但根据类定义中的类型信息正确地转换为`datetime.timedelta`。
- en: The TOML format has one advantage over the JSON format. Specifically, the TOML
    format has a tidy way to serialize datetime objects, a capability the JSON library
    lacks. The TOML format has the disadvantage of not offering a direct way to put
    multiple TOML documents into a single file. This limits TOML’s ability to handle
    vast datasets. Using a TOML file with a simple array of values limits the application
    to the amount of data that can fit into memory.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: TOML格式相对于JSON格式有一个优势。具体来说，TOML格式有一个整洁的方式来序列化datetime对象，这是JSON库所缺乏的。然而，TOML格式的缺点是它没有提供将多个TOML文档直接放入单个文件的方法。这限制了TOML处理大量数据集的能力。使用包含简单值数组的TOML文件将限制应用程序处理的数据量，只能处理适合内存的数据量。
- en: The pickle format can be used with the **Pydantic** package. This format has
    the advantage of preserving all of the Python-type information and is also very
    compact. Unlike JSON, CSV, or TOML, it’s not human-friendly and can be difficult
    to read. The `shelve` module permits the building of a handy database file with
    multiple pickled objects that can be saved and reused. While it’s technically
    possible to execute arbitrary code when reading a pickle file, the pipeline of
    acquisition and cleansing applications does not involve any unknown agencies providing
    data of unknown provenance.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用**Pydantic**包与pickle格式一起使用。这个格式具有保留所有Python类型信息的优势，并且也非常紧凑。与JSON、CSV或TOML不同，它不是人类友好的，并且难以阅读。`shelve`模块允许构建一个方便的数据库文件，其中包含多个pickle对象，可以保存和重复使用。虽然从pickle文件中读取时技术上可能执行任意代码，但获取和清理应用程序的流程不涉及任何未知机构提供来源不明的数据。
- en: A SQL database is also supported by the **Pydantic** package by using an ORM
    model. This means defining two models in parallel. One model is for the ORM layer
    (for example, SQLAlchemy) to create table definitions. The other model, a subclass
    of `pydantic.BaseModel`, uses native **Pydantic** features. The **Pydantic** class
    will have a `from_orm()` method to create native objects from the ORM layer, performing
    validation and cleaning.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pydantic**包也支持SQL数据库，通过使用ORM模型来实现。这意味着需要并行定义两个模型。一个模型用于ORM层（例如，SQLAlchemy）来创建表定义。另一个模型，是`pydantic.BaseModel`的子类，使用原生的**Pydantic**特性。**Pydantic**类将有一个`from_orm()`方法，用于从ORM层创建原生对象，执行验证和清理。'
- en: The YAML format offers the ability to serialize arbitrary Python objects, a
    capability that makes it easy to persist native Python objects. It also raises
    security questions. If care is taken to avoid working with uploaded YAML files
    from insecure sources, the ability to serialize arbitrary Python code is less
    of a potential security problem.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: YAML格式提供了序列化任意Python对象的能力，这一特性使得持久化原生Python对象变得容易。同时，这也引发了一些安全问题。如果小心避免处理来自不安全来源的上传YAML文件，那么序列化任意Python代码的能力就不再是潜在的安全问题。
- en: Of these file formats, the richest set of capabilities seems to be available
    via JSON. Since we’ll often want to record many individual samples in a single
    file, **newline-delimited** (**ND**) JSON seems to be ideal.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些文件格式中，似乎通过JSON可以提供最丰富的功能集。由于我们经常希望在一个文件中记录许多单独的样本，因此**换行符分隔**（**ND**）JSON似乎是最理想的。
- en: In some situations — particularly where spreadsheets will be used for analysis
    purposes — the CSV format offers some value. The idea of moving from a sophisticated
    Jupyter Notebook to a spreadsheet is not something we endorse. The lack of automated
    test capabilities for spreadsheets suggests they are not suitable for automated
    data processing.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下——尤其是当用于分析目的时——CSV 格式提供了一些价值。从复杂的 Jupyter Notebook 转移到电子表格的想法并不是我们支持的。电子表格缺乏自动测试功能表明它们不适合自动化数据处理。
- en: 11.2 Overall approach
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 总体方法
- en: 'For reference see [*Chapter** 9*](ch013.xhtml#x1-2080009), [*Project 3.1: Data
    Cleaning Base Application*](ch013.xhtml#x1-2080009), specifically [*Approach*](ch013.xhtml#x1-2150002).
    This suggests that the `clean` module should have minimal changes from the earlier
    version.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下内容：[*第 9 章*](ch013.xhtml#x1-2080009)、[*项目 3.1：数据清洗基础应用程序*](ch013.xhtml#x1-2080009)，特别是
    [*方法*](ch013.xhtml#x1-2150002)。这表明 `clean` 模块应该与早期版本保持最小变化。
- en: 'A cleaning application will have several separate views of the data. There
    are at least four viewpoints:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 清洗应用程序将具有对数据的几个不同视图。至少有四个观点：
- en: The source data. This is the original data as managed by the upstream applications.
    In an enterprise context, this may be a transactional database with business records
    that are precious and part of day-to-day operations. The data model reflects considerations
    of those day-to-day operations.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源数据。这是上游应用程序管理的原始数据。在企业环境中，这可能是一个包含宝贵业务记录的事务型数据库，这些记录是日常运营的一部分。数据模型反映了这些日常运营的考虑。
- en: Data acquisition interim data, usually in a text-centric format. We’ve suggested
    using ND JSON for this because it allows a tidy dictionary-like collection of
    name-value pairs, and supports quite complex Python data structures. In some cases,
    we may perform some summarization of this raw data to standardize scores. This
    data may be used to diagnose and debug problems with upstream sources. It’s also
    possible that this data only exists in a shared buffer as part of a pipeline between
    an acquire and a cleaning application.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据获取中间数据，通常以文本为中心的格式。我们建议使用 ND JSON，因为它允许整洁的字典样式的键值对集合，并支持相当复杂的 Python 数据结构。在某些情况下，我们可能对原始数据进行一些汇总以标准化分数。这些数据可能用于诊断和调试上游源的问题。也有可能这些数据仅存在于获取和清洗应用程序之间的管道中的共享缓冲区中。
- en: Cleaned analysis data, using native Python data types including `datetime`,
    `timedelta`, `int`, `float`, and `boolean`. These are supplemented with **Pydantic**
    class definitions that act as metadata for proper interpretation of the values.
    These will be used by people to support decision-making. They may be used to train
    AI models used to automate some decision-making.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清洗后的分析数据，使用包括 `datetime`、`timedelta`、`int`、`float` 和 `boolean` 在内的原生 Python
    数据类型。这些数据类型通过 **Pydantic** 类定义进行补充，这些定义作为值的正确解释的元数据。这些数据将被用于支持决策，也可能用于训练用于自动化某些决策的
    AI 模型。
- en: The decision-maker’s understanding of the available information. This viewpoint
    tends to dominate discussions with users when trying to gather, organize, and
    present data. In many cases, the user’s understanding grows and adapts quickly
    as data is presented, leading to a shifting landscape of needs. This requires
    a great deal of flexibility to provide the right data to the right person at the
    right time.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策者对可用信息的理解。当尝试收集、组织和展示数据时，这种观点往往在用户讨论中占主导地位。在许多情况下，随着数据的展示，用户的理解会迅速增长和适应，导致需求格局的变化。这需要极大的灵活性，以便在正确的时间向正确的人提供正确的数据。
- en: 'The **acquire** application overlaps with two of these models: it consumes
    the source data and produces an interim representation. The **clean** application
    also overlaps two of these models: it consumes the interim representation and
    produces the analysis model objects. It’s essential to distinguish these models
    and to use explicit, formal mappings between them.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**acquire** 应用程序与这些模型中的两个重叠：它消耗源数据并生成中间表示。**clean** 应用程序也与这些模型中的两个重叠：它消耗中间表示并生成分析模型对象。区分这些模型并使用它们之间显式、正式的映射是至关重要的。'
- en: This need for a clear separation and obvious mappings is the primary reason
    why we suggest including a “builder” method in a model class. Often we’ve called
    it something like `from_row()` or `from_dict()` or something that suggests the
    model instance is built from some other source of data via explicit assignment
    of individual attributes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对清晰分离和明显映射的需求是我们建议在模型类中包含一个“构建器”方法的主要原因。我们通常将其称为 `from_row()` 或 `from_dict()`
    或其他暗示模型实例是通过显式分配单个属性从其他数据源构建的方法。
- en: 'Conceptually, each model has a pattern similar to the one shown in the following
    snippet:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，每个模型都有一个类似于以下片段中所示的模式：
- en: '-'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '-'
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The transformation functions, `transform1()` and `transform2()`, are often implicit
    when using `pydantic.BaseModel`. This is a helpful simplification of this design
    pattern. The essential idea, however, doesn’t change, since we’re often rearranging,
    combining, and splitting source fields to create useful data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 `pydantic.BaseModel` 时，转换函数 `transform1()` 和 `transform2()` 通常是不显式的。这是对这个设计模式的一种有帮助的简化。然而，基本思想并没有改变，因为我们经常重新排列、组合和拆分源字段以创建有用的数据。
- en: When the final output format is either CSV or JSON, there are two helpful methods
    of `pydantic.BaseModel`. These methods are `dict()` and `json()`. The `dict()`
    method creates a native Python dictionary that can be used by a `csv.DictWriter`
    instance to write CSV output. The `json()` method can be used directly to write
    data in ND JSON format. It’s imperative for ND JSON to make sure the `indent`
    value used by the `json.dump()` function is `None`. Any other value for the `indent`
    parameter will create multi-line JSON objects, breaking the ND JSON file format.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当最终输出格式是 CSV 或 JSON 时，`pydantic.BaseModel` 有两个有用的方法。这些方法是 `dict()` 和 `json()`。`dict()`
    方法创建一个原生的 Python 字典，可以被 `csv.DictWriter` 实例用来写入 CSV 输出。`json()` 方法可以直接用来写入 ND
    JSON 格式的数据。对于 ND JSON 来说，确保 `json.dump()` 函数使用的 `indent` 值是 `None` 是至关重要的。`indent`
    参数的任何其他值都会创建多行 JSON 对象，破坏 ND JSON 文件格式。
- en: The **acquire** application often has to wrestle with the complication of data
    sources that are unreliable. The application should save some history from each
    attempt to acquire data and acquire only the ”missing” data, avoiding the overhead
    of rereading perfectly good data. This can become complicated if there’s no easy
    way to make a request for a subset of data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**acquire** 应用经常需要应对不可靠数据源的复杂性。应用应该保存每次尝试获取数据的历史记录，并且只获取“缺失”的数据，避免重新读取良好数据带来的开销。如果没有简单的方法来请求数据子集，这可能会变得复杂。'
- en: When working with APIs, for example, there’s a `Last-Modified` header that can
    help identify new data. The `If-Modified-Since` header on a request can avoid
    reading data that’s unchanged. Similarly, the `Range` header might be supported
    by an API to permit retrieving parts of a document after a connection is dropped.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当与 API 一起工作时，例如，有一个 `Last-Modified` 标头可以帮助识别新数据。请求上的 `If-Modified-Since` 标头可以避免读取未更改的数据。同样，`Range`
    标头可能由 API 支持，允许在连接断开后检索文档的部分。
- en: When working with SQL databases, some variants of the `SELECT` statement permit
    `LIMIT` and `OFFSET` clauses to retrieve data on separate pages. Tracking the
    pages of data can simplify restarting a long-running query.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当与 SQL 数据库一起工作时，一些 `SELECT` 语句的变体允许使用 `LIMIT` 和 `OFFSET` 子句来检索数据的不同页面。跟踪数据页面可以简化重启长时间运行的查询。
- en: Similarly, the **clean** application needs to avoid re-processing data in the
    unlikely event that it doesn’t finish and needs to be restarted. For very large
    datasets, this might mean scanning the previous, incomplete output to determine
    where to begin cleaning raw data to avoid re-processing rows.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，**clean** 应用需要避免在它没有完成并且需要重新启动的极不可能事件中重新处理数据。对于非常大的数据集，这可能意味着扫描之前的不完整输出，以确定从哪里开始清理原始数据，从而避免重新处理行。
- en: We can think of these operations as being “idempotent” in the cases when they
    have run completely and correctly. We want to be able to run (and re-run) the
    **acquire** application without damaging intermediate result files. We also want
    an additional feature of adding to the file until it’s correct and complete. (This
    isn’t precisely the definition of “idempotent”; we should limit the term to illustrate
    that correct and complete files are not damaged by re-running an application.)
    Similarly, the **clean** application should be designed so it can be run — and
    re-run — until all problems are resolved without overwriting or reprocessing useful
    results.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些操作视为在它们完全且正确运行的情况下是“幂等的”。我们希望能够在不损坏中间结果文件的情况下运行（并重新运行）“获取”应用程序。我们还想添加一个额外的功能，即在文件正确且完整之前继续添加到文件中。（这并不是“幂等”的精确定义；我们应该限制这个术语，以说明正确的完整文件不会被重新运行应用程序所损坏。）同样，设计“清理”应用程序时，应该使其能够运行——并且可以重新运行——直到所有问题都得到解决，而不会覆盖或重新处理有用的结果。
- en: 11.2.1 Designing idempotent operations
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.1 设计幂等操作
- en: Ideally, our applications present a UX that can be summarized as ”pick up where
    they left off.” The application will check for output files, and avoid destroying
    previously acquired or cleaned data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们的应用程序提供的用户体验可以概括为“从上次离开的地方继续”。应用程序将检查输出文件，并避免破坏之前获取或清理的数据。
- en: For many of the carefully curated Kaggle data sets, there will be no change
    to the source data. A time-consuming download can be avoided by examining metadata
    via the Kaggle API to decide if a file previously downloaded is complete and still
    valid.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多精心策划的Kaggle数据集，源数据将不会发生变化。可以通过检查Kaggle API中的元数据来避免耗时的下载，以确定之前下载的文件是否完整且仍然有效。
- en: For enterprise data, in a constant state of flux, the processing must have an
    explicit ”as-of date” or ”operational date” provided as a run-time parameter.
    A common way to make this date (or date-and-time) evident is to make it part of
    a file’s metadata. The most visible location is the file’s name. We might have
    a file named `2023-12-31-manufacturing-orders.ndj`, where the as-of date is clearly
    part of the file name.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于处于不断变化状态的企业数据，处理必须提供一个明确的“截至日期”或“操作日期”，作为运行时参数提供。使这个日期（或日期和时间）显而易见的一种常见方法是将其作为文件元数据的一部分。最明显的地方是文件名。我们可能有一个名为`2023-12-31-manufacturing-orders.ndj`的文件，其中截至日期显然是文件名的一部分。
- en: Idempotency requires programs in the data acquisition and cleaning pipeline
    to check for existing output files and avoid overwriting them unless an explicit
    command-line option permits overwriting. It also requires an application to read
    through the output file to find out how many rows it contains. This number of
    existing rows can be used to tailor the processing to avoid re-processing existing
    rows.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 幂等性要求数据获取和清理管道中的程序检查现有输出文件，并避免在未明确命令行选项允许覆盖的情况下覆盖它们。它还要求应用程序读取输出文件以找出它包含多少行。可以使用现有行数来调整处理，以避免重新处理现有行。
- en: Consider an application that reads from a database to acquire raw data. The
    ”as-of-date” is 2022-01-18, for example. When the application runs and something
    goes wrong in the network, the database connection could be lost after processing
    a subset of rows. We’ll imagine the output file has 42 rows written before the
    network failure caused the application to crash.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个从数据库读取以获取原始数据的程序。例如，“截至日期”是2022-01-18。当应用程序运行且网络出现问题时，数据库连接可能在处理了一部分行之后丢失。我们将想象在网络故障导致应用程序崩溃之前，输出文件已经写入了42行。
- en: 'When the log is checked and it’s clear the application failed, it can be re-run.
    The program can check the output directory and find the file with 42 rows, meaning
    the application is being run in recovery mode. There should be two important changes
    to behavior:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当检查日志并清楚应用程序失败时，它可以重新运行。程序可以检查输出目录并找到包含42行的文件，这意味着应用程序正在以恢复模式运行。应该有两个重要的行为变化：
- en: Add a `LIMIT`` -1`` OFFSET`` 42` clause to the `SELECT` statement to skip the
    42 rows already retrieved. (For many databases, `LIMIT`` -1`` OFFSET`` 0` will
    retrieve all rows; this can be used as a default value.)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`SELECT`语句中添加`LIMIT -1 OFFSET 42`子句以跳过已检索的42行。（对于许多数据库，`LIMIT -1 OFFSET 0`将检索所有行；这可以用作默认值。）
- en: Open the output file in ”append” mode to add new records to the end of the existing
    file.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以“追加”模式打开输出文件，以将新记录添加到现有文件的末尾。
- en: These two changes permit the application to be restarted as many times as required
    to query all of the required data.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个更改允许应用程序根据需要重新启动多次以查询所有所需数据。
- en: For other data sources, there may not be a simple ”limit-offset” parameter in
    the query. This may lead to an application that reads and ignores some number
    of records before processing the remaining records. When the output file doesn’t
    exist, the offset before processing has a value of zero.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他数据源，查询中可能没有简单的“limit-offset”参数。这可能导致一个读取并忽略一定数量的记录然后处理剩余记录的应用程序。当输出文件不存在时，处理前的偏移量值为零。
- en: It’s important to handle date-time ranges correctly.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 正确处理日期时间范围非常重要。
- en: It’s imperative to make sure date and date-time ranges are properly **half-open
    intervals**. The starting date and time are included. The ending date and time
    are excluded.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 确保日期和日期时间范围是正确的**半开区间**至关重要。起始日期和时间包含在内。结束日期和时间不包含。
- en: Consider a weekly extract of data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑每周的数据提取。
- en: One range is 2023-01-14 to 2023-01-21\. The 14th is included. The 21st is not
    included. The next week, the range is 2023-01-21 to 2023-01-28\. The 21st is included
    in this extract.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个范围是2023-01-14到2023-01-21。14日包含在内。21日不包含。下一周，范围是2023-01-21到2023-01-28。21日包含在本提取中。
- en: Using half-open intervals makes it easier to be sure no date is accidentally
    omitted or duplicated.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用半开区间可以更容易确保没有日期被意外遗漏或重复。
- en: Now that we’ve considered the approach to writing the interim data, we can look
    at the deliverables for this project.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经考虑了编写临时数据的方案，我们可以看看这个项目的可交付成果。
- en: 11.3 Deliverables
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 可交付成果
- en: The refactoring of existing applications to formalize the interim file formats
    leads to changes in existing projects. These changes will ripple through to unit
    test changes. There should not be any acceptance test changes when refactoring
    the data model modules.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 将现有应用程序重构为正式化临时文件格式会导致现有项目发生变化。这些变化将波及到单元测试的更改。在重构数据模型模块时，不应有任何接收测试的更改。
- en: Adding a ”pick up where you left off” feature, on the other hand, will lead
    to changes in the application behavior. This will be reflected in the acceptance
    test suite, as well as unit tests.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，添加“从上次离开的地方继续”功能将导致应用程序行为的变化。这将在接收测试套件以及单元测试中得到反映。
- en: The deliverables depend on which projects you’ve completed, and which modules
    need revision. We’ll look at some of the considerations for these deliverables.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 可交付成果取决于你已完成的项目以及哪些模块需要修订。我们将探讨这些可交付成果的一些考虑因素。
- en: 11.3.1 Unit test
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.1 单元测试
- en: A function that creates an output file will need to have test cases with two
    distinct fixtures. One fixture will have a version of the output file, and the
    other fixture will have no output file. These fixtures can be built on top of
    the `pytest.tmp_path` fixture. This fixture provides a unique temporary directory
    that can be populated with files needed to confirm that existing files are appended
    to instead of overwritten.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 创建输出文件的功能需要具有两个不同的测试用例。一个测试用例将包含输出文件的版本，另一个测试用例将不包含输出文件。这些测试用例可以建立在`pytest.tmp_path`测试用例之上。该测试用例提供了一个唯一的临时目录，可以填充所需文件以确认现有文件被追加而不是覆盖。
- en: Some test cases will need to confirm that existing files were properly extended.
    Other test cases will confirm that the file is properly created when it didn’t
    exist. An edge case is the presence of a file of length zero — it was created,
    but no data was written. This can be challenging when there is no previous data
    to read to discover the previous state.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一些测试用例需要确认现有文件已被正确扩展。其他测试用例将确认当文件不存在时，文件被正确创建。一个边缘情况是长度为零的文件的存在——它被创建，但没有写入数据。在没有以前数据可读取以发现以前状态的情况下，这可能具有挑战性。
- en: Another edge case is the presence of a damaged, incomplete row of data at the
    end of the file. This requires some clever use of the `seek()` and `tell()` methods
    of an open file to selectively overwrite the incomplete final record of the file.
    One approach is to use the `tell()` method before reading each sample. If an exception
    is raised by the file’s parser, seek to the last reported `tell()` position, and
    start writing there.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个边缘情况是文件末尾存在损坏的、不完整的行数据。这需要巧妙地使用打开文件的`seek()`和`tell()`方法来选择性地覆盖文件的不完整最后记录。一种方法是读取每个样本之前使用`tell()`方法。如果文件解析器引发异常，则跳转到最后报告的`tell()`位置，并从那里开始写入。
- en: 11.3.2 Acceptance test
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.2 接收测试
- en: 'The acceptance test scenarios will require an unreliable source of data. Looking
    back at [*Chapter** 4*](ch008.xhtml#x1-780004), [*Data Acquisition Features: Web
    APIs and Scraping*](ch008.xhtml#x1-780004), specifically [*Acceptance tests*](ch008.xhtml#x1-910003),
    we can see the acceptance test suite involves using the `bottle` project to create
    a very small web service.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 验收测试场景需要不可靠的数据源。回顾[*第4章*](ch008.xhtml#x1-780004)，[*数据采集功能：Web API和抓取*](ch008.xhtml#x1-780004)，特别是[*验收测试*](ch008.xhtml#x1-910003)，我们可以看到验收测试套件涉及使用`bottle`项目创建一个非常小的网络服务。
- en: 'There are two aspects to the scenarios, each with different outcomes. The two
    aspects are:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 场景有两个方面，每个方面都有不同的结果。两个方面是：
- en: The service or database provides all results or it fails to provide a complete
    set of results.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 服务或数据库提供所有结果，或者未能提供完整的结果集。
- en: The working files are not present — we could call this the “clean start” mode
    — or partial files exist and the application is working in recovery mode.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工作文件不存在——我们可以称之为“干净启动”模式——或者存在部分文件，并且应用程序正在恢复模式下工作。
- en: 'Since each aspect has two alternatives, there are four combinations of scenarios
    for this feature:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个方面有两个替代方案，因此该功能有四种场景组合：
- en: The existing scenario is where the working directory is empty and the API or
    database works correctly. All rows are properly saved.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现有的场景是工作目录为空，而API或数据库工作正常。所有行都得到适当保存。
- en: A new scenario where the working directory is empty and the service or database
    returns a partial result. The returned rows are saved, but the results are marked
    as incomplete, perhaps with an error entry in the log.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个新的场景，其中工作目录为空，而服务或数据库返回部分结果。返回的行被保存，但结果被标记为不完整，可能在日志中有一个错误条目。
- en: A new scenario where the given working directory has partial results and the
    API or database works correctly. The new rows are appended to existing rows, leading
    to a complete result.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个新的场景，其中给定的工作目录有部分结果，而API或数据库工作正常。新行被追加到现有行中，从而得到完整的结果。
- en: A new scenario where the given working directory has partial results and the
    service or database returns a partial result. The cumulative collection of rows
    are usable, but the results are still marked as incomplete.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个新的场景，其中给定的工作目录有部分结果，而服务或数据库返回部分结果。累积收集的行是可用的，但结果仍然被标记为不完整。
- en: A version of the mock RESTful process can return some rows and even after that
    return 502 status codes. The database version of the incomplete results scenarios
    is challenging because SQLite is quite difficult to crash at run-time. Rather
    than try to create a version of SQLite that times out or crashes, it’s better
    to rely on unit testing with a mock database to be sure crashes are handled properly.
    The four acceptance test scenarios will demonstrate that working files are extended
    without being overwritten.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟RESTful过程的某个版本可以返回一些行，甚至在之后返回502状态码。不完整结果场景的数据库版本具有挑战性，因为SQLite在运行时很难崩溃。与其尝试创建一个超时或崩溃的SQLite版本，不如依靠带有模拟数据库的单元测试来确保崩溃得到适当处理。四个验收测试场景将证明工作文件被扩展而没有被覆盖。
- en: 11.3.3 Cleaned up re-runnable application design
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.3 清理后的可重运行应用程序设计
- en: The final application with the ”pick-up-where-you-left-off” feature can be very
    handy for creating robust, reliable analytic tools. The question of ”what do we
    do to recover?” should involve little (or no) thought.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 具有“从上次离开的地方继续”功能的最终应用程序可以非常方便地创建鲁棒、可靠的分析工具。关于“我们如何恢复？”的问题应该涉及很少（或没有）思考。
- en: Creating “idempotent” applications, in general, permits rugged and reliable
    processing. When an application doesn’t work, the root cause must be found and
    fixed, and the application can be run again to finish the otherwise unfinished
    work from the failed attempt. This lets analysts focus on what went wrong — and
    fixing that — instead of having to figure out how to finish the processing.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，创建“幂等”应用程序允许鲁棒和可靠的处理。当应用程序不工作时，必须找到并修复根本原因，然后可以再次运行应用程序以完成失败的尝试中未完成的工作。这使得分析师能够专注于出了什么问题——并修复它——而不是必须弄清楚如何完成处理。
- en: 11.4 Summary
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.4 概述
- en: 'In this chapter, we looked at two important parts of the data acquisition pipeline:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了数据采集管道的两个重要部分：
- en: File formats and data persistence
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件格式和数据持久性
- en: The architecture of applications
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序的架构
- en: There are many file formats available for Python data. It seems like newline
    delimited (ND) JSON is, perhaps, the best way to handle large files of complex
    records. It fits well with Pydantic’s capabilities, and the data can be processed
    readily by Jupyter Notebook applications.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Python 数据，有许多可用的文件格式。看起来换行分隔（ND）JSON 可能是处理复杂记录的大型文件的最佳方式。它与 Pydantic 的功能很好地配合，并且数据可以很容易地由
    Jupyter Notebook 应用程序处理。
- en: The capability to retry a failed operation without losing existing data can
    be helpful when working with large data extractions and slow processing. It can
    be very helpful to be able to re-run the data acquisition without having to wait
    while previously processed data is processed again.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大型数据提取和缓慢处理时，能够重试失败的操作而不丢失现有数据可能很有帮助。能够在不等待先前处理的数据再次处理的情况下重新运行数据获取可能非常有帮助。
- en: 11.5 Extras
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.5 额外内容
- en: Here are some ideas for you to add to these projects.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些想法供您添加到这些项目中。
- en: 11.5.1 Using a SQL database
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.1 使用 SQL 数据库
- en: Using a SQL database for cleaned analytical data can be part of a comprehensive
    database-centric data warehouse. The implementation, when based on **Pydantic**,
    requires the native Python classes as well as the ORM classes that map to the
    database.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SQL 数据库来存储清洗后的分析数据可以是综合数据库中心数据仓库的一部分。当基于 **Pydantic** 实现时，需要本地的 Python 类以及映射到数据库的
    ORM 类。
- en: It also requires some care in handling repeated queries for enterprise data.
    In the ordinary file system, file names can have processing dates. In the database,
    this is more commonly assigned to an attribute of the data. This means multiple
    time periods of data occupy a single table, distinguished by the ”as-of” date
    for the rows.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 它还要求在处理企业数据时对重复查询进行一些小心处理。在普通文件系统中，文件名可以有处理日期。在数据库中，这通常分配给数据的属性。这意味着多个时间段的数据占用单个表，通过行的“as-of”日期来区分。
- en: A common database optimization is to provide a “time dimension” table. For each
    date, the associated date of the week, fiscal weeks, month, quarter, and year
    is provided as an attribute. Using this table saves computing any attributes of
    a date. It also allows the enterprise fiscal calendar to be used to make sure
    that 13-week quarters are used properly, instead of the fairly arbitrary calendar
    month boundaries.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的数据库优化是提供一个“时间维度”表。对于每个日期，提供相关的星期日期、财政周、月份、季度和年份作为属性。使用这个表可以节省计算任何日期属性。它还允许使用企业财政日历来确保正确使用
    13 周的季度，而不是相当任意的日历月份边界。
- en: This kind of additional processing isn’t required but must be considered when
    thinking about using a relational database for analysis data.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这种额外的处理不是必需的，但在考虑使用关系数据库进行分析数据时必须考虑。
- en: This extra project can use SQLAlchemy to define an ORM layer for a SQLite database.
    The ORM layer can be used to create tables and write rows of analysis data to
    those tables. This permits using SQL queries to examine the analysis data, and
    possibly use complex `SELECT-GROUP`` BY` queries to perform some analytic processing.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个额外项目可以使用 SQLAlchemy 为 SQLite 数据库定义一个 ORM 层。ORM 层可以用来创建表并将分析数据的行写入这些表。这允许使用
    SQL 查询来检查分析数据，并且可能使用复杂的 `SELECT-GROUP` 查询来执行一些分析处理。
- en: 11.5.2 Persistence with NoSQL databases
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.2 使用 NoSQL 数据库的持久性
- en: There are many NoSQL databases available. A number of products like MongoDB
    use a JSON-based document store. Database engines like PostgreSQL and SQLite3
    have the capability of storing JSON text in a column of a database table. We’ll
    narrow our focus onto JSON-based databases as a way to avoid looking at the vast
    number of databases available.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的 NoSQL 数据库有很多。像 MongoDB 这样的产品使用基于 JSON 的文档存储。像 PostgreSQL 和 SQLite3 这样的数据库引擎具有在数据库表的列中存储
    JSON 文本的能力。我们将将我们的重点缩小到基于 JSON 的数据库，以避免查看大量可用的数据库。
- en: We can use SQLite3 BLOB columns to store JSON text, creating a NoSQL database
    using the SQLite3 storage engine.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 SQLite3 BLOB 列来存储 JSON 文本，使用 SQLite3 存储引擎创建一个类似 NoSQL 的数据库。
- en: 'A small table with two columns: `doc_id`, and `doc_text`, can create a NoSQL-like
    database. The SQL definition would look like this:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一个包含两列的小表：`doc_id` 和 `doc_text`，可以创建一个类似 NoSQL 的数据库。SQL 定义看起来像这样：
- en: '[PRE1]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This table will have a primary key column that’s populated automatically with
    integer values. It has a text field that can hold the serialized text of a JSON
    document.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表将有一个自动填充整数值的主键列。它有一个可以存储 JSON 文档序列化文本的文本字段。
- en: 'The SQLite3 function `json()` should be used when inserting JSON documents:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在插入JSON文档时应该使用SQLite3的`json()`函数：
- en: '[PRE2]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This will confirm the supplied value of `json_text` is valid JSON, and will
    also minimize the storage, removing needless whitespace. This statement is generally
    executed with the parameter `{"json_text":`` json.dumps(document)` to convert
    a native Python document into JSON text so it can then be persisted into the database.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这将确认提供的`json_text`值是有效的JSON，并且还会最小化存储，移除不必要的空白。这个语句通常与参数`{"json_text":`` json.dumps(document)`一起执行，以便将原生Python文档转换为JSON文本，然后可以将其持久化到数据库中。
- en: 'The attributes of a JSON object can be interrogated using the SQLite `->>`
    operator to extract a field from a JSON document. A query for a document with
    a named field that has a specific value will look like this:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用SQLite的`->>`运算符来查询JSON对象的属性，从而从JSON文档中提取字段。对于具有特定值的命名字段的文档的查询将如下所示：
- en: '[PRE3]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the above SQL, the field’s name, `field`, is fixed as part of the SQL. This
    can be done when the schema is designed to support only a few queries. In the
    more general case, the field name might be provided as a parameter value, leading
    to a query like the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述SQL中，字段名`field`作为SQL的一部分是固定的。这可以在设计模式以支持少量查询时完成。在更一般的情况下，字段名可能作为参数值提供，导致如下查询：
- en: '[PRE4]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This query requires a small dictionary with the keys ”name” and ”value”, which
    will provide the field name and field value used to locate matching documents.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询需要一个包含“name”和“value”键的小字典，这将提供用于定位匹配文档的字段名和字段值。
- en: This kind of database design lets us write processing that’s similar to some
    of the capabilities of a document store without the overhead of installing a document
    store database. The JSON documents can be inserted into this document store. The
    query syntax uses a few SQL keywords as overhead, but the bulk of the processing
    can be JSON-based interrogation of documents to locate the desired subset of available
    documents.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据库设计让我们能够编写类似于文档存储库的一些功能的过程，而不需要安装文档存储数据库的开销。JSON文档可以插入到这个文档存储中。查询语法使用了一些SQL关键字作为开销，但大部分的处理可以通过基于JSON的文档查询来定位所需的可用文档子集。
- en: The idea here is to use a JSON-based document store instead of a file in ND
    JSON format. The Document Store interface to SQLite3 should be a module that can
    be reused in a JupyterLab Notebook to acquire and analyze data. While unit tests
    are required for the database interface, there are a few changes to the acceptance
    test suite required to confirm this changed design.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是使用基于JSON的文档存储而不是ND JSON格式的文件。SQLite3的文档存储接口应该是一个模块，可以在JupyterLab笔记本中重复使用以获取和分析数据。虽然数据库接口需要单元测试，但还需要对验收测试套件进行一些更改以确认这种设计变更。
