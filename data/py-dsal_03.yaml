- en: Principles of Algorithm Design
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法设计原理
- en: Why do we want to study algorithm design? There are of course many reasons,
    and our motivation for learning something is very much dependent on our own circumstances.
    There are without doubt important professional reasons for being interested in
    algorithm design. Algorithms are the foundations of all computing. We think of
    a computer as being a piece of hardware, a hard drive, memory chips, processors,
    and so on. However, the essential component, the thing that, if missing, would
    render modern technology impossible, is algorithms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么要学习算法设计？当然有很多原因，我们学习某件事的动机很大程度上取决于我们自己的情况。毫无疑问，对算法设计感兴趣有重要的专业原因。算法是所有计算的基础。我们认为计算机是一块硬件，包括硬盘、内存芯片、处理器等。然而，本质的组成部分，如果缺失，就会使现代技术变得不可能，那就是算法。
- en: The theoretical foundation of algorithms, in the form of the Turing machine,
    was established several decades before digital logic circuits could actually implement
    such a machine. The Turing machine is essentially a mathematical model that, using
    a predefined set of rules, translates a set of inputs into a set of outputs. The
    first implementations of Turing machines were mechanical and the next generation
    may likely see digital logic circuits replaced by quantum circuits or something
    similar. Regardless of the platform, algorithms play a central predominant role.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的理论基础，以图灵机的形式，在数字逻辑电路能够实际实现这种机器的几十年前就已经确立。图灵机本质上是一个数学模型，它使用一组预定义的规则，将一组输入转换成一组输出。图灵机的最初实现是机械的，下一代可能会看到数字逻辑电路被量子电路或类似的东西所取代。无论平台如何，算法都发挥着中心主导的作用。
- en: Another aspect is the effect algorithms have in technological innovation. As
    an obvious example, consider the page rank search algorithm, a variation of which
    the Google search engine is based on. Using this and similar algorithms allows
    researchers, scientists, technicians, and others to quickly search through vast
    amounts of information extremely quickly. This has a massive effect on the rate
    at which new research can be carried out, new discoveries made, and new innovative
    technologies developed.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，算法在技术创新中的作用也不容忽视。一个明显的例子是页面排名搜索算法，谷歌搜索引擎就是基于这种算法的变体。使用这种以及类似的算法，研究人员、科学家、技术人员和其他人可以非常快速地搜索大量信息。这对新研究开展的速度、新发现以及新创新技术的发展产生了巨大影响。
- en: 'The study of algorithms is also important because it trains us to think very
    specifically about certain problems. It can serve to increase our mental and problem
    solving abilities by helping us isolate the components of a problem and define
    relationships between these components. In summary, there are four broad reasons
    for studying algorithms:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的研究也很重要，因为它训练我们非常具体地思考某些问题。它可以通过帮助我们隔离问题的组成部分并定义这些组成部分之间的关系，来提高我们的心理和问题解决能力。总的来说，学习算法有四个主要原因：
- en: They are essential for computer science and *intelligent* systems.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们对于计算机科学和*智能*系统至关重要。
- en: They are important in many other domains (computational biology, economics,
    ecology, communications, ecology, physics, and so on).
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们在许多其他领域（计算生物学、经济学、生态学、通信、物理学等）也非常重要。
- en: They play a role in technology innovation.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 他们在技术创新中扮演着重要角色。
- en: They improve problem solving and analytical thinking.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们提高了问题解决和分析思维能力。
- en: Algorithms, in their simplest form, are just a sequence of actions, a list of
    instructions. It may just be a linear construct of the form do *x*, then do *y*,
    then do *z*, then finish. However, to make things more useful we add clauses to
    the effect of, *x* then do *y*, in Python the `if-else` statements. Here, the
    future course of action is dependent on some conditions; say the state of a data
    structure. To this we also add the operation, iteration, the while, and for statements.
    Expanding our algorithmic literacy further we add recursion. Recursion can often
    achieve the same result as iteration, however, they are fundamentally different.
    A recursive function calls itself, applying the same function to progressively
    smaller inputs. The input of any recursive step is the output of the previous
    recursive step.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在其最简单形式中，只是一系列动作，一个指令列表。它可能只是形式为执行 *x*，然后执行 *y*，然后执行 *z*，然后结束的线性结构。然而，为了使事情更有用，我们添加了诸如“*x*
    然后执行 *y*”之类的条款，在 Python 中是 `if-else` 语句。在这里，未来行动的路径取决于某些条件；比如说数据结构的状态。对此，我们还添加了操作，迭代，while
    和 for 语句。进一步扩展我们的算法素养，我们添加了递归。递归通常可以达到与迭代相同的结果，然而，它们在本质上不同。递归函数调用自身，将相同的函数应用于越来越小的输入。任何递归步骤的输入是前一个递归步骤的输出。
- en: 'Essentially, we can say that algorithms are composed of the following four
    elements:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，我们可以这样说，算法由以下四个要素组成：
- en: Sequential operations
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序操作
- en: Actions based on the state of a data structure
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于数据结构状态的行动
- en: Iteration, repeating an action a number of times
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代，重复执行一个动作多次
- en: Recursion, calling itself on a subset of inputs
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归，在输入子集上调用自身
- en: Algorithm design paradigms
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法设计范式
- en: 'In general, we can discern three broad approaches to algorithm design. They
    are:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们可以区分三种广泛的问题解决方法。它们是：
- en: Divide and conquer
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分而治之
- en: Greedy algorithms
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贪心算法
- en: Dynamic programming
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态规划
- en: As the name suggests, the divide and conquer paradigm involves breaking a problem
    into smaller sub problems, and then in some way combining the results to obtain
    a global solution. This is a very common and natural problem solving technique,
    and is, arguably, the most commonly used approach to algorithm design.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名所示，分而治之范式涉及将问题分解成更小的子问题，然后以某种方式组合结果以获得全局解决方案。这是一种非常常见且自然的问题解决技术，并且可以说是最常用的算法设计方法。
- en: Greedy algorithms often involve optimization and combinatorial problems; the
    classic example is applying it to the traveling salesperson problem, where a greedy
    approach always chooses the closest destination first. This shortest path strategy
    involves finding the best solution to a local problem in the hope that this will
    lead to a global solution.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 贪心算法通常涉及优化和组合问题；经典的例子是将它应用于旅行推销员问题，其中贪心方法总是首先选择最近的目的地。这种最短路径策略涉及寻找局部问题的最佳解决方案，希望这能导致全局解决方案。
- en: The dynamic programming approach is useful when our sub problems overlap. This
    is different from divide and conquer. Rather than break our problem into independent
    sub problems, with dynamic programming, intermediate results are cached and can
    be used in subsequent operations. Like divide and conquer it uses recursion; however,
    dynamic programming allows us to compare results at different stages. This can
    have a performance advantage over divide and conquer for some problems because
    it is often quicker to retrieve a previously calculated result from memory rather
    than having to recalculate it.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的子问题重叠时，动态规划方法是有用的。这与分而治之不同。而不是将我们的问题分解成独立的子问题，在动态规划中，中间结果被缓存并可用于后续操作。像分而治之一样，它使用递归；然而，动态规划允许我们在不同阶段比较结果。对于某些问题，这可以比分而治之有性能优势，因为从内存中检索先前计算的结果通常比重新计算它要快。
- en: Recursion and backtracking
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归和回溯
- en: 'Recursion is particularly useful for divide and conquer problems; however,
    it can be difficult to understand exactly what is happening, since each recursive
    call is itself spinning off other recursive calls. At the core of a recursive
    function are two types of cases: base cases, which tell the recursion when to
    terminate, and recursive cases that call the function they are in. A simple problem
    that naturally lends itself to a recursive solution is calculating factorials.
    The recursive factorial algorithm defines two cases: the base case when *n* is
    zero, and the recursive case when *n* is greater than zero. A typical implementation
    is the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 递归对于分而治之的问题特别有用；然而，由于每个递归调用本身又会产生其他递归调用，因此理解到底发生了什么可能很困难。递归函数的核心包含两种类型的情形：基础情形，它告诉递归何时终止，以及递归情形，它调用它们所在的函数。一个自然适合递归解决方案的简单问题就是计算阶乘。递归阶乘算法定义了两种情形：当
    *n* 为零时的基础情形，以及当 *n* 大于零时的递归情形。一个典型的实现如下：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This code prints out the digits 1, 2, 4, 24\. To calculate 4 requires four
    recursive calls plus the initial parent call. On each recursion, a copy of the
    methods variables is stored in memory. Once the method returns it is removed from
    memory. The following is a way we can visualize this process:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码打印出数字 1, 2, 4, 24。要计算 4 需要四个递归调用加上初始父调用。在每次递归中，方法变量的副本被存储在内存中。一旦方法返回，它就会被从内存中移除。以下是我们可视化的这种过程的方法：
- en: '![](img/image_03_001.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_03_001.png)'
- en: 'It may not necessarily be clear if recursion or iteration is a better solution
    to a particular problem; after all they both repeat a series of operations and
    both are very well suited to divide and conquer approaches to algorithm design.
    Iteration churns away until the problem is done. Recursion breaks the problem
    down into smaller and smaller chunks and then combines the results. Iteration
    is often easier for programmers, because control stays local to a loop, whereas
    recursion can more closely represent mathematical concepts such as factorials.
    Recursive calls are stored in memory, whereas iterations are not. This creates
    a trade off between processor cycles and memory usage, so choosing which one to
    use may depend on whether the task is processor or memory intensive. The following
    table outlines the key differences between recursion and iteration:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 并不一定总是清楚递归或迭代是解决特定问题的更好方案；毕竟，它们都重复一系列操作，并且都非常适合算法设计中分而治之的方法。迭代会一直进行，直到问题解决。递归将问题分解成越来越小的部分，然后合并结果。对于程序员来说，迭代通常更容易，因为控制始终位于循环内部，而递归可以更紧密地表示数学概念，如阶乘。递归调用被存储在内存中，而迭代则不是。这就在处理器周期和内存使用之间产生了一个权衡，因此选择使用哪一个可能取决于任务是处理器密集型还是内存密集型。以下表格概述了递归和迭代之间的关键区别：
- en: '| **Recursion** | **Iteration** |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| **递归** | **迭代** |'
- en: '| Terminates when a base case is reached | Terminates when a defined condition
    is met |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 当达到基础情形时终止 | 当满足定义的条件时终止 |'
- en: '| Each recursive call requires space in memory | Each iteration is not stored
    in memory |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 每个递归调用都需要内存空间 | 每次迭代不会被存储在内存中 |'
- en: '| An infinite recursion results in a stack overflow error | An infinite iteration
    will run while the hardware is powered |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 无限递归会导致栈溢出错误 | 无限迭代会在硬件供电时运行 |'
- en: '| Some problems are naturally better suited to recursive solutions | Iterative
    solutions may not always be obvious |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 一些问题自然更适合递归解决方案 | 迭代解决方案可能并不总是明显 |'
- en: Backtracking
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回溯
- en: Backtracking is a form of recursion that is particularly useful for types of
    problems such as traversing tree structures, where we are presented with a number
    of options at each node, from which we must choose one. Subsequently we are presented
    with a different set of options, and depending on the series of choices made either
    a goal state or a dead end is reached. If it is the latter, we must backtrack
    to a previous node and traverse a different branch. Backtracking is a divide and
    conquer method for exhaustive search. Importantly backtracking **prunes** branches
    that cannot give a result.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 回溯是一种特别适用于如遍历树结构等问题的递归形式，在每个节点我们都会面临多个选项，必须从中选择一个。随后，我们会面临另一组不同的选项，并且根据所做出的选择系列，要么达到目标状态，要么遇到死胡同。如果是后者，我们必须回溯到前一个节点并遍历不同的分支。回溯是穷举搜索的分而治之方法。重要的是，回溯**剪枝**了无法产生结果的分支。
- en: 'An example of back tracking is given in the following example. Here, we have
    used a recursive approach to generating all the possible permutations of a given
    string, *s*, of a given length *n*:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例给出了回溯的一个例子。在这里，我们使用递归方法生成给定字符串`s`的所有可能的排列，该字符串的长度为`n`：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This generates the following output:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '![](img/image_03_002.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_03_002.png)'
- en: Notice the double list compression and the two recursive calls within this comprehension.
    This recursively concatenates each element of the initial sequence, returned when
    `*n* = 1`, with each element of the string generated in the previous recursive
    call. In this sense it is *backtracking* to uncover previously ingenerated combinations.
    The final string that is returned is all *n* letter combinations of the initial
    string.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到在这个理解中的双重列表压缩和两次递归调用。在这种意义上，它是*回溯*以揭示先前生成的组合。返回的最终字符串是初始字符串的所有`n`个字母组合。
- en: Divide and conquer - long multiplication
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分而治之 - 长乘法
- en: For recursion to be more than just a clever trick, we need to understand how
    to compare it to other approaches, such as iteration, and to understand when its
    use will lead to a faster algorithm. An iterative algorithm that we are all familiar
    with is the procedure we learned in primary math classes, used to multiply two
    large numbers. That is, long multiplication. If you remember, long multiplication
    involved iterative multiplying and carry operations followed by a shifting and
    addition operation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要使递归不仅仅是一个巧妙的技巧，我们需要了解如何将其与其他方法，如迭代，进行比较，并了解何时使用递归将导致更快的算法。我们所有人都熟悉的迭代算法是在小学数学课上学习的过程，用于乘以两个大数。也就是说，长乘法。如果你还记得，长乘法涉及迭代乘法和进位操作，随后是移位和加法操作。
- en: Our aim here is to examine ways to measure how efficient this procedure is and
    attempt to answer the question; is this the most efficient procedure we can use
    for multiplying two large numbers together?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里的目标是检查衡量此过程效率的方法，并尝试回答问题；这是否是我们用于乘以两个大数的最有效过程？
- en: 'In the following figure, we can see that multiplying two 4 digit numbers together
    requires 16 multiplication operations, and we can generalize to say that an *n*
    digit number requires, approximately, *n²* multiplication operations:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中，我们可以看到将两个四位数相乘需要16次乘法操作，我们可以推广说，一个`n`位数大约需要`n²`次乘法操作：
- en: '![](img/image_03_003.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_03_003.png)'
- en: This method of analyzing algorithms, in terms of the number of computational
    primitives such as multiplication and addition, is important because it gives
    us a way to understand the relationship between the time it takes to complete
    a certain computation and the size of the input to that computation. In particular,
    we want to know what happens when the input, the number of digits, n, is very
    large. This topic, called asymptotic analysis, or time complexity, is essential
    to our study of algorithms and we will revisit it often during this chapter and
    the rest of this book.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分析算法的方法，从乘法和加法等计算原语的数量来看，很重要，因为它为我们提供了一种理解完成特定计算所需时间与该计算输入大小之间关系的方式。特别是，我们想知道当输入，即数字的位数，`n`非常大时会发生什么。这个称为渐近分析或时间复杂性的主题对于我们的算法研究至关重要，我们将在本章和本书的其余部分经常回顾它。
- en: Can we do better? A recursive approach
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们能做得更好吗？递归方法
- en: 'It turns out that in the case of long multiplication the answer is yes, there
    are in fact several algorithms for multiplying large numbers that require less
    operations. One of the most well-known alternatives to long multiplication is
    the **Karatsuba algorithm**, first published in 1962\. This takes a fundamentally
    different approach: rather than iteratively multiplying single digit numbers,
    it recursively carries out multiplication operations on progressively smaller
    inputs. Recursive programs call themselves on smaller subsets of the input. The
    first step in building a recursive algorithm is to decompose a large number into
    several smaller numbers. The most natural way to do this is to simply split the
    number in to two halves, the first half of most significant digits, and a second
    half of least significant digits. For example, our four-digit number, 2345, becomes
    a pair of two-digit numbers, 23 and 45\. We can write a more general decomposition
    of any 2 *n* digit numbers, *x* and *y* using the following, where *m* is any
    positive integer less than *n*:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，在长乘法的情况下，答案是肯定的，实际上确实存在几种用于乘以大数的算法，这些算法需要的操作更少。最著名的长乘法替代方案之一是**Karatsuba算法**，该算法首次发表于1962年。它采用了一种根本不同的方法：它不是迭代地乘以单个数字，而是递归地对越来越小的输入执行乘法操作。递归程序在输入的较小子集上调用自己。构建递归算法的第一步是将大数分解为几个较小的数。最自然的方法是将数字简单地分成两半，即最高有效位的前半部分和最低有效位的后半部分。例如，我们的四位数，2345，变成了两个两位数，23和45。我们可以使用以下方式更普遍地分解任何2
    *n*位数字的*x*和*y*，其中*m*是小于*n*的任何正整数：
- en: '![](img/image_03_004.png)![](img/image_03_005.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_03_004.png)![](img/image_03_005.png)'
- en: 'So now we can rewrite our multiplication problem *x*, *y* as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们可以将我们的乘法问题 *x*，*y* 重新写为如下形式：
- en: '![](img/image_03_006.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_03_006.png)'
- en: 'When we expand and gather like terms we get the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们展开并合并同类项时，我们得到以下结果：
- en: '![](img/image_03_007.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_03_007.png)'
- en: 'More conveniently, we can write it like this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 更方便的是，我们可以这样写：
- en: '![](img/image_03_003.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_03_003.jpg)'
- en: 'Where:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '![](img/image_03_004.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_03_004.jpg)'
- en: It should be pointed out that this suggests a recursive approach to multiplying
    two numbers since this procedure does itself involve multiplication. Specifically,
    the products *ac*, *ad*, *bc*, and *bd* all involve numbers smaller than the input
    number and so it is conceivable that we could apply the same operation as a partial
    solution to the overall problem. This algorithm, so far, consists of four recursive
    multiplication steps and it is not immediately clear if it will be faster than
    the classic long multiplication approach.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 应该指出的是，这表明了一种递归方法来乘以两个数，因为此过程本身涉及乘法。具体来说，乘积 *ac*，*ad*，*bc* 和 *bd* 都涉及比输入数小的数字，因此我们可以设想我们可以将相同的操作作为整体问题的部分解决方案来应用。到目前为止，该算法由四个递归乘法步骤组成，而且并不立即清楚它是否比经典的长乘法方法更快。
- en: What we have discussed so far in regards to the recursive approach to multiplication,
    has been well known to mathematicians since the late 19^(th) century. The Karatsuba
    algorithm improves on this is by making the following observation. We really only
    need to know three quantities: *z[2]*= *ac* ; *z[1]=ad +bc*, and *z[0]*= *bd*
    to solve equation 3.1\. We need to know the values of *a, b, c, d* only in so
    far as they contribute to the overall sum and products involved in calculating
    the quantities *z[2]*, *z[1]*, and *z[0]*. This suggests the possibility that
    perhaps we can reduce the number of recursive steps. It turns out that this is
    indeed the situation.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止所讨论的关于乘法的递归方法，自19世纪末以来就已经为数学家所熟知。Karatsuba算法通过以下观察进行了改进。我们实际上只需要知道三个数量：*z[2]*=
    *ac*；*z[1]*= *ad* + *bc*；*z[0]*= *bd*，以解决方程3.1。我们只需要知道*a*，*b*，*c*，*d*的值，只要它们对计算数量*z[2]*，*z[1]*和*z[0]*所涉及的总体和乘积有贡献。这表明，我们可能可以减少递归步骤的数量。结果证明，这确实是情况。
- en: 'Since the products *ac* and *bd* are already in their simplest form, it seems
    unlikely that we can eliminate these calculations. We can however make the following
    observation:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于乘积 *ac* 和 *bd* 已经是最简形式，因此我们似乎不太可能消除这些计算。然而，我们可以做出以下观察：
- en: '![](img/image_03_001.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_03_001.jpg)'
- en: 'When we subtract the quantities *ac* and *bd,* which we have calculated in
    the previous recursive step, we get the quantity we need, namely (*ad* + *bc*):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从之前递归步骤中计算出的数量 *ac* 和 *bd* 中减去时，我们得到所需的数量，即 (*ad* + *bc*)：
- en: '![](img/image_03_002.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_03_002.jpg)'
- en: 'This shows that we can indeed compute the sum of *ad + bc* without separately
    computing each of the individual quantities. In summary, we can improve on equation
    3.1 by reducing from four recursive steps to three. These three steps are as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明我们确实可以计算 *ad + bc* 的和，而无需单独计算每个单独的量。总之，我们可以通过将四个递归步骤减少到三个来改进方程 3.1。这三个步骤如下：
- en: Recursively calculate *ac.*
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归计算 *ac*。
- en: Recursively calculate *bd.*
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归计算 *bd*。
- en: Recursively calculate (*a* +*b*)(*c* + *d*) and subtract *ac* and *bd.*
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归计算 (*a* +*b*)(*c* + *d*) 并减去 *ac* 和 *bd*。
- en: 'The following example shows a Python implementation of the Karatsuba algorithm:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了 Karatsuba 算法的 Python 实现：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To satisfy ourselves that this does indeed work, we can run the following test
    function:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保这个方法确实有效，我们可以运行以下测试函数：
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Runtime analysis
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行时分析
- en: It should be becoming clear that an important aspect to algorithm design is
    gauging the efficiency both in terms of space (memory) and time (number of operations).
    This second measure, called runtime performance, is the subject of this section.
    It should be mentioned that an identical metric is used to measure an algorithm's
    memory performance. There are a number of ways we could, conceivably, measure
    run time and probably the most obvious is simply to measure the time the algorithm
    takes to complete. The major problem with this approach is that the time it takes
    for an algorithm to run is very much dependent on the hardware it is run on. A
    platform-independent way to gauge an algorithm's runtime is to count the number
    of operations involved. However, this is also problematic in that there is no
    definitive way to quantify an operation. This is dependent on the programming
    language, the coding style, and how we decide to count operations. We can use
    this idea, though, of counting operations, if we combine it with the expectation
    that as the size of the input increases the runtime will increase in a specific
    way. That is, there is a mathematical relationship between *n*, the size of the
    input, and the time it takes for the algorithm to run.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 应该越来越清楚，算法设计的一个重要方面是衡量其在空间（内存）和时间（操作数数量）方面的效率。这个第二度量，称为运行时性能，是本节的主题。应该提到的是，一个相同的度量用于衡量算法的内存性能。我们可以以多种方式测量运行时间，可能最明显的方法就是简单地测量算法完成所需的时间。这种方法的主要问题是算法运行所需的时间在很大程度上取决于其运行的硬件。衡量算法运行时间的平台无关方法是计算涉及的操作数数量。然而，这也存在问题，因为没有明确的方法来量化一个操作。这取决于编程语言、编码风格以及我们决定如何计数操作。尽管如此，如果我们结合这个计数操作的想法，并预期随着输入大小的增加，运行时间将以特定方式增加，那么我们可以使用这个想法。也就是说，输入大小
    *n* 和算法运行所需时间之间存在数学关系。
- en: 'Much of the discussion that follows will be framed by the following three guiding
    principles. The rational and importance of these principles should become clearer
    as we proceed. These principles are as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以下讨论的大部分内容将由以下三个指导原则来框架。随着我们继续前进，这些原则的理性和重要性将变得更加清晰。这些原则如下：
- en: Worst case analysis. Make no assumptions on the input data.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最坏情况分析。不对输入数据做任何假设。
- en: Ignore or suppress constant factors and lower order terms. At large inputs higher
    order terms dominate.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忽略或抑制常数因子和低阶项。在大输入情况下，高阶项占主导地位。
- en: Focus on problems with large input sizes.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关注大输入规模的问题。
- en: Worst case analysis is useful because it gives us a tight upper bound that our
    algorithm is guaranteed not to exceed. Ignoring small constant factors, and lower
    order terms is really just about ignoring the things that, at large values of
    the input size, *n*, do not contribute, in a large degree, to the overall run
    time. Not only does it make our work mathematically easier, it also allows us
    to focus on the things that are having the most impact on performance.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最坏情况分析是有用的，因为它给我们一个紧的上界，保证我们的算法不会超过这个上界。忽略小的常数因子和低阶项实际上就是忽略那些在输入大小很大的情况下，对总体运行时间贡献不大的因素。这不仅使我们的工作在数学上更容易，还允许我们关注对性能影响最大的因素。
- en: We saw with the Karatsuba algorithm that the number of multiplication operations
    increased to the square of the size, *n*, of the input. If we have a four-digit
    number the number of multiplication operations is 16; an eight-digit number requires
    64 operations. Typically, though, we are not really interested in the behavior
    of an algorithm at small values of *n*, so we most often ignore factors that increase
    at slower rates, say linearly with *n*. This is because at high values of *n*,
    the operations that increase the fastest as we increase *n*, will dominate.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过Karatsuba算法看到，乘法操作的次数增加到输入大小，*n*，的平方。如果我们有一个四位数，乘法操作的次数是16；一个八位数需要64次操作。然而，通常我们并不真正对算法在小的*n*值时的行为感兴趣，所以我们通常忽略增长速度较慢的因素，比如与*n*线性增长。这是因为当*n*值较高时，随着*n*的增加而增长最快的操作将占主导地位。
- en: We will explain this in more detail with an example, the merge sort algorithm.
    Sorting is the subject of [Chapter 10](0728804d-a35d-4cad-a773-68062f359f9b.xhtml),
    *Sorting*, however, as a precursor and as a useful way to learn about runtime
    performance, we will introduce merge sort here.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个例子详细解释这一点，即归并排序算法。排序是[第10章](0728804d-a35d-4cad-a773-68062f359f9b.xhtml)“排序”的主题，然而，作为先导和了解运行时性能的有用方式，我们在这里介绍归并排序。
- en: The merge sort algorithm is a classic algorithm developed over 60 years ago.
    It is still used widely in many of the most popular sorting libraries. It is relatively
    simple and efficient. It is a recursive algorithm that uses a divide and conquer
    approach. This involves breaking the problem into smaller sub problems, recursively
    solving them, and then somehow combining the results. Merge sort is one of the
    most obvious demonstrations of the divide and conquer paradigm.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 归并排序算法是一种60多年前开发的经典算法。它仍然被广泛应用于许多最受欢迎的排序库中。它相对简单且高效。它是一种使用分治法的递归算法。这涉及到将问题分解成更小的子问题，递归地解决它们，然后以某种方式合并结果。归并排序是分治范式最明显的演示之一。
- en: 'The merge sort algorithm consists of three simple steps:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 归并排序算法由三个简单的步骤组成：
- en: Recursively sort the left half of the input array.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归地对输入数组的左半部分进行排序。
- en: Recursively sort the right half of the input array.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归地对输入数组的右半部分进行排序。
- en: Merge two sorted sub arrays into one.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将两个已排序的子数组合并成一个。
- en: 'A typical problem is sorting a list of numbers into a numerical order. Merge
    sort works by splitting the input into two halves and working on each half in
    parallel. We can illustrate this process schematically with the following diagram:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的问题是将数字列表按数值顺序排序。归并排序通过将输入分成两半并并行处理每个半部分来工作。我们可以用以下示意图来示意这个过程：
- en: '![](img/image_03_012.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_03_012.png)'
- en: 'Here is the Python code for the merge sort algorithm:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是归并排序算法的Python代码：
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We run this program for the following results:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行这个程序得到以下结果：
- en: '![](img/image_03_013.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_03_013.png)'
- en: 'The problem that we are interested in is how we determine the running time
    performance, that is, what is the rate of growth in the time it takes for the
    algorithm to complete relative to the size of *n*. To understand this a bit better,
    we can map each recursive call onto a tree structure. Each node in the tree is
    a recursive call working on progressively smaller sub problems:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的问题是确定运行时间性能，即算法完成所需时间相对于*n*大小的增长速率。为了更好地理解这一点，我们可以将每个递归调用映射到一个树结构上。树中的每个节点都是一个递归调用，它正在处理越来越小的子问题：
- en: '![](img/image_03_014.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_03_014.png)'
- en: Each invocation of merge-sort subsequently creates two recursive calls, so we
    can represent this with a binary tree. Each of the child nodes receives a sub
    set of the input. Ultimately we want to know the total time it takes for the algorithm
    to complete relative to the size of *n*. To begin with we can calculate the amount
    of work and the number of operations at each level of the tree.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 每次调用merge-sort都会随后创建两个递归调用，因此我们可以用二叉树来表示这一点。每个子节点都接收输入的一个子集。最终我们想知道算法相对于*n*大小完成所需的总时间。首先，我们可以计算树中每一层的作业量和操作数。
- en: Focusing on the runtime analysis, at level 1, the problem is split into two
    *n*/2 sub problems, at level 2 there is four *n*/4 sub problems, and so on. The
    question is when does the recursion bottom out, that is, when does it reach its
    base case. This is simply when the array is either zero or one.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 专注于运行时分析，在级别 1 时，问题被分割成两个 *n*/2 子问题，在级别 2 时有四个 *n*/4 子问题，以此类推。问题是递归何时触底，即何时达到其基本案例。这简单地说就是当数组是零或一时。
- en: The number of recursive levels is exactly the number of times you need to divide
    *n* by 2 until you get a number that is at most 1\. This is precisely the definition
    of log2\. Since we are counting the initial recursive call as level 0, the total
    number of levels is log[2]*n* + 1.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 递归级别的数量正好是你需要将 *n* 除以 2 直到得到一个最多为 1 的数的次数。这正是 log2 的定义。由于我们将初始递归调用视为级别 0，所以总级别数是
    log[2]*n* + 1。
- en: Let's just pause to refine our definitions. So far we have been describing the
    number of elements in our input by the letter *n*. This refers to the number of
    elements in the first level of the recursion, that is, the length of the initial
    input. We are going to need to differentiate between the size of the input at
    subsequent recursive levels. For this we will use the letter *m* or specifically
    *m[j]* for the length of the input at recursive level *j.*
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先暂停一下，来细化我们的定义。到目前为止，我们一直用字母 *n* 来描述输入元素的数量。这指的是递归第一级中的元素数量，即初始输入的长度。我们需要区分后续递归级别中输入的大小。为此，我们将使用字母
    *m* 或更具体地，使用 *m[j]* 来表示递归级别 *j* 的输入长度。
- en: Also there are a few details we have overlooked, and I am sure you are beginning
    to wonder about. For example, what happens when *m*/2 is not an integer, or when
    we have duplicates in our input array. It turns out that this does not have an
    important impact on our analysis here; we will revisit some of the finer details
    of the merge sort algorithm in [Chapter 12](f1986aa3-0229-4480-8496-d25a2a9dae46.xhtml),
    *Design Techniques and Strategies*.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些细节我们忽略了，我确信你已经开始对此感到好奇。例如，当 *m*/2 不是一个整数时，或者当我们的输入数组中有重复项时会发生什么。实际上，这对我们的分析没有重要影响；我们将在[第
    12 章](f1986aa3-0229-4480-8496-d25a2a9dae46.xhtml)“设计技术和策略”中重新审视合并排序算法的一些更细致的细节。
- en: The advantage of using a recursion tree to analyze algorithms is that we can
    calculate the work done at each level of the recursion. How to define this work
    is simply as the total number of operations and this of course is related to the
    size of the input. It is important to measure and compare the performance of algorithms
    in a platform independent way. The actual run time will of course be dependent
    on the hardware on which it is run. Counting the number of operations is important
    because it gives us a metric that is directly related to an algorithm's performance,
    independent of the platform.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用递归树分析算法的优势在于我们可以计算递归每一级的完成工作。如何定义这项工作简单地说就是操作的总数，这当然与输入的大小有关。以平台无关的方式衡量和比较算法的性能是很重要的。当然，实际的运行时间将取决于运行它的硬件。计算操作数很重要，因为它给我们一个与算法性能直接相关的度量，与平台无关。
- en: 'In general, since each invocation of merge sort is making two recursive calls,
    the number of calls is doubling at each level. At the same time each of these
    calls is working on an input that is half of its parents. We can formalize this
    and say that:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，由于每次合并排序调用都会进行两次递归调用，所以调用次数在每一级都是翻倍的。同时，这些调用正在处理的是其父项一半大小的输入。我们可以形式化地说：
- en: For level j , where *j* is an integer 0, 1, 2 ... log[2]*n*, there are two ^j
    sub problems each of size *n*/2^j.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于级别 j，其中 *j* 是整数 0, 1, 2 ... log[2]*n*，有两个 ^j 子问题，每个子问题的规模是 *n*/2^j。
- en: To calculate the total number of operations, we need to know the number of operations
    encompassed by a single merge of two sub arrays. Let's count the number of operations
    in the previous Python code. What we are interested in is all the code after the
    two recursive calls have been made. Firstly, we have the three assignment operations.
    This is followed by three while loops. In the first loop we have an if else statement
    and within each of are two operations, a comparison followed by an assignment.
    Since there are only one of these sets of operations within the if else statements,
    we can count this block of code as two operations carried out *m* times. This
    is followed by two while loops with an assignment operation each. This makes a
    total of 4*m* + 3 operations for each recursion of merge sort.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算操作的总数，我们需要知道单个合并两个子数组所包含的操作数。让我们来计算之前Python代码中的操作数。我们感兴趣的是在两次递归调用之后的所有代码。首先，我们有三个赋值操作。接着是三个while循环。在第一个循环中，我们有一个if
    else语句，并且每个if else语句中包含两个操作，一个比较操作后跟一个赋值操作。由于这些操作集在if else语句中只有一个，我们可以将这段代码视为执行*m*次的两条操作。接着是两个包含赋值操作的while循环。这使得每次归并排序的递归调用总共需要4*m*
    + 3次操作。
- en: Since *m* must be at least 1, the upper bound for the number of operations is
    7*m*. It has to be said that this has no pretense at being an exact number. We
    could of course decide to count operations in a different way. We have not counted
    the increment operations or any of the housekeeping operations; however, this
    is not so important as we are more concerned with the rate of growth of the runtime
    with respect to *n* at high values of *n*.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于*m*至少为1，操作数的上限是7*m*。必须说的是，这并不是一个精确的数字。我们当然可以决定以不同的方式计算操作数。我们没有计算增量操作或任何维护操作；然而，这并不那么重要，因为我们更关心在*n*的高值下运行时间的增长率。
- en: This may seem a little daunting since each call of a recursive call itself spins
    off more recursive calls, and seemingly explodes exponentially. The key fact that
    makes this manageable is that as the number of recursive calls doubles, the size
    of each sub problem halves. These two opposing forces cancel out nicely as we
    can demonstrate.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来有点令人畏惧，因为每次递归调用本身会产生更多的递归调用，看起来呈指数级增长。使这变得可管理的关键事实是，随着递归调用数量的加倍，每个子问题的规模减半。这两股相反的力量很好地相互抵消，我们可以证明这一点。
- en: 'To calculate the maximum number of operations at each level of the recursion
    tree we simply multiply the number of sub problems by the number of operations
    in each sub problem as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算递归树每层的最大操作数，我们只需将子问题的数量乘以每个子问题的操作数，如下所示：
- en: '![](img/image_03_005.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_03_005.jpg)'
- en: Importantly this shows that, because the 2^j cancels out the number of operations
    at each level is independent of the level. This gives us an upper bound to the
    number of operations carried out on each level, in this example, 7*n*. It should
    be pointed out that this includes the number of operations performed by each recursive
    call on that level, not the recursive calls made on subsequent levels. This shows
    that the work done, as the number of recursive calls doubles with each level,
    is exactly counter balanced by the fact that the input size for each sub problem
    is halved.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，这表明，因为2^j抵消了每层的操作数，所以它与层无关。这给我们每个层上操作数的上限，在这个例子中是7*n*。应该指出的是，这包括在该层上每个递归调用所执行的操作数，而不是在后续层上所做的递归调用。这表明，随着递归调用数量的加倍，所做的工作与每个子问题的输入大小减半的事实正好相抵消。
- en: 'To find the total number of operations for a complete merge sort we simply
    multiply the number of operations on each level by the number of levels. This
    gives us the following:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到完整的归并排序操作的总数，我们只需将每层的操作数乘以层的数量。这给我们以下结果：
- en: '![](img/image_03_006.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_03_006.jpg)'
- en: 'When we expand this out, we get the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们展开这个公式时，我们得到以下结果：
- en: '![](img/image_03_007.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_03_007.jpg)'
- en: 'The key point to take from this is that there is a logarithmic component to
    the relationship between the size of the input and the total running time. If
    you remember from school mathematics, the distinguishing characteristic of the
    logarithm function is that it flattens off very quickly. As an input variable,
    *x*, increases in size, the output variable, *y* increases by smaller and smaller
    amounts. For example, compare the log function to a linear function:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个例子中可以得出的关键点是，输入大小与总运行时间之间的关系有一个对数成分。如果您还记得学校数学，对数函数的显著特征是它迅速变得平坦。作为一个输入变量*x*增加大小，输出变量*y*增加的量会越来越小。例如，将对数函数与线性函数进行比较：
- en: '![](img/image_03_018.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_03_018.png)'
- en: In the previous example, multiplying the *n*log[2]*n* component and comparing
    it to *n*² .
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，将*n*log[2]*n*组件相乘，并将其与*n*²进行比较。
- en: '![](img/image_03_019.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_03_019.png)'
- en: Notice how for very low values of *n*, the time to complete, *t* , is actually
    lower for an algorithm that runs in n² time. However, for values above about 40,
    the log function begins to dominate, flattening the output until at the comparatively
    moderate size *n* = 100, the performance is more than twice than that of an algorithm
    running in *n*² time. Notice also that the disappearance of the constant factor,
    + 7 is irrelevant at high values of *n*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于非常低的*n*值，完成时间*t*实际上对于运行在n²时间内的算法来说更低。然而，对于大约40以上的值，对数函数开始主导，使输出变得平坦，直到在相对适中的大小*n*
    = 100时，性能是运行在*n*²时间内的算法的两倍以上。注意，在*n*的高值中，常数因子+ 7的消失是无关紧要的。
- en: 'The code used to generate these graphs is as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 生成这些图表所使用的代码如下：
- en: '[PRE5]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You will need to install the matplotlib library, if it is not installed already,
    for this to work. Details can be found at the following address; I encourage you
    to experiment with this list comprehension expression used to generate the plots.
    For example, adding the following plot statement:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果matplotlib库尚未安装，您需要安装它才能使以下功能正常工作。详细信息可以在以下地址找到；我鼓励您尝试使用此列表推导表达式生成图表。例如，添加以下绘图语句：
- en: '[PRE6]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Gives the following output:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/image_03_020.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_03_020.png)'
- en: The preceding graph shows the difference between counting six operations or
    seven operations. We can see how the two cases diverge, and this is important
    when we are talking about the specifics of an application. However, what we are
    more interested in here is a way to characterize growth rates. We are not so much
    concerned with the absolute values, but how these values change as we increase
    *n*. In this way we can see that the two lower curves have similar growth rates,
    when compared to the top (*x*²) curve. We say that these two lower curves have
    the same **complexity class**. This is a way to understand and describe different
    runtime behaviors. We will formalize this performance metric in the next section.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表显示了计数六个操作或七个操作之间的差异。我们可以看到这两种情况是如何分叉的，当我们讨论应用的特定细节时，这一点很重要。然而，我们更感兴趣的是一种表征增长速率的方法。我们不太关心绝对值，而是关注随着*n*的增加这些值是如何变化的。这样我们就可以看到，与顶部的(*x*²)曲线相比，两个较低的曲线具有相似的增长速率。我们说这两个较低的曲线具有相同的**复杂度类**。这是一种理解和描述不同运行行为的方式。我们将在下一节中正式化这个性能指标。
- en: Asymptotic analysis
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 渐近分析
- en: 'There are essentially three things that characterize an algorithm''s runtime
    performance. They are:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的运行性能本质上由三个要素来表征。它们是：
- en: Worst case - Use an input that gives the slowest performance
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最坏情况 - 使用一个导致性能最慢的输入
- en: Best case - Use an input that give, the best results
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳情况 - 使用一个能给出最佳结果的输入
- en: Average case - Assumes the input is random
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均情况 - 假设输入是随机的
- en: To calculate each of these, we need to know the upper and lower bounds. We have
    seen a way to represent an algorithm's runtime using mathematical expressions,
    essentially adding and multiplying operations. To use asymptotic analyses, we
    simply create two expressions, one each for the best and worst cases.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算这些，我们需要知道上下限。我们已经看到了使用数学表达式表示算法运行时间的方法，本质上是通过加法和乘法操作。要使用渐近分析，我们只需创建两个表达式，一个用于最佳情况，一个用于最坏情况。
- en: Big O notation
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大O符号
- en: 'The letter "O" in big *O* notation stands for order, in recognition that rates
    of growth are defined as the order of a function. We say that one function *T*(*n*)
    is a big O of another function, *F*(*n*), and we define this as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 大 *O* 表示法中的字母 "O" 代表阶，以表明增长率被定义为函数的阶。我们说一个函数 *T*(*n*) 是另一个函数 *F*(*n*) 的大 O，我们将其定义为以下内容：
- en: '![](img/image_03_008.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_03_008.jpg)'
- en: 'The function, *g*(*n*), of the input size, *n*, is based on the observation
    that for all sufficiently large values of *n*, *g*(*n*) is bounded above by a
    constant multiple of *f*(*n*). The objective is to find the smallest rate of growth
    that is less than or equal to *f*(*n*). We only care what happens at higher values
    of *n*. The variable *n[0]*represents the threshold below which the rate of growth
    is not important, The function T(n) represents the **tight upper bound** F(n).
    In the following plot we see that *T*(*n*) = *n²* + 500 = *O*(*n²*) with *C* =
    2 and *n[0]* is approximately 23:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 输入大小 *n* 的函数 *g*(*n*) 基于以下观察：对于所有足够大的 *n* 值，*g*(*n*) 被一个常数乘以 *f*(*n*) 限制在上界。目标是找到小于或等于
    *f*(*n*) 的最小增长率。我们只关心 *n* 较大时的行为。变量 *n[0]* 代表增长率不重要的阈值，函数 T(n) 代表 **紧上界** F(n)。在下面的图中，我们看到
    *T*(*n*) = *n²* + 500 = *O*(*n²*)，其中 *C* = 2，而 *n[0]* 大约是 23：
- en: '![](img/image_03_022.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_03_022.png)'
- en: You will also see the notation *f*(*n*) = *O*(*g*(*n*)). This describes the
    fact that *O*(*g*(*n*)) is really a set of functions that include all functions
    with the same or smaller rates of growth than *f*(n). For example, *O*(*n²*) also
    includes the functions *O*(*n*), *O*(*n*log*n*), and so on.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你也会看到表示法 *f*(*n*) = *O*(*g*(*n*))。这描述了 *O*(*g*(*n*)) 实际上是一组函数，包括所有与 *f*(n) 具有相同或更小增长率的函数。例如，*O*(*n²*)
    也包括函数 *O*(*n*)、*O*(*n*log*n*) 等等。
- en: 'In the following table, we list the most common growth rates in order from
    lowest to highest. We sometimes call these growth rates the **time complexity**
    of a function, or the complexity class of a function:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下表中，我们按从低到高的顺序列出最常见的增长率。我们有时将这些增长率称为函数的 **时间复杂度** 或函数的复杂度类：
- en: '| **Complexity Class** | **Name** | **Example operations** |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| **复杂度类** | **名称** | **示例操作** |'
- en: '| O(1) | Constant | append, get item, set item. |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| O(1) | 常数 | 追加、获取项、设置项。 |'
- en: '| O(log*n*) | Logarithmic | Finding an element in a sorted array. |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| O(log*n*) | 对数 | 在排序数组中查找元素。 |'
- en: '| O(n) | Linear | copy, insert, delete, iteration. |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| O(n) | 线性 | 复制、插入、删除、迭代。 |'
- en: '| *n*Log*n* | Linear-Logarithmic | Sort a list, merge - sort. |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| *n*Log*n* | 线性对数 | 排序列表，归并排序。 |'
- en: '| *n²* | Quadratic | Find the shortest path between two nodes in a graph. Nested
    loops. |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| *n²* | 平方 | 在图中两个节点之间找到最短路径。嵌套循环。 |'
- en: '| *n³* | Cubic | Matrix multiplication. |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| *n³* | 立方 | 矩阵乘法。 |'
- en: '| 2*^n* | Exponential | ''Towers of Hanoi'' problem, backtracking. |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 2*^n* | 指数 | 汉诺塔问题、回溯。 |'
- en: Composing complexity classes
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组成复杂度类
- en: Normally, we need to find the total running time of a number of basic operations.
    It turns out that we can combine the complexity classes of simple operations to
    find the complexity class of more complex, combined operations. The goal is to
    analyze the combined statements in a function or method to understand the total
    time complexity of executing several operations. The simplest way to combine two
    complexity classes is to add them. This occurs when we have two sequential operations.
    For example, consider the two operations of inserting an element into a list and
    then sorting that list. We can see that inserting an item occurs in O(*n*) time
    and sorting is O(*n*log*n*) time. We can write the total time complexity as O(*n*
    + *n*log*n*), that is, we bring the two functions inside the O(...). We are only
    interested in the highest order term, so this leaves us with just O(*n*log*n*).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们需要找到多个基本操作的总运行时间。结果是我们可以将简单操作的复杂度类组合起来，以找到更复杂、组合操作的复杂度类。目标是分析函数或方法中的组合语句，以了解执行多个操作的总时间复杂度。将两个复杂度类组合的最简单方法是相加。这发生在我们有两个顺序操作时。例如，考虑将元素插入列表并排序该列表的两个操作。我们可以看到插入项的时间复杂度是
    O(*n*)，而排序的时间复杂度是 O(*n*log*n*)。我们可以将总时间复杂度写成 O(*n* + *n*log*n*)，即，我们将两个函数放入 O(...)。我们只对最高阶项感兴趣，所以这仅留下
    O(*n*log*n*)。
- en: 'If we repeat an operation, for example, in a while loop, then we multiply the
    complexity class by the number of times the operation is carried out. If an operation
    with time complexity O(*f*(*n*)) is repeated O(*n*) times then we multiply the
    two complexities:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们重复一个操作，例如在while循环中，那么我们将复杂度类乘以操作执行的次数。如果一个具有时间复杂度O(*f*(*n*))的操作重复O(*n*)次，那么我们将两个复杂度相乘：
- en: O(*f*(*n*) * O(*n*)) = O(*nf*(*n*)).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: O(*f*(*n*) * O(*n*)) = O(*nf*(*n*)).
- en: 'For example, suppose the function f(...) has a time complexity of O(*n*²) and
    it is executed *n* times in a while loop as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设函数f(...)的时间复杂度为O(*n*²)，并且它在以下while循环中执行*n*次：
- en: '[PRE7]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The time complexity of this loop then becomes O(*n*²) * O(*n*) = O(*n * n²*)
    = O(*n³*). Here we are simply multiplying the time complexity of the operation
    with the number of times this operation executes. The running time of a loop is
    at most the running time of the statements inside the loop multiplied by the number
    of iterations. A single nested loop, that is, one loop nested inside another loop,
    will run in *n*² time assuming both loops run *n* times. For example:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这个循环的时间复杂度因此变为O(*n*²) * O(*n*) = O(*n * n²*) = O(*n³*)。这里我们只是将操作的复杂度与执行此操作的次数相乘。循环的运行时间最多是循环内语句的运行时间乘以迭代次数。一个嵌套的单层循环，即一个循环嵌套在另一个循环中，如果两个循环都运行*n*次，则将在*n*²时间内运行。例如：
- en: '[PRE8]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Each statement is a constant, c, executed *n**n* times, so we can express the
    running time as ; *c**n* *n* = *cn*² = O(*n*2).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 每个语句都是一个常数，c，执行*n**n*次，因此我们可以将运行时间表示为； *c**n* *n* = *cn*² = O(*n*2)。
- en: 'For consecutive statements within nested loops we add the time complexities
    of each statement and multiply by the number of times the statement executed.
    For example:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对于嵌套循环中的连续语句，我们添加每个语句的复杂度，并乘以语句执行的次数。例如：
- en: '[PRE9]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This can be written as *c*[0] +*c*[1]*n* + *cn*² = O(*n*²).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以写成 *c*[0] +*c*[1]*n* + *cn*² = O(*n*²)。
- en: 'We can define (base 2) logarithmic complexity, reducing the size of the problem
    by ½, in constant time. For example, consider the following snippet:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义（以2为底）对数复杂度，在常数时间内减少问题的大小。例如，考虑以下代码片段：
- en: '[PRE10]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Notice that `i` is doubling on each iteration, if we run this with *n* = 10
    we see that it prints out four numbers; 2, 4, 8, and 16\. If we double *n* we
    see it prints out five numbers. With each subsequent doubling of n the number
    of iterations is only increased by 1\. If we assume *k* iterations, we can write
    this as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到`i`在每次迭代中都翻倍，如果我们用*n* = 10运行它，我们会看到它打印出四个数字；2，4，8，和16。如果我们加倍*n*，我们会看到它打印出五个数字。随着n的后续加倍，迭代次数仅增加1。如果我们假设*k*次迭代，我们可以将其写成以下形式：
- en: '![](img/03_009.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/03_009.png)'
- en: From this we can conclude that the total time = **O**(*log(n)*).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里我们可以得出结论，总时间 = **O**(*log(n)*)。
- en: Although Big O is the most used notation involved in asymptotic analysis, there
    are two other related notations that should be briefly mentioned. They are Omega
    notation and Theta notation.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大O符号是参与渐近分析中最常用的符号，但还有两个其他相关的符号应该简要提及。它们是Ω符号和Θ符号。
- en: Omega notation (Ω)
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ω符号（Ω）
- en: 'In a similar way that Big O notation describes the upper bound, Omega notation
    describes a **tight lower bound**. The definition is as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，大O符号描述了上界，Ω符号描述了一个**紧下界**。定义如下：
- en: '![](img/03_010.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/03_010.png)'
- en: The objective is to give the largest rate of growth that is equal to or less
    than the given algorithms, T(*n*), rate of growth.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是给出与给定算法T(*n*)增长率相等或更小的最大增长率。
- en: Theta notation (ϴ)
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Θ符号（ϴ）
- en: 'It is often the case where both the upper and lower bounds of a given function
    are the same and the purpose of Theta notation is to determine if this is the
    case. The definition is as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 经常会出现给定函数的上界和下界相同的情况，Θ符号的目的是确定这种情况是否成立。定义如下：
- en: '![](img/03_011.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/03_011.png)'
- en: Although Omega and Theta notations are required to completely describe growth
    rates, the most practically useful is Big O notation and this is the one you will
    see most often.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Ω和Θ符号需要完全描述增长率，但最实用的还是大O符号，这也是你最常见的符号。
- en: Amortized analysis
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平均分析
- en: Often we are not so interested in the time complexity of individual operations,
    but rather the time averaged running time of sequences of operations. This is
    called amortized analysis. It is different from average case analysis, which we
    will discuss shortly, in that it makes no assumptions regarding the data distribution
    of input values. It does, however, take into account the state change of data
    structures. For example, if a list is sorted it should make any subsequent find
    operations quicker. Amortized analysis can take into account the state change
    of data structures because it analyzes sequences of operations, rather then simply
    aggregating single operations.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我们并不那么关心单个操作的时空复杂度，而是关心一系列操作的平均运行时间。这被称为换算分析。它与我们将很快讨论的平均情况分析不同，因为它对输入值的分布没有做出任何假设。然而，它确实考虑了数据结构的状态变化。例如，如果一个列表已排序，它应该使任何后续的查找操作更快。换算分析可以考虑到数据结构的状态变化，因为它分析的是操作序列，而不是简单地汇总单个操作。
- en: Amortized analysis finds an upper bound on runtime by imposing an artificial
    cost on each operation in a sequence of operations, and then combining each of
    these costs. The artificial cost of a sequence takes in to account that the initial
    expensive operations can make subsequent operations cheaper.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 换算分析通过在一系列操作中对每个操作施加一个人为的成本，然后结合这些成本来找到一个运行时间的上界。一个序列的人为成本考虑到初始的昂贵操作可以使后续操作更便宜。
- en: When we have a small number of expensive operations, such as sorting, and lots
    of cheaper operations such as lookups, standard worst case analysis can lead to
    overly pessimistic results, since it assumes that each lookup must compare each
    element in the list until a match is found. We should take into account that once
    we sort the list we can make subsequent find operations cheaper.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有一些昂贵的操作，如排序，以及大量的便宜操作，如查找时，标准的最坏情况分析可能会导致过于悲观的结论，因为它假设每个查找都必须比较列表中的每个元素，直到找到匹配项。我们应该考虑到一旦我们排序列表，我们就可以使后续的查找操作更便宜。
- en: 'So far in our runtime analysis we have assumed that the input data was completely
    random and have only looked at the effect the size of the input has on the runtime.
    There are two other common approaches to algorithm analysis; they are:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在我们的运行时间分析中，我们假设输入数据是完全随机的，并且只看了输入大小对运行时间的影响。算法分析有两种其他常见的方法；它们是：
- en: Average case analysis
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均情况分析
- en: Benchmarking
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准测试
- en: Average case analysis finds the average running time based on some assumptions
    regarding the relative frequencies of various input values. Using real-world data,
    or data that replicates the distribution of real-world data, is many times on
    a particular data distribution and the average running time is calculated.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 平均情况分析基于对各种输入值相对频率的一些假设来找到平均运行时间。使用现实世界的数据，或者复制现实世界数据分布的数据，在特定数据分布上多次进行，并计算平均运行时间。
- en: Benchmarking is simply having an agreed set of typical inputs that are used
    to measure performance. Both benchmarking and average time analysis rely on having
    some domain knowledge. We need to know what the typical or expected datasets are.
    Ultimately we will try to find ways to improve performance by fine-tuning to a
    very specific application setting.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试只是有一个达成一致的典型输入集，用于衡量性能。基准测试和平均时间分析都依赖于某些领域知识。我们需要知道典型或预期的数据集是什么。最终，我们将尝试通过针对非常具体的应用设置进行微调来找到提高性能的方法。
- en: Let's look at a straightforward way to benchmark an algorithm's runtime performance.
    This can be done by simply timing how long the algorithm takes to complete given
    various input sizes. As we mentioned earlier, this way of measuring runtime performance
    is dependent on the hardware that it is run on. Obviously faster processors will
    give better results, however, the relative growth rates as we increase the input
    size will retain characteristics of the algorithm itself rather than the hardware
    it is run on. The absolute time values will differ between hardware (and software)
    platforms; however, their relative growth will still be bound by the time complexity
    of the algorithm.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看评估算法运行性能的一种简单方法。这可以通过简单地测量算法完成给定各种输入大小所需的时间来完成。正如我们之前提到的，这种测量运行性能的方法取决于其运行的硬件。显然，更快的处理器会给出更好的结果，然而，随着输入大小的增加，相对增长率将保留算法本身的特征，而不是其运行的硬件。绝对时间值将在不同的硬件（和软件）平台上有所不同；然而，它们的相对增长率仍然受算法时间复杂度的限制。
- en: 'Let''s take a simple example of a nested loop. It should be fairly obvious
    that the time complexity of this algorithm is O(n²) since for each n iterations
    in the outer loop there are also n iterations in the inter loop. For example,
    our simple nested for loop consists of a simple statement executed on the inner
    loop:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个嵌套循环的简单例子为例。应该很明显，这个算法的时间复杂度是O(n²)，因为对于外循环中的每个n次迭代，内循环中也有n次迭代。例如，我们的简单嵌套for循环由内循环中执行的一个简单语句组成：
- en: '[PRE11]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following code is a simple test function that runs the nest function with
    increasing values of `n`. With each iteration we calculate the time this function
    takes to complete using the `timeit.timeit` function. The `timeit` function, in
    this example, takes three arguments, a string representation of the function to
    be timed, a setup function that imports the nest function, and an `int` parameter
    that indicates the number of times to execute the main statement. Since we are
    interested in the time the nest function takes to complete relative to the input
    size, `n`, it is sufficient, for our purposes, to call the nest function once
    on each iteration. The following function returns a list of the calculated runtimes
    for each value of n:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个简单的测试函数，它使用递增的`n`值运行`nest`函数。在每次迭代中，我们使用`timeit.timeit`函数计算此函数完成所需的时间。在这个例子中，`timeit`函数接受三个参数，一个表示要计时的函数的字符串表示，一个导入`nest`函数的设置函数，以及一个表示执行主语句次数的`int`参数。由于我们感兴趣的是`nest`函数相对于输入大小`n`完成所需的时间，因此，在我们的目的上，每次迭代只需调用一次`nest`函数。以下函数返回每个`n`值的计算运行时间列表：
- en: '[PRE12]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the following code we run the test2 function and graph the results, together
    with the appropriately scaled n² function for comparison, represented by the dashed
    line:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们运行了`test2`函数，并绘制了结果图，包括用于比较的适当缩放的n²函数，用虚线表示：
- en: '[PRE13]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This gives the following results:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下结果：
- en: '![](img/image_03_026.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_03_026.png)'
- en: As we can see, this gives us pretty much what we expect. It should be remembered
    that this represents both the performance of the algorithm itself as well as the
    behavior of underlying software and hardware platforms, as indicated by both the
    variability in the measured runtime and the relative magnitude of the runtime.
    Obviously a faster processor will result in faster runtimes, and also performance
    will be affected by other running processes, memory constraints, clock speed,
    and so on.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这几乎是我们预期的结果。应该记住，这既代表了算法本身的性能，也代表了底层软件和硬件平台的行为，正如测量运行时间的可变性和运行时间的相对大小所表明的。显然，更快的处理器将导致更快的运行时间，而且性能还会受到其他运行进程、内存限制、时钟速度等因素的影响。
- en: Summary
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we have taken a general overview of algorithm design. Importantly,
    we saw a platform independent way to measure an algorithm's performance. We looked
    at some different approaches to algorithmic problems. We looked at a way to recursively
    multiply large numbers and also a recursive approach for merge sort. We saw how
    to use backtracking for exhaustive search and generating strings. We also introduced
    the idea of benchmarking and a simple platform-dependent way to measure runtime.
    In the following chapters, we will revisit many of these ideas with reference
    to specific data structures. In the next chapter, we will discuss linked lists
    and other pointer structures.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们对算法设计进行了概述。重要的是，我们看到了一种平台无关的方式来衡量算法的性能。我们探讨了算法问题的不同方法。我们研究了递归乘大数和归并排序的递归方法。我们看到了如何使用回溯进行穷举搜索和生成字符串。我们还介绍了基准测试和一种简单的平台相关的方式来衡量运行时间。在接下来的章节中，我们将参考具体的数据结构重新审视许多这些想法。在下一章中，我们将讨论链表和其他指针结构。
