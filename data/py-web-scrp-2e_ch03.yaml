- en: Caching Downloads
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓存下载
- en: In the previous chapter, we learned how to scrape data from crawled web pages
    and save the results to a CSV file. What if we now want to scrape an additional
    field, such as the flag URL? To scrape additional fields, we would need to download
    the entire website again. This is not a significant obstacle for our small example
    website; however, other websites can have millions of web pages, which could take
    weeks to recrawl. One way scrapers avoid these problems is by caching crawled
    web pages from the beginning, so they only need to be downloaded once.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何从爬取的网页中抓取数据并将结果保存到CSV文件中。如果我们现在想抓取额外的字段，比如标志URL，为了抓取额外的字段，我们需要再次下载整个网站。这对我们的示例网站来说不是一个重大的障碍；然而，其他网站可能有数百万个网页，这可能需要几周的时间才能重新抓取。爬虫避免这些问题的方法之一是从一开始就缓存爬取的网页，这样它们就只需要下载一次。
- en: In this chapter, we will cover a few ways to do this using our web crawler.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍几种使用我们的网络爬虫来实现这一功能的方法。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: When to use caching
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 何时使用缓存
- en: Adding cache support to the link crawler
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为链接爬虫添加缓存支持
- en: Testing the cache
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试缓存
- en: Using requests - cache
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用requests - 缓存
- en: Redis cache implementation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redis缓存实现
- en: When to use caching?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 何时使用缓存？
- en: To cache, or not to cache? This is a question many programmers, data scientists,
    and web scrapers need to answer. In this chapter, we will show you how to use
    caching for your web crawlers; but should you use caching?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要缓存还是不要缓存？这是一个许多程序员、数据科学家和网络爬虫需要回答的问题。在本章中，我们将向您展示如何为您的网络爬虫使用缓存；但你应该使用缓存吗？
- en: If you need to perform a large crawl, which may be interrupted due to an error
    or exception, caching can help by not forcing you to recrawl all the pages you
    might have already covered. Caching can also help you by allowing you to access
    those pages while offline (for your own data analysis or development purposes).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要进行大规模的爬取，这可能会因为错误或异常而中断，缓存可以帮助你不必重新爬取你可能已经覆盖的所有页面。缓存还可以通过允许你在离线状态下访问这些页面（用于你的数据分析或开发目的）来帮助你。
- en: However, if having the most up-to-date and current information from the site
    is your highest priority, then caching might not make sense. In addition, if you
    don't plan large or repeated crawls, you might just want to scrape the page each
    time.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你将获取网站最新和最准确的信息作为最高优先级，那么缓存可能就不合适了。此外，如果你不打算进行大量或重复的爬取，你可能只想每次都抓取页面。
- en: You may want to outline how often the pages you are scraping change or how often
    you should scrape new pages and clear the cache before implementing it; but first,
    let's learn how to use caching!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在实施之前，你可能想要概述你正在抓取的页面更改的频率或你应该多久抓取一次新页面并清除缓存；但首先，让我们学习如何使用缓存！
- en: Adding cache support to the link crawler
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为链接爬虫添加缓存支持
- en: 'To support caching, the `download` function developed in [Chapter 1](py-web-scrp-2e_ch01.html),
    *Introduction to Web Scraping*, needs to be modified to check the cache before
    downloading a URL. We also need to move throttling inside this function and only
    throttle when a download is made, and not when loading from a cache. To avoid
    the need to pass various parameters for every download, we will take this opportunity
    to refactor the `download` function into a class so parameters can be set in the
    constructor and reused numerous times. Here is the updated implementation to support
    this:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要支持缓存，需要在[第1章](py-web-scrp-2e_ch01.html)中开发的`download`函数中进行修改，以在下载URL之前检查缓存。我们还需要将节流操作移入此函数中，并且只在下载时进行节流，而不是在从缓存中加载时。为了避免每次下载都需要传递各种参数，我们将利用这个机会将`download`函数重构为类，以便可以在构造函数中设置参数并多次重用。以下是支持此功能的更新实现：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The full source code for the Download class is available at [https://github.com/kjam/wswp/blob/master/code/chp3/downloader.py](https://github.com/kjam/wswp/blob/master/code/chp3/downloader.py).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下载类的完整源代码可在[https://github.com/kjam/wswp/blob/master/code/chp3/downloader.py](https://github.com/kjam/wswp/blob/master/code/chp3/downloader.py)找到。
- en: The interesting part of the `Download` class used in the preceding code is in
    the `__call__` special method, where the cache is checked before downloading.
    This method first checks whether this URL was previously put in the cache. By
    default, the cache is a Python dictionary. If the URL is cached, it checks whether
    a server error was encountered in the previous download. Finally, if no server
    error was encountered, the cached result can be used. If any of these checks fails,
    the URL needs to be downloaded as usual, and the result will be added to the cache.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面代码中使用的`Download`类的有趣部分在于`__call__`特殊方法，在下载之前检查缓存。该方法首先检查此URL是否之前已放入缓存。默认情况下，缓存是一个Python字典。如果URL已缓存，它检查在之前的下载中是否遇到了服务器错误。最后，如果没有遇到服务器错误，可以使用缓存的缓存结果。如果这些检查中的任何一个失败，则需要像往常一样下载URL，并将结果添加到缓存中。
- en: The `download` method of this class is almost the same as the previous `download`
    function, except now it returns the HTTP status code so the error codes can be
    stored in the cache. In addition, instead of calling itself and testing `num_retries`,
    it must first decrease the `self.num_retries` and then recursively use `self.download`
    if there are still retries left. If you just want a simple download without throttling
    or caching, this method can be used instead of `__call__`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类的`download`方法几乎与之前的`download`函数相同，但现在它返回HTTP状态码，因此错误代码可以存储在缓存中。此外，它不再调用自身并测试`num_retries`，而是首先减少`self.num_retries`，然后在还有重试剩余的情况下递归地使用`self.download`。如果你只想进行简单的下载而不需要节流或缓存，可以使用这个方法代替`__call__`。
- en: The `cache` class is used here by calling `result = cache[url]` to load from
    `cache` and `cache[url] = result` to save to `cache`, which is a convenient interface
    from Python's built-in dictionary data type. To support this interface, our `cache`
    class will need to define the `__getitem__()` and `__setitem__()` special class
    methods.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用`result = cache[url]`从`cache`中加载和`cache[url] = result`将结果保存到`cache`，我们使用Python内置字典数据类型的方便接口。为了支持此接口，我们的`cache`类需要定义`__getitem__()`和`__setitem__()`特殊类方法。
- en: 'The link crawler also needs to be slightly updated to support caching by adding
    the `cache` parameter, removing the throttle, and replacing the `download` function
    with the new class, as shown in the following code:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 链接爬虫也需要稍作更新以支持缓存，通过添加`cache`参数、移除节流并替换`download`函数为新类，如下所示：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You'll notice that `num_retries` is now linked to our call. This allows us to
    utilize the number of request retries on a per-URL basis. If we simply use the
    same number of retries without ever resetting the `self.num_retries` value, we
    will run out of retries if we reach a `500` error from one page.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到`num_retries`现在与我们的调用相关联。这允许我们根据每个URL利用请求重试次数。如果我们简单地使用相同的重试次数而不重置`self.num_retries`值，一旦遇到一个页面的`500`错误，我们将耗尽重试次数。
- en: You can check the full code again at the book repository ([https://github.com/kjam/wswp/blob/master/code/chp3/advanced_link_crawler.py](https://github.com/kjam/wswp/blob/master/code/chp3/advanced_link_crawler.py)).
    Now, our web scraping infrastructure is prepared, and we can start building the
    actual cache.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在书籍仓库中再次查看完整代码（[https://github.com/kjam/wswp/blob/master/code/chp3/advanced_link_crawler.py](https://github.com/kjam/wswp/blob/master/code/chp3/advanced_link_crawler.py)）。现在，我们的网络爬取基础设施已经准备就绪，我们可以开始构建实际的缓存。
- en: Disk Cache
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 磁盘缓存
- en: 'To cache downloads, we will first try the obvious solution and save web pages
    to the filesystem. To do this, we will need a way to map URLs to a safe cross-platform
    filename. The following table lists limitations for some popular filesystems:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓存下载，我们首先尝试明显的解决方案，并将网页保存到文件系统中。为此，我们需要一种将URL映射到安全的跨平台文件名的方法。以下表格列出了某些流行文件系统的限制：
- en: '| **Operating system** | **Filesystem** | **Invalid filename characters** |
    **Maximum filename length** |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| **操作系统** | **文件系统** | **无效文件名字符** | **最大文件名长度** |'
- en: '| Linux | Ext3/Ext4 | / and \0 | 255 bytes |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| Linux | Ext3/Ext4 | / and \0 | 255 bytes |'
- en: '| OS X | HFS Plus | : and \0 | 255 UTF-16 code units |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| OS X | HFS Plus | : and \0 | 255 UTF-16 code units |'
- en: '| Windows | NTFS | \, /, ?, :, *, ", >, <, and &#124; | 255 characters |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| Windows | NTFS | \, /, ?, :, *, ", >, <, and &#124; | 255 characters |'
- en: 'To keep our file path safe across these filesystems, it needs to be restricted
    to numbers, letters, and basic punctuation, and it should replace all other characters
    with an underscore, as shown in the following code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在这些文件系统中保持文件路径的安全，它需要限制为数字、字母和基本标点符号，并且应该将所有其他字符替换为下划线，如下所示：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Additionally, the filename and the parent directories need to be restricted
    to 255 characters (as shown in the following code) to meet the length limitations
    described in the preceding table:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，文件名和父目录需要限制为255个字符（如下所示），以满足前面表格中描述的长度限制：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here, no sections of our URL are longer than 255; so our file path hasn''t
    changed. There is also an edge case, which should be considered, where the URL
    path ends with a slash (`/`), and the empty string after this slash would be an
    invalid filename. However, removing this slash to use the parent for the filename
    would prevent saving other URLs. Consider the following URLs:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们的URL的任何部分都没有超过255个字符；因此，我们的文件路径没有改变。还有一个需要考虑的边缘情况，即URL路径以斜杠（`/`）结尾，斜杠后面的空字符串将是一个无效的文件名。然而，删除此斜杠以使用父目录作为文件名将阻止保存其他URL。考虑以下URL：
- en: http://example.webscraping.com/index/
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: http://example.webscraping.com/index/
- en: http://example.webscraping.com/index/1
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: http://example.webscraping.com/index/1
- en: 'If you need to save these, the index needs to be a directory to save the child
    page with filename 1\. The solution our disk cache will use is appending `index.html`
    to the filename when the URL path ends with a slash. The same applies when the
    URL path is empty. To parse the URL, we will use the `urlsplit` function, which
    splits a URL into its components:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要保存这些，索引需要是一个目录，以保存具有文件名1的子页面。我们的磁盘缓存将使用的方法是在URL路径以斜杠结尾时将`index.html`附加到文件名。当URL路径为空时也适用。为了解析URL，我们将使用`urlsplit`函数，该函数将URL分割成其组成部分：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This function provides a convenient interface to parse and manipulate URLs.
    Here is an example using this module to append `index.html` for this edge case:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数提供了一个方便的接口来解析和操作URL。以下是一个使用此模块为这种边缘情况添加`index.html`的示例：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Depending on the site you are scraping, you may want to modify this edge case
    handling. For example, some sites will append `/` on every URL due to the way
    the web server expects the URL to be sent. For these sites, you might be safe
    simply stripping the trailing forward slash for every URL. Again, evaluate and
    update the code for your web crawler to best fit the site(s) you intend to scrape.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你正在抓取的网站，你可能需要修改这种边缘情况的处理。例如，一些网站会由于Web服务器期望URL的发送方式，在每个URL后附加`/`。对于这些网站，你可能只需简单地为每个URL删除尾随的反斜杠。再次评估并更新你的网络爬虫代码，以最好地适应你打算抓取的网站。
- en: Implementing DiskCache
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现DiskCache
- en: 'In the previous section, we covered the limitations of file systems that need
    to be considered when building a disk-based cache, namely the restriction on which
    characters can be used, the filename length, and ensuring a file and directory
    are not created in the same location. Combining this code with logic to map a
    URL to a filename will form the main part of the disk cache. Here is an initial
    implementation of the `DiskCache` class:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了在构建基于磁盘的缓存时需要考虑的文件系统限制，即可以使用的字符限制、文件名长度限制，以及确保文件和目录不在同一位置创建。将此代码与将URL映射到文件名的逻辑相结合，将形成磁盘缓存的主要部分。以下是`DiskCache`类的初始实现：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The class constructor shown in the preceding code takes a parameter to set the
    location of the cache, and then the `url_to_path` method applies the filename
    restrictions that have been discussed so far. Now we just need methods to load
    and save the data with this filename.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码中显示的类构造函数接受一个参数来设置缓存的位置，然后`url_to_path`方法应用了之前讨论的文件名限制。现在我们只需要提供使用此文件名加载数据和保存数据的方法。
- en: 'Here is an implementation of these missing methods:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是这些缺失方法的实现：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In `__setitem__()`, the URL is mapped to a safe filename using `url_to_path()`,
    and then the parent directory is created, if necessary. The `json` module is used
    to serialize the Python and then save it to disk. Also, in `__getitem__()`, the
    URL is mapped to a safe filename. If the filename exists, the content is loaded
    using `json` to restore the original data type. If the filename does not exist
    (that is, there is no data in the cache for this URL), a `KeyError` exception
    is raised.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在`__setitem__()`中，使用`url_to_path()`将URL映射到安全的文件名，然后根据需要创建父目录。使用`json`模块序列化Python对象，然后将其保存到磁盘。在`__getitem__()`中，也将URL映射到安全的文件名。如果文件名存在，则使用`json`加载内容以恢复原始数据类型。如果文件名不存在（即，对于此URL没有缓存中的数据），则引发`KeyError`异常。
- en: Testing the cache
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试缓存
- en: Now we are ready to try `DiskCache` with our crawler by passing it to the `cache` keyword
    argument. The source code for this class is available at [https://github.com/kjam/wswp/blob/master/code/chp3/diskcache.py](https://github.com/kjam/wswp/blob/master/code/chp3/diskcache.py), and
    the cache can be tested in any Python interpreter.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备通过将`DiskCache`传递给`cache`关键字参数来在我们的爬虫中尝试使用它。这个类的源代码可在[https://github.com/kjam/wswp/blob/master/code/chp3/diskcache.py](https://github.com/kjam/wswp/blob/master/code/chp3/diskcache.py)找到，并且可以在任何Python解释器中测试缓存。
- en: IPython comes with a great set of tools for writing and interpreting Python,
    especially Python debugging, using [IPython magic commands](https://ipython.org/ipython-doc/3/interactive/magics.html).
    You can install IPython using pip or conda (`pip install ipython`).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: IPython附带了一套出色的工具，用于编写和解释Python，特别是使用[IPython魔法命令](https://ipython.org/ipython-doc/3/interactive/magics.html)进行Python调试。您可以使用pip或conda安装IPython（`pip
    install ipython`）。
- en: 'Here, we use [IPython](https://ipython.org/) to help time our request to test
    its performance:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用[IPython](https://ipython.org/)来帮助我们计时以测试其性能：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The first time this command is run, the cache is empty, so all the web pages
    are downloaded normally. However, when we run this script a second time, the pages
    will be loaded from the cache, so the crawl should be completed more quickly,
    as shown here:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次运行此命令时，缓存为空，因此所有网页都会正常下载。然而，当我们第二次运行此脚本时，页面将从缓存中加载，因此爬取应该会更快完成，如下所示：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As expected, this time the crawl completed much faster. While downloading with
    an empty cache on my computer, the crawler took over a minute; the second time,
    with a full cache, it took just 1.1 seconds (about 95 times faster!).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，这次爬取完成得更快。在我电脑上使用空缓存下载时，爬虫花费了一分钟多；第二次，使用完整缓存，只需1.1秒（大约快95倍！）。
- en: The exact time on your computer will differ depending on the speed of your hardware
    and Internet connection. However, the disk cache will undoubtedly be faster than
    downloading via HTTP.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的电脑上的确切时间将取决于您硬件的速度和互联网连接速度。然而，磁盘缓存无疑会比通过HTTP下载更快。
- en: Saving disk space
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节省磁盘空间
- en: 'To minimize the amount of disk space required for our cache, we can compress
    the downloaded HTML file. This is straightforward to implement by compressing
    the pickled string with `zlib` before saving to disk. Using our current implementation
    has the benefit of having human readable files. I can look at any of the cache
    pages and see the dictionary in JSON form. I could also reuse these files, if
    needed, and move them to different operating systems for use with non-Python code.
    Adding compression will make these files no longer readable just by opening them
    and might introduce some encoding issues if we are using the downloaded pages
    with other coding languages. To allow compression to be turned on and off, we
    can add it to our constructor along with the file encoding, which we will default
    to UTF-8:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化我们缓存所需的磁盘空间，我们可以压缩下载的HTML文件。通过在保存到磁盘之前使用`zlib`压缩序列化的字符串来实现这一点很简单。使用我们当前的实现方式的好处是文件可读性高。我可以查看任何缓存页面，并看到以JSON形式显示的字典。如果需要，我还可以重用这些文件，并将它们移动到不同的操作系统上，用于非Python代码。添加压缩会使这些文件仅通过打开它们就不再可读，并且如果我们使用其他编码语言下载的页面，可能会引入一些编码问题。为了允许压缩可以开启或关闭，我们可以将其添加到构造函数中，与文件编码一起，我们将默认设置为UTF-8：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, the `__getitem__` and `__setitem__` methods should be updated:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，应该更新`__getitem__`和`__setitem__`方法：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: With this addition of compressing each web page, the cache is reduced from 416
    KB to 156 KB and takes 260 milliseconds to crawl the cached example website on
    my computer.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加压缩每个网页的功能，缓存从416 KB减少到156 KB，在我的电脑上爬取缓存的示例网站需要260毫秒。
- en: Depending on your operating system and Python installation, the wait time may
    be slightly longer with the uncompressed cache (mine was actually shorter). Depending
    on the prioritization of your constraints (speed versus memory, ease of debugging,
    and so on), make informed and measured decisions about whether to use compression
    or not for your crawler.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的操作系统和Python安装，未压缩的缓存等待时间可能会稍微长一些（我的实际上更短）。根据您对约束条件的优先级（速度与内存、调试的简便性等）进行有信息和量化的决策，决定是否为您的爬虫使用压缩。
- en: You can see the updated disk cache code in the book's code repository ([https://github.com/kjam/wswp/blob/master/code/chp3/diskcache.py](https://github.com/kjam/wswp/blob/master/code/chp3/diskcache.py)).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在书籍的代码仓库中看到更新的磁盘缓存代码（[https://github.com/kjam/wswp/blob/master/code/chp3/diskcache.py](https://github.com/kjam/wswp/blob/master/code/chp3/diskcache.py)）。
- en: Expiring stale data
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过期无效数据
- en: Our current version of the disk cache will save a value to disk for a key and
    then return it whenever this key is requested in the future. This functionality
    may not be ideal when caching web pages because of online content changes, so
    the data in our cache will become out of date. In this section, we will add an
    expiration time to our cached data so the crawler knows when to download a fresh
    copy of the web page. To support storing the timestamp of when each web page was
    cached is straightforward.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前版本的磁盘缓存会将值保存到磁盘上的一个键，并在将来请求此键时返回它。由于在线内容的变化，这种功能在缓存网页时可能不是理想的，因此我们缓存中的数据会变得过时。在本节中，我们将为我们的缓存数据添加一个过期时间，这样爬虫就知道何时下载网页的新副本。为了支持存储每个网页被缓存的时间戳，这是直截了当的。
- en: 'Here is an implementation of this:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是这个实现的示例：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the constructor, the default expiration time is set to 30 days with a `timedelta`
    object. Then, the `__set__` method saves the expiration timestamp as a key in
    the `result` dictionary, and the `__get__` method compares the current UTC time
    to the expiration time. To test this expiration, we can try a short timeout of
    5 seconds, as shown here:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在构造函数中，默认的过期时间设置为30天，使用`timedelta`对象。然后，`__set__`方法将过期时间戳作为键存储在`result`字典中，而`__get__`方法将当前UTC时间与过期时间进行比较。为了测试这个过期时间，我们可以尝试一个短暂的超时时间，例如5秒，如下所示：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As expected, the cached result is initially available, and then, after sleeping
    for five seconds, calling the same key raises a `KeyError` to show this cached
    download has expired.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，缓存的结果最初是可用的，然后，在睡眠五秒后，调用相同的键会引发一个`KeyError`，以显示这个缓存的下载已经过期。
- en: Drawbacks of DiskCache
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DiskCache的缺点
- en: 'Our disk-based caching system was relatively simple to implement, does not
    depend on installing additional modules, and the results are viewable in our file
    manager. However, it has the drawback of depending on the limitations of the local
    filesystem. Earlier in this chapter, we applied various restrictions to map URLs
    to safe filenames, but an unfortunate consequence of this system is that some
    URLs will map to the same filename. For example, replacing unsupported characters
    in the following URLs will map them all to the same filename:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的基于磁盘的缓存系统相对简单易实现，不需要安装额外的模块，并且结果可以在我们的文件管理器中查看。然而，它有一个缺点，即依赖于本地文件系统的限制。在本章的早期部分，我们应用了各种限制来将URL映射到安全的文件名，但这个系统的不幸后果是，一些URL会映射到相同的文件名。例如，替换以下URL中的不受支持的字符将使它们都映射到相同的文件名：
- en: http://example.com/?a+b
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://example.com/?a+b](http://example.com/?a+b)'
- en: http://example.com/?a*b
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://example.com/?a*b](http://example.com/?a*b)'
- en: http://example.com/?a=b
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://example.com/?a=b](http://example.com/?a=b)'
- en: http://example.com/?a!b
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://example.com/?a!b](http://example.com/?a!b)'
- en: This means that, if one of these URLs were cached, it would look like the other
    three URLs were cached as well because they map to the same filename. Alternatively,
    if some long URLs only differed after the 255^(th) character, the shortened versions
    would also map to the same filename. This is a particularly important problem
    since there is no defined limit on the maximum length of a URL. However, in practice,
    URLs over 2,000 characters are rare, and older versions of Internet Explorer did
    not support over 2,083 characters.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，如果这些URL中的任何一个被缓存，它们看起来就像其他三个URL也被缓存了一样，因为它们映射到相同的文件名。或者，如果一些长URL在255^(th)个字符之后才有所不同，缩短的版本也会映射到相同的文件名。这是一个特别重要的问题，因为URL的最大长度没有定义的限制。然而，在实践中，超过2,000个字符的URL很少见，而且旧版本的Internet
    Explorer不支持超过2,083个字符。
- en: One potential solution to avoid these limitations is to take the hash of the
    URL and use the hash as the filename. This may be an improvement; however, we
    will eventually face a larger problem many filesystems have, that is, a limit
    on the number of files allowed per volume and per directory. If this cache is
    used in a FAT32 filesystem, the maximum number of files allowed per directory
    is just 65,535\. This limitation could be avoided by splitting the cache across
    multiple directories; however, filesystems can also limit the total number of
    files. My current `ext4` partition supports a little over 31 million files, whereas
    a large website may have excess of 100 million web pages. Unfortunately, the `DiskCache`
    approach has too many limitations to be of general use. What we need instead is
    to combine multiple cached web pages into a single file and index them with a`B+``tree`
    or a similar data structure. Instead of implementing our own, we will use existing
    key-value store in the next section.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 避免这些限制的一个潜在解决方案是取URL的哈希值，并使用哈希值作为文件名。这可能是一个改进；然而，我们最终会面临许多文件系统都有的一个更大的问题，即每个卷和每个目录允许的文件数量限制。如果在这个缓存中使用FAT32文件系统，每个目录允许的文件最大数量仅为65,535。通过将缓存分散到多个目录中可以避免这种限制；然而，文件系统也可能限制文件的总数。我的当前`ext4`分区支持略超过3100万文件，而一个大型网站可能有超过1亿个网页。不幸的是，`DiskCache`方法有太多的限制，不能被普遍使用。我们真正需要的是将多个缓存的网页合并成一个文件，并使用`B+`树或类似的数据结构进行索引。我们不会实现自己的，而是在下一节中使用现有的键值存储。
- en: Key-value storage cache
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 键值存储缓存
- en: To avoid the anticipated limitations to our disk-based cache, we will now build
    our cache on top of an existing key-value storage system. When crawling, we may
    need to cache massive amounts of data and will not need any complex joins, so
    we will use high availability key-value storage, which is easier to scale than
    a traditional relational database or even most NoSQL databases. Specifically,
    our cache will use Redis, which is a very popular key-value store.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免对基于磁盘的缓存的预期限制，我们现在将在现有的键值存储系统之上构建我们的缓存。在爬取时，我们可能需要缓存大量数据，并且不需要任何复杂的连接操作，因此我们将使用高可用性的键值存储，这比传统的数据库或大多数NoSQL数据库更容易扩展。具体来说，我们的缓存将使用Redis，这是一个非常流行的键值存储。
- en: What is key-value storage?
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是键值存储？
- en: '**Key-value storage** is very similar to a Python dictionary, in that each
    element in the storage has a key and a value. When designing the `DiskCache`,
    a key-value model lent itself well to the problem. Redis, in fact, stands for REmote
    DIctionary Server. Redis was first released in 2009, and the API supports clients
    in many different languages (including Python). It differentiates itself from
    some of the more simple key-value stores, such as memcache, because the values
    can be several different structured data types. Redis can scale easily via clusters
    and is used by large companies, such as Twitter, for massive cache storage (such
    as one Twitter BTree with around 65TB allocated heap memory ([highscalability.com/blog/2014/9/8/how-twitter-uses-redis-to-scale-105tb-ram-39mm-qps-10000-ins.html](http://highscalability.com/blog/2014/9/8/how-twitter-uses-redis-to-scale-105tb-ram-39mm-qps-10000-ins.html))).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**键值存储**与Python字典非常相似，因为存储中的每个元素都有一个键和一个值。在设计`DiskCache`时，键值模型非常适合这个问题。实际上，Redis代表远程字典服务器。Redis首次发布于2009年，其API支持多种不同语言的客户端（包括Python）。它与一些更简单的键值存储（如memcache）不同，因为其值可以是几种不同的结构化数据类型。Redis可以通过集群轻松扩展，并被大型公司（如Twitter）用于大量缓存存储（例如，一个大约有65TB分配堆内存的Twitter
    BTree [highscalability.com/blog/2014/9/8/how-twitter-uses-redis-to-scale-105tb-ram-39mm-qps-10000-ins.html](http://highscalability.com/blog/2014/9/8/how-twitter-uses-redis-to-scale-105tb-ram-39mm-qps-10000-ins.html)）。'
- en: For your scraping and crawling needs, there might be instances where you need
    more information for each document or need to be able to search and select based
    on the data in the document. For these instances, I recommend a document-based
    database, such as ElasticSearch or MongoDB. Both key-value stores and document-based
    databases are able to scale and quickly query non-relational data in a clearer
    and easier way than a traditional SQL database with schemas (such as PostgreSQL
    and MySQL).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于您的抓取和爬取需求，可能会有一些情况需要为每个文档提供更多信息，或者需要能够根据文档中的数据进行搜索和选择。对于这些情况，我推荐使用基于文档的数据库，如ElasticSearch或MongoDB。键值存储和基于文档的数据库都能够以比具有模式的传统SQL数据库（如PostgreSQL和MySQL）更清晰、更简单的方式扩展并快速查询非关系型数据。
- en: Installing Redis
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Redis
- en: 'Redis can be installed by compiling the latest source as per the instructions
    on the Redis site ([https://redis.io/topics/quickstart](https://redis.io/topics/quickstart)).
    If you are running Windows, you will need to use MSOpenTech''s project ([https://github.com/MSOpenTech/redis](https://github.com/MSOpenTech/redis))
    or simply install Redis via a VirtualMachine (using Vagrant) or a docker instance.
    The Python client then needs to be installed separately using this command:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 可以按照Redis网站上的说明编译最新源代码来安装Redis（[https://redis.io/topics/quickstart](https://redis.io/topics/quickstart)）。如果你正在运行Windows，你需要使用MSOpenTech的项目（[https://github.com/MSOpenTech/redis](https://github.com/MSOpenTech/redis)）或者简单地通过虚拟机（使用Vagrant）或docker实例安装Redis。然后需要单独使用以下命令安装Python客户端：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To test whether the installation is working, start Redis locally (or on your
    virtual machine or container) using this command:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试安装是否正常工作，请使用以下命令在本地（或虚拟机或容器）启动Redis：
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You should see some text with the version number and the Redis symbol. At the
    end of the text, you will see a message like this:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到一些带有版本号和Redis符号的文本。在文本的末尾，你会看到如下信息：
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Most likely, your Redis server will be using the same port, which is the default
    port (6379). To test our Python client and connect to Redis, we can use a Python
    interpreter (in the following code, I am using IPython), as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能，你的Redis服务器将使用相同的端口，这是默认端口（6379）。为了测试我们的Python客户端并连接到Redis，我们可以使用Python解释器（在下面的代码中，我使用IPython），如下所示：
- en: '[PRE17]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the preceding code, we were able to easily connect to our Redis server and
    then `set` a record with the key `'test'` and value `'answer'`. We were able to
    easily retrieve that record using the `get` command.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们能够轻松地连接到我们的Redis服务器，然后使用键`'test'`和值`'answer'`来`set`一条记录。我们能够使用`get`命令轻松检索该记录。
- en: To see more options on how to set up Redis to run as a background process, I
    recommend using the official Redis Quick Start ([https://redis.io/topics/quickstart](https://redis.io/topics/quickstart))
    or looking up specific instructions for your particular operating system or installation
    using your favorite search engine.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看如何将Redis设置为后台进程运行的更多选项，我建议使用官方的Redis快速入门（[https://redis.io/topics/quickstart](https://redis.io/topics/quickstart)）或使用你喜欢的搜索引擎查找特定操作系统的具体说明或安装说明。
- en: Overview of Redis
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Redis概述
- en: 'Here is an example of how to save some example website data in Redis and then
    load it:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个示例，说明如何在Redis中保存一些示例网站数据，然后加载它：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We can see with the `get` output, that we will receive `bytes` back from our
    Redis storage, even if we have inserted a dictionary, or a string. We can manage
    these serializations the same way we did for our `DiskCache` class, by using the `json`
    module.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`get`输出看到，即使我们插入了一个字典或字符串，我们也会从Redis存储中接收到`bytes`。我们可以像管理我们的`DiskCache`类一样管理这些序列化，通过使用`json`模块。
- en: What happens if we need to update the content of a URL?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要更新URL的内容会发生什么？
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We can see from the above output that the `set` command in Redis will simply
    overwrite the previous value, which makes it great for simple storage such as
    our web crawler. For our needs, we only want one set of content for each URL,
    so it maps well to key-value stores.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的输出中我们可以看到，Redis中的`set`命令会简单地覆盖之前的值，这使得它非常适合像我们的网络爬虫这样的简单存储。对于我们的需求，我们只想为每个URL有一组内容，因此它与键值存储很好地映射。
- en: 'Let''s take a look at what is in our storage, and clean up what we don''t want:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的存储中有什么，并清理我们不需要的内容：
- en: '[PRE20]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The `keys` method returns a list of all available keys, and the `delete` method
    allows us to pass one (or more) keys and delete them from our store. We can also
    delete all keys:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`keys`方法返回所有可用键的列表，`delete`方法允许我们传递一个（或多个）键并将它们从我们的存储中删除。我们还可以删除所有键：'
- en: '[PRE21]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: There are many more commands and utilizations for Redis, so feel free to read
    further in the documentation. For now, we should have all we need to create a
    cache with a Redis backend for our web crawler.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Redis有许多更多的命令和用法，所以请随意阅读文档中的更多内容。目前，我们应该有创建带有Redis后端的缓存所需的所有内容，用于我们的网络爬虫。
- en: The Python Redis client [https://github.com/andymccurdy/redis-py](https://github.com/andymccurdy/redis-py)
    provides great documentation and several use cases for using Python with Redis
    (such as a PubSub pipeline, or as a large connection pool). The official Redis
    documentation [https://redis.io/documentation](https://redis.io/documentation)
    has a long list of tutorials, books, references, and use cases; so if you'd like
    to learn more about how to scale, secure, and deploy Redis, I recommend starting
    there. And if you are using Redis in the cloud or on a server, don't forget to
    implement security for your Redis instance ([https://redis.io/topics/security](https://redis.io/topics/security))!
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Python Redis 客户端 [https://github.com/andymccurdy/redis-py](https://github.com/andymccurdy/redis-py)
    提供了出色的文档和多个使用 Python 与 Redis 一起使用的案例（例如 PubSub 管道或作为大连接池）。官方 Redis 文档 [https://redis.io/documentation](https://redis.io/documentation)
    列出了大量的教程、书籍、参考资料和用例；因此，如果您想了解更多关于如何扩展、安全性和部署 Redis 的信息，我建议从那里开始。如果您在云端或服务器上使用
    Redis，别忘了为您的 Redis 实例实现安全性 ([https://redis.io/topics/security](https://redis.io/topics/security))！
- en: Redis cache implementation
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Redis 缓存实现
- en: 'Now we are ready to build our cache on Redis using the same class interface
    as the earlier `DiskCache` class:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备使用与早期 `DiskCache` 类相同的类接口在 Redis 上构建我们的缓存：
- en: '[PRE22]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `__getitem__` and `__setitem__` methods here should be familiar to you
    from the discussion on how to get and set keys in Redis in the previous section,
    with the exception that we are using the `json` module to control serialization
    and the `setex` method, which allows us to set a key and value with an expiration
    time. `setex` will accept either a `datetime.timedelta` or a number of seconds. This
    is a handy Redis feature that will automatically delete records in a specified
    number of seconds. This means we do not need to manually check whether a record
    is within our expiration guidelines, as in the `DiskCache` class. Let''s try it
    out in IPython (or the interpreter of your choice) using a timedelta of 20 seconds,
    so we can see the cache expire:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 `__getitem__` 和 `__setitem__` 方法应该与上一节中关于如何在 Redis 中获取和设置键的讨论中提到的内容相似，只是我们使用
    `json` 模块来控制序列化，并使用 `setex` 方法，这允许我们设置带有过期时间的键和值。`setex` 可以接受 `datetime.timedelta`
    或秒数。这是一个方便的 Redis 功能，它将自动在指定秒数后删除记录。这意味着我们不需要像在 `DiskCache` 类中那样手动检查记录是否在过期指南内。让我们在
    IPython（或您选择的解释器）中使用 20 秒的时间跨度来尝试它，这样我们就可以看到缓存过期：
- en: '[PRE23]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The results show that our cache is working as intended and able to serialize
    and deserialize between JSON, dictionaries and the Redis key-value store and expire
    results.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，我们的缓存按预期工作，能够将数据在 JSON、字典和 Redis 键值存储之间进行序列化和反序列化，并且能够使结果过期。
- en: Compression
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 压缩
- en: 'To make this cache feature complete compared with the original disk cache,
    we need to add one final feature: **compression**. This can be achieved in a similar
    way to the disk cache by serializing the data and then compressing it with `zlib`,
    as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个缓存功能与原始磁盘缓存相比更加完整，我们需要添加一个最终的功能：**压缩**。这可以通过与磁盘缓存类似的方式实现，通过序列化数据，然后使用 `zlib`
    进行压缩，如下所示：
- en: '[PRE24]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Testing the cache
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试缓存
- en: 'The source code for the `RedisCache` class is available at [https://github.com/kjam/wswp/blob/master/code/chp3/rediscache.py](https://github.com/kjam/wswp/blob/master/code/chp3/rediscache.py)
    and, as with `DiskCache`, the cache can be tested with the link crawler in any
    Python interpreter. Here, we use IPython to employ the `%time` command:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`RedisCache` 类的源代码可在 [https://github.com/kjam/wswp/blob/master/code/chp3/rediscache.py](https://github.com/kjam/wswp/blob/master/code/chp3/rediscache.py)
    找到，并且与 `DiskCache` 类一样，缓存可以通过链接爬虫在任何 Python 解释器中进行测试。在这里，我们使用 IPython 来使用 `%time`
    命令：'
- en: '[PRE25]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The time taken here is about the same as our `DiskCache` for the first iteration.
    However, the speed of Redis is really seen once the cache is loaded, with a more
    than 3X speed increase versus our non-compressed disk cache system. The increased
    readability of our caching code and the ability to scale our Redis cluster to
    a high availability big data solution is just the icing on the cake!
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这里所用的时间与我们的 `DiskCache` 在第一次迭代时大致相同。然而，一旦缓存被加载，Redis 的速度优势就真正显现出来，与我们的非压缩磁盘缓存系统相比，速度提高了3倍以上。我们缓存代码的可读性提高以及将
    Redis 集群扩展到高可用大数据解决方案的能力，简直是锦上添花！
- en: Exploring requests-cache
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 requests-cache
- en: Occasionally, you might want to cache a library that uses `requests` internally
    or maybe you don't want to manage the cache classes and handling yourself. If
    this is the case, `requests-cache` ([https://github.com/reclosedev/requests-cache](https://github.com/reclosedev/requests-cache))
    is a great library that implements a few different backend options for creating
    a cache for the `requests` library. When using `requests-cache`, all `get` requests
    to access a URL via the `requests` library will first check the cache and only
    request the page if it's not found.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，你可能想要缓存一个内部使用`requests`的库，或者你可能不想自己管理缓存类和处理。如果是这种情况，`requests-cache`（[https://github.com/reclosedev/requests-cache](https://github.com/reclosedev/requests-cache)）是一个非常好的库，它实现了为`requests`库创建缓存的一些建后端选项。当使用`requests-cache`时，所有通过`requests`库访问URL的`get`请求将首先检查缓存，只有在未找到时才会请求页面。
- en: '`requests-cache` supports several backends including Redis, MongoDB (a NoSQL
    database), SQLite (a lightweight relational database), and memory (which is not
    persistent, and therefore not recommended). Since we already have Redis set up,
    we can use it as our backend. To get started, we first need to install the library:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`requests-cache`支持包括Redis、MongoDB（一个NoSQL数据库）、SQLite（一个轻量级的关系型数据库）和内存（它不是持久的，因此不建议使用）在内的几个后端。由于我们已经有Redis设置好了，我们可以将其用作后端。要开始，我们首先需要安装这个库：'
- en: '[PRE26]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now we can simply install and test our cache using a few simple commands in
    IPython:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以简单地使用IPython中的几个简单命令来安装和测试我们的缓存：
- en: '[PRE27]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'If we were to use this instead of our own cache class, we would only need to
    instantiate the cache using the `install_cache` command and then every request
    (provided we are utilizing the `requests` library) would be maintained in our
    Redis backend. We can also set expiry using a few simple commands:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用这个代替我们自己的缓存类，我们只需要使用`install_cache`命令实例化缓存，然后每个请求（只要我们正在使用`requests`库）都会在我们的Redis后端中维护。我们还可以使用一些简单的命令设置过期时间：
- en: '[PRE28]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: To test the speed of using `requests-cache` compared to our own implementation,
    we have built a new downloader and link crawler to use. This downloader also implements
    the suggested `requests` hook to allow for throttling, as documented in the `requests-cache`
    User Guide: [https://requests-cache.readthedocs.io/en/latest/user_guide.html](https://requests-cache.readthedocs.io/en/latest/user_guide.html).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试使用`requests-cache`与我们的实现相比的速度，我们构建了一个新的下载器和链接爬虫来使用。这个下载器还实现了在`requests-cache`用户指南中记录的推荐的`requests`钩子，以允许节流：[https://requests-cache.readthedocs.io/en/latest/user_guide.html](https://requests-cache.readthedocs.io/en/latest/user_guide.html)。
- en: 'To see the full code, check out the new downloader ([https://github.com/kjam/wswp/blob/master/code/chp3/downloader_requests_cache.py](https://github.com/kjam/wswp/blob/master/code/chp3/downloader_requests_cache.py))and
    link crawler ([https://github.com/kjam/wswp/blob/master/code/chp3/requests_cache_link_crawler.py)](https://github.com/kjam/wswp/blob/master/code/chp3/requests_cache_link_crawler.py).
    We can test them using IPython to compare the performance:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看完整的代码，请查看新的下载器（[https://github.com/kjam/wswp/blob/master/code/chp3/downloader_requests_cache.py](https://github.com/kjam/wswp/blob/master/code/chp3/downloader_requests_cache.py)）和链接爬虫（[https://github.com/kjam/wswp/blob/master/code/chp3/requests_cache_link_crawler.py](https://github.com/kjam/wswp/blob/master/code/chp3/requests_cache_link_crawler.py)）。我们可以使用IPython来测试它们的性能：
- en: '[PRE29]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We see the `requests-cache` solution is slightly less performant from our own
    Redis solution, but it also took fewer lines of code and was still quite fast
    (and still much faster than our DiskCache solution). Especially if you are using
    another library where `requests` might be managed internally, the `requests-cache`
    implementation is a great tool to have.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现`requests-cache`解决方案在性能上略逊于我们自己的Redis解决方案，但它也使用了更少的代码行数，并且仍然相当快速（而且比我们的DiskCache解决方案快得多）。特别是如果你正在使用另一个可能内部管理`requests`的库，那么`requests-cache`实现是一个非常有用的工具。
- en: Summary
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned that caching downloaded web pages will save time
    and minimize bandwidth when recrawling a website. However, caching pages takes
    up disk space, some of which can be alleviated through compression. Additionally,
    building on top of an existing storage system, such as Redis, can be useful to
    avoid speed, memory, and filesystem limitations.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解到缓存下载的网页可以在重新爬取网站时节省时间和最小化带宽。然而，缓存页面会占用磁盘空间，其中一些可以通过压缩来缓解。此外，基于现有的存储系统，如Redis，可以用来避免速度、内存和文件系统限制。
- en: In the next chapter, we will add further functionalities to our crawler so we
    can download web pages concurrently and crawl the web even faster.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将为我们的爬虫添加更多功能，以便我们可以并发下载网页，从而使网络爬取速度更快。
