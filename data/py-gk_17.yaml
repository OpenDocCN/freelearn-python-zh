- en: '*Chapter 13*: Python and Machine Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第13章*：Python与机器学习'
- en: '**Machine learning** (**ML**) is a branch of **artificial intelligence** (**AI**)
    that is based on building models by learning patterns from data and then using
    those models to make predictions. It is one of the most popular AI techniques
    for helping humans as well as businesses in many ways. For example, it is being
    used for medical diagnosis, image processing, speech recognition, predicting threats,
    data mining, classification, and many more scenarios. We all understand the importance
    and usefulness of machine learning in our lives. Python, being a concise but powerful
    language, is being used extensively to implement machine learning models. Python''s
    ability to process and prepare data using libraries such as NumPy, pandas, and
    PySpark makes it a preferred choice for developers for building and training ML
    models.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）是**人工智能**（**AI**）的一个分支，它基于从数据中学习模式并构建模型，然后使用这些模型进行预测。它是帮助人类以及企业在许多方面最受欢迎的人工智能技术之一。例如，它被用于医疗诊断、图像处理、语音识别、预测威胁、数据挖掘、分类以及许多其他场景。我们都理解机器学习在我们生活中的重要性和实用性。Python作为一种简洁但强大的语言，被广泛用于实现机器学习模型。Python使用NumPy、pandas和PySpark等库处理和准备数据的能力，使其成为开发人员构建和训练ML模型的首选选择。'
- en: In this chapter, we will discuss using Python for machine learning tasks in
    an optimized way. This is especially important because training an ML model is
    a compute-intensive task and optimizing the code is fundamental when using Python
    for machine learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以优化方式使用Python进行机器学习任务。这尤其重要，因为训练ML模型是一个计算密集型任务，当使用Python进行机器学习时，优化代码是基础。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introducing machine learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍机器学习
- en: Using Python for machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python进行机器学习
- en: Testing and evaluating machine learning models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试和评估机器学习模型
- en: Deploying machine learning models in the cloud
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在云中部署机器学习模型
- en: After completing this chapter, you will understand how to use Python to build,
    train, and evaluate machine learning models and how to deploy them in the cloud
    and use them to make predictions.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，你将了解如何使用Python构建、训练和评估机器学习模型，以及如何在云中部署它们并使用它们进行预测。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following are the technical requirements for this chapter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的技术要求如下：
- en: You need to have Python 3.7 or later installed on your computer.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要在你的计算机上安装Python 3.7或更高版本。
- en: You need to install additional libraries for machine learning such as SciPy,
    NumPy, pandas, and scikit-learn.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要安装额外的机器学习库，如SciPy、NumPy、pandas和scikit-learn。
- en: To deploy an ML model on GCP's AI Platform, you will need a GCP account (a free
    trial will work fine).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要在GCP的AI平台上部署机器学习模型，你需要一个GCP账户（免费试用版也可以）。
- en: The sample code for this chapter can be found at [https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter13](https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter13).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例代码可以在[https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter13](https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter13)找到。
- en: We will start our discussion with an introduction to machine learning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个机器学习的介绍开始我们的讨论。
- en: Introducing machine learning
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍机器学习
- en: 'In traditional programming, we provide data and some rules as input to our
    program to get the desired output. Machine learning is a fundamentally different
    programming approach, in which the data and the expected output are provided as
    input to produce a set of rules. This is called a **model** in machine learning
    nomenclature. This concept is illustrated in the following diagram:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的编程中，我们向程序提供数据和一些规则作为输入，以获得期望的输出。机器学习是一种根本不同的编程方法，其中数据和期望的输出被作为输入来生成一组规则。这在机器学习的术语中被称为**模型**。这一概念在以下图中得到了说明：
- en: '![Figure 13.1 – Traditional programming versus machine learning programming'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '![图13.1 – 传统编程与机器学习编程'
- en: '](img/B17189_13_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17189_13_01.jpg)'
- en: Figure 13.1 – Traditional programming versus machine learning programming
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1 – 传统编程与机器学习编程的比较
- en: 'To understand how machine learning works, we need to familiarize ourselves
    with its core components or elements:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解机器学习是如何工作的，我们需要熟悉其核心组件或元素：
- en: '**Dataset**: Without a good set of data, machine learning is nothing. Good
    data is the real power of machine learning. It has to be collected from different
    environments and cover various situations to represent a model close to a real-world
    process or system. Another requirement for data is that it has to be large, and
    by large we mean thousands of records. Moreover, the data should be as accurate
    as possible and have meaningful information in it. Data is used to train the system
    and also to evaluate its accuracy. We can collect data from many sources but most
    of the time, it is in a raw format. We can use data processing techniques by utilizing
    libraries such as pandas, as we discussed in the previous chapters.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集**：没有好的数据集，机器学习就毫无意义。好的数据是机器学习的真正力量。它必须从不同的环境中收集，涵盖各种情况，以使模型接近现实世界的过程或系统。数据集的另一个要求是它必须很大，而我们所说的“大”是指数千条记录。此外，数据应该尽可能准确，并包含有意义的信息。数据用于训练系统，也用于评估其准确性。我们可以从许多来源收集数据，但大多数情况下，数据是以原始格式存在的。我们可以利用如pandas等库来使用数据处理技术，正如我们在前面的章节中讨论的那样。'
- en: '**Feature extraction**: Before using any data to build a model, we need to
    understand what type of data we have and how it is structured. Once we have understood
    that, we can select what features of the data can be used by an ML algorithm to
    build a model. We can also compute additional features based on the original feature
    set. For example, if we have raw image data in the form of pixels, which itself
    may not be useful for training a model, we can use the length or breadth of the
    shape inside an image as features to build rules for our model.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取**：在使用任何数据构建模型之前，我们需要了解我们有什么类型的数据以及它的结构。一旦我们理解了这一点，我们就可以选择哪些数据特征可以被机器学习算法用来构建模型。我们还可以根据原始特征集计算额外的特征。例如，如果我们有以像素形式存在的原始图像数据，它本身可能对训练模型没有用，我们可以使用图像内部形状的长度或宽度作为特征来为我们的模型建立规则。'
- en: '**Algorithm**: This is a program that is used to build an ML model from the
    available data. In mathematical terms, a machine learning algorithm tries to learn
    a target function *f(X)* that can map the input data, *X*, to an output, *y*,
    like so:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算法**：这是一个用于从可用数据构建机器学习模型的程序。从数学的角度来看，机器学习算法试图学习一个目标函数 *f(X)*，该函数可以将输入数据 *X*
    映射到输出 *y*，如下所示：'
- en: '![](img/B17189_13_001.png)'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](img/B17189_13_001.png)'
- en: There are several algorithms available for different types of problems and situations
    because there is not a single algorithm that can solve every problem. A few popular
    algorithms are **linear regression**, **classification and regression trees**,
    and **support vector classifier** (**SVC**). The mathematical details of how these
    algorithms work are beyond the scope of this book. We recommend checking the additional
    links provided in the *Further reading* section for details regarding these algorithms.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于没有一种算法可以解决所有问题，因此针对不同类型的问题和情况有几种不同的算法可用。一些流行的算法包括**线性回归**、**分类和回归树**以及**支持向量分类器**（**SVC**）。这些算法如何工作的数学细节超出了本书的范围。我们建议查看*进一步阅读*部分提供的附加链接，以获取有关这些算法的详细信息。
- en: '**Models**: Often, we hear the term model in machine learning. A model is a
    mathematical or computational representation of a process that is happening in
    our day-to-day life. From a machine learning perspective, it is the output of
    a machine learning algorithm when we apply it to our dataset. This output (model)
    can be a set of rules or some specific data structure that can be used to make
    predictions when used for any real-world data.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：在机器学习中，我们经常听到“模型”这个术语。模型是对我们日常生活中发生的过程的数学或计算表示。从机器学习的角度来看，它是将机器学习算法应用于我们的数据集时的输出。这个输出（模型）可以是一组规则或一些特定的数据结构，当用于任何实际数据时，可以用来进行预测。'
- en: '**Training**: This is not a new component or step in machine learning. When
    we say training a model, this means applying an ML algorithm to a dataset to produce
    an ML model. The model we get as output is said to be trained on a certain dataset.
    There are three different ways to train a model:'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练**：这不是机器学习中的一个新组件或步骤。当我们说训练一个模型时，这意味着将机器学习算法应用于数据集以产生一个机器学习模型。我们得到的输出模型被称为在某个数据集上训练过的。训练模型有三种不同的方法：'
- en: 'a) **Supervised learning**: This includes providing the desired output, along
    with our data records. The goal here is to learn how the input (X) can be mapped
    to the output (Y) using the available data. This approach of learning is used
    for classification and regression problems. Image classification and predicting
    house prices (regression) are a couple of real-world examples of supervised learning.
    In the case of image processing, we can train a model to identify the type of
    animal in an image, such as a cat or a dog, based on the shape, length, and breadth
    of the image. To train our image classification model, we will label each image
    in the training dataset with the animal''s name. To predict house pricing, we
    must provide data about the houses in the location we are looking at, such as
    the area they''re in, the number of rooms and bathrooms, and so on.'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'a) **监督学习**: 这包括提供所需输出以及我们的数据记录。这里的目的是学习如何使用可用数据将输入（X）映射到输出（Y）。这种学习方法用于分类和回归问题。图像分类和预测房价（回归）是监督学习的几个现实世界例子。在图像处理的情况下，我们可以训练一个模型来识别图像中的动物类型，例如猫或狗，基于图像的形状、长度和宽度。为了训练我们的图像分类模型，我们将训练数据集中的每个图像用动物的名字标记。为了预测房价，我们必须提供关于我们关注的地区房屋的数据，例如它们所在的区域、房间数量和浴室数量等。'
- en: 'b) **Unsupervised learning**: In this case, we train a model without knowing
    the desired output. Unsupervised learning is typically applied to clustering and
    association use cases. This type of learning is mainly based on observations and
    finding groups or clusters of data points so that the data points in a group or
    cluster have similar characteristics. This type of learning approach is extensively
    used by online retail stores such as Amazon to find different groups of customers
    (clustering) based on their shopping behavior and offer them items they''re interested
    in. Online stores also try to find an association between different purchases,
    such as how likely that a person buying item A will want to buy item B as well.'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'b) **无监督学习**: 在这种情况下，我们在不知道所需输出的情况下训练模型。无监督学习通常应用于聚类和关联用例。这种类型的学习主要基于观察，并找到具有相似特征的数据点的组或聚类。这种学习方法在在线零售店如亚马逊中得到广泛应用，以根据他们的购物行为找到不同的客户组（聚类），并向他们提供他们感兴趣的商品。在线商店还试图找到不同购买之间的关联，例如，购买物品A的人有多大的可能性也会购买物品B。'
- en: 'c) **Reinforcement learning**: In the case of reinforcement learning, the model
    is rewarded for making an appropriate decision in a particular situation. In this
    case, no training data is available at all, but the model has to learn from experience.
    Autonomous cars are a popular example of reinforcement learning.'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'c) **强化学习**: 在强化学习的情况下，模型在特定情况下做出适当决策时会得到奖励。在这种情况下，根本不存在训练数据，但模型必须从经验中学习。自动驾驶汽车是强化学习的流行例子。'
- en: '**Testing**: We need to test our model on a dataset that is not used to train
    the model. A traditional approach is to train our model using two-thirds of the
    dataset and test the model using the remaining one-third.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试**: 我们需要在没有用于训练模型的数据库集上测试我们的模型。一种传统的方法是使用数据集的三分之二来训练模型，并使用剩余的三分之一来测试模型。'
- en: In addition to the three learning approaches we discussed, we also have deep
    learning. This is an advanced type of machine learning based on the approach of
    how the human brain achieves a certain type of knowledge using neural network
    algorithms. In this chapter, we will use supervised learning to build our sample
    models.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们讨论的三个学习方法之外，我们还有深度学习。这是一种基于人类大脑如何使用神经网络算法获得某种类型知识的先进机器学习方法。在本章中，我们将使用监督学习来构建我们的示例模型。
- en: In the next section, we will explore the options that are available in Python
    for machine learning.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨Python机器学习中可用的选项。
- en: Using Python for machine learning
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Python进行机器学习
- en: Python is a popular language in the data scientist community because of its
    simplicity, cross-platform compatibilities, and rich support for data analysis
    and data processing through its libraries. One of the key steps in machine learning
    is preparing data for building the ML models, and Python is a natural winner in
    doing this. The only challenge in using Python is that it is an interpreted language,
    so the speed of executing code is slow in comparison to languages such as C. But
    this is not a major issue as there are libraries available to maximize Python's
    speed by using multiple cores of **central processing units** (**CPUs**) or **graphics
    processing units** (**GPUs**) in parallel.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Python在数据科学家社区中很受欢迎，因为它简单、跨平台兼容性良好，并且通过其库提供了丰富的数据分析和数据处理支持。机器学习中的一个关键步骤是为构建机器学习模型准备数据，Python在这方面是一个自然的选择。使用Python的唯一挑战是它是一种解释型语言，因此与C语言等语言相比，执行代码的速度较慢。但这个问题并不严重，因为有一些库可以通过并行使用**中央处理器**（**CPUs**）或**图形处理器**（**GPUs**）的多个核心来最大化Python的速度。
- en: In the next subsection, we will introduce a few Python libraries for machine
    learning.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个子节中，我们将介绍一些用于机器学习的Python库。
- en: Introducing machine learning libraries in Python
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍Python中的机器学习库
- en: Python comes with several machine learning libraries. We already mentioned supporting
    libraries such as NumPy, SciPy, and pandas, which are fundamental for data refinement,
    data analysis, and data manipulation. In this section, we will briefly discuss
    the most popular Python libraries for building machine learning models.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Python附带了几种机器学习库。我们已提到支持库，如NumPy、SciPy和pandas，这些库对于数据精炼、数据分析和数据处理是基础性的。在本节中，我们将简要讨论构建机器学习模型最流行的Python库。
- en: scikit-learn
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: scikit-learn
- en: This library is a popular choice because it has a large variety of built-in
    ML algorithms and tools to evaluate the performance of those ML algorithms. These
    algorithms include classification and regression algorithms for supervised learning
    and clustering and association algorithms for unsupervised learning. scikit-learn
    is mostly written in Python and relies on the NumPy library for many operations.
    For beginners, we recommend starting with the scikit-learn library and then moving
    to the next level of libraries, such as TensorFlow. We will use scikit-learn to
    illustrate the concepts of building, training, and evaluating the ML models.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库是一个流行的选择，因为它拥有大量内置的机器学习算法和评估这些算法性能的工具。这些算法包括用于监督学习的分类和回归算法，以及用于无监督学习的聚类和关联算法。scikit-learn主要用Python编写，并依赖于NumPy库进行许多操作。对于初学者，我们建议从scikit-learn库开始，然后过渡到更高层次的库，例如TensorFlow。我们将使用scikit-learn来展示构建、训练和评估机器学习模型的概念。
- en: scikit-learn also offers **gradient boost algorithms**. These algorithms are
    based on the mathematical concept of **Gradient**, which is a slope of a function.
    It measures the change in an error in the ML context. The idea of gradient-based
    algorithms is to fine-tune the parameters iteratively to find the local minimum
    of a function (minimizing errors for the ML models). Gradient boost algorithms
    use the same strategy to improve a model iteratively by taking into account the
    performance of the previous model, by fine-tuning the parameters for the new model,
    and by setting the target to accept the new model if it minimizes the errors more
    than the previous model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn还提供了**梯度提升算法**。这些算法基于**梯度**的数学概念，即函数的斜率。在机器学习语境中，它衡量错误的变化。基于梯度的算法的思路是通过迭代地微调参数来找到函数的局部最小值（最小化机器学习模型的错误）。梯度提升算法使用相同的策略通过考虑先前模型的性能，通过微调新模型的参数，并通过设置目标以接受新模型（如果它比先前模型更少地最小化错误）来迭代地改进模型。
- en: XGBoost
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XGBoost
- en: XGBoost, or **eXtreme Gradient Boosting**, is a library of algorithms that relies
    on gradient boosted decision trees. This library is popular as it is extremely
    fast and offers the best performance compared to other implementations of gradient
    boosting algorithms, as well as traditional machine learning algorithms. scikit-learn
    also offers gradient boost algorithms, which are fundamentally the same as XGBoost,
    though XGBoost is significantly fast. The main reason is the maximal utilization
    of parallelism across different cores of a single machine or in a distributed
    cluster of nodes. XGBoost can also regularize the decision trees to avoid overfitting
    the model to the data. XGBoost is not a full framework for machine learning but
    offers mainly algorithms (models). To use XGBoost, we must use scikit-learn for
    the rest of the utility functions and tools, such as data analysis and data preparation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost，或**极端梯度提升**，是一个基于梯度提升决策树的算法库。这个库因其极端快速和与其他梯度提升算法实现以及传统机器学习算法相比的最佳性能而受到欢迎。scikit-learn也提供了梯度提升算法，其本质与XGBoost相同，尽管XGBoost速度更快。主要原因是充分利用单台机器的不同核心或分布式节点集群的并行性。XGBoost还可以正则化决策树，以避免模型过度拟合数据。XGBoost不是一个完整的机器学习框架，但主要提供算法（模型）。要使用XGBoost，我们必须使用scikit-learn来处理其余的实用函数和工具，例如数据分析和数据准备。
- en: TensorFlow
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow
- en: TensorFlow is another very popular open source library for machine learning,
    developed by the Google Brain team for high-performance computation. TensorFlow
    is particularly useful for training and running deep neural networks and is a
    popular choice in the area of deep learning.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow是另一个非常流行的开源机器学习库，由Google Brain团队开发，用于高性能计算。TensorFlow特别适用于训练和运行深度神经网络，是深度学习领域的热门选择。
- en: Keras
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Keras
- en: This is an open source API for deep learning for neural networks in Python.
    Keras is more of a high-level API on top of TensorFlow. For developers, using
    Keras is more convenient than using TensorFlow directly, so it is recommended
    to use Keras if you are starting to develop deep learning models with Python.
    Keras can work with both CPUs and GPUs.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个用于Python中神经网络深度学习的开源API。Keras更像是TensorFlow之上的高级API。对于开发者来说，使用Keras比直接使用TensorFlow更方便，因此如果你是刚开始用Python开发深度学习模型，建议使用Keras。Keras可以与CPU和GPU一起工作。
- en: PyTorch
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch
- en: PyTorch is another open source machine learning library that is a Python implementation
    of the popular **Torch** library in C.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch是另一个开源机器学习库，它是C语言中流行的**Torch**库的Python实现。
- en: In the next section, we will briefly discuss the best practices for using Python
    for machine learning.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将简要讨论使用Python进行机器学习的最佳实践。
- en: Best practices of training data with Python
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Python进行训练数据的最佳实践
- en: 'We have already highlighted how important the data is when training a machine
    learning model. In this section, we will highlight a few best practices and recommendations
    when preparing and using data to train your ML model. These are as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经强调了在训练机器学习模型时数据的重要性。在本节中，我们将强调在准备和使用数据来训练您的机器学习模型时的几个最佳实践和建议。具体如下：
- en: As we mentioned previously, collecting a large set of data is of key importance
    (a few thousand data records or at least many hundreds). The bigger the size of
    the data, the more accurate the ML model will be.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，收集大量数据至关重要（几千条数据记录或至少数百条）。数据量越大，机器学习模型将越准确。
- en: Clean and refine your data before starting any training. This means that there
    should not be any missing fields or misleading fields in the data. Python libraries
    such as pandas are very handy for such tasks.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在开始任何训练之前，请清理和精炼您的数据。这意味着数据中不应有任何缺失字段或误导性字段。pandas等Python库在执行此类任务时非常方便。
- en: Using a dataset without compromising the privacy and security of data is important.
    You need to make sure you are not using the data of some other organization without
    the appropriate approval.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不损害数据隐私和安全性的情况下使用数据集很重要。您需要确保您没有未经适当批准使用其他组织的某些数据。
- en: GPUs work well with data-intensive applications. We encourage you to use GPUs
    to train your algorithms for faster results. Libraries such as XGBoost, TensorFlow,
    and Keras are well known for using GPUs for training purposes.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU与数据密集型应用程序配合良好。我们鼓励您使用GPU来训练算法，以获得更快的成果。XGBoost、TensorFlow和Keras等库因使用GPU进行训练而闻名。
- en: When dealing with a large set of data for training, it is important to utilize
    the system memory efficiently. We should be loading the data in memory in chunks,
    or utilizing distributed clusters to process the data. We encourage you to use
    the generator function as much as you can.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当处理大量训练数据时，有效地利用系统内存非常重要。我们应该分块加载数据到内存中，或者利用分布式集群来处理数据。我们鼓励您尽可能多地使用生成器函数。
- en: It is also a good practice to watch your memory usage during data-intensive
    tasks (for example, while training a model) and free up memory periodically by
    forcing garbage collection to release unreferenced objects.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在处理数据密集型任务（例如，在训练模型时）期间，监控内存使用情况也是一个好习惯。通过强制垃圾回收释放未引用的对象，定期释放内存。
- en: Now that we've covered the available Python libraries and the best practices
    of using Python for machine learning, it is time to start working with real code
    examples.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了可用的Python库以及使用Python进行机器学习的最佳实践，现在是时候开始使用真实的代码示例进行工作了。
- en: Building and evaluating a machine learning model
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建和评估机器学习模型
- en: Before we start writing a Python program, we will evaluate the process of building
    a machine learning model.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始编写Python程序之前，我们将评估构建机器学习模型的过程。
- en: Learning about an ML model building process
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解ML模型构建过程
- en: 'We discussed the different components of machine learning in the *Introducing
    machine learning* section. The machine learning process uses those elements as
    input to train a model. This process follows a procedure with three main phases,
    and each phase has several steps in it. These phases are shown here:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在“介绍机器学习”部分，我们讨论了机器学习的不同组件。机器学习过程使用这些元素作为输入来训练模型。这个过程遵循一个有三个主要阶段的过程，每个阶段都有几个步骤。这些阶段如下所示：
- en: '![Figure 13.2 – Steps of building an ML model using a classic learning approach'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![图13.2 – 使用经典学习方法构建ML模型的步骤'
- en: '](img/B17189_13_02.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17189_13_02.jpg](img/B17189_13_02.jpg)'
- en: Figure 13.2 – Steps of building an ML model using a classic learning approach
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2 – 使用经典学习方法构建ML模型的步骤
- en: 'Each phase, along with detailed steps of it, is described here:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 每个阶段及其详细步骤在这里都有描述：
- en: '**Data analysis**: In this phase, we collect raw data and transform it into
    a form that can be analyzed and then used to train and test a model. We may discard
    some data, such as records with empty values. Through data analysis, we try to
    select the features (attributes) that can be used to identify patterns in our
    data. Extracting features is a very important step, and a lot depends on these
    features when building a successful model. In many cases, we have to fine-tune
    features after the testing phase to make sure we have the right set of features
    for the data. Typically, we partition the data into two sets; one part is used
    to train the model in the modeling phase, while the other part is used to test
    the trained model for accuracy in the testing phase. We can skip the testing phase
    if we are evaluating the model using other approaches, such as **cross-validation**.
    We recommend having a testing phase in your ML building process and keeping some
    data (unseen to the model) aside for the testing phase, as shown in the preceding
    diagram.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分析**：在这个阶段，我们收集原始数据并将其转换为可以分析和用于训练和测试模型的形式。我们可能会丢弃一些数据，例如包含空值的记录。通过数据分析，我们试图选择可用于识别数据中模式的特征（属性）。提取特征是一个非常关键的步骤，在构建成功的模型时，这些特征起着至关重要的作用。在许多情况下，我们必须在测试阶段之后调整特征，以确保我们有适合数据的正确特征集。通常，我们将数据分为两部分；一部分用于建模阶段训练模型，另一部分用于测试阶段测试训练好的模型以验证其准确性。如果我们使用其他方法（如**交叉验证**）评估模型，则可以跳过测试阶段。我们建议在您的ML构建过程中设置一个测试阶段，并保留一些数据（对模型来说是未知的）用于测试阶段，如图中所示。'
- en: '**Modeling**: This phase is about training our model based on the training
    data and features we extracted in the previous phase. In a traditional ML approach,
    we can use the training data as-is to train our model. But to ensure our model
    has better accuracy, we can use the following additional techniques:'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**建模**：这个阶段是关于根据我们在前一阶段提取的训练数据和特征来训练我们的模型。在传统的机器学习方法中，我们可以直接使用训练数据来训练我们的模型。但为了确保我们的模型有更好的准确性，我们可以使用以下附加技术：'
- en: a) We can partition our training data into slices and use one slice for evaluating
    of our model and use the remaining slices for training the model. We repeat this
    for a different combination of training slices and the evaluation slice. This
    evaluation approach is called cross-validation.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 我们可以将我们的训练数据分割成块，并使用其中一个块来评估我们的模型，其余的块用于训练模型。我们重复这个过程，使用不同的训练块和评估块的组合。这种评估方法被称为交叉验证。
- en: b) ML algorithms come with several parameters that can be used to fine-tune
    the model to best fit the data. Fine-tuning these parameters, also known as **hyperparameters**,
    is typically done along with cross-validation during the modeling phase.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 机器学习算法带有几个参数，可以用来微调模型以最佳地拟合数据。在建模阶段，通常与交叉验证一起进行这些参数（也称为**超参数**）的微调。
- en: The feature values in data may use different scales of measurement, which makes
    it difficult to build rules with a combination of such features. In such cases,
    we can transform the data (feature values) into a common scale or into a normalized
    scale (say 0 to 1). This step is called scaling the data, or normalization. All
    these scaling and evaluation steps (or some of them) can be added to a pipeline
    (such as an Apache Beam pipeline) and can be executed together to evaluate different
    combinations for selecting the best model. The output of this phase is a candidate
    ML model, as shown in the preceding diagram.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集中的特征值可能使用不同的测量尺度，这使得使用这些特征的组合来构建规则变得困难。在这种情况下，我们可以将数据（特征值）转换到公共尺度或归一化尺度（例如0到1）。这一步称为数据缩放或归一化。所有这些缩放和评估步骤（或其中一些）都可以添加到管道（如Apache
    Beam管道）中，并可以一起执行以评估不同的组合，以选择最佳模型。这一阶段的输出是一个候选机器学习模型，如前图所示。
- en: '**Testing**: In the testing phase, we use the data we set aside to test the
    accuracy of the candidate ML model we built in the previous phase. The output
    of this phase can be used to add or remove some features and fine-tune the model
    until we get one with acceptable accuracy.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试**：在测试阶段，我们使用之前阶段预留的数据来测试我们构建的候选机器学习模型的准确性。这一阶段的输出可以用来添加或删除一些特征，并微调模型，直到我们得到一个可接受的准确性。'
- en: Once we are satisfied with the accuracy of our model, we can implement it to
    predict based on the data from the real world.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对模型的准确性满意，我们就可以将其应用于基于现实世界数据的预测。
- en: Building a sample ML model
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建一个示例机器学习模型
- en: In this section, we will build a sample ML model using Python, which will identify
    three types of Iris plants. To build this model, we will use a commonly available
    dataset containing four features (length and width of sepals and petals) and three
    types of Iris plants.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Python构建一个示例机器学习模型，该模型将识别三种类型的Iris植物。为了构建这个模型，我们将使用一个包含四个特征（萼片和花瓣的长度和宽度）和三种Iris植物类型的常用数据集。
- en: 'For this code exercise, we will use the following components:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个代码练习，我们将使用以下组件：
- en: We will use the Iris dataset provided by *UC Irvine Machine Learning Repository*
    (http://archive.ics.uci.edu/ml/). This dataset contains 150 records and three
    expected patterns to identify. This is a refined dataset that comes with the necessary
    features already identified.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用由*UC Irvine机器学习存储库*提供的Iris数据集（http://archive.ics.uci.edu/ml/）。这个数据集包含150条记录和三个预期模式以识别。这是一个经过精炼的数据集，其中已经识别了必要的特征。
- en: 'We will use various Python libraries, as follows:'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用以下各种Python库：
- en: a) The pandas and the matplotlib libraries, for data analysis
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 用于数据分析的pandas和matplotlib库
- en: b) The scikit-learn library, for training and testing our ML model
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 用于训练和测试我们的机器学习模型的scikit-learn库
- en: First, we will write a Python program for analyzing the Iris dataset.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将编写一个Python程序来分析Iris数据集。
- en: Analyzing the Iris dataset
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析Iris数据集
- en: For ease of programming, we downloaded the two files for the Iris dataset (`iris.data`
    and `iris.names`) from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了编程的方便，我们从[https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/)下载了Iris数据集的两个文件（`iris.data`和`iris.names`）。
- en: We can directly access the data file from this repository through Python. But
    in our sample program, we will use a local copy of the files. The scikit-learn
    library also provides several datasets as part of the library and can be used
    directly for evaluation purposes. We decided to use the actual files as this will
    be close to real-world scenarios, where you collect data yourself and then use
    it in your program.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接通过Python从这个仓库访问数据文件。但在我们的示例程序中，我们将使用文件的本地副本。scikit-learn库也提供了几个数据集作为库的一部分，可以直接用于评估目的。我们决定使用实际文件，因为这更接近现实场景，即你自行收集数据，然后在程序中使用它。
- en: 'The Iris data file contains 150 records that are sorted based on the expected
    output. In the data file, the values of four different features are provided.
    These four features are described in the `iris.names` file as `sepal-length`,
    `sepal-width`, `petal-length`, and `petal-width`. The expected output types of
    Iris plant, as per the data file, are `Iris-setosa`, `Iris-versicolor`, and `Iris-virginica`.
    We will load the data into a pandas DataFrame and then analyze it for different
    attributes of interest. Some sample code for analyzing the Iris data is as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Iris数据文件包含150条记录，这些记录是根据预期输出排序的。在数据文件中，提供了四个不同特征的值。这四个特征在`iris.names`文件中描述为`sepal-length`（萼片长度）、`sepal-width`（萼片宽度）、`petal-length`（花瓣长度）和`petal-width`（花瓣宽度）。根据数据文件，Iris植物的预期输出类型是`Iris-setosa`（伊丽莎白）、`Iris-versicolor`（维吉尼亚鸢尾）和`Iris-virginica`（弗吉尼亚鸢尾）。我们将数据加载到pandas
    DataFrame中，然后分析其不同感兴趣属性。以下是一些分析Iris数据的示例代码：
- en: '[PRE0]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the first part of the data analysis, we checked a few metrics about the
    data using the pandas library functions, as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分析的第一部分，我们使用pandas库函数检查了一些关于数据的指标，如下所示：
- en: We used the `shape` method to get the dimension of the DataFrame. This should
    be [150, 5] for the Iris dataset as we have 150 records and five columns (four
    for features and one for the expected output). This step ensures that all the
    data is loaded into our DataFrame correctly.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用了`shape`方法来获取DataFrame的维度。对于Iris数据集，这应该是[150, 5]，因为我们有150条记录和五个列（四个用于特征，一个用于预期输出）。这一步确保所有数据都正确地加载到我们的DataFrame中。
- en: We checked the actual data using the `head` or `tail` method. This is only to
    see the data visually, especially if we have not seen what is inside the data
    file.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`head`或`tail`方法检查了实际数据。这只是为了直观地查看数据，特别是如果我们还没有看到数据文件内部的内容。
- en: 'The `describe` method gave us the different statistical KPIs available for
    the data. The outcome of this method is as follows:'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`describe`方法为我们提供了数据的不同统计KPIs。此方法的结果如下：'
- en: '[PRE1]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: These KPIs can help us select the right algorithm for the dataset.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这些关键绩效指标（KPIs）可以帮助我们选择适合数据集的正确算法。
- en: 'The `groupby` method was used to identify the number of records for each `class`
    (name of the column for the expected output). The output will indicate that there
    are 50 records for each type of Iris plant:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用了`groupby`方法来识别每个`class`（预期输出的列名）的记录数量。输出将表明每种Iris植物类型有50条记录：
- en: '[PRE2]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the second part of the analysis, we tried to use a `describe` method (minimum
    value, first quartile, second quartile (median), third quartile, and the maximum
    value). This plot will tell you if your data is symmetrically distributed or grouped
    in a certain range, or how much of your data is skewed toward one side of the
    distribution. For our Iris dataset, we will get the box plots for our four features
    as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析的第二部分，我们尝试使用`describe`方法（最小值、第一四分位数、第二四分位数（中位数）、第三四分位数和最大值）。这个图将告诉你你的数据是否是对称分布的，或者是否在某个范围内分组，或者你的数据有多大的偏斜倾向分布的一侧。对于我们的Iris数据集，我们将得到以下四个特征的箱线图：
- en: '![Figure 13.3 – Box and whisker plots of Iris dataset features'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '![图13.3 – Iris数据集特征的箱线和须线图'
- en: '](img/B17189_13_03.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17189_13_03.jpg)'
- en: Figure 13.3 – Box and whisker plots of Iris dataset features
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3 – Iris数据集特征的箱线和须线图
- en: 'From these plots, we can see that the **petal-length** and the **petal-width**
    data has the most grouping between the first quartile and the third quartile.
    We can confirm this by analyzing the data distribution by using the histogram
    plots, as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些图中，我们可以看到**花瓣长度**和**花瓣宽度**的数据在第一四分位数和第三四分位数之间有最多的分组。我们可以通过使用直方图分析数据分布来确认这一点，如下所示：
- en: '![Figure 13.4 – Histogram of Iris dataset features'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![图13.4 – Iris数据集特征的直方图'
- en: '](img/B17189_13_04.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17189_13_04.jpg)'
- en: Figure 13.4 – Histogram of Iris dataset features
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4 – Iris数据集特征的直方图
- en: After analyzing the data and selecting the right type of algorithm (model) to
    use, we will move on to the next step, which is training our model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析数据和选择合适的算法（模型）类型之后，我们将继续下一步，即训练我们的模型。
- en: Training and testing a sample ML model
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练和测试一个样本机器学习模型
- en: 'To train and test an ML algorithm (model), we must follow these steps:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练和测试一个机器学习算法（模型），我们必须遵循以下步骤：
- en: 'As a first step, we will split our original dataset into two groups: training
    data and testing data. This approach of splitting the data is called the `train_test_split`
    function to make this split convenient:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为第一步，我们将原始数据集分成两组：训练数据和测试数据。这种数据分割的方法被称为`train_test_split`函数，以便于进行分割：
- en: '[PRE3]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the next step, we will create a model and provide the training data (`X_train`
    and `y_train`) to train this model. The choice of ML algorithm is not that important
    for this exercise. For the Iris dataset, we will use the SVC algorithm with default
    parameters. Some sample Python code is as follows:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一步中，我们将创建一个模型，并提供训练数据（`X_train`和`y_train`）来训练这个模型。对于这个练习来说，机器学习算法的选择并不那么重要。对于Iris数据集，我们将使用默认参数的SVC算法。以下是一些示例Python代码：
- en: '[PRE4]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, the predictions will be evaluated with the expected results, as per
    the test data (`y_test`), using the `accuracy_score` and `classification_report`
    functions of the scikit-learn library, as follows:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将使用测试数据（`y_test`）中的预期结果，通过scikit-learn库的`accuracy_score`和`classification_report`函数来评估预测，如下所示：
- en: '[PRE5]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The console output of this program is as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序的控制台输出如下：
- en: '[PRE6]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The accuracy scope is very high (*0.966*), which indicates that the model can
    predict the Iris plant with nearly 96% accuracy for the testing data. The model
    is doing an excellent job for **Iris-setosa** and **Iris-versicolor** but only
    a decent job (86% precise) in the case of **Iris-virginica**. There are several
    ways to improve the performance of our model, all of which we will discuss in
    the next section.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率范围非常高（*0.966*），这表明模型可以以近96%的准确率预测测试数据中的Iris植物。模型在**Iris-setosa**和**Iris-versicolor**方面表现优秀，但在**Iris-virginica**的情况下仅表现尚可（86%精确）。有几种方法可以提高我们模型的表现，所有这些方法我们将在下一节中讨论。
- en: Evaluating a model using cross-validation and fine tuning hyperparameters
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用交叉验证和微调超参数评估模型
- en: 'For the previous sample model, we kept the training process simple for the
    sake of learning the core steps of building an ML model. For production deployments,
    we cannot rely on a dataset that only contains 150 records. Additionally, we must
    evaluate the model for the best predictions using techniques such as the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于之前的样本模型，我们为了学习构建机器学习模型的核心步骤，保持了训练过程的简单性。对于生产部署，我们不能依赖于只包含150条记录的数据集。此外，我们必须使用以下技术等来评估模型以获得最佳预测：
- en: '**k-fold cross validation**: In the previous model, we shuffled the data before
    splitting it into training and testing datasets using the Holdout method. Due
    to this, the model can give us different results every time we train it, thus
    resulting in an unstable model. It is not trivial to select training data from
    a small dataset that contains 150 records since in our case, that can truly represent
    the data of a real-world system or environment. To make our previous model more
    stable with a small dataset, k-fold cross-validation is the recommended approach.
    This approach is based on dividing our dataset into *k* folds or slices. The idea
    is to use *k-1* slices for training and to use the *kth* slice for evaluating
    or testing. This process is repeated until we use every slice of data for testing
    purposes. This is equivalent to repeating the holdout method *k* times using the
    different slices of data for testing.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**k折交叉验证**：在之前的模型中，我们在使用保留法将数据分割成训练和测试数据集之前对数据进行了洗牌。因此，模型每次训练时都可以给出不同的结果，从而导致模型不稳定。从包含150条记录的小数据集中选择训练数据并不简单，因为在这种情况下，它确实可以代表真实世界系统或环境的数据。为了使我们的先前的模型在小数据集上更加稳定，k折交叉验证是推荐的方法。这种方法基于将我们的数据集分成*k*个折或切片。想法是使用*k-1*个切片进行训练，并使用*kth*个切片进行评估或测试。这个过程会重复进行，直到我们使用每个数据切片进行测试。这相当于重复使用不同的数据切片进行测试的保留法*k*次。'
- en: 'To elaborate further, we must split our whole dataset or training data set
    into five slices, say *k=5*, for 5-fold cross-validation. In the first iteration,
    we can use the first slice (20%) for testing and the remaining four slices (80%)
    for training. In the second iteration, we can use the second slice for testing
    and the remaining four slices for training, and so on. We can evaluate the model
    for all five possible training datasets and select the best model in the end.
    The selection scheme of data for training and testing is shown here:'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了进一步阐述，我们必须将我们的整个数据集或训练数据集分成五个切片，比如说 *k=5*，进行五折交叉验证。在第一次迭代中，我们可以使用第一个切片（20%）进行测试，剩余的四个切片（80%）用于训练。在第二次迭代中，我们可以使用第二个切片进行测试，剩余的四个切片用于训练，依此类推。我们可以评估所有五个可能的训练数据集，并在最后选择最佳模型。数据用于训练和测试的选择方案如下：
- en: '![Figure 13.5 – Cross-validation scheme for five slices of data'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.5 – 五份数据切片的交叉验证方案'
- en: '](img/B17189_13_05.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17189_13_05.jpg)'
- en: Figure 13.5 – Cross-validation scheme for five slices of data
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5 – 五份数据切片的交叉验证方案
- en: The cross-validation accuracy is calculated by taking the average accuracy of
    each model we build in each iteration, as per the value of *k*.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证准确率是通过计算每个迭代中每个模型根据 *k* 值的平均准确率来计算的。
- en: '**Optimizing hyperparameters**: In the previous code example, we used the machine
    learning algorithm with default parameters. Each machine learning algorithm comes
    with many hyperparameters that can be fine-tuned to customize the model, as per
    the dataset. It may be possible for statisticians to set a few parameters manually
    by analyzing the data distribution, but it is tedious to analyze the impact of
    a combination of these parameters. There is a need to evaluate our model by using
    different values of these hyperparameters, which can assist us in selecting the
    best hyperparameter combination in the end. This technique is called fine-tuning
    or optimizing the hyperparameters.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化超参数**：在前面的代码示例中，我们使用了具有默认参数的机器学习算法。每个机器学习算法都带有许多可以微调以定制模型、根据数据集进行调整的超参数。统计学家可能通过分析数据分布手动设置一些参数，但分析这些参数组合的影响是繁琐的。我们需要通过使用这些超参数的不同值来评估我们的模型，这有助于我们在最后选择最佳的超参数组合。这种技术被称为微调或优化超参数。'
- en: 'Cross-validation and fine-tuning hyperparameters are tedious to implement,
    even through programming. The good news is that the scikit-learn library comes
    with tools to achieve these evaluations in a couple of lines of Python code. The
    scikit-learn library offers two types of tools for this evaluation: `GridSearchCV`
    and `RandomizedSearchCV`. We will discuss each of these tools next.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证和微调超参数的实现过程相当繁琐，即使通过编程也是如此。好消息是，scikit-learn 库提供了一些工具，可以在几行 Python 代码中实现这些评估。scikit-learn
    库为此评估提供了两种类型的工具：`GridSearchCV` 和 `RandomizedSearchCV`。我们将在下面讨论这些工具。
- en: GridSearchCV
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GridSearchCV
- en: The `GridSearchCV` tool evaluates any given model by using the cross-validation
    approach for all possible combinations of values provided for the hyperparameters.
    Each combination of values of hyperparameters will be evaluated by using cross-validation
    on dataset slices.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`GridSearchCV` 工具通过交叉验证方法评估给定模型的所有可能超参数值的组合。每个超参数值的组合将通过在数据集切片上使用交叉验证来评估。'
- en: 'In the following code example, we will use the `GridSearchCV` class from the
    scikit-learn library to evaluate the SVC model for a combination of `C` and `gamma`
    parameters. The `C` parameter is a regularization parameter that manages the tradeoff
    between having a low training error versus having a low testing error. A higher
    value of `C` means we can accept a higher number of errors. We will use 0.001,
    0.01, 1, 5, 10, and 100 as values for `C`. The `gamma` parameter is used to define
    the non-linear hyperplanes or non-linear lines for the classification. The higher
    the value of `gamma`, the model can try to fit more data by adding more curvature
    or curve to the hyperplane or the line. We will use values such as 0.001, 0.01,
    1, 5, 10, and 100 for `gamma` as well. The complete code for `GridSearchCV` is
    as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码示例中，我们将使用来自scikit-learn库的`GridSearchCV`类来评估`C`和`gamma`参数组合的SVC模型。`C`参数是一个正则化参数，它管理着低训练错误与低测试错误之间的权衡。`C`的值越高，我们能够接受的错误数量就越多。我们将使用0.001、0.01、1、5、10和100作为`C`的值。`gamma`参数用于定义分类的非线性超平面或非线性线。`gamma`的值越高，模型可以通过添加更多的曲率或曲线到超平面或线来尝试拟合更多的数据。我们也将使用0.001、0.01、1、5、10和100作为`gamma`的值。`GridSearchCV`的完整代码如下：
- en: '[PRE7]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In this code example, the following points need to be highlighted:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码示例中，以下几点需要强调：
- en: We loaded the data directly from the scikit-learn library for illustration purposes.
    You can use the previous code to load the data from a local file as well.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了说明目的，我们直接从scikit-learn库加载数据。您也可以使用前面的代码从本地文件加载数据。
- en: It is important to define the `params` dictionary for fine-tuning hyperparameters
    as a first step. We set the values for the `C` and `gamma` parameters in this
    dictionary.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义`params`字典以微调超参数作为第一步是很重要的。我们在该字典中设置了`C`和`gamma`参数的值。
- en: We set `cv=5`. This will evaluate each parameter combination by using cross-validation
    across the five slices.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将`cv`设置为5。这将通过五个切片进行交叉验证来评估每个参数组合。
- en: 'The output of this program will give us the best combination of `C` and `gamma`
    and the accuracy of the model with cross-validation. The console output for the
    best parameters and the accuracy of the best model is as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序的输出将给我们提供`C`和`gamma`的最佳组合以及带有交叉验证的模型准确率。最佳参数和最佳模型的控制台输出如下：
- en: '[PRE8]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: By evaluating different combinations of parameters and using cross-validation
    with `GridSearchCV`, the overall accuracy of the model is improved to 98% from
    96%, compared to the results we observed without cross-validation and hyperparameter
    fine-tuning. The classification report (not shown in the program output) shows
    that the precision for the three plant types is 100% for our test data. However,
    this tool is not feasible to use when we have a large number of parameter values
    with a large dataset.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 通过评估不同参数组合并使用`GridSearchCV`进行交叉验证，模型的总体准确率从96%提高到了98%，与未进行交叉验证和超参数微调的结果相比。分类报告（未在程序输出中显示）显示，对于三种植物类型，我们的测试数据的精确度为100%。然而，当数据集很大且参数值很多时，这个工具并不实用。
- en: RandomizedSearchCV
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RandomizedSearchCV
- en: In the case of the `RandomizedSearchCV` tool, we only evaluate a model for randomly
    selected hyperparameter values instead of all the different combinations. We can
    provide the parameter values and the number of random iterations to perform as
    input. The `RandomizedSearchCV` tool will randomly select the parameter combination
    as per the number of iterations provided. This tool is useful when we are dealing
    with a large dataset and when many combinations of parameters/values are possible.
    Evaluating all the possible combinations for a large dataset can be a very long
    process that requires a lot of computing resources.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在`RandomizedSearchCV`工具的情况下，我们只评估随机选择的超参数值对应的模型，而不是所有不同的组合。我们可以提供参数值和要执行的随机迭代次数作为输入。`RandomizedSearchCV`工具将根据提供的迭代次数随机选择参数组合。当处理大型数据集且存在许多参数/值组合时，这个工具非常有用。评估大型数据集的所有可能组合可能是一个非常耗时的过程，需要大量的计算资源。
- en: 'The Python code for using `RandomizedSearchCV` is the same as for the `GridSearchCV`
    tool, except for the following additional lines of codes:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`RandomizedSearchCV`的Python代码与`GridSearchCV`工具相同，除了以下额外的代码行：
- en: '[PRE9]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Since we defined `n_iter=5`, `RandomizedSearchCV` will select only five combinations
    of the `C` and `gamma` parameters and evaluate the model accordingly.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们定义了`n_iter=5`，`RandomizedSearchCV`将只选择五个`C`和`gamma`参数的组合，并相应地评估模型。
- en: 'When this part of the program is executed, we will get an output similar to
    the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 当程序的这个部分执行时，我们将得到类似于以下输出的结果：
- en: '[PRE10]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that you may get a different output because this tool may select different
    parameter values for the evaluation. If we increase the number of iterations (`n_iter`)
    for the `RandomizedSearchCV` object, we will observe more accuracy in the output.
    If we do not set `n_iter`, we will run the evaluation for all combinations, which
    means we'll get the same output that `GridSearchCV` provides.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你可能得到不同的输出，因为此工具可能会为评估选择不同的参数值。如果我们增加`RandomizedSearchCV`对象的迭代次数（`n_iter`），我们将观察到输出中更高的准确性。如果我们不设置`n_iter`，我们将对所有组合进行评估，这意味着我们将得到与`GridSearchCV`提供相同的输出。
- en: As we can see, the best parameter combination that's selected by the `GridSearchCV`
    tool is different than the one selected by the `RandomizedSearchCV` tool. This
    is expected because we ran the two tools for a different number of iterations.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，由`GridSearchCV`工具选择的最佳参数组合与由`RandomizedSearchCV`工具选择的组合不同。这是预期的，因为我们为这两个工具运行了不同次数的迭代。
- en: This concludes our discussion on building a sample ML model using the scikit-learn
    library. We covered the core steps and concepts that are required in building
    and evaluating such models. In practice, we also scale the data for normalization.
    This scaling can be achieved either by using built-in scaler classes in the scikit-learn
    library, such as `StandardScaler`, or by building our own scaler class. The scaling
    operation is a data transformation operation and can be combined with the model
    training task under a single pipeline. scikit-learn supports combining multiple
    operations or tasks as a pipeline using the `Pipeline` class. The `Pipeline` class
    can also be used directly with the `RandomizedSearchCV` or `GridSearchCV` tools.
    You can find out more about how to use scalers and pipelines with the scikit-learn
    library by reading the online documentation for the scikit-learn library ([https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html)).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们关于使用scikit-learn库构建示例机器学习模型的讨论。我们涵盖了构建和评估此类模型所需的核心步骤和概念。在实践中，我们还对数据进行缩放以进行归一化。这种缩放可以通过使用scikit-learn库中的内置缩放器类，如`StandardScaler`，或通过构建我们自己的缩放器类来实现。缩放操作是一种数据转换操作，可以在单个管道下与模型训练任务结合。scikit-learn支持使用`Pipeline`类将多个操作或任务作为管道结合。`Pipeline`类也可以直接与`RandomizedSearchCV`或`GridSearchCV`工具一起使用。你可以通过阅读scikit-learn库的在线文档来了解更多关于如何使用scikit-learn库中的缩放器和管道的信息（[https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html))。
- en: In the next section, we will discuss how to save a model to a file and restore
    a model from a file.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论如何将模型保存到文件中，以及如何从文件中恢复模型。
- en: Saving an ML model to a file
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将机器学习模型保存到文件
- en: When we have evaluated a model and selected the best one as per our dataset,
    the next step is to implement this model for future predictions. This model can
    be implemented as part of any Python application, such as web applications, Flask,
    or Django, or it can be used as a microservice or even a cloud function. The real
    question is how to transfer the model object from one program to the other. There
    are a couple of libraries such as `pickle` and `joblib` that can be used to serialize
    a model into a file. The file can then be used in any application to load the
    model again in Python and make predictions using the `predict` method of the model
    object.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们评估了一个模型并选择了一个根据我们的数据集的最佳模型后，下一步就是实现这个模型以用于未来的预测。这个模型可以作为任何Python应用程序的一部分实现，例如Web应用程序、Flask或Django，或者它可以作为一个微服务甚至云函数使用。真正的问题是，如何将模型对象从一个程序转移到另一个程序。有几个库，如`pickle`和`joblib`，可以用来将模型序列化到文件中。然后，该文件可以在任何应用程序中使用，以在Python中再次加载模型并使用模型对象的`predict`方法进行预测。
- en: 'To illustrate this concept, we will save the ML model we created in one of
    the previous code examples (for example, the `model` object in the `iris_build_svm_model.py`
    program) to a file called `model.pkl`. In the next step, we will load the model
    from this file using the pickle library and make a prediction using new data to
    emulate the use of a model in any application. The complete sample code is as
    follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个概念，我们将把我们在之前的代码示例中创建的 ML 模型（例如，`iris_build_svm_model.py` 程序中的 `model`
    对象）保存到一个名为 `model.pkl` 的文件中。在下一步中，我们将使用 pickle 库从这个文件中加载模型，并使用新数据进行预测，以模拟在任何应用程序中使用模型。完整的示例代码如下：
- en: '[PRE11]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The use of the joblib library is simpler than the pickle library but it may
    require you to install this library if it has not been installed as a dependency
    of scikit-learn. The following sample code shows the use of the joblib library
    to save our best model, as per the evaluation of the `GridSearchCV` tool we did
    in the previous section, and then load the model from the file:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 joblib 库比 pickle 库更简单，但如果它尚未作为 scikit-learn 的依赖项安装，则可能需要您安装此库。以下示例代码展示了如何使用
    joblib 库保存我们最佳模型，根据我们在上一节中使用的 `GridSearchCV` 工具进行评估，然后从文件中加载模型：
- en: '[PRE12]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The code for the joblib library is concise and simple. The prediction part of
    the sample code is the same as in the previous code sample for the pickle library.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: joblib 库的代码简洁简单。示例代码中的预测部分与上一节 pickle 库的示例代码相同。
- en: Now that we've learned how the model can be saved in a file, we can take the
    model to any application for deployment and even to a cloud platform, such as
    GCP AI Platform. We will discuss how to deploy our ML model on a GCP platform
    in the next section.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何将模型保存到文件中，我们可以将模型部署到任何应用程序，甚至部署到云平台，如 GCP AI 平台。我们将在下一节中讨论如何在 GCP
    平台上部署我们的 ML 模型。
- en: Deploying and predicting an ML model on GCP Cloud
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 GCP 云上部署和预测 ML 模型
- en: Public cloud providers are offering several AI platforms for training built-in
    models, as well as your custom models, for deploying the models for predictions.
    Google offers the **Vertex AI** platform for ML use cases, whereas Amazon and
    Azure offer the **Amazon SageMaker** and **Azure ML** services, respectively.
    We selected Google because we assume you have set up an account with GCP and that
    you are already familiar with the core concepts of GCP. GCP offers its AI Platform,
    which is part of the Vertex AI Platform, for training and deploying your ML models
    at scale. The GCP AI Platform supports libraries such as scikit-learn, TensorFlow,
    and XGBoost. In this section, we will explore how to deploy our already trained
    model on GCP and then predict the outcome based on that model.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 公共云提供商正在提供多个 AI 平台，用于训练内置模型以及您的自定义模型，以便部署模型进行预测。Google 为 ML 用例提供 **Vertex AI**
    平台，而 Amazon 和 Azure 分别提供 **Amazon SageMaker** 和 **Azure ML** 服务。我们选择 Google，因为我们假设您已经设置了
    GCP 账户，并且您已经熟悉 GCP 的核心概念。GCP 提供其 AI 平台，这是 Vertex AI 平台的一部分，用于大规模训练和部署您的 ML 模型。GCP
    AI 平台支持 scikit-learn、TensorFlow 和 XGBoost 等库。在本节中，我们将探讨如何在 GCP 上部署我们已训练的模型，然后根据该模型预测结果。
- en: Google AI Platform offers its prediction server (compute node) either through
    a global endpoint (`ml.googleapis.com`) or through a regional endpoint (`<region>-ml.googleapis.com`).
    The global API endpoint is recommended for batch predictions, which are available
    for TensorFlow on Google AI Platform. The regional endpoint offers additional
    protection against outages in other regions. We will use a regional endpoint to
    deploy our sample ML model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Google AI 平台通过全局端点（`ml.googleapis.com`）或区域端点（`<region>-ml.googleapis.com`）提供其预测服务器（计算节点）。全局
    API 端点建议用于批量预测，TensorFlow 在 Google AI 平台上提供批量预测。区域端点提供对其他区域故障的额外保护。我们将使用区域端点来部署我们的示例
    ML 模型。
- en: 'Before we start deploying a model in GCP, we will need to have a GCP project.
    We can create a new GCP project or use an existing GCP project that we''ve created
    for previous exercises. The steps of creating a GCP project and associating a
    billing account with it were discussed in [*Chapter 9*](B17189_09_Final_PG_ePub.xhtml#_idTextAnchor247),
    *Python Programming for the Cloud*. Once we have a GCP project ready, we can deploy
    the `model.joblib` model, which we created in the previous section. The steps
    for deploying our model are as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始在 GCP 中部署模型之前，我们需要有一个 GCP 项目。我们可以创建一个新的 GCP 项目或使用我们为之前的练习创建的现有 GCP 项目。创建
    GCP 项目并将其与计费账户关联的步骤在 [*第 9 章*](B17189_09_Final_PG_ePub.xhtml#_idTextAnchor247)，*云端的
    Python 编程* 中讨论过。一旦我们准备好了 GCP 项目，我们就可以部署我们在上一节中创建的 `model.joblib` 模型。部署我们的模型的步骤如下：
- en: 'As a first step, we will create a storage bucket where we will store our model
    file. We can use the following Cloud SDK command to create a new bucket:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为第一步，我们将创建一个存储桶，我们将在这里存储我们的模型文件。我们可以使用以下 Cloud SDK 命令来创建一个新的桶：
- en: '[PRE13]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Once our bucket is ready, we can upload our model file (`model.joblib`) to
    this storage bucket using the following Cloud SDK command:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们的桶准备好，我们可以使用以下 Cloud SDK 命令将我们的模型文件（`model.joblib`）上传到这个存储桶：
- en: '[PRE14]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: gcloud ai-platform models create my_iris_model –  region=us-central1
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: gcloud ai-platform models create my_iris_model –  region=us-central1
- en: '[PRE15]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we can create a version for our model by using the following command:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用以下命令为我们的模型创建一个版本：
- en: '[PRE16]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Using endpoint [https://us-central1-  ml.googleapis.com/]
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用端点 [https://us-central1-  ml.googleapis.com/]
- en: Creating version (this might take a few   minutes)......done.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创建版本（这可能需要几分钟）......完成。
- en: '[PRE17]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To check if our model and version have been deployed correctly, we can use
    the `describe` command under the `versions` context, as shown here:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要检查我们的模型和版本是否已正确部署，我们可以使用 `versions` 上下文下的 `describe` 命令，如下所示：
- en: '[PRE18]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Once our model has been deployed with its version, we can use new data to predict
    the outcome using the model we deployed on Google AI Platform. For testing, we
    added a couple of data records, different from the original dataset, in a JSON
    file (`input.json`), as follows:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们的模型及其版本已经部署，我们可以使用新数据通过我们在 Google AI 平台上部署的模型来预测结果。为了测试，我们在一个 JSON 文件（`input.json`）中添加了几条与原始数据集不同的数据记录，如下所示：
- en: '[PRE19]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can use the following Cloud SDK command to predict the outcome based on
    the records inside the `input.json` file, as follows:'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以使用以下 Cloud SDK 命令根据 `input.json` 文件内的记录预测结果，如下所示：
- en: '[PRE20]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The console output will show the predicted class for each record, as well as
    the following:'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 控制台输出将显示每个记录的预测类别，以及以下内容：
- en: '[PRE21]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: To use the deployed model in our application (local or cloud), we can use *Cloud
    SDK* or *Cloud Shell*, but the recommended approach is to use the Google AI API
    to make any predictions.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 要在我们的应用程序（本地或云）中使用已部署的模型，我们可以使用 *Cloud SDK* 或 *Cloud Shell*，但推荐的方法是使用 Google
    AI API 进行任何预测。
- en: With that, we have covered cloud deployment and the prediction options for our
    ML model using Google AI Platform. However, you can also take your ML model to
    other platforms, such as Amazon SageMaker and Azure ML for deployment and prediction.
    You can find more details about the Amazon SageMaker platform at https://docs.aws.amazon.com/sagemaker/
    and more details about Azure ML at [https://docs.microsoft.com/en-us/azure/machine-learning/](https://docs.microsoft.com/en-us/azure/machine-learning/).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们已经涵盖了使用 Google AI 平台部署我们的机器学习模型以及预测选项。然而，您也可以将您的机器学习模型部署到其他平台，例如 Amazon
    SageMaker 和 Azure ML。您可以在 https://docs.aws.amazon.com/sagemaker/ 找到有关 Amazon SageMaker
    平台的更多详细信息，以及有关 Azure ML 的更多详细信息可以在 [https://docs.microsoft.com/en-us/azure/machine-learning/](https://docs.microsoft.com/en-us/azure/machine-learning/)
    找到。
- en: Summary
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced machine learning and its main components, such
    as datasets, algorithms, and models, as well as training and testing a model.
    This introduction was followed by a discussion of popular machine learning frameworks
    and libraries available for Python. These include scikit-learn, TensorFlow, PyTorch,
    and BGBoost. We also discussed the best practices of refining and managing the
    data for training ML models. To get familiar with the scikit-learn library, we
    built a sample ML model using the SVC algorithm. We trained the model and evaluated
    it using techniques such as k-fold cross-validation and fine-tuning hyperparameters.
    We also learned how to store a trained model in a file and then load that model
    into any program for prediction purposes. In the end, we demonstrated how we can
    deploy our ML model and predict results using the Google AI Platform with a few
    GCP Cloud SDK commands.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了机器学习及其主要组成部分，例如数据集、算法和模型，以及模型的训练和测试。介绍之后，我们讨论了适用于Python的流行机器学习框架和库，包括scikit-learn、TensorFlow、PyTorch和BGBoost。我们还讨论了为训练机器学习模型优化和管理数据的最佳实践。为了熟悉scikit-learn库，我们使用SVC算法构建了一个示例机器学习模型。我们训练了该模型，并使用诸如k折交叉验证和微调超参数等技术对其进行了评估。我们还学习了如何将训练好的模型存储到文件中，然后将其加载到任何程序中进行预测。最后，我们展示了如何使用几个GCP
    Cloud SDK命令在Google AI平台上部署我们的机器学习模型并预测结果。
- en: The concepts and the hands-on exercises included in this chapter are adequate
    to help build the foundation for using Python for machine learning projects. This
    theoretical and hands-on knowledge is beneficial for those who are looking to
    start using Python for machine learning.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中包含的概念和动手练习足以帮助构建使用Python进行机器学习项目的基石。这种理论和实践知识对那些希望开始使用Python进行机器学习的人来说是有益的。
- en: In the next chapter, we will explore how to use Python for network automation.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何使用Python进行网络自动化。
- en: Questions
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What are supervised learning and unsupervised learning?
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是监督学习和无监督学习？
- en: What is k-fold cross-validation and how it is used to evaluate a model?
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是k折交叉验证以及它是如何用于评估模型的？
- en: What is `RandomizedSearchCV` and how it is different from `GridSearchCV`?
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是`RandomizedSearchCV`以及它与`GridSearchCV`有何不同？
- en: What libraries can we use to save a model in a file?
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用哪些库将模型保存到文件中？
- en: Why are regional endpoints preferred option over global endpoints for Google
    AI Platform?
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么区域端点比全局端点是Google AI平台的优选选项？
- en: Further reading
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Machine Learning Algorithms*, by Giuseppe Bonaccorso'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[《机器学习算法》](https://)，作者Giuseppe Bonaccorso'
- en: '*40 Algorithms Every Programmer Should Know*, by Imran Ahmad'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[《40个程序员应该知道的算法》](https://)，作者Imran Ahmad'
- en: '*Mastering Machine Learning with scikit-learn*, by Gavin Hackeling'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[《精通scikit-learn机器学习》](https://)，作者Gavin Hackeling'
- en: '*Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn,
    and TensorFlow 2*, by Sebastian Raschka and Vahid Mirjalili'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[《Python机器学习：使用Python、scikit-learn和TensorFlow 2进行机器学习和深度学习》](https://)，作者Sebastian
    Raschka和Vahid Mirjalili'
- en: '*scikit-learn User Guide*, available at [https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[scikit-learn用户指南](https://scikit-learn.org/stable/user_guide.html)'
- en: '*Google AI Platform Guides* for training and deploying ML models are available
    at [https://cloud.google.com/ai-platform/docs](https://cloud.google.com/ai-platform/docs)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Google AI平台指南](https://cloud.google.com/ai-platform/docs)提供了训练和部署机器学习模型的说明'
- en: Answers
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 答案
- en: In supervised learning, we provide the desired output with the training data.
    The desired output is not included as part of the training data for unsupervised
    learning.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在监督学习中，我们使用训练数据提供所需的输出。在无监督学习中，所需的输出不是训练数据的一部分。
- en: Cross-validation is a statistical technique that's used to measure the performance
    of an ML model. In k-fold cross-validation, we divide the data into *k* folds
    or slices. We train our model using the *k-1* slices of the dataset and test the
    accuracy of the model using the *kth* slice. We repeat this process until each
    *kth* slice is used as testing data. The cross-validation accuracy of the model
    is computed by taking the average of the accuracy of all the models we built through
    *k* iterations.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交叉验证是一种用于衡量机器学习模型性能的统计技术。在k折交叉验证中，我们将数据分为*k*个折叠或切片。我们使用数据集的*k-1*个切片来训练我们的模型，并使用*kth*个切片来测试模型的准确性。我们重复此过程，直到每个*kth*个切片都用作测试数据。模型的交叉验证准确性是通过取所有通过*k*次迭代构建的模型的准确性的平均值来计算的。
- en: '`RandomizedSearchCV` is a tool that''s available with scikit-learn for applying
    cross-validation functionality to an ML model for randomly selected hyperparameters.
    `GridSearchCV` provides similar functionality to `RandomizedSearchCV`, except
    that it validates the model for all the combinations of hyperparameter values
    provided to it as an input.'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`RandomizedSearchCV` 是一个与 scikit-learn 一起提供的工具，用于将交叉验证功能应用于随机选择的超参数的机器学习模型。`GridSearchCV`
    提供与 `RandomizedSearchCV` 相似的功能，但不同之处在于它验证了提供给它的所有超参数值的组合。'
- en: Pickle and Joblib.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pickle 和 Joblib。
- en: Regional endpoints offer additional protection against any outages in other
    regions, and the availability of computing resources is more for regional endpoints
    than global endpoints.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 区域端点提供了对其他区域任何故障的额外保护，并且计算资源对于区域端点比全球端点更为可用。
