- en: '*Chapter 9*: Python Programming for the Cloud'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 9 章*：云编程的 Python'
- en: Cloud computing is a broad term that is used for a wide variety of use cases.
    These use cases include an offering of physical or virtual compute platforms,
    software development platforms, big data processing platforms, storage, network
    functions, software services, and many more. In this chapter, we will explore
    Python for cloud computing from two correlated aspects. First, we will investigate
    the options of using Python for building applications for cloud runtimes. Then,
    we will extend our discussion of data-intensive processing, which we started in
    [*Chapter 8*](B17189_08_Final_PG_ePub.xhtml#_idTextAnchor227), *Scaling Out Python
    using Clusters*, from clusters to cloud environments. The focus of the discussion
    in this chapter will largely center on the three public cloud platforms; that
    is, **Google Cloud Platform** (**GCP**), **Amazon Web Services** (**AWS**), and
    **Microsoft Azure**.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算是一个广泛的概念，用于各种用例。这些用例包括提供物理或虚拟计算平台、软件开发平台、大数据处理平台、存储、网络功能、软件服务等等。在本章中，我们将从两个相关方面探讨云计算中的
    Python。首先，我们将研究使用 Python 为云运行时构建应用程序的选项。然后，我们将从集群扩展到云环境，继续我们关于数据密集型处理的讨论，该讨论始于[*第
    8 章*](B17189_08_Final_PG_ePub.xhtml#_idTextAnchor227)，*使用集群扩展 Python*。本章讨论的重点将主要集中在三个公共云平台；即，**Google
    Cloud Platform**（**GCP**）、**Amazon Web Services**（**AWS**）和**Microsoft Azure**。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Learning about the cloud options for Python applications
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 Python 应用程序的云选项
- en: Building Python web services for cloud deployment
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为云部署构建 Python 网络服务
- en: Using Google Cloud Platform for data processing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Google Cloud Platform 进行数据处理
- en: By the end of this chapter, you will know how to develop and deploy applications
    to a cloud platform and how to use Apache Beam in general and for Google Cloud
    Platform.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将了解如何开发和部署应用程序到云平台，以及如何一般性地使用 Apache Beam，特别是对于 Google Cloud Platform。
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following are the technical requirements for this chapter:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为本章的技术要求：
- en: You need to have Python 3.7 or later installed on your computer.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要在您的计算机上安装 Python 3.7 或更高版本。
- en: You will need a service account for Google Cloud Platform. A free account will
    work fine.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要一个 Google Cloud Platform 的服务帐户。免费帐户即可。
- en: You will need the Google Cloud SDK installed on your computer.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要在您的计算机上安装 Google Cloud SDK。
- en: You will need Apache Beam installed on your computer.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要在您的计算机上安装 Apache Beam。
- en: The sample code for this chapter can be found at [https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter09](https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter09).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例代码可在[https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter09](https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter09)找到。
- en: We will start our discussion by looking at the cloud options that are available
    for developing applications for cloud deployments.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先探讨可用于为云部署开发应用程序的云选项。
- en: Learning about the cloud options for Python applications
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解 Python 应用程序的云选项
- en: Cloud computing is the ultimate frontier for programmers these days. In this
    section, we will investigate how we can develop Python applications using a cloud
    development environment or using a specific **Software Development Kit** (**SDK**)
    for cloud deployment, and then how we can execute the Python code in a cloud environment.
    We will also investigate options regarding data-intensive processing, such as
    Apache Spark on the cloud. We will start with the cloud-based development environments.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算现在是程序员的最终前沿。在本节中，我们将研究如何使用云开发环境或使用特定的**软件开发工具包**（**SDK**）进行云部署来开发 Python
    应用程序，然后如何在云环境中执行 Python 代码。我们还将研究有关数据密集型处理的选择，例如云上的 Apache Spark。我们将从基于云的开发环境开始。
- en: Introducing Python development environments for the cloud
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍云开发环境中的 Python 开发环境
- en: 'When it comes to setting up a Python development environment for one of the
    three main public clouds, two types of models are available:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到为三种主要公共云之一设置 Python 开发环境时，有两种类型的模型可供选择：
- en: Cloud-native **Integrated Development Environment** (**IDE**)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云原生**集成开发环境**（**IDE**）
- en: Locally installed IDE with an integration option for the cloud
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有云集成选项的本地安装 IDE
- en: We will discuss these two modes next.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来讨论这两种模式。
- en: Cloud-native IDE
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 云原生 IDE
- en: 'There are several cloud-native development environments available in general
    that are not specifically attached to the three public cloud providers. These
    include **PythonAnyWhere**, **Repl.it**, **Trinket**, and **Codeanywhere**. Most
    of these cloud environments offer a free license in addition to a paid one. These
    public cloud platforms offer a mixture of tools for development environments,
    as explained here:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，有几个云原生开发环境可供选择，它们并不特定于三个公共云提供商。这些包括**PythonAnyWhere**、**Repl.it**、**Trinket**和**Codeanywhere**。这些云环境大多数除了付费版本外，还提供免费许可证。这些公共云平台提供了开发环境的工具组合，如这里所述：
- en: '**AWS**: This offers a sophisticated cloud IDE in the form of **AWS Cloud9**,
    which can be accessed through a web browser. This cloud IDE has a rich set of
    features for developers and the option of supporting several programming languages,
    including Python. It is important to understand that AWS Cloud9 is offered as
    an application hosted on an Amazon EC2 instance (a virtual machine). There is
    no direct fee for using AWS Cloud9, but there will be a fee for using the underlying
    Amazon EC2 instance and storage space, which is very nominal for limited use.
    The AWS platform also offers tools for building and testing the code for **continuous
    integration** (**CI**) and **continuous delivery** (**CD**) goals.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS**：它提供了一种复杂的云IDE，形式为**AWS Cloud9**，可以通过网页浏览器访问。这个云IDE为开发者提供了一套丰富的功能，并支持多种编程语言，包括Python。重要的是要理解，AWS
    Cloud9是以托管在Amazon EC2实例（虚拟机）上的应用程序的形式提供的。使用AWS Cloud9没有直接费用，但使用底层的Amazon EC2实例和存储空间将产生费用，对于有限的使用来说，这个费用非常微小。AWS平台还提供用于构建和测试代码以实现**持续集成**（**CI**）和**持续交付**（**CD**）目标的工具。'
- en: '**AWS CodeBuild** is another service that''s available that compiles our source
    code, runs tests, and builds software packages for deployment. It is a build server
    similar to Bamboo. **AWS CodeStar** is commonly used with AWS Cloud9 and offers
    a projects-based platform to help develop, build, and deploy software. AWS CodeStar
    offers predefined project templates to define an entire continuous delivery toolchain
    until the code is released.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**AWS CodeBuild**是另一种可用的服务，它编译我们的源代码，运行测试，并为部署构建软件包。它是一个类似于Bamboo的构建服务器。**AWS
    CodeStar**通常与AWS Cloud9一起使用，提供了一个基于项目的平台，以帮助开发、构建和部署软件。AWS CodeStar提供预定义的项目模板，以定义从代码发布到整个持续交付工具链的整个过程。'
- en: '**Microsoft Azure**: This comes with the **Visual Studio** IDE, which is available
    online (cloud-based) if you are part of the Azure DevOps platform. Online access
    to the Visual Studio IDE is based on a paid subscription. Visual Studio IDE is
    well-known for its rich features and capabilities for offering an environment
    for team-level collaboration. Microsoft Azure offers **Azure Pipelines** for building,
    testing, and deploying your code to any platform such as Azure, AWS, and GCP.
    Azure Pipelines support many languages, such as Node.js, Python, Java, PHP, Ruby,
    C/C++, and .NET, and even mobile development toolkits.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Microsoft Azure**：这包括了**Visual Studio**集成开发环境（IDE），如果你是Azure DevOps平台的一部分，它可以在网上（基于云）使用。Visual
    Studio IDE的在线访问基于付费订阅。Visual Studio IDE因其丰富的功能和为团队协作提供的环境而闻名。Microsoft Azure提供**Azure
    Pipelines**用于构建、测试和将代码部署到任何平台，如Azure、AWS和GCP。Azure Pipelines支持多种语言，如Node.js、Python、Java、PHP、Ruby、C/C++和.NET，甚至移动开发工具包。'
- en: '**Google**: Google offers **Cloud Code** to write, test, and deploy the code
    that can be written either through your browser (such as via ASW Cloud9) or using
    a local IDE of your choice. Cloud Code comes with plugins for the most popular
    IDEs, such as IntelliJ IDE, Visual Studio Code, and JetBrains PyCharm. Google
    Cloud Code is available free of charge and is targeted at container runtime environments.
    Like AWS CodeBuild and Azure Pipelines, Google offers an equivalent service that
    is also called **Cloud Build** for the continuous building, testing, and deploying
    of software in multiple environments, such as virtual machines and containers.
    Google also offers Google **Colaboratory** or **Google Colab** that offers Jupyter
    Notebooks remotely. The Google Colab option is popular among data scientists'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google**：Google提供**Cloud Code**，用于编写、测试和部署代码，这些代码可以通过浏览器（如通过ASW Cloud9）或使用你选择的本地IDE编写。Cloud
    Code包含了对最流行的IDE的插件，如IntelliJ IDE、Visual Studio Code和JetBrains PyCharm。Google Cloud
    Code免费提供，针对容器运行时环境。像AWS CodeBuild和Azure Pipelines一样，Google提供了一种等效服务，也称为**Cloud
    Build**，用于在多个环境中持续构建、测试和部署软件，如虚拟机和容器。Google还提供Google **Colaboratory**或**Google
    Colab**，它提供远程的Jupyter Notebooks。Google Colab选项在数据科学家中很受欢迎'
- en: Google Cloud also offers **Tekton** and the **Jenkins** service for building
    CI/CD development and delivery models.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 谷歌云也提供了**Tekton**和**Jenkins**服务，用于构建CI/CD开发和交付模型。
- en: In addition to all these dedicated tools and services, these cloud platforms
    offer online as well as locally installed shell environments. These shell environments
    are also a quick way to manage code in a limited capacity.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 除了所有这些专用工具和服务之外，这些云平台还提供了在线以及本地安装的shell环境。这些shell环境也是以有限容量管理代码的快捷方式。
- en: Next, we will discuss the local IDE options for using Python for the cloud.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论使用Python进行云开发的本地IDE选项。
- en: Local IDE for cloud development
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本地IDE用于云开发
- en: The cloud-native development environment is a great tool for having native integration
    options in the rest of your cloud ecosystem. This makes instantiating on-demand
    resources and then deploying them convenient, and doesn't require any authentication
    tokens. But this comes with some caveats. First, although the tools are mostly
    free, the underlying resources that they are using are not. The second caveat
    is that the offline availability of these cloud-native tools is not seamless.
    Developers like to write code without any online ties so that they can do this
    anywhere, such as on a train or in a park.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 云原生开发环境是一个很好的工具，可以在您的云生态系统中的其他部分实现原生集成选项。这使得即时实例化资源然后部署变得方便，而且不需要任何身份验证令牌。但这也带来了一些注意事项。首先，尽管这些工具大多是免费的，但它们所使用的底层资源并不是。第二个注意事项是，这些云原生工具的离线可用性并不无缝。开发者喜欢在没有在线连接的情况下编写代码，这样他们就可以在任何地方进行，比如在火车上或在公园里。
- en: Due to these caveats, developers like to use local editors or IDEs for developing
    and testing the software before using additional tools to deploy on one of the
    cloud platforms. Microsoft Azure IDEs such as Visual Studio and Visual Studio
    Code are available for local machines. AWS and Google platform offer their own
    SDKs (shell-like environments) and plugins to be integrated with your IDE of choice.
    We will explore these models of development later in this chapter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些注意事项，开发者喜欢在部署到云平台之前，使用本地编辑器或IDE进行软件开发和测试，然后再使用额外的工具。例如，微软Azure的IDE（如Visual
    Studio和Visual Studio Code）适用于本地机器。AWS和谷歌平台提供自己的SDK（类似shell的环境）和插件，以便与您选择的IDE集成。我们将在本章后面探讨这些开发模型。
- en: Next, we will discuss the runtime environments that are available on the public
    clouds.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论公共云上可用的运行时环境。
- en: Introducing cloud runtime options for Python
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍Python的云运行时选项
- en: The simplest way to get a Python runtime environment is to get a Linux virtual
    machine or a container with Python installed. Once we have a virtual machine or
    a container, we can also install the Python version of our choice. For data-intensive
    workloads, the Apache Spark cluster can be set up on the compute nodes of the
    cloud. But this requires us to own all platform-related tasks and maintenance
    in case anything goes wrong. Almost all public cloud platforms offer more elegant
    solutions to simplify developers' and IT administrators' lives. These cloud providers
    offer one or more pre-built runtime environments based on the application types.
    We will discuss a few of the runtime environments that are available from the
    three public cloud providers – Amazon AWS, GCP, and Microsoft Azure.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 获取Python运行时环境的最简单方法是通过获取安装了Python的Linux虚拟机或容器。一旦我们有了虚拟机或容器，我们也可以安装我们选择的Python版本。对于数据密集型工作负载，可以在云的计算节点上设置Apache
    Spark集群。但这要求我们拥有所有平台相关的任务和维护，以防万一出现问题。几乎所有的公共云平台都提供了更优雅的解决方案，以简化开发人员和IT管理员的生活。这些云服务提供商提供基于应用程序类型的一个或多个预构建的运行时环境。我们将讨论来自三个公共云提供商（亚马逊AWS、GCP和微软Azure）的一些可用的运行时环境。
- en: What is a runtime environment?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是运行时环境？
- en: A runtime environment is an execution platform that runs Python code.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时环境是一个运行Python代码的执行平台。
- en: Runtime options offered by Amazon AWS
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 亚马逊AWS提供的运行时选项
- en: 'Amazon AWS offers the following runtime options:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊AWS提供了以下运行时选项：
- en: '**AWS Beanstalk**: This **Platform-as-a-Service** (**PAAS**) offering can be
    used to deploy web applications that have been developed using Java, .NET, PHP,
    Node.js, Python, and many more. This service also offers the option of using Apache,
    Nginx, Passenger, or IIS as a web server. This service provides flexibility in
    managing the underlining infrastructure, which is sometimes required for deploying
    complex applications.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Beanstalk**：这个**平台即服务**（**PaaS**）提供可以用于部署使用Java、.NET、PHP、Node.js、Python等开发的应用程序。这项服务还提供了使用Apache、Nginx、Passenger或IIS作为Web服务器的选项。这项服务在管理底层基础设施方面提供了灵活性，这对于部署复杂应用程序有时是必需的。'
- en: '**AWS App Runner**: This service can be used to run containerized web applications
    and microservices with an API. This service is fully managed, which means you
    have no administrative responsibilities, as well as no access to the underlying
    infrastructure.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS App Runner**：这项服务可用于运行带有API的容器化Web应用程序和微服务。这项服务是完全管理的，这意味着您没有管理责任，也没有访问底层基础设施的权限。'
- en: '**AWS Lambda**: This is a serverless compute runtime that allows you to run
    your code without the worry of managing any underlying servers. This server supports
    multiple languages, including Python. Although Lambda code can be executed directly
    from an application, this is well-suited for cases when we must run a certain
    piece of code in case an event is triggered by the other AWS services.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Lambda**：这是一个无服务器计算运行时，允许您运行代码而无需担心管理任何底层服务器。这个服务器支持多种语言，包括Python。尽管Lambda代码可以直接从应用程序中执行，但这非常适合在触发其他AWS服务的事件时必须运行某些代码的情况。'
- en: '**AWS Batch**: This option is used to run computing jobs in large volumes in
    the form of batches. This is a cloud option from Amazon that''s an alternative
    to the Apache Spark and Hadoop MapReduce cluster options.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Batch**：这个选项用于以批处理的形式运行大量计算作业。这是Amazon提供的一种云选项，是Apache Spark和Hadoop MapReduce集群选项的替代品。'
- en: '**AWS Kinesis**: This service is also for data processing, but for real-time
    streaming data.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Kinesis**：这项服务也是用于数据处理，但针对实时流数据。'
- en: Runtime options offered by GCP
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GCP提供的运行时选项
- en: 'The following are the runtime options that are available from GCP:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从GCP可用的运行时选项：
- en: '**App Engine**: This is a PaaS option from GCP that can be used to develop
    and host web applications at scale. The applications are deployed as containers
    on App Engine, but your source code is packed into a container by the deployment
    tool. This complexity is hidden from developers.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用引擎**：这是GCP提供的一种PaaS选项，可用于大规模开发和托管Web应用程序。应用程序作为容器在应用引擎上部署，但您的源代码由部署工具打包到容器中。这种复杂性对开发者来说是隐藏的。'
- en: '**CloudRun**: This option is used to host any code that has been built as a
    container. The container applications must have HTTP endpoints to be deployed
    on CloudRun. In comparison to App Engine, packaging applications to a container
    is the developer''s responsibility.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CloudRun**：这个选项用于托管已构建为容器的任何代码。容器应用程序必须具有HTTP端点才能在CloudRun上部署。与应用引擎相比，将应用程序打包到容器是开发者的责任。'
- en: '**Cloud Function**: This is an event-driven, serverless, and single-purpose
    solution for hosting lightweight Python code. The hosted code is typically triggered
    by listening to events on other GCP services or via direct HTTP requests. This
    is comparable to the AWS Lambda service.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云函数**：这是一个事件驱动的、无服务器且单一用途的解决方案，用于托管轻量级的Python代码。托管代码通常通过监听其他GCP服务上的事件或通过直接HTTP请求来触发。这类似于AWS
    Lambda服务。'
- en: '**Dataflow**: This is another serverless option but mainly for data processing
    with minimal latency. This simplifies a data scientist''s life by taking away
    the complexity of the underlying processing platform and offering a data pipeline
    model based on Apache Beam.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据流**：这是另一个无服务器选项，但主要用于数据处理，具有最小延迟。通过消除底层处理平台的复杂性，并基于Apache Beam提供数据管道模型，这简化了数据科学家的生活。'
- en: '**Dataproc**: This service offers a computer platform based on Apache Spark,
    Apache Flink, Presto, and many more tools. This platform is suitable for those
    who have data processing jobs with dependencies on a Spark or Hadoop ecosystem.
    This service requires that we manually provision clusters.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据科学**：这项服务提供基于Apache Spark、Apache Flink、Presto等众多工具的计算平台。这个平台适合那些有依赖Spark或Hadoop生态系统的数据处理作业的人。这项服务要求我们手动配置集群。'
- en: Runtime options offered by Microsoft Azure
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微软Azure提供的运行时选项
- en: 'Microsoft Azure offers the following runtime environments:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 微软Azure提供以下运行环境：
- en: '**App Service**: This service is used to build and deploy web apps at scale.
    This web application can be deployed as a container or run on Windows or Linux.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**App Service**：这项服务用于大规模构建和部署Web应用程序。这个Web应用程序可以作为容器部署，或者在Windows或Linux上运行。'
- en: '**Azure Functions**: This is a serverless event-driven runtime environment
    that''s used to execute code based on a certain event or direct request. This
    is comparable to AWS Lambda and GCP CloudRun.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Functions**：这是一个无服务器事件驱动的运行时环境，用于根据特定事件或直接请求执行代码。这与AWS Lambda和GCP CloudRun相当。'
- en: '**Batch**: As its name suggests, this service is used to run cloud-scale jobs
    that require hundreds or thousands of virtual machines.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Batch**：正如其名所示，这项服务用于运行需要数百或数千个虚拟机的云规模作业。'
- en: '**Azure Databricks**: Microsoft has partnered with Databricks to offer this
    Apache Spark-based platform for large-scale data processing.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Databricks**：微软与Databricks合作，提供这个基于Apache Spark的平台，用于大规模数据处理。'
- en: '**Azure Data Factory**: This is a serverless option from Azure that you can
    use to process streaming data and transform the data into meaningful outcomes.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Data Factory**：这是Azure提供的一个无服务器选项，你可以用它来处理流数据并将数据转换为有意义的成果。'
- en: 'As we have seen, the three main cloud providers offer a variety of execution
    environments based on the applications and workloads that are available. The following
    use cases can be deployed on cloud platforms:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，三大云服务提供商提供了基于可用应用程序和工作负载的多种执行环境。以下用例可以在云平台上部署：
- en: Developing web services and web applications
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发网络服务和Web应用程序
- en: Data processing using a cloud runtime
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用云运行时进行数据处理
- en: Microservice-based applications (containers) using Python
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python的基于微服务的应用程序（容器）
- en: Serverless functions or applications for the cloud
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云服务中的无服务器函数或应用程序
- en: We will address the first two use cases in the upcoming sections of this chapter.
    The remaining use cases will be discussed in the upcoming chapters as they require
    more extensive discussion. In the next section, we will start building a web service
    using Python and explore how to deploy it on the GCP App Engine runtime environment.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章接下来的部分中解决前两个用例。剩余的用例将在接下来的章节中讨论，因为它们需要更广泛的讨论。在下一节中，我们将开始使用Python构建一个网络服务，并探讨如何在GCP
    App Engine运行时环境中部署它。
- en: Building Python web services for cloud deployment
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为云部署构建Python网络服务
- en: 'Building an application for cloud deployment is slightly different than doing
    so for a local deployment. There are three key requirements we must consider while
    developing and deploying an application to any cloud. These requirements are as
    follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为云部署构建应用程序与本地部署略有不同。在开发和部署应用程序到任何云平台时，我们必须考虑以下三个关键要求：
- en: '**Web interface**: For most cloud deployments, applications that have a **graphical
    user interface** (**GUI**) or **application programming interface** (**API**)
    are the main candidates. Command-line interface-based applications will not get
    their usability from a cloud environment unless they are deployed in a dedicated
    virtual machine instance, and we can execute them on a VM instance using SSH or
    Telnet. This is why we selected a web interface-based application for our discussion.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Web界面**：对于大多数云部署，具有**图形用户界面**（GUI）或**应用程序编程接口**（API）的应用程序是主要候选者。基于命令行界面的应用程序除非部署在专用的虚拟机实例中，否则在云环境中不会获得其可用性。我们可以使用SSH或Telnet在虚拟机实例上执行它们。这就是为什么我们选择了基于Web界面的应用程序进行讨论。'
- en: '**Environment setup**: All public cloud platforms support multiple languages,
    as well as different versions of a single language. For example, GCP App Engine
    supports Python versions 3.7, 3.8, and 3.9 as of June 2021\. Sometimes, cloud
    services allow you to bring your own version for deployment as well. For web applications,
    it is also important to set an entry point for accessing the code and project-level
    settings. These are typically defined in a single file (a YAML file, in the case
    of the GCP App Engine application).'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境设置**：所有公共云平台都支持多种语言，以及单一语言的多个版本。例如，截至2021年6月，GCP App Engine支持Python 3.7、3.8和3.9版本。有时，云服务还允许你部署自己的版本。对于Web应用程序，设置一个访问代码和项目级设置的入口点也很重要。这些通常定义在一个单独的文件中（在GCP
    App Engine应用程序的情况下是一个YAML文件）。'
- en: '`requirements.txt`) manually or using the `PIP freeze` command. There are other
    elegant ways available to solve this problem as well. One such way is to package
    all third-party libraries with applications into a single file for cloud deployment,
    such as the Java web archive file (`.war` file). Another approach is to bundle
    all the dependencies containing application code and the target execution platform
    into a container and deploy the container directly on a container hosting platform.
    We will explore container-based deployment options in [*Chapter 11*](B17189_11_Final_PG_ePub.xhtml#_idTextAnchor289),
    *Using Python for Microservices Development*.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以手动或使用`PIP freeze`命令来安装`requirements.txt`。还有其他优雅的方法可以解决这个问题，例如，将所有第三方库与应用程序打包成一个文件以进行云部署，例如Java网络归档文件（`.war`文件）。另一种方法是捆绑包含应用程序代码和目标执行平台的所有依赖项到一个容器中，并直接在容器托管平台上部署该容器。我们将在[*第11章*](B17189_11_Final_PG_ePub.xhtml#_idTextAnchor289)中探讨基于容器的部署选项，*使用Python进行微服务开发*。
- en: 'There are at least three options for deploying a Python web service application
    on GCP App Engine, which are as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在GCP App Engine上部署Python网络服务应用程序至少有三种选项，如下所示：
- en: Using the Google Cloud SDK via the CLI interface
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过CLI界面使用Google Cloud SDK
- en: Using the GCP web console (portal) along with Cloud Shell (CLI interface)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GCP网络控制台（门户）以及Cloud Shell（CLI界面）
- en: Using a third-party IDE such as PyCharm
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用第三方IDE，如PyCharm
- en: We will discuss the first option in detail and provide a summary of our experience
    with the other two options.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细讨论第一个选项，并总结我们对其他两个选项的经验。
- en: Important note
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: To deploy a Python application in AWS and Azure, the procedural steps are the
    same in principle, but the details are different, depending on the SDK and API
    support available from each cloud provider.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS和Azure上部署Python应用程序，在原则上步骤相同，但具体细节取决于每个云提供商提供的SDK和API支持。
- en: Using Google Cloud SDK
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Google Cloud SDK
- en: In this section, we will discuss how to use Google Cloud SDK (mainly the CLI
    interface) to create and deploy a sample application. This sample application
    will be deployed on the **Google App Engine** (**GAE**) platform. GAE is a PaaS
    platform and is best suited for deploying web applications using a wide variety
    of programming languages, including Python.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何使用Google Cloud SDK（主要是CLI界面）创建和部署一个示例应用程序。此示例应用程序将部署在**Google App
    Engine**（**GAE**）平台上。GAE是一个PaaS平台，非常适合使用各种编程语言部署网络应用程序，包括Python。
- en: 'To use the Google Cloud SDK for Python application deployment, we must have
    the following prerequisites on our local machine:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Google Cloud SDK进行Python应用程序部署，我们必须在本地机器上满足以下先决条件：
- en: 'Install and initialize the Cloud SDK. Once installed, you can access it via
    the CLI interface and check its version with the following command. Note that
    almost all Cloud SDK commands start with `gcloud`:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装并初始化Cloud SDK。安装后，您可以通过CLI界面访问它，并使用以下命令检查其版本。请注意，几乎所有Cloud SDK命令都以`gcloud`开头：
- en: '[PRE0]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Install the Cloud SDK components to add the App Engine extension for Python
    3\. This can be done by using the following command:'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装Cloud SDK组件以添加Python 3的App Engine扩展。这可以通过以下命令完成：
- en: '[PRE1]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The GCP CloudBuild API must be enabled for the GCP cloud project.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP CloudBuild API必须在GCP云项目中启用。
- en: Cloud billing must be enabled for the GCP cloud project, even if you are using
    a trial account, by associating your GCP billing account with the project.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使您使用的是试用账户，也必须通过将GCP计费账户与项目关联来启用云项目的云计费。
- en: The GCP user privileges to set up a new App Engine application and to enable
    API services should be done at the *Owner* level.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置新的App Engine应用程序和启用API服务的GCP用户权限应在**所有者**级别完成。
- en: Next, we will describe how to set up a GCP cloud project, create a sample web
    service application, and deploy it to the GAE.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将描述如何设置GCP云项目，创建一个示例网络服务应用程序，并将其部署到GAE。
- en: Setting up a GCP cloud project
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置GCP云项目
- en: The concept of a GCP cloud project is the same as we see in most development
    IDEs. A GCP cloud project consists of a set of project-level settings that manage
    how our code interacts with GCP services and tracks the resources in use by the
    project. A GCP project must be associated with a billing account. This is a prerequisite,
    in terms of billing, for tracking how many GCP services and resources are consumed
    on a per-project basis.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: GCP 云项目的概念与我们大多数开发 IDE 中看到的概念相同。一个 GCP 云项目由一组项目级设置组成，这些设置管理我们的代码如何与 GCP 服务交互以及跟踪项目使用的资源。一个
    GCP 项目必须与一个计费账户关联。这是按项目跟踪消耗了多少 GCP 服务和资源的先决条件。
- en: 'Next, we will explain how to set up a project using Cloud SDK:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将解释如何使用 Cloud SDK 设置项目：
- en: 'Log in to the Cloud SDK using the following command. This will take you to
    the web browser so that you can sign in, in case you have not already done so:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令登录到 Cloud SDK。这将带您进入网页浏览器，以便您可以登录，如果您尚未这样做的话：
- en: '[PRE2]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create a new project called `time-wsproj`. The project''s name should be short
    and use only letters and numbers. The use of `-` is allowed for better readability:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `time-wsproj` 的新项目。项目的名称应简短，仅使用字母和数字。为了更好的可读性，允许使用 `-`：
- en: '[PRE3]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Switch your default scope of the Cloud SDK to the newly created project, if
    you haven''t done so already, by using the following command:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您尚未这样做，请使用以下命令将 Cloud SDK 的默认作用域切换到新创建的项目：
- en: '[PRE4]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This will enable Cloud SDK to use this project as a default project for any
    command we push through the Cloud SDK CLI.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将使 Cloud SDK 能够将此项目作为任何通过 Cloud SDK CLI 推送命令的默认项目。
- en: 'Create an App Engine instance under a default project or for any project by
    using the `project` attribute with one of the following commands:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用以下命令之一并带有 `project` 属性，在默认项目或任何项目中创建 App Engine 实例：
- en: '[PRE5]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that this command will reserve cloud resources (mainly compute and storage)
    and will prompt you to select a region and zone to host the resources. You can
    select the region and zone that's closest to you and also more appropriate from
    your audience's point of view.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，此命令将预留云资源（主要是计算和存储），并将提示您选择一个区域和区域来托管资源。您可以选择离您最近且从受众的角度看更合适的区域和区域：
- en: Enable the Cloud Build API service for the current project. As we've discussed
    already, the Google Cloud Build service is used to build the application before
    it's deployed to a Google runtime such as App Engine. The Cloud Build API service
    is easier to enable through the GCP web console as it only takes a few clicks.
    To enable it using Cloud SDK, first, we need to know the exact name of the service.
    We can get the list of available GCP services by using the `gcloud services list`
    command.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为当前项目启用 Cloud Build API 服务。正如我们之前讨论的，Google Cloud Build 服务用于在部署到 Google 运行时（如
    App Engine）之前构建应用程序。通过 GCP 网页控制台启用 Cloud Build API 服务更容易，因为它只需点击几下。要使用 Cloud SDK
    启用它，首先，我们需要知道服务的确切名称。我们可以通过使用 `gcloud services list` 命令来获取可用 GCP 服务的列表。
- en: 'This command will give you a long list of GCP services so that you can look
    for a service related to Cloud Build. You can also use `format`, attributed with
    any command, to beautify the Cloud SDK''s output. To make this even more convenient,
    you can use the Linux `grep` utility (if you are using Linux or macOS) with this
    command to filter the results and then enable the service using the `enable` command:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此命令将为您提供一系列 GCP 服务，以便您可以查找与 Cloud Build 相关的服务。您还可以使用带有任何命令的 `format` 属性来美化 Cloud
    SDK 的输出。为了使这更加方便，您可以使用 Linux 的 `grep` 工具（如果您使用的是 Linux 或 macOS）与该命令一起过滤结果，然后使用
    `enable` 命令启用服务：
- en: '[PRE6]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To enable the Cloud Billing API service for our project, first, we need to
    associate a billing account with our project. Support for billing accounts in
    Cloud SDK hasn''t been achieved with `beta` command presented here:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要为我们的项目启用 Cloud Billing API 服务，首先，我们需要将计费账户与我们的项目关联。Cloud SDK 中尚未通过此处提供的 `beta`
    命令实现计费账户的支持：
- en: '[PRE7]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Enable the Cloud Billing API service for the current project by following the
    same steps we followed for enabling the Cloud Build API. First, we must find the
    name of the API service and then enable it using the following set of Cloud SDK
    commands:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过遵循我们用于启用 Cloud Build API 的相同步骤，为当前项目启用 Cloud Billing API 服务。首先，我们必须找到 API
    服务的名称，然后使用以下 Cloud SDK 命令集启用它：
- en: '[PRE8]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The steps you must follow to set up a cloud project are straightforward for
    an experienced cloud user and will not take more than a few minutes. Once the
    project has been set up, we can get the project configuration details by running
    the following command:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于经验丰富的云用户来说，设置云项目的步骤非常简单，不会超过几分钟。一旦项目设置完成，我们可以通过运行以下命令来获取项目配置详细信息：
- en: '[PRE9]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output of this command will provide the project life cycle''s status, the
    project''s name, the project''s ID, and the project''s number. The following is
    some example output:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令的输出将提供项目生命周期的状态、项目名称、项目 ID 和项目编号。以下是一些示例输出：
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that the project has been set up, we can start developing our Python web
    application. We will do this in the next section.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在项目已经设置好了，我们可以开始开发我们的 Python Web 应用程序。我们将在下一节中这样做。
- en: Building a Python application
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建 Python 应用程序
- en: For cloud deployments, we can build a Python application using an IDE or system
    editor and then emulate the App Engine runtime locally using the Cloud SDK and
    the *app-engine-python component*, which we have installed as a prerequisite.
    As an example, we will build a web service-based application that will provide
    us with the date and time through a REST API. The application can be triggered
    via an API client or using a web browser. We did not enable any authentication
    to keep the deployment simple.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于云部署，我们可以使用 IDE 或系统编辑器构建一个 Python 应用程序，然后使用 Cloud SDK 和 *app-engine-python
    组件* 在本地模拟 App Engine 运行时，这些组件是我们作为先决条件安装的。作为一个例子，我们将构建一个基于 Web 服务的应用程序，通过 REST
    API 提供日期和时间。该应用程序可以通过 API 客户端或使用网页浏览器触发。我们没有启用任何身份验证，以保持部署简单。
- en: To build the Python application, we will set up a Python virtual environment
    using the Python `venv` package. A virtual environment, created using the `venv`
    package, will be used to wrap the Python interpreter, core, and third-party libraries
    and scripts to keep them separate from the system Python environment and other
    Python virtual environments. Creating and managing a virtual environment in Python
    using the `venv` package has been supported in Python since v3.3\. There are other
    tools available for creating virtual environments, such as `virtualenv` and `pipenv`.
    PyPA recommends using `venv` for creating a virtual environment, so we selected
    it for most of the examples presented in this book.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建 Python 应用程序，我们将使用 Python 的 `venv` 包设置 Python 虚拟环境。使用 `venv` 包创建的虚拟环境将用于封装
    Python 解释器、核心和第三方库和脚本，以保持它们与系统 Python 环境和其他 Python 虚拟环境分离。自 Python 3.3 以来，Python
    已经支持使用 `venv` 包创建和管理虚拟环境。还有其他工具可用于创建虚拟环境，例如 `virtualenv` 和 `pipenv`。PyPA 建议使用
    `venv` 创建虚拟环境，因此我们选择了它来展示本书中的大多数示例。
- en: 'As a first step, we will create a web application project directory named `time-wsproj`
    that contains the following files:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们将创建一个名为 `time-wsproj` 的 Web 应用程序项目目录，其中包含以下文件：
- en: '`app.yaml`'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`app.yaml`'
- en: '`main.py`'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`main.py`'
- en: '`requirements.txt`'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`requirements.txt`'
- en: We used the same name for the directory that we used to create the cloud project
    just for convenience, but this is not a requirement. Let's look at these files
    in more detail.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了与创建云项目相同的目录名称，只是为了方便，但这不是必需的。让我们更详细地看看这些文件。
- en: YAML file
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: YAML 文件
- en: 'This file contains the deployment and runtime settings for an App Engine application,
    such as runtime version number. For Python 3, the `app.yaml` file must have at
    least a runtime parameter (`runtime: python38`). Each service in the web application
    can have its own YAML file. For the sake of simplicity, we will use only one YAML
    file. In our case, this YAML file will only contain the runtime attribute. We
    added a few more attributes to the sample YAML file for illustration purposes:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '此文件包含 App Engine 应用程序的部署和运行时设置，例如运行时版本号。对于 Python 3，`app.yaml` 文件必须至少有一个运行时参数（`runtime:
    python38`）。Web 应用程序中的每个服务都可以有自己的 YAML 文件。为了简化，我们将只使用一个 YAML 文件。在我们的情况下，此 YAML
    文件将只包含运行时属性。为了说明目的，我们在示例 YAML 文件中添加了一些其他属性：'
- en: '[PRE11]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: main.py Python file
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '`main.py` Python 文件'
- en: We selected the `Flask` library to build our sample application. `Flask` is
    a well-known library for web development, mainly because of the powerful features
    it offers, along with its ease of use. We will cover Flask in the next chapter
    in detail.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了 `Flask` 库来构建我们的示例应用程序。`Flask` 是一个广为人知的 Web 开发库，主要是因为它提供的强大功能和易用性。我们将在下一章中详细介绍
    Flask。
- en: 'This `main.py` Python module is the entry point of our application. The complete
    code of the application is presented here:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 此 `main.py` Python 模块是应用程序的入口点。应用程序的完整代码在此处展示：
- en: '[PRE12]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This module provides the following key features:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 此模块提供了以下关键功能：
- en: There is a default entrypoint called `app` that's defined in this module. The
    `app` variable is used to redirect the requests that are sent to this module.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在此模块中定义了一个默认的入口点 `app`。`app` 变量用于重定向发送到此模块的请求。
- en: 'Using Flask''s annotation, we have defined handlers for three URLs:'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Flask 的注解，我们已为三个 URL 定义了处理程序：
- en: a) The root `/` URL will trigger a function named `welcome`. The `welcome` function
    returns a greeting message as a string.
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 根 `/` URL 将触发名为 `welcome` 的函数。`welcome` 函数返回一个字符串形式的问候消息。
- en: b) The `/date` URL will trigger the `today` function, which will return today's
    date in JSON format.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `/date` URL 将触发 `today` 函数，该函数将以 JSON 格式返回今天的日期。
- en: c) The `/time` URL will execute the `time` function, which will return the current
    time in JSON format.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) `/time` URL 将执行 `time` 函数，该函数将以 JSON 格式返回当前时间。
- en: At the end of the module, we added a `__main__` function to initiate a local
    web server that comes with Flask for testing purposes.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模块结束时，我们添加了一个 `__main__` 函数来启动 Flask 附带的本地网络服务器，用于测试目的。
- en: Requirements file
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 需求文件
- en: 'This file contains a list of project dependencies for third-party libraries.
    The contents of this file will be used by App Engine to make the required libraries
    available to our application. In our case, we will need the Flask library to build
    our sample web application. The contents of this file for our project are as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件包含项目依赖项列表，用于第三方库。此文件的内容将被 App Engine 用于使所需的库可供我们的应用程序使用。在我们的案例中，我们需要 Flask
    库来构建我们的示例网络应用程序。我们项目的此文件内容如下：
- en: '[PRE13]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Once we have created the project directory and made these files, we must create
    a virtual environment inside or outside the project directory and activate it
    using the source command:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了项目目录并创建了这些文件，我们必须在项目目录内或外创建一个虚拟环境，并使用 `source` 命令激活它：
- en: '[PRE14]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After activating the virtual environment, we must install the necessary dependencies,
    as per the `requirements.txt` file. We will use the `pip` utility from the same
    directory where the `requirements.txt` file resides:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在激活虚拟环境后，我们必须根据 `requirements.txt` 文件安装必要的依赖项。我们将使用位于 `requirements.txt` 文件所在目录的
    `pip` 工具：
- en: '[PRE15]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once the Flask library and its dependencies have been installed, the directory
    structure will look like this in our PyCharm IDE:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Flask 库及其依赖项已安装，在我们的 PyCharm IDE 中的目录结构将如下所示：
- en: '![Figure 9.1 – Directory structure for a sample web application'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.1 – 示例网络应用的目录结构'
- en: '](img/B17189_09_01.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17189_09_01.jpg](img/B17189_09_01.jpg)'
- en: Figure 9.1 – Directory structure for a sample web application
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – 示例网络应用的目录结构
- en: 'Once the project file and dependencies have been set up, we will start the
    web server locally using the following command:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置好项目文件和依赖项，我们将使用以下命令在本地启动网络服务器：
- en: '[PRE16]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The server will start with the following debug messages, which makes it clear
    that this server option is only for testing purposes and not for production environments:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器将以以下调试消息启动，这清楚地表明此服务器选项仅用于测试目的，而不是用于生产环境：
- en: '[PRE17]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Our web service application can be accessed using the following URIs:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下 URI 访问我们的网络服务应用程序：
- en: '`http://localhost:8080/`'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`http://localhost:8080/`'
- en: '`http://localhost:8080/date`'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`http://localhost:8080/date`'
- en: '`http://localhost:8080/time`'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`http://localhost:8080/time`'
- en: 'The response from the web servers for these URIs is shown here:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 URI 的网页服务器的响应如下所示：
- en: '![Figure 9.2 – Response in the web browser from our sample web service application'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.2 – 来自我们的示例网络服务应用程序的网页浏览器中的响应'
- en: '](img/B17189_09_02.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17189_09_02.jpg](img/B17189_09_02.jpg)'
- en: Figure 9.2 – Response in the web browser from our sample web service application
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 来自我们的示例网络服务应用程序的网页浏览器中的响应
- en: The web server will be stopped before we move on to the next phase – that is,
    deploying this application to Google App Engine.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入下一阶段——即将此应用程序部署到 Google App Engine 之前，服务器将被停止。
- en: Deploying to Google App Engine
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署到 Google App Engine
- en: 'To deploy our web service application to GAE, we must use the following command
    from the project directory:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 要将我们的网络服务应用程序部署到 GAE，我们必须从项目目录中使用以下命令：
- en: '[PRE18]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Cloud SDK will read the `app.yaml` file, which provides input for creating
    an App Engine instance for this application. During the deployment, a container
    image is created using the Cloud Build service; then, this container image is
    uploaded to GCP storage for deployment. Once it has been successfully deployed,
    we can access the web service using the following command:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud SDK将读取`app.yaml`文件，该文件为创建此应用程序的App Engine实例提供输入。在部署期间，使用Cloud Build服务创建容器镜像；然后，将此容器镜像上传到GCP存储以进行部署。一旦成功部署，我们可以使用以下命令访问Web服务：
- en: '[PRE19]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This command will open the application using the default browser on your machine.
    The URL of the hosted application will vary, depending on the region and zone
    that was selected during app creation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将使用您机器上的默认浏览器打开应用程序。托管应用程序的URL将根据在应用程序创建期间选择的区域和区域而变化。
- en: 'It is important to understand that every time we execute the `deploy` command,
    it will create a new version of our application in App Engine, which means more
    resources will be consumed. We can check the versions that have been installed
    for a web application using the following command:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解，每次我们执行`deploy`命令时，它都会在App Engine中创建我们应用程序的新版本，这意味着将消耗更多资源。我们可以使用以下命令检查已安装的Web应用程序的版本：
- en: '[PRE20]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The older versions of the application still can be in a serving state with
    slightly different URLs assigned to them. The older versions can be stopped, started,
    or deleted using the `gcloud app versions` Cloud SDK command and the version ID.
    An application can be stopped or started using the `stop` or `start` commands,
    as shown here:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序的老版本仍然可以处于服务状态，只是分配给它们的URL略有不同。可以使用`gcloud app versions` Cloud SDK命令和版本ID来停止、启动或删除老版本。可以使用`stop`或`start`命令停止或启动应用程序，如下所示：
- en: '[PRE21]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The version ID is available when we run the `gcloud app versions list` command.
    This concludes our discussion on building and deploying a Python web application
    to Google Cloud. Next, we will summarize how we can leverage the GCP console to
    deploy the same application.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行`gcloud app versions list`命令时，版本ID是可用的。这标志着我们关于构建和部署Python Web应用程序到Google
    Cloud的讨论结束。接下来，我们将总结如何利用GCP控制台部署相同的应用程序。
- en: Using the GCP web console
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用GCP网络控制台
- en: The GCP console provides an easy-to-use web portal for accessing and managing
    GCP projects, as well as an online version of Google **Cloud Shell**. The console
    also offers customizable dashboards, visibility to cloud resources used by projects,
    billing details, activity logging, and many more features. When it comes to developing
    and deploying a web application using the GCP console, we have some features we
    can use thanks to the web UI, but most of the steps will require the use of Cloud
    Shell. This is a Cloud SDK that's available online through any browser.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: GCP控制台提供了一个易于使用的网络门户，用于访问和管理GCP项目，以及Google **Cloud Shell**的在线版本。控制台还提供可定制的仪表板、查看项目使用的云资源、账单详情、活动日志以及许多其他功能。当涉及到使用GCP控制台开发和部署Web应用程序时，我们可以利用Web
    UI的一些功能，但大多数步骤将需要使用Cloud Shell。这是一个通过任何浏览器在线提供的Cloud SDK。
- en: 'Cloud Shell is more than Cloud SDK in several ways:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud Shell在几个方面都超越了Cloud SDK：
- en: It offers access to the `gcloud` CLI, as well as the `kubectl` CLI. `kubectl`
    is used for managing resources on the GCP Kubernetes engine.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了对`gcloud` CLI以及`kubectl` CLI的访问。`kubectl`用于管理GCP Kubernetes引擎上的资源。
- en: With Cloud Shell, we can develop, debug, build, and deploy our applications
    using **Cloud Shell Editor**.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Cloud Shell，我们可以使用**Cloud Shell编辑器**来开发、调试、构建和部署我们的应用程序。
- en: Cloud Shell also offers an online development server for testing an application
    before deploying it to App Engine.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloud Shell还提供了一个在线开发服务器，用于在部署到App Engine之前测试应用程序。
- en: Cloud Shell comes with tools to upload and download files between the Cloud
    Shell platform and your machine.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloud Shell附带工具，可以在Cloud Shell平台和您的机器之间上传和下载文件。
- en: Cloud Shell comes with the ability to preview the web application on port 8080
    or a port of your choice.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloud Shell具有在端口8080或您选择的端口上预览Web应用程序的能力。
- en: 'The Cloud Shell commands that are required to set up a new project, build the
    application, and deploy to App Engine are the same ones we discussed for Cloud
    SDK. That is why we will leave this for you to explore by following the same steps
    that we described in the previous section. Note that the project can be set up
    using the GCP console. The Cloud Shell interface can be enabled using the Cloud
    Shell icon on the top menu bar, on the right-hand side. Once Cloud Shell has been
    enabled, a command-line interface will appear at the bottom of the console''s
    web page. This is shown in the following screenshot:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 设置新项目、构建应用程序和部署到App Engine所需的Cloud Shell命令与我们之前讨论的Cloud SDK中的命令相同。这就是为什么我们将这个步骤留给你，让你按照我们在上一节中描述的相同步骤进行探索。请注意，项目可以使用GCP控制台设置。可以通过在顶部菜单栏的右侧使用Cloud
    Shell图标来启用Cloud Shell界面。一旦Cloud Shell被启用，命令行界面将出现在控制台网页的底部。这在上面的屏幕截图中显示：
- en: '![Figure 9.3 – GCP console with Cloud Shell'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 9.3 – GCP console with Cloud Shell]'
- en: '](img/B17189_09_03.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17189_09_03.jpg]'
- en: Figure 9.3 – GCP console with Cloud Shell
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – 带有Cloud Shell的GCP控制台
- en: 'As we mentioned earlier, Cloud Shell comes with an editor tool that can be
    started by using the **Open editor** button. The following screenshot shows the
    Python file opened inside **Cloud Shell Editor**:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，Cloud Shell附带一个编辑器工具，可以通过使用**打开编辑器**按钮启动。以下屏幕截图显示了在**Cloud Shell编辑器**中打开的Python文件：
- en: '![Figure 9.4 – GCP console with Cloud Shell Editor ](img/B17189_09_04.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 9.4 – GCP console with Cloud Shell Editor](img/B17189_09_04.jpg)'
- en: Figure 9.4 – GCP console with Cloud Shell Editor
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – 带有Cloud Shell编辑器的GCP控制台
- en: Another option when it comes to building and deploying web applications is using
    third-party IDEs with Google App Engine plugins. Based on our experience, the
    plugins that are available for commonly used IDEs such as PyCharm and Eclipse
    are mostly built for Python 2 and legacy web application libraries. Directly integrating
    IDEs with GCP requires more work and evolution. At the time of writing, the best
    option is to use Cloud SDK or Cloud Shell directly in conjunction with the editor
    or IDE of your choice for application development.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建和部署Web应用程序时，另一个选择是使用带有Google App Engine插件的第三方IDE。根据我们的经验，适用于常用IDE（如PyCharm和Eclipse）的插件大多是为Python
    2和旧版Web应用程序库构建的。直接将IDE与GCP集成需要更多的工作和演变。在撰写本文时，最佳选择是直接使用您选择的编辑器或IDE配合Cloud SDK或Cloud
    Shell进行应用程序开发。
- en: In this section, we covered developing web applications using Python and deploying
    them to the GCP App Engine platform. Amazon offers the AWS Beanstalk service for
    web application deployment. The steps for deploying a web application in AWS Beanstalk
    are nearly the same as for GCP App Engine, except that AWS Beanstalk does not
    need projects to be set up as a prerequisite. Therefore, we can deploy applications
    faster in AWS Beanstalk.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了使用Python开发Web应用程序并将其部署到GCP App Engine平台。Amazon提供了AWS Beanstalk服务用于Web应用程序部署。在AWS
    Beanstalk中部署Web应用程序的步骤几乎与GCP App Engine相同，只是AWS Beanstalk不需要设置项目作为先决条件。因此，我们可以在AWS
    Beanstalk中更快地部署应用程序。
- en: 'To deploy our web service application in AWS Beanstalk, we must provide the
    following information, either using the AWS console or using the AWS CLI:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 要在AWS Beanstalk中部署我们的Web服务应用程序，我们必须提供以下信息，无论是使用AWS控制台还是使用AWS CLI：
- en: Application name
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序名称
- en: Platform (Python version 3.7 or 3.8, in our case)
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平台（Python版本3.7或3.8，在我们的例子中）
- en: Source code version
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源代码版本
- en: Source code, along with a `requirements.txt` file
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源代码，以及一个`requirements.txt`文件
- en: We recommend using the AWS CLI for web applications that have a dependency on
    third-party libraries. We can upload our source code as a ZIP file or as a web
    archive (`WAR` file) from our local machine or copy them from an **Amazon S3**
    location.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议对于依赖于第三方库的Web应用程序使用AWS CLI。我们可以将我们的源代码作为ZIP文件或Web存档（`WAR`文件）从本地机器上传，或者从**Amazon
    S3**位置复制。
- en: The exact steps of deploying a web application on AWS Beanstalk are available
    at [https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create-deploy-python-flask.html](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create-deploy-python-flask.html).
    Azure offers App Service for building and deploying web applications. You can
    find the steps for creating and deploying a web application on Azure at [https://docs.microsoft.com/en-us/azure/app-service/quickstart-python](https://docs.microsoft.com/en-us/azure/app-service/quickstart-python).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AWS Beanstalk 上部署 Web 应用程序的详细步骤可在 [https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create-deploy-python-flask.html](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create-deploy-python-flask.html)
    获取。Azure 提供了 App Service 用于构建和部署 Web 应用程序。您可以在 [https://docs.microsoft.com/en-us/azure/app-service/quickstart-python](https://docs.microsoft.com/en-us/azure/app-service/quickstart-python)
    找到在 Azure 上创建和部署 Web 应用程序的步骤。
- en: Next, we will explore building driver programs for data processing using cloud
    platforms.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨使用云平台构建数据处理的驱动程序程序。
- en: Using Google Cloud Platform for data processing
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Google Cloud Platform 进行数据处理
- en: Google Cloud Platform offers Cloud Dataflow as a data processing service to
    serve both batch and real-time data streaming applications. This service is meant
    for data scientists and analytics application developers so that they can set
    up a processing **pipeline** for data analysis and data processing. Cloud Dataflow
    uses Apache Beam under the hood. **Apache Beam** originated from Google, but it
    is now an open source project under Apache. This project offers a programming
    model for building data processing using pipelines. Such pipelines can be created
    using Apache Beam and then executed using the Cloud Dataflow service.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud Platform 提供了 Cloud Dataflow 作为数据处理服务，旨在服务于批处理和实时数据流应用程序。此服务旨在为数据科学家和数据分析应用程序开发者提供设置数据处理
    **管道** 的能力，以便他们可以进行数据分析。Cloud Dataflow 在底层使用 Apache Beam。**Apache Beam** 起源于 Google，但现在是一个
    Apache 下的开源项目。该项目为使用管道构建数据处理提供了一个编程模型。这些管道可以使用 Apache Beam 创建，然后使用 Cloud Dataflow
    服务执行。
- en: The Google Cloud Dataflow service is similar to `Amazon Kinesis`, Apache Storm, `Apache
    Spark`, and Facebook Flux. Before we discuss how to use Google Dataflow with Python,
    we will introduce Apache Beam and its pipeline concepts.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud Dataflow 服务类似于 `Amazon Kinesis`、Apache Storm、`Apache Spark` 和 Facebook
    Flux。在我们讨论如何使用 Python 进行 Google Dataflow 之前，我们将介绍 Apache Beam 及其管道概念。
- en: Learning the fundamentals of Apache Beam
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习 Apache Beam 的基础知识
- en: In the current era, data is like a cash cow for many organizations. A lot of
    data is generated by applications, by devices, and by human interaction with systems.
    Before consuming the data, it is important to process it. The steps that are defined
    for data processing are typically called pipelines in Apache Beam nomenclature.
    In other words, a data pipeline is a series of actions that are performed on raw
    data that originates from different sources, and then moves that data to a destination
    for consumption by analytic or business applications.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前时代，数据对于许多组织来说就像一头现金牛。应用程序、设备和人类与系统交互都会产生大量数据。在消费数据之前，处理数据非常重要。在 Apache Beam
    的术语中，为数据处理定义的步骤通常被称为管道。换句话说，数据管道是一系列对来自不同来源的原始数据进行操作的动作，然后将这些数据移动到目的地，以便由分析或商业应用程序消费。
- en: Apache Beam is used to break the problem into small bundles of data that can
    be processed in parallel. One of the main use cases of Apache Beam is **Extract,
    Transform, and Load** (**ETL**) applications. These three ETL steps are core for
    a pipeline whenever we have to move the data from a raw form to a refined form
    for data consumption.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Beam 用于将问题分解成可以并行处理的小数据包。Apache Beam 的主要用例之一是 **提取、转换和加载**（**ETL**）应用程序。这三个
    ETL 步骤在将数据从原始形式转换为用于消费的精炼形式时是管道的核心。
- en: 'The core concepts and components of Apache Beam are as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Beam 的核心概念和组件如下：
- en: '**Pipeline**: A pipeline is a scheme for transforming the data from one form
    into the other as part of data processing.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道**：管道是将数据从一种形式转换为另一种形式的方案，作为数据处理的一部分。'
- en: '**PCollection**: A Pcollection, or Parallel Collection, is analogous to RDD
    in Apache Spark. It is a distributed dataset that contains an immutable and unordered
    bag of elements. The size of the dataset can be fixed or bounded, similar to batch
    processing, where we know how many jobs to process in one batch. The size can
    also be flexible or unbounded based on the continuously updating and streaming
    data source.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PCollection**：PCollection，或并行集合，类似于 Apache Spark 中的 RDD。它是一个包含不可变和无序元素集合的分布式数据集。数据集的大小可以是固定的或有限的，类似于批量处理，我们知道在一个批次中要处理多少个作业。大小也可以根据持续更新和流数据源而灵活或无界。'
- en: '**PTransforms**: These are the operations that are defined in a pipeline to
    transform the data. These operations are operated on PCollection objects.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PTransforms**：这些是在管道中定义以转换数据的操作。这些操作在 PCollection 对象上执行。'
- en: '**SDK**: A language-specific software development kit that''s available for
    Java, Python, and Go to build pipelines and submit them to a runner for execution.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SDK**：一种特定于语言的软件开发工具包，可用于 Java、Python 和 Go 来构建管道并将它们提交给运行器执行。'
- en: '`run (Pipeline)` that is asynchronous by default. A few runners that are available
    are Apache Flink, Apache Spark, and Google Cloud Dataflow.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下是异步的 `run (Pipeline)`。一些可用的运行器包括 Apache Flink、Apache Spark 和 Google Cloud
    Dataflow。
- en: '`DoFn`, which operates on a per-element basis. The provided `DoFn` implementation
    is wrapped inside an `ParDo` object that is designed for parallel execution.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DoFn`，它基于每个元素进行操作。提供的 `DoFn` 实现被封装在一个为并行执行设计的 `ParDo` 对象中。'
- en: 'A simple pipeline looks as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的管道看起来如下：
- en: '![Figure 9.5 – Flow of a pipeline with three PTransform operations'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.5 – 具有三个 PTransform 操作的管道流程'
- en: '](img/B17189_09_05.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17189_09_05.jpg)'
- en: Figure 9.5 – Flow of a pipeline with three PTransform operations
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – 具有三个 PTransform 操作的管道流程
- en: 'To design a pipeline, we must typically consider three elements:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 设计管道时，我们通常必须考虑三个要素：
- en: First, we need to understand the source of the data. Is it stored in a file
    or in a database, or is it coming as a stream? Based on this, we will determine
    what type of Read Transform operation we have to implement. As part of the read
    operation or a separate operation, we also need to understand the data format
    or structure.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要了解数据源。它是存储在文件中、数据库中，还是作为流来？基于此，我们将确定需要实现哪种类型的读取转换操作。作为读取操作的一部分或作为单独的操作，我们还需要了解数据格式或结构。
- en: The next step is to define and design what to do with this data. This is our
    main transform operation(s). We can have multiple transform operations one after
    the other in a serial way or in parallel on the same data. The Apache Beam SDK
    provides several pre-built transforms that can be used. It also allows us to write
    our own transforms using ParDo/DoFn functions.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是定义和设计如何处理这些数据。这是我们主要的转换操作。我们可以以串行方式或并行方式在相同的数据上执行多个转换操作。Apache Beam SDK
    提供了几个预构建的转换，可以用于此。它还允许我们使用 ParDo/DoFn 函数编写自己的转换。
- en: Last, we need to know what the output of our pipeline will be and where to store
    the output results. This is shown as a Write Transform in the preceding diagram.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要知道我们的管道输出将是什么以及在哪里存储输出结果。这在前面的图中显示为写入转换。
- en: In this section, we discussed a simple pipeline structure to explain different
    concepts related to Apache Beam and pipelines. In practice, the pipeline can be
    relatively complex. A pipeline can have multiple input data sources and multiple
    output sinks. The PTransforms operations may result in multiple PCollection objects,
    which requires PTransform operations to run in parallel.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了一个简单的管道结构来解释与 Apache Beam 和管道相关的不同概念。在实践中，管道可能相对复杂。管道可以有多个输入数据源和多个输出接收器。PTransforms
    操作可能导致多个 PCollection 对象，这需要并行执行 PTransform 操作。
- en: In the next few sections, we will learn how to create a new pipeline and execute
    a pipeline using an Apache Beam runner or Cloud Dataflow runner.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将学习如何创建一个新的管道以及使用 Apache Beam 运行器或 Cloud Dataflow 运行器执行管道。
- en: Introducing Apache Beam pipelines
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍 Apache Beam 管道
- en: 'In this section, we will discuss how to create Apache Beam pipelines. As we''ve
    discussed already, a pipeline is a set of actions or operations that are orchestrated
    to achieve certain data processing goals. The pipeline requires an input data
    source that can contain in-memory data, local or remote files, or streaming data.
    The pseudocode for a typical pipeline will look as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何创建 Apache Beam 管道。正如我们之前所讨论的，管道是一系列旨在实现特定数据处理目标的动作或操作。管道需要一个可以包含内存数据、本地或远程文件或流数据的输入数据源。典型管道的伪代码将如下所示：
- en: '[PRE22]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The initial PCollection is used as input to the First PTransform operation.
    The output PCollection of First PTransform will be used as input to Second PTransform,
    and so on. The final output of the PCollection of the last PTransform will be
    captured as a Final PCollection object and be used to export the results to the
    target destination.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 初始 PCollection 被用作第一个 PTransform 操作的输入。第一个 PTransform 的输出 PCollection 将用作第二个
    PTransform 的输入，依此类推。最后一个 PTransform 的 PCollection 的最终输出将被捕获为最终 PCollection 对象，并用于将结果导出到目标位置。
- en: To illustrate this concept, we will build a few example pipelines of different
    complexity levels. These examples are designed to show the roles of different
    Apache components and libraries that are used in building and executing a pipeline.
    In the end, we will build a pipeline for a famous *word count* application that
    is also referenced in the Apache Beam and GCP Dataflow documentation. It is important
    to highlight that we must install the `apache-beam` Python library using the `pip`
    utility for all the code examples in this section.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个概念，我们将构建几个不同复杂程度的示例管道。这些示例旨在展示在构建和执行管道时使用的不同 Apache 组件和库的作用。最后，我们将构建一个用于著名
    *词频统计* 应用程序的管道，该应用程序也参考了 Apache Beam 和 GCP Dataflow 文档。重要的是要强调，我们必须使用 `pip` 工具安装
    `apache-beam` Python 库，以便在本节的所有代码示例中使用。
- en: Example 1 – creating a pipeline with in-memory string data
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例 1 – 使用内存中的字符串数据创建管道
- en: 'In this example, we will create an input PCollection from an in-memory collection
    of strings, apply a couple of transform operations, and then print the results
    to the output console. The following is the complete example code:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将从一个内存中的字符串集合创建一个输入 PCollection，应用几个转换操作，然后将结果打印到输出控制台。以下是完全的示例代码：
- en: '[PRE23]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'For this example, it is important to highlight a few points:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，重要的是要强调以下几点：
- en: We used `|` to write different PTransform operations in a pipeline. This is
    an overloaded operator that is more like applying a PTransform to a PCollection
    to produce another PCollection.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用了 `|` 来在管道中写入不同的 PTransform 操作。这是一个重载的操作符，更像是将 PTransform 应用到 PCollection
    上以产生另一个 PCollection。
- en: We used the `>>` operator to name each PTransform operation for logging and
    tracking purposes. The string between `|` and `>>` is used for displaying and
    logging purposes.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用了 `>>` 操作符为每个 PTransform 操作命名，用于日志记录和跟踪目的。`|` 和 `>>` 之间的字符串用于显示和日志记录目的。
- en: 'We used three transform operations; all are part of the Apache Beam library:'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用了三个转换操作；它们都是 Apache Beam 库的一部分：
- en: a) The first transform operation is used to create a PCollection object, which
    is a string containing five subject names.
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 第一个转换操作用于创建一个 PCollection 对象，它是一个包含五个主题名称的字符串。
- en: b) The second transform operation is used to split the string data into a new
    PCollection using a built-in `String` object method (`split`).
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 第二个转换操作用于使用内置的 `String` 对象方法（`split`）将字符串数据拆分到一个新的 PCollection 中。
- en: c) The third transform operation is used to print each entry in the PCollection
    to the console output.
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 第三个转换操作用于将 PCollection 中的每个条目打印到控制台输出。
- en: The console's output will show a list of subject names, with one name in one
    line.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台的输出将显示一个主题名称列表，每个名称占一行。
- en: Example 2 – creating and processing a pipeline with in-memory tuple data
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例 2 – 使用内存中的元组数据创建和处理管道
- en: 'In this code example, we will create a PCollection of tuples. Each tuple will
    have a subject name and a grade associated with it. The core PTransform operation
    of this pipeline is to separate the subject and its grade from the data. The sample
    code is as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码示例中，我们将创建一个元组 PCollection。每个元组将有一个与之关联的主题名称和成绩。该管道的核心 PTransform 操作是从数据中分离主题及其成绩。示例代码如下：
- en: '[PRE24]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In comparison to the first example, we used the `FlatMapTuple` transform operation
    with a custom function to format the tuple data. The console output will show
    each subject's name, along with its grade, in a separate line.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 与第一个示例相比，我们使用了`FlatMapTuple`转换操作和自定义函数来格式化元组数据。控制台输出将显示每个主题的名称及其成绩，每行一个。
- en: Example 3 – creating a pipeline with data from a text file
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例 3 – 使用文本文件数据创建管道
- en: 'In the first two examples, we focused on building a simple pipeline to parse
    string data from a large string and to split tuples from a PCollection of tuples.
    In practice, we are working on a large volume of data that is either loaded from
    a file or storage system or coming from a streaming source. In this example, we
    will read data from a local text file to build our initial PCollection object
    and also output the final results to an output file. The complete code example
    is as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两个示例中，我们专注于构建一个简单的管道来解析来自大字符串的字符串数据，以及从元组的PCollection中拆分元组。在实践中，我们正在处理大量数据，这些数据可能来自文件或存储系统，或者来自流式源。在此示例中，我们将从本地文本文件中读取数据以构建我们的初始PCollection对象，并将最终结果输出到输出文件。完整的代码示例如下：
- en: '[PRE25]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In this code example, we applied a PTransform operation to read the text data
    from a file before applying any data processing-related PTransforms. Finally,
    we applied a PTransform operation to write the data to an output file. We used
    two new functions in this code example called `ReadFromText` and `WriteToText`,
    as explained here:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码示例中，我们在应用任何数据处理相关的PTransform操作之前，先对一个PTransform操作进行了应用，以从文件中读取文本数据。最后，我们应用了一个PTransform操作将数据写入输出文件。在此代码示例中，我们使用了两个新函数`ReadFromText`和`WriteToText`，如上所述：
- en: '`ReadFromText`: This function is part of the Apache Beam I/O module and is
    used to read data from text files into a PCollection of strings. The file path
    or file pattern can be provided as an input argument to read from a local path.
    We can also use `gs://` to access any file in GCS storage locations.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReadFromText`：此函数是Apache Beam I/O模块的一部分，用于将数据从文本文件读取到字符串的PCollection中。文件路径或文件模式可以作为输入参数提供，以从本地路径读取。我们也可以使用`gs://`来访问GCS存储位置中的任何文件。'
- en: '`WriteToText`: This function is used to write PCollection data to a text file.
    This requires the `file_path_prefix` argument at a minimum. We can also provide
    the `file_path_suffix` argument to set the file extension. `shard_name_template`
    is set to empty to create the file with a name using the prefix and suffix arguments.
    Apache Beam supports a shard name template for defining the filename based on
    a template.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WriteToText`：此函数用于将PCollection数据写入文本文件。至少需要提供`file_path_prefix`参数。我们还可以提供`file_path_suffix`参数来设置文件扩展名。`shard_name_template`设置为空，以使用前缀和后缀参数创建文件名。Apache
    Beam支持基于模板定义文件名的分片名称模板。'
- en: When this pipeline is executed locally, it will create a file named `subjects.txt`
    with subject names captured in it, as per the PTransform operation.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 当此管道在本地执行时，它将创建一个名为`subjects.txt`的文件，其中包含PTransform操作捕获的主题名称。
- en: Example 4 – creating a pipeline for an Apache Beam runner with arguments
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例 4 – 使用参数创建Apache Beam运行器的管道
- en: 'So far, we have learned how to create a simple pipeline, how to build a PCollection
    object from a text file, and how to write the results back to a file. In addition
    to these core steps, we need to perform a few more steps, to make sure our driver
    program is ready to submit the job to a GCP Dataflow runner or any other cloud
    runner. These additional steps are as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何创建一个简单的管道，如何从文本文件构建PCollection对象，以及如何将结果写回文件。除了这些核心步骤之外，我们还需要执行一些额外的步骤，以确保我们的驱动程序已准备好将作业提交给GCP
    Dataflow运行器或任何其他云运行器。这些附加步骤如下：
- en: In the previous example, we provided the names of the input file and the output
    file pattern that are set in the driver program. In practice, we should expect
    these parameters to be provided through command-line arguments. We will use the
    `argparse` library to parse and manage command-line arguments.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在上一个示例中，我们在驱动程序中提供了输入文件和输出文件模式的名称。在实际应用中，我们应该期望这些参数通过命令行参数提供。我们将使用`argparse`库来解析和管理命令行参数。
- en: We will add extended arguments such as setting a runner. This argument will
    be used to set the target runner of the pipeline using DirectRunner or a GCP Dataflow
    runner. Note that DirectRunner is a pipeline runtime for your local machine. It
    makes sure that those pipelines follow the Apache Beam model as closely as possible.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将添加扩展参数，例如设置运行器。此参数将用于使用DirectRunner或GCP Dataflow运行器设置管道的目标运行器。请注意，DirectRunner是用于您本地机器的管道运行时。它确保这些管道尽可能紧密地遵循Apache
    Beam模型。
- en: We will also implement and use the `ParDo` function, which will utilize our
    custom-built function for parsing strings from text data. We can achieve this
    using `String` functions, but it has been added here to illustrate how to use
    `ParDo` and `DoFn` with PTransform.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还将实现并使用`ParDo`函数，该函数将利用我们自定义构建的从文本数据中解析字符串的功能。我们可以使用`String`函数来实现这一点，但在此处添加是为了说明如何使用`ParDo`和`DoFn`与PTransform一起使用。
- en: 'Here are the steps:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是步骤：
- en: 'First, we will build the argument parser and define the arguments we expect
    from the command line. We will set the default values for those arguments and
    set additional helping text to be shown with the `help` switch on the command
    line. The `dest` attribute is important because it is used to identify any argument
    to be used in programming statements. We will also define the `ParDo` function,
    which will be used to execute the pipeline. Some sample code is presented here:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将构建参数解析器并定义我们期望从命令行接收的参数。我们将设置这些参数的默认值，并设置附加的帮助文本，以便在命令行的`help`开关中显示。`dest`属性很重要，因为它用于识别任何在编程语句中使用的参数。我们还将定义`ParDo`函数，该函数将用于执行管道。以下是一些示例代码：
- en: '[PRE26]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, we will set `DirectRunner` as our pipeline runtime and name the job to
    be executed. The sample code for this step is as follows:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将设置`DirectRunner`作为我们的管道运行时，并命名要执行的任务。此步骤的示例代码如下：
- en: '[PRE27]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, we will create a pipeline using the `pipeline_options` object that
    we created in the previous step. The pipeline will be reading data from an input
    text file, transforming data as per our `ParDo` function, and then saving the
    results as output:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将使用之前步骤中创建的`pipeline_options`对象创建一个管道。该管道将从输入文本文件中读取数据，根据我们的`ParDo`函数转换数据，然后将结果保存为输出：
- en: '[PRE28]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Apache Beam is a vast topic, so it is not possible to cover all its features
    without writing a few chapters on it. However, we have covered the fundamentals
    by providing code examples that will enable us to start writing simple pipelines
    that we can deploy on GCP Cloud Dataflow. We will cover this in the next section.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Beam是一个庞大的主题，如果不写几章关于它的内容，就不可能涵盖其所有功能。然而，我们已经通过提供代码示例来涵盖基础知识，这些示例将使我们能够开始编写简单的管道，我们可以在GCP
    Cloud Dataflow上部署这些管道。我们将在下一节中介绍这一点。
- en: Building pipelines for Cloud Dataflow
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建Cloud Dataflow的管道
- en: The code examples we've discussed so far focused on building simple pipelines
    and executing them using DirectRunner. In this section, we will build a driver
    program to deploy a word count data processing pipeline on Google Cloud Dataflow.
    This driver program is important for Cloud Dataflow deployments because we will
    set all cloud-related parameters inside the program. Due to this, there will be
    no need to use Cloud SDK or Cloud Shell to execute additional commands.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论的代码示例主要集中在构建简单的管道并使用DirectRunner执行它们。在本节中，我们将构建一个驱动程序来在Google Cloud Dataflow上部署单词计数数据处理管道。此驱动程序对于Cloud
    Dataflow部署非常重要，因为我们将在此程序内设置所有云相关参数。因此，将无需使用Cloud SDK或Cloud Shell来执行额外的命令。
- en: 'The word count pipeline will be an extended version of our `pipeline4.py` example.
    The additional components and steps required to deploy the word count pipeline
    are summarized here:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 单词计数管道将是我们`pipeline4.py`示例的扩展版本。部署单词计数管道所需的附加组件和步骤在此总结：
- en: First, we will create a new GCP cloud project using steps that are similar to
    the ones we followed for our web service application for App Engine deployment.
    We can use Cloud SDK, Cloud Shell, or the GCP console for this task.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将使用与我们在为App Engine部署我们的网络服务应用程序时遵循的步骤类似的步骤创建一个新的GCP云项目。我们可以使用Cloud SDK、Cloud
    Shell或GCP控制台来完成此任务。
- en: We will enable Dataflow Engine API for the new project.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将为新项目启用Dataflow Engine API。
- en: 'Next, we will create a storage bucket for storing the input and output files
    and to provide temporary and staging directories for Cloud Dataflow. We can achieve
    this by using the GCP console, Cloud Shell, or Cloud SDK. We can use the following
    command if we are using Cloud Shell or Cloud SDK to create a new bucket:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个存储桶来存储输入和输出文件，并为 Cloud Dataflow 提供临时和存档目录。我们可以通过使用 GCP 控制台、Cloud
    Shell 或 Cloud SDK 来实现这一点。如果我们使用 Cloud Shell 或 Cloud SDK 创建新存储桶，可以使用以下命令：
- en: '[PRE29]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You may need to associate a service account with the newly created bucket if
    it is not under the same project as the dataflow pipeline job is and select the
    *storage object admin* role for access control.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果新创建的存储桶不在与数据流管道作业相同的项目下，您可能需要将服务帐户与存储桶关联，并选择 *存储对象管理员* 角色以进行访问控制。
- en: 'We must install Apache Beam with the necessary `gcp` libraries. This can be
    achieved by using the `pip` utility, as follows:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们必须使用必要的 `gcp` 库安装 Apache Beam。这可以通过使用 `pip` 工具实现，如下所示：
- en: '[PRE30]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We must create a key for authentication for the GCP service account used for
    the GCP cloud project. This is not required if we will be running the driver program
    from a GCP platform such as Cloud Shell. The service account key must be downloaded
    on your local machine. To make the key available to the Apache Beam SDK, we need
    to set the path of the key file (a JSON file) to an environment variable called
    `GOOGLE_APPLICATION_CREDENTIALS`.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们必须为用于 GCP 云项目的 GCP 服务帐户创建一个用于身份验证的密钥。如果我们将在 GCP 平台（如 Cloud Shell）上运行驱动程序程序，则不需要这样做。服务帐户密钥必须下载到您的本地机器上。为了使密钥对
    Apache Beam SDK 可用，我们需要将密钥文件（一个 JSON 文件）的路径设置为名为 `GOOGLE_APPLICATION_CREDENTIALS`
    的环境变量。
- en: 'Before discussing how to execute a pipeline on Cloud Dataflow, we will take
    a quick look at the sample word count driver program for this exercise. In this
    driver program, we will define command-line arguments, very similar to the ones
    we did in the previous code example (`pipeline4.py`), except we will do the following:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论如何在 Cloud Dataflow 上执行管道之前，我们将快速查看此练习的样本词频驱动程序程序。在这个驱动程序程序中，我们将定义与之前代码示例（`pipeline4.py`）中非常相似的命令行参数，除了我们将执行以下操作：
- en: Instead of setting the `GOOGLE_APPLICATION_CREDENTIALS` environment variable
    through the operating system, we will set it using our driver program for ease
    of execution for testing purposes.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了便于执行和测试，我们不会通过操作系统设置 `GOOGLE_APPLICATION_CREDENTIALS` 环境变量，而是使用我们的驱动程序程序来设置它。
- en: We will upload the `sample.txt` file to Google storage, which is the `gs//muasif/input`
    directory in our case. We will use the path to this Google storage as the default
    value of the `input` argument.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将上传 `sample.txt` 文件到 Google 存储，在我们的例子中是 `gs//muasif/input` 目录。我们将使用此 Google
    存储的路径作为 `input` 参数的默认值。
- en: 'The complete sample code is as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的示例代码如下：
- en: '[PRE31]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In the next step, we will set up extended arguments for the pipeline options
    to execute our pipeline on the Cloud Dataflow runtime. These arguments are as
    follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将设置管道选项的扩展参数，以便在 Cloud Dataflow 运行时上执行我们的管道。这些参数如下：
- en: Runtime platform (runner) for pipeline execution (DataflowRunner, in this case)
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道执行的运行平台（运行器）（在这种情况下为 DataflowRunner）
- en: GCP Cloud Project ID
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP 云项目 ID
- en: GCP region
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP 区域
- en: Google storage bucket paths for storing input, output, and temporary files
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储输入、输出和临时文件的 Google 存储桶路径
- en: Job name for tracking purposes
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于跟踪的作业名称
- en: 'Based on these arguments, we will create a pipeline options object to be used
    for pipeline execution. The sample code for these tasks is as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些参数，我们将创建一个用于管道执行的管道选项对象。这些任务的示例代码如下：
- en: '[PRE32]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, we will implement a pipeline with the pipeline options that have already
    been defined and add our PTransform operations. For this code example, we added
    an extra PTransform operation to build a pair of each word with `1`. In the follow-up
    PTransform operation, we grouped the pairs and applied the `sum` operation to
    count their frequency. This gives us the count of each word in the input text
    file:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将实现一个管道，其中包含已定义的管道选项，并添加我们的 PTransform 操作。对于此代码示例，我们添加了一个额外的 PTransform
    操作来构建每个单词与 `1` 的配对。在后续的 PTransform 操作中，我们将配对分组并应用 `sum` 操作来计算它们的频率。这给出了输入文本文件中每个单词的计数：
- en: '[PRE33]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We set default values for each argument within the driver program. This means
    that we can execute the program directly with the `python wordcount.py` command
    or we can use the following command to pass the arguments through the CLI:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在驱动程序程序中为每个参数设置了默认值。这意味着我们可以直接使用`python wordcount.py`命令执行程序，或者我们可以使用以下命令通过CLI传递参数：
- en: '[PRE34]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output file will contain the results, along with the count of each word
    in the file. GCP Cloud Dataflow provides additional tools for monitoring the progress
    of submitted jobs and for understanding the resource utilization to perform the
    job. The following screenshot of the GCP console shows a list of jobs that have
    been submitted to Cloud Dataflow. The summary view shows their statuses and a
    few key metrics:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 输出文件将包含结果，以及文件中每个单词的计数。GCP Cloud Dataflow为监控提交作业的进度和理解执行作业的资源利用率提供了额外的工具。以下GCP控制台的屏幕截图显示了已提交给Cloud
    Dataflow的作业列表。摘要视图显示了它们的状态和一些关键指标：
- en: '![Figure 9.6 – Cloud Dataflow jobs summary'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.6 – 云数据流作业摘要'
- en: '](img/B17189_09_06.jpg)'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B17189_09_06.jpg)'
- en: Figure 9.6 – Cloud Dataflow jobs summary
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 – 云数据流作业摘要
- en: 'We can navigate to the detailed job view (by clicking any job name), as shown
    in the following screenshot. This view shows the job and environment details on
    the right-hand side and a progress summary of the different PTransforms we defined
    for our pipeline. When the job is running, the status of each PTransform operation
    is updated in real time, as shown here:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以导航到详细作业视图（通过点击任何作业名称），如下面的屏幕截图所示。此视图显示右侧的作业和环境详情以及我们为管道定义的不同PTransforms的进度摘要。当作业运行时，每个PTransform操作的状态会实时更新，如下所示：
- en: '![Figure 9.7 – Cloud Dataflow job detail view with a flowchart and metrics'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.7 – 带流程图和指标的云数据流作业详细视图'
- en: '](img/B17189_09_07.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B17189_09_07.jpg)'
- en: Figure 9.7 – Cloud Dataflow job detail view with a flowchart and metrics
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – 带流程图和指标的云数据流作业详细视图
- en: A very important point to notice is that the different PTransform operations
    are named according to the strings we used with the `>>` operator. This is helpful
    for visualizing the operations conveniently. This concludes our discussion on
    building and deploying a pipeline for Google Dataflow. In comparison to Apache
    Spark, Apache Beam provides more flexibility for parallel and distributed data
    processing. With the availability of cloud data processing options, we can focus
    entirely on modeling the pipelines and leave the job of executing pipelines to
    cloud providers.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常重要的要点是，不同的PTransform操作是根据我们使用`>>`操作符时使用的字符串命名的。这有助于方便地可视化操作。这标志着我们对构建和部署Google
    Dataflow管道的讨论结束。与Apache Spark相比，Apache Beam为并行和分布式数据处理提供了更多的灵活性。随着云数据处理选项的可用性，我们可以完全专注于建模管道，并将执行管道的工作留给云服务提供商。
- en: As we mentioned earlier, Amazon offers a similar service (AWS Kinesis) for deploying
    and executing pipelines. AWS Kinesis is more focused on data streams for real-time
    data. Like AWS Beanstalk, AWS Kinesis does not require that we set up a project
    as a prerequisite. The user guides for data processing using AWS Kinesis are available
    at [https://docs.aws.amazon.com/kinesis/](https://docs.aws.amazon.com/kinesis/).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，亚马逊提供类似的服务（AWS Kinesis）用于部署和执行管道。AWS Kinesis更专注于实时数据的数据流。与AWS Beanstalk类似，AWS
    Kinesis不需要我们作为先决条件设置一个项目。使用AWS Kinesis进行数据处理的用户指南可在[https://docs.aws.amazon.com/kinesis/](https://docs.aws.amazon.com/kinesis/)找到。
- en: Summary
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed the role of Python for developing applications
    for cloud deployment in general, as well as the use of Apache Beam with Python
    for deploying data processing pipelines on Google Cloud Dataflow. We started this
    chapter by comparing three main public cloud providers in terms of what they offer
    for developing, building, and deploying different types of applications. We also
    compared the options that are available from each cloud provider for runtime environments.
    We learned that each cloud provider offers a variety of runtime engines based
    on the application or program. For example, we have separate runtime engines for
    classic web applications, container-based applications, and serverless functions.
    To explore the effectiveness of Python for cloud-native web applications, we built
    a sample application and learned how to deploy such an application on Google App
    Engine by using Cloud SDK. In the last section, we extended our discussion of
    the data process, which we started in the previous chapter. We introduced a new
    modeling approach for data processing (pipelines) using Apache Beam. Once we learned
    how to build pipelines with a few code examples, we extended our discussion to
    how to build pipelines for Cloud Dataflow deployment.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了Python在开发云部署应用程序中的作用，以及使用Python和Apache Beam在Google Cloud Dataflow上部署数据处理管道的使用。我们通过比较三个主要的公共云提供商在开发、构建和部署不同类型应用程序方面所提供的内容开始本章。我们还比较了每个云提供商提供的运行环境选项。我们了解到，每个云提供商都提供基于应用程序或程序的多种运行引擎。例如，我们有针对经典Web应用程序、基于容器的应用程序和无服务器函数的独立运行引擎。为了探索Python在云原生Web应用程序中的有效性，我们构建了一个示例应用程序，并学习了如何使用Cloud
    SDK在Google App Engine上部署此类应用程序。在最后一节中，我们扩展了关于数据处理（管道）的讨论，这是我们在上一章开始的。我们介绍了使用Apache
    Beam的数据处理（管道）的新建模方法。一旦我们通过几个代码示例学习了如何构建管道，我们就将讨论扩展到如何构建用于Cloud Dataflow部署的管道。
- en: This chapter provided a comparative analysis of public cloud service offerings.
    This was followed by hands-on knowledge of building web applications and data
    processing applications for the cloud. The code examples included in this chapter
    will enable you to start creating cloud projects and writing code for Apache Beam.
    This knowledge is important for anyone who wants to solve their big data problems
    using cloud-based data processing services.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了对公共云服务提供的比较分析。随后，我们介绍了构建云应用程序和数据处理应用程序的实战知识。本章包含的代码示例将使您能够开始创建云项目并为Apache
    Beam编写代码。对于任何希望使用基于云的数据处理服务来解决大数据问题的人来说，这种知识非常重要。
- en: In the next chapter, we will explore the power of Python for developing web
    applications using the Flask and Django frameworks.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨使用Flask和Django框架开发Web应用程序的Python的强大功能。
- en: Questions
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: How is AWS Beanstalk different from AWS App Runner?
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AWS Beanstalk与AWS App Runner有何不同？
- en: What is the GCP Cloud Function service?
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GCP云函数服务是什么？
- en: What services from GCP are available for data processing?
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GCP提供哪些数据处理服务？
- en: What is an Apache Beam pipeline?
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Apache Beam管道是什么？
- en: What is the role of PCollection in a data processing pipeline?
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PCollection在数据处理管道中扮演什么角色？
- en: Further reading
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Flask Web Development*, by Miguel Grinberg.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《*Flask Web开发*》，作者：Miguel Grinberg。
- en: '*Advanced Guide to Python 3 Programming*, by John Hunt.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《*Python 3编程高级指南*》，作者：John Hunt。
- en: '*Apache Beam: A Complete Guide*, by Gerardus Blokdyk.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '《*Apache Beam: A Complete Guide*》，作者：Gerardus Blokdyk。'
- en: '*Google Cloud Platform for Developers*, by Ted Hunter, Steven Porter.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《*Google Cloud Platform for Developers*》，作者：Ted Hunter, Steven Porter。
- en: '*Google Cloud Dataflow documentation* is available at[https://cloud.google.com/dataflow/docs](https://cloud.google.com/dataflow/docs).'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《*Google Cloud Dataflow文档*》可在[https://cloud.google.com/dataflow/docs](https://cloud.google.com/dataflow/docs)找到。
- en: '*AWS Elastic Beanstalk documentation* is available at [https://docs.aws.amazon.com/elastic-beanstalk](https://docs.aws.amazon.com/elastic-beanstalk).'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《*AWS Elastic Beanstalk文档*》可在[https://docs.aws.amazon.com/elastic-beanstalk](https://docs.aws.amazon.com/elastic-beanstalk)找到。
- en: '*Azure App Service documentation* is available at [https://docs.microsoft.com/en-us/azure/app-service/](https://docs.microsoft.com/en-us/azure/app-service/).'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《*Azure App Service文档*》可在[https://docs.microsoft.com/en-us/azure/app-service/](https://docs.microsoft.com/en-us/azure/app-service/)找到。
- en: '*AWS Kinesis documentation* is available at [https://docs.aws.amazon.com/kinesis/](https://docs.aws.amazon.com/kinesis/).'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《*AWS Kinesis文档*》可在[https://docs.aws.amazon.com/kinesis/](https://docs.aws.amazon.com/kinesis/)找到。
- en: Answers
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 答案
- en: AWS Beanstalk is a general-purpose PaaS offering for deploying web applications,
    whereas AWS App Runner is a fully managed service for deploying container-based
    web applications.
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AWS Beanstalk 是一种通用的 PaaS 服务，用于部署 Web 应用程序，而 AWS App Runner 是一种完全托管的服务，用于部署基于容器的
    Web 应用程序。
- en: GCP Cloud Function is a serverless, event-driven service for executing a program.
    The specified event can be triggered from another GCP service or through an HTTP
    request.
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GCP Cloud Function 是一种无服务器、事件驱动的服务，用于执行程序。指定的事件可以由另一个 GCP 服务触发，或者通过 HTTP 请求触发。
- en: Cloud Dataflow and Cloud Dataproc are two popular services for data processing
    offered by GCP.
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cloud Dataflow 和 Cloud Dataproc 是 GCP 提供的两种流行的数据处理服务。
- en: An Apache Beam pipeline is a set of actions that have been defined to load the
    data, transform the data from one form into another, and write the data to a destination.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Apache Beam 管道是一组定义好的操作，用于加载数据、将数据从一种形式转换成另一种形式，并将数据写入目的地。
- en: PCollection is like an RDD in Apache Spark that holds data elements. In pipeline
    data processing, a typical PTransform operation takes one or more PCollection
    objects as input and produces the results as one or more PCollection objects.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PCollection 类似于 Apache Spark 中的 RDD，用于存储数据元素。在管道数据处理中，典型的 PTransform 操作以一个或多个
    PCollection 对象作为输入，并以一个或多个 PCollection 对象的形式产生结果。
