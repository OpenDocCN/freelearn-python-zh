- en: Putting It All Together
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整合所有内容
- en: This book has so far introduced scraping techniques using a custom website,
    which helped us focus on learning particular skills. In this chapter, we will
    analyze a variety of real-world websites to show how the techniques we've learned
    in the book can be applied. First, we'll use Google to show a real-world search
    form, then Facebook for a JavaScript-dependent website and API, Gap for a typical
    online store, and finally, BMW for a map interface. Since these are live websites,
    there is a risk they will change by the time you read this. However, this is fine
    because the purpose of this chapter's examples is to show you how the techniques
    learned so far can be applied, rather than to show you how to scrape any particular
    website. If you choose to run an example, first check whether the website structure
    has changed since these examples were made and whether their current terms and
    conditions prohibit scraping.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本书迄今为止介绍了使用自定义网站进行抓取的技术，这有助于我们专注于学习特定的技能。在本章中，我们将分析各种现实世界的网站，以展示我们书中学到的技术如何应用。首先，我们将使用Google来展示现实世界的搜索表单，然后是Facebook的JavaScript依赖网站和API，Gap的典型在线商店，最后是宝马的地图界面。由于这些是实时网站，它们在您阅读此内容时可能会发生变化。然而，这是可以的，因为本章示例的目的在于展示我们学到的技术如何应用，而不是展示如何抓取任何特定的网站。如果您选择运行示例，请首先检查自这些示例制作以来网站结构是否已更改，以及它们的当前条款和条件是否禁止抓取。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Scraping a Google search result web page
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抓取Google搜索结果网页
- en: Investigating the Facebook API
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调查Facebook API
- en: Using multiple threads with the Gap website
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多线程抓取Gap网站
- en: Reverse engineering the BMW dealer locator page
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向工程宝马经销商定位页面
- en: Google search engine
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google搜索引擎
- en: To investigate using our knowledge of CSS selectors, we will scrape Google search
    results. According to the Alexa data used in [Chapter 4](py-web-scrp-2e_ch04.html),
    *Concurrent Downloading*, google.com is the world's most popular website, and
    conveniently, its structure is simple and straightforward to scrape.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调查使用我们对CSS选择器的了解，我们将抓取Google搜索结果。根据[第4章](py-web-scrp-2e_ch04.html)中使用的Alexa数据，“并发下载”，google.com是世界上最受欢迎的网站，并且方便的是，它的结构简单，易于抓取。
- en: International Google may redirect to a country-specific version, depending on
    your location. In these examples, Google is set to the Romanian version, so your
    results may look slightly different.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 国际Google可能会根据您的位置重定向到特定国家的版本。在这些示例中，Google被设置为罗马尼亚版本，因此您的结果可能会有所不同。
- en: 'Here is the Google search homepage loaded with browser tools to inspect the
    form:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用浏览器工具加载的Google搜索主页：
- en: '![](img/google_search-1.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/google_search-1.png)'
- en: We can see here that the search query is stored in an input with name `q`, and
    then the form is submitted to the path `/search` set by the `action` attribute.
    We can test this by doing a test search to submit the form, which would then be
    redirected to a URL, such as [https://www.google.ro/?gws_rd=cr,ssl&ei=TuXYWJXqBsGsswHO8YiQAQ#q=test&*](https://www.google.ro/?gws_rd=cr,ssl&ei=TuXYWJXqBsGsswHO8YiQAQ#q=test&*).
    The exact URL will depend on your browser and location. Also if you have Google
    Instant enabled, AJAX will be used to load the search results dynamically rather
    than submitting the form. This URL has many parameters, but the only one required
    is `q` for the query.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，搜索查询存储在一个名为`q`的输入中，然后表单通过`action`属性设置的路径`/search`提交。我们可以通过进行测试搜索来提交表单进行测试，然后将被重定向到URL，例如[https://www.google.ro/?gws_rd=cr,ssl&ei=TuXYWJXqBsGsswHO8YiQAQ#q=test&*](https://www.google.ro/?gws_rd=cr,ssl&ei=TuXYWJXqBsGsswHO8YiQAQ#q=test&*)。确切的URL将取决于您的浏览器和位置。如果您启用了Google
    Instant，将使用AJAX动态加载搜索结果而不是提交表单。此URL有许多参数，但唯一必需的是用于查询的`q`。
- en: 'The URL [https://www.google.com/search?q=test](https://www.google.com/search?q=test) shows
    we can use this URL to produce a search result, as shown in this screenshot:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: URL [https://www.google.com/search?q=test](https://www.google.com/search?q=test)
    显示我们可以使用这个URL来生成搜索结果，如图所示：
- en: '![](img/image_09_002.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_09_002.jpg)'
- en: 'The structure of the search results can be examined with your browser tools,
    as shown here:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用浏览器工具检查搜索结果的架构，如图所示：
- en: '![](img/google_results.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/google_results.png)'
- en: Here, we see that the search results are structured as links whose parent element
    is a `<h3>` tag with class "`r`".
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到搜索结果被结构化为链接，其父元素是一个带有类名"`r`"的`<h3>`标签。
- en: 'To scrape the search results, we will use a CSS selector, which was introduced
    in [Chapter 2](py-web-scrp-2e_ch02.html), *Scraping the Data*:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了抓取搜索结果，我们将使用CSS选择器，这在[第2章](py-web-scrp-2e_ch02.html)“抓取数据”中已介绍：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: So far, we downloaded the Google search results and used `lxml` to extract the
    links. In the preceding screenshot, the link includes a bunch of extra parameters
    alongside the actual website URL, which are used for tracking clicks.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经下载了谷歌搜索结果，并使用`lxml`提取了链接。在先前的屏幕截图中，链接包含了一堆额外的参数，这些参数与实际网站URL一起使用，用于跟踪点击。
- en: 'Here is the first link we find on the page:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在页面上找到的第一个链接：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The content we want here is `http://www.speedtest.net/`, which can be parsed
    from the query string using the `urlparse` module:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要的内容是`http://www.speedtest.net/`，可以使用`urlparse`模块从查询字符串中解析出来：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This query string parsing can be applied to extract all links.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这种查询字符串解析可以应用于提取所有链接。
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Success! The links from the first page of this Google search have been successfully
    scraped. The full source for this example is available at [https://github.com/kjam/wswp/blob/master/code/chp9/scrape_google.py](https://github.com/kjam/wswp/blob/master/code/chp9/scrape_google.py).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！谷歌搜索第一页的链接已成功抓取。本例的完整源代码可在[https://github.com/kjam/wswp/blob/master/code/chp9/scrape_google.py](https://github.com/kjam/wswp/blob/master/code/chp9/scrape_google.py)找到。
- en: 'One difficulty with Google is that a CAPTCHA image will be shown if your IP
    appears suspicious, for example, when downloading too fast:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Google的一个困难之处在于，如果您的IP看起来可疑，例如下载速度过快时，将会显示验证码图像：
- en: '![](img/4364OS_09_04.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4364OS_09_04.png)'
- en: This CAPTCHA image could be solved using the techniques covered in Chapter 7,
    *Solving CAPTCHA*, though it would be preferable to avoid suspicion and download
    slowly, or use proxies if a faster download rate is required. Overloading Google
    can get your IP or even set of IPs banned from Google domains for a series of
    hours or day; so ensure you are courteous to others' (and your own) use of the
    site so your home or office doesn't get blacklisted.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个验证码图像可以使用第7章中介绍的技术解决，即“解决验证码”，尽管最好是避免引起怀疑并缓慢下载，或者如果需要更快的下载速度，则使用代理。过度使用谷歌可能会使您的IP或一系列IP被谷歌域名禁止数小时或一天；因此，请确保您对他人（以及您自己）使用网站的方式礼貌，以免您的家庭或办公室被列入黑名单。
- en: Facebook
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Facebook
- en: To demonstrate using a browser and API, we will investigate Facebook's site.
    Currently, Facebook is the world's largest social network in terms of monthly
    active users, and therefore, its user data is extremely valuable.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示使用浏览器和API，我们将调查Facebook的网站。目前，Facebook按月活跃用户数计算是世界上最大的社交网络，因此，其用户数据极其有价值。
- en: The website
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网站内容
- en: 'Here is an example Facebook page for Packt Publishing at[https://www.facebook.com/PacktPub](https://www.facebook.com/PacktPub):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '这里是Packt出版社在Facebook上的一个示例页面，[https://www.facebook.com/PacktPub](https://www.facebook.com/PacktPub):'
- en: '![](img/B05679_09_05.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05679_09_05.png)'
- en: 'Viewing the source of this page, you would find that the first few posts are
    available, and that later posts are loaded with AJAX when the browser scrolls.
    Facebook also has a mobile interface, which, as mentioned in [Chapter 1](py-web-scrp-2e_ch01.html),
    *Introduction to Web Scraping*, is often easier to scrape. The same page using
    the mobile interface is available at [https://m.facebook.com/PacktPub](https://m.facebook.com/PacktPub):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 查看此页面的源代码，您会发现前几条帖子是可用的，而后续帖子则在浏览器滚动时通过AJAX加载。Facebook还有一个移动界面，正如在第1章“网络抓取简介”中提到的，它通常更容易抓取。使用移动界面的相同页面可在[https://m.facebook.com/PacktPub](https://m.facebook.com/PacktPub)找到：
- en: '![](img/image_09_004.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_09_004.jpg)'
- en: If we interacted with the mobile website and then checked our browser tools,
    we would find that this interface uses a similar structure for the AJAX events,
    so it isn't easier to scrape. These AJAX events can be reverse engineered; however,
    different types of Facebook pages use different AJAX calls, and from my past experience,
    Facebook often changes the structure of these calls; so, scraping them will require
    ongoing maintenance. Therefore, as discussed in Chapter 5, *Dynamic Content*,
    unless performance is crucial, it would be preferable to use a browser rendering
    engine to execute the JavaScript events for us and give us access to the resulting
    HTML.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们与移动网站互动然后检查浏览器工具，我们会发现这个界面使用与 AJAX 事件相似的架构，因此并不容易抓取。这些 AJAX 事件可以被逆向工程；然而，不同类型的
    Facebook 页面使用不同的 AJAX 调用，根据我的以往经验，Facebook 经常更改这些调用的结构；因此，抓取它们将需要持续维护。因此，如第 5
    章所述，*动态内容*，除非性能至关重要，否则使用浏览器渲染引擎来执行 JavaScript 事件并为我们提供访问结果的 HTML 会更可取。
- en: 'Here is an example snippet using Selenium to automate logging in to Facebook
    and then redirecting to the given page URL:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个使用 Selenium 自动登录 Facebook 并重定向到给定页面 URL 的示例片段：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This function can then be called to load the Facebook page of interest and scrape
    the resulting generated HTML, using a valid Facebook e-mail and password.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数可以被调用以加载感兴趣的 Facebook 页面并抓取生成的 HTML，使用有效的 Facebook 邮箱和密码。
- en: Facebook API
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Facebook API
- en: As mentioned in [Chapter 1](py-web-scrp-2e_ch01.html), *Introduction to Web
    Scraping*, scraping a website is a last resort when the data is not available
    in a structured format. Facebook does offer APIs for a vast majority of the public
    or private (via your user account) data, so we should check whether these APIs
    provide access to what we are after before building an intensive browser scraper.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [第 1 章](py-web-scrp-2e_ch01.html) *网络抓取简介* 中所述，当数据不以结构化格式提供时，抓取网站是最后的手段。Facebook
    为大多数公共或私人（通过您的用户账户）数据提供了 API，因此我们应该在构建密集型浏览器抓取器之前检查这些 API 是否提供了我们想要的数据。
- en: The first thing to do is determine what data is available via the API. To figure
    this out, we should first reference the API documentation. The developer documentation
    available at [https://developers.facebook.com/docs/](https://developers.facebook.com/docs/)
    shows all different types of APIs, including the Graph API, which is the one containing
    the information we desire. If you need to build other interactions with Facebook
    (via the API or SDK), the documentation is regularly updated and easy to use.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的事情是确定通过 API 可用的数据。为了弄清楚这一点，我们应该首先参考 API 文档。可在 [https://developers.facebook.com/docs/](https://developers.facebook.com/docs/)
    找到的开发者文档显示了所有不同类型的 API，包括我们想要的 Graph API。如果您需要通过 API 或 SDK 与 Facebook 建立其他交互，文档会定期更新且易于使用。
- en: 'Also available via the documentation links is the in-browser Graph API Explorer,
    located at [https://developers.facebook.com/tools/explorer/](https://developers.facebook.com/tools/explorer/).
    As shown in the following screenshot, the Explorer is a great place to test queries
    and their results:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过文档链接也提供了浏览器中的 Graph API 探索器，位于 [https://developers.facebook.com/tools/explorer/](https://developers.facebook.com/tools/explorer/)。如下面的截图所示，探索器是测试查询及其结果的好地方：
- en: '![](img/graph_explorer.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图形探索器](img/graph_explorer.png)'
- en: Here, I can search the API to retrieve the PacktPub Facebook Page ID. This Graph
    Explorer can also be used to generate access tokens, which we will use to navigate
    the API.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我可以搜索 API 以检索 PacktPub Facebook 页面 ID。这个图形探索器也可以用来生成访问令牌，我们将使用它来导航 API。
- en: 'To utilize the Graph API with Python, we need to use special access tokens
    with slightly more advanced requests. Luckily, there is already a well-maintained
    library for us, called `facebook-sdk` ([https://facebook-sdk.readthedocs.io](https://facebook-sdk.readthedocs.io)).
    We can easily install it using pip:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Python 利用 Graph API，我们需要使用稍微更高级的请求的特殊访问令牌。幸运的是，已经有一个维护得很好的库供我们使用，名为 `facebook-sdk`
    ([https://facebook-sdk.readthedocs.io](https://facebook-sdk.readthedocs.io))。我们可以使用
    pip 轻易地安装它：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here is an example of using Facebook''s Graph API to extract data from the
    Packt Publishing page:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用 Facebook 的 Graph API 从 Packt 出版页面提取数据的示例：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We see the same results as from the browser-based Graph Explorer. We can request
    more information about the page by passing some extra details we would like to
    extract. To determine which details, we can see all available fields for pages
    in the Graph documentation [https://developers.facebook.com/docs/graph-api/reference/page/](https://developers.facebook.com/docs/graph-api/reference/page/).
    Using the keyword argument `fields`, we can extract these extra available fields
    from the API:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到的结果与基于浏览器的图形探索器相同。我们可以通过传递一些我们想要提取的额外详细信息来请求更多关于页面的信息。为了确定哪些详细信息，我们可以查看图形文档中页面所有可用的字段 [https://developers.facebook.com/docs/graph-api/reference/page/](https://developers.facebook.com/docs/graph-api/reference/page/)。使用关键字参数
    `fields`，我们可以从 API 中提取这些额外的可用字段：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can see that this response is a well-formatted Python dictionary, which we
    can easily parse.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这个响应是一个格式良好的 Python 字典，我们可以轻松解析。
- en: The Graph API provides many other calls to access user data, which are documented
    on Facebook's developer page at [https://developers.facebook.com/docs/graph-api](https://developers.facebook.com/docs/graph-api).
    Depending on the data you need, you may also want to create a Facebook developer
    application, which can give you a longer usable access token.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图形 API 提供了许多其他调用以访问用户数据，这些调用在 Facebook 开发者页面上有文档记录，网址为 [https://developers.facebook.com/docs/graph-api](https://developers.facebook.com/docs/graph-api)。根据你需要的数据，你可能还想要创建一个
    Facebook 开发者应用程序，这可以给你一个更长时间可用的访问令牌。
- en: Gap
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gap
- en: To demonstrate using a Sitemap to investigate content, we will use the Gap website.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示使用网站地图来调查内容，我们将使用 Gap 网站。
- en: 'Gap has a well structured website with a `Sitemap` to help web crawlers locate
    their updated content. If we use the techniques from [Chapter 1](py-web-scrp-2e_ch01.html),
    *Introduction to Web Scraping*, to investigate a website, we would find their
    `robots.txt` file at [http://www.gap.com/robots.txt](http://www.gap.com/robots.txt),
    which contains a link to this Sitemap:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Gap 拥有一个结构良好的网站，有一个 `Sitemap` 来帮助网络爬虫定位其更新的内容。如果我们使用第 1 章（py-web-scrp-2e_ch01.html）中介绍的网络爬取技术来调查一个网站，我们会找到他们的
    `robots.txt` 文件在 [http://www.gap.com/robots.txt](http://www.gap.com/robots.txt)，其中包含指向此网站地图的链接：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here are the contents of the linked `Sitemap` file:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是链接的 `Sitemap` 文件的内容：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As shown here, this `Sitemap` link is just an index and contains links to other
    `Sitemap` files. These other `Sitemap` files then contain links to thousands of
    product categories, such as [http://www.gap.com/products/womens-jogger-pants.jsp](http://www.gap.com/products/womens-jogger-pants.jsp):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如此所示，这个 `Sitemap` 链接只是一个索引，并包含指向其他 `Sitemap` 文件的链接。这些其他 `Sitemap` 文件然后包含指向数千个产品类别的链接，例如
    [http://www.gap.com/products/womens-jogger-pants.jsp](http://www.gap.com/products/womens-jogger-pants.jsp)：
- en: '![](img/gap_pants.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/gap_pants.png)'
- en: There is a lot of content to crawl here, so we will use the threaded crawler
    developed in [Chapter 4](py-web-scrp-2e_ch04.html), *Concurrent Downloading*.
    You may recall that this crawler supports a URL pattern to match on the page.
    We can also define a `scraper_callback` keyword argument variable, which will
    allow us to parse more links.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多内容需要爬取，因此我们将使用在第 4 章（py-web-scrp-2e_ch04.html）中开发的线程化爬虫，*并发下载*。你可能还记得这个爬虫支持一个
    URL 模式来匹配页面。我们还可以定义一个 `scraper_callback` 关键字参数变量，这将允许我们解析更多链接。
- en: 'Here is an example callback to crawl the Gap `Sitemap` link:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个用于爬取 Gap `Sitemap` 链接的示例回调：
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This callback first checks the downloaded URL extension. If the extension is
    `.xml`, the downloaded URL is for a `Sitemap` file, and the `lxml``etree` module
    is used to parse the XML and extract the links from it. Otherwise, this is a category
    URL, although this example does not implement scraping the category. Now we can
    use this callback with the threaded crawler to crawl `gap.com`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个回调首先检查下载的 URL 扩展名。如果扩展名是 `.xml`，则下载的 URL 是一个 `Sitemap` 文件，并使用 `lxml` `etree`
    模块来解析 XML 并从中提取链接。否则，这是一个分类 URL，尽管这个示例没有实现爬取分类。现在我们可以使用这个回调与线程化爬虫来爬取 `gap.com`：
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Unfortunately, `lxml` expects to load content from bytes or XML fragments, and
    we have instead stored the Unicode response (so we could parse using regular expressions
    and easily save to disk in Chapter 3, *Caching Downloads* and Chapter 4, *Concurrent
    Downloading*). However, we do have access to the URL in this function. Although
    it is inefficient, we could load the page again; if we only do this for XML pages,
    it should keep the number of requests down and therefore not add too much load
    time. Of course, if we are using caching this also makes it more efficient.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，`lxml`期望从字节或XML片段加载内容，而我们却存储了Unicode响应（因此我们可以在第3章“缓存下载”和第4章“并发下载”中使用正则表达式进行解析，并轻松保存到磁盘）。然而，我们在这个函数中可以访问URL。虽然这样做效率不高，但我们可以再次加载页面；如果我们只为XML页面这样做，应该可以减少请求数量，因此不会增加太多加载时间。当然，如果我们使用缓存，这也会使其更有效率。
- en: 'Let''s try rewriting the callback function:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试重写回调函数：
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, if we try running it again, we see success:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们再次尝试运行它，我们看到 成功：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As expected, the `Sitemap` files were first downloaded and then the clothing
    categories. You'll find throughout your web scraping projects that you may need
    to modify and adapt your code and classes so they fit with new problems. This
    is just one of the many exciting challenges of scraping content from the Internet.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，首先下载了`Sitemap`文件，然后是服装类别。你会在你的网络爬取项目中发现，你可能需要修改和调整你的代码和类，以便它们适应新的问题。这只是从互联网上抓取内容时许多令人兴奋的挑战之一。
- en: BMW
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 宝马
- en: 'To investigate how to reverse engineer a new website, we will take a look at
    the BMW site. The BMW website has a search tool to find local dealerships, available
    at [https://www.bmw.de/de/home.html?entryType=dlo](https://www.bmw.de/de/home.html?entryType=dlo):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究如何逆向工程一个新网站，我们将查看宝马网站。宝马网站有一个用于查找当地经销商的搜索工具，可在[https://www.bmw.de/de/home.html?entryType=dlo](https://www.bmw.de/de/home.html?entryType=dlo)找到：
- en: '![](img/image_09_006.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_09_006.jpg)'
- en: 'This tool takes a location and then displays the points near it on a map, such
    as this search for `Berlin`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此工具接受一个位置，然后在地图上显示其附近的点，例如对`柏林`的搜索：
- en: '![](img/image_09_007.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_09_007.jpg)'
- en: 'Using browser developer tools such as the Network tab, we find that the search
    triggers this AJAX request:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用浏览器开发者工具，如网络标签，我们发现搜索触发了此AJAX请求：
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here, the `maxResults` parameter is set to `99`. However, we can increase this
    to download all locations in a single query, a technique covered in [Chapter 1](py-web-scrp-2e_ch01.html),
    *Introduction to Web Scraping*. Here is the result when `maxResults` is increased
    to `1000`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`maxResults`参数设置为`99`。然而，我们可以将其增加到`1000`以在单个查询中下载所有位置，这是在[第1章](py-web-scrp-2e_ch01.html)，“网络爬取简介”中介绍的技术。当`maxResults`增加到`1000`时，这是结果：
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This AJAX request provides the data in **JSONP** format, which stands for **JSON
    with padding**. The padding is usually a function to call, with the pure JSON
    data as an argument, in this case the `callback` function call. The padding is
    not easily understood by parsing libraries, so we need to remove it to properly
    parse the data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此AJAX请求以**JSONP**格式提供数据，代表**带有填充的JSON**。填充通常是一个要调用的函数，其中纯JSON数据作为参数，在本例中是`callback`函数调用。填充不容易被解析库理解，因此我们需要移除它以正确解析数据。
- en: 'To parse this data with Python''s `json` module, we need to first strip this
    padding, which we can do with slicing:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Python的`json`模块解析此数据，我们首先需要移除此填充，我们可以通过切片来完成：
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We now have all the German BMW dealers loaded in a JSON object-currently, 715
    of them. Here is the data for the first dealer:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已将所有德国宝马经销商加载到JSON对象中-目前有715家。以下是第一家经销商的数据：
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can now save the data of interest. Here is a snippet to write the name and
    latitude and longitude of these dealers to a spreadsheet:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以保存感兴趣的数据。以下是将这些经销商的名称和经纬度写入电子表格的代码片段：
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After running this example, the contents of the `bmw.csv` spreadsheet will
    look similar to this:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此示例后，`bmw.csv`电子表格的内容将类似于以下内容：
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The full source code for scraping this data from BMW is available at [https://github.com/kjam/wswp/blob/master/code/chp9/bmw_scraper.py](https://github.com/kjam/wswp/blob/master/code/chp9/bmw_scraper.py).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 从宝马网站抓取此数据的完整源代码可在[https://github.com/kjam/wswp/blob/master/code/chp9/bmw_scraper.py](https://github.com/kjam/wswp/blob/master/code/chp9/bmw_scraper.py)找到。
- en: Translating foreign content
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译外语内容
- en: You may have noticed that the first screenshot for BMW was in German, but the
    second was in English. This is because the text for the second was translated
    using the Google Translate browser extension. This is a useful technique when
    trying to understand how to navigate a website in a foreign language. When the
    BMW website is translated, the website still works as usual. Be aware, though,
    as Google Translate will break some websites, for example, if the content of a
    select box is translated and a form depends on the original value.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，宝马的第一张截图是德语，而第二张是英语。这是因为第二张截图使用了Google Translate浏览器扩展进行翻译。当尝试理解如何在外语网站上导航时，这是一个有用的技术。当宝马网站被翻译时，网站仍然可以正常工作。不过，请注意，Google
    Translate可能会破坏一些网站，例如，如果选择框的内容被翻译，而表单依赖于原始值。
- en: Google Translate is available as the `Google Translate` extension for Chrome,
    the `Google Translator` add-on for Firefox, and can be installed as the`Google
    Toolbar` for Internet Explorer. Alternatively, [http://translate.google.com](http://translate.google.com)
    can be used for translations; however, this is only useful for raw text as the
    formatting is not preserved.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Google Translate作为Chrome的`Google Translate`扩展、Firefox的`Google Translator`插件，以及可以安装到Internet
    Explorer的`Google Toolbar`提供。或者，可以使用[http://translate.google.com](http://translate.google.com)进行翻译；然而，这仅适用于原始文本，因为格式不会被保留。
- en: Summary
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter analyzed a variety of prominent websites and demonstrated how the
    techniques covered in this book can be applied to them. We used CSS selectors
    to scrape Google results, tested a browser renderer and an API for Facebook pages,
    used a `Sitemap` to crawl Gap, and took advantage of an AJAX call to scrape all
    BMW dealers from a map.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 本章分析了各种知名网站，并展示了本书中涵盖的技术如何应用于这些网站。我们使用CSS选择器抓取谷歌搜索结果，测试了浏览器渲染器和Facebook页面的API，使用`Sitemap`爬取Gap，并利用AJAX调用从地图中抓取所有宝马经销商。
- en: You can now apply the techniques covered in this book to scrape websites that
    contain data of interest to you. As demonstrated by this chapter, the tools and
    methods you have learned throughout the book can help you scrape many different
    sites and content from the Internet. I hope this begins a long and fruitful career
    in extracting content from the Web and automating data extraction with Python!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以将本书中涵盖的技术应用于抓取包含你感兴趣的数据的网站。正如本章所示，你在本书中学到的工具和方法可以帮助你抓取许多不同的网站和互联网上的内容。我希望这能开启一段漫长而富有成效的职业道路，通过Python从网络中提取内容并自动化数据提取！
