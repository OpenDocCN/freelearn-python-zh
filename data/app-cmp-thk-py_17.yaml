- en: '*Chapter 14*: Using Computational Thinking and Python in Statistical Analysis'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第14章*：在统计分析中使用计算思维和Python'
- en: In this chapter, we will use Python and the elements of computational thinking
    to solve problems that require statistical analysis algorithms. We will use **pandas
    DataFrames** to create statistical analysis algorithms within the Python environment.
    Additional packages in Python will be needed to create statistical analyses, such
    as **NumPy**, **pytz**, and more. We will use those packages when they are needed
    for the code we will work with and when learning what the libraries help us do,
    such as organizing data with pandas, for example.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Python和计算思维的元素来解决需要统计分析算法的问题。我们将使用**pandas DataFrames**在Python环境中创建统计分析算法。在Python中还需要其他软件包来创建统计分析，例如**NumPy**、**pytz**等。当我们需要处理的代码和学习这些库帮助我们做什么时，我们将使用这些软件包，比如使用pandas整理数据。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Defining the problem and Python data selection
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义问题和Python数据选择
- en: Preprocessing data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Processing, analyzing, and summarizing data using visualizations
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用可视化处理、分析和总结数据
- en: By the end of this chapter, you will be able to design algorithms that best
    fit the scenarios you are presented with. You will also be able to identify Python
    functions that best align with the problems presented and generalize your solutions.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将能够设计最适合您所面临情况的算法。您还将能够确定与所提出问题最符合的Python函数，并概括您的解决方案。
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need the latest version of Python for running the code in this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要最新版本的Python来运行本章的代码。
- en: You will need to have the **pandas**, **NumPy**, **SciPy**, and **Scikit-Learn**
    packages installed for the problems in this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的问题中，您需要安装**pandas**、**NumPy**、**SciPy**和**Scikit-Learn**软件包。
- en: 'You can find the full source used in this chapter here: [https://github.com/PacktPublishing/Applied-Computational-Thinking-with-Python/tree/master/Chapter14](https://github.com/PacktPublishing/Applied-Computational-Thinking-with-Python/tree/master/Chapter14)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本章中找到使用的完整源代码：[https://github.com/PacktPublishing/Applied-Computational-Thinking-with-Python/tree/master/Chapter14](https://github.com/PacktPublishing/Applied-Computational-Thinking-with-Python/tree/master/Chapter14)
- en: Defining the problem and Python data selection
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义问题和Python数据选择
- en: Before we look at the pandas library, let's define what data analysis is. When
    we talk about data analysis, we are talking about the process of inspecting, cleansing,
    transforming, and modeling data with the intent of discovering useful data, notifying
    conclusions, and supporting decision-making. Decision-making is critical. We don't
    just want to see what the data says has happened in the past. We want to use data
    in order to make informed decisions for the future.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们看pandas库之前，让我们定义一下数据分析是什么。当我们谈论数据分析时，我们指的是检查、清洗、转换和建模数据的过程，目的是发现有用的数据，通知结论，并支持决策。决策是至关重要的。我们不只是想看看数据在过去发生了什么。我们希望利用数据来为未来做出知情决策。
- en: 'Take a look at some of the uses of data analysis:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下数据分析的一些用途：
- en: '**Business**: It helps when making decisions based on customer trends and behavior
    prediction, increasing business productivity, and driving effective decision-making.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**商业**：它有助于基于客户趋势和行为预测做出决策，提高业务生产力，并推动有效的决策。'
- en: '**Weather forecasting**: Data about the atmosphere (temperature, humidity,
    wind, and more) is collected and analyzed to understand atmospheric processes
    (meteorology) to determine how the atmosphere will evolve in the future.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**天气预报**：收集和分析大气数据（温度、湿度、风力等）以了解大气过程（气象学），从而确定大气将来的演变。'
- en: '**Transportation**: Data can be used to determine trends, including traffic,
    accidents, and more, helping us make decisions about traffic patterns, traffic
    light durations, and much more.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交通**：数据可用于确定趋势，包括交通、事故等，帮助我们做出关于交通模式、交通灯持续时间等的决策。'
- en: The aforementioned uses are only some of the possible applications, of course.
    Data analytics is used for a very wide range of things, and by businesses and
    educational organizations to make critical decisions, provide resources to the
    community, fund our schools and colleges, and so much more.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 上述用途只是可能应用的一部分。数据分析被用于非常广泛的事情，由企业和教育组织做出关键决策，为社区提供资源，资助我们的学校和大学，以及更多。
- en: So, let's take a look at what tools we have available to analyze that data.
    One of the main libraries used in data analysis in Python is the pandas package.
    The greatness of pandas lies in its ease of use, easy data structure, and high
    performance. Using pandas simplifies our work in data analysis.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们看看我们有哪些工具可用于分析数据。在Python中用于数据分析的主要库之一是pandas软件包。pandas的伟大之处在于其易用性、简单的数据结构和高性能。使用pandas简化了我们在数据分析中的工作。
- en: Defining pandas
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义pandas
- en: One thing that's important to note is that pandas is built on top of NumPy.
    **NumPy** is a package that helps us work with arrays. Python doesn't have arrays
    per se, so the packages allow us to create them, use them, and then build upon
    that capability.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的一点是pandas是建立在NumPy之上的。**NumPy**是一个帮助我们处理数组的软件包。Python本身并没有数组，因此这些软件包允许我们创建、使用数组，然后在此基础上构建。
- en: Pandas provides a flexible and easy data structure to simplify your work in
    data analysis. It is a great tool for **big data**. When we talk about big data,
    we're talking about structured and unstructured datasets that are analyzed so
    that we can get better insights and aid in decision making or strategies for businesses
    and organizations. pandas can handle importing different formats, such as `.csv`
    files, **SQL**, and **JSON**, as well as all sorts of manipulation, such as selecting
    data, merging, reshaping, and cleaning the data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas提供了一种灵活且简单的数据结构，可以简化您在数据分析中的工作。它是处理**大数据**的强大工具。当我们谈论大数据时，我们指的是结构化和非结构化数据集，这些数据集经过分析，以便我们可以获得更好的见解，并帮助业务和组织制定决策或策略。pandas可以处理导入不同格式的数据，如`.csv`文件、**SQL**和**JSON**，以及各种操作，如选择数据、合并、重塑和清理数据。
- en: 'There are two different ways to store data in pandas—series and DataFrames:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在pandas中有两种不同的存储数据的方式——系列和数据框：
- en: '**Series** are one-dimensional arrays that hold any data type (integer, string,
    or float); series represent one column of data.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Series**是一维数组，可以容纳任何数据类型（整数、字符串或浮点数）；系列代表一列数据。'
- en: '**DataFrames** are two-dimensional objects that can have multiple columns and
    data types. They take inputs such as dictionaries, series, lists, and other DataFrames.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据框**是可以具有多个列和数据类型的二维对象。它接受诸如字典、系列、列表和其他数据框的输入。'
- en: Let's now learn when to use pandas.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们学习何时使用pandas。
- en: Determining when to use pandas
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确定何时使用pandas
- en: Pandas is extremely useful in general, but it is really a great tool when we
    are looking at large data and working with **comma-separated values** (**CSV**)
    files. These files are stored as tables, such as spreadsheets. The other thing
    is that we can establish *chunks* in pandas. Yes, there's a `chunksize` parameter
    in pandas that helps us break down our data. Let's say we have 5,000,000 rows.
    We can decide to use `chunksize` to break that down by 1,000,000 rows.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，Pandas在一般情况下非常有用，但当我们处理大量数据并使用**逗号分隔值**（**CSV**）文件时，它真的是一个很好的工具。这些文件被存储为表格，如电子表格。另一件事是我们可以在pandas中建立*块*。是的，在pandas中有一个`chunksize`参数，可以帮助我们分解数据。假设我们有500万行。我们可以决定使用`chunksize`将其分解为100万行。
- en: In addition, we sometimes have massive data files but only want to look at some
    of the components. Pandas allows us to identify the columns we want to include
    and those we want to ignore.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，有时我们有大量的数据文件，但只想查看其中的一些组件。Pandas允许我们标识要包含的列和要忽略的列。
- en: Working with pandas series
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理pandas系列
- en: 'As mentioned in the previous section—*Defining pandas*—series are one-dimensional.
    We can create an empty pandas series using some simple code. Note that we are
    importing the pandas library first, as is usual when working with packages and
    libraries, as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述——*定义pandas*——系列是一维的。我们可以使用一些简单的代码创建一个空的pandas系列。请注意，通常在使用包和库时，我们首先导入pandas库，如下所示：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The default series created will have a type of `float` because we didn''t establish
    any other type in our algorithm. However, the console prints a warning that in
    the future, empty series **dtypes** will be set as an object rather than `float`.
    `dtype` stands for data type; in this case, it''s a float. Take a look at the
    following screenshot, which shows the output when we run our algorithm:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 默认创建的系列将具有`float`类型，因为我们在算法中没有建立其他类型。然而，控制台打印了一个警告，即在将来，空系列的**dtypes**将被设置为对象，而不是`float`。`dtype`代表数据类型；在这种情况下，它是一个浮点数。看一下以下截图，显示了我们运行算法时的输出：
- en: '![Figure 14.1 – Output when creating empty series in pandas without identifying
    dtype'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.1 – 在pandas中创建空系列时未识别dtype时的输出'
- en: '](image/Figure_14.01_B15413.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Figure_14.01_B15413.jpg)'
- en: Figure 14.1 – Output when creating empty series in pandas without identifying
    dtype
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1 – 在pandas中创建空系列时未识别dtype时的输出
- en: As you can see, there really isn't an error, just a warning about how the data
    was stored.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，这里实际上并没有错误，只是关于数据存储方式的警告。
- en: 'Now, let''s create a series with defined elements. To do that, we''ll need
    to import both pandas and `numpy` so that we can create the arrays and then the
    series. Take a look at the following snippet of code:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个具有定义元素的系列。为此，我们需要导入pandas和`numpy`，以便我们可以创建数组，然后创建系列。看一下以下代码片段：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As you can see from the preceding code, we stored our array using `numpy`,
    then created a series using that array. Finally, we printed the series. The output
    is a table, as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您从前面的代码中看到的，我们使用`numpy`存储了我们的数组，然后使用该数组创建了一个系列。最后，我们打印了系列。输出是一个表格，如下所示：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can also get the exact same thing if we created a list first. Take a look
    at this snippet of code:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过首先创建一个列表来获得完全相同的结果。看一下这段代码：
- en: ch14_seriesDemo2.py
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ch14_seriesDemo2.py
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice that we created the series directly from the list. We're not going to
    show the output for this particular snippet of code because it's exactly the same
    as the previous snippet's output.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们直接从列表中创建了系列。我们不打算展示这段代码的输出，因为它与上一段代码的输出完全相同。
- en: 'We can also get a series from a dictionary. Let''s take a look at that in the
    following snippet of code:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以从字典中获得一个系列。让我们看一下以下代码片段：
- en: ch14_seriesDemo3.py
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ch14_seriesDemo3.py
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding code is just a demo, but we do get a table series that contains
    the values of our dictionary when we run the algorithm. Let''s take a look at
    that output:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码只是一个演示，但当我们运行算法时，我们确实得到了一个包含字典值的表系列。让我们看一下输出：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, we have the two columns based on the key-value pairs, and the
    type, which is `object`. One thing that will become important is that series don't
    have column titles. For that, we'll need to use DataFrames.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们有基于键值对的两列，以及类型为`object`。一个重要的一点是系列没有列标题。为此，我们需要使用数据框。
- en: '*What if we want to access a specific element in a series?* Well, to access
    the first element, let''s use the following snippet of code:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果我们想要访问系列中的特定元素怎么办？* 好吧，要访问第一个元素，让我们使用以下代码片段：'
- en: ch14_demo4.py
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ch14_demo4.py
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output of the preceding code is simply `Miguel`. That''s because we used
    index 0 to identify what we wanted from that dictionary, so it''ll give us the
    value for the first key-value pair. If we wanted the value pairs for the first
    two key-value pair elements, we''d replace `(mySeries[0])` with `(mySeries[:2])`.
    Then, the output would be as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出只是`Miguel`。这是因为我们使用索引0来标识我们从字典中想要的内容，因此它将为我们提供第一个键值对的值。如果我们想要前两个键值对元素的值对，我们将`(mySeries[0])`替换为`(mySeries[:2])`。然后，输出将如下所示：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: There are many other things that can be done with series, so play around with
    the indexes and creating different types of series using lists, dictionaries,
    or NumPy arrays. For now, let's move on to DataFrames.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 系列还有许多其他事情可以做，因此可以尝试使用索引和使用列表、字典或NumPy数组创建不同类型的系列。现在，让我们继续学习数据框。
- en: Working with pandas DataFrames
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用pandas数据框
- en: 'Now, let''s take a look at how we work with DataFrames. First, let''s take
    a look at a `.csv` file with pandas. We''re going to use the `demo.csv` file in
    the snippet of code that follows. Please replace the location of the file to match
    the location where you have saved the file, which can be found in the GitHub repository:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看如何处理数据框。首先，让我们来看一个带有pandas的`.csv`文件。我们将在接下来的代码片段中使用`demo.csv`文件。请将文件的位置替换为您保存文件的位置，该位置可以在GitHub存储库中找到：
- en: ch14_csvDemo.py
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ch14_csvDemo.py
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding code does three things. It imports the pandas package so that
    we can use the data capabilities we need, it tells the program to open the data
    file we''ll be working with, and then it gives us the first few rows of data in
    the file so that we can see what we''re working with. The following screenshot
    shows the result, or output, from the preceding code:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码做了三件事。它导入了pandas包，以便我们可以使用我们需要的数据功能，它告诉程序打开我们将要使用的数据文件，然后它给出了文件中的前几行数据，以便我们可以看到我们正在处理的内容。以下截图显示了前面代码的结果或输出：
- en: '![Figure 14.2 – Output showing the first few rows of the dataset'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.2 - 显示数据集的前几行输出'
- en: '](image/Figure_14.02_B15413.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Figure_14.02_B15413.jpg)'
- en: Figure 14.2 – Output showing the first few rows of the dataset
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2 - 显示数据集的前几行输出
- en: As you can see from the preceding screenshot, the table does not show all the
    values included in our file. While our file is not full of big data, it does include
    more rows of information. This is just a way for us to get a preview of our data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的截图中可以看出，表格未显示文件中包含的所有值。虽然我们的文件不是充满大数据，但它确实包含更多的信息行。这只是让我们预览我们的数据的一种方式。
- en: But this is a *clean* dataset. *What happens if we have a dataset with rows
    or columns that are missing information?* Well, pandas allows us to work with
    the file to prepare it for us. DataFrames also prepare our data so that we can
    create visual representations. These visuals, or plots, will allow us to see trends,
    make predictions, determine what values we can use for training, and much more.
    The DataFrame is really just the backbone of everything else we can then do with
    a dataset.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 但这是一个*干净*的数据集。*如果我们有一个数据集的行或列缺少信息会发生什么？* 好吧，pandas允许我们处理文件以准备好使用。数据框还可以准备我们的数据，以便我们可以创建可视化表示。这些可视化或图表将允许我们看到趋势，进行预测，确定我们可以用于训练的值，等等。数据框实际上只是我们可以用数据集做的其他一切的基础。
- en: In this section, we learned about problems, how to work with pandas, and some
    of the capabilities of pandas series and DataFrames.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们学习了问题，如何使用pandas以及pandas系列和数据框的一些功能。
- en: As a note, for the rest of this chapter, we'll be a lot more focused on DataFrames
    than series. But before we get into an application, let's take a look at how we
    can avoid errors and pitfalls by preprocessing our DataFrame.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种注意，在本章的其余部分，我们将更加专注于数据框而不是系列。但在进入应用程序之前，让我们看看如何通过预处理数据框来避免错误和陷阱。
- en: Preprocessing data
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: '**Preprocessing data** is a technique that transforms raw data into a useable
    and efficient format. It is, in fact, the most important step in the data mining
    and machine learning process.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**预处理数据**是一种将原始数据转换为可用和高效格式的技术。实际上，这是数据挖掘和机器学习过程中最重要的步骤。'
- en: When we are preprocessing data, we are really cleaning it, transforming it,
    or doing a data reduction. In this section, we will take a look at what these
    all mean.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们预处理数据时，我们实际上是在清理它、转换它或进行数据减少。在本节中，我们将看看这些都意味着什么。
- en: Data cleaning
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据清理
- en: '**Data cleaning** refers to the process of making our dataset more efficient.
    If we go through data cleaning in really large datasets, we can expedite the algorithm,
    avoid errors, and get better results. There are two things we deal with when data
    cleaning:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据清理**指的是使我们的数据集更高效的过程。如果我们在真正大型的数据集中进行数据清理，我们可以加快算法，避免错误，并获得更好的结果。在数据清理时，我们处理两件事情：'
- en: '**Missing data**: This can be fixed by ignoring the data or manually entering
    a value for the missing data.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺失数据**：这可以通过忽略数据或手动输入缺失数据的值来解决。'
- en: '**Noisy data**: This can be fixed/improved by using binning, regression, or
    clustering, among other processes.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嘈杂的数据**：这可以通过使用分箱、回归或聚类等其他过程来修复/改进。'
- en: We're going to look at each of these things in more detail.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将更详细地看看这些事情。
- en: Working with missing data
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理缺失数据
- en: 'Let''s take a look at how we deal with missing data. First, we''re going to
    learn how to ignore missing data. We can use pandas to find rows with missing
    values. When we do that, we''re cleaning our dataset. Now, we''re not going to
    go through every method we can use, just one where we get rid of rows with missing
    values. As always, the dataset used is available in our GitHub repository and
    you''ll need to update your file location. Let''s look at the following code snippet:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何处理缺失数据。首先，我们将学习如何忽略缺失数据。我们可以使用pandas找到具有缺失值的行。当我们这样做时，我们正在清理我们的数据集。现在，我们不会介绍我们可以使用的每种方法，只介绍一种可以去除具有缺失值的行的方法。与往常一样，使用的数据集可在我们的GitHub存储库中找到，并且您需要更新文件位置。让我们看看以下代码片段：
- en: ch14_cleaningDemo1.py
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ch14_cleaningDemo1.py
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the preceding code, the first `print` statement is for our own sake so that
    we can see what our dataset looks like. *You''ll never want to do that with huge
    files!* The following screenshot shows the first `print` output:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，第一个`print`语句是为了我们自己看到我们的数据集是什么样子的。*您绝对不会想在大文件中这样做！*以下截图显示了第一个`print`输出：
- en: '![Figure 14.3 – First print statement, the original dataset'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.3 - 第一个打印语句，原始数据集'
- en: '](image/Figure_14.03_B15413.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Figure_14.03_B15413.jpg)'
- en: Figure 14.3 – First print statement, the original dataset
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3 - 第一个打印语句，原始数据集
- en: 'Notice that the **1**, **Blue** column has a value of **NaN** under **Countries**
    and the next column (**2**, **Yellow**) has a missing value under the **Numbers**
    column. When we use `dropna()`, the algorithm will drop the rows with missing
    values. The following screenshot shows the printed statement with the altered
    dataset:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，**1**，**Blue**列在**Countries**下有一个**NaN**值，下一列（**2**，**Yellow**）在**Numbers**列下有一个缺失值。当我们使用`dropna()`时，算法将删除具有缺失值的行。以下截图显示了带有修改后数据集的打印语句：
- en: '![Figure 14.4 – Printed clean dataset'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.4 - 打印干净的数据集'
- en: '](image/Figure_14.04_B15413.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Figure_14.04_B15413.jpg)'
- en: Figure 14.4 – Printed clean dataset
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.4 - 打印干净的数据集
- en: As you can see from the preceding screenshot, the two rows that were missing
    values were eliminated in our new dataset. Now, we could run whatever analysis
    we wanted to for this data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的截图中可以看到，缺少值的两行在我们的新数据集中被消除了。现在，我们可以对这些数据运行任何我们想要的分析。
- en: 'If you wanted to check only one column to verify whether missing values exist,
    you could use the following code snippet:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只想检查一个列以验证是否存在缺失值，您可以使用以下代码片段：
- en: ch14_cleaningDemo2.py
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ch14_cleaningDemo2.py
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Notice in the preceding algorithm that we used the `Countries` column heading
    to verify that particular column. When we run the algorithm, here''s what our
    output looks like:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的算法中，我们使用`Countries`列标题来验证特定列。当我们运行算法时，我们的输出如下：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As you can see, the second row in our dataset has a missing value in the `Countries`
    column.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们的数据集中的第二行在`Countries`列中有一个缺失值。
- en: While we're not going to go into every method, you can also remove columns.
    You can choose to remove rows and/or columns with a certain number of missing
    values. For example, you could choose to remove only rows or columns that have
    more than two missing values. If you did that, you'd still need to worry about
    the values that may still be missing in columns or rows that were not removed
    because there was only one missing value. For those, you may choose to replace
    the missing values with something.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不会详细介绍每种方法，但您也可以删除列。您可以选择删除具有一定数量缺失值的行和/或列。例如，您可以选择仅删除具有两个以上缺失值的行或列。如果这样做，您仍然需要担心可能仍然缺少值的列或行，因为只有一个缺失值而未被删除。对于这些情况，您可以选择用其他值替换缺失值。
- en: 'Let''s say you want to replace a value given the column. To do so, let''s take
    a look at the following code snippet:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您想要根据列替换一个值。要这样做，让我们看一下以下代码片段：
- en: ch14_cleaningDemo3.py
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ch14_cleaningDemo3.py
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'From the preceding code, notice that we are filling each empty cell with the
    value `0`. When we run the algorithm, we get the following output:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中可以注意到，我们正在用值`0`填充每个空单元格。当我们运行算法时，我们得到以下输出：
- en: '![Figure 14.5 – Replaced missing values'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.5 - 替换缺失值'
- en: '](image/Figure_14.05_B15413.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Figure_14.05_B15413.jpg)'
- en: Figure 14.5 – Replaced missing values
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.5 - 替换缺失值
- en: Notice the highlighted values in the preceding screenshot. Those are the values
    replaced by our algorithm. Now, let's take a look at how we deal with noisy data.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意前面截图中突出显示的值。这些是我们的算法替换的值。现在，让我们看一下我们如何处理嘈杂数据。
- en: Working with noisy data
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理嘈杂数据
- en: First, let's define what we mean by **noisy data**. When we have a really large
    amount of data and some of it is not useful for our analysis, we say it is noisy
    data. Noisy data is also used to refer to data corruption. *Really, it's just
    useless data*.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义一下**嘈杂数据**的含义。当我们有大量数据，其中一些对我们的分析没有用处时，我们称之为嘈杂数据。嘈杂数据也用于指代数据损坏。*实际上，它只是无用的数据*。
- en: 'Three of the ways that we deal with noisy data are binning, regression, and
    clustering:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们处理嘈杂数据的三种方法是分箱、回归和聚类：
- en: '**Binning** uses neighboring data to smoothen a sorted data value. The sorted
    values go in bins, which are groups created within the algorithm.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分箱**使用相邻数据来平滑排序后的数据值。排序后的值放入箱中，这些箱是算法内创建的组。'
- en: The **clustering** method identifies and removes outliers in a dataset.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**方法识别并移除数据集中的异常值。'
- en: The **regression** method smoothens data by fitting it into regression functions.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**方法通过将数据拟合到回归函数中来平滑数据。'
- en: The purpose of binning is to reduce some errors. In binning, data is divided
    into small buckets or bins. The data is then replaced with a calculated bin value.
    When we go through the binning process, we are smoothing the data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 分箱的目的是减少一些错误。在分箱中，数据被分成小的桶或箱。然后用计算出的箱值替换数据。当我们进行分箱过程时，我们正在平滑数据。
- en: 'Here''s an example with a simple, numerical dataset in the algorithm. The following
    snippet of code will create bins with equal frequency:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是算法中一个简单的数值数据集的示例。以下代码片段将创建具有相等频率的箱：
- en: ch14_binning1.py
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ch14_binning1.py
- en: '[PRE13]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'When looking at the preceding code, you can see that the number of bins is
    defined as `5`, so the data will be binned into five lists. Binning really is
    a way to group information. We tell the algorithm we want to do it and how many
    bins we want, and it provides the data in those bins, or groups. In this case,
    we get those five lists. Take a look at the output:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看前面的代码时，您可以看到箱的数量被定义为`5`，因此数据将被分成五个列表。分箱实际上是一种分组信息的方法。我们告诉算法我们想要这样做以及我们想要多少个箱，它就会提供这些箱或组中的数据。在这种情况下，我们得到了这五个列表。看一下输出：
- en: '[PRE14]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see, the algorithm created five bins with three values in each of
    the bins.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，算法创建了五个箱，每个箱中有三个值。
- en: 'Important Note:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示：
- en: Note that the binning process doesn't organize our data for us. So, if we reordered
    our values, they would still be binned in the order that the data was entered.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，分箱过程并不为我们组织数据。因此，如果我们重新排序我们的值，它们仍然会按照数据输入的顺序进行分箱。
- en: 'Now, let''s take a look at a binning algorithm that uses equal width:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一个使用等宽度的分箱算法：
- en: ch14_binning2.py
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ch14_binning2.py
- en: '[PRE15]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The preceding snippet of code breaks down our data into three bins. The goal
    of equal width binning is to divide the dataset into bins of equal size, which
    in the case of equal width binning means equal range. The data will be split,
    but it is important to note that we''re talking about range here, so the bins
    won''t have the same number of elements in each of them for this particular dataset.
    The output for the preceding snippet of code is as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 上面代码片段将我们的数据分成了三个箱。等宽度分箱的目标是将数据集分成相等大小的箱，这在等宽度分箱的情况下意味着相等的范围。数据将被分割，但重要的是要注意，我们在这里谈论的是范围，因此对于这个特定数据集，这些箱中的元素数量不会相同。上面代码片段的输出如下：
- en: '[PRE16]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As you can see, the binning produces an output that doesn't look quite as clean
    as the equal frequency output, but is actually more popular.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，分箱产生的输出看起来并不像等频输出那样干净，但实际上更受欢迎。
- en: Now, let's talk about transforming data.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们谈谈数据转换。
- en: Transforming data
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换数据
- en: 'Pandas allows us to transform data. Here are some of the ways that we can transform
    it:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas允许我们转换数据。以下是一些我们可以转换数据的方法：
- en: '**Normalization** transforms values into new range; the most popular is **min-max
    normalization**, given as follows:![](image/Formula_B15413_14_001.jpg)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归一化**将值转换为新的范围；最流行的是**最小-最大归一化**，如下所示：![](image/Formula_B15413_14_001.jpg)'
- en: '**Attribute selection** is the process of transforming data by replacing an
    attribute with a different attribute or attributes.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**属性选择**是通过用不同的属性或属性替换属性来转换数据的过程。'
- en: '**Concept hierarchy** is actually a transformation done by reducing data. It
    is done by replacing concepts such as numbers (*10*, *15*, *40*) with higher-level
    concepts, such as qualifiers (*short*, *lengthy*, *extremely lengthy*).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概念层次结构**实际上是通过减少数据来进行的转换。它是通过用更高级的概念（如*短*、*长*、*极长*）替换数字（*10*、*15*、*40*）等概念来完成的。'
- en: In the next section, we will glance through the reduction of data.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将浏览数据的减少。
- en: Reducing data
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 减少数据
- en: '**Data reduction** refers to a process that allows us to get similar or even
    the same results from a dataset but only after having reduced the representation
    of the data in volume.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 数据减少是指一种过程，允许我们从数据集中减少数据的表示，但只有在减少数据的体积后才能获得类似甚至相同的结果。
- en: 'We won''t go into too much depth with all the concepts here because they are
    much easier to look at in examples, but here are some of the ways that data reduction
    is done:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在这里深入讨论所有概念，因为在示例中查看它们会更容易，但以下是一些数据减少的方法：
- en: Removing invalid data from the dataset
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据集中删除无效数据
- en: Creating summaries for the data at different levels
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为不同级别的数据创建汇总
- en: Think of removing invalid data as taking care of any outliers. The data may
    have been entered incorrectly, conditions may have not been optimal, or similar.
    When we have a data point that doesn't fit the entirety of the dataset, especially
    where we have a large number of data points to compare it to, we can remove that
    data point as invalid or as an outlier.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 将无效数据移除可以看作是处理任何异常值。数据可能已被错误输入，条件可能不是最佳的，或者类似情况。当我们有一个数据点与整个数据集不符合时，特别是当我们有大量数据点进行比较时，我们可以将该数据点作为无效数据或异常值移除。
- en: When creating summaries at different levels, we are aggregating the dataset
    and testing and producing summaries at each of those levels. Say you had 100 data
    points (datasets will often be in the thousands, but it's easier to explain with
    smaller numbers). We could create a summary for the first 20 data points. Then,
    we could do the same for the first 40, then the first 60, and so on. When compared,
    we could see the trends and use those smaller sub-sections of our dataset to make
    our predictions if the trends hold true. That's what data reduction helps with,
    simplifying our dataset while still getting accurate results.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建不同级别的汇总时，我们对数据集进行聚合，并在每个级别进行测试和生成汇总。假设你有100个数据点（数据集通常会有数千个，但用较小的数字更容易解释）。我们可以为前20个数据点创建一个汇总。然后，我们可以对前40个数据点做同样的操作，然后是前60个，依此类推。通过比较，我们可以看到趋势，并利用数据集的这些较小子部分来进行预测，如果趋势成立的话。这就是数据减少的作用，简化数据集的同时仍然获得准确的结果。
- en: In this section, we learned how to work with data that needs to be cleaned.
    We learned some of the ways that we can clean data, such as eliminating missing
    data or replacing missing points. We also learned about noisy data and how to
    address problems with noisy data in our datasets. Finally, we learned about data
    reduction and how we can get accurate results by removing invalid data and creating
    aggregate summaries of our data.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们学习了如何处理需要清理的数据。我们学习了一些清理数据的方法，比如消除缺失数据或替换缺失点。我们还了解了嘈杂数据以及如何解决数据集中的嘈杂数据问题。最后，我们学习了数据减少以及如何通过删除无效数据和创建数据的汇总来获得准确的结果。
- en: This is just an introduction to the types of things we do when we're working
    with data. So, let's look at an example so that we can put some of this into context.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是我们处理数据时所做的一些类型的简介。所以，让我们看一个例子，以便我们能把其中一些放入上下文中。
- en: Processing, analyzing, and summarizing data using visualizations
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用可视化处理、分析和汇总数据
- en: We're working in real estate now, and since we want to do well, we really want
    to build an algorithm that helps us analyze data and predict housing prices. But
    let's think about that for a second. We can define that problem very broadly or
    narrowly. We can do a pricing analysis for all houses in a state or houses with
    three bedrooms or more in a neighborhood. *Does performing the analysis matter?
    Maybe*. *But isn't that why we want to look at this problem?*
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在在房地产领域工作，因为我们想做得好，我们真的想建立一个帮助我们分析数据并预测房价的算法。但是让我们再想一想。我们可以广义地或狭义地定义这个问题。我们可以对一个州的所有房屋或一个社区中三个或更多卧室的房屋进行定价分析。*进行分析重要吗？也许*。*但这难道不是我们想要研究这个问题的原因吗？*
- en: Let's take a look at how we can process the data first.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先来看看我们如何处理数据。
- en: Processing data
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理数据
- en: 'Let''s start by gathering some data. For this problem, we''re using the `kv_house_data.csv`
    dataset, which is available in our GitHub repository. To look at this dataset,
    we''ll need quite a few libraries. We''ve been talking about pandas mostly, yes,
    but we want to also do visualizations and perform some analysis, so we''ll need
    **Seaborn**, **SciPy**, and **Scikit-Learn**. The full algorithm can be found
    in the `ch14_housePrice_prediction.py` file. We''ll look at it in snippets to
    discuss what we''re doing along the way:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先收集一些数据。对于这个问题，我们使用的是`kv_house_data.csv`数据集，它可以在我们的GitHub存储库中找到。要查看这个数据集，我们需要很多库。我们主要谈论的是Pandas，是的，但我们还想进行可视化和一些分析，所以我们还需要**Seaborn**、**SciPy**和**Scikit-Learn**。完整的算法可以在`ch14_housePrice_prediction.py`文件中找到。我们将逐步查看它，讨论我们在进行过程中所做的事情：
- en: ch14_housePrice_prediction.py
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ch14_housePrice_prediction.py
- en: '[PRE17]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '*So, what are all these libraries?* Pandas we know about. We''re using that
    for organizing our data. NumPy helps us with the arrays. Seaborn and Matplotlib
    are both used for visualizations. If we were taking this further to create models
    by training using the dataset, we''d also need Scikit-Learn. For this example,
    we''re going to stick with some of the plots we can get before training.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*那么这些库都是什么？*我们知道Pandas。我们使用它来组织我们的数据。NumPy帮助我们处理数组。Seaborn和Matplotlib都用于可视化。如果我们要进一步创建模型并使用数据集进行训练，我们还需要Scikit-Learn。对于这个例子，我们将坚持在训练之前获得一些图表。'
- en: 'Now, let''s import our dataset. Remember from [*Chapter 12*](B15413_12_Final_SK_ePub.xhtml#_idTextAnchor159),
    *Using Python in Experimental and Data Analysis Problems*, that you can set the
    directory of your file directly. You can also provide the entire path to the data
    file, as we have done previously in this chapter, such as in the cleaning demo.
    You can use `os.chdir()` to establish the directory, adding the location of your
    file in parentheses, then use the following code snippet to read the `.csv` file:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们导入我们的数据集。还记得[*第12章*](B15413_12_Final_SK_ePub.xhtml#_idTextAnchor159)中提到的吗，*在实验和数据分析问题中使用Python*，您可以直接设置文件的目录。您也可以提供数据文件的完整路径，就像我们在本章中之前做过的那样，比如在清理演示中。您可以使用`os.chdir()`来建立目录，将文件的位置添加到括号中，然后使用以下代码片段来读取`.csv`文件：
- en: '[PRE18]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We are using a pandas function here. *See that* `pd.read_csv()`*?* That `pd`
    is pandas, since we imported pandas as `pd`, and `read_csv()` is the function
    that allows the algorithm to get the information in the file.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用了一个pandas函数。*看到那个* `pd.read_csv()`*了吗*？那个`pd`就是pandas，因为我们将pandas导入为`pd`，而`read_csv()`是允许算法获取文件中信息的函数。
- en: 'If you forget to enter your directory or include the wrong location for it,
    you will receive an error code, as seen in the following screenshot:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您忘记输入您的目录或者包含错误的位置，您将会收到一个错误代码，如下面的截图所示：
- en: '![Figure 14.6 – Path to file error'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.6 - 文件路径错误'
- en: '](image/Figure_14.06_B15413.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Figure_14.06_B15413.jpg)'
- en: Figure 14.6 – Path to file error
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6 - 文件路径错误
- en: As you can see, Python will make sure you know that you've made a mistake. *That
    can get aggravating at times, but it is certainly helpful.*
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，Python会确保您知道自己犯了一个错误。*有时这可能会让人恼火，但肯定是有帮助的。*
- en: Now that we have our data, we'll need to check it and clean it. This is where
    all the content we shared previously in the chapter comes into play. Let's see
    what that looks like in practice.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了我们的数据，我们需要检查并清理它。这就是我们之前在本章中分享的所有内容发挥作用的地方。让我们看看实际情况是什么样子。
- en: Analyzing and summarizing data
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析和总结数据
- en: 'Now, there''s something we haven''t really talked about much, and that''s Python''s
    variable explorer. More specifically, it''s **Spyder''s Python variable explorer**.
    Spyder is an integrated environment and is free of charge. It works with Python,
    running Python as usual, but also provides us with better editing tools. The following
    screenshot shows how it should look when you import your dataset from the variable
    explorer in Python. When we run the `ch14_housePrice_prediction.py` Python algorithm
    in Spyder, we can see our variables in the variable explorer. The following screenshot
    shows the data we get from the variable explorer when we run this algorithm:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有一件事我们并没有谈论太多，那就是Python的变量资源管理器。更具体地说，是**Spyder的Python变量资源管理器**。Spyder是一个集成环境，是免费的。它可以与Python一起工作，像平常一样运行Python，但也为我们提供了更好的编辑工具。下面的截图显示了当您从Python的变量资源管理器中导入数据集时应该看起来的样子。当我们在Spyder中运行`ch14_housePrice_prediction.py`
    Python算法时，我们可以在变量资源管理器中看到我们的变量。下面的截图显示了当我们运行这个算法时从变量资源管理器中获取的数据：
- en: '![Figure 14.7 – Variable explorer view in Spyder'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.7 - Spyder中的变量资源管理器视图'
- en: '](image/Figure_14.07_B15413.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Figure_14.07_B15413.jpg)'
- en: Figure 14.7 – Variable explorer view in Spyder
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.7 - Spyder中的变量资源管理器视图
- en: When we are dealing with a lot of data and larger algorithms, this tool becomes
    really critical. We can get a lot of information from just this tool. For example,
    let's look at the `housing_data` variable. In *Figure 14.7*, you can see that
    the type for this variable in our algorithm is **DataFrame** with a size of **(21613,
    21)**.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理大量数据和更大的算法时，这个工具变得非常关键。我们可以从这个工具中获得很多信息。例如，让我们看看`housing_data`变量。在*图14.7*中，您可以看到我们算法中这个变量的类型是**DataFrame**，大小为**(21613,
    21)**。
- en: 'If you double-click on the variable in the variable explorer, you get what''s
    shown in the following screenshot (please note that the screenshot may look different
    depending on the environment you are using. When running this code using environments
    such as Spyder or Jupyter, depending on your theme settings and choices, the table
    may look different, with different color schemes or no color schemes):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在变量资源管理器中双击变量，你会得到以下截图中显示的内容（请注意，截图可能会因你使用的环境而有所不同。在使用Spyder或Jupyter等环境运行此代码时，取决于你的主题设置和选择，表格可能会有所不同，有不同的颜色方案或没有颜色方案）：
- en: '![Figure 14.8 – DataFrame variable view in Spyder'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.8 - Spyder中的DataFrame变量视图'
- en: '](image/Figure_14.08_B15413.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Figure_14.08_B15413.jpg)'
- en: Figure 14.8 – DataFrame variable view in Spyder
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.8 - Spyder中的DataFrame变量视图
- en: This is only one way to get some of the information for our DataFrame. Spyder
    allows us to resize the window as well, so we can take a look at more columns,
    scroll through them to find values, and so on. It's not that easy if we're in
    the Python console. You can get the information, just not as easily.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是获取DataFrame的一些信息的一种方式。Spyder还允许我们调整窗口大小，这样我们可以查看更多列，滚动查找值等。如果我们在Python控制台中，这并不容易。你可以获取信息，只是不那么容易。
- en: 'Here''s the code that can give us some of the information:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是可以为我们提供一些信息的代码：
- en: '[PRE19]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The preceding code will show us the first five rows of our dataset. Take a
    look at the following screenshot; we have ellipses (**…**) between **price** and
    **long**. That''s because Python wants to let us know that there are additional
    columns between those two:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将显示我们数据集的前五行。看一下以下截图；在**价格**和**经度**之间有省略号（**…**）。这是因为Python想让我们知道这两者之间还有其他列：
- en: '![Figure 14.9 – First few rows of the DataFrame'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.9 - DataFrame的前几行'
- en: '](image/Figure_14.09_B15413.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Figure_14.09_B15413.jpg)'
- en: Figure 14.9 – First few rows of the DataFrame
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.9 - DataFrame的前几行
- en: 'As you can see, the rows help us see what our dataset looks like, but nothing
    else. So, we can also take a look at the size of our DataFrame by using the following
    code:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，行帮助我们看到我们的数据集是什么样子，但没有其他信息。因此，我们还可以使用以下代码查看我们的DataFrame的大小：
- en: '[PRE20]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'When we run the preceding code, we get the following output:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行上述代码时，我们会得到以下输出：
- en: '[PRE21]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As you can see, now we have the shape or size of our DataFrame. *What does it
    mean?* It means we have `21,613` rows of data in `21` columns. Regardless of whether
    you were in Spyder, the Python console, or another environment of your choice,
    you can see that your data was successfully imported.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，现在我们有了DataFrame的形状或大小。*这是什么意思？*这意味着我们有`21,613`行数据，`21`列。无论你是在Spyder、Python控制台还是其他你选择的环境中，你都可以看到你的数据已成功导入。
- en: 'Now that we have the raw data imported, let''s see whether we can get more
    information. We can use the following code to get a summary:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经导入了原始数据，让我们看看是否可以获得更多信息。我们可以使用以下代码来获取摘要：
- en: '[PRE22]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `describe()` function generates a summary that includes details such as
    the mean, standard deviation, and percentile. This percentile is a part of your
    five-number summary for datasets, used to create **boxplots**. While we won''t
    create a boxplot in this problem, that visual representation can be helpful depending
    on our goals for our algorithms. Take a look at the following screenshot for the
    results of using the `describe()` function:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`describe()`函数生成一个包括均值、标准差和百分位数等细节的摘要。这个百分位数是数据集五数概括的一部分，用于创建**箱线图**。虽然我们不会在这个问题中创建箱线图，但这种可视化表示对我们算法的目标可能有所帮助。看一下使用`describe()`函数的结果的以下截图：'
- en: '![Figure 14.10 – Using the describe() function'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.10 - 使用describe()函数'
- en: '](image/Figure_14.10_B15413.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Figure_14.10_B15413.jpg)'
- en: Figure 14.10 – Using the describe() function
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.10 - 使用describe()函数
- en: 'Now, we should note that the function analyzes numeric values from the DataFrame.
    It excludes `NaN` values. `NaN` stands for **Not a Number** values in Python and
    represents any value that is undefined or unrepresentable. `NaN` can also represent
    missing numbers. We talked about those missing numbers earlier in the *Working
    with missing data* section and some of the ways we could tackle them. Let''s look
    at that in context now. We want to find our missing values. For that, we can run
    the following code snippet:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们应该注意，该函数分析DataFrame中的数值。它排除了`NaN`值。`NaN`代表Python中的**不是数字**值，并表示任何未定义或不可表示的值。`NaN`也可以表示缺失的数字。我们之前在*处理缺失数据*部分讨论过这些缺失的数字以及我们可以解决它们的一些方法。现在让我们在这个背景下看一下。我们想找到我们的缺失值。为此，我们可以运行以下代码片段：
- en: '[PRE23]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The preceding snippet will let us know if there are missing data points in
    each of the columns in our dataset. Then, it will aggregate the values as a sum.
    So, if we had two missing values under the `date` column, we''d expect to see
    a `2` there. Our results can be seen in the following screenshot:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的片段将让我们知道我们的数据集中每个列中是否有缺失数据点。然后，它将聚合值作为总和。因此，如果在“日期”列下有两个缺失值，我们期望在那里看到“2”。我们的结果可以在以下截图中看到：
- en: '![Figure 14.11 – Results from running the isnull() and .sum() functions'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.11 - 运行isnull()和.sum()函数的结果'
- en: '](image/Figure_14.11_B15413.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Figure_14.11_B15413.jpg)'
- en: Figure 14.11 – Results from running the isnull() and .sum() functions
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.11 - 运行isnull()和.sum()函数的结果
- en: '*We don''t have to clean this dataset!* *That''s a clean dataset*. However,
    learning how to identify whether or not we would need to do so is really important.
    Now that we know that we have a clean dataset, we can start working on building
    some visualizations.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们不必清理这个数据集！* *这是一个干净的数据集*。然而，学习如何识别我们是否需要这样做非常重要。现在我们知道我们有一个干净的数据集，我们可以开始构建一些可视化。'
- en: Using data visualization
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用数据可视化
- en: 'Remember that this is still part of our initial algorithm file, `ch15_housePrice_prediction.py`.
    If you open that file, the code that follows starts at *line 25*. We have added
    comments for descriptions between the lines of code:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这仍然是我们初始算法文件`ch15_housePrice_prediction.py`的一部分。如果你打开该文件，接下来的代码从*第25行*开始。我们在代码行之间添加了描述性的注释：
- en: '[PRE24]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The first thing we''re doing is identifying the columns that we''ll be using
    for our plot. After we do that, we''ll make a DataFrame and save it as `df`:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的第一件事是确定我们将用于图表的列。在这样做之后，我们将制作一个DataFrame并将其保存为`df`：
- en: '[PRE25]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The preceding code snippet creates the correlations for our DataFrame. In the
    next code snippet from the file, we''ll create our figure; that is, we''ll work
    on the data visualization:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段创建了我们DataFrame的相关性。在文件的下一个代码片段中，我们将创建我们的图表；也就是说，我们将进行数据可视化：
- en: '[PRE26]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In the preceding code, we named our figure, added the subplot, and identified
    our colors. Next, we''ll have to set some of our properties, such as tick marks,
    the distance between tick marks, and labels for axes and tick marks:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们命名了我们的图表，添加了子图，并确定了我们的颜色。接下来，我们需要设置一些属性，比如刻度标记、刻度标记之间的距离以及轴和刻度标记的标签：
- en: '[PRE27]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'After we''ve set some of the properties, we can ask for the plot to use `tight_layout()`.
    This will help us see all the details of the plot and labels. If we do not use
    `tight_layout()`, we''ll sometimes have some labels that will not be visible in
    our graphs:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置了一些属性之后，我们可以要求图表使用`tight_layout()`。这将帮助我们看到图表和标签的所有细节。如果我们不使用`tight_layout()`，有时在我们的图表中会有一些标签是不可见的：
- en: '[PRE28]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In the preceding snippet, we also created a save file and defined the figure''s
    size. Finally, we asked the algorithm to show us the correlations. The following
    screenshot shows us the result of the preceding code:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们还创建了一个保存文件并定义了图表的大小。最后，我们要求算法显示我们的相关性。以下截图显示了前面代码的结果：
- en: '![Figure 14.12 – Correlations plot for housing data'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.12 - 住房数据的相关性图'
- en: '](image/Figure_14.12_B15413.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Figure_14.12_B15413.jpg)'
- en: Figure 14.12 – Correlations plot for housing data
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.12 - 住房数据的相关性图
- en: As you can see, we just created a **correlation matrix** using Python. And it's
    a pretty great matrix. *But what is a correlation matrix?* A correlation matrix
    looks at all the values in our DataFrame and then calculates how closely they
    are correlated, giving a value between **-1** and **1**. The closer the value
    is to **1**, the more correlated the values are. Each value is compared to itself,
    which is a perfect correlation, of course, and seen as the diagonal in our matrix.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们刚刚使用Python创建了一个**相关性矩阵**。而且这是一个相当不错的矩阵。*但是什么是相关性矩阵呢？* 相关性矩阵查看我们DataFrame中的所有值，然后计算它们之间的相关程度，给出一个在**-1**和**1**之间的值。数值越接近**1**，这些值之间的相关性就越高。每个值与自身进行比较，这是完美的相关性，当然，在我们的矩阵中被视为对角线。
- en: 'The rest of the graph, where all the values are compared to each other, has
    to be looked at more closely. The closer to the yellow color, the closer the values
    are correlated. So, take a look at the *y*-axis **sqft_above** value and the corresponding
    value for **sqft_living** on the *x* axis. That value is close to the yellow value
    of **1**, but not quite. We''ve highlighted those values in the following screenshot:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的图表，所有的值都相互比较，必须更仔细地观察。越接近黄色，值之间的相关性就越高。因此，看一下*y*轴的**sqft_above**值和*x*轴上**sqft_living**的对应值。该值接近于黄色值**1**，但不完全相等。我们在以下截图中突出显示了这些值：
- en: '![Figure 14.13 – Highlighted correlation of sqft_above and sqft_living'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.13 - 突出显示的sqft_above和sqft_living的相关性'
- en: '](image/Figure_14.13_B15413.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Figure_14.13_B15413.jpg)'
- en: Figure 14.13 – Highlighted correlation of sqft_above and sqft_living
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.13 - 突出显示的sqft_above和sqft_living的相关性
- en: There are other values that show some correlation, but not quite strong enough.
    This plot helps us then make decisions to maybe take a closer look at that correlation,
    find more information about it, and so on.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他一些值显示出一定的相关性，但不够强。这个图表帮助我们做出决策，也许要更仔细地观察这种相关性，找到更多关于它的信息等。
- en: 'Now, we can also take a look at a pretty intense plot matrix that''s called
    *pair plotting*. A **pairs plot** shows us the distribution of single variables
    and the relationships between two variables. If we ran a pairs plot for the data
    that is included in our algorithm, we get a massive graph, as shown in the following
    screenshot (please note that this plot can take up to a few minutes to generate
    because of the amount of data being analyzed and plotted):'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们还可以看一下一个相当密集的图表矩阵，称为*对图*。**对图**向我们展示了单个变量的分布以及两个变量之间的关系。如果我们对我们算法中包含的数据运行对图，我们会得到一个庞大的图表，如下截图所示（请注意，由于分析和绘制的数据量较大，这个图表可能需要几分钟的时间来生成）：
- en: '![Figure 14.14 – Full DataFrame pairs plot'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.14 - 完整的DataFrame对图'
- en: '](image/Figure_14.14_B15413.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Figure_14.14_B15413.jpg)'
- en: Figure 14.14 – Full DataFrame pairs plot
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.14 - 完整的DataFrame对图
- en: As you can see, this is a fairly intense, hard-to-read, almost-impossible-to-analyze
    graph. You'd have to zoom in to each of the paired figures to make some determinations.
    It should also be mentioned that it takes time for the algorithm to run and produce
    this graphic. *It takes some processing power to create something so complex from
    such a large dataset!*
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，这是一个相当密集的、难以阅读的、几乎不可能分析的图表。你需要放大到每对图形来做出一些决定。还应该提到的是，算法需要时间来运行并生成这个图形。*从如此庞大的数据集中创建如此复杂的东西需要一定的处理能力！*
- en: '*Do we ever want to see a big plot of graphs such as this one?* *Actually,
    yes*. *If we have a dataset with fewer variables, absolutely!* We can also make
    this more friendly by using other color schemes, for example. This may make identifying
    trends easier. But let''s be clear; this isn''t very helpful. What we can do is
    create a pairs plot of some of the variables we may be interested in. Remember
    we talked about `sqft_living` and`sqft_above` possibly having a strong positive
    correlation? We also really do want to compare things to pricing, right? So, let''s
    create a pairs plot using just `sqft_living`, `pricing`, and `sqft_above`. Take
    a look at the relevant code snippet from our file:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否想要看到像这样的大量图表？实际上是的。如果我们有较少的变量数据集，那绝对可以！我们还可以使用其他颜色方案来使其更加友好，例如。这可能会使识别趋势变得更容易。但让我们明确一点；这并不是非常有帮助。我们可以创建一些感兴趣的变量的成对图。记得我们谈到过`sqft_living`和`sqft_above`可能有很强的正相关性吗？我们也确实想要将事物与定价进行比较，对吧？所以，让我们只使用`sqft_living`、`pricing`和`sqft_above`创建一个成对图。看一下我们文件中相关代码片段：
- en: '[PRE29]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, when we run this part of the algorithm, we get the graph shown in the
    following screenshot. This graph provides the correlations for these three values
    and we can definitely see some positive correlations happening:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们运行算法的这一部分时，我们得到了下面截图中显示的图表。这个图表提供了这三个值的相关性，我们确实可以看到一些正相关发生：
- en: '![Figure 14.15 – Pairs plot of price, sqft_living, and sqft_above'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.15 - 价格、sqft_living和sqft_above的成对图'
- en: '](image/Figure_14.15_B15413.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Figure_14.15_B15413.jpg)'
- en: Figure 14.15 – Pairs plot of price, sqft_living, and sqft_above
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.15 - 价格、sqft_living和sqft_above的成对图
- en: Notice in particular the graphs that pair **sqft_living** and **sqft_above**.
    The relationship between those two is fairly linear and positive. This confirms
    what we observed from *Figure 14.13*, where the correlation was closer to **1**
    than the other variables.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 特别注意一下成对图中的**sqft_living**和**sqft_above**。这两者之间的关系相当线性和正向。这证实了我们从*图14.13*中观察到的情况，其中相关性比其他变量更接近**1**。
- en: 'But it would also help to analyze another three variables so that we can see
    what happens when the correlation is not strong. We''ll keep `price` and `sqft_living`
    so that we are only changing one of the variables for comparison purposes. Looking
    at *Figure 14.12*, **sqft_living** and **zipcode** don''t seem to have a strong
    positive correlation at all. So, let''s run the algorithm again, swapping `zipcode`
    with `sqft_above`. Let''s take a look at the result shown in the following screenshot:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，分析另外三个变量也会有所帮助，这样我们就可以看到当相关性不强时会发生什么。我们将保留`price`和`sqft_living`，这样我们只改变一个变量进行比较。看一下*图14.12*，**sqft_living**和**zipcode**似乎根本没有强烈的正相关性。因此，让我们再次运行算法，将`zipcode`替换为`sqft_above`。让我们看一下下面截图中显示的结果：
- en: '![Figure 14.16 – Pairs plot of price, sqft_living, and zipcode'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.16 - 价格、sqft_living和zipcode的成对图'
- en: '](image/Figure_14.16_B15413.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Figure_14.16_B15413.jpg)'
- en: Figure 14.16 – Pairs plot of price, sqft_living, and zipcode
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.16 - 价格、sqft_living和zipcode的成对图
- en: As you can see, **sqft_living** and **zipcode** show no correlation at all.
    They look more like bar graphs, with no diagonal in sight. Before we move on from
    these plots, it's worth mentioning that these pairs plots only provide scatterplots
    for the compared variables and histograms for each of the variables in the plots.
    If we wanted to go deeper, we could look at other visualization tools, such as
    those in **Seaborn**.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，**sqft_living**和**zipcode**根本没有相关性。它们看起来更像条形图，看不到对角线。在我们离开这些图之前，值得一提的是，这些成对图仅为比较的变量提供散点图，并为图中的每个变量提供直方图。如果我们想深入了解，我们可以查看其他可视化工具，比如**Seaborn**中的工具。
- en: We're pausing this analysis here. We've used visualization to understand where
    our data has correlations. If we were to take this problem further, we could use
    the data to create training models and help us in predictions. Even with the data
    we have from the graphics, we can see our correlations and make predictions using
    that. If we had `sqft_living`, we could predict `sqft_above`, for example, because
    of their strong correlation.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里暂停分析。我们使用可视化来了解我们的数据有哪些相关性。如果我们进一步处理这个问题，我们可以使用数据来创建训练模型，并帮助我们进行预测。即使只是从图形数据中，我们也可以看到我们的相关性，并利用它进行预测。例如，如果我们有`sqft_living`，我们可以预测`sqft_above`，因为它们之间有很强的相关性。
- en: Python allows us to look at data in a wide variety of ways. That's one of the
    great assets of a tool such as this one.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: Python允许我们以多种方式查看数据。这是这样一个工具的伟大优势之一。
- en: Summary
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we've learned about how to work with data in Python and tackled
    a housing dataset using some of the concepts we learned about in the chapter.
    We learned about the pandas package and how it helps us organize and prepare data.
    We also learned about the need to preprocess datasets, especially in very large
    datasets. We worked through missing and noisy data, as well as data transformation
    and the reduction of data. We also learned how to use visualization, creating
    plots for our datasets that can aid us in identifying correlations and trends.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用Python处理数据，并使用本章学到的一些概念来处理房屋数据集。我们了解了pandas包以及它如何帮助我们组织和准备数据。我们还了解了预处理数据集的必要性，特别是在非常大的数据集中。我们处理了缺失和嘈杂的数据，以及数据转换和数据减少。我们还学会了如何使用可视化，为我们的数据集创建图表，可以帮助我们识别相关性和趋势。
- en: The topics in this chapter are pretty broad, with entire books written about
    them. But we felt it important to share some of the capabilities of the Python
    programming language before moving on to the next two chapters of the book.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主题非常广泛，有整本书专门讨论这些主题。但在我们继续本书的下两章之前，我们觉得有必要分享一些Python编程语言的能力。
- en: In the next chapters, we will focus entirely on applications, using problem
    scenarios and topics to share some exciting applications of Python and computational
    thinking in designing algorithms.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将完全专注于应用程序，使用问题场景和主题来分享Python和计算思维在设计算法中的一些令人兴奋的应用。
