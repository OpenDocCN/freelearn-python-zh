- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Network Data Analysis with Elastic Stack
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Elastic Stack 进行网络数据分析
- en: 'In *Chapter 7*, *Network Monitoring with Python – Part 1*, and *Chapter 8*,
    *Network Monitoring with Python – Part 2*, we discussed the various ways to monitor
    a network. In the two chapters, we looked at two different approaches for network
    data collection: we can either retrieve data from network devices such as SNMP,
    or we can listen for the data sent by network devices using flow-based exports.
    After the data is collected, we will need to store the data in a database, then
    analyze the data to gain insights to decide what the data means. Most of the time,
    the analyzed results are displayed in a graph, whether a line graph, bar graph,
    or pie chart. We can use individual tools such as PySNMP, Matplotlib, and Pygal
    for each step, or we can leverage all-in-one tools such as Cacti or ntop for monitoring.
    The tools introduced in those two chapters gave us basic monitoring and understanding
    of the network.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第7章*，*使用 Python 进行网络监控（第一部分*）和 *第8章*，*使用 Python 进行网络监控（第二部分*）中，我们讨论了监控网络的各种方法。在这两章中，我们探讨了两种不同的网络数据收集方法：我们可以从网络设备（如
    SNMP）检索数据，或者我们可以通过基于流的导出监听网络设备发送的数据。收集数据后，我们需要将数据存储在数据库中，然后分析数据以获得洞察力，从而决定数据的意义。大多数情况下，分析结果以图表的形式显示，无论是折线图、柱状图还是饼图。我们可以为每个步骤使用单独的工具，如
    PySNMP、Matplotlib 和 Pygal，或者我们可以利用一体化的工具，如 Cacti 或 ntop 进行监控。这两章中介绍的工具为我们提供了基本的监控和网络理解。
- en: We then moved on to *Chapter 9*, *Building Network Web Services with Python*,
    to build API services to abstract our network from higher-level tools. In *Chapter
    11*, *AWS Cloud Networking*, and *Chapter 12*, *Azure Cloud Networking*, we extended
    our on-premises network to the cloud using AWS and Azure. We have covered much
    ground in these chapters and have a solid set of tools to help us make our network
    programmable.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们继续到 *第9章*，*使用 Python 构建网络 Web 服务*，以构建 API 服务来抽象我们的网络，使其从高级工具中分离出来。在 *第11章*，*AWS
    云网络* 和 *第12章*，*Azure 云网络* 中，我们使用 AWS 和 Azure 将本地网络扩展到云端。在这些章节中，我们覆盖了大量的内容，并拥有了一套坚实的工具来帮助我们使网络可编程。
- en: Starting with this chapter, we will build on our toolsets from previous chapters
    and look at other tools and projects that I have found useful in my journey once
    I was comfortable with the tools covered in previous chapters. In this chapter,
    we will take a look at an open source project, Elastic Stack ([https://www.elastic.co](https://www.elastic.co)),
    that can help us with analyzing and monitoring our network beyond what we have
    seen before.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 从本章开始，我们将基于前几章的工具集，并查看我在熟悉了前几章中介绍的工具后，在旅程中找到的其他有用工具和项目。在本章中，我们将探讨一个开源项目，Elastic
    Stack ([https://www.elastic.co](https://www.elastic.co))，它可以帮助我们在之前所见的范围之外分析和监控我们的网络。
- en: 'In this chapter, we will look at the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨以下主题：
- en: What is the Elastic (or ELK) Stack?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 Elastic（或 ELK）Stack？
- en: Elastic Stack installation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elastic Stack 安装
- en: Data ingestion with Logstash
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Logstash 进行数据摄取
- en: Data ingestion with Beats
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Beats 进行数据摄取
- en: Search with Elasticsearch
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Elasticsearch 进行搜索
- en: Data visualization with Kibana
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Kibana 进行数据可视化
- en: 'Let’s begin by answering the question: what exactly is the Elastic Stack?'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先回答一个问题：Elastic Stack 究竟是什么？
- en: What is the Elastic Stack?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 Elastic Stack？
- en: 'The Elastic Stack is also known as the “ELK” Stack. So, what is it? Let’s see
    what the developers have to say in their own words ([https://www.elastic.co/what-is/elk-stack](https://www.elastic.co/what-is/elk-stack)):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 'Elastic Stack 也被称为“ELK” Stack。那么，它究竟是什么呢？让我们看看开发者们用自己的话是如何描述的 ([https://www.elastic.co/what-is/elk-stack](https://www.elastic.co/what-is/elk-stack)):'
- en: '”ELK” is the acronym for three open source projects: Elasticsearch, Logstash,
    and Kibana. Elasticsearch is a search and analytics engine. Logstash is a serverside
    data processing pipeline that ingests data from multiple sources simultaneously,
    transforms it, and then sends it to a “stash” like Elasticsearch. Kibana lets
    users visualize data with charts and graphs in Elasticsearch. The Elastic Stack
    is the next evolution of the ELK Stack.'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “ELK”是三个开源项目的缩写：Elasticsearch、Logstash 和 Kibana。Elasticsearch 是一个搜索和分析引擎。Logstash
    是一个服务器端数据处理管道，可以同时从多个来源摄取数据，对其进行转换，然后将数据发送到类似 Elasticsearch 的“储藏室”。Kibana 允许用户在
    Elasticsearch 中使用图表和图形可视化数据。Elastic Stack 是 ELK Stack 的下一进化阶段。
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B18403_13_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![包含图形用户界面的图片 自动生成描述](img/B18403_13_01.png)'
- en: 'Figure 13.1: Elastic Stack (source: https://www.elastic.co/what-is/elk-stack)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1：Elastic Stack（来源：https://www.elastic.co/what-is/elk-stack）
- en: As we can see from the statement, the Elastic Stack is a collection of different
    projects working together to cover the whole spectrum of data collection, storage,
    retrieval, analytics, and visualization. What is nice about the stack is that
    it is tightly integrated, but each component can also be used separately. If we
    dislike Kibana for visualization, we can easily plug in Grafana for the graphs.
    What if we have other data ingestion tools that we want to use? No problem, we
    can use the RESTful API to post our data to Elasticsearch. At the center of the
    stack is Elasticsearch, an open source, distributed search engine. The other projects
    were created to enhance and support the search function. This might sound a bit
    confusing at first, but as we look deeper at the components of the project, it
    will become clearer.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从声明中我们可以看出，Elastic Stack 是由不同项目组成的集合，这些项目协同工作，覆盖了数据收集、存储、检索、分析和可视化的整个范围。这个堆栈的优点在于它紧密集成，但每个组件也可以单独使用。如果我们不喜欢
    Kibana 用于可视化，我们可以轻松地插入 Grafana 用于图表。如果我们想使用其他数据摄取工具呢？没问题，我们可以使用 RESTful API 将我们的数据发布到
    Elasticsearch。堆栈的中心是 Elasticsearch，这是一个开源的分布式搜索引擎。其他项目都是为了增强和支持搜索功能而创建的。一开始这可能听起来有点令人困惑，但当我们更深入地了解项目的组件时，它将变得更加清晰。
- en: Why did they change the name of ELK Stack to Elastic Stack? In 2015, Elastic
    introduced a family of lightweight, single-purpose data shippers called Beats.
    They were an instant hit and continue to be very popular, but the creators could
    not come up with a good acronym for the “B” and decided to just rename the whole
    stack to Elastic Stack.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 他们为什么将 ELK Stack 的名字改为 Elastic Stack？2015 年，Elastic 引入了一系列轻量级、单用途的数据传输工具，称为
    Beats。它们立刻受到欢迎，并且继续非常受欢迎，但创造者无法为“B”想出一个好的首字母缩略词，因此决定将整个堆栈重命名为 Elastic Stack。
- en: We will focus on the network monitoring and data analysis aspects of the Elastic
    Stack. Still, the stack has many use cases, including risk management, e-commerce
    personalization, security analysis, fraud detection, and more. It is being used
    by various organizations, from web companies such as Cisco, Box, and Adobe, to
    government agencies such as NASA JPL, the United States Census Bureau, and others
    ([https://www.elastic.co/customers/](https://www.elastic.co/customers/)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点关注 Elastic Stack 的网络监控和数据分析方面。尽管如此，这个堆栈有许多用例，包括风险管理、电子商务个性化、安全分析、欺诈检测等。它被各种组织使用，从思科、Box
    和 Adobe 这样的网络公司，到美国宇航局喷气推进实验室、美国人口普查局等政府机构（[https://www.elastic.co/customers/](https://www.elastic.co/customers/))。
- en: When we talk about Elastic, we are referring to the company behind the Elastic
    Stack. The tools are open source and the company makes money by selling support,
    hosted solutions, and consulting around open source projects. The company stock
    is publicly traded on the New York Stock Exchange with the ESTC symbol.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论 Elastic 时，我们指的是 Elastic Stack 背后的公司。这些工具是开源的，公司通过销售支持、托管解决方案和围绕开源项目的咨询服务来赚钱。公司的股票在纽约证券交易所上市，股票代码为
    ESTC。
- en: Now that we have a better idea of what the ELK Stack is, let’s take a look at
    the lab topology for this chapter.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 ELK Stack 有了一个更好的了解，让我们看看本章的实验室拓扑。
- en: Lab topology
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验室拓扑
- en: For the network lab, we will reuse the network topology we used in *Chapter
    8*, *Network Monitoring with Python – Part 2\.* The network gear will have the
    management interfaces in the `192.168.2.0/24` management network with the interconnections
    in the `10.0.0.0/8` network and the subnets in `/30s`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于网络实验室，我们将重用我们在 *第 8 章*，*Python 网络监控第二部分* 中使用的网络拓扑。网络设备将具有位于 `192.168.2.0/24`
    管理网络的管理接口，以及位于 `10.0.0.0/8` 网络的互连和 `/30s` 子网。
- en: Where can we install the ELK Stack in the lab? In production, we should run
    the ELK Stack in a dedicated cluster. In our lab, however, we can quickly spin
    up a testing instance via Docker containers. If a refresher of Docker is needed,
    please refer to *Chapter 5*, *Docker Containers for Network Engineers*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在哪里可以安装 ELK Stack 到实验室？在生产环境中，我们应该在专用集群中运行 ELK Stack。然而，在我们的实验室中，我们可以通过 Docker
    容器快速启动一个测试实例。如果需要 Docker 的复习，请参阅 *第 5 章*，*网络工程师的 Docker 容器*。
- en: 'Following is a graphical representation of our network lab topology:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们网络实验室拓扑的图形表示：
- en: '![Diagram  Description automatically generated](img/B18403_13_02.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B18403_13_02.png)'
- en: 'Figure 13.2: Lab Topology'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2：实验室拓扑
- en: '| **Device** | **Management IP** | **Loopback IP** |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| **设备** | **管理 IP** | **环回 IP** |'
- en: '| r1 | `192.168.2.218` | `192.168.0.1` |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| r1 | `192.168.2.218` | `192.168.0.1` |'
- en: '| r2 | `192.168.2.219` | `192.168.0.2` |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| r2 | `192.168.2.219` | `192.168.0.2` |'
- en: '| r3 | `192.168.2.220` | `192.168.0.3` |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| r3 | `192.168.2.220` | `192.168.0.3` |'
- en: '| r5 | `192.168.2.221` | `192.168.0.4` |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| r5 | `192.168.2.221` | `192.168.0.4` |'
- en: '| r6 | `192.168.2.222` | `192.168.0.5` |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| r6 | `192.168.2.222` | `192.168.0.5` |'
- en: 'The Ubuntu hosts information is as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Ubuntu主机信息如下：
- en: '| **Device Name** | **External Link Eth0** | **Internal IP Eth1** |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| **设备名称** | **外部链接Eth0** | **内部IP Eth1** |'
- en: '| Client | `192.168.2.211` | `10.0.0.9` |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 客户端 | `192.168.2.211` | `10.0.0.9` |'
- en: '| Server | `192.168.2.212` | `10.0.0.5` |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 服务器 | `192.168.2.212` | `10.0.0.5` |'
- en: 'To run multiple containers, we should allocate at least 4 GB RAM or more to
    the host. Let’s start Docker Engine, if not done already, then pull the image
    from Docker Hub:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行多个容器，我们应该至少为宿主机分配4 GB RAM或更多。如果尚未启动，请先启动Docker Engine，然后从Docker Hub拉取镜像：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When the Docker container is run, the generated default Elastic user password
    and Kibana enrollment token are output to the terminal; please take a note of
    them as we will need them later. You might need to scroll up the screen a bit
    to find them:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当Docker容器运行时，生成的默认Elastic用户密码和Kibana注册令牌将输出到终端；请记住它们，因为我们稍后会用到。你可能需要稍微向上滚动屏幕以找到它们：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once the Elasticsearch container runs, we can test out the instance by browsing
    to `https://<your ip>:9200`:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Elasticsearch容器启动，我们可以通过浏览到`https://<你的IP>:9200`来测试实例：
- en: '![Text  Description automatically generated](img/B18403_13_03.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![文本  自动生成的描述](img/B18403_13_03.png)'
- en: 'Figure 13.3: Elasticsearch Initial Result'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3：Elasticsearch初始结果
- en: 'We can then pull and run the Kibana container image from a separate terminal:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以从另一个终端拉取并运行Kibana容器镜像：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once Kibana boots up, we can access it via port 5601:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Kibana启动，我们就可以通过端口5601访问它：
- en: '![Graphical user interface, text, application, chat or text message  Description
    automatically generated](img/B18403_13_04.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，聊天或文本消息  自动生成的描述](img/B18403_13_04.png)'
- en: 'Figure 13.4: Kibana Start Page'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4：Kibana启动页面
- en: 'Notice it is asking for the enrolment token we jotted down before. We can paste
    that in and click on **Configure Elastic**. It will prompt us for a token, which
    is now displayed on the Kibana terminal. Once that is authenticated, Kibana will
    start to configure Elastic:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意它正在请求我们之前记下的注册令牌。我们可以将其粘贴并点击**配置Elastic**。它将提示我们输入令牌，该令牌现在显示在Kibana终端上。一旦认证通过，Kibana将开始配置Elastic：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B18403_13_05.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序  自动生成的描述](img/B18403_13_05.png)'
- en: 'Figure 13.5: Configurating Elastic'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5：配置Elastic
- en: 'Finally, we should be able to access the Kibana interface at `http://<ip>:5601`.
    We do not need any integration at this point; we will pick **Explore on my own**:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们应该能够通过`http://<ip>:5601`访问Kibana界面。目前我们不需要任何集成；我们将选择**自行探索**：
- en: '![](img/B18403_13_06.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18403_13_06.png)'
- en: 'Figure 13.6:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6：
- en: 'We will be presented with an option to load some sample data. This is a great
    way to get our feet wet with the tool, so let’s import this data:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到一个选项来加载一些示例数据。这是了解这个工具的好方法，所以让我们导入这些数据：
- en: '![Graphical user interface, application, website  Description automatically
    generated](img/B18403_13_07.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，网站  自动生成的描述](img/B18403_13_07.png)'
- en: 'Figure 13.7: Kibana Home Page'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.7：Kibana主页
- en: 'We will choose **Try sample data** and add the sample eCommerce orders, sample
    flight data, and sample web logs:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将选择**尝试示例数据**并添加示例电子商务订单、示例航班数据和示例网络日志：
- en: '![](img/B18403_13_08.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18403_13_08.png)'
- en: 'Figure 13.8: Adding Sample Data'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.8：添加示例数据
- en: 'To summarize, we now have Elasticsearch and Kibana running as containers with
    forwarded ports on the management host:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们现在已经在管理主机上以转发端口的形式运行了Elasticsearch和Kibana容器：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Great! We are almost done. The last piece of the puzzle is Logstash. Since
    we will be working with different Logstash configuration files, modules, and plugins,
    we will install it on the management host with a package instead of a Docker container.
    Logstash requires Java to run:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们几乎完成了。最后一部分是Logstash。由于我们将使用不同的Logstash配置文件、模块和插件，我们将使用软件包而不是Docker容器在管理主机上安装它。Logstash需要Java来运行：
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can download the Logstash bundled package:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以下载Logstash捆绑包：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will modify a few fields in the Logstash configuration file:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将修改Logstash配置文件中的几个字段：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We will not start Logstash just yet. We will wait until we have installed the
    network-related plugins and created the necessary configuration file later in
    the chapter to start the Logstash process.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在不会启动 Logstash。我们将等待在本章的后面安装与网络相关的插件并创建必要的配置文件后，再启动 Logstash 进程。
- en: Let’s take a moment to look at deploying the ELK Stack as a hosted service in
    the next section.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下一节中花点时间看看如何将 ELK Stack 作为托管服务进行部署。
- en: Elastic Stack as a service
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作为服务的 Elastic Stack
- en: 'Elasticsearch is a popular service available as a hosted option by both Elastic.co
    and other cloud providers. Elastic Cloud ([https://www.elastic.co/cloud/](https://www.elastic.co/cloud/))
    does not have an infrastructure of its own, but it offers the option to spin up
    deployments on AWS, Google Cloud Platform, or Azure. Because Elastic Cloud is
    built on other public cloud VM offerings, the cost will be a bit more than getting
    it directly from a cloud provider, such as AWS:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch 是一种流行的服务，由 Elastic.co 和其他云提供商提供托管选项。Elastic Cloud ([https://www.elastic.co/cloud/](https://www.elastic.co/cloud/))
    没有自己的基础设施，但它提供了在 AWS、Google Cloud Platform 或 Azure 上启动部署的选项。由于 Elastic Cloud 是基于其他公共云
    VM 提供的，因此成本将略高于直接从云提供商（如 AWS）获取：
- en: '![Timeline  Description automatically generated](img/B18403_13_09.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![时间线描述自动生成](img/B18403_13_09.png)'
- en: 'Figure 13.9: Elastic Cloud Offerings'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.9：Elastic Cloud 产品
- en: AWS offers the hosted OpenSearch product ([https://aws.amazon.com/opensearch-service/](https://aws.amazon.com/opensearch-service/))
    tightly integrated with the existing AWS offerings. For example, AWS CloudWatch
    Logs can be streamed directly to the AWS OpenSearch instance ([https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html)
    ).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 提供了与现有 AWS 产品紧密集成的托管 OpenSearch 产品 ([https://aws.amazon.com/opensearch-service/](https://aws.amazon.com/opensearch-service/))。例如，AWS
    CloudWatch 日志可以直接流式传输到 AWS OpenSearch 实例 ([https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html)
    )。
- en: From my own experience, as attractive as the Elastic Stack is for its advantages,
    it is a project that I feel is easy to get started but hard to scale without a
    steep learning curve. The learning curve is even steeper when we do not deal with
    Elasticsearch on a daily basis. If you, like me, want to take advantage of the
    features Elastic Stack offers but do not want to become a full-time Elastic engineer,
    I would highly recommend using one of the hosted options for production.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从我的个人经验来看，尽管 Elastic Stack 因其优势而具有吸引力，但它是一个容易上手但如果没有陡峭的学习曲线则难以扩展的项目。如果我们不每天处理
    Elasticsearch，学习曲线将更加陡峭。如果你像我一样，想利用 Elastic Stack 提供的功能，但又不想成为全职的 Elastic 工程师，我强烈建议使用托管选项进行生产。
- en: Which hosted provider to choose depends on your preference of cloud provider
    lockdown and if you want to use the latest features. Since Elastic Cloud is built
    by the folks behind the Elastic Stack project, they tend to offer the latest features
    faster than AWS. On the other hand, if your infrastructure is fully built in the
    AWS cloud, having a tightly integrated OpenSearch instance saves you the time
    and effort required to maintain a separate cluster.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 选择哪个托管提供商取决于你对云提供商锁定和是否想使用最新功能的偏好。由于 Elastic Cloud 是由 Elastic Stack 项目背后的团队开发的，因此他们通常比
    AWS 更快地提供最新功能。另一方面，如果你的基础设施完全建立在 AWS 云中，拥有一个紧密集成的 OpenSearch 实例可以节省你维护单独集群所需的时间和精力。
- en: Let’s look at an end-to-end example from data ingestion to visualization in
    the next section.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下一节中查看一个从数据摄取到可视化的端到端示例。
- en: First End-to-End example
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一个端到端示例
- en: One of the most common pieces of feedback from people new to Elastic Stack is
    the amount of detail you need to understand to get started. To get the first usable
    record in the Elastic Stack, the user needs to build a cluster, allocate master
    and data nodes, ingest the data, create the index, and manage it via the web or
    command-line interface. Over the years, Elastic Stack has simplified the installation
    process, improved its documentation, and created sample datasets for new users
    to get familiar with the tools before using the stack in production.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 新手对 Elastic Stack 最常见的反馈之一是需要了解多少细节才能开始。要在 Elastic Stack 中获得第一个可用的记录，用户需要构建一个集群，分配主节点和数据节点，摄取数据，创建索引，并通过网页或命令行界面进行管理。多年来，Elastic
    Stack 简化了安装过程，改进了其文档，并为新用户创建了示例数据集，以便在使用堆栈进行生产之前熟悉工具。
- en: Running the components in Docker containers helps with some of the pain of installation
    but increases the complexity of maintenance. It is a balancing act to choose between
    running them in a virtual machine vs. containers.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Docker 容器中运行组件有助于减轻安装的一些痛苦，但增加了维护的复杂性。在虚拟机与容器之间运行它们是一种权衡。
- en: Before we dig deeper into the different components of the Elastic Stack, it
    is helpful to look at an example that spans Logstash, Elasticsearch, and Kibana.
    By going over this end-to-end example, we will become familiar with the function
    that each component provides. When we look at each component in more detail later
    in the chapter, we can compartmentalize where the particular component fits into
    the overall picture.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨 Elastic Stack 的不同组件之前，查看一个跨越 Logstash、Elasticsearch 和 Kibana 的示例是有帮助的。通过回顾这个端到端示例，我们将熟悉每个组件提供的功能。当我们在本章的后面更详细地查看每个组件时，我们可以将特定组件在整体图景中的位置进行分类。
- en: 'Let’s start by putting our log data into Logstash. We will configure each of
    the routers to export the log data to the Logstash server:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先将我们的日志数据放入 Logstash。我们将配置每个路由器将日志数据导出到 Logstash 服务器：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'On our Elastic Stack host, with all of the components installed, we will create
    a simple Logstash configuration that listens on UDP port `5144` and outputs the
    data to the console in JSON format as well as the Elasticsearch host:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 Elastic Stack 主机上，所有组件都已安装完毕后，我们将创建一个简单的 Logstash 配置，该配置监听 UDP 端口 `5144`，并将数据以
    JSON 格式输出到控制台以及 Elasticsearch 主机：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The configuration file consists of only an input section and an output section
    without modifying the data. The type, `syslog-ios`, is a name we picked to identify
    this index. In the `output` section, we configure the index name with variables
    representing today’s date. We can run the Logstash process directly from the binary
    directory in the foreground:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件仅包含一个输入部分和一个输出部分，不修改数据。类型 `syslog-ios` 是我们选择的用于识别此索引的名称。在 `output` 部分中，我们使用代表今天日期的变量配置索引名称。我们可以直接从二进制目录在前台运行
    Logstash 进程：
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'By default, Elasticsearch allows automatic index generation when data is sent
    to it. We can generate some log data on the router by resetting the interface,
    reloading BGP, or simply going into the configuration mode and exiting out. Once
    there are some new logs generated, we will see the `cisco-syslog-<date>` index
    being created:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Elasticsearch 允许在向其发送数据时自动生成索引。我们可以在路由器上通过重置接口、重新加载 BGP 或简单地进入配置模式并退出来生成一些日志数据。一旦生成了一些新的日志，我们将看到创建的
    `cisco-syslog-<date>` 索引：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'At this point, we can do a quick `curl` to see the index created on Elasticsearch.
    The `curl` command use the `insecure` flag to accommodate the self-signed certificate.
    The URL is in the “`https://<username>:<password>@<ip><port>/<path`>” format.
    `"_cat/indices/cisco*"` shows the category of indices, then match the indices
    name:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以快速执行 `curl` 命令来查看在 Elasticsearch 上创建的索引。`curl` 命令使用 `insecure` 标志来适应自签名证书。URL
    格式为“`https://<username>:<password>@<ip><port>/<path`>`。`"_cat/indices/cisco*"`
    显示索引类别，然后匹配索引名称：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can now use Kibana to create the index by going to **Menu -> Management
    -> Stack Management**:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过转到 **菜单 -> 管理 -> 堆栈管理** 来使用 Kibana 创建索引：
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B18403_13_10.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，团队  自动生成的描述](img/B18403_13_10.png)'
- en: 'Figure 13.10: Stack Management'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.10：堆栈管理
- en: 'Under **Data -> Index Management**, we can see the newly created **cisco-syslog**
    index:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **数据 -> 索引管理** 下，我们可以看到新创建的 **cisco-syslog** 索引：
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B18403_13_11.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，团队  自动生成的描述](img/B18403_13_11.png)'
- en: 'Figure 13.11: Index Management'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.11：索引管理
- en: We can now move to **Stack Management -> Kibana -> Data Views** to create a
    data view.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以转到 **堆栈管理 -> Kibana -> 数据视图** 来创建数据视图。
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B18403_13_12.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，电子邮件  自动生成的描述](img/B18403_13_12.png)'
- en: 'Figure 13.12: Create New Data Views Step 1'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.12：创建新数据视图步骤 1
- en: 'Since the index is already in Elasticsearch, we will only need to match the
    index name. Remember that our index name is a variable based on time; we can use
    a star wildcard (`*`) to match all the current and future indices starting with
    **cisco-syslog**:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 由于索引已经在 Elasticsearch 中，我们只需匹配索引名称。请记住，我们的索引名称是基于时间的变量；我们可以使用星号通配符 (`*`) 来匹配所有以
    **cisco-syslog** 开头的当前和未来索引：
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B18403_13_13.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，电子邮件  自动生成的描述](img/B18403_13_13.png)'
- en: 'Figure 13.13: Create New Data Views Step 2'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.13：创建新数据视图步骤 2
- en: Our index is time-based, that is, we have a field that can be used as a timestamp,
    and we can search based on time. We should specify the field that we designated
    as the timestamp. In our case, Elasticsearch was already smart enough to pick
    a field from our syslog for the timestamp; we just need to choose it in the second
    step from the drop-down menu.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的索引是基于时间的，也就是说，我们有一个可以用作时间戳的字段，并且我们可以根据时间进行搜索。我们应该指定我们指定的用作时间戳的字段。在我们的例子中，Elasticsearch
    已经足够智能，可以从我们的 syslog 中选择一个字段作为时间戳；我们只需要在第二步中选择它从下拉菜单中。
- en: 'After the index pattern is created, we can use the **Menu -> Discover** (under
    **Analytics**) tab to look at the entries. Make sure you pick the right indices
    and the time range:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在索引模式创建后，我们可以使用 **菜单 -> 发现**（在 **分析** 下）选项卡来查看条目。确保你选择了正确的索引和时间范围：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B18403_13_14.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序  自动生成的描述](img/B18403_13_14.png)'
- en: 'Figure 13.14: Elasticsearch Index Document Discovery'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.14：Elasticsearch 索引文档发现
- en: After we have collected some more log information, we can stop the Logstash
    process by using *Ctrl + C* on the Logstash process. This first example shows
    how we can leverage the Elastic Stack pipeline from data ingestion, storage, and
    visualization. The data ingestion used in Logstash (or Beats) is a continuous
    data stream that automatically flows into Elasticsearch. The Kibana visualization
    tool provides a way for us to analyze the data in Elasticsearch in a more intuitive
    way, then create a permanent visualization once we are happy with the result.
    There are more visualization graphs we can create with Kibana, which we will see
    more examples of later in the chapter.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们收集了一些更多的日志信息之后，我们可以通过在 Logstash 进程上使用 *Ctrl + C* 来停止 Logstash 进程。这个第一个例子展示了我们如何利用
    Elastic Stack 管道从数据摄取、存储和可视化。在 Logstash（或 Beats）中使用的数据摄取是一个连续的数据流，它会自动流入 Elasticsearch。Kibana
    可视化工具为我们提供了一种更直观地分析 Elasticsearch 中的数据的方法，一旦我们对结果满意，就可以创建一个永久的可视化。我们可以使用 Kibana
    创建更多的可视化图表，我们将在本章后面看到更多示例。
- en: Even with just one example, we can see that the most important part of the workflow
    is Elasticsearch. It is the simple RESTful interface, storage scalability, automatic
    indexing, and quick search result that gives the stack the power to adapt to our
    network analysis needs.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 即使只有一个例子，我们也可以看到工作流程中最重要的部分是 Elasticsearch。它简单的 RESTful 接口、存储可伸缩性、自动索引和快速搜索结果赋予了堆栈适应我们网络分析需求的能力。
- en: In the next section, we will look at how we can use Python to interact with
    Elasticsearch.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨如何使用 Python 与 Elasticsearch 进行交互。
- en: Elasticsearch with a Python client
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Python 客户端连接到 Elasticsearch
- en: 'We can interact with Elasticsearch via its HTTP RESTful API using a Python
    library. For instance, in the following example, we will use the `requests` library
    to perform a `GET` operation to retrieve information from the Elasticsearch host.
    For example, we know that `HTTP GET` for the following URL endpoint can retrieve
    the current indices starting with `kibana`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过 Python 库与 Elasticsearch 进行交互。例如，在下面的例子中，我们将使用 `requests` 库执行 `GET` 操作以从
    Elasticsearch 主机检索信息。例如，我们知道以下 URL 端点的 `HTTP GET` 可以检索以 `kibana` 开头的当前索引：
- en: '[PRE12]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can use the `requests` library to make a similar function in a Python script,
    `Chapter13_1.py`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `requests` 库在 Python 脚本中创建类似的功能，`Chapter13_1.py`：
- en: '[PRE13]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Executing the script will give us a list of indices starting with `kibana`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 执行脚本将给我们一个以 `kibana` 开头的索引列表：
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can also use the Python Elasticsearch client, [https://elasticsearch-py.readthedocs.io/en/master/](https://elasticsearch-py.readthedocs.io/en/master/).
    It is designed as a thin wrapper around Elasticsearch’s RESTful API to allow for
    maximum flexibility. Let’s install it and run a simple example:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 Python Elasticsearch 客户端，[https://elasticsearch-py.readthedocs.io/en/master/](https://elasticsearch-py.readthedocs.io/en/master/)。它被设计为一个围绕
    Elasticsearch RESTful API 的薄包装，以允许最大灵活性。让我们安装它并运行一个简单的示例：
- en: '[PRE15]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The example, `Chapter13_2`, simply connects to the Elasticsearch cluster and
    does a search for anything that matches the indices that start with `kibana`:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 `Chapter13_2` 简单地连接到 Elasticsearch 集群，并搜索以 `kibana` 开头的索引：
- en: '[PRE16]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'By default, the result will return the first 10,000 entries:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，结果将返回前 10,000 条条目：
- en: '[PRE17]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Using the simple script, the advantage of the client library is not obvious.
    However, the client library is very helpful when we create a more complex search
    operation, such as a scroll where we need to use the returned token per query
    to continue executing the subsequent queries until all the results are returned.
    The client can also help with more complicated administrative tasks, such as when
    we need to re-index an existing index. We will see more examples using the client
    library in the remainder of the chapter.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 使用简单的脚本，客户端库的优势并不明显。然而，当我们需要创建更复杂的搜索操作，例如需要使用每次查询返回的令牌来继续执行后续查询直到所有结果返回的滚动操作时，客户端库非常有帮助。客户端还可以帮助处理更复杂的行政任务，例如当我们需要重新索引现有索引时。我们将在本章剩余部分看到更多使用客户端库的示例。
- en: In the next section, we will look at more data ingestion examples from our Cisco
    device syslogs.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将查看更多来自我们的Cisco设备syslog的数据摄取示例。
- en: Data ingestion with Logstash
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Logstash进行数据摄取
- en: 'In the last example, we used Logstash to ingest log data from network devices.
    Let’s build on that example and add a few more configuration changes in `network_config/config_2.cfg`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一个示例中，我们使用Logstash从网络设备摄取日志数据。让我们在此基础上构建示例，并在`network_config/config_2.cfg`中添加一些额外的配置更改：
- en: '[PRE18]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the input section, we will listen on two UDP ports, `5144` and `5145`. When
    the logs are received, we will tag the log entries with either `syslog-core` or
    `syslog-edge`. We will also add a filter section to the configuration to specifically
    match the `syslog-edge` type and apply a regular expression section, `Grok`, for
    the message section. In this case, we will match everything and add an extra field,
    `received_at`, with the value of the timestamp.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入部分，我们将监听两个UDP端口，`5144`和`5145`。当接收到日志时，我们将使用`syslog-core`或`syslog-edge`对日志条目进行标记。我们还将向配置中添加一个过滤器部分，以特别匹配`syslog-edge`类型，并在消息部分应用正则表达式部分`Grok`。在这种情况下，我们将匹配所有内容，并添加一个额外的字段`received_at`，其值为时间戳。
- en: 'For more information on Grok, take a look at the following documentation: [https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html](https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Grok的更多信息，请参阅以下文档：[https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html](https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html).
- en: 'We will change `r5` and `r6` to send syslog information to UDP port `5145`:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把`r5`和`r6`改为将syslog信息发送到UDP端口`5145`：
- en: '[PRE19]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'When we start the Logstash server, we will see that both ports are now listening:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们启动Logstash服务器时，我们会看到两个端口现在都在监听：
- en: '[PRE20]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'By separating out the entries using different types, we can specifically search
    for the types in the Kibana **Discover** dashboard:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用不同类型分离条目，我们可以在Kibana **Discover**仪表板中特别搜索这些类型：
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B18403_13_15.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，电子邮件  自动生成的描述](img/B18403_13_15.png)'
- en: 'Figure 13.15: Syslog Index'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.15：Syslog索引
- en: 'If we expand on the entry with the `syslog-edge` type, we can see the new field
    that we added:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们扩展具有`syslog-edge`类型的条目，我们可以看到我们添加的新字段：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B18403_13_16.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序  自动生成的描述](img/B18403_13_16.png)'
- en: 'Figure 13.16: Syslog Timestamp'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.16：Syslog时间戳
- en: The Logstash configuration file provides many options in the input, filter,
    and output. In particular, the **Filter** section provides ways for us to enhance
    the data by selectively matching the data and further processing it before outputting
    it to Elasticsearch. Logstash can be extended with modules; each module provides
    a quick end-to-end solution for ingesting data and visualizations with purpose-built
    dashboards.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Logstash配置文件在输入、过滤和输出部分提供了许多选项。特别是，**过滤**部分提供了我们通过选择性匹配数据并在输出到Elasticsearch之前进一步处理数据来增强数据的方法。Logstash可以通过模块进行扩展；每个模块都提供了一个快速端到端解决方案，用于摄取数据和可视化，并具有专门构建的仪表板。
- en: 'For more information on the Logstash modules, take a look at the following
    document: [https://www.elastic.co/guide/en/logstash/8.4/logstash-modules.html](https://www.elastic.co/guide/en/logstash/8.4/logstash-modules.html)
    .'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Logstash模块的更多信息，请参阅以下文档：[https://www.elastic.co/guide/en/logstash/8.4/logstash-modules.html](https://www.elastic.co/guide/en/logstash/8.4/logstash-modules.html)
    .
- en: Elastic Beats are similar to Logstash modules. They are single-purpose data
    shippers, usually installed as an agent, that collect data on the host and send
    the output data either directly to Elasticsearch or Logstash for further processing.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Elastic Beats与Logstash模块类似。它们是单一用途的数据传输工具，通常作为代理安装，用于收集主机上的数据并将输出数据直接发送到Elasticsearch或Logstash进行进一步处理。
- en: There are hundreds of different downloadable Beats, such as Filebeat, Metricbeat,
    Packetbeat, Heartbeat, and so on. In the next section, we will see how we can
    use Filebeat to ingest Syslog data into Elasticsearch.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 有数百种不同的可下载Beats，例如Filebeat、Metricbeat、Packetbeat、Heartbeat等。在下一节中，我们将看到如何使用Filebeat将Syslog数据摄取到Elasticsearch中。
- en: Data ingestion with Beats
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Beats进行数据摄取
- en: As good as Logstash is, the data ingestion process can get complicated and hard
    to scale. If we expand on our network log example, we can see that even with just
    network logs, it can get complicated trying to parse different log formats from
    IOS routers, NXOS routers, ASA firewalls, Meraki wireless controllers, and more.
    What if we need to ingest log data from Apache web logs, server host health, and
    security information? What about data formats such as NetFlow, SNMP, and counters?
    The more data we need to aggregate, the more complicated it can get.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Logstash功能强大，但数据摄取过程可能会变得复杂且难以扩展。如果我们以网络日志为例进行扩展，我们可以看到，即使只是网络日志，尝试解析来自IOS路由器、NXOS路由器、ASA防火墙、Meraki无线控制器等不同日志格式也可能变得复杂。如果我们需要摄取来自Apache网络日志、服务器主机健康和安全信息的日志数据呢？至于像NetFlow、SNMP和计数器这样的数据格式呢？我们需要聚合的数据越多，情况可能就越复杂。
- en: 'While we cannot completely get away from aggregation and the complexity of
    data ingestion, the current trend is to move toward a more lightweight, single-purpose
    agent that sits as close to the data source as possible. For example, we can have
    a data collection agent installed directly on our Apache server specialized in
    collecting web log data; or we can have a host that only collects, aggregates,
    and organizes Cisco IOS logs. Elastic Stack collectively calls these lightweight
    data shippers Beats: [https://www.elastic.co/products/beats](https://www.elastic.co/products/beats).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们无法完全摆脱聚合和数据摄取的复杂性，但当前的趋势是向更轻量级、单一用途的代理移动，该代理尽可能靠近数据源。例如，我们可以在我们的Apache服务器上安装专门收集网络日志数据的数据收集代理；或者我们可以有一个只收集、聚合和组织Cisco
    IOS日志的主机。Elastic Stack将这些轻量级数据传输工具统称为Beats：[https://www.elastic.co/products/beats](https://www.elastic.co/products/beats)。
- en: Filebeat is a version of Elastic Beats software intended for forwarding and
    centralizing log data. It looks for the log file we specified in the configuration
    to be harvested; once it has finished processing, it will send the new log data
    to an underlying process that aggregates the events and outputs to Elasticsearch.
    In this section, we will look at using Filebeat with the Cisco modules to collect
    network log data.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Filebeat是Elastic Beats软件的一个版本，旨在转发和集中日志数据。它寻找配置中指定的要收集的日志文件；一旦处理完成，它将新的日志数据发送到聚合事件并输出到Elasticsearch的底层进程。在本节中，我们将探讨如何使用Filebeat与Cisco模块收集网络日志数据。
- en: 'Let’s install Filebeat and set up the Elasticsearch host with the bundled visualization
    template and index:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们安装Filebeat并设置带有捆绑可视化模板和索引的Elasticsearch主机：
- en: '[PRE21]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The directory layout can be confusing because they are installed in various
    `/usr`, `/etc/`, and `/var` locations:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 目录布局可能会让人困惑，因为它们被安装在不同的`/usr`、`/etc`和`/var`位置：
- en: '![Graphical user interface, application  Description automatically generated](img/B18403_13_17.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序描述自动生成](img/B18403_13_17.png)'
- en: 'Figure 13.17: Elastic Filebeat File Locations (source: https://www.elastic.co/guide/en/beats/filebeat/8.4/directory-layout.html)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.17：Elastic Filebeat文件位置（来源：https://www.elastic.co/guide/en/beats/filebeat/8.4/directory-layout.html）
- en: 'We will make a few changes to the configuration file, `/etc/filebeat/filebeat.yml`,
    for the location of Elasticsearch and Kibana:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对配置文件`/etc/filebeat/filebeat.yml`进行一些更改，以设置Elasticsearch和Kibana的位置：
- en: '[PRE22]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Filebeat can be used to set up the index templates and example Kibana dashboards:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用Filebeat来设置索引模板和示例Kibana仪表板：
- en: '[PRE23]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s enable the `cisco` module for Filebeat:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为Filebeat启用`cisco`模块：
- en: '[PRE24]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let’s configure the `cisco` module for `syslog` first. The file is located
    under `/etc/filebeat/modules.d/cisco.yml`. In our case, I am also specifying a
    custom log file location:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先为`syslog`配置`cisco`模块。该文件位于`/etc/filebeat/modules.d/cisco.yml`下。在我们的例子中，我还指定了一个自定义的日志文件位置：
- en: '[PRE25]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can start, stop, and check the status of the Filebeat service using the
    common Ubuntu Linux command `service Filebeat` [`start`|`stop`|`status`]:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用常见的Ubuntu Linux命令`service Filebeat`来启动、停止和检查Filebeat服务的状态`start`|`stop`|`status`：
- en: '[PRE26]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Modify or add UDP port `514` for syslog on our devices. We should be able to
    see the syslog information under the **filebeat-*** index search:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的设备上修改或添加UDP端口`514`用于syslog。我们应该能够在**filebeat-***索引搜索下看到syslog信息：
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B18403_13_18.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，电子邮件  自动生成的描述](img/B18403_13_18.png)'
- en: 'Figure 13.18: Elastic Filebeat Index'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.18：Elastic Filebeat索引
- en: 'If we compare that to the previous syslog example, we can see that there are
    a lot more fields and meta information associated with each record, such as `agent.version`,
    `event.code`, and `event.severity`:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将它与之前的syslog示例进行比较，我们可以看到与每条记录关联的字段和元信息要多得多，例如`agent.version`、`event.code`和`event.severity`：
- en: '![Graphical user interface, application  Description automatically generated](img/B18403_13_19.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序  自动生成的描述](img/B18403_13_19.png)'
- en: 'Figure 13.19: Elastic Filebeat Cisco Log'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.19：Elastic Filebeat Cisco日志
- en: Why do the extra fields matter? Among other advantages, the fields make search
    aggregation easier, and this, in turn, allows us to graph the results better.
    We will see graphing examples in the upcoming section where we discuss Kibana.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么额外的字段很重要？除了其他优点外，字段使得搜索聚合更加容易，而这反过来又使我们能够更好地绘制结果。我们将在下一节中看到绘制示例，其中我们将讨论Kibana。
- en: Besides the `cisco` module, there are modules for Palo Alto Networks, AWS, Google
    Cloud, MongoDB, and many more. The most up-to-date module list can be viewed at
    [https://www.elastic.co/guide/en/beats/filebeat/8.4/filebeat-modules.html](https://www.elastic.co/guide/en/beats/filebeat/8.4/filebeat-modules.html).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`cisco`模块外，还有针对Palo Alto Networks、AWS、Google Cloud、MongoDB等许多模块。最新的模块列表可以在[https://www.elastic.co/guide/en/beats/filebeat/8.4/filebeat-modules.html](https://www.elastic.co/guide/en/beats/filebeat/8.4/filebeat-modules.html)查看。
- en: 'What if we want to monitor NetFlow data? No problem, there is a module for
    that! We will run through the same process with the Cisco module by enabling the
    module and setting up the dashboard:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想监控NetFlow数据呢？没问题，有一个模块可以做到这一点！我们将通过启用模块并设置仪表板来运行与Cisco模块相同的流程：
- en: '[PRE27]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then, configure the module configuration file, `/etc/filebeat/modules.d/netflow.yml`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，配置模块配置文件，`/etc/filebeat/modules.d/netflow.yml`：
- en: '[PRE28]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We will configure the devices to send the NetFlow data to port `2055`. If you
    need a refresher, please read the relevant configuration in *Chapter 8*, *Network
    Monitoring with Python – Part 2*. We should be able to see the new `netflow` data
    input type:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将配置设备将NetFlow数据发送到端口`2055`。如果您需要复习，请阅读*第8章*，*使用Python进行网络监控 – 第2部分*中的相关配置。我们应该能够看到新的`netflow`数据输入类型：
- en: '![Graphical user interface, text, website  Description automatically generated](img/B18403_13_20.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，网站  自动生成的描述](img/B18403_13_20.png)'
- en: 'Figure 13.20: Elastic NetFlow Input'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.20：Elastic NetFlow输入
- en: 'Remember that each module came pre-bundled with visualization templates? Not
    to jump ahead too much into visualization, but if we click on the **visualization**
    tab on the left panel, then search for **netflow**, we can see a few visualizations
    that were created for us:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 记住每个模块都预装了可视化模板吗？不要过多地跳入可视化，但如果我们点击左侧面板上的**可视化**选项卡，然后搜索**netflow**，我们可以看到为我们创建的一些可视化：
- en: '![](img/B18403_13_21.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18403_13_21.png)'
- en: 'Figure 13.21: Kibana Visualization'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.21：Kibana可视化
- en: 'Click on the **Conversation Partners [Filebeat Netflow]** option, which will
    give us a nice table of the top talkers that we can reorder by each of the fields:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**会话伙伴 [Filebeat Netflow]**选项，这将给我们一个很好的表格，我们可以通过每个字段重新排序顶级对话者：
- en: '![Table  Description automatically generated](img/B18403_13_22.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![表格  自动生成的描述](img/B18403_13_22.png)'
- en: 'Figure 13.22: Kibana Table'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.22：Kibana表格
- en: In the next section, we will focus on the Elasticsearch part of the ELK Stack.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将专注于ELK堆栈的Elasticsearch部分。
- en: Search with Elasticsearch
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Elasticsearch进行搜索
- en: We need more data in Elasticsearch to make the search and graph more interesting.
    I would recommend reloading a few of the lab devices to have the log entries for
    interface resets, BGP and OSPF establishments, as well as device boot-up messages.
    Otherwise, feel free to use the sample data we imported at the beginning of this
    chapter for this section.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在Elasticsearch中添加更多数据，以便使搜索和图形更加有趣。我建议重新加载几个实验室设备，以便有接口重置、BGP和OSPF建立以及设备启动消息的日志条目。否则，请随意使用本章开头导入的样本数据来处理本节。
- en: 'If we look back at the `Chapter13_2.py` script example, when we searched, there
    were two pieces of information that could potentially change from each query:
    the index and query body. What I typically like to do is to break that information
    into input variables that I can dynamically change at runtime to separate the
    logic of the search and the script itself. Let’s make a file called `query_body_1.json`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回顾一下`Chapter13_2.py`脚本示例，当我们搜索时，有两项信息可能会因每个查询而异：索引和查询体。我通常喜欢将这些信息分解为可以在运行时动态更改的输入变量，以分离搜索逻辑和脚本本身。让我们创建一个名为`query_body_1.json`的文件：
- en: '[PRE29]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We will create a script, `Chapter13_3.py`, that uses `argparse` to take the
    user input at the command line:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个名为`Chapter13_3.py`的脚本，该脚本使用`argparse`在命令行接收用户输入：
- en: '[PRE30]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can then use the two input values to construct the search the same way we
    have done before:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这两个输入值以与我们之前相同的方式构建搜索：
- en: '[PRE31]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can use the `help` option to see what arguments should be supplied with
    the script. Here are the results when we use the same query against the two different
    indices we created:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`help`选项查看应该与脚本一起提供的参数。以下是当我们使用相同的查询针对我们创建的两个不同索引时的结果：
- en: '[PRE32]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: When developing our search, it usually takes a few tries before we get the result
    we are looking for. One of the tools Kibana provides is a developer console that
    allows us to play around with the search criteria and view the search results
    on the same page. The tool is under the menu section *Management for Dev Tools*.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发我们的搜索时，通常需要尝试几次才能得到我们想要的结果。Kibana提供的一个工具是开发者控制台，它允许我们在同一页面上玩转搜索条件并查看搜索结果。该工具位于菜单部分*Management
    for Dev Tools*。
- en: 'For example, in the following figure, we execute the same search we have done
    now and we’re able to see the returned JSON result. This is one of my favorite
    tools on the Kibana interface:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在下面的图中，我们执行了我们现在所做的相同搜索，并能够看到返回的JSON结果。这是我在Kibana界面上的最爱工具之一：
- en: '![Graphical user interface, text  Description automatically generated](img/B18403_13_23.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本描述自动生成](img/B18403_13_23.png)'
- en: 'Figure 13.23: Kibana Dev Tools'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.23：Kibana Dev Tools
- en: 'Much of the network data is based on time, such as the log and NetFlow data
    we have collected. The values are taken at a snapshot in time, and we will likely
    group the value in a time scope. For example, we might want to know, “who are
    the NetFlow top talkers in the last 7 days?” or “which device has the most BGP
    reset messages in the last hour?” Most of these questions have to do with aggregation
    and time scope. Let’s look at a query that limits the time range, `query_body_2.json`:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数网络数据都是基于时间的，例如我们收集的日志和NetFlow数据。这些值是在某个时间点拍摄的快照，我们可能会在时间范围内分组这些值。例如，我们可能想知道，“在过去的7天里，谁是NetFlow顶级通信者？”或者“在过去1小时内，哪个设备有最多的BGP重置消息？”这些问题大多与聚合和时间范围有关。让我们看看一个限制时间范围的查询，`query_body_2.json`：
- en: '[PRE33]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This is a Boolean query, [https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-bool-query.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-bool-query.html),
    which means it can take a combination of other queries. In our query, we use the
    filter to limit the time range to be the last 10 minutes. We copy the `Chapter13_3.py`
    script to `Chapter13_4.py` and modify the output to grab the number of hits as
    well as a loop over the actual returned results list:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个布尔查询，[https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-bool-query.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-bool-query.html)，这意味着它可以组合其他查询。在我们的查询中，我们使用过滤器将时间范围限制为过去10分钟。我们将`Chapter13_3.py`脚本复制到`Chapter13_4.py`，并修改输出以获取命中次数以及遍历实际返回的结果列表：
- en: '[PRE34]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Executing the script will show that we only have `23` hits in the last 10 minutes:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 执行脚本将显示我们在过去10分钟内只有`23`次命中：
- en: '[PRE35]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can add another filter option in the query to limit the source IP via `query_body_3.json`:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在查询中添加另一个过滤器选项，通过`query_body_3.json`限制源IP：
- en: '[PRE36]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The result will be limited by both the source IP of r1’s loopback IP in the
    last 10 minutes:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将受到 r1 的 loopback IP 在过去 10 分钟内的源 IP 的限制：
- en: '[PRE37]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let’s modify the search body one more time to add an aggregation, [https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket.html),
    that takes a sum of all the network bytes from our previous search:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次修改搜索主体，添加一个聚合，[https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket.html)，它从我们之前的搜索中计算所有网络字节的总和：
- en: '[PRE38]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The result will be different every time we run the script `Chapter13_5.py`.
    The current result is about 1 MB for me when I run the script consecutively:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 每次运行脚本 `Chapter13_5.py` 时，结果都会不同。当我连续运行脚本时，当前结果大约为 1 MB：
- en: '[PRE39]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'As you can see, building a search query is an iterative process; you typically
    start with a wide net and gradually narrow the criteria to fine-tune the results.
    In the beginning, you will probably spend a lot of time reading the documentation
    and searching for the exact syntax and filters. As you gain more experience under
    your belt, the search syntax will become easier. Going back to the previous visualization
    we saw from the `netflow` module setup for the NetFlow top talker, we can use
    the inspection tool to see the **Request** body:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，构建搜索查询是一个迭代的过程；您通常从一个广泛的网络开始，并逐渐缩小标准以微调结果。一开始，您可能会花很多时间阅读文档并搜索确切的语法和过滤器。随着您经验的积累，搜索语法将变得更加容易。回到我们从
    NetFlow 模块设置中看到的上一个可视化，我们可以使用检查工具来查看 **请求** 主体：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B18403_13_24.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，自动生成的描述](img/B18403_13_24.png)'
- en: 'Figure 13.24: Kibana Request'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.24：Kibana 请求
- en: 'We can put that into a query JSON file, `query_body_5.json`, and execute it
    with the `Chapter13_6.py` file. We will receive the raw data that the graph was
    based on:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将它放入查询 JSON 文件 `query_body_5.json` 中，并使用 `Chapter13_6.py` 文件执行它。我们将收到图表基于的原始数据：
- en: '[PRE40]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'In the next section, let’s take a deeper look at the visualization part of
    the Elastic Stack: Kibana.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，让我们更深入地了解 Elastic Stack 的可视化部分：Kibana。
- en: Data visualization with Kibana
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Kibana 进行数据可视化
- en: So far, we have used Kibana to discover data, manage indices in Elasticsearch,
    use developer tools to develop queries, and use a few other features. We also
    saw the pre-populated visualization charts from NetFlow, which gave us the top
    talker pair from our data. In this section, we will walk through the steps of
    creating our own graphs. We will start by creating a pie chart.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用 Kibana 来发现数据，管理 Elasticsearch 中的索引，使用开发者工具来开发查询，以及使用一些其他功能。我们还看到了来自
    NetFlow 的预填充可视化图表，它为我们提供了数据中的顶级对话对。在本节中，我们将逐步介绍创建我们自己的图表的步骤。我们将从创建一个饼图开始。
- en: 'A pie chart is great at visualizing a portion of a component in relation to
    the whole. Let’s create a pie chart based on the Filebeat index that graphs the
    top 10 source IP addresses based on the number of record counts. We will select
    **Dashboard -> Create dashboard -> Create visualization -> Pie**:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 饼图非常适合可视化组件的一部分相对于整体的部分。让我们基于 Filebeat 索引创建一个饼图，根据记录计数绘制前 10 个源 IP 地址。我们将选择
    **仪表板 -> 创建仪表板 -> 创建可视化 -> 饼图**：
- en: '![Graphical user interface, application  Description automatically generated](img/B18403_13_25.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，自动生成的描述](img/B18403_13_25.png)'
- en: 'Figure 13.25: Kibana Pie Chart'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.25：Kibana 饼图
- en: 'Then we will type **netflow** in the search bar to pick our **[Filebeat NetFlow]**
    indices:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在搜索栏中输入 **netflow** 以选择我们的 **[Filebeat NetFlow]** 索引：
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B18403_13_26.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，团队，自动生成的描述](img/B18403_13_26.png)'
- en: 'Figure 13.26: Kibana Pie Chart Source'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.26：Kibana 饼图来源
- en: 'By default, we are given the total count of all the records in the default
    time range. The time range can be dynamically changed:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，我们得到了默认时间范围内所有记录的总数。时间范围可以动态更改：
- en: '![](img/B18403_13_27.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18403_13_27.png)'
- en: 'Figure 13.27: Kibana Time Range'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.27：Kibana 时间范围
- en: 'We can assign a custom label for the graph:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为图表分配一个自定义标签：
- en: '![Icon  Description automatically generated](img/B18403_13_28.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图标，自动生成的描述](img/B18403_13_28.png)'
- en: 'Figure 13.28: Kibana Chart Label'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.28：Kibana 图表标签
- en: Let’s click on the **Add** option to add more buckets. We will choose to split
    the slices, pick the terms for aggregation, and select the **source.ip** field
    from the drop-down menu. We will leave the **order** **Descending** but increase
    S**ize** to **10**.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们点击**添加**选项来添加更多桶。我们将选择分割切片，选择聚合的术语，并从下拉菜单中选择**source.ip**字段。我们将保持**顺序**为**降序**，但将**大小**增加到**10**。
- en: 'The change will only be applied when you click the **Apply** button at the
    top. It is a common mistake to expect the change to happen in real time when using
    a modern website and not by clicking on the **Apply** button:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当你点击顶部**应用**按钮时，更改才会生效。在使用现代网站时，期望更改实时发生而不是通过点击**应用**按钮是一种常见的错误：
- en: '![Graphical user interface, application  Description automatically generated](img/B18403_13_29.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序  自动生成描述](img/B18403_13_29.png)'
- en: 'Figure 13.29: Kibana Play Button'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.29：Kibana播放按钮
- en: 'We can click on **Options** at the top to turn off **Donut** and turn on **Show
    labels**:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以点击顶部的**选项**来关闭**环形图**并打开**显示标签**：
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B18403_13_30.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，团队  自动生成描述](img/B18403_13_30.png)'
- en: 'Figure 13.30: Kibana Chart Options'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.30：Kibana图表选项
- en: 'The final graph is a nice pie chart showing the top IP sources based on the
    number of document counts:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的图表是一个展示基于文档计数数量的顶级IP来源的饼图：
- en: '![Chart, pie chart  Description automatically generated](img/B18403_13_31.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图表，饼图  自动生成描述](img/B18403_13_31.png)'
- en: 'Figure 13.31: Kibana Pie Chart'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.31：Kibana饼图
- en: 'As with Elasticsearch, the Kibana graph is also an iterative process that typically
    takes a few tries to get right. What if we split the result into different charts
    instead of slices on the same chart? Yeah, that is not very visually appealing:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 与Elasticsearch一样，Kibana的图表也是一个迭代过程，通常需要尝试几次才能正确设置。如果我们把结果分成不同的图表而不是同一图表上的切片呢？是的，这并不非常直观：
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B18403_13_32.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，团队  自动生成描述](img/B18403_13_32.png)'
- en: 'Figure 13.32: Kibana Split Chart'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.32：Kibana分割图表
- en: Let’s stick to splitting things into slices on the same pie chart and change
    the time range to **Last 1 hour**, then save the chart so that we can come back
    to it later.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们坚持在同一饼图上分割切片，并将时间范围更改为**过去1小时**，然后保存图表以便稍后返回：
- en: 'Note that we can also share the graph either in an embedded URL (if Kibana
    is accessible from a shared location) or a snapshot:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们还可以通过嵌入的URL（如果Kibana可以从共享位置访问）或快照来共享图表：
- en: '![Chart, pie chart  Description automatically generated](img/B18403_13_33.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图表，饼图  自动生成描述](img/B18403_13_33.png)'
- en: 'Figure 13.33: Kibana Save Chart'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.33：Kibana保存图表
- en: 'We can also do more with the metrics operations. For example, we can pick the
    data table chart type and repeat our previous bucket breakdown with the source
    IP. But we can also add a second metric by adding up the total number of network
    bytes per bucket:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用度量操作做更多的事情。例如，我们可以选择数据表图表类型，并使用源IP重复之前的桶分解。但我们还可以通过添加每个桶的网络字节总数来添加第二个度量：
- en: '![Graphical user interface, application  Description automatically generated](img/B18403_13_34.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序  自动生成描述](img/B18403_13_34.png)'
- en: 'Figure 13.34: Kibana Metrics'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.34：Kibana度量
- en: 'The result is a table showing both the number of document counts as well as
    the sum of the network bytes. This can be downloaded in CSV format for local storage:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个表格，显示了文档计数数量以及网络字节的总和。这可以以CSV格式下载以供本地存储：
- en: '![Graphical user interface, table  Description automatically generated with
    medium confidence](img/B18403_13_35.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，表格  中度自信自动生成](img/B18403_13_35.png)'
- en: 'Figure 13.35: Kibana Tables'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.35：Kibana表格
- en: Kibana is a very powerful visualization tool in the Elastic Stack. We are just
    scratching the surface of its visualization capabilities. Besides many other graph
    options to better tell the story of your data, we can also group multiple visualizations
    onto a dashboard to be displayed. We can also use Timelion ([https://www.elastic.co/guide/en/kibana/8.4/timelion.html](https://www.elastic.co/guide/en/kibana/8.4/timelion.html))
    to group independent data sources for a single visualization or use Canvas ([https://www.elastic.co/guide/en/kibana/current/canvas.html](https://www.elastic.co/guide/en/kibana/current/canvas.html))
    as a presentation tool based on data in Elasticsearch.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 是 Elastic Stack 中一个非常强大的可视化工具。我们只是触及了其可视化功能的表面。除了许多其他图表选项，以更好地讲述您数据的故事外，我们还可以将多个可视化组合到一个仪表板上进行展示。我们还可以使用
    Timelion ([https://www.elastic.co/guide/en/kibana/8.4/timelion.html](https://www.elastic.co/guide/en/kibana/8.4/timelion.html))
    将独立的数据源组合成单个可视化，或者使用 Canvas ([https://www.elastic.co/guide/en/kibana/current/canvas.html](https://www.elastic.co/guide/en/kibana/current/canvas.html))
    作为基于 Elasticsearch 数据的演示工具。
- en: Kibana is typically used at the end of the workflow to present our data meaningfully.
    We have covered the basic workflow from data ingestion to storage, retrieval,
    and visualization in the span of a chapter. It still amazes me that we can accomplish
    so much in a short period with the aid of an integrated, open source stack such
    as Elastic Stack.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 通常用于工作流程的末端，以有意义的方式展示我们的数据。我们在本章范围内涵盖了从数据摄取到存储、检索和可视化的基本工作流程。令人惊讶的是，借助如
    Elastic Stack 这样的集成开源堆栈，我们可以在短时间内完成这么多工作。
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we used the Elastic Stack to ingest, analyze, and visualize
    network data. We used Logstash and Beats to ingest the network syslog and NetFlow
    data. Then we used Elasticsearch to index and categorize the data for easier retrieval.
    Finally, we used Kibana to visualize the data. We used Python to interact with
    the stack and help us gain more insights into our data. Together, Logstash, Beats,
    Elasticsearch, and Kibana present a powerful all-in-one project that can help
    us understand our data better.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用了 Elastic Stack 来摄取、分析和可视化网络数据。我们使用了 Logstash 和 Beats 来摄取网络系统日志和 NetFlow
    数据。然后我们使用 Elasticsearch 对数据进行索引和分类，以便于检索。最后，我们使用 Kibana 来可视化数据。我们使用 Python 与该堆栈交互，帮助我们更深入地了解我们的数据。Logstash、Beats、Elasticsearch
    和 Kibana 一起构成了一个强大的全能型项目，可以帮助我们更好地理解我们的数据。
- en: In the next chapter, we will look at using Git for network development with
    Python.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何使用 Python 结合 Git 进行网络开发。
- en: Join our book community
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的书籍社区
- en: 'To join our community for this book – where you can share feedback, ask questions
    to the author, and learn about new releases – follow the QR code below:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 要加入这本书的社区——在那里您可以分享反馈、向作者提问，并了解新书发布——请扫描下面的二维码：
- en: '[https://packt.link/networkautomationcommunity](https://packt.link/networkautomationcommunity)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/networkautomationcommunity](https://packt.link/networkautomationcommunity)'
- en: '![](img/QR_Code2903617220506617062.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code2903617220506617062.png)'
