- en: Multiprocessing and HPC in Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python中的多处理和HPC
- en: '**High-performance computing** (**HPC**), quite simply, is the use of parallel
    processing during the execution of an application to spread the computational
    load across multiple processors, often across multiple machines. There are several
    MPC strategies to choose from, ranging from custom applications that leverage
    local multiprocessor computer architecture through to dedicated MPC systems, such
    as Hadoop or Apache Spark.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**高性能计算**（**HPC**）简单来说，就是在应用程序执行过程中使用并行处理来将计算负载分布到多个处理器上，通常跨越多台机器。有几种MPC策略可供选择，从利用本地多处理器计算机架构的定制应用程序到专用的MPC系统，如Hadoop或Apache
    Spark。'
- en: 'In this chapter, we will explore and apply different Python capabilities, building
    from executing a baseline algorithm against elements in a dataset one element
    at a time, and look at the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索并应用不同的Python功能，从针对数据集中的元素逐个执行基线算法开始，并研究以下主题：
- en: Building parallel processing approaches that exploit locally available multiprocessor
    architectures, and the limitations of those approaches using Python's `multiprocessing`
    module
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建利用本地可用的多处理器架构的并行处理方法，并使用Python的`multiprocessing`模块来限制这些方法
- en: Defining and implementing an approach across multiple machines to parallelize
    the baseline serial process—essentially creating a basic computational cluster
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义并实施一种跨多台机器的方法来并行化基线串行过程，从根本上创建一个基本的计算集群
- en: Exploring how to use Python code in dedicated, industry-standard HPC clusters
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索如何在专用的、行业标准的HPC集群中使用Python代码
- en: Common factors to consider
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 需要考虑的共同因素
- en: Code that executes in a parallel manner has a few additional factors to consider
    as it's being developed. The first consideration is the input to the program.
    If the primary operations against any set of data are wrapped in a function or
    method, then the data is handed off to the function. The function does whatever
    it needs to do, and control is handed back to the code where the function was
    called. In a parallel processing scenario, that same function might be called
    any number of times, with different data, with control passing back to the calling
    code in a different order than their execution started in. As the datasets get
    larger, or more processing power is made available to parallelize the function,
    more control has to be exerted over how that function is called, as well as when
    (under what circumstances), in order to reduce or eliminate that possibility.
    There may also be a need to control how much data is being worked on at any given
    time, if only to avoid overwhelming the machine that the code is running on.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 以并行方式执行的代码在开发过程中还有一些额外的因素需要考虑。第一个考虑因素是程序的输入。如果针对任何一组数据的主要操作被包装在一个函数或方法中，那么数据就被传递给函数。函数执行其需要做的任何事情，然后控制权被交还给调用函数的代码。在并行处理的情况下，同一个函数可能会被调用任意次数，使用不同的数据，控制权以不同于它们开始执行的顺序返回给调用代码。随着数据集变得更大，或者提供更多的处理能力来并行化函数，就必须对调用该函数的方式以及何时（在什么情况下）进行更多的控制，以减少或消除这种可能性。还可能需要控制在任何给定时间内正在处理的数据量，即使只是为了避免使代码运行的机器不堪重负。
- en: 'An example of this scenario seems to be in order. Consider three calls to the
    same function, all within a few milliseconds, where the first and third call complete
    in one second, but the second call, for whatever reason, takes ten seconds. The
    order of calls to the function would be as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况的一个例子似乎是有必要的。考虑对同一个函数的三次调用，都在几毫秒内完成，其中第一次和第三次调用在一秒内完成，但是第二次调用由于某种原因需要十秒钟。对该函数的调用顺序将如下：
- en: 'Call #1'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一通话
- en: 'Call #2'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二通话
- en: 'Call #3'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三通话
- en: 'The order that those return in, though, is as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它们返回的顺序如下：
- en: 'Call #1 (in one second)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一通话（一秒钟后）
- en: 'Call #3 (also in one second)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三通话（同样是一秒钟后）
- en: 'Call #2 (in *ten* seconds)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二通话（十秒钟后）
- en: 'The potential concern is that if the returns from the function are expected
    to come back in the same order they were called, even if it''s only implicitly,
    with dependencies on Call #2 needed by Call #3, the expected data won''t be present,
    and Call #3 will fail, probably in a very confusing manner.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在的问题是，如果期望函数的返回按照它们被调用的顺序返回，即使只是隐含地如此，对第三通话需要第二通话的依赖，那么期望的数据将不会出现，第三通话将以一种非常令人困惑的方式失败。
- en: This collection of controls over the input data, and as a result over when,
    how, and how often the parallelized process is executed, has several names, but
    we'll use the term orchestration here. Orchestration can take many forms, from
    simple loops over a small dataset, launching parallel processes for each element
    in the dataset, to large-scale, over-the-wire message-based process request-and-response
    mechanisms.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些对输入数据的控制，以及作为结果的并行化过程何时、如何以及多频率执行的控制，有几种名称，但我们将在这里使用术语“编排”。编排可以采用多种形式，从对小数据集的简单循环，为数据集中的每个元素启动并行进程，到大规模的、基于消息的过程请求-响应机制。
- en: The output from a set of parallel processes also has to be considered in some
    detail. Some of the parallelization methods available in Python simply do not
    allow the results of a function call to be directly returned to the calling code
    (at least not yet). Others may allow it, but only when the active process is complete
    and the code actively attaches to the process, blocking access to any other processes
    until the targeted one has completed. One of the more common strategies for dealing
    with output is to create the processes to be parallelized so that they are fire-and-forget
    calls—calling the function deals with the actual processing of the data and with
    sending the results to some common destination. Destinations can include multiprocess-aware
    queues (provided by the multiprocessing module as a `Queue` class), writing data
    to files, storing the results to a database, or sending some sort of asynchronous
    message to somewhere that stores the results independent of the orchestration
    or execution of those processes. There may be several different terms for these
    processes, but we'll use dispatch in our exploration here. Dispatch may also be
    controlled to some extent by whatever orchestration processes are in play, or
    might have their own independent orchestration, depending on the complexity of
    the processes.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 还必须对一组并行进程的输出进行详细考虑。Python中可用的一些并行化方法根本不允许将函数调用的结果直接返回给调用代码（至少目前还不允许）。其他方法可能允许，但只有在活动进程完成并且代码主动附加到该进程时才允许，阻塞对任何其他进程的访问，直到目标进程完成为止。处理输出的更常见策略之一是创建要并行化的进程，使它们成为“发射并忘记”调用——调用函数处理数据的实际处理，并将结果发送到某个共同的目的地。目的地可以包括多进程感知队列（由多进程模块提供的`Queue`类）、将数据写入文件、将结果存储到数据库，或者发送某种异步消息到某个地方，该地方独立于这些进程的编排或执行存储结果。这些进程可能有几种不同的术语，但在这里我们将使用“分派”进行探索。分派也可能在一定程度上受到正在进行的编排进程的控制，或者根据进程的复杂性可能有它们自己的独立编排。
- en: The processes themselves, and any post-dispatch use of their results, also need
    to be given some additional thought, at least potentially. Since the goal, ultimately,
    is to have some number of independent processes working on multiple elements of
    the dataset at the same time, and there is no sure way to anticipate how long
    any individual process might take to complete, there is a very real possibility
    that two or more processes will resolve and dispatch their data at different rates.
    That may be true even if the expected runtime for the relevant data elements is
    the same. There is no guarantee, then, for any given sequence of elements to be
    processed, that the results will be dispatched in the same sequence that the processes
    against those elements were started. This is particularly true in distributed
    processing architectures, since the individual machines that are actually doing
    the work may have other programs consuming their available CPU cycles, memory,
    or other resources that are needed to run the process.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些过程本身，以及它们的结果的任何后续使用，也需要额外考虑，至少在潜在上是这样。由于最终的目标是让一些独立的进程同时处理数据集的多个元素，而且没有确切的方法来预测任何单个进程可能需要多长时间来完成，因此有很大可能会出现两个或更多个进程以不同的速度解决和分派它们的数据。即使相关数据元素的预期运行时间相同，这也可能是真实的。因此，对于任何给定的元素处理顺序，不能保证结果将以启动对这些元素的进程的相同顺序进行分派。这在分布式处理架构中尤其如此，因为实际执行工作的个别机器可能有其他程序在消耗它们可用的CPU周期、内存或其他运行进程所需的资源。
- en: Keeping the processes and the dispatch of their results independent, as much
    as possible, will go a long way toward mitigating that particular concern. Independent
    processes won't interact with or depend on any other processes, eliminating any
    potential for cross-process conflicts, and independent dispatches eliminate the
    potential for cross-results data contamination. If there is a need for processes
    that have dependencies, those can still be implemented, but additional effort
    (most likely in the form of dispatch-focused orchestration) may be needed to prevent
    conflicts from arising as results from parallel processes become available.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽可能保持进程和它们的结果的独立性，将在很大程度上有助于减轻特定的担忧。独立的进程不会与或依赖于任何其他进程进行交互，消除了任何跨进程冲突的潜力，而独立的分派则消除了跨结果数据污染的可能性。如果需要具有依赖关系的进程，仍然可以实现，但可能需要额外的工作（很可能是以分派为重点的编排）来防止并行进程的结果可用时产生冲突。
- en: A simple but expensive algorithm
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个简单但昂贵的算法
- en: 'First, we need to solve a problem. In order to keep the focus on the various
    mechanisms for parallel processing, that domain of that problem needs to be easily
    understood. At the same time, it needs to allow for processing of arbitrarily
    large datasets, preferably with unpredictable runtimes per element in the dataset,
    and with results that are unpredictable. To that end, the problem we''re going
    to solve is determining all of the factors of every number in some range of integer
    values. That is, for any given positive integer value, `x`, we want to be able
    to calculate and return a list of all the integer values that `x` is evenly divisible
    by. The function to calculate and return the list of factors for a single number
    (`factors_of`) is relatively simple:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要解决一个问题。为了保持对并行处理的各种机制的关注，该问题的领域需要容易理解。同时，它需要允许处理任意大的数据集，最好是具有不可预测的数据集中每个元素的运行时间，并且结果是不可预测的。为此，我们要解决的问题是确定某个整数值范围内每个数字的所有因子。也就是说，对于任何给定的正整数值`x`，我们希望能够计算并返回`x`能够被整除的所有整数值的列表。计算并返回单个数字的因子列表（`factors_of`）的函数相对简单：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Although this function, by itself, only deals with a single number, a process
    that calls it over and over again for any set of numbers can be scaled out to
    any number of numbers to process, giving us the arbitrarily large dataset capabilities
    when needed. The runtimes are somewhat predictable—it should be possible to get
    a reasonable runtime estimate for numbers across various ranges, though they will
    vary based on how large the number is. If a truly unpredictable runtime simulation
    is needed, we'd be able to pre-generate the list of numbers to be processed, then
    randomly select them, one at a time. Finally, the results on a number-by-number
    basis aren't predictable.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个函数本身只处理一个数字，但调用它多次以处理任何一组数字的过程可以扩展到任意数量的数字，从而在需要时为我们提供任意大的数据集能力。运行时间有些可预测——应该可以对各种范围内的数字得到合理的运行时间估计，尽管它们会根据数字的大小而变化。如果需要一个真正不可预测的运行时间模拟，我们可以预先生成要处理的数字列表，然后逐个随机选择它们。最后，逐个数字的结果是不可预测的。
- en: Some testing setup
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些测试设置
- en: 'It may be useful to capture some runtime information for a sample set of numbers,
    say from `10,000,000` to `10,001,000`, capturing both the total runtime and the
    average time per number. A simple script (`serial_baseline.py`), executing the
    `factors_of` function against each of those numbers one at a time (serially),
    is easily assembled:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有用的是捕获一组样本数字的运行时信息，比如从`10,000,000`到`10,001,000`，捕获总运行时间和每个数字的平均时间。可以轻松组装一个简单的脚本（`serial_baseline.py`），对每个数字依次执行`factors_of`函数（串行）：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Assuming that any/all machines involved in the calculation processes are essentially
    identical in terms of processing power, the output from this script gives a reasonable
    estimate for how long it takes to perform the `factors_of` calculation against
    a number near a value of `10,000,000`. The output from a fairly new powerful laptop,
    where this code was initially tested, looked like this:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 假设参与计算过程的所有机器在处理能力方面基本相同，那么这个脚本的输出可以合理估计执行“factors_of”计算对接近“10,000,000”值的数字所需的时间。最初在一台性能强大的新笔记本电脑上测试时，输出如下：
- en: '![](assets/ea12fc7a-a77f-4e4f-84b3-4a5e5f7c2d43.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ea12fc7a-a77f-4e4f-84b3-4a5e5f7c2d43.png)'
- en: 'For testing purposes further down the line, we''ll also create a constant list
    of test numbers (`TEST_NUMBERS`), chosen to provide a fairly wide range of processing
    times:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了后续的测试目的，我们还将创建一个常量测试数字列表（`TEST_NUMBERS`），选择以提供相当广泛的处理时间范围。
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: These seven numbers were chosen to provide a good range of larger and smaller
    numbers, varying the individual runtimes for calls of the `factors_of` function.
    Since there are only seven numbers, any test runs that make use of them (instead
    of the 1,000 numbers used in the preceding code) will take substantially less
    time to execute, while still providing some insight into the individual runtimes
    if needed.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 选择这七个数字是为了提供一系列较大和较小数字，以及调用`factors_of`函数的各个运行时间。由于只有七个数字，任何使用它们的测试运行（而不是前面代码中使用的1,000个数字）将需要较少的时间来执行，同时仍然可以在需要时提供一些关于各个运行时间的见解。
- en: Local parallel processing
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地并行处理
- en: The primary focus for the local parallelization of processing will be on the
    `multiprocessing` module. There are a couple of other modules that might be usable
    for some parallelization efforts (and those will be discussed later), but `multiprocessing`
    provides the best combination of flexibility and power with the least potential
    for restrictions from the Python interpreter or other OS-level interference.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本地处理并行化的主要重点将放在`multiprocessing`模块上。还有一些其他模块可能可用于一些并行化工作（这些将在后面讨论），但`multiprocessing`提供了最好的灵活性和能力组合，同时对来自Python解释器或其他操作系统级干扰的限制最小。
- en: As might be expected from the module's name, `multiprocessing` provides a class
    (`Process`) that facilitates the creation of child processes. It also provides
    a number of other classes that can be used to make working with child processes
    easier, including `Queue` (a multiprocess-aware queue implementation that can
    be used as a data destination), and `Value` and `Array`, which allow single and
    multiple values (of a single type) to be stored in a memory space that is shared
    across multiple processes, respectively.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如从模块的名称可以预期的那样，“multiprocessing”提供了一个类（`Process`），它便于创建子进程。它还提供了许多其他类，可以用来使与子进程的工作更容易，包括`Queue`（一个多进程感知的队列实现，可用作数据目的地），以及`Value`和`Array`，它们允许单个和多个值（相同类型的）分别存储在跨多个进程共享的内存空间中。
- en: 'The full life cycle of a `Process` object involves the following steps:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`Process`对象的完整生命周期包括以下步骤：'
- en: Creating the `Process` object, defining what function or method will be executed
    when it is started, and any arguments that should be passed to it
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`Process`对象，定义启动时将执行的函数或方法，以及应传递给它的任何参数
- en: Starting the `Process`, which begins its execution
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动`Process`，开始执行
- en: Joining the `Process`, which waits for the process to complete, blocking further
    execution from the calling process until it is complete
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加入`Process`，等待进程完成，阻止调用进程的进一步执行，直到它完成
- en: 'For comparison purposes, a multiprocessing-based baseline timing test script,
    equivalent to the `serial_baseline.py` script, was created. The significant differences
    between the two scripts start with the import of the multiprocessing module:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较，创建了一个基于多进程的基准定时测试脚本，相当于`serial_baseline.py`脚本。这两个脚本之间的显着差异始于导入多进程模块：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Because there are multiple processes being created, and because they will need
    to be polled after all of them have been created, we create a list of `processes`,
    and append each new `process` as it''s created. As the process objects are being
    created, we''re specifying a `name` as well—that has no bearing or impact on the
    functionality, but does make things a bit more convenient for display purposes,
    should it be needed in testing:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因为正在创建多个进程，并且因为它们需要在全部创建后进行轮询，所以我们创建了一个“processes”列表，并在创建每个新的“process”时将其附加。在创建进程对象时，我们还指定了一个“name”，这对功能没有影响，但在测试中如果需要显示，这会使事情变得更加方便：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As soon as `process.start()` is called for each `process`, it launches and
    runs in the background until it''s complete. The individual processes don''t terminate
    once they''re complete, though: that happens when `process.join()` is called and
    the process that has been joined has completed. Since we want all of the processes
    to start executing before joining to any of them (which blocks the continuation
    of the loop), we handle all of the joins separately—which also gives every process
    that has started some time to run until they''re complete:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦为每个`process`调用`process.start（）`，它就会在后台启动和运行，直到完成。尽管各个进程在完成后不会终止：但是当调用`process.join（）`并且已加入的进程已完成时，才会发生这种情况。由于我们希望所有进程在加入任何一个进程之前开始执行（这会阻止循环的继续），因此我们单独处理所有的加入-这也给了已启动的每个进程一些时间运行直到完成：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output from this test script on the same machine that the previous script
    was run on, and with the same programs running in the background, shows some significant
    improvement in the raw runtime:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在与之前的脚本在同一台机器上运行，并且在后台运行相同的程序的情况下，此测试脚本的输出显示了原始运行时间的显着改善：
- en: '![](assets/d1227bae-c30a-45fa-b940-83ab838083b0.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/d1227bae-c30a-45fa-b940-83ab838083b0.png)'
- en: This is an improvement, even without any sort of orchestration driving it other
    than whatever is managed by the underlying OS (it just throws the same 1,000 numbers
    at `Process` instances that call the `factors_of` function): the total runtime
    is about 55% of the time that the serial processing took.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个改进，即使没有任何驱动它的编排，除了底层操作系统管理的任何东西（它只是将相同的1,000个数字传递给调用`factors_of`函数的`Process`实例）：总运行时间约为串行处理所需时间的55％。
- en: Why only 55%? Why not 25%, or at least close to that? Without some sort of orchestration
    to control how many processes were being run, this created a 1,000 processes,
    with all the attendant overhead at the operating system level, and had to give
    time to each of them in turn, so there was a lot of context shifting going on.
    A more carefully tuned orchestration process should be able to reduce that runtime
    more, but might not reduce it by much.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么只有55％？为什么不是25％，或者至少接近25％？没有一种编排来控制运行多少进程，这创建了1,000个进程，并且在操作系统级别产生了所有相关的开销，并且必须依次给它们每个人一些时间，因此发生了很多上下文切换。更仔细调整的编排过程应该能够减少运行时间，但可能不会减少太多。
- en: 'The next step toward a useful multiprocessing solution would be to actually
    be able to retrieve the results of the child process operations. In order to provide
    some visibility into what''s actually happening, we''re going to print several
    items through the process as well. We''ll also randomize the sequence of test
    numbers so that each run will execute them in a different order, which will (often)
    show how the processes are interwoven:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 朝着有用的多进程解决方案迈出的下一步将是实际能够检索子进程操作的结果。为了提供一些实际发生的可见性，我们还将通过整个过程打印几个项目。我们还将随机排列测试数字的顺序，以便每次运行都以不同的顺序执行它们，这将（通常）显示进程是如何交织在一起的：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We''re going to use `TEST_NUMBERS` we set up earlier, and randomly arrange
    them into a list:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用之前设置的“TEST_NUMBERS”，并将它们随机排列成一个列表：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In order to actually capture the results, we''ll need somewhere that they can
    be sent when they are calculated: an instance of `multiprocessing.Queue`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实际捕获结果，我们需要一个可以在计算时发送它们的地方：`multiprocessing.Queue`的一个实例：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The resultant `queue` object, as noted earlier, lives in memory that is shared
    by and accessible to the top-level process (the `multiprocessing_tests.py` script)
    and by all of the child `Process` objects' processes when they execute.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，结果`queue`对象存储在顶级进程（`multiprocessing_tests.py`脚本）和所有子`Process`对象的进程都可以访问的内存中。
- en: 'Since we''re going to be storing results in the `queue` object as they are
    calculated, we need to modify the `factors_of` function to handle that. We''ll
    also add in some `print()` calls to display when the function is called, and when
    it''s done with its work:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将把结果存储在“queue”对象中，因此需要修改“factors_of”函数来处理这一点。我们还将添加一些“print（）”调用来显示函数何时被调用以及何时完成其工作：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The type and value checking remains unchanged:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 类型和值检查保持不变：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The actual calculation of the factors of `number` remains unchanged, though
    we''re assigning the results to a variable instead of returning them so that we
    can deal with them differently as the function completes:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`number`的因子的实际计算保持不变，尽管我们将结果分配给一个变量，而不是返回它们，以便我们可以在函数完成时以不同的方式处理它们：'
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Instead of returning the calculated values, we''re going to use `queue.put()`
    to add them to the results that `queue` is keeping track of. The `queue` object
    doesn''t particularly care what data gets added to it—any object will be accepted—but
    for consistency''s sake, and to assure that each result that gets sent back has
    both the number and the factors of that number, we''ll `put` a `tuple` with both
    of those values:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`queue.put（）`而不是返回计算的值，将它们添加到`queue`正在跟踪的结果中。`queue`对象并不特别关心添加到其中的数据是什么-任何对象都将被接受-但是为了保持一致性，并确保每个发送回来的结果都具有该数字和该数字的因子，我们将`put`一个具有这两个值的`tuple`：
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'With all of that prepared, we can start the main body of the test script:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好所有这些后，我们可以开始测试脚本的主体：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We need to keep track of the starting time for the calculation of the runtime
    later:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要跟踪开始时间以便稍后计算运行时间：
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Creating and starting the processes that call `factors_of` is the same basic
    structure that we used earlier:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 创建和启动调用`factors_of`的进程与之前使用的基本结构相同：
- en: '[PRE16]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'At this point, we have a set of started but possibly incomplete child processes
    running in the background. If the first few that were created and started were
    for the smaller numbers, they may have already completed, and are just waiting
    for a `join()` to finish their execution and terminate. If, on the other hand,
    one of the *larger* numbers was the first to be executed against, that first child
    process may well still be running for some time, while the others, with shorter
    individual runtimes, may be idling in the background, waiting for a `join()`.
    In any event, we can simply iterate over the list of process items, and `join()`
    each one in turn until they''re all done:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们有一组已启动但可能不完整的子进程在后台运行。如果最初创建和启动的几个进程是针对较小的数字，它们可能已经完成，只是在等待`join()`来完成它们的执行并终止。另一方面，如果*较大*的数字是第一个被执行的，那么第一个子进程可能会在一段时间内继续运行，而其他具有较短单独运行时间的进程可能会在后台空转，等待`join()`。无论如何，我们可以简单地迭代进程项列表，并依次`join()`每个进程，直到它们全部完成：
- en: '[PRE17]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Once all of the `join()` calls have completed, the `queue` will have all of
    the results for all of the numbers, in an arbitrary order. The heavy lifting of
    the child processes is all complete, so we can calculate the final runtime and
    show the relevant information:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有的`join()`调用都完成了，`queue`将会以任意顺序包含所有数字的结果。子进程的繁重工作已经全部完成，所以我们可以计算最终的运行时间并显示相关信息：
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Actually accessing the results, which in this case is just for display purposes,
    requires calling the `get` method of the queue object—each `get` call fetches
    and removes one item that was put into the queue earlier, and for now we can simply
    print `queue.get()` until the `queue` is empty:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 实际访问结果，本例中仅用于显示目的，需要调用队列对象的`get`方法——每次`get`调用都会获取并移除队列中之前放入的一个项目，现在我们可以简单地打印`queue.get()`直到`queue`为空为止：
- en: '[PRE19]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'There are several noteworthy items that appear in the results of the test run,
    as shown in the following screenshot:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试运行结果中有几个值得注意的项目，如下图所示：
- en: '![](assets/2b828037-44b1-4a8f-b54b-9f2546cfa4cd.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/2b828037-44b1-4a8f-b54b-9f2546cfa4cd.png)'
- en: All of the lines that begin with `==>` show where the calls to the `factors_of`
    function occurred during the run. Unsurprisingly, they are all near the beginning
    of the process. The lines beginning with `***` show where the processes were joined—one
    of which happened in the middle of a run of `Process` creation events. Lines beginning
    with `<==` show where the `factors_of` calls were completed, after which they
    remained idle until the corresponding `process.join()` was called.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 所有以`==>`开头的行显示了在运行过程中`factors_of`函数的调用发生的位置。毫不奇怪，它们都在进程的开始附近。以`***`开头的行显示了进程的加入位置——其中一个发生在`Process`创建事件的中间。以`<==`开头的行显示了`factors_of`的调用完成位置，之后它们保持空闲状态，直到对应的`process.join()`被调用。
- en: The randomized sequence of test numbers, judging by the calls to `factors_of`,
    was `11, 101, 102`, `1000000001`, `16`, `1000001`, and `1001`. The sequence of
    calls completed was 11, `101`, `102`, `16`, `1001`, `1000001`, and `100000000`—a
    slightly different sequence, and the *joins* sequence (and thus the sequence of
    the final **results**) was slightly different from that as well. All of these
    confirm that the various processes were starting, executing, and completing independently
    of the main process (the `for number in TEST_NUMBERS` loop).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 根据对`factors_of`的调用，测试数字的随机序列是`11, 101, 102`, `1000000001`, `16`, `1000001`和`1001`。完成的调用序列是11,
    `101`, `102`, `16`, `1001`, `1000001`和`100000000`——一个略有不同的序列，*joins*序列（因此最终**结果**的序列）也略有不同。所有这些都证实了各个进程独立于主进程（`for
    number in TEST_NUMBERS`循环）开始、执行和完成。
- en: 'With the `Queue` instance in place, and a way established for accessing the
    results of the child processes, that''s everything really needed for basic local
    multiprocess-based parallelization. There are a few things that could be tweaked
    or enhanced, if there were functional needs for them:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有了`Queue`实例，并建立了一种访问子进程结果的方式，这就是基本的本地多进程并行化所需的一切。如果有功能需求，还有一些可以调整或增强的地方：
- en: 'If throttling of the number of active child processes were needed, or any finer
    control over how or when they were created, started, and joined, a more structured Orchestrator
    of some sort could be constructed:'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要限制活跃子进程的数量，或者对它们的创建、启动和加入进行更精细的控制，可以构建一个更结构化的编排器：
- en: The number of processes allowed could be limited based on the number of available
    CPUs on the machine, which can be retrieved with `multiprocessing.cpu_count()`.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许的进程数量可以根据机器上可用的CPU数量进行限制，可以使用`multiprocessing.cpu_count()`来获取。
- en: Regardless of how the number of processes allowed was determined, limiting the
    number of active processes could be managed in several ways, including a `Queue`
    for pending requests, another for results, and a third for requests that were
    ready to be joined. Overriding each `Queue` object's `put` so that it would check
    the other queues' status, and trigger whatever actions/code was appropriate in
    those other queues, could allow a single queue to control the entire process.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无论进程数量是如何确定的，限制活跃进程的数量可以通过多种方式进行管理，包括使用一个`Queue`来处理挂起的请求，另一个用于结果，第三个用于准备加入的请求。覆盖每个`Queue`对象的`put`方法，以便检查其他队列的状态，并在这些其他队列中触发适当的操作/代码，可以让单个队列控制整个过程。
- en: Orchestration functionality could, itself, be wrapped in a `Process`, as could
    whatever data handling was needed after the dispatch of the child process data.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编排功能本身可以包装在一个`Process`中，与分发子进程数据后可能需要的任何数据处理也可以包装在`Process`中。
- en: 'The multiprocessing module also provides other object types that might prove
    useful for certain multiprocessing scenarios, including the following:'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多进程模块还提供了其他对象类型，可能对某些多进程场景有用，包括以下内容：
- en: The `multiprocessing.pool.Pool` class—objects that provide/control a pool of
    Worker processes to which jobs can be submitted, with support for asynchronous
    results, timeouts and callbacks, and more
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`multiprocessing.pool.Pool`类——提供/控制一组工作进程的对象，可以向其提交作业，支持异步结果、超时和回调等功能'
- en: A variety of manager-object options that provide ways to create data that can
    be shared between different processes—including sharing over a network between
    processes running on different machines
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供多种管理器对象选项，可以在不同进程之间共享数据，包括在不同机器上运行的进程之间通过网络共享
- en: Threads
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线程
- en: Python has another local parallelization library—`thread`. The `thread` objects
    it provides are created and used in much the same way that `multiprocessing.Process`
    objects are, but thread-based processes run in the same memory space as the parent
    process, while `Process` objects, when they are started, actually create a new
    Python interpreter instance (with some connection capabilities to the parent Python
    interpreter).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Python还有另一个本地并行化库——`thread`。它提供的`thread`对象的创建和使用方式与`multiprocessing.Process`对象的方式非常相似，但基于线程的进程在与父进程相同的内存空间中运行，而`Process`对象在启动时实际上会创建一个新的Python解释器实例（具有与父Python解释器的一些连接能力）。
- en: Because threads run in the same interpreter and memory space, they are not capable
    of accessing multiple processors the same way that a `Process` can.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因为线程在同一个解释器和内存空间中运行，它们无法像“进程”一样访问多个处理器。
- en: A thread's access to multiple CPUs on a machine is a function of the Python
    interpreter that's used to run the code. The standard interpreter that ships with
    Python (Cpython) and the alternative PyPy interpreter both share this limitation.
    IronPython, an interpreter that runs under/in the .NET framework, and Jython,
    which runs in a Java runtime environment, do not have that limitation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 线程对机器上多个CPU的访问是由用于运行代码的Python解释器的功能决定的。随Python一起提供的标准解释器（CPython）和另一种选择的PyPy解释器都共享这一限制。IronPython是在.NET框架下运行的解释器，而Jython在Java运行时环境中运行，它们没有这种限制。
- en: Thread-based parallelization is also far more likely to encounter conflicts
    with Python's **global interpreter lock** (**GIL**). The GIL actively prevents
    multiple threads from executing or altering the same Python bytecode at the same
    time. There are some potentially long-running processes that happen outside the
    GIL's control—I/O, networking, some image processing functionality, and various
    libraries such as NumPy—but outside those exceptions, any multithreaded Python
    program that spends a lot of its execution time interpreting or manipulating Python
    bytecode will eventually hit a GIL bottleneck, losing its parallelization in the
    process.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 基于线程的并行化也更有可能遇到与Python的全局解释器锁（GIL）冲突。GIL积极地阻止多个线程同时执行或更改相同的Python字节码。除了一些潜在的长时间运行的进程，这些进程发生在GIL的控制之外——如I/O、网络、一些图像处理功能以及各种库，如NumPy——除了这些例外，任何大部分执行时间用于解释或操作Python字节码的多线程Python程序最终都会遇到GIL瓶颈，从而失去其并行化。
- en: More information about the GIL, why it exists, what it does, and so on, can
    be found on the Python wiki at [https://wiki.python.org/moin/GlobalInterpreterLock](https://wiki.python.org/moin/GlobalInterpreterLock).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 有关GIL的更多信息，为什么存在，它的作用等等，可以在Python维基上找到[https://wiki.python.org/moin/GlobalInterpreterLock](https://wiki.python.org/moin/GlobalInterpreterLock)。
- en: Parallelizing across multiple machines
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨多台机器并行化
- en: Another common parallelization strategy is to spread the workload of computational
    processes across multiple machines (physical or virtual). Where local parallelization
    is limited, ultimately, by the number of CPUs, or the number of cores, or the
    combination of both on a single machine, machine-level parallelization is limited
    by the number of machines that can be thrown at a problem. In this day and age,
    with immense reservoirs of virtual machines able to be made available in public
    clouds and private data centers, it's relatively easy to scale the number of available
    machines to match the computational needs of a problem.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的并行化策略是将计算过程的工作负载分布到多台机器（物理或虚拟）上。在本地并行化受到限制的情况下，最终受限于单台机器上的CPU数量、核心数量或两者的组合，机器级并行化受限于可以用于解决问题的机器数量。在当今这个时代，有大量的虚拟机可以在公共云和私人数据中心中提供，相对容易地将可用机器的数量扩展到与问题的计算需求相匹配的数量。
- en: 'The basic design for this kind of horizontally scalable solution is more complicated
    than the design for a local solution—it has to accomplish the same tasks, but
    separate the ability to do those tasks so that they can be made available on any
    number of machines, and provide mechanisms for executing processes and accepting
    the results from the remote tasks as they complete. In order to be reasonably
    fault-tolerant, there also needs to be more visibility into the status of the
    remote process machines, and those, in turn, have to be proactive about sending
    notifications to the central controller when something occurs that will disrupt
    their ability to do their jobs. A typical logical architecture, at a high level,
    looks like this:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的横向可扩展解决方案的基本设计比本地解决方案的设计更复杂——它必须完成相同的任务，但要分离执行这些任务的能力，以便它们可以在任意数量的机器上使用，并提供执行进程和接受远程任务完成时的结果的机制。为了具有合理的容错能力，还需要更多地了解远程进程机器的状态，并且这些机器必须主动向中央控制器发送通知，以防发生会干扰它们工作能力的事件。典型的逻辑架构在高层次上看起来是这样的：
- en: '![](assets/d8c021c1-d206-4497-858d-fe2755a42279.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/d8c021c1-d206-4497-858d-fe2755a42279.png)'
- en: 'Where:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里：
- en: The Orchestrator is a process running on one machine that is responsible for
    taking bits of the Process dataset, and sending them to the next available Worker.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编排器是在一台机器上运行的进程，负责获取进程数据集的部分，并将其发送给下一个可用的工作节点。
- en: It also keeps track of what Worker nodes are available, and probably what each Worker's
    capacity is.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它还跟踪可用的工作节点，可能还跟踪每个工作节点的容量。
- en: In order to accomplish that, the Orchestrator would have to be capable of registering
    and unregistering Worker nodes.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了实现这一点，Orchestrator必须能够注册和注销Worker节点。
- en: The Orchestrator should probably also keep track of the general health/availability
    of each of its Worker nodes, and be able to associate tasks with those nodes—if
    one becomes unavailable, and still has pending tasks, it can then reassign those
    tasks to other, available Worker nodes.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Orchestrator可能还需要跟踪每个Worker节点的一般健康/可用性，并能够将任务与这些节点关联起来——如果一个节点变得不可用，并且仍有待处理的任务，那么它可以重新分配这些任务给其他可用的Worker节点。
- en: Each Worker node is a process running on an individual machine that, while running,
    accepts process instructions in incoming message items, executes the process(es)
    necessary to generate the results, and sends a results message to the Dispatcher
    when complete.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个Worker节点是在单独的机器上运行的进程，当运行时，它接受传入消息项中的进程指令，执行生成结果所需的进程，并在完成时向Dispatcher发送结果消息。
- en: Each Worker node would also have to announce to the Orchestrator when it becomes
    available, in order to be registered, and when it is shutting down normally so
    that the Orchestrator could unregister it accordingly.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个Worker节点还必须在变为可用时向Orchestrator宣布，以便注册，并在正常关闭时通知Orchestrator，以便相应地注销它。
- en: If processing an incoming message wasn't possible because of an error, a Worker
    should also be able to relay that information back to the Orchestrator, allowing
    it to reassign the task to another Worker when it can.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果由于错误而无法处理传入消息，Worker还应该能够将该信息传回Orchestrator，使其在可能时将任务重新分配给另一个Worker。
- en: The Dispatcher is a process running on one machine that is responsible for accepting
    result message data, and doing whatever needs to be done with it—storing it in
    a database, writing it to a file, and so on. The Dispatcher could, conceivably,
    be the same machine, or even the same process as the Orchestrator—so long as dispatch-related
    message items get handled appropriately and without bogging down the orchestration
    processes, where it lives is a matter of preference.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dispatcher是在一台机器上运行的进程，负责接受结果消息数据，并根据需要执行相应的操作——将其存储在数据库中，写入文件等。Dispatcher可以是同一台机器，甚至是Orchestrator的同一进程——只要处理与调度相关的消息项得到适当处理，而不会拖累编排过程，它在哪里都可以。
- en: 'The basic structure of this kind of system could be implemented with the code
    that was already shown in [Chapter 16](9e235ce2-5611-4e7d-a16b-3332561fe85b.xhtml),
    *The Artisan Gateway Service*:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这种系统的基本结构可以使用已经在[第16章](9e235ce2-5611-4e7d-a16b-3332561fe85b.xhtml)中展示的代码来实现，*工匠网关服务*：
- en: The Orchestrator and Worker nodes could be implemented as a daemon, similar
    to `ArtisanGatewayDaemon`. If it were determined that the Dispatcher needed to
    be independent, it, too, could be a similar daemon.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Orchestrator和Worker节点可以被实现为类似于`ArtisanGatewayDaemon`的守护进程。如果确定Dispatcher需要独立，它也可以是类似的守护进程。
- en: The messaging between them could be handled with a variant of `DaemonMessage`
    objects, providing the same signed message security, transmitted over a RabbitMQ
    message system.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们之间的消息传递可以使用`DaemonMessage`对象的变体来处理，提供相同的签名消息安全性，通过RabbitMQ消息系统传输。
- en: That message transmission process could leverage the `RabbitMQSender` class
    that was already defined (also from [Chapter 16](9e235ce2-5611-4e7d-a16b-3332561fe85b.xhtml),
    *The Artisan Gateway Service*).
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该消息传输过程可以利用已经定义的`RabbitMQSender`类（也来自[第16章](9e235ce2-5611-4e7d-a16b-3332561fe85b.xhtml)，*工匠网关服务*）。
- en: A complete implementation of this approach is outside the scope of this book,
    but the critical aspects of it can be examined in enough detail to write an implementation
    if the reader so desires.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的完整实现超出了本书的范围，但它的关键方面可以被详细检查，以便读者如果愿意的话可以编写实现。
- en: Common functionality
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 共同功能
- en: 'The existing `DaemonMessage` class would need to be altered or overridden to
    accept different operations at the Orchestrator, Worker, and Dispatcher levels,
    creating new `namedtuple` constants that are applicable for each. Initially, the
    Worker node would only be concerned with accepting calls to its `factors_of` method,
    and its allowed operations would reflect this:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的`DaemonMessage`类需要被修改或重写，以接受Orchestrator、Worker和Dispatcher级别的不同操作，创建适用于每个级别的新的`namedtuple`常量。最初，Worker节点只关心接受对其`factors_of`方法的调用，其允许的操作将反映这一点：
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The corresponding change to the setter method for the operation property could
    use the appropriate `namedtuple` constant to control accepted values (for example,
    replacing `_OPERATIONS` with `WORKER_OPERATIONS`, in some fashion, for a Worker
    node''s implementation):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 操作属性的setter方法对应的更改可以使用适当的`namedtuple`常量来控制接受的值（例如，以某种方式用`WORKER_OPERATIONS`替换`_OPERATIONS`，以适用于Worker节点的实现）：
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Similarly, all three components would potentially need to know about all the
    possible `origin` values, in order to be able to assign message origins appropriately:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这三个组件可能需要了解所有可能的`origin`值，以便能够适当地分配消息来源：
- en: '[PRE22]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `main` method of any of the individual daemons would remain essentially
    unchanged from how `ArtisanGatewayDaemon` implemented it.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 任何单个守护进程的`main`方法基本上与`ArtisanGatewayDaemon`的实现方式保持不变。
- en: In this approach, there are only a few distinct variations of a few class members
    for each of the daemon classes (Worker Node, Orchestrator, and Dispatcher), but
    they are worth noting because of their distinct nature. The bulk of the differences
    is in the `_handle_message` methods of each daemon class, and each would have
    to implement its own instance methods for the operations they map process requests
    to as well.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，每个守护进程类（Worker节点、Orchestrator和Dispatcher）只有少数几个类成员的不同变体，但由于它们的独特性，值得注意。大部分差异在于每个守护进程类的`_handle_message`方法中，每个都必须实现自己的实例方法，以将其映射到的操作。
- en: The Worker nodes
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Worker节点
- en: 'All of the operations that were defined in the previous section for a hypothetical
    Worker daemon would have to be handled in the class'' `_handle_message` method—to
    start with, that''s nothing more than the `factors_of` method:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节为假设的工作节点守护进程定义的所有操作都必须在类的`_handle_message`方法中处理——起初，这只是`factors_of`方法：
- en: '[PRE23]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The implementation of the `factors_of` method would not be substantially different
    from the original `factors_of` function, as defined at the beginning of this chapter,
    except that it would have to send a results message to the Dispatcher''s message
    queue rather than returning a value:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`factors_of`方法的实现与本章开头定义的`factors_of`函数并无实质性不同，只是它必须将结果消息发送到调度程序的消息队列，而不是返回一个值：'
- en: '[PRE24]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The Worker node daemons, which need to notify the Orchestrator when they become
    available and are becoming unavailable, can do so in their `preflight` and `cleanup`
    methods, respectively:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点守护进程需要在它们的`preflight`和`cleanup`方法中通知编排者它们何时变为可用和不可用：
- en: '[PRE25]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: They would also have to implement the `dispatcher_queue`, `worker_id`, and `orchestrator_queue`
    properties that these methods use, providing a unique identifier of the worker
    node (which could be as simple as a random `UUID`) and the common Orchestrator
    and Dispatcher queue names (probably from a configuration file that's common to
    all Worker instances).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 他们还必须实现这些方法使用的`dispatcher_queue`、`worker_id`和`orchestrator_queue`属性，提供工作节点的唯一标识符（可以简单地是一个随机的`UUID`）和共同的编排者和调度程序队列名称（可能来自一个对所有工作节点实例都通用的配置文件）。
- en: The Orchestrator
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编排者
- en: 'The Orchestrator would be concerned with registration, unregistration, and
    pulse operations (allowing the Workers to send messages to the Orchestrator, essentially
    saying "I''m still alive"):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 编排者将关注注册、注销和脉冲操作（允许工作节点向编排者发送消息，基本上是在说“我还活着”）：
- en: '[PRE26]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The Orchestrator''s `_handle_message` would have to map each operation to the
    appropriate method:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 编排者的`_handle_message`必须将每个操作映射到适当的方法：
- en: '[PRE27]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The Dispatcher
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调度程序
- en: 'Initially, the Dispatcher, if it were an independent process and not folded
    into the Orchestrator, would be concerned with dispatch result operations only:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，如果调度程序是独立进程而不是合并到编排者中，它将只关注调度结果操作：
- en: '[PRE28]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Its `_handle_message` method would be constructed accordingly:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 它的`_handle_message`方法将相应地构建：
- en: '[PRE29]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Integrating Python with large-scale, cluster computing frameworks
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将Python与大规模集群计算框架集成
- en: 'Large-scale, cluster computing frameworks, in order to provide as much compatibility
    with custom written operations as possible, will probably accept input in only
    two different ways: as command-line arguments, or using standard input, with the
    latter being more common for systems that are targeted for big data operations.
    In either case, what''s needed to allow a custom process to be executed at and
    scaled to a clustered environment is a self-contained, command-line executable
    that usually returns its data to standard output.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模的集群计算框架，为了尽可能与自定义的操作兼容，可能只接受两种不同的输入方式：作为命令行参数，或者使用标准输入，后者更常见于针对大数据操作的系统。无论哪种情况，允许自定义进程在集群环境中执行并扩展所需的是一个自包含的命令行可执行文件，通常将其数据返回到标准输出。
- en: 'A minimal script that accepts standard input—whether by passing data into it
    with a pipe, or by reading the contents of a file and using that—could be implemented
    like this:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一个接受标准输入的最小脚本——无论是通过管道传递数据进入它，还是通过读取文件内容并使用——可以这样实现：
- en: '[PRE30]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Standard input is available through Python''s `sys` module as `sys.stdin`.
    It''s a file-like object, and can be both read and iterated over on a line-by-line
    basis:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 标准输入可以通过Python的`sys`模块作为`sys.stdin`获得。它是一个类似文件的对象，可以按行读取和迭代：
- en: '[PRE31]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `factors_of` function should probably be included directly in the script
    code, if only so that the entire script is totally self-contained, and won''t
    require any custom software installation to be usable. For the sake of keeping
    the code shorter and easier to walk through, though, we''ll just import it:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`factors_of`函数应该直接包含在脚本代码中，这样整个脚本就是完全自包含的，不需要任何自定义软件安装即可使用。为了使代码更短、更易于阅读，我们只是导入它：'
- en: '[PRE32]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'If the script is executed directly—`python factors_stdin.py`—then we''ll actually
    execute the process, starting with acquiring all of the numbers from `stdin`.
    They may come in as multiple lines, each of which could have multiple numbers,
    so the first step is to extract all of them so that we end up with one list of
    numbers to process:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果脚本直接执行——`python factors_stdin.py`——那么我们实际上会执行该进程，首先从`stdin`获取所有数字。它们可能作为多行输入，每行可能有多个数字，所以第一步是提取所有数字，这样我们就得到一个要处理的数字列表：
- en: '[PRE33]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'With all of the numbers ready, we can iterate over them, convert each value
    from the string value that was in the input into an actual `int`, and process
    them. If a value in the input can''t be converted to an `int`, we''ll simply skip
    it for now, though depending on the calling cluster framework, there may be specific
    ways to handle—or at least log—any bad values as errors:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 有了所有准备好的数字，我们可以对它们进行迭代，将输入中的每个值从字符串值转换为实际的`int`，并对它们进行处理。如果输入中的值无法转换为`int`，我们暂时将其跳过，尽管根据调用集群框架的不同，可能有特定的方法来处理——或至少记录——任何错误的值：
- en: '[PRE34]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The script can be tested by echoing a list of numbers, and piping that into
    `python factors_stdin.py`. The results are printed, one result per line, which
    would be accepted by a calling program as standard output, ready to be passed
    to some other process that accepted standard input:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过回显数字列表并将其传输到`python factors_stdin.py`来测试脚本。结果将被打印，每行一个结果，这将被调用程序接受为标准输出，准备传递给接受标准输入的其他进程：
- en: '![](assets/86edbbca-29df-41ae-a84a-1510adbec05b.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/86edbbca-29df-41ae-a84a-1510adbec05b.png)'
- en: 'If the source numbers are in a file (`hugos_numbers.txt`, in the chapter code),
    those can be used just as easily, and generate the same results:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果源数字在一个文件中（在本章代码中为`hugos_numbers.txt`），那么它们可以同样轻松地使用，并生成相同的结果：
- en: '![](assets/d218df9b-3663-413d-88e0-8db471ab13be.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/d218df9b-3663-413d-88e0-8db471ab13be.png)'
- en: 'If the cluster environment expects command-line arguments to be passed, a script
    can be written to accommodate that as well. It starts with much the same code:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果集群环境期望传递命令行参数，那么可以编写一个脚本来适应这一点。它从很大程度上与相同的代码开始：
- en: '[PRE35]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Where it deviates is in acquiring the numbers to be processed. Since they are
    passed as command-line values, they will be part of the `argv` list (another item
    provided by Python''s `sys` module), after the script name. The balance of this
    process is identical to the `stdin` based script:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 它的不同之处在于获取要处理的数字。由于它们作为命令行值传递，它们将成为`argv`列表的一部分（Python的`sys`模块提供的另一个项目），在脚本名称之后。这个过程的平衡与基于`stdin`的脚本完全相同：
- en: '[PRE36]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output, as with the previous script, is simply printed to the console,
    and would be accepted as standard input by any other processes that it was handed
    off to:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的脚本一样，输出只是简单地打印到控制台，并且会被传递给任何其他进程作为标准输入。
- en: '![](assets/ed182f45-e7e2-4955-8673-7123a4fdb861.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ed182f45-e7e2-4955-8673-7123a4fdb861.png)'
- en: Python, Hadoop, and Spark
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python、Hadoop和Spark
- en: It's likely that the most common or popular of the large-scale, cluster computing
    frameworks available is Hadoop. Hadoop is a collection of software that provides
    cluster computing capabilities across networked computers, as well as a distributed
    storage mechanism that can be thought of as a network-accessible filesystem.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模集群计算框架中最常见或最受欢迎的可能是Hadoop。Hadoop是一组软件，提供了网络计算机上的集群计算能力，以及可以被视为网络可访问文件系统的分布式存储机制。
- en: Among the utilities it provides is Hadoop Streaming ([https://hadoop.apache.org/docs/r1.2.1/streaming.html](https://hadoop.apache.org/docs/r1.2.1/streaming.html)),
    which allows for the creation and execution of Map/Reduce jobs using any executable
    or script as a mapper and/or reducer. Hadoop's operational model, at least for
    processes that can use Streaming, is file-centric, so processes written in Python
    and executed under Hadoop will tend to fall into the `stdin` based category that
    we discussed earlier more often than not.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 它提供的实用程序之一是Hadoop Streaming（[https://hadoop.apache.org/docs/r1.2.1/streaming.html](https://hadoop.apache.org/docs/r1.2.1/streaming.html)），它允许使用任何可执行文件或脚本作为映射器和/或减速器来创建和执行Map/Reduce作业。至少对于可以使用Streaming的进程，Hadoop的操作模型是以文件为中心的，因此在Hadoop下编写并执行的进程往往更多地属于我们之前讨论过的基于`stdin`的类别。
- en: Apache Spark is another option in the large-scale, cluster computing frameworks
    arena. Spark is a distributed, general-purpose framework, and has a Python API
    (`pyspark`, [http://spark.apache.org/docs/2.2.0/api/python/pyspark.html](http://spark.apache.org/docs/2.2.0/api/python/pyspark.html))
    available for installation with `pip`, allowing for more direct access to its
    capabilities.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是大规模集群计算框架领域的另一个选择。Spark是一个分布式的通用框架，并且有一个Python API（`pyspark`，[http://spark.apache.org/docs/2.2.0/api/python/pyspark.html](http://spark.apache.org/docs/2.2.0/api/python/pyspark.html)）可用于使用`pip`进行安装，从而更直接地访问其功能。
- en: Summary
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have covered all of the basic permutations (serial and parallel,
    local and remote/distributed) of multiprocessing in Python, as it would apply
    to custom HPC operations. The basics needed for integrating a process written
    in Python to be executed by a large-scale cluster computing system such as Hadoop
    are quite basic—simple executable scripts—and the integration prospects with those
    system are as varied as the systems themselves.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经涵盖了Python中多处理的所有基本排列（串行和并行，本地和远程/分布式），因为它适用于自定义HPC操作。将Python编写的进程集成到Hadoop等大规模集群计算系统中所需的基础知识非常基础——简单的可执行脚本——并且与这些系统的集成前景与系统本身一样多样。
