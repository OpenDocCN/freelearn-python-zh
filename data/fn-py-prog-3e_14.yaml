- en: '14'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '14'
- en: The Multiprocessing, Threading, and Concurrent.Futures Modules
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 多进程、多线程和Concurrent.Futures模块
- en: When we eliminate the complexities of shared state and design around pure functions
    with non-strict processing, we can leverage concurrency and parallelism to improve
    performance. In this chapter, we’ll look at some of the multiprocessing and multithreading
    techniques that are available to us. Python library packages become particularly
    helpful when applied to algorithms designed from a functional viewpoint.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们消除共享状态的复杂性，并围绕非严格处理的纯函数进行设计时，我们可以利用并发和并行来提高性能。在本章中，我们将探讨一些可用的多进程和多线程技术。当应用于从函数式视角设计的算法时，Python库包变得特别有帮助。
- en: The central idea here is to distribute a functional program across several threads
    within a process or across several processes in a CPU. If we’ve created a sensible
    functional design, we can avoid complex interactions among application components;
    we have functions that accept argument values and produce results. This is an
    ideal structure for a process or a thread.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的核心思想是将函数式程序分布在进程内的几个线程或CPU内的几个进程之间。如果我们创建了一个合理的函数式设计，我们可以避免应用程序组件之间的复杂交互；我们有接受参数值并产生结果的函数。这对于进程或线程来说是一个理想的结构。
- en: 'In this chapter, we’ll focus on several topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将关注几个主题：
- en: The general idea of functional programming and concurrency.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数式编程和并发的一般概念。
- en: What concurrency really means when we consider cores, CPUs, and OS-level concurrency
    and parallelism. It’s important to note that concurrency won’t magically make
    a bad algorithm faster.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们考虑核心、CPU和操作系统级别的并发与并行时，并发真正意味着什么。需要注意的是，并发并不能神奇地让一个糟糕的算法变快。
- en: Using the built-in `multiprocessing` and `concurrent.futures` modules. These
    modules allow a number of parallel execution techniques. The external `dask` package
    can do much of this as well.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用内置的`multiprocessing`和`concurrent.futures`模块。这些模块允许多种并行执行技术。外部的`dask`包也能做很多这方面的工作。
- en: We’ll focus on process-level parallelism more than multithreading. Using process
    parallelism allows us to completely ignore Python’s Global Interpreter Lock (GIL).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将更多地关注进程级别的并行性，而不是多线程。使用进程并行性允许我们完全忽略Python的全局解释器锁（GIL）。
- en: For more information on Python’s GIL, see [https://docs.python.org/3/glossary.html#term-global-interpreter-lock](https://docs.python.org/3/glossary.html#term-global-interpreter-lock).
    Also see [https://peps.python.org/pep-0684/](https://peps.python.org/pep-0684/)
    for a proposal to alter the way the GIL operates. Additionally, see [https://github.com/colesbury/nogil](https://github.com/colesbury/nogil)
    for a project that proposes a way to remove the GIL entirely.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Python的全局解释器锁（GIL）的更多信息，请参阅[https://docs.python.org/3/glossary.html#term-global-interpreter-lock](https://docs.python.org/3/glossary.html#term-global-interpreter-lock)。有关修改GIL操作方式的提案，请参阅[https://peps.python.org/pep-0684/](https://peps.python.org/pep-0684/)。此外，请参阅[https://github.com/colesbury/nogil](https://github.com/colesbury/nogil)以了解一个提出完全移除GIL的项目。
- en: The GIL is very much part of Python 3.10, meaning some kinds of compute-intensive
    multithreading won’t show significant speedups.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: GIL在Python 3.10中是不可或缺的一部分，这意味着某些类型的计算密集型多线程不会显示出明显的速度提升。
- en: We’ll focus on concurrency throughout the chapter. Concurrent work is interleaved,
    distinct from parallel work, which requires multiple cores or multiple processors.
    We don’t want to dig too deeply into the nuanced distinctions between concurrency
    and parallelism. Our focus is on leveraging a functional approach, more than exploring
    all of the ways work can be accomplished in a modern CPU with a multi-processing
    OS.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中关注并发。并发工作交织在一起，与需要多个核心或多个处理器的并行工作不同。我们不想深入探讨并发和并行之间的细微差别。我们的重点是利用函数式方法，而不是探索在现代多进程操作系统上完成工作所有可能的方式。
- en: 14.1 Functional programming and concurrency
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1 函数式编程和并发
- en: The most effective concurrent processing occurs when there are no dependencies
    among the tasks being performed. The biggest difficulty in developing concurrent
    (or parallel) programming is the complications arising from coordinating updates
    to shared resources, where tasks depend on a common resource.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行的任务之间没有依赖关系时，最有效的并发处理才会发生。在开发并发（或并行）编程时，最大的困难来自于协调对共享资源的更新，其中任务依赖于一个公共资源。
- en: When following functional design patterns, we tend to avoid stateful programs.
    A functional design should minimize or eliminate concurrent updates to shared
    objects. If we can design software where lazy, non-strict evaluation is central,
    we can also design software where concurrent evaluation is helpful. In some cases,
    parts of an application can have an embarrassingly parallel design, where most
    of the work can be done concurrently with few or no interactions among computations.
    Mappings and filterings, in particular, benefit from parallel processing; reductions
    typically can’t be done in parallel.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当遵循函数式设计模式时，我们倾向于避免有状态的程序。函数式设计应该最小化或消除对共享对象的并发更新。如果我们能够设计出以懒加载、非严格评估为中心的软件，我们也可以设计出有助于并发评估的软件。在某些情况下，应用程序的某些部分可以具有令人尴尬的并行设计，其中大部分工作可以并发完成，计算之间几乎没有交互。映射和过滤尤其受益于并行处理；归约通常不能并行执行。
- en: The frameworks we’ll focus on all make use of an essential `map()` function
    to allocate work to multiple workers in a pool. This fits nicely with the higher-order
    functional design we’ve been looking at throughout this book. If we’ve built our
    application with a particular focus on the `map()` function, then partitioning
    the work into processes or threads should not involve a breaking change.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要关注的框架都使用一个基本的 `map()` 函数来将工作分配给池中的多个工作者。这与我们在整本书中一直在探讨的高级函数式设计非常契合。如果我们已经针对
    `map()` 函数构建了我们的应用程序，那么将工作分区成进程或线程不应该涉及破坏性的变更。
- en: 14.2 What concurrency really means
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2 并发真正意味着什么
- en: In a small computer, with a single processor and a single core, all evaluations
    are serialized through the one and only core of the processor. The OS will interleave
    multiple processes and multiple threads through clever time-slicing arrangements
    to make it appear as if things are happening concurrently.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在一台小型计算机中，只有一个处理器和一个核心，所有的评估都通过处理器唯一的那个核心进行序列化。操作系统将通过巧妙的时间切片安排来交错多个进程和多个线程，使其看起来像是在并发发生。
- en: On a computer with multiple CPUs or multiple cores in a single CPU, there can
    be some actual parallel processing of CPU instructions. All other concurrency
    is simulated through time slicing at the OS level. A macOS X laptop can have 200
    concurrent processes that share the CPU; this is many more processes than the
    number of available cores. From this, we can see that OS time slicing is responsible
    for most of the apparently concurrent behavior of the system as a whole.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有多个 CPU 或单个 CPU 中有多个核心的计算机上，可以有一些实际的并行处理 CPU 指令。所有其他并发性都是通过操作系统级别的时间切片来模拟的。一台
    macOS X 笔记本电脑可以有 200 个并发进程共享 CPU；这比可用的核心数量多得多。从这一点来看，我们可以看出操作系统的时间切片负责了整个系统大部分看似并发的行为。
- en: 14.2.1 The boundary conditions
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.2.1 边界条件
- en: Let’s consider a hypothetical algorithm that has a complexity described by O(n²).
    This generally means two nested `for` statements, each of which is processing
    n items. Let’s assume the inner `for` statement’s body involves 1,000 Python operation
    codes. When processing 10,000 objects, this could execute 100 billion Python operations.
    We can call this the essential processing budget. We can try to allocate as many
    processes and threads as we feel might be helpful, but the processing budget can’t
    change.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个假设的算法，其复杂度由 O(n²) 描述。这通常意味着两个嵌套的 `for` 语句，每个语句都在处理 n 个项目。让我们假设内部 `for`
    语句的主体涉及 1,000 个 Python 操作码。当处理 10,000 个对象时，这可能执行 1000 亿次 Python 操作。我们可以称之为基本处理预算。我们可以尝试分配尽可能多的进程和线程，但我们无法改变处理预算。
- en: The individual CPython bytecodes—the internal implementation of Python statements
    and expressions—don’t all share a single, uniform execution time. However, a long-term
    average on a macOS X laptop shows that we can expect about 60 MB of bytecode operations
    to be executed per second. This means that our 100 billion bytecode operation
    could take about 1,666 seconds, or 28 minutes.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 单个 CPython 字节码——Python 语句和表达式的内部实现——并不都共享相同的、统一的执行时间。然而，在 macOS X 笔记本电脑上的长期平均数据显示，我们每秒可以期望执行大约
    60 MB 的字节码操作。这意味着我们的 1000 亿次字节码操作可能需要大约 1,666 秒，或 28 分钟。
- en: 'If we have a dual-processor, four-core computer, then we might cut the elapsed
    time to 25% of the original total: about 7 minutes. This presumes that we can
    partition the work into four (or more) independent OS processes.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一台双核四核的计算机，那么我们可能将所需的时间缩短到原始总时间的 25%：大约 7 分钟。这假设我们可以将工作分成四个（或更多）独立的操作系统进程。
- en: The important consideration here is that the overall budget of 100 billion bytecodes
    can’t be changed. Concurrency won’t magically reduce the workload. It can only
    change the schedule to perhaps reduce the elapsed time to execute all those bytecodes.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的重要考虑是，100亿字节码的整体预算是无法改变的。并发不会神奇地减少工作量。它只能改变调度，以可能减少执行所有这些字节码的经过时间。
- en: Switching to a better algorithm with a complexity of O(nlog n) can reduce the
    workload dramatically. We need to measure the actual speedup to determine the
    impact; the following example includes a number of assumptions. Instead of doing
    10,000² iterations, we may only do 10,000log 10,000 ≈ 132,877 iterations, dropping
    from 100 billion operations to a number on the order of 133 thousand operations.
    This could be as small as ![7100-](img/file128.jpg) of the original time. Concurrency
    can’t provide the kind of dramatic improvements that algorithm change will have.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 转换为具有O(nlog n)复杂度的更好算法可以显著减少工作量。我们需要测量实际的加速来确定影响；以下示例包含了一些假设。我们可能不需要进行10,000²次迭代，而只需进行10,000log
    10,000 ≈ 132,877次迭代，从100亿操作减少到大约133,000次操作。这可能是原始时间的![7100-](img/file128.jpg)小。并发无法提供算法更改将带来的那种戏剧性改进。
- en: 14.2.2 Sharing resources with process or threads
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.2.2 与进程或线程共享资源
- en: The OS assures us there is little or no interaction between processes. When
    creating an application where multiple processes must interact, a common OS resource
    must be explicitly shared. This can be a common file, a shared-memory object,
    or a semaphore with a shared state between the processes. Processes are inherently
    independent; interaction among them is the exception, not the rule.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统向我们保证，进程之间几乎没有或没有交互。当创建需要多个进程交互的应用程序时，必须显式共享一个常见的操作系统资源。这可能是一个公共文件、共享内存对象或具有进程之间共享状态的信号量。进程本质上是独立的；它们之间的交互是例外，而不是规则。
- en: Multiple threads, in contrast, are part of a single process; all threads of
    a process generally share resources, with one special case. Thread-local memory
    can be freely used without interference from other threads. Outside thread-local
    memory, operations that write to memory can set the internal state of the process
    in a potentially unpredictable order. One thread can overwrite the results of
    another thread. A technique for mutually exclusive access—often a form of locking—must
    be used to avoid problems. As noted previously, the overall sequence of instruction
    from concurrent threads and processes are generally interleaved among the cores
    in an unpredictable order. With this concurrency comes the possibility of destructive
    updates to shared variables and the need for mutually exclusive access.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，多个线程属于单个进程的一部分；一个进程的所有线程通常共享资源，有一个特殊情况。线程局部内存可以自由使用，而不会受到其他线程的干扰。在线程局部内存之外，写入内存的操作可能会以不可预测的顺序设置进程的内部状态。一个线程可以覆盖另一个线程的结果。为了防止问题，必须使用互斥访问技术——通常是一种锁定形式。如前所述，来自并发线程和进程的指令的整体顺序通常以不可预测的顺序在核心之间交错。这种并发带来了对共享变量的破坏性更新的可能性，以及需要互斥访问。
- en: The existence of concurrent object updates can create havoc when trying to design
    multithreaded applications. Locking is one way to avoid concurrent writes to shared
    objects. Avoiding shared objects in general is another viable design technique.
    The second technique—avoiding writes to shared objects—is often also applicable
    to functional programming.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当尝试设计多线程应用程序时，并发对象更新的存在可能会造成混乱。锁定是一种避免对共享对象进行并发写入的方法。避免使用共享对象通常也是一种可行的设计技术。第二种技术——避免写入共享对象——通常也适用于函数式编程。
- en: In CPython, the GIL is used to ensure that OS thread scheduling will not interfere
    with the internals of maintaining Python data structures. In effect, the GIL changes
    the granularity of scheduling from machine instructions to groups of Python virtual
    machine operations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPython中，全局解释器锁（GIL）用于确保操作系统线程调度不会干扰维护Python数据结构的内部机制。实际上，GIL改变了调度的粒度，从机器指令到Python虚拟机操作组。
- en: Pragmatically, the performance impact of the GIL on a wide variety of application
    types is often negligible. For the most part, compute-intensive applications tend
    to see the largest impact from GIL scheduling. I/O-intensive applications see
    little impact because the threads spend more time waiting for I/O to complete.
    A far greater impact on performance comes from the fundamental inherent complexity
    of the algorithm being implemented.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从实用主义的角度来看，全局解释器锁（GIL）对各种应用类型性能的影响通常是可以忽略不计的。大部分情况下，计算密集型应用倾向于看到GIL调度的最大影响。I/O密集型应用的影响很小，因为线程花更多的时间等待I/O完成。对性能影响更大的因素是正在实施算法的基本内在复杂性。
- en: 14.2.3 Where benefits will accrue
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.2.3 利益将如何积累
- en: A program that does a great deal of calculation and relatively little I/O will
    not see much benefit from concurrent processing on a single core. If a calculation
    has a budget of 28 minutes of computation, then interleaving the operations in
    different ways won’t have a dramatic impact. Using eight cores for parallel computation
    may cut the time by approximately one-eighth. The actual time savings depend on
    the OS and language overheads, which are difficult to predict.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一个进行大量计算和相对较少I/O的程序在单核上的并发处理中不会看到太多好处。如果一个计算有28分钟的预算，那么以不同方式交错操作不会产生显著影响。使用八个核心进行并行计算可能将时间缩短大约八分之一。实际的时间节省取决于操作系统和语言开销，这些开销很难预测。
- en: When a calculation involves a great deal of I/O, then interleaving CPU processing
    while waiting for I/O requests to complete can dramatically improve performance.
    The idea is to do computations on some pieces of data while waiting for the OS
    to complete the I/O of other pieces of data. Because I/O generally involves a
    great deal of waiting, an eight-core processor can interleave the work from dozens
    (or hundreds) of concurrent I/O requests.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当计算涉及大量的I/O时，在等待I/O请求完成的同时交错CPU处理可以显著提高性能。这个想法是在等待操作系统完成其他数据块的I/O时，对某些数据块进行计算。由于I/O通常涉及大量的等待，一个八核处理器可以交错处理来自数十（或数百）个并发I/O请求的工作。
- en: Concurrency is a core principle behind Linux. If we couldn’t do computation
    while waiting for I/O, then our computer would freeze while waiting for each network
    request to finish. A website download would involve waiting for the initial HTML
    and then waiting for each individual graphic to arrive. All the while, the keyboard,
    mouse, and display would not work.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 并发是Linux的核心原则之一。如果我们不能在等待I/O的同时进行计算，那么我们的计算机在等待每个网络请求完成时会冻结。一个网站下载将涉及等待初始HTML，然后等待每个单独的图形到达。在此期间，键盘、鼠标和显示器将无法工作。
- en: 'Here are two approaches to designing applications that interleave computation
    and I/O:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两种设计交错计算和I/O的应用程序的方法：
- en: We can create a pipeline of processing stages. An individual item must move
    through all of the stages where it is read, filtered, computed, aggregated, and
    written. The idea of multiple concurrent stages means there will be distinct data
    objects in each stage. Time slicing among the stages will allow computation and
    I/O to be interleaved.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以创建一个由处理阶段组成的流水线。单个项目必须通过所有阶段，在这些阶段中它被读取、过滤、计算、聚合，并写入。多个并发阶段的概念意味着每个阶段都会有不同的数据对象。阶段之间的时间切片将允许计算和I/O交错进行。
- en: We can create a pool of concurrent workers, each of which performs all of the
    processing for a data item. The data items are assigned to workers in the pool
    and the results are collected from the workers.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以创建一个并发工作者池，每个工作者执行数据项的所有处理。数据项被分配给池中的工作者，结果从工作者那里收集。
- en: The differences between these approaches aren’t crisp. It’s common to create
    a hybrid mixture where one stage of a pipeline involves a pool of workers to make
    that stage as fast as the other stages. There are some formalisms that make it
    somewhat easier to design concurrent programs. The Communicating Sequential Processes
    (CSP) paradigm can help design message-passing applications. Packages such as
    `pycsp` can be used to add CSP formalisms to Python.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法之间的区别并不清晰。通常，会创建一个混合混合体，其中流水线的一个阶段涉及一个工作者池，以便使该阶段与其他阶段一样快。有一些形式化方法使设计并发程序变得相对容易。通信顺序进程（CSP）范式可以帮助设计消息传递应用程序。可以使用如`pycsp`之类的包将CSP形式化添加到Python中。
- en: I/O-intensive programs often gain the most dramatic benefits from concurrent
    processing. The idea is to interleave I/O and processing. CPU-intensive programs
    will see smaller benefits from concurrent processing.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: I/O密集型程序通常能从并发处理中获得最显著的好处。其思路是将I/O和数据处理交织在一起。CPU密集型程序从并发处理中获得的益处较小。
- en: 14.3 Using multiprocessing pools and tasks
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.3 使用多进程池和任务
- en: Python’s `multiprocessing` package introduces the concept of a `Pool` object.
    A `Pool` object contains a number of worker processes and expects these processes
    to be executed concurrently. This package allows OS scheduling and time slicing
    to interleave execution of multiple processes. The intention is to keep the overall
    system as busy as possible.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Python的`multiprocessing`包引入了`Pool`对象的概念。`Pool`对象包含多个工作进程，并期望这些进程并发执行。这个包允许操作系统调度和时分复用来交织多个进程的执行。目的是让整个系统尽可能忙碌。
- en: To make the most of this capability, we need to decompose our application into
    components for which non-strict, concurrent execution is beneficial. The overall
    application must be built from discrete tasks that can be processed in an indefinite
    order.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分利用这一功能，我们需要将我们的应用程序分解成对非严格并发执行有益的组件。整个应用程序必须由可以按不确定顺序处理的离散任务构建而成。
- en: An application that gathers data from the internet through web scraping, for
    example, is often optimized through concurrent processing. A number of individual
    processes can be waiting for the data to download, while others are performing
    the scraping operation on data that’s been received. We can create a `Pool` object
    of several identical workers, which implement the website scraping. Each worker
    is assigned tasks in the form of URLs to be analyzed. Multiple workers waiting
    for downloads have little processing overhead. The workers with completely downloaded
    pages, on the other hand, can perform the real work of extracting data from the
    content.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个通过网络爬虫从互联网收集数据的应用程序，通常通过并发处理进行优化。多个单独的进程可以等待数据下载，而其他进程则在已接收的数据上执行爬取操作。我们可以创建一个包含几个相同工作者的`Pool`对象，这些工作者执行网站爬取。每个工作者被分配以URL形式的分析任务。等待下载的多个工作者几乎没有处理开销。另一方面，已经完全下载页面的工作者可以执行从内容中提取数据的实际工作。
- en: An application that analyzes multiple log files is also a good candidate for
    concurrent processing. We can create a `Pool` object of analytical workers. We
    can assign each log file to a worker; this allows reading and analysis to proceed
    concurrently among the various workers in the `Pool` object. Each individual worker
    will be performing both I/O and computation. However, some workers can be analyzing
    while other workers are waiting for I/O to complete.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 分析多个日志文件的应用程序也是并发处理的良好候选者。我们可以创建一个分析工作者`Pool`对象。我们可以将每个日志文件分配给一个工作者；这允许在`Pool`对象中的各种工作者之间并发进行读取和分析。每个工作者将执行I/O和计算。然而，一些工作者可以在其他工作者等待I/O完成时进行分析。
- en: Because the benefits depend on difficult-to-predict timing for input and output
    operations, multiprocessing always involves experimentation. Changing the pool
    size and measuring elapsed time is an essential part of implementing concurrent
    applications.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于益处取决于难以预测的输入和输出操作的时机，多进程总是涉及实验。更改池大小并测量经过时间是实现并发应用程序的基本部分。
- en: 14.3.1 Processing many large files
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.3.1 处理大量大文件
- en: 'Here is an example of a multiprocessing application. We’ll parse Common Log
    Format (CLF) lines in web log files. This is the generally used format for web
    server access logs. The lines tend to be long, but look like the following when
    wrapped to the book’s margins:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个多进程应用程序的示例。我们将解析网络日志文件中的通用日志格式（CLF）行。这是通常用于Web服务器访问日志的格式。这些行通常很长，但当你将其折叠到书的边缘时，看起来如下所示：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We often have large numbers of files that we’d like to analyze. The presence
    of many independent files means that concurrency will have some benefit for our
    scraping process. Some workers will be waiting for data, while others can be doing
    the compute-intensive portion of the work.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常有大量的文件需要分析。许多独立文件的存在意味着并发处理对我们的爬取过程将有一些好处。一些工作者将等待数据，而其他工作者可以执行计算密集型的工作部分。
- en: 'We’ll decompose the analysis into two broad areas of functionality. The first
    phase of processing is the essential parsing of the log files to gather the relevant
    pieces of information. We’ll further decompose the parsing phase into four stages.
    They are as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分析分解为两个广泛的功能区域。处理的第一阶段是解析日志文件以收集相关信息的基本解析。我们将进一步将解析阶段分解为四个阶段。具体如下：
- en: All the lines from multiple source log files are read.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从多个源日志文件中读取所有行。
- en: Then, we create simple `NamedTuple` objects from the lines of log entries in
    a collection of files.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们从一个文件集合中的日志条目行创建简单的`NamedTuple`对象。
- en: The details of more complex fields such as dates and URLs are parsed separately.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 日期和URL等更复杂字段的详细信息将单独解析。
- en: Uninteresting paths from the logs are rejected, leaving the interesting paths
    for further processing.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从日志中拒绝无趣的路径，留下有趣的路径以供进一步处理。
- en: Once past the parsing phase, we can perform a large number of analyses. For
    our purposes in demonstrating the `multiprocessing` module, we’ll look at a simple
    analysis to count occurrences of specific paths.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦过了解析阶段，我们可以执行大量分析。为了演示`multiprocessing`模块的目的，我们将查看一个简单的分析来计数特定路径的出现次数。
- en: The first portion is reading from the source files. Python’s use of file iterators
    will translate into lower-level OS requests for the buffering of data. Each OS
    request means that the process must wait for the data to become available.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分是从源文件中读取。Python对文件迭代器的使用将转换为底层OS请求以缓冲数据。每个OS请求意味着进程必须等待数据可用。
- en: Clearly, we want to interleave the other operations so that they are not all
    waiting for I/O to complete. The operations can be imagined to form a spectrum,
    from processing individual rows to processing whole files. We’ll look at interleaving
    whole files first, as this is relatively simple to implement.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们希望交织其他操作，以便它们不是都在等待I/O完成。这些操作可以想象成从处理单个行到处理整个文件的一个光谱。我们将首先查看整个文件的交织，因为这相对容易实现。
- en: 'The functional design for parsing Apache CLF files can look as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 解析Apache CLF文件的功能设计可能如下所示：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This function decomposes the larger parsing problem into a number of functions.
    The `local_gzip()` function reads rows from locally cached GZIP files. The `access_iter()`
    function creates a `NamedTuple` object for each row in the access log. The `access_detail_iter()`
    function expands on some of the more difficult-to-parse fields. Finally, the `path_filter()`
    function discards some paths and file extensions that aren’t of much analytical
    value.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将更大的解析问题分解为多个函数。`local_gzip()`函数从本地缓存的GZIP文件中读取行。`access_iter()`函数为访问日志中的每一行创建一个`NamedTuple`对象。`access_detail_iter()`函数扩展了一些更难解析的字段。最后，`path_filter()`函数丢弃了一些没有太多分析价值的路径和文件扩展名。
- en: 'It can help to visualize this kind of design as a shell-like pipeline of processing,
    as shown here:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 将这种设计可视化为一个类似壳体的处理管道可能会有所帮助，如下所示：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This uses borrows the shell notation of a pipe (—) to pass data from process
    to process. Python doesn’t have this operator, directly.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这借鉴了管道（—）的shell符号来从进程到进程传递数据。Python没有这个操作符，直接使用。
- en: 'Pragmatically, we can use the `toolz` module to define this pipeline:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 实用上，我们可以使用`toolz`模块来定义这个管道：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For more on the `toolz` module, see [Chapter 11](Chapter_11.xhtml#x1-23500011),
    [The Toolz Package](Chapter_11.xhtml#x1-23500011).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于`toolz`模块的信息，请参阅[第11章](Chapter_11.xhtml#x1-23500011)，[The Toolz Package](Chapter_11.xhtml#x1-23500011)。
- en: We’ll focus on designing these four functions that process data in stages. The
    idea is to interleave intensive processing with waiting for I/O to finish.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于设计这四个按阶段处理数据的函数。想法是将密集处理与等待I/O完成交织在一起。
- en: 14.3.2 Parsing log files – gathering the rows
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.3.2 解析日志文件 – 收集行
- en: 'Here is the first stage in parsing a large number of files: reading each file
    and producing a simple sequence of lines. As the log files are saved in the `.gzip`
    format, we need to open each file with the `gzip.open()` function.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 解析大量文件的第一阶段如下：读取每个文件并生成一个简单的行序列。由于日志文件以`.gzip`格式保存，我们需要使用`gzip.open()`函数打开每个文件。
- en: 'The following `local_gzip()` function reads lines from locally cached files:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下`local_gzip()`函数从本地缓存的文件中读取行：
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The function iterates through all lines of a file. We’ve created a composite
    function that encapsulates the details of opening a log file compressed with the
    `.gzip` format, breaking a file into a sequence of lines, and stripping the newline
    (`\n`) characters.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 函数遍历文件的所有行。我们创建了一个复合函数，用于封装以`.gzip`格式压缩的日志文件打开的细节，将文件分解为一系列行，并删除换行符（`\n`）。
- en: Additionally, this function also encapsulates a non-standard encoding for the
    files. Instead of Unicode, encoded in a standard format like UTF-8 or UTF-16,
    the files are encoded in old US-ASCII. This is very similar to UTF-8\. In order
    to be sure the log entries are read properly, the exact encoding is supplied.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，此函数还封装了文件的非标准编码。与以UTF-8或UTF-16等标准格式编码的Unicode不同，文件使用旧的US-ASCII编码。这与UTF-8非常相似。为了确保日志条目被正确读取，提供了确切的编码。
- en: This function is a close fit with the way the `multiprocessing` module works.
    We can create a worker pool and map tasks (such as `.gzip` file reading) to the
    pool of processes. If we do this, we can read these files in parallel; the open
    file objects will be part of separate processes, and the resource consumption
    and wait time will be managed by the OS.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数与`multiprocessing`模块的工作方式非常吻合。我们可以创建一个工作池并将任务（如`.gzip`文件读取）映射到进程池。如果我们这样做，我们可以并行读取这些文件；打开的文件对象将是单独进程的一部分，资源消耗和等待时间将由操作系统管理。
- en: An extension to this design can include a second function to transfer files
    from the web host using SFTP or a RESTful API if one is available. As the files
    are collected from the web server, they can be analyzed using the `local_gzip()`
    function.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此设计的扩展可以包括第二个功能，用于使用SFTP或RESTful API（如果可用）从网络主机传输文件。当文件从网络服务器收集时，可以使用`local_gzip()`函数进行分析。
- en: The results of the `local_gzip()` function are used by the `access_iter()` function
    to create named tuples for each row in the source file that describes file access
    by the web server.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`local_gzip()`函数的结果被`access_iter()`函数用于为源文件中的每一行创建描述网络服务器文件访问的命名元组。'
- en: 14.3.3 Parsing log lines into named tuples
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.3.3 将日志行解析为命名元组
- en: Once we have access to all of the lines of each log file, we can extract details
    of the access that’s described. We’ll use a regular expression to decompose the
    line. From there, we can build a `NamedTuple` object.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了每个日志文件的所有行，我们就可以提取描述的访问细节。我们将使用正则表达式分解行。从那里，我们可以构建一个`NamedTuple`对象。
- en: 'Each individual access can be summarized as a subclass of `NamedTuple`, as
    follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单独的访问可以总结为`NamedTuple`的一个子类，如下所示：
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The method for building an `Access` object, `create()`, from the source text
    contains a lengthy regular expression to parse lines in a CLF file. This is quite
    complex, but we can use a railroad diagram to help simplify it. The following
    image shows the various elements and how they’re identified by the regular expression:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从源文本构建`Access`对象的方法`create()`包含一个长的正则表达式，用于解析CLF文件中的行。这相当复杂，但我们可以使用铁路图来帮助简化它。以下图像显示了各种元素以及它们如何被正则表达式识别：
- en: '![hd.hsinisunus[tat]srarssdssbnbsrarsuausoiopdodpsospimnimpeneptaitapyoypenepsnspsgsaeneaeneaeyeaquyquatgtatntafeyfeaeyeatitcn-ncr-rcceecuiuce-ecrrcrrctetistieseessestsessseeeeaaetptpttprrggyayaaeecccnneeett](img/file129.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![hd.hsinisunus[tat]srarssdssbnbsrarsuausoiopdodpsospimnimpeneptaitapyoypenepsnspsgsaeneaeneaeyeaquyquatgtatntafeyfeaeyeatitcn-ncr-rcceecuiuce-ecrrcrrctetistieseessestsessseeeeaaetptpttprrggyayaaeecccnneeett](img/file129.jpg)'
- en: 'Figure 14.1: Regular expression diagram for parsing log files'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1：解析日志文件的正则表达式图
- en: This diagram shows the sequence of clauses in the regular expression. Each rectangular
    box represents a named capture group. For example, `(?P<host>[\d\.]+)` is a group
    named `host`. The ovals and circles are classes of characters (e.g., digit) or
    specific characters (e.g., `.`) that comprise the contents of the capture group.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此图显示了正则表达式中子句的顺序。每个矩形框代表一个命名的捕获组。例如，`(?P<host>[\d\.]+)`是一个名为`host`的组。椭圆形和圆形是字符类（例如，数字）或特定字符（例如，`.`）的类别，它们构成了捕获组的内容。
- en: We used this regular expression to break each row into a dictionary of nine
    individual data elements. The use of `[]` and `"` to delimit complex fields such
    as the `time`, `request`, `referer`, and `user_agent` parameters can be handled
    elegantly by transforming the text into a `NamedTuple` object.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用这个正则表达式将每一行分解为包含九个单独数据元素的字典。使用`[]`和`"`来界定复杂字段（如`time`、`request`、`referer`和`user_agent`参数）可以通过将文本转换为`NamedTuple`对象来优雅地处理。
- en: We’ve taken pains to ensure that the `NamedTuple` field names match the regular
    expression group names in the `(?P<name>)` constructs for each portion of the
    record. By making sure the names match, we can very easily transform the parsed
    dictionary into a tuple for further processing. This means we’ve spelled referrer
    wrong to fit with the RFC documentation.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们努力确保`NamedTuple`字段名称与每个记录部分的`(?P<name>)`构造中的正则表达式组名称相匹配。通过确保名称匹配，我们可以非常容易地将解析后的字典转换为一个元组以进行进一步处理。这意味着我们为了与RFC文档兼容，错误地拼写了“referrer”。
- en: 'Here is the `access_iter()` function, which requires each file to be represented
    as an iterator over the lines of the file:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是`access_iter()`函数，它要求每个文件都表示为文件行的迭代器：
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The output from the `local_gzip()` function is a sequence of strings. The outer
    sequence is based on the lines from individual log files. If the line matches
    the given pattern, it’s a file access of some kind. We can create an `Access`
    instance from the dictionary of text parsed by the regular expression. Non-matching
    lines are quietly discarded.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`local_gzip()`函数的输出是一系列字符串。外层序列基于单个日志文件的行。如果某行与给定的模式匹配，则表示某种类型的文件访问。我们可以从正则表达式解析的文本字典中创建一个`Access`实例。不匹配的行会被静默地丢弃。'
- en: The essential design pattern here is to build an immutable object from the results
    of a parsing function. In this case, the parsing function is a regular expression
    matcher. Other kinds of parsing can also fit this design pattern.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这里关键的设计模式是从解析函数的结果构建一个不可变对象。在这种情况下，解析函数是一个正则表达式匹配器。其他类型的解析也可以适应这种设计模式。
- en: 'There are some alternative ways to do this. For example, here’s a function
    that applies `map()` and `filter()`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些替代方法可以做到这一点。例如，这里有一个应用`map()`和`filter()`的函数：
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This `access_iter_2()` function transforms the output from the `local_gzip()`
    function into a sequence of `Access` instances. In this case, we apply the `Access.create()`
    function to the string iterator that results from reading a collection of files.
    The `filter()` function removes any `None` objects from the result of the `map()`
    function.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`access_iter_2()`函数将`local_gzip()`函数的输出转换为一个`Access`实例的序列。在这种情况下，我们将`Access.create()`函数应用于从读取文件集合得到的字符串迭代器。`filter()`函数从`map()`函数的结果中移除任何`None`对象。'
- en: Our point here is to show that we have a number of functional styles for parsing
    files. In [Chapter 4](Chapter_04.xhtml#x1-740004), [Working with Collections](Chapter_04.xhtml#x1-740004),
    we looked at very simple parsing. Here, we’re performing more complex parsing,
    using a variety of techniques.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里的目的是展示我们有一系列用于解析文件的函数式风格。在[第4章](Chapter_04.xhtml#x1-740004)，[处理集合](Chapter_04.xhtml#x1-740004)中，我们查看了一种非常简单的解析方法。在这里，我们执行更复杂的解析，使用各种技术。
- en: 14.3.4 Parsing additional fields of an Access object
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.3.4 解析访问对象的附加字段
- en: The initial `Access` object created previously doesn’t decompose some inner
    elements in the nine fields that comprise an access log line. We’ll parse those
    items separately from the overall decomposition into high-level fields. Doing
    these parsing operations separately makes each stage of processing simpler. It
    also allows us to replace one small part of the overall process without breaking
    the general approach to analyzing logs.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 之前创建的初始`Access`对象没有分解访问日志行中包含的九个字段中的某些内部元素。我们将从整体分解中单独解析这些项。单独执行这些解析操作使每个处理阶段更简单。这也允许我们在不破坏分析日志的一般方法的情况下替换整体过程的一个小部分。
- en: 'The resulting object from the next stage of parsing will be a `NamedTuple`
    subclass, `AccessDetails`, which wraps the original `Access` tuple. It will have
    some additional fields for the details parsed separately:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个解析阶段的输出对象将是一个`NamedTuple`子类，`AccessDetails`，它包装了原始的`Access`元组。它将包含一些用于解析的附加字段：
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `access` attribute is the original `Access` object, a collection of simple
    strings. The `time` attribute is the parsed `access.time` string. The `method`,
    `url`, and `protocol` attributes come from decomposing the `access.request` field.
    The `referrer` attribute is a parsed URL.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`access`属性是原始的`Access`对象，一组简单的字符串集合。`time`属性是解析的`access.time`字符串。`method`、`url`和`protocol`属性来自分解`access.request`字段。`referrer`属性是一个解析后的URL。'
- en: The `agent` attribute can also be broken down into fine-grained fields. The
    rules are quite complex, and we’ve decided that a dictionary mapping names to
    their associated values will be good enough.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`agent`属性也可以分解成更细粒度的字段。规则相当复杂，我们决定使用一个将名称映射到其相关值的字典就足够了。'
- en: 'Here are the three detail-level parsers for the fields to be decomposed:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是分解字段的三种详细级别解析器：
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We’ve written three parsers for the HTTP request, the time stamp, and the user
    agent information. The request value in a log is usually a three-word string such
    as `GET`` /some` `/path`` HTTP/1.1`. The `parse_request()` function extracts these
    three space-separated values. In the unlikely event that the path has spaces in
    it, we’ll extract the first word and the last word as the method and protocol;
    all the remaining words are part of the path.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为HTTP请求、时间戳和用户代理信息编写了三个解析器。日志中的请求值通常是一个三个单词的字符串，例如`GET /some/path HTTP/1.1`。`parse_request()`函数提取这三个空格分隔的值。在不太可能的情况下，如果路径中有空格，我们将提取第一个单词和最后一个单词作为方法和协议；所有剩余的单词都是路径的一部分。
- en: Time parsing is delegated to the `datetime` module. We’ve provided the proper
    format in the `parse_time()` function.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 时间解析委托给了`datetime`模块。我们在`parse_time()`函数中提供了适当的格式。
- en: Parsing the user agent is challenging. There are many variations; we’ve chosen
    a common one for the `parse_agent()` function. If the user agent text matches
    the given regular expression, we’ll use the attributes of the `AgentDetails` class.
    If the user agent information doesn’t match the regular expression, we’ll use
    the `None` value instead. The original text will be available in the `Access`
    object in either case.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 解析用户代理具有挑战性。有许多变化；我们为`parse_agent()`函数选择了一个常见的选项。如果用户代理文本与给定的正则表达式匹配，我们将使用`AgentDetails`类的属性。如果用户代理信息不匹配正则表达式，我们将使用`None`值代替。在两种情况下，原始文本都将可在`Access`对象中找到。
- en: 'We’ll use these three parsers to build `AccessDetails` instances from the given
    `Access` objects. The main body of the `access_detail_iter()` function looks like
    this:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这三个解析器从给定的`Access`对象构建`AccessDetails`实例。`access_detail_iter()`函数的主体看起来是这样的：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We’ve used a similar design pattern to the previous `access_iter()` function.
    A new object is built from the results of parsing some input object. The new `AccessDetails`
    object will wrap the previous `Access` object. This technique allows us to use
    immutable objects, yet still contains more refined information.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了与之前的`access_iter()`函数类似的设计模式。从解析某些输入对象的结果中构建一个新的对象。新的`AccessDetails`对象将包装之前的`Access`对象。这种技术允许我们使用不可变对象，同时仍然包含更详细的信息。
- en: 'This function is essentially a mapping from an `Access` object to a sequence
    of `AccessDetails` objects. Here’s an alternative design using the `map()` high-level
    function:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数本质上是一个从`Access`对象到`AccessDetails`对象序列的映射。这里是一个使用`map()`高级函数的替代设计：
- en: '[PRE11]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As we move forward, we’ll see that this variation fits in nicely with the way
    the `multiprocessing` module works.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们继续前进，我们会看到这种变化很好地与`multiprocessing`模块的工作方式相匹配。
- en: In an object-oriented programming environment, these additional parsers might
    be method functions or properties of a class definition. The advantage of an object-oriented
    design with lazy parsing methods is that items aren’t parsed unless they’re needed.
    This particular functional design parses everything, assuming that it’s going
    to be used.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在面向对象的编程环境中，这些额外的解析器可能是类定义的方法函数或属性。具有惰性解析方法的面向对象设计的优点是，除非需要，否则不会解析项目。这个特定的函数式设计解析了所有内容，假设它将被使用。
- en: It’s possible to create a lazy functional design. It can rely on the three parser
    functions to extract and parse the various elements from a given `Access` object
    as needed. Rather than using the `details.time` attribute, we’d use the `parse_time(access.time)`
    function. The syntax is longer, but it ensures that the attribute is only parsed
    as needed. We could also make it a property that preserves the original syntax.
    We’ve left this as an exercise for the reader.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能创建一个惰性函数式设计。它可以依赖于三个解析函数，根据需要从给定的`Access`对象中提取和解析各种元素。我们不会使用`details.time`属性，而是使用`parse_time(access.time)`函数。语法更长，但它确保属性仅在需要时才被解析。我们也可以将其作为一个保留原始语法的属性。我们将这个作为练习留给读者。
- en: 14.3.5 Filtering the access details
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.3.5 过滤访问详情
- en: We’ll look at several filters for the `AccessDetails` objects. The first is
    a collection of filters that reject a lot of overhead files that are rarely interesting.
    The second filter will be part of the analysis functions, which we’ll look at
    later.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看 `AccessDetails` 对象的几个过滤器。第一个是一组拒绝许多很少有趣的冗余文件的过滤器。第二个过滤器将是分析函数的一部分，我们将在稍后查看。
- en: 'The `path_filter()` function is a combination of three functions:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`path_filter()` 函数是三个函数的组合：'
- en: Exclude empty paths
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排除空路径
- en: Exclude some specific filenames
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排除一些特定的文件名
- en: Exclude files that have a given extension
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排除具有给定扩展名的文件
- en: 'A flexible design can define each test as a separate first-class, filter-style
    function. For example, we might have a function such as the following to handle
    empty paths:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 灵活的设计可以定义每个测试为一个单独的一等，过滤器风格的函数。例如，我们可能有一个如下所示的函数来处理空路径：
- en: '[PRE12]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This function ensures that the path contains a name. We can write similar tests
    for the `non_excluded_names()` and `non_excluded_ext()` functions. Names like
    `favicon.ico` and `robots.txt` need to be excluded. Similarly, extensions like
    `.js` and `.css` need to be excluded as well. We’ve left these two additional
    filters as exercises for the reader.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数确保路径包含一个名称。我们可以为 `non_excluded_names()` 和 `non_excluded_ext()` 函数编写类似的测试。像
    `favicon.ico` 和 `robots.txt` 这样的名称需要被排除。同样，像 `.js` 和 `.css` 这样的扩展也需要被排除。我们将这两个额外的过滤器留给读者作为练习。
- en: 'The entire sequence of `filter()` functions will look like this:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 整个 `filter()` 函数序列将看起来像这样：
- en: '[PRE13]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This style of stacked filters has the advantage of being slightly easier to
    expand when we add new filter criteria.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这种堆叠过滤器的风格具有在添加新的过滤器标准时稍微容易扩展的优点。
- en: The use of generator functions (such as the `filter()` function) means that
    we aren’t creating large intermediate objects. Each of the intermediate variables,
    `non_empty`, `nx_name`, and `nx_ext`, is a proper lazy generator function; no
    processing is done until the data is consumed by a client process.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器函数（如 `filter()` 函数）的使用意味着我们不会创建大型中间对象。每个中间变量 `non_empty`、`nx_name` 和 `nx_ext`
    都是一个合适的惰性生成器函数；只有在客户端进程消耗数据之前，不会进行任何处理。
- en: While elegant, this suffers from inefficiency because each function will need
    to parse the path in the `AccessDetails` object. In order to make this more efficient,
    we could wrap a `path.split(’/’)` function with the `@cache` decorator. An alternative
    is to split the path on the `/` characters, and save the list in the `AccessDetails`
    object.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种风格很优雅，但因为它需要每个函数都解析 `AccessDetails` 对象中的路径，所以它效率不高。为了提高效率，我们可以使用 `@cache`
    装饰器包装 `path.split(’/’)` 函数。另一种选择是在 `/` 字符上拆分路径，并将列表保存在 `AccessDetails` 对象中。
- en: 14.3.6 Analyzing the access details
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.3.6 分析访问详情
- en: We’ll look at two analysis functions that we can use to filter and analyze the
    individual `AccessDetails` objects. The first function will filter the data and
    pass only specific paths. The second function will summarize the occurrences of
    each distinct path.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看两个分析函数，我们可以使用这些函数来过滤和分析单个 `AccessDetails` 对象。第一个函数将过滤数据，并仅传递特定的路径。第二个函数将总结每个不同路径的出现次数。
- en: 'We’ll define a small `book_in_path()` function and combine this with the built-in
    `filter()` function to apply the function to the details. Here is the composite
    `book_filter()` function:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个小的 `book_in_path()` 函数，并将其与内置的 `filter()` 函数结合使用，以将函数应用于细节。以下是复合的 `book_filter()`
    函数：
- en: '[PRE14]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We’ve defined a rule, through the `book_in_path()` function, which we’ll apply
    to each `AccessDetails` object. If the path has at least two components and the
    first component of the path is `’book’`, then we’re interested in these objects.
    All other `AccessDetails` objects can be quietly rejected.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过 `book_in_path()` 函数定义了一条规则，我们将将其应用于每个 `AccessDetails` 对象。如果路径至少有两个组件，并且路径的第一个组件是
    `’book’`，那么我们对这些对象感兴趣。所有其他 `AccessDetails` 对象都可以被默默地拒绝。
- en: 'The `reduce_book_total()` function is the final reduction that we’re interested
    in:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的最终归约函数是 `reduce_book_total()`。
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This function will produce a `Counter()` object that shows the frequency of
    each path in an `AccessDetails` object. In order to focus on a particular set
    of paths, we’ll use the `reduce_total(book_filter(details))` expression. This
    provides a summary of only items that are passed by the given filter.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将生成一个 `Counter()` 对象，显示 `AccessDetails` 对象中每个路径的频率。为了专注于特定的路径集，我们将使用 `reduce_total(book_filter(details))`
    表达式。这仅提供了通过给定过滤器的项目摘要。
- en: Because `Counter` objects can be applied to a wide variety of types, a type
    hint is required to provide a narrow specification. In this case, the hint is
    `dict[str,`` int]` to show the mypy tool that string representations of paths
    will be counted.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `Counter` 对象可以应用于各种类型，需要一个类型提示来提供狭窄的规范。在这种情况下，提示是 `dict[str, int]`，以向 mypy
    工具显示路径的字符串表示将被计数。
- en: 14.3.7 The complete analysis process
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.3.7 完整分析过程
- en: 'Here is the composite `analysis()` function that digests a collection of log
    files:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是处理日志文件集合的复合 `analysis()` 函数：
- en: '[PRE16]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `analysis()` function uses the `local_gzip()` function to work with a single
    path. It applies a stack of parsing functions, `access_detail_iter()` and `access_iter()`,
    to create an iterable sequence of `AccessDetails` objects. It then applies a stack
    of filters to exclude paths that aren’t interesting. Finally, it applies a reduction
    to a sequence of `AccessDetails` objects. The result is a `Counter` object that
    shows the frequency of access for certain paths.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`analysis()` 函数使用 `local_gzip()` 函数处理单个路径。它应用一系列解析函数 `access_detail_iter()`
    和 `access_iter()`，以创建一个 `AccessDetails` 对象的可迭代序列。然后，它应用一系列过滤器来排除不感兴趣的路径。最后，它将一个
    `AccessDetails` 对象序列应用于归约。结果是显示某些路径访问频率的 `Counter` 对象。'
- en: A sample collection of saved `.gzip` format log files totals about 51 MB. Processing
    the files serially with this function takes over 140 seconds. Can we do better
    using concurrent processing?
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 保存的 `.gzip` 格式日志文件样本总量约为 51 MB。使用此函数按顺序处理文件需要超过 140 秒。我们能否通过并发处理做得更好？
- en: 14.4 Using a multiprocessing pool for concurrent processing
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.4 使用多进程池进行并发处理
- en: One elegant way to make use of the `multiprocessing` module is to create a processing
    `Pool` object and assign work to the various workers in that pool. We will depend
    on the OS to interleave execution among the various processes. If each of the
    processes has a mixture of I/O and computation, we should be able to ensure that
    our processor (and disk) are kept very busy. When processes are waiting for the
    I/O to complete, other processes can do their computations. When an I/O operation
    finishes, the process waiting for this will be ready to run and can compete with
    others for processing time.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一种优雅地使用 `multiprocessing` 模块的方法是创建一个处理 `Pool` 对象，并将工作分配给该池中的各个工作者。我们将依赖操作系统在各个进程之间交错执行。如果每个进程都有
    I/O 和计算的混合，我们应该能够确保我们的处理器（和磁盘）保持非常忙碌。当进程等待 I/O 完成时，其他进程可以执行它们的计算。当一个 I/O 操作完成时，等待该操作的进程将准备好运行，并可以与其他进程竞争处理时间。
- en: 'The recipe for mapping work to a separate process looks like this:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 将工作映射到单独进程的配方看起来像这样：
- en: '[PRE17]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This function creates a `Pool` object with separate worker processes and assigns
    this `Pool` object to the `workers` variable. We then map the analytical function,
    `analysis`, to an iterable queue of work to be done using the pool of processes.
    Each process in the `workers` pool gets assigned items from the iterable queue.
    In this case, the queue is the result of the `root.glob(LOG_PATTERN)` attribute,
    which is a sequence of file names.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数创建一个具有单独工作者进程的 `Pool` 对象，并将此 `Pool` 对象分配给 `workers` 变量。然后，我们使用进程池将分析函数 `analysis`
    映射到要执行的工作的可迭代队列。`workers` 池中的每个进程都会从可迭代队列中分配项目。在这种情况下，队列是 `root.glob(LOG_PATTERN)`
    属性的结果，它是一系列文件名。
- en: As each worker completes the `analysis()` function and returns a result, the
    parent process that created the `Pool` object can collect those results. This
    allows us to create several concurrently built `Counter` objects and to merge
    them into a single, composite result.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 随着每个工作者完成 `analysis()` 函数并返回结果，创建 `Pool` 对象的父进程可以收集这些结果。这允许我们创建多个并发构建的 `Counter`
    对象，并将它们合并成一个单一的综合结果。
- en: If we start p processes in the pool, our overall application will include p
    + 1 processes. There will be one parent process and p children. This often works
    out well because the parent process will have little to do after the subprocess
    pools are started. Generally, the workers will be assigned to separate CPUs (or
    cores) and the parent will share a CPU with one of the children in the `Pool`
    object.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在池中启动 p 个进程，我们的整体应用程序将包括 p + 1 个进程。将有一个父进程和 p 个子进程。这通常效果很好，因为父进程在启动子进程池后几乎没有什么事情可做。通常，工作者将被分配到单独的
    CPU（或核心），而父进程将与 `Pool` 对象中的一个子进程共享一个 CPU。
- en: The ordinary Linux parent/child process rules apply to the subprocesses created
    by this module. If the parent crashes without properly collecting the final status
    from the child processes, then zombie processes can be left running. For this
    reason, a process `Pool` object is also a context manager. When we use a pool
    through the `with` statement, at the end of the context, the children are properly
    collected.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 普通的Linux父/子进程规则适用于此模块创建的子进程。如果父进程崩溃而没有从子进程中正确收集最终状态，则可能会留下僵尸进程运行。因此，进程`Pool`对象也是一个上下文管理器。当我们通过`with`语句使用池时，在上下文结束时，子进程将被正确收集。
- en: 'By default, a `Pool` object will have a number of workers based on the value
    of the `multiprocessing.cpu_count()` function. This number is often optimal, and
    simply using the `with`` multiprocessing.Pool()`` as`` workers`: attribute might
    be sufficient.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，一个`Pool`对象将根据`multiprocessing.cpu_count()`函数的值拥有一定数量的工作者。这个数字通常是最佳的，仅使用`with
    multiprocessing.Pool() as workers:`属性可能就足够了。
- en: In some cases, it can help to have more workers than CPUs. This might be true
    when each worker has I/O-intensive processing. Having many worker processes waiting
    for I/O to complete can improve the overall runtime of an application.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，拥有比CPU更多的工作者可能会有所帮助。这可能是在每个工作者都有I/O密集型处理时的情况。拥有许多工作者进程等待I/O完成可以提高应用程序的整体运行时间。
- en: If a given `Pool` object has p workers, this mapping can cut the processing
    time to almost ![1 p](img/file130.jpg) of the time required to process all of
    the logs serially. Pragmatically, there is some overhead involved with communication
    between the parent and child processes in the `Pool` object. These overheads will
    limit the effectiveness of subdividing the work into very small concurrent pieces.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果给定的`Pool`对象有p个工作者，这种映射可以将处理时间减少到几乎![1 p](img/file130.jpg)处理所有日志所需的时间。从实际的角度来看，`Pool`对象中父进程和子进程之间的通信会有一些开销。这些开销将限制将工作细分为非常小的并发部分的有效性。
- en: 'The multiprocessing `Pool` object has several map-like methods to allocate
    work to a pool. We’ll look at `map()`, `imap()`, `imap_unordered()`, and `starmap()`.
    Each of these is a variation on the common theme of assigning a function to a
    pool of processes and mapping data items to that function. Additionally, there
    are two async variants: `map_async()` and `starmap_async()`. These functions differ
    in the details of allocating work and collecting results:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 多进程`Pool`对象有几种类似于`map()`的方法来分配工作到池中。我们将查看`map()`、`imap()`、`imap_unordered()`和`starmap()`。这些方法都是将函数分配给进程池并将数据项映射到该函数的常见主题的变体。此外，还有两个异步变体：`map_async()`和`starmap_async()`。这些函数在分配工作和收集结果方面的细节上有所不同：
- en: The `map(function,`` iterable)` method allocates items from the iterable to
    each worker in the pool. The finished results are collected in the order they
    were allocated to the `Pool` object so that order is preserved.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map(function, iterable)`方法将可迭代对象中的项目分配给池中的每个工作者。完成的结果将按照它们分配给`Pool`对象的顺序收集，以保持顺序。'
- en: The `imap(function,`` iterable)` method is lazier than `map()`. By default,
    it sends each individual item from the iterable to the next available worker.
    This might involve more communication overhead. For this reason, a chunk size
    larger than 1 is suggested.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`imap(function, iterable)`方法比`map()`更懒惰。默认情况下，它将可迭代对象中的每个单独的项目发送到下一个可用的工作者。这可能涉及更多的通信开销。因此，建议使用大于1的块大小。'
- en: The `imap_unordered(function,`` iterable)` method is similar to the `imap()`
    method, but the order of the results is not preserved. Allowing the mapping to
    be processed out of order means that, as each process finishes, the results are
    collected.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`imap_unordered(function, iterable)`方法与`imap()`方法类似，但结果顺序不被保留。允许映射按顺序处理，意味着每个进程完成时，结果将被收集。'
- en: The `starmap(function,`` iterable)` method is similar to the `itertools.starmap()`
    function. Each item in the iterable must be a tuple; the tuple is passed to the
    function using the * modifier so that each value of the tuple becomes a positional
    argument value. In effect, it’s performing `function(*iterable[0])`, `function(*iterable[1])`,
    and so on.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`starmap(function, iterable)`方法与`itertools.starmap()`函数类似。可迭代对象中的每个项目必须是一个元组；元组通过*修饰符传递给函数，以便元组的每个值都成为位置参数值。实际上，它执行`function(*iterable[0])`、`function(*iterable[1])`等等。'
- en: The two `_async` variants don’t simply return a result; they return an `AsyncResult`
    object. This object has some status information. We can, for example, see if the
    work has been completed in general, or if it has been completed without an exception.
    The most important method of an `AsyncResult` object is the `.get()` method, which
    interrogates the worker for the result.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 两个`_async`变体不仅返回一个结果，还返回一个`AsyncResult`对象。这个对象有一些状态信息。例如，我们可以查看工作是否已经完成，或者是否在没有异常的情况下完成。`AsyncResult`对象最重要的方法是`.get()`方法，它查询工作者以获取结果。
- en: This extra complexity works well when the duration of processing is highly variable.
    We can collect results from workers as the results become available. The behavior
    for the non-`_async` variants is to collect results in the order the work was
    started, preserving the order of the original source data for the map-like operation.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理时间高度可变时，这种额外的复杂性可以很好地工作。我们可以随着结果变得可用而收集工作者的结果。对于非`_async`变体，行为是按照工作开始时的顺序收集结果，保留原始源数据在类似map操作中的顺序。
- en: 'Here is the `map_async()` variant of the preceding mapping theme:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是前面映射主题的`map_async()`变体：
- en: '[PRE18]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We’ve created a `Counter()` function that we’ll use to consolidate the results
    from each worker in the pool. We created a pool of subprocesses based on the number
    of available CPUs, and used the `Pool` object as a context manager. We then mapped
    our `analysis()` function to each file in our file-matching pattern. The resulting
    `Counter` objects from the `analysis()` function are combined into a single resulting
    counter.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个`Counter()`函数，我们将用它来合并池中每个工作者的结果。我们根据可用的CPU数量创建了一个子进程池，并使用`Pool`对象作为上下文管理器。然后，我们将我们的`analysis()`函数映射到我们的文件匹配模式中的每个文件。`analysis()`函数产生的`Counter`对象被合并成一个单一的计数器。
- en: This version took about 68 seconds to analyze a batch of log files. The time
    to analyze the logs was cut dramatically using several concurrent processes. The
    single-process baseline time was 150 seconds. Other experiments need to be run
    with larger pool sizes to determine how many workers are required to make the
    system as busy as possible.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这个版本分析一批日志文件大约需要68秒。通过使用多个并发进程，分析日志的时间被大幅缩短。单进程基线时间是150秒。其他实验需要运行在更大的池大小上，以确定需要多少个工作者来使系统尽可能忙碌。
- en: We’ve created a two-tiered map-reduce process with the `multiprocessing` module’s
    `Pool.map_async()` function. The first tier was the `analysis()` function, which
    performed a map-reduce on a single log file. We then consolidated these reductions
    in a higher-level reduce operation.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`multiprocessing`模块的`Pool.map_async()`函数创建了一个两层的map-reduce过程。第一层是`analysis()`函数，它对一个单个日志文件执行map-reduce操作。然后我们在更高层次的reduce操作中合并这些减少。
- en: 14.4.1 Using apply() to make a single request
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.4.1 使用apply()进行单个请求
- en: 'In addition to the map-like variants, a pool also has an `apply(function,`` *args,`` **kw)`
    method that we can use to pass one value to the worker pool. We can see that the
    various `map()` methods are really a `for` statement wrapped around the `apply()`
    method. We can, for example, use the following command to process a number of
    files:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 除了类似map的变体之外，池还有一个`apply(function, *args, **kw)`方法，我们可以用它将一个值传递给工作池。我们可以看到，各种`map()`方法实际上是一个`for`语句包裹在`apply()`方法周围。例如，我们可以使用以下命令处理多个文件：
- en: '[PRE19]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: It’s not clear, for our purposes, that this is a significant improvement. Almost
    everything we need to do can be expressed as a `map()` function.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的来说，这并不明显是一个重大的改进。我们几乎所有需要做的事情都可以用`map()`函数来表示。
- en: 14.4.2 More complex multiprocessing architectures
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.4.2 更复杂的多进程架构
- en: The `multiprocessing` package supports a wide variety of architectures. We can
    create multiprocessing structures that span multiple servers and provide formal
    authentication techniques to create a necessary level of security. We can pass
    objects from process to process using queues and pipes. We can share memory between
    processes. We can also share lower-level locks between processes as a way to synchronize
    access to shared resources such as files.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`multiprocessing`包支持多种架构。我们可以创建跨越多个服务器的多进程结构，并提供正式的认证技术来创建必要的安全级别。我们可以使用队列和管道在进程之间传递对象。我们可以在进程之间共享内存。我们还可以在进程之间共享低级锁，作为同步访问共享资源（如文件）的一种方式。'
- en: Most of these architectures involve explicitly managing states among several
    working processes. Using locks and shared memory, in particular, is imperative
    in nature and doesn’t fit in well with a functional programming approach.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些架构都涉及在几个工作进程之间显式管理状态。特别是使用锁和共享内存，在本质上强制执行，并且与函数式编程方法不太兼容。
- en: We can, with some care, treat queues and pipes in a functional manner. Our objective
    is to decompose a design into producer and consumer functions. A producer can
    create objects and insert them into a queue. A consumer will take objects out
    of a queue and process them, perhaps putting intermediate results into another
    queue. This creates a network of concurrent processors and the workload is distributed
    among these various processes.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以小心地以函数式方式处理队列和管道。我们的目标是分解设计为生产者和消费者函数。生产者可以创建对象并将它们插入队列。消费者将从队列中取出对象并处理它们，可能将中间结果放入另一个队列。这创建了一个并发处理器的网络，工作负载在这些不同的进程之间分配。
- en: This design technique has some advantages when designing a complex application
    server. The various subprocesses can exist for the entire life of the server,
    handling individual requests concurrently.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计复杂的应用服务器时，这种设计技术有一些优点。各种子进程可以存在于服务器的整个生命周期中，并发处理单个请求。
- en: 14.4.3 Using the concurrent.futures module
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.4.3 使用`concurrent.futures`模块
- en: In addition to the `multiprocessing` package, we can also make use of the `concurrent.futures`
    module. This also provides a way to map data to a concurrent pool of threads or
    processes. The module API is relatively simple and similar in many ways to the
    `multiprocessing.Pool()` function’s interface.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`multiprocessing`包，我们还可以利用`concurrent.futures`模块。这个模块也提供了一种将数据映射到线程或进程并发池的方法。模块API相对简单，在很多方面与`multiprocessing.Pool()`函数的接口相似。
- en: 'Here is an example to show how similar they are:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例，展示了它们之间的相似性：
- en: '[PRE20]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The most significant change between the preceding example and the previous examples
    is that we’re using an instance of the `concurrent.futures.ProcessPoolExecutor`
    object instead of a `multiprocessing.Pool` object. The essential design pattern
    is to map the `analysis()` function to the list of filenames using the pool of
    available workers. The resulting `Counter` objects are consolidated to create
    a final result.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的示例相比，最显著的变化是我们使用`concurrent.futures.ProcessPoolExecutor`对象实例而不是`multiprocessing.Pool`对象。基本设计模式是将`analysis()`函数映射到可用工作进程的列表。结果`Counter`对象被合并以创建最终结果。
- en: The performance of the `concurrent.futures` module is nearly identical to the
    `multiprocessing` module.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`concurrent.futures`模块的性能几乎与`multiprocessing`模块相同。'
- en: 14.4.4 Using concurrent.futures thread pools
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.4.4 使用`concurrent.futures`线程池
- en: The `concurrent.futures` module offers a second kind of executor that we can
    use in our applications. Instead of creating a `concurrent.futures.ProcessPoolExecutor`
    object, we can use the `ThreadPoolExecutor` object. This will create a pool of
    threads within a single process.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`concurrent.futures`模块为我们提供了第二种类型的执行器，我们可以在应用程序中使用它。我们不需要创建`concurrent.futures.ProcessPoolExecutor`对象，而是可以使用`ThreadPoolExecutor`对象。这将在一个进程内创建一个线程池。'
- en: The syntax for thread pools is almost identical to using a `ProcessPoolExecutor`
    object. The performance, however, can be remarkably different. CPU-intensive processing
    doesn’t often show improvement in a multi-threaded environment because there’s
    no computation while waiting for I/O to complete. Processing that is I/O-intensive
    can benefit from multi-threading.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 线程池的语法几乎与使用`ProcessPoolExecutor`对象相同。然而，性能可能会有显著差异。在多线程环境中，CPU密集型处理通常不会显示出改进，因为在等待I/O完成时没有计算。I/O密集型处理可以从多线程中受益。
- en: 'Using sample log files and a small four-core laptop running macOS X, these
    are the kinds of results that indicate the difference between threads that share
    I/O resources and processes:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 使用样本日志文件和一台运行macOS X的小型四核笔记本电脑，以下是表明共享I/O资源的线程和进程之间差异的结果：
- en: Using the `concurrent.futures` thread pool, the elapsed time was 168 seconds
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`concurrent.futures`线程池，耗时为168秒
- en: Using a process pool, the elapsed time was 68 seconds
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用进程池，耗时为68秒
- en: 'In both cases, the `Pool` object’s size was `4`. The single-process and single-thread
    baseline time was 150 seconds; adding threads made processing run more slowly.
    This result is typical of programs doing a great deal of computation with relatively
    little waiting for input and output. The `multithreading` module is often more
    appropriate for the following kinds of applications:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，`Pool`对象的大小都是`4`。单进程和单线程基线时间是150秒；添加线程使处理速度变慢。这种结果对于执行大量计算且相对较少等待输入和输出的程序来说是典型的。`multithreading`模块通常更适合以下类型的应用程序：
- en: User interfaces where threads are idle for long periods of time, while waiting
    for the person to move the mouse or touch the screen
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户界面在长时间空闲等待用户移动鼠标或触摸屏幕时
- en: Web servers where threads are idle while waiting for data to transfer from a
    large, fast server through a network to a (relatively) slow client
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在等待从大型、快速服务器通过网络传输数据到（相对较慢的）客户端时线程空闲的Web服务器
- en: Web clients that extract data from multiple web servers, especially where these
    clients must wait for data to percolate through a network
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从多个Web服务器提取数据的Web客户端，特别是当这些客户端必须等待数据在网络中渗透时
- en: It’s important to benchmark and measure performance.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 做基准测试和性能测量很重要。
- en: 14.4.5 Using the threading and queue modules
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.4.5 使用线程和队列模块
- en: The Python `threading` package involves a number of constructs helpful for building
    imperative applications. This module is not focused on writing functional applications.
    We can make use of thread-safe queues in the `queue` module to pass objects from
    thread to thread.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Python的`threading`包包含许多有助于构建命令式应用程序的构造。此模块不专注于编写函数式应用程序。我们可以使用`queue`模块中的线程安全队列在线程之间传递对象。
- en: A queue permits safe data sharing. Since the queue processing involves using
    OS services, it can also mean applications using queues may observe less interference
    from the GIL.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 队列允许安全的数据共享。由于队列处理涉及使用OS服务，这也可能意味着使用队列的应用程序可能会观察到来自GIL的干扰较少。
- en: The `threading` module doesn’t have a simple way of distributing work to various
    threads. The API isn’t ideally suited to functional programming.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`threading`模块没有简单的方法将工作分配给各个线程。API并不适合函数式编程。'
- en: As with the more primitive features of the `multiprocessing` module, we can
    try to conceal the stateful and imperative nature of locks and queues. It seems
    easier, however, to make use of the `ThreadPoolExecutor` method in the `concurrent.futures`
    module. The `ThreadPoolExecutor.map()` method provides us with a very pleasant
    interface to concurrently process the elements of a collection.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 与`multiprocessing`模块的更原始功能一样，我们可以尝试隐藏锁和队列的状态性和命令性本质。然而，似乎更易于使用`concurrent.futures`模块中的`ThreadPoolExecutor`方法。`ThreadPoolExecutor.map()`方法为我们提供了一个非常愉悦的接口，可以并发处理集合中的元素。
- en: The use of the `map()` function primitive to allocate work seems to fit nicely
    with our functional programming expectations. For this reason, it’s best to focus
    on the `concurrent.futures` module as the most accessible way to write concurrent
    functional programs.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`map()`函数原语分配工作似乎与我们的函数式编程期望很好地匹配。因此，最好专注于`concurrent.futures`模块，作为编写并发函数式程序最易于访问的方式。
- en: 14.4.6 Using async functions
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.4.6 使用异步函数
- en: The `asyncio` module helps us work with `async` functions to—perhaps—better
    interleave processing and computation. It’s important to understand that `async`
    processing leverages the `threading` model. This means that it can effectively
    interleave waiting for I/O with computation. It does not effectively interleave
    pure computation.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`asyncio`模块帮助我们使用`async`函数来——也许——更好地交织处理和计算。重要的是要理解`async`处理利用了`threading`模型。这意味着它可以有效地交织等待I/O与计算。它不能有效地交织纯计算。'
- en: 'In order to make use of the `asyncio` module, we need to do the following four
    things:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用`asyncio`模块，我们需要做以下四件事：
- en: Add the `async` keyword to our various parsing and filtering functions to make
    them coroutines.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的各种解析和过滤函数中添加`async`关键字，使它们成为协程。
- en: Add `await` keywords to collect results from one coroutine before passing them
    to another coroutine.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在将结果传递给另一个协程之前，添加`await`关键字以收集一个协程的结果。
- en: Create an overall event loop to coordinate the `async`/`await` processing among
    the coroutines.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个整体的事件循环，以协调协程之间的`async`/`await`处理。
- en: Create a thread pool to handle file reading.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个线程池来处理文件读取。
- en: The first three steps listed above don’t involve deep complexity. The `asyncio`
    module helps us create tasks to parse each file, and then run the collection of
    tasks. The event loop ensures that coroutines will pause at the `await` statements
    to collect results. It also ensures coroutines with available data are eligible
    to process. The interleaving of the coroutines happens in a single thread. As
    noted previously, the number of bytecode operations is not magically made smaller
    by changing the order of execution.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 列出的前三个步骤不涉及深层次的复杂性。`asyncio`模块帮助我们创建解析每个文件的任务，然后运行任务集合。事件循环确保协程将在`await`语句处暂停以收集结果。它还确保具有可用数据的协程有资格进行处理。协程的交织发生在单个线程中。如前所述，通过改变执行顺序，字节码操作的数量并不会神奇地变小。
- en: The tricky part of this is dealing with input and output operations that are
    not part of the `asyncio` module. Specifically, reading and writing local files
    is not part of `asyncio`. Any time we attempt to read (or write) a file, the operating
    system request could block waiting for the operation to complete. Unless this
    blocking request is in a separate thread, it stops the event loop, and stops all
    of Python’s cleverly interleaved coroutine processing. See [https://docs.python.org/3/library/asyncio-eventloop.html#id14](https://docs.python.org/3/library/asyncio-eventloop.html#id14)
    for more information on using a thread pool.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的难点在于处理不属于`asyncio`模块的输入和输出操作。具体来说，读取和写入本地文件不是`asyncio`的一部分。每次我们尝试读取（或写入）文件时，操作系统请求可能会阻塞，等待操作完成。除非这个阻塞请求在单独的线程中，否则它会停止事件循环，并停止Python的所有巧妙交织的协程处理。有关使用线程池的更多信息，请参阅[https://docs.python.org/3/library/asyncio-eventloop.html#id14](https://docs.python.org/3/library/asyncio-eventloop.html#id14)。
- en: To work with local files, we would need to use a `concurrent.futures.ThreadPoolExecutor`
    object to manage the file input and output operations. This will allocate the
    work to threads outside the main event loop. Consequently, a design for local
    file processing based on `async`/`await` will not be dramatically better than
    one using `concurrent.futures` directly.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要与本地文件一起工作，我们需要使用一个`concurrent.futures.ThreadPoolExecutor`对象来管理文件输入和输出操作。这将把工作分配给主事件循环之外的线程。因此，基于`async`/`await`的本地文件处理设计不会比直接使用`concurrent.futures`有显著的优势。
- en: For network servers and complex clients, the `asyncio` module can make the application
    very responsive to a user’s inputs. The fine-grained switching among the coroutines
    within a thread works best when most of the coroutines are waiting for data.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 对于网络服务器和复杂的客户端，`asyncio`模块可以使应用程序对用户的输入非常响应。当大多数协程都在等待数据时，线程内协程的细粒度切换效果最佳。
- en: 14.4.7 Designing concurrent processing
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.4.7 设计并发处理
- en: 'From a functional programming perspective, we’ve seen three ways to use the
    `map()` function concept applied to data items concurrently. We can use any one
    of the following:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 从函数式编程的角度来看，我们已经看到了将`map()`函数概念应用于数据项并发使用的三种方式。我们可以使用以下任何一个：
- en: '`multiprocessing.Pool`'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`multiprocessing.Pool`'
- en: '`concurrent.futures.ProcessPoolExecutor`'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`concurrent.futures.ProcessPoolExecutor`'
- en: '`concurrent.futures.ThreadPoolExecutor`'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`concurrent.futures.ThreadPoolExecutor`'
- en: These are almost identical in the way we interact with them; all three of these
    process pools support variations of a `map()` method that applies a function to
    items of an iterable collection. This fits in elegantly with other functional
    programming techniques. The performance of each pool may be different because
    of the nature of concurrent threads versus concurrent processes.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这些在交互方式上几乎相同；这三个进程池都支持将函数应用于可迭代集合项的`map()`方法的不同变体。这巧妙地与其他函数式编程技术相结合。每个池的性能可能不同，因为并发线程与并发进程的性质不同。
- en: 'As we stepped through the design, our log analysis application decomposed into
    two overall areas:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们逐步设计的过程中，我们的日志分析应用分解为两个主要区域：
- en: 'The lower-level parsing: This is generic parsing that will be used by almost
    any log analysis application'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较低级别的解析：这是一种通用的解析，几乎任何日志分析应用都会使用。
- en: 'The higher-level analysis application: This is more specific filtering and
    reduction focused on our application’s needs'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较高级别的分析应用：这是更具体的过滤和减少，专注于我们应用的需求。
- en: 'The lower-level parsing can be decomposed into four stages:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 较低级别的解析可以分解为四个阶段：
- en: Reading all the lines from multiple source log files. This was the `local_gzip()`
    mapping from file name to a sequence of lines.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从多个源日志文件中读取所有行。这是从文件名到行序列的`local_gzip()`映射。
- en: Creating named tuples from the lines of log entries in a collection of files.
    This was the `access_iter()` mapping from text lines to `Access` objects.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从文件集合中日志条目的行创建命名元组。这是从文本行到`Access`对象的`access_iter()`映射。
- en: Parsing the details of more complex fields such as dates and URLs. This was
    the `access_detail_iter()` mapping from `Access` objects to `AccessDetails` objects.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析更复杂字段如日期和URL的细节。这是从`Access`对象到`AccessDetails`对象的`access_detail_iter()`映射。
- en: Rejecting uninteresting paths from the logs. We can also think of this as passing
    only the interesting paths. This was more of a filter than a map operation. This
    was a collection of filters bundled into the `path_filter()` function.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从日志中拒绝无趣的路径。我们也可以将其视为仅传递有趣的路径。这更多的是一个过滤操作而不是映射操作。这是一个将过滤操作捆绑到`path_filter()`函数中的过滤器集合。
- en: We defined an overall `analysis()` function that parsed and analyzed a given
    log file. It applied the higher-level filter and reduction to the results of the
    lower-level parsing. It can also work with a wildcard collection of files.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个整体的`analysis()`函数，它解析并分析给定的日志文件。它将高级别的过滤和归约应用于低级别解析的结果。它还可以与一组通配符文件一起工作。
- en: 'Given the number of mappings involved, we can see several ways to decompose
    this problem into work designed to use a pool of threads or processes. Each mapping
    is an opportunity for concurrent processing. Here are some of the mappings we
    can consider as design alternatives:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 由于涉及到的映射数量，我们可以看到几种将这个问题分解为使用线程池或进程池的工作的方法。每个映射都是并发处理的机会。以下是我们可以考虑作为设计替代方案的一些映射：
- en: Map the `analysis()` function to individual files. We used this as a consistent
    example throughout this chapter.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`analysis()`函数映射到单个文件。我们在这个章节中一直使用这个作为一致的示例。
- en: Refactor the `local_gzip()` function out of the overall `analysis()` function.
    This refactoring permits mapping a revised `analysis()` function to the results
    of the `local_gzip()` function.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`local_gzip()`函数从整体`analysis()`函数中重构出来。这种重构允许将修订版的`analysis()`函数映射到`local_gzip()`函数的结果。
- en: Refactor the `access_iter(local_gzip(pattern))` function out of the overall
    `analysis()` function. This revised `analysis()` function can be applied via `map()`
    to the iterable sequence of the `Access` objects.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`access_iter(local_gzip(pattern))`函数从整体`analysis()`函数中重构出来。这个修订版的`analysis()`函数可以通过`map()`应用于`Access`对象的迭代序列。
- en: Refactor the `access_detail_iter(access_iter(local_gzip(pattern)))` function
    into two separate iterables. This permits using `map()` to apply one function
    to create `AccessDetail` objects. A separate, higher-level filter and reduction
    against the iterable sequence of the `AccessDetail` objects can be a separate
    process.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`access_detail_iter(access_iter(local_gzip(pattern)))`函数重构为两个独立的迭代器。这允许使用`map()`应用一个函数来创建`AccessDetail`对象。对`AccessDetail`对象的迭代序列进行单独的、高级别的过滤和归约可以是一个单独的进程。
- en: We can also refactor the lower-level parsing into a function to keep it separate
    from the higher-level analysis. We can map the analysis filter and reduction against
    the output from the lower-level parsing.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们也可以将低级别的解析重构为一个函数，以保持其与高级别分析分离。我们可以将分析过滤和归约映射到低级别解析的输出。
- en: All of these are relatively simple methods to restructure the example application.
    The benefit of using functional programming techniques is that each part of the
    overall process can be defined as a mapping, a filter, or a reduction. This makes
    it practical to consider different architectures to locate an optimal design.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都是相对简单的方法来重构示例应用程序。使用函数式编程技术的优势在于，整体过程的每个部分都可以定义为映射、过滤或归约。这使得考虑不同的架构以找到最佳设计变得实用。
- en: In this case, however, we need to distribute the I/O processing to as many CPUs
    or cores as we have available. Most of these potential refactorings will perform
    all of the I/O in the parent process; these will only distribute the computation
    portions of the work to multiple concurrent processes with little resulting benefit.
    Because of these, we want to focus on the mappings, as these distribute the I/O
    to as many cores as possible.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这种情况下，我们需要将I/O处理分配到尽可能多的CPU或核心。大多数这些潜在的重构将在父进程中执行所有I/O；这些只会将工作计算部分分布到多个并发进程，几乎没有带来什么好处。因此，我们希望专注于映射，因为这些映射可以将I/O分配到尽可能多的核心。
- en: It’s often important to minimize the amount of data being passed from process
    to process. In this example, we provided just short filename strings to each worker
    process. The resulting `Counter` object was considerably smaller than the 10 MB
    of compressed detail data in each log file.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，最小化从进程到进程传递的数据量很重要。在这个例子中，我们只为每个工作进程提供了简短的文件名字符串。结果`Counter`对象比每个日志文件中10MB的压缩详细数据小得多。
- en: It’s also essential to run benchmarking experiments to confirm the actual timing
    between computation, input, and output. This information is essential to uncover
    optimal allocation of resources, and a design that better balances computation
    against waiting for I/O to complete.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 运行基准测试实验以确认计算、输入和输出之间的实际时间也非常重要。这些信息对于揭示资源的最优分配，以及更好地平衡计算与等待I/O完成的设计至关重要。
- en: 'The following table contains some preliminary results:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 下表包含了一些初步结果：
- en: '|'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Approach | Duration |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 持续时间 |'
- en: '| `concurrent.futures/threadpool` | 106.58s |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| `concurrent.futures/threadpool` | 106.58s |'
- en: '| `concurrent.futures/processpool` | 40.81s |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| `concurrent.futures/processpool` | 40.81s |'
- en: '| `multiprocessing/imap_unordered` | 27.26s |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| `multiprocessing/imap_unordered` | 27.26s |'
- en: '| `multiprocessing/map_async` | 27.45s |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| `multiprocessing/map_async` | 27.45s |'
- en: '|'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  |  |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: We can see that a thread pool doesn’t permit any useful serialization of the
    work. This is not unexpected, and provides a kind of worst-case benchmark.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到线程池不允许任何有用的工作序列化。这并不意外，提供了一种最坏情况的基准。
- en: The `concurrent.futures/processpool` row shows the time with 4 workers. This
    variant used the `map()` to parcel requests to the workers. The need to process
    the work and collect the results in a specific order may have caused relatively
    slow processing.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '`concurrent.futures/processpool`行显示了4个工作者的时间。这个变体使用了`map()`将请求分发给工作者。可能需要以特定顺序处理工作和收集结果，这可能导致相对较慢的处理。'
- en: The `multiprocessing` modules used the default number of cores, which is 8 for
    the computer being used. The time was cut almost to ![1 4](img/file131.jpg) the
    baseline time. In order to make better use of the available processors, it might
    make sense to further decompose the processing to create batches of lines for
    analysis, and have separate worker pools for analysis and file parsing. Because
    the workloads are very difficult to predict, a flexible, functional design allows
    the restructuring of the work, searching for a way to maximize CPU use.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的`multiprocessing`模块默认使用核心数，对于所使用的计算机来说是8个。时间几乎缩短到![1 4](img/file131.jpg)基线时间。为了更好地利用可用的处理器，进一步分解处理以创建分析行批次的做法可能是有意义的，并为分析和文件解析设置单独的工人池。由于工作负载很难预测，灵活的函数式设计允许重新组织工作，寻找最大化CPU使用的方法。
- en: 14.5 Summary
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.5 总结
- en: 'In this chapter, we’ve looked at two ways to support the concurrent processing
    of multiple pieces of data:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了两种支持多个数据并发处理的方法：
- en: 'The `multiprocessing` module: Specifically, the `Pool` class and the various
    kinds of mappings available to a pool of workers.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`multiprocessing`模块：特别是`Pool`类和提供给工作池的各种映射类型。'
- en: 'The `concurrent.futures` module: Specifically, the `ProcessPoolExecutor` and
    `ThreadPoolExecutor` classes. These classes also support a mapping that will distribute
    work among workers that are threads or processes.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`concurrent.futures`模块：特别是`ProcessPoolExecutor`和`ThreadPoolExecutor`类。这些类还支持一种映射，它将在线程或进程的工人之间分配工作。'
- en: We’ve also noted some alternatives that don’t seem to fit in well with functional
    programming. There are numerous other features of the `multiprocessing` module,
    but they’re not a good fit with functional design. Similarly, the `threading`
    and `queue` modules can be used to build multithreaded applications, but the features
    aren’t a good fit with functional programs.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还注意到了一些似乎不太适合函数式编程的替代方案。`multiprocessing`模块有许多其他功能，但它们与函数式设计不太匹配。同样，`threading`和`queue`模块可以用来构建多线程应用程序，但这些功能与函数程序不太匹配。
- en: In the next chapter, we’ll look at how we can apply functional programming techniques
    to build web service applications. The idea of HTTP can be summarized as `response`` =`
    `httpd(request)`. When the HTTP processing is stateless, this seems to be a perfect
    match for functional design.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何将函数式编程技术应用于构建Web服务应用程序。HTTP的概念可以总结为`response` `=` `httpd(request)`。当HTTP处理是无状态的，这似乎与函数式设计完美匹配。
- en: Adding stateful cookies to this is analogous to providing a response value which
    is expected as an argument to a later request. We can think of it as `response``,`
    `cookie` `=` `httpd``(``request``,` `cookie``)`, where the cookie object is opaque
    to the client.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 向此添加状态性cookie相当于提供一个作为后续请求参数期望的响应值。我们可以将其视为`response`,`cookie` `=` `httpd``(``request``,`
    `cookie``)`，其中cookie对象对客户端是透明的。
- en: 14.6 Exercises
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.6 练习
- en: This chapter’s exercises are based on code available from Packt Publishing on
    GitHub. See [https://github.com/PacktPublishing/Functional-Python-Programming-3rd-Edition](https://github.com/PacktPublishing/Functional-Python-Programming-3rd-Edition).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的练习基于GitHub上Packt Publishing提供的代码。请参阅[https://github.com/PacktPublishing/Functional-Python-Programming-3rd-Edition](https://github.com/PacktPublishing/Functional-Python-Programming-3rd-Edition)。
- en: In some cases, the reader will notice that the code provided on GitHub includes
    partial solutions to some of the exercises. These serve as hints, allowing the
    reader to explore alternative solutions.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，读者会注意到GitHub上提供的代码包括一些练习的部分解决方案。这些作为提示，允许读者探索其他解决方案。
- en: In many cases, exercises will need unit test cases to confirm they actually
    solve the problem. These are often identical to the unit test cases already provided
    in the GitHub repository. The reader should replace the book’s example function
    name with their own solution to confirm that it works.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，练习需要单元测试用例来确认它们确实解决了问题。这些通常与GitHub存储库中已经提供的单元测试用例相同。读者应将书籍中的示例函数名替换为自己的解决方案以确认其工作。
- en: 14.6.1 Lazy parsing
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.6.1 延迟解析
- en: In the [Parsing additional fields of an Access object](#x1-2930004) section,
    we looked at a function that did the initial decomposition of a Common Log File
    (CLF) line into an initial set of easy-to-separate fields.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在[解析访问对象额外字段](#x1-2930004)部分，我们查看了一个将通用日志文件（CLF）行初步分解为易于分离字段的函数。
- en: We then applied three separate functions to parse the details of the timestamp,
    request, the time, and the user agent information. These three functions were
    applied eagerly, decomposing these three fields, even if they were never used
    for further analysis.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后应用了三个独立的功能来解析时间戳、请求、时间和用户代理信息的细节。这三个功能被积极应用，分解了这三个字段，即使它们从未被用于进一步的分析。
- en: 'There are two commonly-used ways to implement lazy parsing of these fields:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这些字段的延迟解析有两种常用的方法：
- en: Rather than parse the text to create a `details.time` attribute, we can define
    a `parse_time()` method to parse the `access.time` value. The syntax is longer,
    but it ensures that the attribute is only parsed as needed.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 而不是解析文本来创建`details.time`属性，我们可以定义一个`parse_time()`方法来解析`access.time`值。语法更长，但它确保属性仅在需要时解析。
- en: Once we have this function, we can make it into a property.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦我们有了这个函数，我们可以将其变成一个属性。
- en: First, redefine a new `Access_Details` class to use three separate methods to
    parse the complex fields.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，重新定义一个新的`Access_Details`类，使用三个独立的方法来解析复杂字段。
- en: Once this works, make these methods into properties to provide values as if
    they had been parsed eagerly. Make sure the new property method names match the
    original attribute names in the class shown earlier.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这工作，将这些方法变成属性，以便提供像它们已经被积极解析的值。确保新的属性方法名称与之前显示的类中的原始属性名称匹配。
- en: To compare the performance, we need to know how often these additional property
    parsing methods are used. Two simple assumptions are 100% of the time and 0% of
    the time. To compare the two designs, we’ll need some statistical summary functions
    that work with the `Access_Details` objects.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较性能，我们需要知道这些额外的属性解析方法被使用的频率。有两个简单的假设是100%的时间和0%的时间。为了比较这两种设计，我们需要一些与`Access_Details`对象一起工作的统计摘要函数。
- en: Create a function that fetches the values of all attributes, to compute a number
    of histograms, for example. Create another that uses only the status value to
    compute a histogram of status only. Compare the performance of the two `Access_Details`
    class variants and the two analytic approaches to see which is faster. The expectation
    is that lazy parsing will be faster. The question is ”how much faster?”
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个函数来获取所有属性的值，以计算多个直方图，例如。再创建另一个仅使用状态值来计算状态直方图的函数。比较两个`Access_Details`类变体和两种分析方法的性能，以查看哪个更快。预期是延迟解析会更快。问题是“快多少？”
- en: 14.6.2 Filter access path details
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.6.2 过滤访问路径细节
- en: In the [Filtering the access details](#x1-2940005) section of this chapter,
    we showed a function to exclude empty paths from further analysis.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的 [过滤访问详情](#x1-2940005) 部分，我们展示了一个排除空路径以进行进一步分析的功能。
- en: We can write similar test functions for the `non_excluded_names()` and `non_excluded_ext()`
    functions. Names like `’favicon.ico’` and `’robots.txt’` need to be excluded.
    Similarly, extensions like `’.js’` and `’.css’` need to be excluded, also.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为 `non_excluded_names()` 和 `non_excluded_ext()` 函数编写类似的测试函数。像 `’favicon.ico’`
    和 `’robots.txt’` 这样的名称需要排除。同样，像 `’.js’` 和 `’.css’` 这样的扩展也需要排除。
- en: Write these two functions to complete the implementation of the `path_filter()`
    function. These require some unit test cases, as does the overall `path_filter()`
    function that exploits three separate path function filters.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 编写这两个函数以完成 `path_filter()` 函数的实现。这需要一些单元测试用例，就像利用三个独立的路径函数过滤器的整体 `path_filter()`
    函数一样。
- en: All of these functions work with a decomposed path name. Is it sensible to try
    to write a single, complex function for all three operations? Does it make more
    sense to decompose the three separate rules and combine them through an overall
    path filtering function?
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些函数都使用分解的路径名。尝试为所有三个操作编写一个单一、复杂的函数是否合理？通过整体路径过滤函数组合三个独立的规则是否更有意义？
- en: 14.6.3 Add @cache decorators
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.6.3 添加 @cache 装饰器
- en: The implementation of the `path_filter()` function applies three separate filters.
    Each filter function will parse the path in the `AccessDetails` object. In order
    to make this more efficient, it can help to wrap lower-level parsing, like a `path.split(’/’)`
    function, with the `@cache` decorator.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '`path_filter()` 函数的实现应用了三个独立的过滤器。每个过滤器函数将解析 `AccessDetails` 对象中的路径。为了提高效率，可以将像
    `path.split(’/’)` 这样的底层解析包装在 `@cache` 装饰器中。'
- en: Write (or rewrite) these three filter functions to make use of the `@cache`
    decorator.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 编写（或重写）这三个过滤器函数以使用 `@cache` 装饰器。
- en: Be sure to compare performance of the filter functions with caching and without
    caching. This can be challenging because when we use a simple `@cache` decorator,
    the original, uncached function is no longer available.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 一定要比较带有缓存和不带缓存的过滤器函数的性能。这可能具有挑战性，因为当我们使用简单的 `@cache` 装饰器时，原始的未缓存函数就不再可用。
- en: If, on the other hand, we use something like `func_c`` =`` cache(func)`, we
    can preserve both the original (uncached) function and the counterpart with caching.
    See [Chapter 12](Chapter_12.xhtml#x1-25000012), [Decorator Design Techniques](Chapter_12.xhtml#x1-25000012),
    for more on how this works. Doing this lets us gather timing data for cached and
    uncached implementations.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用类似 `func_c`` =`` cache(func)` 的方法，我们可以保留原始（未缓存）函数及其带有缓存的对应函数。有关如何工作的更多信息，请参阅
    [第 12 章](Chapter_12.xhtml#x1-25000012)，[装饰器设计技术](Chapter_12.xhtml#x1-25000012)。这样做可以让我们收集缓存和未缓存实现的计时数据。
- en: 14.6.4 Create sample data
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.6.4 创建样本数据
- en: The design shown uses a mapping from filenames to summary counts. Each file
    is processed concurrently by a pool of workers. In order to determine if this
    is optimal, it’s essential to have a high volume of data to measure performance.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 所示的设计使用从文件名到摘要计数的映射。每个文件由一组工作者并发处理。为了确定这是否最优，需要有一个高数据量来衡量性能。
- en: For a lightly-used website, the log files can average about 10 Mb per month.
    Write a Python script to generate synthetic log rows in batches averaging about
    10 Mb per file. Using simplistic random strings isn’t the best approach because
    the application design expects that the request path will have a recognizable
    pattern. This requires some care to generate synthetic data that fits the expected
    pattern.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个使用频率较低的网站，日志文件每月平均约为 10 Mb。编写一个 Python 脚本以批量生成平均约 10 Mb 的合成日志行。使用简单的随机字符串并不是最佳方法，因为应用程序设计预期请求路径将具有可识别的模式。这需要一些细心来生成符合预期模式的合成数据。
- en: 'The application to create synthetic data needs some unit test cases. The overall
    analysis application is the final acceptance test case: does the analysis application
    identify the data patterns built into the synthetic rows of log entries?'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 创建合成数据的程序需要一些单元测试用例。整体分析程序是最终的验收测试用例：分析程序是否能够识别合成日志行中嵌入的数据模式？
- en: 14.6.5 Change the pipeline structure
  id: totrans-288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.6.5 修改管道结构
- en: For a lightly-used website, the log files can average about 10 Mb per month.
    Using Python 3.10 on a MacBook Pro, each file takes about 16 seconds to process.
    A collection of six 10 Mb files has a worst-case performance of 96 seconds. On
    a computer with over six cores, the best case would be 16 seconds.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个使用频率较低的网站，日志文件每月平均约为10 Mb。在MacBook Pro上使用Python 3.10，每个文件处理大约需要16秒。六个10
    Mb文件的集合在最坏情况下的性能为96秒。在拥有超过六个核心的计算机上，最佳情况下的处理时间将是16秒。
- en: The design shown in this chapter allocates each file to a separate worker.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中所示的设计将每个文件分配给一个单独的工作进程。
- en: 'Is this the right level of granularity? It’s impossible to know without exploring
    alternatives. This requires sample data files created by the previous exercise.
    Consider implementing alternative designs and comparing throughput. Here are some
    suggested alternatives:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这是否是合适的粒度级别？没有探索替代方案，是无法知道的。这需要由前一个练习生成的样本数据文件。考虑实现替代设计并比较吞吐量。以下是一些建议的替代方案：
- en: 'Create two pools of workers: one pool reads files and returns lines in blocks
    of 1,024\. The second pool of workers comprises the bulk of the `analysis()` function.
    This second pool has workers to parse each line in a block to create an `Access`
    object, create an `AccessDetails` object, apply the filters, and summarize the
    results. This leads to two tiers of mapping to pass work from the parsing workers
    to the analysis workers.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建两个工作池：一个工作池读取文件并按1,024行的块返回行。第二个工作池包含`analysis()`函数的大部分。这个第二个工作池有工作进程来解析块中的每一行以创建一个`Access`对象，创建一个`AccessDetails`对象，应用过滤器，并总结结果。这导致了从解析工作进程到分析工作进程的映射有两个层级。
- en: Decompose the 10 Mb log files into smaller sizes. Write an application to read
    a log file and write new files, each of which is limited to 4,096 individual log
    entries. Apply the analysis application to this larger collection of small files
    instead of the smaller collection of the original large log files.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将10 Mb的日志文件分解成更小的尺寸。编写一个应用程序来读取日志文件并写入新文件，每个文件限制为4,096个单独的日志条目。将分析应用程序应用于这个更大的小文件集合，而不是原始大日志文件的小集合。
- en: Decompose the `analysis()` function to use three separate pools of workers.
    One pool parses files and returns blocks of `Access` objects. Another pool transforms
    `Access` objects into `AccessDetails` objects. The third pool of workers applies
    filters and summarizes the `AccessDetails` objects.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`analysis()`函数分解为使用三个独立的工作池。一个工作池解析文件并返回`Access`对象的块。另一个工作池将`Access`对象转换为`AccessDetails`对象。第三个工作池的工作进程应用过滤器并总结`AccessDetails`对象。
- en: Summarize the results of using distinct processing pipelines to analyze large
    volumes of data.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 总结使用不同的处理管道分析大量数据的结果。
