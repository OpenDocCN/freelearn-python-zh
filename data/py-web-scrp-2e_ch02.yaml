- en: Scraping the Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 爬取数据
- en: In the previous chapter, we built a crawler which follows links to download
    the web pages we want. This is interesting but not useful-the crawler downloads
    a web page, and then discards the result. Now, we need to make this crawler achieve
    something by extracting data from each web page, which is known as **scraping**.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们构建了一个爬虫，该爬虫会跟随链接下载我们想要的网页。这很有趣，但并不实用——爬虫下载了一个网页，然后丢弃了结果。现在，我们需要让这个爬虫通过从每个网页中提取数据来实现一些功能，这被称为**爬取**。
- en: We will first cover browser tools to examine a web page, which you may already
    be familiar with if you have a web development background. Then, we will walk
    through three approaches to extract data from a web page using regular expressions,
    Beautiful Soup and lxml. Finally, the chapter will conclude with a comparison
    of these three scraping alternatives.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍浏览器工具来检查网页，如果您有网络开发背景，您可能已经熟悉这些工具。然后，我们将通过使用正则表达式、Beautiful Soup和lxml三种方法来提取网页数据。最后，本章将以这三种爬取替代方案的比较结束。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Analyzing a web page
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析网页
- en: Approaches to scrape a web page
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爬取网页的方法
- en: Using the console
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用控制台
- en: xpath selectors
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: xpath选择器
- en: Scraping results
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爬取结果
- en: Analyzing a web page
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析网页
- en: 'To understand how a web page is structured, we can try examining the source
    code. In most web browsers, the source code of a web page can be viewed by right-clicking
    on the page and selecting the View page source option:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解网页的结构，我们可以尝试检查源代码。在大多数网络浏览器中，可以通过右键点击页面并选择“查看页面源代码”选项来查看网页的源代码：
- en: '![](img/4364OS_02_01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4364OS_02_01.jpg)'
- en: 'For our example website, the data we are interested in is found on the country
    pages. Take a look at page source (via browser menu or right click browser menu).
    In the source for the example page for the United Kingdom ([http://example.webscraping.com/view/United-Kingdom-239](http://example.webscraping.com/view/United-Kingdom-239))
    you will find a table containing the country data (you can use search to find
    this in the page source code):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例网站，我们感兴趣的数据位于国家页面。查看页面源代码（通过浏览器菜单或右键点击浏览器菜单）。在示例页面的英国源代码（[http://example.webscraping.com/view/United-Kingdom-239](http://example.webscraping.com/view/United-Kingdom-239)）中，您将找到一个包含国家数据的表格（您可以在页面源代码中使用搜索来找到它）：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The lack of white space and formatting is not an issue for a web browser to
    interpret, but it is difficult for us to read. To help us interpret this table,
    we can use browser tools. To find your browser's developer tools, you can usually
    simply right click and select an option like Developer Tools. Depending on the
    browser you use, you may have different developer tool options, but nearly every
    browser will have a tab titled Elements or HTML. In Chrome and Firefox, you can
    simply right click on an element on the page (what you are interested in scraping)
    and select Inspect Element. For Internet Explorer, you need to open the Developer toolbar
    by pressing *F12*. Then you can select items by clicking *Ctrl *+ *B*. If you
    use a different browser without built-in developer tools, you may want to try the
    Firebug Lite extension, which is available for most web browsers at [https://getfirebug.com/firebuglite](https://getfirebug.com/firebuglite).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于浏览器来说，缺乏空白和格式化不是问题，但对我们来说阅读起来很困难。为了帮助我们解释这个表格，我们可以使用浏览器工具。要找到您浏览器的开发者工具，您通常可以简单地右键点击并选择一个选项，如“开发者工具”。根据您使用的浏览器，您可能有不同的开发者工具选项，但几乎每个浏览器都会有一个名为“元素”或“HTML”的标签页。在Chrome和Firefox中，您可以在页面上右键点击一个元素（您感兴趣要爬取的内容）并选择“检查元素”。对于Internet
    Explorer，您需要通过按*F12*键打开开发者工具栏。然后您可以通过点击*Ctrl*+*B*来选择项目。如果您使用的是没有内置开发者工具的不同浏览器，您可能想尝试Firebug
    Lite扩展，该扩展适用于大多数网络浏览器，网址为[https://getfirebug.com/firebuglite](https://getfirebug.com/firebuglite)。
- en: 'When I right click on the table on the page and click Inspect Element using
    Chrome, I see the following open panel with the surrounding HTML hierarchy of
    the selected element:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在网页上的表格上右键点击，并使用Chrome的“检查元素”功能时，我会看到一个包含所选元素周围HTML层次结构的打开面板：
- en: '![](img/chrome_inspect.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/chrome_inspect.png)'
- en: 'In this screenshot, I can see that the `table` element sits inside a `form`
    element. I can also see that the attributes for the country are included in `tr` 
    or table row elements with different CSS IDs (shown via the `id="places_national_flag__row"`).
    Depending on your browser, the coloring or layout might be different, but you
    should be able to click on the elements and navigate through the hierarchy to
    see the data on the page. If I expand the `tr` elements further by clicking on
    the arrows next to them, I notice the data for each of these rows is included
    is included within a `<td>` element of class `w2p_fw`, which is the child of a
    `<tr>` element, shown as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个屏幕截图上，我可以看到 `table` 元素位于 `form` 元素内部。我还可以看到国家的属性包含在 `tr` 或具有不同CSS ID的表格行元素中（通过
    `id="places_national_flag__row"` 显示）。根据你的浏览器，颜色或布局可能会有所不同，但你应该能够点击元素并遍历层次结构以查看页面上的数据。如果我通过点击它们旁边的箭头进一步展开
    `tr` 元素，我会注意到这些行的数据都包含在类为 `w2p_fw` 的 `<td>` 元素中，这是 `<tr>` 元素的子元素，如下所示：
- en: '![](img/expanded_elements.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/expanded_elements.png)'
- en: Now that we have investigated the page with our browser tools, we know the HTML
    hierarchy of the country data table, and have the necessary information to scrape
    that data from the page.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经使用浏览器工具调查了页面，我们知道国家数据表的HTML层次结构，并拥有从页面爬取这些数据的必要信息。
- en: Three approaches to scrape a web page
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 爬取网页的三种方法
- en: Now that we understand the structure of this web page we will investigate three
    different approaches to scraping its data, first with regular expressions, then
    with the popular `BeautifulSoup` module, and finally with the powerful `lxml`
    module.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了这个网页的结构，我们将探讨三种不同的方法来爬取其数据，首先是使用正则表达式，然后是使用流行的 `BeautifulSoup` 模块，最后是使用强大的
    `lxml` 模块。
- en: Regular expressions
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则表达式
- en: If you are unfamiliar with regular expressions or need a reminder, there is
    a thorough overview available at [https://docs.python.org/3/howto/regex.html](https://docs.python.org/3/howto/regex.html).
    Even if you use regular expressions (or regex) with another programming language,
    I recommend stepping through it for a refresher on regex with Python.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不熟悉正则表达式或需要提醒，可以在 [https://docs.python.org/3/howto/regex.html](https://docs.python.org/3/howto/regex.html)
    找到详细的概述。即使你使用另一种编程语言使用正则表达式（或regex），我也建议通过Python来复习正则表达式。
- en: Because each chapter might build or use parts of previous chapters, we recommend
    setting up your file structure similar to that in [the book repository](https://github.com/kjam/wswp).
    All code can then be run from the `code` directory in the repository so imports
    work properly. If you would like to set up a different structure, note that you
    will need to change all imports from other chapters (such as the `from chp1.advanced_link_crawler `
    in the following code).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个章节可能会构建或使用前几章的部分，我们建议设置你的文件结构类似于 [本书仓库](https://github.com/kjam/wswp) 中的结构。然后所有代码都可以从仓库中的
    `code` 目录运行，以便导入正常工作。如果你想要设置不同的结构，请注意，你需要更改其他章节的所有导入（例如以下代码中的 `from chp1.advanced_link_crawler`）。
- en: 'To scrape the country area using regular expressions, we will first try matching
    the contents of the `<td>` element, as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用正则表达式爬取国家区域，我们首先尝试匹配 `<td>` 元素的正文，如下所示：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This result shows that the `<td class="w2p_fw">` tag is used for multiple country
    attributes. If we simply wanted to scrape the country area, we can select the
    second matching element, as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果显示 `<td class="w2p_fw">` 标签用于多个国家属性。如果我们只想爬取国家区域，我们可以选择第二个匹配元素，如下所示：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This solution works but could easily fail if the web page is updated. Consider
    if this table is changed and the area is no longer in the second matching element.
    If we just need to scrape the data now, future changes can be ignored. However,
    if we want to re-scrape this data at some point, we want our solution to be as
    robust against layout changes as possible. To make this regular expression more
    specific, we can include the parent `<tr>` element, which has an ID, so it ought
    to be unique:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案是可行的，但如果网页被更新，它可能会很容易失败。考虑如果这个表格被更改，并且区域不再位于第二个匹配元素中。如果我们现在只需要爬取数据，未来的更改可以忽略。然而，如果我们想在某个时候重新爬取这些数据，我们希望我们的解决方案尽可能地对布局更改具有鲁棒性。为了使这个正则表达式更加具体，我们可以包括具有ID的父
    `<tr>` 元素，这样它应该是唯一的：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This iteration is better; however, there are many other ways the web page could
    be updated in a way that still breaks the regular expression. For example, double
    quotation marks might be changed to single, extra spaces could be added between
    the `<td>` tags, or the `area_label` could be changed. Here is an improved version
    to try and support these various possibilities:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个迭代更好；然而，网页还有许多其他可能的方式更新，仍然会破坏正则表达式。例如，双引号可能被改为单引号，`<td>`标签之间可能添加额外的空格，或者`area_label`可能被更改。这里有一个改进的版本，尝试支持这些各种可能性：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This regular expression is more future-proof but is difficult to construct,
    and quite unreadable. Also, there are still plenty of other minor layout changes
    that would break it, such as if a title attribute was added to the `<td>` tag
    or if the `tr` or `td` elements changed their CSS classes or IDs.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个正则表达式更具未来性，但构建起来比较困难，而且相当难以阅读。此外，还有许多其他微小的布局更改可能会使其失效，例如如果向`<td>`标签添加了标题属性，或者`tr`或`td`元素改变了它们的CSS类或ID。
- en: From this example, it is clear that regular expressions provide a quick way
    to scrape data but are too brittle and easily break when a web page is updated.
    Fortunately, there are better data extraction solutions such as the other scraping
    libraries we will cover throughout this chapter.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个例子中可以看出，正则表达式提供了一种快速抓取数据的方法，但它们太脆弱，当网页更新时很容易失效。幸运的是，有更好的数据提取解决方案，例如我们将在本章中涵盖的其他抓取库。
- en: Beautiful Soup
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 美味汤
- en: '**Beautiful Soup** is a popular library that parses a web page and provides
    a convenient interface to navigate content. If you do not already have this module,
    the latest version can be installed using this command:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**Beautiful Soup**是一个流行的库，它解析网页并提供了一个方便的接口来导航内容。如果你还没有这个模块，可以使用以下命令安装最新版本：'
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The first step with Beautiful Soup is to parse the downloaded HTML into a soup
    document. Many web pages do not contain perfectly valid HTML and Beautiful Soup
    needs to correct improper open and close tags. For example, consider this simple
    web page containing a list with missing attribute quotes and closing tags:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Beautiful Soup的第一步是将下载的HTML解析成一个soup文档。许多网页不包含完全有效的HTML，Beautiful Soup需要纠正不正确的开放和闭合标签。例如，考虑这个包含缺失属性引号和闭合标签的简单网页列表：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If the `Population` item is interpreted as a child of the `Area` item instead
    of the list, we could get unexpected results when scraping. Let us see how Beautiful
    Soup handles this:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将`Population`项解释为`Area`项的子项而不是列表，在抓取时可能会得到意外的结果。让我们看看Beautiful Soup是如何处理这个问题的：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can see that using the default `html.parser` did not result in properly
    parsed HTML. We can see from the previous snippet that it has used nested `li`
    elements, which might make it difficult to navigate. Luckily there are more options
    for parsers. We can install **LXML** (as described in the next section) or we
    can also use **html5lib**. To install **html5lib**, simply use pip:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，使用默认的`html.parser`并没有正确解析HTML。从前面的代码片段中我们可以看到，它使用了嵌套的`li`元素，这可能会使其导航变得困难。幸运的是，还有更多的解析器选项。我们可以安装**LXML**（如下一节所述）或者我们也可以使用**html5lib**。要安装**html5lib**，只需使用pip：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we can repeat this code, changing only the parser like so:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以重复这段代码，只需更改解析器，如下所示：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, `BeautifulSoup` using `html5lib` was able to correctly interpret the missing
    attribute quotes and closing tags, as well as add the `<html>` and `<body>` tags
    to form a complete HTML document. You should see similar results if you used `lxml`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，使用`html5lib`的`BeautifulSoup`能够正确地解释缺失的属性引号和闭合标签，并且添加了`<html>`和`<body>`标签以形成一个完整的HTML文档。如果你使用了`lxml`，你应该会看到类似的结果。
- en: 'Now, we can navigate to the elements we want using the `find()` and `find_all()`
    methods:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`find()`和`find_all()`方法导航到我们想要的元素：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: For a full list of available methods and parameters, the official Beautiful
    Soup documentation is available at [http://www.crummy.com/software/BeautifulSoup/bs4/doc/](http://www.crummy.com/software/BeautifulSoup/bs4/doc/).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取所有可用方法和参数的完整列表，官方Beautiful Soup文档可在[http://www.crummy.com/software/BeautifulSoup/bs4/doc/](http://www.crummy.com/software/BeautifulSoup/bs4/doc/)找到。
- en: 'Now, using these techniques, here is a full example to extract the country
    area from our example website:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用这些技术，这里有一个完整的示例来从我们的示例网站中提取国家面积：
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This code is more verbose than regular expressions but easier to construct and
    understand. Also, we no longer need to worry about problems in minor layout changes,
    such as extra white space or tag attributes. We also know if the page contains
    broken HTML that Beautiful Soup can help clean the page and allow us to extract
    data from very broken website code.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码比正则表达式更冗长，但更容易构建和理解。此外，我们也不再需要担心微小布局变化中的问题，例如额外的空白或标签属性。我们还知道，如果页面包含Beautiful
    Soup可以帮助清理页面并允许我们从非常损坏的网站代码中提取数据的损坏HTML。
- en: Lxml
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Lxml
- en: '**Lxml** is a Python library built on top of the `libxml2` XML parsing library
    written in C, which helps make it faster than Beautiful Soup but also harder to
    install on some computers, specifically Windows. The latest installation instructions
    are available at [http://lxml.de/installation.html](http://lxml.de/installation.html).
    If you run into difficulties installing the library on your own, you can also
    use Anaconda to do so:  [https://anaconda.org/anaconda/lxml](https://anaconda.org/anaconda/lxml).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**Lxml** 是一个基于C语言编写的`libxml2` XML解析库构建的Python库，这使得它比Beautiful Soup更快，但也更难在一些计算机上安装，特别是Windows系统。最新的安装说明可在[http://lxml.de/installation.html](http://lxml.de/installation.html)找到。如果您在安装库时遇到困难，也可以使用Anaconda来完成：[https://anaconda.org/anaconda/lxml](https://anaconda.org/anaconda/lxml)。'
- en: If you are unfamiliar with Anaconda, it is a package and environment manager
    primarily focused on open data science packages built by the folks at Continuum
    Analytics. You can download and install Anaconda by following their setup instructions
    here: [https://www.continuum.io/downloads](https://www.continuum.io/downloads).
    Note that using the Anaconda quick install will set your `PYTHON_PATH` to the
    Conda installation of Python.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不熟悉Anaconda，它是一个由Continuum Analytics团队开发的包和环境管理器，主要专注于开放数据科学包。您可以通过遵循他们的设置说明来下载和安装Anaconda：[https://www.continuum.io/downloads](https://www.continuum.io/downloads)。请注意，使用Anaconda快速安装将设置您的`PYTHON_PATH`为Conda安装的Python。
- en: 'As with Beautiful Soup, the first step when using `lxml` is parsing the potentially
    invalid HTML into a consistent format. Here is an example of parsing the same
    broken HTML:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 与Beautiful Soup一样，使用`lxml`的第一步是将可能无效的HTML解析成一致格式。以下是一个解析相同损坏HTML的示例：
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As with `BeautifulSoup`, `lxml` was able to correctly parse the missing attribute
    quotes and closing tags, although it did not add the `<html>` and `<body>` tags.
    These are not requirements for standard XML and so are unnecessary for `lxml`
    to insert.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与`BeautifulSoup`一样，`lxml`能够正确解析缺失的属性引号和关闭标签，尽管它没有添加`<html>`和`<body>`标签。这些不是标准XML的要求，因此对于`lxml`插入来说是不必要的。
- en: 'After parsing the input, `lxml` has a number of different options to select
    elements, such as XPath selectors and a `find()` method similar to Beautiful Soup.
    Instead, we will use CSS selectors here, because they are more compact and can
    be reused later in [Chapter 5](py-web-scrp-2e_ch05.html), *Dynamic Content* when
    parsing dynamic content. Some readers will already be familiar with them from
    their experience with jQuery selectors or use in front-end web application development.
    Later in this chapter we will compare performance of these selectors with XPath.
    To use CSS selectors, you might need to install the `cssselect` library like so:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 解析输入后，`lxml`有多个不同的选项来选择元素，例如XPath选择器和类似于Beautiful Soup的`find()`方法。相反，我们将在这里使用CSS选择器，因为它们更紧凑，可以在[第5章](py-web-scrp-2e_ch05.html)“动态内容”解析动态内容时重复使用。一些读者可能已经从他们的jQuery选择器经验或前端Web应用开发中使用中熟悉它们。在本章的后面部分，我们将比较这些选择器的性能与XPath。要使用CSS选择器，您可能需要像这样安装`cssselect`库：
- en: '[PRE13]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now we can use the `lxml` CSS selectors to extract the area data from the example
    page:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用`lxml` CSS选择器从示例页面中提取区域数据：
- en: '[PRE14]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: By using the `cssselect` method on our tree, we can utilize CSS syntax to select a
    table row element with the `places_area__row` ID, and then the child table data
    tag with the `w2p_fw` class. Since `cssselect` returns a list, we then index the
    first result and call the `text_content` method, which will iterate over all child
    elements and return concatenated text of each element. In this case, we only have
    one element, but this functionality is useful to know for more complex extraction
    examples.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在我们的树结构上使用`cssselect`方法，我们可以利用CSS语法选择具有`places_area__row` ID的表格行元素，然后是具有`w2p_fw`类的子表格数据标签。由于`cssselect`返回一个列表，我们然后索引第一个结果并调用`text_content`方法，该方法将遍历所有子元素并返回每个元素的连接文本。在这种情况下，我们只有一个元素，但了解这种功能对于更复杂的提取示例是有用的。
- en: 'You can see this code and the other code for this chapter in the book code
    repository: [https://github.com/kjam/wswp/blob/master/code/chp2.](https://github.com/kjam/wswp/blob/master/code/chp2.)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书的代码存储库中查看此代码和其他章节的代码：[https://github.com/kjam/wswp/blob/master/code/chp2.](https://github.com/kjam/wswp/blob/master/code/chp2.)
- en: CSS selectors and your Browser Console
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CSS 选择器和您的浏览器控制台
- en: 'Like the notation we used to extract using `cssselect`, CSS selectors are patterns
    used for selecting HTML elements. Here are some examples of common selectors you
    should know:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们使用 `cssselect` 提取时使用的符号类似，CSS 选择器是用于选择 HTML 元素的模式。以下是一些您应该了解的常见选择器示例：
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `cssselect` library implements most CSS3 selectors, and details on unsupported
    features (primarily browser interactions) are available at [https://cssselect.readthedocs.io/en/latest/#supported-selectors](https://cssselect.readthedocs.io/en/latest/#supported-selectors).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`cssselect` 库实现了大多数 CSS3 选择器，有关不支持的功能（主要是浏览器交互）的详细信息，请参阅[https://cssselect.readthedocs.io/en/latest/#supported-selectors](https://cssselect.readthedocs.io/en/latest/#supported-selectors)。'
- en: The CSS3 specification was produced by the W3C and is available for viewing
    at [http://www.w3.org/TR/2011/REC-css3-selectors-20110929/](http://www.w3.org/TR/2011/REC-css3-selectors-20110929/).
    There is also a useful and more accessible documentation from Mozilla on their
    developer's reference for CSS: [https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors ](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: CSS3 规范是由 W3C 制定的，可在[http://www.w3.org/TR/2011/REC-css3-selectors-20110929/](http://www.w3.org/TR/2011/REC-css3-selectors-20110929/)查看。Mozilla
    还提供了一份有用且更易于访问的文档，介绍了 CSS 的开发者参考：[https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors)。
- en: Sometimes it is useful to test CSS selectors as we might not write them perfectly
    the first time. It is also a good idea to test them somewhere to debug any selection
    issues before writing many lines of Python code which may or may not work.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 有时测试 CSS 选择器很有用，因为我们可能不会第一次就写得完美。在编写大量可能无法正常工作的 Python 代码之前，在某个地方测试它们以调试任何选择问题也是一个好主意。
- en: When a site uses JQuery, it's very easy to test CSS Selectors in the browser
    console. The console is a part of your browser developer tools and allows you
    to execute JavaScript (and, if supported, JQuery) on the current page.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个网站使用 jQuery 时，在浏览器控制台中测试 CSS 选择器非常容易。控制台是浏览器开发者工具的一部分，允许您在当前页面上执行 JavaScript（如果支持，还可以执行
    jQuery）。
- en: To learn more about JQuery, there are several free online courses. The Code
    School course at [http://try.jquery.com/](http://try.jquery.com/) has a variety
    of exercises if you are interested in diving a bit deeper.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 jQuery 的信息，有几个免费的在线课程。如果您想深入了解，可以在[http://try.jquery.com/](http://try.jquery.com/)的
    Code School 课程中找到各种练习。
- en: 'The only syntax you need to know for using CSS selectors with JQuery is the
    simple object selection (i.e. `$(''div.class_name'');`). JQuery uses the `$` and
    parenthesis to select objects. Within the parenthesis you can write any CSS selector.
    Doing so in your browser console on a site that supports JQuery will allow you
    to look at the objects you have selected. Since we know the example website uses
    JQuery (either by inspecting the source code, or watching the Network tab and
    looking for JQuery to load, or using the `detectem` module), we can try selecting
    all `tr` elements using a CSS selector:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要了解的唯一语法是使用 jQuery 的简单对象选择（即 `$('div.class_name');`）。jQuery 使用 `$` 和括号来选择对象。在括号内，您可以编写任何
    CSS 选择器。在支持 jQuery 的网站上的浏览器控制台中这样做，您可以看到您选择的对象。由于我们知道示例网站使用 jQuery（可以通过检查源代码，或者查看“网络”选项卡并寻找
    jQuery 加载，或者使用 `detectem` 模块），我们可以尝试使用 CSS 选择器选择所有 `tr` 元素：
- en: '![](img/console.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/console.png)'
- en: And simply by using the tag name, I can see every row for the country data.
    I can also try selecting elements using a longer CSS selector. Let's try selecting
    all `td` elements with class `w2p_fw`, since I know this is where the primary
    data on the page lies.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 仅通过使用标签名，我就可以看到国家数据的每一行。我还可以尝试使用更长的 CSS 选择器来选择元素。让我们尝试选择所有具有类 `w2p_fw` 的 `td`
    元素，因为我知道这是页面上的主要数据所在位置。
- en: '![](img/td_console.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/td_console.png)'
- en: You may also notice that when using your mouse to click on the returned elements,
    you can expand them and also highlight them in the above window (depending on
    what browser you are using). This is a tremendously useful way to test data. If
    the site you are scraping doesn't load JQuery or any other selector friendly libraries
    from your browser, you can perform the same lookups with the `document` object
    using simple JavaScript. The documentation for the `querySelector` method is available
    on **Mozilla Developer Network**: [https://developer.mozilla.org/en-US/docs/Web/API/Document/querySelector](https://developer.mozilla.org/en-US/docs/Web/API/Document/querySelector).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能也会注意到，当使用鼠标点击返回的元素时，你可以展开它们，并在上面的窗口中突出显示它们（取决于你使用的浏览器）。这是一种极其有用的测试数据的方法。如果你正在抓取的网站没有从你的浏览器加载JQuery或其他任何与选择器友好的库，你可以使用简单的JavaScript通过`document`对象执行相同的查找。`querySelector`方法的文档可在**Mozilla开发者网络**上找到：[https://developer.mozilla.org/en-US/docs/Web/API/Document/querySelector](https://developer.mozilla.org/en-US/docs/Web/API/Document/querySelector)。
- en: Even after using CSS selectors in your console and with `lxml`, it can be useful
    to learn XPath, which is what `lxml` converts all of your CSS selectors to before
    evaluating them. To keep learning how to use XPath, read on!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在控制台和`lxml`中使用CSS选择器之后，学习XPath也可能很有用，因为`lxml`在评估之前会将所有的CSS选择器转换为XPath。为了继续学习如何使用XPath，请继续阅读！
- en: XPath Selectors
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XPath选择器
- en: There are times when using CSS selectors will not work. This is especially the
    case with very broken HTML or improperly formatted elements. Despite the best
    efforts of libraries like `BeautifulSoup` and `lxml` to properly parse and clean
    up the code; it will not always work - and in these cases, XPath can help you
    build very specific selectors based on hierarchical relationships of elements
    on the page.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候使用CSS选择器可能不起作用。这尤其适用于非常损坏的HTML或不正确格式化的元素。尽管像`BeautifulSoup`和`lxml`这样的库尽力正确解析和清理代码，但它并不总是有效
    - 在这种情况下，XPath可以帮助你根据页面元素之间的层次关系构建非常具体的选择器。
- en: XPath is a way of describing relationships as an hierarchy in XML documents.
    Because HTML is formed using XML elements, we can also use XPath to navigate and
    select elements from an HTML document.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: XPath是一种描述XML文档中关系的方式，作为一个层次结构。因为HTML是使用XML元素形成的，所以我们也可以使用XPath在HTML文档中导航和选择元素。
- en: To read more about XPath, check out the **Mozilla developer documentation**: [https://developer.mozilla.org/en-US/docs/Web/XPath](https://developer.mozilla.org/en-US/docs/Web/XPath).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于XPath的信息，请查看**Mozilla开发者文档**：[https://developer.mozilla.org/en-US/docs/Web/XPath](https://developer.mozilla.org/en-US/docs/Web/XPath)。
- en: XPath follows some basic syntax rules and has some similarities with CSS selectors.
    Take a look at the following chart for some quick references between the two.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: XPath遵循一些基本的语法规则，并且与CSS选择器有一些相似之处。请查看以下图表，了解两者之间的快速参考。
- en: '| **Selector description** | **XPath Selector** | **CSS selector** |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| **选择器描述** | **XPath选择器** | **CSS选择器** |'
- en: '| Select all links | ''//a'' | ''a'' |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 选择所有链接 | ''//a'' | ''a'' |'
- en: '| Select div with class "main" | ''//div[@class="main"]'' | ''div.main'' |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 选择具有“main”类的div | ''//div[@class="main"]'' | ''div.main'' |'
- en: '| Select ul with ID "list" | ''//ul[@id="list"]'' | ''ul#list'' |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 选择ID为“list”的ul | ''//ul[@id="list"]'' | ''ul#list'' |'
- en: '| Select text from all paragraphs | ''//p/text()'' | ''p''* |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 从所有段落中选择文本 | ''//p/text()'' | ''p*'' |'
- en: '| Select all divs which contain ''test'' in the class | ''//div[contains(@class,
    ''test'')]'' | ''div [class*="test"]'' |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 选择类中包含''test''的所有div | ''//div[contains(@class, ''test'')]'' | ''div [class*="test"]''
    |'
- en: '| Select all divs with links or lists in them | ''//div[a&#124;ul] '' | ''div
    a, div ul'' |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 选择包含链接或列表的所有div | ''//div[a|ul]'' | ''div a, div ul'' |'
- en: '| Select a link with google.com in the href | ''//a[contains(@href, "google.com")]
    | ''a''* |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 选择href中包含google.com的链接 | ''//a[contains(@href, "google.com")] | ''a*'' |'
- en: As you can see from the previous table, there are many similarities between
    the syntax. However, in the chart there are certain CSS selectors noted with a
    `*`. These indicate that it is not exactly possible to select these elements using
    CSS, and we have provided the best alternative. In these cases, if you were using `cssselect` you
    will need to do further manipulation or iteration within Python and/or `lxml`.
    Hopefully this comparison has shown an introduction to XPath and convinced you
    that it is more exacting and specific than simply using CSS.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从前面的表中可以看出，语法之间有许多相似之处。然而，在图表中，有一些 CSS 选择器用 `*` 标记，这表明使用 CSS 无法精确地选择这些元素，我们已提供了最佳替代方案。在这些情况下，如果您使用
    `cssselect`，您将需要在 Python 和/或 `lxml` 中进行进一步的操作或迭代。希望这个比较已经向您介绍了 XPath，并让您相信它比简单地使用
    CSS 更精确和具体。
- en: 'Now that we have a basic introduction to the XPath syntax, let''s see how we
    can use it for our example website:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对 XPath 语法有了基本的了解，让我们看看我们如何可以将其用于我们的示例网站：
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Similar to CSS selectors, you can also test XPath selectors in your browser
    console. To do so, on a page with selectors simply use the `$x('pattern_here');`
    selector. Similarly, you can also use the `document` object from simple JavaScript
    and call the `evaluate` method.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 与 CSS 选择器类似，您也可以在浏览器控制台中测试 XPath 选择器。要做到这一点，在具有选择器的页面上，只需使用 `$x('pattern_here');`
    选择器。同样，您也可以使用简单的 JavaScript 中的 `document` 对象并调用 `evaluate` 方法。
- en: 'The Mozilla developer network has a useful introduction to using XPath with
    JavaScript tutorial here:  [https://developer.mozilla.org/en-US/docs/Introduction_to_using_XPath_in_JavaScript](https://developer.mozilla.org/en-US/docs/Introduction_to_using_XPath_in_JavaScript)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Mozilla 开发者网络有一个有用的介绍，介绍了如何使用 XPath 与 JavaScript 教程：[https://developer.mozilla.org/en-US/docs/Introduction_to_using_XPath_in_JavaScript](https://developer.mozilla.org/en-US/docs/Introduction_to_using_XPath_in_JavaScript)
- en: 'If we wanted to test looking for `td` elements with images in them to get the
    flag data from the country pages, we could test our XPath pattern in our browser
    first:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要测试寻找包含图像的 `td` 元素以从国家页面获取标志数据，我们可以在浏览器中首先测试我们的 XPath 模式：
- en: '![](img/xpath_console.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/xpath_console.png)'
- en: Here we can see that we can use attributes to specify the data we want to extract
    (such as `@src`). By testing in the browser, we save debugging time by getting
    immediate and easy-to-read results.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们可以使用属性来指定我们想要提取的数据（例如 `@src`）。通过在浏览器中进行测试，我们可以节省调试时间，并立即获得易于阅读的结果。
- en: We will be using both XPath and CSS selectors throughout this chapter and further
    chapters, so you can become more familiar with them and feel confident using them
    as you advance your web scraping capabilities.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章和后续章节中使用 XPath 和 CSS 选择器，这样您可以更熟悉它们，并在提高您的网络爬虫能力时更有信心地使用它们。
- en: LXML and Family Trees
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LXML 和家谱
- en: '`lxml` also has the ability to traverse family trees within the HTML page.
    What is a family tree? When you used your browser''s developer tools to investigate
    the elements on the page and you were able to expand or retract them, you were
    observing family relationships in the HTML. Every element on a web page can have
    parents, siblings and children. These relationships can help us more easily traverse
    the page.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`lxml` 还具有在 HTML 页面内遍历家谱的能力。什么是家谱？当您使用浏览器的开发者工具调查页面上的元素，并且能够展开或收起它们时，您正在观察
    HTML 中的家谱关系。网页上的每个元素都可以有父元素、兄弟元素和子元素。这些关系可以帮助我们更轻松地遍历页面。'
- en: For example, if I want to find all the elements at the same node depth level
    on the page, I would be looking for their siblings. Or maybe I want every element
    that is a child of a particular element on the page. `lxml` allows us to use many
    of these relationships with simple Python code.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我想在页面上找到同一节点深度级别的所有元素，我就会寻找它们的兄弟元素。或者，也许我想找到页面上特定元素的子元素。`lxml` 允许我们通过简单的
    Python 代码使用这些关系中的许多。
- en: 'As an example, let''s investigate all children of the `table` element on the
    example page:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们调查示例页面上 `table` 元素的所有子元素：
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can also see the table''s siblings and parent elements:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到表格的兄弟元素和父元素：
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: If you need a more general way to access elements on the page, traversing the
    familial relationships combined with XPath expressions is a good way to ensure
    you don't miss any content. This can help you extract content from many different
    types of pages where you might be able to identify some important parts of the
    page simply by identifying content that appears near those elements on the page.
    This method will also work even when the elements do not have identifiable CSS
    selectors.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要一个更通用的方法来访问页面上的元素，结合遍历家族关系和 XPath 表达式是一种确保不遗漏任何内容的好方法。这可以帮助你从许多不同类型的页面上提取内容，在这些页面上，你只需通过识别出现在页面元素附近的内容，就能简单地识别页面的某些重要部分。这种方法即使在元素没有可识别的
    CSS 选择器的情况下也能工作。
- en: Comparing performance
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能比较
- en: 'To help evaluate the trade-offs between the three scraping approaches described
    in the section, *Three approaches to scrape a web page*, it would be helpful to
    compare their relative efficiency. Typically, a scraper would extract multiple
    fields from a web page. So, for a more realistic comparison, we will implement
    extended versions of each scraper which extract all the available data from a
    country''s web page. To get started, we need to return to our browser to check
    the format of the other country features, as shown here:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助评估本节中描述的三个抓取方法之间的权衡，即“三种抓取网页的方法”，比较它们的相对效率将是有帮助的。通常，抓取器会从一个网页中提取多个字段。因此，为了进行更现实的比较，我们将实现每个抓取器的扩展版本，这些版本将从一个国家的网页中提取所有可用数据。为了开始，我们需要回到我们的浏览器中检查其他国家特征的格式，如下所示：
- en: '![](img/4364OS_02_04.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4364OS_02_04.jpg)'
- en: 'By using our browser''s inspect capabilities, we can see each table row has
    an ID starting with `places_` and ending with `__row`. The country data is contained
    within these rows in the same format as the area example. Here are implementations
    that use this information to extract all of the available country data:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用我们浏览器的检查功能，我们可以看到每一行表格都有一个以 `places_` 开头并以 `__row` 结尾的 ID。国家数据以与区域示例相同的格式包含在这些行中。以下是一些使用这些信息提取所有可用国家数据的实现示例：
- en: '[PRE19]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Scraping results
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抓取结果
- en: 'Now that we have complete implementations for each scraper, we will test their
    relative performance with this snippet. The imports in the code expect your directory
    structure to be similar to the book''s repository, so please adjust as necessary:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为每个抓取器完成了完整的实现，我们将使用此片段测试它们的相对性能。代码中的导入期望你的目录结构与本书的存储库相似，所以请根据需要调整：
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This example will run each scraper 1000 times, check whether the scraped results
    are as expected, and then print the total time taken. The `download` function
    used here is the one defined in the preceding chapter. Note the highlighted line
    calling `re.purge()`; by default, the regular expression module will cache searches
    and this cache needs to be cleared to make a fair comparison with the other scraping
    approaches.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例将运行每个抓取器1000次，检查抓取结果是否符合预期，然后打印出总耗时。这里使用的 `download` 函数是前面章节中定义的。注意高亮显示的调用
    `re.purge()` 的行；默认情况下，正则表达式模块会缓存搜索，并且需要清除这个缓存，以便与其他抓取方法进行公平的比较。
- en: 'Here are the results from running this script on my computer:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我在我的电脑上运行此脚本的结果：
- en: '[PRE21]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The results on your computer will quite likely be different because of the different
    hardware used. However, the relative difference between each approach should be
    similar. The results show Beautiful Soup is over six times slower than the other
    approaches when used to scrape our example web page. This result could be anticipated
    because `lxml` and the regular expression module were written in C, while `BeautifulSoup`
    is pure Python. An interesting fact is that `lxml` performed comparatively well
    with regular expressions, since `lxml` has the additional overhead of having to
    parse the input into its internal format before searching for elements. When scraping
    many features from a web page, this initial parsing overhead is reduced and `lxml`
    becomes even more competitive. As we can see with the XPath parser, `lxml` is
    able to directly compete with regular expressions. It really is an amazing module!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用的硬件不同，你电脑上的结果很可能会有所不同。然而，每种方法之间的相对差异应该是相似的。结果显示，当用于抓取我们的示例网页时，Beautiful
    Soup比其他方法慢了六倍以上。这个结果是可以预料的，因为`lxml`和正则表达式模块是用C编写的，而`BeautifulSoup`是纯Python编写的。一个有趣的事实是，`lxml`与正则表达式相比表现相当好，因为`lxml`在搜索元素之前必须将输入解析为其内部格式的额外开销。当我们从网页抓取许多特征时，这种初始解析开销减少，`lxml`变得更加有竞争力。正如我们通过XPath解析器所看到的，`lxml`能够直接与正则表达式竞争。这真是一个令人惊叹的模块！
- en: Although we strongly encourage you to use `lxml` for parsing, the biggest performance
    bottleneck for web scraping is usually the network. We will discuss approaches
    to parallelize workflows, allowing you to increase the speed of your crawlers
    by having multiple requests work in parallel.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们强烈建议你使用`lxml`进行解析，但网络抓取的最大性能瓶颈通常是网络。我们将讨论并行化工作流程的方法，通过让多个请求并行工作来提高你爬虫的速度。
- en: Overview of Scraping
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抓取概述
- en: 'The following table summarizes the advantages and disadvantages of each approach
    to scraping:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了每种抓取方法的优缺点：
- en: '| **Scraping approach** | **Performance** | **Ease of use** | **Ease to install**
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| **抓取方法** | **性能** | **易用性** | **安装难度** |'
- en: '| Regular expressions | Fast | Hard | Easy (built-in module) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 正则表达式 | 快速 | 困难 | 容易（内置模块） |'
- en: '| Beautiful Soup | Slow | Easy | Easy (pure Python) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Beautiful Soup | 慢速 | 容易 | 容易（纯Python） |'
- en: '| Lxml | Fast | Easy | Moderately difficult |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Lxml | 快速 | 容易 | 中等难度 |'
- en: If speed is not an issue to you and you prefer to only install libraries via
    pip, it would not be a problem to use a slower approach, such as Beautiful Soup.
    Or, if you just need to scrape a small amount of data and want to avoid additional
    dependencies, regular expressions might be an appropriate choice. However, in
    general, `lxml` is the best choice for scraping, because it is fast and robust,
    while regular expressions and Beautiful Soup are not as speedy or as easy to modify.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果速度对你来说不是问题，并且你更喜欢只通过pip安装库，那么使用较慢的方法，例如Beautiful Soup，就不会有问题。或者，如果你只需要抓取少量数据并且想避免额外的依赖，正则表达式可能是一个合适的选择。然而，总的来说，`lxml`是抓取的最佳选择，因为它既快又健壮，而正则表达式和Beautiful
    Soup则不那么快，也不容易修改。
- en: Adding a scrape callback to the link crawler
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在链接爬虫中添加抓取回调
- en: 'Now that we know how to scrape the country data, we can integrate this into
    the link crawler built in [Chapter 1](py-web-scrp-2e_ch01.html), *Introduction
    to Web Scraping*. To allow reusing the same crawling code to scrape multiple websites,
    we will add a `callback` parameter to handle the scraping. A `callback` is a function
    that will be called after certain events (in this case, after a web page has been
    downloaded). This scrape `callback` will take a `url` and `html` as parameters
    and optionally return a list of further URLs to crawl. Here is the implementation,
    which is simple in Python:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何抓取国家数据，我们可以将此集成到[第1章](py-web-scrp-2e_ch01.html)中构建的链接爬虫中，*网络抓取简介*。为了允许重用相同的爬虫代码抓取多个网站，我们将添加一个`callback`参数来处理抓取。`callback`是一个函数，将在某些事件（在这种情况下，网页下载后）之后被调用。这个抓取`callback`将接受一个`url`和`html`作为参数，并可选择返回一个要爬取的进一步URL列表。以下是Python中的实现，它很简单：
- en: '[PRE22]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The new code for the scraping `callback` function are highlighted in the preceding
    snippet, and the full source code for this version of the link crawler is available
    at [https://github.com/kjam/wswp/blob/master/code/chp2/advanced_link_crawler.py](https://github.com/kjam/wswp/blob/master/code/chp2/advanced_link_crawler.py).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段中突出显示了抓取`回调`函数的新代码，这个版本的链接爬虫的完整源代码可在[https://github.com/kjam/wswp/blob/master/code/chp2/advanced_link_crawler.py](https://github.com/kjam/wswp/blob/master/code/chp2/advanced_link_crawler.py)找到。
- en: 'Now, this crawler can be used to scrape multiple websites by customizing the
    function passed to `scrape_callback`. Here is a modified version of the `lxml`
    example scraper that can be used for the `callback` function:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个爬虫可以通过自定义传递给`scrape_callback`的函数来用于抓取多个网站。下面是一个修改后的`lxml`示例爬虫，它可以用于`回调`函数：
- en: '[PRE23]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This `callback` function will scrape the country data and print it out. We
    can test it by importing the two functions and calling them with our regular expression
    and URL:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`回调`函数将抓取国家数据并将其打印出来。我们可以通过导入这两个函数并使用我们的正则表达式和URL来测试它：
- en: '[PRE24]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You should now see output showing the downloading of pages as well as some
    rows showing the URL and scraped data, like so:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在应该能看到输出显示正在下载页面，以及一些显示URL和抓取数据的行，如下所示：
- en: '[PRE25]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Usually, when scraping a website, we want to reuse the data rather than simply
    print it, so we will extend this example to save results to a CSV spreadsheet,
    as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当我们抓取网站时，我们希望重用数据而不是简单地打印它，因此我们将扩展这个示例以将结果保存到CSV电子表格中，如下所示：
- en: '[PRE26]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: To build this `callback`, a class was used instead of a function so that the
    state of the `csv` writer could be maintained. This `csv` writer is instantiated
    in the constructor, and then written to multiple times in the `__call__` method.
    Note that `__call__` is a special method that is invoked when an object is "called"
    as a function, which is how the `cache_callback` is used in the link crawler.
    This means that `scrape_callback(url, html)` is equivalent to calling `scrape_callback.__call__(url,
    html)`. For further details on Python's special class methods, refer to [https://docs.python.org/3/reference/datamodel.html#special-method-names](https://docs.python.org/3/reference/datamodel.html#special-method-names).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建这个`回调`，使用了类而不是函数，以便可以维护`csv`写入器的状态。这个`csv`写入器在构造函数中实例化，然后在`__call__`方法中多次写入。请注意，`__call__`是一个特殊方法，当对象作为函数“调用”时会被调用，这就是链接爬虫中`cache_callback`的使用方式。这意味着`scrape_callback(url,
    html)`等同于调用`scrape_callback.__call__(url, html)`。有关Python特殊类方法的更多详细信息，请参阅[https://docs.python.org/3/reference/datamodel.html#special-method-names](https://docs.python.org/3/reference/datamodel.html#special-method-names)。
- en: 'Here is how to pass this callback to the link crawler:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何将这个回调传递给链接爬虫的示例：
- en: '[PRE27]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Note that the `CsvCallback` expects there to be a `data` directory on the same
    level as the parent folder from where you are running the code. This can also
    be modified, but we advise you to follow good coding practices and keep your code
    and data separate -- allowing you to keep your code under version control while
    having your `data` folder in the `.gitignore` file. Here''s an example directory
    structure:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`CsvCallback`期望在运行代码的父文件夹同一级别上有一个`data`目录。这也可以修改，但我们建议你遵循良好的编码实践，并将你的代码和数据分开--这样你可以将代码置于版本控制之下，同时将`data`文件夹放在`.gitignore`文件中。以下是一个示例目录结构：
- en: '[PRE28]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, when the crawler is run with this `scrape_callback`, it will save results
    to a CSV file that can be viewed in an application such as Excel or LibreOffice.
    It might take a bit longer to run than the first time, as it is actively collecting
    information. When the scraper exits, you should be able to view your CSV with
    all the data:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当使用这个`scrape_callback`运行爬虫时，它将结果保存到CSV文件中，可以在Excel或LibreOffice等应用程序中查看。它可能比第一次运行时间稍长，因为它正在积极收集信息。当爬虫退出时，你应该能够查看包含所有数据的CSV文件：
- en: '![](img/countries.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![国家](img/countries.png)'
- en: Success! We have completed our first working scraper.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！我们已经完成了我们的第一个工作爬虫。
- en: Summary
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we walked through a variety of ways to scrape data from a web
    page. Regular expressions can be useful for a one-off scrape or to avoid the overhead
    of parsing the entire web page, and `BeautifulSoup` provides a high-level interface
    while avoiding any difficult dependencies. However, in general, `lxml` will be
    the best choice because of its speed and extensive functionality, so we will use
    it in future examples.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了从网页中抓取数据的多种方法。正则表达式对于一次性抓取或避免解析整个网页的开销非常有用，而`BeautifulSoup`提供了一个高级接口，同时避免了任何复杂的依赖。然而，总的来说，由于`lxml`的速度和广泛的功能，它将是最佳选择，因此我们将在未来的示例中使用它。
- en: We also learned how to inspect HTML pages using browser tools and the console
    and define CSS selectors and XPath selectors to match and extract content from
    the downloaded pages.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学习了如何使用浏览器工具和控制台检查HTML页面，并定义CSS选择器和XPath选择器以匹配和从下载的页面中提取内容。
- en: In the next chapter we will introduce caching, which allows us to save web pages
    so they only need be downloaded the first time a crawler is run.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍缓存，这允许我们保存网页，以便在第一次运行爬虫时只需下载一次。
