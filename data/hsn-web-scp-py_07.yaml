- en: Web Scraping Using Scrapy and Beautiful Soup
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scrapy和Beautiful Soup进行网络抓取
- en: So far, we have learned about web-development technologies, data-finding techniques,
    and accessing various Python libraries to scrape data from the web.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了Web开发技术、数据查找技术，并访问了各种Python库，以从Web上抓取数据。
- en: In this chapter, we will be learning about and exploring two Python libraries
    that are popular for document parsing and scraping activities: Scrapy and Beautiful
    Soup.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习和探索两个用于文档解析和抓取活动的流行Python库：Scrapy和Beautiful Soup。
- en: Beautiful Soup deals with document parsing. Parsing a document is done for element
    traversing and extracting its content. Scrapy is a web crawling framework written
    in Python. It provides a project-oriented scope for web scraping. Scrapy provides
    plenty of built-in resources for email, selectors, items, and so on, and can be
    used from simple to API-based content extraction.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Beautiful Soup处理文档解析。解析文档是为了遍历元素并提取其内容。Scrapy是用Python编写的网络爬虫框架。它为网络抓取提供了面向项目的范围。Scrapy提供了大量内置资源，用于电子邮件、选择器、项目等，并可用于从简单到基于API的内容提取。
- en: 'In this chapter, we will learn about the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下内容：
- en: Web scraping using Beautiful Soup
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Beautiful Soup进行网络抓取
- en: Web scraping using Scrapy
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Scrapy进行网络抓取
- en: Deploying a web crawler (learning how to deploy scraping code using [https://www.scrapinghub.com](https://www.scrapinghub.com)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署网络爬虫（学习如何使用[https://www.scrapinghub.com](https://www.scrapinghub.com)部署抓取代码）
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'A web browser (Google Chrome or Mozilla Firefox) is required and we will be
    using the application and Python libraries listed here:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 需要一个网络浏览器（Google Chrome或Mozilla Firefox），我们将使用此应用程序和列出的Python库：
- en: Latest Python 3.7* or Python 3.0* (installed)
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最新的Python 3.7*或Python 3.0*（已安装）
- en: 'The Python libraries required are the following:'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所需的Python库如下：
- en: '`lxml`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lxml`'
- en: '`requests`, ``urllib``'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`requests`，`urllib`'
- en: '`bs4` or `beautifulsoup4`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bs4`或`beautifulsoup4`'
- en: '`scrapy`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scrapy`'
- en: For setting up or installation refer to [Chapter 2](b9919ebf-2d5c-4721-aa76-5c1378262473.xhtml),
    *Python and the Web – Using urllib and Requests*, *Setting things up* section.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有关设置或安装，请参阅[第2章](b9919ebf-2d5c-4721-aa76-5c1378262473.xhtml)，*Python和Web - 使用urllib和Requests*，*设置事项*部分。
- en: Code files are available online at GitHub: [https://github.com/PacktPublishing/Hands-On-Web-Scraping-with-Python/tree/master/Chapter05](https://github.com/PacktPublishing/Hands-On-Web-Scraping-with-Python/tree/master/Chapter05).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 代码文件可在GitHub上找到：[https://github.com/PacktPublishing/Hands-On-Web-Scraping-with-Python/tree/master/Chapter05](https://github.com/PacktPublishing/Hands-On-Web-Scraping-with-Python/tree/master/Chapter05)。
- en: Web scraping using Beautiful Soup
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Beautiful Soup进行网络抓取
- en: Web scraping is a procedure for extracting data from web documents. For data
    collection or extracting data from web documents, identifying and traversing through
    elements (of HTML, XML) is the basic requirement. Web documents are built with
    various types of elements that can exist either individually or nested together.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Web scraping是从Web文档中提取数据的过程。对于数据收集或从Web文档中提取数据，识别和遍历元素（HTML、XML）是基本要求。Web文档由各种类型的元素构建，可以单独存在或嵌套在一起。
- en: Parsing is an activity of breaking down, exposing, or identifying the components
    with contents from any given web content. Such activity enhances features such
    as searching and collecting content from the desired element or elements. Web
    documents obtained, parsed, and traversed through looking for required data or
    content is the basic scraping task.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 解析是从任何给定的Web内容中分解、暴露或识别具有内容的组件的活动。这种活动增强了搜索和收集所需元素的内容的功能。获取、解析和遍历Web文档，以查找所需的数据或内容，是基本的抓取任务。
- en: In [Chapter 3](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml), *Using LXML, XPath,
    and CSS Selectors*, we explored lxml for a similar task and used XPath and CSS
    Selectors for data-extraction purposes. lxml is also used for scraping and parsing
    because of its memory-efficient features and extensible libraries.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml)中，*使用LXML、XPath和CSS选择器*，我们探索了lxml进行类似的任务，并使用XPath和CSS选择器进行数据提取。lxml也用于抓取和解析，因为它具有内存高效的特性和可扩展的库。
- en: In the next subsection, we will learn and explore features of the Python `bs4` library (for
    Beautiful Soup).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一小节中，我们将学习和探索Python `bs4`库（用于Beautiful Soup）的特性。
- en: Introduction to Beautiful Soup
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Beautiful Soup简介
- en: Beautiful Soup is generally identified as a parsing library, and is also known
    as an HTML parser that is used to parse web documents either in HTML or XML. It
    generates a parsed tree similar to lxml (ElementTree), which is used to identify
    and traverse through elements to extract data and perform web scraping.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Beautiful Soup通常被识别为解析库，也被称为用于解析Web文档的HTML解析器或XML。它生成类似于lxml（ElementTree）的解析树，用于识别和遍历元素以提取数据和进行网络抓取。
- en: Beautiful Soup provides complete parsing-related features that are available
    using `lxml` and `htmllib`. Collections of simple and easy-to-use methods, plus
    properties to deal with navigation, searching, and parsing-related activity, make
    Beautiful Soup a favorite among other Python libraries.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Beautiful Soup提供了完整的解析相关功能，可以使用`lxml`和`htmllib`。一系列简单易用的方法，以及用于导航、搜索和解析相关活动的属性，使Beautiful
    Soup成为其他Python库中的首选。
- en: Document encoding can be handled manually using the Beautiful Soup constructor,
    but Beautiful Soup handles encoding-related tasks automatically unless specified
    by the constructor.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用Beautiful Soup构造函数手动处理文档编码，但除非构造函数指定，否则Beautiful Soup会自动处理与编码相关的任务。
- en: One of the distinguishing features of Beautiful Soup, over other libraries and
    parsers, is that it can also be used to parse broken HTML or files with incomplete
    or missing tags. For more information on Beautiful Soup, please visit [https://www.crummy.com/software/BeautifulSoup](https://www.crummy.com/software/BeautifulSoup)*.*
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Beautiful Soup的一个显著特点是，它可以用来解析损坏的HTML或具有不完整或缺失标签的文件。有关Beautiful Soup的更多信息，请访问[https://www.crummy.com/software/BeautifulSoup](https://www.crummy.com/software/BeautifulSoup)。
- en: Let's now explore and learn some of the major tools and methods relevant to
    the data-extraction process using Beautiful Soup.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探索并学习使用Beautiful Soup进行数据提取过程的一些主要工具和方法。
- en: Exploring Beautiful Soup
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索美丽汤
- en: 'The Python `bs4` library contains a `BeautifulSoup` class, which is used for
    parsing. For more details on Beautiful Soup and installing the library, please
    refer to the official documentation on installing Beautiful Soup at [https://www.crummy.com/software/BeautifulSoup/](https://www.crummy.com/software/BeautifulSoup/).
    On successful installation of the library, we can obtain the details as shown
    in the following screenshot, using Python IDE:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Python的`bs4`库包含一个用于解析的`BeautifulSoup`类。有关Beautiful Soup和安装该库的更多详细信息，请参阅[https://www.crummy.com/software/BeautifulSoup/](https://www.crummy.com/software/BeautifulSoup/)上的官方文档。在成功安装库后，我们可以使用Python
    IDE获取如下屏幕截图中显示的详细信息：
- en: '![](assets/215ed3ea-bb33-426e-8a7f-e2859bbc5e07.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/215ed3ea-bb33-426e-8a7f-e2859bbc5e07.png)'
- en: Successful installation of bs4 with details
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 成功安装带有详细信息的bs4
- en: Also, the collection of simple (named) and explainable methods available as
    seen in the preceding screenshot and encoding support makes it more popular among
    developers.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，简单（命名）和可解释的方法集合以及编码支持使其在开发人员中更受欢迎。
- en: 'Let''s import `BeautifulSoup` and `SoupStrainer` from `bs4`, as seen in the
    following code:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`bs4`中导入`BeautifulSoup`和`SoupStrainer`，如下所示：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will be using the HTML as shown in the following snippet or `html_doc` as
    a sample to explore some of the fundamental features of Beautiful Soup. The response
    obtained for any chosen URL, using `requests` or `urllib`, can also be used for
    content in real scraping cases:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下片段或`html_doc`中显示的HTML作为示例，来探索Beautiful Soup的一些基本特性。还可以使用`requests`或`urllib`获取任何选择的URL的响应，以在真实的抓取案例中用于内容：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To proceed with parsing and accessing Beautiful Soup methods and properties,
    a Beautiful Soup object, generally known as a soup object, must be created. Regarding
    the type of string or markup content provided in the constructor, a few examples
    of creating Beautiful Soup objects, along with the parameters mentioned earlier,
    are listed next:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要继续解析和访问Beautiful Soup的方法和属性，通常需要创建一个Beautiful Soup对象，通常称为soup对象。关于构造函数中提供的字符串或标记内容的类型，下面列出了创建Beautiful
    Soup对象的一些示例，以及前面提到的参数：
- en: '`soup = Beautifulsoup(html_markup)`'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`soup = Beautifulsoup(html_markup)`'
- en: '`soup = Beautifulsoup(html_markup, ''lxml'')`'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`soup = Beautifulsoup(html_markup, ''lxml'')`'
- en: '`soup = Beautifulsoup(html_markup, ''lxml'', parse_from=SoupStrainer("a"))`'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`soup = Beautifulsoup(html_markup, ''lxml'', parse_from=SoupStrainer("a"))`'
- en: '`soup = Beautifulsoup(html_markup, ''html.parser'')`'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`soup = Beautifulsoup(html_markup, ''html.parser'')`'
- en: '``soup = Beautifulsoup(html_markup, ''html5lib'')``'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '``soup = Beautifulsoup(html_markup, ''html5lib'')``'
- en: '`soup = Beautifulsoup(xml_markup, ''xml'')`'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`soup = Beautifulsoup(xml_markup, ''xml'')`'
- en: '`soup = Beautifulsoup(some_markup, from_encoding=''ISO-8859-8'')`'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`soup = Beautifulsoup(some_markup, from_encoding=''ISO-8859-8'')`'
- en: '`soup = Beautifulsoup(some_markup, exclude_encodings=[''ISO-8859-7''])`'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`soup = Beautifulsoup(some_markup, exclude_encodings=[''ISO-8859-7''])`'
- en: 'The Beautiful Soup constructor plays an important part and we will explore
    some of the important parameters here:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Beautiful Soup构造函数起着重要作用，我们将在这里探索一些重要的参数：
- en: '`markup`: The first parameter passed to the constructor accepts a string or
    objects to be parsed.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`markup`：传递给构造函数的第一个参数接受要解析的字符串或对象。'
- en: '`features`: The name of the parser or type of markup to be used for `markup`.
    The parser can be `lxml`, `lxml-xml`, `html.parser`, or `html5lib`. Similarly,
    markup types that can be used are `html`, `html5`, and `xml`. Different types
    of supported parsers can be used with Beautiful Soup. If we just want to parse
    some HTML, we can simply pass the markup to Beautiful Soup and it will use the
    appropriate parser installed accordingly. For more information on parsers and
    their installation, please visit installing a parser at [https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`features`：解析器的名称或要用于`markup`的标记类型。解析器可以是`lxml`、`lxml-xml`、`html.parser`或`html5lib`。同样，可以使用的标记类型包括`html`、`html5`和`xml`。可以使用不同类型的支持解析器与Beautiful
    Soup。如果我们只想解析一些HTML，我们可以简单地将标记传递给Beautiful Soup，它将相应地使用安装的适当解析器。有关解析器及其安装的更多信息，请访问[https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser)。'
- en: '`parse_only`: Accepts a `bs4.SoupStrainer` object, that is, only parts of the
    document matching the `SoupStrainer` object will be used to parse. It''s pretty
    useful for scraping when only part of the document is to be parsed considering
    the effectiveness of the code and memory-related issues. For more information
    on `SoupStrainer`, please visit parsing only part of a document at [https://www.crummy.com/software/BeautifulSoup/bs4/doc/#parsing-only-part-of-a-document](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#parsing-only-part-of-a-document).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parse_only`：接受一个`bs4.SoupStrainer`对象，即只有与`SoupStrainer`对象匹配的文档部分将用于解析。在只有部分文档需要解析时，这非常有用，考虑到代码的有效性和与内存相关的问题。有关`SoupStrainer`的更多信息，请访问[https://www.crummy.com/software/BeautifulSoup/bs4/doc/#parsing-only-part-of-a-document](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#parsing-only-part-of-a-document)。'
- en: '`from_encoding`: Strings indicating the proper encoding are used to parse the
    markup. This is usually provided if Beautiful Soup is using the wrong encoding.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`from_encoding`：用于解析标记的字符串指示正确编码。如果Beautiful Soup使用错误的编码，通常会提供这个。'
- en: '`exclude_encodings`: A list of strings indicating the wrong encodings if used
    by Beautiful Soup.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`exclude_encodings`：指示Beautiful Soup使用的错误编码的字符串列表。'
- en: Response time is a considerable factor when using Beautiful Soup. As Beautiful
    Soup uses the parsers (`lxml`, `html.parser`, and `html5lib`), there is always
    a concern regarding the extra time consumption. Using a parser is always recommended
    to obtain similar results across platforms and systems. Also, for speeding up,
    it is recommended to use `lxml` as the parser with Beautiful Soup.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Beautiful Soup时，响应时间是一个重要因素。由于Beautiful Soup使用解析器（`lxml`、`html.parser`和`html5lib`），因此总是存在额外的时间消耗的问题。建议始终使用解析器以在各个平台和系统上获得类似的结果。此外，为了加快速度，建议使用`lxml`作为Beautiful
    Soup的解析器。
- en: For this particular case, we will be creating the `soupA` object using `lxml` as
    a parser, along with the `SoupStrainer` object `tagsA` (parsing only `<a>`, that
    is, the elements or anchor tag of HTML). We can obtain partial content to parse
    using `SoupStrainer`, which is very useful when dealing with heavy content.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种特殊情况，我们将使用`lxml`作为解析器创建`soupA`对象，以及`SoupStrainer`对象`tagsA`（仅解析`<a>`，即HTML的元素或锚标签）。我们可以使用`SoupStrainer`获取要解析的部分内容，这在处理大量内容时非常有用。
- en: '`soupA`, an object of Beautiful Soup, presents all of the `<a>` elements found
    for the `SoupStrainer` object `tagsA`as used in the following code; as seen in
    the output, only the `<a>` tag has been collected, or the parsed document is the `SoupStrainer` object parsed
    using `lxml`*:*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`soupA`，Beautiful Soup的一个对象，呈现了`SoupStrainer`对象`tagsA`中找到的所有`<a>`元素，如下面的代码中所使用的；如输出所示，只收集了`<a>`标签，或者解析的文档是使用`lxml`*解析的`SoupStrainer`对象`parsed`*：'
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: HTML content, available from the website, might not always be formatted in a
    clean string. It would be difficult and time-consuming to read page content that
    is presented as paragraphs rather than as a line-by-line code.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 来自网站的HTML内容可能并不总是以干净的字符串格式呈现。阅读以段落而不是逐行代码呈现的页面内容将是困难且耗时的。
- en: 'The Beautiful Soup `prettify()` function returns a Unicode string, presents
    the string in a clean, formatted structure that is easy to read, and identifies
    the elements in a tree structure as seen in the following code; the `prettify()` function
    also accepts the parameter encoding:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Beautiful Soup的`prettify()`函数返回一个Unicode字符串，呈现为干净、格式化的结构，易于阅读，并且以树结构标识元素，如下面的代码所示；`prettify()`函数还接受编码参数：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Document-based elements (such as HTML tags) in a parsed tree can have various
    attributes with predefined values. Element attributes are important resources
    as they provide identification and content together within the element. Verifying
    whether the element contains certain attributes can be handy when traversing through
    the tree.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 解析树中的基于文档的元素（如HTML标签）可以具有具有预定义值的各种属性。元素属性是重要的资源，因为它们在元素内提供了标识和内容。在遍历树时，验证元素是否包含某些属性可能很方便。
- en: 'For example, as seen in the following code, the HTML `<a>` element contains
    the `class`, `href`, and `id` attributes, each carrying predefined values, as
    seen in the following snippet:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如下面的代码所示，HTML`<a>`元素包含`class`、`href`和`id`属性，每个属性都带有预定义的值，如下面的片段所示：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `has_attr()` function from Beautiful Soup returns a Boolean response to
    the searched attribute name for the chosen element, as seen in the following code
    element `a`:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Beautiful Soup的`has_attr()`函数返回所选元素的搜索属性名称的布尔响应，如下所示：
- en: Returns `False` for the `name` attribute
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`name`属性返回`False`
- en: Returns `True` for the `class` attribute
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`class`属性返回`True`
- en: 'We can use the `has_attr()` function to confirm the attribute keys by name,
    if it exists inside the parsed document as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`has_attr()`函数来确认文档中是否存在指定名称的属性键，如下所示：
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With a basic introduction to Beautiful Soup and a few methods explored in this
    section, we will now move forward for searching, traversing, and iterating through
    the parsed tree looking for elements and their content in the upcoming section.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对Beautiful Soup进行基本介绍并在本节中探讨了一些方法，我们现在将继续搜索、遍历和迭代解析树，寻找即将到来的部分中的元素和它们的内容。
- en: Searching, traversing, and iterating
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 搜索、遍历和迭代
- en: Beautiful Soup provides a lot of methods and properties to traverse and search
    elements in the parsed tree. These methods are often named in a similar way to
    their implementation, describing the task they perform. There are also a number
    of properties and methods that can be linked together and used to obtain a similar
    result.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Beautiful Soup提供了许多方法和属性来遍历和搜索解析树中的元素。这些方法通常以与它们执行的任务描述相似的方式命名。还有许多属性和方法可以链接在一起，用于获得类似的结果。
- en: 'The `find()` function returns the first child that is matched for the searched
    criteria or parsed element. It''s pretty useful in scraping context for finding
    elements and extracting details, but only for the single result. Additional parameters
    can also be passed to the `find()` function to identify the exact element, as
    listed:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`find()`函数返回与搜索条件或解析元素匹配的第一个子元素。在爬取上下文中查找元素和提取细节非常有用，但仅适用于单个结果。还可以传递其他参数给`find()`函数，以识别确切的元素，如下所示：'
- en: '`attrs`: A dictionary with a key-value pair'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attrs`：一个带有键值对的字典'
- en: '`text`: With element text'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text`：带有元素文本'
- en: '`name`: HTML tag name'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`：HTML标签名称'
- en: 'Let''s implement the `find()` function with different, allowed parameters in
    the code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在代码中使用不同的允许参数来实现`find()`函数：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here is a list of short descriptions of codes implemented in the preceding
    example:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在前面的示例中实现的代码的简短描述列表：
- en: '`find("a") or find(name="a")`: Search the HTML `<a>` element or tag name provided
    that `a` returns the first existence of `<a>` found in `soupA`'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`find("a")或find(name="a")`：搜索HTML`<a>`元素或提供的标签名称，`a`返回`soupA`中找到的第一个`<a>`的存在'
- en: '`find("a",attrs={''class'':''sister''})`: Search element `<``a>`, with attribute
    key as class and value as sister'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`find("a",attrs={''class'':''sister''})`：搜索带有属性键为`class`和值为`sister`的元素`<a>`'
- en: '`find("a",attrs={''class'':''sister''}, text="Lacie")`: Search the `<a>` element with
    the `class` attribute key and the `sister` value and text with the `Lacie` value'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`find("a",attrs={'class':'sister'}, text="Lacie")`：搜索具有`class`属性键和`sister`值以及文本为`Lacie`值的`<a>`元素
- en: '`find("a",attrs={''id'':''link3''})`: Search the `<a>` element with the `id` attribute
    key and the `link3` value'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`find("a",attrs={''id'':''link3''})`：搜索具有`id`属性键和`link3`值的`<a>`元素'
- en: '`find("a",id="link2")`: Search the `<a>` element for the `id` attribute with
    the `link2` value'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`find("a",id="link2")`：搜索具有`id`属性和`link2`值的`<a>`元素'
- en: 'The `find_all()` function works in a similar way to the `find()` function with
    the additional `attrs` and `text` as a parameters and returns a list of matched
    (multiple) elements for the provided criteria or `name` attribute as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`find_all()`函数的工作方式类似于`find()`函数，还有额外的`attrs`和`text`作为参数，并返回满足条件或`name`属性的多个匹配元素的列表，如下所示：'
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The additional `limit` parameter, which accepts numeric values, controls the total
    count of the elements to be returned using the `find_all()` function.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`limit`参数接受数字值，控制使用`find_all()`函数返回的元素的总数。'
- en: 'The string, list of strings, regular expression objects, or any of these, can
    be provided to the `name` and `text` attributes as a value for `attrs` parameters,
    as seen in the code used in the following snippet:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将字符串、字符串列表、正则表达式对象或这些内容之一提供给`name`和`text`属性作为`attrs`参数的值，如下面代码中所示：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `find_all()` function has in-built support for global attributes such as
    class name along with a name as seen in the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`find_all()`函数内置支持全局属性，例如类名，以及名称，如下所示：'
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Multiple `name` and `attrs` values can also be passed through a list as shown
    in the following syntax:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 多个`name`和`attrs`值也可以通过列表传递，如下面的语法所示：
- en: '`soup.find_all("p",attrs={''class'':["title","story"]})`: Finding all the `<p>`
    elements with the class attribute `title` and `story` values'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`soup.find_all("p",attrs={''class'':["title","story"]})`：查找所有具有`title`和`story`值的类属性的`<p>`元素'
- en: '`soup.find_all(["p","li"])`: Finding all the `<p>` and `<li>` elements from
    the soup object'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`soup.find_all(["p","li"])`：从soup对象中查找所有`<p>`和`<li>`元素'
- en: 'The preceding syntax can be observed in the following code:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在下面的代码中观察到前面的语法：
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can also use element text to search and list the content. A `string` parameter,
    similar to a `text` parameter, is used for such cases; it can also be used with,
    or without, any tag names as in the following code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用元素文本来搜索和列出内容。类似于`text`参数的`string`参数用于这种情况；它也可以与任何标签名称一起使用或不使用，如下面的代码所示：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Iteration through elements can also be achieved using the `find_all()` function.
    As can be seen in the following code, we are retrieving all of the `<li>` elements
    found inside the `<ul>` element and printing their tag name, attribute data, ID,
    and text:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用`find_all()`函数进行元素迭代。如下面的代码所示，我们正在检索`<ul>`元素内找到的所有`<li>`元素，并打印它们的标签名称、属性数据、ID和文本：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The elements `value` attribute can be retrieved using the `get()` function as
    seen in the preceding code. Also, the presence of attributes can be checked using
    the `has_attr()` function.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`get()`函数检索元素的`value`属性。还可以使用`has_attr()`函数检查属性的存在。
- en: 'Element traversing can also be done with just a tag name, and with, or without,
    using the `find()` or `find_all()` functions as seen in the following code:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 元素遍历也可以只使用标签名称，并且可以使用或不使用`find()`或`find_all()`函数，如下面的代码所示：
- en: '[PRE13]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `text` and `string` attributes or the `get_text()` method can be used with
    the elements to extract their text while traversing through the elements used
    in the following code. There''s also a parameter `text` and `string` in the `find()`
    or `find_all()` functions, which are used to search the content as shown in the
    following code:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`text`和`string`属性或`get_text()`方法与元素一起用于提取它们的文本，同时遍历用于搜索内容的元素中也有`text`和`string`参数，如下面的代码所示：
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this section, we explored searching and traversing using elements and by
    implementing important functions such as the `find()` and `find_all()` functions
    alongside their appropriate parameters and criteria.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探索了使用元素进行搜索和遍历，并实现了重要函数，如`find()`和`find_all()`函数以及它们的适当参数和条件。
- en: In the next sections, we will explore elements based on their positions in the
    parsed tree.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将根据解析树中的位置探索元素。
- en: Using children and parents
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用子元素和父元素
- en: 'For parsed documents, traversing through children or child elements can be
    achieved using the `contents`, `children`, and `descendants` elements:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于解析的文档，可以使用`contents`、`children`和`descendants`元素遍历子元素或子元素：
- en: '`contents` collect children for the provided criteria in a list.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`contents`在列表中收集满足条件的子元素。'
- en: '`children` are used for iteration that has direct children.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`children`用于具有直接子元素的迭代。'
- en: '`descendants` work slightly differently to the `contents` and `children`elements.
    It allows iteration over all children, not just the direct ones, that is, the
    element tag and the contents inside the tag are actually two separate children.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`descendants`与`contents`和`children`元素的工作方式略有不同。它允许迭代所有子元素，而不仅仅是直接子元素，也就是说，元素标签和标签内的内容实际上是两个独立的子元素。'
- en: 'The preceding list showed the features that can also be used for iteration.
    The following code illustrates the use of these features with output:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的列表显示了也可以用于迭代的特性。以下代码演示了如何使用这些特性并输出：
- en: '[PRE15]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Selected `children` and `descendants` tag names can be obtained using the `name` attribute.
    Parsed strings and the `\n` function (newline) are returned as `None`, which can
    be filtered out, as in the following code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`name`属性获取所选的`children`和`descendants`标签名称。解析的字符串和`\n`函数（换行符）返回为`None`，可以在下面的代码中进行过滤：
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Similar to the `find()` and `find_all()` functions, we can also traverse child
    elements using the `findChild()` and `findChildren()` functions. The `findChild()` function
    is used to retrieve the single child and the `findChildren()` function retrieves
    a list of children as illustrated in the following code:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 与`find()`和`find_all()`函数类似，我们还可以使用`findChild()`和`findChildren()`函数来遍历子元素。`findChild()`函数用于检索单个子元素，而`findChildren()`函数检索子元素的列表，如下面的代码所示：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Similar to the `children` element, the`parent`element returns the parent object
    found for the searched criteria. The main difference here is that the `parent` element
    returns the single parent object from the tree as seen in the following code:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 与`children`元素类似，`parent`元素返回了搜索条件找到的父对象。这里的主要区别是`parent`元素返回树中的单个父对象，如下面的代码所示：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The limitation of the single parents returned can be overcome by using the `parents` element;
    this returns multiple existing parent elements and matches the searched criteria
    provided in the `find()` function as seen in code here, which is normally used
    for iteration:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`parents`元素可以克服返回单个父元素的限制；这将返回多个现有的父元素，并匹配在`find()`函数中提供的搜索条件，如下面的代码中所示，通常用于迭代：
- en: '[PRE19]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As seen in the preceding output, `[document]` refers to the soup object and
    `html` refers to the complete HTML block found in the soup. The Beautiful Soup
    object that created itself is a parsed element.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的输出所示，`[document]`指的是soup对象，`html`指的是在soup中找到的完整HTML块。Beautiful Soup对象本身创建的是一个解析元素。
- en: Similar to the functions that exist for child traversing, parents can also be
    traversed and retrieved using the `findParent()` and `findParents()` search functions.
    The `findParent()` function traverses to the immediate parent, while the `findParents()` function
    returns all parents found for the criteria provided.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 与用于遍历子元素的函数类似，父元素也可以使用`findParent()`和`findParents()`搜索函数进行遍历和检索。`findParent()`函数遍历到直接父元素，而`findParents()`函数返回为提供的条件找到的所有父元素。
- en: 'It must also be noted that the children and parent traversing functions are
    used with the `find()` function where necessary arguments and conditions are provided,
    as seen in the following code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 还必须注意，子元素和父元素的遍历函数是与`find()`函数一起使用的，其中提供了必要的参数和条件，如下面的代码所示：
- en: '[PRE20]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We explored traversing and searching with the children and parent element using
    a varied handful of functions. In the next section, we'll explore and use positional
    elements from the parsed tree.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了各种函数来探索遍历和搜索子元素和父元素。在下一节中，我们将探索并使用解析树中的位置元素。
- en: Using next and previous
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用next和previous
- en: Similar to traversing through parsed children and parents in the tree, Beautiful
    Soup also has the support to traverse and iterate elements located previous to
    and next to the provided criteria.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 与在树中遍历解析的子元素和父元素类似，Beautiful Soup还支持遍历和迭代位于提供的条件之前和之后的元素。
- en: 'The properties `next` and `next_element` return the immediately parsed content
    for the selected criteria. We can also append the `next` and `next_element` functions
    to create a chain of code for traversal, as seen in the following code:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 属性`next`和`next_element`返回所选条件的立即解析内容。我们还可以将`next`和`next_element`函数附加到一起创建遍历的代码链，如下面的代码所示：
- en: '[PRE21]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Similar to the `next `and `next_elements` functions, there also exist properties
    with traversal result that returns results from prior or previous parsed elements,
    such as the `previous `and `previous_element`, which are opposite to work reversely
    when compared to the `next `and `next_element` functions.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 与`next`和`next_elements`函数类似，还存在着返回先前或之前解析元素结果的遍历结果的属性，例如`previous`和`previous_element`，与`next`和`next_element`函数相比，它们在工作时是相反的。
- en: 'As seen in the following code, the `previous `and `previous_element` can also
    be appended to themselves to create a traversal series:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如下面的代码所示，`previous`和`previous_element`也可以附加到自身以创建一个遍历系列：
- en: '[PRE22]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We now combine the `next `or `next_element` and `previous `or `previous_element` elements
    together to traverse as seen in the following:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将`next`或`next_element`和`previous`或`previous_element`元素组合在一起进行遍历，如下所示：
- en: '[PRE23]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Iterating features for the `next_element` and `previous_element` are obtained
    using the `next_elements` and `previous_elements`, respectively. These iterators
    are used to move to the next or previous parsed content as seen in the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`next_element`和`previous_element`的迭代特性是通过`next_elements`和`previous_elements`获得的。这些迭代器用于移动到下一个或上一个解析内容，如下所示：
- en: '[PRE24]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `find_next()` function implements the `next_elements` but returns only a
    single element that is found after the `next` or `next_element` element. There's
    also an advantage of using the `find_next()` function over the `next_elements` as
    we can implement additional search logic for elements.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`find_next()`函数实现了`next_elements`，但只返回在`next`或`next_element`元素之后找到的单个元素。使用`find_next()`函数的优势在于我们可以为元素实现额外的搜索逻辑。'
- en: 'The following code demonstrates the use of the `find_next()` function, with,
    and without, search conditions; it also displays the outputs from the `next` element
    and `next_elements` to compare the actual usage as shown in the following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码演示了`find_next()`函数的使用，带有和不带有搜索条件；它还显示了`next`元素和`next_elements`的输出，以便比较实际的用法，如下所示：
- en: '[PRE25]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The **`find_all_next()` **function works in a similar way to the `find_next()` function,
    but returns all of the next elements. It''s also used as an iterating version
    of the `find_next()` function. Additional search criteria and arguments such as `limit` can
    be used to search and control the results returned as used in the following code:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`find_all_next()`函数的工作方式与`find_next()`函数类似，但返回所有下一个元素。它也被用作`find_next()`函数的迭代版本。可以使用额外的搜索条件和参数，如`limit`，来搜索和控制返回的结果，如下面的代码所示：'
- en: '[PRE26]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `find_previous()` function implements `previous``_elements` but returns
    only the single element that was found before the `previous` or `previous_element`.
    It also has an advantage over the `previous``_elements` as we can implement additional
    search logic for elements. The following code demonstrates the use of the `find_previous()` function
    and the `previous` function:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`find_previous()`函数实现了`previous_elements`，但只返回在`previous`或`previous_element`之前找到的单个元素。它还比`previous_elements`具有优势，因为我们可以为元素实现额外的搜索逻辑。下面的代码演示了`find_previous()`函数和`previous`函数的用法：'
- en: '[PRE27]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `find_all_previous()` function is an iterated version of the `find_previous()`;
    it returns all previous elements satisfied with the available criteria as seen
    in the following code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`find_all_previous()`函数是`find_previous()`的迭代版本；它返回满足可用条件的所有先前元素，如下面的代码所示：'
- en: '[PRE28]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '`next_sibling` and `previous_sibling` are yet another way of traversing along
    the parsed tree looking for next and previous siblings. A sibling or siblings
    are termed to the element that appears or is found on the same level, in the parsed
    tree or those elements that share the same parent. The following code illustrates
    the use of the `next_sibling` and `previous_sibling` elements:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`next_sibling`和`previous_sibling`是沿着解析树寻找下一个和上一个兄弟姐妹的另一种方式。兄弟姐妹是指出现在相同级别或在解析树中找到的元素，或者共享相同父元素的元素。下面的代码说明了`next_sibling`和`previous_sibling`元素的用法：'
- en: '[PRE29]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Iteration is also possible with siblings, using the `next_siblings` and `previous_siblings` elements
    as shown in the following code:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代也可以使用兄弟姐妹，使用`next_siblings`和`previous_siblings`元素，如下面的代码所示：
- en: '[PRE30]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Similar to the `find_next()` and `find_all_next()` functions for the next elements,
    there's also functions available for siblings, that is,
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于`find_next()`和`find_all_next()`函数用于下一个元素，还有可用于兄弟姐妹的函数，即
- en: 'the `find_next_sibling()` and `find_next_siblings()` functions. These functions
    implement the `next_siblings` function to iterate and search for available siblings.
    As seen in following code, the `find_next_sibling()` function returns a single
    element, whereas the `find_next_siblings()` function returns all matched siblings:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`find_next_sibling()`和`find_next_siblings()`函数。这些函数实现了`next_siblings`函数来迭代和搜索可用的兄弟姐妹。如下面的代码所示，`find_next_sibling()`函数返回单个元素，而`find_next_siblings()`函数返回所有匹配的兄弟姐妹：'
- en: '[PRE31]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The `find_previous_sibling()` and `find_previous_siblings()` functions work
    in a similar way to the `find_next_sibling()` and `find_next_siblings()` functions,
    but result in elements traced through the `previous_siblings` function. Additional
    search criteria and a result-controlling parameter `limit` can also be applied
    to the iterating version, such as the `find_previous_siblings()` function.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`find_previous_sibling()`和`find_previous_siblings()`函数的工作方式与`find_next_sibling()`和`find_next_siblings()`函数类似，但结果是通过`previous_siblings`函数跟踪的元素。还可以应用额外的搜索条件和结果控制参数`limit`到迭代版本，例如`find_previous_siblings()`函数。'
- en: 'As seen in the following code, the `find_previous_sibling()` function returns
    a single sibling element, whereas the `find_previous_siblings()` function returns
    all siblings available previously to the given criteria:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如下面的代码所示，`find_previous_sibling()`函数返回单个兄弟元素，而`find_previous_siblings()`函数返回先前满足给定条件的所有兄弟元素：
- en: '[PRE32]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We have explored various ways of searching and traversing through the parsed
    tree with the functions and properties explored in this section.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探索了在本节中探讨的函数和属性中搜索和遍历解析树的各种方法。
- en: 'The following is a list of tips that can be helpful in remembering and planning
    for search and traversing activities using Beautiful Soup:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些提示列表，可以帮助记住和规划使用Beautiful Soup进行搜索和遍历活动：
- en: 'A function name that starts with the `find` function is used to search and
    iterate for providing criteria and parameters:'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以`find`函数开头的函数名称用于搜索和迭代提供条件和参数：
- en: A plural version of the `find` function works for iteration, such as the `findChildren()` and `findParents()` elements
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`find`函数的复数版本用于迭代，例如`findChildren()`和`findParents()`元素'
- en: A singular version of the `find` function returns a single element such as the `find()`,
    `findChild()`, or `findParent()` functions
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`find`函数的单数版本返回单个元素，例如`find()`、`findChild()`或`findParent()`函数'
- en: A function name that starts with the word `find_all` returns all matched elements and is
    used to search and iterate with provided criteria and parameters such as the `find_all()`,
    `find_all_next()`, and `find_all_previous()` functions
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以`find_all`开头的函数名称返回所有匹配的元素，并用于使用提供的条件和参数进行搜索和迭代，例如`find_all()`、`find_all_next()`和`find_all_previous()`函数
- en: Properties with a plural name are used for iteration purposes such as the `next_elements`,
    `previous_elements`, `parents`, `children`, `contents`, `descendants`, `next_siblings`,
    and `previous_siblings` elements
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有复数名称的属性用于迭代目的，例如`next_elements`、`previous_elements`、`parents`、`children`、`contents`、`descendants`、`next_siblings`和`previous_siblings`元素
- en: Properties with a singular name return single elements and can also be appended
    to form a chain of traversal code such as the `parent`, `next`, `previous`, `next_element`,
    `previous_element`, `next_sibling`, and `previous_sibling` functions
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有单数名称的属性返回单个元素，也可以附加在一起形成遍历代码链，例如`parent`、`next`、`previous`、`next_element`、`previous_element`、`next_sibling`和`previous_sibling`函数
- en: Using CSS Selectors
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CSS选择器
- en: We have used plenty of properties and functions in the preceding sections, looking
    for desired elements and their content. Beautiful Soup also supports CSS Selectors
    (with library SoupSieve at [https://facelessuser.github.io/soupsieve/selectors/](https://facelessuser.github.io/soupsieve/selectors/)),
    which enhances its use and allows developers to write effective and efficient
    codes to traverse the parsed tree.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的部分中使用了大量的属性和函数，寻找所需的元素和它们的内容。Beautiful Soup还支持CSS选择器（使用库SoupSieve在[https://facelessuser.github.io/soupsieve/selectors/](https://facelessuser.github.io/soupsieve/selectors/)），这增强了它的使用，并允许开发人员编写有效和高效的代码来遍历解析树。
- en: CSS Selectors (CSS query or CSS Selector query) are defined patterns used by
    CSS to select HTML elements, by element name or by using global attributes (`ID`,
    `Class`). For more information on CSS Selectors, please refer to [Chapter 3](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml),
    *Using LXML, XPath and CSS Selectors*, *Introduction to XPath and CSS Selector*
    section.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: CSS选择器（CSS查询或CSS选择器查询）是CSS使用的定义模式，用于选择HTML元素，可以按元素名称或使用全局属性（`ID`，`Class`）进行选择。有关CSS选择器的更多信息，请参考[第3章](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml)，*使用LXML、XPath和CSS选择器*，*XPath和CSS选择器简介*部分。
- en: For Beautiful Soup, the `select()` function is used to execute the CSS Selectors.
    We can perform the searching, traversing, and iteration of elements by defining
    CSS Selectors. The `select()` function is implemented individually, that is, it
    is not extended with other functions and properties found in Beautiful Soup, creating
    a chain of codes. The `select()` function returns a list of elements matched to
    the CSS Selectors provided. It's also notable that code using CSS Selectors are
    quite short in length compared to the code used in the preceding sections for
    a similar purpose.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Beautiful Soup，`select()`函数用于执行CSS选择器。我们可以通过定义CSS选择器来执行元素的搜索、遍历和迭代。`select()`函数是独立实现的，即它没有与Beautiful
    Soup中找到的其他函数和属性扩展，从而创建了一系列代码。`select()`函数返回与提供的CSS选择器匹配的元素列表。此外，使用CSS选择器的代码长度相对于前面部分用于类似目的的代码来说也是相当短的。
- en: We will explore a few examples using `select()` to process CSS Selectors.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`select()`来处理CSS选择器的几个示例。
- en: Example 1 – listing <li> elements with the data-id attribute
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例1 - 列出具有data-id属性的<li>元素
- en: 'In the following example, we will use the `select()` function to list the `<li>` element
    with the `data-id` attribute:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们将使用`select()`函数列出具有`data-id`属性的`<li>`元素：
- en: '[PRE33]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As seen in the preceding code, the `li[data-id]` selector queries the `<li>` element with
    the attribute key named as `data-id`. The Value for `data-id` is empty, which
    allows traversing through all `<li>` possessing `data-id`. The result is obtained
    as a list of objects, in which indexes can be applied to fetch the exact elements
    as seen in the following code:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，`li[data-id]`选择器查询具有名为`data-id`的属性键的`<li>`元素。`data-id`的值为空，这允许遍历所有具有`data-id`的`<li>`。结果以对象列表的形式获得，可以应用索引来获取确切的元素，如下面的代码所示：
- en: '[PRE34]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If we wish to extract the first match that has resulted the from CSS query,
    we can use either the list index, that is, `0` (zero) or the `select_one()` function in
    place of the `select()` function as seen in the following code. The `select_one()` function
    returns the string of objects, not the list:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望提取CSS查询结果中的第一个匹配项，我们可以使用列表索引，即`0`（零），或者在以下代码中看到的`select()`函数的位置上使用`select_one()`函数。`select_one()`函数返回对象的字符串，而不是列表：
- en: '[PRE35]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Example 2 – traversing through elements
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例2 - 遍历元素
- en: 'CSS Selectors have various combinators such as +, >, a space character, and
    so on, which show relationships between the elements. A few such combinators are
    used in the following example code:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: CSS选择器有各种组合符号，如+，>，空格字符等，显示元素之间的关系。在以下示例代码中使用了一些这样的组合符号：
- en: '[PRE36]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Example 3 – searching elements based on attribute values
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例3 - 根据属性值搜索元素
- en: 'There are various ways of finding elements in Beautiful Soup, such as using
    functions starting with the word `find` or using attributes in CSS Selectors.
    Patterns can be searched for attributes keys using `*` in CSS Selectors as illustrated
    in the following code:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在Beautiful Soup中有各种查找元素的方法，比如使用以`find`开头的函数或在CSS选择器中使用属性。可以使用CSS选择器中的`*`来搜索属性键，如下面的代码所示：
- en: '[PRE37]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We were searching for the `<a>` element with the text `example.com`, which might
    exist in the value of the `href` attribute. Also, we were searching for the `<a>` element,
    which contains an attribute ID with a text link.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在搜索具有文本`example.com`的`<a>`元素，该文本可能存在于`href`属性的值中。此外，我们正在搜索包含带有文本链接的属性ID的`<a>`元素。
- en: With basic knowledge of CSS Selectors, we can deploy it with Beautiful Soup
    for various purposes. Using the `select()` function is quite effective when dealing
    with elements, but there are also limitations we might face, such as extracting
    text or content from the obtained element.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 有了对CSS选择器的基本了解，我们可以在Beautiful Soup中使用它来实现各种目的。当处理元素时，使用`select()`函数非常有效，但我们可能会遇到一些限制，比如从获取的元素中提取文本或内容。
- en: We have introduced and explored the elements of Beautiful Soup in the preceding
    sections. To wrap up the concept, we will create a crawler example in the upcoming
    section.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的部分介绍和探讨了Beautiful Soup的元素。为了总结这个概念，我们将在接下来的部分创建一个爬虫示例。
- en: Building a web crawler
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建网络爬虫
- en: In this section, we will build a web crawler to demonstrate the real content-based
    scraping, targeting web content.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建一个网络爬虫，以演示基于实际内容的爬取，目标是网页内容。
- en: We will be scraping quotes from [http://toscrape.com/](http://toscrape.com/) and
    targeting quotes from authors found at [http://quotes.toscrape.com/](http://quotes.toscrape.com/).
    The crawler will collect the quote and author information from the first five
    listing pages and write the data into a CSV file. We will also explore the individual
    author page and extract information about the authors.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从[http://toscrape.com/](http://toscrape.com/)上爬取名言，并从[http://quotes.toscrape.com/](http://quotes.toscrape.com/)上找到作者的名言。爬虫将从前五个列表页面收集名言和作者信息，并将数据写入CSV文件。我们还将探索单个作者页面，并提取有关作者的信息。
- en: 'To begin with the basic planning and identification of the fields that we are
    willing to collect information from, please refer to [Chapter 3](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml),
    *Using LXML, XPath, and CSS Selectors*, *Using web browser developer tools for
    accessing web content* section:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，基本规划和识别我们愿意从中收集信息的字段，请参考[第3章](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml)，*使用LXML、XPath和CSS选择器*，*使用Web浏览器开发者工具访问Web内容*部分：
- en: '[PRE38]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'In the preceding code there are a few libraries and objects found as listed
    and described here:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，有几个库和对象列在此处并在此处描述：
- en: '`sourceUrl`: Represents the URL of the main page to be scraped for data for
    category web scraping'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sourceUrl`：表示要为类别网页抓取的数据而抓取的主页面的URL'
- en: '`keys`: The Python list contains the columns name that will be used while writing
    records to an external file'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keys`：Python列表包含在向外部文件写入记录时将使用的列名'
- en: '`requests`: This library is imported to use for making an HTTP request to page
    URLs with quote listings and receiving a response'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`requests`：导入此库以使用在带引用列表的页面URL上发出HTTP请求并接收响应'
- en: '`csv`: This library will be used to write scraped data to an external CSV file'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`csv`：此库将用于将抓取的数据写入外部CSV文件'
- en: '`bs4`: Library for implementing and using Beautiful Soup'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bs4`：用于实现和使用Beautiful Soup的库'
- en: The first line in a CSV file contains column names. We need to write these columns
    before appending records with real content in the CSV file.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: CSV文件的第一行包含列名。我们需要在向CSV文件中附加实际内容的记录之前写入这些列。
- en: 'The `read_url()` function, as found in the following code, will be used to
    make a request and receive a response using the `requests` function. This function
    will accept a `url` argument for pages:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_url()` 函数，如下面的代码中所示，将用于使用`requests`函数发出请求并接收响应。此函数将接受一个`url`参数用于页面：'
- en: '[PRE39]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '`dataSet `is a handle defined to manage the external file `quotes.csv`. `csv.writer()` file
    handle is use for accessing CSV-based properties. The `writerow()` function is
    passed with keys for writing a row containing the column names from the list keys to
    the external file as shown in the following:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`dataSet`是一个句柄，用于管理外部文件`quotes.csv`。`csv.writer()`文件句柄用于访问基于CSV的属性。`writerow()`函数传递了键，用于将包含列表键中的列名的行写入到外部文件中，如下所示：'
- en: '[PRE40]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The implemented `get_details()` function is being coded for pagination and
    scraping logic. The `read_url()` function is supplied with a dynamically generated
    page URL to manage the pagination as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 正在实现的`get_details()`函数正在编写用于分页和抓取逻辑。`read_url()`函数将提供动态生成的页面URL以管理分页，如下所示：
- en: '[PRE41]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'As used in the following code, the `response` element from the `read_url()` function is
    parsed using `lxml` to obtain the `soup` element. The rows obtained using the
    soup list all of the quotes available in a single page (that is, the element block
    containing the single quote details) found inside the `<div class="quote">` function
    and will be iterated to scrape data for individual items such as `quote_tags`,
    `author_url`, and `author_name` traversing through the quote element:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如下面的代码中所示，使用`lxml`解析`read_url()`函数中的`response`元素以获取`soup`元素。使用soup获取的行列出了单页中所有的引用（即包含单个引用详细信息的元素块）在`<div
    class="quote">`函数中找到，并将被迭代以抓取`quote_tags`、`author_url`和`author_name`等个别项目的数据：
- en: '![](assets/6768697a-286f-43c5-87e0-e4b42d3f5c83.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/6768697a-286f-43c5-87e0-e4b42d3f5c83.png)'
- en: Page source with quote element
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 带引用元素的页面源代码
- en: The individual items received are scraped, cleaned, and collected in a list
    maintaining the order of their column names and are written to the file using
    the `writerow()` function (appends the list of values to the file) accessed through
    the `csv` library and file handle.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 接收到的各个项目将被抓取、清理并收集到一个列表中，保持其列名的顺序，并使用`csv`库和文件句柄访问的`writerow()`函数将其写入文件（将值列表附加到文件）。
- en: 'The `quotes.csv` data file will contain scraped data as seen in the following
    screenshot:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`quotes.csv`数据文件将包含如下截图中所见的抓取数据：'
- en: '![](assets/211306fb-7fd4-4914-b588-d35f0694b630.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/211306fb-7fd4-4914-b588-d35f0694b630.png)'
- en: Rows with scraped data from http://quotes.toscrape.com/
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 从http://quotes.toscrape.com/抓取的数据行
- en: In this section, we explored various ways to traverse and search using Beautiful
    Soup. In the upcoming section, we will be using Scrapy, a web crawling framework.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了使用Beautiful Soup进行遍历和搜索的各种方法。在接下来的部分中，我们将使用Scrapy，一个网络爬虫框架。
- en: Web scraping using Scrapy
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scrapy进行网页抓取
- en: We have used and explored various libraries and techniques for web scraping
    so far in this book. The latest libraries available adapt to new concepts and
    implement the techniques in a more effective, diverse, and easy way; Scrapy is
    among one of those libraries.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在本书中已经使用和探索了各种库和技术进行网页抓取。最新的库可以适应新的概念，并以更有效、多样和简单的方式实现这些技术；Scrapy就是其中之一。
- en: We will be introducing and using Scrapy (an open source web crawling framework
    written in Python) in this section. For more detailed information on Scrapy, please
    visit the official documentation at [http://docs.scrapy.org/en/latest/](http://docs.scrapy.org/en/latest/).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍并使用Scrapy（一个用Python编写的开源网络爬虫框架）。有关Scrapy的更详细信息，请访问官方文档[http://docs.scrapy.org/en/latest/](http://docs.scrapy.org/en/latest/)。
- en: In this section, we will be implementing scraping features and building a project
    demonstrating useful concepts.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现抓取功能并构建一个演示有用概念的项目。
- en: Introduction to Scrapy
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scrapy简介
- en: Scrapy is a web crawling framework written in Python used for crawling websites
    with effective and minimal coding. According to the official website of Scrapy
    ([https://scrapy.org/](https://scrapy.org/)), it is <q>"An open source and collaborative
    framework for extracting the data you need from websites. In a fast, simple, yet
    extensible way."</q>
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy是一个用Python编写的网络爬虫框架，用于以有效和最小的编码方式爬取网站。根据Scrapy的官方网站（[https://scrapy.org/](https://scrapy.org/)）的说法，它是<q>“一个用于从网站中提取所需数据的开源和协作框架。以一种快速、简单但可扩展的方式。”</q>
- en: Scrapy provides a complete framework that is required to deploy a crawler with
    built-in tools. Scrapy was originally designed for web scraping; with its popularity
    and development, it is also used to extract data from APIs. Scrapy-based web crawlers
    are also easy to manage and maintain because of their structure. In general, Scrapy provides
    a project-based scope for projects dealing with web scraping.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy提供了一个完整的框架，用于部署具有内置工具的爬虫。Scrapy最初是为网页抓取而设计的；随着其流行和发展，它也用于从API中提取数据。基于Scrapy的网络爬虫也易于管理和维护，因为其结构。总的来说，Scrapy为处理网页抓取的项目提供了基于项目的范围。
- en: 'The following are some of the features and distinguishable points that make
    Scrapy a favorite among developers:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些使Scrapy成为开发人员喜爱的功能和显著点：
- en: Scrapy provides built-in support for document parsing, traversing, and extracting
    data using XPath, CSS Selectors, and regular expressions.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scrapy提供了内置支持，用于使用XPath、CSS选择器和正则表达式解析、遍历和提取数据。
- en: The crawler is scheduled and managed asynchronously allowing multiple links
    to be crawled at the same time.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爬虫被安排和异步管理，允许同时爬取多个链接。
- en: It automates HTTP methods and actions, that is, there's no need for importing
    libraries such as `requests` or `urllib` manually for code. Scrapy handles requests
    and responses using its built-in libraries.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它自动化了HTTP方法和操作，也就是说，不需要手动导入诸如`requests`或`urllib`之类的库来编写代码。Scrapy使用其内置库处理请求和响应。
- en: There's built-in support for feed export, pipelines (items, files, images, and
    media), that is, exporting, downloading, and storing data in JSON, CSV, XML, and
    database.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有内置支持的feed导出、管道（项目、文件、图像和媒体），即以JSON、CSV、XML和数据库导出、下载和存储数据。
- en: The availability of the middleware and the large collection of built-in extensions
    can handle cookies, sessions, authentication, `robots.txt`, logs, usage statistics,
    email handling, and so on.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中间件的可用性和大量内置扩展可以处理cookie、会话、身份验证、`robots.txt`、日志、使用统计、电子邮件处理等。
- en: Scrapy-driven projects are composed of easy-to-use distinguishable components
    and files, which can be handled with basic Python skills and many more.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scrapy驱动的项目由易于识别的组件和文件组成，可以用基本的Python技能处理，还有更多。
- en: Please refer to the official documentation of Scrapy at [https://docs.scrapy.org/en/latest/intro/overview.html](https://docs.scrapy.org/en/latest/intro/overview.html) for
    an in-depth and detailed overview.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅Scrapy的官方文档[https://docs.scrapy.org/en/latest/intro/overview.html](https://docs.scrapy.org/en/latest/intro/overview.html)进行深入和详细的概述。
- en: With a basic introduction to Scrapy, we now begin setting up a project and exploring
    the framework in more detail in the next sections.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对Scrapy的基本介绍，我们现在开始在接下来的章节中设置项目并更详细地探索框架。
- en: Setting up a project
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置项目
- en: We will require a Python library with `scrapy` successfully installed on the
    system before proceeding with the project setup. For setting up or installation
    refer to [Chapter 2](b9919ebf-2d5c-4721-aa76-5c1378262473.xhtml), *Python and
    the Web – Using urllib and Requests,* *Setting things up* section or, for more
    details on Scrapy installation, please refer to the official installation guide
    at [https://docs.scrapy.org/en/latest/intro/overview.html](https://docs.scrapy.org/en/latest/intro/overview.html).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行项目设置之前，我们需要在系统上成功安装了`scrapy`的Python库。有关设置或安装，请参阅[第2章](b9919ebf-2d5c-4721-aa76-5c1378262473.xhtml)，*Python和Web-使用urllib和Requests*，*设置事项*部分，或者有关Scrapy安装的更多详细信息，请参阅官方安装指南[https://docs.scrapy.org/en/latest/intro/overview.html](https://docs.scrapy.org/en/latest/intro/overview.html)。
- en: 'Upon successful installation, we can obtain the details shown in the following
    screenshot, using Python IDE:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 安装成功后，我们可以使用Python IDE获得以下截图中显示的细节：
- en: '![](assets/37562e7b-756c-4844-b365-4768f849c1d7.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/37562e7b-756c-4844-b365-4768f849c1d7.png)'
- en: Successful installation of Scrapy with details
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 成功安装Scrapy并显示细节
- en: With the successful installation of the `scrapy` library, there's also the availability
    of the `scrapy` command-line tool. This command-line tool contains a number of
    commands, which are used at various stages of a project from starting or creating
    a project through to it being fully up and running.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 通过成功安装`scrapy`库，还可以使用`scrapy`命令行工具。这个命令行工具包含一些命令，在项目的各个阶段使用，从创建项目到完全运行。
- en: 'To begin with creating a project, let''s follow the steps:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始创建一个项目，让我们按照以下步骤进行：
- en: Open Terminal or command-line interface
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开终端或命令行界面
- en: Create a folder (`ScrapyProjects`) as shown in the following screenshot or select
    a folder in which to place Scrapy projects
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个文件夹（`ScrapyProjects`），如下截图所示，或选择一个放置Scrapy项目的文件夹
- en: Inside the selected folder, run or execute the `scrapy` command
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在选择的文件夹中，运行或执行`scrapy`命令
- en: 'A list of available commands and their brief details will appear, similar to
    the following screenshot:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将出现一个可用命令及其简要详情的列表，类似于以下截图：
- en: '![](assets/e8b9f1ab-2fb1-4bb5-9916-79eb18f96788.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/e8b9f1ab-2fb1-4bb5-9916-79eb18f96788.png)'
- en: List of available commands for Scrapy
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy的可用命令列表
- en: We will be creating a `Quotes` project to obtain author quotes related to web
    scraping from [http://toscrape.com/](http://toscrape.com/), accessing information
    from the first five pages or less which exists using the URL [http://quotes.toscrape.com/](http://quotes.toscrape.com/).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个“Quotes”项目，从[http://toscrape.com/](http://toscrape.com/)获取与网页抓取相关的作者引用，访问存在的前五页或更少的信息，使用URL
    [http://quotes.toscrape.com/](http://quotes.toscrape.com/)。
- en: '[We are now going to start the `Quotes` project. From the Command Prompt, run
    or execute the `scrapy startproject Quotes` command as seen in the following screenshot:](http://quotes.toscrape.com/)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[我们现在将开始“Quotes”项目。从命令提示符中运行或执行“scrapy startproject Quotes”命令，如下截图所示：](http://quotes.toscrape.com/)'
- en: '![](assets/20cbfc7a-2f6c-4d59-bf1b-b0d6ac50c8a5.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/20cbfc7a-2f6c-4d59-bf1b-b0d6ac50c8a5.png)'
- en: Starting a project (using command: scrapy startproject Quotes)
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 开始一个项目（使用命令：`scrapy startproject Quotes`）
- en: 'If successful, the preceding command will be the creation of a new folder named
    `Quotes` (that is, the project root directory) with additional files and subfolders
    as shown in the following screenshot:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如果成功，上述命令将创建一个名为`Quotes`的新文件夹（即项目根目录），并包含如下截图所示的其他文件和子文件夹：
- en: '![](assets/2a93ab9e-a72e-4396-8970-1077983632c8.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/2a93ab9e-a72e-4396-8970-1077983632c8.png)'
- en: Contents for project folder ScrapyProjects\Quotes
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 项目文件夹ScrapyProjects\Quotes的内容
- en: 'With the project successfully created, let''s explore the individual components
    inside the project folder:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 项目成功创建后，让我们来探索项目文件夹中的各个组件：
- en: '`scrapy.cfg` is a configuration file in which default project-related settings
    for deployment are found and can be added.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scrapy.cfg`是一个配置文件，其中包含部署的默认项目相关设置，可以进行添加。'
- en: 'Subfolder will find `Quotes` named same as project directory, which is actually
    a Python module. We will find additional Python files and other resources in this
    module as follows:'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子文件夹将找到与项目目录同名的`Quotes`，实际上是一个Python模块。我们将在这个模块中找到其他的Python文件和其他资源。
- en: '![](assets/b0dcc05f-a4e5-43b9-a55c-50a790da7441.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/b0dcc05f-a4e5-43b9-a55c-50a790da7441.png)'
- en: Contents for project folder ScrapyProjects\Quotes\Quotes
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 项目文件夹ScrapyProjects\Quotes\Quotes的内容
- en: 'As seen in the preceding screenshot, the module is contained in the `spiders` folder and
    the `items.py`, `pipelines.py`, and `settings.py` Python files. These content
    found inside the `Quotes` module has specific implementation regarding the project
    scope explored in the following list:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的截图所示，模块包含在`spiders`文件夹和`items.py`、`pipelines.py`和`settings.py` Python文件中。`Quotes`模块中的内容在以下列表中具有特定的实现：
- en: '`spiders`: This folder will contain Spider classes or Spider writing in Python.
    Spiders are classes that contain code that is used for scraping. Each individual
    Spider class is designated to specific scraping activities.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spiders`：这个文件夹将包含用Python编写的蜘蛛类或蜘蛛。蜘蛛是包含用于抓取的代码的类。每个单独的蜘蛛类都指定了特定的抓取活动。'
- en: '`items.py`: This Python file contains item containers, that is, Python class
    files inheriting `scrapy. Items` are used to collect the scraped data and use
    it inside spiders. Items are generally declared to carry values and receive built-in
    support from other resources in the main project. An item is like a Python dictionary
    object, where keys are fields or objects of `scrapy.item.Field`, which will hold
    certain values.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`items.py`：这个Python文件包含项目容器，即继承`scrapy. Items`的Python类文件，用于收集抓取的数据并在蜘蛛中使用。项目通常被声明为携带值，并从主项目中的其他资源获得内置支持。项目就像一个Python字典对象，其中键是`scrapy.item.Field`的字段或对象，将保存特定的值。'
- en: Although the default project creates the `items.py` for the item-related task,
    it's not compulsory to use it inside the spider. We can use any lists or collect
    data values and process them in our own way such as writing them into a file,
    appending them to a list, and so on.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管默认项目为项目相关任务创建了`items.py`，但在蜘蛛中使用它并不是强制的。我们可以使用任何列表或收集数据值，并以我们自己的方式处理，比如将它们写入文件，将它们附加到列表等。
- en: '`pipelines.py`: This part is executed after the data is scraped. The scraped
    items are sent to the pipeline to perform certain actions. It also decides whether
    to process the received scraped items or drop them.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pipelines.py`：这部分在数据被抓取后执行。抓取的项目被发送到管道执行某些操作。它还决定是否处理接收到的抓取项目或丢弃它们。'
- en: '`settings.py`: This is the most important file in which settings for the project
    can be adjusted. According to the preference of the project, we can adjust the
    settings. Please refer to the official documentation from Scrapy for settings
    at [https://scrapy2.readthedocs.io/en/latest/topics/settings.html](https://scrapy2.readthedocs.io/en/latest/topics/settings.html)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`settings.py`：这是最重要的文件，可以在其中调整项目的设置。根据项目的偏好，我们可以调整设置。请参考Scrapy的官方文档[https://scrapy2.readthedocs.io/en/latest/topics/settings.html](https://scrapy2.readthedocs.io/en/latest/topics/settings.html)'
- en: In this section, we have successfully created a project and the required files
    using Scrapy. These files will be used and updated as described in the following
    sections.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已成功使用Scrapy创建了一个项目和所需的文件。这些文件将如下节所述被使用和更新。
- en: Generating a Spider
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成一个蜘蛛
- en: We need to generate a Spider to collect the data. The Spider will perform the
    crawling activity. An empty default folder named `spiders` does exist inside the `ScrapyProjects\Quotes\Quotes` folder.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要生成一个蜘蛛来收集数据。蜘蛛将执行爬行活动。在`ScrapyProjects\Quotes\Quotes`文件夹中存在一个名为`spiders`的空默认文件夹。
- en: From the `ScrapyProjects\Quotes` project folder, run or execute the `scrapy
    genspider quotes quotes.toscrape.com` command.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 从`ScrapyProjects\Quotes`项目文件夹中运行或执行`scrapy genspider quotes quotes.toscrape.com`命令。
- en: 'Successful execution of the command will create a `quotes.py` file, that is,
    a Spider inside the `ScrapyProjects\Quotes\Quotes\spiders\` path. The generated
    Spider class `QuotesSpider` inherits Scrapy features from `scrapy.Spider`. There''s
    also a few required properties and functions found inside `QuotesSpider` as seen
    in the following code:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 成功执行该命令将在`ScrapyProjects\Quotes\Quotes\spiders\`路径下创建一个`quotes.py`文件，即一个蜘蛛。生成的`QuotesSpider`类继承自`scrapy.Spider`，在`QuotesSpider`中还有一些必需的属性和函数，如下代码所示：
- en: '[PRE42]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The `QuotesSpider` Spider class contains automatically generated properties
    that are assigned for specific tasks, as explored in the following list:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '`QuotesSpider`蜘蛛类包含自动生成的属性，用于特定任务，如下列表所示：'
- en: '`name`: This variable holds value, that is, the name of the Spider quotes as
    seen in the preceding code. The name identifies the Spider and can be used to
    access it. The value of the name is provided through the command-line instructions
    while issuing `scrapy genspider quotes`, which is the first parameter after `genspider`.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`：这个变量保存值，即蜘蛛quotes的名称，如前面的代码所示。名称标识了蜘蛛，并可以用于访问它。名称的值是通过命令行指令提供的，比如在`genspider`之后的第一个参数`scrapy
    genspider quotes`。'
- en: '`allowed_domains`: The created Spiders are allowed to crawl within the listed
    domains found in the `allowed_domains`. The last parameter passed is the `quotes.toscrape.com` parameter,
    while generating a Spider is actually a domain name that will be listed inside
    an `allowed_domains` list.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`allowed_domains`：创建的Spider允许在`allowed_domains`中列出的域内爬行。传递的最后一个参数是`quotes.toscrape.com`参数，生成Spider实际上是一个将列在`allowed_domains`列表中的域名。'
- en: A domain name passed to `allowed_domains` will generate URLs for `start_urls`.
    If there are any chances of URL redirection, such URL domain names need to be
    mentioned inside the `allowed_domains`.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传递给`allowed_domains`的域名将为`start_urls`生成URL。如果存在URL重定向的可能性，则需要在`allowed_domains`中提及这些URL域名。
- en: '`start_urls`: These contain a list of URLs that are actually processed by Spider
    to crawl. The domain names found or provided to the `allowed_domains` are automatically
    added to this list and can be manually added or updated. Scrapy generates the
    URLs for `start_urls` adding HTTP protocols. On a few occasions, we might also
    need to change or fix the URLs manually, for example, `www` added to the domain
    name needs to be removed. `start_urls` after the update will be seen as in the
    following code:'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_urls`：这些包含Spider实际处理的URL列表。找到或提供给`allowed_domains`的域名将自动添加到此列表中，并且可以手动添加或更新。Scrapy生成`start_urls`的URL添加了HTTP协议。在某些情况下，我们可能还需要手动更改或修复URL，例如，需要删除添加到域名的`www`。更新后的`start_urls`将如下代码所示：'
- en: '[PRE43]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '`parse()`: This function is implemented with the logic relevant to data extraction
    or processing. `parse()` acts as a main controller and starting point for scraping
    activity. Spiders created for the main project will begin processing the provided
    URLs or `start_urls` from, or inside, the `parse()`. XPath-and CSS Selector-related
    expressions and codes are implemented, and extracted values are also added to
    the item (that is, the `QuotesItem` from the `item.py` file).'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parse()`：此函数实现了与数据提取或处理相关的逻辑。`parse()`充当了抓取活动的主控制器和起点。为主项目创建的Spider将开始处理提供的URL或`start_urls`，或者在`parse()`内部。实现了与XPath和CSS选择器相关的表达式和代码，并且提取的值也被添加到item（即来自`item.py`文件的`QuotesItem`）。'
- en: 'We can also verify the successful creation of Spider by executing these commands:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过执行以下命令来验证Spider的成功创建：
- en: '`scrapy list`'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scrapy list`'
- en: '`scrapy list spide*r*`'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scrapy list spide*r*`'
- en: 'Both of these commands will list the Spider displaying its name, which is found
    inside the `spiders` folder as seen in the following screenshot:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个命令都将列出Spider的名称，该名称在`spiders`文件夹中找到，如下面的屏幕截图所示：
- en: '![](assets/af5972eb-6066-4dc7-9fb7-28298299376f.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/af5972eb-6066-4dc7-9fb7-28298299376f.png)'
- en: Listing Spiders from Command Prompt
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 从命令提示符中列出Spider
- en: In this section, we have generated a Spider named `quotes` for our scraping
    task. In the upcoming section, we will create Item fields that will work with
    Spider and help with collecting data.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们为我们的抓取任务生成了一个名为`quotes`的Spider。在接下来的部分中，我们将创建与Spider一起工作并帮助收集数据的Item字段。
- en: Creating an item
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个item
- en: Proceeding with the scraping task and the project folder, we will find a file
    named `item.py` or item, containing the Python class `QuotesItem`. The item is
    also automatically generated by Scrapy while issuing the `scrapy startproject
    Quotes` command. The `QuotesItem` class inherits the `scrapy.Item` for built-in
    properties and methods such as the `Field`. The `Item` or `QuotesItem` in Scrapy
    represents a container for collecting values and the `Fields` listed as shown
    in the following code, including quotes, tags, and so on, which will acts as the
    keys to the values which we will obtain using the `parse()` function. Values for
    the same fields will be extracted and collected across the found pages.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 继续进行抓取任务和项目文件夹，我们将找到一个名为`item.py`或item的文件，其中包含Python类`QuotesItem`。该item也是由Scrapy在发出`scrapy
    startproject Quotes`命令时自动生成的。`QuotesItem`类继承了`scrapy.Item`，具有内置属性和方法，如`Field`。在Scrapy中，`Item`或`QuotesItem`代表了一个用于收集值的容器，如下面的代码所示，包括引用、标签等，这些将作为我们使用`parse()`函数获取的值的键。相同字段的值将在找到的页面上被提取和收集。
- en: 'The item is accessed as a Python dictionary with the provided fields as keys
    with their values extracted. It''s effective to declare the fields in the item
    and use them in Spider but is not compulsory to use `item.py` as shown in the
    following example:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: item被视为Python字典，提供的字段作为键，其提取的值作为值。在Spider中声明字段并在Spider中使用它们是有效的，但不是强制使用`item.py`，如下面的示例所示：
- en: '[PRE44]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We need to import the `QuotesItem` when the item is required inside the Spider,
    as seen in the following code, and process it by creating an object and accessing
    the declared fields, that is, `quote`, `tags`, `author`, and so on:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 当Spider内部需要item时，我们需要导入`QuotesItem`，如下面的代码所示，并通过创建对象并访问声明的字段，即`quote`、`tags`、`author`等来处理它：
- en: '[PRE45]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: In this section, we declared the `item` fields that we are willing to retrieve
    data from a website. In the upcoming section, we will explore different methods
    of data extraction and link them to the item fields.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们声明了我们愿意从网站中检索数据的`item`字段。在接下来的部分中，我们将探索不同的数据提取方法，并将它们与项目字段相关联。
- en: Extracting data
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取数据
- en: With Spider generated and the item declared with the fields required, we will
    now proceed to extract the values or data required for specific item fields. Extraction-related
    logic can be applied using XPath, CSS Selectors, and regular expressions and we
    also can implement Python-related libraries such as `bs4` (Beautiful Soup), `pyquery`,
    and so on.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 有了生成的Spider和声明所需字段的item，我们现在将继续提取特定项目字段所需的值或数据。可以使用XPath、CSS选择器和正则表达式应用与提取相关的逻辑，我们还可以实现Python相关的库，如`bs4`（Beautiful
    Soup）、`pyquery`等。
- en: With proper `start_urls` and item (`QuotesItem`) being set up for the Spider
    to crawl, we can now proceed with the extraction logic using `parse()` and using selectors
    at [https://docs.scrapy.org/en/latest/topics/selectors.html](https://docs.scrapy.org/en/latest/topics/selectors.html).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 通过为Spider设置适当的`start_urls`和项目（`QuotesItem`）来进行爬取，我们现在可以使用`parse()`和在[https://docs.scrapy.org/en/latest/topics/selectors.html](https://docs.scrapy.org/en/latest/topics/selectors.html)中使用选择器进行提取逻辑。
- en: Using XPath
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用XPath
- en: The `parse()` function inside Spider is the place to implement all logical processes
    for scraping data. As seen in the following code, we are using XPath expressions
    in this Spider to extract the values for the required fields in `QuotesItem`.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: Spider内的`parse()`函数是实现所有抓取数据的逻辑过程的地方。如下所示，我们在此Spider中使用XPath表达式来提取`QuotesItem`中所需字段的值。
- en: For more information on XPath and obtaining XPath Query, using browser-based
    developer tools, please refer to [Chapter 3](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml),
    *Using LXML, XPath and CSS Selectors*, *XPath and CSS Selectors using DevTools*
    section. Similarly, for more information on the `pyquery` Python library, please
    refer to [Chapter 4](30c30342-63a5-4452-9f61-a05a2e69e256.xhtml), *Scraping Using
    pyquery – a Python Library.*
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 有关XPath和使用基于浏览器的开发工具获取XPath查询的更多信息，请参阅[第3章](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml)，*使用LXML、XPath和CSS选择器*，*使用DevTools的XPath和CSS选择器*部分。同样，有关`pyquery`
    Python库的更多信息，请参阅[第4章](30c30342-63a5-4452-9f61-a05a2e69e256.xhtml)，*使用pyquery -
    一个Python库进行抓取*。
- en: As seen in the next code snippet an `item` object from `QuotesItem` is used
    to collect individual field-related data and it's finally being collected and
    iterated using the Python keyword `yield`. `parse()` is actually a generator that
    is returning object `item` from `QuotesItem`.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 如下一段代码片段所示，从`QuotesItem`中使用`item`对象收集单个字段相关数据，并最终使用Python关键字`yield`进行收集和迭代。`parse()`实际上是一个返回`QuotesItem`中的`item`对象的生成器。
- en: Python keyword `yield` is used to return a generator. Generators are functions
    that return an object that can be iterated. The Python function can be treated
    as a generator using the yield in place of the return.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: Python关键字`yield`用于返回一个生成器。生成器是返回可迭代对象的函数。Python函数可以使用`yield`代替`return`来作为生成器处理。
- en: '`parse()` has an additional argument `response`; this is an object of `scrapy.http.response.html.HtmlResponse` that
    is returned by Scrapy with the page content of the accessed or crawled URL. The
    response obtained can be used with XPath and CSS Selectors for further scraping
    activities:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`parse()`有一个额外的参数`response`；这是Scrapy返回的一个`scrapy.http.response.html.HtmlResponse`对象，其中包含所访问或爬取的URL的页面内容。获取的响应可以与XPath和CSS选择器一起用于进一步的抓取活动：'
- en: '[PRE46]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: As seen in the following code, the XPath expression is being applied to the
    response using the `xpath()` expression and is used as a `response.xpath()`. XPath
    expressions or queries provided to `response.xpath()` are parsed as rows, that
    is, an element block containing the desired elements for fields.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，XPath表达式被应用于响应，使用`xpath()`表达式并用作`response.xpath()`。提供给`response.xpath()`的XPath表达式或查询被解析为行，即包含所需字段的元素块。
- en: 'The obtained rows will be iterated for extracting individual element values
    by providing the XPath query and using additional functions as listed here:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 获取的行将通过提供XPath查询并使用此处列出的其他函数进行迭代，以提取单个元素值：
- en: '`extract()`: Extract all the elements matching the provided expression.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extract()`: 提取与提供的表达式匹配的所有元素。'
- en: '`extract_first()`: Extract only the first element that matches the provided
    expression.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extract_first()`: 仅提取与提供的表达式匹配的第一个元素。'
- en: '`strip()`: Clears the whitespace characters from the beginning and the end
    of the string. We need to be careful using this function to the extracted content
    if they result in a type other than string such as `NoneType` or `List`, and so
    on as it can result in an error.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strip()`：清除字符串开头和结尾的空白字符。我们需要小心使用此函数来处理提取的内容，如果结果不是字符串类型，例如`NoneType`或`List`等，可能会导致错误。'
- en: In this section, we have collected quotes listings details using XPath; in the
    next section, we will cover the same process but using CSS Selectors.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用XPath收集了引用列表的详细信息；在下一节中，我们将使用CSS选择器来完成相同的过程。
- en: Using CSS Selectors
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CSS选择器
- en: 'In this section, we will be using CSS Selectors with their extensions such
    as `::text` and `::attr` along with `extract()` and `strip()`. Similar to `response.xpath()`,
    available to run XPath expressions, CSS Selectors can be run using `response.css()`.
    The `css()` selector matches the elements using the provided expressions:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用CSS选择器及其扩展，如`::text`和`::attr`，以及`extract()`和`strip()`。与`response.xpath()`类似，可以使用`response.css()`来运行CSS选择器。`css()`选择器使用提供的表达式匹配元素：
- en: '[PRE47]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: As seen in the preceding code, `rows` represent individual elements with the `post-item` class, iterated
    for obtaining the `Item` fields.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，`rows`代表具有`post-item`类的单个元素，用于获取`Item`字段。
- en: For more information on CSS Selectors and obtaining CSS Selectors using browser-based
    development tools, please refer to [Chapter 3](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml), *Using
    LXML, XPath, and CSS Selectors*, *CSS Selectors* section and *XPath and CSS Selectors
    using DevTools* section, respectively.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 有关CSS选择器和使用基于浏览器的开发工具获取CSS选择器的更多信息，请参阅[第3章](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml)，*使用LXML、XPath和CSS选择器*，*CSS选择器*部分和*使用DevTools的XPath和CSS选择器*部分。
- en: For more detailed information on selectors and their properties, please refer
    to the Scrapy official documentation on selectors at [https://docs.scrapy.org/en/latest/topics/selectors.html](https://docs.scrapy.org/en/latest/topics/selectors.html).
    In the upcoming section, we will learn to scrape data from multiple pages.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 有关选择器及其属性的更详细信息，请参阅[https://docs.scrapy.org/en/latest/topics/selectors.html](https://docs.scrapy.org/en/latest/topics/selectors.html)上的Scrapy官方文档。在接下来的部分中，我们将学习如何从多个页面中抓取数据。
- en: Data from multiple pages
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 来自多个页面的数据
- en: In the preceding section, we tried scraping data for the URL in `start_urls`,
    that is, [http://quotes.toscrape.com/](http://quotes.toscrape.com/). It's also
    to be noted that this particular URL results in quotes listings for the first
    page only.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们尝试对`start_urls`中的URL进行数据抓取，即[http://quotes.toscrape.com/](http://quotes.toscrape.com/)。还要注意的是，这个特定的URL只会返回第一页的引用列表。
- en: 'Quotes listings are found across multiple pages and we need to access each
    one of those pages to collect the information. A pattern for pagination links
    is found in the following list:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 引用列表分布在多个页面上，我们需要访问每一页来收集信息。下面的列表中找到了分页链接的模式：
- en: '[http://quotes.toscrape.com/](http://quotes.toscrape.com/) (first page)'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://quotes.toscrape.com/](http://quotes.toscrape.com/)（第一页）'
- en: '[http://quotes.toscrape.com/page/2/](http://quotes.toscrape.com/page/2/)'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://quotes.toscrape.com/page/2/](http://quotes.toscrape.com/page/2/)'
- en: '[http://quotes.toscrape.com/page/3/](http://quotes.toscrape.com/page/3/)'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://quotes.toscrape.com/page/3/](http://quotes.toscrape.com/page/3/)'
- en: XPath and CSS Selectors used inside the `parse()`, as found in codes from the
    preceding section, will be scraping data from the first page or page 1 only. Pagination
    links found across pages can be requested and extracted by passing the link to
    `parse()` inside Spider using the `callback` argument from a `scrapy.Request`.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在`parse()`中使用的XPath和CSS选择器，如前一节中的代码所示，将从第一页或第1页中抓取数据。跨页面找到的分页链接可以通过将链接传递给Spider中的`parse()`并使用`scrapy.Request`的`callback`参数来请求和提取。
- en: 'As seen in the following code, a link to page 2 found on page 1 is extracted
    and passed to `scrapy.Request`, making a request to the `nextPage` processing
    plus yielding the item fields using `parse()`. Similarly, the iteration takes
    place until the link to the next page or `nextPage` exists:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如下面的代码所示，在第1页上找到的第2页的链接被提取并传递给`scrapy.Request`，发出对`nextPage`的请求并使用`parse()`来产生项目字段。类似地，迭代会一直进行，直到下一页或`nextPage`的链接存在为止：
- en: '[PRE48]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We can also obtain the pagination-based result by making changes only to `start_urls` as
    seen in the code next. Using this process doesn't require the use of `nextPage` or `scrapy.Request` as
    used in the preceding code.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过仅对`start_urls`进行更改来获得基于分页的结果，如下面的代码所示。使用这个过程不需要像前面的代码中使用的`nextPage`或`scrapy.Request`。
- en: 'URLs to be crawled can be listed inside `start_url` and are recursively implemented
    by `parse()` as seen in the following code:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 要爬取的URL可以在`start_url`中列出，并且通过`parse()`递归实现，如下面的代码所示：
- en: '[PRE49]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We can also obtain a list of URLs using the Python list comprehension technique.
    The `range()` function used in the following code accepts the start and end of
    the argument, that is, 1 and 4, and will result in the numbers 1, 2, and 3 as
    follows:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用Python的列表推导技术获取URL列表。下面的代码中使用的`range()`函数接受参数的开始和结束，即1和4，将得到1、2和3这些数字：
- en: '[PRE50]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: With extraction logic along with pagination and the item declared, in the next
    section, we will run the crawler quotes and export the item to the external files.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将运行爬虫quotes并将项目导出到外部文件中，使用提取逻辑以及分页和声明的项目。
- en: Running and exporting
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行和导出
- en: 'We need to run a Spider and look for data for item fields in the provided URLs.
    We can start running the Spider from the command line by issuing the `scrapy crawl
    quotes` command or as seen in the following screenshot:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要运行一个Spider，并在提供的URL中查找项目字段的数据。我们可以通过在命令行中发出`scrapy crawl quotes`命令或如下截图中所示的方式来开始运行Spider：
- en: '![](assets/bf9bff49-7694-4fc4-b31c-44867857c418.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/bf9bff49-7694-4fc4-b31c-44867857c418.png)'
- en: Running a Spider (scrapy crawl quotes)
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 运行Spider（scrapy crawl quotes）
- en: The Scrapy argument crawl is provided with a Spider name (`quotes`) in the command.
    A successful run of the command will result in information about Scrapy, bots,
    Spider, crawling stats, and HTTP methods, and will list the item data as a dictionary.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令中提供了Scrapy参数`crawl`和Spider名称(`quotes`)。成功运行该命令将得到有关Scrapy、机器人、Spider、爬取统计和HTTP方法的信息，并将列出项目数据作为字典。
- en: 'While executing a Spider we will receive various forms of information, such
    as `INFO`/`DEBUG`/`scrapy` statistics and so on, as found in the following code:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行Spider时，我们将收到各种形式的信息，例如`INFO`/`DEBUG`/`scrapy`统计数据等，如下面的代码中所示：
- en: '[PRE51]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The Scrapy statistics are as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy的统计数据如下：
- en: '[PRE52]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: We can also run the Spider and save the item found or data scraped to the external
    files. Data is exported or stored in files for easy access, usage, and convenience
    in sharing and managing.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以运行Spider并将找到的项目或抓取的数据保存到外部文件中。数据被导出或存储在文件中，以便于访问、使用和分享管理。
- en: 'With Scrapy, we can export scraped data to external files using crawl commands
    as seen in the following list:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Scrapy，我们可以使用爬行命令将抓取的数据导出到外部文件，如下面的列表中所示：
- en: 'To extract data to a CSV file we can use the `C:\ScrapyProjects\Quotes> scrapy
    crawl quotes -o quotes.csv` command as seen in the following screenshot:'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要将数据提取到CSV文件中，我们可以使用`C:\ScrapyProjects\Quotes> scrapy crawl quotes -o quotes.csv`命令，如下面的截图所示：
- en: '![](assets/4e5379b9-976f-4d5c-a890-a8e245b2c850.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/4e5379b9-976f-4d5c-a890-a8e245b2c850.png)'
- en: Contents from file quotes.csv
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 来自文件quotes.csv的内容
- en: 'To extract data to JSON file format, we can use the `C:\ScrapyProjects\Quotes> scrapy
    crawl quotes -o quotes.json` command as seen in the following:'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要将数据提取到JSON文件格式中，我们可以使用`C:\ScrapyProjects\Quotes> scrapy crawl quotes -o quotes.json`命令，如下所示：
- en: '![](assets/a62a3fae-a342-4d81-90a1-09cad3d9d2c2.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/a62a3fae-a342-4d81-90a1-09cad3d9d2c2.png)'
- en: Contents from file quotes.json
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 来自文件quotes.json的内容
- en: The `-o` parameter followed by a filename will be generated inside the main
    project folder. Please refer to the official Scrapy documentation about feed exports
    at [http://docs.scrapy.org/en/latest/topics/feed-exports.html](http://docs.scrapy.org/en/latest/topics/feed-exports.html)
    for more detailed information and file types that can be used to export data.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在主项目文件夹中将生成`-o`参数后跟随的文件名。有关feed导出的更详细信息和可以用于导出数据的文件类型，请参阅官方Scrapy文档[http://docs.scrapy.org/en/latest/topics/feed-exports.html](http://docs.scrapy.org/en/latest/topics/feed-exports.html)。
- en: In this section, we learned about Scrapy and used it to create a Spider to scrape
    data and export the data scraped to external files. In the next section, we will
    deploy the crawler on the web.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了Scrapy并使用它创建了一个爬虫来抓取数据并将抓取的数据导出到外部文件。在下一节中，我们将在网络上部署爬虫。
- en: Deploying a web crawler
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署网络爬虫
- en: Deploying a web crawler online or on a live server will certainly improve the
    effectiveness of the crawling activity, with its speed, updated technology, web
    spaces, anytime usage, and so on. Local tests and confirmation are required before
    deploying online. We need to own or buy web spaces with web-hosting companies
    or the cloud server.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在线部署网络爬虫或在实时服务器上部署将显著提高爬取活动的效果，具有速度、更新的技术、网络空间、随时使用等优势。在线部署之前需要进行本地测试和确认。我们需要拥有或购买网络空间，与网络托管公司或云服务器合作。
- en: 'Scrapy Cloud at [https://scrapinghub.com/scrapy-cloud](https://scrapinghub.com/scrapy-cloud) from
    Scrapinghub at [https://scrapinghub.com/](https://scrapinghub.com/) is one of
    the best platforms to deploy and manage the Scrapy Spider. The Scrapy Cloud provides
    an easy and interactive interface to deploy Scrapy and is free, with some of the
    additional features listed here:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy Cloud位于[https://scrapinghub.com/scrapy-cloud](https://scrapinghub.com/scrapy-cloud)，来自[https://scrapinghub.com/](https://scrapinghub.com/)的Scrapinghub是部署和管理Scrapy
    Spider的最佳平台之一。Scrapy Cloud提供了一个简单而交互式的界面来部署Scrapy，并且是免费的，以下是一些额外功能：
- en: Coding/managing and running Spider
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码/管理和运行Spider
- en: Deploying Spider to cloud
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将Spider部署到云端
- en: Downloading and sharing data
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载和分享数据
- en: API access with resource management
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API访问与资源管理
- en: 'The following are the steps performed to deploy projects using Scrapy Cloud:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用Scrapy Cloud部署项目的步骤：
- en: Open the web browser and go to [https://scrapinghub.com/](https://scrapinghub.com/).
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开网络浏览器并转到[https://scrapinghub.com/](https://scrapinghub.com/)。
- en: 'From the navigation menu, select PRODUCTS and choose SCRAPY CLOUD as seen in
    the following screenshot:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从导航菜单中选择“产品”，并选择SCRAPY CLOUD，如下面的屏幕截图所示：
- en: '![](assets/3e9545c1-6223-458e-8aea-ef367303a2ad.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/3e9545c1-6223-458e-8aea-ef367303a2ad.png)'
- en: Scrapinghub products
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapinghub产品
- en: 'Log in or register on the page loaded from [https://scrapinghub.com/scrapy-cloud](https://scrapinghub.com/scrapy-cloud)
    (or open the login page: [https://app.scrapinghub.com/account/login/](https://app.scrapinghub.com/account/login/)):'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录或注册页面加载自[https://scrapinghub.com/scrapy-cloud](https://scrapinghub.com/scrapy-cloud)（或打开登录页面：[https://app.scrapinghub.com/account/login/](https://app.scrapinghub.com/account/login/)）：
- en: '![](assets/d0711538-4f69-4af7-a9e6-d06cec4bee7e.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/d0711538-4f69-4af7-a9e6-d06cec4bee7e.png)'
- en: Log in and register page from scraping hub
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 从scraping hub登录和注册页面
- en: 'After completing registration and logging in, users are provided with an interactive
    dashboard and an option to CREATE A PROJECT, as seen in the following screenshot:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成注册和登录后，用户将获得一个交互式仪表板，并有一个“创建项目”的选项，如下面的屏幕截图所示：
- en: '![](assets/1c2cec76-2f0d-4308-bb4d-cc2034660438.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/1c2cec76-2f0d-4308-bb4d-cc2034660438.png)'
- en: User dashboard
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 用户仪表板
- en: 'Clicking CREATE PROJECT will pop up a window, as seen in the following screenshot:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击“创建项目”将弹出一个窗口，如下面的屏幕截图所示：
- en: '![](assets/2437a186-ba19-4040-b946-6b6329f26fd3.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/2437a186-ba19-4040-b946-6b6329f26fd3.png)'
- en: Create a new project from Scrapy Cloud
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 从Scrapy Cloud创建新项目
- en: Create a project named as seen in the screenshot and choose technology SCRAPY to
    deploy the spiders; click CREATE.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个项目，如屏幕截图所示，并选择部署Spider的技术SCRAPY；点击“创建”。
- en: 'A Dashboard with Scrapy Cloud Projects will be loaded, listing newly created
    projects as seen in the following screenshot:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将加载带有Scrapy Cloud项目的仪表板，列出新创建的项目，如下面的屏幕截图所示：
- en: '![](assets/572b3ba0-d5a4-42ea-8efb-b2654391db66.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/572b3ba0-d5a4-42ea-8efb-b2654391db66.png)'
- en: Scrapy Cloud Projects listings with option CREATE PROJECT
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy Cloud项目列表，带有“创建项目”选项
- en: To deploy the codes for the created project, select the project listed from
    the Scrapy Cloud Projects listings.
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要部署创建项目的代码，请从Scrapy Cloud项目列表中选择项目。
- en: 'The project dashboard will be loaded with various options. Choose the option
    Code & Deploys:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 项目仪表板将加载各种选项。选择“代码和部署”选项：
- en: '![](assets/dba071c6-eb37-4947-810d-fb45cea3a322.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dba071c6-eb37-4947-810d-fb45cea3a322.png)'
- en: Project dashboard with various options
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 带有各种选项的项目仪表板
- en: Deploy the code using either the command line or the GitHub.
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用命令行或GitHub部署代码。
- en: 'The successful deployment will list the Spider as seen in the following screenshot:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 成功部署将列出Spider，如下面的屏幕截图所示：
- en: '![](assets/725a4e3a-1b86-40e2-a11a-7e7b86adc66a.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/725a4e3a-1b86-40e2-a11a-7e7b86adc66a.png)'
- en: Listing of Spider after code deploy
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 代码部署后Spider的列表
- en: 'Click the listed Spider, and detailed information and available options will
    be displayed as shown in the following screenshot:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击列出的Spider，将显示详细信息和可用选项，如下面的屏幕截图所示：
- en: '![](assets/e0162ce0-62df-4762-a648-5764f8b670de.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/e0162ce0-62df-4762-a648-5764f8b670de.png)'
- en: Spider details
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: Spider详情
- en: 'Click RUN to start crawling the chosen Spider as seen here:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“运行”开始爬取所选的Spider，如下所示：
- en: '![](assets/d64c0eaa-93ad-4307-b60f-c3a36ce4c6d0.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/d64c0eaa-93ad-4307-b60f-c3a36ce4c6d0.png)'
- en: Spider Run window
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 爬虫运行窗口
- en: Click RUN with the default options.
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“运行”使用默认选项。
- en: 'Crawling jobs will be listed as seen in the following screenshot. We can browse
    through the Completed jobs for details on Items, Requests, Errors, Logs, and so
    on:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 爬取作业将列在如下屏幕截图所示。我们可以浏览“已完成的作业”以获取有关项目、请求、错误、日志等的详细信息：
- en: '![](assets/879a0440-028a-4a14-b199-09ada3f8b84e.png)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/879a0440-028a-4a14-b199-09ada3f8b84e.png)'
- en: Jobs details for Spider
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: Spider的作业详情
- en: 'When exploring items for completed jobs, options such as filters, data export,
    and downloading with crawling job details for requests, logs, stats, and so on
    are available in the job details. More information can be loaded by clicking a
    particular Spider listed:'
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在浏览已完成作业的项目时，可以使用筛选器、数据导出和下载等选项，以及有关请求、日志、统计等的爬取作业详细信息。单击列出的特定Spider可以加载更多信息：
- en: '![](assets/559fca95-9b82-41d3-9d1c-5cc3032783b6.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/559fca95-9b82-41d3-9d1c-5cc3032783b6.png)'
- en: Listing items from Spider
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 从Spider列出项目
- en: Using the actions listed previously, we can deploy Scrapy Spider successfully
    using the Scraping hub.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面列出的操作，我们可以成功地使用Scraping hub部署Scrapy Spider。
- en: In this section, we used and explored the Scraping hub to deploy the Scrapy
    Spider.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用和探索了Scraping hub来部署Scrapy Spider。
- en: Summary
  id: totrans-389
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Selecting the right libraries and frameworks does depend on the project scope.
    Users are free to choose libraries and experience the online process.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的库和框架取决于项目的范围。用户可以自由选择库并体验在线过程。
- en: 'In this chapter, we have used and explored various aspects of traversing web
    documents using Beautiful Soup and have explored a framework built for crawling
    activities using Spiders: Scrapy. Scrapy provides a complete framework to develop
    a crawler and is effective using XPath and CSS Selectors with support for the
    data export. Scrapy projects can also be deployed using Scraping hub to experience
    the live performance of the deployed Spider and enjoy features provided by the
    Scrapings hub (Scrapy Cloud) at [https://scrapinghub.com/scrapy-cloud](https://scrapinghub.com/scrapy-cloud).'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用和探索了使用Beautiful Soup遍历web文档的各个方面，并探索了一个用于爬虫活动的框架：Scrapy。Scrapy提供了一个完整的框架来开发爬虫，并且可以有效地使用XPath和CSS选择器来支持数据导出。Scrapy项目也可以使用Scraping
    hub部署，以体验部署Spider的实时性能，并享受Scraping hub（Scrapy Cloud）提供的功能。
- en: In the next chapter, we will explore more information regarding scraping data
    from the web.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探索更多有关从网页中抓取数据的信息。
- en: Further reading
  id: totrans-393
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Scrapy: [https://docs.scrapy.org/en/latest/intro/overview.html](https://docs.scrapy.org/en/latest/intro/overview.html)
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scrapy：[https://docs.scrapy.org/en/latest/intro/overview.html](https://docs.scrapy.org/en/latest/intro/overview.html)
- en: Learn Scrapy: [https://learn.scrapinghub.com/scrapy/](https://learn.scrapinghub.com/scrapy/)
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习Scrapy：[https://learn.scrapinghub.com/scrapy/](https://learn.scrapinghub.com/scrapy/)
- en: Beautiful Soup: [https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beautiful Soup：[https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- en: SoupSieve: [https://facelessuser.github.io/soupsieve/selectors/](https://facelessuser.github.io/soupsieve/selectors/)
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SoupSieve：[https://facelessuser.github.io/soupsieve/selectors/](https://facelessuser.github.io/soupsieve/selectors/)
- en: XPath tutorial: [https://doc.scrapy.org/en/xpath-tutorial/topics/xpath-tutorial.html](https://doc.scrapy.org/en/xpath-tutorial/topics/xpath-tutorial.html)
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XPath教程：[https://doc.scrapy.org/en/xpath-tutorial/topics/xpath-tutorial.html](https://doc.scrapy.org/en/xpath-tutorial/topics/xpath-tutorial.html)
- en: CSS Selector reference: [https://www.w3schools.com/cssref/css_selectors.asp](https://www.w3schools.com/cssref/css_selectors.asp)
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSS选择器参考：[https://www.w3schools.com/cssref/css_selectors.asp](https://www.w3schools.com/cssref/css_selectors.asp)
- en: Feed exports: [http://docs.scrapy.org/en/latest/topics/feed-exports.html](http://docs.scrapy.org/en/latest/topics/feed-exports.html)
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feed导出：[http://docs.scrapy.org/en/latest/topics/feed-exports.html](http://docs.scrapy.org/en/latest/topics/feed-exports.html)
- en: Scraping hub (Scrapy Cloud): [https://scrapinghub.com/](https://scrapinghub.com/)
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scraping hub（Scrapy Cloud）：[https://scrapinghub.com/](https://scrapinghub.com/)
