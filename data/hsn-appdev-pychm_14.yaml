- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Building a Data Pipeline in PyCharm
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在PyCharm中构建数据管道
- en: The term *data pipeline* generally denotes a step-wise procedure that entails
    collecting, processing, and analyzing data. This term is widely used in the industry
    to express the need for a reliable workflow that takes raw data and converts it
    into actionable insights. Some data pipelines work at massive scales, such as
    a **marketing technology** (**MarTech**) company ingesting millions of data points
    from Kafka streams, storing them in large data stores such as **Hadoop** or **Clickhouse**,
    and then cleansing, enriching, and visualizing that data. Other times, the data
    is smaller but far more impactful, such as the project we’ll be working on in
    this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “数据管道”这个术语通常表示一个逐步的过程，包括收集、处理和分析数据。在行业中，这个术语被广泛用来表达对可靠工作流程的需求，该工作流程将原始数据转换为可操作的见解。一些数据管道在巨大的规模上工作，例如一家**营销技术**（**MarTech**）公司从Kafka流中摄取数百万数据点，将它们存储在像**Hadoop**或**Clickhouse**这样的大型数据存储中，然后清洗、丰富和可视化这些数据。有时，数据量较小但影响更大，例如我们将在本章中工作的项目。
- en: 'In this chapter, we will learn about the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下主题：
- en: How to work with and maintain datasets
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何处理和维护数据集
- en: How to clean and preprocess data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何清理和预处理数据
- en: How to visualize data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何可视化数据
- en: How to utilize **machine** **learning** (**ML**)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何利用**机器学习**（**ML**）
- en: Throughout this chapter, you will be able to apply what you have learned about
    the topic of scientific computing so far to a real project with PyCharm. This
    serves as a hands-on discussion to conclude this topic of working with scientific
    computing and data science projects.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将能够将迄今为止关于科学计算主题所学的知识应用到PyCharm的实际项目中。这作为了一个动手讨论，以结束与科学计算和数据科学项目合作的主题。
- en: I want to specifically point out that I am heavily leveraging the text, code,
    and data from the first edition, which was written by a different author, Quan
    Nguyen. In this second edition, my main job was to update the existing content.
    Quan’s treatment in this chapter was excellent, so most of what I did to update
    this chapter was use the newer version of PyCharm, update the libraries used to
    the latest versions, and then re-write this chapter in my own words so that the
    writing style matches the rest of this book. There is no way I could have pulled
    this off without Quan’s original work and I wanted to tip my hat to the original
    Python data science kung fu master.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我要特别指出，我大量借鉴了第一版中的文本、代码和数据，该版由不同的作者Quan Nguyen编写。在第二版中，我的主要工作是更新现有内容。Quan在本章中的处理非常出色，因此我更新本章的大部分工作都是使用PyCharm的新版本，更新到最新版本的库，然后用我自己的话重写本章，以便写作风格与本书的其他部分相匹配。没有Quan的原始作品，我无法完成这项工作，我想向这位原始的Python数据科学功夫大师致敬。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To proceed through this chapter, you will need the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要继续阅读本章，您需要以下内容：
- en: Anaconda, which is a Python distribution tailored to data science workloads.
    You can find it, along with installation instructions for your OS, at [https://anaconda.com](https://anaconda.com).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anaconda，这是一个针对数据科学工作负载定制的Python发行版。您可以在[https://anaconda.com](https://anaconda.com)找到它，以及适用于您操作系统的安装说明。
- en: Likewise, instead of the usual `pip`, I’ll be leveraging `conda`, which is Anaconda’s
    package manager. It is installed alongside Anaconda.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，我将使用Anaconda的包管理器conda，而不是通常的`pip`。conda是与Anaconda一起安装的。
- en: An installed and working copy of PyCharm. Its installation was covered in [*Chapter
    2*](B19644_02.xhtml#_idTextAnchor028), *Installation and Configuration*, in case
    you are jumping into the middle of this book.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装并运行PyCharm的副本。如果您是在本书的中间部分开始阅读，其安装已在[*第2章*](B19644_02.xhtml#_idTextAnchor028)，“安装和配置”中介绍。
- en: This book’s sample source code from GitHub. We covered cloning the code in [*Chapter
    2*](B19644_02.xhtml#_idTextAnchor028), *Installation and Configuration*. You’ll
    find this chapter’s code at [https://github.com/PacktPublishing/Hands-On-Application-Development-with-PyCharm---Second-Edition/tree/main/chapter-14](https://github.com/PacktPublishing/Hands-On-Application-Development-with-PyCharm---Second-Edition/tree/main/chapter-14).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本书样本源代码来自GitHub。我们在[*第2章*](B19644_02.xhtml#_idTextAnchor028)，“安装和配置”中介绍了如何克隆代码。您可以在[https://github.com/PacktPublishing/Hands-On-Application-Development-with-PyCharm---Second-Edition/tree/main/chapter-14](https://github.com/PacktPublishing/Hands-On-Application-Development-with-PyCharm---Second-Edition/tree/main/chapter-14)找到本章的代码。
- en: Working with datasets
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理数据集
- en: Datasets are the backbone of any data science project. With a good, well-structured
    dataset, we have the opportunity to explore, ideate, and discover important insights
    from the data. The terms *good* and *well-structured* are key. In the real world,
    this rarely happens by accident. I am the lead developer on a project that does
    data science every day. We ingest diagnostic, utilization, and performance data
    from various hardware platforms such as storage arrays, switches, virtualization
    nodes (such as VMware), backup devices, and more. We collect it for the entire
    enterprise; every device in every data center. Our software then turns that raw
    data into visualizations that provide insights, allowing organizations to effectively
    manage their IT estate through consolidating health monitoring, utilization and
    performance reporting, and capacity planning.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是任何数据科学项目的骨架。有了良好、结构化的数据集，我们就有机会从数据中探索、构思，并发现重要的见解。术语“良好”和“结构化”是关键。在现实世界中，这种情况很少是偶然发生的。我是每天进行数据科学项目的首席开发者。我们从各种硬件平台（如存储阵列、交换机、虚拟化节点（如VMware）、备份设备等）收集诊断、利用和性能数据。我们收集整个企业的数据；每个数据中心中的每个设备。然后我们的软件将原始数据转换为可视化，提供见解，使组织能够通过整合健康监控、利用和性能报告以及容量规划，有效地管理其IT资产。
- en: I’ve been at it for 10 years now and we’re always looking to support new devices
    and systems. Our challenge, though, is getting the data we need. When I started
    10 years ago, getting data out of a NetApp storage array was very hard because
    its diagnostic data is dumped as unstructured text. Contrast that with more modern
    arrays, which dump data in XML or JSON, or even better, have their own SDKs for
    interfacing with hardware and extracting the data we need.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经从事这个领域10年了，我们一直在寻找支持新的设备和系统。然而，我们的挑战是获取所需的数据。10年前，从NetApp存储阵列中获取数据非常困难，因为它的诊断数据以非结构化文本的形式被丢弃。与之相比，更现代的阵列将数据以XML或JSON格式丢弃，或者更好的是，它们有自己的SDK，用于与硬件接口并提取我们所需的数据。
- en: A great deal of effort goes into taking data from various sources and working
    to mold the raw data into something useful. Sometimes it’s easy, and sometimes
    it is very difficult. Poorly formatted data can lead to erroneous conclusions
    and false insights.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从各种来源收集数据并努力将原始数据塑造成有用的东西需要投入大量的努力。有时这很容易，有时却非常困难。格式不佳的数据可能导致错误的结论和错误的见解。
- en: A great cautionary tale comes from a large shoe manufacturer. About 20 years
    ago, I worked for a company that sold software designed to manage factory production.
    We consulted with the shoe company and told them exactly how to model their data
    for the best results. They ignored us and went a different way. We told them it
    wouldn’t work. They thanked us for our input. Their projections were galactically
    wrong, so they did what any big company with boards and shareholders would do
    – they blamed the software. Our CEO did the circuit on the business shows, but
    the damage was done. Our company stock tanked and a lot of people lost their jobs
    that year, including me. To this day, I won’t wear their shoes. Bad data can cost
    livelihoods, reputations, and, beyond the context of shoes, even lives. We must
    have tools and processes at our disposal that help us get things right.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一个令人深思的例子来自一家大型鞋制造商。大约20年前，我在一家销售用于管理工厂生产的软件的公司工作。我们与鞋厂进行了咨询，并告诉他们如何建模他们的数据以获得最佳结果。他们没有听取我们的意见，而是选择了另一条路。我们告诉他们这不会奏效。他们感谢我们的建议。他们的预测完全错误，因此他们像任何有董事会和股东的大公司一样做了——他们责怪软件。我们的首席执行官在商业节目中四处奔波，但损害已经造成。我们的公司股价暴跌，那一年很多人失去了工作，包括我。时至今日，我都不会穿他们的鞋。糟糕的数据可能会危及生计、声誉，甚至在鞋之外，甚至生命。我们必须拥有工具和流程，帮助我们确保一切正确。
- en: Let’s go over a few steps of that process.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下这个过程的一些步骤。
- en: Starting with a question
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从一个问题开始
- en: 'Everything in science starts with a question. For our purposes, we’ll consider
    two possible scenarios:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 科学中的每一件事都是从一个问题开始的。就我们的目的而言，我们将考虑两种可能的场景：
- en: We have a specific question in mind and we need to collect and analyze appropriate
    data to answer that question
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们心中有一个具体的问题，我们需要收集和分析适当的数据来回答这个问题
- en: We already have data, and during exploration, a question has arisen
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经有了数据，在探索过程中，一个问题已经出现
- en: In our case, we’re going to recreate the data analysis phase of a potentially
    important breakthrough in the field of medical diagnosis. I’ll be presenting an
    example from Kaggle taken from a paper titled *High-accuracy detection of early
    Parkinson’s Disease using multiple characteristics of finger movement while typing,*
    which was conducted by Warwick Adams in 2017\. You’ll find the full study paper
    and the dataset links in the *Further reading* section of this chapter.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们将重新创建医学诊断领域一个可能的重要突破的数据分析阶段。我将展示一个来自Kaggle的例子，该例子取自一篇题为《利用打字时手指运动的多项特征高精度检测早期帕金森病》的论文，该论文由沃里克·亚当斯于2017年进行。你可以在本章的“进一步阅读”部分找到完整的研究论文和数据集链接。
- en: Note
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Kaggle is an online data community designed for data scientists and ML engineers.
    The site provides competitions, datasets, playgrounds, and other educational activities
    to promote the growth of data science, both in academia and the industry. More
    information about the website can be found on its home page: [https://www.kaggle.com/](https://www.kaggle.com/).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle是一个为数据科学家和机器学习工程师设计的在线数据社区。该网站提供竞赛、数据集、游乐场和其他教育活动，以促进数据科学在学术界和工业界的增长。更多关于网站的信息可以在其主页上找到：[https://www.kaggle.com/](https://www.kaggle.com/)。
- en: '**Parkinson’s Disease** (**PD**) is a condition that affects the brain and
    causes problems with movement. It’s a progressive disease, which means it gets
    worse over time. More than 6 million people around the world have this disease.
    In PD, a specific type of brain cell that produces a chemical called *dopamine*
    starts to die off. This leads to a variety of symptoms, including difficulty with
    movement and other non-movement-related issues.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**帕金森病**（**PD**）是一种影响大脑并导致运动问题的疾病。它是一种进行性疾病，这意味着随着时间的推移会变得更糟。全世界有超过600万人患有这种疾病。在PD中，一种产生多巴胺这种化学物质的特定类型的大脑细胞开始死亡。这导致了一系列的症状，包括运动困难和与其他非运动相关的问题。'
- en: At the time of writing, doctors don’t have a definite test to diagnose PD, especially
    in the early stages when the symptoms might not be very obvious. This results
    in mistakes in diagnosing the disease, with up to 25% of cases being misdiagnosed
    by doctors who aren’t specialists in PD. Some people can have PD for many years
    before they are correctly diagnosed.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，医生们还没有一个明确的测试来诊断PD，尤其是在早期阶段，症状可能不是很明显。这导致诊断疾病时出现错误，高达25%的病例被非PD专科医生误诊。有些人可能患有PD多年，直到被正确诊断。
- en: This leads us to a question…
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了一个问题……
- en: How can we effectively and accurately diagnose PD using some test, metric, or
    diagnostic data point without specialized clinical training?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何有效地使用某些测试、指标或诊断数据点来诊断PD，而不需要专业的临床培训？
- en: Adams suggested a test that uses computer typing data collected over time. Since
    typing involves fine motor movement, and since this fine motor movement is the
    first thing to go during the early onset of PD, Adams hoped it would be possible
    to use the mundane task of typing as a diagnostic tool. The researchers tested
    this method on 103 people; 32 of them had mild PD and the rest, the control group,
    didn’t have PD. The computer analysis of their typing patterns was able to tell
    the difference between the people with early-stage PD and those without it. This
    method correctly identified PD with 96% accuracy in detecting those who had it,
    and 97% accuracy in correctly identifying those who didn’t. This suggests that
    this method might be good at distinguishing between the two groups. Let’s see
    whether we can draw the same conclusion given their study’s data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 亚当斯提出了一种测试方法，该方法使用收集到的随时间推移的计算机打字数据。由于打字涉及精细运动，而这种精细运动是PD早期发病时首先出现的问题，亚当斯希望利用日常的打字任务作为诊断工具。研究人员在103人身上测试了这种方法；其中32人患有轻度PD，其余的为对照组，没有PD。对他们的打字模式进行计算机分析能够区分早期PD患者和无PD患者。这种方法在检测PD患者时准确率达到96%，在正确识别非PD患者时准确率达到97%。这表明这种方法可能擅长区分这两组人群。让我们看看是否可以根据他们研究的数据得出相同的结论。
- en: Archived user data
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存档用户数据
- en: 'Within this chapter’s source code, you’ll find a data science project called
    `pipeline`. The project contains a data folder containing our datasets in two
    folders: `Archived users` and `Tappy Data`.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的源代码中，你可以找到一个名为`pipeline`的数据科学项目。该项目包含一个数据文件夹，其中包含我们的数据集，分为两个文件夹：`Archived
    users`和`Tappy Data`。
- en: 'The data within the `Archived users` folder is in text file format and appears
    like this:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在“存档用户”文件夹中的数据是文本文件格式，看起来是这样的：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For the sake of immersion, let’s demystify this a little bit. These are the
    fields we have in each record:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地沉浸其中，让我们稍微揭开一些神秘的面纱。以下是每个记录中包含的字段：
- en: '`Birth Year: 1952`: This person was born in 1952.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`出生年份：1952`：这个人出生于1952年。'
- en: '`Gender: Female`: This person identifies as female.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`性别：女性`：这个人自认为是女性。'
- en: '`Parkinsons: True`: The person has been diagnosed with PD.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`帕金森病：是`：这个人已被诊断为PD。'
- en: '`Tremors: True`: Tremors, which are involuntary shaking movements, are present
    in this person. Tremors are a common symptom of PD.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`震颤：是`：这个人有震颤，这是一种不由自主的颤抖运动。震颤是PD的常见症状。'
- en: '`DiagnosisYear: 2000`: The person was diagnosed with PD in 2000.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`诊断年份：2000`：这个人于2000年被诊断为PD。'
- en: '`Sided: Left`: The term *sided* in this context likely refers to the side of
    the body where the symptoms are more pronounced. In this case, the symptoms are
    more noticeable on the left-hand side of the body.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`侧：左`：在这个上下文中，“侧”这个术语可能指的是症状更明显的身体部位。在这种情况下，症状在身体的左侧更为明显。'
- en: '`UPDRS: Don''t know`: The **Unified Parkinson’s Disease Rating Scale** (**UPDRS**)
    is a tool that’s used to assess the severity of PD. In this case, it’s not known
    what the specific UPDRS score is for this individual.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UPDRS：不知道`：**统一帕金森病评分量表**（**UPDRS**）是一种用于评估PD严重程度的工具。在这种情况下，不知道这个个体的具体UPDRS评分是多少。'
- en: '`Impact: Severe`: The impact of PD on this person’s life is considered severe,
    indicating that the symptoms have a significant effect on their daily activities
    and quality of life.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`影响：严重`：PD对这个人生活的影响被认为是严重的，这表明症状对其日常活动和生活质量有显著影响。'
- en: '`Levodopa: True`: Levodopa is a common medication used to manage the symptoms
    of PD. This person is taking Levodopa as part of their treatment.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`左旋多巴：是`：左旋多巴是一种常用的药物，用于管理PD的症状。这个人正在接受左旋多巴作为其治疗的一部分。'
- en: '`DA: True`: **Dopamine agonists** (**DAs**) are another type of medication
    used to manage Parkinson’s symptoms. This person is taking dopamine agonists as
    part of their treatment.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DA：是`：**多巴胺激动剂**（**DAs**）是另一种用于管理PD症状的药物。这个人正在接受多巴胺激动剂作为其治疗的一部分。'
- en: '`MAOB: False`: **Monoamine oxidase B inhibitors** (**MOABs**) are medications
    that can help manage Parkinson’s symptoms by increasing dopamine levels in the
    brain. In this case, the person is not taking MAOBs.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MAOB：否`：**单胺氧化酶B抑制剂**（**MOABs**）是一种可以增加大脑多巴胺水平的药物，有助于管理PD的症状。在这种情况下，这个人没有服用MAOBs。'
- en: '`Other: False`: If I were recreating this study for real, I would likely contact
    the original researcher if this data point wasn’t explained directly in the publication.
    Since I’m not, I’ll guess that it won’t affect our project. This likely refers
    to other specific medications or treatments for PD, indicating that the person
    is not undergoing any other specialized treatments beyond Levodopa and DAs.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`其他：否`：如果我要为真实的研究重新创建这个研究，我可能会联系原始研究者，如果这个数据点没有在出版物中直接解释。由于我没有这样做，我会猜测这不会影响我们的项目。这很可能指的是PD的其他特定药物或治疗方法，表明这个人除了左旋多巴和DAs之外没有接受任何其他特殊治疗。'
- en: In summary, this individual was a 65-year-old woman at the time of the study
    who was diagnosed with PD in 2000\. She experiences tremors, particularly on the
    left-hand side of her body. The impact of the disease on her life is severe. She
    is undergoing treatment with Levodopa and DAs to manage her symptoms, but she
    is not using MAOBs or any other specialized treatments. The specific severity
    of her symptoms, as measured by the UPDRS, is not provided in the given information.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这位个体在研究时是一位65岁的女性，于2000年被诊断为PD。她有震颤，尤其是在身体的左侧。疾病对她的生活影响严重。她正在接受左旋多巴和DAs的治疗来管理她的症状，但她没有使用MAOBs或任何其他特殊治疗。根据UPDRS测量的具体严重程度，在提供的信息中没有提供。
- en: The filenames in the folder are important. It isn’t ethical to publish `User_0EA27ICBLF.txt`
    file.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 文件夹中的文件名很重要。发布`User_0EA27ICBLF.txt`文件是不道德的。
- en: Tappy data
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Tappy数据
- en: The study methodology uses an application called Tappy, which runs on Windows
    and records each subject’s keypress timing, along with positional data about each
    key. If you remember from our earlier discussion of the user data, the sidedness
    is a factor. The motor cortex is the region of the brain that is responsible for
    planning, controlling, and executing voluntary movements. It’s located in the
    cerebral cortex, which is the outermost layer of the brain.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 研究方法使用一个名为Tappy的应用程序，该应用程序在Windows上运行，并记录每个受试者的按键时间和每个键的位置数据。如果你还记得我们之前关于用户数据的讨论，侧向性是一个因素。运动皮层是大脑中负责计划、控制和执行自愿运动的部分。它位于大脑皮层，即大脑的最外层。
- en: 'The motor cortex, along with most of the rest of the brain, is divided into
    two hemispheres: the left hemisphere and the right hemisphere. Each hemisphere
    controls the voluntary movements of the opposite side of the body. In other words,
    the left hemisphere of the motor cortex controls movements on the right-hand side
    of the body, and the right hemisphere controls movements on the left-hand side
    of the body. Since this is true, knowing which side of the keyboard the keypress
    data is coming from is potentially of diagnostic importance.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 运动皮层，连同大脑的大部分区域，分为两个半球：左半球和右半球。每个半球控制身体对侧的自愿运动。换句话说，运动皮层的左半球控制身体右侧的运动，而右半球控制身体左侧的运动。由于这一点是真实的，知道按键数据来自键盘的哪一侧可能具有潜在的诊断重要性。
- en: 'Let’s open a Tappy dataset and see what’s inside:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打开一个Tappy数据集，看看里面有什么：
- en: '![Figure 14.1: I’ve opened the first file in the Tappy data folder and I can
    see it is tab-separated data](img/B19644_14_01.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图14.1：我已经打开了Tappy数据文件夹中的第一个文件，并且可以看到它是制表符分隔的数据](img/B19644_14_01.jpg)'
- en: 'Figure 14.1: I’ve opened the first file in the Tappy data folder and I can
    see it is tab-separated data'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1：我已经打开了Tappy数据文件夹中的第一个文件，并且可以看到它是制表符分隔的数据
- en: I can see a warning at the top stating the file is large, by code editor standards,
    and that code insight is not available. This is spurious since the data folder
    in a scientific project in PyCharm is excluded from indexing and code insights
    anyway. You can safely ignore the warning.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以看到顶部有一个警告，说明文件很大，按照代码编辑器的标准，代码洞察不可用。这是无谓的，因为PyCharm中科学项目的数据文件夹本来就不被索引和代码洞察。你可以安全地忽略这个警告。
- en: I can also see that the file is tab-delimited, which will play nicely in a data
    pipeline. It is always encouraging to see your data come to you in an easily parsable
    format. This is effectively structured data that would be suitable for import
    into a spreadsheet or database table. That isn’t necessarily what we will do with
    this data, but if we can do those kinds of imports with a given data file, we
    can pretty much do anything with the data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我还可以看到文件是制表符分隔的，这将很好地在数据管道中运行。看到你的数据以易于解析的格式到来总是令人鼓舞。这实际上是适合导入电子表格或数据库表的结构化数据。这并不一定是我们将要处理的数据，但如果我们可以用给定的数据文件进行这些类型的导入，我们几乎可以处理任何数据。
- en: As before, the filenames are significant. The first part of the file, delineated
    by an underscore, is the ID of the subject from the `Archived users` folder. We
    will be able to relate each subject’s performance data found in the `Tappy Data`
    folder with their demographical data found in the `Archived` `users` folder.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，文件名是有意义的。文件的第一部分，由下划线分隔，是来自`存档用户`文件夹的受试者ID。我们将能够将`Tappy数据`文件夹中找到的每个受试者的表现数据与`存档``用户`文件夹中找到的他们的人口统计数据相关联。
- en: 'The fields from the Tappy data file are as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Tappy数据文件中的字段如下：
- en: Patient ID
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 患者ID
- en: The date of data collection
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据收集日期
- en: The timestamp of each keystroke
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个按键的时间戳
- en: Which hand performed the keystroke (*L* for left and *R* for right)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪只手执行了按键操作（*L*代表左手，*R*代表右手）
- en: Hold time (time between press and release, in milliseconds)
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持时间（按下和释放之间的时间，以毫秒为单位）
- en: The transition from the last keystroke
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一个按键的转换
- en: Latency time (time from pressing the previous key, in milliseconds)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 延迟时间（从按下前一个键到的时间，以毫秒为单位）
- en: Flight time (time from releasing the previous key, in milliseconds)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 飞行时间（从释放前一个键到的时间，以毫秒为单位）
- en: We have established that we have raw data in a workable format. Honestly, I’d
    call this a good day. It isn’t completely perfect; we’ll still need to do some
    munging, but it’s a very good starting point.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确定我们拥有可工作的原始数据格式。说实话，这真是个好日子。它并不完全完美；我们仍然需要进行一些整理，但这是一个非常好的起点。
- en: Jargon alert – munging
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 术语警告 - 整理
- en: '**Munging** is a colloquial term used in computer programming and data processing
    to describe the process of manipulating, cleaning, or transforming data from one
    format into another. It often involves altering the structure or content of data
    to make it more suitable for a particular purpose, such as analysis, storage,
    or presentation. Munging can include activities such as the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**Munging**是一个在计算机编程和数据处理中使用的俚语，用来描述将数据从一种格式转换到另一种格式的过程。这通常涉及改变数据的结构或内容，使其更适合特定目的，例如分析、存储或展示。Munging可能包括以下活动：'
- en: '- **Data cleaning**: Removing errors, inconsistencies, or irrelevant information
    from datasets'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '- **数据清洗**：从数据集中删除错误、不一致或不相关信息'
- en: '- **Data transformation**: Changing the format, structure, or representation
    of data to fit a specific requirement'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '- **数据转换**：改变数据的格式、结构或表示，以适应特定要求'
- en: '- **Data parsing**: Extracting specific pieces of information from a larger
    dataset'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '- **数据解析**：从更大的数据集中提取特定的信息'
- en: '- **Data aggregation**: Combining multiple sets of data into a single dataset'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '- **数据聚合**：将多个数据集合并成一个单一的数据集'
- en: '- **Data filtering**: Selecting or excluding data based on certain criteria'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '- **数据过滤**：根据某些标准选择或排除数据'
- en: '- **Data formatting**: Changing the way data is presented or encoded for compatibility
    with a certain system or software'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '- **数据格式化**：改变数据展示或编码的方式，以适应特定的系统或软件'
- en: The term *munging* is informal and comes from a blend of *mangle* and *modify*.
    It’s often used in a context where data needs to be prepared or adjusted for analysis,
    integration, or some other data-related task.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 术语*munging*是非正式的，来源于*mangle*和*modify*的混合。它通常用于需要准备或调整数据以进行分析、集成或其他数据相关任务的环境中。
- en: 'We have a good start for our project, but we have a question: can we detect
    early-onset PD using a typing test? We have raw data from a study that implemented
    such a typing test. We’re ready to roll up our sleeves and get into it!'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的项目有一个良好的开端，但我们有一个问题：我们能否通过打字测试来检测早期帕金森病？我们有一项研究实施的此类打字测试的原始数据。我们准备好卷起袖子大干一场了！
- en: Data collection
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据收集
- en: We’re lucky. I’ve already found our data and included it for your consideration.
    In the real world, we would have needed to have performed the normal step of data
    collection. While there are entire tomes on this topic – most 4-year scientific
    university degree programs focus heavily on this topic – I don’t plan on doing
    a deep dive here. However, I will at least give you an overview should you be
    new to what we’re trying to accomplish.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很幸运。我已经找到了我们的数据，并包括了它供您考虑。在现实世界中，我们本需要执行数据收集的正常步骤。虽然关于这个主题有整本书的论述——大多数4年的科学大学学位课程都高度重视这个主题——但我并不打算在这里深入研究。然而，如果您对我们试图完成的事情不熟悉，我至少会提供一个概述。
- en: Downloading from an external source
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从外部来源下载
- en: This is the case for our example dataset since I downloaded it from Kaggle.
    When using a dataset downloaded from the internet, we should always make sure
    to check its copyright license. Most of the time, if it is in the public domain,
    we can freely use and distribute it without any worry. The example dataset we
    are using is an instance of this. On the other hand, if the dataset is copyrighted,
    we might still be able to use it by asking for permission from the author/owner
    of the dataset. I have found that, after reaching out to them via email and explaining
    how their datasets will be used in detail, dataset owners are often willing to
    share their data with others.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例数据集中，情况就是这样，因为我从Kaggle下载了它。当使用从互联网下载的数据集时，我们应始终确保检查其版权许可。大多数情况下，如果它在公共领域，我们可以自由使用和分发，无需担忧。我们正在使用的数据集就是这种情况的一个例子。另一方面，如果数据集受版权保护，我们可能仍然可以通过请求数据集的作者/所有者的许可来使用它。我发现，通过电子邮件与他们联系并详细解释他们的数据集将如何被使用后，数据集所有者通常愿意与他人分享他们的数据。
- en: Manually collecting/web scraping
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手动收集/网络爬虫
- en: If the data we want is available online but not formatted in tables or CSV files,
    most of the time, we need to collect it and manually put it in a dataset ourselves.
    At most, we can write a web scraper that can send requests to the websites containing
    the target data and parse the returned HTML text. When you have to collect your
    data this way, it is also important to ensure that you are not doing it illegally.
    For example, it is against the law to have a program scrape data off some websites;
    sometimes, you might need to design the scraper so that only a certain number
    of requests are made at a given point. An example of this was when LinkedIn filed
    a lawsuit against many people who anonymously scraped their data in 2016\. For
    this reason, it is always a good practice to find the terms of use for the data
    you are trying to collect this way.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要的数据在线上有，但不是以表格或CSV文件格式，大多数情况下，我们需要自己收集它并手动将其放入数据集中。最多，我们可以编写一个网络爬虫，可以向包含目标数据的网站发送请求并解析返回的HTML文本。当你必须以这种方式收集数据时，确保你没有非法行事也很重要。例如，从某些网站上抓取数据是违法的；有时，你可能需要设计爬虫，以便在特定时刻只发送一定数量的请求。一个例子是LinkedIn在2016年对许多匿名抓取其数据的人提起诉讼。因此，始终寻找你试图以这种方式收集的数据的使用条款是一个好习惯。
- en: Collecting data via third parties
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过第三方收集数据
- en: Students and researchers who find that the data they are looking for in their
    study cannot be collected online often rely on third-party services to collect
    that data for them (for example, via crowd-sourcing). Amazon **Mechanical Turk**
    (**MTurk**) is one such service – you can enter any type of question to make a
    survey and MTurk will introduce that survey to its users. Participants receive
    money for taking the survey, which is paid by the owner of the survey. This option
    is, again, specifically applicable when you want a representative dataset that
    is not available online anywhere.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 学生和研究人员在他们的研究中发现，他们寻找的数据无法在线收集时，通常会依赖第三方服务来帮他们收集这些数据（例如，通过众包）。Amazon **Mechanical
    Turk** (**MTurk**) 就是这样的服务之一——你可以输入任何类型的问题来制作调查问卷，MTurk 将该调查问卷介绍给其用户。参与者可以通过完成调查获得报酬，这笔费用由调查的所有者支付。这个选项再次特别适用于当你想要一个在线任何地方都不可获得的代表性数据集时。
- en: Database exports
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据库导出
- en: This is most likely the case if you are working with data from your company
    or organization. Luckily, PyCharm offers many useful features in terms of working
    with databases and their data sources. This process was discussed in [*Chapter
    11*](B19644_11.xhtml#_idTextAnchor266), and I highly recommend you check it out
    if you haven’t already.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在处理公司或组织的数据，这很可能是这种情况。幸运的是，PyCharm在处理数据库及其数据源方面提供了许多有用的功能。这个过程在[*第11章*](B19644_11.xhtml#_idTextAnchor266)中进行了讨论，如果你还没有看过，我强烈推荐你查看。
- en: Version control for datasets
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集的版本控制
- en: Since we took a quick little side journey to discuss data collection, I hope
    you’ll indulge me once more while we talk about using data in a version control
    system such as Git. A little earlier, we opened a data file and PyCharm immediately
    complained about the size of the file. By modern standards, an 8 MB file isn’t
    very big. However, consider that most code files, PyCharm’s raison d’être, are
    on average well under 100K in size. If your files are very large, that’s a code
    smell and you should figure out what you’re doing wrong.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们刚刚简要地讨论了数据收集，我希望在谈论使用数据在版本控制系统如Git中时，你能再次宽容我。早些时候，我们打开了一个数据文件，PyCharm立即对文件的大小提出了抱怨。按照现代标准，一个8MB的文件并不大。然而，考虑到大多数代码文件，PyCharm的宗旨，平均大小都在100K以下。如果你的文件非常大，那可能是一个代码问题，你应该找出你做错了什么。
- en: Here, we’re presenting PyCharm with a file that is about 8,000% bigger than
    what it is used to. Git is also primarily used to deal with small files coming
    out of an IDE. I’m bringing this up because there is somewhat of a crisis of reproducibility
    in the data science and scientific computing community. This is when one data
    team can extract a specific insight from a dataset but others cannot, even when
    using the same methods. Many instances of this are because the data used across
    these different teams is not compatible with each other. Some might be using the
    same but outdated dataset, while other datasets might have been collected from
    a different source.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们向 PyCharm 提供了一个大约比其通常处理的文件大 8,000% 的文件。Git 也主要用于处理来自 IDE 的小文件。我提这一点是因为数据科学和科学计算社区中存在某种程度的可重复性问题。这就是一个数据团队能够从一个数据集中提取特定的见解，但其他人即使使用相同的方法也无法做到。许多这种情况的发生是因为不同团队之间使用的数据不兼容。有些人可能使用的是相同的但过时的数据集，而其他数据集可能来自不同的来源。
- en: Version control for datasets is an important topic to consider. Git normally
    has a hard limit of 100 MB for any file, and I can tell you from experience there
    is an upper limit to the total size of your projects in total on GitHub. The same
    limitations exist in other version control systems. I used to teach game development
    with a tool called Unity 3D, and we were always struggling with these limitations
    since video games typically have very large assets in the projects that aren’t
    necessarily code, but that could benefit from revision control.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的版本控制是一个需要考虑的重要话题。Git 通常对任何文件都有 100 MB 的硬限制，我可以根据经验告诉您，GitHub 上您项目的总大小有一个上限。在其他版本控制系统中也存在相同的限制。我过去曾用名为
    Unity 3D 的工具教授游戏开发，我们总是因为这些限制而苦苦挣扎，因为视频游戏项目通常包含非常大的资源，这些资源不一定是代码，但可以从版本控制中受益。
- en: Using Git Large File Support
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Git 大文件支持
- en: Since the problem is endemic, Git (and others) have added the ability to track
    larger assets through **Git Large File Support** (**Git LFS**). When we add a
    file using Git LFS, the system will replace that file with a pointer that simply
    references it. When the file is placed under version control, Git will only have
    a reference to the actual file, which is now stored in an external filesystem,
    possibly on another server. Git LFS allows us to apply version control to large
    files (in this case, datasets) with Git, without actually storing the files in
    Git.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于问题普遍存在，Git（以及其他）添加了通过 **Git 大文件支持**（**Git LFS**）跟踪大型资源的能力。当我们使用 Git LFS 添加文件时，系统会用一个简单地引用它的指针来替换该文件。当文件被置于版本控制之下时，Git
    只会保留对实际文件的引用，该文件现在存储在外部文件系统中，可能位于另一台服务器上。Git LFS 允许我们使用 Git 对大型文件（在这种情况下，是数据集）进行版本控制，而实际上并不存储这些文件在
    Git 中。
- en: 'This feature is normally installed with modern Git installers. *Figure 14**.2*
    shows me installing Git for Windows, where LFS is part of the default installation:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 此功能通常与现代 Git 安装程序一起安装。*图 14.2* 展示了我安装 Git for Windows 的过程，其中 LFS 是默认安装的一部分：
- en: '![Figure 14.2: LFS is installed by default in Windows](img/B19644_14_02.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.2：在 Windows 中，LFS 默认已安装](img/B19644_14_02.jpg)'
- en: 'Figure 14.2: LFS is installed by default in Windows'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.2：在 Windows 中，LFS 默认已安装
- en: 'You can check your installation, regardless of which OS you use, using the
    command line:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用命令行检查您的安装，无论您使用的是哪种操作系统：
- en: '[PRE1]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'My result from running this command in GitBash in Windows 11 is shown in *Figure
    14**.3*:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 Windows 11 的 GitBash 中运行此命令的结果如 *图 14.3* 所示：
- en: '![Figure 14.3: If LFS is installed, it should tell you the version number](img/B19644_14_03.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.3：如果已安装 LFS，它应该会告诉您版本号](img/B19644_14_03.jpg)'
- en: 'Figure 14.3: If LFS is installed, it should tell you the version number'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.3：如果已安装 LFS，它应该会告诉您版本号
- en: 'The only reason I have Windows (besides Ghost Recon and Steam in general) is
    so I can use Microsoft Word to write this book. This wasn’t my idea. I was going
    to write the whole thing in raw LaTeX using vi. Not vim. Not neovim. Original
    gangsta vi, which I naturally would be compiling from source. My editor said no.
    She’s so super polite! If our roles were reversed, who knows what would have been
    said? Anyway, the rest of my real work is done on **Pop_OS**, which is a variant
    of Ubuntu Linux. When I throw the command into that environment, I get a less
    hospitable answer, as shown in *Figure 14**.4*:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我拥有 Windows 的唯一原因（除了一般的 Ghost Recon 和 Steam）是，我可以使用 Microsoft Word 来编写这本书。这不是我的主意。我本来打算用原始的
    LaTeX 和 vi 编写整个内容。不是 vim。不是 neovim。原始的 gangsta vi，我会自然地从源代码编译它。我的编辑器说不行。她非常礼貌！如果我们的角色互换，谁知道会说什么呢？无论如何，我剩下的真正工作是在
    **Pop_OS** 上完成的，这是一个 Ubuntu Linux 的变种。当我把命令扔进那个环境时，我得到了一个不那么友好的回答，如 *图 14**.4*
    所示：
- en: '![Figure 14.4: My installer is not modern enough to have LFS pre-installed](img/B19644_14_04.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.4：我的安装程序不够现代，无法预装 LFS](img/B19644_14_04.jpg)'
- en: 'Figure 14.4: My installer is not modern enough to have LFS pre-installed'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4：我的安装程序不够现代，无法预装 LFS
- en: 'I don’t have it! I have to install it using these commands:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有！我必须使用以下命令来安装它：
- en: '[PRE2]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With that done, I can test again:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，我可以再次测试：
- en: '![Figure 14.5: Success! If you use some other Linux distribution, check your
    package management system for the git-lfs package if your installation lacks it](img/B19644_14_05.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.5：成功！如果你使用的是其他 Linux 分发版，如果你的安装中没有 git-lfs 软件包，请检查你的包管理系统](img/B19644_14_05.jpg)'
- en: 'Figure 14.5: Success! If you use some other Linux distribution, check your
    package management system for the git-lfs package if your installation lacks it'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.5：成功！如果你使用的是其他 Linux 分发版，如果你的安装中没有 git-lfs 软件包，请检查你的包管理系统
- en: The `git-lfs` package specific to your Linux distribution.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于你 Linux 分发的特定 `git-lfs` 软件包。
- en: Using Git LFS
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Git LFS
- en: 'We’re getting a little ahead of ourselves. If you’re going to follow along
    in this little sidetrack exercise, it would be best if you made a new folder somewhere
    outside of this book’s code repository. Let’s assume you’ve something like this
    in your OS’s terminal:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能有点过于急切了。如果你打算跟随这个小小的旁路练习，最好在本书代码库之外创建一个新的文件夹。让我们假设你在操作系统的终端中已经有了类似这样的文件夹：
- en: '[PRE3]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This series of commands will work in any of the popular OSs (Windows, macOS,
    or Linux). If you are using Windows, this series of commands can be run in PowerShell
    and assumes you have the Git client for Windows installed. The installer is available
    at [https://git-scm.com/downloads](https://git-scm.com/downloads).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这一系列命令可以在任何流行的操作系统（Windows、macOS 或 Linux）中运行。如果你使用 Windows，这一系列命令可以在 PowerShell
    中运行，并假设你已经安装了 Windows 的 Git 客户端。安装程序可在 [https://git-scm.com/downloads](https://git-scm.com/downloads)
    获取。
- en: The first command takes you to your `home` folder. The second creates a new
    folder called `git-lfs-test`. Next, we change the directory to the `git-lfs-test`
    folder we just made and we initialize a new repository. Now, we are ready to set
    up support for Git LFS.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令将你带到你的 `home` 文件夹。第二个命令创建一个名为 `git-lfs-test` 的新文件夹。接下来，我们将目录更改为我们刚刚创建的
    `git-lfs-test` 文件夹，并初始化一个新的仓库。现在，我们已经准备好设置 Git LFS 的支持。
- en: Don’t forget the chapter files are already in a Git repo
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记章节文件已经在 Git 仓库中
- en: If you’re following along with this chapter’s source, don’t forget that the
    files are already in a Git repo. Creating a second repo within the existing repo
    won’t work. If you want to practice, make a completely separate folder outside
    of this book’s repo, and copy the project files into your folder. When you copy,
    you specifically want to avoid copying the `.git` folder into your target.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在跟随本章的源代码，别忘了文件已经在一个 Git 仓库中。在现有仓库内创建第二个仓库是不行的。如果你想练习，请在这个书的仓库之外创建一个完全独立的文件夹，并将项目文件复制到你的文件夹中。当你复制时，你特别想避免将
    `.git` 文件夹复制到你的目标文件夹中。
- en: 'In our project, we’re going to use Git LFS to track files of a given extension,
    specifically text files with the `.txt` extension. Given these files are naturally
    plain text, you could get creative with the extension without affecting how they
    are used, but we’ll stick to just `.txt`. I’ll run this command in my terminal
    window:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的项目中，我们将使用 Git LFS 来跟踪具有特定扩展名的文件，特别是具有 `.txt` 扩展名的文本文件。鉴于这些文件自然是纯文本，你可以对扩展名进行一些创意性的处理，而不会影响它们的使用方式，但我们将坚持使用
    `.txt`。我将在我的终端窗口中运行这个命令：
- en: '[PRE4]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can see my test run in *Figure 14**.6*:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 *图 14**.6* 中看到我的测试运行：
- en: '![Figure 14.6: Git LFS is now tracking all files with the .txt extension](img/B19644_14_06.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图14.6：Git LFS现在正在跟踪所有.txt扩展名的文件](img/B19644_14_06.jpg)'
- en: 'Figure 14.6: Git LFS is now tracking all files with the .txt extension'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6：Git LFS现在正在跟踪所有.txt扩展名的文件
- en: 'To complete my LFS test, I’ll copy the file we examined earlier, `0EA27ICBLF_1607.txt`,
    from the `Tappy Data` folder into the `git-lfs-test` folder we’re using for the
    experiment. Just to be clear, *Figure 14**.7* shows my folder. We’re not doing
    this within any sub-folder within this book’s code repository since creating a
    repository inside another repository is a big no-no:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成我的LFS测试，我将把之前检查过的文件`0EA27ICBLF_1607.txt`从`Tappy Data`文件夹复制到我们用于实验的`git-lfs-test`文件夹。为了清楚起见，*图14**.7*显示了我的文件夹。我们不会在本书代码存储库的任何子文件夹内进行此操作，因为在一个存储库内部创建存储库是大忌：
- en: '![Figure 14.7: I’ve copied 0EA27ICBLF_1607.txt into the git-lfs-test folder](img/B19644_14_07.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图14.7：我已经将0EA27ICBLF_1607.txt复制到git-lfs-test文件夹](img/B19644_14_07.jpg)'
- en: 'Figure 14.7: I’ve copied 0EA27ICBLF_1607.txt into the git-lfs-test folder'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.7：我已经将0EA27ICBLF_1607.txt复制到git-lfs-test文件夹
- en: 'Now, let’s add the newly copied text file to the repository:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将新复制的文本文件添加到存储库中：
- en: '[PRE5]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We covered the first two Git commands extensively in [*Chapter 5*](B19644_05.xhtml#_idTextAnchor112).
    The last command will list all files being tracked by LFS in this repository.
    You can see my output in *Figure 14**.8*:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第五章*](B19644_05.xhtml#_idTextAnchor112)中详细介绍了前两个Git命令。最后一个命令将列出在这个存储库中由LFS跟踪的所有文件。您可以在*图14**.8*中看到我的输出：
- en: '![Figure 14.8: I can see that my text file is being tracked by LFS](img/B19644_14_08.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图14.8：我可以看到我的文本文件正在被LFS跟踪](img/B19644_14_08.jpg)'
- en: 'Figure 14.8: I can see that my text file is being tracked by LFS'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.8：我可以看到我的文本文件正在被LFS跟踪
- en: 'You now understand how to use Git LFS to track large files. If this were a
    real repository we were interested in keeping, there’s one last thing we’d need
    to do. When we commanded Git to track our text files, a special file called `.gitattributes`
    was created on our behalf. We should add and commit that file:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经了解了如何使用Git LFS跟踪大文件。如果这是一个我们感兴趣的真正存储库，我们还需要做最后一件事。当我们命令Git跟踪我们的文本文件时，代表我们创建了一个名为`.gitattributes`的特殊文件。我们应该添加并提交该文件：
- en: '[PRE6]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You’re all set! Let’s move on to our next formal step in the process of data
    analysis, which entails data cleansing and preprocessing.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经准备好了！让我们继续到数据分析流程中的下一个正式步骤，这包括数据清洗和预处理。
- en: Data cleansing and preprocessing
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据清洗和预处理
- en: As I mentioned earlier, we’ve been pretty lucky. Some of the data my team works
    with can be downright filthy. When we use terms such as “dirty," “filthy,” and
    “cleansing” concerning data, what we’re talking about is addressing the format
    of the data, as well as the fitness of the data for processing. Data is only useful
    if it’s in a format we can work with. Structured data is what we always prefer.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如我之前提到的，我们非常幸运。我们团队处理的一些数据可能非常糟糕。当我们使用“脏”、“污秽”和“清洗”等术语来描述数据时，我们谈论的是解决数据的格式问题，以及数据适合处理的情况。如果数据格式是我们能处理的形式，那么数据才有用。结构化数据是我们始终首选的。
- en: Structured data refers to data that is split into identifiable fields. We’ve
    seen comma-separated and tab-separated text. Other examples of structured data
    include formats such as XML, JSON, Parquet, and HDF5\. The first two, XML and
    JSON, are very common and have the advantage of being text formats. The latter
    two, Parquet and HDF5, are binary files and are specialized for storing larger
    datasets than would be comfortable when working with text. As we’ve seen, most
    tools, including PyCharm, buckle when they try to read very large text files.
    You need tools specialized for working with large files if you want to peruse
    or edit them in place.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化数据指的是分割成可识别字段的数据。我们已经看到了逗号分隔和制表符分隔的文本。结构化数据的其他示例包括XML、JSON、Parquet和HDF5等格式。前两种，XML和JSON，非常常见，并且具有文本格式的优势。后两种，Parquet和HDF5，是二进制文件，专门用于存储比文本处理更舒适的大数据集。正如我们所见，包括PyCharm在内的大多数工具在尝试读取非常大的文本文件时都会崩溃。如果您想就地查看或编辑这些文件，则需要专门用于处理大文件的工具。
- en: 'When I talk about dirty versus clean data, I’m looking for things such as missing
    or invalid field data. Recall our earlier data sample:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当我谈论脏数据与干净数据时，我寻找的是诸如缺失或无效字段数据等问题。回想我们之前的数据样本：
- en: '[PRE7]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `UPDRS` field is marked as unknown. This isn’t ideal. If the field is included,
    I’d like to see a value there. In this case, there’s no way to backfill it, but
    in a perfect world, that might be a candidate for an exercise in data cleansing.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`UPDRS`字段标记为未知。这并不理想。如果包含该字段，我希望看到那里的值。在这种情况下，无法进行回填，但在一个完美的世界里，这可能是一个数据清洗练习的候选者。'
- en: A toxic data example peripherally involving ninjas
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个涉及忍者的有毒数据示例
- en: The most relatable example I’ve ever encountered with dirty – or in this case,
    toxic – data came from the corporate world rather than from a data science experiment.
    My company was consulting for a large aviation company, which is also a contractor
    for the US Department of Defense. I won’t be naming real names here because I
    am generally averse to government ninjas kicking in my door at 2 A.M., or worse,
    being flagged for a tax audit for what I’ve written here. So, we’ll keep this
    more or less theoretical.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我遇到的最相关的例子是来自企业界，而不是来自数据科学实验中的脏数据——或者在这个案例中，是有毒的数据。我的公司正在为一家大型航空公司提供咨询服务，该公司也是美国国防部的承包商。在这里我不会提及真实姓名，因为我通常不喜欢政府在凌晨2点踢开我的门，或者更糟糕的是，因为我在这里写的内容而被标记为税务审计。所以，我们将保持这个理论上的讨论。
- en: The aviation company did business with lots of vendors, and when you do business
    with vendors at scale, it isn’t uncommon to see discounts applied to whatever
    you might be buying based on volume. If you or I go to Hammers R Us and buy a
    hammer, we might pay $12.95 for a hammer. But if the aviation company buys 5,000
    hammers across many orders in a single quarter, they might get a discount of up
    to 60%. It is the aviation company’s job to track what they buy and from whom
    so that they can cash in on whatever bulk purchasing deals their company has negotiated
    with their suppliers.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这家航空公司与许多供应商做生意，当你大规模地与供应商做生意时，根据购买量对所购买的商品应用折扣并不罕见。如果你或我去Hammers R Us买一把锤子，我们可能为这把锤子支付12.95美元。但是，如果航空公司在一个季度内通过多个订单购买了5,000把锤子，他们可能会获得高达60%的折扣。航空公司的任务是跟踪他们购买的商品和供应商，以便他们可以兑现公司与供应商协商的任何批量购买协议。
- en: When it’s time to run the discount reports, an accounting analyst might query
    a database filled with data entered by hundreds or even thousands of people working
    in the field on behalf of the aviation company. Since these operatives are human,
    their ability to enter clean, standardized data into a poorly designed system
    without any kind of validation is virtually nil. In this case, the software used
    for order entry allowed users to type the name of the company into a text field,
    which was never validated against any sort of approved vendors list.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 当是时候运行折扣报告时，一个会计分析师可能会查询一个由数百甚至数千人在代表航空公司工作的领域输入的数据数据库。由于这些操作员是真人，他们在没有任何验证的情况下将干净、标准化的数据输入到一个设计不良的系统中的能力几乎为零。在这种情况下，用于订单输入的软件允许用户在一个文本字段中输入公司的名称，而这个字段从未与任何批准的供应商名单进行过验证。
- en: One guy enters a purchase with the vendor listed as “Hammers R Us.” Another
    enters it as "HRUS" (naturally that’s the stock symbol), and another as "H.R.U.S."
    Someone else misspells it as “Hammers Are Us” and yet another as “Hammers-R-Us.”
    Now, we have five different references to the same company, which dilutes our
    ability to figure out how much of a discount we can ask for. If there are 5 spellings
    and the purchase quantities are even across 5 orders, each order will only be
    for 1,000 hammers and our discount is only 20% instead of 60%. Our toxic data
    problem is costing the company serious money!
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个人将采购记录为“Hammers R Us”的供应商。另一个人将其输入为"HRUS"（自然地，那是股票代码），还有一个人将其输入为"H.R.U.S。"另一个人将其拼写错误为“Hammers
    Are Us”，还有另一个人将其拼写为“Hammers-R-Us。”现在，我们对同一家公司有五种不同的引用，这削弱了我们确定我们可以要求多少折扣的能力。如果有五种拼写，并且采购数量在五份订单中平均分配，那么每份订单将只有1,000把锤子，我们的折扣只有20%，而不是60%。我们的有毒数据问题正在使公司损失大量金钱！
- en: The aviation company hired my company to do **data cleansing**. It was our job
    to clean all the data up and standardize all the references to Hammers R Us. The
    project was successful for us because all we had to do was charge the client a
    few dollars less than what they were losing, which was substantial. Then, we helped
    them fix their software to make it impossible to enter toxic data after that.
    It was a win for everyone! I even got a free hammer from Hammers R Us, at least
    in my version of the story that entails me not getting audited or visited by ninjas.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 飞机制造公司雇佣了我的公司进行**数据清洗**。我们的任务是清理所有数据并标准化所有对Hammers R Us的引用。对我们来说，这个项目是成功的，因为我们只需向客户收取比他们损失少几美元的费用，而这笔费用相当可观。然后，我们帮助他们修复软件，使其之后无法输入有毒数据。这对每个人都是个胜利！我甚至从Hammers
    R Us得到了一把免费的锤子，至少在我的故事版本中是这样的，这个版本意味着我没有被审计或遭到忍者的袭击。
- en: Exploratory analysis in PyCharm
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在PyCharm中进行探索性分析
- en: While data cleansing in a data science project isn’t usually financially profitable,
    it is a very necessary step. As you begin to examine your data for the first time,
    you will often hear this process referred to as **exploratory data analysis**,
    where we are exploring and analyzing the data at the same time. What we’re doing
    though is taking stock to see what we can do with our data. It would be very difficult
    to perform a tabulation, such as computing sums, means, and standard deviations,
    without first making sure all our necessary data is both there and in a usable
    numerical format. We might also look for outliers. Maybe a hammer order was misentered
    and we have an order for a million hammers that was canceled via a separate transaction.
    These kinds of outliers would likely need to be removed before we begin our analysis
    in earnest.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在数据科学项目中数据清洗通常不会带来经济上的利润，但它是一个非常必要的步骤。当你开始第一次检查你的数据时，你经常会听到这个过程被称为**探索性数据分析**，在这个过程中我们同时探索和分析数据。但我们实际上是在盘点，看看我们能用我们的数据做什么。如果没有首先确保所有必要的数据都存在并且以可用的数值格式，那么执行诸如计算总和、平均值和标准差之类的制表操作将会非常困难。我们可能还会寻找异常值。也许一个锤子订单被误输入，我们有一个取消的订单，该订单为100万把锤子。这些类型的异常值在我们真正开始分析之前可能需要被移除。
- en: 'In the case of our data, a few things are bothering me:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们数据的情况下，有几件事情让我感到烦恼：
- en: The study says it examined 103 subjects; however, there are 277 user files in
    the `Archived users` folder. I suspect that not every user has matching collected
    data. We’ll need a way to check that each user in the `Archived users` folder
    has a related dataset in the `Tappy` `Data` folder.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究称它检查了103名受试者；然而，在`存档用户`文件夹中有277个用户文件。我怀疑并非每个用户都有匹配的收集数据。我们需要一种方法来检查`存档用户`文件夹中的每个用户在`Tappy`
    `数据`文件夹中都有一个相关的数据集。
- en: Our raw data is purely textual, which means when we import it into Python by
    reading the files, the data will be expressed as strings. This is not ideal for
    data analysis. I’d like numbers to be converted into number types, dates into
    date types, Booleans into Boolean, and so on.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的数据是纯文本的，这意味着当我们通过读取文件将其导入Python时，数据将以字符串的形式表达。这对于数据分析来说并不理想。我希望数字能转换为数字类型，日期转换为日期类型，布尔值转换为布尔值，等等。
- en: The `Impact` column should be fully standardized to account for missing values
    in the data. Naturally, this applies to any other column where I can see or suspect
    the data might contain missing values.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Impact`列应该完全标准化，以处理数据中的缺失值。自然地，这也适用于任何我可以看到或怀疑可能包含缺失值的列。'
- en: We can convert some of the fields in the user datasets into a binary format
    to make analysis easier. Specific examples include `Parkinsons`, `Tremors`, `Levadopa`,
    `DA`, `MAOB`, and `Other`.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将用户数据集中的某些字段转换为二进制格式，以使分析更容易。具体例子包括`Parkinsons`、`Tremors`、`Levadopa`、`DA`、`MAOB`和`Other`。
- en: We can use a process called one-hot encoding to more easily process the fields
    labeled `Sided`, `UPDRS`, and `Impact`. I’ll go into detail on one-hot encoding
    once we’re ready to perform this process.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用一种称为独热编码的过程来更容易地处理标记为`Sided`、`UPDRS`和`Impact`的字段。一旦我们准备好执行这个过程，我会详细介绍独热编码。
- en: This is just what I see at first glance. There may be other opportunities for
    cleansing that present themselves once we get underway.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是我第一眼看到的。一旦我们开始着手，可能会有其他清洁的机会出现。
- en: Reading the data from text files
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从文本文件中读取数据
- en: 'Let’s look at what we need to do with preprocessing our data. If you open the
    `data_clean.py` file, you’ll see our clean-up script, which uses the cell mode
    discussed in [*Chapter 12*](B19644_12.xhtml#_idTextAnchor298). Our first cell
    handles our imports:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们需要对数据进行预处理的工作。如果你打开 `data_clean.py` 文件，你会看到我们的清理脚本，该脚本使用了在 [*第12章*](B19644_12.xhtml#_idTextAnchor298)
    中讨论的单元格模式。我们的第一个单元格处理我们的导入：
- en: '[PRE8]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If you’re following along with this chapter’s code, don’t forget to create a
    virtual environment using the `requirements.txt` file. Here we’re importing a
    few old friends. `numpy` and `pandas` are standard analysis libraries. The `os`
    package will be needed for working with the file directories, and the `gc` package
    allows us to control the **garbage collection** (**GC**) process. If you’ve never
    heard of this before, it is because most programming languages, including Python,
    handle GC automatically. One common occurrence of GC happens when a variable,
    which will have memory allocated to store its value, goes out of scope and is
    no longer needed. In the C programming language, you would need to allocate that
    memory yourself before you could use the variable. When you were finished with
    the variable, you’d need to deallocate that memory “by hand.” If you didn’t, you’d
    be using more memory than you needed, and that’s the kind of thing that gets you
    uninvited to the Pi Day pizza party.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在跟随本章的代码，别忘了使用 `requirements.txt` 文件创建一个虚拟环境。在这里，我们导入了一些老朋友。`numpy` 和 `pandas`
    是标准的分析库。`os` 包将用于处理文件目录，而 `gc` 包允许我们控制 **垃圾回收**（**GC**）过程。如果你之前从未听说过这个，那是因为大多数编程语言，包括Python，都自动处理GC。GC的一个常见发生情况是当一个变量，它将分配内存来存储其值，超出作用域并且不再需要时。在C编程语言中，你需要在能够使用变量之前自己分配该内存。当你完成变量后，你需要“手动”释放该内存。如果你不这样做，你会使用比所需的更多内存，这就是你被邀请参加Pi
    Day披萨派对的原因。
- en: Most modern languages handle this allocation and deallocation automatically
    in a process called GC. However, there are times, especially when you are loading
    and manipulating large amounts of data, that it makes sense to take a more active
    role when the garbage gets taken out, which frees up memory for further exploits.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代语言在称为GC（垃圾回收）的过程中自动处理这种分配和释放。然而，有时，尤其是在你加载和处理大量数据时，当垃圾被清理出来时，采取更积极的角色是有意义的，这可以释放内存以供进一步使用。
- en: 'With our imports out of the way, let’s read some data with the following cell:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的导入完成之后，让我们使用以下单元格读取一些数据：
- en: '[PRE9]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `os.listdir` method takes our `data/Archived users/` folder and gives us
    an iterable list of files from that folder. This is important because we need
    a list of the IDs for each user, which is contained in the filename.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`os.listdir` 方法接受我们的 `data/Archived users/` 文件夹，并给我们一个来自该文件夹的文件可迭代列表。这很重要，因为我们需要一个包含在每个用户文件名中的ID列表。'
- en: We create a variable called `user_set_v1`, and we instantiate a set. In Python,
    a `set` is a built-in data type that represents an unordered collection of unique
    elements. This means that a set cannot contain duplicate values, and the order
    in which elements are stored is not guaranteed to be the same as the order in
    which they were added.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个名为 `user_set_v1` 的变量，并实例化了一个集合。在Python中，`set` 是一个内置的数据类型，它表示一个无序的唯一元素集合。这意味着集合不能包含重复的值，并且存储元素的顺序不一定与它们被添加的顺序相同。
- en: 'We fill this `set` with data using a `map` statement, which iterates over our
    list of files in the `Archived users` folder. For each iteration of the map, we
    use a lambda function to extract a portion of each filename in `user_file_list`.
    Specifically, it takes a substring from the 5th to the 15th character of each
    filename. This is intended to extract user IDs from the filenames. Next, we’ll
    need to do roughly the same thing to the `Tappy` `Data` files:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `map` 语句填充这个 `set`，该语句遍历 `Archived users` 文件夹中的文件列表。对于 `map` 的每次迭代，我们使用一个lambda函数从
    `user_file_list` 中的每个文件名中提取一部分。具体来说，它从每个文件名的第5个到第15个字符提取子字符串。这是从文件名中提取用户ID的目的。接下来，我们还需要对
    `Tappy` 数据文件做类似的事情：
- en: '[PRE10]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now, we have two sets, one from the user files and one from the Tappy data files.
    We need to find the intersection between the sets.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有两个集合，一个来自用户文件，一个来自Tappy数据文件。我们需要找到这两个集合的交集。
- en: In **set theory**, the term *intersection* refers to an operation that combines
    two sets to create a new set containing only the elements that are common to both
    of the original sets. The intersection of two sets, often denoted by the ∩ symbol,
    represents the overlap or shared elements between the sets.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **集合论** 中，术语 *交集* 指的是一种操作，它将两个集合组合起来创建一个新集合，该集合只包含两个原始集合共有的元素。两个集合的交集，通常用
    ∩ 符号表示，代表集合之间的重叠或共享元素。
- en: Mathematically, if you have two sets, A and B, the intersection of A and B is
    a new set that contains all the elements that are both in set A and set B.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，如果你有两个集合，A 和 B，A 和 B 的交集是一个新集合，它包含所有同时存在于集合 A 和集合 B 中的元素。
- en: I know all you math geeks out there love your symbols, and I also know that
    your brains are wired to scan for patterns rather than word-for-word reading,
    so I’ll help you out. Symbolically, it is represented as A ∩ B = {x ∣ x ∈ A and
    x ∈ B}.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道你们这些数学爱好者都喜欢你们的符号，我也知道你们的头脑被编程去寻找模式而不是逐字阅读，所以我会帮你们。从符号上讲，它表示为 A ∩ B = {x
    ∣ x ∈ A and x ∈ B}。
- en: In the context of programming in Python, the `intersection()` method of `set`
    performs this mathematical operation. Given two sets, it returns a new set containing
    only the elements that exist in both sets.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 编程的背景下，`set` 类型的 `intersection()` 方法执行这个数学运算。给定两个集合，它返回一个新集合，只包含两个集合中都存在的元素。
- en: 'For example, let’s say you have the following two sets:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你有两个以下集合：
- en: Set A = {1,2,3,4}
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集合 A = {1,2,3,4}
- en: Set B = {3,4,5,6}
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集合 B = {3,4,5,6}
- en: 'The intersection of A and B would be A ∩ B = {3, 4} since 3 and 4 are in both
    sets. In our case, it is important to get the intersection because the study text
    stated it examined 103 subjects, yet there are 227 subjects listed in the `Archived
    users` folder. I could make a list, and then go through and visually compare the
    contents of the `Tappy Data` folder to make sure everyone is accounted for, but
    that would be boring, time-consuming, and error-prone. I’ll just have Python do
    it for me:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 集合 A 和 B 的交集将是 A ∩ B = {3, 4}，因为 3 和 4 都在两个集合中。在我们的情况下，获取交集很重要，因为研究文本指出它考察了
    103 个受试者，但在“存档用戶”文件夹中列出了 227 个受试者。我可以制作一个列表，然后逐一检查“Tappy Data”文件夹的内容，以确保每个人都已计入，但这会很无聊，耗时且容易出错。我将让
    Python 帮我完成：
- en: '[PRE11]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Don’t you just love Python’s one-liners? Sure, there was some setup (hee hee,
    `intersection` method and we have our new set, which is all funk and no junk!
    Let’s see what we’ve got by printing out the length:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 你难道不喜欢 Python 的单行代码吗？当然，有一些设置（嘿嘿，`intersection` 方法，我们得到了一个新集合，它既有趣又无杂乱！让我们通过打印长度来看看我们得到了什么：
- en: '[PRE12]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'I’m going to run the first two cells using the green arrows indicated in *Figure
    14**.9*:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用 *图 14**.9* 中指示的绿色箭头运行前两个单元格：
- en: '![Figure 14.9: I’m running the first two cells we’ve covered so far using the
    green arrows at the top of each cell](img/B19644_14_09.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.9：我正在使用每个单元格顶部的绿色箭头运行到目前为止我们所覆盖的前两个单元格](img/B19644_14_09.jpg)'
- en: 'Figure 14.9: I’m running the first two cells we’ve covered so far using the
    green arrows at the top of each cell'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.9：我正在使用每个单元格顶部的绿色箭头运行到目前为止我们所覆盖的前两个单元格
- en: 'The result of the run is shown in *Figure 14**.10*:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 运行结果如 *图 14**.10* 所示：
- en: '![Figure 14.10: I have a relatively clean list of users after our first steps
    of cleaning the data](img/B19644_14_10.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.10：在数据清理的第一步之后，我得到了一个相对干净的用戶列表](img/B19644_14_10.jpg)'
- en: 'Figure 14.10: I have a relatively clean list of users after our first steps
    of cleaning the data'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.10：在数据清理的第一步之后，我得到了一个相对干净的用戶列表
- en: We got 217 users with correlated data between the two sets, so we’ve managed
    to eliminate 60 user files we aren’t going to use. The number doesn’t match the
    103 subjects reported in the test, but that’s OK – the day is still young, and
    we might eliminate more later. Even if we don’t, there might be other reasons
    to eliminate properly matched data later on. Our new set can be used to iterate
    over data in either data folder since the filenames in both use the ID as a major
    part of the filename. This will be very useful in the next step in our data preparation
    process.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了两个集合之间相关数据的 217 个用戶，所以我们成功地消除了我们不会使用的 60 个用户文件。这个数字与测试中报告的 103 个受试者不符，但没关系——日子还很长，我们可能会稍后消除更多。即使我们不会，也可能有其他原因稍后消除正确匹配的数据。我们的新集合可以用来遍历任一数据文件夹中的数据，因为两个文件夹中的文件名都使用
    ID 作为文件名的主要部分。这将在我们数据准备过程的下一步中非常有用。
- en: In *Figure 14**.10*, I’m clicking on the view link to see my list with the **SciView**
    panel. It isn’t particularly exciting since it’s just a list of IDs, but the ability
    to easily inspect as we work without performing additional prints is very useful.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 14**.10* 中，我点击了查看链接来查看带有 **SciView** 面板的我的列表。它并不特别令人兴奋，因为它只是一个 ID 列表，但能够在不执行额外打印的情况下轻松检查我们正在工作的内容是非常有用的。
- en: Getting our data into a pandas DataFrame
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将数据导入 pandas DataFrame
- en: Our next cell contains code designed to take our loaded dataset and pull that
    data into a pandas DataFrame. pandas is a library that allows for easy analysis
    of tabular data and even provides a lot of very useful methods for loading data
    directly into a DataFrame, which is a tabular structure within pandas. A DataFrame
    object is a lot like an in-memory spreadsheet without the editor. You can perform
    all kinds of calculations with minimal effort.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下一个单元格中的代码是为了将加载的数据集拉入 pandas DataFrame。pandas 是一个库，它允许轻松分析表格数据，甚至提供了许多非常实用的方法，可以直接将数据加载到
    DataFrame 中，DataFrame 是 pandas 中的一个表格结构。DataFrame 对象很像一个没有编辑器的内存中的电子表格。你可以用最小的努力执行各种计算。
- en: 'Let’s examine the code from the next cell:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查下一单元格中的代码：
- en: '[PRE13]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Don’t forget that `#%%` is a special formatting comment in PyCharm. It isn’t
    part of Python. We covered this back in [*Chapter 13*](B19644_13.xhtml#_idTextAnchor318).
    These characters are used to split cells in our code, which allows us to use one
    script but operate step-wise from one cell to the next. At the end of the day,
    it is still a comment, so we should include some documentation to explain what
    is happening in the cell.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记在 PyCharm 中 `#%%` 是一个特殊的格式化注释。它不是 Python 的一部分。我们之前在 [*第 13 章*](B19644_13.xhtml#_idTextAnchor318)
    中讨论过这一点。这些字符用于分割我们的代码中的单元格，这使得我们可以使用一个脚本，但可以逐步从一个单元格操作到下一个单元格。最终，它仍然是一个注释，因此我们应该包含一些文档来解释单元格中正在发生的事情。
- en: 'Next, we’ll create a function that reads the data from the files in the `Archived`
    `users` folder:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个函数，从 `Archived` `users` 文件夹中的文件读取数据：
- en: '[PRE14]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The function simply takes a filename as an argument and opens the file. It
    then reads the file line by line. For each line, we’re using the `split` string
    function to split the line into chunks as a list. This allows us to grab only
    the parts we need. As you may recall, a few lines of data for these files look
    like this:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数简单地接受一个文件名作为参数并打开文件。然后逐行读取文件。对于每一行，我们使用 `split` 字符串函数将行分割成块作为列表。这使得我们可以只获取我们需要的部分。如您所回忆的那样，这些文件的一些数据行看起来像这样：
- en: '[PRE15]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The separation between the field name and the data is a colon and a space (`:`
    ). We’re using that as our splitter, so if you split `"BirthYear: 1952".split('':
    '')`, you’ll get back a list: `["BirthYear", "1952"]`. We don’t care about the
    field name right now, we care about the value. To get that, we grab `[1]`, which
    gives us `"1952"`, which is the value, but there is a newline character at the
    end of each line, and that was included in our split. The last thing we do, then,
    before moving on with the next iteration, is clear off the newline character with
    the Python split operator, `[:-1]`, which effectively says “go to the end of the
    string,” as evidenced by the fact that the number is after the colon, “and slice
    off one character from the end,” as denoted by the negative number. Rather than
    using a loop, we’ve used list comprehension, which is an alternative way to iterate
    a list. These are generally more performant than a normal `for` loop. The result
    of the list comprehension is a new list that contains only the data we want.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '字段名和数据之间的分隔符是一个冒号和一个空格（`:`）。我们将其用作分隔符，因此如果你使用 `"BirthYear: 1952".split('':
    '')`，你会得到一个列表：`["BirthYear", "1952"]`。我们现在不关心字段名，我们关心的是值。为了得到这个值，我们取 `[1]`，这给我们
    `"1952"`，这是值，但每行末尾都有一个换行符，并且这个换行符被包含在我们的分割中。在我们继续下一个迭代之前，我们最后做的事情是使用 Python 的分割操作符
    `[:-1]` 清除换行符，这实际上意味着“到达字符串的末尾”，正如事实所证明的那样，数字在冒号之后，并且“从末尾切掉一个字符”，正如负数所表示的那样。我们不是使用循环，而是使用了列表推导，这是一种迭代列表的替代方法。这些通常比普通的
    `for` 循环更高效。列表推导的结果是一个新列表，它只包含我们想要的数据。'
- en: 'The next few lines are setting us up for filling in a pandas DataFrame. First,
    we get a list of files in the `Archived` `users` folder:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的几行代码是为了填充 pandas DataFrame 而设置的。首先，我们获取 `Archived` `users` 文件夹中的文件列表：
- en: '[PRE16]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we create a list of fields. We’ve already set up a function to rip the
    data out of the files without the field name. Ripping the names at the same time
    might add a lot of time since it is the same thing over and over; this is simply
    more efficient:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个字段列表。我们已经设置了一个函数，可以从文件中提取数据而不包括字段名。同时提取名称可能会增加很多时间，因为这是重复的操作；这只是一个更有效的方法：
- en: '[PRE17]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we make an empty DataFrame as a starting point using our `columns` list.
    Think of this like making a new spreadsheet, and filling in the first row of your
    sheet with your column names:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`columns`列表创建一个空的DataFrame作为起点。想象一下，就像创建一个新的电子表格，并在你的工作表的第一行填写列名：
- en: '[PRE18]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, let’s loop through `user_set`, which we created in the previous cell.
    Remember, this is the list of user IDs that have data in the `Tappy Data` folder.
    Recall that the structure of the filename for this file is the word `User` followed
    by an underscore followed by the user ID and appended with the `.txt` file extension:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们遍历`user_set`，这是我们之前单元格中创建的。记住，这是包含`Tappy Data`文件夹中数据的用户ID列表。回想一下，这个文件的文件名结构是单词`User`后跟一个下划线，然后是用户ID，最后以`.txt`文件扩展名结尾：
- en: '[PRE19]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we make sure that the file is there. It should be since we did our `set`
    operation earlier, but it is a good idea to check. If the file isn’t there, our
    analysis set will crash. This isn’t a big deal for a few hundred files, but it
    can be heartbreaking if you’re going through tens of thousands. Assuming the file
    is there, we read it into a variable called `temp_data` using the function we
    created earlier. Remember, that function returns a list of data values that look
    just like the cells in a row of a spreadsheet. Then, we insert that data into
    the DataFrame using the user ID as the index for the row:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们确保文件存在。既然我们之前已经做了`set`操作，它应该存在，但检查一下是个好主意。如果文件不存在，我们的分析集将会崩溃。对于几百个文件来说，这不算什么大问题，但如果你要处理成千上万的数据，那就可能让人心碎。假设文件存在，我们使用之前创建的函数将其读入一个名为`temp_data`的变量中。记住，那个函数返回的数据值看起来就像电子表格中的一行单元格。然后，我们使用用户ID作为行的索引，将数据插入DataFrame中：
- en: '[PRE20]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Naturally, we want to check, but we don’t want every row – we just want the
    first few to make sure they are formatted as we expect:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们想检查，但我们不想检查每一行——我们只想检查前几行，以确保它们格式化得符合我们的预期：
- en: '[PRE21]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'When I run this cell, I get the following output:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 当我运行这个单元格时，我得到以下输出：
- en: '![Figure 14.11: My run of our latest cell shows we have a populated pandas
    DataFrame](img/B19644_14_11.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图14.11：我的最新单元格运行结果显示我们有一个填充的pandas DataFrame](img/B19644_14_11.jpg)'
- en: 'Figure 14.11: My run of our latest cell shows we have a populated pandas DataFrame'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.11：我们最新单元格的运行结果显示我们有一个填充的pandas DataFrame
- en: 'Remember, you can view the DataFrame in **SciView** by clicking the **View
    as DataFrame** button indicated by the arrow in *Figure 14**.11*. Mine is shown
    in *Figure 14**.12*:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，你可以通过点击*图14.11*中箭头所指的**以DataFrame查看**按钮在**SciView**中查看DataFrame。我的显示在*图14.12*中：
- en: '![Figure 14.12: Viewing the DataFrame I created in the previous step is easy
    and colorful in PyCharm](img/B19644_14_12.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图14.12：在PyCharm中查看我在上一步创建的DataFrame既简单又多彩](img/B19644_14_12.jpg)'
- en: 'Figure 14.12: Viewing the DataFrame I created in the previous step is easy
    and colorful in PyCharm'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.12：在PyCharm中查看我在上一步创建的DataFrame既简单又多彩
- en: I can see from this that I still have work to do. The diagnosis year is messy,
    as are a few of the other fields. Let’s keep chipping away at it.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里我可以看出，我还有很多工作要做。诊断年份很混乱，其他一些字段也是如此。让我们继续一点一点地解决它。
- en: Data cleansing
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据清洗
- en: Now that we can see our data in a tabular format, there are some ways we can
    improve the format of this data with the express purpose of performing numerical
    analysis across any dimensions we might choose.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经可以看到我们的数据以表格格式呈现，有一些方法可以帮助我们改进数据的格式，以便在可能选择的任何维度上执行数值分析。
- en: Changing numeric data into actual numbers
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将数值数据转换为实际数字
- en: 'Our next cell contains a few lines of code designed to convert numeric values
    into numeric types. Remember, everything is coming in as text and is treated like
    a string until you tell pandas otherwise. Here’s the code for the cell:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下一个单元格包含几行代码，用于将数值值转换为数值类型。记住，所有数据都是以文本形式进入的，并且直到你告诉pandas否则，它们都被当作字符串处理。以下是该单元格的代码：
- en: '[PRE22]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: An application programmer would be tempted to process the data line by line
    and handle type conversions field by field. The neat thing about pandas is that
    once you have your data in a DataFrame, you can operate on entire rows and columns.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序程序员可能会倾向于逐行处理数据，逐字段处理类型转换。pandas 的好处在于，一旦你的数据在 DataFrame 中，你就可以对整个行和列进行操作。
- en: In this code, we’re doing just that. `BirthYear` and `DiagnosisYear` are being
    converted into numbers using the `pd.to_numeric` method. The second argument,
    `errors='coerce'`, will attempt to force a data conversion to a numeric type.
    If this is impossible, such as with a value of “`-------`” (a bunch of dashes),
    which we saw in the `NaN`, or “not a number.” While `NaN` isn’t computationally
    valuable, it does at least standardize all non-numeric values to just this one,
    which will make these rows easier to ignore should we choose.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们正是这样做的。`BirthYear` 和 `DiagnosisYear` 正在使用 `pd.to_numeric` 方法转换为数字。第二个参数
    `errors='coerce'` 将尝试强制数据转换为数值类型。如果不可能，例如，当值是 “`-------`”（一串破折号），就像我们在 `NaN` 中看到的那样，或者
    “不是数字”。虽然 `NaN` 在计算上没有价值，但它至少将所有非数值值标准化为这个值，这将使我们在选择忽略这些行时更容易。
- en: The mention of `NaN` also indicates it’s time to bake some delicious bread in
    your mom’s tandoori oven. Some authors do Patreon, and I do bread, but it has
    to be your mom’s recipe. That means you have to call her and tell her you love
    her. Do it now, even if she doesn’t have a tandoori oven and can’t bake bread!
    I’ll wait.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 提到 `NaN` 也意味着是时候在你妈妈的坦度炉里烤一些美味的面包了。一些作者做 Patreon，我烤面包，但必须是你的妈妈的食谱。这意味着你必须给她打电话，告诉她你爱她。现在就做，即使她没有坦度炉，不能烤面包！我会等着。
- en: 'While you were on the phone, I ran the cell; my result is shown in *Figure
    14**.13*:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在电话上时，我运行了单元格；我的结果显示在 *图 14**.13* 中：
- en: '![Figure 14.13: The year fields are not actual numbers. Wherever there was
    invalid data, we now see a standardized value of nan](img/B19644_14_13.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.13：年份字段实际上不是数字。无论在哪里有无效数据，我们现在都看到一个标准化的 nan 值](img/B19644_14_13.jpg)'
- en: 'Figure 14.13: The year fields are not actual numbers. Wherever there was invalid
    data, we now see a standardized value of nan'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.13：年份字段实际上不是数字。无论在哪里有无效数据，我们现在都看到一个标准化的 nan 值
- en: Binarizing data
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 二值化数据
- en: 'Any place where we can convert data that is essentially binary, we should.
    Within our data, gender is reported with two possible values, male and female,
    representing a possibility of representing it in a binary format. Likewise, many
    of the fields are presented as binaries, as shown here:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在任何可以将数据转换为本质上二进制的地方进行转换。在我们的数据中，性别以两种可能值报告，即男性和女性，表示可以用二进制格式表示的可能性。同样，许多字段都呈现为二进制，如下所示：
- en: '[PRE23]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In these cases, we just need to standardize the values as actual binaries,
    which may result in renaming or expanding our list of field names. Let’s look
    at the cell code:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，我们只需要将值标准化为实际的二进制值，这可能会导致重命名或扩展我们的字段名称列表。让我们看看单元格代码：
- en: '[PRE24]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the preceding code, we renamed the column in our DataFrame from `Gender`
    to `Female`. The second line changes the value in each row for the newly renamed
    column to the result of an expression comparing the current value versus the word
    `Female`. It either is or isn’t `Female`, so we get back a `True` or `False` value.
    The third line converts the Boolean type into an integer, making it more amenable
    to analysis.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们将 DataFrame 中的列名从 `Gender` 改为 `Female`。第二行将新列中每行的值更改为与单词 `Female`
    进行比较的表达式的结果。它要么是 `Female`，要么不是，因此我们得到 `True` 或 `False` 的值。第三行将布尔类型转换为整数，使其更适合分析。
- en: 'Next, we’ll turn our attention to the previously listed columns and do the
    same conversion. This time, we’re checking for the word “True” in our expression.
    The value is either `True` or it isn’t, which results in a Boolean value:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将关注之前列出的列，并执行相同的转换。这次，我们在表达式中检查单词 “True”。值要么是 `True`，要么不是，这会产生布尔值：
- en: '[PRE25]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Running this code yields changes to our DataFrame, as shown in *Figure 14**.14*:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码会导致我们的 DataFrame 发生变化，如图 *图 14**.14* 所示：
- en: '![Figure 14.14: We’ve successfully binarized our fields](img/B19644_14_14.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.14：我们已经成功地将字段二值化](img/B19644_14_14.jpg)'
- en: 'Figure 14.14: We’ve successfully binarized our fields'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.14：我们已经成功地将字段二值化
- en: We can see that our fields are now binary numbers! This is going to make things
    easier later!
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们的字段现在都是二进制数字！这将使后续工作更容易！
- en: 'Let’s jump into the next cell since the first part of the code is doing some
    more cleanup, similar to what we have done so far. In the first part of the cell,
    we are cleaning up the `Impact` field. We’re standardizing any value that isn’t
    `Mild`, `Medium`, or `Severe` as `None`:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们跳到下一个单元格，因为代码的第一部分正在进行一些清理工作，类似于我们之前所做的那样。在单元格的第一部分，我们正在清理`Impact`字段。我们将任何不是`Mild`、`Medium`或`Severe`的值标准化为`None`：
- en: '[PRE26]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Next, while staying in the same cell, we’re going to explore a powerful and
    popular technique that is used by an ML algorithm when preparing data for analysis.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在保持同一单元格的同时，我们将探索一个强大且流行的技术，该技术被机器学习算法在准备数据分析数据时使用。
- en: One-hot encoding
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独热编码
- en: For some reason, when I first heard the term **one-hot encoding**, I immediately
    thought of hot dogs and how I would love to encode one with mustard and sweet
    relish on a nice steamed bun, or maybe the NaN y’all are doing to send me. For
    the record, I know that’s not how the bread is spelled, and I don’t care. The
    joke only works if I spell it incorrectly. I don’t know why I’m telling you that,
    but here we are.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 由于某种原因，当我第一次听到**独热编码**这个词时，我立刻想到了热狗，还有我多么希望能在美味的蒸面包上用芥末和甜美酱编码一个，或者也许是我要发送给你的NaN。记录在案，我知道面包的拼写不是这样的，我也不在乎。只有当我拼错时，这个笑话才有效。我不知道为什么我要告诉你这些，但就是这样。
- en: 'One-hot coding is a technique that allows you to take data that isn’t inherently
    Boolean, and make it so. When I was in the market for a new Jeep Wrangler, there
    were only a few colors I considered:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码是一种技术，它允许你将不是固有的布尔值的数据转换为布尔值。当我正在寻找一辆新的Jeep Wrangler时，我只考虑了少数几种颜色：
- en: Firecracker Red
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爆竹红
- en: Ocean Blue Metallic
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 海洋蓝金属色
- en: Mojito!
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 莫吉托！
- en: Hellayella
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hellayella
- en: 'There are more colors than that, but they are all boring variants of black,
    white, or gray. I can’t get an orange Jeep because people will think I went to
    Oklahoma State University, and we can’t have that. I can ignore those colors,
    leaving me with a list that will fit on the page. Now, let’s one-hot encode that
    list:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 还有更多的颜色，但它们都是黑色、白色或灰色的无聊变体。我无法得到一辆橙色的吉普车，因为人们会认为我去了俄克拉荷马州立大学，我们不能有那样的事情。我可以忽略那些颜色，留下一个可以放在页面上的列表。现在，让我们对这个列表进行独热编码：
- en: '| **Color_Firecracker_Red** | **Color_Ocean_Blue_Metallic** | **Color_Mojito**
    | **Color_Hellayella** |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| **Color_Firecracker_Red** | **Color_Ocean_Blue_Metallic** | **Color_Mojito**
    | **Color_Hellayella** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | 0 | 0 | 0 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 | 0 |'
- en: '| 0 | 1 | 0 | 0 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 0 | 0 |'
- en: '| 0 | 0 | 1 | 0 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 1 | 0 |'
- en: '| 0 | 0 | 0 | 0 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 | 0 |'
- en: '| 0 | 0 | 0 | 1 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 | 1 |'
- en: You can easily see how one-hot encoding works – it pivots the fields and then
    makes them binary. If you’re a relational database guru, you have probably just
    lost your lunch. Data scientists do things a little differently. In the one-hot
    encoded representation, each observation gets a “1” in the column corresponding
    to its category and a “0” in all other columns. This encoding ensures that the
    categorical information is preserved in a way that ML algorithms can understand
    and use effectively. For the record, I went with *Hellayella* based on the idea
    that if I got my Jeep stuck somewhere inaccessible, such as the deserts of Big
    Bend National Park, or deep in the Piney Woods region of east Texas, the rescue
    helicopters would easily find my corpse.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以轻松地看到如何进行独热编码——它将字段旋转并使它们变为二进制。如果你是关系型数据库的大师，你可能刚刚失去了你的午餐。数据科学家做事的方式略有不同。在独热编码表示中，每个观察值在其类别对应的列中得到一个“1”，在其他所有列中得到一个“0”。这种编码确保了分类信息以机器学习算法可以理解和有效使用的方式被保留。记录在案，我选择了*Hellayella*，基于这样的想法：如果我的吉普车被卡在某个无法到达的地方，比如大弯国家公园的沙漠中，或者东德克萨斯州深处的松树林地区，救援直升机会很容易找到我的尸体。
- en: One-hot encoding is commonly used for features such as categorical variables,
    which can’t be directly used as numerical inputs in many ML algorithms. It’s an
    important step in data preprocessing to convert such variables into a suitable
    format for training models.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码通常用于诸如分类变量等特征，这些特征不能直接用作许多机器学习算法的数值输入。它是数据预处理的重要步骤，将此类变量转换为适合训练模型的合适格式。
- en: 'Let’s go back to our code for the current cell. We’ve explained the first few
    lines, so let’s move on to setting up for one-hot encoding on several fields:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到当前单元格的代码。我们已经解释了前几行，所以让我们继续设置对多个字段进行独热编码：
- en: '[PRE27]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We’re going to encode these three columns. One of the columns under consideration
    is the `Impact` column, which we just standardized as a lead-in for this step.
    We’ll perform the one-hot encoding for all three columns here:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要编码这三个列。考虑中的一个列是`Impact`列，我们刚刚将其标准化作为这一步的引导。我们将在这里对这三个列执行one-hot编码：
- en: '[PRE28]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Within the loop, the code performs the following steps:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在循环中，代码执行以下步骤：
- en: '`user_df.iloc[:, : user_df.columns.get_loc(column)]`: Selects the columns to
    the left of the current column being processed. This preserves the columns before
    the one being one-hot encoded.'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`user_df.iloc[:, : user_df.columns.get_loc(column)]`：选择当前正在处理的列左侧的列。这保留了正在one-hot编码之前的列。'
- en: '`pd.get_dummies(user_df[column], prefix=str(column))`: Applies one-hot encoding
    to the current column using the `pd.get_dummies()` method. It creates a DataFrame
    with binary columns representing the different categories in the column. The `prefix`
    parameter adds a prefix to the column names to indicate which original column
    they were derived from.'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`pd.get_dummies(user_df[column], prefix=str(column))`：使用`pd.get_dummies()`方法对当前列进行one-hot编码。它创建一个具有二进制列的DataFrame，代表列中的不同类别。`prefix`参数为列名添加前缀，以指示它们是从哪个原始列派生出来的。'
- en: '`user_df.iloc[:, user_df.columns.get_loc(column) + 1 :]`: Selects the columns
    to the right of the current column being processed. This preserves the columns
    after the one being one-hot encoded.'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`user_df.iloc[:, user_df.columns.get_loc(column) + 1 :]`：选择当前正在处理的列右侧的列。这保留了正在one-hot编码之后的列。'
- en: 'When fed into the `pd.concat` method, these steps effectively replace each
    of the three categorical columns with one-hot encoded binary columns while keeping
    the rest of the DataFrame intact. When you run the cell, you should see results
    like mine, as shown in *Figure 14**.15*:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入到`pd.concat`方法中时，这些步骤有效地将三个分类列替换为one-hot编码的二进制列，同时保持DataFrame的其余部分完整。当您运行单元格时，您应该看到像我一样的结果，如*图14.15*所示：
- en: '![Figure 14.15: I’ve scrolled to the right so that you can see the newly added
    one-hot encoded columns that were added](img/B19644_14_15.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![图14.15：我已经向右滚动，以便您可以看到新添加的one-hot编码列](img/B19644_14_15.jpg)'
- en: 'Figure 14.15: I’ve scrolled to the right so that you can see the newly added
    one-hot encoded columns that were added'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.15：我已经向右滚动，以便您可以看到新添加的one-hot编码列
- en: One-hot encoding will have added many new columns to the DataFrame, so you might
    need to scroll to the right to see them all.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: One-hot编码将在DataFrame中添加许多新列，因此您可能需要向右滚动才能看到它们全部。
- en: Exploring the second dataset
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索第二个数据集
- en: 'With our user data fairly well-cleaned up and sitting in a pandas DataFrame,
    we are now ready to tackle the Tappy data. To keep things relatable, I’m going
    to arbitrarily pick one file from the `Tappy Data` set. Let’s look at the code
    in our next cell:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的用户数据被相当干净地整理并存储在pandas DataFrame中后，我们现在准备处理Tappy数据。为了使内容更具相关性，我将任意选择`Tappy
    Data`集中的一个文件。让我们看看下一个单元格中的代码：
- en: '[PRE29]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'As I said, I picked one arbitrary file to examine. We opened one of these files
    earlier and noted they were all in tab-separated format. pandas has a method that
    will easily read this file directly into a DataFrame. Despite the method being
    called `read_csv`, you get to specify a delimiter, which doesn’t have to be a
    comma. The method will read any kind of delimited file:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我说的，我随机选择了一个文件来检查。我们之前打开了一个这样的文件，并注意到它们都是制表符分隔的格式。pandas有一个方法可以直接将此类文件直接读入DataFrame。尽管该方法被称为`read_csv`，但您可以指定一个分隔符，它不必是逗号。该方法将读取任何类型的分隔文件：
- en: '[PRE30]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'For our purposes, we don’t need the `UserKey` field:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，我们不需要`UserKey`字段：
- en: '[PRE31]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'When we run this cell, we create a new DataFrame called `df`. Be sure to pick
    it from the console variables panel shown in *Figure 14**.16*:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这个单元格时，我们创建了一个名为`df`的新DataFrame。请确保从*图14.16*中显示的控制台变量面板中选择它：
- en: '![Figure 14.16: Our new DataFrame can be viewed by clicking the View as DataFrame
    button](img/B19644_14_16.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![图14.16：可以通过点击“查看为DataFrame”按钮来查看我们的新DataFrame](img/B19644_14_16.jpg)'
- en: 'Figure 14.16: Our new DataFrame can be viewed by clicking the View as DataFrame
    button'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.16：可以通过点击“查看为DataFrame”按钮来查看我们的新DataFrame
- en: Formatting datetime data
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 格式化日期时间数据
- en: 'The next cell fixes our `datetime` data:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个单元格修复我们的`datetime`数据：
- en: '[PRE32]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This first line tries to force the values in the `Date` column to be dates.
    If the coercion doesn’t work, we’ll see `NaT` (not a time), which is disappointing
    since there’s no food joke to be made. Next, we’ll do some more coercion on the
    `Hold time`, `Latency time`, and `Flight` `time` fields:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这第一行尝试强制`Date`列中的值成为日期。如果强制转换不起作用，我们将看到`NaT`（不是一个时间），这很令人失望，因为没有食物玩笑可以讲。接下来，我们将在`Hold
    time`、`Latency time`和`Flight` `time`字段上进行更多的强制转换：
- en: '[PRE33]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Any observations lacking time data should be dropped:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 任何缺少时间数据的观察结果都应该被删除：
- en: '[PRE34]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let’s print the result for inspection:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印出结果以供检查：
- en: '[PRE35]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let’s run it! My cell run results are shown in *Figure 14**.17*:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行它！我的单元格运行结果显示在*图14**.17*中：
- en: '![Figure 14.17: Our datetime data is now numeric and any observation with missing
    time data, being useless, has been dropped](img/B19644_14_17.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![图14.17：我们的日期时间数据现在是数值型，任何缺少时间数据的观察结果，因为无用，已被删除](img/B19644_14_17.jpg)'
- en: 'Figure 14.17: Our datetime data is now numeric and any observation with missing
    time data, being useless, has been dropped'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.17：我们的日期时间数据现在是数值型，任何缺少时间数据的观察结果，因为无用，已被删除
- en: Washing hands and fixing direction
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 洗手并确定方向
- en: 'The next cell cleans up the hand and direction columns:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个单元格清理了手和方向列：
- en: '[PRE36]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This code uses a logical `OR` to filter out anything that doesn’t have a value
    of `L`, `R`, or `S`. Since it is presented as an `OR`, anything outside the three
    desirable possibilities will return as `false`, and be excluded.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使用逻辑“或”来过滤掉任何不具有“L”、“R”或“S”值的值。由于它被表示为“或”，任何超出三个期望可能性的值都将返回为“false”，并被排除。
- en: 'Let’s do the same thing with direction, which has more possibilities:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用同样的方法处理方向，它有更多的可能性：
- en: '[PRE37]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Of course, we’ll print the result:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们会打印出结果：
- en: '[PRE38]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Go ahead and run the cell. All rows containing invalid data have been removed.
    This result isn’t as visual as most have been, so I don’t think we need a screenshot
    for this one.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，运行这个单元格。所有包含无效数据的行都已删除。这个结果不像大多数那样直观，所以我认为我们不需要为这个结果截图。
- en: Summarizing data
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结数据
- en: 'Our next cell provides an example of how to summarize our data, which we have
    been working so hard to set up for analysis. We’re ready! Let’s try something
    simple. As usual, the first line of code in the cell just marks the beginning
    of the cell:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下一个单元格提供了一个如何总结我们数据的例子，这是我们一直在努力设置以进行分析的。我们已经准备好了！让我们尝试一些简单的东西。像往常一样，单元格中的第一行代码只是标记单元格的开始：
- en: '[PRE39]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Recall that the data we have been working with so far is typing speed data for
    a specific subject at a given time. A subject (`User`) is simply a single data
    point within our first dataset, and we would like to combine the two datasets
    somehow, so we need a way to aggregate our current data into a single data point.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们到目前为止一直在处理的是特定时间特定主题的打字速度数据。一个主题（`User`）只是我们第一个数据集中单个数据点，我们希望以某种方式将两个数据集结合起来，因此我们需要一种方法来聚合我们当前的数据到一个单一的数据点。
- en: 'Since we are working with numerical data (typing time), we can take the average
    (mean) of the time data across different columns as a way to summarize the data
    of a given user. We can achieve this with the `groupby()` function from pandas:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在处理数值数据（打字时间），我们可以通过对不同列的时间数据进行平均（平均值）来总结给定用户的数据。我们可以使用pandas的`groupby()`函数来实现这一点：
- en: '[PRE40]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Of course, we should print it:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们应该打印它：
- en: '[PRE41]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The result of the run is shown in *Figure 14**.18*. The code puts the results
    in a new DataFrame called `direction_group_df`, so be sure you select it as shown
    in the figure:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 运行结果显示在*图14**.18*中。代码将结果放入一个新的DataFrame中，名为`direction_group_df`，所以请确保您按照图中的方式选择它：
- en: '![Figure 14.18: Hooray! We have our first calculated insight!](img/B19644_14_18.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![图14.18：太好了！我们得到了第一个计算洞察！](img/B19644_14_18.jpg)'
- en: 'Figure 14.18: Hooray! We have our first calculated insight!'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.18：太好了！我们得到了第一个计算洞察！
- en: This is exciting! We have the mechanics working, but now, we need to concentrate
    on making this work with many data files instead of just one.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这很令人兴奋！我们已经让机制工作起来，但现在，我们需要专注于让它能够处理多个数据文件，而不仅仅是单个文件。
- en: Refactoring for scale
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为扩展进行重构
- en: Our exploration of the Tappy data has focused on one file to establish in an
    easily verifiable way that our code is working. We’ve determined that it is, so
    now, we should refactor our code so that we can process thousands of files. To
    do this, we should consolidate some of our cells into a function. The code in
    the next cell is long but familiar since it is just all the code we’ve written
    so far combined into one function. If you’re an application developer, and you
    understand the design principle known as the **single responsibility principle**
    (**SRP**), you know this is an antipattern. Remember, though, this isn’t application
    code. Nobody will run this beyond performing the analysis, so the rigors of SOLID
    principles that normally apply to software development are not observed in data
    science work.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 Tappy 数据的探索主要集中在单个文件上，以便以易于验证的方式确认我们的代码正在正常工作。我们已经确定它是可行的，因此现在，我们应该重构我们的代码，以便我们可以处理成千上万的文件。为此，我们应该将一些单元格合并成一个函数。下一个单元格中的代码虽然很长，但很熟悉，因为它只是我们将到目前为止所写的所有代码合并成一个函数的结果。如果你是一个应用程序开发者，并且理解被称为**单一职责原则**（**SRP**）的设计原则，你知道这是一个反模式。然而，请记住，这并不是应用程序代码。没有人会运行这段代码来执行分析之外的操作，因此通常适用于软件开发中的
    SOLID 原则的严格性在数据科学工作中并未得到体现。
- en: Processing the Tappy data with one function
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用一个函数处理 Tappy 数据
- en: 'Here’s the function:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是函数：
- en: '[PRE42]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Here, we’re reading in the CSV filename passed as an argument to our function.
    We enrich the data with hardcoded field names:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在读取作为函数参数传递的 CSV 文件名。我们使用硬编码的字段名丰富数据：
- en: '[PRE43]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We drop the unneeded column:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们删除了不需要的列：
- en: '[PRE44]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We fix the dates:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们修正了日期：
- en: '[PRE45]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Always wash your hands by getting rid of invalid values:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 总是通过去除无效值来洗手：
- en: '[PRE46]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Do the same with direction data values:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 对方向数据值也做同样的处理：
- en: '[PRE47]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We’re doing our math! This is where the manual GC process comes in. It’s a
    good thing we washed our hands, right? In the following code, we’re doing our
    calculations. The results are being returned as a new DataFrame, so to save memory,
    we’re deleting the old DataFrames as we go. This frees up memory since this kind
    of work is memory intensive:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在做数学运算！这是手动 GC 过程介入的地方。我们洗手洗得很好，对吧？在以下代码中，我们正在进行计算。结果作为新的 DataFrame 返回，因此为了节省内存，我们在进行过程中删除旧的
    DataFrames。这释放了内存，因为这种工作对内存密集型：
- en: '[PRE48]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'With our new result, we re-index and then sort:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们新的结果，我们重新索引然后排序：
- en: '[PRE49]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This line returns the flattened NumPy array, which contains the mean values
    of the grouped data. The `.values.flatten()` method converts the DataFrame into
    a two-dimensional NumPy array and then flattens it into a one-dimensional array
    for ease of use:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码返回一个展平的 NumPy 数组，其中包含分组数据的均值。`.values.flatten()` 方法将 DataFrame 转换为二维 NumPy
    数组，然后将其展平为一维数组，以便于使用：
- en: '[PRE50]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Processing the users with a function
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用函数处理用户
- en: 'Within the same cell is a second function:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一个单元格中还有一个第二个功能：
- en: '[PRE51]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'This line initializes an empty NumPy array named `running_user_data`. This
    array will be used to accumulate data as the function iterates through filenames,
    which is what the following block does:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码初始化了一个名为 `running_user_data` 的空 NumPy 数组。这个数组将在函数遍历文件名时累积数据，这正是以下代码块所做的工作：
- en: '[PRE52]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: This loop iterates through the list of filenames. If the provided user ID is
    found in the filename, it calls the `read_tappy()` function (which returns a flattened
    NumPy array of mean values) and appends its contents to the `running_user_data`
    array.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这个循环遍历文件名列表。如果提供的用户 ID 在文件名中找到，它将调用 `read_tappy()` 函数（该函数返回包含均值值的展平 NumPy 数组）并将内容追加到
    `running_user_data` 数组中。
- en: 'After iterating through the filenames and appending the data, the following
    line reshapes the `running_user_data` array into a two-dimensional array, with
    each row containing 27 columns. This flattening of time data allows for further
    analysis:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在遍历文件名并追加数据后，以下行将 `running_user_data` 数组重塑为二维数组，每行包含 27 列。这种时间数据的展平允许进行进一步的分析：
- en: '[PRE53]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The last line calculates the mean values along the rows (`axis=0`) of the `running_user_data`
    array using `np.nanmean()`. The `np.nanmean()` function ignores `NaN` values while
    calculating the mean:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行使用 `np.nanmean()` 函数计算 `running_user_data` 数组沿行（`axis=0`）的均值。`np.nanmean()`
    函数在计算均值时忽略 `NaN` 值：
- en: '[PRE54]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: To summarize, the `process_user` function processes data for a specific user
    by iterating through relevant filenames, aggregating the data using the `read_tappy`
    function, reshaping the data, and calculating the mean values while ignoring `NaN`
    values. The final result is an array of mean values for each column of the data.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，`process_user`函数通过迭代相关文件名，使用`read_tappy`函数聚合数据，重塑数据，并计算平均值（忽略`NaN`值），来处理特定用户的数据。最终结果是数据每一列的平均值数组。
- en: Processing all the data
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理所有数据
- en: 'This one’s for all the marbles! The following cell processes the data for all
    available users by aggregating and calculating mean values based on the Tappy
    data. First, there’s a little housekeeping. We’re going to ignore any warnings:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 这是为了所有宝石！下面的单元处理所有可用用户的数据，通过聚合和基于Tappy数据计算平均值。首先，有一些家务要做。我们将忽略任何警告：
- en: '[PRE55]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We’ll make one more trip through the `Tappy` `Data` folder:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次访问`Tappy`数据文件夹：
- en: '[PRE56]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Next, we’ll make some column names for the final DataFrame:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将为最终的DataFrame创建一些列名：
- en: '[PRE57]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Next, let’s loop through the user indexes and use our `process_user` function:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们遍历用户索引并使用我们的`process_user`函数：
- en: '[PRE58]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'These next few lines do a little interim cleaning by ensuring any NaN values
    are substituted with zeros, and any negative numeric data is also normalized to
    zero:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的几行通过确保任何NaN值被替换为零，以及任何负数值也被归一化到零，进行了一些中间清理：
- en: '[PRE59]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'And then, we print like we’ve never printed before! OK, that’s not true – we’ve
    done this a lot:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将像从未打印过一样打印！好吧，这不是真的——我们经常这样做：
- en: '[PRE60]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Saving the processed data
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保存处理后的数据
- en: 'The last code cell likely doesn’t need much explanation:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个代码单元可能不需要太多解释：
- en: '[PRE61]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'First, we concatenate the two DataFrames together:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将两个DataFrame连接起来：
- en: '[PRE62]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Finally, we save it to a CSV file:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将其保存到CSV文件中：
- en: '[PRE63]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: This is generally a good practice in a given data pipeline. Saving the processed,
    cleaned version of a dataset can save data engineers a lot of effort if something
    goes wrong along the way. It also offers flexibility, if and when we want to change
    or extend our pipeline further.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常在给定的数据管道中是一个好的实践。保存数据集的处理和清理版本可以在过程中出现问题时为数据工程师节省大量精力。它也提供了灵活性，如果我们想改变或进一步扩展我们的管道，它也提供了灵活性。
- en: 'I’ll open the CSV file in PyCharm for one last look before we start doing the
    real analysis work. You can see mine in *Figure 14**.19*:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始真正的分析工作之前，我将在PyCharm中打开CSV文件进行最后一次查看。你可以看到我的*图14.19*：
- en: '![Figure 14.19: Our hard work has paid off! Our data is ready for analysis](img/B19644_14_19.jpg)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![图14.19：我们的辛勤工作得到了回报！我们的数据已准备好分析](img/B19644_14_19.jpg)'
- en: 'Figure 14.19: Our hard work has paid off! Our data is ready for analysis'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.19：我们的辛勤工作得到了回报！我们的数据已准备好分析
- en: With that, we are ready to start exploring our dataset and searching for insights.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就准备好探索我们的数据集并寻找洞察了。
- en: Data analysis and insights
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分析和洞察
- en: Remember what we said about the importance of having a question in mind when
    starting to work on a data science project? This is especially true during this
    phase, where we explore our dataset and extract insights, which should revolve
    around our initial question – the connection between typing speed and whether
    a patient has PD or not.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们之前提到的，在开始数据科学项目时，心中要有问题意识的重要性吗？这一点在这个阶段尤其正确，在这个阶段，我们将探索我们的数据集并提取洞察，这些都应围绕我们的初始问题展开——打字速度与患者是否患有PD之间的联系。
- en: Throughout this section, we will be working with the `EDA.ipynb` file, located
    in the `notebooks` folder of our current project. In the following subsections,
    we will be looking at the code included in this `notebooks` folder. Go ahead and
    open this Jupyter notebook in your PyCharm editor, or, if you are following our
    discussions and entering your own code, create a new Jupyter notebook.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用位于当前项目`notebooks`文件夹中的`EDA.ipynb`文件。在接下来的子节中，我们将查看这个`notebooks`文件夹中包含的代码。请打开这个Jupyter笔记本到你的PyCharm编辑器中，或者，如果你正在跟随我们的讨论并输入自己的代码，创建一个新的Jupyter笔记本。
- en: Starting the notebook and reading in our processed data
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动笔记本并读取我们的处理数据
- en: 'Remember that when you open a Jupyter notebook in Python, you can see the code,
    but Jupyter won’t run unless you click the **Run** button. You can see PyCharm
    ready for this in *Figure 14**.20*:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，当你用Python打开一个Jupyter笔记本时，你可以看到代码，但除非你点击**运行**按钮，否则Jupyter不会运行。你可以在*图14.20*中看到PyCharm已经准备好了：
- en: '![Figure 14.20: The notebook is open, I’ve clicked in the first cell (In 1),
    and I’ll now click the Run button indicated by the arrow](img/B19644_14_20.jpg)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![图14.20：笔记本已打开，我已点击第一个单元格（In 1），现在我将点击箭头所示的运行按钮](img/B19644_14_20.jpg)'
- en: 'Figure 14.20: The notebook is open, I’ve clicked in the first cell (In 1),
    and I’ll now click the Run button indicated by the arrow'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.20：笔记本已打开，我已点击第一个单元格（In 1），现在我将点击箭头所示的运行按钮
- en: 'Once you click the **Run** button, a Jupyter server will start and run the
    first cell in the notebook, which handles our imports and reads in our cleaned
    dataset:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你点击**运行**按钮，Jupyter服务器将启动并运行笔记本中的第一个单元格，该单元格处理我们的导入并读取我们的清洗后的数据集：
- en: '[PRE64]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Since the last line has us printing the first five lines of our output, you’ll
    see them appear below the code and next to a marker that says **Out 2**, as shown
    in *Figure 14**.21*:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 由于最后一行让我们打印输出前五行，你将看到它们出现在代码下方，并紧挨着一个标记为**Out 2**的标记，如图*图14.21*所示：
- en: '![Figure 14.21: The output from the head statement in In 2 is shown in Out
    2 and is horizontally scrollable](img/B19644_14_21.jpg)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![图14.21：In 2中的头部语句输出显示在Out 2中，并且可以水平滚动](img/B19644_14_21.jpg)'
- en: 'Figure 14.21: The output from the head statement in In 2 is shown in Out 2
    and is horizontally scrollable'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.21：In 2中的头部语句输出显示在Out 2中，并且可以水平滚动
- en: Now that our cleaned data has been loaded up, we can move on to analysis techniques.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载了清洗后的数据，我们可以继续进行数据分析技术。
- en: Using charts and graphs
  id: totrans-387
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用图表和图形
- en: Visualization is normally the end goal for most of my work, so for me, this
    is a natural next step. I’m going to start by creating a bar graph that will show
    me the distribution of the counts of unique values within the data. I think this
    might give us some insight into which factor would affect the dependent variable
    in this study, which is whether a subject has early-onset PD. However, there’s
    still a problem. As shown in *Figure 14**.21*, there are still some holes in the
    data I will need to account for before I begin analysis in earnest.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化通常是大多数工作的最终目标，因此对我来说，这是一个自然的下一步。我将首先创建一个条形图，以显示数据中唯一值的计数分布。我认为这可能会让我们对哪个因素会影响本研究中的因变量有所了解，这个因变量是是否患有早发性PD。然而，仍然存在问题。如图*图14.21*所示，在开始认真分析之前，我需要考虑数据中的一些空白。
- en: 'What I’m going to do first is create a bar chart to visualize our missing data.
    The following code cell handles this:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 我将要做的第一件事是创建一个条形图来可视化我们的缺失数据。以下代码单元格处理此操作：
- en: '[PRE65]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Running this code produces the visualization shown in *Figure 14**.22*:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码将产生如图*图14.22*所示的可视化：
- en: '![Figure 14.22: The missing data is visualized in the bar chart](img/B19644_14_22.jpg)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![图14.22：缺失数据在柱状图中进行了可视化](img/B19644_14_22.jpg)'
- en: 'Figure 14.22: The missing data is visualized in the bar chart'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.22：缺失数据在柱状图中进行了可视化
- en: Thankfully, our chart is very sparse. There is only a small amount of data that
    is missing, or incomplete. There are some missing values for `BirthYear` and `DiagnosisYear`.
    You can even see one in the preview shown in *Figure 14**.21*. Analyzing missing
    values is important, and we will come back to the process of filling in these
    values later on. But for now, let’s continue with the visualization process.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们的图表非常稀疏。只有少量数据缺失或不完整。对于`BirthYear`和`DiagnosisYear`有一些缺失值。你甚至可以在*图14.21*的预览中看到其中一个。分析缺失值很重要，我们将在稍后回到填充这些值的流程。但现在，让我们继续可视化过程。
- en: 'A great feature in Matplotlib is subplots, which allow us to generate multiple
    visualizations side by side. In the following code cell, we are creating multiple
    visualizations with this feature to highlight potential differences between patients
    with and without Parkinson’s:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: Matplotlib的一个很棒的功能是子图，它允许我们并排生成多个可视化。在下面的代码单元格中，我们使用此功能创建多个可视化，以突出显示患有和未患有帕金森病的患者之间的潜在差异：
- en: '[PRE66]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'After running this code cell, a visualization will be generated, as shown in
    *Figure 14**.23*:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码单元格后，将生成如图*图14.23*所示的可视化：
- en: '![Figure 14.23: Four plots drawn together from the previous cell](img/B19644_14_23.jpg)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![图14.23：从前一个单元格中绘制出的四个图表](img/B19644_14_23.jpg)'
- en: 'Figure 14.23: Four plots drawn together from the previous cell'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.23：从前一个单元格中绘制出的四个图表
- en: The top two visualizations represent the distribution in the year of birth of
    people with (top right) and without (top left) Parkinson’s. We can see that these
    distributions roughly follow the normal bell curve. If you were to encounter a
    distribution that is skewed or in a strange shape, it might be worth digging into
    that data further. Note that we can also apply the same visualization for the
    `DiagnosisYear` column.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部两个可视化表示有帕金森病（右上角）和无帕金森病（左上角）的人的出生年份分布。我们可以看到这些分布大致遵循正态钟形曲线。如果你遇到偏斜或形状奇怪的分布，可能值得进一步挖掘那些数据。注意，我们也可以将相同的可视化应用于`诊断年份`列。
- en: In the bottom-left visualization, we have a bar chart representing the count
    of male patients (two bars on the left) and female patients (two bars on the right).
    Patients with Parkinson’s are counted with the orange bars, and patients without
    are counted with the blue bars. In this visualization, we can see that while there
    are more patients with the disease than the ones without, the breakdown across
    the two genders is roughly the same.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在左下角的视觉图中，我们有一个柱状图，表示男性患者（左侧两根柱状图）和女性患者（右侧两根柱状图）的数量。帕金森病患者用橙色柱状图表示，无帕金森病患者用蓝色柱状图表示。在这个可视化中，我们可以看到虽然患病的患者比无病患者多，但两性之间的分布大致相同。
- en: The bottom-right visualization, on the other hand, illustrates the breakdown
    between patients with tremors (two bars on the right) and those without tremors
    (two bars on the left). From this visualization, we can see that tremors are significantly
    more common in patients with Parkinson’s, which is quite intuitive and can serve
    as a sanity check for our analyses so far.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，右下角的可视化展示了患有震颤（右侧两根柱状图）和无震颤（左侧两根柱状图）的患者的差异。从这个可视化中，我们可以看到震颤在帕金森病患者中显著更常见，这一点相当直观，也可以作为我们到目前为止分析的合理性检查。
- en: 'Next, we will move on to box plots. Specifically, we will use box plots to
    visualize the distributions of different time data (`Hold time`, `Latency time`,
    and `Flight time`) among patients with and without Parkinson’s. Once again, we
    will use the subplots feature to generate multiple visualizations at the same
    time:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将转向箱线图。具体来说，我们将使用箱线图来可视化有和无帕金森病的患者在不同的时间数据（`保持时间`、`延迟时间`和`飞行时间`）中的分布。再次使用子图功能来同时生成多个可视化：
- en: '[PRE67]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'In this code cell, each subplot will visualize data of a specific direction
    type (`LL`, `LR`, `LS`, and so on) and will contain different splits denoting
    patients with and without the disease. You should obtain the visualization shown
    in *Figure 14**.24*:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码单元格中，每个子图将可视化特定方向类型（`LL`、`LR`、`LS`等）的数据，并将包含表示有病和无病患者的不同分割。你应该获得*图14.24*所示的可视化。
- en: '![Figure 14.24: The plots from the previous run cell](img/B19644_14_24.jpg)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![图14.24：前一个运行单元格的图表](img/B19644_14_24.jpg)'
- en: 'Figure 14.24: The plots from the previous run cell'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.24：前一个运行单元格的图表
- en: What we can gather from this visualization is that, surprisingly, the distribution
    of typing speed among patients without Parkinson’s can span across higher values
    and have more variance than that among patients with Parkinson’s, which might
    contradict the intuition some might have that patients with Parkinson’s take more
    time to press keystrokes.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个可视化中我们可以得出，令人惊讶的是，无帕金森病患者打字速度的分布可以跨越更高的值，并且比帕金森病患者有更多的变异性，这可能与一些人的直觉相矛盾，即帕金森病患者按键盘键需要更多的时间。
- en: Overall, bar charts, distribution plots, and box plots are some of the most
    common visualization techniques in data science tasks, mostly because they are
    both simple to understand and powerful enough to highlight important patterns
    in our datasets. In the next and final subsection on the topic of data analysis,
    we will consider more advanced techniques – namely, the correlation matrix between
    attributes and leveraging ML models.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，柱状图、分布图和箱线图是数据科学任务中最常见的可视化技术之一，主要是因为它们既易于理解，又足够强大，能够突出我们数据集中的重要模式。在接下来和最后的关于数据分析主题的子节中，我们将考虑更高级的技术——即属性之间的相关矩阵和利用机器学习模型。
- en: Machine learning-based insights
  id: totrans-410
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于机器学习的见解
- en: Unlike the previous analysis methods, the methods discussed in this subsection
    and other similar ones are based on more complex mathematical models and ML algorithms.
    Given the scope of this book, we will not be going into the specific theoretical
    details for these models, but it’s still worth seeing some of them in action by
    applying them to our dataset.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前分析的方法不同，本小节中讨论的方法和其他类似方法基于更复杂的数学模型和ML算法。鉴于本书的范围，我们不会深入探讨这些模型的具体理论细节，但仍然值得通过将它们应用于我们的数据集来观察它们的一些实际应用。
- en: First, let’s consider the feature correlation matrix for our dataset. As the
    name suggests, this model is a matrix (a 2D table) that contains the correlation
    between each pair of numerical attributes (or features) within our dataset. A
    correlation between two features is a real number between -1 and 1, indicating
    the magnitude and direction of the correlation. The higher the value, the more
    correlated the two features are.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们考虑我们的数据集的特征相关矩阵。正如其名称所暗示的，该模型是一个矩阵（一个二维表），其中包含我们数据集中每对数值属性（或特征）之间的相关性。两个特征之间的相关性是一个介于-1和1之间的实数，表示相关性的大小和方向。值越高，两个特征的相关性越强。
- en: 'To obtain the feature correlation matrix from a pandas DataFrame, we must call
    the `corr()` method, as shown here:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 要从pandas DataFrame中获得特征相关矩阵，我们必须调用`corr()`方法，如下所示：
- en: '[PRE68]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We usually visualize a correlation matrix using a heat map, as implemented
    in the same code cell:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常使用热图来可视化相关矩阵，如同一代码单元格中实现的那样：
- en: '[PRE69]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'This code will produce the visualization shown in *Figure 14**.25*:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将产生*图14.25*所示的可视化：
- en: '![Figure 14.25: A heatmap is ideal for visualizing correlation matrices](img/B19644_14_25.jpg)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
  zh: '![图14.25：热图非常适合可视化相关矩阵](img/B19644_14_25.jpg)'
- en: 'Figure 14.25: A heatmap is ideal for visualizing correlation matrices'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.25：热图非常适合可视化相关矩阵
- en: Next, we will try applying an ML model to our dataset. Contrary to popular belief,
    in many data science projects, we don’t take advantage of ML models for predictive
    tasks, where we train our models to be able to predict future data. Instead, we
    feed our dataset to a specific model so that we can extract more insights from
    that current dataset.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将尝试将ML模型应用于我们的数据集。与普遍看法相反，在许多数据科学项目中，我们并不利用ML模型进行预测任务，即训练我们的模型以预测未来的数据。相反，我们将数据集输入到特定的模型中，以便我们可以从中提取更多见解。
- en: 'Here, we are using the linear **support vector classifier** (**SVC**) model
    from scikit-learn to analyze the data we have and return the feature importance
    list:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用scikit-learn中的线性**支持向量分类器**（**SVC**）模型来分析我们的数据，并返回特征重要性列表：
- en: '[PRE70]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Note that before we feed the data we have to the ML model, we need to fill in
    the missing values we have in the two columns we identified earlier – `BirthYear`
    and `DiagnosisYear`. Most ML models cannot handle missing values very well, and
    it is up to the data engineers to choose how these values should be filled.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在我们将数据输入到ML模型之前，我们需要填写我们在之前确定的两个列中存在的缺失值——`BirthYear`和`DiagnosisYear`。大多数ML模型都不能很好地处理缺失值，这取决于数据工程师选择如何填充这些值。
- en: Here, we are using the `coef_` attribute of the model afterward.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用模型之后的`coef_`属性。
- en: 'This attribute contains the feature importance list, which is visualized by
    the last section of the code:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 此属性包含特征重要性列表，该列表通过代码的最后部分进行可视化：
- en: '[PRE71]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Running this code produces the visualization shown in *Figure 14**.26*:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码将产生*图14.26*所示的可视化：
- en: '![Figure 14.26: A graph of the feature important list identifies features used
    extensively while training an ML model](img/B19644_14_26.jpg)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![图14.26：特征重要列表的图表显示了在训练ML模型时广泛使用的特征](img/B19644_14_26.jpg)'
- en: 'Figure 14.26: A graph of the feature important list identifies features used
    extensively while training an ML model'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.26：特征重要列表的图表显示了在训练ML模型时广泛使用的特征
- en: From the feature importance list, we can identify any features that were used
    extensively by the ML model while training. A feature with a very high importance
    value could be correlated with the target attribute (whether someone has Parkinson’s
    or not) in some interesting way. For example, we can see that `Tremors` (which
    we know is quite correlated to our target attribute) is the third most important
    feature of our current ML model.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 从特征重要性列表中，我们可以识别出在训练过程中被ML模型广泛使用的任何特征。具有非常高的重要性值的特征可能与目标属性（某人是否患有帕金森病）以某种有趣的方式进行关联。例如，我们可以看到`震颤`（我们知道它与我们的目标属性高度相关）是我们当前ML模型的第三重要特征。
- en: That’s our last discussion point regarding analyzing our dataset. In the last
    section of this chapter, we will have a brief discussion on deciding how to write
    a script for a Python data science project.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是关于分析我们数据集的最后一个讨论点。在本章的最后部分，我们将简要讨论如何为Python数据科学项目编写脚本。
- en: Scripts versus notebooks in data science
  id: totrans-432
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学中的脚本与笔记本
- en: 'In the preceding data science pipeline, there are two main sections: data cleaning,
    where we remove inconsistent data, fill in missing data, and appropriately encode
    the attributes, and data analysis, where we generate visualizations and insights
    from our cleaned dataset.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的数据科学流程中，有两个主要部分：数据清洗，其中我们删除不一致的数据，填补缺失数据，并适当地编码属性；数据分析，其中我们从清洗后的数据集中生成可视化和洞察。
- en: The data cleaning process was implemented by a Python script while the data
    analysis process was done with a Jupyter notebook. In general, deciding whether
    a Python program should be done in a script or a notebook is quite an important,
    yet often overlooked, aspect while working on a data science project.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗过程是通过Python脚本实现的，而数据分析过程则是使用Jupyter笔记本完成的。一般来说，在数据科学项目中，决定是否应该在脚本或笔记本中完成Python程序是一个相当重要但常常被忽视的方面。
- en: As we discussed in the previous chapter, Jupyter notebooks are perfect for iterative
    development processes, where we can transform and manipulate our data as we go.
    A Python script, on the other hand, offers no such dynamism. We need to enter
    all of the code necessary in the script and run it as a complete program.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在上一章中讨论的，Jupyter笔记本非常适合迭代开发过程，在这个过程中我们可以边走边转换和操作我们的数据。另一方面，Python脚本则没有这种动态性。我们需要在脚本中输入所有必要的代码，并作为一个完整的程序运行。
- en: However, as illustrated in the *Data cleansing and preprocessing* section, PyCharm
    allows us to divide a traditional Python script into separate code cells and inspect
    the data we have as we go using the **SciView** panel. The dynamism offered by
    Jupyter notebooks can also be found within PyCharm.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如*数据清洗和预处理*部分所示，PyCharm允许我们将传统的Python脚本划分为独立的代码单元格，并使用**SciView**面板在执行过程中检查我们的数据。Jupyter笔记本提供的动态性也可以在PyCharm中找到。
- en: Now, another core difference between regular Python scripts and Jupyter notebooks
    is the fact that printed output and visualizations are included inside a notebook,
    together with the code cells that generated them. While looking at this from the
    perspective of data scientists, we can see that this feature is considerably useful
    when making reports and presentations.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来看，常规Python脚本和Jupyter笔记本之间的另一个核心区别是，笔记本中包含了打印输出和可视化结果，以及生成它们的代码单元格。从数据科学家的角度来看，我们可以看到这个特性在制作报告和演示时非常有用。
- en: Say you are tasked with finding actionable insights from a dataset in a company
    project, and you need to present your final findings, as well as how you came
    across them with your team. A Jupyter notebook can effectively serve as the main
    platform for your presentation. Not only will people be able to see which specific
    commands were used to process and manipulate the original data but you will also
    be able to include Markdown texts to further explain any subtle discussion points.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在一个公司项目中负责从数据集中找到可操作的见解，并且需要向团队展示你的最终发现以及你是如何找到它们的。Jupyter笔记本可以有效地作为你演示的主要平台。人们不仅能够看到用于处理和操作原始数据的特定命令，你还可以包括Markdown文本来进一步解释任何细微的讨论点。
- en: Regular Python scripts can simply be used for low-level tasks where the general
    workflow has already been agreed upon, and you will not need to present it to
    anyone else. In our current example, I chose to clean the dataset using a Python
    script as most of the cleaning and formatting changes we applied to the dataset
    don’t generate any actionable insights that can address our initial question.
    I only used a notebook for data analysis tasks, where there were many visualizations
    and insights worthy of further discussion.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 常规Python脚本可以简单地用于已经达成共识的通用工作流程的低级任务，你不需要将其展示给其他人。在我们的当前示例中，我选择使用Python脚本清洗数据集，因为我们应用的大多数清洗和格式化更改并没有生成任何可以解决我们初始问题的可操作见解。我只使用了笔记本进行数据分析任务，其中有许多值得进一步讨论的可视化和见解。
- en: Overall, the decision to use either a traditional Python script or a Jupyter
    notebook solely depends on your tasks and purposes. We simply need to remember
    that, for whichever tool we would like to use, PyCharm offers incredible support
    that can streamline our workflow.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，决定使用传统的 Python 脚本还是 Jupyter 笔记本完全取决于你的任务和目的。我们只需要记住，对于我们想要使用的任何工具，PyCharm
    都提供了令人难以置信的支持，可以简化我们的工作流程。
- en: Summary
  id: totrans-441
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we walked through the hands-on process of working on a data
    science pipeline. First, we discussed the importance of having version control
    for not just our code and project-related files but also our datasets; we then
    learned how to use Git LFS to apply version control to large files and datasets.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了数据科学管道的动手操作过程。首先，我们讨论了为什么需要对代码和项目相关的文件以及数据集进行版本控制的重要性；然后我们学习了如何使用
    Git LFS 对大文件和数据集进行版本控制。
- en: Next, we looked at various data cleaning and preprocessing techniques that are
    specific to the example dataset. Using the **SciView** panel in PyCharm, we can
    dynamically inspect the current state of our data and variables and see how they
    change after each command.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们研究了针对示例数据集的各种数据清洗和预处理技术。使用 PyCharm 中的 **SciView** 面板，我们可以动态检查我们数据的状态和变量，并查看它们在每次命令执行后的变化。
- en: Finally, we considered several techniques to generate visualizations and extract
    insights from our dataset. Using the Jupyter editor in PyCharm, we were able to
    avoid working with a Jupyter server and work on our notebook entirely within PyCharm.
    Having walked through this process, you are now ready to tackle real-life data
    science problems and projects using the same tools and functionalities that we
    have discussed so far.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们考虑了几种生成可视化并从我们的数据集中提取洞察的技术。使用 PyCharm 中的 Jupyter 编辑器，我们能够避免与 Jupyter 服务器交互，并在
    PyCharm 内部完全处理我们的笔记本。经过这个过程，你现在已经准备好使用我们迄今为止讨论过的相同工具和功能来解决现实生活中的数据科学问题和项目。
- en: So, we have finished our discussion on using PyCharm in the context of scientific
    computing and data science. In the next chapter, we will finally consider a topic
    that we have mentioned multiple times through our previous chapters – PyCharm
    plugins.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经完成了在科学计算和数据科学背景下使用 PyCharm 的讨论。在下一章中，我们最终将考虑我们在前几章中多次提到的主题——PyCharm 插件。
- en: Questions
  id: totrans-446
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Answer the following questions to test your knowledge of this chapter:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 回答以下问题以测试你对本章知识的掌握：
- en: What are some of the main ways of collecting datasets for a data science project?
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于数据科学项目，有哪些主要的数据集收集方式？
- en: Can Git LFS be used with Git? If so, what is the overall process?
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Can Git LFS be used with Git? If so, what is the overall process?
- en: Which type of attribute can have its missing values filled out with the mean?
    What about the mode?
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪种类型的属性可以使用平均值来填充其缺失值？对于众数呢？
- en: What problem does one-hot encoding address? What problem can arise from using
    one-hot encoding?
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: What problem does one-hot encoding address? What problem can arise from using
    one-hot encoding?
- en: Which type of attribute can benefit from bar charts? What about distribution
    plots?
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪种类型的属性可以从条形图中受益？对于分布图呢？
- en: Why is it important to consider the feature correlation matrix for a dataset?
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么考虑数据集的特征相关性矩阵很重要？
- en: Aside from predictive tasks, what can we use ML models for (like we did in this
    chapter)?
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了预测任务之外，我们还能用机器学习模型做什么（就像我们在本章中做的那样）？
- en: Further reading
  id: totrans-455
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Be sure to check out the companion website for this book at [https://www.pycharm-book.com](https://www.pycharm-book.com).
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 一定要查看本书的配套网站：[https://www.pycharm-book.com](https://www.pycharm-book.com)。
- en: 'More information can be found in the following articles and reading materials:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息可以在以下文章和阅读材料中找到：
- en: Adams, W. R. (2017). *High-accuracy detection of early Parkinson’s Disease using
    multiple characteristics of finger movement while typing*. PloS one, *12*(11),
    e0188226.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adams, W. R. (2017). *High-accuracy detection of early Parkinson’s Disease using
    multiple characteristics of finger movement while typing*. PloS one, *12*(11),
    e0188226。
- en: 'The *Tappy Keystroke Data with Parkinson’s Patients* data, uploaded by Patrick
    DeKelly: [https://www.kaggle.com/valkling/tappy-keystroke-data-with-parkinsons-patients](https://www.kaggle.com/valkling/tappy-keystroke-data-with-parkinsons-patients).'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由 Patrick DeKelly 上传的 *Tappy Keystroke Data with Parkinson’s Patients* 数据：[https://www.kaggle.com/valkling/tappy-keystroke-data-with-parkinsons-patients](https://www.kaggle.com/valkling/tappy-keystroke-data-with-parkinsons-patients)。
- en: '*Building a Data Pipeline from Scratch*, by Alan Marazzi: [https://medium.com/the-data-experience/building-a-data-pipeline-from-scratch-32b712cfb1db](https://medium.com/the-data-experience/building-a-data-pipeline-from-scratch-32b712cfb1db).'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*从零开始构建数据管道*，作者 Alan Marazzi：[https://medium.com/the-data-experience/building-a-data-pipeline-from-scratch-32b712cfb1db](https://medium.com/the-data-experience/building-a-data-pipeline-from-scratch-32b712cfb1db)。'
- en: '*A Business Perspective to Designing an Enterprise-Level Data Science Pipeline*,
    by Vikram Reddy: [https://www.datascience.com/blog/designing-an-enterprise-level-data-science-pipeline](https://www.datascience.com/blog/designing-an-enterprise-level-data-science-pipeline).'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*设计企业级数据科学管道的商业视角*，作者 Vikram Reddy：[https://www.datascience.com/blog/designing-an-enterprise-level-data-science-pipeline](https://www.datascience.com/blog/designing-an-enterprise-level-data-science-pipeline)。'
- en: '*Data Science for Startups: Data Pipelines*, by Ben Weber: [https://towardsdatascience.com/data-science-for-startups-data-pipelines-786f6746a59a](https://towardsdatascience.com/data-science-for-startups-data-pipelines-786f6746a59a).'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《初创公司数据科学：数据管道》*，作者 Ben Weber：[https://towardsdatascience.com/data-science-for-startups-data-pipelines-786f6746a59a](https://towardsdatascience.com/data-science-for-startups-data-pipelines-786f6746a59a)。'
- en: 'Documentation for the pandas library: [https://pandas.pydata.org/pandas-docs/stable/](https://pandas.pydata.org/pandas-docs/stable/).'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas 库的文档：[https://pandas.pydata.org/pandas-docs/stable/](https://pandas.pydata.org/pandas-docs/stable/)。
- en: 'Part 5: Plugins and Conclusion'
  id: totrans-464
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5部分：插件和结论
- en: This part will introduce readers to the concept of PyCharm plugins and walk
    through the process of downloading plugins and adding them to their PyCharm environment.
    It will also go into details regarding the most popular plugins and how they can
    optimize a programmer’s productivity even further. We’ll also gloss over important
    topics discussed in previous chapters of the book and offers a comprehensive view
    on PyCharm’s most popular features.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分将向读者介绍 PyCharm 插件的概念，并指导他们下载插件并将其添加到 PyCharm 环境中。它还将详细介绍最受欢迎的插件以及它们如何进一步优化程序员的效率。我们还将简要回顾书中前几章讨论的重要主题，并全面展示
    PyCharm 最受欢迎的功能。
- en: 'This part has the following chapters:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 15*](B19644_15.xhtml#_idTextAnchor379), *More Possibilities with
    PyCharm Plugins*'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第15章*](B19644_15.xhtml#_idTextAnchor379)，*使用 PyCharm 插件拓展更多可能性*'
- en: '[*Chapter 16*](B19644_16.xhtml#_idTextAnchor401), *Future Developments*'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第16章*](B19644_16.xhtml#_idTextAnchor401)，*未来发展趋势*'
