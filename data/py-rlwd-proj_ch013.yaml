- en: Chapter 9
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章
- en: 'Project 3.1: Data Cleaning Base Application'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 项目3.1：数据清理基础应用程序
- en: Data validation, cleaning, converting, and standardizing are steps required
    to transform raw data acquired from source applications into something that can
    be used for analytical purposes. Since we started using a small data set of very
    clean data, we may need to improvise a bit to create some ”dirty” raw data. A
    good alternative is to search for more complicated, raw data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据验证、清理、转换和标准化是将从源应用程序获取的原始数据转换为可用于分析目的所需的步骤。由于我们开始使用一个非常干净的小数据集，我们可能需要稍作改进以创建一些“脏”原始数据。一个好的替代方案是寻找更复杂、更原始的数据。
- en: This chapter will guide you through the design of a data cleaning application,
    separate from the raw data acquisition. Many details of cleaning, converting,
    and standardizing will be left for subsequent projects. This initial project creates
    a foundation that will be extended by adding features. The idea is to prepare
    for the goal of a complete data pipeline that starts with acquisition and passes
    the data through a separate cleaning stage. We want to exploit the Linux principle
    of having applications connected by a shared buffer, often referred to as a shell
    pipeline.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将指导您设计一个数据清理应用程序，与原始数据获取分开。许多清理、转换和标准化的细节将留给后续项目。这个初始项目通过添加功能来扩展基础。目标是准备一个完整的数据管道的目标，从获取开始，并通过一个单独的清理阶段传递数据。我们希望利用Linux原则，即通过共享缓冲区连接应用程序，通常称为shell管道。
- en: 'This chapter will cover a number of skills related to the design of data validation
    and cleaning applications:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖与数据验证和清理应用程序设计相关的多个技能：
- en: CLI architecture and how to design a pipeline of processes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLI架构以及如何设计流程管道
- en: The core concepts of validating, cleaning, converting, and standardizing raw
    data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证、清理、转换和标准化原始数据的核心概念
- en: 'We won’t address all the aspects of converting and standardizing data in this
    chapter. Projects in [*Chapter** 10*](ch014.xhtml#x1-22900010), [*Data Cleaning
    Features*](ch014.xhtml#x1-22900010) will expand on many conversion topics. The
    project in [*Chapter** 12*](ch016.xhtml#x1-27600012), [*Project 3.8: Integrated
    Data* *Acquisition Web Service*](ch016.xhtml#x1-27600012) will address the integrated
    pipeline idea. For now, we want to build an adaptable base application that can
    be extended to add features.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章不会涉及转换和标准化数据的所有方面。第10章[*数据清理功能*](ch014.xhtml#x1-22900010)的项目将扩展许多转换主题。第12章[*项目3.8：集成数据*
    *获取Web服务*](ch016.xhtml#x1-27600012)的项目将解决集成管道的概念。目前，我们希望构建一个可扩展的基础应用程序，可以添加功能。
- en: We’ll start with a description of an idealized data cleaning application.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从对理想化数据清理应用程序的描述开始。
- en: 9.1 Description
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 描述
- en: We need to build a data validating, cleaning, and standardizing application.
    A data inspection notebook is a handy starting point for this design work. The
    goal is a fully-automated application to reflect the lessons learned from inspecting
    the data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要构建一个数据验证、清理和标准化的应用程序。数据检查笔记本是进行此设计工作的便捷起点。目标是创建一个完全自动化的应用程序，以反映从检查数据中学到的经验教训。
- en: 'A data preparation pipeline has the following conceptual tasks:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备管道具有以下概念任务：
- en: Validate the acquired source text to be sure it’s usable and to mark invalid
    data for remediation.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证获取的源文本以确保其可用性，并标记无效数据以进行修复。
- en: Clean any invalid raw data where necessary; this expands the available data
    in those cases where sensible cleaning can be defined.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在必要时清理任何无效的原始数据；这扩大了在可以定义合理清理的情况下可用的数据。
- en: Convert the validated and cleaned source data from text (or bytes) to usable
    Python objects.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将验证和清理后的源数据从文本（或字节）转换为可用的Python对象。
- en: Where necessary, standardize the code or ranges of source data. The requirements
    here vary with the problem domain.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在必要时，标准化源数据的代码或范围。这里的要求因问题域而异。
- en: 'The goal is to create clean, standardized data for subsequent analysis. Surprises
    occur all the time. There are several sources:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是创建干净、标准化的数据，以便进行后续分析。意外情况时常发生。有几个来源：
- en: Technical problems with file formats of the upstream software. The intent of
    the acquisition program is to isolate physical format issues.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上游软件文件格式的问题。获取程序的目标是隔离物理格式问题。
- en: Data representation problems with the source data. The intent of this project
    is to isolate the validity and standardization of the values.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源数据的数据表示问题。本项目的目的是隔离值的验证和标准化。
- en: Once cleaned, the data itself may still contain surprising relationships, trends,
    or distributions. This is discovered with later projects that create analytic
    notebooks and reports. Sometimes a surprise comes from finding the *Null Hypothesis*
    is true and the data only shows insignificant random variation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦清理完毕，数据本身可能仍然包含令人惊讶的关系、趋势或分布。这是通过后续项目创建分析笔记本和报告来发现的。有时，惊喜来自于发现**零假设**是正确的，而数据只显示出不显著的随机变化。
- en: In many practical cases, the first three steps — validate, clean, and convert
    — are often combined into a single function call. For example, when dealing with
    numeric values, the `int()` or `float()` functions will validate and convert a
    value, raising an exception for invalid numbers.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多实际情况下，前三个步骤——验证、清理和转换——通常被合并成一个单独的函数调用。例如，当处理数值时，`int()` 或 `float()` 函数将验证并转换一个值，对于无效的数字将引发异常。
- en: In a few edge cases, these steps need to be considered in isolation – often
    because there’s a tangled interaction between validation and cleaning. For example,
    some data is plagued by dropping the leading zeros from US postal codes. This
    can be a tangled problem where the data is superficially invalid but can be reliably
    cleaned before attempting validation. In this case, validating the postal code
    against an official list of codes comes after cleaning, not before. Since the
    data will remain as text, there’s no actual conversion step after the clean-and-validate
    composite step.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些边缘情况下，需要单独考虑这些步骤——通常是因为验证和清理之间存在复杂的交互。例如，某些数据受到美国邮政编码前导零丢失的困扰。这可能是一个复杂的问题，数据表面上无效，但在尝试验证之前可以可靠地清理。在这种情况下，验证邮政编码是否与官方代码列表一致是在清理之后而不是之前。由于数据将保持为文本格式，在清理和验证组合步骤之后没有实际的转换步骤。
- en: 9.1.1 User experience
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 用户体验
- en: The overall **User Experience** (**UX**) will be two command-line applications.
    The first application will acquire the raw data, and the second will clean the
    data. Each has options to fine-tune the `acquire` and `cleanse` steps.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 整体 **用户体验** （**UX**） 将是两个命令行应用程序。第一个应用程序将采集原始数据，第二个将清理数据。每个应用程序都有选项来微调 `acquire`
    和 `cleanse` 步骤。
- en: 'There are several variations on the `acquire` command, shown in earlier chapters.
    Most notably, [*Chapter** 3*](ch007.xhtml#x1-560003), [*Project 1.1: Data Acquisition
    Base Application*](ch007.xhtml#x1-560003), [*Chapter** 4*](ch008.xhtml#x1-780004),
    [*Data Acquisition Features: Web APIs and Scraping*](ch008.xhtml#x1-780004), and
    [*Chapter** 5*](ch009.xhtml#x1-1140005), [*Data Acquisition Features: SQL Database*](ch009.xhtml#x1-1140005).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中展示了 `acquire` 命令的几种变体。最值得注意的是，[**第3章**](ch007.xhtml#x1-560003)，[**项目1.1：数据采集基础应用程序**](ch007.xhtml#x1-560003)，[**第4章**](ch008.xhtml#x1-780004)，[**数据采集功能：Web
    API 和抓取**](ch008.xhtml#x1-780004)，以及 [**第5章**](ch009.xhtml#x1-1140005)，[**数据采集功能：SQL
    数据库**](ch009.xhtml#x1-1140005)。
- en: 'For the **clean** application, the expected command-line should like something
    like this:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 **clean** 应用程序，预期的命令行应该类似于以下内容：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `-o`` analysis` specifies a directory into which the resulting clean data
    is written.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`-o` `analysis` 指定了一个目录，结果清理后的数据将被写入该目录。'
- en: The `-i`` quartet/Series_1.ndjson` specifies the path to the source data file.
    This is a file written by the acquisition application.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`-i` `quartet/Series_1.ndjson` 指定了源数据文件的路径。这是一个由采集应用程序编写的文件。'
- en: Note that we’re not using a positional argument to name the input file. The
    use of a positional argument for a filename is a common provision in many — but
    not all — Linux commands. The reason to avoid positional arguments is to make
    it easier to adapt this to become part of a pipeline of processing stages.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们不是使用位置参数来命名输入文件。对于文件名使用位置参数是许多——但不是所有——Linux 命令的常见规定。避免使用位置参数的原因是为了使它更容易适应成为处理阶段的一部分的管道。
- en: 'Specifically, we’d like the following to work, also:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们希望以下内容也能正常工作：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This shell line has two commands, one to do the raw data acquisition, and the
    other to perform the validation and cleaning. The acquisition command uses the
    `-s`` Series_1Pair` argument to name a specific series extraction class. This
    class will be used to create a single series as output. The `--csv`` source.csv`
    argument names the input file to process. Other options could name RESTful APIs
    or provide a database connection string.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这条shell行包含两个命令，一个用于原始数据获取，另一个用于验证和清理。获取命令使用`-s``Series_1Pair`参数来命名一个特定的系列提取类。这个类将被用来创建单个系列作为输出。`--csv``source.csv`参数命名了要处理的输入文件。其他选项可以命名RESTful
    API或提供数据库连接字符串。
- en: The second command reads the output from the first command and writes this to
    a file. The file is named by the `-o` argument value in the second command.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个命令读取第一个命令的输出并将其写入文件。文件名由第二个命令中的`-o`参数值命名。
- en: This pipeline concept, made available with the shell’s `|` operator, means these
    two processes will run concurrently. This means data is passed from one process
    to the other as it becomes available. For very large source files, cleaning data
    as it’s being acquired can reduce processing time dramatically.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个管道概念，通过shell的`|`操作符提供，意味着这两个进程将并发运行。这意味着数据在可用时从一个进程传递到另一个进程。对于非常大的源文件，在获取数据时清理数据可以显著减少处理时间。
- en: 'In [*Project 3.6: Integration to create an acquisition pipeline*](ch014.xhtml#x1-2510005)
    we’ll expand on this design to include some ideas for concurrent processing.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*项目3.6：创建获取管道的集成*](ch014.xhtml#x1-2510005)中，我们将扩展这个设计，包括一些并发处理的想法。
- en: Now that we’ve seen an overview of the application’s purpose, let’s take a look
    at the source data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了应用程序目的的概述，让我们来看看源数据。
- en: 9.1.2 Source data
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.2 源数据
- en: The earlier projects produced source data in an approximately consistent format.
    These projects focused on acquiring data that is text. The individual samples
    were transformed into small JSON-friendly dictionaries, using the NDJSON format.
    This can simplify the validation and cleaning operation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 早期项目以大约一致的形式产生了源数据。这些项目专注于获取文本数据。单个样本被转换成小型的JSON友好字典，使用NDJSON格式。这可以简化验证和清理操作。
- en: The NDJSON file format is described at [http://ndjson.org](http://ndjson.org)
    and [https://jsonlines.org](https://jsonlines.org).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: NDJSON文件格式在[http://ndjson.org](http://ndjson.org)和[https://jsonlines.org](https://jsonlines.org)中描述。
- en: 'There are two design principles behind the **acquire** application:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**acquire**应用程序背后有两个设计原则：'
- en: Preserve the original source data as much as possible.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能保留原始源数据。
- en: Perform the fewest text transformations needed during acquisition.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在获取过程中进行最少的文本转换。
- en: Preserving the source data makes it slightly easier to locate problems when
    there are unexpected changes to source applications. Minimizing the text transformations,
    similarly, keeps the data closer to the source. Moving from a variety of representations
    to a single representation simplifies the data cleaning and transformation steps.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 保留源数据在源应用程序出现意外变化时稍微容易一些找到问题。同样，最小化文本转换，使数据更接近源。从多种表示形式到单一表示形式简化了数据清理和转换步骤。
- en: All of the data acquisition projects involve some kind of textual transformation
    from a source representation to ND JSON.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据获取项目都涉及从源表示形式到ND JSON的某种文本转换。
- en: '|'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Chapter** | **Section** | **Source** |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| **章节** | **节** | **来源** |  |'
- en: '|'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| [3](ch007.xhtml#x1-560003) | [*Chapter** 3*](ch007.xhtml#x1-560003), [*Project
    1.1: Data Acquisition Base Application*](ch007.xhtml#x1-560003) | CSV parsing
    |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| [3](ch007.xhtml#x1-560003) | [*章节3*](ch007.xhtml#x1-560003)，[*项目1.1：数据获取基础应用程序*](ch007.xhtml#x1-560003)
    | CSV解析 |  |'
- en: '| [4](ch008.xhtml#x1-780004) | [*Project 1.2: Acquire data from a web service*](ch008.xhtml#x1-790001)
    | Zipped CSV or JSON |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| [4](ch008.xhtml#x1-780004) | [*项目1.2：从网络服务获取数据*](ch008.xhtml#x1-790001) |
    压缩CSV或JSON |  |'
- en: '| [4](ch008.xhtml#x1-780004) | [*Project 1.3: Scrape data from a web page*](ch008.xhtml#x1-970002)
    | HTML |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| [4](ch008.xhtml#x1-780004) | [*项目1.3：从网页抓取数据*](ch008.xhtml#x1-970002) | HTML
    |  |'
- en: '| [5](ch009.xhtml#x1-1140005) | [*Project 1.5: Acquire data from a SQL extract*](ch009.xhtml#x1-1260002)
    | SQL Extract |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| [5](ch009.xhtml#x1-1140005) | [*项目1.5：从SQL提取获取数据*](ch009.xhtml#x1-1260002)
    | SQL提取 |  |'
- en: '|'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  |  |  |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |'
- en: In some cases — i.e., extracting HTML — the textual changes to peel the markup
    away from the data is profound. The SQL database extract involves undoing the
    database’s internal representation of numbers or dates and writing the values
    as text. In some cases, the text transformations are minor.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下——例如，提取HTML——从数据中剥离标记的文本更改是深刻的。SQL数据库提取涉及撤销数据库对数字或日期的内部表示，并将值作为文本写入。在某些情况下，文本转换是微小的。
- en: 9.1.3 Result data
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.3 结果数据
- en: 'The cleaned output files will be ND JSON; similar to the raw input files. We’ll
    address this output file format in detail in [*Chapter** 11*](ch015.xhtml#x1-26400011),
    [*Project 3.7: Interim Data* *Persistence*](ch015.xhtml#x1-26400011). For this
    project, it’s easiest to stick with writing the JSON representation of a Pydantic
    dataclass.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 清洗后的输出文件将是ND JSON；类似于原始输入文件。我们将在[**第11章**](ch015.xhtml#x1-26400011)[**项目3.7：临时数据持久化**](ch015.xhtml#x1-26400011)中详细讨论这种输出文件格式。对于这个项目，最简单的方法是坚持编写Pydantic数据类的JSON表示。
- en: For Python’s native `dataclasses`, the `dataclasses.asdict()` function will
    produce a dictionary from a dataclass instance. The `json.dumps()` function will
    convert this to text in JSON syntax.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Python的本地`dataclasses`，`dataclasses.asdict()`函数将从数据类实例生成字典。`json.dumps()`函数将此转换为JSON语法的文本。
- en: For Pydantic dataclasses, however, the `asdict()` function can’t be used. There’s
    no built-in method for emitting the JSON representation of a `pydantic` dataclass
    instance.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于Pydantic数据类，`asdict()`函数不能使用。没有内置的方法来生成`pydantic`数据类实例的JSON表示。
- en: 'For version 1 of **Pydantic**, a slight change is required to write ND JSON.
    An expression like the following will emit a JSON serialization of a `pydantic`
    dataclass:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**Pydantic**的版本1，需要稍作修改来写入ND JSON。以下表达式将生成`pydantic`数据类的JSON序列化：
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This central feature is the `default=pydantic_encoder` argument value for the
    `json.dumps()` function. This will handle the proper decoding of the dataclass
    structure into JSON notation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这个核心功能是`json.dumps()`函数的`default=pydantic_encoder`参数值。这将处理将数据类结构正确解码为JSON记法。
- en: For version 2 of **pydantic**, there will be a slightly different approach.
    This makes use of a `RootModel[classname](object)` construct to extract the root
    model for a given class from an object. In this case, `RootModel[SeriesSample](result).model_dump()`
    will create a root model that can emit a nested dictionary structure. No special
    `pydantic_encoder` will be required for version 2.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**pydantic**的版本2，将采用略有不同的方法。这使用`RootModel[classname](object)`结构从对象中提取给定类的根模型。在这种情况下，`RootModel[SeriesSample](result).model_dump()`将创建一个可以生成嵌套字典结构的根模型。对于版本2，不需要特殊的`pydantic_encoder`。
- en: Now that we’ve looked at the inputs and outputs, we can survey the processing
    concepts. Additional processing details will wait for later projects.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了输入和输出，我们可以调查处理概念。额外的处理细节将留待后续项目。
- en: 9.1.4 Conversions and processing
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.4 转换和处理
- en: For this project, we’re trying to minimize the processing complications. In
    the next chapter, [*Chapter** 10*](ch014.xhtml#x1-22900010), [*Data Cleaning Features*](ch014.xhtml#x1-22900010),
    we’ll look at a number of additional processing requirements that will add complications.
    As a teaser for the projects in the next chapter, we’ll describe some of the kinds
    of field-level validation, cleaning, and conversion that may be required.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我们正在尝试最小化处理复杂性。在下一章[**第10章**](ch014.xhtml#x1-22900010)[**数据清洗功能**](ch014.xhtml#x1-22900010)中，我们将探讨一些额外的处理需求，这些需求将增加复杂性。作为下一章项目的预告，我们将描述一些可能需要的字段级验证、清理和转换的类型。
- en: One example we’ve focused on, Anscombe’s Quartet data, needs to be converted
    to a series of floating-point values. While this is painfully obvious, we’ve held
    off on the conversion from the text to the Python `float` object to illustrate
    the more general principle of separating the complications of acquiring data from
    analyzing the data. The output from this application will have each resulting
    ND JSON document with `float` values instead of `string` values.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关注的一个例子，Anscombe的四重奏数据，需要转换为一系列浮点值。虽然这很明显，但我们推迟了从文本到Python `float`对象的转换，以说明将获取数据的复杂性从分析数据的复杂性中分离出来的更普遍原则。此应用程序的输出将具有每个结果ND
    JSON文档，其中包含`float`值而不是`string`值。
- en: 'The distinction in the JSON documents will be tiny: the use of `"` for the
    raw-data strings. This will be omitted for the `float` values.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: JSON文档中的区别将非常小：对于原始数据字符串使用`"`。对于`float`值，这将被省略。
- en: This tiny detail is important because every data set will have distinct conversion
    requirements. The data inspection notebook will reveal data domains like text,
    integers, date-time stamps, durations, and a mixture of more specialized domains.
    It’s essential to examine the data before trusting any schema definition or documentation
    about the data.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这个小小的细节很重要，因为每个数据集都会有独特的转换要求。数据检查笔记本将揭示数据域，如文本、整数、日期时间戳、持续时间以及更多专业领域的混合。在信任任何关于数据的模式定义或文档之前，检查数据是至关重要的。
- en: 'We’ll look at three common kinds of complications:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨三种常见的复杂情况：
- en: Fields that must be decomposed.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须分解的字段。
- en: Fields which must be composed.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须组合的字段。
- en: Unions of sample types in a single collection.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单一集合中样本类型的并集。
- en: ”Opaque” codes used to replace particularly sensitive information.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于替换特别敏感信息的“不透明”代码。
- en: One complication is when multiple source values are collapsed into a single
    field. This single source value will need to be decomposed into multiple values
    for analysis. With the very clean data sets available from Kaggle, a need for
    decomposition is unusual. Enterprise data sets, on the other hand, will often
    have fields that are not properly decomposed into atomic values, reflecting optimizations
    or legacy processing requirements. For example, a product ID code may include
    a line of business and a year of introduction as part of the code. For example,
    a boat’s hull ID number might include ”421880182,” meaning it’s a 42-foot hull,
    serial number 188, completed in January 1982\. Three disparate items were all
    coded as digits. For analytical purposes, it may be necessary to separate the
    items that comprise the coded value. In other cases, several source fields will
    need to be combined. An example data set where a timestamp is decomposed into
    three separate fields can be found when looking at tidal data.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一种复杂情况是当多个源值合并到一个单一字段时。这个单一源值需要分解成多个值以进行分析。在Kaggle提供的非常干净的数据集中，分解的需求是不常见的。另一方面，企业数据集通常会有字段没有正确分解成原子值，这反映了优化或遗留处理需求。例如，产品ID代码可能包括业务线和引入年份作为代码的一部分。例如，一艘船的船体ID号码可能包括“421880182”，这意味着它是一艘42英尺的船体，序列号188，1982年1月完成。三个不同的项目都被编码为数字。出于分析目的，可能需要将构成编码值的项分开。在其他情况下，需要将几个源字段组合在一起。当查看潮汐数据时，可以找到一个将时间戳分解为三个单独字段的数据集示例。
- en: See [https://tidesandcurrents.noaa.gov/tide_predictions.html](https://tidesandcurrents.noaa.gov/tide_predictions.html)
    for Tide Predictions around the US. This site supports downloads in a variety
    of formats, as well as RESTful API requests for tide predictions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 查看美国潮汐预测的[https://tidesandcurrents.noaa.gov/tide_predictions.html](https://tidesandcurrents.noaa.gov/tide_predictions.html)。此网站支持多种格式的下载，以及针对潮汐预测的RESTful
    API请求。
- en: 'Each of the tidal events in an annual tide table has a timestamp. The timestamp
    is decomposed into three separate fields: the date, the day of the week, and the
    local time. The day of the week is helpful, but it is entirely derived from the
    date. The date and time need to be combined into a single datetime value to make
    this data useful. It’s common to use `datetime.combine(date,`` time)` to merge
    separate date and time values into a single value.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 年度潮汐表中每个潮汐事件都有一个时间戳。时间戳被分解为三个单独的字段：日期、星期几和当地时间。星期几是有帮助的，但它完全来自日期。日期和时间需要组合成一个单一的日期时间值，以便使这些数据有用。通常使用`datetime.combine(date,
    time)`将单独的日期和时间值合并成一个值。
- en: Sometimes a data set will have records of a variety of subtypes merged into
    a single collection. The various types are often discriminated from each other
    by the values of a field. A financial application might include a mixture of invoices
    and payments; many fields overlap, but the meanings of these two transaction types
    are dramatically different. A single field with a code value of ”I” or ”P” may
    be the only way to distinguish the types of business records represented.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 有时一个数据集将会有多种子类型记录合并到一个单一集合中。这些不同类型通常通过字段的值来区分。一个财务应用程序可能包括发票和付款的混合；许多字段重叠，但这两类交易类型的意义却截然不同。一个字段具有“
    I”或“ P”代码值的记录可能是区分表示的业务记录类型的唯一方式。
- en: When multiple subtypes are present, the collection can be called a *discriminated*
    *union* of subtypes; sometimes simply called a **union**. The discriminator and
    the subtypes suggest a class hierachy is required to describe the variety of sample
    types. A common base class is needed to describe the common fields, including
    the discriminator. Each subclass has a distinct definition for the fields unique
    to the subclass.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在多个子类型时，该集合可以称为子类型的**区分** **联合**；有时简单地称为**联合**。区分器和子类型表明需要一个类层次结构来描述样本类型的多样性。需要一个公共基类来描述包括区分器在内的公共字段。每个子类都有对子类独特字段的特定定义。
- en: One additional complication stems from data sources with ”opaque” data. These
    are string fields that can be used for equality comparison, but nothing else.
    These values are often the result of a data analysis approach called **masking**,
    **deidentification**, or **pseudonymization**. This is sometimes also called ”tokenizing”
    because an opaque token has replaced the sensitive data. In banking, for example,
    it’s common for analytical data to have account numbers or payment card numbers
    transformed into opaque values. These can be used to aggregate behavior, but cannot
    be used to identify an individual account holder or payment card. These fields
    must be treated as strings, and no other processing can be done.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个额外的复杂性来自“不透明”的数据源。这些是可以用于相等比较的字符串字段，但不能用于其他目的。这些值通常是数据分析方法**掩码**、**去标识化**或**匿名化**的结果。有时这也被称为“标记化”，因为不透明的标记已经替换了敏感数据。例如，在银行业务中，分析数据将账户号码或支付卡号码转换为不透明值是常见的。这些可以用于汇总行为，但不能用于识别个人账户持有人或支付卡。这些字段必须被视为字符串，不能进行其他处理。
- en: For now, we’ll defer the implementation details of these complications to a
    later chapter. The ideas should inform design decisions for the initial, foundational
    application.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们将推迟这些复杂性的实现细节到后面的章节。这些想法应该为初始、基础应用程序的设计决策提供信息。
- en: In addition to clean valid data, the application needs to produce information
    about the invalid data. Next, we’ll look at the logs and error reports.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 除了干净的有效数据外，应用程序还需要产生关于无效数据的信息。接下来，我们将查看日志和错误报告。
- en: 9.1.5 Error reports
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.5 错误报告
- en: The central feature of this application is to output files with valid, useful
    data for analytic purposes. We’ve left off some details of what happens when an
    acquired document isn’t actually usable.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用程序的核心功能是输出用于分析目的的有效、有用的数据文件。我们已经省略了一些关于获取的文档实际上不可用时会发生什么细节。
- en: 'Here are a number of choices related to the observability of invalid data:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些与无效数据可观察性相关的选择：
- en: Raise an overall exception and stop. This is appropriate when working with carefully-curated
    data sets like the Anscombe Quartet.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抛出一个总体异常并停止。这在处理像Anscombe四重奏这样的精心策划的数据集时是合适的。
- en: Make all of the bad data observable, either through the log or by writing bad
    data to a separate rejected samples file.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使所有坏数据可观察，无论是通过日志还是将坏数据写入单独的拒绝样本文件。
- en: Silently reject the bad data. This is often used with large data sources where
    there is no curation or quality control over the source.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 悄悄拒绝坏数据。这通常用于没有对源数据进行策划或质量控制的大型数据源。
- en: In all cases, the summary counts of acquired data, usable analytic data, and
    cleaned, and rejected data are essential. It’s imperative to be sure the number
    of raw records read is accounted for, and the provenance of cleaned and rejected
    data is clear. The summary counts, in many cases, are the primary way to observe
    changes in data sources. A non-zero error count, may be so important that it’s
    used as the final exit status code for the cleaning application.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有情况下，获取的数据、可用的分析数据和清理以及拒绝的数据的总结计数都是必不可少的。确保读取的原始记录数被计入，并且清理和拒绝的数据的来源清晰。在许多情况下，总结计数是观察数据源变化的主要方式。非零错误计数可能非常重要，以至于它被用作清理应用程序的最终退出状态代码。
- en: 'In addition to the observability of bad data, we may be able to clean the source
    data. There are several choices here, also:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 除了坏数据可观察性之外，我们可能能够清理源数据。这里也有几个选择：
- en: Log the details of each object where cleaning is done. This is often used with
    data coming from a spreadsheet where the unexpected data may be rows that need
    to be corrected manually.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录每个对象在清理过程中所做的详细情况。这通常用于来自电子表格的数据，其中意外的数据可能是需要手动纠正的行。
- en: Count the number of items cleaned without the supporting details. This is often
    used with large data sources where changes are frequent.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算没有支持细节的已清洗项目数量。这通常用于数据源变化频繁的大型数据源。
- en: Quietly clean the bad data as an expected, normal operational step. This is
    often used when raw data comes directly from measurement devices in unreliable
    environments, perhaps in space, or at the bottom of the ocean.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将不良数据作为预期、正常的操作步骤进行静默清理。这通常用于原始数据直接来自不可靠环境中的测量设备时，例如在太空或海底。
- en: Further, each field may have distinct rules for whether or not cleaning bad
    data is a significant concern or a common, expected operation. The intersection
    of observability and automated cleaning has a large number of alternatives.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，每个字段可能都有不同的规则，以确定清洗不良数据是否是一个重要的问题或一个常见、预期的操作。可观察性和自动化清洗的交集有许多替代方案。
- en: The solutions to data cleaning and standardization are often a matter of deep,
    ongoing conversations with users. Each data acquisition pipeline is unique with
    regard to error reporting and data cleaning.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗和标准化的解决方案通常需要与用户进行深入、持续的对话。每个数据采集管道在错误报告和数据清洗方面都是独特的。
- en: It’s sometimes necessary to have a command-line option to choose between logging
    each error or simply summarizing the number of errors. Additionally, the application
    might return a non-zero exit code when any bad records are found; this permits
    a parent application (e.g., a shell script) to stop processing in the presence
    of errors.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 有时需要命令行选项来选择记录每个错误或简单地总结错误数量。此外，当发现任何不良记录时，应用程序可能会返回非零退出代码；这允许父应用程序（例如，shell脚本）在出现错误时停止处理。
- en: We’ve looked at the overall processing, the source files, the result files,
    and some of the error-reporting alternatives that might be used. In the next section,
    we’ll look at some design approaches we can use to implement this application.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了整体处理、源文件、结果文件以及可能使用的某些错误报告替代方案。在下一节中，我们将探讨我们可以用来实现此应用的一些设计方法。
- en: 9.2 Approach
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 方法
- en: We’ll take some guidance from the C4 model ( [https://c4model.com](https://c4model.com))
    when looking at our approach.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们审视我们的方法时，我们将从C4模型（[https://c4model.com](https://c4model.com)）中汲取一些指导。
- en: '**Context**: For this project, the context diagram has expanded to three use
    cases: acquire, inspect, and clean.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文**：对于本项目，上下文图已扩展到三个用例：获取、检查和清洗。'
- en: '**Containers**: There’s one container for the various applications: the user’s
    personal computer.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器**：有一个容器用于各种应用：用户的个人电脑。'
- en: '**Components**: There are two significantly different collections of software
    components: the acquisition program and the cleaning program.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组件**：有两个显著不同的软件组件集合：采集程序和清洗程序。'
- en: '**Code**: We’ll touch on this to provide some suggested directions.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码**：我们将简要提及，以提供一些建议的方向。'
- en: A context diagram for this application is shown in [*Figure 9.1*](#9.1).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用的上下文图如图[*图9.1*](#9.1)所示。
- en: '![Figure 9.1: Context Diagram ](img/file40.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1：上下文图](img/file40.jpg)'
- en: 'Figure 9.1: Context Diagram'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1：上下文图
- en: A component diagram for the conversion application isn’t going to be as complicated
    as the component diagrams for acquisition applications. One reason for this is
    there are no choices for reading, extracting, or downloading raw data files. The
    source files are the ND JSON files created by the acquisition application.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 转换应用的组件图不会像采集应用的组件图那样复杂。一个原因是读取、提取或下载原始数据文件没有选择。源文件是由采集应用创建的ND JSON文件。
- en: The second reason the conversion programs tend to be simpler is they often rely
    on built-in Python-type definitions, and packages like `pydantic` to provide the
    needed conversion processing. The complications of parsing HTML or XML sources
    were isolated in the acquisition layer, permitting this application to focus on
    the problem domain data types and relationships.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 转换程序通常更简单的原因是它们通常依赖于内置的Python类型定义，以及像`pydantic`这样的包来提供所需的转换处理。解析HTML或XML源的复杂性被隔离在采集层，允许此应用专注于问题域数据类型和关系。
- en: The components for this application are shown in [*Figure 9.2*](#9.2).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用的组件如图[*图9.2*](#9.2)所示。
- en: '![Figure 9.2: Component Diagram ](img/file41.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2：组件图](img/file41.jpg)'
- en: 'Figure 9.2: Component Diagram'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：组件图
- en: Note that we’ve used a dotted ”depends on” arrow. This does not show the data
    flow from acquire to clean. It shows how the clean application depends on the
    acquire application’s output.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用了点划线的“依赖于”箭头。这并不显示从获取到清洗的数据流。它显示了清洗应用如何依赖于获取应用的输出。
- en: The design for the **clean** application often involves an almost purely functional
    design. Class definitions — of course — can be used. Classes don’t seem to be
    helpful when the application processing involves stateless, immutable objects.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**清洗**应用的设计通常涉及几乎完全功能性的设计。当然，可以使用类定义。当应用处理涉及无状态、不可变对象时，类似乎没有帮助。'
- en: In rare cases, a cleaning application will be required to perform dramatic reorganizations
    of data. It may be necessary to accumulate details from a variety of transactions,
    updating the state of a composite object. For example, there may be multiple payments
    for an invoice that must be combined for reconciliation purposes. In this kind
    of application, associating payments and invoices may require working through
    sophisticated matching rules.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在罕见情况下，清洗应用可能需要进行数据的大规模重组。可能需要从各种交易中积累细节，更新复合对象的状态。例如，可能存在多个付款项需要合并以进行对账。在这种应用中，关联付款和发票可能需要通过复杂的匹配规则进行操作。
- en: Note that the **clean** application and the **acquire** application will both
    share a common set of dataclasses. These classes represent the source data, the
    output from the **acquire** application. They also define the input to the **clean**
    application. A separate set of dataclasses represent the working values used for
    later analysis applications.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，**清洗**应用和**获取**应用将共享一组共同的数据类。这些类代表源数据，**获取**应用的输出。它们还定义了**清洗**应用的输入。另一组数据类代表用于后续分析应用的作业值。
- en: 'Our goal is to create three modules:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是创建三个模块：
- en: '`clean.py`: The main application.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean.py`：主要应用。'
- en: '`analytical_model.py`: A module with dataclass definitions for the pure-Python
    objects that we’ll be working with. These classes will — generally — be created
    from JSON-friendly dictionaries with string values.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`analytical_model.py`：一个包含用于纯Python对象的dataclass定义的模块。这些类通常将由具有字符串值的JSON友好字典创建。'
- en: '`conversions.py`: A module with any specialized validation, cleaning, and conversion
    functions.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conversions.py`：一个包含任何专用验证、清洗和转换函数的模块。'
- en: If needed, any application-specific conversion functions may be required to
    transform source values to ”clean,” usable Python objects. If this can’t be done,
    the function can instead raise `ValueError` exceptions for invalid data, following
    the established pattern for functions like Python’s built-in `float()` function.
    Additionally, `TypeError` exceptions may be helpful when the object — as a whole
    — is invalid. In some cases, the `assert` statement is used, and an `AssertionError`
    may be raised to indicate invalid data.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，可能需要任何特定应用转换函数来将源值转换为“清洗”的、可用的Python对象。如果无法完成此操作，函数可以改为抛出`ValueError`异常，以遵循Python内置`float()`函数等函数的既定模式。此外，当整个对象无效时，`TypeError`异常可能很有帮助。在某些情况下，使用`assert`语句，并可能抛出`AssertionError`来指示无效数据。
- en: For this baseline application, we’ll stick to the simpler and more common design
    pattern. We’ll look at individual functions that combine validation and cleaning.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个基线应用，我们将坚持使用更简单、更常见的模式。我们将查看结合验证和清洗的单独函数。
- en: 9.2.1 Model module refactoring
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 模型模块重构
- en: 'We appear to have two distinct models: the ”as-acquired” model with text fields,
    and the ”to-be-analyzed” model with proper Python types, like `float` and `int`.
    The presence of multiple variations on the model means we either need a lot of
    distinct class names, or two distinct modules as namespaces to keep the classes
    organized.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们似乎有两个不同的模型：带有文本字段的“获取”模型和带有适当Python类型（如`float`和`int`）的“待分析”模型。模型存在多个变体意味着我们需要很多不同的类名，或者需要两个不同的模块作为命名空间来组织类。
- en: The cleaning application is the only application where the acquire and analysis
    models are **both** used. All other applications either acquire raw data or work
    with clean analysis data.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 清洗应用是唯一一个同时使用获取和分析模型的应用。其他应用要么获取原始数据，要么处理清洗后的分析数据。
- en: 'The previous examples had a single `model.py` module with dataclasses for the
    acquired data. At this point, it has become more clear that this was not a great
    long-term decision. Because there are two distinct variations on the data model,
    the generic `model` module name needs to be refactored. To distinguish the acquired
    data model from the analytic data model, a prefix should be adequate: the module
    names can be `acquire_model` and `analysis_model`.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的例子中有一个单独的 `model.py` 模块，用于存储获取的数据的 dataclasses。此时，已经更加明确，这并不是一个长期的好决定。因为数据模型有两种不同的变体，所以通用的
    `model` 模块名称需要重构。为了区分获取的数据模型和分析数据模型，前缀应该足够：模块名称可以是 `acquire_model` 和 `analysis_model`。
- en: (The English parts of speech don’t match exactly. We’d rather not have to type
    ”acquisition_model”. The slightly shorter name seems easier to work with and clear
    enough.)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: （英语的词性不完全匹配。我们宁愿不用输入 “acquisition_model”。略短的名称似乎更容易处理，并且足够清晰。）
- en: Within these two model files, the class names can be the same. We might have
    names `acquire_model.SeriesSample` and `analysis_model.SeriesSample` as distinct
    classes.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个模型文件中，类名可以是相同的。我们可能有 `acquire_model.SeriesSample` 和 `analysis_model.SeriesSample`
    这样的不同类名。
- en: To an extent, we can sometimes copy the acquired model module to create the
    analysis model module. We’d need to change `from`` dataclasses`` import`` dataclass`
    to the **Pydantic** version, `from`` pydantic`` import`` dataclass`. This is a
    very small change, which makes it easy to start with. In some older versions of
    **Pydantic** and **mypy**, the **Pydantic** version of `dataclass` doesn’t expose
    the attribute types in a way that is transparent to the **mypy** tool.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在一定程度上，我们有时可以复制获取的模型模块来创建分析模型模块。我们需要将 `from dataclasses import dataclass` 改为
    **Pydantic** 版本的 `from pydantic import dataclass`。这是一个非常小的改动，使得开始变得容易。在某些较旧的 **Pydantic**
    和 **mypy** 版本中，**Pydantic** 版本的 `dataclass` 没有以对 **mypy** 工具透明的方式暴露属性类型。
- en: In many cases, it can work out well to import `BaseModel` and use this as the
    parent class for the analytic models. Using the `pydantic.BaseModel` parent class
    often has a better coexistence with the **mypy** tool. This requires a larger
    change when upgrading from dataclasses to leverage the **pydantic** package. Since
    it’s beneficial when using the **mypy** tool, it’s the path we recommend following.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，导入 `BaseModel` 并将其用作分析模型的父类通常可以更好地与 **mypy** 工具共存。当从 dataclasses 升级到利用
    **pydantic** 包时，这需要较大的改动。由于它在使用 **mypy** 工具时很有益，所以我们推荐遵循此路径。
- en: This **Pydantic** version of `dataclass` introduces a separate validator method
    that will be used (automatically) to process fields. For simple class definitions
    with a relatively clear mapping from the acquire class to the analysis class,
    a small change is required to the class definition.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 此 **Pydantic** 版本的 `dataclass` 引入了一个单独的验证器方法，该方法将（自动）用于处理字段。对于从获取类到分析类的映射相对清晰的简单类定义，需要修改类定义。
- en: 'One common design pattern for this new analysis model class is shown in the
    following example for **Pydantic** version 1:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个新的分析模型类，以下是一个 **Pydantic** 版本 1 的常见设计模式示例：
- en: '[PRE3]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This design defines a class-level method, `clean_value()`, to handle cleaning
    the source data when it’s a string. The validator has the `@validator()` decorator
    to provide the attribute names to which this function applies, as well as the
    specific stage in the sequence of operations. In this case, `pre=True` means this
    validation applies **before** the individual fields are validated and converted
    to useful types.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 此设计定义了一个类级别的方法 `clean_value()`，用于处理当数据是字符串时的数据清洗。验证器使用 `@validator()` 装饰器提供该函数应用的属性名称，以及操作序列中的特定阶段。在这种情况下，`pre=True`
    表示此验证在各个字段被验证并转换为有用类型之前进行。
- en: This will be replaced by a number of much more flexible alternatives in **Pydantic**
    version 2\. The newer release will step away from the `pre=True` syntax used to
    assure this is done prior to the built-in handler accessing the field.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在 **Pydantic** 版本 2 中被许多更加灵活的替代方案所取代。新版本将放弃用于确保在内置处理程序访问字段之前完成此操作的 `pre=True`
    语法。
- en: The Pydantic 2 release will introduce a radically new approach using annotations
    to specify validation rules. It will also retain a decorator that’s very similar
    to the old version 1 validation.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Pydantic 2 版本的发布将引入一种使用注解来指定验证规则的根本性新方法。它还将保留一个与旧版本 1 验证非常相似的装饰器。
- en: One migration path is to replace `validator` with `field_validator`. This will
    require changing the `pre=True` or `post=True` with a more universal `mode=’before’`
    or `mode=’after’`. This new approach permits writing field validators that “wrap”
    the conversion handler with both before and after processing.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一种迁移路径是将 `validator` 替换为 `field_validator`。这将需要将 `pre=True` 或 `post=True` 更改为更通用的
    `mode=’before’` 或 `mode=’after’`。这种新方法允许编写在前后处理中“包装”转换处理器的字段验证器。
- en: To use **Pydantic** version two, use `@field_validator(’x’,`` ’y’,`` mode=’before’)`
    to replace the `@validator` decorator in the example. The `import` must also change
    to reflect the new name of the decorator.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 **Pydantic** 第二版，使用 `@field_validator(‘x’，‘y’，mode=‘before’)` 来替换示例中的 `@validator`
    装饰器。`import` 也必须更改以反映装饰器的新名称。
- en: This validator function handles the case where the string version of source
    data can include Unicode `U+200B`, a special character called the zero-width space.
    In Python, we can use `"\N{ZERO`` WIDTH`` SPACE}"` to make this character visible.
    While lengthy, this name seems better than the obscure `"\u200b"`.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个验证器函数处理源数据的字符串版本可能包含 Unicode `U+200B` 的情况，这是一个称为零宽度的特殊字符。在 Python 中，我们可以使用
    `"\N{ZERO WIDTH SPACE}"` 来使这个字符可见。虽然名称很长，但似乎比神秘的 `"\u200b"` 好一些。
- en: (See [https://www.fileformat.info/info/unicode/char/200b/index.htm](https://www.fileformat.info/info/unicode/char/200b/index.htm)
    for details of this character.)
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: （有关此字符的详细信息，请参阅 [https://www.fileformat.info/info/unicode/char/200b/index.htm](https://www.fileformat.info/info/unicode/char/200b/index.htm)。）
- en: When a function works in the `pre=True` or `mode=’before’` phase, then **pydantic**
    will automatically apply the final conversion function to complete the essential
    work of validation and conversion. This additional validator function can be designed,
    then, to focus narrowly only on cleaning the raw data.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个函数在 `pre=True` 或 `mode=’before’` 阶段工作时，那么 **pydantic** 将自动应用最终的转换函数来完成验证和转换的基本工作。因此，可以设计这个额外的验证器函数，仅专注于清洗原始数据。
- en: 'The idea of a validator function must reflect two separate use cases for this
    class:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 验证器函数的想法必须反映这个类两个不同的用例：
- en: Cleaning and converting acquired data, generally strings, to more useful analytical
    data types.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清洗和转换获取的数据，通常是字符串，到更有用的分析数据类型。
- en: Loading already cleaned analytical data, where type conversion is not required.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载已经清洗的分析数据，其中不需要类型转换。
- en: 'Our primary interest at this time is in the first use case, cleaning and conversion.
    Later, starting in chapter [*Chapter** 13*](ch017.xhtml#x1-29700013), [*Project
    4.1: Visual Analysis Techniques*](ch017.xhtml#x1-29700013) we’ll switch over to
    the second case, loading clean data.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前的主要兴趣在于第一个用例，即清洗和转换。稍后，从第 [*第 13 章*](ch017.xhtml#x1-29700013) 开始，我们将切换到第二个用例，即加载清洗数据。
- en: These two use cases are reflected in the type hint for the validator function.
    The parameter is defined as `value:`` str`` |`` float`. The first use case, conversion,
    expects a value of type `str`. The second use case, loading cleaned data, expects
    a cleaned value of type `float`. This kind of type of union is helpful with validator
    functions.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '这两个用例反映在验证器函数的类型提示中。参数定义为 `value: str | float`。第一个用例，转换，期望值为 `str` 类型。第二个用例，加载清洗数据，期望值为
    `float` 类型的清洗值。这种类型的联合对验证器函数很有帮助。'
- en: Instances of the analytic model will be built from `acquire_model` objects.
    Because the acquired model uses `dataclasses`, we can leverage the `dataclasses.asdict()`
    function to transform a source object into a dictionary. This can be used to perform
    Pydantic validation and conversion to create the analytic model objects.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 分析模型的实例将从 `acquire_model` 对象构建。因为获取的模型使用 `dataclasses`，我们可以利用 `dataclasses.asdict()`
    函数将源对象转换为字典。这可以用于执行 Pydantic 验证和转换以创建分析模型对象。
- en: 'We can add the following method in the dataclass definition:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在数据类定义中添加以下方法：
- en: '[PRE4]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This method extracts a dictionary from the acquired data model’s version of
    the `SeriesSample` class and uses it to create an instance of the analytic model’s
    variation of this class. This method pushes all of the validation and conversion
    work to the **Pydantic** declarations. This method also requires `from`` dataclasses`` import`` asdict`
    to introduce the needed `asdict()` function.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法从获取数据模型版本的 `SeriesSample` 类中提取字典，并使用它来创建分析模型变体类的实例。此方法将所有验证和转换工作推送到 **Pydantic**
    声明。此方法还需要 `from dataclasses import asdict` 来引入所需的 `asdict()` 函数。
- en: In cases where the field names don’t match, or some other transformation is
    required, a more complicated dictionary builder can replace the `asdict(acquired)`
    processing. We’ll see examples of this in [*Chapter** 10*](ch014.xhtml#x1-22900010),
    [*Data* *Cleaning Features*](ch014.xhtml#x1-22900010), where acquired fields need
    to be combined before they can be converted.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在字段名称不匹配或需要其他转换的情况下，可以使用更复杂的字典构建器来替换`asdict(acquired)`处理。我们将在[*第10章*](ch014.xhtml#x1-22900010)、[*数据清理功能*](ch014.xhtml#x1-22900010)中看到这些示例，其中获取的字段需要在转换之前进行组合。
- en: We’ll revisit some aspects of this design decision in [*Chapter** 11*](ch015.xhtml#x1-26400011),
    [*Project 3.7:* *Interim Data Persistence*](ch015.xhtml#x1-26400011). First, however,
    we’ll look at **pydantic** version 2 validation, which offers a somewhat more
    explict path to validation functions.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第11章*](ch015.xhtml#x1-26400011)、[*项目3.7：中间数据持久化*](ch015.xhtml#x1-26400011)中重新审视这个设计决策的一些方面。然而，首先，我们将查看**pydantic**版本2的验证，它为验证函数提供了一条更为明确的路径。
- en: 9.2.2 Pydantic V2 validation
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 Pydantic V2验证
- en: While **pydantic** version 2 will offer a `@field_validator` decorator that’s
    very similar to the legacy `@validator` decorator, this approach suffers from
    an irksome problem. It can be confusing to have the decorator listing the fields
    to which the validation rule applies. Some confusion can arise because of the
    separation between the field definition and the function that validates the values
    for the field. In our example class, the validator applies to the `x` and `y`
    fields, a detail that might be difficult to spot when first looking at the class.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然**pydantic**版本2将提供一个与遗留的`@validator`装饰器非常相似的`@field_validator`装饰器，但这种方法存在一个令人烦恼的问题。列出应用验证规则的字段可能会令人困惑。由于字段定义和验证字段值的函数之间的分离，可能会产生一些混淆。在我们的示例类中，验证器应用于`x`和`y`字段，这是一个在首次查看类时可能难以注意到的细节。
- en: 'The newer design pattern for the analysis model class is shown in the following
    example for Pydantic version 2:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了Pydantic版本2的分析模型类的新设计模式：
- en: '[PRE5]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We’ve omitted the `from_acquire_dataclass()` method definition, since it doesn’t
    change.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们省略了`from_acquire_dataclass()`方法定义，因为它没有变化。
- en: The cleaning function is defined outside the class, making it more easily reused
    in a complicated application where a number of rules may be widely reused in several
    models. The `Annotated[]` type hint combines the base type with a sequence of
    validator objects. In this example, the base type is `float` and the validator
    objects are `BeforeValidator` objects that contain the function to apply.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 清理函数是在类外部定义的，这使得它在复杂的应用程序中更容易重用，在这些应用程序中，许多规则可能在多个模型中被广泛重用。`Annotated[]`类型提示将基本类型与一系列验证器对象结合起来。在这个例子中，基本类型是`float`，验证器对象是包含要应用函数的`BeforeValidator`对象。
- en: To reduce the obvious duplication, a `TypeAlias` can be used. For example,
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少明显的重复，可以使用`TypeAlias`。例如，
- en: '[PRE6]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Using an alias permits the model to use the type hint `CleanFloat`. For example
    `x:`` CleanFloat`.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '使用别名允许模型使用类型提示`CleanFloat`。例如`x: CleanFloat`。'
- en: Further, the `Annotated` hints are composable. An annotation can add features
    to a previously-defined annotation. This ability to build more sophisticated annotations
    on top of foundational annotations offers a great deal of promise for defining
    classes in a succinct and expressive fashion.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`Annotated`提示是可组合的。一个注解可以给之前定义的注解添加功能。在基础注解之上构建更复杂的注解的能力，为以简洁和表达的方式定义类提供了巨大的潜力。
- en: Now that we’ve seen how to implement a single validation, we need to consider
    the alternatives, and how many different kinds of validation functions an application
    might need.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了如何实现单个验证，我们需要考虑替代方案，以及一个应用程序可能需要多少种不同的验证函数。
- en: 9.2.3 Validation function design
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.3 验证函数设计
- en: The `pydantic` package offers a vast number of built-in conversions based entirely
    on annotations. While these can cover a large number of common cases, there are
    still some situations that require special validators, and perhaps even special
    type definitions.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`pydantic`包提供了基于注解的大量内置转换。虽然这些可以覆盖大量常见情况，但仍有一些情况需要特殊的验证器，甚至可能需要特殊的类型定义。'
- en: 'In [*Conversions and processing*](#x1-2130004), we considered some of the kinds
    of processing that might be required. These included the following kinds of conversions:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*转换和处理*](#x1-2130004)中，我们考虑了一些可能需要的处理类型。这些包括以下类型的转换：
- en: Decomposing source fields into their atomic components.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将源字段分解为其原子组件。
- en: Merging separated source fields to create proper value. This is common with
    dates and times, for example.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将分离的源字段合并以创建适当的值。例如，日期和时间通常是这样做的。
- en: 'Multiple subentities may be present in a feed of samples. This can be called
    a discriminated union: the feed as a whole is a unique of disjoint types, and
    a discriminator value (or values) distinguishes the various subtypes.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本流中可能存在多个子实体。这可以称为区分联合：整个流是不同类型的唯一组合，而区分值（或值）区分各种子类型。
- en: A field may be a “token” used to deidentify something about the original source.
    For example, a replacement token for a driver’s license number may replace the
    real government-issued number to make the individual anonymous.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字段可能是一个“令牌”，用于去除原始源信息。例如，用于替换驾照号码的替换令牌可以替换真实政府颁发的号码，以使个人匿名。
- en: Additionally, we may have observability considerations that lead us to write
    our own a unique validator that can write needed log entries or update counters
    showing how many times a particular validation found problems. This enhanced visibility
    can help pinpoint problems with data that is often irregular or suffers from poor
    quality control.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可能需要考虑可观察性，这会引导我们编写自己的独特验证器，以便写入所需的日志条目或更新计数器，显示特定验证发现问题的次数。这种增强的可视性有助于确定数据中经常不规则或质量控制不佳的问题。
- en: We’ll dive into these concepts more deeply in [*Chapter** 10*](ch014.xhtml#x1-22900010),
    [*Data Cleaning* *Features*](ch014.xhtml#x1-22900010). In [*Chapter** 10*](ch014.xhtml#x1-22900010),
    [*Data Cleaning Features*](ch014.xhtml#x1-22900010), we’ll also look at features
    for handling primary and foreign keys. For now, we’ll focus on the built-in type
    conversion functions that are part of Python’s built-in functions, and the standard
    library. But we need to recognize that there are going to be extensions and exceptions.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第10章](ch014.xhtml#x1-22900010)[*数据清洗功能*](ch014.xhtml#x1-22900010)中更深入地探讨这些概念。在第10章[*数据清洗功能*](ch014.xhtml#x1-22900010)中，我们还将探讨处理主键和外键的功能。目前，我们将专注于Python内置函数和标准库中的内置类型转换函数。但我们需要认识到，可能会有扩展和例外。
- en: We’ll look at the overall design approach in the next section.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中探讨整体设计方法。
- en: 9.2.4 Incremental design
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.4 增量设计
- en: The design of the cleaning application is difficult to finalize without detailed
    knowledge of the source data. This means the cleaning application depends on lessons
    learned by making a data inspection notebook. One idealized workflow begins with
    “understand the requirements” and proceeds to “write the code,” treating these
    two activities as separate, isolated steps. This conceptual workflow is a bit
    of a fallacy. It’s often difficult to understand the requirements without a detailed
    examination of the actual source data to reveal the quirks and oddities that are
    present. The examination of the data often leads to the first drafts of data validation
    functions. In this case, the requirements will take the form of draft versions
    of the code, not a carefully-crafted document.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 没有对源数据的详细了解，很难最终确定清洗应用程序的设计。这意味着清洗应用程序依赖于通过制作数据检查笔记本学到的经验教训。一个理想化的工作流程从“理解需求”开始，然后进行“编写代码”，将这两个活动视为独立的、隔离的步骤。这个概念工作流程有点谬误。通常，没有对实际源数据进行详细审查以揭示存在的怪癖和异常，很难理解需求。数据审查通常会导致数据验证函数的第一稿。在这种情况下，需求将以代码草案的形式出现，而不是一份精心制作的文档。
- en: 'This leads to a kind of back-and-forth between *ad-hoc* inspection and a formal
    data cleaning application. This iterative work often leads to a module of functions
    to handle the problem domain’s data. This module can be shared by inspection notebooks
    as well as automated applications. Proper engineering follows the **DRY** (**Don’t
    Repeat Yourself**) principle: code should not be copied and pasted between modules.
    It should be put into a shared module so it can be reused properly.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了一种在*临时*检查和正式数据清洗应用程序之间的来回。这种迭代工作通常会导致一个函数模块来处理问题域的数据。这个模块可以被检查笔记本以及自动化应用程序共享。适当的工程遵循**DRY**（**不要重复自己**）原则：代码不应在模块之间复制和粘贴。它应该放入共享模块中，以便可以正确地重用。
- en: In some cases, two data cleaning functions will be similar. Finding this suggests
    some kind of decomposition is appropriate to separate the common parts from the
    unique parts. The redesign and refactoring are made easier by having a suite of
    unit tests to confirm that no old functionality was broken when the functions
    were transformed to remove duplicated code.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，两个数据清理函数可能相似。发现这一点表明，某种分解是合适的，以将公共部分与独特部分分开。通过有一套单元测试来确认在将函数转换为删除重复代码时没有破坏旧功能，可以简化重构和重构。
- en: The work of creating cleaning applications is iterative and incremental. Rare
    special cases are — well — rare, and won’t show up until well after the processing
    pipeline seems finished. The unexpected arrival special case data is something
    like birders seeing a bird outside its expected habitat. It helps to think of
    a data inspection notebook like a bird watcher’s immense spotting scope, used
    to look closely at one unexpected, rare bird, often in a flock of birds with similar
    feeding and roosting preferences. The presence of the rare bird becomes a new
    datapoint for ornithologists (and amateur enthusiasts). In the case of unexpected
    data, the inspection notebook’s lessons become a new code for the conversions
    module.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 创建清理应用程序的工作是迭代和逐步的。罕见特殊情况很少见，而且通常在处理管道似乎完成很久之后才会出现。意外出现的特殊情况数据类似于观鸟者在其预期栖息地外看到的鸟。将数据检查笔记本想象成观鸟者的大号望远镜，用来仔细观察一只意外、罕见的鸟，通常是在一群具有相似觅食和栖息偏好的鸟群中。罕见鸟的存在成为鸟类学家（和业余爱好者）的新数据点。在意外数据的情况下，检查笔记本的教训成为转换模块的新代码。
- en: The overall main module in the data cleaning application will implement the
    **command-line interface** (**CLI**). We’ll look at this in the next section.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清理应用程序的整体主模块将实现**命令行界面**（**CLI**）。我们将在下一节中探讨这一点。
- en: 9.2.5 CLI application
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.5 CLI应用程序
- en: 'The UX for this application suggests that it operates in the following distinct
    contexts:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用程序的用户体验表明，它操作在以下不同的上下文中：
- en: As a standalone application. The user runs the `src/acquire.py` program. Then,
    the user runs the `src/clean.py` program.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为独立应用程序。用户运行`src/acquire.py`程序。然后，用户运行`src/clean.py`程序。
- en: 'As a stage in a processing pipeline. The user runs a shell command that pipes
    the output from the `src/acquire.py` program into the `src/clean.py` program.
    This is the subject of [*Project 3.6: Integration* *to create an acquisition pipeline*](ch014.xhtml#x1-2510005).'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为处理管道的一个阶段。用户运行一个shell命令，将`src/acquire.py`程序输出的内容管道到`src/clean.py`程序。这是[*项目3.6：集成*
    *创建获取管道*](ch014.xhtml#x1-2510005)的主题。
- en: 'This leads to the following two runtime contexts:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下两个运行时上下文：
- en: When the application is provided an input path, it’s being used as a stand-alone
    application.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当应用程序提供输入路径时，它被用作独立应用程序。
- en: When no input path is provided, the application reads from `sys.stdin`.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当没有提供输入路径时，应用程序从`sys.stdin`读取。
- en: A similar analysis can apply to the **acquire** application. If an output path
    is provided, the application creates and writes the named file. If no output path
    is provided, the application writes to `sys.stdout`.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的分析可以应用于**获取**应用程序。如果提供了输出路径，应用程序将创建并写入命名的文件。如果没有提供输出路径，应用程序将写入`sys.stdout`。
- en: One essential consequence of this is all logging **must** be written to `sys.stderr`.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这的一个基本后果是所有日志**必须**写入`sys.stderr`。
- en: Use **stdin** and **stdout** exclusively for application data, nothing else.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用**stdin**和**stdout**作为应用程序数据，不使用其他任何内容。
- en: Use a consistent, easy-to-parse text format like ND JSON for application data.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一致、易于解析的文本格式，如ND JSON，用于应用程序数据。
- en: Use **stderr** as the destination for all control and error messages.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有控制和错误消息的输出目的地设置为**stderr**。
- en: This means `print()` may require the `file=sys.stderr` to direct debugging output
    to **stderr**. Or, avoid simple `print()` and use `logger.debug()` instead.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着`print()`可能需要`file=sys.stderr`来将调试输出定向到**stderr**。或者，避免使用简单的`print()`，而使用`logger.debug()`代替。
- en: 'For this project, the stand-alone option is all that’s needed. However, it’s
    important to understand the alternatives that will be added in later projects.
    See [*Project 3.6: Integration to create an acquisition pipeline*](ch014.xhtml#x1-2510005)
    for this more tightly-integrated alternative.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，独立选项就足够了。然而，了解将在后续项目中添加的替代方案很重要。参见[*项目3.6：集成创建获取管道*](ch014.xhtml#x1-2510005)以了解这种更紧密集成的替代方案。
- en: Redirecting stdout
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重定向标准输出
- en: Python provides a handy tool for managing the choice between ”write to an open
    file” and ”write to **stdout**”. It involves the following essential design principle.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Python 提供了一个方便的工具来管理“写入打开文件”和“写入 **stdout**”之间的选择。它涉及以下基本设计原则。
- en: Always provide file-like objects to functions and methods processing data.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 总是为处理数据的函数和方法提供类似文件的对象。
- en: 'This suggests a data-cleaning function like the following:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明需要一个类似以下的数据清理函数：
- en: '[PRE7]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This function can use `json.loads()` to parse each document from the `acquire_file`.
    It uses `json.dumps()` to save each document to the `analysis_file` to be used
    for later analytics.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数可以使用 `json.loads()` 解析来自 `acquire_file` 的每个文档。它使用 `json.dumps()` 将每个文档保存到
    `analysis_file` 以供后续分析使用。
- en: 'The overall application can then make a choice among four possible ways to
    use this `clean_all()` function:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 整个应用可以选择以下四种可能的方式来使用此 `clean_all()` 函数：
- en: '**Stand-alone**: This means `with` statements manage the open files created
    from the `Path` names provided as argument values.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立**：这意味着 `with` 语句管理从作为参数值提供的 `Path` 名称创建的打开文件。'
- en: '**Head** **of a pipeline**: A `with` statement can manage an open file passed
    to `acquire_file`. The value of `analysis_file` is `sys.stdout`.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道** **头部**：一个 `with` 语句可以管理传递给 `acquire_file` 的打开文件。`analysis_file` 的值是
    `sys.stdout`。'
- en: '**Tail of a pipeline**: The acquired input file is `sys.stdin`. A `with` statement
    manages an open file (in write mode) for the `analysis_file`.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道** **尾部**：获取的输入文件是 `sys.stdin`。一个 `with` 语句管理用于 `analysis_file` 的打开文件（写入模式）。'
- en: '**Middle of** **a pipeline**: The `acquire_file` is `sys.stdin`; the `analysis_file`
    is `sys.stdout`.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道** **中间**：`acquire_file` 是 `sys.stdin`；`analysis_file` 是 `sys.stdout`。'
- en: Now that we’ve looked at a number of technical approaches, we’ll turn to the
    list of deliverables for this project in the next section.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了多种技术方法，接下来将在下一节中转向本项目的可交付成果列表。
- en: 9.3 Deliverables
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 可交付成果
- en: 'This project has the following deliverables:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 此项目有以下可交付成果：
- en: Documentation in the `docs` folder.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docs` 文件夹中的文档。'
- en: Acceptance tests in the `tests/features` and `tests/steps` folders.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tests/features` 和 `tests/steps` 文件夹中的接收测试。'
- en: Unit tests for the application modules in the `tests` folder.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tests` 文件夹中的应用模块的单元测试。'
- en: Application to clean some acquired data and apply simple conversions to a few
    fields. Later projects will add more complex validation rules.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用到清理一些获取的数据，并对几个字段进行简单的转换。后续项目将添加更复杂的验证规则。
- en: We’ll look at a few of these deliverables in a little more detail.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将更详细地查看其中一些可交付成果。
- en: When starting a new kind of application, it often makes sense to start with
    acceptance tests. Later, when adding features, the new acceptance tests may be
    less important than new unit tests for the features. We’ll start by looking at
    a new scenario for this new application.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 当开始一种新的应用时，通常从接收测试开始是有意义的。后来，当添加功能时，新的接收测试可能不如新的单元测试重要。我们将首先查看这种新应用的新场景。
- en: 9.3.1 Acceptance tests
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 接收测试
- en: 'As we noted in [*Chapter** 4*](ch008.xhtml#x1-780004), [*Data Acquisition Features:
    Web APIs and Scraping*](ch008.xhtml#x1-780004), we can provide a large block of
    text as part of a Gherkin scenario. This can be the contents of an input file.
    We can consider something like the following scenario.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第4章*](ch008.xhtml#x1-780004)、[*数据获取功能：Web API 和抓取*](ch008.xhtml#x1-780004)中提到的，我们可以将一大块文本作为
    Gherkin 场景的一部分提供。这可以是输入文件的内容。我们可以考虑以下场景。
- en: '[PRE8]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This kind of scenario lets us define source documents with valid data. We can
    also define source documents with invalid data.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这种场景让我们能够定义具有有效数据的源文档。我们还可以定义具有无效数据的源文档。
- en: We can use the `Then` steps to confirm additional details of the output. For
    example, if we’ve decided to make all of the cleaning operations visible, the
    test scenario can confirm the output contains all of the cleanup operations that
    were applied.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `Then` 步骤来确认输出的一些额外细节。例如，如果我们决定使所有清理操作都可见，测试场景可以确认输出包含所有已应用的清理操作。
- en: The variety of bad data examples and the number of combinations of good and
    bad data suggest there can be a lot of scenarios for this kind of application.
    Each time new data shows up that is acquired, but cannot be cleaned, new examples
    will be added to these acceptance test cases.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 坏数据示例的多样性和好坏数据组合的数量表明，这种应用可能有多种场景。每次出现新的获取数据，但无法清理时，新的示例将被添加到这些接收测试用例中。
- en: It can, in some cases, be very helpful to publish the scenarios widely so all
    of the stakeholders can understand the data cleaning operations. The Gherkin language
    is designed to make it possible for people with limited technical skills to contribute
    to the test cases.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，广泛发布场景可能非常有帮助，以便所有利益相关者都能理解数据清理操作。Gherkin语言被设计成让技术技能有限的人能够为测试用例做出贡献。
- en: We also need scenarios to run the application from the command-line. The `When`
    step definition for these scenarios will be `subprocess.run()` to invoke the **clean**
    application, or to invoke a shell command that includes the **clean** application.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要场景来从命令行运行应用程序。这些场景的`When`步骤定义将是`subprocess.run()`来调用**清理**应用程序，或者调用包含**清理**应用程序的shell命令。
- en: 9.3.2 Unit tests for the model features
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 模型特征的单元测试
- en: It’s important to have automated unit tests for the model definition classes.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型定义类，拥有自动化的单元测试是很重要的。
- en: It’s also important to **not** test the `pydantic` components. We don’t, for
    example, need to test the ordinary string-to-float conversions the `pydantic`
    module already does; we can trust this works perfectly.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是**不要**测试`pydantic`组件。例如，我们不需要测试`pydantic`模块已经完成的普通字符串到浮点数的转换；我们可以信任这会完美工作。
- en: We **must** test the validator functions we’ve written. This means providing
    test cases to exercise the various features of the validators. Additionally, any
    overall `from_acquire_dataclass()` method needs to have test cases.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们**必须**测试我们编写的验证器函数。这意味着提供测试用例来测试验证器的各种功能。此外，任何整体的`from_acquire_dataclass()`方法都需要有测试用例。
- en: Each of these test scenarios works with a given acquired document with the raw
    data. When the `from_acquire_dataclass()` method is evaluated, then there may
    be an exception or a resulting analytic model document is created.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 每个这些测试场景都与一个给定的获取文档和原始数据一起工作。当`from_acquire_dataclass()`方法被评估时，可能会有异常，或者会创建一个分析模型文档。
- en: The exception testing can make use of the `pytest.raises()` context manager.
    The test is written using a `with` statement to capture the exception.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 异常测试可以利用`pytest.raises()`上下文管理器。测试是用`with`语句编写的，用于捕获异常。
- en: See [https://docs.pytest.org/en/7.2.x/how-to/assert.html#assertions-about-expected-exceptions](https://docs.pytest.org/en/7.2.x/how-to/assert.html#assertions-about-expected-exceptions)
    for examples.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅[https://docs.pytest.org/en/7.2.x/how-to/assert.html#assertions-about-expected-exceptions](https://docs.pytest.org/en/7.2.x/how-to/assert.html#assertions-about-expected-exceptions)以获取示例。
- en: Of course, we also need to test the processing that’s being done. By design,
    there isn’t very much processing involved in this kind of application. The bulk
    of the processing can be only a few lines of code to consume the raw model objects
    and produce the analytical objects. Most of the work will be delegated to modules
    like `json` and `pydantic`.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们还需要测试正在进行的处理。按照设计，这类应用中涉及的处理并不多。大部分处理可能只有几行代码来消费原始模型对象并生成分析对象。大部分工作将委托给`json`和`pydantic`等模块。
- en: 9.3.3 Application to clean data and create an NDJSON interim file
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.3 应用以清理数据并创建NDJSON临时文件
- en: Now that we have acceptance and unit test suites, we’ll need to create the `clean`
    application. Initially, we can create a place-holder application, just to see
    the test suite fail. Then we can fill in the various pieces until the application
    – as a whole – works.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了验收和单元测试套件，我们需要创建`clean`应用程序。最初，我们可以创建一个占位符应用程序，只是为了看到测试套件失败。然后我们可以填充各个部分，直到应用程序作为一个整体工作。
- en: 'Flexibility is paramount in this application. In the next chapter, [*Chapter** 10*](ch014.xhtml#x1-22900010),
    [*Data Cleaning Features*](ch014.xhtml#x1-22900010), we will introduce a large
    number of data validation scenarios. In [*Chapter** 11*](ch015.xhtml#x1-26400011),
    [*Project 3.7: Interim Data Persistence*](ch015.xhtml#x1-26400011) we’ll revisit
    the idea of saving the cleaned data. For now, it’s imperative to create clean
    data; later, we can consider what format might be best.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个应用程序中，灵活性至关重要。在下一章（[*第10章*](ch014.xhtml#x1-22900010)，[*数据清理功能*](ch014.xhtml#x1-22900010)）中，我们将介绍大量数据验证场景。在第11章（[*项目3.7：临时数据持久化*](ch015.xhtml#x1-26400011)）中，我们将重新审视保存清理数据的想法。现在，创建干净的数据是至关重要的；稍后，我们可以考虑哪种格式可能最好。
- en: 9.4 Summary
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 总结
- en: 'This chapter has covered a number of aspects of data validation and cleaning
    applications:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 本章已经涵盖了数据验证和清理应用的一些方面：
- en: CLI architecture and how to design a simple pipeline of processes.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLI架构以及如何设计简单的流程管道。
- en: The core concepts of validating, cleaning, converting, and standardizing raw
    data.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核心概念包括验证、清理、转换和标准化原始数据。
- en: In the next chapter, we’ll dive more deeply into a number of data cleaning and
    standardizing features. Those projects will all build on this base application
    framework. After those projects, the next two chapters will look a little more
    closely at the analytical data persistence choices, and provide an integrated
    web service for providing cleaned data to other stakeholders.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将更深入地探讨一些数据清理和标准化的功能。这些项目都将基于这个基础应用程序框架。在这些项目之后，接下来的两章将更仔细地研究分析数据持久性选择，并为提供清洗数据给其他利益相关者提供一个集成网络服务。
- en: 9.5 Extras
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 额外内容
- en: Here are some ideas for you to add to this project.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些想法供您添加到这个项目中。
- en: 9.5.1 Create an output file with rejected samples
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.1 创建一个包含拒绝样本的输出文件
- en: In [*Error reports*](#x1-2140005) we suggested there are times when it’s appropriate
    to create a file of rejected samples. For the examples in this book — many of
    which are drawn from well-curated, carefully managed data sets — it can feel a
    bit odd to design an application that will reject data.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*错误报告*](#x1-2140005)中，我们建议有时创建一个拒绝样本的文件是合适的。对于本书中的例子——其中许多是从精心整理、仔细管理的数据集中抽取的——设计一个会拒绝数据的程序可能会感觉有点奇怪。
- en: For enterprise applications, data rejection is a common need.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 对于企业应用来说，数据拒绝是一个常见需求。
- en: 'It can help to look at a data set like this: [https://datahub.io/core/co2-ppm](https://datahub.io/core/co2-ppm).
    This contains data same with measurements of CO2 levels measures with units of
    ppm, parts per million.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看看这样的数据集：[https://datahub.io/core/co2-ppm](https://datahub.io/core/co2-ppm)。这个数据集包含与使用ppm（百万分之一）单位测量的CO2水平相同的测量数据。
- en: This has some samples with an invalid number of days in the month. It has some
    samples where a monthly CO2 level wasn’t recorded.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有部分样本在月份的天数上无效。还有一些样本没有记录月度CO2水平。
- en: It can be insightful to use a rejection file to divide this data set into clearly
    usable records, and records that are not as clearly usable.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 使用拒绝文件将这个数据集划分为清晰可用的记录和不太清晰可用的记录可能会有所启发。
- en: The output will **not** reflect the analysis model. These objects will reflect
    the acquire model; they are the items that would not convert properly from the
    acquired structure to the desired analysis structure.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将**不会**反映分析模型。这些对象将反映获取模型；它们是从获取结构到所需分析结构无法正确转换的项目。
