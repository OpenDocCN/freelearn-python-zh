- en: Scrapy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scrapy
- en: '**Scrapy** is a popular web scraping and crawling framework utilizing high-level
    functionality to make scraping websites easier. In this chapter, we will get to
    know Scrapy by using it to scrape the example website, just as we did in [Chapter
    2](py-web-scrp-2e_ch02.html), *Scraping the Data*. Then, we will cover **Portia**,
    which is an application based on Scrapy which allows you to scrape a website through
    a point and click interface.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scrapy**是一个流行的网络抓取和爬取框架，利用高级功能使抓取网站变得更加容易。在本章中，我们将通过使用它来抓取示例网站来了解Scrapy，就像我们在[第2章](py-web-scrp-2e_ch02.html)“抓取数据”中所做的那样。然后，我们将介绍**Portia**，这是一个基于Scrapy的应用程序，它允许您通过点击界面来抓取网站。'
- en: 'In this chapter we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Getting started with Scrapy
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scrapy入门
- en: Creating a Spider
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个蜘蛛
- en: Comparing different spider types
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较不同的蜘蛛类型
- en: Crawling with Scrapy
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Scrapy进行爬取
- en: Visual Scraping with Portia
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Portia进行可视化抓取
- en: Automated Scraping with Scrapely
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Scrapely进行自动化抓取
- en: Installing Scrapy
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Scrapy
- en: 'Scrapy can be installed with the `pip` command, as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy可以使用`pip`命令安装，如下所示：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Scrapy relies on some external libraries, so if you have trouble installing
    it there is additional information available on the official website at: [http://doc.scrapy.org/en/latest/intro/install.html](http://doc.scrapy.org/en/latest/intro/install.html).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy依赖于一些外部库，所以如果您在安装时遇到问题，可以在官方网站上找到更多信息：[http://doc.scrapy.org/en/latest/intro/install.html](http://doc.scrapy.org/en/latest/intro/install.html)。
- en: 'If Scrapy is installed correctly, a `scrapy` command will now be available
    in the terminal:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Scrapy安装正确，现在终端中将会出现一个`scrapy`命令：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will use the following commands in this chapter:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下命令：
- en: '`startproject`: Creates a new project'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`startproject`：创建新项目'
- en: '`genspider`: Generates a new spider from a template'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`genspider`：从模板生成新的蜘蛛'
- en: '`crawl`: Runs a spider'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crawl`：运行蜘蛛'
- en: '`shell`: Starts the interactive scraping console'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shell`：启动交互式抓取控制台'
- en: For detailed information about these and other commands available, refer to
    [http://doc.scrapy.org/en/latest/topics/commands.html](http://doc.scrapy.org/en/latest/topics/commands.html)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些和其他可用命令的详细信息，请参阅[http://doc.scrapy.org/en/latest/topics/commands.html](http://doc.scrapy.org/en/latest/topics/commands.html)。
- en: Starting a project
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动项目
- en: Now that Scrapy is installed, we can run the `startproject` command to generate
    the default structure for our first Scrapy project.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在Scrapy已经安装，我们可以运行`startproject`命令来生成我们第一个Scrapy项目的默认结构。
- en: 'To do this, open the terminal and navigate to the directory where you want
    to store your Scrapy project, and then run `scrapy startproject <project name>`.
    Here, we will use `example` for the project name:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要这样做，请打开终端并导航到您想要存储Scrapy项目的目录，然后运行`scrapy startproject <项目名称>`。在这里，我们将使用`example`作为项目名称：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here are the files generated by the `scrapy` command:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是`scrapy`命令生成的文件：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The important files for this chapter (and in general for Scrapy use) are as
    follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本章（以及一般而言对于Scrapy的使用）的重要文件如下：
- en: '`items.py`: This file defines a model of the fields that will be scraped'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`items.py`：此文件定义了将要抓取的字段模型'
- en: '`settings.py`: This file defines settings, such as the user agent and crawl
    delay'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`settings.py`：此文件定义设置，例如用户代理和爬取延迟'
- en: '`spiders/`: The actual scraping and crawling code are stored in this directory'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spiders/`：实际的抓取和爬取代码存储在这个目录中'
- en: Additionally, Scrapy uses `scrapy.cfg` for project configuration, `pipelines.py`
    to process the scraped fields and `middlewares.``py` to control request and response
    middleware, but they will not need to be modified for this example.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Scrapy使用`scrapy.cfg`进行项目配置，`pipelines.py`处理抓取的字段，以及`middlewares.py`控制请求和响应中间件，但在这个示例中它们不需要修改。
- en: Defining a model
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义一个模型
- en: 'By default, `example/items.py` contains the following code:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`example/items.py`包含以下代码：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `ExampleItem` class is a template which needs to be replaced with the details
    we''d like to extract from the example country page. For now, we will just scrape
    the country name and population, rather than all the country details. Here is
    an updated model to support this:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExampleItem`类是一个模板，需要替换为我们从示例国家页面中想要提取的详细信息。目前，我们只抓取国家名称和人口，而不是所有国家详情。以下是支持此功能的更新模型：'
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Full documentation for defining items is available at [http://doc.scrapy.org/en/latest/topics/items.html](http://doc.scrapy.org/en/latest/topics/items.html)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 定义项目的完整文档可在[http://doc.scrapy.org/en/latest/topics/items.html](http://doc.scrapy.org/en/latest/topics/items.html)找到
- en: Creating a spider
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个蜘蛛
- en: 'Now, we can build the actual crawling and scraping code, known as a **spider**
    in Scrapy. An initial template can be generated with the `genspider` command,
    which takes the name you want to call the spider, the domain, and an optional
    template:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以构建实际的爬取和抓取代码，在Scrapy中被称为**蜘蛛**。可以使用`genspider`命令生成一个初始模板，该命令接受蜘蛛的名称、域名以及可选的模板：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We used the built-in `crawl` template which utilizes the Scrapy library's `CrawlSpider`.
    A Scrapy `CrawlSpider` has special attributes and methods available when crawling
    the web rather than a simple scraping spider.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了内置的`crawl`模板，该模板利用了Scrapy库的`CrawlSpider`。Scrapy的`CrawlSpider`在爬取网页时具有特殊的属性和方法，而不是简单的抓取蜘蛛。
- en: 'After running the `genspider` command, the following code is generated in `example/spiders/country.py`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`genspider`命令后，以下代码在`example/spiders/country.py`中生成：
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The initial lines import the required Scrapy libraries and encoding definition.
    Then, a class is created for the spider, which contains the following class attributes:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 初始几行导入所需的Scrapy库和编码定义。然后，创建一个蜘蛛类，其中包含以下类属性：
- en: '`name`: A string to identify the spider'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`：用于标识蜘蛛的字符串'
- en: '`allowed_domains`: A list of the domains that can be crawled -- if this isn''t set, any
    domain can be crawled'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`allowed_domains`：可以爬取的域名列表——如果未设置，则可以爬取任何域名'
- en: '`start_urls`: A list of URLs to begin the crawl.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_urls`：开始爬取的URL列表。'
- en: '`rules`: This attribute is a tuple of `Rule` objects defined by regular expressions
    which tell the crawler what links to follow and what links have useful content
    to scrape'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rules`：此属性是`Rule`对象的元组，由正则表达式定义，告诉爬虫要跟随哪些链接以及哪些链接包含有用的内容可以抓取'
- en: You will notice the defined `Rule` has a `callback` attribute which sets the
    callback to `parse_item`, the method defined just below. This method is the main
    data extraction method for `CrawlSpider` objects, and the generated Scrapy code
    within that method has an example of extracting content from the page.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到定义的`Rule`具有一个`callback`属性，该属性将回调设置为`parse_item`，这是定义在下面的方法。这个方法是`CrawlSpider`对象的主要数据提取方法，在该方法中生成的Scrapy代码有一个从页面提取内容的示例。
- en: Because Scrapy is a high-level framework, there is a lot going on here in only
    a few lines of code. The official documentation has further details about building
    spiders, and can be found at [http://doc.scrapy.org/en/latest/topics/spiders.html](http://doc.scrapy.org/en/latest/topics/spiders.html).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Scrapy是一个高级框架，这里只有几行代码就发生了很多事情。官方文档有关于构建蜘蛛的更多详细信息，可以在[http://doc.scrapy.org/en/latest/topics/spiders.html](http://doc.scrapy.org/en/latest/topics/spiders.html)找到。
- en: Tuning settings
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整设置
- en: Before running the generated crawl spider, the Scrapy settings should be updated
    to avoid the spider being blocked. By default, Scrapy allows up to 16 concurrent
    downloads for a domain with no delay between downloads, which is much faster than
    a real user would browse. This behavior is easy for a server to detect and block.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行生成的爬取蜘蛛之前，应更新Scrapy设置以避免蜘蛛被阻止。默认情况下，Scrapy允许对没有延迟的域进行最多16个并发下载，这比真实用户浏览的速度要快得多。这种行为很容易被服务器检测和阻止。
- en: 'As mentioned in Chapter 1, the example website we are scraping is configured
    to temporarily block crawlers which consistently download at faster than one request
    per second, so the default settings would ensure our spider is blocked. Unless
    you are running the example website locally, I recommend adding these lines to
    `example/settings.py` so the crawler only downloads a single request per domain
    at a time with a reasonable 5 second delay between downloads:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如第一章所述，我们正在抓取的示例网站配置为暂时阻止下载速度持续超过每秒一个请求的爬虫，因此默认设置将确保我们的蜘蛛被阻止。除非你在本地运行示例网站，否则我建议将以下行添加到`example/settings.py`中，以便爬虫每次只下载一个域的单个请求，并在下载之间有合理的5秒延迟：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You can also search and find those settings in the documentation, modify and
    uncomment them with the above values. Note that Scrapy will not use this precise
    delay between requests, because this would also make a crawler easier to detect
    and block. Instead, it adds a random offset within this delay between requests.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在文档中搜索并找到这些设置，使用上述值修改并取消注释它们。请注意，Scrapy不会使用请求之间的精确延迟，因为这也会使爬虫更容易被检测和阻止。相反，它在请求之间的延迟中添加一个随机偏移量。
- en: For details about these settings and the many others available, refer to [http://doc.scrapy.org/en/latest/topics/settings.html](http://doc.scrapy.org/en/latest/topics/settings.html).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些设置以及许多其他可用设置的详细信息，请参阅[http://doc.scrapy.org/en/latest/topics/settings.html](http://doc.scrapy.org/en/latest/topics/settings.html)。
- en: Testing the spider
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试蜘蛛
- en: 'To run a spider from the command line, the `crawl` command is used along with
    the name of the spider:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要从命令行运行蜘蛛，使用`crawl`命令并附带蜘蛛的名称：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The script runs to completion with no output. Take note of the `-s LOG_LEVEL=ERROR`
    flag-this is a Scrapy setting and is equivalent to defining `LOG_LEVEL = 'ERROR'`
    in the `settings.py` file. By default, Scrapy will output all log messages to
    the terminal, so here the log level was raised to isolate error messages. Here,
    no output means our spider completed without error -- great!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本运行完成，没有输出。注意`-s LOG_LEVEL=ERROR`标志——这是一个Scrapy设置，相当于在`settings.py`文件中定义`LOG_LEVEL
    = 'ERROR'`。默认情况下，Scrapy会将所有日志消息输出到终端，所以这里日志级别被提升以隔离错误消息。这里没有输出意味着我们的蜘蛛在没有错误的情况下完成了——太好了！
- en: 'In order to actually scrape some content from the pages, we need to add a few
    lines to the spider file. To ensure we can start building and extracting our items,
    we have to first start using our `CountryItem` and also update our crawler rules.
    Here is an updated version of the spider:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实际上从页面中抓取一些内容，我们需要在蜘蛛文件中添加几行代码。为了确保我们可以开始构建和提取我们的项目，我们必须首先开始使用我们的`CountryItem`并更新我们的爬虫规则。以下是蜘蛛的更新版本：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In order to extract structured data, we should use our `CountryItem` class which
    we created. In this added code, we are importing the class and instantiating an
    object as the `i` (or item) in our `parse_item` method.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提取结构化数据，我们应该使用我们创建的`CountryItem`类。在这段新增的代码中，我们导入了这个类并实例化了一个对象作为`parse_item`方法中的`i`（或项目）。
- en: Additionally, we need to add rules so our spider can find data and extract it.
    The default rule searched the url pattern `r'/Items'` which is not matched on
    the example site. Instead, we can create two new rules from what we know already
    about the site. The first rule will crawl the index pages and follow their links,
    and the second rule will crawl the country pages and pass the downloaded response
    to the `callback` function for scraping.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要添加规则，以便我们的蜘蛛可以找到数据并提取它。默认规则搜索的URL模式是`r'/Items'`，在示例网站上不匹配。相反，我们可以根据我们对网站的已知信息创建两个新的规则。第一个规则将爬取索引页面并跟随它们的链接，第二个规则将爬取国家页面并将下载的响应传递给`callback`函数进行抓取。
- en: 'Let''s see what happens when this improved spider is run with the log level
    set to `DEBUG` to show more crawling messages:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当这个改进的蜘蛛以`DEBUG`日志级别运行时会发生什么，这将显示更多的爬取消息：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This log output shows the index pages and countries are being crawled and duplicate
    links are filtered, which is handy. We can also see our installed middlewares
    and other important information output when we first start the crawler.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这条日志输出显示索引页面和国家正在被爬取，并且重复链接被过滤，这很有用。我们还可以看到当我们第一次启动爬虫时，安装的中间件和其他重要信息被输出。
- en: However, we also notice the spider is wasting resources by crawling the login
    and register forms linked from each web page, because they match the `rules` regular
    expressions. The login URL in the preceding command ends with `_next=%2Findex%2F1`,
    which is a URL encoding equivalent to `_next=/index/1`, defining a post-login
    redirect. To prevent these URLs from being crawled, we can use the `deny` parameter
    of the rules, which also expects a regular expression and will prevent crawling
    every matching URL.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们也注意到蜘蛛正在通过爬取每个网页中链接的登录和注册表单而浪费资源，因为这些表单与`rules`正则表达式匹配。前一个命令中的登录URL以`_next=%2Findex%2F1`结尾，这是URL编码的`_next=/index/1`，定义了登录后的重定向。为了防止这些URL被爬取，我们可以使用规则的`deny`参数，它也期望一个正则表达式，并将防止爬取每个匹配的URL。
- en: 'Here is an updated version of code to prevent crawling the user login and registration
    forms by avoiding the URLs containing `/user/`:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是防止爬取包含`/user/`的URL的用户登录和注册表单的代码更新版本：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Further documentation about how to use the LinkExtractor class is available
    at [http://doc.scrapy.org/en/latest/topics/link-extractors.html](http://doc.scrapy.org/en/latest/topics/link-extractors.html).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何使用LinkExtractor类的进一步文档可在[http://doc.scrapy.org/en/latest/topics/link-extractors.html](http://doc.scrapy.org/en/latest/topics/link-extractors.html)找到。
- en: 'To stop the current crawl and restart with the new code, you can send a quit
    signal using *Ctrl* + *C* or *cmd* + *C*. You should then see a message similar
    to this one:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要停止当前爬取并使用新代码重新启动，你可以使用 *Ctrl* + *C* 或 *cmd* + *C* 发送退出信号。然后你应该会看到一条类似以下的消息：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: It will finish queued requests and then stop. You'll see some extra statistics
    and debugging at the end, which we will cover later in this section.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 它将完成队列中的请求然后停止。你会在最后看到一些额外的统计信息和调试信息，我们将在本节后面讨论。
- en: In addition to adding deny rules to the crawler, you can use the `process_links`
    argument for the `Rule` object. This allow you to create a function which iterates
    through the found links and makes any modifications (such as removing or adding
    parts of query strings). More information about crawling rules is available in
    the documentation: [https://doc.scrapy.org/en/latest/topics/spiders.html#crawling-rules](https://doc.scrapy.org/en/latest/topics/spiders.html#crawling-rules)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 除了向爬虫添加拒绝规则外，你还可以为 `Rule` 对象使用 `process_links` 参数。这允许你创建一个函数，该函数遍历找到的链接并对它们进行任何修改（例如删除或添加查询字符串的部分）。有关爬取规则的更多信息，请参阅文档：[https://doc.scrapy.org/en/latest/topics/spiders.html#crawling-rules](https://doc.scrapy.org/en/latest/topics/spiders.html#crawling-rules)
- en: Different Spider Types
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同的爬虫类型
- en: 'In this Scrapy example, we have utilized the Scrapy `CrawlSpider`, which is
    particularly useful when crawling a website or series of websites. Scrapy has
    several other spiders you may want to use depending on the site and your extraction
    needs. These spiders fall under the following categories:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个 Scrapy 示例中，我们使用了 Scrapy 的 `CrawlSpider`，这在爬取网站或一系列网站时特别有用。Scrapy 还有其他一些你可能想要使用的爬虫，具体取决于网站和你的提取需求。这些爬虫属于以下类别：
- en: '`Spider`: A normal scraping spider. This is usually used for just scraping
    one type of page.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Spider`：一个普通的爬取爬虫。这通常用于仅爬取一种类型的页面。'
- en: '`CrawlSpider`: A crawl spider; usually used for traversing a domain and scraping
    one (or several) types of pages from the pages it finds by crawling links.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CrawlSpider`：一个爬取爬虫；通常用于遍历域名并从它通过爬取链接找到的页面中爬取一种（或几种）类型的页面。'
- en: '`XMLFeedSpider`: A spider which traverses an XML feed and extracts content
    from each node.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`XMLFeedSpider`：一个遍历 XML 提要并从每个节点中提取内容的爬虫。'
- en: '`CSVFeedSpider`: Similar to the XML spider, but instead can parse CSV rows
    within the feed.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CSVFeedSpider`：类似于 XML 爬虫，但可以解析提要中的 CSV 行。'
- en: '`SitemapSpider`: A spider which can crawl a site with differing rules by first
    parsing the Sitemap.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SitemapSpider`：一个可以首先解析 Sitemap 以使用不同规则的爬虫。'
- en: Each of these spiders are included in your default Scrapy installation, so you
    can access them whenever you may want to build a new web scraper. In this chapter,
    we'll finish building our first crawl spider as a first example of how to use
    Scrapy tools.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这些爬虫都包含在你的默认 Scrapy 安装中，因此你可以随时访问它们，以便在需要构建新的网络爬虫时使用。在本章中，我们将完成第一个爬取爬虫的构建，作为如何使用
    Scrapy 工具的第一个示例。
- en: Scraping with the shell command
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 shell 命令进行爬取
- en: Now that Scrapy can crawl the countries, we can define what data to scrape.
    To help test how to extract data from a web page, Scrapy comes with a handy command
    called `shell` which presents us with the Scrapy API via an Python or IPython
    interpreter.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 Scrapy 可以爬取国家，我们可以定义要爬取的数据。为了帮助测试如何从网页中提取数据，Scrapy 提供了一个名为 `shell` 的便捷命令，它通过
    Python 或 IPython 解释器向我们展示 Scrapy API。
- en: 'We can call the command using the URL we would like to start with, like so:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用我们想要开始的 URL 来调用命令，如下所示：
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We can now query the `response` object to check what data is available.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以查询 `response` 对象以检查可用的数据。
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Scrapy uses `lxml` to scrape data, so we can use the same CSS selectors as
    those in [Chapter 2](py-web-scrp-2e_ch02.html), *Scraping the Data*:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy 使用 `lxml` 爬取数据，因此我们可以使用与 [第 2 章](py-web-scrp-2e_ch02.html) 中相同的 CSS 选择器，*爬取数据*：
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The method returns a list with an `lxml` selector. You may also recognize some
    of the XPath syntax Scrapy and `lxml` use to select the item. As we learned in
    [Chapter 2](py-web-scrp-2e_ch02.html),  *Scraping the Data*, `lxml` converts all
    CSS Selectors to XPath before extracting content.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法返回一个包含 `lxml` 选择器的列表。你也可能认识 Scrapy 和 `lxml` 用于选择项的一些 XPath 语法。正如我们在 [第 2
    章](py-web-scrp-2e_ch02.html) 中所学的，*爬取数据*，`lxml` 在提取内容之前将所有 CSS 选择器转换为 XPath。
- en: 'In order to actually get the text from this country row, we must call the `extract()`
    method:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实际上从这一行获取文本，我们必须调用 `extract()` 方法：
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As we can see from the output above, the Scrapy `response` object can be parsed
    using both `css` and `xpath`, making it very versatile for getting obvious and
    harder-to-reach content.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们从上面的输出中可以看到，Scrapy的`response`对象可以使用`css`和`xpath`进行解析，这使得它在获取明显和难以到达的内容方面非常灵活。
- en: 'These selectors can then be used in the `parse_item()` method generated earlier
    in `example/spiders/country.py`. Note we set attributes of the `scrapy.Item` object
    using dictionary syntax:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这些选择器可以用于之前在`example/spiders/country.py`中生成的`parse_item()`方法。注意，我们使用字典语法设置`scrapy.Item`对象的属性：
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Checking results
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查结果
- en: 'Here is the completed version of our spider:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们蜘蛛的完成版本：
- en: '[PRE19]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: To save the results, we could define a Scrapy pipeline or set up an output setting
    in our `settings.py` file. However, Scrapy also provides a handy `--output` flag
    to easily save scraped items automatically in CSV, JSON, or XML format.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要保存结果，我们可以定义一个Scrapy管道或在`settings.py`文件中设置输出设置。然而，Scrapy还提供了一个方便的`--output`标志，可以轻松地将抓取的项目自动保存为CSV、JSON或XML格式。
- en: 'Here are the results when the final version of the spider is run with the output
    to a CSV file and the log level is set to `INFO`, to filter out less important
    messages:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当蜘蛛的最终版本运行并输出到CSV文件，并将日志级别设置为`INFO`时，以下是结果，用于过滤掉不太重要的消息：
- en: '[PRE20]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: At the end of the crawl, Scrapy outputs some statistics to give an indication
    of how the crawl performed. From these statistics, we know that 280 web pages
    were crawled and 252 items were scraped, which is the expected number of countries
    in the database, so we know the crawler was able to find them all.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在爬取结束时，Scrapy输出一些统计信息，以指示爬取的表现。从这些统计信息中，我们知道爬取了280个网页，抓取了252个项目，这是数据库中预期的国家数量，因此我们知道爬虫能够找到它们所有。
- en: You need to run Scrapy spider and crawl commands from within the generated folder
    Scrapy creates (for our project this is the `example/` directory we created using
    the `startproject` command). The spiders use the `scrapy.cfg` and `settings.py`
    files to determine how and where to scrape and to set spider paths for crawling
    or scraping use.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要从Scrapy创建的生成文件夹中运行Scrapy蜘蛛和爬取命令（对于我们的项目，这是使用`startproject`命令创建的`example/`目录）。蜘蛛使用`scrapy.cfg`和`settings.py`文件来确定如何以及在哪里进行爬取，并设置蜘蛛路径以用于爬取或抓取。
- en: 'To verify these countries were scraped correctly we can check the contents
    of `countries.csv`:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证这些国家是否正确抓取，我们可以检查`countries.csv`文件的内容：
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As expected this CSV contains the name and population for each country. Scraping
    this data required writing less code than the original crawler built in [Chapter
    2](py-web-scrp-2e_ch02.html), *Scraping the Data* because Scrapy provides high-level
    functionality and nice built-in features like built-in CSV writers.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，这个CSV文件包含了每个国家的名称和人口。抓取这些数据所需的代码比[第2章](py-web-scrp-2e_ch02.html)中构建的原生爬虫要少，因为Scrapy提供了高级功能以及内置的CSV编写器等优秀的内置功能。
- en: In the following section on Portia we will re-implement this scraper writing
    even less code.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下关于Portia的部分，我们将重新实现这个抓取器，编写更少的代码。
- en: Interrupting and resuming a crawl
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 中断和恢复爬取
- en: Sometimes when scraping a website, it can be useful to pause the crawl and resume
    it at a later time without needing to start over from the beginning. For example,
    you may need to interrupt the crawl to reset your computer after a software update,
    or perhaps, the website you are crawling is returning errors and you want to continue
    the crawl later.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 有时在爬取网站时，暂停爬取并在稍后时间恢复它，而不需要从头开始，这可能很有用。例如，您可能需要在软件更新后中断爬取以重置计算机，或者，您正在爬取的网站正在返回错误，您希望在稍后继续爬取。
- en: Conveniently, Scrapy comes with built-in support to pause and resume crawls
    without needing to modify our example spider. To enable this feature, we just
    need to define the `JOBDIR` setting with a directory where the current state of
    a crawl can be saved. Note separate directories must be used to save the state
    of multiple crawls.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 便利的是，Scrapy自带了暂停和恢复爬取的功能，无需修改我们的示例蜘蛛。要启用此功能，我们只需定义`JOBDIR`设置，指定一个目录，用于保存爬取的当前状态。注意，必须使用不同的目录来保存多个爬取的状态。
- en: 'Here is an example using this feature with our spider:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个使用此功能的蜘蛛示例：
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here, we see a `^C` in the line that says `Received SIG_SETMASK` which is the
    same *Ctrl* + *C* or *cmd* + *C* we used earlier in the chapter to stop our scraper.
    To have Scrapy save the crawl state, you must wait here for the crawl to shut
    down gracefully and resist the temptation to enter the termination sequence again
    to force immediate shutdown! The state of the crawl will now be saved in the data
    directory in `crawls/country`. We can see the saved files if we look in that directory
    (Note this command and directory syntax will need to be altered for Windows users):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在显示“Received SIG_SETMASK”的行中，我们看到了一个`^C`，这与我们在本章早期用来停止爬虫的相同*Ctrl* + *C* 或 *cmd*
    + *C*。为了使Scrapy保存爬取状态，你必须在这里等待爬取优雅地关闭，并抵制再次输入终止序列以强制立即关闭的诱惑！爬取的状态现在将被保存在数据目录下的`crawls/country`中。如果我们查看该目录，我们可以看到保存的文件（注意，对于Windows用户，这个命令和目录语法需要做出调整）：
- en: '[PRE23]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The crawl can be resumed by running the same command:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过运行相同的命令来恢复爬取：
- en: '[PRE24]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The crawl now resumes from where it paused and continues as normal. This feature
    is not particularly useful for our example website because the number of pages
    to download is manageable. However, for larger websites which could take months
    to crawl, being able to pause and resume crawls is quite convenient.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在爬取从暂停的地方恢复，并继续正常进行。这个特性对于我们的示例网站来说并不特别有用，因为要下载的页面数量是可管理的。然而，对于可能需要数月才能爬取的大型网站，能够暂停和恢复爬取是非常方便的。
- en: There are some edge cases not covered here that can cause problems when resuming
    a crawl, such as expiring cookies and sessions. These are mentioned in the Scrapy
    documentation available at [http://doc.scrapy.org/en/latest/topics/jobs.html](http://doc.scrapy.org/en/latest/topics/jobs.html).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里没有涵盖的一些边缘情况可能会在恢复爬取时引起问题，例如过期的cookies和会话。这些内容在Scrapy文档中有提及，文档可在[http://doc.scrapy.org/en/latest/topics/jobs.html](http://doc.scrapy.org/en/latest/topics/jobs.html)找到。
- en: Scrapy Performance Tuning
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scrapy性能调优
- en: If we check the initial full scrape of the example site and take a look at the
    start and end times, we can see the scrape took approximately 1,697 seconds. If
    we calculate how many seconds per page (on average), that is ~6 seconds per page.
    Knowing we did not use the Scrapy concurrency features and fully aware that we
    also added a delay of ~5 seconds between requests, this means Scrapy is parsing
    and extracting data at around 1s per page (Recall from [Chapter 2](py-web-scrp-2e_ch02.html),
    *Scraping the Data*, that our fastest scraper using XPath took 1.07s). I gave
    a talk at PyCon 2014 comparing web scraping library speed, and even then, Scrapy
    was massively faster than any other scraping frameworks I could find. I was able
    to write a simple Google search scraper that was returning (on average) 100 requests
    a second. Scrapy has come a long way since then, and I always recommend it for
    the most performant Python scraping framework.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查示例网站的初始完整爬取，并查看开始和结束时间，我们可以看到爬取大约花费了1,697秒。如果我们计算每页的平均秒数，那么大约是每页6秒。知道我们没有使用Scrapy的并发功能，并且完全清楚我们在请求之间添加了大约5秒的延迟，这意味着Scrapy每页大约以1秒的速度解析和提取数据（回想一下[第2章](py-web-scrp-2e_ch02.html)，*爬取数据*，我们使用XPath的最快爬虫花费了1.07秒）。我在2014年的PyCon上发表了一个关于比较网络爬虫库速度的演讲，即使那时，Scrapy也比我能找到的任何其他爬虫框架都要快得多。我能够编写一个简单的Google搜索爬虫，它平均每秒返回100个请求。自从那时以来，Scrapy已经取得了长足的进步，我总是推荐它作为最有效的Python爬虫框架。
- en: In addition to leveraging the concurrency Scrapy uses (via Twisted), Scrapy
    can be tuned to use things like page caches and other performance considerations
    (such as utilizing proxies to allow more concurrent requests to a single site).
    In order to install the cache, you should first read the cache middleware documentation
    ([https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpcache](https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpcache)).
    You might have already seen in the `settings.py` file, there are several good
    examples of how to implement the proper cache settings. For implementing proxies,
    there are some great helper libraries (as Scrapy only gives access to a simple
    middleware class). The current most popular and updated library is [https://github.com/aivarsk/scrapy-proxies](https://github.com/aivarsk/scrapy-proxies),
    which has Python3 support and is fairly easy to integrate.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 除了利用Scrapy使用的并发性（通过Twisted），Scrapy还可以调整以使用页面缓存和其他性能考虑因素（例如利用代理以允许对单个站点进行更多并发请求）。为了安装缓存，您应首先阅读缓存中间件文档（[https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpcache](https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpcache)）。您可能已经在`settings.py`文件中看到了一些如何实现适当的缓存设置的示例。对于代理的实现，有一些优秀的辅助库（因为Scrapy只提供了简单的中间件类访问）。目前最受欢迎且更新最快的库是[https://github.com/aivarsk/scrapy-proxies](https://github.com/aivarsk/scrapy-proxies)，它支持Python3并且相对容易集成。
- en: As always, libraries and recommended setup can change, so reading the latest
    Scrapy documentation should always be your first stop when it comes to checking
    performance and making spider changes.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 与往常一样，库和推荐设置可能会变化，因此当检查性能和进行蜘蛛更改时，阅读最新的Scrapy文档始终是您的首选。
- en: Visual scraping with Portia
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Portia进行视觉抓取
- en: Portia is a an open-source tool built on top of Scrapy that supports building
    a spider by clicking on the parts of a website which need to be scraped. This
    method can be more convenient than creating the CSS or XPath selectors manually.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Portia是一个基于Scrapy构建的开源工具，支持通过点击需要抓取的网站部分来构建蜘蛛。这种方法可能比手动创建CSS或XPath选择器更方便。
- en: Installation
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装
- en: Portia is a powerful tool, and it depends on multiple external libraries for
    its functionality. It is also relatively new, so currently, the installation steps
    are somewhat involved. In case the installation is simplified in future, the latest
    documentation can be found at [https://github.com/scrapinghub/portia#running-portia](https://github.com/scrapinghub/portia#running-portia).
    The current recommended way to run Portia is to use Docker (the open-source container
    framework). If you don't have Docker installed, you'll need to do so first by
    following the latest instructions ([https://docs.docker.com/engine/installation/](https://docs.docker.com/engine/installation/)).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Portia是一个强大的工具，它依赖于多个外部库来实现其功能。它也是相对较新的，因此目前安装步骤相对复杂。如果将来简化了安装过程，最新文档可以在[https://github.com/scrapinghub/portia#running-portia](https://github.com/scrapinghub/portia#running-portia)找到。当前推荐运行Portia的方式是使用Docker（开源容器框架）。如果您尚未安装Docker，您需要首先按照最新说明进行安装（[https://docs.docker.com/engine/installation/](https://docs.docker.com/engine/installation/)）。
- en: 'Once Docker is installed and running, you can pull the `scrapinghub` image
    and get started. First, you should be in the directory you''d like to create your
    new portia project and run the command like so:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Docker安装并运行后，您可以拉取`scrapinghub`镜像并开始使用。首先，您应该位于您想要创建新Portia项目的目录中，并运行如下命令：
- en: '[PRE25]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In the command, we created a new folder at `~/portia_projects`. If you'd rather
    have your portia projects stored elsewhere, change the `-v` command to point to
    the absolute file path where you would like to store your portia files.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令中，我们在`~/portia_projects`创建了一个新文件夹。如果您希望将Portia项目存储在其他位置，请将`-v`命令更改为指向您希望存储Portia文件的绝对文件路径。
- en: These last few lines show that the Portia website is now up and running. The
    site will now be accessible in your web browser at `http://localhost:9001/`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最后几行显示Portia网站现在已上线并运行。现在您可以在您的网络浏览器中通过`http://localhost:9001/`访问该网站。
- en: 'Your initial screen should look similar to this:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 您的初始屏幕应类似于以下内容：
- en: '![](img/portia_home.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![Portia首页](img/portia_home.png)'
- en: If you have problems during installation it's worth checking the Portia Issues
    page at [https://github.com/scrapinghub/portia/issues](https://github.com/scrapinghub/portia/issues),
    in case someone else has experienced the same problem and found a solution. In
    this book I have used the specific Portia image I used (`scrapinghub/portia:portia-2.0.7`),
    but you can also try using the latest official release `scrapinghub/portia`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在安装过程中遇到问题，建议检查[https://github.com/scrapinghub/portia/issues](https://github.com/scrapinghub/portia/issues)页面，以防其他人遇到过相同的问题并找到了解决方案。在这本书中，我使用了特定的Portia图像（`scrapinghub/portia:portia-2.0.7`），但您也可以尝试使用最新的官方版本`scrapinghub/portia`。
- en: In addition, I recommend always using the latest recommended instructions as
    documented in the README file and Portia documentation, even if they differ from
    the ones covered in this section. Portia is under active development and instructions
    could change after the publication of this book.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我建议始终使用README文件和Portia文档中记录的最新推荐指令，即使它们与本节中涵盖的指令不同。Portia正在积极开发中，指令可能在本书出版后发生变化。
- en: Annotation
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标注
- en: At the Portia start page, the page prompts you to enter a project. Once you
    enter that text, then there is a textbox to enter the URL of the website you want
    to scrape, such as [http://example.webscraping.com](http://example.webscraping.com).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在Portia起始页面上，页面提示您输入一个项目。一旦输入该文本，就会出现一个文本框，用于输入您想要抓取的网站URL，例如[http://example.webscraping.com](http://example.webscraping.com)。
- en: 'When you''ve typed that, Portia will then load the project view:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当您输入后，Portia将加载项目视图：
- en: '![](img/portia_project.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/portia_project.png)'
- en: 'Once you click the New Spider button, you will see the following Spider view:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦点击“新建蜘蛛”按钮，您将看到以下蜘蛛视图：
- en: '![](img/portia_spider_view.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/portia_spider_view.png)'
- en: You will start to recognize some of the fields from the Scrapy spider we already
    built earlier in this chapter (such as start pages and link crawling rules). By
    default, the spider name is set to the domain (example.webscraping.com), which
    can be modified by clicking on the labels.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 您将开始识别一些来自本章前面已构建的Scrapy蜘蛛的字段（例如起始页面和链接抓取规则）。默认情况下，蜘蛛名称设置为域名（example.webscraping.com），可以通过点击标签进行修改。
- en: 'Next, click on the "New Sample" button to start collecting data from the page:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，点击“新示例”按钮开始从页面收集数据：
- en: '![](img/portia_select_area.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/portia_select_area.png)'
- en: Now when you roll over the different elements of the page, you will see them
    highlighted. You can also see the CSS selector in the Inspector tab to the right
    of the website area.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当您将鼠标悬停在页面上的不同元素上时，您将看到它们被突出显示。您还可以在网站区域右侧的检查器标签中看到CSS选择器。
- en: Because we want to scrape the population elements on the individual country
    pages, we first need to navigate from this homepage to the individual country
    pages. To do so, we first need to click "Close Sample" and then click on any country.
    When the country page loads, we can once again click "New Sample".
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们想要抓取个人国家页面上的人口元素，我们首先需要从主页导航到个人国家页面。为此，我们首先需要点击“关闭示例”，然后点击任何国家。当国家页面加载时，我们再次点击“新示例”。
- en: 'To start adding fields to our items for extraction, we can click on the population
    field. When we do, an item is added and we can see the extracted information:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始添加字段到我们的提取项中，我们可以点击“population”字段。当我们这样做时，将添加一个项，我们可以看到提取的信息：
- en: '![](img/portia_add_field.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/portia_add_field.png)'
- en: We can rename the field by using the left text field area and simply typing
    in the new name "population". Then, we can click the "Add Field" button. To add
    more fields, we can do the same for the country name and any other fields we are
    interested in by first clicking on the large + button and then selecting the field
    values in the same way. The annotated fields will be highlighted in the web page
    and you can see the extracted data in the extracted items section.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用左侧的文本字段区域并简单地输入新名称“population”来重命名字段。然后，我们可以点击“添加字段”按钮。要添加更多字段，我们可以通过首先点击大号+按钮，然后以相同的方式选择字段值来实现。标注的字段将在网页中被突出显示，您可以在提取项部分看到提取的数据。
- en: '![](img/portia_extracted_items.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/portia_extracted_items.png)'
- en: 'If you want to delete any fields, you can simply use the red - sign next to
    the field name. When the annotations are complete, click on the blue "Close sample" button
    at the top. If you then wanted to download the spider to run in a Scrapy project,
    you can do so by clicking the link next to the spider name:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要删除任何字段，你只需简单地使用字段名旁边的红色减号即可。当注释完成时，点击顶部蓝色的“关闭样本”按钮。如果你当时想要下载蜘蛛以在 Scrapy
    项目中运行，你可以通过点击蜘蛛名称旁边的链接来完成：
- en: '![](img/portia_download.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![下载 Portia](img/portia_download.png)'
- en: You can also see all of your spiders and the settings in the mounted folder
    `~/portia_projects`.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在挂载文件夹 `~/portia_projects` 中看到你所有的蜘蛛和设置。
- en: Running the Spider
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行蜘蛛
- en: 'If you are running Portia as a Docker container, you can run the `portiacrawl`
    command using the same Docker image. First, stop your current container using
    *Ctrl* + *C*. Then, you can run the following command:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在以 Docker 容器形式运行 Portia，你可以使用相同的 Docker 镜像运行 `portiacrawl` 命令。首先，使用 *Ctrl*
    + *C* 停止你的当前容器。然后，你可以运行以下命令：
- en: '`docker run -i -t --rm -v ~/portia_projects:/app/data/projects:rw -v <OUTPUT_FOLDER>:/mnt:rw
    -p 9001:9001 scrapinghub/portia portiacrawl /app/data/projects/<PROJECT_NAME>
    example.webscraping.com -o /mnt/example.webscraping.com.jl`'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run -i -t --rm -v ~/portia_projects:/app/data/projects:rw -v <OUTPUT_FOLDER>:/mnt:rw
    -p 9001:9001 scrapinghub/portia portiacrawl /app/data/projects/<PROJECT_NAME>
    example.webscraping.com -o /mnt/example.webscraping.com.jl`'
- en: 'Make sure to update the OUTPUT_FOLDER in an absolute path where you want to
    store your output files and PROJECT_NAME variables is the name you used when starting
    your project (mine was my_example_site). You should see output similar to output
    you notice when running Scrapy. You may notice error messages (this is due to
    not changing the download delay or parallel requests -- both of which can be done
    in the web interface by changing the project and spider settings). You can also
    pass extra settings to your spider when it is run using the `-s` flag. My command
    looks like this:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 确保更新 OUTPUT_FOLDER 为你想要存储输出文件的绝对路径，而 PROJECT_NAME 变量是你开始项目时使用的名称（我的例子是 my_example_site）。你应该会看到与运行
    Scrapy 时类似的输出。你可能还会注意到错误信息（这是由于没有更改下载延迟或并行请求——这两者都可以通过在网页界面中更改项目和蜘蛛设置来完成）。你还可以在运行蜘蛛时使用
    `-s` 标志传递额外的设置。我的命令看起来像这样：
- en: '``docker run -i -t --rm -v ~/portia_projects:/app/data/projects:rw -v ~/portia_output:/mnt:rw
    -p 9001:9001 scrapinghub/portia portiacrawl /app/data/projects/my_example_site example.webscraping.com
    -o /mnt/example.webscraping`.com.jl`-s CONCURRENT_REQUESTS_PER_DOMAIN=1 -s DOWNLOAD_DELAY=5``'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '``docker run -i -t --rm -v ~/portia_projects:/app/data/projects:rw -v ~/portia_output:/mnt:rw
    -p 9001:9001 scrapinghub/portia portiacrawl /app/data/projects/my_example_site example.webscraping.com
    -o /mnt/example.webscraping`.com.jl`-s CONCURRENT_REQUESTS_PER_DOMAIN=1 -s DOWNLOAD_DELAY=5``'
- en: Checking results
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查结果
- en: 'When the spider is finished, you can check your results in the output folder
    you created:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当蜘蛛完成工作后，你可以在你创建的输出文件夹中检查你的结果：
- en: '`$ head ~/portia_output/example.webscraping.com.jl {"_type": "Example web scraping
    website1", "url": "http://example.webscraping.com/view/Antigua-and-Barbuda-10",
    "phone_code": ["+1-268"], "_template": "98ed-4785-8e1b", "country_name": ["Antigua
    and Barbuda"], "population": ["86,754"]} {"_template": "98ed-4785-8e1b", "country_name":
    ["Antarctica"], "_type": "Example web scraping website1", "url": "http://example.webscraping.com/view/Antarctica-9",
    "population": ["0"]} {"_type": "Example web scraping website1", "url": "http://example.webscraping.com/view/Anguilla-8",
    "phone_code": ["+1-264"], "_template": "98ed-4785-8e1b", "country_name": ["Anguilla"],
    "population": ["13,254"]} ...`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ head ~/portia_output/example.webscraping.com.jl {"_type": "示例网络爬虫网站1", "url":
    "http://example.webscraping.com/view/Antigua-and-Barbuda-10", "phone_code": ["+1-268"],
    "_template": "98ed-4785-8e1b", "country_name": ["安提瓜和巴布达"], "population": ["86,754"]}
    {"_template": "98ed-4785-8e1b", "country_name": ["南极洲"], "_type": "示例网络爬虫网站1",
    "url": "http://example.webscraping.com/view/Antarctica-9", "population": ["0"]}
    {"_type": "示例网络爬虫网站1", "url": "http://example.webscraping.com/view/Anguilla-8",
    "phone_code": ["+1-264"], "_template": "98ed-4785-8e1b", "country_name": ["安圭拉"],
    "population": ["13,254"]} ...`'
- en: Here are a few examples of the results of your scrape. As you can see, they
    are in JSON format. If you wanted to export in CSV format, you can simply change
    the output file name to end with `.csv`.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些你爬取结果的示例。如你所见，它们是 JSON 格式。如果你想要导出为 CSV 格式，你只需简单地更改输出文件名，使其以 `.csv` 结尾。
- en: With just a few clicks on a site and a few instructions for Docker, you've scraped
    the example website! Portia is a handy tool to use, especially for straightforward
    websites, or if you need to collaborate with non-developers. On the other hand,
    for more complex websites, you always have the option to develop the Scrapy crawler
    directly in Python or use Portia to develop the first iteration and expand it
    using your own Python skills.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 只需在网站上点击几下，并为Docker提供一些指令，您就已经抓取了示例网站！Portia是一个方便的工具，特别是对于简单的网站，或者如果您需要与非开发者协作。另一方面，对于更复杂的网站，您始终有直接在Python中开发Scrapy爬虫或使用Portia开发第一迭代并使用自己的Python技能进行扩展的选择。
- en: Automated scraping with Scrapely
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scrapely进行自动化抓取
- en: For scraping the annotated fields Portia uses a library called **Scrapely ([https://github.com/scrapy/scrapely](https://github.com/scrapy/scrapely))**,
    which is a useful open-source tool developed independently from Portia. Scrapely
    uses training data to build a model of what to scrape from a web page. The trained
    model can then be applied to scrape other web pages with the same structure.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于抓取标注字段，Portia使用一个名为**Scrapely ([https://github.com/scrapy/scrapely](https://github.com/scrapy/scrapely))**的库，这是一个独立于Portia开发的非常有用的开源工具。Scrapely使用训练数据来构建从网页上抓取内容的模型。训练好的模型可以应用于抓取具有相同结构的其他网页。
- en: 'You can install it using pip:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用pip进行安装：
- en: '[PRE26]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here is an example to show how it works:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子来展示它是如何工作的：
- en: '[PRE27]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: First, Scrapely is given the data we want to scrape from the `Afghanistan` web
    page to train the model (here, the country name and population). This model is
    then applied to a different country page and Scrapely uses the trained model to
    correctly return the country name and population here as well.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Scrapely被赋予从`阿富汗`网页上抓取的数据来训练模型（这里，是国家名称和人口）。然后，这个模型被应用于不同国家的网页，Scrapely使用训练好的模型正确地返回这里的国家名称和人口。
- en: This workflow allows scraping web pages without needing to know their structure,
    only the desired content you want to extract for the training case (or multiple
    training cases). This approach can be particularly useful if the content of a
    web page is static, but the layout is changing. For example, with a news website,
    the text of the published article will most likely not change, though the layout
    may be updated. In this case, Scrapely can then be retrained using the same data
    to generate a model for the new website structure. For this example to work properly,
    you would need to store your training data somewhere for reuse.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这种工作流程允许在不了解网页结构的情况下抓取网页，只需知道您想要从训练案例（或多个训练案例）中提取的所需内容。如果网页的内容是静态的，但布局在变化，这种方法尤其有用。例如，对于新闻网站，已发布的文章文本很可能不会改变，尽管布局可能会更新。在这种情况下，Scrapely可以使用相同的数据重新训练，以生成针对新网站结构的模型。为了使这个例子正常工作，您需要将您的训练数据存储在某个地方以供重用。
- en: The example web page used here to test Scrapely is well structured with separate
    tags and attributes for each data type so Scrapely was able to correctly and easily
    train a model. For more complex web pages, Scrapely can fail to locate the content
    correctly. The Scrapely documentation warns you should "train with caution". As
    machine learning becomes faster and easier, perhaps a more robust automated web
    scraping library will be released; for now, it is still quite useful to know how
    to scrape a website directly using the techniques covered throughout this book.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这里用来测试Scrapely的示例网页结构良好，每个数据类型都有独立的标签和属性，因此Scrapely能够正确且容易地训练一个模型。对于更复杂的网页，Scrapely可能无法正确定位内容。Scrapely文档警告您应该“谨慎训练”。随着机器学习的速度和易用性不断提高，可能一个更健壮的自动化网络抓取库将被发布；目前，了解如何使用本书中涵盖的技术直接抓取网站仍然非常有用。
- en: Summary
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter introduced Scrapy, a web scraping framework with many high-level
    features to improve efficiency when scraping websites. Additionally, we covered
    Portia, which provides a visual interface to generate Scrapy spiders. Finally,
    we tested Scrapely, the library used by Portia to scrape web pages automatically
    by first training a simple model.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了Scrapy，这是一个具有许多高级功能的网络抓取框架，可以提高抓取网站时的效率。此外，我们还介绍了Portia，它提供了一个可视化界面来生成Scrapy爬虫。最后，我们测试了Scrapely，这是Portia用来通过首先训练一个简单模型来自动抓取网页的库。
- en: In the next chapter, we will apply the skills learned so far to some real-world
    websites.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把到目前为止学到的技能应用到一些真实世界的网站上。
