- en: Chapter 4. Parallel Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章 并行处理
- en: With parallel processing you can increase the amount of calculations your program
    can do in a given time without needing a faster processor. The main idea is to
    divide a task into many sub-units and employ multiple processors to solve them
    independently.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 使用并行处理，你可以在不使用更快的处理器的情况下，在给定时间内增加程序可以完成的计算量。主要思想是将任务划分为许多子单元，并使用多个处理器独立解决它们。
- en: CPUs containing several cores (2, 4, 6, 8, ...) have become a common trend in
    technology. Increasing the speed of a single processor is costly and problematic;
    while leveraging the parallel capabilities of cheaper multi-core processors is
    a feasible route to increase performance.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 包含多个核心（2、4、6、8、...）的 CPU 已成为技术界的普遍趋势。提高单个处理器的速度成本高昂且问题重重；而利用价格更低的多个核心处理器的并行能力是提高性能的可行途径。
- en: Parallel processing lets you tackle large scale problems. Scientists and engineers
    commonly run parallel code on supercomputers—huge networks of standard processors—to
    simulate massive systems. Parallel techniques can also take advantage of graphics
    chips (a hardware optimized for parallelization).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 并行处理让你能够处理大规模问题。科学家和工程师通常在超级计算机上运行并行代码——由大量标准处理器组成的庞大网络——以模拟庞大的系统。并行技术还可以利用图形芯片（一种针对并行化优化的硬件）。
- en: Python can be used in all of these domains, allowing us to apply parallel processing
    to all sorts of problems with simplicity and elegance, opening the door to infinite
    possibilities.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Python 可以用于所有这些领域，使我们能够以简单和优雅的方式将并行处理应用于各种问题，开启无限可能的大门。
- en: 'In this chapter, we will:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将：
- en: Briefly introduce the fundamentals of parallel processing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简要介绍并行处理的基本原理
- en: Illustrate how to parallelize simple problems with the multiprocessing Python
    library
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 multiprocessing Python 库说明如何并行化简单问题
- en: Learn how to write programs with the **IPython parallel** framework
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何使用**IPython parallel**框架编写程序
- en: Further optimize our program using multithreading with Cython and OpenMP
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Cython 和 OpenMP 进一步优化我们的程序
- en: Introduction to parallel programming
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行编程简介
- en: In order to parallelize a program, we need to divide the problem into sub-units
    that can run independently (or almost independently) from each other.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了并行化一个程序，我们需要将问题划分为可以独立（或几乎独立）运行的子单元。
- en: A problem where the sub-units are totally independent from each other is called
    **embarrassingly parallel**. An element-wise operation on an array is a typical
    example—the operation needs only to know the element it is handling at the moment.
    Another example, is our particle simulator—since there are no interactions, each
    particle can evolve in time independently from the others. Embarrassingly parallel
    problems are very easy to implement and they perform optimally on parallel architectures.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当子单元之间完全独立时，这样的问题被称为**令人尴尬的并行**。数组上的元素级操作是一个典型例子——操作只需要知道它当前处理的元素。另一个例子是我们的粒子模拟器——由于没有相互作用，每个粒子可以独立于其他粒子在时间上进化。令人尴尬的并行问题很容易实现，并且在并行架构上表现最优。
- en: Other problems may be divided into sub-units but have to share some data to
    perform their calculations. In those cases, the implementation is less straightforward
    and can lead to performance issues because of the communication costs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 其他问题可能可以划分为子单元，但必须共享一些数据以执行它们的计算。在这些情况下，实现方式不太直接，并且由于通信成本可能导致性能问题。
- en: 'We will illustrate the concept with an example. Imagine you have a particle
    simulator, but this time the particles attract other particles within a certain
    distance (as shown in the following figure). To parallelize this problem we divide
    the simulation box in regions and assign each region to a different processor.
    If we evolve the system for one step, some particles will interact with particles
    in a neighboring region. To perform the next iteration, the new particle positions
    of the neighboring region are required:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个例子来说明这个概念。想象你有一个粒子模拟器，但这次粒子在特定距离内会吸引其他粒子（如图所示）。为了并行化这个问题，我们将模拟区域划分为区域，并将每个区域分配给不同的处理器。如果我们对系统进行一步进化，一些粒子将与相邻区域的粒子相互作用。为了执行下一次迭代，需要相邻区域的新粒子位置：
- en: '![Introduction to parallel programming](img/8458OS_04_1.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![并行编程简介](img/8458OS_04_1.jpg)'
- en: 'Communication between processes is costly and can seriously hinder the performance
    of parallel programs. There exists two main ways to handle data communication
    in parallel programs:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 进程间的通信成本高昂，可能会严重阻碍并行程序的性能。在并行程序中处理数据通信存在两种主要方法：
- en: '**Shared memory**'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**共享内存**'
- en: '**Distributed memory**'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式内存**'
- en: In shared memory, the sub-units have access to the same memory space. The advantage
    of this approach, is that you don't have to explicitly handle the communication
    as it is sufficient to write or read from the shared memory. However, problems
    arise when multiple processes try to access and change the same memory location
    at the same time. Care should be taken to avoid such conflict using synchronization
    techniques.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在共享内存中，子单元可以访问相同的内存空间。这种方法的优点是，你不需要显式地处理通信，因为从共享内存中写入或读取就足够了。然而，当多个进程同时尝试访问和更改相同的内存位置时，就会出现问题。应小心使用同步技术来避免此类冲突。
- en: In the distributed memory model each process is completely separated from the
    others and possesses its own memory space. In this case, communication is handled
    explicitly between the processes. The communication overhead is typically costlier
    compared to shared memory, as data can potentially travel through a network interface.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式内存模型中，每个进程与其他进程完全分离，并拥有自己的内存空间。在这种情况下，进程间的通信是显式处理的。与共享内存相比，通信开销通常更昂贵，因为数据可能需要通过网络接口传输。
- en: One common way to achieve parallelism with the shared memory model is **threads**.
    Threads are independent sub-tasks that originate from a process and share resources
    such as memory.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在共享内存模型中实现并行性的一个常见方法是 **线程**。线程是从进程派生出来的独立子任务，并共享资源，如内存。
- en: Python can spawn and handle threads, but they can't be used to increase performance
    due to the Python interpreter design—only one Python instruction is allowed to
    run at a time. This mechanism is called **Global Interpreter Lock** (**GIL**).
    What happens is that, each time a thread executes a Python statement, a lock is
    acquired which prevents other threads to run until it is released. The GIL avoids
    conflicts between threads, simplifying the implementation of the **CPython** interpreter.
    Despite this limitation, threads can still be used to provide concurrency in situations
    where the lock can be released, such as in time-consuming I/O operations or in
    C extensions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Python 可以创建和处理线程，但由于 Python 解释器的设计，它们不能用来提高性能——一次只能允许一个 Python 指令运行。这种机制被称为
    **全局解释器锁**（**GIL**）。其工作原理是，每次线程执行 Python 语句时，都会获取一个锁，这阻止了其他线程在锁释放之前运行。GIL 避免了线程之间的冲突，简化了
    **CPython** 解释器的实现。尽管存在这种限制，但在可以释放锁的情况，例如耗时的 I/O 操作或 C 扩展中，线程仍然可以用来提供并发性。
- en: 'The GIL can be completely avoided by using processes instead of threads. Processes
    don''t share the same memory area and are independent from each other—each process
    has its own interpreter. By using processes, we''ll have very few disadvantages:
    inter-process communication is less efficient than shared memory, but it is more
    flexible and explicit.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过使用进程而不是线程来完全避免 GIL。进程不共享相同的内存区域，彼此独立——每个进程都有自己的解释器。通过使用进程，我们将几乎没有缺点：进程间通信比共享内存效率低，但它更灵活且更明确。
- en: '![Introduction to parallel programming](img/8458OS_04_2.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![并行编程简介](img/8458OS_04_2.jpg)'
- en: The multiprocessing module
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多进程模块
- en: The standard `multiprocessing` module can be used to quickly parallelize simple
    tasks by spawning several processes. Its interface is easy-to-use and includes
    several utilities to handle task submission and synchronization.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的 `multiprocessing` 模块可以通过创建多个进程来快速并行化简单任务。它的接口易于使用，并包括一些用于处理任务提交和同步的实用工具。
- en: The Process and Pool classes
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进程和池类
- en: 'You can create a process that runs independently by subclassing `multiprocessing.Process`.
    You can extend the `__init__` method to initialize resources and you can write
    the portion of the code destined to the subprocess by implementing a `Process.run`
    method. In the following code, we define a process that will wait for one second
    and print its assigned `id`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过继承 `multiprocessing.Process` 来创建一个独立运行的进程。你可以扩展 `__init__` 方法来初始化资源，并且可以通过实现
    `Process.run` 方法来编写分配给子进程的代码部分。在以下代码中，我们定义了一个将等待一秒并打印其分配的 `id` 的进程：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To spawn the process, we have to initialize our `Process` object and call the
    `Process.start` method. Notice that you don''t directly call `Process.run`: the
    call to `Process.start` will create a new process and, in turn, call the `Process.run`
    method. We can add the following lines at the end of the script to initialize
    and start the new process:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建进程，我们必须初始化我们的 `Process` 对象并调用 `Process.start` 方法。请注意，您不会直接调用 `Process.run`：调用
    `Process.start` 将创建一个新的进程，并依次调用 `Process.run` 方法。我们可以在脚本末尾添加以下行来初始化并启动新进程：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The instructions after `Process.start` will be executed immediately without
    waiting for the process `p` to finish. To wait for the task completion you can
    use the method `Process.join`, as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `Process.start` 之后的指令将立即执行，而不需要等待进程 `p` 完成。要等待任务完成，您可以使用 `Process.join` 方法，如下所示：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can launch in the same way four different processes that will run in parallel.
    In a serial program, the total required time would be four seconds. Since we run
    it parallelly, each process will run at the same time, resulting in a 1-second
    wallclock time. In the following code, we create four processes and start them
    parallelly:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用相同的方式启动四个不同的进程，它们将并行运行。在一个串行程序中，总共需要四秒钟。由于我们并行运行，每个进程将同时运行，从而实现1秒的墙钟时间。在以下代码中，我们创建了四个进程并并行启动它们：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice that the order of the execution of parallel processes is unpredictable,
    it ultimately depends on how the operating system schedules the process execution.
    You can verify this behavior by running the program multiple times—the order will
    be different at each run.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，并行进程的执行顺序是不可预测的，它最终取决于操作系统如何调度进程执行。您可以通过多次运行程序来验证此行为——每次运行的顺序都会不同。
- en: The `multiprocessing` module exposes a convenient interface that makes it easy
    to assign and distribute tasks to a set of processes, the `multiprocessing.Pool`
    class.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`multiprocessing` 模块提供了一个方便的接口，使得将任务分配和分发到一组进程变得容易，`multiprocessing.Pool` 类。'
- en: The `multiprocessing.Pool` class spawns a set of processes—called **workers**—and
    lets submit tasks through the methods `apply`/`apply_async` and `map`/`map_async`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`multiprocessing.Pool` 类创建了一组进程——称为 **工作者**——并允许通过 `apply`/`apply_async` 和
    `map`/`map_async` 方法提交任务。'
- en: The `Pool.map` method applies a function to each element of a list and returns
    the list of results. Its usage is equivalent to the built-in (serial) `map`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pool.map` 方法将函数应用于列表中的每个元素，并返回结果列表。它的用法与内置（串行）`map` 相当。'
- en: 'To use a parallel map, you should first initialize a `multiprocessing.Pool`
    object. It takes the number of workers as its first argument; if not provided,
    that number will be equal to the number of cores in the system. You can initialize
    a `multiprocessing.Pool` object in the following way:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用并行映射，您首先需要初始化一个 `multiprocessing.Pool` 对象。它将工作进程的数量作为其第一个参数；如果没有提供，则该数量将与系统中的核心数相等。您可以通过以下方式初始化一个
    `multiprocessing.Pool` 对象：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s see `Pool.map` in action. If you have a function that computes the square
    of a number, you can map the function to the list by calling `Pool.map` and passing
    the function and the list of inputs as arguments, as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 `Pool.map` 的实际应用。如果您有一个计算数字平方的函数，您可以通过调用 `Pool.map` 并传递函数及其输入列表作为参数，将该函数映射到列表上，如下所示：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `Pool.map_async` method is just like `Pool.map` but returns an `AsyncResult`
    object instead of the actual result. When we call the normal `map`, the execution
    of the main program is stopped until all the workers are finished processing the
    result. With `map_async`, the `AsyncResult` object is returned immediately without
    blocking the main program and the calculations are done in the background. We
    can then retrieve the result by using the `AsyncResult.get` method at any time,
    as shown in the following lines:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pool.map_async` 方法与 `Pool.map` 类似，但返回一个 `AsyncResult` 对象而不是实际的结果。当我们调用正常的
    `map` 时，主程序的执行将停止，直到所有工作进程完成处理结果。使用 `map_async`，`AsyncResult` 对象将立即返回，而不会阻塞主程序，计算将在后台进行。然后我们可以通过使用
    `AsyncResult.get` 方法在任何时候检索结果，如下所示：'
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`Pool.apply_async` assigns a task consisting of a single function to one of
    the workers. It takes the function and its arguments and returns an `AsyncResult`
    object. We can obtain an effect similar to `map` by using `apply_async`, as shown
    in the following code:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pool.apply_async` 将由单个函数组成的任务分配给一个工作进程。它接受函数及其参数，并返回一个 `AsyncResult` 对象。我们可以通过使用
    `apply_async` 来获得类似于 `map` 的效果，如下所示：'
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As an example, we will implement a canonical, embarassingly parallel program:
    the **Monte Carlo approximation of pi**.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 作为例子，我们将实现一个典型的、令人尴尬的并行程序：**蒙特卡洛法估算π**。
- en: Monte Carlo approximation of pi
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蒙特卡洛法估算π
- en: Imagine we have a square with a side length of 2 units; its area will be 4 units.
    Now, we inscribe a circle with a radius 1 unit in this square, the area of the
    circle will be `pi * r^2`. By substituting the value of `r` in the previous equation
    we get that the numerical value for the area of the circle is `pi * (1)^2 = pi`.
    You can refer to the following figure for a graphical representation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 想象我们有一个边长为2个单位的正方形；其面积将是4个单位。现在，我们在正方形内画一个半径为1个单位的圆，圆的面积将是 `π * r^2`。通过将 `r`
    的值代入前面的方程，我们得到圆的面积数值为 `π * (1)^2 = π`。你可以参考以下图示进行图形表示。
- en: 'If we shoot a lot of random points on this figure, some points will fall into
    the circle—we''ll call them **hits**—while the remaining points—**misses**—will
    be outside the circle. The idea of the Monte Carlo method is that the area of
    the circle will be proportional to the number of hits, while the area of the square
    will be proportional to the total number of shots. To get the value of `pi`, it
    is sufficient to divide the area of the circle (equal to `pi`) by the area of
    the square (equal to 4) and solve for `pi`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在该图形上随机射击很多点，一些点将落在圆内——我们将它们称为**命中**——而剩余的点——**未命中**——将位于圆外。蒙特卡洛方法的思想是圆的面积将与命中的数量成正比，而正方形的面积将与射击的总数成正比。为了得到π的值，只需将圆的面积（等于π）除以正方形的面积（等于4）并解出π：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Monte Carlo approximation of pi](img/8458OS_04_3.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![蒙特卡洛法估算π的图](img/8458OS_04_3.jpg)'
- en: 'The strategy we will employ in our program will be:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们程序中将采用的战略是：
- en: Generate a lot of sample (*x*, *y*) numbers in the range (-1, 1)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在范围（-1，1）内生成大量的样本（*x*，*y*）数字
- en: Test if those numbers lie inside the circle by checking if `x**2 + y**2 == 1`
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过检查 `x**2 + y**2 == 1` 来测试这些数字是否位于圆内
- en: 'We first write a serial version and check if it works. Then, we can write the
    parallel version. The implementation of the serial program is as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先编写一个串行版本并检查它是否工作。然后，我们可以编写并行版本。串行程序的实现如下：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The accuracy of our approximation will improve as we increase the number of
    samples. You can notice that each loop iteration is independent from the other—this
    problem is embarassingly parallel.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 随着样本数量的增加，我们的近似精度将提高。你可以注意到每个循环迭代都是独立的——这个问题是令人尴尬的并行。
- en: 'To parallelize this code, we can write a function called `sample` that corresponds
    to a single hit-miss check. If the sample hits the circle, the function will return
    `1`; otherwise it will return `0`. By running `sample` multiple times and summing
    the results, we''ll get the total number of hits. We can run `sample` over multiple
    processors with `apply_async` and get the results in the following way:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了并行化此代码，我们可以编写一个名为 `sample` 的函数，它对应于单个命中-未命中检查。如果样本击中圆，则函数返回 `1`；否则返回 `0`。通过多次运行
    `sample` 并汇总结果，我们将得到总命中数。我们可以通过 `apply_async` 在多个处理器上运行 `sample` 并以下列方式获取结果：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can wrap the two versions in the functions `pi_serial` and `pi_apply_async`
    (you can find their implementation in the `pi.py` file) and benchmark the execution
    speed as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这两个版本封装在函数 `pi_serial` 和 `pi_apply_async` 中（你可以在 `pi.py` 文件中找到它们的实现）并按如下方式基准测试执行速度：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As shown in the previous benchmark, our first parallel version literally cripples
    our code. The reason is that the time spent doing the actual calculation is small
    compared to the overhead required to send and distribute the tasks to the workers.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如前基准测试所示，我们的第一个并行版本实际上削弱了我们的代码。原因是实际计算所需的时间与发送和分配任务到工作者的开销相比非常小。
- en: 'To solve the issue, we have to make the overhead negligible compared to the
    calculation time. For example, we can ask each worker to handle more than one
    sample at a time, thus reducing the task communication overhead. We can write
    a function `sample_multiple` that processes more than one hit and modifies our
    parallel version by splitting our problem in 10, more intensive tasks as shown
    in the following code:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们必须使开销与计算时间相比可以忽略不计。例如，我们可以要求每个工作者一次处理多个样本，从而减少任务通信开销。我们可以编写一个名为 `sample_multiple`
    的函数，通过将问题分解为10个更密集的任务来修改我们的并行版本，如下面的代码所示：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can wrap this in a function called `pi_apply_async_chunked` and run it as
    follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个功能封装在一个名为 `pi_apply_async_chunked` 的函数中，并按如下方式运行：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The results are much better; we more than doubled the speed of our program.
    You can also notice that the `user` metric is larger than `real`: the total CPU
    time is larger than the total time because more than one CPU worked at the same
    time. If you increase the number of samples, you will notice that the ratio of
    communication to calculation decreases, giving even better speedups.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 结果要好得多；我们使程序的速度提高了不止一倍。你还可以注意到`user`指标大于`real`：总CPU时间大于总时间，因为同时使用了多个CPU。如果你增加样本数量，你会注意到通信与计算的比率降低，从而提供更好的加速。
- en: Everything is nice and simple when dealing with embarassingly parallel problems.
    But sometimes, you have to share data between processes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理令人尴尬的并行问题时，一切都很简单。但有时，你必须在进程之间共享数据。
- en: Synchronization and locks
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 同步和锁
- en: 'Even if `multiprocessing` uses processes (with their own independent memory),
    it lets you define certain variables and arrays as shared memory. You can define
    a shared variable by using `multiprocessing.Value` passing its data type as a
    string (`i` integer, `d` double, `f` float, and so on). You can update the content
    of the variable through the `value` attribute, as shown in the following code
    snippet:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 即使`multiprocessing`使用进程（它们有自己独立的内存），它也允许你定义某些变量和数组作为共享内存。你可以通过使用`multiprocessing.Value`并将数据类型作为字符串传递（`i`表示整数，`d`表示双精度，`f`表示浮点数等）来定义一个共享变量。你可以通过`value`属性更新变量的内容，如下面的代码片段所示：
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'When using shared memory, you should be aware of concurrent accesses. Imagine
    you have a shared integer variable and each process increments its value multiple
    times. You would define a process class as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用共享内存时，你应该意识到并发访问。想象你有一个共享的整数变量，每个进程多次增加它的值。你可以定义一个进程类如下：
- en: '[PRE15]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You can initialize the shared variable in the main program and pass it to `4`
    processes, as shown in the following code:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在主程序中初始化共享变量，并将其传递给`4`个进程，如下面的代码所示：
- en: '[PRE16]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: If you run this program (`shared.py` in the code directory) you will notice
    that the final value of `counter` is not 4000, but it has random values (on my
    machine they are between 2000 and 2500). If we assume that the arithmetic is correct,
    we can conclude that there's a problem with the parallelization.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这个程序（代码目录中的`shared.py`），你会注意到`counter`的最终值不是4000，而是有随机值（在我的机器上它们在2000到2500之间）。如果我们假设算术是正确的，我们可以得出结论，并行化存在问题。
- en: What happens is that multiple processes are trying to access the same shared
    variable at the same time. The situation is best explained by looking at the following
    figure. In a serial execution, the first process reads (the number `0`), increments
    it, and writes the new value (`1`); the second process reads the new value (`1`),
    increments it, and writes it again (`2`). In the parallel execution, the two processes
    read the value (`0`), increment it, and write it (`1`) at the same time, leading
    to a wrong answer.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 发生的情况是多个进程同时尝试访问相同的共享变量。这种情况最好通过以下图示来解释。在串行执行中，第一个进程读取（数字`0`），增加它，并写入新值（`1`）；第二个进程读取新值（`1`），增加它，并再次写入（`2`）。在并行执行中，两个进程同时读取值（`0`），增加它，并写入它（`1`），导致错误的结果。
- en: '![Synchronization and locks](img/8458OS_04_4.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![同步和锁](img/8458OS_04_4.jpg)'
- en: To solve this problem, we need to synchronize the access to this variable so
    that only one process at a time can access, increment, and write the value on
    the shared variable. This feature is provided by the `multiprocessing.Lock` class.
    A lock can be acquired and released through the `acquire` and `release` methods,
    or by using the lock as a context manager. When a process acquires a lock, other
    processes are prevented to acquire it until the lock is released.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们需要同步对这个变量的访问，以确保一次只有一个进程可以访问、增加并在共享变量上写入值。这个功能由`multiprocessing.Lock`类提供。锁可以通过`acquire`和`release`方法获取和释放，或者通过将锁用作上下文管理器来使用。当一个进程获取锁时，其他进程将无法获取它，直到锁被释放。
- en: 'We can define a global lock, and use it as a context manager to restrict the
    access to the counter, as shown in the following code snippet:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义一个全局锁，并使用它作为上下文管理器来限制对计数器的访问，如下面的代码片段所示：
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Synchronization primitives such as locks are essential to solve many problems
    but you should avoid overusing them because they can decrease the performance
    of your program.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 同步原语，如锁，对于解决许多问题是必不可少的，但你应该避免过度使用它们，因为它们可能会降低你程序的性能。
- en: Note
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '`multiprocessing` includes other communication and synchronization tools, you
    can refer to the official documentation for a complete reference:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`multiprocessing` 包含其他通信和同步工具，你可以参考官方文档以获取完整的参考信息：'
- en: '[http://docs.python.org/3/library/multiprocessing.html](http://docs.python.org/3/library/multiprocessing.html)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://docs.python.org/3/library/multiprocessing.html](http://docs.python.org/3/library/multiprocessing.html)'
- en: IPython parallel
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IPython parallel
- en: IPython's power is not limited to its advanced shell. Its `parallel` package
    includes a framework to setup and run calculations on single and multi-core machines,
    as well as on multiple nodes connected to a network. IPython is great because
    it gives an interactive twist to parallel computing and provides a common interface
    to different communication protocols.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: IPython 的强大之处不仅限于其高级壳。它的 `parallel` 包包括一个框架，用于在单核和多核机器上以及连接到网络的多个节点上设置和运行计算。IPython
    很棒，因为它为并行计算增添了交互式特性，并为不同的通信协议提供了一个统一的接口。
- en: To use `IPython.parallel`, you have to start a set of workers— **Engines**—that
    are managed by a **Controller** (an entity that mediates the communication between
    the client and the engines). The approach is totally different from multiprocessing;
    you start the worker processes separately, and they will wait indefinitely, listening
    for commands from the client.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `IPython.parallel`，你必须启动一组由 **Controller**（一个在客户端和引擎之间进行通信调度的实体）管理的 **Engines**（引擎）。这种方法与多进程完全不同；你将单独启动工作进程，它们将无限期地等待，监听来自客户端的命令。
- en: 'To start the controller and a set of engines (by default, one engine per processing
    unit) you can use the `ipcluster` shell command, as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动控制器和一组引擎（默认情况下，每个处理单元一个引擎），你可以使用 `ipcluster` 壳命令，如下所示：
- en: '[PRE18]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'With `ipcluster` you can also set up multiple nodes to distribute your calculations
    over a network by writing a custom profile. You can refer to the official documentation
    for specific instructions at the following website:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `ipcluster`，你还可以通过编写自定义配置文件来设置多个节点，以在网络中分配你的计算。你可以参考以下网站上的官方文档以获取具体说明：
- en: '[http://ipython.org/ipython-doc/dev/parallel/parallel_process.html](http://ipython.org/ipython-doc/dev/parallel/parallel_process.html)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://ipython.org/ipython-doc/dev/parallel/parallel_process.html](http://ipython.org/ipython-doc/dev/parallel/parallel_process.html)'
- en: 'After starting the controller and the engines, we can use an IPython shell
    to perform calculations in parallel. IPython provides two basic interfaces (or
    views): **direct** and **task-based**.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动控制器和引擎后，我们可以使用 IPython 壳来并行执行计算。IPython 提供了两个基本接口（或视图）：**直接**和**基于任务的**。
- en: Direct interface
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 直接接口
- en: The direct interface lets you issue commands explicitly to each of the computing
    units. The interface is intuitive, flexible, and easy-to-use, especially when
    used in an interactive session.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 直接接口允许你明确地向每个计算单元发送命令。该接口直观、灵活且易于使用，尤其是在交互式会话中使用时。
- en: 'After starting the engines, you have to start an IPython session in a separate
    shell to interact with them. By creating a client, you can establish a connection
    to the controller. In the following code, we import the `Client` class and create
    an instance:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动引擎后，你必须在单独的壳中启动 IPython 会话来与之交互。通过创建一个客户端，你可以建立与控制器的连接。在以下代码中，我们导入 `Client`
    类并创建一个实例：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The attribute `Client.ids` will give you a list of integers representing the
    available engines, as shown in the following code snippet:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 属性 `Client.ids` 将会给你一个表示可用引擎的整数列表，如下代码片段所示：
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can issue commands to the engines by obtaining a `DirectView` instance.
    You can get a `DirectView` instance by either indexing the `Client` instance or
    by calling the `DirectView.direct_view` method. The following code shows different
    ways to obtain a `DirectView` instance from the previously created `Client`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过获取一个 `DirectView` 实例来向引擎发送命令。你可以通过索引 `Client` 实例或调用 `DirectView.direct_view`
    方法来获取一个 `DirectView` 实例。以下代码展示了从先前创建的 `Client` 中获取 `DirectView` 实例的不同方法：
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You can treat the engines like fresh IPython sessions. At the finest level,
    you can execute commands remotely by using the `DirectView.execute` method:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将引擎视为新的 IPython 会话。在最细粒度上，你可以通过使用 `DirectView.execute` 方法远程执行命令：
- en: '[PRE22]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The command will be sent and executed individually by each engine. The return
    value will be an `AsyncResult` object and the actual return value can be retrieved
    using the `get` method.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 命令将由每个引擎单独发送和执行。返回值将是一个 `AsyncResult` 对象，实际返回值可以通过 `get` 方法检索。
- en: 'As shown in the following code, you can retrieve the data contained in a remote
    variable by using the `DirectView.pull` method and send the data to a remote variable
    with the `DirectView.push` method. The `DirectView` class also supports a convenient
    dictionary-like interface:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下代码所示，你可以使用`DirectView.pull`方法检索远程变量中的数据，并使用`DirectView.push`方法将数据发送到远程变量。`DirectView`类还支持方便的类似字典的接口：
- en: '[PRE23]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: It is possible to send and retrieve every object that can be serialized using
    the `pickle` module. On top of that, special handling is reserved for data structures
    such as **NumPy** arrays to increase the efficiency.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`pickle`模块发送和检索所有可序列化的对象。除此之外，还专门处理了如**NumPy**数组等数据结构以提高效率。
- en: 'If you issue a statement that causes an exception, you will receive a summary
    of the exceptions in each engine:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发出一个会导致异常的语句，你将收到每个引擎中异常的摘要：
- en: '[PRE24]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Engines should be treated as independent IPython sessions, and imports and
    custom-defined functions must be synchronized over the network. To import some
    libraries, both locally and in the engines, you can use the `DirectView.sync_imports`
    context manager:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 应将引擎视为独立的IPython会话，并且必须通过网络同步导入和自定义函数。要导入一些库，包括本地和引擎中，可以使用`DirectView.sync_imports`上下文管理器：
- en: '[PRE25]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: To submit calculations to the engines, `DirectView` provides some utilities
    for common use cases such as map and apply. The `DirectView.map` method works
    similarly to `Pool.map_async`, as shown in the following code snippet. You map
    a function to a sequence, returning an `AsyncResult` object
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 要将计算提交给引擎，`DirectView`提供了一些用于常见用例的实用工具，例如map和apply。`DirectView.map`方法的工作方式类似于`Pool.map_async`，如下面的代码片段所示。你将函数映射到序列，返回一个`AsyncResult`对象。
- en: '[PRE26]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'IPython provides a more convenient map implementation through the `DirectView.parallel`
    decorator. If you apply the decorator on a function, the function will now have
    a `map` method that can be applied to a sequence. In the following code, we apply
    the parallel decorator to the `square` function and map it over a series of numbers:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: IPython通过`DirectView.parallel`装饰器提供了一个更方便的map实现。如果你在函数上应用装饰器，该函数现在将具有一个可以应用于序列的`map`方法。在以下代码中，我们将并行装饰器应用于`square`函数并将其映射到一系列数字上：
- en: '[PRE27]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Tip
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: To get the non-blocking version of `map`, you can either use the `DirectView.map_sync`
    method or pass the `block=True` option to the `DirectView.parallel` decorator.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取`map`的非阻塞版本，你可以使用`DirectView.map_sync`方法，或者将`block=True`选项传递给`DirectView.parallel`装饰器。
- en: 'The `DirectView.apply` method behaves in a different way than `Pool.apply_async`.
    The function gets executed on *every* engine. For example, if we have selected
    four engines and we apply the `square` function, the function gets executed once
    per engine and it returns four results, as shown in the following code snippet:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`DirectView.apply`方法的行为与`Pool.apply_async`不同。函数将在每个引擎上执行。例如，如果我们选择了四个引擎并应用`square`函数，该函数在每个引擎上执行一次，并返回四个结果，如下面的代码片段所示：'
- en: '[PRE28]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `DirectiView.remote` decorator lets you create a function that will run
    directly on each engine. Its usage is as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`DirectiView.remote`装饰器允许你创建一个将在每个引擎上直接运行的函数。其用法如下：'
- en: '[PRE29]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `DirectView` also provides two other kinds of communication scheme: **scatter**
    and **gather**.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`DirectView`还提供了两种其他类型的通信方案：**scatter**和**gather**。'
- en: 'Scatter distributes a list of inputs to the engines. Imagine you have four
    inputs and four engines; you can distribute those inputs in a remote variable
    with `DirectView.scatter`, as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Scatter将输入列表分配给引擎。想象一下你有四个输入和四个引擎；你可以使用`DirectView.scatter`在远程变量中分配这些输入，如下所示：
- en: '[PRE30]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Scatter will try to distribute the inputs as equally as possible even when
    the number of inputs is not a multiple of the number of engines. The following
    code shows how a list of 11 computations gets processed in three batches of three
    items per batch and one batch of two items:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Scatter会尽可能均匀地分配输入，即使输入的数量不是引擎数量的倍数。以下代码展示了如何将11个计算处理成三个每批三个项目的批次和一个每批两个项目的批次：
- en: '[PRE31]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `gather` function simply retrieves the scattered values and merges them
    back. In the following snippet, we merge back the scattered results:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`gather`函数简单地检索分散的值并将它们合并。在以下代码片段中，我们将分散的结果合并回来：'
- en: '[PRE32]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We can use the `scatter` and `gather` functions to parallelize one of our simulations.
    In our system, each particle is independent from the other, therefore, we can
    use `scatter` and `gather` to divide the particles equally between the available
    engines, evolve them, and get the particles back from the engines.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `scatter` 和 `gather` 函数来并行化我们的模拟之一。在我们的系统中，每个粒子与其他粒子是独立的，因此我们可以使用 `scatter`
    和 `gather` 将粒子平均分配到可用的引擎之间，演化它们，并从引擎中获取粒子。
- en: At first, we have to set up the engines. The `ParticleSimulator` class should
    be made available to all the engines.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须设置引擎。`ParticleSimulator` 类应该对所有引擎可用。
- en: 'Remember that the engines have started in a separate process and the `simul`
    module should be importable by them. You can achieve this in two ways:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，引擎是在一个单独的进程中启动的，`simul` 模块应该可以被它们导入。你可以通过两种方式实现这一点：
- en: By launching `ipcluster` in the directory, where `simul.py` is located
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在 `simul.py` 所在目录中启动 `ipcluster`
- en: By adding that directory to `PYTHONPATH`
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将那个目录添加到 `PYTHONPATH`
- en: If you're using the code examples, don't forget to compile the Cython extensions
    using `setup.py`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用代码示例，别忘了使用 `setup.py` 编译 Cython 扩展。
- en: 'In the following code, we create the particles and obtain a `DirectView` instance:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们创建粒子并获取一个 `DirectView` 实例：
- en: '[PRE33]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, we can scatter the particles to a remote variable `particle_chunk`, perform
    the particle evolution using `DirectView.execute` and retrieve the particles.
    We do this using `scatter`, `execute`, and `gather`, as shown in the following
    code:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将粒子散射到远程变量 `particle_chunk`，使用 `DirectView.execute` 执行粒子演化并检索粒子。我们使用
    `scatter`、`execute` 和 `gather` 来完成这项工作，如下面的代码所示：
- en: '[PRE34]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We can now wrap the parallel version and benchmark it against the serial one
    (refer to the file `simul_parallel.py`) in the following way:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以封装并行版本，并将其与串行版本（参考文件 `simul_parallel.py`）进行基准测试，如下所示：
- en: '[PRE35]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The code is extremely simple and gives us a 2x speedup, scalable on any number
    of engines.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 代码非常简单，给我们带来了2倍的速度提升，并且可以在任何数量的引擎上进行扩展。
- en: Task-based interface
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于任务的接口
- en: IPython has an interface that can handle computing tasks in a smart way. While
    this implies a less flexible interface from the user point of view, it can improve
    performance by balancing the load on the engines and by re-submitting failed jobs.
    In this section, we will introduce the `map` and `apply` functions in the task-based
    interface.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: IPython 有一个可以智能处理计算任务的接口。虽然这从用户的角度来看意味着一个不太灵活的接口，但它可以通过平衡引擎的负载和重新提交失败的作业来提高性能。在本节中，我们将介绍基于任务的接口中的
    `map` 和 `apply` 函数。
- en: 'The task interface is provided by the `LoadBalancedView` class, which can be
    obtained from a client using the `load_balanced_view` method, as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 任务接口由 `LoadBalancedView` 类提供，可以通过客户端使用 `load_balanced_view` 方法获得，如下所示：
- en: '[PRE36]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: At this point we can run some tasks using `map` and `apply`. The `LoadBalancedView`
    class works similarly to `multiprocessing.Pool`, the tasks are submitted and handled
    by a scheduler; in the case of `LoadBalancedView`, the task assignment is based
    on how much load is present on an engine at a given time, ensuring that all the
    engines are working without downtimes.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以使用 `map` 和 `apply` 运行一些任务。`LoadBalancedView` 类与 `multiprocessing.Pool`
    类似，任务由调度器提交和处理；在 `LoadBalancedView` 的情况下，任务分配基于特定时间引擎上的负载量，确保所有引擎都在工作，没有停机时间。
- en: 'It''s helpful to explain an important difference between `apply` in `DirectView`
    and `LoadBalancedView`. A call to `DirectView.apply` will run on *every* selected
    engine, while a call to `LoadBalancedView.apply` will schedule a *single* task
    to one of the engines. In the first case, the result will be a list, and in the
    latter, it will be a single value, as shown in the following code snippet:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 解释 `DirectView` 中的 `apply` 和 `LoadBalancedView` 之间的一个重要区别是有帮助的。对 `DirectView.apply`
    的调用将在 *每个* 选定的引擎上运行，而对 `LoadBalancedView.apply` 的调用将调度一个 *单个* 任务到某个引擎。在前一种情况下，结果将是一个列表，在后一种情况下，它将是一个单个值，如下面的代码片段所示：
- en: '[PRE37]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '`LoadBalancedView` is also able to handle failures and run tasks on engines
    when certain conditions are met. This feature is provided through a dependency
    system. We will not cover this aspect in this book, but interested readers can
    refer to the official documentation at the following link:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`LoadBalancedView` 也能够处理失败，并在满足某些条件时在引擎上运行任务。这个功能是通过一个依赖系统提供的。我们不会在本书中涵盖这个方面，但感兴趣的读者可以参考以下链接中的官方文档：'
- en: '[http://ipython.org/ipython-doc/rel-1.1.0/parallel/parallel_task.html](http://ipython.org/ipython-doc/rel-1.1.0/parallel/parallel_task.html)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://ipython.org/ipython-doc/rel-1.1.0/parallel/parallel_task.html](http://ipython.org/ipython-doc/rel-1.1.0/parallel/parallel_task.html)'
- en: Parallel Cython with OpenMP
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于OpenMP的并行Cython
- en: Cython provides a convenient interface to perform shared-memory parallel processing
    through **OpenMP**. This lets you write extremely efficient parallel code directly
    in Cython without having to create a C wrapper.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Cython提供了一个方便的接口，通过**OpenMP**执行共享内存并行处理。这使得你可以在Cython中直接编写非常高效的并行代码，而无需创建C包装器。
- en: OpenMP is a specification to write multithreaded programs, and includes series
    of C preprocessor directives to manage threads; these include communication patterns,
    load balancing, and synchronization features. Several C/C++ and Fortran compilers
    (including GCC) implement the OpenMP API.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: OpenMP是一个用于编写多线程程序的规范，包括一系列C预处理器指令来管理线程；这些包括通信模式、负载均衡和同步功能。几个C/C++和Fortran编译器（包括GCC）实现了OpenMP
    API。
- en: 'Let''s introduce Cython parallel features with a small example. Cython provides
    a simple API based on OpenMP in the `cython.parallel` module. The simplest construct
    is `prange`: a construct that automatically distributes loop operations in multiple
    threads.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个小的例子来介绍Cython的并行功能。Cython在`cython.parallel`模块中提供了一个基于OpenMP的简单API。最简单的结构是`prange`：一个自动在多个线程中分配循环操作的构造。
- en: First of all, we can write a serial version of a program that computes the square
    of each element of a NumPy array in the `hello_parallel.pyx` file. We get a buffer
    as input and we create an output array by populating it with the squares of the
    input array elements.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以在`hello_parallel.pyx`文件中编写一个计算NumPy数组每个元素平方的程序。我们得到一个缓冲区作为输入，并通过填充输入数组元素的平方来创建一个输出数组。
- en: 'The serial version, `square_serial`, is shown in the following code snippet:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段显示了串行版本`square_serial`：
- en: '[PRE38]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Now, we can change the loop in a parallel version by substituting the range
    call with `prange`. There's a caveat, you need to make sure that the body of the
    loop is interpreter-free. As already explained, to make use of threads we need
    to release the GIL, since interpreter calls acquire and release the GIL, we should
    avoid them. Failure in doing so will result in compilation errors.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过将范围调用替换为`prange`来更改并行版本中的循环。有一个注意事项，你需要确保循环体中没有解释器。正如已经解释的，为了使用线程，我们需要释放GIL，因为解释器调用会获取和释放GIL，所以我们应避免它们。这样做失败会导致编译错误。
- en: 'In Cython, you can release the GIL by using `nogil`, as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在Cython中，你可以使用`nogil`来释放GIL，如下所示：
- en: '[PRE39]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Alternatively, you can use the convenient option `nogil=True` of `prange` that
    will automatically wrap the loop in a `nogil` block:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以使用`prange`的方便选项`nogil=True`，这将自动将循环包装在`nogil`块中：
- en: '[PRE40]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Attempts to call Python code in a `prange` block results in an error. This
    includes assignment operations, function calls, objects initialization, and so
    on. To include such operations in a `prange` block (you may want to do so for
    debugging purposes) you have to re-enable the GIL using the `with gil` statement:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在`prange`块中尝试调用Python代码会导致错误。这包括赋值操作、函数调用、对象初始化等。要在`prange`块中包含此类操作（你可能想这样做以进行调试），你必须使用`with
    gil`语句重新启用GIL：
- en: '[PRE41]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'At this point, we need to recompile our extension. We need to change `setup.py`
    to enable OpenMP support. You have to specify the GCC option `-fopenmp` using
    the `Extension` class in `distutils` and pass it to the `cythonize` function.
    The following code shows the complete `setup.py` file:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们需要重新编译我们的扩展。我们需要更改`setup.py`以启用OpenMP支持。你必须使用`distutils`中的`Extension`类指定GCC选项`-fopenmp`，并将其传递给`cythonize`函数。以下代码显示了完整的`setup.py`文件：
- en: '[PRE42]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Now that we know how to use `prange`, we can quickly parallelize the Cython
    version of our `ParticleSimulator`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何使用`prange`，我们可以快速并行化我们的`ParticleSimulator`的Cython版本。
- en: 'In the following code, we can take a look at the `c_evolve` function contained
    in the Cython module `cevolve.pyx` that we wrote in [Chapter 2](ch02.html "Chapter 2. Fast
    Array Operations with NumPy"), *Fast Array Operations with NumPy*:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们可以查看Cython模块`cevolve.pyx`中包含的`c_evolve`函数，这是我们[第2章](ch02.html "第2章。使用NumPy进行快速数组操作")“使用NumPy进行快速数组操作”中编写的：
- en: '[PRE43]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The first thing we have to do is invert the order of the loops; we want the
    outermost loop to be the parallel one, where each iteration is independent from
    the other. Since the particles don''t interact with each other, we can change
    the order of iteration safely, as shown in the following code snippet:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先要做的是反转循环的顺序；我们希望最外层的循环是并行循环，其中每个迭代都是独立的。由于粒子之间没有相互作用，我们可以安全地改变迭代的顺序，如下面的代码片段所示：
- en: '[PRE44]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'At that point we can parallelize the loop using `prange`, we already removed
    the interpreter-related calls when we added static typing, so the `nogil` block
    can be applied safely, as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以使用 `prange` 并行化循环，因为我们添加静态类型时已经移除了与解释器相关的调用，所以可以安全地应用 `nogil` 块，如下所示：
- en: '[PRE45]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We can now wrap the two different versions into separate functions and we can
    time them, as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将两个不同的版本封装成单独的函数，并对其进行计时，如下所示：
- en: '[PRE46]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: With OpenMP, we are able to obtain a significant speedup compared to the serial
    Cython version by changing a single line of code.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 OpenMP，我们能够通过更改一行代码与串行 Cython 版本相比获得显著的加速。
- en: Summary
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Parallel processing is an effective way to increase the speed of your programs
    or to handle large amounts of data. Embarassingly parallel problems are excellent
    candidates for parallelization and lead to a straightforward implementation and
    optimal scaling.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 并行处理是提高程序速度或处理大量数据的有效方法。令人尴尬的并行问题是非常好的并行化候选者，并且导致实现简单和最佳扩展。
- en: In this chapter, we illustrated the basics of parallel programming in Python.
    We learned how to use multiprocessing to easily parallelize programs with the
    tools already included in Python. Another more powerful tool for parallel processing
    is IPython parallel. This package allows you to interactively prototype parallel
    programs and manage a network of computing nodes effectively. Finally, we explored
    the easy-to-use multithreading capabilities of Cython and OpenMP.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了 Python 并行编程的基础。我们学习了如何使用 Python 中已包含的工具轻松并行化程序。另一个更强大的并行处理工具是 IPython
    parallel。这个包允许你交互式地原型设计并行程序并有效地管理计算节点网络。最后，我们探讨了 Cython 和 OpenMP 的易于使用的多线程功能。
- en: During the course of this book, we learned the most effective techniques to
    design, benchmark, profile, and optimize Python applications. NumPy can be used
    to elegantly rewrite Python loops, and if it is not enough, you can use Cython
    to generate efficient C code. At the last stage, you can easily parallelize your
    program using the tools presented in this chapter.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的整个过程中，我们学习了设计、基准测试、分析和优化 Python 应用程序的最有效技术。NumPy 可以优雅地重写 Python 循环，如果还不够，你可以使用
    Cython 生成高效的 C 代码。在最后阶段，你可以使用本章中介绍的工具轻松地并行化你的程序。
