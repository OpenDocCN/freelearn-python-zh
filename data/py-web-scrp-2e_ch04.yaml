- en: Concurrent Downloading
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并发下载
- en: In the previous chapters, our crawlers downloaded web pages sequentially, waiting
    for each download to complete before starting the next one. Sequential downloading
    is fine for the relatively small example website but quickly becomes impractical
    for larger crawls. To crawl a large website of one million web pages at an average
    of one web page per second would take over 11 days of continuous downloading.
    This time can be significantly improved by downloading multiple web pages simultaneously.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们的爬虫是顺序下载网页，每次下载完成后才开始下一个下载。对于相对较小的示例网站，顺序下载是可行的，但对于较大的爬取来说很快就会变得不切实际。以平均每秒下载一个网页的速度来爬取一百万个网页的网站，需要超过11天的连续下载时间。通过同时下载多个网页，可以显著提高这个时间。
- en: This chapter will cover downloading web pages with multiple threads and processes
    and comparing the performance with sequential downloading.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍使用多线程和多进程下载网页，并比较其与顺序下载的性能。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下主题：
- en: One million web pages
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一百万个网页
- en: Sequential crawler
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序爬虫
- en: Threaded crawler
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程爬虫
- en: Multiprocessing crawler
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多进程爬虫
- en: One million web pages
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一百万个网页
- en: To test the performance of concurrent downloading, it would be preferable to
    have a larger target website. For this reason, we will use the Alexa list, which
    tracks the top one million most popular websites according to users who have installed
    the Alexa Toolbar. Only a small percentage of people use this browser plugin,
    so the data is not authoritative, but it's fine for our purposes and gives us
    a larger list to crawl.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试并发下载的性能，拥有一个更大的目标网站会更理想。因此，我们将使用Alexa排名列表，该列表根据安装了Alexa工具栏的用户追踪了最受欢迎的前一百万个网站。只有一小部分人使用这个浏览器插件，所以数据并非权威，但对于我们的目的来说足够好，并且为我们提供了更大的列表进行爬取。
- en: These top one million web pages can be browsed on the Alexa website at [http://www.alexa.com/topsites](http://www.alexa.com/topsites).
    Additionally, a compressed spreadsheet of this list is available at [http://s3.amazonaws.com/alexa-static/top-1m.csv.zip](http://s3.amazonaws.com/alexa-static/top-1m.csv.zip),
    so scraping Alexa is not necessary.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这些前一百万个网页可以在Alexa网站上浏览，网址为[http://www.alexa.com/topsites](http://www.alexa.com/topsites)。此外，该列表的压缩电子表格可在[http://s3.amazonaws.com/alexa-static/top-1m.csv.zip](http://s3.amazonaws.com/alexa-static/top-1m.csv.zip)找到，因此不需要从Alexa抓取数据。
- en: Parsing the Alexa list
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解析Alexa列表
- en: 'The Alexa list is provided in a spreadsheet with columns for the rank and domain:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Alexa排名列表以电子表格的形式提供，包含排名和域名的列：
- en: '![](img/4364OS_04_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4364OS_04_01.png)'
- en: 'Extracting this data requires a number of steps, as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 提取这些数据需要以下步骤：
- en: Download the `.zip` file.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载`.zip`文件。
- en: Extract the CSV file from the `.zip` file.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`.zip`文件中提取CSV文件。
- en: Parse the CSV file.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析CSV文件。
- en: Iterate each row of the CSV file to extract the domain.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历CSV文件的每一行以提取域名。
- en: 'Here is an implementation to achieve this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个实现这一点的示例：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You may have noticed that the downloaded zipped data is wrapped with the `BytesIO` class
    and passed to `ZipFile`. This is necessary because `ZipFile` expects a file-like
    interface rather than a raw byte object. We also utilize `stream=True`, which
    helps speed up the request. Next, the CSV filename is extracted from the filename
    list. The `.zip` file only contains a single file, so the first filename is selected.
    Then, the CSV file is read using a `TextIOWrapper` to help handle encoding and
    read issues. This file is then iterated, and the domain in the second column is
    added to the URL list. The `http://` protocol is prepended to each domain to make
    them valid URLs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到下载的压缩数据被`BytesIO`类包装，并传递给`ZipFile`。这是必要的，因为`ZipFile`期望一个文件接口而不是原始字节对象。我们还利用`stream=True`来加速请求。接下来，从文件名列表中提取CSV文件名。`.zip`文件只包含一个文件，因此选择第一个文件名。然后，使用`TextIOWrapper`读取CSV文件，以帮助处理编码和读取问题。然后迭代该文件，并将第二列中的域名添加到URL列表中。在每个域名前添加`http://`协议，使它们成为有效的URL。
- en: 'To reuse this function with the crawlers developed earlier, it needs to be
    modified to an easily callable class:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够重用之前开发的爬虫函数，需要将其修改为一个易于调用的类：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A new input argument was added here, called `max_urls`, which sets the number
    of URLs to extract from the Alexa file. By default, this is set to 500 URLs because
    downloading a million web pages takes a long time (as mentioned in the chapter
    introduction, more than 11 days when downloaded sequentially).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里添加了一个新的输入参数，称为`max_urls`，它设置从Alexa文件中提取的URL数量。默认情况下，这设置为500个URL，因为下载一百万个网页需要很长时间（如章节介绍中所述，顺序下载需要超过11天）。
- en: Sequential crawler
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 顺序爬虫
- en: 'We can now use `AlexaCallback` with a slightly modified version of the link
    crawler we developed earlier to download the top 500 Alexa URLs sequentially.
    To update the link crawler, it will now take either a start URL or a list of start
    URLs:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用稍作修改的链接爬虫，与之前开发的版本一起下载前500个Alexa URL，以顺序方式下载。要更新链接爬虫，它现在可以接受一个起始URL或一组起始URL：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We also need to update the way the `robots.txt` is handled for each site. We
    use a simple dictionary to store the parsers per domain (see: [https://github.com/kjam/wswp/blob/master/code/chp4/advanced_link_crawler.py#L53-L72](https://github.com/kjam/wswp/blob/master/code/chp4/advanced_link_crawler.py#L53-L72)).
    We also need to handle the fact that not every URL we encounter will be relative,
    and some of them aren''t even URLs we can visit, such as e-mail addresses with `mailto:`
    or `javascript:` event commands. Additionally, due to some sites not having the `robots.txt`
    files and other poorly formed URLs, there are some additional error-handling sections
    added and a new `no_robots` variable, which allows us to continue crawling if
    we cannot, in good faith, find a `robots.txt` file. Finally, we added a `socket.setdefaulttimeout(60)`
    to handle timeouts for the `robotparser` and some additional `timeout` arguments
    for the `Downloader` class in [Chapter 3](py-web-scrp-2e_ch03.html), *Caching
    Downloads*,.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要更新处理每个网站的`robots.txt`的方式。我们使用一个简单的字典来存储每个域的解析器（见：[https://github.com/kjam/wswp/blob/master/code/chp4/advanced_link_crawler.py#L53-L72](https://github.com/kjam/wswp/blob/master/code/chp4/advanced_link_crawler.py#L53-L72)）。我们还需要处理我们遇到的不一定是相对URL的事实，其中一些甚至不是我们可以访问的URL，例如带有`mailto:`或`javascript:`事件命令的电子邮件地址。此外，由于一些网站没有`robots.txt`文件和其他格式不佳的URL，我们添加了一些额外的错误处理部分和一个新的`no_robots`变量，允许我们在无法在良好信念下找到`robots.txt`文件时继续爬取。最后，我们添加了`socket.setdefaulttimeout(60)`来处理`robotparser`的超时，以及[第3章](py-web-scrp-2e_ch03.html)中`Caching
    Downloads`的`Downloader`类的额外`timeout`参数。
- en: 'The primary code to handle these cases is available at [https://github.com/kjam/wswp/blob/master/code/chp4/advanced_link_crawler.py](https://github.com/kjam/wswp/blob/master/code/chp4/advanced_link_crawler.py).
    The new crawler can then be used directly with the `AlexaCallback` and run from
    the command line as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这些情况的主要代码可在[https://github.com/kjam/wswp/blob/master/code/chp4/advanced_link_crawler.py](https://github.com/kjam/wswp/blob/master/code/chp4/advanced_link_crawler.py)找到。新的爬虫可以直接与`AlexaCallback`一起使用，并在命令行中按以下方式运行：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Taking a look at the code that runs in the `__main__` section of the file,
    we use `''$^''` as our pattern to avoid collecting links from each page. You can
    also try to crawl all links on every page using `''.''` to match everything. (Warning:
    This will take a long time, potentially days!)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 查看文件`__main__`部分运行的代码，我们使用`'$^'`作为我们的模式以避免从每个页面收集链接。你也可以尝试使用`'.'`来匹配所有内容，以爬取每页上的所有链接。（警告：这可能会花费很长时间，可能需要几天！）
- en: The time for only crawling the first page is as expected for sequential downloading,
    with an average of ~2.7 seconds per URL (this includes the time to test the `robots.txt`
    file). Depending on your ISP speeds, and if you run the script on a server in
    the cloud, you might see much faster results.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 仅爬取第一页的时间与顺序下载的预期时间相同，平均每个URL大约需要2.7秒（这包括测试`robots.txt`文件的时间）。根据你的ISP速度，以及如果你在云服务器上运行脚本，你可能会看到更快的结果。
- en: Threaded crawler
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线程爬虫
- en: Now we will extend the sequential crawler to download the web pages in parallel.
    Note that, if misused, a threaded crawler could request content too quickly and
    overload a web server or cause your IP address to be blocked.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将扩展顺序爬虫以并行下载网页。请注意，如果不当使用，线程爬虫可能会请求内容过快，从而超载一个Web服务器或导致你的IP地址被封锁。
- en: To avoid this, our crawlers will have a `delay` flag to set the minimum number
    of seconds between requests to the same domain.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，我们的爬虫将有一个`delay`标志来设置对同一域名请求之间的最小秒数。
- en: The Alexa list example used in this chapter covers one million separate domains,
    so this particular problem does not apply here. However, a delay of at least one
    second between downloads should be considered when crawling many web pages from
    a single domain in the future.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的Alexa列表示例涵盖了100万个独立域名，因此这个特定问题在这里不适用。然而，在将来从单个域名爬取许多网页时，应考虑在下载之间至少延迟一秒钟。
- en: How threads and processes work
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线程和进程的工作原理
- en: 'Here is a diagram of a process containing multiple threads of execution:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是包含多个执行线程的进程图：
- en: '![](img/4364OS_04_02.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4364OS_04_02.png)'
- en: When a Python script or any other computer program is run, a process is created,
    containing the code and state, as well as the stack. These processes are executed
    by the CPU cores of a computer. However, each core can only execute a single thread at
    a time and will quickly switch between them to give the impression that multiple
    programs are running simultaneously. Similarly, within a process, the program
    execution can switch between multiple threads, with each thread executing different
    parts of the program.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行Python脚本或任何其他计算机程序时，会创建一个进程，包含代码和状态，以及堆栈。这些进程由计算机的CPU核心执行。然而，每个核心一次只能执行一个线程，并且会快速在这些线程之间切换，以给人一种同时运行多个程序的感觉。同样，在进程内部，程序执行可以在多个线程之间切换，每个线程执行程序的不同部分。
- en: This means that when one thread is waiting for a web page to download, the process
    can switch and execute another thread to avoid wasting CPU cycles. So, using all
    the compute resources on our computer to download data as fast as possible requires
    distributing our downloads across multiple threads and processes.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着当一个线程正在等待网页下载时，进程可以切换并执行另一个线程，以避免浪费CPU周期。因此，为了尽可能快地使用我们计算机上的所有计算资源来下载数据，需要将我们的下载分配到多个线程和进程中。
- en: Implementing a multithreaded crawler
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现多线程爬虫
- en: 'Fortunately, Python makes threading relatively straightforward. This means
    we can keep a similar queuing structure to the link crawler developed in [Chapter
    1](py-web-scrp-2e_ch01.html), *Introduction to Web Scraping*, but start the crawl
    loop in multiple threads to download these links in parallel. Here is a modified
    version of the start of the link crawler with the `crawl` loop moved into a function:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Python使得线程相对简单。这意味着我们可以保持与第1章中开发的链接爬虫类似的排队结构，但将爬取循环移入一个函数中。以下是修改后的链接爬虫的开始部分，将`crawl`循环移动到函数中：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here is the remainder of the `threaded_crawler` function to start `process_queue`
    in multiple threads and wait until they have completed:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是`threaded_crawler`函数的剩余部分，用于在多个线程中启动`process_queue`并等待它们完成：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The loop in the preceding code will keep creating threads while there are URLs
    to crawl until it reaches the maximum number of threads set. During the crawl,
    threads may also prematurely shut down when there are currently no more URLs in
    the queue. For example, consider a situation when there are two threads and two
    URLs to download. When the first thread finishes its download, the crawl queue
    is empty so this thread exits. However, the second thread may then complete its
    download and discover additional URLs to download. The `thread` loop will then
    notice that there are still more URLs to download, and the maximum number of threads
    has not been reached, so it will create a new download thread.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码中的循环将在有要爬取的URL时持续创建线程，直到达到设置的线程最大数量。在爬取过程中，如果队列中没有更多的URL，线程可能会提前关闭。例如，考虑有两个线程和两个要下载的URL的情况。当第一个线程完成下载后，爬取队列为空，因此该线程退出。然而，第二个线程可能完成下载并发现更多要下载的URL。此时，`thread`循环将注意到仍有更多URL要下载，且线程数量尚未达到最大值，因此它将创建一个新的下载线程。
- en: 'We might also want to add parsing to this threaded crawler later. To do so,
    we can add a section for a function callback using the returned HTML. We likely
    want to return even more links from this logic or extraction, so we need to also
    expand the links we parse in the later `for` loop:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可能希望稍后为此线程化爬虫添加解析功能。为此，我们可以添加一个用于函数回调的HTML返回值的部分。我们可能希望从这个逻辑或提取中返回更多链接，因此我们还需要在后面的`for`循环中扩展我们解析的链接：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The fully updated code can be viewed at [https://github.com/kjam/wswp/blob/master/code/chp4/threaded_crawler.py.](https://github.com/kjam/wswp/blob/master/code/chp4/threaded_crawler.py)
    To have a fair test, you will also need to flush your `RedisCache` or use a different
    default database. If you have the `redis-cli` installed, you can do so easily
    from your command line:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 完全更新的代码可以在[https://github.com/kjam/wswp/blob/master/code/chp4/threaded_crawler.py.](https://github.com/kjam/wswp/blob/master/code/chp4/threaded_crawler.py)查看。为了进行公平测试，您还需要刷新您的`RedisCache`或使用不同的默认数据库。如果您已安装`redis-cli`，您可以从命令行轻松完成此操作：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To exit, use your normal program exit (usually *Ctrl* + *C* or c*md* + *C*).
    Now, let''s test the performance of this multi-threaded version of the link crawler
    with the following command:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要退出，请使用正常的程序退出（通常是*Ctrl* + *C*或*cmd* + *C*）。现在，让我们使用以下命令测试这个多线程版本的链接爬虫的性能：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If you take a look at the `__main__` section of this crawler, you will note
    that you can easily pass arguments to this script including `max_threads` and `url_pattern`.
    In the previous example, we are using the defaults of `max_threads=5` and `url_pattern='$^'`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看这个爬虫的`__main__`部分，您会注意到您可以轻松地将参数传递给此脚本，包括`max_threads`和`url_pattern`。在先前的示例中，我们使用默认值`max_threads=5`和`url_pattern='$^'`。
- en: Since there are five threads, downloading is nearly four times faster! Again,
    your results might vary depending on your ISP or if you run the script from a
    server. Further analysis of thread performance will be covered in the *Performance*
    section.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有五个线程，下载速度几乎快了四倍！再次提醒，您的结果可能会根据您的ISP或是否从服务器运行脚本而有所不同。关于线程性能的进一步分析将在*性能*部分进行讨论。
- en: Multiprocessing crawler
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多进程爬虫
- en: To improve the performance further, the threaded example can be extended to
    support multiple processes. Currently, the crawl queue is held in local memory,
    which means other processes cannot contribute to the same crawl. To address this,
    the crawl queue will be transferred to Redis. Storing the queue independently
    means that even crawlers on separate servers could collaborate on the same crawl.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提高性能，可以将多线程示例扩展以支持多个进程。目前，爬取队列存储在本地内存中，这意味着其他进程不能为相同的爬取做出贡献。为了解决这个问题，爬取队列将被转移到Redis。独立存储队列意味着即使在不同服务器上的爬虫也可以合作进行相同的爬取。
- en: 'For more robust queuing, a dedicated distributed task tool, such as Celery,
    should be considered; however, Redis will be reused here to minimize the number
    of technologies and dependencies introduced. Here is an implementation of the
    new Redis-backed queue:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更健壮的队列，应考虑使用专门的分布式任务工具，如Celery；然而，在这里将重用Redis以最小化引入的技术和依赖项。以下是新Redis支持的队列的实现：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We can see in the preceding `RedisQueue` class that we are maintaining a few
    different data types. First, we have the expected Redis list type, which is handled
    via the `lpush` and `rpop` commands, and the name of the queue is stored in the `self.name`
    attribute.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的`RedisQueue`类中，我们可以看到我们维护了几种不同的数据类型。首先，我们有预期的Redis列表类型，它通过`lpush`和`rpop`命令处理，队列的名称存储在`self.name`属性中。
- en: Next we have a Redis set, which functions similarly to a Python set with a unique
    membership. The set name is stored in `self.seen_set` and is managed via the `sadd`
    and `sismember` methods (to add new keys and test membership).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有一个Redis集合，它的工作方式类似于具有唯一成员资格的Python集合。集合名称存储在`self.seen_set`中，并通过`sadd`和`sismember`方法（用于添加新键和测试成员资格）进行管理。
- en: Finally, we have moved the depth functionality to the `set_depth` and `get_depth` methods,
    which use a normal Redis hash table with the name stored in `self.depth` and each
    URL as the key with the depth as the value. One useful addition to the code would
    be to set the last time a domain was accessed so we can make a more efficient `delay`
    functionality for our `Downloader` class. This is left as an exercise for the
    reader.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将深度功能移动到了`set_depth`和`get_depth`方法中，这些方法使用一个普通的Redis哈希表，名称存储在`self.depth`中，每个URL作为键，深度作为值。代码中的一个有用补充是设置域名最后访问的时间，这样我们可以为我们的`Downloader`类提供一个更高效的`delay`功能。这留作读者的练习。
- en: If you want a queue with more functionality but with the same availability as
    Redis, I recommend looking at `python-rq`  ([http://python-rq.org/](http://python-rq.org/)),
    which is an easy-to-use-and-install Python job queue similar to Celery but with
    less functionality and dependencies.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要一个具有更多功能但与Redis相同可用性的队列，我建议查看`python-rq`（[http://python-rq.org/](http://python-rq.org/)），这是一个易于使用和安装的Python作业队列，类似于Celery，但功能较少且依赖项较少。
- en: 'Continuing with our current `RedisQueue` implementation, we need to make a
    few updates to the threaded crawler to support the new queue type, which are highlighted
    here:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用我们当前的`RedisQueue`实现，我们需要对线程爬虫进行一些更新以支持新的队列类型，这些更新在此处突出显示：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The first change is replacing our Python list with the new Redis-based queue,
    named `Redis``Queue`. This queue handles duplicate URLs internally, so the `seen`
    variable is no longer required. Finally, the `RedisQueue` `len` method is called
    to determine if there are still URLs in the queue. Further logic changes to handle
    the depth and seen functionality are shown here:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个变化是将我们的Python列表替换为新的基于Redis的队列，命名为`RedisQueue`。这个队列内部处理重复的URL，因此不再需要`seen`变量。最后，调用`RedisQueue`的`len`方法来确定队列中是否还有URL。进一步处理深度和已见功能的逻辑更改如下所示：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The full code can be seen at [http://github.com/kjam/wswp/blob/master/code/chp4/threaded_crawler_with_queue.py](http://github.com/kjam/wswp/blob/master/code/chp4/threaded_crawler_with_queue.py).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可以在[http://github.com/kjam/wswp/blob/master/code/chp4/threaded_crawler_with_queue.py](http://github.com/kjam/wswp/blob/master/code/chp4/threaded_crawler_with_queue.py)中查看。
- en: 'This updated version of the threaded crawler can then be started using multiple
    processes with this snippet:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个片段可以启动更新后的线程爬虫，使用多个进程：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This structure might look familiar because the multiprocessing module follows
    a similar interface to the threading module used earlier in the chapter. This
    code either utilizes the number of CPUs available (eight on my machine) or the
     `num_procs` as passed via arguments when starting the script. Then, each process starts
    the threaded crawler and waits for all the processes to complete execution.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构可能看起来很熟悉，因为multiprocessing模块遵循与本章早期使用的threading模块相似的接口。此代码要么利用可用的CPU数量（在我的机器上是八个），要么通过启动脚本时传递的参数`num_procs`。然后，每个进程启动线程爬虫并等待所有进程完成执行。
- en: 'Now, let''s test the performance of this multiprocess version of the link crawler
    using the following command. The code for `mp_threaded``_crawler` is available
    at [http://github.com/kjam/wswp/blob/master/code/chp4/threaded_crawler_with_queue.py](http://github.com/kjam/wswp/blob/master/code/chp4/threaded_crawler_with_queue.py):'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下命令测试这个多进程版本的链接爬虫的性能。`mp_threaded_crawler`的代码可以在[http://github.com/kjam/wswp/blob/master/code/chp4/threaded_crawler_with_queue.py](http://github.com/kjam/wswp/blob/master/code/chp4/threaded_crawler_with_queue.py)中找到。
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As detected by the script, my machine has eight CPUs (four physical cores and
    four virtual cores), and the default setting for threads is five.  To use a different
    combination, you can see the arguments expected by using the `-h` command, as
    follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 根据脚本的检测，我的机器有八个CPU（四个物理核心和四个虚拟核心），线程的默认设置是五个。要使用不同的组合，你可以使用`-h`命令查看预期的参数，如下所示：
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `-h` command is also available for testing different values in the `threaded_crawler.py`
    script.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`-h`命令也适用于测试`threaded_crawler.py`脚本中的不同值。'
- en: For the default options with eight processes and five threads per process, the
    running time is ~1.8X faster than that of the previous threaded crawler using
    a single process. In the next section, we will further investigate the relative
    performances of these three approaches.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于默认选项，即每个进程五个线程和八个进程，运行时间比之前使用单个进程的线程爬虫快约1.8倍。在下一节中，我们将进一步研究这三种方法的相对性能。
- en: Performance
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能
- en: 'To further understand how increasing the number of threads and processes affects
    the time required when downloading, here is a table of results for crawling 500
    web pages:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步了解增加线程和进程数量如何影响下载所需的时间，以下是爬取500个网页的结果表：
- en: '| **Script** | **Number of threads** | **Number of processes** | **Time** |
    **Comparison with sequential** | **Errors Seen?** |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| **脚本** | **线程数** | **进程数** | **时间** | **与顺序比较** | **是否出现错误** |'
- en: '| Sequential | 1 | 1 | 1349.798s | 1 | N |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 顺序 | 1 | 1 | 1349.798秒 | 1 | 否 |'
- en: '| Threaded | 5 | 1 | 361.504s | 3.73 | N |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 线程 | 5 | 1 | 361.504秒 | 3.73 | 否 |'
- en: '| Threaded | 10 | 1 | 275.492s | 4.9 | N |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 线程 | 10 | 1 | 275.492秒 | 4.9 | 否 |'
- en: '| Threaded | 20 | 1 | 298.168s | 4.53 | Y |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 线程 | 20 | 1 | 298.168秒 | 4.53 | 是 |'
- en: '| Processes | 2 | 2 | 726.899s | 1.86 | N |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 进程 | 2 | 2 | 726.899秒 | 1.86 | 否 |'
- en: '| Processes | 2 | 4 | 559.93s | 2.41 | N |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 进程 | 2 | 4 | 559.93秒 | 2.41 | 否 |'
- en: '| Processes | 2 | 8 | 451.772s | 2.99 | Y |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 进程 | 2 | 8 | 451.772秒 | 2.99 | 是 |'
- en: '| Processes | 5 | 2 | 383.438s | 3.52 | N |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 进程 | 5 | 2 | 383.438秒 | 3.52 | 否 |'
- en: '| Processes | 5 | 4 | 156.389s | 8.63 | Y |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 进程 | 5 | 4 | 156.389秒 | 8.63 | 是 |'
- en: '| Processes | 5 | 8 | 296.610s | 4.55 | Y |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 进程 | 5 | 8 | 296.610秒 | 4.55 | 是 |'
- en: The fifth column shows the proportion of time in comparison to the base case
    of sequential downloading. We can see that the increase in performance is not
    linearly proportional to the number of threads and processes but appears logarithmic,
    that is, until adding more threads actually decreases performance. For example,
    one process and five threads lead to 4X better performance, but 10 threads only
    leads to 5X better performance, and using 20 threads actually decreases performance.
    Depending on your system, these performance gains and losses may vary; however,
    it's well known that each extra thread helps expedite execution but is less effective
    than the previously added thread (that is, it is not a linear speedup). This is
    to be expected, considering the process has to switch between more threads and
    can devote less time to each.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 第五列显示了与顺序下载的基本情况相比的时间比例。我们可以看到，性能的提升并不是与线程和进程的数量线性成比例，而是呈现出对数增长，也就是说，直到添加更多线程实际上会降低性能。例如，一个进程和五个线程可以带来4倍的性能提升，但十个线程只能带来5倍的性能提升，而使用20个线程实际上会降低性能。根据您的系统，这些性能的提升和损失可能会有所不同；然而，众所周知，每个额外的线程都有助于加快执行速度，但效果不如之前添加的线程（也就是说，它不是线性加速）。考虑到进程需要在更多线程之间切换，并且可以分配给每个线程的时间更少，这是可以预料的。
- en: Additionally, the amount of bandwidth available for downloading is limited,
    so eventually adding additional threads will not lead to a faster download speed.
    If you run these yourself, you may notice errors, such as `urlopen error [Errno
    101] Network is unreachable`, sprinkled throughout your testing, particularly
    when using high numbers of threads or processes. This is obviously suboptimal
    and leads to more frequent downloading errors than you would experience when choosing
    a lower number of threads. Of course, network constraints will be different if
    you are running this on a more distributed setup or in a cloud server environment.
    The final column in the preceding table tracks the errors experienced in these
    trials from my single laptop using a normal ISP cable connection.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，可用的下载带宽是有限的，因此最终添加额外的线程不会导致下载速度更快。如果您亲自运行这些操作，您可能会在测试过程中注意到一些错误，例如`urlopen
    error [Errno 101] Network is unreachable`，尤其是在使用大量线程或进程时。这显然是不理想的，并且会导致比选择较少线程时更频繁的下载错误。当然，如果您在更分布式设置或云服务器环境中运行此操作，网络限制可能会有所不同。前表中最后一列跟踪了我使用普通ISP电缆连接的单个笔记本电脑在这些试验中遇到的错误。
- en: Your results may vary, and this chart was built using a laptop rather than a
    server (which would have better bandwidth and fewer background processes); so,
    I challenge you to build a similar chart for your computer and/or servers. Once
    you discover the bounds of your machine(s), achieving greater performance would
    require distributing the crawl across multiple servers, all pointing to the same
    Redis instance.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 您的结果可能会有所不同，此图表是使用笔记本电脑而不是服务器构建的（服务器会有更好的带宽和更少的后台进程）；因此，我挑战您为您的计算机和/或服务器构建一个类似的图表。一旦您发现了您机器的界限，要实现更高的性能就需要将爬取工作分布到多个服务器上，所有服务器都指向同一个Redis实例。
- en: Python multiprocessing and the GIL
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python多进程和GIL
- en: For a longer performance review of Python's threads and processes, one must
    first understand the **Global Interpreter Lock** (**GIL**). The GIL is a mechanism
    used by the Python interpreter to execute code using only one thread at a time,
    meaning Python code will only execute linearly (even when using multiprocessing
    and multiple cores). This design decision was made so Python could run quickly
    but still be thread-safe.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Python线程和进程的更长时间性能评估，首先必须理解**全局解释器锁**（**GIL**）。GIL是Python解释器用来一次只执行一个线程的机制，这意味着Python代码将只线性执行（即使在使用多进程和多核的情况下）。这个设计决策是为了让Python运行得更快，同时仍然保持线程安全。
- en: If you haven't already seen it, I recommend watching David Beazley's  Understanding
    the GIL talk from
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有看过，我建议您观看David Beazley的《理解GIL》演讲。
- en: PyCon 2010 ([https://www.youtube.com/watch?v=Obt-vMVdM8s](https://www.youtube.com/watch?v=Obt-vMVdM8s)).
    Beazley also has numerous write-ups on his blog and some interesting talks on
    the GILectomy (attempting to remove the GIL from Python for speedier multiprocessing).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: PyCon 2010 ([https://www.youtube.com/watch?v=Obt-vMVdM8s](https://www.youtube.com/watch?v=Obt-vMVdM8s))。比泽利在他的博客上也有许多关于GILectomies（尝试从Python中移除GIL以实现更快的多进程）的撰写，以及一些有趣的关于GILectomies的演讲。
- en: The GIL puts an extra performance burden on high I/O operations, like what we
    are doing with our web scraper. There are also ways to utilize Python's multiprocessing
    library for better shared data across processes and threads.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: GIL（全局解释器锁）给高I/O操作带来了额外的性能负担，就像我们在使用我们的爬虫时所做的那样。还有方法可以利用Python的多进程库来更好地在进程和线程之间共享数据。
- en: We could have written our scraper as a map with a worker pool or queue to compare
    Python's own multiprocessing internals with our Redis-based system. We could also
    use asynchronous programming to better thread performance and higher network utilization.
    Asynchronous libraries, such as async, tornado, or even NodeJS, allow rograms
    to execute in a non-blocking manner, meaning processes can switch to a different
    thread when waiting for responses from the web server. It is likely some of these implementations
    might be faster for our use case.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将我们的爬虫编写为一个带有工作池或队列的映射，以比较Python自己的多进程内部机制与基于Redis的系统。我们还可以使用异步编程来提高线程性能和更高的网络利用率。异步库，如async、tornado或甚至NodeJS，允许程序以非阻塞方式执行，这意味着当等待来自Web服务器的响应时，进程可以切换到不同的线程。这些实现中的一些可能对我们的用例来说可能更快。
- en: Additionally, we can use projects such as PyPy ([https://pypy.org/](https://pypy.org/))
    to help increase threading and multiprocessing speed. That said, measure your
    performance and evaluate your needs before implementing optimizations (don't optimize
    prematurely). It is a good rule to ask just how important speed is over clarity
    and how correct intuition is over actual observation. Remember the Zen of Python
    and proceed accordingly!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以使用像PyPy（[https://pypy.org/](https://pypy.org/））这样的项目来帮助提高线程和进程的速度。话虽如此，在实施优化之前（不要过早优化）衡量你的性能并评估你的需求是一个好习惯。速度与清晰度之间的重要性，以及正确直觉与实际观察之间的重要性，是一个好的规则。记住Python的Zen，并据此行事！
- en: Summary
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter covered why sequential downloading creates performance bottlenecks.
    We looked at how to download large numbers of web pages efficiently across multiple
    threads and processes and compared when optimizations or increasing threads and
    processes might be useful and when they could be harmful. We also implemented
    a new Redis queue which we can use across several machines or processes.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了为什么顺序下载会创建性能瓶颈。我们探讨了如何在多个线程和进程中高效地下载大量网页，并比较了何时优化或增加线程和进程可能是有用的，何时可能是有害的。我们还实现了一个新的Redis队列，我们可以使用它跨多台机器或多个进程。
- en: In the next chapter, we will cover how to scrape content from web pages which load
    their content dynamically using JavaScript.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍如何从使用JavaScript动态加载内容的网页中抓取内容。
