- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Creating Coroutines, Events, and Message-Driven Transactions
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建协程、事件和消息驱动的交易
- en: The FastAPI framework is an asynchronous framework that runs over the asyncio
    platform, which utilizes the ASGI protocol. It is well known for its 100% support
    for asynchronous endpoints and non-blocking tasks. This chapter will focus on
    how we create highly scalable applications with asynchronous tasks and event-driven
    and message-driven transactions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: FastAPI 框架是一个基于 asyncio 平台的异步框架，它利用了 ASGI 协议。它因其对异步端点和非阻塞任务100%的支持而闻名。本章将重点介绍我们如何通过异步任务和事件驱动以及消息驱动的交易来创建高度可扩展的应用程序。
- en: We learned in [*Chapter 2*](B17975_02.xhtml#_idTextAnchor033)*, Exploring the
    Core Features*, that *Async/Await* or asynchronous programming is a design pattern
    that enables other services or transactions to run outside the main thread. The
    framework uses the `async` keyword to create asynchronous processes that will
    run on top of other thread pools and will be *awaited*, instead of invoking them
    directly. The number of external threads is defined during the Uvicorn server
    startup through the `--worker` option.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [*第 2 章*](B17975_02.xhtml#_idTextAnchor033) *探索核心功能* 中了解到，*Async/Await*
    或异步编程是一种设计模式，它允许其他服务或交易在主线程之外运行。该框架使用 `async` 关键字创建将在其他线程池之上运行的异步进程，并将被 *await*，而不是直接调用它们。外部线程的数量在
    Uvicorn 服务器启动期间通过 `--worker` 选项定义。
- en: 'In this chapter, we will delve into the framework and scrutinize the various
    components of the FastAPI Framework that can run asynchronously using multiple
    threads. The following highlights will help us understand how asynchronous FastAPI
    is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入研究框架并仔细审查 FastAPI 框架的各个组件，这些组件可以使用多个线程异步运行。以下要点将帮助我们理解异步 FastAPI：
- en: Implementing coroutines
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现协程
- en: Creating asynchronous background tasks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建异步后台任务
- en: Understanding Celery tasks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Celery 任务
- en: Building message-driven transactions using RabbitMQ
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 RabbitMQ 构建消息驱动的交易
- en: Building publish/subscribe messaging using Kafka
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Kafka 构建发布/订阅消息
- en: Applying reactive programming in tasks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在任务中应用响应式编程
- en: Customizing events
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义事件
- en: Implementing asynchronous **Server-Sent Events** (**SSE**)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现异步 **服务器端事件**（**SSE**）
- en: Building an asynchronous WebSocket
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建异步 WebSocket
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter will cover asynchronous features, software specifications, and
    the components of a *newsstand management system* prototype. The discussions will
    use this online newspaper management system prototype as a specimen to understand,
    explore, and implement asynchronous transactions that will manage the *newspaper
    content*, *subscription*, *billing*, *user profiles*, *customers*, and other business-related
    transactions. The code has all been uploaded to [https://github.com/PacktPublishing/Building-Python-Microservices-with-FastAPI](https://github.com/PacktPublishing/Building-Python-Microservices-with-FastAPI)
    under the `ch08` project.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖异步功能、软件规范以及 *新闻亭管理系统* 原型的组件。讨论将使用这个在线报纸管理系统原型作为样本来理解、探索和实现异步交易，这些交易将管理
    *报纸内容*、*订阅*、*计费*、*用户资料*、*客户* 以及其他与业务相关的交易。所有代码都已上传到 [https://github.com/PacktPublishing/Building-Python-Microservices-with-FastAPI](https://github.com/PacktPublishing/Building-Python-Microservices-with-FastAPI)
    下的 `ch08` 项目。
- en: Implementing coroutines
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现协程
- en: In the FastAPI framework, a *thread pool* is always present to execute both
    synchronous API and non-API transactions for every request. For ideal cases where
    both the transactions have minimal performance overhead with *CPU-bound* and *I/O-bound*
    transactions, the overall performance of using the FastAPI framework is still
    better than those frameworks that use non-ASGI-based platforms. However, when
    contention occurs due to high CPU-bound traffic or heavy CPU workloads, the performance
    of FastAPI starts to wane due to *thread switching*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在 FastAPI 框架中，始终存在一个 *线程池* 来执行每个请求的同步 API 和非 API 交易。对于理想情况，即这两种交易都具有最小的性能开销，无论是
    *CPU 密集型* 还是 *I/O 密集型* 交易，使用 FastAPI 框架的整体性能仍然优于那些使用非 ASGI 基础平台的框架。然而，当由于高 CPU
    密集型流量或重 CPU 工作负载导致竞争时，由于 *线程切换*，FastAPI 的性能开始下降。
- en: Thread switching is a context switch from one thread to another within the same
    process. So, if we have several transactions with varying workloads running in
    the background and on the browser, FastAPI will run these transactions in the
    thread pool with several context switches. This scenario will cause contention
    and degradation to lighter workloads. To avoid performance issues, we apply *coroutine
    switching* instead of threads.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 线程切换是在同一进程内从一个线程切换到另一个线程的上下文切换。因此，如果我们有多个具有不同工作负载的事务在后台和浏览器上运行，FastAPI 将在线程池中运行这些事务，并执行多个上下文切换。这种情况将导致对较轻工作负载的竞争和性能下降。为了避免性能问题，我们采用
    *协程切换* 而不是线程。
- en: Applying coroutine switching
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用协程切换
- en: 'The FastAPI framework works at the optimum speed with a mechanism called *coroutine
    switching*. This approach allows transaction-tuned tasks to work cooperatively
    by allowing other running processes to pause so that the thread can execute and
    finish more urgent tasks, and resume "awaited" transactions without preempting
    the thread. These coroutine switches are programmer-defined components and not
    kernel-related or memory-related features. In FastAPI, there are two ways of implementing
    coroutines: (a) applying the `@asyncio.coroutine` decorator, and (b) using the
    `async`/`await` construct.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: FastAPI 框架通过称为 *协程切换* 的机制以最佳速度运行。这种方法允许事务调优的任务通过允许其他运行进程暂停，以便线程可以执行并完成更紧急的任务，并在不抢占线程的情况下恢复
    "awaited" 事务。这些协程切换是程序员定义的组件，与内核或内存相关的功能无关。在 FastAPI 中，有两种实现协程的方法：(a) 应用 `@asyncio.coroutine`
    装饰器，和 (b) 使用 `async`/`await` 构造。
- en: Applying @asyncio.coroutine
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用 @asyncio.coroutine
- en: '`asyncio` is a Python extension that implements the Python concurrency paradigm
    using a single-threaded and single-process model and provides API classes and
    methods for running and managing coroutines. This extension provides an `@asyncio.coroutine`
    decorator that transforms API and native services into *generator-based coroutines*.
    However, this is an old approach and can only be used in FastAPI that uses Python
    3.9 and below. The following is a login service transaction of our *newsstand
    management system* prototype implemented as a coroutine:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`asyncio` 是一个 Python 扩展，它使用单线程和单进程模型实现 Python 并发范式，并提供用于运行和管理协程的 API 类和方法。此扩展提供了一个
    `@asyncio.coroutine` 装饰器，将 API 和原生服务转换为基于生成器的协程。然而，这是一个旧的方法，只能在使用 Python 3.9 及以下版本的
    FastAPI 中使用。以下是我们 *新闻亭管理系统* 原型中实现为协程的登录服务事务：'
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`build_user_list()` is a native service that converts all login records into
    the `str` format. It is decorated with the `@asyncio.coroutine` decorator to transform
    the transaction into an asynchronous task or coroutine. A coroutine can invoke
    another coroutine function or method using only the `yield from` clause. This
    construct pauses the coroutine and passes the control of the thread to the coroutine
    function invoked. By the way, the `asyncio.sleep()` method is one of the most
    widely used asynchronous utilities of the `asyncio` module, which can pause a
    process for a few seconds, but is not the ideal one. On the other hand, the following
    code is an API service implemented as a coroutine that can minimize contention
    and performance degradation in client-side executions:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`build_user_list()` 是一个原生服务，它将所有登录记录转换为 `str` 格式。它使用 `@asyncio.coroutine` 装饰器将事务转换为异步任务或协程。协程可以使用
    `yield from` 子句调用另一个协程函数或方法。顺便说一句，`asyncio.sleep()` 方法是 `asyncio` 模块中最广泛使用的异步实用工具之一，它可以使进程暂停几秒钟，但并不是理想的。另一方面，以下代码是一个作为协程实现的
    API 服务，它可以最小化客户端执行中的竞争和性能下降：'
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `list_login()` API service retrieves all the login details of the application’s
    users through a coroutine CRUD transaction implemented in *GINO ORM*. The API
    service again uses the `yield from` clause to run and execute the `get_all_login()`
    coroutine function.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`list_login()` API 服务通过在 *GINO ORM* 中实现的协程 CRUD 事务检索应用程序用户的全部登录详情。API 服务再次使用
    `yield from` 子句来运行和执行 `get_all_login()` 协程函数。'
- en: 'A coroutine function can invoke and await multiple coroutines concurrently
    using the `asyncio.gather()` utility. This `asyncio` method manages a list of
    coroutines and waits until all its coroutines have completed their tasks. Then,
    it will return a list of results from the corresponding coroutines. The following
    code is an API that retrieves login records through an asynchronous CRUD transaction
    and then invokes `count_login()` and `build_user_list()` concurrently to process
    these records:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 协程函数可以使用 `asyncio.gather()` 工具并发调用和等待多个协程。这个 `asyncio` 方法管理一个协程列表，并等待直到所有协程完成其任务。然后，它将返回对应协程的结果列表。以下是一个通过异步
    CRUD 事务检索登录记录的 API，然后并发调用 `count_login()` 和 `build_user_list()` 来处理这些记录：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`list_login_records()` uses `asyncio.gather()` to run the `count_login()` and
    `build_user_list()` tasks and later extract their corresponding returned values
    for processing.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`list_login_records()` 使用 `asyncio.gather()` 来运行 `count_login()` 和 `build_user_list()`
    任务，并在之后提取它们对应的返回值进行处理。'
- en: Using the async/await construct
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 async/await 构造
- en: 'Another way of implementing a coroutine is using `async`/`await` constructs.
    As with the previous approach, this syntax creates a task that can pause anytime
    during its operation before it reaches the end. But the kind of coroutine that
    this approach produces is called a *native coroutine*, which is not iterable in
    the way that the generator type is. The `async`/`await` syntax also allows the
    creation of other asynchronous components such as the `async with` context managers
    and `async for` iterators. The following code is the `count_login()` task previously
    invoked in the generator-based coroutine service, `list_login_records()`:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 实现协程的另一种方法是使用 `async`/`await` 构造。与之前的方法一样，这种语法创建了一个任务，在执行过程中可以随时暂停，直到到达末尾。但这种方法产生的协程被称为
    *原生协程*，它不能像生成器类型那样迭代。`async`/`await` 语法还允许创建其他异步组件，例如 `async with` 上下文管理器和 `async
    for` 迭代器。以下代码是之前在基于生成器的协程服务 `list_login_records()` 中调用的 `count_login()` 任务：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `count_login()` native service is a native coroutine because of the `async`
    keyword placed before its method definition. It only uses `await` to invoke other
    coroutines. The `await` keyword suspends the execution of the current coroutine
    and passes the control of the thread to the invoked coroutine function. After
    the invoked coroutine finishes its process, the thread control will yield back
    to the caller coroutine. Using the `yield from` construct instead of `await` will
    raise an error because our coroutine here is not generator-based. The following
    is an API service implemented as a native coroutine that manages data entry for
    the new administrator profiles:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`count_login()` 原生服务是一个原生协程，因为它在方法定义之前放置了 `async` 关键字。它只使用 `await` 来调用其他协程。`await`
    关键字暂停当前协程的执行，并将线程控制权传递给被调用的协程函数。在被调用的协程完成其处理后，线程控制权将返回给调用协程。使用 `yield from` 构造而不是
    `await` 会引发错误，因为我们的协程不是基于生成器的。以下是一个作为原生协程实现的 API 服务，用于管理新管理员资料的录入：'
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Both generator-based and native coroutines are monitored and managed by an *event
    loop*, which represents an infinite loop inside a thread. Technically, it is an
    object found in the *thread*, and each *thread* in the thread pool can only have
    one event loop, which contains a list of helper objects called *tasks*. Each task,
    pre-generated or manually created, executes one coroutine. For instance, when
    the previous `add_admin()` API service invokes the `insert_admin()` coroutine
    transaction, the event loop will suspend `add_admin()` and tag its task as an
    *awaited* task. Afterward, the event loop will assign a task to run the `insert_admin()`
    transaction. Once the task has completed its execution, it will yield the control
    back to `add_admin()`. The thread that manages the FastAPI application is not
    interrupted during these shifts of execution since it is the event loop and its
    tasks that participate in the *coroutine switching* mechanism. Let us now use
    these coroutines to build our application
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于生成器和原生的协程都由一个 *事件循环* 监控和管理，它代表线程内部的一个无限循环。技术上，它是在 *线程* 中找到的一个对象，线程池中的每个 *线程*
    只能有一个事件循环，其中包含一个称为 *任务* 的辅助对象列表。每个任务，无论是预先生成的还是手动创建的，都会执行一个协程。例如，当先前的 `add_admin()`
    API 服务调用 `insert_admin()` 协程事务时，事件循环将挂起 `add_admin()` 并将其任务标记为 *等待* 任务。之后，事件循环将分配一个任务来运行
    `insert_admin()` 事务。一旦任务完成执行，它将控制权交还给 `add_admin()`。在执行转换期间，管理 FastAPI 应用程序的线程不会被中断，因为事件循环及其任务参与了
    *协程切换* 机制。现在让我们使用这些协程来构建我们的应用程序
- en: Designing asynchronous transactions
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计异步事务
- en: 'There are a few programming paradigms that we can follow when creating coroutines
    for our application. Utilizing more coroutine switching in the process can help
    improve the software performance. In our *newsstand* application, there is an
    endpoint, `/admin/login/list/enc`, in the `admin.py` router that returns a list
    of encrypted user details. In its API service, shown in the following code, each
    record is managed by an `extract_enc_admin_profile()` transaction call instead
    of passing the whole data record to a single call, thus allowing the concurrent
    executions of tasks. This strategy is better than running the bulk of transactions
    in a thread without *context switches*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在为我们的应用程序创建协程时，我们可以遵循几种编程范式。在过程中利用更多的协程切换可以帮助提高软件性能。在我们的 *newsstand* 应用程序中，`admin.py`
    路由器中有一个端点 `/admin/login/list/enc`，它返回一个加密的用户详情列表。在其API服务中，如下所示代码所示，每条记录都由一个 `extract_enc_admin_profile()`
    事务调用管理，而不是将整个数据记录传递给单个调用，从而允许任务的并发执行。这种策略比在没有 *上下文切换* 的情况下运行大量事务在线程中更好：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, the `extract_enc_admin_profile()` coroutine, shown in the following code,
    implements a chaining design pattern, where it calls the other smaller coroutines
    through a chain. Simplifying and breaking down the monolithic and complex processes
    into smaller but more robust coroutines will improve the application’s performance
    by utilizing more context switches. In this API, `extract_enc_admin_profile()`
    creates three context switches in a chain, better than thread switches:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`extract_enc_admin_profile()` 协程，如下所示代码所示，实现了一个链式设计模式，其中它通过链调用其他较小的协程。将单体和复杂的过程简化并分解成更小但更健壮的协程，通过利用更多的上下文切换来提高应用程序的性能。在这个API中，`extract_enc_admin_profile()`
    在链中创建了三个上下文切换，比线程切换更优：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'On the other hand, the following implementation is the smaller subroutines
    awaited and executed by `extract_enc_admin_profile()`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，以下实现是 `extract_enc_admin_profile()` 等待和执行的较小子程序：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: These three subroutines will give the main coroutine the encrypted `str` that
    contains the details of an administrator profile. All these encrypted strings
    will be collated by the API service using the `asyncio.gather()` utility.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个子程序将给主协程提供包含管理员配置文件详情的加密 `str`。所有这些加密字符串都将由API服务使用 `asyncio.gather()` 工具汇总。
- en: 'Another programming approach to utilizing the coroutine switching is the use
    of pipelines created by `asyncio.Queue`. In this programming design, the queue
    structure is the common point between two tasks: (a) the task that will place
    a value to the queue called the *producer*, and (b) the task that will fetch the
    item from the queue, the *consumer*. We can implement a *one producer/one consumer*
    interaction or a *multiple producers/multiple consumers* setup with this approach.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 利用协程切换的另一种编程方法是使用 `asyncio.Queue` 创建的管道。在这种编程设计中，队列结构是两个任务之间的共同点：（a）将值放入队列的任务称为
    *生产者*，（b）从队列中获取项的任务称为 *消费者*。我们可以使用这种方法实现 *单生产者/单消费者* 交互或 *多生产者/多消费者* 设置。
- en: 'The following code highlights the `process_billing()` native service that builds
    a *producer/consumer* transaction flow. The `extract_billing()` coroutine is the
    producer that retrieves the billing records from the database and passes each
    record one at a time to the queue. `build_billing_sheet()`, on the other hand,
    is the consumer that fetches the record from the queue structure and generates
    the billing sheet:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码突出了构建 *生产者/消费者* 交易流程的本地服务 `process_billing()`。`extract_billing()` 协程是生产者，它从数据库中检索账单记录，并将每个记录逐个传递到队列中。另一方面，`build_billing_sheet()`
    是消费者，它从队列结构中获取记录并生成账单表：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this programming design, the `build_billing()` coroutine will explicitly
    wait for the record queued by `extract_billing()`. This setup is possible due
    to the `asyncio.create_task()` utility, which directly assigns and schedules a
    task to each coroutine.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种编程设计中，`build_billing()` 协程将明确等待由 `extract_billing()` 队列的记录。这种设置是由于 `asyncio.create_task()`
    工具，它直接分配和调度任务给每个协程。
- en: 'The queue is the only method parameter common to the coroutines because it
    is their common point. The `join()`of `asyncio.Queue` ensures that all the items
    passed to the pipeline by `extract_billing()` are fetched and processed by `build_billing_sheet()`.
    It also blocks the external controls that would affect the coroutine interactions.
    The following code shows how to create `asyncio.Queue` and schedule a task for
    execution:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 队列是协程的唯一公共方法参数，因为它它们的共同点。`asyncio.Queue` 的 `join()` 确保所有通过 `extract_billing()`
    传递到管道的项都被 `build_billing_sheet()` 获取和处理。它还阻止影响协程交互的外部控制。以下代码展示了如何创建 `asyncio.Queue`
    并调度任务执行：
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: By the way, always pass `cancel()to` the task right after its coroutine has
    completed the process. On the other hand, we can also apply other ways so that
    the performance of our coroutines can improve.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，总是在协程完成处理后立即传递 `cancel()` 给任务。另一方面，我们也可以采用其他方法，以便提高我们协程的性能。
- en: Using the HTTP/2 protocol
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 HTTP/2 协议
- en: 'Coroutine execution can be faster in applications running on the *HTTP/2* protocol.
    We can replace the *Uvicorn* server with *Hypercorn*, which now supports ASGI-based
    frameworks such as FastAPI. But first, we need to install `hypercorn` using `pip`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行在 *HTTP/2* 协议上的应用程序中，协程执行可以更快。我们可以用 *Hypercorn* 替换 *Uvicorn* 服务器，现在它支持基于
    ASGI 的框架，如 FastAPI。但首先，我们需要使用 `pip` 安装 `hypercorn`：
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'For *HTTP/2* to work, we need to create an SSL certificate. Using OpenSSL,
    our app has two *PEM* files for our *newsstand* prototype: (a) the private encryption
    (`key.pem`) and (b) the certificate information (`cert.pem`.) We place these files
    in the main project folder before executing the following `hypercorn` command
    to run our FastAPI application:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要使 *HTTP/2* 工作，我们需要创建一个 SSL 证书。使用 OpenSSL，我们的应用程序有两个 *PEM* 文件用于我们的 *newsstand*
    原型：（a）私有加密密钥（`key.pem`）和（b）证书信息（`cert.pem`）。我们在执行以下 `hypercorn` 命令以运行我们的 FastAPI
    应用程序之前，将这些文件放置在主项目文件夹中：
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now, let us explore other FastAPI tasks that can also use coroutines.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索其他可以使用协程的 FastAPI 任务。
- en: Creating asynchronous background tasks
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建异步后台任务
- en: In [*Chapter 2*](B17975_02.xhtml#_idTextAnchor033)*, Exploring the Core Features*,
    we first showcased the `BackgroundTasks` injectable API class, but we didn’t mention
    creating asynchronous background tasks. In this discussion, we will be focusing
    on creating asynchronous background tasks using the `asyncio` module and coroutines.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 2 章*](B17975_02.xhtml#_idTextAnchor033)*，探索核心功能*中，我们首先展示了 `BackgroundTasks`
    注入式 API 类，但并未提及创建异步后台任务。在本讨论中，我们将专注于使用 `asyncio` 模块和协程创建异步后台任务。
- en: Using the coroutines
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用协程
- en: 'The framework supports the creation and execution of asynchronous background
    processes using the `async`/`await` structure. The following native service is
    an asynchronous transaction that generates a billing sheet in CSV format in the
    background:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 框架支持使用 `async`/`await` 结构创建和执行异步后台进程。以下原生服务是一个异步事务，在后台以 CSV 格式生成账单表：
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This `generate_billing_sheet()` coroutine service will be executed as a background
    task in the following API service, `save_vendor_billing()`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 `generate_billing_sheet()` 协程服务将在以下 API 服务 `save_vendor_billing()` 中作为后台任务执行：
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now, nothing has changed when it comes to defining background processes. We
    usually inject `BackgroundTasks` into the API service method and apply `add_task()`
    to provide task schedules, assignments, and execution for a specific process.
    But since the approach is now to utilize coroutines, the background task will
    use the event loop instead of waiting for the current thread to finish its jobs.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在定义后台进程方面，并没有什么变化。我们通常将 `BackgroundTasks` 注入 API 服务方法，并使用 `add_task()` 提供任务调度、分配和执行。但由于现在的方法是利用协程，后台任务将使用事件循环而不是等待当前线程完成其工作。
- en: If the background process requires arguments, we pass these arguments to `add_task()`
    right after its *first parameter*. For instance, the arguments for the `billing_date`
    and `query_list` parameters of `generate_billing_sheet()` should be placed after
    the `generate_billing_sheet` injection into `add_task()`. Moreover, the `billing_date`
    value should be passed before the `result` argument because `add_task()` still
    follows the order of parameter declaration in `generate_billing_sheet()` to avoid
    a type mismatch.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果后台进程需要参数，我们将在 `add_task()` 的 *第一个参数* 之后传递这些参数。例如，`generate_billing_sheet()`
    的 `billing_date` 和 `query_list` 参数的参数应该在将 `generate_billing_sheet` 注入 `add_task()`
    之后放置。此外，`billing_date` 的值应该在 `result` 参数之前传递，因为 `add_task()` 仍然遵循 `generate_billing_sheet()`
    中参数声明的顺序，以避免类型不匹配。
- en: All asynchronous background tasks will continuously execute and will not be
    *awaited* even if their coroutine API service has already returned a response
    to the client.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所有异步后台任务将持续执行，即使它们的协程 API 服务已经向客户端返回了响应，也不会被 *await*。
- en: Creating multiple tasks
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建多个任务
- en: '`BackgroundTasks` allows the creation of multiple asynchronous transactions
    that will execute concurrently in the background. In the `save_vendor_billing()`
    service, there is another task created for a new transaction called the `create_total_payables_year()`
    transaction, which requires the same arguments as `generate_billing_sheet()`.
    Again, this newly created task will be utilizing the event loop instead of the
    thread.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`BackgroundTasks` 允许创建多个异步事务，这些事务将在后台并发执行。在 `save_vendor_billing()` 服务中，为一个新的交易创建了一个名为
    `create_total_payables_year()` 的事务，它需要与 `generate_billing_sheet()` 相同的参数。再次强调，这个新创建的任务将使用事件循环而不是线程。'
- en: The application always encounters performance issues when the background processes
    have high-CPU workloads. Also, tasks generated by `BackgroundTasks` are not capable
    of returning values from the transactions. Let us look for another solution where
    tasks can manage high workloads and execute processes with returned values.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当后台进程具有高 CPU 工作负载时，应用程序总是遇到性能问题。此外，由 `BackgroundTasks` 生成的任务无法从事务中返回值。让我们寻找另一种解决方案，其中任务可以管理高工作量并执行带有返回值的进程。
- en: Understanding Celery tasks
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Celery 任务
- en: '*Celery* is a non-blocking task queue that runs on a distributed system. It
    can manage asynchronous background processes that are huge and heavy with CPU
    workloads. It is a third-party tool, so we need to install it first through `pip`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*Celery* 是一个在分布式系统上运行的非阻塞任务队列。它可以管理具有大量和重 CPU 工作负载的异步后台进程。它是一个第三方工具，因此我们需要首先通过
    `pip` 安装它：'
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'It schedules and runs tasks concurrently on a single server or distributed
    environment. But it requires a message transport to send and receive messages,
    such as *Redis*, an in-memory database that can be used as a message broker for
    messages in strings, dictionaries, lists, sets, bitmaps, and stream types. Also,
    we can install Redis on Linux, macOS, and Windows. Now, after the installation,
    run its `redis-server.exe` command to start the server. In Windows, the Redis
    service is set to run by default after installation, which causes a *TCP bind
    listener* error. So, we need to stop it before running the startup command. *Figure
    8.1* shows Windows **Task Manager** with the Redis service giving a **Stopped**
    status:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 它在单个服务器或分布式环境中并发地安排和运行任务。但它需要一个消息传输来发送和接收消息，例如*Redis*，一个可以用于字符串、字典、列表、集合、位图和流类型的消息代理的内存数据库。我们还可以在Linux、macOS和Windows上安装Redis。现在，安装后，运行其`redis-server.exe`命令以启动服务器。在Windows中，Redis服务默认设置为安装后运行，这会导致*TCP绑定监听器*错误。因此，在运行启动命令之前，我们需要停止它。*图8.1*显示了Redis服务在Windows**任务管理器**中处于**停止**状态：
- en: '![Figure 8.1 – Stopping the Redis service'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.1 – 停止Redis服务'
- en: '](img/Figure_8.01_B17975.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_8.01_B17975.jpg]'
- en: Figure 8.1 – Stopping the Redis service
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 停止Redis服务
- en: 'After stopping the service, we should now see Redis running as shown in *Figure
    8.2*:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 停止服务后，我们现在应该看到Redis正在运行，如图*图8.2*所示：
- en: '![Figure 8.2 – A running Redis server'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.2 – 运行中的Redis服务器'
- en: '](img/Figure_8.02_B17975.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_8.02_B17975.jpg]'
- en: Figure 8.2 – A running Redis server
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 运行中的Redis服务器
- en: Creating and configuring the Celery instance
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建和配置Celery实例
- en: 'Before creating Celery tasks, we need a Celery instance placed in a dedicated
    module of our application. The *newsstand* prototype has the Celery instance in
    the `/services/billing.py` module, and the following is part of the code that
    shows the process of Celery instantiation:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建Celery任务之前，我们需要将Celery实例放置在我们应用程序的专用模块中。*newsstand*原型将Celery实例放在`/services/billing.py`模块中，以下代码展示了Celery实例化的过程：
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To create the Celery instance, we need the following details:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建Celery实例，我们需要以下详细信息：
- en: The name of the current module containing the Celery instance (the first argument)
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含Celery实例的当前模块的名称（第一个参数）
- en: The URL of Redis as our message broker (`broker`)
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为我们的消息代理的Redis的URL（`broker`）
- en: The backend result where the results of tasks are stored and monitored (`backend`)
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储和监控任务结果的后端（`backend`）
- en: The list of other modules used in the message body or by the Celery task (`include`)
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在消息体或Celery任务中使用的其他模块的列表（`include`）
- en: After the instantiation, we need to set the appropriate serializer and content
    types to process the incoming and outgoing message body of the tasks involved,
    if there are any. To allow the passing of full Python objects with non-JSON-able
    values, we need to include `pickle` as a supported content type, then declare
    a default task and result serializer to the object stream. However, using a `pickle`
    serializer poses some security issues because it tends to expose some transaction
    data. To avoid compromising the app, apply sanitation to message objects, such
    as removing sensitive values or credentials, before pursuing the messaging operation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化后，如果有的话，我们需要设置适当的序列化和内容类型来处理涉及的任务的传入和传出消息体。为了允许传递具有非JSON可序列化值的完整Python对象，我们需要将`pickle`作为支持的内容类型包括在内，然后向对象流声明默认的任务和结果序列化器。然而，使用`pickle`序列化器会带来一些安全问题，因为它倾向于暴露一些事务数据。为了避免损害应用程序，在执行消息操作之前，对消息对象进行清理，例如删除敏感值或凭据。
- en: Apart from the serialization options, other important properties such as `task_create_missing_queues`,
    `task_ignore_result`, and error-related configuration should also be part of the
    `CeleryConfig` class. Now, we declare all these details in a custom class, which
    we will inject into the `config_from_object()` method of the Celery instance.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 除了序列化选项之外，其他重要的属性，如`task_create_missing_queues`、`task_ignore_result`和与错误相关的配置，也应该成为`CeleryConfig`类的一部分。现在，我们在一个自定义类中声明所有这些细节，然后将其注入到Celery实例的`config_from_object()`方法中。
- en: Additionally, we can create a Celery logger through its `get_task_logger()`with
    the name of the current task.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以通过其`get_task_logger()`方法创建一个Celery日志记录器，其名称为当前任务。
- en: Creating the task
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建任务
- en: The main goal of the *Celery instance* is to annotate Python methods to become
    tasks. The Celery instance has a `task()` decorator that we can apply to all callable
    procedures we want to define as asynchronous tasks. Part of the `task()` decorator
    is the task’s `name`, an optional unique name composed of the *package*, *module
    name(s)*, and the *method name of the transaction*. It has other attributes that
    can add more refinement to the task definition, such as the `auto_retry` list,
    which registers `Exception` classes that may cause execution retries when emitted,
    and `max_tries`, which limits the number of retry executions of a task. By the
    way, Celery 5.2.3 and below can only define tasks from *non-coroutine methods*.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*Celery实例*的主要目标是注释Python方法以成为任务。Celery实例有一个`task()`装饰器，我们可以将其应用于我们想要定义为异步任务的所有可调用过程。`task()`装饰器的一部分是任务的`name`，这是一个可选的唯一名称，由*package*、*module
    name(s)*和*transaction*的*method name*组成。它还有其他属性，可以添加更多细化到任务定义中，例如`auto_retry`列表，它注册了可能引起执行重试的`Exception`类，以及`max_tries`，它限制了任务的重试执行次数。顺便说一下，Celery
    5.2.3及以下版本只能从*non-coroutine methods*定义任务。'
- en: 'The `services.billing.tasks.create_total_payables_year_celery` task shown here
    adds all the payable amounts per date and returns the total amount:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这里显示的`services.billing.tasks.create_total_payables_year_celery`任务将每天的应付金额相加，并返回总额：
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The given task has only five (`5`) retries to recover when it encounters either
    `ValueError` or `TypeError` at runtime. Also, it is a function that returns a
    computed amount, which is impossible to create when using `BackgroundTasks`. All
    functional tasks use the *Redis* database as the temporary storage for their returned
    values, which is the reason there is a backend parameter in the Celery constructor.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 给定的任务在运行时遇到`ValueError`或`TypeError`时只有五次(`5`)重试来恢复。此外，它是一个返回计算金额的函数，当使用`BackgroundTasks`时，这是不可能创建的。所有功能任务都使用*Redis*数据库作为它们返回值的临时存储，这也是为什么在Celery构造函数中有后端参数的原因。
- en: Calling the task
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调用任务
- en: 'FastAPI services can call these tasks using the `apply_async()`or `delay()`function.
    The latter is the easier option since it is preconfigured and only needs the parameters
    for the transaction to get the result. The `apply_async()` function is a better
    option since it accepts more details that can optimize the task execution. These
    details are `queue`, `time_limit`, `retry`, `ignore_result`, `expires`, and some
    `kwargs` of arguments. But both these functions return an `AsyncResult` object,
    which returns resources such as the task’s `state`, the `wait()` function to help
    the task finish its operation, and the `get()` function to return its computed
    value or an exception. The following code is a coroutine API service that calls
    the `services.billing.tasks.create_total_payables_year_celery` task using the
    `apply_async` method:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: FastAPI服务可以使用`apply_async()`或`delay()`函数调用这些任务。后者是一个更简单的选项，因为它预先配置好，只需要提供事务的参数即可获取结果。`apply_async()`函数是一个更好的选项，因为它接受更多细节，可以优化任务执行。这些细节包括`queue`、`time_limit`、`retry`、`ignore_result`、`expires`以及一些`kwargs`参数。但这两个函数都返回一个`AsyncResult`对象，该对象返回资源，如任务的`state`、帮助任务完成操作的`wait()`函数以及返回其计算值或异常的`get()`函数。以下是一个协程API服务，它使用`apply_async`方法调用`services.billing.tasks.create_total_payables_year_celery`任务：
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Setting `task_create_missing_queues` to `True` at the `CeleryConfig` setup is
    always recommended because it automatically creates the task `queue`, default
    or not, once the worker server starts. The worker server places all the loaded
    tasks in a task `queue` for execution, monitoring, and result retrieval. Thus,
    we should always define a task `queue` in the `apply_async()` function’s argument
    before extracting `AsyncResult`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在`CeleryConfig`设置中将`task_create_missing_queues`设置为`True`总是推荐的做法，因为它会在工作服务器启动时自动创建任务`queue`，无论是默认的还是有其他名称。工作服务器将所有加载的任务放置在任务`queue`中以便执行、监控和检索结果。因此，在提取`AsyncResult`之前，我们应该始终在`apply_async()`函数的参数中定义一个任务`queue`。
- en: The `AsyncResult` object has a `get()` method that releases the returned value
    of the task from the `AsyncResult` instance, with or without a timeout. In the
    `compute_payables_yearly()` service, the amount payable in `AsyncResult` is retrieved
    by the `get()` function with a timeout of 5 seconds. Let us now deploy and run
    our tasks using the Celery server
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`AsyncResult`对象有一个`get()`方法，它从`AsyncResult`实例中释放任务返回的值，无论是否有超时。在`compute_payables_yearly()`服务中，使用5秒的超时通过`get()`函数检索`AsyncResult`中的应付金额。现在让我们使用Celery服务器部署和运行我们的任务。'
- en: Starting the worker server
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动工作服务器
- en: 'Running the Celery worker creates a single process that handles and manages
    all the queued tasks. The worker needs to know in which module the Celery instance
    is created, together with the tasks to establish the server process. In our prototype,
    the `services.billing` module is where we place our Celery application. Thus,
    the complete command to start the worker is the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 Celery 工作进程创建一个处理和管理所有排队任务的单个进程。工作进程需要知道 Celery 实例是在哪个模块中创建的，以及任务以建立服务器进程。在我们的原型中，`services.billing`
    模块是我们放置 Celery 应用程序的地方。因此，启动工作进程的完整命令如下：
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, `-A` specifies the module of our Celery object and tasks. The `-Q` option
    indicates that the worker will be using a *low-*, *normal-*, *or high-priority*
    queue. But first, we need to set `task_create_missing_queues` to `True` in the
    Celery setup. We also need to indicate the number of threads that the worker needs
    for task execution by adding the `-c` option. The `-P` option specifies the type
    of *thread pool* that the worker will be utilizing. By default, the Celery worker
    uses the `prefork pool` applicable to most CPU-bound transactions. Other options
    are *solo*, *eventlet*, and *gevent*, but our setup will be utilizing *solo*,
    the most suitable choice for running CPU-intensive tasks in a microservice environment.
    On the other hand, the `-l` option enables the logger we set using `get_task_logger()`
    during the setup. Now, there are also ways to monitor our running tasks and one
    of those options is to use the Flower tool.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`-A` 指定了我们的 Celery 对象和任务的模块。`-Q` 选项表示工作进程将使用 *低*、*正常* 或 *高* 优先级队列。但首先，我们需要在
    Celery 设置中将 `task_create_missing_queues` 设置为 `True`。我们还需要通过添加 `-c` 选项来指定工作进程执行任务所需的线程数。`-P`
    选项指定工作进程将利用的 *线程池* 类型。默认情况下，Celery 工作进程使用适用于大多数 CPU 密集型事务的 `prefork pool`。其他选项有
    *solo*、*eventlet* 和 *gevent*，但我们的设置将使用 *solo*，这是在微服务环境中运行 CPU 密集型任务的最佳选择。另一方面，`-l`
    选项启用了我们在设置期间使用 `get_task_logger()` 设置的记录器。现在，也有方法来监控我们的运行任务，其中之一就是使用 Flower 工具。
- en: Monitoring the tasks
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控任务
- en: '*Flower* is Celery’s monitoring tool that observes and monitors all tasks executions
    by generating a real-time audit on a web-based platform. But first, we need to
    install it using `pip`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*花* 是芹菜的监控工具，它通过在基于网页的平台生成实时审计来观察和监控所有任务执行。但首先，我们需要使用 `pip` 来安装它：'
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'And then, we run the following `celery` command with the `flower` option:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用以下带有 `flower` 选项的 `celery` 命令：
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To view the audit, we run `http://localhost:5555/tasks` on a browser. *Figure
    8.3* shows a *Flower* snapshot of an execution log incurred by the `services.billing.tasks.create_total_payables_year_celery`
    task:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看审计，我们在浏览器中运行 `http://localhost:5555/tasks`。*图 8.3* 显示了由 `services.billing.tasks.create_total_payables_year_celery`
    任务引起的执行日志的 *Flower* 快照：
- en: '![Figure 8.3 – The Flower monitoring tool'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.3 – Flower 监控工具'
- en: '](img/Figure_8.03_B17975.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_8.03_B17975.jpg)'
- en: Figure 8.3 – The Flower monitoring tool
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – Flower 监控工具
- en: So far, we have used Redis as our in-memory backend database for task results
    and a message broker. Let us now use another asynchronous message broker that
    can replace Redis, *RabbitMQ*.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用 Redis 作为我们的内存后端数据库来存储任务结果和消息代理。现在让我们使用另一个可以替代 Redis 的异步消息代理，即 *RabbitMQ*。
- en: Building message-driven transactions using RabbitMQ
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 RabbitMQ 构建消息驱动的交易
- en: RabbitMQ is a lightweight asynchronous message broker that supports multiple
    messaging protocols such as *AMQP*, *STOM*, *WebSocket*, and *MQTT*. It requires
    *erlang* before it works properly in Windows, Linux, or macOS. Its installer can
    be downloaded from [https://www.rabbitmq.com/download.html](https://www.rabbitmq.com/download.html).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: RabbitMQ 是一个轻量级的异步消息代理，支持多种消息协议，如 *AMQP*、*STOM*、*WebSocket* 和 *MQTT*。在 Windows、Linux
    或 macOS 上正常工作之前，它需要 *erlang*。其安装程序可以从 [https://www.rabbitmq.com/download.html](https://www.rabbitmq.com/download.html)
    下载。
- en: Creating the Celery instance
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 Celery 实例
- en: Instead of using Redis as the broker, RabbitMQ is a better replacement as a
    message broker that will mediate messages between the client and the Celery worker
    threads. For multiple tasks, RabbitMQ can command the Celery worker to work on
    these tasks one at a time. The RabbitMQ broker is good for huge messages and it
    saves these messages to disk memory.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用 Redis 作为代理相比，RabbitMQ 作为消息代理是一个更好的替代品，它将在客户端和 Celery 工作线程之间中继消息。对于多个任务，RabbitMQ
    可以命令 Celery 工作进程一次处理这些任务中的一个。RabbitMQ 代理适用于大量消息，并将这些消息保存到磁盘内存中。
- en: 'To start, we need to set up a new Celery instance that will utilize the RabbitMQ
    message broker using its *guest* account. We will use the AMQP protocol as the
    mechanism for a producer/consumer type of messaging setup. Here is the snippet
    that will replace the previous Celery configuration:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要设置一个新的 Celery 实例，该实例将使用其 *guest* 账户利用 RabbitMQ 消息代理。我们将使用 AMQP 协议作为生产者/消费者类型消息设置的机制。以下是替换先前
    Celery 配置的代码片段：
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Redis will still be the backend resource, as indicated in Celery’s `backend_result`,
    since it is still simple and easy to control and manage when message traffic increases.
    Let us now use the RabbitMQ to create and manage message-driven transactions.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如 Celery 的 `backend_result` 所示，Redis 仍然是后端资源，因为它在消息流量增加时仍然简单且易于控制和管理工作。现在让我们使用
    RabbitMQ 来创建和管理消息驱动的交易。
- en: Monitoring AMQP messaging
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控 AMQP 消息
- en: 'We can configure the RabbitMQ management dashboard to monitor the messages
    handled by RabbitMQ. After the setup, we can log in to the dashboard using the
    account details to set the broker. *Figure 8.4* shows a screenshot of RabbitMQ’s
    analytics of a situation where the API services called the `services.billing.tasks.create_total_payables_year_celery`
    task several times:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以配置 RabbitMQ 管理仪表板以监控 RabbitMQ 处理的消息。设置完成后，我们可以使用账户详情登录仪表板来设置代理。*图 8.4* 展示了
    RabbitMQ 对 API 服务多次调用 `services.billing.tasks.create_total_payables_year_celery`
    任务的截图：
- en: '![Figure 8.4 – The RabbitMQ management tool'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.4 – RabbitMQ 管理工具'
- en: '](img/Figure_8.04_B17975.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_8.04_B17975.jpg)'
- en: Figure 8.4 – The RabbitMQ management tool
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – RabbitMQ 管理工具
- en: If the RabbitMQ dashboard fails to capture the behavior of the tasks, the *Flower*
    tool will always be an option for gathering the details about the arguments, `kwargs`,
    UUID, state, and processing date of the tasks. And if RabbitMQ is not the right
    messaging tool, we can always resort to *Apache Kafka*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 RabbitMQ 仪表板无法捕获任务的执行行为，*Flower* 工具将始终是收集关于任务参数、`kwargs`、UUID、状态和处理日期详情的选项。如果
    RabbitMQ 不是合适的消息工具，我们总是可以求助于 *Apache Kafka*。
- en: Building publish/subscribe messaging using Kafka
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Kafka 构建发布/订阅消息
- en: As with RabbitMQ, *Apache Kafka* is an asynchronous messaging tool used by applications
    to send and store messages between producers and consumers. However, it is faster
    than RabbitMQ because it uses *topics* with partitions where producers can append
    various types of messages across these minute folder-like structures. In this
    architecture, the consumers can consume all these messages in a parallel mode,
    unlike in queue-based messaging, which enables producers to send multiple messages
    to a queue that can only allow message consumption sequentially. Within this publish/subscribe
    architecture, Kafka can handle an exchange of large quantities of data per second
    in continuous and real-time mode.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 与 RabbitMQ 类似，*Apache Kafka* 是一种异步消息工具，应用程序使用它来在生产者和消费者之间发送和存储消息。然而，它比 RabbitMQ
    更快，因为它使用带有分区的 *topics*，生产者可以在这些类似文件夹的结构中追加各种类型的消息。在这种架构中，消费者可以并行消费所有这些消息，这与基于队列的消息传递不同，后者允许生产者向只能按顺序消费消息的队列发送多个消息。在这个发布/订阅架构中，Kafka
    可以以连续和实时模式每秒处理大量数据交换。
- en: 'There are three Python extensions that we can use to integrate the FastAPI
    services with Kafka, namely the `kafka-python`, `confluent-kafka`, and `pykafka`
    extensions. Our online *newsstand* prototype will use `kafka-python`, so we need
    to install it using the `pip` command:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用三个 Python 扩展将 FastAPI 服务与 Kafka 集成，即 `kafka-python`、`confluent-kafka`
    和 `pykafka` 扩展。我们的在线 *newsstand* 原型将使用 `kafka-python`，因此我们需要使用 `pip` 命令安装它：
- en: '[PRE22]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Among the three extensions, it is only with `kafka-python` that we can channel
    and apply Java API libraries to Python for the implementation of a client. We
    can download Kafka from [https://kafka.apache.org/downloads](https://kafka.apache.org/downloads).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这三个扩展中，只有 `kafka-python` 可以将 Java API 库通道并应用于 Python，以实现客户端的实现。我们可以从 [https://kafka.apache.org/downloads](https://kafka.apache.org/downloads)
    下载 Kafka。
- en: Running the Kafka broker and server
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行 Kafka 代理和服务器
- en: 'Kafka has a *ZooKeeper* server that manages and synchronizes the exchange of
    messages within Kafka’s distributed system. The ZooKeeper server runs as the broker
    that monitors and maintains the Kafka nodes and topics. The following command
    starts the server:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 有一个 *ZooKeeper* 服务器，它管理和同步 Kafka 分布式系统内消息的交换。ZooKeeper 服务器作为代理运行，监控和维护
    Kafka 节点和主题。以下命令启动服务器：
- en: '[PRE23]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we can start the Kafka server by running the following console command:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过运行以下控制台命令来启动 Kafka 服务器：
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: By default, the server will run on localhost at port `9092`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，服务器将在本地的 `9092` 端口上运行。
- en: Creating the topic
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建主题
- en: 'When the two servers have started, we can now create a topic called `newstopic`
    through the following command:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个服务器都已启动时，我们现在可以通过以下命令创建一个名为 `newstopic` 的主题：
- en: '[PRE25]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The `newstopic` topic has three (`3`) partitions that will hold all the appended
    messages of our FastAPI services. These are also the points where the consumers
    will simultaneously access all the published messages.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`newstopic` 主题有三个（`3`）分区，将保存我们 FastAPI 服务的所有附加消息。这些也是消费者将同时访问所有已发布消息的点。'
- en: Implementing the publisher
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施发布者
- en: 'After creating the topic, we can now implement a producer that publishes messages
    to the Kafka cluster. The `kafka-python` extension has a `KafkaProducer` class
    that instantiates a single thread-safe producer for all the running FastAPI threads.
    The following is an API service that sends a newspaper messenger record to the
    Kafka `newstopic` topic for the consumer to access and process:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建主题之后，我们现在可以实施一个生产者，该生产者将消息发布到 Kafka 集群。`kafka-python` 扩展有一个 `KafkaProducer`
    类，它为所有运行的 FastAPI 线程实例化一个线程安全的生产者。以下是一个 API 服务，它将报纸信使记录发送到 Kafka 的 `newstopic`
    主题，以便消费者访问和处理：
- en: '[PRE26]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The coroutine API service, `send_messenger_details()`, asks for details about
    a newspaper messenger and stores them in a `BaseModel` object. And then, it sends
    the dictionary of profile details to the cluster in byte format. Now, one of the
    options to consume Kafka tasks is to run its built-in `kafka-console-consumer.bat`
    command.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 协程 API 服务 `send_messenger_details()` 请求有关报纸信使的详细信息，并将它们存储在 `BaseModel` 对象中。然后，它以字节格式将配置文件详细信息的字典发送到集群。现在，消费
    Kafka 任务的选项之一是运行其内置的 `kafka-console-consumer.bat` 命令。
- en: Running a consumer on a console
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在控制台上运行消费者
- en: 'Running the following command from the console is one way to consume the current
    messages from the `newstopic` topic:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台运行以下命令是消费 `newstopic` 主题当前消息的一种方法：
- en: '[PRE27]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This command creates a consumer that will connect to the Kafka cluster to read
    in real time the current messages from `newtopic` sent by the producer. *Figure
    8.5* shows the capture of the consumer while it is running on the console:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令创建一个消费者，该消费者将连接到 Kafka 集群，实时读取生产者发送的 `newtopic` 中的当前消息。*图 8.5* 展示了消费者在控制台上运行时的捕获情况：
- en: '![Figure 8.5 – The Kafka consumer'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 8.5 – The Kafka consumer]'
- en: '](img/Figure_8.05_B17975.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_8.05_B17975.jpg]'
- en: Figure 8.5 – The Kafka consumer
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – Kafka 消费者
- en: 'If we want the consumer to read all the messages sent by the producer starting
    from the point where the Kafka server and broker began running, we need to add
    the `--from-beginning` option to the command. The following will read all the
    messages from `newstopic` and continuously capture incoming messages in real time:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望消费者从 Kafka 服务器和代理开始运行的位置读取所有由生产者发送的消息，我们需要在命令中添加 `--from-beginning` 选项。以下命令将读取
    `newstopic` 中的所有消息，并持续实时捕获传入的消息：
- en: '[PRE28]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Another way of implementing a consumer using the FastAPI framework is through
    SSE. Typical API service implementation will not work with the Kafka consumer
    requirement since we need a continuously running service that subscribes to `newstopic`
    for real-time data. So, let us now explore how we create SSE in the FastAPI framework
    and how it will consume Kafka messages.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 FastAPI 框架实现消费者的一种方法是使用 SSE（服务器发送事件）。典型的 API 服务实现无法满足 Kafka 消费者的需求，因为我们需要一个持续运行的服务，该服务订阅
    `newstopic` 以获取实时数据。因此，现在让我们探讨如何在 FastAPI 框架中创建 SSE 以及它将如何消费 Kafka 消息。
- en: Implementing asynchronous Server-Sent Events (SSE)
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施异步服务器发送事件（SSE）
- en: SSE is a server push mechanism that sends data to the browser without reloading
    the page. Once subscribed, it generates event-driven streams in real time for
    various purposes.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: SSE 是一种服务器推送机制，它在不重新加载页面的情况下将数据发送到浏览器。一旦订阅，它就会为各种目的实时生成事件驱动的流。
- en: 'Creating SSE in the FastAPI framework only requires the following:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在 FastAPI 框架中创建 SSE 只需要以下步骤：
- en: The `EventSourceResponse` class from the `sse_starlette.see` module
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 `sse_starlette.see` 模块的 `EventSourceResponse` 类
- en: An event generator
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件生成器
- en: 'Above all, the framework also allows non-blocking implementation of the whole
    server push mechanism using coroutines that can run even on *HTTP/2*. The following
    is a coroutine API service that implements a Kafka consumer using SSE’s open and
    lightweight protocol:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 总而言之，该框架还允许使用协程实现整个服务器推送机制的非阻塞实现，这些协程甚至可以在 *HTTP/2* 上运行。以下是一个使用 SSE 的开放和轻量级协议实现
    Kafka 消费者的协程 API 服务：
- en: '[PRE29]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '`send_message_stream()` is a coroutine API service that implements the whole
    SSE. It returns a special response generated by an `EventSourceResponse` function.
    While the HTTP stream is open, it continuously retrieves data from its source
    and converts any internal events into SSE signals until the connection is closed.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`send_message_stream()` 是一个协程 API 服务，它实现了整个 SSE。它返回由 `EventSourceResponse`
    函数生成的特殊响应。当 HTTP 流打开时，它持续从其源检索数据，并将任何内部事件转换为 SSE 信号，直到连接关闭。'
- en: 'On the other hand, event generator functions create internal events, which
    can also be asynchronous. `send_message_stream()`, for instance, has a nested
    generator function, `event_provider()`, which consumes the last message sent by
    the producer service using the `consumer.poll()` method. If the message is valid,
    the generator converts the message retrieved into a `dict` object and inserts
    all its details into the database through `MessengerRepository`. Then, it yields
    all the internal details for the `EventSourceResponse` function to convert into
    SSE signals. *Figure 8.6* shows the data streams generated by `send_message_stream()`rendered
    from the browser:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，事件生成函数创建内部事件，这些事件也可以是异步的。例如，`send_message_stream()` 有一个嵌套的生成函数 `event_provider()`，它使用
    `consumer.poll()` 方法消费生产者服务发送的最后一条消息。如果消息有效，生成器将检索到的消息转换为 `dict` 对象，并通过 `MessengerRepository`
    将所有详细信息插入到数据库中。然后，它为 `EventSourceResponse` 函数产生所有内部详细信息，以便转换为 SSE 信号。*图 8.6* 展示了由
    `send_message_stream()` 渲染的浏览器生成数据流：
- en: '![Figure 8.6 – The SSE data streams'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.6 – SSE 数据流'
- en: '](img/Figure_8.06_B17975.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_8.06_B17975.jpg)'
- en: Figure 8.6 – The SSE data streams
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6 – SSE 数据流
- en: Another way to implement a Kafka consumer is through *WebSocket*. But this time,
    we will focus on the general procedure of how to create an asynchronous WebSocket
    application using the FastAPI framework.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种实现 Kafka 消费者是通过 *WebSocket*。但这次，我们将关注如何使用 FastAPI 框架创建异步 WebSocket 应用程序的一般步骤。
- en: Building an asynchronous WebSocket
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建异步 WebSocket
- en: Unlike in SSE, connection in WebSocket is always *bi-directional*, which means
    the server and client communicate with each other using a long TCP socket connection.
    The communication is always in real time and it doesn’t require the client or
    the server to reply to every event sent.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 与 SSE 不同，WebSocket 的连接始终是**双向的**，这意味着服务器和客户端通过一个长 TCP 套接字连接相互通信。通信总是实时的，并且不需要客户端或服务器对发送的每个事件进行回复。
- en: Implementing the asynchronous WebSocket endpoint
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现异步 WebSocket 端点
- en: 'The FastAPI framework allows the implementation of an asynchronous WebSocket
    that can also run on the *HTTP/2* protocol. The following is an example of an
    asynchronous WebSocket created using the coroutine block:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: FastAPI 框架允许实现一个可以在 *HTTP/2* 协议上运行的异步 WebSocket。以下是一个使用协程块创建的异步 WebSocket 的示例：
- en: '[PRE30]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: First, we decorate a coroutine function with `@router.websocket()` when using
    APIRouter, or `@api.websocket()` when using the FastAPI decorator to declare a
    WebSocket component. The decorator must also define a unique endpoint URL for
    the WebSocket. Then, the WebSocket function must have an injected `WebSocket`
    as its first method argument. It can also include other parameters such as query
    and header parameters.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，当使用 APIRouter 时，我们用 `@router.websocket()` 装饰协程函数，或者当使用 FastAPI 装饰器时，用 `@api.websocket()`
    来声明 WebSocket 组件。装饰器还必须为 WebSocket 定义一个唯一的端点 URL。然后，WebSocket 函数必须将 `WebSocket`
    注入为其第一个方法参数。它还可以包括其他参数，如查询和头参数。
- en: The `WebSocket` injectable has four ways for sending messages, namely `send()`,
    `send_text()`, `send_json()`, and `send_bytes()`. Applying `send()` will always
    manage every message as plain text by default. The previous `customer_list_ws()`coroutine
    is a WebSocket that sends every customer record in JSON format.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`WebSocket` 注入器有四种发送消息的方式，即 `send()`、`send_text()`、`send_json()` 和 `send_bytes()`。应用
    `send()` 将默认将每个消息管理为纯文本。之前的 `customer_list_ws()` 协程是一个以 JSON 格式发送每个客户记录的 WebSocket。'
- en: On the other hand, there are also four methods the WebSocket injectable can
    provide, and these are the `receive()`, `receive_text()`, `receive_json()`, and
    `receive_bytes()` methods. The `receive()` method expects the message to be in
    plain-text format by default. Now, our `customer_list_ws()` endpoint expects a
    JSON reply from a client because it invokes the `receive_json()` method after
    its send message operation.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，WebSocket 注入器还可以提供四种方法，这些是 `receive()`、`receive_text()`、`receive_json()`
    和 `receive_bytes()` 方法。`receive()` 方法默认期望消息是纯文本格式。现在，我们的 `customer_list_ws()`
    端点期望从客户端接收 JSON 响应，因为它在其发送消息操作之后调用了 `receive_json()` 方法。
- en: The WebSocket endpoint must close the connection right after its transaction
    is done.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: WebSocket 端点必须在交易完成后立即关闭连接。
- en: Implementing the WebSocket client
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现WebSocket客户端
- en: 'There are many ways to create a WebSocket client but this chapter will focus
    on utilizing a coroutine API service that will perform a handshake with the asynchronous
    `customer_list_ws()` endpoint once called on a browser or a `curl` command. Here
    is the code of our WebSocket client implemented using the `websockets` library
    that runs on top of the `asyncio` framework:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 WebSocket 客户端有许多方法，但这一章将专注于利用一个协程 API 服务，该服务在浏览器或 `curl` 命令被调用时将与异步的 `customer_list_ws()`
    端点进行握手。以下是使用 `websockets` 库实现的 WebSocket 客户端代码，该库运行在 `asyncio` 框架之上：
- en: '[PRE31]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: After a successful handshake is created by the `websockets.connect()` method,
    `customer_list_ws_client()` will have a loop running continuously to fetch all
    incoming consumer details from the WebSocket endpoint. The message received will
    be converted into its dictionary needed by other processes. Now, our client also
    sends an acknowledgment notification message back to the WebSocket coroutine with
    JSON data containing the *customer ID* of the profile. The loop will stop once
    the WebSocket endpoint closes its connection.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 `websockets.connect()` 方法成功建立握手之后，`customer_list_ws_client()` 将会持续运行一个循环，从
    WebSocket 端点获取所有传入的消费者详情。接收到的消息将被转换成其他进程所需的字典格式。现在，我们的客户端也会向 WebSocket 协程发送一个包含配置文件
    *客户ID* 的 JSON 数据的确认通知消息。一旦 WebSocket 端点关闭其连接，循环将停止。
- en: Let us now explore other asynchronous programming features that can work with
    the FastAPI framework.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在探索其他可以与 FastAPI 框架一起工作的异步编程特性。
- en: Applying reactive programming in tasks
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在任务中应用反应式编程
- en: Reactive programming is a paradigm that involves the generation of streams that
    undergo a series of operations to propagate some changes during the process. Python
    has an *RxPY* library that offers several methods that we can apply to these streams
    asynchronously to extract the terminal result as desired by the subscribers.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 反应式编程是一种涉及生成一系列操作以在过程中传播某些变化的流生成范式。Python 有一个 *RxPY* 库，它提供了我们可以应用于这些流以异步提取订阅者所需终端结果的方法。
- en: In the reactive programming paradigm, all intermediate operators working along
    the streams will execute to propagate some changes if there is an `Observable`
    instance beforehand and an `Observer` that subscribes to this instance. The main
    goal of this paradigm is to achieve the desired result at the end of the propagation
    process using functional programming.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在反应式编程范式中，所有在流中工作的中间操作符都会执行，以传播一些变化，如果在此之前有一个 `Observable` 实例和一个订阅此实例的 `Observer`。这个范式的主要目标是使用函数式编程在传播过程的最后达到预期的结果。
- en: Creating the Observable data using coroutines
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用协程创建 Observable 数据
- en: 'It all starts with the implementation of a coroutine function that will emit
    these streams of data based on a business process. The following is an `Observable`
    function that emits publication details in `str` format for those publications
    that did well in sales:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这一切都始于一个协程函数的实现，该函数将根据业务流程生成这些数据流。以下是一个 `Observable` 函数，它以 `str` 格式发布那些销售表现良好的出版物的详细信息：
- en: '[PRE32]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'An `Observable` function can be synchronous or asynchronous. Our target is
    to create an asynchronous one such as `process_list()`. The coroutine function
    should have the following callback methods to qualify as an `Observable` function:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `Observable` 函数可以是同步的或异步的。我们的目标是创建一个异步的，例如 `process_list()`。协程函数应该有以下回调方法，以符合
    `Observable` 函数的资格：
- en: An `on_next()` method that emits items given a certain condition
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `on_next()` 方法，在给定一定条件下发出项目
- en: An `on_completed()` method that is executed once when the function has completed
    the operation
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `on_completed()` 方法，在函数完成操作时执行一次
- en: An `on_error()` method that is called when an error occurs on `Observable`
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当在 `Observable` 上发生错误时被调用的 `on_error()` 方法
- en: 'Our `process_list()` emits the details of the publication that gained some
    profit. Then, we create an `asyncio` task for the call of the `process_list()`
    coroutine. We created a nested function, `evaluate_profit()`, which returns the
    `Disposable` task required by RxPY’s `create()` method for the production of the
    `Observable` stream. The cancellation of this task happens when the `Observable`
    stream is all consumed. The following is the complete implementation for the execution
    of the asynchronous `Observable` function and the use of the `create()` method
    to generate streams of data from this `Observable` function:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `process_list()` 发射获得一些利润的出版物的详细信息。然后，我们为 `process_list()` 协程的调用创建一个 `asyncio`
    任务。我们创建了一个嵌套函数 `evaluate_profit()`，它返回 RxPY 的 `create()` 方法所需的 `Disposable` 任务，用于生成
    Observable 流。当 Observable 流全部被消费时，此任务会被取消。以下是对执行异步 Observable 函数和使用 `create()`
    方法从该 Observable 函数生成数据流的完整实现：
- en: '[PRE33]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The subscriber created by `create_observable()`is our application’s `list_sales_by_quota()`
    API service. It needs to get the current event loop running for the method to
    generate the observable. Afterward, it invokes the `subscribe()` method to send
    a subscription to the stream and extract the needed result. The Observable’s `subscribe()`
    method is invoked for a client to subscribe to the stream and observe the occurring
    propagations:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 由 `create_observable()` 创建的订阅者是我们应用程序的 `list_sales_by_quota()` API 服务。它需要获取当前运行的事件循环，以便方法可以生成
    Observable。之后，它调用 `subscribe()` 方法向流发送订阅并提取所需的结果。Observable 的 `subscribe()` 方法被调用以使客户端订阅流并观察发生的传播：
- en: '[PRE34]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The `list_sales_by_quote()` coroutine service shows us how to subscribe to
    an `Observable`. A subscriber should utilize the following callback methods:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`list_sales_by_quote()` 协程服务展示了如何订阅一个 Observable。订阅者应利用以下回调方法：'
- en: An `on_next()` method to consume all the items from the stream
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `on_next()` 方法来消费流中的所有项目
- en: An `on_completed()` method to indicate the end of the subscription
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `on_completed()` 方法来指示订阅的结束
- en: An `on_error()` method to flag an error during the subscription process
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个在订阅过程中标记错误的 `on_error()` 方法
- en: And since the `Observable` processes run asynchronously, the scheduler is an
    optional argument that provides the right manager to schedule and run these processes.
    The API service used `AsyncIOScheduler` as the appropriate schedule for the subscription.
    But there are other shortcuts to generating Observables that do not use a custom
    function.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Observable 的处理是异步的，因此调度器是一个可选参数，它提供了正确的管理器来调度和运行这些进程。API 服务使用 `AsyncIOScheduler`
    作为订阅的适当调度。但还有其他生成 Observables 的快捷方式，这些快捷方式不使用自定义函数。
- en: Creating background process
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建后台进程
- en: 'As when we create continuously running Observables, we use the `interval()`
    function instead of using a custom `Observable` function. Some observables are
    designed to end successfully, but some are created to run continuously in the
    background. The following Observable runs in the background periodically to provide
    some updates on the total amount received from newspaper subscriptions:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们创建持续运行的 Observables 时，我们使用 `interval()` 函数而不是使用自定义的 `Observable` 函数。一些 Observables
    被设计为成功结束，但一些则是为了在后台持续运行。以下 Observable 会定期在后台运行，以提供关于从报纸订阅中收到的总金额的一些更新：
- en: '[PRE35]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The `interval()` method creates a stream of data periodically in seconds. But
    this Observable imposes some propagations on its stream because of the execution
    of the `pipe()` method. The Observable’s `pipe()` method creates a pipeline of
    reactive operators called the intermediate operators. This pipeline can consist
    of a chain of operators running one at a time to change items from the streams.
    It seems that this series of operations creates multiple subscriptions on the
    subscriber. So, `fetch_records()` has a `map()` operator in its pipeline to extract
    the result from the `compute_subcription()` method. It uses `merge_all()` at the
    end of the pipeline to merge and flatten all substreams created into one final
    stream, the stream expected by the subscriber. Now, we can also generate Observable
    data from files or API response.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`interval()`方法以秒为单位定期创建数据流。但由于`pipe()`方法的执行，这个Observable对其流施加了一些传播。Observable的`pipe()`方法创建了一个称为中间操作符的反应式操作符的管道。这个管道可以由一系列一次运行一个操作符的链式操作符组成，以改变流中的项目。似乎这一系列操作在订阅者上创建了多个订阅。因此，`fetch_records()`在其管道中有一个`map()`操作符，用于从`compute_subcription()`方法中提取结果。它在管道的末尾使用`merge_all()`来合并和展平创建的所有子流，形成一个最终的流，这是订阅者期望的流。现在，我们也可以从文件或API响应生成Observable数据。'
- en: Accessing API resources
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 访问API资源
- en: Another way of creating an Observable is using the `from_()` method, which extracts
    resources from files, databases, or API endpoints. The Observable function retrieves
    its data from a JSON document generated by an API endpoint from our application.
    The assumption is that we are running the application using `hypercorn`, which
    uses *HTTP/2*, and so we need to bypass the TLS certificate by setting the `verify`
    parameter of `httpx.AsyncClient()` to `False`.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 创建Observable的另一种方法是使用`from_()`方法，它从文件、数据库或API端点提取资源。Observable函数从我们的应用程序的API端点生成的JSON文档中检索其数据。假设我们正在使用`hypercorn`运行应用程序，它使用*HTTP/2*，因此我们需要通过将`httpx.AsyncClient()`的`verify`参数设置为`False`来绕过TLS证书。
- en: 'The following code highlights the `from_()` in the `fetch_subscription()` operation,
    which creates an Observable that emits streams of `str` data from the `https://localhost:8000/ch08/subscription/list/all`
    endpoint. These reactive operators of the Observable, namely `filter()`, `map()`,
    and `merge_all()`, are used to propagate the needed contexts along the stream:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码突出了`fetch_subscription()`操作中的`from_()`，它创建了一个Observable，从`https://localhost:8000/ch08/subscription/list/all`端点发出`str`数据流。Observable的这些反应式操作符，即`filter()`、`map()`和`merge_all()`，用于在流中传播所需上下文：
- en: '[PRE36]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The `filter()` method is another pipeline operator that returns Boolean values
    from a validation rule. It executes the following `filter_within_dates()` to verify
    whether the record retrieved from the JSON document is within the date range specified
    by the subscriber:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`filter()`方法是另一个管道操作符，它从验证规则返回布尔值。它执行以下`filter_within_dates()`以验证从JSON文档检索的记录是否在订阅者指定的日期范围内：'
- en: '[PRE37]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'On the other hand, the following `convert_str()` is a coroutine function executed
    by the `map()` operator to generate a concise profile detail of the newspaper
    subscribers derived from the JSON data:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，以下`convert_str()`是一个由`map()`操作符执行的协程函数，用于生成从JSON数据派生的报纸订阅者的简洁配置文件详情：
- en: '[PRE38]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Running these two functions modifies the original emitted data stream from
    JSON to a date-filtered stream of `str` data. The coroutine `list_dated_subscription()`API
    service, on the other hand, subscribes to `fetch_subscription()` to extract the
    newspaper subscriptions within the `min_date` and `max_date` range:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这两个函数将原始发出的数据流从JSON修改为日期过滤的`str`数据流。另一方面，`list_dated_subscription()`协程API服务订阅`fetch_subscription()`以提取`min_date`和`max_date`范围内的报纸订阅：
- en: '[PRE39]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Although the FastAPI framework does not yet fully support reactive programming,
    we can still create coroutines that can work with various RxPY utilities. Now,
    we will explore how coroutines are not only for background processes but also
    for FastAPI event handlers.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管FastAPI框架尚未完全支持反应式编程，我们仍然可以创建可以与各种RxPY实用程序一起工作的协程。现在，我们将探讨协程不仅用于后台进程，也用于FastAPI事件处理器。
- en: Customizing events
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定制事件
- en: The FastAPI framework has special functions called *event handlers* that execute
    before the application starts up and during shutdown. These events are activated
    every time the `uvicorn` or `hypercorn` server reloads. Event handlers can also
    be coroutines.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: FastAPI 框架具有一些称为 *事件处理器* 的特殊功能，这些功能在应用程序启动之前和关闭期间执行。每当 `uvicorn` 或 `hypercorn`
    服务器重新加载时，都会激活这些事件。事件处理器也可以是协程。
- en: Defining the startup event
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义启动事件
- en: 'The *startup event* is an event handler that the server executes when it starts
    up. We decorate the function with the `@app.on_event("startup")` decorator to
    create a startup event. Applications may require a startup event to centralize
    some transactions, such as the initial configuration of some components or the
    set up of data-related resources. The following example is the application startup
    event that opens a database connection for the GINO repository transactions:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '*启动事件* 是服务器在启动时执行的事件处理器。我们使用 `@app.on_event("startup")` 装饰器来创建启动事件。应用程序可能需要一个启动事件来集中处理某些事务，例如某些组件的初始配置或与数据相关的资源的设置。以下是一个应用程序启动事件示例，该事件为
    GINO 存储库事务打开数据库连接：'
- en: '[PRE40]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This `initialize()` event is defined in our application’s `main.py` file so
    that GINO can only create the connection once every server reload or restart.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 `initialize()` 事件定义在我们的应用程序的 `main.py` 文件中，以便 GINO 可以在每次服务器重新加载或重启时仅创建一次连接。
- en: Defining shutdown events
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义关闭事件
- en: 'Meanwhile, the *shutdown event* cleans up unwanted memory, destroys unwanted
    connections, and logs the reason for shutting down the application. The following
    is the shutdown event of our application that closes the GINO database connection:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，*关闭事件*清理不需要的内存，销毁不需要的连接，并记录关闭应用程序的原因。以下是我们应用程序的关闭事件，该事件关闭了 GINO 数据库连接：
- en: '[PRE41]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We can define startup and shutdown events in APIRouter but be sure this will
    not cause transaction overlapping or collision with other routers. Moreover, event
    handlers do not work in mounted sub-applications.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 APIRouter 中定义启动和关闭事件，但请确保这不会导致事务重叠或与其他路由器冲突。此外，事件处理器在挂载的子应用程序中不起作用。
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The use of coroutines is one of the factors that makes the FastAPI microservice
    application fast, aside from its use of an ASGI-based server. This chapter has
    proven that using coroutines to implement API services will improve the performance
    better than utilizing more threads in the thread pool. Since the framework runs
    on an asyncio platform, we can utilize asyncio utilities to design various design
    patterns to manage the CPU-bound and I/O-bound services.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用基于 ASGI 的服务器之外，协程的使用是使 FastAPI 微服务应用程序快速的因素之一。本章已经证明，使用协程实现 API 服务比在线程池中利用更多线程能更好地提高性能。由于框架运行在
    asyncio 平台上，我们可以利用 asyncio 工具设计各种设计模式来管理 CPU 密集型和 I/O 密集型服务。
- en: This chapter used Celery and Redis for creating and managing asynchronous background
    tasks for behind-the-scenes transactions such as logging, system monitoring, time-sliced
    computations, and batch jobs. We learned that RabbitMQ and Apache Kafka provided
    an integrated solution for building asynchronous and loosely coupled communication
    between FastAPI components, especially for the message-passing part of these interactions.
    Most importantly, coroutines were applied to create these asynchronous and non-blocking
    background processes and message-passing solutions to enhance performance. Reactive
    programming was also introduced in this chapter through the RxPy extension module.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用了 Celery 和 Redis 来创建和管理后台异步任务，例如日志记录、系统监控、时间分片计算和批量作业。我们了解到 RabbitMQ 和 Apache
    Kafka 为构建 FastAPI 组件之间的异步和松散耦合通信提供了一个集成解决方案，特别是对于这些交互的消息传递部分。最重要的是，协程被应用于创建这些异步和非阻塞的后台进程以及消息传递解决方案，以提升性能。本章还介绍了通过
    RxPy 扩展模块引入的反应式编程。
- en: This chapter, in general, concludes that the FastAPI framework is ready to build
    a microservice application that has a *reliable*, *asynchronous*, *message-driven*,
    *real-time message-passing*, and *distributed core system*. The next chapter will
    highlight other FastAPI features that provide integrations with UI-related tools
    and frameworks, API documentation using OpenAPI Specification, session handling,
    and circumventing CORS.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，本章得出结论，FastAPI 框架已准备好构建一个具有 *可靠*、*异步*、*消息驱动*、*实时消息传递* 和 *分布式核心系统* 的微服务应用程序。下一章将突出介绍其他
    FastAPI 功能，这些功能提供了与 UI 相关的工具和框架的集成、使用 OpenAPI 规范的 API 文档、会话处理以及绕过 CORS。
- en: 'Part 3: Infrastructure-Related Issues, Numerical and Symbolic Computations,
    and Testing Microservices'
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3部分：基础设施相关问题、数值和符号计算以及微服务测试
- en: In this final part of the book, we will discuss other essential microservice
    features, such as distributed tracing and logging, service registries, virtual
    environments, and API metrics. Serverless deployment using Docker and Docker Compose
    with NGINX as a reverse proxy will also be covered. Furthermore, we will look
    at FastAPI as a framework for building scientific applications using numerical
    algorithms from the `numpy`, `scipy`, `sympy`, and `pandas` modules to model,
    analyze, and visualize the mathematical and statistical solutions of its API services.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的最后一部分，我们将讨论其他重要的微服务功能，例如分布式跟踪和日志记录、服务注册、虚拟环境和API度量。还将介绍使用Docker和Docker Compose进行无服务器部署，其中NGINX作为反向代理。此外，我们还将探讨FastAPI作为构建科学应用的框架，使用来自`numpy`、`scipy`、`sympy`和`pandas`模块的数值算法来模拟、分析和可视化其API服务的数学和统计解决方案。
- en: 'This part comprises the following chapters:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包括以下章节：
- en: '[*Chapter 9*](B17975_09.xhtml#_idTextAnchor266)*, Utilizing Other Advanced
    Features*'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B17975_09.xhtml#_idTextAnchor266)*，利用其他高级功能*'
- en: '[*Chapter 10*](B17975_10.xhtml#_idTextAnchor292)*, Solving Numerical, Symbolic,
    and Graphical Problems*'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B17975_10.xhtml#_idTextAnchor292)*，解决数值、符号和图形问题*'
- en: '[*Chapter 11*](B17975_11.xhtml#_idTextAnchor321)*, Adding Other Microservice
    Features*'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第11章*](B17975_11.xhtml#_idTextAnchor321)*，添加其他微服务功能*'
