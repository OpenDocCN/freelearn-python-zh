- en: '*Chapter 6*: Automatic Differentiation and Accelerated Linear Algebra for Machine
    Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 6 章*：机器学习的自动微分和加速线性代数'
- en: With the recent explosion of data and data generating systems, machine learning
    has grown to be an exciting field, both in research and industry. However, implementing
    a machine learning model might prove to be a difficult endeavor. Specifically,
    common tasks in machine learning, such as deriving the loss function and its derivative,
    using gradient descent to find the optimal combination of model parameters, or
    using the kernel method for nonlinear data, demand clever implementations to make
    predictive models efficient.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 随着最近数据和数据生成系统的爆炸式增长，机器学习在研究和工业领域都成为一个令人兴奋的领域。然而，实现机器学习模型可能是一项艰巨的任务。具体来说，机器学习中的常见任务，如推导损失函数及其导数、使用梯度下降找到模型参数的最佳组合，或使用核方法处理非线性数据，都需要巧妙的实现来使预测模型高效。
- en: In this chapter, we will discuss the JAX library, the premier high-performance
    machine learning tool in Python. We will explore some of its most powerful features,
    such as automatic differentiation, JIT compilation, and automatic vectorization.
    These features streamline the tasks that are central to machine learning mentioned
    previously, making training a predictive model as simple and accessible as possible.
    All these discussions will revolve around a hands-on example of a binary classification
    problem.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论 JAX 库，这是 Python 中首屈一指的高性能机器学习工具。我们将探索其一些最强大的功能，例如自动微分、即时编译和自动向量化。这些功能简化了之前提到的机器学习核心任务，使得训练预测模型尽可能简单和易于访问。所有这些讨论都将围绕一个二元分类问题的实际案例展开。
- en: By the end of the chapter, you will be able to use JAX to power your machine
    learning applications and explore the more advanced features that it offers.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够使用 JAX 为你的机器学习应用提供动力，并探索它提供的更高级功能。
- en: 'The list of topics covered in this chapter is as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主题列表如下：
- en: A crash course in machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习速成课程
- en: Getting JAX up and running
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动 JAX
- en: Automatic differentiation for loss minimization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于损失最小化的自动微分
- en: Just-In-Time compilation for improved efficiency
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即时编译以提高效率
- en: Automatic vectorization for efficient kernels
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于高效核的自动向量化
- en: A crash course in machine learning
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习速成课程
- en: To fully appreciate the functionalities that JAX offers, let's first talk about
    the principal components of a typical workflow of training a machine learning
    model. If you are already familiar with the basics, feel free to skip to the next
    section, where we begin discussing JAX.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要充分欣赏 JAX 提供的功能，让我们首先谈谈训练机器学习模型的典型工作流程的主要组成部分。如果你已经熟悉基础知识，请随意跳到下一节，我们将开始讨论 JAX。
- en: In machine learning, we set out to solve the problem of predicting an unknown
    target value of interest of a data point by considering its observable features.
    The goal is to design a predictive model that processes the observable features
    and outputs an estimate of what the target value might be. For example, image
    recognition models analyze the pixel values of an image to predict which object
    the image depicts, while a model processing weather data could predict the probability
    of rainy weather for tomorrow by accounting for temperature, wind, and humidity.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们旨在通过考虑数据点的可观察特征来解决预测未知目标值的问题。目标是设计一个预测模型，该模型处理可观察特征并输出目标值可能是什么的估计。例如，图像识别模型分析图像的像素值以预测图像描绘的对象，而处理天气数据的模型可以通过考虑温度、风速和湿度来预测明天下雨的概率。
- en: In general, a machine learning model could be viewed as a general mathematical
    function that takes in the observable features, typically referred to as *X*,
    and produces a prediction about the target value, *Y*. Implicitly assumed by the
    model is the fact that there is a certain mathematical relationship between the
    features (input) and the target values (output). Machine learning models in turn
    aim to learn about this relationship by studying a large number of examples. As
    such, data is crucial in any machine learning workflow.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，机器学习模型可以被视为一个通用数学函数，它接受可观察特征（通常称为 *X*）并产生关于目标值 *Y* 的预测。模型隐含地假设特征（输入）和目标值（输出）之间存在某种数学关系。机器学习模型反过来通过研究大量示例来学习这种关系。因此，数据在任何机器学习工作流程中都是至关重要的。
- en: A **dataset** is a collection of *labeled* examples – data points whose observable
    features, *X*, and target values, *Y*, are both available. It is through these
    labeled points that a model will attempt to uncover the relationship between *X*
    and *Y*; this is known as *training* the model. After training, the learned model
    can then take in the features of *unlabeled* data points, whose target values
    are unknown to us, and output its predictions. These predictions are what the
    model believes the target values of the unlabeled data points to be, given the
    observable features that they have.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据集**是一组**标记**的示例——数据点，其可观察的特征，*X*，和目标值，*Y*，都是可用的。模型将通过这些标记点来尝试揭示*X*和*Y*之间的关系；这被称为**训练**模型。训练完成后，学习到的模型可以接受**未标记**数据点的特征，这些数据点的目标值对我们来说是未知的，并输出其预测。这些预测是模型认为未标记数据点的目标值，基于它们具有的可观察特征。'
- en: In this section, we will talk about how training is facilitated for many common
    machine learning models. Specifically, it consists of three core components—the
    model's parameters, the model's loss function, and the minimization of that loss
    function—which we will consider one by one.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何为许多常见的机器学习模型提供训练。具体来说，它包括三个核心组件——模型的参数、模型的损失函数以及该损失函数的最小化——我们将逐一考虑。
- en: 'We will ground our discussions with an explicit example problem. Assume that
    we have a number of points in a two-dimensional space (which is simply the *xy*
    plane), each of which belongs to either the positive class or the negative class.
    The following is an example where the yellow points are the positives, and the
    purple ones are the negatives:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个具体的示例问题来具体化我们的讨论。假设我们在二维空间（这仅仅是 *xy* 平面）中有一些点，每个点属于正类或负类。以下是一个示例，其中黄色点是正类，紫色点是负类：
- en: '![Figure 6.1 – An example binary classification problem ](img/Figure_6.1_17499.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 一个二元分类问题的示例](img/Figure_6.1_17499.jpg)'
- en: Figure 6.1 – An example binary classification problem
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 一个二元分类问题的示例
- en: In this case, the observable features are the *x* and *y* coordinates of the
    points, so *X* consists of two features, while the target value *y* is their class
    membership, a positive or a negative. Our goal is to train a machine learning
    model on a set of labeled data and make predictions on unlabeled points. For example,
    after training, our model should be able to predict whether point (2, 1) is positive
    or negative.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，可观察的特征是点的 *x* 和 *y* 坐标，因此 *X* 由两个特征组成，而目标值 *y* 是它们的类别成员，是正类还是负类。我们的目标是使用一组标记数据训练机器学习模型，并对未标记的点进行预测。例如，经过训练后，我们的模型应该能够预测点
    (2, 1) 是正类还是负类。
- en: It is quite clear to us what the separation between the positives and the negatives
    is, where the lower-right corner corresponds to the positive class, and the upper-left
    corner corresponds to the negative class. Point (2, 1) specifically is likely
    a positive (a yellow point). But how does a machine learning model do this automatically,
    without humans having to hardcode such a classification rule? We will see this
    in the next section.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们来说，正类和负类之间的分离是非常清晰的，其中右下角对应正类，左上角对应负类。具体来说，点 (2, 1) 很可能是一个正类（一个黄色点）。但是，机器学习模型是如何自动做到这一点，而不需要人类硬编码这样的分类规则呢？我们将在下一节中看到这一点。
- en: Model parameters
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型参数
- en: A predictive model starts out with a formula that attempts to explain the relationship
    between the observable features and the target value. We will consider the class
    of linear models (one of the most common and simplest types of machine learning
    models), which assume that the target value is a linear combination of the features
    (hence the model's name)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一个预测模型从试图解释可观察特征和目标值之间关系的公式开始。我们将考虑线性模型类（机器学习中最常见和最简单类型之一），它假设目标值是特征的线性组合（因此得名模型）
- en: '![](img/Formula_6.1_B17499.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_6.1_B17499.png)'
- en: where ![](img/Formula_6.2_B17499.png) and ![](img/Formula_6.3_B17499.png) are
    the *x* and *y* coordinates of the points, and the numbers ![](img/Formula_6.4_B17499.png)
    are the model's *parameters*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/Formula_6.2_B17499.png) 和 ![](img/Formula_6.3_B17499.png) 是点的 *x*
    和 *y* 坐标，而数字 ![](img/Formula_6.4_B17499.png) 是模型的**参数**。
- en: We do not know the values of these parameters, and each combination of values
    for these parameters defines a unique relationship between *x* and *y*, a unique
    hypothesis. It is the values of these parameters that we need to identify during
    the training phase, such that the aforementioned relationship fits the labeled
    data that we have access to.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不知道这些参数的值，这些参数的每个值组合定义了 *x* 和 *y* 之间独特的关系，一个独特的假设。在训练阶段，我们需要识别这些参数的值，以便上述关系适合我们能够访问的标记数据。
- en: 'In our classification model, we constrain the target values *y* to be either
    positive (+1) or negative (-1), but *y* as defined by the linear relationship
    discussed earlier can take on any real value. As such, it is customary to transform
    a real-valued *y* to be either +1 or -1 by simply taking the sign of the value.
    In other words, the actual model we are working with is:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的分类模型中，我们将目标值 *y* 限制为正数（+1）或负数（-1），但根据前面讨论的线性关系定义的 *y* 可以取任何实数值。因此，通常通过简单地取值的符号来将实数值的
    *y* 转换为 +1 或 -1。换句话说，我们实际使用的模型是：
- en: '![](img/Formula_6.5_B17499.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![公式 6.5_B17499](img/Formula_6.5_B17499.png)'
- en: 'Performing predictions is at the heart of a common machine learning task. Given
    the linear relationship between features *X* and the target value *y*, predicting
    the target value ![](img/Formula_6.6_B17499.png) of a new data point with features
    ![](img/Formula_6.7_B17499.png) and ![](img/Formula_6.8_B17499.png) simply involves
    applying the mathematical model we have:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 进行预测是常见机器学习任务的核心。给定特征 *X* 和目标值 *y* 之间的线性关系，预测具有特征 ![](img/Formula_6.7_B17499.png)
    和 ![](img/Formula_6.8_B17499.png) 的新数据点的目标值 ![](img/Formula_6.6_B17499.png) 简单地涉及应用我们已有的数学模型：
- en: '![](img/Formula_6.9_B17499.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![公式 6.9_B17499.png](img/Formula_6.9_B17499.png)'
- en: 'The only missing piece of our puzzle is how to identify the correct values
    for our parameters ![](img/Formula_6.10_B17499.png). Again, we need to determine
    the values for the model''s parameters to fit our training data. But how can we
    exactly quantify how well a specific value combination for the parameters explains
    a given labeled dataset? This question leads to the next component of a machine
    learning workflow: the model''s loss function.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们谜题中唯一缺失的部分是如何确定参数 ![](img/Formula_6.10_B17499.png) 的正确值。同样，我们需要确定模型参数的值以适应我们的训练数据。但我们如何精确量化特定参数值组合如何解释给定的标记数据集？这个问题引出了机器学习工作流程的下一个组件：模型的损失函数。
- en: Loss function
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: A model's loss function quantifies a property about the model that we'd like
    to minimize. This could, in general, be the cost incurred when the model is trained
    a certain way. Within our context, the loss function is specifically the amount
    of predictive error that the model makes on the training dataset. This quantity
    approximates the error made on future unseen data, which is what we really care
    about. (In different settings, you might encounter **utility functions**, which,
    most of the time, are simply the negative of corresponding loss functions, which
    are to be maximized.) The lower the loss, the better our model is likely to perform
    (barring technical problems such as overfitting).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的损失函数量化了关于模型的一个我们希望最小化的属性。在一般情况下，这可能是以某种方式训练模型时产生的成本。在我们的上下文中，损失函数是模型在训练数据集上造成的预测误差量。这个量近似于在未来的未见数据上犯的错误，这是我们真正关心的。（在不同的设置中，你可能会遇到**效用函数**，这通常只是相应损失函数的负值，这些函数是要最大化的。）损失越低，我们的模型可能表现越好（排除技术问题，如过拟合）。
- en: In a classification problem like ours, there are many ways to design an appropriate
    loss function. For example, the 0-1 loss simply counts how many instances there
    are in which the model makes an incorrect prediction, while the binary cross-entropy
    loss is a more popular choice as the function is smoother than 0-1 and thus easy
    to optimize (we will come back to this point later).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的分类问题中，有许多方法可以设计合适的损失函数。例如，0-1 损失简单地计算模型做出错误预测的实例数量，而二元交叉熵损失是一个更受欢迎的选择，因为该函数比
    0-1 更平滑，因此更容易优化（我们稍后会回到这一点）。
- en: 'For our example, we will use a version of the support-vector loss function,
    which is formalized as:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的例子，我们将使用支持向量损失函数的一个版本，其形式化如下：
- en: '![](img/Formula_6.11_B17499.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![公式 6.11_B17499.jpg](img/Formula_6.11_B17499.jpg)'
- en: where *i*, indicating the index of unique examples in the training set, ranges
    from 1 to *n*, the training set size. ![](img/Formula_6.12_B17499.png) is the
    true label of the *i*-th example, while ![](img/Formula_6.13_B17499.png) is the
    corresponding label predicted by the learning model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *i* 表示训练集中独特示例的索引，范围从1到 *n*，即训练集的大小。![img/Formula_6.12_B17499.png] 是第 *i*
    个示例的真实标签，而 ![img/Formula_6.13_B17499.png] 是学习模型预测的相应标签。
- en: This non-negative function calculates the average of the ![](img/Formula_6.14_B17499.png)
    term across all training examples. It is easy to see that for each example *i*,
    if ![](img/Formula_6.15_B17499.png) and ![](img/Formula_6.16_B17499.png) are similar
    to each other, the max term will evaluate to a small value. On the other hand,
    if ![](img/Formula_6.17_B17499.png) and ![](img/Formula_6.18_B17499.png) have
    opposite signs (in other words, if the model misclassifies the example), the max
    term will increase. As such, this function is appropriate to quantify the error
    that our predictive model is making for a given parameter combination.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个非负函数计算所有训练示例中 ![img/Formula_6.14_B17499.png] 项的平均值。很容易看出，对于每个示例 *i*，如果 ![img/Formula_6.15_B17499.png]
    和 ![img/Formula_6.16_B17499.png] 相似，最大项将评估为小值。另一方面，如果 ![img/Formula_6.17_B17499.png]
    和 ![img/Formula_6.18_B17499.png] 具有相反的符号（换句话说，如果模型错误地分类了示例），最大项将增加。因此，这个函数适合量化我们的预测模型对于给定的参数组合所犯的错误。
- en: 'Given this loss function, the problem of training our linear model reduces
    to finding the optimal parameter ![](img/Formula_6.19_B17499.png) that minimizes
    the loss (preferably at 0). Of course, the search space of ![](img/Formula_6.20_B17499.png)
    is very large—each parameter could be any real-valued number—so exhaustively trying
    out all possible combinations of values for ![](img/Formula_6.21_B17499.png) is
    out of the question. This leads us to the final subsection: intelligently minimizing
    a loss function.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这个损失函数，训练我们的线性模型的问题就转化为寻找最优参数 ![img/Formula_6.19_B17499.png]，使其最小化损失（最好是0）。当然，![img/Formula_6.20_B17499.png]
    的搜索空间非常大——每个参数可以是任何实数值——因此尝试所有可能的![img/Formula_6.21_B17499.png] 值组合是不现实的。这导致我们进入最后一个小节：智能地最小化损失函数。
- en: Loss minimization
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失最小化
- en: Without going into too much technical detail, you can mathematically find the
    minimum of a given function by utilizing its *derivative* information. Derivates
    of a function denote the rate of change in the function value with respect to
    the rate of change of its input. By analyzing the value of the derivative, or
    more commonly referred to in machine learning as the *gradient*, we could algorithmically
    identify input locations at which the objective function is likely to be low-valued.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 不深入技术细节，你可以通过利用其 *导数* 信息在数学上找到给定函数的最小值。函数的导数表示函数值相对于其输入变化率的改变率。通过分析导数的值，或者更常见地，在机器学习中称为
    *梯度*，我们可以算法性地识别出目标函数可能具有低值的输入位置。
- en: However, derivatives are only informative in helping us locate the function
    minimum if the function itself is *smooth*. Smoothness is a mathematical property
    that states that if the input value of a function only changes by a little, the
    function value (output) should not change by too much. Notable examples of non-smooth
    functions include the step function and the absolute value function, but most
    functions that you will encounter are likely to be smooth. The smoothness property
    explains our preference for the binary cross-entropy loss over the 0-1 loss, the
    latter of which is a non-smooth function. The support vector-style loss function
    we will be using is also smooth.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，只有当函数本身是 *平滑的* 时，导数才有助于我们找到函数的极小值。平滑性是一个数学属性，它表明如果函数的输入值只发生微小变化，函数值（输出）不应该变化太多。非平滑函数的显著例子包括阶梯函数和绝对值函数，但你所遇到的函数很可能是平滑的。平滑性属性解释了我们为什么偏好二元交叉熵损失而不是0-1损失，后者是一个非平滑函数。我们将使用的支持向量式损失函数也是平滑的。
- en: 'The overall takeaway is that if we have access to the derivate of a smooth
    function, finding its minimum will be easy to do. But how do we do this exactly?
    Many gradient-based optimization routines have been studied, but we will consider
    arguably the most common method: *gradient descent*. The high-level idea is to
    inspect the gradient of the loss function at a given location and move in the
    direction *opposite* to that gradient (in other words, descending the gradient,
    hence the name), illustrated next:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，如果我们能够访问一个光滑函数的导数，找到它的最小值将会很容易。但我们是如何做到这一点的呢？已经研究了许多基于梯度的优化程序，但我们将考虑最常见的方法：*梯度下降*。高级思想是在给定位置检查损失函数的梯度，并朝着与该梯度相反的方向移动（换句话说，沿着梯度下降，因此得名），如下所示：
- en: '![Figure 6.2 – Illustration of gradient descent ](img/Figure_6.2_B17499.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – 梯度下降的示意图](img/Figure_6.2_B17499.jpg)'
- en: Figure 6.2 – Illustration of gradient descent
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 梯度下降的示意图
- en: Due to the definition of the gradient, the function value *f(x')* evaluated
    at the point *x'* that is immediately in the opposite direction of the gradient
    with respect to a given point *x* is lower than *f(x)*. As such, by using the
    gradient of the loss function as a guide and incrementally moving in the opposite
    direction, we can theoretically arrive at the function minimum. One important
    consideration is that, if we move our evaluation by a large amount, even in the
    correct direction, we could end up missing the true minimum. To address this,
    we typically adjust our step to be only a fraction of the loss gradient.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于梯度的定义，相对于给定点 *x*，在梯度方向相反的点 *x'* 上评估的函数值 *f(x')* 低于 *f(x)*。因此，通过使用损失函数的梯度作为指南，并逐步朝相反方向移动，我们可以理论上到达函数的最小值。一个重要的考虑因素是，如果我们以很大的幅度移动评估，即使在正确的方向上，我们可能会错过真正的最小值。为了解决这个问题，我们通常将步长调整为损失梯度的分数之一。
- en: 'Concretely, denote ![](img/Formula_6.22_B17499.png) as the gradient of the
    loss function *L* with respect to the *i*-th parameter ![](img/Formula_6.23_B17499.png),
    so the gradient descent *update rule* is to adjust the value of ![](img/Formula_6.24_B17499.png)
    as:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，将 ![](img/Formula_6.22_B17499.png) 表示为损失函数 *L* 对第 *i*-个参数 ![](img/Formula_6.23_B17499.png)
    的梯度，因此梯度下降的 *更新规则* 是调整 ![](img/Formula_6.24_B17499.png) 的值，如下所示：
- en: '![](img/Formula_6.25_B17499.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_6.25_B17499.png)'
- en: where *γ* is a small number (much lower than 1) that acts as a step size. By
    repeatedly applying this update rule, we could incrementally adjust the values
    of the model's parameters to lower the model's loss, thereby improving its predictive
    performance.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *γ* 是一个小数（远小于1），充当步长。通过反复应用此更新规则，我们可以逐步调整模型参数的值，以降低模型的损失，从而提高其预测性能。
- en: And with that, we have finished sketching out the main components of a typical
    machine learning workflow. As a summary, we work with a predictive model (in our
    case, a linear one) that has a certain number of adjusting parameters that seek
    to explain the data we have. Each parameter combination leads to a different hypothesis
    whose validity is quantified by the loss function. Finally, by taking the derivative
    of the loss function and using gradient descent, we have a way to incrementally
    update the model's parameters to achieve better performance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们已经概述了典型机器学习工作流程的主要组件。总结一下，我们与一个预测模型（在我们的例子中，是一个线性模型）一起工作，该模型具有一定数量的调整参数，旨在解释我们所拥有的数据。每个参数组合导致一个不同的假设，其有效性由损失函数量化。最后，通过对损失函数求导和使用梯度下降，我们有一种方法可以逐步更新模型的参数，以实现更好的性能。
- en: 'We will now begin the main topic of this chapter: the JAX library.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将开始本章的主要内容：JAX库。
- en: Getting JAX up and running
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动JAX
- en: As briefly mentioned, JAX is a combination of different tools for developing
    accelerated, high-performance computations with a focus on machine learning applications.
    Remember from the last chapter that the NumPy library offers optimized computation
    for numerical operations such as finding the min/max or taking the sum of the
    average along an axis. We can think of JAX as the NumPy equivalent for machine
    learning, where common tasks in machine learning could be done in highly optimized
    code. These, as we will see, include automatic differentiation, accelerated linear
    algebra using a Just-In-Time compiler, and efficient vectorization and parallelization
    of code, among other things.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，JAX是不同工具的组合，用于开发加速、高性能计算，重点在于机器学习应用。记得从上一章中，NumPy库为数值运算提供了优化计算，例如寻找最小/最大值或沿轴求和的平均值。我们可以将JAX视为机器学习的NumPy等价物，其中机器学习的常见任务可以在高度优化的代码中完成。这些，正如我们将看到的，包括自动微分、使用即时编译器的加速线性代数以及代码的有效向量化和平行化，等等。
- en: JAX offers these functionalities through what's known as **functional transformations**.
    In the simplest sense, a functional transformation in JAX converts a function,
    typically one that we build ourselves, to an optimized version where different
    functionalities are facilitated. This is done via a simple API, as we will see
    later.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: JAX通过所谓的**函数转换**提供这些功能。在最简单的意义上，JAX中的函数转换将一个函数（通常是自行构建的）转换为优化版本，其中提供了不同的功能。这通过一个简单的API实现，我们将在后面看到。
- en: But first, we will talk about installing the library on a local system.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，我们将讨论在本地系统上安装库。
- en: Installing JAX
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装JAX
- en: 'A barebones version of JAX could be simply installed using the regular Python
    package manager `pip`:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 可以简单地使用常规Python包管理器`pip`安装JAX的基础版本：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'These commands will install a CPU-only version of JAX, which is recommended
    if you are planning to try it out on a local system such as a laptop. The full
    version of JAX, with support for NVIDIA GPUs (commonly used by deep learning models),
    could also be installed via a more involved procedure, outlined in their documentation:
    [https://jax.readthedocs.io/en/latest/developer.html](https://jax.readthedocs.io/en/latest/developer.html).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令将安装一个仅CPU版本的JAX，如果你计划在本地系统（如笔记本电脑）上尝试它，这是推荐的。JAX的完整版本，支持NVIDIA GPU（深度学习模型常用），也可以通过更复杂的流程安装，具体流程可在其文档中找到：[https://jax.readthedocs.io/en/latest/developer.html](https://jax.readthedocs.io/en/latest/developer.html)。
- en: Furthermore, note that JAX can only be installed on Linux (Ubuntu 16.04 or above)
    or macOS (10.12 or above). To run JAX on a Windows machine, you would need the
    Windows Subsystem for Linux, which lets you run a GNU/Linux environment on Windows.
    If you typically run your machine learning jobs on a cluster, you should talk
    to your system administrator about finding the correct way to install JAX.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，JAX只能在Linux（Ubuntu 16.04或更高版本）或macOS（10.12或更高版本）上安装。要在Windows机器上运行JAX，你需要Windows
    Subsystem for Linux，它允许你在Windows上运行GNU/Linux环境。如果你通常在集群上运行机器学习作业，你应该与系统管理员联系，了解安装JAX的正确方法。
- en: 'If you are an independent machine learning practitioner, you might find these
    technical requirements to install the full version of JAX intimidating. Luckily,
    there is a free platform that we could use to see JAX in action: *Google Colab*.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一名独立的机器学习从业者，你可能觉得安装JAX完整版本的技术要求令人畏惧。幸运的是，有一个免费的平台我们可以使用来查看JAX的实际应用：*Google
    Colab*。
- en: Using Google Colab
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Google Colab
- en: Generally, Google Colab is a free Jupyter notebook platform that is integrated
    with a high-performance backend and machine learning tools on which you could
    build prototypes of machine learning models with ease.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，Google Colab是一个免费的Jupyter笔记本平台，它集成了高性能后端和机器学习工具，你可以轻松地在这个平台上构建机器学习模型的原型。
- en: There are two main advantages to using Google Colab. First, the platform comes
    with most machine learning-related libraries (for example, TensorFlow, PyTorch,
    and scikit-learn) and tools preinstalled, including a **GPU** (**graphics processing
    unit**) and a **TPU** (**tensor processing unit**), free of charge! This allows
    independent machine learning researchers and students to try out expensive hardware
    on their machine learning pipelines at no cost. Second, Google Colab allows collaborative
    editing between different users, enabling streamlined group work when building
    models.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Google Colab有两个主要优势。首先，该平台预装了大多数机器学习相关的库（例如，TensorFlow、PyTorch和scikit-learn）和工具，包括免费的**GPU**（**图形处理单元**）和**TPU**（**张量处理单元**）。这使得独立的机器学习研究人员和学生可以在不花钱的情况下尝试昂贵硬件的机器学习流程。其次，Google
    Colab允许不同用户之间的协作编辑，在构建模型时可以简化团队工作。
- en: To utilize this platform, all you need is a Google/Gmail account when signing
    in at [https://colab.research.google.com/](https://colab.research.google.com/).
    Using this platform is generally simple and intuitive, but you can also refer
    to several readings included at the end of this chapter for further content relating
    to Google Colab. Note that the code presented in the remainder of this chapter
    is run on Google Colab, which means you could go to **[INSERT LINK]** and simply
    run the code yourself to follow along with our later discussions.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要利用这个平台，你只需要在[https://colab.research.google.com/](https://colab.research.google.com/)登录时有一个Google/Gmail账户。使用这个平台通常简单直观，但你也可以参考本章末尾包含的几篇阅读材料，以获取更多有关Google
    Colab的内容。请注意，本章余下的代码是在Google Colab上运行的，这意味着你可以访问**[INSERT LINK]**并直接运行代码，以跟随我们后面的讨论。
- en: And with that, we are ready to begin talking about the functionalities that
    JAX offers.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们就准备好开始讨论JAX提供的功能了。
- en: Automatic differentiation for loss minimization
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失最小化的自动微分
- en: Recall from our previous discussion that to fit a predictive model to a training
    dataset, we first analyze an appropriate loss function, derive the gradient of
    this loss, and then adjust the parameters of the model in the opposite direction
    of the gradient to achieve a lower loss. This procedure is only possible if we
    have access to the derivative of the loss function.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们之前的讨论，为了将预测模型拟合到训练数据集，我们首先分析一个合适的损失函数，推导出这个损失函数的梯度，然后调整模型参数以与梯度的反方向一致，以达到更低的损失。这个步骤只有在我们可以访问损失函数的导数时才可行。
- en: Earlier machine learning models were able to do this because researchers derived
    the derivatives of common loss functions by hand using calculus, which were then
    hardcoded into the training algorithm so that a loss function could be minimized.
    Unfortunately, taking the derivative of a function could be difficult to do at
    times, especially if the loss function being used is not well behaved. In the
    past, you would have to choose a different, more mathematically convenient loss
    function to make your model run even if the new function was less appropriate,
    potentially sacrificing some of the expressiveness of the original function.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的机器学习模型能够做到这一点，是因为研究人员通过微积分手动推导出常见损失函数的导数，然后将这些导数硬编码到训练算法中，以便最小化损失函数。不幸的是，有时求函数的导数可能很困难，特别是如果使用的损失函数表现不佳。在过去，你可能不得不选择一个不同、更数学上方便的损失函数，即使新函数不太合适，也要让模型运行，这可能会牺牲一些原始函数的表达能力。
- en: Recent advances in computational algebra have resulted in the technique called
    **automatic differentiation**. Exactly as it sounds, this technique allows for
    an automated process to numerically evaluate the derivative of a given function,
    expressed in computer code, without relying on a human to provide the closed-form
    solution for that derivative. On the highest level, automatic differentiation
    traces the order in which the considered function is computed and which arithmetic
    operations (for example, addition, multiplication, and exponentiation) were involved,
    and then applies the chain rule to obtain the numerical value of the derivative
    in an automated manner. By intelligently taking advantage of the information present
    in the computational calculation of the function itself, automatic differentiation
    can compute its derivative efficiently and accurately.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 计算代数领域的近期进展导致了一种称为**自动微分**的技术。正如其名所示，这项技术允许自动过程对给定函数（以计算机代码表示）进行数值求导，而不依赖于人类提供该导数的封闭形式解。在最高层次上，自动微分追踪考虑的函数的计算顺序以及涉及的算术运算（例如，加法、乘法和指数运算），然后以自动方式应用链式法则来获得导数的数值。通过智能地利用函数自身计算中的信息，自动微分可以高效且准确地计算其导数。
- en: 'Recall that in our running toy problem, we would like to minimize the loss
    function:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在我们的运行玩具问题中，我们希望最小化损失函数：
- en: '![](img/Formula_6.26_B17499.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_6.26_B17499.jpg)'
- en: where ![](img/Formula_6.27_B17499.png) is the prediction made by our linear
    predictive model for the *i*-th data point. Of course, we could derive the gradient
    of this loss function when implementing gradient descent (which would require
    us to brush up on our calculus!), but with automatic differentiation, we will
    not have to.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/Formula_6.27_B17499.png) 是我们对第*i*个数据点的线性预测模型的预测。当然，在实现梯度下降时，我们可以推导出这个损失函数的梯度（这需要我们复习微积分！），但有了自动微分，我们就不必这样做。
- en: The technique is widely used in deep learning tools such as TensorFlow and PyTorch,
    where computing the gradient of a loss function is a central task. We could use
    the API for automatic differentiation from these libraries to compute the loss
    gradient, but by using JAX's API, we will be able to utilize other powerful functionalities
    that we will get to later. For now, let's build our model and the gradient descent
    algorithm using JAX's automatic differentiation tool.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术在深度学习工具如TensorFlow和PyTorch中得到广泛应用，在这些工具中，计算损失函数的梯度是一个核心任务。我们可以使用这些库的自动微分API来计算损失梯度，但通过使用JAX的API，我们将能够利用我们稍后将会得到的其他强大功能。现在，让我们使用JAX的自动微分工具构建我们的模型和梯度下降算法。
- en: Making the dataset
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 制作数据集
- en: 'First, we need to generate the toy dataset shown in *Figure 6.1*. To do this,
    we will make use of the `make_blobs` function from the `datasets` module in scikit-learn.
    If you are running your code locally, you would need to install scikit-learn using
    the `pip` command:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要生成*图6.1*中所示的小型数据集。为此，我们将利用scikit-learn模块中的`make_blobs`函数。如果你在本地运行代码，你需要使用`pip`命令安装scikit-learn：
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Otherwise, if you are using Google Colab, scikit-learn, as well as all the other
    external libraries used in this chapter's code, already comes preinstalled, so
    you only need to import it into your program.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，如果你使用Google Colab，scikit-learn以及本章代码中使用的所有其他外部库已经预先安装，所以你只需要将其导入到你的程序中。
- en: 'The `make_blobs` function, as the name suggests, generates blobs (in other
    words, clusters) of data points. Its API is simple and readable:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`make_blobs`函数，正如其名所示，生成数据点的云团（换句话说，聚类）。其API简单易读：'
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here we are creating a two-dimensional (specified by `n_features`) dataset of
    500 points (specified by `n_samples`). These data points should belong to one
    of two classes (specified by `centers`). The `cluster_std` argument controls how
    tightly the points belonging to the same class cluster together; in this case,
    we are setting it to `0.5`. At this point, we have a dataset of 500 points, each
    of which has two features and belongs to either the positive class or the negative
    class. The variable `X` is a NumPy array that contains the features, having a
    shape of `(500, 2)`, while `y` is a 500-long, one-dimensional, binary array containing
    the labels.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个二维（由 `n_features` 指定）的包含 500 个点的数据集（由 `n_samples` 指定）。这些数据点应该属于两个类别中的一个（由
    `centers` 指定）。`cluster_std` 参数控制属于同一类别的点如何紧密地聚集在一起；在这种情况下，我们将它设置为 `0.5`。到这一点，我们有一个包含
    500 个点的数据集，每个点都有两个特征，属于正类或负类。变量 `X` 是一个包含特征的 NumPy 数组，形状为 `(500, 2)`，而 `y` 是一个长度为
    500 的一维二进制数组，包含标签。
- en: 'By default, the negative class has the label of `0` in `y`. As a data preprocessing
    step, we will convert all the instances of `0` in `y` to `-1` using NumPy''s convenient
    conditional indexing syntax:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，负类在 `y` 中的标签为 `0`。作为一个数据预处理步骤，我们将使用 NumPy 方便的条件索引语法将 `y` 中所有的 `0` 实例转换为
    `-1`：
- en: '[PRE3]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We are also adding to `X` a third feature column that only contains instances
    of `1`. This column is a constant and corresponds to the free coefficient ![](img/Formula_6.28.1_B17499.png)
    in our model:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在 `X` 中添加了一个第三特征列，该列只包含 `1` 的实例。这个列是一个常数，对应于我们模型中的自由系数 ![](img/Formula_6.28.1_B17499.png)：
- en: '![](img/Formula_6.28_B17499.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_6.28_B17499.png)'
- en: On the other hand, ![](img/Formula_6.29_B17499.png) and ![](img/Formula_6.30_B17499.png)
    are the coefficients for the first and second features in `X`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，![](img/Formula_6.29_B17499.png) 和 ![](img/Formula_6.30_B17499.png) 是 `X`
    中第一和第二个特征的系数。
- en: 'We have now generated our toy example. If you now call Matplotlib to generate
    a scatter plot on `X`, for example, via the following code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经生成了我们的玩具示例。如果你现在调用 Matplotlib 在 `X` 上生成散点图，例如，通过以下代码：
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You will obtain the same plot as in *Figure 6.1*. And with that, we are ready
    to begin building our predictive model.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你将获得与 *图 6.1* 中相同的图表。有了这个，我们就准备好开始构建我们的预测模型了。
- en: Building a linear model
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建线性模型
- en: 'Remember two core components of a machine learning pipeline: the model and
    the loss function. To build our model, we first need to write functions that correspond
    to these components. First, the model comes in the form of the `predict` function:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 记住机器学习管道的两个核心组件：模型和损失函数。要构建我们的模型，我们首先需要编写与这些组件相对应的函数。首先，模型以 `predict` 函数的形式出现：
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This function takes in the model's parameters `w`, implicitly assumed to be
    in the form of a NumPy array, and the features of the training data points `X`,
    a NumPy two-dimensional matrix. Here we are using the `dot` function from the
    NumPy module of JAX, aliased `jnp` in the code, to take the dot product of `w`
    and `X`. If you are unfamiliar with the dot product, it is simply an algebraic
    operation that is a shorthand for the linear combination ![](img/Formula_6.31_B17499.png)
    that corresponds to our model. Overall, this function encodes for our model that
    the prediction we make about the label of a data point is (the sign of) the dot
    product of the parameters and the data features.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数接收模型的参数 `w`，隐式地假设它是以 NumPy 数组的格式存在的，以及训练数据点的特征 `X`，一个 NumPy 二维矩阵。在这里，我们使用
    JAX 的 NumPy 模块中的 `dot` 函数，在代码中别名为 `jnp`，来计算 `w` 和 `X` 的点积。如果你不熟悉点积，它是一种代数运算，是线性组合的简写，对应于我们的模型。总的来说，这个函数为我们的模型编码了这样的预测：关于数据点标签的预测是参数和数据特征的点积（符号）。
- en: 'While not a focus of our discussion, it is important at this point to note
    that the `numpy` module of JAX provides nearly identical APIs for mathematical
    operations as NumPy. The `dot` function is simply one example; others could be
    found on their documentation site: [https://jax.readthedocs.io/en/latest/jax.numpy.html](https://jax.readthedocs.io/en/latest/jax.numpy.html).
    This is to say that if you have accumulated a significant amount of code in NumPy
    but would like to convert it to JAX, replacing your import statement of `import
    numpy as np` with `import jax.numpy as np` would, in most cases, do the trick;
    no complicated, painstaking conversion is necessary.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这不是我们讨论的重点，但在此处需要注意的是，JAX的`numpy`模块提供了与NumPy几乎相同的数学运算API。`dot`函数只是一个例子；其他可以在他们的文档网站上找到：[https://jax.readthedocs.io/en/latest/jax.numpy.html](https://jax.readthedocs.io/en/latest/jax.numpy.html)。这意味着如果你在NumPy中积累了大量的代码，但想将其转换为JAX，在大多数情况下，只需将`import
    numpy as np`替换为`import jax.numpy as np`即可；不需要复杂的、费力的转换。
- en: So, we have implemented the prediction function of our model. Our next task
    is to write the loss function
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经实现了我们模型的预测函数。我们的下一个任务是编写损失函数
- en: '![](img/Formula_6.32_B17499.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![公式6.32](img/Formula_6.32_B17499.png)'
- en: 'The loss function can be written with the following code:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数可以用以下代码编写：
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let's take a moment to break this code down. This `loss` function is a function
    of `w`, since, if `w` changes, the loss will change accordingly.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花一点时间来分解这段代码。这个`loss`函数是`w`的函数，因为，如果`w`发生变化，损失也会相应地变化。
- en: 'Now, let''s focus on the term inside the summation symbol in the previous formula:
    it is the max between 0 and ![](img/Formula_6.33_B17499.png), where ![](img/Formula_6.34_B17499.png)
    is the true label of a data point and ![](img/Formula_6.35_B17499.png) is the
    corresponding prediction made by the model. As such, we compute this quantity
    with `jnp.clip(1 - jnp.multiply(y, preds), a_min=0)`, before which we call the
    `predict` function to obtain the values for ![](img/Formula_6.36_B17499.png),
    stored in the `preds` variable.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们关注上一公式中求和符号内的项：它是0和![公式6.33](img/Formula_6.33_B17499.png)之间的最大值，其中![公式6.34](img/Formula_6.34_B17499.png)是数据点的真实标签，![公式6.35](img/Formula_6.35_B17499.png)是模型做出的相应预测。因此，我们通过`jnp.clip(1
    - jnp.multiply(y, preds), a_min=0)`计算这个量，在此之前我们调用`predict`函数以获得存储在`preds`变量中的![公式6.36](img/Formula_6.36_B17499.png)的值。
- en: If you are familiar with the NumPy equivalent, you might have noticed that these
    functions that we are using, `clip` and `multiply`, have an identical interface
    in NumPy; again, JAX tries to make the transition for NumPy users as seamless
    as possible. Finally, we compute the mean of this max term across all training
    examples with the `mean` function.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉NumPy的等效函数，你可能会注意到我们正在使用的这些函数，`clip`和`multiply`，在NumPy中具有相同的接口；再次强调，JAX试图使NumPy用户的使用尽可能无缝。最后，我们使用`mean`函数计算所有训练示例中这个最大项的平均值。
- en: 'Believe it or not, we have successfully implemented our linear model! With
    a specific value for `w`, we will be able to predict what the label of a data
    point is using the `predict` function and compute our loss with the `loss` function.
    At this point, we can try out different values for `w`; as a common practice,
    we will initialize `w` to be random numbers:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 信不信由你，我们已经成功实现了我们的线性模型！给定一个`w`的具体值，我们将能够使用`predict`函数预测数据点的标签，并使用`loss`函数计算我们的损失。在这个阶段，我们可以尝试不同的`w`值；作为一个常见的做法，我们将`w`初始化为随机数：
- en: '[PRE7]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The last step of our training procedure is to find the best value for `w` that
    minimizes the loss, which we will implement next.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练过程的最后一步是找到使损失最小化的`w`的最佳值，我们将在下一部分实现它。
- en: Gradient descent with automatic differentiation
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带有自动微分的梯度下降
- en: Recall that in the gradient descent algorithm, we compute the gradient of the
    loss function for the current value of `w` and then adjust `w` by subtracting
    a fraction of the gradient (effectively moving in the opposite direction of the
    gradient).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在梯度下降算法中，我们计算当前`w`值的损失函数的梯度，然后通过减去梯度的一部分（实际上是在梯度的反方向移动）来调整`w`。
- en: 'Again, in the past, we would need to derive the gradient in closed form and
    implement it in our code. With automatic differentiation, we simply need the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去，我们需要以闭式形式推导梯度并在我们的代码中实现它。有了自动微分，我们只需要以下内容：
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`loss` is the function we implemented in the last subsection. Here we are using
    the `grad` function from JAX to obtain the *gradient function* of `loss`. This
    is to say that if we were to call `loss_grad`, the gradient function, on a given
    value of `w` using `loss_grad(w)`, we would obtain the gradient of the `loss`
    function at `w`, in the same way `loss(w)` gives us the loss itself at `w`.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`loss`是我们在上一个子节中实现的函数。这里我们使用JAX的`grad`函数来获取`loss`的*梯度函数*。这意味着如果我们调用`loss_grad`，即梯度函数，在给定的`w`值上使用`loss_grad(w)`，我们将获得在`w`处的`loss`函数的梯度，就像`loss(w)`给我们提供了在`w`处的损失本身一样。'
- en: This is a so-called *function transformation*, which takes in a function in
    Python (`loss` in our case) and returns a related function (`loss_grad`) that
    could then be called on actual inputs to compute quantities that we are interested
    in. Function transformations are a powerful interface that makes it easy to work
    with functional logic and are the main way in which you use JAX. We will see that
    all other functionalities offered by JAX discussed in this chapter are function
    transformations.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个所谓的*函数转换*，它接受一个Python中的函数（在我们的例子中是`loss`）并返回一个相关的函数（`loss_grad`），然后可以对该函数的实际输入进行调用以计算我们感兴趣的数量。函数转换是一个强大的接口，使得处理函数逻辑变得容易，并且是使用JAX的主要方式。我们将看到本章中讨论的JAX提供的所有其他功能都是函数转换。
- en: 'With our gradient function in hand, we now implement gradient descent using
    a simple `for` loop:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在手头有了我们的梯度函数后，我们现在使用简单的`for`循环实现梯度下降：
- en: '[PRE9]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If you are looking at the Google Colab notebook, you might see some other book-keeping
    code that is used to show a real-time progress bar, but the preceding code is
    the main logic of gradient descent.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在查看Google Colab笔记本，你可能会看到一些其他用于显示实时进度条的记账代码，但前面的代码是梯度下降的主要逻辑。
- en: 'At each iteration of the `for` loop, we compute the gradient of the loss function
    at the current value of `w`, and then use that gradient to adjust the value of
    `w`: `w = w - lr * grads`. How big of a step we will take in the opposite direction
    of the gradient is controlled by the `lr` variable, set to `0.01`. This variable
    is typically referred to as the **learning rate** in the language of machine learning.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在`for`循环的每次迭代中，我们计算在当前`w`值处的损失函数的梯度，然后使用该梯度来调整`w`的值：`w = w - lr * grads`。我们将朝着梯度的反方向迈出多大的步子由`lr`变量控制，设置为`0.01`。这个变量在机器学习的语言中通常被称为**学习率**。
- en: After the adjustment, we will check to see whether this new value of `w` does,
    in fact, minimize our loss at 0, in which case we simply exit the `for` loop with
    the `break` statement. If not, we repeat this process until some termination condition
    is satisfied; in our case, we simply say that we only do this for at most 200
    times (specified by the `n_iters` variable).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后，我们将检查这个新的`w`值是否实际上在0处最小化了我们的损失，如果是这样，我们就简单地使用`break`语句退出`for`循环。如果不是，我们将重复此过程，直到满足某个终止条件；在我们的例子中，我们只是说我们最多只做200次（由`n_iters`变量指定）。
- en: 'When we run the code, we will observe that as the `for` loop progresses, the
    value of the loss decreases for each adjustment. At the end of the loop, our loss
    decreases to a small number: roughly 0.013\. Using Matplotlib, we could quickly
    sketch out the progression of this decrease with `plt.plot(losses)`, which produces
    the following plot:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，我们会观察到随着`for`循环的进行，每次调整后损失的值都会减少。循环结束时，我们的损失减少到一个很小的数字：大约0.013。使用Matplotlib，我们可以快速绘制出这种减少的进程，使用`plt.plot(losses)`，可以得到以下图表：
- en: '![Figure 6.3 – The decrease in loss of the linear model ](img/Figure_6.3_17499.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – 线性模型损失减少](img/Figure_6.3_17499.jpg)'
- en: Figure 6.3 – The decrease in loss of the linear model
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 线性模型损失减少
- en: We see that with the random guess for the value of `w` at the beginning, we
    receive a loss of roughly 2.5, but using gradient descent, we have reduced it
    to almost zero. At this point, we can be confident that the value of `w` that
    we currently have after gradient descent fits our data well.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，在开始时对`w`的值进行随机猜测，我们得到了大约2.5的损失，但使用梯度下降，我们已经将其减少到几乎为零。在这个时候，我们可以有信心，经过梯度下降后我们目前拥有的`w`的值很好地拟合了我们的数据。
- en: This concludes the training of our linear predictive model. A customary task
    that follows training a model is to examine the predictions made by the model
    across a whole feature space. This process helps ensure that our model is learning
    appropriately. When working in a low-dimensional space such as ours, we can even
    visualize this space.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们的线性预测模型训练的结束。在训练模型之后的一个常规任务是检查模型在整个特征空间中做出的预测。这个过程有助于确保我们的模型正在适当地学习。在我们这样的低维空间中工作，我们甚至可以可视化这个空间。
- en: 'Specifically, our decision boundary is a straight line that (hopefully) separates
    the two classes that we''d like to perform classification: ![](img/Formula_6.37_B17499.png),
    where ![](img/Formula_6.38_B17499.png) and ![](img/Formula_6.39_B17499.png) are
    coordinates of points within our two-dimensional space. As such, we can draw this
    line by first generating a fine-grid array for the *x* coordinates ![](img/Formula_6.40_B17499.png):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们的决策边界是一条直线，(希望)将我们想要进行分类的两个类别分开：![](img/Formula_6.37_B17499.png)，其中![](img/Formula_6.38_B17499.png)和![](img/Formula_6.39_B17499.png)是我们二维空间内点的坐标。因此，我们可以通过首先为*x*坐标![](img/Formula_6.40_B17499.png)生成一个精细网格数组来绘制这条线：
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can derive the *y* coordinates ![](img/Formula_6.41_B17499.png) using the
    equation for the line we had previously:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用之前得到的直线方程推导出*y*坐标![](img/Formula_6.41_B17499.png)：
- en: '[PRE11]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Plotting this line together with the scattered points corresponding to our
    training data, we obtain the following plot:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 将这条线与对应于我们训练数据的散点一起绘制，我们得到以下图形：
- en: '![Figure 6.4 – The decision boundary of the learned model ](img/Figure_6.4_17499.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4 – 学习到的模型的决策边界](img/Figure_6.4_17499.jpg)'
- en: Figure 6.4 – The decision boundary of the learned model
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 学习到的模型的决策边界
- en: We see that our decision boundary nicely separates the two classes that we have.
    Now, when we'd like to make a prediction regarding which class an unseen point
    belongs to, we simply put its features through our `predict` function and look
    at the sign of the output.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们的决策边界很好地将我们拥有的两个类别分开。现在，当我们想要预测一个未见过的点属于哪个类别时，我们只需将其特征通过我们的`predict`函数，并查看输出的符号。
- en: 'And that is all there is to it! Of course, when we work with more complicated
    models, other more specialized considerations need to be made. However, the high-level
    process remains unchanged: designing a model and its loss function and then using
    gradient descent (or some other loss minimization strategy) to find the optimal
    combination of parameters. We have seen that using the automatic differentiation
    module in JAX, we were able to do this easily with minimal code while avoiding
    the hairy math that is usually involved with deriving the gradient of a loss function.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 就这么简单！当然，当我们处理更复杂的模型时，需要考虑更多更专业的问题。然而，高级过程保持不变：设计模型及其损失函数，然后使用梯度下降（或某些其他损失最小化策略）来找到参数的最优组合。我们已经看到，通过使用JAX的自动微分模块，我们能够以最少的代码轻松完成这项工作，同时避免了通常与推导损失函数梯度相关的复杂数学。
- en: With that said, the benefits that JAX offers don't stop there. In our next section,
    we will see how to make our current code more efficient by using JAX's internal
    **Just-In-Time** (**JIT**) **compiler**.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，JAX提供的优势并不止于此。在我们下一节中，我们将看到如何通过使用JAX的内部**即时编译器**（**JIT**）来使我们的当前代码更加高效。
- en: Just-In-Time compilation for improved efficiency
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时编译以提高效率
- en: As we have learned from the last chapter, JIT compilation allows a piece of
    code that is expected to run many times to be executed more efficiently. This
    process is specifically useful in machine learning where functions such as the
    loss or the gradient of the loss of a model need to be computed many times during
    the loss minimization phase. We hence expect that by leveraging a JIT compiler,
    we can make our machine learning models train faster.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中学到的，即时编译允许预期运行多次的代码以更高效的方式执行。这个过程在机器学习中特别有用，因为在损失最小化阶段，模型的损失或损失梯度需要被多次计算。因此，我们期望通过利用JIT编译器，我们可以使我们的机器学习模型训练更快。
- en: You might think that to do this, we would need to hook one of the JIT compilers
    we considered in the last chapter into JAX. However, JAX comes with its own JIT
    compiler, which requires minimal code to integrate in an existing program. We
    will see how to use it by modifying the training loop we made in the last section.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为了做到这一点，我们需要将我们在上一章中考虑的JIT编译器之一钩入JAX。然而，JAX自带了自己的JIT编译器，它需要最少的代码就可以集成到现有的程序中。我们将通过修改我们在上一节中制作的训练循环来展示如何使用它。
- en: 'First, we reset the ![](img/Formula_6.10_B17499.png) parameters of our models:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们重置我们模型的![](img/Formula_6.10_B17499.png)参数：
- en: '[PRE12]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, the way we will integrate the JIT compiler into our program is to point
    it to the gradient of our loss function. Remember that we can reap the benefits
    of JIT compilation by applying it to a piece of code that we expect to execute
    many times, which we have mentioned is exactly the case for the loss gradient.
    Recall that we were able to derive this gradient in the last section using the
    `grad` function transformation:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将JIT编译器集成到我们的程序中的方法是将它指向损失函数的梯度。记住，我们可以通过将JIT编译应用于我们预期会多次执行的一段代码来获得JIT编译的好处，这正是我们提到的损失梯度的情形。回想一下，我们是在上一节中通过使用`grad`函数转换来推导出这个梯度的：
- en: '[PRE13]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To obtain the JIT-compiled version of this function, we can follow a very similar
    process, in which we transform this loss gradient function using `jit`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得此函数的JIT编译版本，我们可以遵循一个非常类似的过程，在这个过程中，我们使用`jit`转换这个损失梯度函数：
- en: '[PRE14]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Amazingly, that is all we need to do. Now, if we run the same training loop
    we currently have, with this one line changed, we will obtain the same result,
    including the learning curve and the decision boundary we plotted earlier.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 惊人的是，我们只需要做这么多。现在，如果我们运行目前相同的训练循环，只更改这一行，我们将获得相同的结果，包括我们之前绘制的学习曲线和决策边界。
- en: 'However, upon inspecting the running time of the training loop, we will notice
    that the training loop is now much more efficient: within our Google Colab notebook,
    the speed goes from roughly 77 iterations to 175 iterations per second! Note that
    although these numbers might vary across different runs of the code, the improvement
    should stay apparent. While this speedup might not be meaningful to us and our
    simple linear model, it will prove useful for more heavyweight models such as
    neural networks, with millions to billions of parameters, which take weeks and
    months to train.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当我们检查训练循环的运行时间时，我们会注意到训练循环现在要高效得多：在我们的Google Colab笔记本中，速度从大约77次迭代每秒提高到175次迭代每秒！请注意，尽管这些数字可能在不同代码运行之间有所不同，但改进应该是明显的。虽然这种加速对我们和我们的简单线性模型可能没有意义，但它将证明对于更重的模型，如具有数百万到数十亿参数的神经网络，非常有用，这些模型需要数周甚至数月的时间来训练。
- en: Furthermore, this speedup was achieved with a single line of code changed using
    the function transformation, `jit`. This means that however complicated the functions
    you are working with are, if they are JAX-compatible, you can simply pass them
    through `jit` to enjoy the benefits of JIT compilation.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这种加速是通过更改单行代码并使用函数转换`jit`实现的。这意味着无论你正在使用的函数有多复杂，如果它们与JAX兼容，你只需通过`jit`传递它们，就可以享受JIT编译的好处。
- en: 'At this point, we have explored two powerful techniques to accelerate our machine
    learning pipeline: automatic differentiation and JIT compilation. In the next
    section, we will look at a final valuable feature of JAX – automatic vectorization.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探索了两种强大的技术来加速我们的机器学习流程：自动微分和JIT编译。在下一节中，我们将探讨JAX的最后一个有价值的特性——自动向量化。
- en: Automatic vectorization for efficient kernels
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动向量化以高效内核
- en: You might remember from our discussions on NumPy that the library is efficient
    at applying numerical operations to all elements in an array or the elements along
    specific axes. By exploiting the fact that the same operation is to be applied
    to multiple elements, the library optimizes low-level code that performs the operation,
    making the computation much more efficient than doing the same thing via an iterative
    loop. This process is called **vectorization**.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，在我们的NumPy讨论中，这个库在将数值运算应用于数组中的所有元素或特定轴上的元素方面非常高效。通过利用相同的操作要应用于多个元素的事实，该库优化了执行该操作的底层代码，使得计算比通过迭代循环做同样的事情要高效得多。这个过程被称为**向量化**。
- en: When working with machine learning models, we would like to go through a procedure
    of vectorizing a specific function, rather than looping through an array or a
    matrix, to gain performance speedup. Vectorization is typically not easy to do
    and might involve clever tricks to rewrite the function that we'd like to vectorize
    into another form that admits vectorization easily.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当与机器学习模型一起工作时，我们希望通过将特定函数向量化来通过一个过程，而不是通过循环数组或矩阵来提高性能。向量化通常不容易实现，可能需要巧妙的方法来重写我们想要向量化函数的形式，使其易于向量化。
- en: JAX addresses this concern by providing a function transformation that automatically
    vectorizes a given function, even if the function is only designed to take in
    single-valued variables (which means traditionally, an iterative loop would be
    necessary). In this section, we will see how to use this feature by going through
    the process of kernelizing our predictive model. First, we need to briefly discuss
    what kernels are as well as their place in machine learning.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: JAX通过提供一种函数转换来解决这个担忧，该转换可以自动向量化给定的函数，即使该函数仅设计为接受单值变量（这意味着传统上，需要一个迭代循环）。在本节中，我们将通过核化我们的预测模型的过程来了解如何使用这个特性。首先，我们需要简要讨论什么是核以及它们在机器学习中的位置。
- en: Data that is not linearly separable
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非线性不可分的数据
- en: Remember that the predictive model we currently have assumes that the target
    we'd like to predict for can be expressed as a linear combination of the data
    features; the model is thus a linear one. This assumption is quite restrictive
    in practice, as data can present highly nonlinear and still meaningful patterns.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们目前拥有的预测模型假设我们想要预测的目标可以表示为数据特征的线性组合；因此，该模型是线性的。在实践中，这个假设相当限制性，因为数据可以呈现高度非线性且仍然有意义的模式。
- en: 'For example, here we use the `make_moons` function from the same `datasets`
    module of scikit-learn to generate another toy dataset and visualize it, again
    using a scatter plot:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在这里我们使用来自scikit-learn相同`datasets`模块的`make_moons`函数来生成另一个玩具数据集并可视化它，再次使用散点图：
- en: '[PRE15]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This will generate the following plot:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图表：
- en: '![Figure 6.5 – A linearly non-separable dataset ](img/Figure_6.5_17499.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – 一个线性不可分的数据集](img/Figure_6.5_17499.jpg)'
- en: Figure 6.5 – A linearly non-separable dataset
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 一个线性不可分的数据集
- en: 'Once again, this is a binary classification problem where we need to distinguish
    between the yellow and the dark blue points. As you can see, these data points
    are not linearly separable since, unlike our previous dataset, a straight line
    that perfectly separates the two classes here does not exist. In fact, if we were
    to try to fit a linear model to this data, we would obtain a model with a relatively
    high loss (roughly 0.7) and a decision boundary that is clearly unsuitable, as
    shown here:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这又是一个二元分类问题，我们需要区分黄色和深蓝色的点。正如你所见，这些数据点不是线性可分的，因为与我们的前一个数据集不同，这里不存在完美分离两个类别的直线。实际上，如果我们尝试将线性模型拟合到这些数据上，我们会得到一个相对较高的损失（大约0.7）和一个显然不合适的决策边界，如图所示：
- en: '![Figure 6.6 – Fitting a linear model on nonlinear data ](img/Figure_6.6_17499.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6 – 在非线性数据上拟合线性模型](img/Figure_6.6_17499.jpg)'
- en: Figure 6.6 – Fitting a linear model on nonlinear data
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 在非线性数据上拟合线性模型
- en: How, then, could we modify our current model so that it can handle nonlinear
    data? A typical solution in these kinds of situations is the kernel method in
    machine learning, which we will briefly discuss next.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何修改我们当前的模型，使其能够处理非线性数据呢？在这些情况下，一个典型的解决方案是机器学习中的核方法，我们将在下一节中简要讨论。
- en: The kernel method in machine learning
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习中的核方法
- en: Without going into much technical detail, the kernel method (sometimes referred
    to as the **kernel trick**) refers to the process of transforming a low-dimensional
    dataset, such as ours, that is nonlinear to higher dimensions, with the hope that
    in higher dimensions, linear hyperplanes that separate the data will exist.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 不深入技术细节，核方法（有时被称为**核技巧**）指的是将低维数据集（如我们的数据集）转换到更高维度的过程，希望在高维中，存在能够分离数据的线性超平面。
- en: To take a data point to higher dimensions, additional features are created from
    the features already included in the original data. In our running example, we
    are given the *x* and *y* coordinates of the data points; hence, our data is two-dimensional.
    A common way to compose more features in this case is to compute polynomials of
    these coordinates up to a degree, *d*. For instance, polynomials that are up to
    degree 2 for features *x* and *y* include *xy* and ![](img/Formula_6.42_B17499.png).
    The larger *d* is, the more expressiveness we will gain from these new features
    we are creating, but we will also incur a higher computational cost.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 要将数据点提升到更高维度，需要从原始数据中包含的特征创建额外的特征。在我们的运行示例中，我们给出了数据点的 *x* 和 *y* 坐标；因此，我们的数据是二维的。在这种情况下，一种常见的创建更多特征的方法是计算这些坐标的多项式，直到一个度数
    *d*。例如，对于特征 *x* 和 *y*，直到二次的多项式包括 *xy* 和 ![](img/Formula_6.42_B17499.png)。*d* 越大，我们从这些新创建的特征中获得的表现力就越高，但我们也需要承担更高的计算成本。
- en: Notice that these polynomial features are nonlinear, which motivates their use
    in helping us find a model that separates our nonlinear data. With these new,
    nonlinear features in hand, we will then fit our predictive model on this bigger,
    more expressive dataset we just engineered. Note that while the predictive model
    remains in the same form as the linear parameters ![](img/Formula_6.43_B17499.png),
    the data features that the model learns from are nonlinear, so the resulting decision
    boundary will also be nonlinear.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到这些多项式特征是非线性的，这促使我们使用它们来帮助我们找到一个可以分离非线性数据的模型。有了这些新的非线性特征，我们将在这个我们刚刚构建的更大、更具表达力的数据集上拟合我们的预测模型。请注意，虽然预测模型的形式与线性参数
    ![](img/Formula_6.43_B17499.png) 相同，但模型学习的数据特征是非线性的，因此得到的决策边界也将是非线性的。
- en: I mentioned earlier that the more expressive we want the features we are synthetically
    creating to be, the more computational costs we will incur. Here, kernels are
    a way to manipulate the data points so that we can interact with them in higher
    dimensions without having to explicitly compute the high-dimensional features.
    In essence, a kernel is simply a function that takes in a pair of data points
    and returns the inner product of some transformation of the feature vectors. This
    is just to say that it returns a matrix that represents the two data points in
    higher dimensions.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前提到，我们希望合成的特征越有表现力，我们将承担的计算成本就越高。在这里，核是操纵数据点的一种方法，这样我们就可以在不显式计算高维特征的情况下以更高维度与之交互。本质上，核只是一个函数，它接受一对数据点并返回特征向量变换的内积。这只是为了说明它返回一个矩阵，该矩阵表示更高维度的两个数据点。
- en: 'In this chapter, we will use one of the most popular kernels in machine learning:
    the radial basis function, or RBF, kernel, which is defined as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用机器学习中最受欢迎的核之一：径向基函数（RBF）核，其定义如下：
- en: '![](img/Formula_6.44_B17499.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![公式 6.44_B17499](img/Formula_6.44_B17499.png)'
- en: where *x* and *x'* are the low-dimensional feature vectors of the two data points,
    and *l* is commonly referred to as the length scale, a tunable parameter of the
    kernel. There are many benefits to using the RBF kernel that we won't be going
    into in much detail here, but on the highest level, RBF can be regarded as an
    infinite-dimensional kernel, which means that it will provide us with a high level
    of expressiveness in the features we are computing.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *x* 和 *x'* 是两个数据点的低维特征向量，*l* 通常被称为长度尺度，是核的可调参数。使用 RBF 核有许多好处，我们在这里不会详细讨论，但总的来说，RBF
    可以被视为一个无限维核，这意味着它将为我们计算的特征提供高水平的表达能力。
- en: 'The new model using this kernel method, modified from our previous linear model,
    now makes the following assumption for each pair of feature vector *x* and label
    *y*:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此核方法的新模型，经过对我们先前线性模型的修改，现在对每个特征向量 *x* 和标签 *y* 的每一对都做出以下假设：
- en: '![](img/Formula_6.45_B17499.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![公式 6.45_B17499](img/Formula_6.45_B17499.png)'
- en: 'where ![](img/Formula_6.46_B17499.png) is the *i*-th data point in the training
    set and ![](img/Formula_6.47_B17499.png) are the model parameters to be optimized.
    While this assumption is written to contrast the linear assumption that we had
    earlier:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/Formula_6.46_B17499.png) 是训练集中的第 *i* 个数据点，![](img/Formula_6.47_B17499.png)
    是要优化的模型参数。虽然这个假设是为了与之前我们提到的线性假设进行对比：
- en: '![](img/Formula_6.48_B17499.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![公式 6.48_B17499](img/Formula_6.48_B17499.png)'
- en: '![](img/Formula_6.49_B17499.png) now refers to the entire feature vector of
    a data point, not the *i*-th feature of a data point. Further, we now have *n*
    terms of ![](img/Formula_6.50_B17499.png), as opposed to 2 (or *d*) terms of ![](img/Formula_6.51_B17499.png)
    as before.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '![公式 6.49_B17499](img/Formula_6.49_B17499.png)现在指的是数据点的整个特征向量，而不是数据点的*i*-th特征。此外，我们现在有*n*个![公式
    6.50_B17499](img/Formula_6.50_B17499.png)项，而不是之前的2（或*d*）个![公式 6.51_B17499](img/Formula_6.51_B17499.png)项。'
- en: We see the role of the kernel here is to transform the feature of *x* by computing
    the inner product of its features and those of each of the data points in the
    training set. The output of the kernel is then used as the new features to be
    multiplied by the model parameters ![](img/Formula_6.52_B17499.png), whose values
    will be optimized as part of the training process.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到核在这里的作用是通过计算其特征与训练集中每个数据点的特征的点积来转换*x*的特征。核的输出随后被用作新的特征，乘以模型参数![公式 6.52_B17499](img/Formula_6.52_B17499.png)，其值将在训练过程中作为一部分进行优化。
- en: 'Let''s now implement this kernel in our code:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来实现代码中的这个核函数：
- en: '[PRE16]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here we are setting the length scale at 0.3, but feel free to play around with
    this value and observe its effect on the learned model afterward. The `rbf_kernel`
    function takes in `x` and `z`, two given data points. The code inside the function
    is self-explanatory: we compute the squared norm of the difference between `x`
    and `z` (`linalg.norm` in JAX follows the same API as in NumPy), divide it by
    `lengthscale`, and use its negative as the input of the natural exponential function,
    `exp`.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将长度尺度设置为0.3，但你可以随意调整这个值，并观察其对学习模型的影响。`rbf_kernel`函数接受`x`和`z`两个给定的数据点。函数内部的代码是自我解释的：我们计算`x`和`z`之间差异的平方范数（JAX中的`linalg.norm`遵循与NumPy相同的API），然后除以`lengthscale`，并使用其负值作为自然指数函数`exp`的输入。
- en: Now, we need to implement the new version of the `predict` function for our
    kernelized model, which we will see how to do in the next subsection.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要为我们的核化模型实现新的`predict`函数版本，我们将在下一小节中看到如何实现。
- en: Automatic vectorization for kernelized models
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 核化模型的自动向量化
- en: 'You might remember that the job of the `prediction` function is to implement
    the assumption our model is making, which in this case is:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，`prediction`函数的职责是实现模型所做出的假设，在这个例子中是：
- en: '![](img/Formula_6.53_B17499.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![公式 6.53_B17499](img/Formula_6.53_B17499.png)'
- en: The challenge here is in the sum ![](img/Formula_6.54_B17499.png), where ![](img/Formula_6.55_B17499.png),
    ![](img/Formula_6.56_B17499.png), …, ![](img/Formula_6.57_B17499.png) are the
    individual data points in our training set. In a naïve implementation, we would
    iterate through each data point ![](img/Formula_6.58_B17499.png), compute ![](img/Formula_6.59_B17499.png),
    multiply it by ![](img/Formula_6.60_B17499.png), and finally, add these terms
    together. Computing the inner product of two vectors is generally a computationally
    expensive operation, so repeating this computation *n* times would be prohibitively
    costly.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的挑战在于求和![公式 6.54_B17499](img/Formula_6.54_B17499.png)，其中![公式 6.55_B17499](img/Formula_6.55_B17499.png)，![公式
    6.56_B17499](img/Formula_6.56_B17499.png)，…，![公式 6.57_B17499](img/Formula_6.57_B17499.png)是我们训练集中的各个数据点。在一种天真实现中，我们会遍历每个数据点![公式
    6.58_B17499](img/Formula_6.58_B17499.png)，计算![公式 6.59_B17499](img/Formula_6.59_B17499.png)，乘以![公式
    6.60_B17499](img/Formula_6.60_B17499.png)，最后将这些项相加。计算两个向量的内积通常是一个计算成本高昂的操作，因此重复这个计算*n*次将会非常昂贵。
- en: 'To make this kernelized model more practical, we''d like to *vectorize* this
    procedure. Normally, this would require reimplementing our kernel function using
    various tricks to enable vectorization. However, with JAX, we can effortlessly
    obtain the vectorized version of the `rbf_kernel` function using the `vmap` transformation:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个核化模型更加实用，我们希望对这个过程进行*向量化*。通常，这需要使用各种技巧重新实现我们的核函数以实现向量化。然而，使用JAX，我们可以轻松地使用`vmap`转换获得`rbf_kernel`函数的向量化版本：
- en: '[PRE17]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Don't let the last line of code intimidate you. Here we are simply getting the
    vectorized form of our kernel function along the first axis (remember the kernel
    has two inputs) with `vmap(kernel, (0, None)`, and then vectorizing that very
    vectorized-along-the-first-axis kernel along the second axis with `vmap(vmap(kernel,
    (0, None)), (None, 0))`. Finally, we derive the JIT-compiled version of the function
    with `jit`.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 不要让最后一行代码吓到你。这里我们只是简单地通过`vmap(kernel, (0, None))`获取我们的核函数在第一个轴上的向量化形式（记住核函数有两个输入），然后通过`vmap(vmap(kernel,
    (0, None)), (None, 0))`将这个沿着第一个轴向量化了的向量再次沿着第二个轴进行向量化。最后，我们使用`jit`推导出函数的即时编译版本。
- en: 'Aside from being a concise one-liner, this code perfectly illustrates why function
    transformation (JAX''s design choice) is so useful: we can compose different function
    transformations in a nested way, repeatedly calling a transformation on the output
    of another transformation; in this case, we have the JIT-compiled function of
    a twice-vectorized function.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 除了是一个简洁的一行代码之外，这段代码完美地说明了为什么函数转换（JAX的设计选择）如此有用：我们可以以嵌套的方式组合不同的函数转换，反复调用一个转换的输出；在这种情况下，我们有两次向量化函数的JIT编译函数。
- en: 'With this vectorized kernel function composed, we are now ready to implement
    the corresponding `predict` and `loss` functions:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 组合了这个向量化核函数后，我们现在可以实施相应的`predict`和`loss`函数：
- en: '[PRE18]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Of note here is the `predict` function, where we are calling `vec_kernel(X,
    X_test)` to compute the vector of ![](img/Formula_6.61_B17499.png) via vectorization.
    Remember that without `vmap`, we would need to either iterate through the individual
    data points (for example, using a `for` loop), which is specifically more inefficient,
    or rewrite our `rbf_kernel` function so that the function itself facilitates the
    vectorization. In the end, we simply compute the dot product between the returned
    output and the vector of the parameters we'd like to optimize, `alphas`.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这里值得注意的是`predict`函数，我们在这里调用`vec_kernel(X, X_test)`来通过向量化计算![](img/Formula_6.61_B17499.png)的向量。记住，如果没有`vmap`，我们可能需要遍历单个数据点（例如，使用`for`循环），这特别低效，或者重新编写我们的`rbf_kernel`函数，以便函数本身促进向量化。最终，我们只是计算返回的输出与我们要优化的参数向量`alphas`之间的点积。
- en: 'Our loss function stays the same. We now can apply the same training procedure
    as we have before, this time on the `alphas` variable:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的损失函数保持不变。我们现在可以应用之前相同的训练过程，这次是在`alphas`变量上：
- en: '[PRE19]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can also plot out the progression of the loss throughout this training:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以绘制出整个训练过程中的损失变化：
- en: '![Figure 6.7 – The decrease in loss of the kernelized model ](img/Figure_6.7_17499.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7 – 核化模型的损失下降](img/Figure_6.7_17499.jpg)'
- en: Figure 6.7 – The decrease in loss of the kernelized model
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – 核化模型的损失下降
- en: We see a steady decrease in loss, indicating that the model is fitting its parameters
    to the data. In the end, our loss is well below `0.7`, which was the loss of the
    purely linear model we previously had.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到损失持续下降，这表明模型正在将其参数拟合到数据上。最终，我们的损失远低于`0.7`，这是我们之前纯线性模型的损失。
- en: 'Finally, we''d like to visualize what our model has learned. This is a bit
    more involved as we no longer have a nice linear equation that we can translate
    into a line as before. Instead, we visualize the predictions made by the model
    itself. In the following figure, we show these predictions made on a fine grid
    that spans across our training data points, where the colors show the predicted
    classes, and the hues show the confidence in the predictions:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们想可视化我们的模型学到了什么。这有点复杂，因为我们不再有一个漂亮的线性方程可以像以前那样转换成一条线。相反，我们可视化模型本身做出的预测。在下面的图中，我们展示了在细网格上做出的这些预测，该网格跨越我们的训练数据点，其中颜色表示预测的类别，色调表示预测的置信度：
- en: '![Figure 6.8 – Predictions made by the learned kernelized model ](img/Figure_6.8_17499.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8 – 学习到的核化模型做出的预测](img/Figure_6.8_17499.jpg)'
- en: Figure 6.8 – Predictions made by the learned kernelized model
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – 由学习到的核化模型做出的预测
- en: We see that the model was able to identify the nonlinear pattern in our data,
    indicated by the yellow and dark blue blobs lying right in the center of the moons.
    As such, we have successfully modified our predictive model to learn from nonlinear
    data, which was done by using a kernel to create nonlinear features and JAX's
    automatic vectorization transformation to accelerate this computation.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到模型能够识别出我们数据中的非线性模式，这由月亮中心的黄色和深蓝色块表示。因此，我们已经成功修改了我们的预测模型，使其能够从非线性数据中学习，这是通过使用核来创建非线性特征和JAX的自动向量化转换来加速这一计算实现的。
- en: 'Here, the speed we achieve is roughly 160 iterations per second. To once again
    see how big a speedup the JIT compiler offers our program, we remove the `jit()`
    function calls from the implementation of our model and rerun the code with the
    following changes:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们达到的速度大约是每秒160次迭代。为了再次看到JIT编译器为我们程序提供的速度提升有多大，我们从我们模型的实现中移除`jit()`函数调用，并使用以下更改重新运行代码：
- en: '[PRE20]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This time, the speedup is even more impressive: without JIT, the speed drops
    to 47 iterations per second.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，速度提升更加令人印象深刻：没有JIT，速度下降到每秒47次迭代。
- en: 'This concludes our discussion on JAX and some of its main features. However,
    there are other more advanced tools included in JAX that we didn''t cover but
    could prove useful in your machine learning applications such as asynchronous
    dispatch, parallelization, or computing convolutions. Information on these features
    may be found on the documentation page: [https://jax.readthedocs.io/en/latest/](https://jax.readthedocs.io/en/latest/).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们对JAX及其主要特性的讨论结束。然而，JAX中还包括其他一些更高级的工具，我们没有涉及，但它们可能在你的机器学习应用中非常有用，例如异步调度、并行化或计算卷积。有关这些功能的信息可以在文档页面上找到：[https://jax.readthedocs.io/en/latest/](https://jax.readthedocs.io/en/latest/)。
- en: Summary
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: JAX is a Python- and NumPy-friendly library that offers high-performance tools
    that are specific to machine learning tasks. JAX centers its API around function
    transformations, allowing users, in one line of code, to pass in generic Python
    functions and receive transformed versions of the functions that would otherwise
    either be expensive to compute or require more advanced implementations. The syntax
    of function transformations also enables flexible and complex compositions of
    functions, which are common in machine learning.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: JAX是一个Python和NumPy友好的库，它提供针对机器学习任务的高性能工具。JAX将它的API集中在函数转换上，使用户能够在一行代码中传入通用的Python函数，并接收这些函数的转换版本，否则这些函数可能计算成本高昂或需要更高级的实现。函数转换的语法还使得灵活和复杂的函数组合成为可能，这在机器学习中很常见。
- en: Throughout this chapter, we have seen how to utilize JAX to compute the gradient
    of machine learning loss functions using automatic differentiation, JIT-compile
    our code for further optimization, and vectorize kernel functions via a binary
    classification example. However, these tasks are present in most use cases, and
    you will be able to seamlessly apply what we have discussed here to your own machine
    learning needs.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了如何利用JAX通过自动微分计算机器学习损失函数的梯度，通过JIT编译我们的代码以进行进一步优化，并通过二分类示例将核函数向量化。然而，这些任务在大多数用例中都是存在的，你将能够无缝地将我们在这里讨论的内容应用到你的机器学习需求中。
- en: At this point, we have reached the end of the first part of this book, in which
    we discuss Python-native and various other techniques to accelerate our Python
    applications. In the second part of the book, we examine parallel and concurrent
    programming, the techniques of which allow us to distribute computational loads
    across multiple threads and processes, making our applications more efficient
    on a linear scale.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经到达了本书第一部分的结尾，在这一部分中，我们讨论了Python原生和多种其他技术来加速我们的Python应用程序。在书的第二部分，我们将探讨并行和并发编程，这些技术允许我们将计算负载分配到多个线程和进程中，使我们的应用程序在线性尺度上更加高效。
- en: Questions
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What are the main components of a machine learning pipeline?
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器学习管道的主要组成部分是什么？
- en: How is the loss function of a machine learning model typically minimized and
    how does JAX help with this process?
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器学习模型的损失函数通常是如何最小化的，以及JAX是如何帮助这一过程的？
- en: How can the predictive model used in this chapter handle nonlinear data and
    how does JAX help with this process?
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本章中使用的预测模型如何处理非线性数据，以及JAX是如何帮助这一过程的？
- en: Further reading
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Linear models in machine learning: [https://blog.dataiku.com/top-machine-learning-algorithms-how-they-work-in-plain-english-1](https://blog.dataiku.com/top-machine-learning-algorithms-how-they-work-in-plain-english-1)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习中的线性模型：[https://blog.dataiku.com/top-machine-learning-algorithms-how-they-work-in-plain-english-1](https://blog.dataiku.com/top-machine-learning-algorithms-how-they-work-in-plain-english-1)
- en: 'The JAX ecosystem: [https://moocaholic.medium.com/jax-a13e83f49897](https://moocaholic.medium.com/jax-a13e83f49897)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JAX生态系统：[https://moocaholic.medium.com/jax-a13e83f49897](https://moocaholic.medium.com/jax-a13e83f49897)
- en: 'A brief tutorial that further explores JAX: [https://colinraffel.com/blog/you-don-t-know-jax.html](https://colinraffel.com/blog/you-don-t-know-jax.html)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个进一步探索JAX的简要教程：[https://colinraffel.com/blog/you-don-t-know-jax.html](https://colinraffel.com/blog/you-don-t-know-jax.html)
