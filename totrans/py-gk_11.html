<html><head></head><body>
		<div><h1 id="_idParaDest-189"><em class="italic"><a id="_idTextAnchor227"/>Chapter 8</em>: Scaling out Python Using Clusters</h1>
			<p>In the previous chapter, we discussed parallel processing for a single machine using threads and processes. In this chapter, we will extend our discussion of parallel processing from a single machine to multiple machines in a cluster. A cluster is a group of computing devices that work together to perform compute-intensive tasks such as data processing. In particular, we will study Python's capabilities in the area of data-intensive computing. Data-intensive computing typically uses clusters for processing large volumes of data in parallel. Although there are quite a few frameworks and tools available for data-intensive computing, we will focus on <strong class="bold">Apache Spark</strong> as a data processing engine and PySpark as a Python library to build such applications. </p>
			<p>If Apache Spark with Python is properly configured and implemented, the performance of your application can increase manyfold and surpass competitor platforms such as <strong class="bold">Hadoop MapReduce</strong>. We will also look into how distributed datasets are utilized in a clustered environment. This chapter will help you to understand the use of cluster computing platforms for large-scale data processing and how to implement data processing applications using Python. To illustrate the practical use of Python for applications with cluster computing requirements, we will include two case studies; the first one is to compute the value of Pi (π) and the second one is to generate a word cloud from a data file.</p>
			<p>We will cover the following topics in this chapter:</p>
			<ul>
				<li>Learning about the cluster options for parallel processing </li>
				<li>Introducing <strong class="bold">resilient distributed datasets</strong> (<strong class="bold">RDD</strong>) </li>
				<li>Using PySpark for parallel data processing</li>
				<li>Case studies of using Apache Spark and PySpark</li>
			</ul>
			<p>By the end of this chapter, you will know how to work with Apache Spark and how you can write Python applications for data processing that can be executed on the worker nodes of an Apache Spark cluster.</p>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor228"/>Technical requirements</h1>
			<p>The following are the technical requirements for this chapter:</p>
			<ul>
				<li>Python 3.7 or later installed on your computer</li>
				<li>An Apache Spark single-node cluster</li>
				<li>PySpark installed on top of Python 3.7 or later for driver program development<p class="callout-heading">Note</p><p class="callout">The Python version used with Apache Spark has to match the Python version that is used to run the driver program.</p></li>
			</ul>
			<p>The sample code for this chapter can be found at <a href="https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter08">https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter08</a>.</p>
			<p>We will start our discussion by looking at the cluster options available for parallel processing in general.</p>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor229"/>Learning about the cluster options for parallel processing </h1>
			<p>When <a id="_idIndexMarker832"/>we have a large volume of data to process, it is not<a id="_idIndexMarker833"/> efficient and sometimes even not feasible to use a single machine with multiple cores to process the data efficiently. This is especially a challenge when working with real-time streaming data. For such scenarios, we need multiple systems that can process data in a distributed manner and perform these tasks on multiple machines in parallel. Using multiple machines to process compute-intensive tasks in parallel and in a distributed manner is called <strong class="bold">cluster computing</strong>. There<a id="_idIndexMarker834"/> are several big data distributed frameworks available to coordinate the execution of jobs in a cluster, but Hadoop MapReduce and Apache Spark are the leading contenders in this race. Both frameworks are open source projects from Apache. There are many variants (for example, Databricks) of these two platforms available with add-on features as well as maintenance support, but the fundamentals remain the same.  </p>
			<p>If we look at the market, the number of Hadoop MapReduce deployments may be higher compared to Apache Spark, but with its increasing popularity, Apache Spark is going to turn the tables <a id="_idIndexMarker835"/>eventually. Since Hadoop MapReduce is still <a id="_idIndexMarker836"/>very relevant due to its large install base, it is important to discuss what exactly Hadoop MapReduce is and how Apache Spark is becoming a better choice. Let's have a quick overview of the two in the next subsections.</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor230"/>Hadoop MapReduce</h2>
			<p>Hadoop<a id="_idIndexMarker837"/> is a general-purpose distributed processing framework that offers the execution of large-scale data processing jobs across hundreds or thousands of computing nodes in a Hadoop cluster. The three core components<a id="_idIndexMarker838"/> of Hadoop are included in the following figure:</p>
			<div><div><img src="img/B17189_08_01.jpg" alt="Figure 8.1 – Apache Hadoop MapReduce ecosystem &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Apache Hadoop MapReduce ecosystem </p>
			<p>The three core<a id="_idIndexMarker839"/> components are described as follows:</p>
			<ul>
				<li><strong class="bold">Hadoop Distributed File System (HDFS)</strong>: This<a id="_idIndexMarker840"/> is a Hadoop-native filesystem used to stores files such that those files can be parallelized across a cluster.</li>
				<li><strong class="bold">Yet Another Resource Negotiator (YARN)</strong>: This is a system that processes data stored in HDFS and schedules<a id="_idIndexMarker841"/> the submitted jobs (for data processing) to be run by a processing system. The processing systems can be used for graph processing, stream processing, or batch processing.</li>
				<li><code>map</code>) and reducer (<code>reduce</code>) functions are the same as we discussed in <a href="B17189_06_Final_PG_ePub.xhtml#_idTextAnchor188"><em class="italic">Chapter 6</em></a>, <em class="italic">Advanced Tips and Tricks in Python</em>. The key difference is that we use many <code>map</code> and <code>reduce</code> functions in parallel to process several datasets at the same time. <p>After breaking the large dataset into small datasets, we can provide the small datasets as input to many mapper functions for processing on different nodes of a Hadoop cluster. Each mapper function takes one set of data as an input, processes the data based on the goal set by the programmer, and produces the output as key-value pairs. Once the output of all the small datasets is available, one or multiple reducer functions will take the output from the mapper functions and aggregate the results as per the goals of the reducer functions. </p><p>To explain it in a bit more detail, we can take an example of counting particular words such as <em class="italic">attack</em> and <em class="italic">weapon</em> in a large source of text data. The text data can be divided into small datasets, for example, eight datasets. We can have eight mapper functions that count the two words within the dataset provided to them. Each mapper function provides us with the count of the words <em class="italic">attack</em> and <em class="italic">weapon</em> as an output for the dataset provided to it. In the next phase, the outputs of all the mapper functions are provided to two reducer functions, one for each word. Each reducer function aggregates the count for each word and provides the aggregated results as an output. The operating of the MapReduce framework for this word count example is shown next. Note that the mapper function is typically implemented as <code>map</code> and the reducer function as <code>reduce</code> in Python programming:</p></li>
			</ul>
			<div><div><img src="img/B17189_08_02.jpg" alt="Figure 8.2 – Working of the MapReduce framework&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – Working of the MapReduce framework</p>
			<p>We will skip the next levels of Hadoop components as they are not relevant to our discussion in this chapter. Hadoop is built mainly in Java, but any programming language, such as Python, can be used to write custom mapper and reducer components for the MapReduce module. </p>
			<p>Hadoop MapReduce<a id="_idIndexMarker846"/> is good for processing a large chunk of data by breaking it into small blocks. The cluster nodes process these blocks separately and then the results are aggregated before being sent to the requester. Hadoop MapReduce processes the data from a filesystem and thus is not very efficient in terms of performance. However, it works very well if the speed of processing is not a critical requirement, for instance, if data processing can be scheduled to occur at night. </p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor231"/>Apache Spark</h2>
			<p>Apache Spark<a id="_idIndexMarker847"/> is an open source cluster computing framework for real-time as well as batch data processing. The main feature of Apache Spark is that it is an in-memory data processing framework, which makes it efficient in terms of achieving low latency and makes it suitable for many real-world scenarios because of the following additional factors:</p>
			<ul>
				<li>It gets results quickly for mission-critical and time-sensitive applications such as real-time or near real-time scenarios.</li>
				<li>It's good for performing tasks repeatedly or iteratively in an efficient way due to in-memory processing.</li>
				<li>You can utilize out-of-the-box machine learning algorithms. </li>
				<li>You can leverage the support of additional programming languages such as Java, Python, Scala, and R.</li>
			</ul>
			<p>In fact, Apache Spark <a id="_idIndexMarker848"/>covers a wide range of workloads, including batch data, iterative processing, and streaming data. The beauty of Apache Spark is that it can use Hadoop (via YARN) as a deployment cluster as well, but it has its own cluster manager as well. </p>
			<p>At a high level, the main components of Apache Spark are segregated into three layers, as shown in the following figure:</p>
			<div><div><img src="img/B17189_08_03.jpg" alt="Figure 8.3 – Apache Spark ecosystem&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – Apache Spark ecosystem</p>
			<p>These layers are discussed next.</p>
			<h3>Support languages</h3>
			<p>Scala is a<a id="_idIndexMarker849"/> native language of Apache Spark, so it is quite popular for development. Apache Spark <a id="_idIndexMarker850"/>also provides high-level APIs for Java, Python, and R. In Apache Spark, multi-language support is provided by using the <strong class="bold">Remote Procedure Call</strong> (<strong class="bold">RPC</strong>) interface. There is an <a id="_idIndexMarker851"/>RPC adapter written for each language in Scala that transforms the client requests written in a different language to the native Scala requests. This makes its adoption easier across the development community. </p>
			<h3>Core components</h3>
			<p>A brief overview of each of the<a id="_idIndexMarker852"/> core components is discussed next:</p>
			<ul>
				<li><strong class="bold">Spark Core and RDDs</strong>: Spark Core <a id="_idIndexMarker853"/>is a core engine of Spark and is responsible for providing abstraction to RDDs, scheduling and distributing jobs to a cluster, interacting<a id="_idIndexMarker854"/> with storage systems such as HDFS, Amazon S3, or an RDBMS, and managing memory and fault recoveries. An RDD is a resilient distributed dataset that is an immutable and distributable collection of data. RDDs are partitioned to be executed on the different nodes of a cluster. We will discuss RDDs in more detail in the next section.</li>
				<li><strong class="bold">Spark SQL</strong>: This<a id="_idIndexMarker855"/> module is for querying data stored both in RDDs and in external data sources using abstracted interfaces. Using these common interfaces enables the developers to mix the SQL commands with the analytics tools for a given application. </li>
				<li><strong class="bold">Spark Streaming</strong>: This <a id="_idIndexMarker856"/>module is used to process real-time data, which is critical to analyze live data streams with low latency.</li>
				<li><strong class="bold">MLlib</strong>: The <strong class="bold">Machine Learning Library</strong> (<strong class="bold">MLlib</strong>) is used<a id="_idIndexMarker857"/> to apply machine learning algorithms in Apache Spark.</li>
				<li><strong class="bold">GraphX</strong>: This <a id="_idIndexMarker858"/>module provides an API for graph-based parallel computing. This module comes with a variety of graph algorithms. Note that a graph is a mathematical concept based on vertices and edges that represents how a set of objects are related or dependent on each other. The objects are represented by vertices and their relationships by the edges. </li>
			</ul>
			<h3>Cluster management</h3>
			<p>Apache Spark<a id="_idIndexMarker859"/> supports a few cluster managers, such as Standalone, Mesos, YARN, and Kubernetes. The key function of a cluster manager is to schedule and execute the jobs on cluster nodes and manage the resources on cluster nodes. But to interact with one or more cluster managers, there is a special object used in the main or driver program <a id="_idIndexMarker860"/>called <code>SparkContext</code> object was considered as an entry point, but its API is <a id="_idIndexMarker861"/>now wrapped as part of the <code>SparkSession</code> object. Conceptually, the following figure <a id="_idIndexMarker862"/>shows the interaction of a <code>SparkSession</code> (<strong class="bold">SparkContext</strong>), and the <strong class="bold">worker nodes</strong> in a <a id="_idIndexMarker863"/>cluster:</p>
			<div><div><img src="img/B17189_08_04.jpg" alt="Figure 8.4 – Apache Spark ecosystem"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.4 – Apache Spark ecosystem</p>
			<p>The <code>SparkSession</code> object can connect to different types of cluster managers. Once connected, the executors are acquired on the cluster nodes through the cluster managers. The executors are the Spark processes that run the jobs and store the computational job results. The cluster manager on a master node is responsible for sending the application code to the executor processes on the worker nodes. Once the application code and data (if applicable) are moved to the worker nodes, the <code>SparkSession</code> object in a driver program interacts directly with executor processes for the execution of tasks. </p>
			<p>As per Apache Spark release 3.1, the following cluster managers are supported:</p>
			<ul>
				<li><strong class="bold">Standalone</strong>: This<a id="_idIndexMarker864"/> is a simple cluster manager that is included as part of the Spark Core engine. The Standalone cluster is based on master and worker (or slave) processes. A master process is basically a cluster manager, and the worker processes host the executors. Although the masters and the workers can be hosted on a single machine, this is not the real deployment scenario of a Spark Standalone cluster. It is recommended to distribute workers to different machines for the best outcome. The Standalone cluster is easy to set up and provides most of the features required from a cluster. </li>
				<li><strong class="bold">Apache Mesos</strong>: This is<a id="_idIndexMarker865"/> another general-purpose cluster manager that can also run Hadoop MapReduce. For large-scale cluster environments, Apache Mesos is the <a id="_idIndexMarker866"/>preferred option. The idea of this cluster manager is that it aggregates the physical resources into a single virtual resource that acts as a cluster and provides a node-level abstraction. It is a distributed cluster manager by design. </li>
				<li><strong class="bold">Hadoop YARN</strong>: This <a id="_idIndexMarker867"/>cluster manager is <a id="_idIndexMarker868"/>specific to Hadoop. This is also a distributed framework by nature.</li>
				<li><strong class="bold">Kubernetes</strong>: This is more<a id="_idIndexMarker869"/> in the experimental phase. The purpose of this cluster manager is to automate the deployment and scaling of the containerized applications. The latest<a id="_idIndexMarker870"/> release of Apache Spark includes the Kubernetes scheduler. </li>
			</ul>
			<p>Before concluding this section, it is worth mentioning another framework, <strong class="bold">Dask</strong>, which is an <a id="_idIndexMarker871"/>open source library written in Python for parallel computing. The Dask framework works directly with distributed hardware platforms such as Hadoop. The Dask framework utilizes industry-proven libraries and Python projects such as NumPy, pandas, and scikit-learn. Dask is <a id="_idIndexMarker872"/>a small and lightweight framework compared to Apache Spark and can handle small to medium-sized clusters. In comparison, Apache Spark supports multiple languages and is the most appropriate choice for large-scale clusters. </p>
			<p>After introducing the cluster options for parallel computing, we will discuss in the next section the core data structure of Apache Spark, which is the RDD. </p>
			<h1 id="_idParaDest-194"><a id="_idTextAnchor232"/>Introducing RDDs </h1>
			<p>The RDD <a id="_idIndexMarker873"/>is the core data structure in Apache Spark. This data structure is not only a distributed collection of objects but is also partitioned in such a way that each dataset can be processed and computed on different nodes of a cluster. This makes the RDD a core element of distributed data processing. Moreover, an RDD object is resilient in the sense that it is fault-tolerant and the framework can rebuild the data in the case of a failure. When we create an RDD object, the master node replicates the RDD object to multiple executors or worker nodes. If any executor process or worker node fails, the master node detects the failure and enables an executor process on another node to take over the execution. The new executor node will already have a copy of the RDD object, and it can start the execution immediately. Any data processed by the original executor node before failing will be lost data that will be computed again by the new executor node. </p>
			<p>In the next subsections, we will learn about two key RDD operations and how to create RDD objects from different data sources. </p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor233"/>Learning RDD operations</h2>
			<p>An RDD is an immutable object, which <a id="_idIndexMarker874"/>means once it is created, it cannot be altered. But two types of operations can be performed on the data of an RDD. These are <strong class="bold">transformations</strong> and <strong class="bold">actions</strong>. These operations are described next. </p>
			<h3>Transformations</h3>
			<p>These operations are<a id="_idIndexMarker875"/> applied on an RDD object and result in creating a new RDD object. This type of operation takes an RDD as input and produces one or more RDDs as an output. We also need to remember that these transformations are lazy in nature. This means they will only be executed when an action is triggered on them, which is another type of operation. To explain the concept of lazy evaluation, we can assume that we are transforming numeric data in an RDD by subtracting 1 from each element and then adding arithmetically (the action) all elements to the output RDD from the transformation step. Because of the lazy evaluation, the transformation operation will not happen until we call the action operation (the addition, in this case).</p>
			<p>There are several built-in transformation functions available with Apache Spark. The commonly used transformation functions are as follows:</p>
			<ul>
				<li><code>map</code>: The <code>map</code> function <a id="_idIndexMarker876"/>iterates every element or line of an RDD object and applies the defined <code>map</code> function for each element.</li>
				<li><code>filter</code>: This<a id="_idIndexMarker877"/> function will filter the data from the original RDD and provide a new RDD with the filtered results.<strong class="bold"> </strong></li>
				<li><code>union</code>: This function is applied to two <a id="_idIndexMarker878"/>RDDs if they are of the same type and results in producing another RDD that is a union of the input RDDs.</li>
			</ul>
			<h3>Actions</h3>
			<p>Actions are <a id="_idIndexMarker879"/>computational operations applied on an RDD and the results of such operations are to be returned to the driver program (for example, <code>SparkSession</code>). There are several built-in action functions available with Apache Spark. The commonly used action functions are as follows:</p>
			<ul>
				<li><code>count</code>: The <code>count</code> action <a id="_idIndexMarker880"/>returns the number of elements in an RDD.</li>
				<li><code>collect</code>: This<a id="_idIndexMarker881"/> action returns the entire RDD to the driver program.<strong class="bold"> </strong></li>
				<li><code>reduce</code>: This action will reduce<a id="_idIndexMarker882"/> the elements from an RDD. A simple example is an addition operation on an RDD dataset.<strong class="bold"> </strong></li>
			</ul>
			<p>For a complete list of transformation and action functions, we suggest you check the official documentation of Apache Spark. Next, we will study how to create RDDs. </p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor234"/>Creating RDD objects</h2>
			<p>There are three main <a id="_idIndexMarker883"/>approaches to create RDD objects, which are described next.</p>
			<h3>Parallelizing a collection</h3>
			<p>This is one of the<a id="_idIndexMarker884"/> more simple approaches used in Apache Spark to create RDDs. In this approach, a collection is created or loaded into a program and then passed to the <code>parallelize</code> method of the <code>SparkContext</code> object. This approach is not used beyond development and testing. This is because it requires an entire dataset to be available on one machine, which is not convenient for a large amount of data.</p>
			<h3>External datasets</h3>
			<p>Apache Spark supports distributed <a id="_idIndexMarker885"/>datasets from a local filesystem, HDFS, HBase, or even Amazon S3. In this approach of creating RDDs, the data is loaded directly from an external data source. There are convenient methods available with the <code>SparkContext</code> object that can be used to load all sorts of data into RDDs. For example, the <code>textFile</code> method can be used to load text data from local or remote resources using an appropriate URL (for example, <code>file://</code>, <code>hdfs://</code>, or <code>s3n://</code>).</p>
			<h3>From existing RDDs</h3>
			<p>As discussed previously, RDDs<a id="_idIndexMarker886"/> can be created using transformation operations. This is one of the differentiators of Apache Spark from Hadoop MapReduce. The input RDD is not changed as it is an immutable object, but new RDDs can be created from existing RDDs. We have already seen some examples of how to create RDDs from existing RDDs using the <code>map</code> and <code>filter</code> functions.</p>
			<p>This concludes our introduction of RDDs. In the next section, we will provide further details with Python code examples using the PySpark library.</p>
			<h1 id="_idParaDest-197"><a id="_idTextAnchor235"/>Using PySpark for parallel data processing</h1>
			<p>As discussed previously, Apache Spark<a id="_idIndexMarker887"/> is written in Scala language, which<a id="_idIndexMarker888"/> means there is no native support for Python. There is a large community of data scientists and analytics experts who prefer to use Python for data processing because of the rich set of libraries available with Python. Hence, it is not convenient to switch to using another programming language only for distributed data processing. Thus, integrating Python with Apache Spark is not only beneficial for the data science community but also opens the doors for many others who would like to adopt Apache Spark without learning or switching to a new programming language. </p>
			<p>The Apache Spark community has built a Python library, <strong class="bold">PySpark</strong>, to facilitate working with Apache Spark using Python. To make the Python code work with Apache Spark, which is built on Scala (and Java), a Java library, <strong class="bold">Py4J</strong>, has been developed. This Py4J library is bundled with PySpark and allows<a id="_idIndexMarker889"/> the Python code to interact with JVM objects. This is the reason that when we install PySpark, we need to have JVM installed on our system first. </p>
			<p>PySpark offers almost the same features and advantages as Apache Spark. These include in-memory computation, the ability to parallelize workloads, the use of the lazy evaluation design pattern, and support for multiple cluster managers such as Spark, YARN, and Mesos. </p>
			<p>Installing PySpark (and Apache Spark) is beyond the scope of this chapter. The focus of this chapter is to discuss the use of PySpark to utilize the power of Apache Spark and not how to install Apache Spark and PySpark. But it is worth mentioning some installation options and dependencies. </p>
			<p>There are many installation guides available online for each version of Apache Spark/PySpark and the various target platforms (for example Linux, macOS, and Windows). PySpark is included in the official release of Apache Spark, which can now be downloaded from the Apache Spark<a id="_idIndexMarker890"/> website (<a href="https://spark.apache.org/">https://spark.apache.org/</a>). PySpark is also available via the <code>pip</code> utility from PyPI, which can be used for a local setup or to connect to a remote cluster. Another option when installing PySpark is using <strong class="bold">Anaconda</strong>, which<a id="_idIndexMarker891"/> is another popular package and environment management system. If we are installing PySpark along with Apache Spark, we need the following to be available or installed on the target machine:</p>
			<ul>
				<li>JVM</li>
				<li>Scala</li>
				<li>Apache Spark</li>
			</ul>
			<p>For the code examples that will be discussed later, we have installed Apache Spark version 3.1.1 on macOS with PySpark included. PySpark comes with<a id="_idIndexMarker892"/> the <code>SparkSession</code> and <code>SparkContext</code> objects automatically, which can be used to interact with the core Apache Spark engine. The following figure shows the initialization of the PySpark shell:</p>
			<div><div><img src="img/B17189_08_05.jpg" alt="Figure 8.5 – PySpark shell&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.5 – PySpark shell</p>
			<p>From the initialization steps of the<a id="_idIndexMarker893"/> PySpark shell, we can observe the following: </p>
			<ul>
				<li>The <code>SparkContext</code> object is already created, and its instance is available in the shell as <code>sc</code>.</li>
				<li>The <code>SparkSession</code> object is also created, and its instance is available as <code>spark</code>. Now, <code>SparkSession</code> is an entry point to the PySpark framework to dynamically create RDD and DataFrame objects. The SparkSession object can also be created programmatically, and we will discuss this later with a code example. </li>
				<li>Apache Spark comes with a web UI and a web server to host the web UI, and it is initiated at <code>http://192.168.1.110:4040</code> for our local machine installation. Note that the IP address mentioned in this URL is a private address that is specific to our machine. Port <code>4040</code> is selected as the default port by Apache Spark. If this port is in use, Apache Spark will try to host on the next available port, such as <code>4041</code> or <code>4042</code>.</li>
			</ul>
			<p>In the next subsections, we will learn how to create <code>SparkSession</code> objects, explore PySpark for RDD operations, and learn how to use PySpark DataFrames and PySpark SQL. We will start with creating a Spark session using Python.</p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor236"/>Creating SparkSession and SparkContext programs</h2>
			<p>Prior to <a id="_idIndexMarker894"/>Spark release 2.0, <code>SparkContext</code> was used as an entry point to <a id="_idIndexMarker895"/>PySpark. Since Spark release 2.0, <code>SparkSession</code> has been introduced as an entry point to the PySpark underlying framework to work with RDDs and DataFrames. <code>SparkSession</code> also includes all the APIs available in <code>SparkContext</code>, <code>SQLContext</code>, <code>StreamingContext</code>, and <code>HiveContext</code>. Now, <code>SparkSession</code> can also be created using the <code>SparkSession</code> class by using its <code>builder</code> method, which is illustrated in the next code example:</p>
			<pre>import pyspark
from pyspark.sql import SparkSession
spark1 = SparkSession.<strong class="bold">builder</strong>.<strong class="bold">master</strong>("local[2]")
    .<strong class="bold">appName</strong>('New App').<strong class="bold">getOrCreate</strong>()</pre>
			<p>When we run this code in the PySpark shell, which already has a default <code>SparkSession</code> object created as <code>spark</code>, it will return the same session as an output of this <code>builder</code> method. The following console output shows the location of the two <code>SparkSession</code> objects (<code>spark</code> and <code>spark1</code>), which confirms that they are pointing to the same <code>SparkSession</code> object:</p>
			<pre>&gt;&gt;&gt; spark
&lt;pyspark.sql.session.SparkSession object at 0x1091019e8&gt;
&gt;&gt;&gt; spark1
&lt;pyspark.sql.session.SparkSession object at 0x1091019e8&gt;</pre>
			<p>A few key concepts to understand regarding the <code>builder</code> method are as follows:</p>
			<ul>
				<li><code>getOrCreate</code>: This method is the reason that we will get an already created session in the case of the PySpark shell. This method will create a new session if no session already exists; otherwise, it returns an already existing session.</li>
				<li><code>master</code>: If we want to create a session connected to a cluster, we will provide the master name, which can be instance name of the Spark, or YARN, or Mesos cluster manager. If we are using a locally deployed Apache Spark option, we can use <code>local[n]</code>, where <code>n</code> is an integer greater than zero. The <code>n</code> will determine the number of partitions to be created for the RDD and DataFrame. For a local setup, <code>n</code> can be the number of CPU <a id="_idIndexMarker896"/>cores on the system. If we set it to <code>local[*]</code>, which is a common practice, this will create as many worker threads as there are logical cores on the system.</li>
			</ul>
			<p>If a new <code>SparkSession</code> object needs to be created, we can use the <code>newSession</code> method, which is available at the <a id="_idIndexMarker897"/>instance level of an existing <code>SparkSession</code> object. A code example of creating a new <code>SparkSession</code> object is shown next: </p>
			<pre>import pyspark
from pyspark.sql import SparkSession
spark2 = spark.<strong class="bold">newSession()</strong></pre>
			<p>The console output for the <code>spark2</code> object confirms that this is a different session than the previously created <code>SparkSession</code> objects: </p>
			<pre>&gt;&gt;&gt; spark2
&lt;pyspark.sql.session.SparkSession object at 0x10910df98&gt;</pre>
			<p>The <code>SparkContext</code> object can also be created programmatically. The easiest way to get a <code>SparkContext</code> object from a <code>SparkSession</code> instance is by using the <code>sparkContext</code> attribute. There is also a <code>SparkConext</code> class in the PySpark library that can also be used to create a <code>SparkContext</code> object directly, which was a common approach prior to Spark release 2.0. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">We can have multiple <code>SparkSession</code> objects but only one <code>SparkContext</code> object per JVM.</p>
			<p>The <code>SparkSession</code> class<a id="_idIndexMarker898"/> offers a few more useful methods and attributes that are summarized next: </p>
			<ul>
				<li> <code>getActiveSession</code>: This method returns an active <code>SparkSession</code> under the current Spark thread. </li>
				<li><code>createDataFrame</code>: This method creates a DataFrame object from an RDD, a list of objects, or a pandas DataFrame object.</li>
				<li><code>conf</code>: This attribute returns the configuration interface for a Spark session. </li>
				<li><code>catalog</code>: This attribute provides an interface to create, update, or query associated databases, functions, and tables.</li>
			</ul>
			<p>A complete list of methods and attributes can be explored using the PySpark documentation<a id="_idIndexMarker899"/> for the <code>SparkSession</code> class at <a href="https://spark.apache.org/docs/latest/api/python/reference/api/">https://spark.apache.org/docs/latest/api/python/reference/api/</a>.</p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor237"/>Exploring PySpark for RDD operations</h2>
			<p>In the <em class="italic">Introducing RDDs</em> section, we <a id="_idIndexMarker900"/>covered some of the key functions<a id="_idIndexMarker901"/> and operations of RDDs. In this section, we will extend the discussion in the context of PySpark with code examples. </p>
			<h3>Creating RDDs from a Python collection and from an external file</h3>
			<p>We discussed a few<a id="_idIndexMarker902"/> ways to create RDDs in the previous section. In <a id="_idIndexMarker903"/>the following code examples, we will discuss<a id="_idIndexMarker904"/> how to create RDDs from an in-memory <a id="_idIndexMarker905"/>Python collection and from an external file resource. These two approaches are described next:</p>
			<ul>
				<li>To create an RDD from a Python data collection, we have a <code>parallelize</code> method available under the <code>sparkContext</code> instance. This method distributes the collection to form an RDD object. The method takes a collection as a parameter. An optional second parameter is available with the <code>parallelize</code> method to set the number of partitions to be created. By default, this method creates the partitions acocording to the number of cores available on the local machine or the number of cores set at the time of creating the <code>SparkSession</code> object. </li>
				<li>To create an RDD from an external file, we will use the <code>textFile</code> method available under the <code>sparkContext</code> instance. The <code>textFile</code> method can load a file as an RDD from HDFS or from a local filesystem (to be available on all cluster nodes). For local system-based deployment, an absolute and/or relative path can be provided. It is possible to set the minimum number of partitions to be created for the RDD using this method.</li>
			</ul>
			<p>Some quick <a id="_idIndexMarker906"/>sample code (<code>rddcreate.py</code>) is shown <a id="_idIndexMarker907"/>next to illustrate the exact syntax of the <a id="_idIndexMarker908"/>PySpark statements to be used for the creation of a <a id="_idIndexMarker909"/>new RDD: </p>
			<pre>data = [5, 4, 6, 3, 2, 8, 9, 2, 8, 7,
        8, 4, 4, 8, 2, 7, 8, 9, 6, 9]
rdd1 = spark.sparkContext.<strong class="bold">parallelize</strong>(data)
print(rdd1.getNumPartitions())
rdd2 = spark.sparkContext.<strong class="bold">textFile</strong>('sample.txt')
print(rdd2.getNumPartitions())</pre>
			<p>Note that the <code>sample.txt</code> file has random text data, and its contents are not relevant for this code example. </p>
			<h3>RDD transformation operations with PySpark</h3>
			<p>There are <a id="_idIndexMarker910"/>several built-in transformation operations <a id="_idIndexMarker911"/>available with PySpark. To illustrate how to implement a transformation operation such as <code>map</code> using PySpark, we will take a text file as an input and use the <code>map</code> function available with RDDs to transform it to another RDD. The sample code (<code>rddtranform1.py</code>) is shown next:</p>
			<pre>rdd1 = spark.sparkContext.<strong class="bold">textFile</strong>('sample.txt') 
rdd2 = rdd1.map(<strong class="bold">lambda lines: lines.lower()</strong>)
rdd3 = rdd1.map(<strong class="bold">lambda lines: lines.upper()</strong>)
print(rdd2.collect())
print(rdd3.collect())</pre>
			<p>In this sample code, we applied two lambda functions with the <code>map</code> operation to convert the text in the RDD to lowercase and uppercase. In the end, we used the <code>collect</code> operation to get the contents of the RDD objects. </p>
			<p>Another popular transformation operation is <code>filter</code>, which can be used to filter out some entries of data. Some<a id="_idIndexMarker912"/> example code (<code>rddtranform2.py</code>) is<a id="_idIndexMarker913"/> shown next that is developed to filter all the even numbers from an RDD:</p>
			<pre>data = [5, 4, 6, 3, 2, 8, 9, 2, 8, 7,
        8, 4, 4, 8, 2, 7, 8, 9, 6, 9]
rdd1 = spark.sparkContext.<strong class="bold">parallelize</strong>(data)
rdd2 = rdd1.filter(lambda x: x % 2 !=0 )
print(rdd2.collect())</pre>
			<p>When you execute this code, it will provide console output with 3, 7, 7, and 9 as collection entries. Next, we will explore a few action examples with PySpark.</p>
			<h3>RDD action operations with PySpark</h3>
			<p>To illustrate the<a id="_idIndexMarker914"/> implementation of action operations, we will use<a id="_idIndexMarker915"/> an RDD created from the Python collection and then apply a few built-in action operations that come with the PySpark library. The sample code (<code>rddaction1.py</code>) is shown next:</p>
			<pre>data = [5, 4, 6, 3, 2, 8, 9, 2, 8, 7,
        8, 4, 4, 8, 2, 7, 8, 9, 6, 9]
rdd1 = spark.sparkContext.<strong class="bold">parallelize</strong>(data)
print("RDD contents with partitions:" + str(rdd1.<strong class="bold">glom</strong>().  <strong class="bold">collect</strong>()))
print("Count by values: " +str(rdd1.<strong class="bold">countByValue</strong>()))
print("reduce function: " + str(rdd1.<strong class="bold">glom</strong>().<strong class="bold">collect</strong>()))
print("Sum of RDD contents:"+str(rdd1.<strong class="bold">sum</strong>()))
print("top: " + str(rdd1.<strong class="bold">top</strong>(5)))
print("count: " + str(rdd1.<strong class="bold">count</strong>()))
print("max: "+ str(rdd1.<strong class="bold">max</strong>()))
print("min" + str(rdd1.<strong class="bold">min</strong>()))
time.sleep(60)</pre>
			<p>Some of the action operations used in this code example are self-explanatory and trivial (<code>count</code>, <code>max</code>, <code>min</code>, <code>count</code>, and <code>sum</code>). The rest of the action operations (non-trivial) are explained next:</p>
			<ul>
				<li><code>glom</code>: This results in an <a id="_idIndexMarker916"/>RDD that is created by coalescing all data entries with each partition into a list.</li>
				<li><code>collect</code>: This method returns all the elements of an RDD as a list.</li>
				<li><code>reduce</code>: This is a generic <a id="_idIndexMarker917"/>function to apply to the RDD to reduce the number of elements in it. In our case, we used a lambda function to combine two elements into one, and so on. This results in adding all the elements in the RDD. </li>
				<li><code>top(x)</code>: This action returns the top <code>x</code> elements in the array if the elements in the array are ordered. </li>
			</ul>
			<p>We have covered how to create RDDs using PySpark and how to implement transformation and action operations on an RDD. In the next section, we will cover the PySpark DataFrame, which is another popular data structure used mainly for analytics. </p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor238"/>Learning about PySpark DataFrames</h2>
			<p>The<strong class="bold"> PySpark DataFrame </strong>is a<a id="_idIndexMarker918"/> tabular data structure consisting of rows and columns, like the tables we have in a relational database and like the pandas DataFrame, which we introduced in <a href="B17189_06_Final_PG_ePub.xhtml#_idTextAnchor188"><em class="italic">Chapter 6</em></a><em class="italic">, Advanced Tips and Tricks in Python</em>. In comparison to pandas DataFrames, the key difference is that PySpark DataFrame objects are distributed in the cluster, which means data is stored across different nodes in a cluster. The use of a DataFrame is mainly to process a large collection of structured or unstructured data, which may reach into the petabytes, in a distributed manner. Like RDDs, PySpark DataFrames are immutable and based on lazy evaluation, which means evaluation will be delayed until it needs to be done. </p>
			<p>We can store numeric as well as string data types in a DataFrame. The columns in a PySpark DataFrame cannot be empty; they must have the same data type and must be of the same length. Rows in a DataFrame can have data of different data types. Row names in a DataFrame are required to be unique. </p>
			<p>In the next subsections, we will learn how to create a DataFrame and cover some key operations on DataFrames using PySpark.</p>
			<h3>Creating a DataFrame object</h3>
			<p>A PySpark DataFrame can <a id="_idIndexMarker919"/>be created using one of the following sources of data:</p>
			<ul>
				<li>Python collections such as lists, tuples, and dictionaries.</li>
				<li>Files (CSV, XML, JSON, Parquet, and so on).</li>
				<li>RDDs, by using the <code>toDF</code> method or the <code>createDataFrame</code> method of PySpark.</li>
				<li>Apache Kafka streaming messages can be converted to PySpark DataFrames by using the <code>readStream</code> method of the <code>SparkSession</code> object.</li>
				<li>Database (for example, Hive and HBase) tables can be queried using traditional SQL commands and the output will be transformed into a PySpark DataFrame.</li>
			</ul>
			<p>We will start creating a DataFrame from a Python collection, which is the simplest approach, but it is more helpful for illustration purposes. The next bit of sample code shows us how to create a PySpark DataFrame from a collection of employees data:</p>
			<pre>data = [('James','','Bylsma','HR','M',40000),
  ('Kamal','Rahim','','HR','M',41000),
  ('Robert','','Zaine','Finance','M',35000),
  ('Sophia','Anne','Richer','Finance','F',47000),
  ('John','Will','Brown','Engineering','F',65000)
]
columns = ["firstname","middlename","lastname",
           "department","gender","salary"]
df = spark.<strong class="bold">createDataFrame</strong>(data=data, schema = columns)
print(df.<strong class="bold">printSchema</strong>())
print(df.<strong class="bold">show</strong>())</pre>
			<p>In this code example, we first <a id="_idIndexMarker920"/>created the row data as a list of employees and then created a schema with column names. When the schema is only a list of column names, the data type of each column is determined by the data, and each column is marked as nullable by default. A more advanced API (<code>StructType</code> or <code>StructField</code>) can be used to define the DataFrame schema manually, which includes setting the data type and marking a column as nullable or not nullable. The console output of this sample code is shown next, which shows the schema first and then the DataFrame contents as a table:</p>
			<pre>root
 |-- firstname: string (nullable = true)
 |-- middlename: string (nullable = true)
 |-- lastname: string (nullable = true)
 |-- department: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- salary: long (nullable = true)
+---------+----------+--------+-----------+------+-------+
|firstname|middlename|lastname| department|gender|salary|
+---------+----------+--------+-----------+------+-------+
|    James|          |  Bylsma|         HR|     M|  40000|
|    Kamal|     Rahim|        |         HR|     M|  41000|
|   Robert|          |   Zaine|    Finance|     M|  35000|
|   Sophia|      Anne|  Richer|    Finance|     F|  47000|
|     John|      Will|   Brown|Engineering|     F|  65000|
+---------+----------+--------+-----------+------+-------+ </pre>
			<p>In the next code example, we will create a DataFrame from a CSV file. The CSV file will have the same entries as we used in the previous code example. In this sample code (<code>dfcreate2.py</code>), we also defined <a id="_idIndexMarker921"/>the schema manually by using the <code>StructType</code> and <code>StructField</code> objects:</p>
			<pre>schemas = StructType([ \
    StructField("firstname",StringType(),True), \
    StructField("middlename",StringType(),True), \
    StructField("lastname",StringType(),True), \
    StructField("department", StringType(), True), \
    StructField("gender", StringType(), True), \
    StructField("salary", IntegerType(), True) \
  ])
df = spark.<strong class="bold">read.csv</strong>('df2.csv', <strong class="bold">header=True</strong>, schema=schemas)
print(df.printSchema())
print(df.show())</pre>
			<p>The console outcome of this code will be the same as shown for the previous code example. The importing of JSON, text, or XML files into a DataFrame is supported by the <code>read</code> method using a similar syntax. The support of other data sources, such as RDDs and databases, is left for <a id="_idIndexMarker922"/>you to evaluate and implement as an exercise. </p>
			<h3>Working on a PySpark DataFrame</h3>
			<p>Once we have created<a id="_idIndexMarker923"/> a DataFrame from some data, regardless of the source of the data, we are ready to analyze it, transform it, and take some actions on it to get meaningful results from it. Most of the operations supported by the PySpark DataFrame are similar to RDDs and pandas DataFrames. For illustration purposes, we will load the same data as in the previous code example into a DataFrame object and then perform the following operations:</p>
			<ol>
				<li>Select one or more columns from the DataFrame object using the <code>select</code> method.</li>
				<li>Replace the values in a column using a dictionary and the <code>replace</code> method. There are more options to replace data in a column available in the PySpark library.</li>
				<li>Add a new column with values based on an existing column's data.</li>
			</ol>
			<p>The complete sample code (<code>dfoperations.py</code>) is shown next:</p>
			<pre>data = [('James','','Bylsma','HR','M',40000),
  ('Kamal','Rahim','','HR','M',41000),
  ('Robert','','Zaine','Finance','M',35000),
  ('Sophia','Anne','Richer','Finance','F',47000),
  ('John','Will','Brown','Engineering','F',65000)
]
columns = ["firstname","middlename","lastname",
           "department","gender","salary"]
df = spark.<strong class="bold">createDataFrame</strong>(data=data, schema = columns)
#show two columns
print(df.<strong class="bold">select</strong>([df.firstname, df.salary]).show())
#replacing values of a column
myDict = {'F':'Female','M':'Male'}
df2 = df.<strong class="bold">replace</strong>(myDict, subset=['gender'])
#adding a new colum Pay Level based on an existing column   values
df3 = df2.<strong class="bold">withColumn</strong>("Pay Level",
      <strong class="bold">when</strong>((df2.salary &lt; 40000), lit("10")) \
     .<strong class="bold">when</strong>((df.salary &gt;= 40000) &amp; (df.salary &lt;= 50000),           lit("11")) \
     .<strong class="bold">otherwise</strong>(lit("12")) \
  )
print(df3.show())</pre>
			<p>Following is the<a id="_idIndexMarker924"/> output for the preceding code example:</p>
			<div><div><img src="img/B17189_08_06.jpg" alt="Figure 8.6 – Console output of the dfoperations.py program&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.6 – Console output of the dfoperations.py program</p>
			<p>The first table shows the <a id="_idIndexMarker925"/>result of the <code>select</code> operation. The next table shows the result of the <code>replace</code> operation on the <code>gender</code> column and also a new column, <code>Pay Level</code>.</p>
			<p>There are many built-in operations available to work with PySpark DataFrames, and many of them are the same as we discussed for pandas DataFrames. The details of those operations can be explored by using the Apache Spark official documentation for the software release you have. </p>
			<p>There is one legitimate question that anyone would ask at this point, which is, <em class="italic">Why should we use the PySpark DataFrame when we already have pandas DataFrame offering the same types of operations? </em>The answer is very simple. PySpark offers distributed DataFrames, and the operations on such DataFrames are meant to be executed on a cluster of nodes in parallel. This makes the PySpark DataFrame's performance significantly better than the pandas DataFrame's. </p>
			<p>We have seen so far that, as <a id="_idIndexMarker926"/>programmers, we are not actually having to program anything regarding how to delegate distributed RDDs and DataFrames to different executors in a standalone or distributed cluster. Our focus is only on the programming aspect of the data processing. Coordination and communication with a local or remote cluster of nodes is automatically taken care of by <code>SparkSession</code> and <code>SparkContext</code>. This is the beauty of Apache Spark and PySpark: letting programmers focus on solving the real problems instead of worrying about how workloads will be executed. </p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor239"/>Introducing PySpark SQL</h2>
			<p>Spark SQL is one <a id="_idIndexMarker927"/>of the key modules of Apache Spark; it is used for structured data processing and acts as a distributed SQL query engine. As you can imagine, Spark SQL is highly scalable, being a distributed processing engine. Usually, the data source for Spark SQL is a database, but SQL queries can be applied to temporary views, which can be built from RDDs and DataFrames. </p>
			<p>To demonstrate using the<a id="_idIndexMarker928"/> PySpark library with Spark SQL, we will use the same DataFrame as in the previous sample code, using employees data to build a <code>TempView</code> instance for SQL queries. In our code example, we will do the following:</p>
			<ol>
				<li value="1">We will create a PySpark DataFrame for the employees data from a Python collection as we did for the previous code example.</li>
				<li>We will create a <code>TempView</code> instance from the PySpark DataFrame using the <code>createOrReplaceTempView</code> method.</li>
				<li>Using the <code>sql</code> method of the Spark Session object, we will execute the conventional SQL queries on the <code>TempView</code> instance, such as querying all employee records, querying employees with salaries higher than 45,000, querying the count of employees per gender type, and using the <code>group by</code> SQL command for the <code>gender</code> column.</li>
			</ol>
			<p>The complete code example (<code>sql1.py</code>) is as follows:</p>
			<pre>data = [('James','','Bylsma','HR','M',40000),
  ('Kamal','Rahim','','HR','M',41000),
  ('Robert','','Zaine','Finance','M',35000),
  ('Sophia','Anne','Richer','Finance','F',47000),
  ('John','Will','Brown','Engineering','F',65000)
]
columns = ["firstname","middlename","lastname",
           "department","gender","salary"]
df = spark.createDataFrame(data=data, schema = columns)
df.<strong class="bold">createOrReplaceTempView</strong>("EMP_DATA")
df2 = spark.<strong class="bold">sql</strong>("<strong class="bold">SELECT * FROM EMP_DATA</strong>")
print(df2.show())
df3 = spark.<strong class="bold">sql</strong>("<strong class="bold">SELECT firstname,middlename,lastname,</strong>    <strong class="bold">salary FROM EMP_DATA WHERE SALARY &gt; 45000</strong>")
print(df3.show())
df4 = spark.<strong class="bold">sql</strong>(("<strong class="bold">SELECT gender, count(*) from EMP_DATA </strong>    <strong class="bold">group by gender</strong>"))
print(df4.show())</pre>
			<p>The console output will show the results of the three SQL queries: </p>
			<pre>+---------+----------+--------+-----------+------+------+
|firstname|middlename|lastname| department|gender|salary|
+---------+----------+--------+-----------+------+------+
|    James|          |  Bylsma|         HR|     M| 40000|
|    Kamal|     Rahim|        |         HR|     M| 41000|
|   Robert|          |   Zaine|    Finance|     M| 35000|
|   Sophia|      Anne|  Richer|    Finance|     F| 47000|
|     John|      Will|   Brown|Engineering|     F| 65000|
+---------+----------+--------+-----------+------+------+
+---------+----------+--------+------+
|firstname|middlename|lastname|salary|
+---------+----------+--------+------+
|   Sophia|      Anne|  Richer| 47000|
|     John|      Will|   Brown| 65000|
+---------+----------+--------+------+
+------+--------+
|gender|count(1)|
+------+--------+
|     F|       2|
|     M|       3|
+------+--------+</pre>
			<p>Spark SQL<a id="_idIndexMarker929"/> is a big topic within Apache Spark. We provided only an introduction to Spark SQL to show the power of using SQL commands on top of Spark data structures without knowing the source of the data. This concludes our discussion of the use of PySpark for data processing and data analysis. In the next section, we will discuss a couple of case studies to build some real-world applications. </p>
			<h1 id="_idParaDest-202"><a id="_idTextAnchor240"/>Case studies of using Apache Spark and PySpark</h1>
			<p>In previous sections, we covered the fundamental concepts and architecture of Apache Spark and PySpark. In this section, we will discuss two case studies for implementing two interesting and popular applications for Apache Spark.</p>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor241"/>Case study 1 – Pi (π) calculator on Apache Spark</h2>
			<p>We will <a id="_idIndexMarker930"/>calculate Pi (π) using the Apache Spark cluster that is running on our local machine. Pi is the area of a circle when its radius is 1. Before discussing the algorithm and the driver program for this application, it is important to introduce the Apache Spark setup used for this case study. </p>
			<h3>Setting up the Apache Spark cluster</h3>
			<p>In all previous code <a id="_idIndexMarker931"/>examples, we used PySpark locally installed on our machine without a cluster. For this case study, we will set up an Apache Spark cluster by using multiple virtual machines. There are many virtualization software tools available, such<a id="_idIndexMarker932"/> as <strong class="bold">VirtualBox</strong>, and any of these software tools will work for building this kind of setup. </p>
			<p>We used Ubuntu <strong class="bold">Multipass</strong> (<a href="https://multipass.run/">https://multipass.run/</a>) to build the virtual machines on top of macOS. Multipass <a id="_idIndexMarker933"/>works on Linux and on Windows as well. Multipass is a lightweight virtualization manager and is designed specifically for developers to create virtual machines with a single command. Multipass has very few commands, which makes it easier to use. If you decide to use Multipass, we recommend that you use the official documentation for installation and configuration. In our virtual machines setup, we have the following virtual machines created using Multipass:</p>
			<div><div><img src="img/B17189_08_07.jpg" alt="Figure 8.7 – Virtual machines created for our Apache Spark cluster&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.7 – Virtual machines created for our Apache Spark cluster</p>
			<p>We installed <em class="italic">Apache Spark 3.1.1</em> on each virtual machine by using the <code>apt-get</code> utility. We started Apache Spark as the master on <code>vm1</code> and then started Apache Spark as the worker on <code>vm2</code> and <code>vm3</code> by providing the master Spark URI, which is <code>Spark://192.168.64.2.7077</code> in our case. The complete Spark cluster setup will look as shown here:</p>
			<div><div><img src="img/B17189_08_08.jpg" alt="Figure 8.8 – Details of the Apache Spark cluster nodes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.8 – Details of the Apache Spark cluster nodes</p>
			<p>The web UI for the<a id="_idIndexMarker934"/> master Spark node will look as shown here:</p>
			<div><div><img src="img/B17189_08_09.jpg" alt="Figure 8.9 – Web UI for the master node in the Apache Spark cluster&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.9 – Web UI for the master node in the Apache Spark cluster</p>
			<p>A summary of the web<a id="_idIndexMarker935"/> UI for the master node is given here:</p>
			<ul>
				<li>The web UI provides the node name with the Spark URL. In our case, we used the IP address as the host name, which is why we have an IP address in the URL.</li>
				<li>There are the details of the worker nodes, of which there are two in our case. Each worker node uses 1 CPU core and 1 GB of memory.</li>
				<li>The web UI also provides details of the running and completed applications.</li>
			</ul>
			<p>The web UI for the <a id="_idIndexMarker936"/>worker nodes will look as follows:</p>
			<div><div><img src="img/B17189_08_10.jpg" alt="Figure 8.10 – Web UI for the worker nodes in the Apache Spark cluster&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.10 – Web UI for the worker nodes in the Apache Spark cluster</p>
			<p>A summary of the <a id="_idIndexMarker937"/>web UI for the worker nodes is given here:</p>
			<ul>
				<li>The web UI provides<a id="_idIndexMarker938"/> the worker IDs as well as the node names and the ports where the workers are listening for requests. </li>
				<li>The master node URL is also provided in the web UI.</li>
				<li>Details of the CPU core and memory allocated to the worker nodes are also available.</li>
				<li>The web UI provides details of jobs in progress (<strong class="bold">Running Executors</strong>) and jobs that are already finished.</li>
			</ul>
			<h3>Writing a driver program for Pi calculation</h3>
			<p>To calculate Pi, we are using a <a id="_idIndexMarker939"/>commonly used <a id="_idIndexMarker940"/>algorithm (the <strong class="bold">Monte Carlo</strong><em class="italic"> </em>algorithm) that<a id="_idIndexMarker941"/> assumes a square having an area equal to 4 that is circumscribing a unit circle (circle with a radius value equal to 1). The idea is to generate a huge amount of random numbers in the domain of a square with sides having a length of 2. We can assume there is a circle inside the square with the same diameter value as the length of the side of the square. This means that the circle will be inscribed inside the square. The value of Pi is estimated by calculating the ratio of the number of points that lie inside the circle to the total number of generated points.</p>
			<p>The complete sample code<a id="_idIndexMarker942"/> for the driver program is shown next. In this program, we decided to use two partitions as we have two workers available to us. We used 10,000,000 points for each worker. Another important thing to note is that we used the Spark<a id="_idIndexMarker943"/> master node URL as a master attribute <a id="_idIndexMarker944"/>when creating the Apache Spark session:</p>
			<pre>#<strong class="bold">casestudy1.py</strong>: Pi calculator
from operator import add
from random import random
from pyspark.sql import SparkSession
spark = SparkSession.builder.master
        ("<strong class="bold">spark://192.168.64.2:7077</strong>") \
    .appName("Pi claculator app") \
    .getOrCreate()
partitions = 2
n = 10000000 * partitions
def func(_):
    x = random() * 2 – 1
    y = random() * 2 – 1
    return 1 if x ** 2 + y ** 2 &lt;= 1 else 0
count = spark.sparkContext.<strong class="bold">parallelize</strong>(range(1, n + 1),     partitions).map(func).<strong class="bold">reduce</strong>(add)
print("Pi is roughly %f" % (4.0 * count / n))</pre>
			<p>The console output is as follows:</p>
			<pre>Pi is roughly 3.141479 </pre>
			<p>The Spark web UI will provide<a id="_idIndexMarker945"/> the status of the application when running and even after it completes its execution. In the following screenshot, we can see that two <a id="_idIndexMarker946"/>workers were engaged to <a id="_idIndexMarker947"/>complete the job: </p>
			<div><div><img src="img/B17189_08_11.jpg" alt="Figure 8.11 – Pi calculator status in the Spark web UI &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.11 – Pi calculator status in the Spark web UI </p>
			<p>We can click on the application name to go to the next level of detail for the application, as shown in <em class="italic">Figure 8.12</em>. This screenshot shows which workers are involved in completing the tasks and what resources are being used (if things are still running): </p>
			<div><div><img src="img/B17189_08_12.jpg" alt="Figure 8.12 – Pi calculator application executor-level details &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.12 – Pi calculator application executor-level details </p>
			<p>In this case study, we <a id="_idIndexMarker948"/>covered how we can set up an Apache Spark cluster for testing and experimentation purposes and how we can build a driver program in Python using<a id="_idIndexMarker949"/> the PySpark library to connect to<a id="_idIndexMarker950"/> Apache Spark and submit our jobs to be processed on two different cluster nodes. </p>
			<p>In the next case study, we will build a word cloud using the PySpark library. </p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor242"/>Case study 2 – Word cloud using PySpark</h2>
			<p>A<strong class="bold"> word cloud</strong> is a<a id="_idIndexMarker951"/> visual representation of the frequency of words that appear in some text data. Put simply, if a specific word appears more frequently in a text, it appears bigger and <a id="_idIndexMarker952"/>bolder in the word cloud. These are also<a id="_idIndexMarker953"/> known as <strong class="bold">tag clouds</strong> or <strong class="bold">text clouds</strong> and are very useful tools to identify what parts of some textual <a id="_idIndexMarker954"/>data are more important. A practical <a id="_idIndexMarker955"/>use case of this tool is the analysis of content on social media, which has many applications for marketing, business analytics, and security. </p>
			<p>For illustration purposes, we have built a simple word cloud application that reads a text file from the local filesystem. The text file is imported into an RDD object that is then processed to count the number of times each word occurred. We process the data further to filter out the words that are repeated fewer than two times and also filter out words that are of a length that's less than four letters. The word frequency data is fed to the <code>WordCloud</code> library object. To display the word cloud, we used the <code>matplotlib</code> library. The complete <a id="_idIndexMarker956"/>sample code<a id="_idIndexMarker957"/> is shown next: </p>
			<pre>#<strong class="bold">casestudy2</strong>.py: word count application
import matplotlib.pyplot as plt
from pyspark.sql import SparkSession
from wordcloud import WordCloud
spark = SparkSession.builder.master("local[*]")\
    .appName("word cloud app")\
    .getOrCreate()
wc_threshold = 1
wl_threshold = 3
textRDD = spark.sparkContext.<strong class="bold">textFile</strong>('wordcloud.txt',3)
flatRDD = textRDD.<strong class="bold">flatMap</strong>(lambda x: x.split(' '))
wcRDD = flatRDD.<strong class="bold">map</strong>(lambda word: (word, 1)).\
    reduceByKey(lambda v1, v2: v1 + v2)
# filter out words with fewer than threshold occurrences
filteredRDD = wcRDD.<strong class="bold">filter</strong>(lambda pair: pair[1] &gt;=     wc_threshold)
filteredRDD2 = filteredRDD.<strong class="bold">filter</strong>(lambda pair:     len(pair[0]) &gt; wl_threshold)
word_freq = dict(filteredRDD2.collect())
# Create the wordcloud object
wordcloud = WordCloud(width=480, height=480, margin=0).\
    <strong class="bold">generate_from_frequencies</strong>(word_freq)
# Display the generated cloud image
plt.<strong class="bold">imshow</strong>(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.margins(x=0, y=0)
plt.<strong class="bold">show</strong>()</pre>
			<p>The output of this <a id="_idIndexMarker958"/>program is plotted as a window application and the output will <a id="_idIndexMarker959"/>look as shown here, based on the sample text (<code>wordcloud.txt</code>) provided to the application: </p>
			<div><div><img src="img/B17189_08_13.jpg" alt="Figure 8.13 – Word cloud built using PySpark RDDs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.13 – Word cloud built using PySpark RDDs</p>
			<p>Note that we have not used a very big sample of textual data for this illustration. In the real world, the source data can be extremely large, which justifies processing using an Apache Spark cluster. </p>
			<p>These two case<a id="_idIndexMarker960"/> studies have provided you with skills in using Apache Spark for<a id="_idIndexMarker961"/> large-scale data processing. They provide a foundation for those of you who are interested in the fields <a id="_idIndexMarker962"/>of <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>), text analysis, and sentimental analysis. These skills are important for you if you are a data scientist and your day-to-day job requires data processing for analytics and building algorithms for NLP. </p>
			<h1 id="_idParaDest-205"><a id="_idTextAnchor243"/>Summary</h1>
			<p>In this chapter, we explored how to execute data-intensive jobs on a cluster of machines to achieve parallel processing. Parallel processing is important for large-scale data, which is also known as big data. We started by evaluating the different cluster options available for data processing. We provided a comparative analysis of Hadoop MapReduce and Apache Spark, which are the two main competing platforms for clusters. The analysis showed that Apache Spark has more flexibility in terms of supported languages and cluster management systems, and it outperforms Hadoop MapReduce for real-time data processing because of its in-memory data processing model. </p>
			<p>Once we had established that Apache Spark is the most appropriate choice for a variety of data processing applications, we started looking into its fundamental data structure, which is the RDD. We discussed how to create RDDs from different sources of data and introduced two types of operations, transformations and actions. </p>
			<p>In the core part of this chapter, we explored using PySpark to create and manage RDDs using Python. This included several code examples of transformation and action operations. We also introduced PySpark DataFrames for the next level of data processing in a distributed manner. We concluded the topic by introducing PySpark SQL with a few code examples. </p>
			<p>Finally, we looked at two case studies using Apache Spark and PySpark. These case studies included the calculation of Pi and the building of a word cloud from text data. We also covered in the case studies how we can set up a Standalone Apache Spark instance on a local machine for testing purposes. </p>
			<p>This chapter gave you a lot of experience in setting up Apache Spark locally as well as setting up Apache Spark clusters using virtualization. There are plenty of code examples provided in this chapter for you to enhance your practical skills. This is important for anyone who wants to process their big data problems using clusters for efficiency and scale.</p>
			<p>In the next chapter, we will explore options for leveraging frameworks such as Apache Beam and extend our discussion of using public clouds for data processing.</p>
			<h1 id="_idParaDest-206"><a id="_idTextAnchor244"/>Questions</h1>
			<ol>
				<li value="1">How is Apache Spark different from Hadoop MapReduce?</li>
				<li>How are transformations different from actions in Apache Spark?</li>
				<li>What is lazy evaluation in Apache Spark?</li>
				<li>What is <code>SparkSession</code>?</li>
				<li>How is the PySpark DataFrame different from the pandas DataFrame?</li>
			</ol>
			<h1 id="_idParaDest-207"><a id="_idTextAnchor245"/>Further reading</h1>
			<ul>
				<li><em class="italic">Spark in Action, Second Edition</em> by Jean-Georges Perrin</li>
				<li><em class="italic">Learning PySpark</em> by Tomasz Drabas, Denny Lee</li>
				<li><em class="italic">PySpark Recipes</em> by Raju Kumar Mishra</li>
				<li><em class="italic">Apache Spark documentation</em> for the release you are using (<a href="https://spark.apache.org/docs/rel#">https://spark.apache.org/docs/rel#</a>) </li>
				<li><em class="italic">Multipass documentation</em> available at <a href="https://multipass.run/docs">https://multipass.run/docs</a></li>
			</ul>
			<h1 id="_idParaDest-208"><a id="_idTextAnchor246"/>Answers</h1>
			<ol>
				<li value="1">Apache Spark is an in-memory data processing engine, whereas Hadoop MapReduce has to read from and write to the filesystem.</li>
				<li>Transformation is applied to convert or translate data from one form to another, and the results stay within the cluster. Actions are the functions applied to data to get the results that are returned to the driver program. </li>
				<li>Lazy evaluation is applied mainly for transformation operations, which means transformation operations are not executed until an action is triggered on a data object.</li>
				<li><code>SparkSession</code> is an entry point to the Spark application to connect to one or more cluster managers and to work with executors for task execution.</li>
				<li>The PySpark DataFrame is distributed and is meant to be available on multiple nodes of an Apache Spark cluster for parallel processing.</li>
			</ol>
		</div>
	</body></html>