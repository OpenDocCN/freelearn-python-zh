["```py\n#!/usr/bin/env python\n\n# Python Network Programming Cookbook -- Chapter - 6\n# This program is optimized for Python 2.7.\n# It may run on any other version with/without modifications.\n\nfrom pygeocoder import Geocoder\n\ndef search_business(business_name):\n\n  results = Geocoder.geocode(business_name)\n\n  for result in results:\n    print result\n\nif __name__ == '__main__':\n  business_name =  \"Argos Ltd, London\" \n  print \"Searching %s\" %business_name\n  search_business(business_name)\n```", "```py\n$ python 6_1_search_business_addr.py\nSearching Argos Ltd, London \n\nArgos Ltd, 110-114 King Street, London, Greater London W6 0QP, UK\n\n```", "```py\n#!/usr/bin/env python\n# Python Network Programming Cookbook -- Chapter - 6\n# This program is optimized for Python 2.7.\n# It may run on any other version with/without modifications.\nimport argparse\nimport os\nimport urllib\n\nERROR_STRING = '<error>'\n\ndef find_lat_long(city):\n  \"\"\" Find geographic coordinates \"\"\"\n  # Encode query string into Google maps URL\n    url = 'http://maps.google.com/?q=' + urllib.quote(city) + \n'&output=js'\n    print 'Query: %s' % (url)\n\n  # Get XML location from Google maps\n    xml = urllib.urlopen(url).read()\n\n    if ERROR_STRING in xml:\n      print '\\nGoogle cannot interpret the city.'\n      return\n    else:\n    # Strip lat/long coordinates from XML\n      lat,lng = 0.0,0.0\n      center = xml[xml.find('{center')+10:xml.find('}',xml.find('{center'))]\n      center = center.replace('lat:','').replace('lng:','')\n      lat,lng = center.split(',')\n      print \"Latitude/Longitude: %s/%s\\n\" %(lat, lng)\n\n    if __name__ == '__main__':\n      parser = argparse.ArgumentParser(description='City Geocode \nSearch')\n      parser.add_argument('--city', action=\"store\", dest=\"city\", \nrequired=True)\n      given_args = parser.parse_args() \n\n      print \"Finding geographic coordinates of %s\" \n%given_args.city\n      find_lat_long(given_args.city)\n```", "```py\n$ python 6_2_geo_coding_by_google_maps.py --city=London \nFinding geograhic coordinates of London \nQuery: http://maps.google.com/?q=London&output=js \nLatitude/Longitude: 51.511214000000002/-0.119824 \n\n```", "```py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Python Network Programming Cookbook -- Chapter - 6\n# This program is optimized for Python 2.7.\n# It may run on any other version with/without modifications\n\nimport argparse\nimport re\nimport yaml\nimport urllib\nimport urllib2\n\nSEARCH_URL = 'http://%s.wikipedia.org/w/api.php?action=query&list=search&srsearch=%s&sroffset=%d&srlimit=%d&format=yaml'\n\nclass Wikipedia:\n\n  def __init__(self, lang='en'):\n    self.lang = lang\n\n  def _get_content(self, url):\n    request = urllib2.Request(url)\n    request.add_header('User-Agent', 'Mozilla/20.0')\n\n    try:\n      result = urllib2.urlopen(request)\n      except urllib2.HTTPError, e:\n        print \"HTTP Error:%s\" %(e.reason)\n      except Exception, e:\n        print \"Error occurred: %s\" %str(e)\n      return result\n\n  def search_content(self, query, page=1, limit=10):\n    offset = (page - 1) * limit\n    url = SEARCH_URL % (self.lang, urllib.quote_plus(query), \noffset, limit)\n    content = self._get_content(url).read()\n\n    parsed = yaml.load(content)\n    search = parsed['query']['search']\n    if not search:\n    return\n\n    results = []\n    for article in search:\n      snippet = article['snippet']\n      snippet = re.sub(r'(?m)<.*?>', '', snippet)\n      snippet = re.sub(r'\\s+', ' ', snippet)\n      snippet = snippet.replace(' . ', '. ')\n      snippet = snippet.replace(' , ', ', ')\n      snippet = snippet.strip()\n\n    results.append({\n      'title' : article['title'].strip(),\n'snippet' : snippet\n    })\n\n    return results\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser(description='Wikipedia search')\n  parser.add_argument('--query', action=\"store\", dest=\"query\", \nrequired=True)\n  given_args = parser.parse_args()\n\n  wikipedia = Wikipedia()\n  search_term = given_args.query\n  print \"Searching Wikipedia for %s\" %search_term \n  results = wikipedia.search_content(search_term)\n  print \"Listing %s search results...\" %len(results)\n  for result in results:\n    print \"==%s== \\n \\t%s\" %(result['title'], result['snippet'])\n  print \"---- End of search results ----\"\n```", "```py\n$ python 6_3_search_article_in_wikipedia.py --query='Islam' \nSearching Wikipedia for Islam \nListing 10 search results... \n==Islam== \n Islam. (\nˈ\n | \nɪ\n | s | l | \nɑː\n | m \nالإسلام\n, ar | ALA | al-\nʾ\nIsl\nā\nm  æl\nʔɪ\ns\nˈ\nlæ\nː\nm | IPA | ar-al_islam. ... \n\n==Sunni Islam== \n Sunni Islam (\nˈ\n | s | u\nː\n | n | i or \nˈ\n | s | \nʊ\n | n | i |) is the \nlargest branch of Islam ; its adherents are referred to in Arabic as ... \n==Muslim== \n A Muslim, also spelled Moslem is an adherent of Islam, a monotheistic Abrahamic religion based on the Qur'an —which Muslims consider the ... \n==Sharia== \n is the moral code and religious law of Islam. Sharia deals with \nmany topics addressed by secular law, including crime, politics, and ... \n==History of Islam== \n The history of Islam concerns the Islamic religion and its \nadherents, known as Muslim s. \" \"Muslim\" is an Arabic word meaning \n\"one who ... \n\n==Caliphate== \n a successor to Islamic prophet Muhammad ) and all the Prophets \nof Islam. The term caliphate is often applied to successions of \nMuslim ... \n==Islamic fundamentalism== \n Islamic ideology and is a group of religious ideologies seen as \nadvocating a return to the \"fundamentals\" of Islam : the Quran and \nthe Sunnah. ... \n==Islamic architecture== \n Islamic architecture encompasses a wide range of both secular \nand religious styles from the foundation of Islam to the present day. ... \n---- End of search results ---- \n\n```", "```py\n#!/usr/bin/env python\n# Python Network Programming Cookbook -- Chapter - 6\n# This program is optimized for Python 2.7.\n# It may run on any other version with/without modifications. \n\nimport argparse\nimport urllib\nimport re\nfrom datetime import datetime\n\nSEARCH_URL = 'http://finance.google.com/finance?q='\n\ndef get_quote(symbol):\n  content = urllib.urlopen(SEARCH_URL + symbol).read()\n  m = re.search('id=\"ref_694653_l\".*?>(.*?)<', content)\n  if m:\n    quote = m.group(1)\n  else:\n    quote = 'No quote available for: ' + symbol\n  return quote\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser(description='Stock quote \nsearch')\n  parser.add_argument('--symbol', action=\"store\", dest=\"symbol\", \nrequired=True)\n  given_args = parser.parse_args() \n  print \"Searching stock quote for symbol '%s'\" %given_args.symbol \n  print \"Stock  quote for %s at %s: %s\" %(given_args.symbol , \ndatetime.today(),  get_quote(given_args.symbol))\n```", "```py\n$ python 6_4_google_stock_quote.py --symbol=goog \nSearching stock quote for symbol 'goog' \nStock quote for goog at 2013-08-20 18:50:29.483380: 868.86 \n\n```", "```py\n#!/usr/bin/env python\n# Python Network Programming Cookbook -- Chapter - 6\n# This program is optimized for Python 2.7.\n# It may run on any other version with/without modifications.\n\nSEARCH_URL_BASE = 'https://api.github.com/repos'\n\nimport argparse\nimport requests\nimport json\n\ndef search_repository(author, repo, search_for='homepage'):\n  url = \"%s/%s/%s\" %(SEARCH_URL_BASE, author, repo)\n  print \"Searching Repo URL: %s\" %url\n  result = requests.get(url)\n  if(result.ok):\n    repo_info = json.loads(result.text or result.content)\n    print \"Github repository info for: %s\" %repo\n    result = \"No result found!\"\n    keys = [] \n    for key,value in repo_info.iteritems():\n      if  search_for in key:\n          result = value\n      return result\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser(description='Github search')\n  parser.add_argument('--author', action=\"store\", dest=\"author\", \nrequired=True)\n  parser.add_argument('--repo', action=\"store\", dest=\"repo\", \nrequired=True)\n  parser.add_argument('--search_for', action=\"store\", \ndest=\"search_for\", required=True)\n\n  given_args = parser.parse_args() \n  result = search_repository(given_args.author, given_args.repo, \ngiven_args.search_for)\n  if isinstance(result, dict):\n    print \"Got result for '%s'...\" %(given_args.search_for)\n    for key,value in result.iteritems():\n    print \"%s => %s\" %(key,value)\n  else:\n    print \"Got result for %s: %s\" %(given_args.search_for, \nresult)\n```", "```py\n$ python 6_5_search_code_github.py --author=django --repo=django --search_for=owner \nSearching Repo URL: https://api.github.com/repos/django/django \nGithub repository info for: django \nGot result for 'owner'... \nfollowing_url => https://api.github.com/users/django/following{/other_user} \nevents_url => https://api.github.com/users/django/events{/privacy} \norganizations_url => https://api.github.com/users/django/orgs \nurl => https://api.github.com/users/django \ngists_url => https://api.github.com/users/django/gists{/gist_id} \nhtml_url => https://github.com/django \nsubscriptions_url => https://api.github.com/users/django/subscriptions \navatar_url => https://1.gravatar.com/avatar/fd542381031aa84dca86628ece84fc07?d=https%3A%2F%2Fidenticons.github.com%2Fe94df919e51ae96652259468415d4f77.png \nrepos_url => https://api.github.com/users/django/repos \nreceived_events_url => https://api.github.com/users/django/received_events \ngravatar_id => fd542381031aa84dca86628ece84fc07 \nstarred_url => https://api.github.com/users/django/starred{/owner}{/repo} \nlogin => django \ntype => Organization \nid => 27804 \nfollowers_url => https://api.github.com/users/django/followers \n\n```", "```py\n$ pip install feedparser\n\n```", "```py\n$ easy_install feedparser\n\n```", "```py\n#!/usr/bin/env python\n# Python Network Programming Cookbook -- Chapter - 6\n# This program is optimized for Python 2.7\\. \n# It may run on any other version with/without modifications.\n\nfrom datetime import datetime\nimport feedparser \nBBC_FEED_URL = 'http://feeds.bbci.co.uk/news/%s/rss.xml'\n\ndef read_news(feed_url):\n  try:\n    data = feedparser.parse(feed_url)\n  except Exception, e:\n    print \"Got error: %s\" %str(e)\n\n  for entry in data.entries:\n    print(entry.title)\n    print(entry.link)\n    print(entry.description)\n    print(\"\\n\") \n\nif __name__ == '__main__':\n  print \"==== Reading technology news feed from bbc.co.uk \n(%s)====\" %datetime.today()\n\n  print \"Enter the type of news feed: \"\n  print \"Available options are: world, uk, health, sci-tech, \nbusiness, technology\"\n  type = raw_input(\"News feed type:\")\n  read_news(BBC_FEED_URL %type)\n  print \"==== End of BBC news feed =====\"\n```", "```py\n$ python 6_6_read_bbc_news_feed.py \n==== Reading technology news feed from bbc.co.uk (2013-08-20 19:02:33.940014)==== \nEnter the type of news feed:\nAvailable options are: world, uk, health, sci-tech, business, technology \nNews feed type:technology \nXbox One courts indie developers \nhttp://www.bbc.co.uk/news/technology-23765453#sa-ns_mchannel=rss&ns_source=PublicRSS20-sa \nMicrosoft is to give away free Xbox One development kits to encourage independent developers to self-publish games for its forthcoming console. \n\nFast in-flight wi-fi by early 2014 \nhttp://www.bbc.co.uk/news/technology-23768536#sa-ns_mchannel=rss&ns_source=PublicRSS20-sa \nPassengers on planes, trains and ships may soon be able to take advantage of high-speed wi-fi connections, says Ofcom. \n\nAnonymous 'hacks council website' \nhttp://www.bbc.co.uk/news/uk-england-surrey-23772635#sa-ns_mchannel=rss&ns_source=PublicRSS20-sa \nA Surrey council blames hackers Anonymous after references to a Guardian journalist's partner detained at Heathrow Airport appear on its website. \n\nAmazon.com website goes offline \nhttp://www.bbc.co.uk/news/technology-23762526#sa-ns_mchannel=rss&ns_source=PublicRSS20-sa \nAmazon's US website goes offline for about half an hour, the latest high-profile internet firm to face such a problem in recent days. \n\n[TRUNCATED]\n\n```", "```py\n#!/usr/bin/env python\n# Python Network Programming Cookbook -- Chapter - 6\n# This program is optimized for Python 2.7.\n# It may run on any other version with/without modifications.\n\nimport argparse\nimport sys\nimport httplib\nimport re\n\nprocessed = []\n\ndef search_links(url, depth, search):\n  # Process http links that are not processed yet\n  url_is_processed = (url in processed)\n  if (url.startswith(\"http://\") and (not url_is_processed)):\n    processed.append(url)\n    url = host = url.replace(\"http://\", \"\", 1)\n    path = \"/\"\n\n    urlparts = url.split(\"/\")\n    if (len(urlparts) > 1):\n      host = urlparts[0]\n      path = url.replace(host, \"\", 1)\n\n     # Start crawling\n     print \"Crawling URL path:%s%s \" %(host, path)\n     conn = httplib.HTTPConnection(host)\n     req = conn.request(\"GET\", path)\n     result = conn.getresponse()\n\n    # find the links\n    contents = result.read()\n    all_links = re.findall('href=\"(.*?)\"', contents)\n\n    if (search in contents):\n      print \"Found \" + search + \" at \" + url\n\n      print \" ==> %s: processing %s links\" %(str(depth), \nstr(len(all_links)))\n      for href in all_links:\n      # Find relative urls\n      if (href.startswith(\"/\")):\n        href = \"http://\" + host + href\n\n        # Recurse links\n        if (depth > 0):\n          search_links(href, depth-1, search)\n    else:\n      print \"Skipping link: %s ...\" %url\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser(description='Webpage link \ncrawler')\n  parser.add_argument('--url', action=\"store\", dest=\"url\", \nrequired=True)\n  parser.add_argument('--query', action=\"store\", dest=\"query\", \nrequired=True)\n  parser.add_argument('--depth', action=\"store\", dest=\"depth\", \ndefault=2)\n\n  given_args = parser.parse_args() \n\n  try:\n    search_links(given_args.url,  \ngiven_args.depth,given_args.query)\n    except KeyboardInterrupt:\n      print \"Aborting search by user request.\"\n```", "```py\n$ python 6_7_python_link_crawler.py --url='http://python.org' --query='python' \nCrawling URL path:python.org/ \nFound python at python.org \n ==> 2: processing 123 links \nCrawling URL path:www.python.org/channews.rdf \nFound python at www.python.org/channews.rdf \n ==> 1: processing 30 links \nCrawling URL path:www.python.org/download/releases/3.4.0/ \nFound python at www.python.org/download/releases/3.4.0/ \n ==> 0: processing 111 links \nSkipping link: https://ep2013.europython.eu/blog/2013/05/15/epc20145-call-proposals ... \nCrawling URL path:www.python.org/download/releases/3.2.5/ \nFound python at www.python.org/download/releases/3.2.5/ \n ==> 0: processing 113 links \n...\nSkipping link: http://www.python.org/download/releases/3.2.4/ ... \nCrawling URL path:wiki.python.org/moin/WikiAttack2013 \n^CAborting search by user request. \n\n```"]