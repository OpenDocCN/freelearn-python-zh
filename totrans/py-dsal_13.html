<html><head></head><body>
        

            
                <h1 class="header-title">Implementations, Applications, and Tools</h1>
            

            
                
<p>Learning about algorithms without any real-life application remains a purely academic pursuit. In this chapter, we will explore data structures and algorithms that are shaping our world.</p>
<p>One of the golden nuggets of this age is the abundance of data. E-mails, phone numbers, text, and image documents contain large amounts of data. In this data is found valuable information that makes the data become more important. But to extract this information from the raw data, we will have to use data structures, processes, and algorithms specialized for this task.</p>
<p>Machine learning employs a significant number of algorithms to analyze and predict the occurrence of certain variables. Analyzing data on a purely numerical basis still leaves much of the latent information buried in the raw data. Presenting data visually thus enables one to understand and gain valuable insights too.</p>
<p>By the end of this chapter, you should be able to do the following:</p>
<ul>
<li>Prune and present data accurately</li>
<li>Use both supervised and unsupervised learning algorithms for the purposes of prediction</li>
<li>Visually represent data in order to gain more insight</li>
</ul>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Tools of the trade</h1>
            

            
                
<p>In order to proceed with this chapter, you will need to install the following packages. These packages will be used to preprocess and visually represent the data being processed. Some of the packages also contain well-written and perfected algorithms that will operate on our data.</p>
<p>Preferably, these modules should be installed within a virtual environment such as <kbd>pip</kbd>:</p>
<pre>
<strong>% pip install numpy</strong><br/><strong>% pip install scikit-learn</strong><br/><strong>% pip install matplotlib</strong><br/><strong>% pip install pandas</strong><br/><strong>% pip install textblob</strong>  
</pre>
<p>These packages may require other platform-specific modules to be installed first. Take note and install all dependencies:</p>
<ul>
<li><strong>Numpy</strong>: A library with functions to operate on n-dimensional arrays and matrices.</li>
<li><strong>Scikit-learn</strong>: A highly advanced module for machine learning. It contains a good number of algorithms for classification, regression, and clustering, among others.</li>
<li><strong>Matplotlib</strong>: This is a plotting library that makes use of NumPy to graph a good variety of charts, including line plots, histograms, scatter plots, and even 3D graphs.</li>
<li><strong>Pandas</strong>: This library deals with data manipulation and analysis.</li>
</ul>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Data preprocessing</h1>
            

            
                
<p>Collection of data from the real world is fraught with massive challenges. The raw data collected is plagued with a lot of issues, so much so that we need to adopt ways to sanitize the data to make it suitable for use in further studies.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Why process raw data?</h1>
            

            
                
<p>Raw data as collected from the field is rigged with human error. Data entry is a major source of error when collecting data. Even technological methods of collecting data are not spared. Inaccurate reading of devices, faulty gadgetry, and changes in environmental factors can introduce significant margins of errors as data is collected.</p>
<p>The data collected may also be inconsistent with other records collected over time. The existence of duplicate entries and incomplete records warrant that we treat the data in such a way as to bring out hidden and buried treasure. The raw data may also be shrouded in a sea of irrelevant data.</p>
<p>To clean the data up, we can totally discard irrelevant data, better known as noise. Data with missing parts or attributes can be replaced with sensible estimates. Also, where the raw data suffers from inconsistency, detecting and correcting them becomes necessary.</p>
<p>Let us explore how we can use NumPy and pandas for data preprocessing techniques.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Missing data</h1>
            

            
                
<p>Data collection is tedious and, as such, once data is collected, it should not be easily discarded. Just because a dataset has missing fields or attributes does not mean it is not useful. Several methods can be used to fill up the nonexistent parts. One of these methods is by either using a global constant, using the mean value in the dataset, or supplying the data manually. The choice is based on the context and sensitivity of what the data is going to be used for.</p>
<p>Take, for instance, the following data:</p>
<pre>
    import numpy as np <br/>    data = pandas.DataFrame([ <br/>        [4., 45., 984.], <br/>        [np.NAN, np.NAN, 5.], <br/>        [94., 23., 55.], <br/>    ]) 
</pre>
<p>As we can see, the data elements <kbd>data[1][0]</kbd> and <kbd>data[1][1]</kbd> have values being <kbd>np.NAN</kbd>, representing the fact that they have no value. If the <kbd>np.NAN</kbd> values are undesired in a given dataset, they can be set to some constant figure.</p>
<p>Let's set data elements with the value <kbd>np.NAN</kbd> to 0.1:</p>
<pre>
    print(data.fillna(0.1)) 
</pre>
<p>The new state of the data becomes the following:</p>
<pre>
<strong>0     1      2</strong><br/><strong>0   4.0  45.0  984.0</strong><br/><strong>1   0.1   0.1    5.0</strong><br/><strong>2  94.0  23.0   55.0</strong>
</pre>
<p>To apply the mean values instead, we do the following:</p>
<pre>
    print(data.fillna(data.mean())) 
</pre>
<p><br/>
The mean value for each column is calculated and inserted in those data areas with the <kbd>np.NAN</kbd> value:</p>
<pre>
<strong>0     1      2</strong><br/><strong>0   4.0  45.0  984.0</strong><br/><strong>1  49.0  34.0    5.0</strong><br/><strong>2  94.0  23.0   55.0</strong>
</pre>
<p>For the first column, column <kbd>0</kbd>, the mean value was obtained by <kbd>(4 + 94)/2</kbd>. The resulting <kbd>49.0</kbd> is then stored at <kbd>data[1][0]</kbd>. A similar operation is carried out for columns <kbd>1</kbd> and <kbd>2</kbd>. </p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Feature scaling</h1>
            

            
                
<p>The columns in a data frame are known as its features. The rows are known as records or observations. Now examine the following data matrix. This data will be referenced in subsections so please do take note:</p>
<pre>
<strong>[[  58.    1.   43.]</strong><br/><strong> [  10.  200.   65.]</strong><br/><strong> [  20.   75.    7.]]</strong>
</pre>
<p>Feature 1, with data <kbd>58</kbd>, <kbd>10</kbd>, <kbd>20</kbd>, has its values lying between <kbd>10</kbd> and <kbd>58</kbd>. For feature 2, the data lies between <kbd>1</kbd> and <kbd>200</kbd>. Inconsistent results will be produced if we supply this data to any machine learning algorithm. Ideally, we will need to scale the data to a certain range in order to get consistent results.</p>
<p>Once again, a closer inspection reveals that each feature (or column) lies around different mean values. Therefore, what we would want to do is to align the features around similar means.</p>
<p>One benefit of feature scaling is that it boosts the learning parts of machine learning.</p>
<p>The <kbd>scikit</kbd> module has a considerable number of scaling algorithms that we shall apply to our data.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Min-max scalar</h1>
            

            
                
<p>The min-max scalar form of normalization uses the mean and standard deviation to box all the data into a range lying between a certain min and max value. For most purposes, the range is set between 0 and 1. At other times, other ranges may be applied but the 0 to 1 range remains the default:</p>
<pre>
    scaled_values = preprocessing.MinMaxScaler(feature_range=(0,1)) <br/>    results = scaled_values.fit(data).transform(data) <br/>    print(results) 
</pre>
<p>An instance of the <kbd>MinMaxScaler</kbd> class is created with the range <kbd>(0,1)</kbd> and passed to the variable <kbd>scaled_values</kbd>. The <kbd>fit</kbd> function is called to make the necessary calculations that will be used internally to change the dataset. The <kbd>transform</kbd> function effects the actual operation on the dataset, returning the value to <kbd>results</kbd>:</p>
<pre>
<strong>[[ 1.          0.          0.62068966]</strong><br/><strong> [ 0.          1.          1.        ]</strong><br/><strong> [ 0.20833333  0.3718593   0.        ]]</strong>
</pre>
<p>We can see from the preceding output that all the data is normalized and lies between 0 and 1. This kind of output can now be supplied to a machine learning algorithm.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Standard scalar</h1>
            

            
                
<p>The mean values for the respective features in our initial dataset or table are 29.3, 92, and 38. To make all the data have a similar mean, that is, a zero mean and a unit variance across the data, we shall apply the standard scalar algorithm:</p>
<pre>
    stand_scalar =  preprocessing.StandardScaler().fit(data) <br/>    results = stand_scalar.transform(data) <br/>    print(results)
</pre>
<p><kbd>data</kbd> is passed to the <kbd>fit</kbd> method of the object returned from instantiating the <kbd>StandardScaler</kbd> class. The <kbd>transform</kbd> method acts on the data elements in the data and returns the output to the results:</p>
<pre>
<strong>[[ 1.38637564 -1.10805456  0.19519899]</strong><br/><strong> [-0.93499753  1.31505377  1.11542277]</strong><br/><strong> [-0.45137812 -0.2069992  -1.31062176]]</strong>
</pre>
<p>Examining the results, we observe that all our features are now evenly distributed.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Binarizing data</h1>
            

            
                
<p>To binarize a given feature set, we make use of a threshold. If any value within a given dataset is greater than the threshold, the value is replaced by 1. If the value is less than the threshold 1, we will replace it:</p>
<pre>
    results = preprocessing.Binarizer(50.0).fit(data).transform(data) <br/>    print(results) 
</pre>
<p>An instance of <kbd>Binarizer</kbd> is created with the argument 50.0. 50.0 is the threshold that will be used in the binarizing algorithm:</p>
<pre>
<strong>[[ 1.  0.  0.]</strong><br/><strong> [ 0.  1.  1.]</strong><br/><strong> [ 0.  1.  0.]]</strong> 
</pre>
<p>All values in the data that are less than 50 will have 0 in their stead. The opposite also holds true.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Machine learning</h1>
            

            
                
<p>Machine learning is a subfield of artificial intelligence. We know that we can never truly create machines that actually "think" but we can supply machines with enough data and models by which sound judgment can be reached. Machine learning focuses on creating autonomous systems that can continue the process of decision making, with little or no human intervention.</p>
<p>In order to teach the machine, we need data drawn from the real world. For instance, to shift through which e-mails constitute spam and which ones don't, we need to feed the machine with samples of each. After obtaining this data, we have to run the data through models (algorithms) that will use probability and statistics to unearth patterns and structure from the data. If this is properly done, the algorithm by itself will be able to analyze e-mails and properly categorize them. Sorting e-mails is just one example of what machines can do if they are "trained".</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Types of machine learning</h1>
            

            
                
<p>There are three broad categories of machine learning, as follows:</p>
<ul>
<li><strong>Supervised learning</strong>: Here, an algorithm is fed a set of inputs and their corresponding outputs. The algorithm then has to figure out what the output will be for an unfamiliar input. Examples of such algorithms include naive Bayes, linear regression, and decision tree algorithms.</li>
<li><strong>Unsupervised learning</strong>: Without using the relationship that exists between a set of input and output variables, the unsupervised learning algorithm uses only the inputs to unearth groups, patterns, and clusters within the data. Examples of such algorithms include hierarchical clustering and k-means clustering.</li>
<li><strong>Reinforcement learning</strong>: The computer in this kind of learning dynamically interacts with its environment in such a way as to improve its performance.</li>
</ul>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Hello classifier</h1>
            

            
                
<p>To invoke the blessing of the programming gods in our quest to understand machine learning, we begin with an hello world example of a text classifier. This is meant to be a gentle introduction to machine learning.</p>
<p>This example will predict whether a given text carries a negative or positive connotation. Before this can be done, we need to train our algorithm (model) with some data.</p>
<p>The naive Bayes model is suited for text classification purposes. Algorithms based on naive Bayes models are generally fast and produce accurate results. The whole model is based on the assumption that features are independent from each other. To accurately predict the occurrence of rainfall, three conditions need to be considered. These are wind speed, temperature, and the amount of humidity in the air. In reality, these factors do have an influence on each other to tell the likelihood of rainfall. But the abstraction in naive Bayes is to assume that these features are unrelated in any way and thus independently contribute to chances of rainfall. Naive Bayes is useful in predicting the class of an unknown dataset, as we will see soon.</p>
<p>Now back to our hello classifier. After we have trained our mode, its prediction will fall into either the positive or negative category:</p>
<pre>
    from textblob.classifiers import NaiveBayesClassifier <br/>    train = [ <br/>        ('I love this sandwich.', 'pos'), <br/>        ('This is an amazing shop!', 'pos'), <br/>        ('We feel very good about these beers.', 'pos'), <br/>        ('That is my best sword.', 'pos'), <br/>        ('This is an awesome post', 'pos'), <br/>        ('I do not like this cafe', 'neg'), <br/>        ('I am tired of this bed.', 'neg'), <br/>        ("I can't deal with this", 'neg'), <br/>        ('She is my sworn enemy!', 'neg'), <br/>        ('I never had a caring mom.', 'neg') <br/>    ] 
</pre>
<p>First, we will import the <kbd>NaiveBayesClassifier</kbd> class from the <kbd>textblob</kbd> package. This classifier is very easy to work with and is based on the Bayes theorem.</p>
<p>The <kbd>train</kbd> variable consists of tuples that each holds the actual training data. Each tuple contains the sentence and the group it is associated with.</p>
<p>Now, to train our model, we will instantiate a <kbd>NaiveBayesClassifier</kbd> object by passing the train to it:</p>
<pre>
    cl = NaiveBayesClassifier(train) 
</pre>
<p>The updated naive Bayesian model <kbd>cl</kbd> will predict the category that an unknown sentence belongs to. Up to this point, our model knows of only two categories that a phrase can belong to, <kbd>neg</kbd> and <kbd>pos</kbd>.</p>
<p>The following code runs the following tests using our model:</p>
<pre>
    print(cl.classify("I just love breakfast")) <br/>    print(cl.classify("Yesterday was Sunday")) <br/>    print(cl.classify("Why can't he pay my bills")) <br/>    print(cl.classify("They want to kill the president of Bantu")) 
</pre>
<p>The output of our test is as follows:</p>
<pre>
<strong>pos <br/>pos <br/>neg <br/>neg</strong> 
</pre>
<p>We can see that the algorithm has had some degree of success in classifying the input phrases into their categories well.</p>
<p>This contrived example is overly simplistic but it does show promise that if given the right amounts of data and a suitable algorithm or model, it is possible for a machine to carry out tasks without any human help.</p>
<p>The specialized class <kbd>NaiveBayesClassifier</kbd> also did some heavy lifting for us in the background so we could not appreciate the innards by which the algorithm arrived at the various predictions. Our next example will use the <kbd>scikit</kbd> module to predict the category that a phrase may belong to.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">A supervised learning example</h1>
            

            
                
<p>Assume that we have a set of posts to categorize. As with supervised learning, we need to first train the model in order for it to accurately predict the category of an unknown post.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Gathering data</h1>
            

            
                
<p>The <kbd>scikit</kbd> module comes with a number of sample data we will use for training our model. In this case, we will use the newsgroups posts. To load the posts, we will use the following lines of code:</p>
<pre>
    from sklearn.datasets import fetch_20newsgroups <br/>    training_data = fetch_20newsgroups(subset='train',     <br/>        categories=categories, shuffle=True, random_state=42) 
</pre>
<p>After we have trained our model, the results of a prediction must belong to one of the following categories:</p>
<pre>
    categories = ['alt.atheism', <br/>                  'soc.religion.christian','comp.graphics', 'sci.med'] 
</pre>
<p>The number of records we are going to use as training data is obtained by the following:</p>
<pre>
    print(len(training_data)) 
</pre>
<p>Machine learning algorithms do not mix well with textual attributes so the categories that each post belongs to are presented as numbers:</p>
<pre>
    print(set(training_data.target)) 
</pre>
<p>The categories have integer values that we can map back to the categories themselves with <kbd>print(training_data.target_names[0])</kbd>.</p>
<p>Here, 0 is a numerical random index picked from <kbd>set(training_data.target)</kbd>.</p>
<p>Now that the training data has been obtained, we must feed the data to a machine learning algorithm. The bag of words model will break down the training data in order to make it ready for the learning algorithm or model.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Bag of words</h1>
            

            
                
<p>The bag of words is a model that is used for representing text data in such a way that it does not take into consideration the order of words but rather uses word counts to segment words into regions.</p>
<p>Take the following sentences:</p>
<pre>
    sentence_1 = "As fit as a fiddle"<br/>    sentence_2 = "As you like it"
</pre>
<p>The bag of words enables us to decompose text into numerical feature vectors represented by a matrix.</p>
<p>To reduce our two sentences into the bag of words model, we need to obtain a unique list of all the words:</p>
<pre>
    set((sentence_1 + sentence_2).split(" ")) 
</pre>
<p>This set will become our columns in the matrix. The rows in the matrix will represent the documents that are being used in training. The intersection of a row and column will store the number of times that word occurs in the document. Using our two sentences as examples, we obtain the following matrix:</p>
<table class="table">
<tbody>
<tr>
<td/>
<td>
<p><strong>As</strong></p>
</td>
<td>
<p><strong>Fit</strong></p>
</td>
<td>
<p><strong>A</strong></p>
</td>
<td>
<p><strong>Fiddle</strong></p>
</td>
<td>
<p><strong>You</strong></p>
</td>
<td>
<p><strong>Like</strong></p>
</td>
<td>
<p><strong>it</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>Sentence 1</strong></p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p><strong>Sentence 2</strong></p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p>The preceding data alone will not enable us to predict accurately the category that new documents or articles will belong to. The table has some inherent flaws. There may be situations where longer documents or words that occur in many of the posts reduce the precision of the algorithm. Stop words can be removed to make sure only relevant data is analyzed. Stop words include is, are, was, and so on. Since the bag of words model does not factor grammar into its analysis, the stop words can safely be dropped. It is also possible to add to the list of stop words that one feels should be exempted from final analysis.</p>
<p>To generate the values that go into the columns of our matrix, we have to tokenize our training data:</p>
<pre>
    from sklearn.feature_extraction.text import CountVectorizer <br/>    from sklearn.feature_extraction.text import TfidfTransformer <br/>    from sklearn.naive_bayes import MultinomialNB <br/>    count_vect = CountVectorizer() <br/>    training_matrix = count_vect.fit_transform(training_data.data) 
</pre>
<p>The <kbd>training_matrix</kbd> has a dimension of (2257, 35788). This means that 2257 corresponds to the dataset while 35788 corresponds to the number of columns that make up the unique set of words in all posts.</p>
<p>We instantiate the <kbd>CountVectorizer</kbd> class and pass the <kbd>training_data.data</kbd> to the <kbd>fit_transform</kbd> method of the <kbd>count_vect</kbd> object. The result is stored in <kbd>training_matrix</kbd>. The <kbd>training_matrix</kbd> holds all the unique words and their respective frequencies.</p>
<p>To mitigate the problem of basing prediction on frequency count alone, we will import the <kbd>TfidfTransformer</kbd> that helps to smooth out the inaccuracies in our data:</p>
<pre>
    matrix_transformer = TfidfTransformer() <br/>    tfidf_data = matrix_transformer.fit_transform(training_matrix) <br/><br/>    print(tfidf_data[1:4].todense()) 
</pre>
<p><kbd>tfidf_data[1:4].todense()</kbd> only shows a truncated list of a three rows by 35,788 columns matrix. The values seen are the term frequency--inverse document frequency that reduce the inaccuracy resulting from using a frequency count:</p>
<pre>
    model = MultinomialNB().fit(tfidf_data, training_data.target) 
</pre>
<p><kbd>MultinomialNB</kbd> is a variant of the naive Bayes model. We pass the rationalized data matrix, <kbd>tfidf_data</kbd> and categories, <kbd>training_data.target</kbd>, to its <kbd>fit</kbd> method.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Prediction</h1>
            

            
                
<p>To test whether our model has learned enough to predict the category that an unknown post is likely to belong to, we have the following sample data:</p>
<pre>
    test_data = ["My God is good", "Arm chip set will rival intel"] <br/>    test_counts = count_vect.transform(test_data) <br/>    new_tfidf = matrix_transformer.transform(test_counts) 
</pre>
<p>The list <kbd>test_data</kbd> is passed to the <kbd>count_vect.transform</kbd> function to obtain the vectorized form of the test data. To obtain the term frequency--inverse document frequency representation of the test dataset, we call the <kbd>transform</kbd> method of the <kbd>matrix_transformer</kbd> object.</p>
<p>To predict which category the docs may belong to, we do the following:</p>
<pre>
    prediction = model.predict(new_tfidf)  
</pre>
<p>The loop is used to iterate over the prediction, showing the categories they are predicted to belong to:</p>
<pre>
    for doc, category in zip(test_data, prediction): <br/>        print('%r =&gt; %s' % (doc, training_data.target_names[category])) 
</pre>
<p>When the loop has run to completion, the phrase, together with the category that it may belong to, is displayed. A sample output is as follows:</p>
<pre>
<strong>'My God is good' =&gt; soc.religion.christian</strong><br/><strong>'Arm chip set will rival intel' =&gt; comp.graphics</strong>
</pre>
<p>All that we have seen up to this point is a prime example of supervised learning. We started by loading posts whose categories are already known. These posts were then fed into the machine learning algorithm most suited for text processing based on the naive Bayes theorem. A set of test post fragments were supplied to the model and the category was predicted.</p>
<p>To explore an example of an unsupervised learning algorithm, we shall study the k-means algorithm for clustering some data.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">An unsupervised learning example</h1>
            

            
                
<p>A category of learning algorithms is able to discover inherent groups that may exist in a set of data. An example of these algorithms is the k-means algorithm.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">K-means algorithm</h1>
            

            
                
<p>The k-means algorithm uses the mean points in a given dataset to cluster and discover groups within the dataset. K is the number of clusters that we want and are hoping to discover. After the k-means algorithm has generated the groupings, we can pass it additional but unknown data for it to predict to which group it will belong.</p>
<p>Note that in this kind of algorithm, only the raw uncategorized data is fed to the algorithm. It is up to the algorithm to find out if the data has inherent groups within it.</p>
<p>To understand how this algorithm works, we will examine 100 data points consisting of x and y values. We will feed these values to the learning algorithm and expect that the algorithm will cluster the data into two sets. We will color the two sets so that the clusters are visible.</p>
<p>Let's create a sample data of 100 records of <em>x</em> and <em>y</em> pairs:</p>
<pre>
    import numpy as np <br/>    import matplotlib.pyplot as plt <br/>    original_set = -2 * np.random.rand(100, 2) <br/>    second_set = 1 + 2 * np.random.rand(50, 2) <br/>    original_set[50: 100, :] = second_set 
</pre>
<p>First, we create 100 records with <kbd>-2 * np.random.rand(100, 2)</kbd>. In each of the records, we will use the data in it to represent x and y values that will eventually be plotted.</p>
<p>The last 50 numbers in <kbd>original_set</kbd> will be replaced by <kbd>1 + 2 * np.random.rand(50, 2)</kbd>. In effect, what we have done is to create two subsets of data, where one set has numbers in the negative while the other set has numbers in the positive. It is now the responsibility of the algorithm to discover these segments appropriately.</p>
<p>We instantiate the <kbd>KMeans</kbd> algorithm class and pass it <kbd>n_clusters=2</kbd>. That makes the algorithm cluster all its data under only two groups. It is through a series of trial and error that this figure, <kbd>2</kbd>, is obtained. But for academic purposes, we already know this number. It is not at all obvious when working with unfamiliar datasets from the real world:</p>
<pre>
    from sklearn.cluster import KMeans <br/>    kmean = KMeans(n_clusters=2) <br/><br/>    kmean.fit(original_set) <br/><br/>    print(kmean.cluster_centers_) <br/><br/>    print(kmean.labels_) 
</pre>
<p>The dataset is passed to the <kbd>fit</kbd> function of <kbd>kmean</kbd>, <kbd>kmean.fit(original_set)</kbd>. The clusters generated by the algorithm will revolve around a certain mean point. The points that define these two mean points are obtained by <kbd>kmean.cluster_centers_</kbd>.</p>
<p>The mean points when printed appear as follows:</p>
<pre>
<strong>[[ 2.03838197  2.06567568]</strong><br/><strong> [-0.89358725 -0.84121101]]</strong>
</pre>
<p>Each data point in <kbd>original_set</kbd> will belong to a cluster after our k-means algorithm has finished its training. The k-mean algorithm represents the two clusters it discovers as 1s and 0s. If we had asked the algorithm to cluster the data into four, the internal representation of these clusters would have been 0, 1, 2, and 3. To print out the various clusters that each dataset belongs to, we do the following:</p>
<pre>
    print(kmean.labels_) 
</pre>
<p>This gives the following output:</p>
<pre>
<strong>[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1  <br/> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 <br/> 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]</strong>
</pre>
<p>There are 100 1s and 0s. Each shows the cluster that each data point falls under. By using <kbd>matplotlib.pyplot</kbd>, we can chart the points of each group and color it appropriately to show the clusters:</p>
<pre>
    import matplotlib.pyplot as plt <br/>    for i in set(kmean.labels_): <br/>        index = kmean.labels_ == i <br/>        plt.plot(original_set[index,0], original_set[index,1], 'o') 
</pre>
<p><kbd>index = kmean.labels_ == i</kbd> is a nifty way by which we select all points that correspond to the group <kbd>i</kbd>. When <kbd>i=0</kbd>, all points belonging to the group 0 are returned to index. It's the same for <kbd>index =1, 2</kbd> ... , and so on.</p>
<p><kbd>plt.plot(original_set[index,0], original_set[index,1], 'o')</kbd> then plots these data points using o as the character for drawing each point.</p>
<p>Next, we will plot the centroids or mean values around which the clusters have formed:</p>
<pre>
    plt.plot(kmean.cluster_centers_[0][0],kmean.cluster_centers_[0][1], <br/>             '*', c='r', ms=10) <br/>    plt.plot(kmean.cluster_centers_[1][0],kmean.cluster_centers_[1][1], <br/>             '*', c='r', ms=10) 
</pre>
<p>Lastly, we show the whole graph with the two means illustrated by a star:</p>
<pre>
    plt.show()
</pre>
<div><img class=" image-border" height="352" src="img/image_12_001.png" width="469"/></div>
<p>The algorithm discovers two distinct clusters in our sample data. The two mean points of the two clusters are denoted with the red star symbol.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Prediction</h1>
            

            
                
<p>With the two clusters that we have obtained, we can predict the group that a new set of data might belong to.</p>
<p>Let's predict which group the points <kbd>[[-1.4, -1.4]]</kbd> and <kbd>[[2.5, 2.5]]</kbd> will belong to:</p>
<pre>
    sample = np.array([[-1.4, -1.4]]) <br/>    print(kmean.predict(sample)) <br/><br/>    another_sample = np.array([[2.5, 2.5]]) <br/>    print(kmean.predict(another_sample)) 
</pre>
<p>The output is seen as follows:</p>
<pre>
<strong>[1]</strong><br/><strong>[0]</strong> 
</pre>
<p>At the barest minimum, we can expect the two test datasets to belong to different clusters. Our expectation is proved right when the <kbd>print</kbd> statement prints 1 and 0, thus confirming that our test data does indeed fall under two different clusters.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Data visualization</h1>
            

            
                
<p>Numerical analysis does not sometimes lend itself to easy understanding. Indeed, a single image is worth 1,000 words and in this section, an image would be worth 1,000 tables comprised of numbers only. Images present a quick way to analyze data. Differences in size and lengths are quick markers in an image upon which conclusions can be drawn. In this section, we will take a tour of the different ways to represent data. Besides the graphs listed here, there is more that can be achieved when chatting data.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Bar chart</h1>
            

            
                
<p>To chart the values 25, 5, 150, and 100 into a bar graph, we will store the values in an array and pass it to the <kbd>bar</kbd> function. The bars in the graph represent the magnitude along the <em>y</em>-axis:</p>
<pre>
    import matplotlib.pyplot as plt <br/><br/>    data = [25., 5., 150., 100.] <br/>    x_values = range(len(data)) <br/>    plt.bar(x_values, data) <br/><br/>    plt.show() 
</pre>
<p><kbd>x_values</kbd> stores an array of values generated by <kbd>range(len(data))</kbd>. Also, <kbd>x_values</kbd> will determine the points on the <em>x</em>-axis where the bars will be drawn. The first bar will be drawn on the <em>x</em>-axis where x is 0. The second bar with data 5 will be drawn on the <em>x</em>-axis where x is 1:</p>
<div><img class=" image-border" height="282" src="img/image_12_002.png" width="376"/></div>
<p>The width of each bar can be changed by modifying the following line:</p>
<pre>
    plt.bar(x_values, data, width=1.)  
</pre>
<p>This should produce the following graph:</p>
<div><img class=" image-border" height="267" src="img/image_12_003.png" width="356"/></div>
<p>However, this is not visually appealing because there is no space anymore between the bars, which makes it look clumsy. Each bar now occupies one unit on the <em>x</em>-axis.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Multiple bar charts</h1>
            

            
                
<p>In trying to visualize data, stacking a number of bars enables one to further understand how one piece of data or variable varies with another:</p>
<pre>
    data = [ <br/>            [8., 57., 22., 10.], <br/>            [16., 7., 32., 40.],<br/>           ] <br/><br/>    import numpy as np <br/>    x_values = np.arange(4) <br/>    plt.bar(x_values + 0.00, data[0], color='r', width=0.30) <br/>    plt.bar(x_values + 0.30, data[1], color='y', width=0.30) <br/><br/>    plt.show() 
</pre>
<p>The y values for the first batch of data are <kbd>[8., 57., 22., 10.]</kbd>. The second batch is <kbd>[16., 7., 32., 40.]</kbd>. When the bars are plotted, 8 and 16 will occupy the same x position, side by side.</p>
<p><kbd>x_values = np.arange(4)</kbd> generates the array with values <kbd>[0, 1, 2, 3]</kbd>. The first set of bars are drawn first at position <kbd>x_values + 0.30</kbd>. Thus, the first x values will be plotted at <kbd>0.00, 1.00, 2.00 and 3.00</kbd>.</p>
<p>The second batch of <kbd>x_values</kbd> will be plotted at <kbd>0.30, 1.30, 2.30 and 3.30</kbd>:</p>
<div><img class=" image-border" height="285" src="img/image_12_004.png" width="379"/></div>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Box plot</h1>
            

            
                
<p>The box plot is used to visualize the median value and low and high ranges of a distribution. It is also referred to as a box and whisker plot.</p>
<p>Let's chart a simple box plot.</p>
<p>We begin by generating 50 numbers from a normal distribution. These are then passed to <kbd>plt.boxplot(data)</kbd> to be charted:</p>
<pre>
    import numpy as np <br/>    import matplotlib.pyplot as plt <br/><br/>    data = np.random.randn(50) <br/><br/>    plt.boxplot(data) <br/>    plt.show() 
</pre>
<p>The following figure is what is produced:</p>
<div><img class=" image-border" height="233" src="img/image_12_005.png" width="310"/></div>
<p>A few comments on the preceding figure: the features of the box plot include a box spanning the interquartile range, which measures the dispersion; the outer fringes of the data are denoted by the whiskers attached to the central box; the red line represents the median.</p>
<p>The box plot is useful to easily identify the outliers in a dataset as well as determining in which direction a dataset may be skewed.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Pie chart</h1>
            

            
                
<p>The pie chart interprets and visually presents data as if to fit into a circle. The individual data points are expressed as sectors of a circle that add up to 360 degrees. This chart is good for displaying categorical data and summaries too:</p>
<pre>
    import matplotlib.pyplot as plt <br/>    data = [500, 200, 250] <br/><br/>    labels = ["Agriculture", "Aide", "News"] <br/><br/>    plt.pie(data, labels=labels,autopct='%1.1f%%') <br/>    plt.show() 
</pre>
<p>The sectors in the graph are labeled with the strings in the labels array:</p>
<div><img class=" image-border" height="272" src="img/image_12_006.png" width="362"/></div>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Bubble chart</h1>
            

            
                
<p>Another variant of the scatter plot is the bubble chart. In a scatter plot, we only plot the x, y points of the data. Bubble charts add another dimension by illustrating the size of the points. This third dimension may represent sizes of markets or even profits:</p>
<pre>
    import numpy as np <br/>    import matplotlib.pyplot as plt <br/><br/><br/>    n = 10 <br/>    x = np.random.rand(n) <br/>    y = np.random.rand(n) <br/>    colors = np.random.rand(n) <br/>    area = np.pi * (60 * np.random.rand(n))**2 <br/><br/>    plt.scatter(x, y, s=area, c=colors, alpha=0.5) <br/>    plt.show() 
</pre>
<p>With the variable <kbd>n</kbd>, we specify the number of randomly generated x and y values. This same number is used to determine the random colors for our x and y coordinates. Random bubble sizes are determined by <kbd>area = np.pi * (60 * np.random.rand(n))**2</kbd>.</p>
<p>The following figure shows this bubble chart:</p>
<div><img class="image-border" height="338" src="img/B05630_12_07.png" width="450"/></div>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Summary</h1>
            

            
                
<p>In this chapter, we have explored how data and algorithms come together to aid machine learning. Making sense of huge amounts of data is made possible by first pruning our data through normalization processes. Feeding this data to specialized algorithms, we are able to predict the categories and sets that our data will fall into.</p>
<p>Lastly, charting and plotting the condensed data helps to better understand and make insightful discoveries.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    </body></html>