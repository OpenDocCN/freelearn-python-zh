["```py\nimport csv \nfrom zipfile import ZipFile \nfrom io import BytesIO, TextIOWrapper \nimport requests\n\nresp = requests.get('http://s3.amazonaws.com/alexa-static/top-1m.csv.zip', stream=True) \nurls = [] # top 1 million URL's will be stored in this list \nwith ZipFile(BytesIO(resp.content)) as zf:\n    csv_filename = zf.namelist()[0]\n    with zf.open(csv_filename) as csv_file:\n        for _, website in csv.reader(TextIOWrapper(csv_file)):\n            urls.append('http://' + website)\n\n```", "```py\nclass AlexaCallback:\n    def __init__(self, max_urls=500):\n        self.max_urls = max_urls\n        self.seed_url = 'http://s3.amazonaws.com/alexa-static/top-1m.csv.zip'\n        self.urls = []\n\n    def __call__(self):\n        resp = requests.get(self.seed_url, stream=True)\n        with ZipFile(BytesIO(resp.content)) as zf:\n            csv_filename = zf.namelist()[0]\n            with zf.open(csv_filename) as csv_file:\n                for _, website in csv.reader(TextIOWrapper(csv_file)):\n                    self.urls.append('http://' + website)\n                    if len(self.urls) == self.max_urls:\n                        break\n\n```", "```py\n# In link_crawler function\n\nif isinstance(start_url, list):\n    crawl_queue = start_url\nelse:\n    crawl_queue = [start_url]\n\n```", "```py\npython chp4/advanced_link_crawler.py\n...\nTotal time: 1349.7983705997467s\n\n```", "```py\nimport time \nimport threading \n...\nSLEEP_TIME = 1 \n\ndef threaded_crawler(..., max_threads=10, scraper_callback=None): \n    ...\n\n    def process_queue(): \n        while crawl_queue: \n            ... \n\n```", "```py\nthreads = [] \n    while threads or crawl_queue: \n        # the crawl is still active \n        for thread in threads: \n            if not thread.is_alive(): \n                # remove the stopped threads \n                threads.remove(thread) \n        while len(threads) < max_threads and crawl_queue: \n            # can start some more threads \n            thread = threading.Thread(target=process_queue) \n            # set daemon so main thread can exit when receives ctrl-c \n            thread.setDaemon(True) \n            thread.start() \n            threads.append(thread) \n        # all threads have been processed # sleep temporarily so CPU can focus execution elsewhere \n        for thread in threads:\n            thread.join()        \n        time.sleep(SLEEP_TIME))\n\n```", "```py\nhtml = D(url, num_retries=num_retries)\nif not html:\n    continue\nif scraper_callback:\n    links = scraper_callback(url, html) or []\nelse:\n    links = []\n# filter for links matching our regular expression\nfor link in get_links(html) + links:\n    ...\n\n```", "```py\n$ redis-cli\n127.0.0.1:6379> FLUSHALL\nOK\n127.0.0.1:6379>\n\n```", "```py\n$ python code/chp4/threaded_crawler.py\n...\nTotal time: 361.50403571128845s\n\n```", "```py\n# Based loosely on the Redis Cookbook FIFO Queue:\n# http://www.rediscookbook.org/implement_a_fifo_queue.html\nfrom redis import StrictRedis\n\nclass RedisQueue:\n    \"\"\" RedisQueue helps store urls to crawl to Redis\n        Initialization components:\n        client: a Redis client connected to the key-value database for\n                the web crawling cache (if not set, a localhost:6379\n                default connection is used).\n        db (int): which database to use for Redis\n        queue_name (str): name for queue (default: wswp)\n    \"\"\"\n\n    def __init__(self, client=None, db=0, queue_name='wswp'):\n        self.client = (StrictRedis(host='localhost', port=6379, db=db)\n                       if client is None else client)\n        self.name = \"queue:%s\" % queue_name\n        self.seen_set = \"seen:%s\" % queue_name\n        self.depth = \"depth:%s\" % queue_name\n\n    def __len__(self):\n        return self.client.llen(self.name)\n\n    def push(self, element):\n        \"\"\"Push an element to the tail of the queue\"\"\"\n        if isinstance(element, list):\n            element = [e for e in element if not self.already_seen(e)]\n            self.client.lpush(self.name, *element)\n            self.client.sadd(self.seen_set, *element)\n        elif not self.client.already_seen(element):\n            self.client.lpush(self.name, element)\n            self.client.sadd(self.seen_set, element)\n\n    def pop(self):\n        \"\"\"Pop an element from the head of the queue\"\"\"\n        return self.client.rpop(self.name)\n\n    def already_seen(self, element):\n       \"\"\" determine if an element has already been seen \"\"\"\n       return self.client.sismember(self.seen_set, element)\n\n    def set_depth(self, element, depth):\n        \"\"\" Set the seen hash and depth \"\"\"\n        self.client.hset(self.depth, element, depth)\n\n    def get_depth(self, element):\n        \"\"\" Get the seen hash and depth \"\"\"\n        return self.client.hget(self.depth, element)\n\n```", "```py\ndef threaded_crawler_rq(...): \n    ... \n    # the queue of URL's that still need to be crawled \n    crawl_queue = RedisQueue() \n    crawl_queue.push(seed_url) \n\n    def process_queue(): \n        while len(crawl_queue):\n            url = crawl_queue.pop()\n        ...\n\n```", "```py\n## inside process_queue\nif no_robots or rp.can_fetch(user_agent, url):\n    depth = crawl_queue.get_depth(url) or 0\n    if depth == max_depth:\n        print('Skipping %s due to depth' % url)\n        continue\n    html = D(url, num_retries=num_retries)\n    if not html:\n        continue\n    if scraper_callback:\n        links = scraper_callback(url, html) or []\n    else:\n        links = []\n    # filter for links matching our regular expression\n    for link in get_links(html, link_regex) + links:\n        if 'http' not in link:\n            link = clean_link(url, domain, link)\n        crawl_queue.push(link)\n        crawl_queue.set_depth(link, depth + 1)\n\n```", "```py\nimport multiprocessing \n\ndef mp_threaded_crawler(args, **kwargs): \n    num_procs = kwargs.pop('num_procs')\n    if not num_procs:\n        num_cpus = multiprocessing.cpu_count() \n    processes = [] \n    for i in range(num_procs): \n        proc = multiprocessing.Process(\n            target=threaded_crawler_rq, args=args,                    \n            kwargs=kwargs) \n        proc.start() \n        processes.append(proc) \n    # wait for processes to complete \n    for proc in processes: \n        proc.join()\n\n```", "```py\n $ python threaded_crawler_with_queue.py\n ...\n Total time: 197.0864086151123s\n\n```", "```py\n$ python threaded_crawler_with_queue.py -h\nusage: threaded_crawler_with_queue.py [-h]\n [max_threads] [num_procs] [url_pattern]\n\nMultiprocessing threaded link crawler\n\npositional arguments:\n max_threads maximum number of threads\n num_procs number of processes\n url_pattern regex pattern for url matching\n\noptional arguments:\n -h, --help show this help message and exit\n\n```"]