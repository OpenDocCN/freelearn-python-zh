<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-42"><a id="_idTextAnchor041"/>2</h1>
<h1 id="_idParaDest-43"><a id="_idTextAnchor042"/>Programmable Networks</h1>
<p>Initially, computer networks were something physical and static, with wires and hardware, but with advanced computation, virtualization, and connectivity, networks have become more flexible and configurable by software. In this chapter, we’re going to talk about how software has changed the picture for computer networks. We are going first to examine several different technologies used today to create networks via software, then we are going to examine the current standard technology, known as <strong class="bold">software-defined networks</strong> (<strong class="bold">SDNs</strong>).</p>
<p>As we saw in the first chapter, computer networks can be quite complex and difficult to maintain. There are several different sets of equipment that range from routers, switches, and NATs to load balancers and more. In addition, within each piece of equipment, there are several different types of operation, such as <em class="italic">core</em> or <em class="italic">access</em> routers. Network equipment is typically configured individually with interfaces that are very different between each vendor. Although there are management systems that can help centralize the configuration in one single place, network equipment is configured at an individual level. This kind of operation means complexity in operation and slow innovation for new features.</p>
<p>In this chapter, we are going to cover the following topics:</p>
<ul>
<li><a id="_idTextAnchor043"/>Exploring the history of programmable networks and looking at those used in the present day</li>
<li>Virtual network technologies</li>
<li>SDNs and OpenFlow</li>
<li>Understanding cloud computing </li>
<li>Using OpenStack for networking</li>
</ul>
<h1 id="_idParaDest-44"><a id="_idTextAnchor044"/>Exploring the history of programmable networks and looking at those used in the present day </h1>
<p>Several years have passed since <strong class="bold">programmable networks</strong> were initially conceived by engineers, so let’s touch<a id="_idIndexMarker208"/> on a few historical milestones before we get into the current technologies.</p>
<h2 id="_idParaDest-45"><a id="_idTextAnchor045"/>Active networking</h2>
<p>The <strong class="bold">Defense Advanced Research Projects Agency</strong> (<strong class="bold">DARPA</strong>) began funding research<a id="_idIndexMarker209"/> in the mid-1990s to create a network<a id="_idIndexMarker210"/> that could easily be changed<a id="_idIndexMarker211"/> and customized by programming, called the <strong class="bold">active networking</strong> project. The main goal of the project was to create network technologies that, in contrast to then-current networks, were easy to innovate and evolve, allowing fast application and protocol development.</p>
<p>But it was not easy to create such a flexible network in the 1990s because programming languages, signaling and network protocols, and operating systems were not mature enough to accommodate such innovative ideas. For instance, operation systems were monolithic and adding features required recompilation and reboot. In addition, service APIs were non-existent and distributed programming languages were still in early development.</p>
<p>The active networking research programs explored radical alternatives to the services provided by the traditional internet<a id="_idIndexMarker212"/> stack via IP. Examples of<a id="_idIndexMarker213"/> this work can be found in projects such as <strong class="bold">Global Environment for Network Innovations</strong> (<strong class="bold">GENIs</strong>), which can be viewed<a id="_idIndexMarker214"/> at <a href="https://www.geni.net/">https://www.geni.net/</a>, <strong class="bold">Nacional Science Foundation</strong> (<strong class="bold">NSF</strong>), <strong class="bold">Future Internet Design</strong> (<strong class="bold">FIND</strong>) which can be viewed<a id="_idIndexMarker215"/> at <a href="http://www.nets-find.net/">http://www.nets-find.net/</a>, and <strong class="bold">Future Internet Research and Experimentation Initiative</strong> (<strong class="bold">EU FIRE</strong>).</p>
<p>At the time, the active networking research community pursued two programming models:</p>
<ul>
<li>The <strong class="bold">capsule model</strong>, where the code to execute<a id="_idIndexMarker216"/> at the nodes was carried in-band in data packets</li>
<li>The <strong class="bold">programmable router/switch model</strong>, where the code to execute at the<a id="_idIndexMarker217"/> nodes was established<a id="_idIndexMarker218"/> by out-of-band<a id="_idIndexMarker219"/> mechanisms</li>
</ul>
<p class="callout-heading">Important note</p>
<p class="callout">Further reading can be found in <em class="italic">Active Networking – One View of the Past, Present, and Future</em>. Jonathan M. Smith and Scott M. Nettles – <em class="italic">IEEE TRANSACTIONS ON SYSTEMS - PART C: Application and Reviews, Vol 34 No 1, February 2004.</em></p>
<p>Let’s dive into one of the first attempts at creating a node that was programmable, known as <strong class="bold">NodeOS</strong>.</p>
<h2 id="_idParaDest-46"><a id="_idTextAnchor046"/>NodeOS</h2>
<p>One of the first goals<a id="_idIndexMarker220"/> of the active networking<a id="_idIndexMarker221"/> project was to create <strong class="bold">NodeOS</strong>. It was an operating system whose primary purpose was to support packet forwarding in an active network. NodeOS ran at the lowest level in an active node and multiplexed the node resources, such as memory and CPU, among the packet flows that traverse the node. NodeOS provided several important services to active network execution environments, including resource scheduling and accounting and fast packet input-output. The two important<a id="_idIndexMarker222"/> design milestones on NodeOS were the creation of <strong class="bold">application programming interfaces</strong> (<strong class="bold">APIs</strong>) and resource management.</p>
<p class="callout-heading">Important note</p>
<p class="callout">Additional reading on NodeOS can be found at <em class="italic">An OS Interface for Active Routers April 2001</em> – <em class="italic">IEEE Journal on Selected Areas in Communications 19(3):473 – 487.</em></p>
<p>Following this, we are now going to explore a few projects that were the early attempts from the community toward SDN.</p>
<h2 id="_idParaDest-47"><a id="_idTextAnchor047"/>Data and control plane separation</h2>
<p>One of the major<a id="_idIndexMarker223"/> steps toward<a id="_idIndexMarker224"/> programmable<a id="_idIndexMarker225"/> networks and <strong class="bold">SDNs</strong> was the separation of the <strong class="bold">control</strong> and <strong class="bold">data </strong>planes. In <a href="B18165_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, we discussed the difference between the control and data planes, and here we are going to discuss a bit of the history behind them. It’s worth<a id="_idIndexMarker226"/> remembering that the data plane is also known as the <strong class="bold">forwarding plane</strong>.</p>
<p>By the 1990s, such separation was already present on public telephone networks but was not yet implemented in computer networks or on the internet. As network complexity increased and internet services started to become the main revenue for several backbone providers, reliability, predictability, and performance were key points for network operators to seek better approaches for managing networks.</p>
<p>In the early 2000s, a community of researchers who either worked for or regularly interacted with network operators started to explore pragmatic approaches using either standard protocols or other imminent technologies that were just about to become deployable. At that time, routers and switches had tight integration between the control and forwarding planes. This coupling made various network management tasks difficult, such as debugging configuration problems and controlling routing behavior. To address these challenges, various efforts to separate the forwarding and control planes began to emerge. Let’s explore a few of the earlier efforts in the following sections.</p>
<h3>IETF ForCES</h3>
<p><strong class="bold">Forwarding and Control Element Separation </strong>(<strong class="bold">IETF ForCES</strong>) working groups intended<a id="_idIndexMarker227"/> to create a framework, a list of requirements, a solution protocol, a logical function block library, and other associated<a id="_idIndexMarker228"/> documents in support of data and control element separation (<a href="https://datatracker.ietf.org/wg/forces/about/">https://datatracker.ietf.org/wg/forces/about/</a>).</p>
<h3>The NetLink interface</h3>
<p><strong class="bold">NetLink</strong> was perhaps the clearest separation<a id="_idIndexMarker229"/> of the control plane and data plane<a id="_idIndexMarker230"/> on the Linux<a id="_idIndexMarker231"/> kernel. In 2003, IETF published the <em class="italic">RFC3549</em> describing the separation of the <strong class="bold">control plane components</strong> (<strong class="bold">CPCs</strong>) and the <strong class="bold">forwarding engine components</strong> (<strong class="bold">FECs</strong>). <em class="italic">Figure 2.1</em> (from the original RFC) illustrates how Linux was using Netlink<a id="_idIndexMarker232"/> as the main separator between the control and data planes (<a href="https://datatracker.ietf.org/doc/html/rfc3549">https://datatracker.ietf.org/doc/html/rfc3549</a>).</p>
<div><div><img alt="Figure 2.1 – Control and data plane separation, as shown in RFC3549" src="img/B18165_02_001.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – Control and data plane separation, as shown in RFC3549</p>
<p> Netlink<a id="_idIndexMarker233"/> was first created on series 2.0 of the Linux kernel. </p>
<h3>Routing Control Platform</h3>
<p><strong class="bold">Routing Control Platform</strong> (<strong class="bold">RCP</strong>) is a pragmatic design to separate the control<a id="_idIndexMarker234"/> and data planes. The idea was to create a centralized control where all routing information was collected and then run an algorithm to select the best routing path for each of the routers of the network.</p>
<p>RCP was implemented by collecting routing tables from external and internal <strong class="bold">Border Gateway Protocol</strong> (<strong class="bold">BGP</strong>) from the routers in the current<a id="_idIndexMarker235"/> network, using this information in a centralized manner to choose the best path to each of the routers. With this approach, it was possible to leverage the existing network devices and have control plane<a id="_idIndexMarker236"/> and data plane separation.</p>
<p class="callout-heading">Important note</p>
<p class="callout">More about RCP using BGP can be found in the paper <em class="italic">Design and implementation of a routing control platform</em> – Authors:  Matthew Caesar, Donald Caldwell, Nick Feamster, Jennifer Rexford, Aman Shaikh, Jacobus van der Merwe – <em class="italic">NSDI’05: Proceedings of the 2nd conference on Symposium on Networked Systems Design &amp; Implementation – Volume 2 May 2005 Pages 15–28.</em></p>
<h3>SoftRouter</h3>
<p>The <strong class="bold">SoftRouter</strong> idea was presented at a conference<a id="_idIndexMarker237"/> in 2004 and patented in 2005. Again, the architecture had separation between control plane functions and data plane functions.</p>
<p>All control plane functions<a id="_idIndexMarker238"/> were implemented on general-purpose servers called <strong class="bold">control elements</strong> (<strong class="bold">CEs</strong>), which may be multiple hops away from the <strong class="bold">forwarding elements</strong> (<strong class="bold">FEs</strong>). There were two main types of network<a id="_idIndexMarker239"/> entities in the SoftRouter architecture, which<a id="_idIndexMarker240"/> were the FEs and CEs. Together, they constituted a <strong class="bold">network element</strong> (<strong class="bold">NE</strong>) router. The key difference from a traditional router was the absence of any control logic (such as OSPF or BGP) running locally. Instead, the control logic was hosted remotely.</p>
<p class="callout-heading">Important note</p>
<p class="callout">More details on SoftRouter can be found in the original 2004 paper <em class="italic">The SoftRouter Architecture</em> – T. V. Lakshman, T. Nandagopal, R. Ramjee, K. Sabnani, T. Woo – <em class="italic">Bell Laboratories, Lucent Technologies, ACM HOTNETS - January 2004.</em></p>
<h3>The path computation element architecture</h3>
<p>In 2006, the IETF Network Working Group<a id="_idIndexMarker241"/> published an RFC describing an architecture of a centralized controlled entity to make route path decisions, which they called the <strong class="bold">path computation element</strong> (<strong class="bold">PCE</strong>) architecture.</p>
<p>Initially, PCE architecture<a id="_idIndexMarker242"/> was invented to solve a problem in <strong class="bold">multiprotocol label switching</strong> (<strong class="bold">MPLS</strong>) where the <strong class="bold">label switch path</strong> (<strong class="bold">LSP</strong>) calculations were becoming<a id="_idIndexMarker243"/> very slow and heavy for each router to calculate. It was designed to do the calculations on a server inside or outside the network instead.</p>
<p class="callout-heading">Important note</p>
<p class="callout">More details on PCE<a id="_idIndexMarker244"/> can be found in <em class="italic">RFC4655:</em> <a href="https://datatracker.ietf.org/doc/html/rfc4655">https://datatracker.ietf.org/doc/html/rfc4655</a></p>
<p>Let’s now look at the most important project of all, which was the work that went toward OpenFlow and SDNs.</p>
<h3>Ethane</h3>
<p><strong class="bold">Ethane</strong> was one of the most important projects <a id="_idIndexMarker245"/>and culminated in the creation of OpenFlow and SDNs. Initially, it was just a project from a PhD student that defined a network as a group of data flows and network policies to control the traffic, which is another way to see the separation between the data plane and the control plane.</p>
<p>The Ethane project had the idea of centralizing all network policies in one place. A new device joining the Ethane network should have all its communication turned off by default. The new device should get explicit permissions from the centralized server before connecting and its data flow should only be allowed on the permitted paths.</p>
<p class="callout-heading">Important note</p>
<p class="callout">More on the Ethane project can be found in the 2007 original paper <em class="italic">Ethane: taking control of the enterprise</em> – Authors: M. Casado, M. J. Freedman, J. Pettit, J. Luo, N. McKeown, Scott Shenker – <em class="italic">SIGCOMM ‘07: Proceedings of the 2007 conference, pages 1–12.</em></p>
<p>In this section, we explored<a id="_idIndexMarker246"/> a bit of the history behind programmable networks. We also explored a few of the main projects that led to the separation of the control and data planes, which was an important milestone toward SDNs. You should now be able to identify the significance of the separation and why it happened.</p>
<h1 id="_idParaDest-48"><a id="_idTextAnchor048"/>Virtual network technologies </h1>
<p><strong class="bold">Network virtualization</strong> is when software acts like network hardware<a id="_idIndexMarker247"/> and it is accomplished by using<a id="_idIndexMarker248"/> logically simulated hardware platforms.</p>
<p>Virtualization of networks is not a new concept, and we can find one of the first implementations in the mid-1970s with virtual circuits on X.25 networks. Later, other technologies also started using virtual concepts, such as Frame Relay and ATM, but they are now obsolete.</p>
<p>Loopback interfaces were based on electronics where loopbacks are used to create electric loops for the signal to return to its source for testing purposes. In 1981, the IETF referred<a id="_idIndexMarker249"/> to the reserved address range <code>127.rrr.rrr.rrr</code> with <code>127.rrr.rrr.rrr</code> was officially called <code>127.0.0.1</code>)</p>
<p>Another early implementation<a id="_idIndexMarker252"/> of network virtualization was the <strong class="bold">virtual</strong> <strong class="bold">LAN</strong> or <strong class="bold">VLAN</strong>. By 1981, David Sincoskie was testing segmenting voice-over-Ethernet networks to facilitate fault tolerance, something similar to what VLAN does. However, it was only after 17 years that, in 1998, VLAN was finally published as a standard by IEEE by the name <em class="italic">802.1Q</em>. By the 2000s, switched networks dominated the landscape with switches, repeaters, and bridges, making VLANs commonplace. A LAN without a VLAN is virtually impossible today.</p>
<p>There are several other network virtualization technologies that are used today. Let’s explore the important<a id="_idIndexMarker253"/> ones in the following sections.</p>
<h2 id="_idParaDest-49"><a id="_idTextAnchor049"/>Virtual private networks</h2>
<p>This is the concept of creating<a id="_idIndexMarker254"/> an isolated secured network overlay that is implemented on network carriers, service providers, and over the internet.</p>
<p>In other words, <strong class="bold">virtual private network</strong> (<strong class="bold">VPN</strong>) is a generic term that describes the use of public or private networks to create groups of users that are isolated from other network users, allowing them to communicate between themselves as if they were on a private network.</p>
<p>VPNs use end-to-end traffic encryption to enhance data separation, especially when using public networks, but this is not necessarily the case for all implementations. For instance, when using VPNs in MPLS networks, the traffic is not encrypted as it runs over private domains, and data separation exists only by packet encapsulation.</p>
<p>VPN is a generic name, but more specific names can be found, such as L3VPN, L2VPN, VPLS, Pseudo Wires, and VLLS, among others.</p>
<p class="callout-heading">Important note</p>
<p class="callout">More on VPN and all related<a id="_idIndexMarker255"/> families can be found at <a href="https://datatracker.ietf.org/doc/html/rfc2764">https://datatracker.ietf.org/doc/html/rfc2764</a> and <a href="https://datatracker.ietf.org/doc/html/rfc4026">https://datatracker.ietf.org/doc/html/rfc4026</a>.</p>
<p>The VLAN was perhaps one of the most important virtualizations created in L2 networks. Let’s now look at an interesting virtualization created for router gateways.</p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor050"/>The Virtual Router Redundancy Protocol</h2>
<p>This protocol<a id="_idIndexMarker256"/> was initially created<a id="_idIndexMarker257"/> by Cisco in 1998 with the name <strong class="bold">Hot Standby Router Protocol</strong> (<strong class="bold">HSRP</strong>), defined in <em class="italic">RFC2281</em>. As the use of HSRP was very popular at the time, the IETF Network Working Group created the <strong class="bold">Virtual Router Redundancy Protocol</strong> (<strong class="bold">VRRP</strong>) (<em class="italic">RFC3768</em>).</p>
<p>The concept is simple, giving computers only one default gateway on their routing table by acquiring automatically using DHCP or by configuring manually. To use two routers redundantly, you might need to update all computers or use VRRP. </p>
<p>VRRP uses a virtual Ethernet address to associate with an IP address; this IP address is the default gateway to all computers on the network. <em class="italic">Figure 2.2</em> illustrates <code>10.0.0.1</code> using a virtual MAC address that is associated with both routers.</p>
<div><div><img alt="Figure 2.2 – VRRP using a virtual Ethernet address as the default gateway" src="img/B18165_02_002.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – VRRP using a virtual Ethernet address as the default gateway</p>
<p class="callout-heading">Important note</p>
<p class="callout">More details<a id="_idIndexMarker258"/> on VRRP and HSRP<a id="_idIndexMarker259"/> can be found at <a href="https://datatracker.ietf.org/doc/html/rfc2281">https://datatracker.ietf.org/doc/html/rfc2281</a> and <a href="https://datatracker.ietf.org/doc/html/rfc3768">https://datatracker.ietf.org/doc/html/rfc3768</a>.</p>
<p>VLANs were created<a id="_idIndexMarker260"/> a long time ago, but its concept was used to extend to a more flexible usage, as we are going to see in the next section.</p>
<h2 id="_idParaDest-51"><a id="_idTextAnchor051"/>The Virtual Extensible Local Area Network</h2>
<p>Perhaps the most important<a id="_idIndexMarker261"/> of all in virtualization today is the <strong class="bold">Virtual Extensible Local Area Network</strong> (<strong class="bold">VXLAN</strong>). This standard was published in 2014 and is heavily used for network virtualization to provide connectivity. With VXLANs, it’s possible to create a network with interfaces connected back-to-back to routers like they are physical entities, but in reality, they are virtual.</p>
<p>A VXLAN encapsulates data link layer Ethernet frames (layer 2) within the transport layer using UDP datagrams (layer 4). VXLAN endpoints, which terminate VXLAN<a id="_idIndexMarker262"/> tunnels and may be either virtual or physical switch ports, are known as <strong class="bold">Virtual Tunnel Endpoints</strong> (<strong class="bold">VTEPs</strong>).</p>
<p class="callout-heading">Important note</p>
<p class="callout">More about VXLANs<a id="_idIndexMarker263"/> can be found at <a href="https://datatracker.ietf.org/doc/html/rfc7348">https://datatracker.ietf.org/doc/html/rfc7348</a>.</p>
<p>Let’s now explore an open source project that puts in place several virtual network technologies, including VLANs, VRRP, and VXLANs.</p>
<h2 id="_idParaDest-52"><a id="_idTextAnchor052"/>Open vSwitch</h2>
<p>This open source project is perhaps the most<a id="_idIndexMarker264"/> important in<a id="_idIndexMarker265"/> network virtualization today. <strong class="bold">Open vSwitch</strong> (<strong class="bold">OVS</strong>) runs on any Linux-based virtualization platform (kernel 3.10 and newer) and is used to create connectivity in virtual and physical environments. The majority of the code is written in C, and it supports several protocols including VXLAN, IPSEC, and GRE, among others. OVS is an OpenStack component of SDNs and perhaps the most popular implementation of OpenFlow. A basic architecture of how OVS works can be found in <em class="italic">Figure 2.3</em>.</p>
<div><div><img alt="Figure 2.3 – Simplified OVS architecture" src="img/B18165_02_003.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Simplified OVS architecture</p>
<p>More details<a id="_idIndexMarker266"/> on OVS can be found at <a href="https://github.com/openvswitch/ovs.git">https://github.com/openvswitch/ovs.git</a>.</p>
<h2 id="_idParaDest-53"><a id="_idTextAnchor053"/>Linux Containers</h2>
<p><strong class="bold">Linux Containers</strong> (<strong class="bold">LXC</strong>) provides operating-system-level<a id="_idIndexMarker267"/> virtualization<a id="_idIndexMarker268"/> using CPU, network, memory, and I/O space isolation. Its first implementation was on Linux kernel 2.6.24 in January 2008, but the concept is old and can be found<a id="_idIndexMarker269"/> in a FreeBSD implementation called <strong class="bold">jails</strong> implemented in 1999 and published on FreeBSD 4.0 in March 2000 (details at: docs.freebsd.org/en/books/handbook/jails/).</p>
<p>Today, more and more implementations of LXC can be found, but the concept of CPU, network, memory, and I/O space isolation is the same. The most popular<a id="_idIndexMarker270"/> LXC implementation today is <strong class="bold">Docker</strong>.</p>
<p>With LXC and Open vSwitch, it’s possible to create an entire<a id="_idIndexMarker271"/> virtual network topology<a id="_idIndexMarker272"/> with hundreds of routers. A powerful example is <strong class="bold">Mininet</strong> (<a href="http://mininet.org/">http://mininet.org/</a> and  <a href="https://github.com/mininet/mininet">https://github.com/mininet/mininet</a>).</p>
<p class="callout-heading">Important note</p>
<p class="callout">More on LXC<a id="_idIndexMarker273"/> and FreeBSD jail<a id="_idIndexMarker274"/> can be found at <a href="https://en.wikipedia.org/wiki/LXC">https://en.wikipedia.org/wiki/LXC</a> and <a href="https://en.wikipedia.org/wiki/FreeBSD_jail">https://en.wikipedia.org/wiki/FreeBSD_jail</a>.</p>
<p>Containers for Linux<a id="_idIndexMarker275"/> can create most virtualizations, however<a id="_idIndexMarker276"/> they are limited by using the same operational system because containers share the same kernel. Virtual machines, as we’ll see next, can be used to virtualize a wide range of other operating systems.</p>
<h2 id="_idParaDest-54"><a id="_idTextAnchor054"/>Virtual machines</h2>
<p>LXC is powerful in isolating<a id="_idIndexMarker277"/> parts of the operating system; however, they aren’t able to run applications that require a different CPU or hardware. So, <strong class="bold">virtual machines</strong> (<strong class="bold">VMs</strong>) are there to add this extra virtualization by simulating physical hardware and CPU.</p>
<p>A VM can further isolate the operating system by creating a whole new layer of CPU, I/O, memory, and network. For instance, in network virtualization, it’s possible to run different operating systems with different CPUs, such as Juniper JunOS using Intel CPUs, and Cisco IOS using MIPS CPUs.</p>
<p>The most popular open source<a id="_idIndexMarker278"/> implementation of VMs is <strong class="bold">Xen</strong> (<a href="https://xenproject.org/">https://xenproject.org/</a>).</p>
<p>We do have much more<a id="_idIndexMarker279"/> to talk about regarding network virtualization, but that would be a topic for another book. At least for the time being, what we have examined in this section is sufficient to identify the main technologies used by programmable networks. At this point, you should be able to identify these technologies easily if you encounter them.</p>
<h1 id="_idParaDest-55"><a id="_idTextAnchor055"/>SDNs and OpenFlow</h1>
<p>We have investigated<a id="_idIndexMarker280"/> a few historical milestones of programmable<a id="_idIndexMarker281"/> networks and network virtualization that form the base of what we know today as SDNs. Next, let’s talk about the details behind SDNs.</p>
<p>In order for SDNs to be successful, they need to be flexible and programmable, making it simple to deploy and control traffic and manage their components. None of this could be done without separation between the control plane and the forwarding plane (the data plane).</p>
<p>SDN implementation is done by having an application that uses the decoupling of these two planes to construct the data flows of the network. This application can run in a network server or in a VM, which sends control packets to the network devices using an OpenFlow protocol when possible.</p>
<h2 id="_idParaDest-56"><a id="_idTextAnchor056"/>History of OpenFlow</h2>
<p>OpenFlow is a standard protocol<a id="_idIndexMarker282"/> used in SDNs. Its origins can be traced back to 2006 with the project mentioned earlier in this chapter called Ethane. Eventually, the Ethane<a id="_idIndexMarker283"/> project led to what became known as OpenFlow, thanks to a joint research effort by teams at Stanford and Berkeley universities. The initial idea was to centrally manage policies using a flow-based network and a controller with a focus on network security; that is the reason for <em class="italic">Flow</em> being in the name <em class="italic">OpenFlow</em>.</p>
<p>After the initial work by Berkeley and Stanford, companies such as Nicira and Big Switch Networks started to raise significant amounts of venture capital funding to help push their products with ideas on a flow-based controlled network, but at that time no standards were yet published. A protocol was needed to move network control out of proprietary network switches and into control software that was open source and locally managed. This is the reason that the name <em class="italic">OpenFlow</em> has the word <em class="italic">Open</em> in it.</p>
<p>By 2011, the <strong class="bold">Open Networking Foundation</strong> (<strong class="bold">ONF</strong>) had been created with the aim of standardizing emerging<a id="_idIndexMarker284"/> technologies for networking and data center management. The founding members were Google, Facebook, and Microsoft, while Citrix, Cisco, Dell, HP, F5 Networks, IBM, NEC, Huawei, Juniper Networks, Oracle, and VMware joined later.</p>
<p>The ONF working group<a id="_idIndexMarker285"/> released the first version of the OpenFlow protocol in December 2009, and in February 2011 they made version 1.1 public. The most updated version<a id="_idIndexMarker286"/> is from March 2015 – version 1.5.1 (<a href="https://opennetworking.org/wp-content/uploads/2014/10/openflow-switch-v1.5.1.pdf">https://opennetworking.org/wp-content/uploads/2014/10/openflow-switch-v1.5.1.pdf</a>).</p>
<h2 id="_idParaDest-57"><a id="_idTextAnchor057"/>SDN architecture</h2>
<p>The simple architecture<a id="_idIndexMarker287"/> of an SDN<a id="_idIndexMarker288"/> is shown in <em class="italic">Figure 2.4</em>. The SDN<a id="_idIndexMarker289"/> controller has <strong class="bold">northbound interfaces</strong> (<strong class="bold">NBIs</strong>) toward business-level applications and <strong class="bold">southbound interfaces</strong> (<strong class="bold">SBIs</strong>) toward network devices.</p>
<p>To communicate with network devices, the SBI requires a control protocol. It is desirable for the control protocol to be OpenFlow; however, other protocols can be used if the device does not support it, such as Cisco OpFlex, SNMP, or even CLI via SSH (this will be covered in the next chapter).</p>
<p>The NBI is used to collect information from the business or for the business to collect information from the network (in <em class="italic">Figure 2.4</em>, this is represented by the application plane), for instance, allowing administrators to access the SDN controller to retrieve information about the network. Access to the controller is normally done via an API protocol.</p>
<p>Normally, the NBI<a id="_idIndexMarker290"/> is used for the following:</p>
<ul>
<li>Getting information from devices</li>
<li>Getting the status of physical interfaces</li>
<li>Configuring devices</li>
<li>Constructing data flows between devices</li>
</ul>
<p>But the available methods on the NBI API will depend on the SDN application and what the vendor made available.</p>
<p class="callout-heading">Important note</p>
<p class="callout">It’s important to emphasize that the NBI API for the SDN<a id="_idIndexMarker291"/> has no responsibility for managing the network devices, such as attributing configuration or doing software updates. The main responsibility of the SDN NBI API is to allow administrators and businesses to give directions to the SDN controller in order to make decisions on how traffic will flow through the network devices based on pre-defined criteria.</p>
<p>Now, let’s look at the simple<a id="_idIndexMarker292"/> architecture of an SDN: </p>
<div><div><img alt="Figure 2.4 – Basic SDN architecture" src="img/B18165_02_004.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – Basic SDN architecture</p>
<p>Despite being used<a id="_idIndexMarker293"/> in SDN and being a very well-known term in the internet community, OpenFlow’s future might be not that bright. Let’s find out why.</p>
<h2 id="_idParaDest-58"><a id="_idTextAnchor058"/>OpenFlow and its future</h2>
<p>Looking at how the OpenFlow<a id="_idIndexMarker294"/> standard track is being updated and how vendors are implementing it, its future doesn’t look promising.</p>
<p>The first usable version of OpenFlow<a id="_idIndexMarker295"/> was published in 2011, known as version 1.1. Since then, updates have been incorporated until 2015 with version 1.5.1. But more than six years have passed and no updates have been published yet.</p>
<p>Version 1.6 of OpenFlow has been available<a id="_idIndexMarker296"/> since 2016, but only for members of the ONF, which does not help the user’s confidence in OpenFlow’s future.</p>
<p>In addition to the lack of updates, Cisco (one of the major network vendors) has been working on its own version of OpenFlow called OpFlex since 2014 because it saw<a id="_idIndexMarker297"/> limitations in OpenFlow’s approach. Cisco also has made OpFlex open, allowing others to use without restriction and has started working<a id="_idIndexMarker298"/> on an RFC to publish OpFlex (<a href="https://datatracker.ietf.org/doc/draft-smith-opflex/">https://datatracker.ietf.org/doc/draft-smith-opflex/</a>). </p>
<p>So, the SBIs described in <em class="italic">Figure 2.4</em> do not necessarily use OpenFlow. Today, SDN implementations vary and may use different types of SBIs that are associated to the methods available for device communication for creation of the traffic flow policies.</p>
<p>Other methods and protocols besides OpenFlow are being used with SDN communication, such as OpenStack, OpFlex, CLI vis SSH, SNMP, and NETCONF, among others.</p>
<p>As we’ve seen in this section, the SDN<a id="_idIndexMarker299"/> is a very well-delineated concept on how to work with programmable networks; however, because of the lack of OpenFlow adoption, SDNs have become more of a concept than a standard. From now on, you should have enough knowledge to decide whether your network automation should follow OpenFlow or not.</p>
<h1 id="_idParaDest-59"><a id="_idTextAnchor059"/>Understanding cloud computing </h1>
<p>The goal of <strong class="bold">cloud computing</strong> is to allow users to benefit from virtual<a id="_idIndexMarker300"/> technologies without having in-depth knowledge about them. The objective of cloud computing is to cut costs and help users focus on their core business instead of the physical infrastructure.</p>
<p>Cloud computing advocates for <strong class="bold">Everything as a Service</strong> (<strong class="bold">EaaS</strong>), including <strong class="bold">Infrastructure as a Service</strong> (<strong class="bold">IaaS</strong>), which is implemented by providing<a id="_idIndexMarker301"/> high-level APIs used<a id="_idIndexMarker302"/> to abstract various low-level details of underlying network infrastructure such as physical computing resources, locations, data partitioning, scaling, security, and backup.</p>
<p>Our focus here will be the networking<a id="_idIndexMarker303"/> services offered by cloud computing, which<a id="_idIndexMarker304"/> we usually refer to as cloud networking services.</p>
<h2 id="_idParaDest-60"><a id="_idTextAnchor060"/>Commercial cloud computing</h2>
<p>Perhaps the most popular<a id="_idIndexMarker305"/> cloud computing service today<a id="_idIndexMarker306"/> is the 2002 Amazon-created subsidiary called <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>). AWS uses its proprietary API to offer cloud services; one of them is created by using <strong class="bold">AWS CloudFormation</strong> to provide infrastructure as<a id="_idIndexMarker307"/> code.</p>
<p>In 2008, Google started offering cloud services; in 2010, Microsoft started offering Microsoft Azure; in 2011, IBM announced IBM SmartCloud; and in 2012, Oracle start offering Oracle Cloud.</p>
<p>There are hundreds of other<a id="_idIndexMarker308"/> providers and a list of all can be found at the following link: <a href="https://www.intricately.com/industry/cloud-hosting">https://www.intricately.com/industry/cloud-hosting</a>.</p>
<h2 id="_idParaDest-61"><a id="_idTextAnchor061"/>The OpenStack Foundation</h2>
<p>The <strong class="bold">OpenStack Foundation</strong> was the first initiative created by NASA<a id="_idIndexMarker309"/> and Rackspace to start an open source cloud service software. The foundation<a id="_idIndexMarker310"/> eventually changed its name to the <strong class="bold">OpenInfra Foundation</strong>, and today they have more than 500 members. Their work has been tremendous, and they created a great set of open source code for cloud<a id="_idIndexMarker311"/> computing. More details can be found at <a href="https://openinfra.dev/about/">https://openinfra.dev/about/</a>. </p>
<h2 id="_idParaDest-62"><a id="_idTextAnchor062"/>Cloud Native Computing Foundation</h2>
<p>It sounds<a id="_idIndexMarker312"/> a bit<a id="_idIndexMarker313"/> confusing, but the <strong class="bold">CloudStack Foundation</strong> and the <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>) focus on different aspects of cloud services. The CNCF was basically created by Kubernetes as a Linux-container-based idea, and CloudStack is a bit older and based on VMs.</p>
<p>The CNCF is a Linux Foundation project that was founded in 2015 to help advance Linux container technologies and help to align the tech industry around its evolution. It was announced alongside Kubernetes 1.0, which was given to the Linux Foundation by Google as a seed technology.</p>
<p>We’ve covered quite a bit about cloud computing, but the key takeaway is that even though it was originally intended to add programmability to computers, cloud computing is also growing in the network space. One of the most programmable networks in this space is OpenStack, which we are going to explore next.</p>
<h1 id="_idParaDest-63"><a id="_idTextAnchor063"/>Using OpenStack for networking</h1>
<p>In contrast to OpenFlow, OpenStack<a id="_idIndexMarker314"/> has been busy and promising. It started in 2010 after a joint project between NASA and Rackspace. Rackspace wanted to rewrite the infrastructure code running its cloud servers and at the same time, Anso Labs (contracting for NASA) had published beta code for Nova, a Python-based cloud computing fabric controller.</p>
<p>By 2012, the OpenStack Foundation was established to promote OpenStack software to the cloud community. By 2018, more than 500 companies had joined the OpenStack Foundation. By the end of 2020, the foundation announced<a id="_idIndexMarker315"/> that would change its name starting in 2021 to the <strong class="bold">Open Infrastructure Foundation</strong>. The reason is that the foundation started to add other projects to OpenStack, and therefore the name would not reflect their goals.</p>
<p>OpenStack tracks its versions with different names; the first version in 2010 was called Austin, which included<a id="_idIndexMarker316"/> two components (Nova and Swift). By 2015, the new version of OpenStack<a id="_idIndexMarker317"/> had arrived, which was called Kilo and had 12 components. By October 2021, OpenStack Xena had<a id="_idIndexMarker318"/> been released, with 38 service components (<a href="https://docs.openstack.org/xena/">https://docs.openstack.org/xena/</a>).</p>
<p>For us, what matters in OpenStack are the components that will allow us to automate the network infrastructure. Although not designed for physical devices, the API methods for networking might be extended<a id="_idIndexMarker319"/> to physical devices instead of being only used in cloud virtual environments. </p>
<h2 id="_idParaDest-64"><a id="_idTextAnchor064"/>OpenStack Neutron</h2>
<p>The goal of OpenStack<a id="_idIndexMarker320"/> is to create standard services that allow software engineers to integrate their applications with cloud computing services. The Xena version released in October 2021 had 38 services available.</p>
<p>One of the most important<a id="_idIndexMarker321"/> services for networking is called <strong class="bold">Neutron</strong> (or <strong class="bold">OpenStack Networking</strong>), which is an OpenStack project<a id="_idIndexMarker322"/> aimed at providing <em class="italic">network connectivity as a service</em> between interface devices. It implements the OpenStack networking API.</p>
<p class="callout-heading">Important note</p>
<p class="callout">Neutron API definitions<a id="_idIndexMarker323"/> can be found at the following link: <a href="https://docs.openstack.org/api-ref/network/">https://docs.openstack.org/api-ref/network/</a>.</p>
<p>Neutron manages<a id="_idIndexMarker324"/> all networking configurations for the <strong class="bold">virtual networking infrastructure</strong> (<strong class="bold">VNI</strong>) and the access layer aspects of the <strong class="bold">physical networking infrastructure</strong> (<strong class="bold">PNI</strong>). It also enables projects to create advanced virtual network<a id="_idIndexMarker325"/> topologies, which may include services such as a firewall and a VPN. It provides networks, subnets, and routers as object abstractions. Each abstraction has functionality that mimics its physical counterpart: networks contain subnets, and routers route traffic between different subnets and networks.</p>
<p>For more details<a id="_idIndexMarker326"/> on Neutron, visit the following link: <a href="https://docs.openstack.org/neutron">https://docs.openstack.org/neutron</a>.</p>
<h2 id="_idParaDest-65"><a id="_idTextAnchor065"/>The Neutron API</h2>
<p>The <strong class="bold">Neutron API</strong> is a RESTful HTTP service that uses all<a id="_idIndexMarker327"/> aspects of the HTTP protocol, including methods, URIs, media types, response codes, and more. API clients can use existing features of the protocol, including caching, persistent connections, and content compression.</p>
<p>As an example, let’s look at the <code>HTTP GET</code> BGP peers’ method.</p>
<p>To obtain a list of BGP peers use a <code>HTTP</code> <code>GET</code> request to <code>/v2.0/bgp-peers</code>. The possible responses are as follows:</p>
<ul>
<li>Normal response codes: <code>200</code></li>
<li>Error response codes: <code>400, 401, 403</code></li>
</ul>
<p>Fields that can be added to the API request: </p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-1">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Name</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">In</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Type</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Description</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>fields </code>(optional)</p>
</td>
<td class="No-Table-Style">
<p>Query</p>
</td>
<td class="No-Table-Style">
<p>String</p>
</td>
<td class="No-Table-Style">
<p>The fields that you want the server to return. If a <code>fields</code> query parameter is not specified, the networking API returns all attributes allowed by the policy settings. By using the <code>fields</code> parameter, the API returns only the requested set of attributes. A <code>fields</code> parameter can be specified multiple times. For example, if you specify <code>fields=id&amp;fields=name</code> in the request URL, only the <code>id</code> and <code>name</code> attributes will be returned.</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 2.1 – API request fields</p>
<p>The parameters that are returned in the API response are as follows:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table002">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Name</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">In</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Type</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Description</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>bgp_peers</code></p>
</td>
<td class="No-Table-Style">
<p>Body</p>
</td>
<td class="No-Table-Style">
<p>Array</p>
</td>
<td class="No-Table-Style">
<p>A list of <code>bgp_peer</code> objects. Each <code>bgp_peer</code> object represents real BGP infrastructure, such as routers, route reflectors, and route servers.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>remote_as</code></p>
</td>
<td class="No-Table-Style">
<p>Body</p>
</td>
<td class="No-Table-Style">
<p>String</p>
</td>
<td class="No-Table-Style">
<p>The remote autonomous system number of the BGP peer.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>name</code></p>
</td>
<td class="No-Table-Style">
<p>Body</p>
</td>
<td class="No-Table-Style">
<p>String</p>
</td>
<td class="No-Table-Style">
<p>A more descriptive name of the BGP peer.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>peer_ip</code></p>
</td>
<td class="No-Table-Style">
<p>Body</p>
</td>
<td class="No-Table-Style">
<p>String</p>
</td>
<td class="No-Table-Style">
<p>The IP address of the peer.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>id</code></p>
</td>
<td class="No-Table-Style">
<p>Body</p>
</td>
<td class="No-Table-Style">
<p>String</p>
</td>
<td class="No-Table-Style">
<p>The ID of the BGP peer.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>tenant_id</code></p>
</td>
<td class="No-Table-Style">
<p>Body</p>
</td>
<td class="No-Table-Style">
<p>String</p>
</td>
<td class="No-Table-Style">
<p>The ID of the tenant.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><code>project_id</code></p>
</td>
<td class="No-Table-Style">
<p>Body</p>
</td>
<td class="No-Table-Style">
<p>String</p>
</td>
<td class="No-Table-Style">
<p>The ID of the project.</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 2.2 – API response fields</p>
<p>The following<a id="_idIndexMarker328"/> is an example of the API response:</p>
<pre class="source-code">
{ 
    "bgp_peers":[ 
    { 
        "auth_type":"none", 
        "remote_as":1001, 
        "name":"bgp-peer",
        "tenant_id":"34a6e17a48cf414ebc890367bf42266b", 
        "peer_ip":"10.0.0.3", 
        "id":"a7193581-a31c-4ea5-8218-b3052758461f" 
    }
    ]
}</pre>
<p>The API<a id="_idIndexMarker329"/> is well documented at the following link: <a href="https://docs.openstack.org/api-ref/network/">https://docs.openstack.org/api-ref/network/</a>.</p>
<p>As we have seen<a id="_idIndexMarker330"/> in this section, OpenStack is perhaps the cloud computing platform that is closest to network programming, as demonstrated by the CloudStack Neutron API. Additional features are probably going to be added as more network elements are migrated to the cloud. You should now be familiar with OpenStack terms and be able to explore them in depth if necessary.</p>
<h1 id="_idParaDest-66"><a id="_idTextAnchor066"/>Summary </h1>
<p>In this chapter, we talked about how programmable networks have evolved up until the present day. We discussed the history of data plane and control plane separation. We’ve seen how network virtualization has improved over time. We also looked at some technologies and standards for SDNs and cloud networking, such as OpenFlow and OpenStack. </p>
<p>You now have the knowledge required to understand why some technologies are used today to automate and code networks.  </p>
<p>In the next chapter, we’re going to dive deeper into the methods, protocols, and standards used to configure and communicate with network devices.</p>
</div>
</body></html>