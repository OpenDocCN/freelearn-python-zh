["```py\nimport pyspark\nfrom pyspark.sql import SparkSession\nspark1 = SparkSession.builder.master(\"local[2]\")\n    .appName('New App').getOrCreate()\n```", "```py\n>>> spark\n<pyspark.sql.session.SparkSession object at 0x1091019e8>\n>>> spark1\n<pyspark.sql.session.SparkSession object at 0x1091019e8>\n```", "```py\nimport pyspark\nfrom pyspark.sql import SparkSession\nspark2 = spark.newSession()\n```", "```py\n>>> spark2\n<pyspark.sql.session.SparkSession object at 0x10910df98>\n```", "```py\ndata = [5, 4, 6, 3, 2, 8, 9, 2, 8, 7,\n        8, 4, 4, 8, 2, 7, 8, 9, 6, 9]\nrdd1 = spark.sparkContext.parallelize(data)\nprint(rdd1.getNumPartitions())\nrdd2 = spark.sparkContext.textFile('sample.txt')\nprint(rdd2.getNumPartitions())\n```", "```py\nrdd1 = spark.sparkContext.textFile('sample.txt') \nrdd2 = rdd1.map(lambda lines: lines.lower())\nrdd3 = rdd1.map(lambda lines: lines.upper())\nprint(rdd2.collect())\nprint(rdd3.collect())\n```", "```py\ndata = [5, 4, 6, 3, 2, 8, 9, 2, 8, 7,\n        8, 4, 4, 8, 2, 7, 8, 9, 6, 9]\nrdd1 = spark.sparkContext.parallelize(data)\nrdd2 = rdd1.filter(lambda x: x % 2 !=0 )\nprint(rdd2.collect())\n```", "```py\ndata = [5, 4, 6, 3, 2, 8, 9, 2, 8, 7,\n        8, 4, 4, 8, 2, 7, 8, 9, 6, 9]\nrdd1 = spark.sparkContext.parallelize(data)\nprint(\"RDD contents with partitions:\" + str(rdd1.glom().  collect()))\nprint(\"Count by values: \" +str(rdd1.countByValue()))\nprint(\"reduce function: \" + str(rdd1.glom().collect()))\nprint(\"Sum of RDD contents:\"+str(rdd1.sum()))\nprint(\"top: \" + str(rdd1.top(5)))\nprint(\"count: \" + str(rdd1.count()))\nprint(\"max: \"+ str(rdd1.max()))\nprint(\"min\" + str(rdd1.min()))\ntime.sleep(60)\n```", "```py\ndata = [('James','','Bylsma','HR','M',40000),\n  ('Kamal','Rahim','','HR','M',41000),\n  ('Robert','','Zaine','Finance','M',35000),\n  ('Sophia','Anne','Richer','Finance','F',47000),\n  ('John','Will','Brown','Engineering','F',65000)\n]\ncolumns = [\"firstname\",\"middlename\",\"lastname\",\n           \"department\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data=data, schema = columns)\nprint(df.printSchema())\nprint(df.show())\n```", "```py\nroot\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- department: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n+---------+----------+--------+-----------+------+-------+\n|firstname|middlename|lastname| department|gender|salary|\n+---------+----------+--------+-----------+------+-------+\n|    James|          |  Bylsma|         HR|     M|  40000|\n|    Kamal|     Rahim|        |         HR|     M|  41000|\n|   Robert|          |   Zaine|    Finance|     M|  35000|\n|   Sophia|      Anne|  Richer|    Finance|     F|  47000|\n|     John|      Will|   Brown|Engineering|     F|  65000|\n+---------+----------+--------+-----------+------+-------+ \n```", "```py\nschemas = StructType([ \\\n    StructField(\"firstname\",StringType(),True), \\\n    StructField(\"middlename\",StringType(),True), \\\n    StructField(\"lastname\",StringType(),True), \\\n    StructField(\"department\", StringType(), True), \\\n    StructField(\"gender\", StringType(), True), \\\n    StructField(\"salary\", IntegerType(), True) \\\n  ])\ndf = spark.read.csv('df2.csv', header=True, schema=schemas)\nprint(df.printSchema())\nprint(df.show())\n```", "```py\ndata = [('James','','Bylsma','HR','M',40000),\n  ('Kamal','Rahim','','HR','M',41000),\n  ('Robert','','Zaine','Finance','M',35000),\n  ('Sophia','Anne','Richer','Finance','F',47000),\n  ('John','Will','Brown','Engineering','F',65000)\n]\ncolumns = [\"firstname\",\"middlename\",\"lastname\",\n           \"department\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data=data, schema = columns)\n#show two columns\nprint(df.select([df.firstname, df.salary]).show())\n#replacing values of a column\nmyDict = {'F':'Female','M':'Male'}\ndf2 = df.replace(myDict, subset=['gender'])\n#adding a new colum Pay Level based on an existing column   values\ndf3 = df2.withColumn(\"Pay Level\",\n      when((df2.salary < 40000), lit(\"10\")) \\\n     .when((df.salary >= 40000) & (df.salary <= 50000),           lit(\"11\")) \\\n     .otherwise(lit(\"12\")) \\\n  )\nprint(df3.show())\n```", "```py\ndata = [('James','','Bylsma','HR','M',40000),\n  ('Kamal','Rahim','','HR','M',41000),\n  ('Robert','','Zaine','Finance','M',35000),\n  ('Sophia','Anne','Richer','Finance','F',47000),\n  ('John','Will','Brown','Engineering','F',65000)\n]\ncolumns = [\"firstname\",\"middlename\",\"lastname\",\n           \"department\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data=data, schema = columns)\ndf.createOrReplaceTempView(\"EMP_DATA\")\ndf2 = spark.sql(\"SELECT * FROM EMP_DATA\")\nprint(df2.show())\ndf3 = spark.sql(\"SELECT firstname,middlename,lastname,    salary FROM EMP_DATA WHERE SALARY > 45000\")\nprint(df3.show())\ndf4 = spark.sql((\"SELECT gender, count(*) from EMP_DATA     group by gender\"))\nprint(df4.show())\n```", "```py\n+---------+----------+--------+-----------+------+------+\n|firstname|middlename|lastname| department|gender|salary|\n+---------+----------+--------+-----------+------+------+\n|    James|          |  Bylsma|         HR|     M| 40000|\n|    Kamal|     Rahim|        |         HR|     M| 41000|\n|   Robert|          |   Zaine|    Finance|     M| 35000|\n|   Sophia|      Anne|  Richer|    Finance|     F| 47000|\n|     John|      Will|   Brown|Engineering|     F| 65000|\n+---------+----------+--------+-----------+------+------+\n+---------+----------+--------+------+\n|firstname|middlename|lastname|salary|\n+---------+----------+--------+------+\n|   Sophia|      Anne|  Richer| 47000|\n|     John|      Will|   Brown| 65000|\n+---------+----------+--------+------+\n+------+--------+\n|gender|count(1)|\n+------+--------+\n|     F|       2|\n|     M|       3|\n+------+--------+\n```", "```py\n#casestudy1.py: Pi calculator\nfrom operator import add\nfrom random import random\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master\n        (\"spark://192.168.64.2:7077\") \\\n    .appName(\"Pi claculator app\") \\\n    .getOrCreate()\npartitions = 2\nn = 10000000 * partitions\ndef func(_):\n    x = random() * 2 – 1\n    y = random() * 2 – 1\n    return 1 if x ** 2 + y ** 2 <= 1 else 0\ncount = spark.sparkContext.parallelize(range(1, n + 1),     partitions).map(func).reduce(add)\nprint(\"Pi is roughly %f\" % (4.0 * count / n))\n```", "```py\nPi is roughly 3.141479 \n```", "```py\n#casestudy2.py: word count application\nimport matplotlib.pyplot as plt\nfrom pyspark.sql import SparkSession\nfrom wordcloud import WordCloud\nspark = SparkSession.builder.master(\"local[*]\")\\\n    .appName(\"word cloud app\")\\\n    .getOrCreate()\nwc_threshold = 1\nwl_threshold = 3\ntextRDD = spark.sparkContext.textFile('wordcloud.txt',3)\nflatRDD = textRDD.flatMap(lambda x: x.split(' '))\nwcRDD = flatRDD.map(lambda word: (word, 1)).\\\n    reduceByKey(lambda v1, v2: v1 + v2)\n# filter out words with fewer than threshold occurrences\nfilteredRDD = wcRDD.filter(lambda pair: pair[1] >=     wc_threshold)\nfilteredRDD2 = filteredRDD.filter(lambda pair:     len(pair[0]) > wl_threshold)\nword_freq = dict(filteredRDD2.collect())\n# Create the wordcloud object\nwordcloud = WordCloud(width=480, height=480, margin=0).\\\n    generate_from_frequencies(word_freq)\n# Display the generated cloud image\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.show()\n```"]