["```py\n{\n  \"questions\": [\n    {\n      \"title\": \"Ending of John Carpenter's The Thing\",\n      \"body\": \"In the ending of John Carpenter's classic 1982 sci-fi horror film The Thing, is ...\",\n      \"author\": \"JMFB\",\n      \"answers\": [\n        {\n          \"body\": \"This is the million dollar question, ... Unfortunately, he is notoriously ... \",\n           \"author\": \"Richard\",\n        },\n        {\n          \"body\": \"Not to point out what may seem obvious, but Childs isn't breathing. Note the total absence of \",\n          \"author\": \"user42\"\n          }\n      ]\n    },\n    {\n      \"title\": \"Was it ever revealed what pedaling the bicycles in the second episode was doing?\",\n      \"body\": \"I'm going to assume they were probably some sort of turbine...electricity...something, but I'd prefer to know for sure.\",\n       \"author\": \"bartz\",\n      \"answers\": [\n        {\n          \"body\": \"The Wikipedia synopsis states: most citizens make a living pedaling exercise bikes all day in order to generate power for their environment\",\n          \"author\": \"Jack Nimble\"\n        }\n      ]\n    }\n  ]\n}\n```", "```py\n$ pip  install requests  beautifulsoup4\n\n```", "```py\nfrom bs4 import BeautifulSoup\nimport requests\nimport json\n\nSO_URL = \"http://scifi.stackexchange.com\"\nQUESTION_LIST_URL = SO_URL + \"/questions\"\nMAX_PAGE_COUNT = 20\n\nglobal_results = []\ninitial_page = 1 #first page is page 1\n\ndef get_author_name(body):\n  link_name = body.select(\".user-details a\")\n  if len(link_name) == 0:\n    text_name = body.select(\".user-details\")\n    return text_name[0].text if len(text_name) > 0 else 'N/A'\n  else:\n    return link_name[0].text\n\ndef get_question_answers(body):\n  answers = body.select(\".answer\")\n  a_data = []\n  if len(answers) == 0:\n    return a_data\n\n  for a in answers:\n    data = {\n      'body': a.select(\".post-text\")[0].get_text(),\n      'author': get_author_name(a)\n    }\n    a_data.append(data)\n  return a_data\n\ndef get_question_data ( url ):\n  print \"Getting data from question page: %s \" % (url)\n  resp = requests.get(url)\n  if resp.status_code != 200:\n    print \"Error while trying to scrape url: %s\" % (url)\n    return\n  body_soup = BeautifulSoup(resp.text)\n  #define the output dict that will be turned into a JSON structue\n  q_data = {\n    'title': body_soup.select('#question-header .question-hyperlink')[0].text,\n    'body': body_soup.select('#question .post-text')[0].get_text(),\n    'author': get_author_name(body_soup.select(\".post-signature.owner\")[0]),\n    'answers': get_question_answers(body_soup)\n  }\n  return q_data\n\ndef get_questions_page ( page_num, partial_results ):\n  print \"=====================================================\"\n  print \" Getting list of questions for page %s\" % (page_num)\n  print \"=====================================================\"\n\n  url = QUESTION_LIST_URL + \"?sort=newest&page=\" + str(page_num)\n  resp = requests.get(url)\n  if resp.status_code != 200:\n    print \"Error while trying to scrape url: %s\" % (url)\n    return\n  body = resp.text\n  main_soup = BeautifulSoup(body)\n\n  #get the urls for each question\n  questions = main_soup.select('.question-summary .question-hyperlink')\n  urls = [ SO_URL + x['href'] for x in questions]\n  for url in urls:\n    q_data = get_question_data(url)\n    partial_results.append(q_data)\n  if page_num < MAX_PAGE_COUNT:\n    get_questions_page(page_num + 1, partial_results)\n\nget_questions_page(initial_page, global_results)\nwith open('scrapping-results.json', 'w') as outfile:\n  json.dump(global_results, outfile, indent=4)\n\nprint '----------------------------------------------------'\nprint 'Results saved'\n```", "```py\n#analyzer.py\nimport operator\nimport string\nimport nltk\nfrom nltk.util import ngrams\nimport json\nimport re\nimport visualizer\n\nSOURCE_FILE = './scrapping-results.json'\n\n# Load the json file and return the resulting dict\ndef load_json_data(file):\n  with open(file) as input_file:\n    return json.load(input_file)\n\ndef analyze_data(d):\n  return {\n    'shortest_answer': get_shortest_answer(d),\n    'most_active_users': get_most_active_users(d, 10),\n    'most_active_topics': get_most_active_topics(d, 10),\n    'most_helpful_user': get_most_helpful_user(d, 10),\n    'most_answered_questions': get_most_answered_questions(d, 10),\n    'most_common_phrases':  get_most_common_phrases(d, 10, 4),\n  }\n\n# Creates a single, lower cased string from the bodies of all questions\ndef flatten_questions_body(data):\n  body = []\n  for q in data:\n    body.append(q['body'])\n  return '. '.join(body)\n\n# Creates a single, lower cased string from the titles of all questions\ndef flatten_questions_titles(data):\n  body = []\n  pattern = re.compile('(\\[|\\])')\n  for q in data:\n    lowered = string.lower(q['title'])\n    filtered = re.sub(pattern, ' ', lowered)\n    body.append(filtered)\n  return '. '.join(body)\n```", "```py\n# Returns the top \"limit\" users with the most questions asked\ndef get_most_active_users(data, limit):\n  names = {}\n  for q in data:\n    if q['author'] not in names:\n      names[q['author']] = 1\n    else:\n      names[q['author']] += 1\n  return sorted(names.items(), reverse=True, key=operator.itemgetter(1))[:limit]\n\ndef get_node_content(node):\n  return ' '.join([x[0] for x in node])\n\n# Tries to extract the most common topics from the question's titles\ndef get_most_active_topics(data, limit):\n  body = flatten_questions_titles(data)\n  sentences = nltk.sent_tokenize(body)\n  sentences = [nltk.word_tokenize(sent) for sent in sentences]\n  sentences = [nltk.pos_tag(sent) for sent in sentences]\n  grammar = \"NP: {<JJ>?<NN.*>}\"\n  cp = nltk.RegexpParser(grammar)\n  results = {}\n  for sent in sentences:\n    parsed = cp.parse(sent)\n    trees = parsed.subtrees(filter=lambda x: x.label() == 'NP')\n    for t in trees:\n      key = get_node_content(t)\n      if key in results:\n        results[key] += 1\n      else:\n        results[key] = 1\n  return sorted(results.items(), reverse=True, key=operator.itemgetter(1))[:limit]\n\n# Returns the user that has the most answers\ndef get_most_helpful_user(data, limit):\n  helpful_users = {}\n  for q in data:\n    for a in q['answers']:\n      if a['author'] not in helpful_users:\n        helpful_users[a['author']] = 1\n      else:\n        helpful_users[a['author']] += 1\n\n  return sorted(helpful_users.items(), reverse=True, key=operator.itemgetter(1))[:limit]\n\n# returns the top \"limit\" questions with the most amount of answers\ndef get_most_answered_questions(d, limit):\n  questions = {}\n\n  for q in d:\n    questions[q['title']] = len(q['answers'])\n  return sorted(questions.items(), reverse=True, key=operator.itemgetter(1))[:limit]\n\n# Finds a list of the most common phrases of 'length' length\ndef get_most_common_phrases(d, limit, length):\n  body = flatten_questions_body(d)\n  phrases = {}\n  for sentence in nltk.sent_tokenize(body):\n    words = nltk.word_tokenize(sentence)\n    for phrase in ngrams(words, length):\n      if all(word not in string.punctuation for word in phrase):\n        key = ' '.join(phrase)\n        if key in phrases:\n          phrases[key] += 1\n        else:\n         phrases[key] = 1\n\n  return sorted(phrases.items(), reverse=True, key=operator.itemgetter(1))[:limit]\n\n# Finds the answer with the least amount of characters\ndef get_shortest_answer(d):\n\n  shortest_answer = {\n    'body': '',\n    'length': -1\n  }\n  for q in d:\n    for a in q['answers']:\n      if len(a['body']) < shortest_answer['length'] or shortest_answer['length'] == -1:\n        shortest_answer = {\n          'question': q['body'],\n          'body': a['body'],\n          'length': len(a['body'])\n        }\n  return shortest_answer\n```", "```py\ndata_dict = load_json_data(SOURCE_FILE)\n\nresults = analyze_data(data_dict)\n\nprint \"=== ( Shortest Answer ) === \"\nvisualizer.displayShortestAnswer(results['shortest_answer'])\n\nprint \"=== ( Most Active Users ) === \"\nvisualizer.displayMostActiveUsers(results['most_active_users'])\n\nprint \"=== ( Most Active Topics ) === \"\nvisualizer.displayMostActiveTopics(results['most_active_topics'])\n\nprint \"=== ( Most Helpful Users ) === \"\nvisualizer.displayMostHelpfulUser(results['most_helpful_user'])\n\nprint \"=== ( Most Answered Questions ) === \"\nvisualizer.displayMostAnsweredQuestions(results['most_answered_questions'])\n\nprint \"=== ( Most Common Phrases ) === \"\nvisualizer.displayMostCommonPhrases(results['most_common_phrases'])\n```", "```py\n#visualizer.py\ndef displayShortestAnswer(data):\n  print \"A: %s\" % (data['body'])\n  print \"Q: %s\" % (data['question'])\n  print \"Length: %s characters\" % (data['length'])\n\ndef displayMostActiveUsers(data):\n  index = 1\n  for u in data:\n    print \"%s - %s (%s)\" % (index, u[0], u[1])\n    index += 1\n\ndef displayMostActiveTopics(data):\n  index = 1\n  for u in data:\n    print \"%s - %s (%s)\" % (index, u[0], u[1])\n    index += 1\n\ndef displayMostHelpfulUser(data):\n  index = 1\n  for u in data:\n    print \"%s - %s (%s)\" % (index, u[0], u[1])\n    index += 1\n\ndef displayMostAnsweredQuestions(data):\n  index = 1\n  for u in data:\n    print \"%s - %s (%s)\" % (index, u[0], u[1])\n    index += 1\n\ndef displayMostCommonPhrases(data):\n  index = 1\n  for u in data:\n    print \"%s - %s (%s)\" % (index, u[0], u[1])\n    index += 1\n```", "```py\n$ time python scraper.py\n\n```", "```py\nfrom bs4 import BeautifulSoup\nimport requests\nimport json\nimport threading\n\nSO_URL = \"http://scifi.stackexchange.com\"\nQUESTION_LIST_URL = SO_URL + \"/questions\"\nMAX_PAGE_COUNT = 20\n\nclass ThreadManager:\n  instance = None\n  final_results = []\n  threads_done = 0\n  totalConnections = 4 #Number of parallel threads working, will affect the total amount of pages per thread\n\n  @staticmethod\n  def notify_connection_end( partial_results ):\n    print \"==== Thread is done! =====\"\n    ThreadManager.threads_done += 1\n    ThreadManager.final_results += partial_results\n    if ThreadManager.threads_done == ThreadManager.totalConnections:\n      print \"==== Saving data to file! ====\"\n      with open('scrapping-results-optimized.json', 'w') as outfile:\n        json.dump(ThreadManager.final_results, outfile, indent=4)\n```", "```py\ndef get_author_name(body):\n  link_name = body.select(\".user-details a\")\n  if len(link_name) == 0:\n    text_name = body.select(\".user-details\")\n    return text_name[0].text if len(text_name) > 0 else 'N/A'\n  else:\n    return link_name[0].text\n\ndef get_question_answers(body):\n  answers = body.select(\".answer\")\n  a_data = []\n  if len(answers) == 0:\n    return a_data\n\n  for a in answers:\n    data = {\n      'body': a.select(\".post-text\")[0].get_text(),\n      'author': get_author_name(a)\n    }\n    a_data.append(data)\n  return a_data\n\ndef get_question_data ( url ):\n  print \"Getting data from question page: %s \" % (url)\n  resp = requests.get(url)\n  if resp.status_code != 200:\n    print \"Error while trying to scrape url: %s\" % (url)\n    return\n  body_soup = BeautifulSoup(resp.text)\n  #define the output dict that will be turned into a JSON structue\n  q_data = {\n    'title': body_soup.select('#question-header .question-hyperlink')[0].text,\n    'body': body_soup.select('#question .post-text')[0].get_text(),\n    'author': get_author_name(body_soup.select(\".post-signature.owner\")[0]),\n    'answers': get_question_answers(body_soup)\n  }\n  return q_data\n\ndef get_questions_page ( page_num, end_page, partial_results  ):\n  print \"=====================================================\"\n  print \" Getting list of questions for page %s\" % (page_num)\n  print \"=====================================================\"\n\n  url = QUESTION_LIST_URL + \"?sort=newest&page=\" + str(page_num)\n  resp = requests.get(url)\n  if resp.status_code != 200:\n    print \"Error while trying to scrape url: %s\" % (url)\n  else:\n    body = resp.text\n    main_soup = BeautifulSoup(body)\n\n    #get the urls for each question\n    questions = main_soup.select('.question-summary .question-hyperlink')\n    urls = [ SO_URL + x['href'] for x in questions]\n    for url in urls:\n      q_data = get_question_data(url)\n     partial_results.append(q_data)\n  if page_num + 1 < end_page:\n    get_questions_page(page_num + 1,  end_page, partial_results)\n  else:\n    ThreadManager.notify_connection_end(partial_results)\npages_per_connection = MAX_PAGE_COUNT / ThreadManager.totalConnections\nfor i in range(ThreadManager.totalConnections):\n init_page = i * pages_per_connection\n end_page = init_page + pages_per_connection\n t = threading.Thread(target=get_questions_page,\n args=(init_page, end_page, [],  ),\n name='connection-%s' % (i))\n  t.start()\n```", "```py\n#analyzer_cython.pyx\nimport operator\nimport string\nimport nltk\nfrom nltk.util import ngrams\nimport json\nimport re\n\nSOURCE_FILE = './scrapping-results.json'\n\n# Returns the top \"limit\" users with the most questions asked\ndef get_most_active_users(data, int limit ):\n  names = {}\n  for q in data:\n    if q['author'] not in names:\n      names[q['author']] = 1\n    else:\n      names[q['author']] += 1\n  return sorted(names.items(), reverse=True, key=operator.itemgetter(1))[:limit]\n\ndef get_node_content(node):\n  return ' '.join([x[0] for x in node])\n\n# Tries to extract the most common topics from the question's titles\ndef get_most_active_topics(data, int limit ):\n  body = flatten_questions_titles(data)\n  sentences = nltk.sent_tokenize(body)\n  sentences = [nltk.word_tokenize(sent) for sent in sentences]\n  sentences = [nltk.pos_tag(sent) for sent in sentences]\n  grammar = \"NP: {<JJ>?<NN.*>}\"\n  cp = nltk.RegexpParser(grammar)\n  results = {}\n  for sent in sentences:\n    parsed = cp.parse(sent)\n    trees = parsed.subtrees(filter=lambda x: x.label() == 'NP')\n    for t in trees:\n      key = get_node_content(t)\n      if key in results:\n        results[key] += 1\n      else:\n        results[key] = 1\n  return sorted(results.items(), reverse=True, key=operator.itemgetter(1))[:limit]\n\n# Returns the user that has the most answers\ndef get_most_helpful_user(data, int limit ):\n  helpful_users = {}\n  for q in data:\n    for a in q['answers']:\n      if a['author'] not in helpful_users:\n        helpful_users[a['author']] = 1\n      else:\n        helpful_users[a['author']] += 1\n\n  return sorted(helpful_users.items(), reverse=True, key=operator.itemgetter(1))[:limit]\n\n# returns the top \"limit\" questions with the most amount of answers\ndef get_most_answered_questions(d, int limit ):\n  questions = {}\n\n  for q in d:\n    questions[q['title']] = len(q['answers'])\n  return sorted(questions.items(), reverse=True, key=operator.itemgetter(1))[:limit]\n\n# Creates a single, lower cased string from the bodies of all questions\ndef flatten_questions_body(data):\n  body = []\n  for q in data:\n    body.append(q['body'])\n  return '. '.join(body)\n\n# Creates a single, lower cased string from the titles of all questions\ndef flatten_questions_titles(data):\n  body = []\n  pattern = re.compile('(\\[|\\])')\n  for q in data:\n    lowered = string.lower(q['title'])\n    filtered = re.sub(pattern, ' ', lowered)\n    body.append(filtered)\n  return '. '.join(body)\n\n# Finds a list of the most common phrases of 'length' length\ndef get_most_common_phrases(d, int limit , int length ):\n  body = flatten_questions_body(d)\n  phrases = {}\n  for sentence in nltk.sent_tokenize(body):\n    words = nltk.word_tokenize(sentence)\n    for phrase in ngrams(words, length):\n      if all(word not in string.punctuation for word in phrase):\n        key = ' '.join(phrase)\n        if key in phrases:\n          phrases[key] += 1\n        else:\n          phrases[key] = 1\n\n  return sorted(phrases.items(), reverse=True, key=operator.itemgetter(1))[:limit]\n\n# Finds the answer with the least amount of characters\ndef get_shortest_answer(d):\n  cdef int shortest_length = 0;\n\n  shortest_answer = {\n    'body': '',\n    'length': -1\n  }\n  for q in d:\n    for a in q['answers']:\n if len(a['body']) < shortest_length or shortest_length == 0:\n shortest_length = len(a['body'])\n        shortest_answer = {\n          'question': q['body'],\n          'body': a['body'],\n          'length': shortest_length\n        }\n  return shortest_answer\n\n# Load the json file and return the resulting dict\ndef load_json_data(file):\n  with open(file) as input_file:\n    return json.load(input_file)\n\ndef analyze_data(d):\n  return {\n    'shortest_answer': get_shortest_answer(d),\n    'most_active_users': get_most_active_users(d, 10),\n    'most_active_topics': get_most_active_topics(d, 10),\n    'most_helpful_user': get_most_helpful_user(d, 10),\n    'most_answered_questions': get_most_answered_questions(d, 10),\n    'most_common_phrases':  get_most_common_phrases(d, 10, 4),\n  }\n```", "```py\n#analyzer-setup.py\nfrom distutils.core import setup\nfrom Cython.Build import cythonize\n\nsetup(\n  name = 'Analyzer app',\n  ext_modules = cythonize(\"analyzer_cython.pyx\"),\n)\n```", "```py\n#analyzer-use-cython.py\nimport analyzer_cython as analyzer\nimport visualizer\n\ndata_dict = analyzer.load_json_data(analyzer.SOURCE_FILE)\n\nresults = analyzer.analyze_data(data_dict)\n\nprint \"=== ( Shortest Answer ) === \"\nvisualizer.displayShortestAnswer(results['shortest_answer'])\n\nprint \"=== ( Most Active Users ) === \"\nvisualizer.displayMostActiveUsers(results['most_active_users'])\n\nprint \"=== ( Most Active Topics ) === \"\nvisualizer.displayMostActiveTopics(results['most_active_topics'])\n\nprint \"=== ( Most Helpful Users ) === \"\nvisualizer.displayMostHelpfulUser(results['most_helpful_user'])\n\nprint \"=== ( Most Answered Questions ) === \"\nvisualizer.displayMostAnsweredQuestions(results['most_answered_questions'])\n\nprint \"=== ( Most Common Phrases ) === \"\nvisualizer.displayMostCommonPhrases(results['most_common_phrases'])\n```", "```py\n$ python analyzer-setup.py build_ext â€“inplace\n\n```"]