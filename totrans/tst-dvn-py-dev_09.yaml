- en: Chapter 9. Unit Testing Patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, we have looked at various patterns and anti-patterns in
    TDD. In this chapter, you are going to take a look at some additional patterns
    that we haven't discussed before in this book. In the process of doing so, you
    will also take a look at some more advanced features provided by the Python `unittest`
    module, such as test loaders, test runners, and skipping tests.
  prefs: []
  type: TYPE_NORMAL
- en: Pattern – fast tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the key goals of TDD is to write tests that execute quickly. We will
    be running the tests often when doing TDD— possibly even every few minutes. The
    TDD habit is to run the tests multiple times when developing code, refactoring,
    before checkins, and before deployments. If tests run any longer, we will be reluctant
    to run them often, which defeats the purpose of the tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that in mind, some techniques for keeping tests fast are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Disable unwanted external services**: Some services are not central to the
    purpose of the application and can be disabled. For instance, perhaps we use a
    service to collect analytics on how users use our application. Our application
    might be making a call to this service on every action. Such services can be disabled,
    enabling tests to run faster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mock out external services**: Other external services such as servers, databases,
    caches, and so on might be central to the functioning of the application. External
    services take time to start up, shut down, and communicate with. We want to mock
    these out and have our tests run against the mock.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use fast variants of services**: If we must use a service, then make sure
    it is fast. For example, replace a database with an in-memory database, which
    is much faster and takes little time to start and shut down. Similarly, we can
    replace a call to an e-mail server with a fake in-memory e-mail server that just
    records the e-mails to be sent, without actually sending the e-mail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Externalize configuration**: What does configuration have to do with unit
    testing? Simple: if we need to enable or disable services, or replace services
    with fake services, then we need to have different configurations for the regular
    application and for when running unit tests. This requires us to design the application
    in a way that allows us to easily switch between multiple configurations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Run tests for the current module only**: Both the `unittest` test runner
    and third-party runners allow us to run a subset of tests—tests for a specific
    module, class, or even a single test. This is a great feature for large tests
    suites with thousands of tests, as it allows us to run just the tests for the
    module we are working on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pattern – running a subset of tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We already saw a simple way of running a subset of tests by simply specifying
    the module or test class on the command line, as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This works for the common case of when we want to run a subset based on the
    module. What if we want to run tests based on some other parameter? Maybe we want
    to run a set of basic smoke tests, or we want to run only integration tests, or
    we want to skip tests when running on a specific platform or Python version.
  prefs: []
  type: TYPE_NORMAL
- en: The `unittest` module allows us to create test suites. A **test suite** is a
    collection of test classes that are run. By default, `unittest` performs an autodiscovery
    for tests and internally creates a test suite with all the tests that match the
    discovery pattern. However, we can also manually create different test suites
    and run them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Test suites are created using the `unittest.TestSuite` class. The `TestSuite`
    class has the following two methods of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '`addTest`: This method takes a `TestCase` or another `TestSuite` and adds it
    to the suite'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`addTests`: Similar to `addTest`, this method takes a list of `TestCase` or
    `TestSuite` and adds it to the suite'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, how do we use this function?
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we write a function that makes the suite and returns it, as shown in
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We can choose the specific tests that we want in the suite. We've added a single
    test to the suite over here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to write a script to run this suite, as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, we create a `TextTestRunner` that will run the tests and pass it the suite
    or tests. `unittest.TextTestRunner` is a test runner that accepts a suite of tests
    and runs the suite, showing the results of the test, run on the console.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`unittest.TextTestRunner` is the default test runner that we have been using
    so far. It is possible to write our own test runners. For example, we might write
    a custom test runner to implement a GUI interface, or one that writes test output
    into an XML file.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run this script, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, we can create different suites for different subsets of tests—for
    example, a separate suite containing just integration tests—and run only specific
    suites as per our needs.
  prefs: []
  type: TYPE_NORMAL
- en: Test loaders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the problems with the suite function is that we have to add each test
    individually into the suite. This is a cumbersome process if we have a lot of
    tests. Fortunately, we can simplify the process by using a `unittest.TestLoader`
    object to load a bunch of tests for us, as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here, the loader extracts all the tests from the `StockCrossOverSignalTest`
    class and creates a suite out of it. We can return the suite directly if that
    is all we want, or we can create a new suite with additional tests. In the example
    above, we create a suite containing a single test from the `StockTest` class and
    all the tests from the `StockCrossOverSignalTest` class.
  prefs: []
  type: TYPE_NORMAL
- en: '`unittest.TestLoader` also contains some other methods for loading tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '`loadTestsFromModule`: This method takes a module and returns a suite of all
    the tests in that module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loadTestsFromName`: This method takes a string reference to a module, class,
    or function and extracts the tests from there. If it is a function, the function
    is called and the test suite returned by the function is returned. The string
    reference is in dotted format, meaning we can pass in something like `stock_alerter.tests.test_stock`
    or `stock_alerter.tests.test_stock.StockTest`, or even `stock_alerter.tests.test_stock.suite`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`discover`: This method executes the default autodiscovery process and returns
    the collected tests as a suite. The method takes three parameters: the start directory,
    the pattern to find `test` module (default `test*.py`), and the top-level directory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using these methods, we can create test suites with just the tests that we want.
    We can create different suites for different purposes and execute them from the
    test script.
  prefs: []
  type: TYPE_NORMAL
- en: Using the load_tests protocol
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A simpler way to create test suites is with the `load_tests` function. As we
    saw in [Chapter 7](ch07.html "Chapter 7. Executable Documentation with doctest"),
    *Executable Documentation with doctest*, the `unittest` framework calls the `load_tests`
    function if it is present in the test module. The function should return a `TestSuite`
    object containing the tests to be run. `load_tests` is a better solution when
    we want to just slightly modify the default autodiscovery process.
  prefs: []
  type: TYPE_NORMAL
- en: '`load_tests` passes three parameters: the loader being used to load the tests,
    a suite of tests that are going to be loaded by default, and the test pattern
    that has been specified for the search.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we do not want to run the `StockCrossOverSignalTest` tests if the current
    platform is Windows. We can write a `load_tests` function like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now the `StockCrossOverSignalTest` tests will be run only on non-windows platforms.
    When using the `load_tests` method, we don't need to write a separate script to
    run tests or create a test runner. It hooks into the autodiscovery process and
    is therefore much simpler to use.
  prefs: []
  type: TYPE_NORMAL
- en: Skipping tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous section, we used the `load_tests` mechanism to skip some tests
    if the platform was Windows. The `unittest` module gives a simpler way to do the
    same using the `skip` decorator. Simply decorate a class or method with the decorator
    and the test will be skipped, as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The decorator takes a parameter where we specify the reason that the test is
    being skipped. When we run all the tests, we get an output like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'And when the tests are run in verbose mode, we get an output like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `skip` decorator skips a test unconditionally, but `unittest` provides two
    more decorators, `skipIf` and `skipUnless`, which allow us to specify a condition
    to skip the tests. These decorators take a `Boolean` value as the first parameter
    and a message as the second parameter. `skipIf` will skip the test if the `Boolean`
    is `True`, while `skipUnless` will skip the test if the `Boolean` is `False`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following test will run on all platforms except windows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'While the following test will run only on windows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `skip`, `skipIf`, and `skipUnless` decorators can be used on test methods
    as well as test classes. When applied to classes, all the tests in the class are
    skipped.
  prefs: []
  type: TYPE_NORMAL
- en: Pattern – using attributes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `nose2` test runner has a useful `attrib` plugin that allows us to set attributes
    on test cases and select tests that match particular attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following test has three attributes set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'When nose2 is run via the following command, then the plugin is enabled, and
    only the tests that have the integration attribute set to `True` are executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The plugin can also run all tests that have a specific value in a list. Take
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will run all tests that have the `python` attribute set
    to `2.6` or containing the value `2.6` in a list. It will select and run the `test_stock_update`
    test, shown previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'The plugin can also run all tests that *do not* have an attribute set. Take
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will run all tests that are not marked as slow.
  prefs: []
  type: TYPE_NORMAL
- en: 'The plugin can also take complex conditions, so we can give the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This test runs all the tests that have the `integration` attribute, as well
    as `2.6` in the `python` attribute list. Note that we used the `-E` switch to
    specify that we are giving a `python` condition expression.
  prefs: []
  type: TYPE_NORMAL
- en: The attribute plugin is a great way to run specific subsets of tests without
    having to manually make test suites out of each and every combination that we
    might want to run.
  prefs: []
  type: TYPE_NORMAL
- en: Attributes with vanilla unittests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `attrib` plugin requires nose2 to work. What if we are using the regular
    `unittest` module? The design of the `unittest` module allows us to easily write
    a simplified version in just a few lines of code, as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This little piece of code will only run those tests that have the `integration`
    attribute set on the test function. Let us look a little deeper into the code.
  prefs: []
  type: TYPE_NORMAL
- en: First, we subclass the default `unittest.TestLoader` class and create our own
    loader called `AttribLoader`. Remember, the **loader** is the class responsible
    for loading the tests from a class or module.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we override the `getTestCaseNames` method. This method returns a list
    of test case names from a class. Here, we call the parent method to get the default
    list of tests, and then select those test function that have the required attribute.
    This filtered list is returned, and it is only these tests that will be executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'So why have we overridden the `loadTestsFromModule` method as well? Well, simple:
    the default behavior for loading tests is to match by the `test` prefix on the
    method, but if the `load_tests` function is present, then everything is delegated
    to the `load_tests` function. Therefore, all modules that have the `load_tests`
    function defined will take priority over our attribute filtering scheme.'
  prefs: []
  type: TYPE_NORMAL
- en: When using our loader, we call the default implementation, but set the `use_load_tests`
    parameter to `False`. This means that none of the `load_tests` functions will
    be executed, and the tests to be loaded will be determined only by the filtered
    list that we return. If we would like to give priority to `load_tests` (as is
    the default behavior), then we just need to remove this method from `AttribLoader`.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, now that the loader is ready, we then modify our test running script to
    use this loader, instead of the default loader. We get the loaded test suite by
    calling the `discover` method, which, in turn, calls our overridden `getTestCaseNames`.
    We pass this suite to the runner and run the tests.
  prefs: []
  type: TYPE_NORMAL
- en: The loader can be easily modified to support selecting tests that *don't* have
    a given attribute or to support more complex conditionals. We can then add support
    to the script to accept the attribute on the command line and pass it on to the
    loader.
  prefs: []
  type: TYPE_NORMAL
- en: Pattern – expected failures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, we have tests that are failing, but, for whatever reason, we don't
    want to fix it yet. It could be that we found a bug and wrote a failing test that
    demonstrates the bug (a very good practice), but we have decided to fix the bug
    later. Now, the whole test suite is failing.
  prefs: []
  type: TYPE_NORMAL
- en: On one hand, we don't want the suite to fail because we know this bug and want
    to fix it later. On the other hand, we don't want to remove the test from the
    suite because it reminds us that we need to fix the bug. What do we do?
  prefs: []
  type: TYPE_NORMAL
- en: 'Python''s `unittest` module provides a solution: marking tests as expected
    failures. We can do this by applying the `unittest.expectedFailure` decorator
    to the test. The following is an example of it in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output when the tests are executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'And the following is the verbose output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Pattern – data-driven tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We briefly explored data-driven tests earlier. Data-driven tests reduce the
    amount of boilerplate test code by allowing us to write a single test execution
    flow and run it with different combinations of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example using the nose2 parameterization plugin that we
    looked at earlier in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Running tests like this requires the use of nose2\. Is there a way to do something
    similar using the regular `unittest` module? For a long time there was no way
    to do this without resorting to metaclasses, but a new feature added with Python
    3.4 has made this possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'This new feature is the `unittest.subTest` context manager. All the code within
    the context manager block will be treated as a separate test, and any failures
    are reported independently. The following is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the test loops through the different scenarios and asserts
    each one. The whole Arrange-Act-Assert pattern occurs inside the `subTest` context
    manager. The context manager takes any keyword arguments as parameters and these
    are used in displaying error messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run the test, we get an output like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the whole test is considered a single test and it shows that
    the test passed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we change the test to make it fail in two of the three cases, as shown
    in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the output becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As we can see in the preceding output, it shows a single test was run, but each
    failure is reported individually. In addition, the values that were used when
    the test failed are appended to the end of the test name, making it easy to see
    exactly which condition failed. The values displayed here are the parameters that
    were passed into the `subTest` context manager.
  prefs: []
  type: TYPE_NORMAL
- en: Pattern – integration and system tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Throughout this book, we''ve stressed the fact that unit tests are not integration
    tests. They have a different purpose to validating that the system works when
    integrated. Having said that, integration tests are also important and shouldn''t
    be ignored. Integration tests can be written using the same `unittest` framework
    that we use for writing unit tests. The key points to keep in mind when writing
    integration tests are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Still disable non-core services**: Keep non-core services such as analytics
    or logging disabled. These do not affect the functionality of the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enable all core services**: Every other service should be live. We don''t
    want to mock or fake these because this defeats the whole purpose of an integration
    test.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use attributes to tag integration tests**: By doing this, we can easily select
    only the unit tests to run during development, while enabling integration tests
    to be run during continuous integration or before deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Try to minimize setup and teardown time**: For example, don''t start and
    stop a server for each and every test. Instead, use module or package level fixtures
    to start and stop a service once for the entire set of tests. When doing this,
    we have to be careful that our tests do not mess up the state of the service in-between
    tests. In particular, a failing test or a test error should not leave the service
    in an inconsistent state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pattern – spies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mocks allow us to replace an object or class with a dummy mock object. We've
    seen how we can then make the mock return predefined values, so that the class
    under test doesn't even know that it has made a call to a mock object. However,
    sometimes we might want to just record that the call was made to an object, but
    allow the execution flow to continue to the real object and return. Such an object
    is known as a **spy**. A spy retains the functionality of recording calls and
    being able to assert on the calls afterwards, but it does not replace a real object
    like a regular mock does.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `wraps` parameter when creating a `mock.Mock` object allows us to create
    spy behavior in our code. It takes an object as a value, and all calls to the
    mock are forwarded to the object we pass, and the return value is sent back to
    the caller. The following is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In the above example, we are creating a spy for the `rule` object. The spy is
    nothing but a regular mock object that wraps the real object, as specified in
    the `wraps` parameter. We then pass the spy to the alert. When `alert.check_rule`
    is executed, the method called the `matches` method on the spy. The spy records
    the call details, and then forwards the call to the real rule object and returns
    the value from the real object. We can then assert on the spy to validate the
    call.
  prefs: []
  type: TYPE_NORMAL
- en: Spies are typically used when we would like to avoid over-mocking and use a
    real object, but we also would like to assert on specific calls. They are also
    used when it is difficult to calculate mock return values by hand, and it is better
    to just do the real calculation and return the value.
  prefs: []
  type: TYPE_NORMAL
- en: Pattern – asserting a sequence of calls
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sometimes, we want to assert that a particular sequence of calls occurred across
    multiple objects. Consider the following test case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In this test, we are asserting that a call was made to the `rule.matches` method
    as well as a call being made to the `action.execute` method. The way we have written
    the assertions does not check the order of these two calls. This test will still
    pass even if the `matches` method is called after the `execute` method. What if
    we want to specifically check that the call to the `matches` method happened before
    the call to the `execute` method?
  prefs: []
  type: TYPE_NORMAL
- en: 'Before answering this question, let us take a look at this interactive Python
    session. First, we create a mock object, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we get two child objects that are attributes of the mock, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Mock objects by default return new mocks whenever an attribute is accessed that
    doesn't have a `return_value` configured. So `child_obj1` and `child_obj2` will
    also be mock objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we call some methods on our mock objects, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Again, no `return_value` is configured, so the default behavior for the method
    call is to return new mock objects. We can ignore those for this example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let us look at the `mock_calls` attribute for the child objects. This
    attribute contains a list of all the recorded calls on the mock object, as shown
    in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The mock objects have the appropriate method calls recorded, as expected. Let
    us now take a look at the attribute on the main `obj` mock object, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Now, this is surprising! The main mock object seems to not only have details
    of its own calls, but also all the calls made by the child mocks!
  prefs: []
  type: TYPE_NORMAL
- en: So, how can we use this feature in our test to assert the order of the calls
    made across different mocks?
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, what if we wrote the above test like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Here, we create a main mock object called `main_mock`, and the `rule` and `action`
    mocks are child mocks of this. We then use the mocks as usual. The difference
    is that we use `main_mock` in the assert section. Since `main_mock` has a record
    of the order in which calls are made to the child mocks, this assertion can check
    the order of calls to the `rule` and `action` mocks.
  prefs: []
  type: TYPE_NORMAL
- en: Let us go a step further. The `assert_has_calls` method only asserts that the
    calls were made and that they were in that particular order. The method *does
    not* guarantee that these were the *only* calls made. There could have been other
    calls before the first call or after the last call, or even in-between the two
    calls. The assertion will pass as long as the calls we are asserting were made,
    and that between them the order was maintained.
  prefs: []
  type: TYPE_NORMAL
- en: 'To strictly match the calls, we can simply do an `assertEqual` on the `mock_calls`
    attribute like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In the above, we assert the `mock_calls` with a list of expected calls. The
    list must match exactly—no missing calls, no extra calls, nothing different. The
    thing to be careful about is that we must list out *every* call. There is a call
    to `rule.depends_on`, which is done in the `alert.connect` method. We have to
    specify that call, even though it is not related to the functionality we are trying
    to test.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, matching every call will lead to verbose tests as all calls that are
    tangential to the functionality being tested also need to be put in the expected
    output. It also leads to brittle tests as even slight change in calls elsewhere,
    which might not lead to change in behavior in this particular test, will still
    cause the test to fail. This is why the default behavior for `assert_has_calls`
    is to only determine whether the expected calls are present, instead of checking
    for an exact match of calls. In the rare cases where an exact match is required,
    we can always assert on the `mock_calls` attribute directly.
  prefs: []
  type: TYPE_NORMAL
- en: Pattern – patching the open function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most common cases of mocking is to mock out file access. This is
    actually a little cumbersome because the `open` function can be used in a number
    of different ways. It can be used as a regular function or as a context manager.
    The data can be read using many methods such as `read`, `readlines`, and so on.
    In turn, some of these functions, return iterators that can be iterated upon.
    It is a pain to sit and mock all these out in order to be able to use them in
    tests.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, the mocking library provides an extremely helpful `mock_open` function,
    which returns a mock that handles all these situations. Let us see how we can
    use this function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code for a `FileReader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This class reads stock updates from a file and returns each update, one by one.
    The method is a generator, and uses the `yield` keyword to return updates, one
    at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**A quick primer on generators**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generators** are functions that use a `yield` statement instead of a `return`
    statement to return values. Each time the generator is executed, the execution
    does not start at the beginning of the function, but instead continues running
    from the previous `yield` statement. In the example above, when the generator
    is executed, it parses the first line of the file, then yields the value. The
    next time it is executed, it continues once more through the loop and returns
    the second value, then the third value, and so on until the loop is over. Each
    execution of the generator returns one stock update. For more on generators, check
    out the Python documentation or online articles. One such article can be found
    at [http://www.jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/](http://www.jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to test the `get_update` method, we will need to create different
    kinds of file data and verify that the method reads them properly and returns
    values as expected. In order to do this, we will mock out the open function. The
    following is one such test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: In the above test, we are starting with patching the `builtins.open` function.
    The `patch` decorator can take a second parameter, in which we can specify the
    mock object to be used after patching. We call the `mock.mock_open` function to
    create an appropriate mock object, which we pass to the `patch` decorator.
  prefs: []
  type: TYPE_NORMAL
- en: The `mock_open` function takes a `read_data` parameter, in which we can specify
    what data should be returned when the mocked file is read. We use this parameter
    to specify the file data we want to test against.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the test is fairly simple. The only thing to note is in the following
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Since `get_updates` is a generator function, a call to the `get_updates` method
    does not actually return a stock update, but instead returns the generator object.
    This generator object is stored in the `updater` variable. We use the built-in
    `next` function to get the stock update from the generator and assert that it
    is as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Pattern – mocking with mutable args
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One gotcha that can bite us is when arguments to mocked out objects are mutable.
    Take a look at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Whoa! What happened there? The error says the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Actual call was `mock([''123''])`? But we called the mock as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: It's pretty clear that we called it with `["abc"]`. So why is this failing?
  prefs: []
  type: TYPE_NORMAL
- en: The answer is that the mock object only stores a reference to the call arguments.
    So, when the line `param[0] = "123"` was executed, it affected the value that
    was saved as the call argument in the mock. In the assertion, it looks at the
    saved call argument, and sees that the call was made with the data `["123"]`,
    so the assertion fails.
  prefs: []
  type: TYPE_NORMAL
- en: 'The obvious question is: why is it that the mock stores a reference to the
    parameters? Why doesn''t it make a copy of the arguments so that the stored copy
    doesn''t get changed if the object passed as a parameter is changed later on?
    The answer is that making a copy creates a new object, so all assertions where
    object identity is compared in the argument list will fail.'
  prefs: []
  type: TYPE_NORMAL
- en: So what do we do now? How do we make this test work?
  prefs: []
  type: TYPE_NORMAL
- en: 'Simple: we just inherit from `Mock` or `MagicMock` and change the behavior
    to make a copy of the arguments, as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: This mock just makes a copy of the arguments and then invokes the default behavior
    passing in the copy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The assertion now passes, as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Keep in mind that when we use `CopyingMock`, we cannot use any object identity
    comparisons with the arguments as they will now fail, as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you looked at some other patterns for unit testing. You looked
    at how to speed up tests and how you can run specific subsets of tests. You looked
    at various patterns for running subset of tests, including creating your own test
    suites and using the `load_tests` protocol. You saw how to use the nose2 attrib
    plugin to run a subset of tests based on test attributes and how to implement
    that functionality with the default unit test runner. We then examined features
    for skipping tests and marking tests as expected failures. You finally looked
    at how we could write data-driven tests.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we moved on to some mocking patterns, starting with how to implement spy
    functionality. You also looked at the problem of validating a sequence of mock
    calls across multiple mocks. You then looked at the `mock_open` function to help
    us easily mock filesystem access, and in the process you took a peek at how to
    work with generator functions. Finally, you looked at the problem of using mocks
    when the arguments are mutable.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is the final chapter in this book, where you will look at other
    tools that we can use in our TDD practice.
  prefs: []
  type: TYPE_NORMAL
