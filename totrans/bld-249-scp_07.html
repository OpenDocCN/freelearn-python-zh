<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;Creating Custom Shaders and Textures with Pynodes"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Creating Custom Shaders and Textures with Pynodes</h1></div></div></div><p>It is sometimes said that although Blender has a powerful and versatile system to define materials, it lacks a proper shader language to define completely new <span class="strong"><strong>shaders</strong></span>, for example, to create materials that react to light in novel ways. This is, however, not entirely true.<a class="indexterm" id="id428"/>
</p><p>Blender does not have a compiled shader language but it does a have a powerful <span class="strong"><strong>node</strong></span> system to combine textures and materials and these nodes can be Python scripts. This enables users to define completely new textures and materials.</p><p>In this chapter, we will learn:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How to write Pynodes that create simple color patterns</li><li class="listitem" style="list-style-type: disc">How to write Pynodes that produce patterns with normals</li><li class="listitem" style="list-style-type: disc">How to write animated Pynodes</li><li class="listitem" style="list-style-type: disc">How to write height-and slope-dependent materials</li><li class="listitem" style="list-style-type: disc">How to create shaders that react to the angle of incident light</li></ul></div><p>To illustrate some of its power, we start by looking at a script that creates regular color patterns made of triangles, rectangles, or hexagons.</p><div class="mediaobject"><img alt="Creating Custom Shaders and Textures with Pynodes" src="graphics/0400-07-01.jpg"/></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note20"/>Note</h3><p>
<span class="strong"><strong>Materials</strong></span>, <span class="strong"><strong>shaders</strong></span>, <span class="strong"><strong>and</strong></span> <span class="strong"><strong>textures</strong></span> are terms that are often used as synonyms although there are differences in meaning. For our purposes we try to adhere to the following definitions: A<a class="indexterm" id="id429"/> <span class="strong"><strong>texture</strong></span> is a basic building block, for example, a color or normal pattern or simply some function that returns a value depending on the position on a surface. A <a class="indexterm" id="id430"/>
<a class="indexterm" id="id431"/>
<span class="strong"><strong>shader</strong></span> will take any number of textures or just a basic color and will return a color based on the influence of incident light and possibly the view direction. A <span class="strong"><strong>material</strong></span> is a collection of textures, shaders, and all sorts of properties that can be applied to an object. Pynodes can be textures as well as shaders.</p></div></div><div class="section" title="The basics"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec53"/>The basics</h1></div></div></div><a class="indexterm" id="id432"/><p>When we design a Pynode we basically design something that provides a function that is called for every pixel on the screen that needs to be shaded by that node (or even more than once, if <span class="strong"><strong>oversampling</strong></span> is in effect). This function gets among other things the x, y, and z coordinates of the point on the object being shaded that corresponds to the pixel on the screen we are currently calculating. The function is then expected to return something useful such as a color, an intensity value, or something a little less intuitive such as a normal.</p><a class="indexterm" id="id433"/><p>In Blender's Node editor window every material node, including a Pynode, is represented by a box which has its inputs on the left and its outputs on the right. These inputs and outputs are often called <span class="strong"><strong>sockets</strong></span> and are represented by little colored circles (see the next screenshot). These sockets can be used to string nodes together; by clicking on an output socket of one node and dragging the mouse to the input socket of another node, these nodes will be connected. By combining as many different nodes as needed, very complex and powerful shaders can be created.</p><div class="section" title="From nodes to Pynodes"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec67"/>From nodes to Pynodes</h2></div></div></div><a class="indexterm" id="id434"/><p>The power of Blender's Node system not only stems from its many predefined node types and the many ways these nodes may be connected, but also from the fact that we can write new nodes in Python that may be connected in the same way as ordinary nodes.</p><p>Pynodes need a way to access the information provided by the input sockets and a way to send their calculated results to the output sockets. The concept of a node and its sockets is structured along an object-oriented model. Let's first look at some example code to prove that this doesn't need to be scary (object-oriented veterans: look the other way or peek through your fingers to just pick up the class definition from the following example):</p><div class="informalexample"><pre class="programlisting">from Blender import Node

class MyNode(Node.Scripted):

   def __init__(self, sockets):
      <span class="strong"><strong>sockets.input = [Node.Socket('Coords', val= 3*[1.0])]</strong></span>
	  sockets.output = [Node.Socket('Color', val = 4*[1.0])]

   def __call__(self):
      x,y,z = self.input.Coords
      self.output.Color = [abs(x),abs(y),abs(z),1.0]</pre></div><p>Before we look at this code in detail try it in Blender to see how it actually works:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Open a new file in the text editor and give it a distinguishable name.</li><li class="listitem">Copy the example code.</li><li class="listitem">Create a simple scene, for example, a simple UV sphere at the origin with a couple of lamps and a camera.</li><li class="listitem">Assign a <code class="literal">Node</code> material to the sphere like you normally would.</li><li class="listitem">Finally, add in a <span class="emphasis"><em>dynamic</em></span> node in the Node editor (<span class="strong"><strong>Add | Dynamic</strong></span>) and select the name of the file that you edited by clicking on the selection button of the <span class="emphasis"><em>dynamic</em></span> node and picking the file.</li></ol></div><p>The resulting network of nodes (often called a <span class="strong"><strong>noodle</strong></span>) may look like this:</p><div class="mediaobject"><img alt="From nodes to Pynodes" src="graphics/0400-07-02.jpg"/></div><p>If you render the sphere the result is a colorful ball not unlike a color selection widget.</p><p>Now back to the code.</p><a class="indexterm" id="id435"/><p>In the first line we import the <code class="literal">Node</code> module from Blender because we will be implementing a new type of node, but most of its behavior is already defined in the <code class="literal">Node</code> module.</p><p>Then we define a class <code class="literal">MyNode</code>, a subclass of <code class="literal">Node.Scripted</code>, which will behave just like a <code class="literal">Scripted</code> node except for the parts that we will redefine.</p><p>Next, we define the <code class="literal">__init__()</code> function that will be called the first time we create this type of Pynode in the node editor or any time we click on the <span class="strong"><strong>Update</strong></span> button. When this happens Blender will pass two arguments to this function: <code class="literal">self</code>, a pointer to the node we are using, and <code class="literal">sockets</code>, a reference to an object that will point to our lists of input and output sockets. These are the nodes in the node editor we will receive input from or send data to.</p><p>In the highlighted line we define a list of input socket definitions; only one in this case and it is called <code class="literal">Coords</code>. It is a vector input because it is initialized with a list of three floats that define the default values, if this input socket is not connected to another node. Vector nodes are represented as blue circles in the node editor.</p><p>Other types of input socket are possible as well and the type is determined by the value of the <code class="literal">val</code> argument. Output sockets are defined in the same way. A list of three floats will define a vector socket, a list of four floats a color socket (with a red, green, blue, and alpha component), and a socket representing a simple value such as intensity is initialized by a single float. Note that we cannot distinguish between inputs that need to be filled in by the user or ones that should be connected to another node. We use input sockets for both and will have to document their intended use. Currently, there is no facility to add buttons or other widgets to a Pynode.</p><p>Our sample Pynode needs output as well so we define a list consisting of a single output <a class="indexterm" id="id436"/>socket called <code class="literal">Color</code>. It has four float values as a default specifying the red, green, blue, and alpha values respectively.</p><p>Next we define a function <code class="literal">__call__()</code> that is called each time a pixel is shaded. It takes no arguments but <code class="literal">self</code>—a reference to the current node that is used in the following lines to access the input and output sockets.</p><p>In the body of <code class="literal">__call__()</code> we retrieve the three components from the input socket called <code class="literal">Coords</code> and assign them to easy-to-remember variables. Finally, we create a new four-component list that represents our calculated color and assign it to the output socket called <code class="literal">Color</code>.</p><p>This is the basis to define simple textures but there is more information available to the node (as we will see in the following sections) so some pretty sophisticated effects can be designed. We end this section with a slightly more elaborate node that builds on the same principles we saw earlier but creates more useful patterns.</p></div></div></div>
<div class="section" title="Regular tilings"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec54"/>Regular tilings</h1></div></div></div><a class="indexterm" id="id437"/><a class="indexterm" id="id438"/><p>The checkerboard texture is perhaps the simplest texture that you can imagine and is therefore often used as an example when programming textures. Because Blender already has a built-in checker texture (since version 2.49, in the texture context of the nodes window) we go one step further and create a texture node that displays not only a checkerboard texture but <span class="strong"><strong>tilings</strong></span> of triangles and hexagons as well.</p><div class="informalexample"><pre class="programlisting">from Blender import Node,Noise,Scene
from math import sqrt,sin,cos,pi,exp,floor
from Blender.Mathutils import Vector as vec

# create regular tilings to be used as a color map

class Tilings(Node.Scripted):
   def __init__(self, sockets):
      sockets.input = [Node.Socket('type' , val= 2.0, min = 1.0, max = 3.0),
                       Node.Socket('scale' , val= 2.0, min = 0.1, max = 10.0),
                       Node.Socket('color1', val= [1.0,0.0,0.0,1.0]),
                       Node.Socket('color2', val= [0.0,1.0,0.0,1.0]),
                       Node.Socket('color3', val= [0.0,0.0,1.0,1.0]),
                       Node.Socket('Coords', val= 3*[1.0])]

      sockets.output = [Node.Socket('Color', val = 4*[1.0])]</pre></div><p>The first few lines start off by defining our input and output sockets. The output will simply be a color in all cases but we have a more varied set of input sockets. We define three different input colors because the hexagon pattern needs three colors to give each hexagon a color that is distinguishable from its neighbor.</p><p>We also define a <code class="literal">Coords</code> input. This input socket may hook up to any output of a <a class="indexterm" id="id439"/>geometry socket. In this way we have many possibilities to map our color texture to the object that we are texturing. A <code class="literal">Scale</code> socket is defined as well to control the size of our texture.</p><p>Finally, we define a <code class="literal">Type</code> socket to select the pattern that we wish to generate. As the Pynode API does not provide a drop-down box or any other simple selection widget we make do with a value socket and arbitrarily pick values to represent our choice: <code class="literal">1.0</code> for triangles, <code class="literal">2.0</code> for checkers, and <code class="literal">3.0</code> for hexagons.</p><p>We end our <code class="literal">__init__()</code> function with the definition of a number of constants and a dictionary of color mappings that we will use when generating a hexagonal texture.</p><div class="informalexample"><pre class="programlisting">      self.cos45 = cos(pi/4)
      self.sin45 = sin(pi/4)
      self.stretch = 1/sqrt(3.0)
      self.cmap = { (0,0):None,(0,1):2,   (0,2):0,(1,0):0,   (1,1):1,   (1,2):None,(2,0):2,   (2,1):None,(2,2):1 }</pre></div><p>The next step is to define the <code class="literal">__call__()</code> function:</p><div class="informalexample"><pre class="programlisting">   def __call__(self):
      
      tex_coord = self.input.Coords
      # we disregard any z coordinate
      x = tex_coord[0]*self.input.scale 
      y = tex_coord[1]*self.input.scale
      
      c1 = self.input.color1
      c2 = self.input.color2
      c3 = self.input.color3
      
      col= c1</pre></div><p>The <code class="literal">__call__()</code> function starts off by defining some shorthands for input values and multiplying the input coordinates by the chosen scale to stretch or shrink the generated pattern. The next step is to establish the kind of pattern that is desired and call the appropriate function to calculate the output color for the given coordinates. The resulting color is assigned to our only output socket:</p><div class="informalexample"><pre class="programlisting">      if self.input.type&lt;= 1.0:
         col = self.triangle(x,y,c1,c2)
      elif self.input.type &lt;= 2.0:
         col = self.checker(x,y,c1,c2)
      else:
         col = self.hexagon(x,y,c1,c2,c3)
         
      self.output.Color = col</pre></div><a class="indexterm" id="id440"/><p>The various pattern-generating functions are all very similar; they take x and y coordinates and two or three colors as arguments and return a single color. As these are member functions of a class, they take an additional first argument of <code class="literal">self</code> as well.</p><div class="informalexample"><pre class="programlisting">   def checker(self,x,y,c1,c2):
      if int(floor(x%2)) ^ int(floor(y%2)):
         return c1
      return c2</pre></div><p>The <code class="literal">checker</code> function checks in which row and column we are and if the row number and the column number are both odd or even (that is what the exclusive <code class="literal">or</code> operator establishes) it returns one color, if not it returns the other color.</p><div class="informalexample"><pre class="programlisting">   def triangle(self,x,y,c1,c2):
      y *= self.stretch
      x,y = self.cos45*x - self.sin45*y, self.sin45*x + self.cos45*y
      if int(floor(x%2)) ^ int(floor(y%2)) ^ <span class="strong"><strong>int(y%2&gt;x%2)</strong></span> : return c1
      return c2</pre></div><p>The <code class="literal">triangle</code> function first rotates both x and y coordinates together by a 45 degree angle (changing squares into upright lozenges). It then determines the color based on row and column numbers just like in the <code class="literal">checker</code> function but with a twist: the third term (highlighted) checks whether we are on the left of the diagonal crossing a square and because we have rotated our grid, we really check whether or not the coordinates are above the horizontal line dividing our lozenge. This may sound a bit complicated but you can check the following screenshot to get the idea:</p><div class="mediaobject"><img alt="Regular tilings" src="graphics/0400-07-03.jpg"/></div><div class="informalexample"><pre class="programlisting">   def hexagon(self,x,y,c1,c2,c3):
      y *= self.stretch
      x,y = self.cos45*x - self.sin45*y, self.sin45*x + self.cos45*y
      xf = int(floor(x%3))
      yf = int(floor(y%3))
      top = int((y%1)&gt;(x%1))
	  <span class="strong"><strong>c = self.cmap[(xf,yf)]</strong></span>
      if c == None:
         if top :
            c = self.cmap[(xf,(yf+1)%3)]
         else :
            c = self.cmap[(xf,(yf+2)%3)]
      return (c1,c2,c3)[c]</pre></div><a class="indexterm" id="id441"/><p>The <code class="literal">hexagon</code> function is like the <code class="literal">triangle</code> function in many respects (after all a hexagon is six triangles glued together). Therefore, it performs the same rotation trick but instead of picking the color by using a straightforward formula, things are a bit more involved and hence we use a color map here (highlighted in the previous code snippet). Basically, we divide the screen into horizontal and vertical strips and pick the color based on the strips we are in.</p><p>The final piece of magic is in the last line of our script:</p><div class="informalexample"><pre class="programlisting">__node__ = Tilings</pre></div><p>The way Pynodes are currently implemented, Blender needs this assignment to identify a class as a node. Our node will show up in the pop-up menu of a script node as <span class="strong"><strong>Tilings</strong></span>. The full code is available as <code class="literal">tilings.py</code> in <code class="literal">tilings.blend</code> together with a sample node setup. Some of the possible patterns are shown in the next screenshot:</p><div class="mediaobject"><img alt="Regular tilings" src="graphics/0400-07-04.jpg"/></div><p>The corresponding node setup is shown in the next screenshot. Note that we have not connected any node to the color inputs but even more elaborate patterns can be created if we do.</p><div class="mediaobject"><img alt="Regular tilings" src="graphics/0400-07-05.jpg"/></div><div class="section" title="Anti-aliasing"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec68"/>Anti-aliasing</h2></div></div></div><a class="indexterm" id="id442"/><p>If you would look closely at the diagonal boundaries of the hexagonal or triangular tilings you would notice some staircase-like artifacts even if oversampling was set to a high value.</p><p>Blender itself is smart enough to apply the chosen <span class="strong"><strong>anti-aliasing</strong></span> level to things such as object boundaries, but in most cases textures on a surface will have to take care of anti-aliasing themselves. Blender's built-in textures are designed that way of course, but our own textures produced with Pynodes should address this explicitly.</p><p>There are numerous mathematical techniques available to reduce aliasing in generated textures but most are not easy to implement or require specific knowledge about the way a pattern is generated. Fortunately, Blender provides us with the <span class="strong"><strong>Full OSA</strong></span> option (<span class="strong"><strong>Buttons windows | Shading context | Material buttons | Links and pipeline tab</strong></span>). If we enable this option, Blender is forced to oversample each pixel in our texture by the amount selected in the render buttons. This is an expensive option but will get rid of aliasing effects without the need to implement specific filtering options in our Pynode texture.</p></div><div class="section" title="Indexing a texture by vector"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec69"/>Indexing a texture by vector</h2></div></div></div><a class="indexterm" id="id443"/><p>In our tiling patterns we have limited the colors to the minimum number needed to distinguish each neighboring tile. But would it be possible to assign random colors based on some noise texture? This way we might color fish scales in a way that follows an overall random pattern yet colors each individual scale uniformly.</p><p>We cannot simply connect a colored texture to the color inputs as this leads to interesting patterns, perhaps, but each tile would not have a uniform color. The solution is to modify our Pynode to produce a unique vector that is uniform within any given tile. This vector may then be connected to any noise texture that takes a vector as input as all Blender textures do. This vector is used by the noise texture node to point to a single point in the random texture and this way we can produce randomly colored but uniform tiles.</p><a class="indexterm" id="id444"/><p>To provide this functionality we modify our code by removing the color inputs and replacing the color output by a vector output (not shown). The code inside the <code class="literal">__call__()</code> function will now have to produce a vector instead of a color. Here we show the modified <code class="literal">triangle</code> function (full code available as <code class="literal">tilingsv.py</code> in <code class="literal">tilingsv.blend</code>):</p><div class="informalexample"><pre class="programlisting">def triangle(self,x,y):
      y *= self.stretch
      x,y = self.cos45*x - self.sin45*y, self.sin45*x + self.cos45*y

      if int(floor(x%2)) ^ int(floor(y%2)) ^ int(y%2&gt;x%2) :
	     <span class="strong"><strong>return [floor(x),floor(y),0.0]</strong></span>
      return [floor(x)+0.5,floor(y),0.0]</pre></div><p>The logic is largely the same but, as shown in the highlighted line, we return a vector that is dependent on the position. However, due to the <code class="literal">floor()</code> operation it is constant within a triangle. Note that for the alternate triangle we add a slight offset; it doesn't matter which offset we choose as long as it is constant and produces a vector distinct from the other triangle.</p><p>The results show a random pattern of triangles that follows the large correlations in the noise yet leaves each individual triangle with a uniform color. The sample on the right has a larger noise size for the cloud texture used:</p><div class="mediaobject"><img alt="Indexing a texture by vector" src="graphics/0400-07-06(2).jpg"/></div><p>A possible node setup is shown in the following screenshot:</p><div class="mediaobject"><img alt="Indexing a texture by vector" src="graphics/0400-07-07(2).jpg"/></div></div><div class="section" title="A fresh breeze—textures with normals"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec70"/>A fresh breeze—textures with normals</h2></div></div></div><a class="indexterm" id="id445"/><p>A texture can have more than just a geometric input. If you need a texture to change its behavior based on another texture in a way that cannot be achieved by a simple node setup you may provide it with extra input sockets. We will develop a Pynode that generates a normal map that simulates the little patches of <span class="strong"><strong>wavelets</strong></span> on a pond on an almost windless day.</p><p>Where those patches appear is controlled by an extra input socket that may be linked to almost any noise texture. We will give this input socket the name <code class="literal">amplitude</code> because we use it to multiply it with our calculated <span class="strong"><strong>normal</strong></span>. This way our wavelets will disappear wherever our noisy texture is zero.</p><p>The wavelength of the ripples is controlled by yet another input called <code class="literal">wavelength</code> and our <code class="literal">Ripples</code> node will have an input socket for the coordinates as well.</p><a class="indexterm" id="id446"/><p>The fourth and final input is called <code class="literal">direction</code>—a vector that controls the orientation of our wavelets. It may be set by hand by the user but if desired, may be linked up to a normal node that provides an easy way to manipulate the direction with the mouse.</p><p>The resulting node setup that combines all of this is shown in the screenshot of the node editor:</p><div class="mediaobject"><img alt="A fresh breeze—textures with normals" src="graphics/0400-07-08.jpg"/></div><p>The script for the node is straightforward; after importing some necessary definitions we then define the numerous input sockets and our single output socket:</p><div class="informalexample"><pre class="programlisting">from Blender import Node
from math import cos
from Blender.Mathutils import Vector as vec

class Ripples(Node.Scripted):
   def __init__(self, sockets):
      sockets.input = [Node.Socket('amplitude' , val= 1.0, min = 0.001, max = 1.0),
                       Node.Socket('wavelength', val= 1.0, min = 0.01, max = 1000.0),
                       Node.Socket('direction' , val= [1.0,0.0,0.0]),
                       Node.Socket('Coords'    , val= 3*[1.0])]

      sockets.output = [Node.Socket('Normal', val = [0.0,0.0,1.0])]
	  
   <span class="strong"><strong>def __call__(self):</strong></span>
   
      norm = vec(0.0,0.0,1.0)
      
      p = vec(self.input.Coords)
      d = vec(self.input.direction)
      x = p.dot(d)*self.input.wavelength
      norm.x=-self.input.amplitude*cos(x)
      
      n = norm.normalize()
      
      self.output.Normal = n*.01
      
__node__ = Ripples</pre></div><a class="indexterm" id="id447"/><p>Again, all real work is done in the <code class="literal">__call__()</code> function (highlighted in the preceding code snippet). We first define the shorthands <code class="literal">p</code> and <code class="literal">d</code> for the coordinates and the direction vectors respectively. Our wavelets are sinus functions and the location on this sinus curve is determined by the projection of the position on the direction vector. This projection is calculated by taking the "in product" or "dot product"—an operation provided by the <code class="literal">dot()</code> method of a <code class="literal">Vector</code> object.</p><div class="mediaobject"><img alt="A fresh breeze—textures with normals" src="graphics/0400-07-09.jpg"/></div><p>The projection is then multiplied by the wavelength. If we would calculate the sinus we would have the height of our wave. We are, however, not interested in the height but in the normal. The normal always points upward and moves along with our sine wave (see the next diagram). It can be shown that this normal is a vector with a z-component of 1.0 and an x-component equal to the negative derivative of the sine function, that is, minus cosine. The script (<code class="literal">ripples.py</code>) and an example node setup are available as <code class="literal">ripples.blend</code>.</p><div class="mediaobject"><img alt="A fresh breeze—textures with normals" src="graphics/0400-07-10.jpg"/></div><a class="indexterm" id="id448"/><p>In the node setup that we showed earlier you might have noticed that instead of linking up the geometry node directly to our ripples node, we added a second texture node and combined this node with the geometry input by adding and scaling the normal output of the texture node. We could have mixed in some noise in the ripples node itself but this way we give the user far more control over the type and amount of noise he wants to add (if any). This is a general pattern: nodes should be designed as simple as possible to facilitate reuse in different settings.</p><p>These ripples were not designed to be animated but in the following section we will design a node that can.</p><div class="mediaobject"><img alt="A fresh breeze—textures with normals" src="graphics/0400-07-11.jpg"/></div></div></div>
<div class="section" title="Raindrops&#x2014;animated Pynodes"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec55"/>Raindrops—animated Pynodes</h1></div></div></div><a class="indexterm" id="id449"/><a class="indexterm" id="id450"/><p>Many patterns are not static but change in time. One example is the ripples formed by <span class="strong"><strong>raindrops</strong></span> falling in a pond. Blender exposes render-time parameters such as start frame, frame rate, and current frame so we have plenty of hooks to make our Pynodes time dependent. We will see how to use those hooks in a script that generates a raindrop pattern. A pattern that changes realistically resembling the outward expanding ripples caused by drops falling in a pond. On the way we also pick up some useful tricks to speed up calculations by storing the results of expensive calculations in the Pynode itself for later reuse.</p><div class="section" title="Render-time parameters"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec71"/>Render-time parameters</h2></div></div></div><a class="indexterm" id="id451"/><p>The most relevant render parameters when dealing with time-dependent things are the current frame number and the frame rate (the number of frames per second). These parameters are provided grouped together as a rendering context by the <code class="literal">Scene</code> module, most via function calls, some as variables:</p><div class="informalexample"><pre class="programlisting">scn               = Scene.GetCurrent()
context           = scn.getRenderingContext()
current_frame     = context.currentFrame()
start_frame       = context.startFrame()
end_frame         = context.endFrame()
frames_per_second = context.fps</pre></div><p>With this information we can now calculate the time, either absolute or relative to the start frame:</p><div class="informalexample"><pre class="programlisting">absolute_time = current_frame/<span class="strong"><strong>float</strong></span>(frames_per_second)
relative_time = (current_frame-start_frame)/float(frames_per_second)</pre></div><a class="indexterm" id="id452"/><p>Note the conversion to float in the denominator (highlighted). That way we ensure that the division is treated as a floating point operation. This is not strictly necessary since <code class="literal">fps</code> is returned as a float but many people assume the frame rate to be some integer value such as 25 or 30. This is, however, not always the case (for example, NTSC encoding uses a fractional frame rate) so we better make this explicit. Also note that we cannot do away with this division, otherwise when people would change their mind about their chosen frame rate the speed of the animation would change.</p></div><div class="section" title="What looks good, is good"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec72"/>What looks good, is good</h2></div></div></div><a class="indexterm" id="id453"/><p>Accurately simulating the look of ripples caused by falling droplets may seem difficult but is straightforward, albeit a bit involved. Readers interested in the underlying mathematics might want to check some reference (for example <a class="ulink" href="http://en.wikipedia.org/wiki/Wave">http://en.wikipedia.org/wiki/Wave</a>). Our goal, however, is not to simulate the real world as accurately as possible but to provide the artist with a texture that looks good and is controllable so that the texture may even be applied in situations which are not realistic.</p><p>So instead of making the speed at which the ripple travels dependent on things, such as the viscosity of the water, we provide speed as a tunable input to our Pynode. Likewise for the height and width of the ripple and the rate at which the height of the ripple diminishes as it expands. Basically, we approximate our little packet of ripples as it radiates outward from the point of impact of a droplet by a cosine function multiplied by an exponential function and a damping factor. This may sound dangerously like mathematics again, but it can be easily visualized:</p><div class="mediaobject"><img alt="What looks good, is good" src="graphics/0400-07-12.jpg"/></div><a class="indexterm" id="id454"/><p>To calculate the height at any position x, y in our texture the above can be implemented as follows:</p><div class="informalexample"><pre class="programlisting">position_of_maximum=speed*time
damping = 1.0/(1.0+dampf*position_of_maximum)
distance = sqrt((x-dropx)**2+(y-dropy)**2)
height = damping*a*exp(-(distance-position_of_maximum)**2/c)* \cos(freq*(distance-position_of_maximum))</pre></div><p>Here, <code class="literal">dropx</code> and <code class="literal">dropy</code> are the positions of impact of a drop and <code class="literal">a</code> is our tunable height parameter.</p><p>The effects of more drops dropped at different times and at different locations may simply be calculated by summing the resulting heights.</p></div><div class="section" title="Storing expensive results for reuse"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec73"/>Storing expensive results for reuse</h2></div></div></div><a class="indexterm" id="id455"/><p>A single drop is not rain of course, so we would like to see the effects of many random drops added together. Therefore, we have to choose random impact locations and times for as many droplets as we'd like to simulate.</p><p>We would have to do this every time a call to the <code class="literal">__call__()</code> method is made (this is, for every visible pixel in our texture). However, this would be a tremendous waste of processing power because calculating many random numbers and allocating and releasing memory for possibly a large number of drops is expensive.</p><p>Fortunately, we can store these results as instance variables of our Pynode. Of course, we should be careful to check that no input parameters have changed between invocations of <code class="literal">__call__()</code> and take appropriate action if they have changed. The general pattern would look like this:</p><div class="informalexample"><pre class="programlisting">class MyNode(Node.Scripted):

   def __init__(self, sockets):
      sockets.input   = [Node.Socket('InputParam', val = 1.0)]
      sockets.output  = [Node.Socket('OutputVal' , val = 1.0)]
      self.InputParam = None
      self.Result     = None

   def __call__(self):
      if self.InputParam == None or \
         self.InputParam != self.input.InputParam :
         self.InputParam = self.input.InputParam
         self.Result     = expensive_calculation ...
      self.output.OutputVal = other_calculations_using_Result …</pre></div><a class="indexterm" id="id456"/><p>This pattern works only if the input parameter changes infrequently, for example, only if the user changes it. If the input changes every pixel because the input socket is connected to the output of another node—the suggested scheme only costs time instead of saving some.</p></div><div class="section" title="Calculating normals"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec74"/>Calculating normals</h2></div></div></div><a class="indexterm" id="id457"/><p>Our goal is to generate a ripple pattern that can be used as a normal. so we need some way to derive the normal from the calculated heights. Blender does not provide us with such a conversion node for materials so we have to devise a scheme ourselves.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note21"/>Note</h3><p>Contrary to materials nodes, Blender's texture nodes do provide a conversion function called 'Value to Normal' that is available in the texture node editor from the menu <span class="strong"><strong>Add|Convertor|Value</strong></span> to Normal.</p></div></div><p>Now, as in the case of ripples, we could, in principle, calculate an exact normal for our rain drops as well, but instead of going the mathematical way again we adapt a method used by many built-in noise textures to calculate the normal that works irrespective of the underlying function.</p><p>As long as we can evaluate a function at three points: <code class="literal">f(x,y),f(x+nabla,y)</code>, and <code class="literal">f(x,y+nabla)</code> we can estimate the direction of the normal at x,y by looking at the slopes of our function in the x and y direction. The surface normal will be the unit vector perpendicular to the plane defined by these two slopes. We can take any small value for <code class="literal">nabla</code> to start and if it doesn't look good, we can make it smaller.</p></div><div class="section" title="Putting it all together"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec75"/>Putting it all together</h2></div></div></div><a class="indexterm" id="id458"/><p>Taking all of these ideas from the preceding paragraphs, we can cook up the following code for our raindrops Pynode (with <code class="literal">import</code> statements omitted):</p><div class="informalexample"><pre class="programlisting">class Raindrops(Node.Scripted):
   def __init__(self, sockets):
      sockets.input = [Node.Socket('Drops_per_second'  , val = 5.0,min = 0.01, max = 100.0),
                       Node.Socket('a',val=5.0,min=0.01,max=100.0),
                       Node.Socket('c',val=0.04,min=0.001,max=10.0),
                       Node.Socket('speed',val=1.0,min=0.001,max=10.0),
                       Node.Socket('freq',val=25.0,min=0.1,max=100.0),
                       Node.Socket('dampf',val=1.0,min=0.01,max=100.0),
                       Node.Socket('Coords', val = 3*[1.0])]

      sockets.output = [Node.Socket('Height', val = 1.0),
                        Node.Socket('Normal', val = 3 *[0.0])]

      self.drops_per_second = None
      self.ndrops = None</pre></div><p>The initialization code defines a number of input sockets besides the coordinates. <code class="literal">Drops_per_second</code> should be self explanatory. <code class="literal">a</code> and <code class="literal">c</code> are the overall height and width of the ripples traveling outward from the point of impact. <code class="literal">speed</code> and <code class="literal">freq</code> determine how fast our ripples travel and how close ripples are together. How fast the height of the ripples diminishes as they travel outward is determined by <code class="literal">dampf</code>.</p><p>We also define two output sockets: <code class="literal">Height</code> will contain the calculated height and <code class="literal">Normal</code> will contain the corresponding normal at that same point. The <code class="literal">Normal</code> is what you would normally use to obtain the rippling surface effect, but the calculated height might be useful for example to attenuate the reflectance value of the surface.</p><p>The initialization ends with the definition of some instance variables that will be used to determine if we need to calculate the position of the drop impacts again as we will see in the definition of the <code class="literal">__call__()</code> function.</p><p>The definition of the <code class="literal">__call__()</code> function starts off with the initialization of a number of local variables. One notable point is that we set the random seed used by the functions of the <code class="literal">Noise</code> module (highlighted in the following code). In this way, we make sure that each time we recalculate the points of impact we get repeatable results, that is if we set the number of drops per second first to ten, later to twenty, and then back to ten, the generated pattern will be the same. If you would like to change this you could add an extra input socket to be used as input for the <code class="literal">setRandomSeed()</code> function:</p><div class="informalexample"><pre class="programlisting">   def __call__(self):
      
      twopi = 2*pi
      
      col = [0,0,0,1]
      nor = [0,0,1]
      tex_coord = self.input.Coords
      x = tex_coord[0] 
      y = tex_coord[1]
      
      a = self.input.a
      c = self.input.c
	  
	  <span class="strong"><strong>Noise.setRandomSeed(42)</strong></span>

      scn               = Scene.GetCurrent()
      context           = scn.getRenderingContext()
      current_frame     = context.currentFrame()
      start_frame       = context.startFrame()
      end_frame         = context.endFrame()
      frames_per_second = context.fps
      time              = current_frame/float(frames_per_second)</pre></div><a class="indexterm" id="id459"/><p>The next step is to determine whether we have to calculate the positions of the points of impact of the drops anew. This is necessary only when the value of the input socket <code class="literal">Drops_per_second</code> is changed by the user (you could hook up this input to some other node that changes this value at every pixel, but that wouldn't be a good idea) or when the start or stop frame of the animation changes, as this influences the number of drops we have to calculate. This test is performed in the highlighted line of the following code by comparing the newly obtained values to the ones stored in the instance variables:</p><div class="informalexample"><pre class="programlisting">      drops_per_second = self.input.Drops_per_second
      # calculate the number of drops to generate
      # in the animated timeframe
      ndrops = 1 + int(drops_per_second * (float(end_frame) –
               start_frame+1)/frames_per_second )
	  
	  <span class="strong"><strong>if self.drops_per_second != drops_per_second
	  or self.ndrops != ndrops:</strong></span>
         self.drop = [ (Noise.random(), Noise.random(),
                       Noise.random() + 0.5) for i in range(ndrops)]
         self.drops_per_second = drops_per_second
         self.ndrops = ndrops</pre></div><p>If we do have to calculate the position of the drops anew we assign a list of tuples to the <code class="literal">self.drop</code> instance variable, each consisting of the x and y position of the drop and a random drop size that will attenuate the height of the ripples.</p><a class="indexterm" id="id460"/><p>The rest of the lines are all executed each time <code class="literal">__call__()</code> is called but the highlighted line does show a significant optimization. Because drops that have not yet fallen in the current frame do not contribute to the height, we exclude those from the calculation:</p><div class="informalexample"><pre class="programlisting">      speed=self.input.speed
      freq=self.input.freq
      dampf=self.input.dampf
      
      height = 0.0
      height_dx = 0.0
      height_dy = 0.0
      nabla = 0.01
	  <span class="strong"><strong>for i in range(1+int(drops_per_second*time)):</strong></span>
         dropx,dropy,dropsize = self.drop[i]
         position_of_maximum=speed*time-i/float(drops_per_second)
         damping = 1.0/(1.0+dampf*position_of_maximum)
         distance = sqrt((x-dropx)**2+(y-dropy)**2)
         height += damping*a*dropsize*
            exp(-(distance-position_of_maximum)**2/c)*
         cos(freq*(distance-position_of_maximum))
         distance_dx = sqrt((x+nabla-dropx)**2+(y-dropy)**2)
         height_dx += damping*a*dropsize*
exp(-(distance_dx-position_of_maximum)**2/c)*
cos(freq*(distance_dx-position_of_maximum))
         distance_dy = sqrt((x-dropx)**2+(y+nabla-dropy)**2)
         height_dy += damping*a*dropsize*
exp(-(distance_dy-position_of_maximum)**2/c)*
cos(freq*(distance_dy-position_of_maximum))</pre></div><p>In the preceding code we actually calculate the height at three different positions to be able to approximate the normal (as explained previously). These values are used in the following lines to determine the x and y components of the normal (the z component is set to one). The calculated height itself is divided by the number of drops (so the average height will not change when the number of drops is changed) and by the overall scaling factor <code class="literal">a</code>, which may be set by the user before it is assigned to the output socket (highlighted): </p><div class="informalexample"><pre class="programlisting">      nor[0]=height-height_dx
      nor[1]=height-height_dy
      
	  <span class="strong"><strong>height /= ndrops * a</strong></span>
      self.output.Height = height
      
      N = (vec(self.shi.surfaceNormal)+0.2*vec(nor)).normalize()
      self.output.Normal= N
      
__node__ = Raindrops</pre></div><a class="indexterm" id="id461"/><p>The calculated normal is then added to the surface normal at the pixel where we are calculating so the ripples will still look good on a curved surface and normalized before assigning it to the output socket. The final line as usual defines a meaningful name for this Pynode. The full code and a sample node setup are available as <code class="literal">raindrops.py</code> in <code class="literal">raindrops.blend</code>. A sample frame from an animation is shown in the next screenshot:</p><div class="mediaobject"><img alt="Putting it all together" src="graphics/0400-07-13.jpg"/></div><p>A sample node setup is shown in the following screenshot:</p><div class="mediaobject"><img alt="Putting it all together" src="graphics/0400-07-14.jpg"/></div></div></div>
<div class="section" title="Wuthering heights&#x2014;a slope-dependent material"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec56"/>Wuthering heights—a slope-dependent material</h1></div></div></div><a class="indexterm" id="id462"/><p>In Blender it is quite simple to generate a fractal terrain (just add a plane, go to <span class="emphasis"><em>edit</em></span> mode, select all, and then subdivide fractal a few times <span class="emphasis"><em>W → 3</em></span>). If you want something more elaborate a few excellent scripts exist to help you (see for example <a class="ulink" href="http://sites.google.com/site/androcto/Home/python-scripts/ANTLandscape_104b_249.py">http://sites.google.com/site/androcto/Home/python-scripts/ANTLandscape_104b_249.py</a>). But how would you apply textures to such a terrain? In this example, we will examine a method to choose between different material inputs based on the slope of the surface that we're shading. This will allow us to create the effect that very steep slopes are generally devoid of greenery even though they might be well below the tree line. Combined with a height-dependent material we should be able to shade a mountainous terrain in a pretty convincing way.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note22"/>Note</h3><p>
<span class="strong"><strong>Reducing computation time:</strong></span>
</p><p>Pynodes are computationally intensive as they are called for every visible pixel. Clever coding can sometimes reduce the amount of computation needed but if a further speedup is required a just-in-time compiler might help. <span class="strong"><strong>psyco</strong></span> is such a compiler and we will encounter it in the last chapter where we will apply it on Pynodes and see whether it has any appreciable effect.</p></div></div><div class="section" title="Determining the slope"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec76"/>Determining the slope</h2></div></div></div><a class="indexterm" id="id463"/><p>The <span class="strong"><strong>slope</strong></span> can be defined as the angle between the floor plane and a line tangent to the surface at the point of interest.</p><div class="mediaobject"><img alt="Determining the slope" src="graphics/0400-07-15.jpg"/></div><p>Because we assume our (imaginary) floor plane to stretch horizontally along the x and y axis this angle is completely determined by the z-component of the surface normal at the same point. Now we can calculate this angle exactly (it is <span class="inlinemediaobject"><img alt="Determining the slope" src="graphics/0400-07-16.jpg"/></span>), but as artists we may want to have some extra control anyway so we simply take the normalized z-component of the surface normal and modify this output intensity with any color ramp node that we like. Within a Pynode a surface normal is a readily available vector entity: <code class="literal">self.input.shi.surfaceNormal</code>. There is a snag however...</p></div><div class="section" title="World space versus camera space"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec77"/>World space versus camera space</h2></div></div></div><a class="indexterm" id="id464"/><p>The surface normal that we have available happens to be defined in camera space. This means that, for example, when the surface normal is pointing straight at the camera it is defined as (0, 0,-1). Now we want our surface normals to be defined in world space. A normal that is pointing straight up for instance should have a value of (0,0,1) irrespective of the position or tilt of the camera (after all, mountainside vegetation does not normally change with the camera angle). Fortunately, we can convert from <span class="strong"><strong>camera space</strong></span> to <span class="strong"><strong>world space</strong></span> by taking the camera's world space matrix and multiplying the surface normal with the rotation part of this matrix. The resulting code looks like this:</p><div class="informalexample"><pre class="programlisting">class Slope(Node.Scripted):
   def __init__(self, sockets):
      sockets.output = [Node.Socket('SlopeX', val = 1.0),Node.Socket('SlopeY', val = 1.0),Node.Socket('SlopeZ', val = 1.0),]
      self.offset =  vec([1,1,1])
      self.scale =  0.5</pre></div><a class="indexterm" id="id465"/><p>Note that the initialization code does not define an input socket. We will get the surface normal at the position of the pixel that we are shading from the shader input (highlighted in the next piece of code). We do define three separate output sockets for the x, y, and z components of the slope for ease of use in a node setup. As we mostly will be using just the z-component of the slope, having it available in a separate socket saves use from extracting it from a vector with an additional vector manipulation node.</p><div class="informalexample"><pre class="programlisting">   def __call__(self):
      
      scn=Scene.GetCurrent()
      cam=scn.objects.camera
      rot=cam.getMatrix('worldspace').rotationPart().resize4x4();
      
      N = vec(<span class="strong"><strong>self.shi.surfaceNormal</strong></span>).normalize().resize4D() * rot
      N = (N + self.offset ) * self.scale
      self.output.SlopeX=N[0]
      self.output.SlopeY=N[1]
      self.output.SlopeZ=N[2]
      
__node__ = Slope</pre></div><p>The transformation from camera space to world space is done in the line that references the surface normal (highlighted). The orientation is dependent only on the rotation, therefore we extract only the rotation part of the camera's transformation matrix before we multiply the surface normal with it. As the normalized result may point downward we force the z-component to lie in the range [0, 1] by adding 1 and multiplying by 0.5. The full code is available as <code class="literal">slope.py</code> in <code class="literal">slope.blend</code>.</p><p>There is one important thing to be aware of: the surface normal that we use here is not interpolated and hence equal everywhere along the surface of a single face, even if the <code class="literal">smooth</code> attribute of a face is set. This shouldn't be a problem in a finely subdivided landscape where the slope input is not used directly, However, this is different from what you might expect. In the present implementation of Pynodes, this limitation is difficult if not impossible to overcome.</p><p>The following illustration shows an example of what is possible.</p><div class="mediaobject"><img alt="World space versus camera space" src="graphics/0400-07-17.jpg"/></div><p>The effects shown above were realized by combining different materials in the node setup shown in the next screenshot. This setup is available in <code class="literal">slope.blend </code>as well. The lower two materials were mixed using our slope-dependent node and the resulting material was mixed with the upper material based on a Pynode that calculates the height.</p><div class="mediaobject"><img alt="World space versus camera space" src="graphics/0400-07-18.jpg"/></div></div></div>
<div class="section" title="Soap bubbles&#x2014;a view-dependent shader"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec57"/>Soap bubbles—a view-dependent shader</h1></div></div></div><a class="indexterm" id="id466"/><a class="indexterm" id="id467"/><p>Some materials change the way they look depending on the angle at which we look at them. Bird feathers, some fancy car paints, oil spills on water, and <span class="strong"><strong>soap bubbles</strong></span> are some examples. This phenomenon of changing colors is known as <span class="strong"><strong>iridescence</strong></span><a class="indexterm" id="id468"/>. If we want to implement something like that we need access to the view vector and the surface normal. In our soap bubble shader we see one way of doing this.</p><p>
<span class="emphasis"><em>First</em></span> <span class="emphasis"><em>some</em></span> <span class="emphasis"><em>mathematics</em></span>: Why is it that soap bubbles show all those different colors? Soap bubbles are basically curved sheets of water (with a little soap), and at the interface between air and water, light is reflected. An incident ray will therefore be partially reflected when it hits the outer surface of the bubble and be reflected again when it reaches the inner surface. The reflected light that reaches the eye is therefore a mixture of light that has traveled different distances; part of it has traveled the extra distance of twice the thickness of the soap bubble.</p><p>Now, light behaves like a wave and waves that interfere can either dampen or amplify each other depending on their phase, and two light rays that have traveled distances whose difference is not an exact multiple of their wavelength will cancel each other. The result is that white light (a continuum of colors) reflecting off a soap bubble with a thickness equal to half the wavelength of some specific color will show only that single color because all of the other colors are dampened as they do not "fit" properly between the inner and outer surface. (There is much more to soap bubbles. For more and more accurate information refer to: <a class="ulink" href="http://www.exploratorium.edu/ronh/bubbles/bubble_colors.html">http://www.exploratorium.edu/ronh/bubbles/bubble_colors.html</a>.)</p><div class="mediaobject"><img alt="Soap bubbles—a view-dependent shader" src="graphics/0400-07-19.jpg"/></div><p>Now we know that the distance traveled between the two reflecting surfaces determines the color we perceive, we can also understand why there will be color variations in a soap bubble. The first factor is the curvature of the bubble. The distance traveled will be dependent on the angle between the incident light and the surface: the shallower this angle, the longer the distance the light has to travel between the surfaces will be. As the angle of incidence changes as the surface curves so will the distance and, hence the color. <a class="indexterm" id="id469"/>
<a class="indexterm" id="id470"/>The second source of color variation is the unevenness of the surface; slight variations due to gravity or swirls caused by air currents or temperature differences also cause different colors.</p><p>All this information translates to a surprisingly short piece of code (the full code is available as <code class="literal">irridescence.py</code> in <code class="literal">irridescence.blend</code> together with a sample node setup).</p><p>Beside the coordinates, we have two input sockets—one for the thickness of the water film and one for the variation. The variation will get added to the thickness and can be hooked up to a texture node to generate swirls and the like. We have a single output socket for the calculated distance:</p><div class="informalexample"><pre class="programlisting">class Iridescence(Node.Scripted):
   def __init__(self, sockets):
      sockets.input = [ Node.Socket('Coords', val= 3*[1.0]),Node.Socket('Thickness', val=275.0,min=100.0, max=1000.0),Node.Socket('Variation', val=0.5, min=0.0,max=1.0)]

      sockets.output = [Node.Socket('Distance', val=0.5, min=0.0,max=1.0)]</pre></div><a class="indexterm" id="id471"/><a class="indexterm" id="id472"/><p>The calculations of the reflected color start off with getting a list of all lamps in the scene as we will want to calculate the angle of the incident light rays. For now, we take into account only the contribution of the first lamp that we find. However, a more complete implementation would consider all lamps and maybe even their color. For our calculations we have to make certain that the surface normal <code class="literal">N</code> and the incidence vector of the light <code class="literal">L</code> are in the same space. As the surface normal provided will be in camera space we will have to transform this vector by the transformation matrix of the camera as we did for our slope-dependent shader (highlighted in the following code snippet):</p><div class="informalexample"><pre class="programlisting">   def __call__(self):
      
      P = vec(self.input.Coords)
      scn=Scene.GetCurrent()
      lamps = [ob for ob in scn.objects if ob.type == 'Lamp']
      
      lamp = lamps[0]
      
      cam=scn.objects.camera
      rot=cam.getMatrix('worldspace').rotationPart().resize4x4();
	  <span class="strong"><strong>N = vec(self.shi.surfaceNormal).normalize().resize4D() * rot</strong></span>
      
      N = N.negate().resize3D()
      L = vec(lamp.getLocation('worldspace'))
      I = (P – L).normalize()</pre></div><p>Next, we calculate the angle between the surface normal and the incidence vector (<code class="literal">VecT</code> is an alias for <code class="literal">Mathutils.angleBetweenVecs()</code>) and use this incidence angle to calculate the angle between the surface normal <span class="emphasis"><em>inside</em></span> the water film as this will determine the distance the light travels. We use <span class="strong"><strong>Snell's law</strong></span> to calculate this and use <code class="literal">1.31</code> as the index of refraction of the water film. Calculating the distance is then a matter of simple trigonometry (highlighted below):</p><div class="informalexample"><pre class="programlisting">      angle = VecT(I,N)
      
      angle_in = pi*angle/180
      sin_in = sin(angle_in)
      sin_out = sin_in/1.31
      angle_out = asin(sin_out)
      
      thickness = self.input.Thickness + self.input.Variation
<span class="strong"><strong>      distance = 2.0 * (thickness / cos (angle_out))</strong></span>
</pre></div><p>The calculated distance is equal to the wavelength of the color that we will perceive. However, Blender does not work with wavelengths but with RGB colors so we still need to convert this wavelength to a (R, G, B) tuple that represents the same color. This might be done by applying some spectral formula (see for example <a class="ulink" href="http://www.philiplaven.com/p19.html">http://www.philiplaven.com/p19.html</a>) but it might even be more versatile to scale this calculated distance and use it as an input for a color band. In this way we might produce non-physically accurate iridescence (if desired):</p><div class="informalexample"><pre class="programlisting">      self.output.Distance = distance</pre></div><a class="indexterm" id="id473"/><a class="indexterm" id="id474"/><p>To use this Pynode there are some things to keep in mind. First, make sure that the calculated color only affects the specular color of the soap bubble material otherwise everything will show up washed out.</p><p>Furthermore, it is important to add some variation to the thickness of the layer as no real soap bubble has an exactly uniform thickness. The choice of noise texture can make quite a difference to the appearance. In the next node setup example, we have added the contribution of a slightly noisy wood texture to obtain the swirly bands often seen on soap films.</p><p>Finally, make the material of the soap film very transparent but with a high specular reflectance. Experiment with the values to get the exact effect desired and do take into account the lighting setup. The example shown in the illustration has been tweaked to get some of the issues across in a black and white rendition and is therefore not realistic, but the setup in the example file <code class="literal">iridescence.blend</code> is tweaked to produce a pleasingly colorful result when rendered.</p><div class="mediaobject"><img alt="Soap bubbles—a view-dependent shader" src="graphics/0400-07-20.jpg"/></div><a class="indexterm" id="id475"/><a class="indexterm" id="id476"/><p>The use of a color ramp and a noise texture is shown in the previous screenshot where we added some division nodes to scale our distance to a range within [0,1] that can be used as input for the color ramp:</p><div class="mediaobject"><img alt="Soap bubbles—a view-dependent shader" src="graphics/0400-07-21.jpg"/></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec58"/>Summary</h1></div></div></div><p>In this chapter, we saw that Blender's lack of a compiled shader language does not prevent its use from designing custom patterns and shaders. Pynodes are an integrated part of Blender's node system and we saw how to use them to create effects from simple color patterns to fairly-complex animated ripples. Specifically, we learned:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How to write Pynodes that create simple color patterns</li><li class="listitem" style="list-style-type: disc">How to write Pynodes that produce patterns with normals</li><li class="listitem" style="list-style-type: disc">How to write animated Pynodes</li><li class="listitem" style="list-style-type: disc">How to write height and slope dependent materials</li><li class="listitem" style="list-style-type: disc">How to create shaders that react to the angle of incident light</li></ul></div><p>In the next chapter, we will look into the automation of the rendering process as a whole.</p></div></body></html>