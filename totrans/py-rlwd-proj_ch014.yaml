- en: Chapter 10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Cleaning Features
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of techniques for validating and converting data to native
    Python objects for subsequent analysis. This chapter guides you through three
    of these techniques, each appropriate for different kinds of data. The chapter
    moves on to the idea of standardization to transform unusual or atypical values
    into a more useful form. The chapter concludes with the integration of acquisition
    and cleansing into a composite pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will expand on the project in [*Chapter** 9*](ch013.xhtml#x1-2080009),
    [*Project 3.1: Data Cleaning* *Base Application*](ch013.xhtml#x1-2080009). The
    following additional skills will be emphasized:'
  prefs: []
  type: TYPE_NORMAL
- en: CLI application extension and refactoring to add features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pythonic approaches to validation and conversion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques for uncovering key relationships.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline architectures. This can be seen as a first step toward a processing
    **DAG** (**Directed Acyclic Graph**) in which various stages are connected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll start with a description of the first project to expand on the previous
    chapters on processing. This will include some new **Pydantic** features to work
    with more complex data source fields.
  prefs: []
  type: TYPE_NORMAL
- en: '10.1 Project 3.2: Validate and convert source fields'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [*Chapter** 9*](ch013.xhtml#x1-2080009), [*Project 3.1: Data Cleaning Base
    Application*](ch013.xhtml#x1-2080009) we relied on the foundational behavior of
    the **Pydantic** package to convert numeric fields from the source text to Python
    types like `int`, `float`, and `Decimal`. In this chapter, we’ll use a dataset
    that includes date strings so we can explore some more complex conversion rules.'
  prefs: []
  type: TYPE_NORMAL
- en: This will follow the design pattern from the earlier project. It will use a
    distinct data set, however, and some unique data model definitions.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.1 Description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This project’s intent is to perform data validation, cleaning, and standardization.
    This project will expand on the features of the `pydantic` package to do somewhat
    more sophisticated data validations and conversions.
  prefs: []
  type: TYPE_NORMAL
- en: This new data cleaning application can be designed around a data set like [https://tidesandcurrents.noaa.gov/tide_predictions.html](https://tidesandcurrents.noaa.gov/tide_predictions.html).
    The tide predictions around the US include dates, but the fields are decomposed,
    and our data cleaning application needs to combine them.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a specific example, see [https://tidesandcurrents.noaa.gov/noaatideannual.html?id=8725769](https://tidesandcurrents.noaa.gov/noaatideannual.html?id=8725769).
    Note that the downloaded `.txt` file is a tab-delimited CSV file with a very complicated
    multi-line header. This will require some sophisticated acquisition processing
    similar to the examples shown in [*Chapter** 3*](ch007.xhtml#x1-560003), [*Project
    1.1: Data* *Acquisition Base Application*](ch007.xhtml#x1-560003).'
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative example is the CO2 PPM — Trends in Atmospheric Carbon Dioxide
    data set, available at [https://datahub.io/core/co2-ppm](https://datahub.io/core/co2-ppm).
    This has dates that are provided in two forms: as a `year-month-day` string and
    as a decimal number. We can better understand this data if we can reproduce the
    decimal number value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second example data set is [https://datahub.io/core/co2-ppm/r/0.html](https://datahub.io/core/co2-ppm/r/0.html)
    This is an HTML file, requiring some acquisition processing similar to the examples
    from [*Chapter** 4*](ch008.xhtml#x1-780004), [*Data Acquisition Features: Web
    APIs and* *Scraping*](ch008.xhtml#x1-780004).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The use case for this cleaning application is identical to the description
    shown in [*Chapter** 9*](ch013.xhtml#x1-2080009), [*Project 3.1: Data Cleaning
    Base Application*](ch013.xhtml#x1-2080009). The acquired data — pure text, extracted
    from the source files — will be cleaned to create **Pydantic** models with fields
    of useful Python internal types.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll take a quick look at the tide table data on the [https://tidesandcurrents.noaa.gov](https://tidesandcurrents.noaa.gov)
    website.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The data to be acquired has two interesting structural problems:'
  prefs: []
  type: TYPE_NORMAL
- en: There’s a 19-line preamble containing some useful metadata. Lines 2 to 18 have
    a format of a label and a value, for example, `State:`` FL`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The data is tab-delimited CSV data. There appear to be six column titles. However,
    looking at the tab characters, there are eight columns of header data followed
    by nine columns of data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The acquired data should fit the dataclass definition shown in the following
    fragment of a class definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The example omits the details of the `from_row()` method. If a CSV reader is
    used, this method needs to pick out columns from the CSV-format file, skipping
    over the generally empty columns. If regular expressions are used to parse the
    source lines, this method will use the groups from the match object.
  prefs: []
  type: TYPE_NORMAL
- en: Since this looks like many previous projects, we’ll look at the distinct technical
    approach next.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.2 Approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The core processing of the data cleaning application should be — except for
    a few module changes — very similar to the earlier examples. For reference, see
    [*Chapter** 9*](ch013.xhtml#x1-2080009), [*Project 3.1: Data Cleaning Base Application*](ch013.xhtml#x1-2080009),
    specifically [*Approach*](ch013.xhtml#x1-2150002). This suggests that the `clean`
    module should have minimal changes from the earlier version.'
  prefs: []
  type: TYPE_NORMAL
- en: The principle differences should be two different implementations of the `acquire_model`
    and the `analysis_model`. For the tide data example, a class is shown in the [*Description*](#x1-2310001)
    section that can be used for the acquire model.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to maintain a clear distinction between the acquired data, which
    is often text, and the data that will be used for later analysis, which can be
    a mixture of more useful Python object types.
  prefs: []
  type: TYPE_NORMAL
- en: The two-step conversion from source data to the interim acquired data format,
    and from the acquired data format to the clean data format can — sometimes — be
    optimized to a single conversion.
  prefs: []
  type: TYPE_NORMAL
- en: An optimization to combine processing into a single step can also make debugging
    more difficult.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll show one approach to defining the enumerated set of values for the state
    of the tide. In the source data, codes of `’H’` and `’L’` are used. The following
    class will define this enumeration of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll rely on the enumerated type and two other annotated types to define a
    complete record. We’ll return to the annotated types after showing the record
    as a whole first. A complete tide prediction record looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This shows how the source columns’ `date` and `time` are combined into a single
    text value prior to validation. This is done by the `from_acquire_dataclass()`
    method, so it happens before invoking the `TidePrediction` constructor.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `TideCleanDateTime` and `TideCleanHighLow` type hints will leverage annotated
    types to define validation rules for each of these attributes. Here are the two
    definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `TideCleanDateTime` type uses the `clean_date()` function to clean up the
    `date` string prior to any attempt at conversion. Similarly, the `TideCleanHighLow`
    type uses a lambda to transform the value to upper case before validation against
    the `HighLow` enumerated type.
  prefs: []
  type: TYPE_NORMAL
- en: The `clean_date()` function works by applying the one (and only) expected date
    format to the string value. This is not designed to be flexible or permissive.
    It’s designed to confirm the data is an exact match against expectations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If the data doesn’t match the expected format, the `strptime()` function will
    raise a `ValueError` exception. This will be incorporated into a `pydantic.ValidationError`
    exception that enumerates all of the errors encountered. The `match` statement
    will pass non-string values through to the **pydantic** handler for validation;
    we don’t need to handle any other types.
  prefs: []
  type: TYPE_NORMAL
- en: 'This model can also be used for analysis of clean data. (See [*Chapter** 13*](ch017.xhtml#x1-29700013),
    [*Project 4.1: Visual Analysis Techniques*](ch017.xhtml#x1-29700013).) In this
    case, the data will already be a valid `datetime.datetime` object, and no conversion
    will need to be performed. The use of a type hint of `str`` |`` datetime.datetime`
    emphasizes the two types of values this method will be applied to.'
  prefs: []
  type: TYPE_NORMAL
- en: This two-part ”combine and convert” operation is broken into two steps to fit
    into the **Pydantic** design pattern. The separation follows the principle of
    minimizing complex initialization processing and creating class definitions that
    are more declarative and less active.
  prefs: []
  type: TYPE_NORMAL
- en: It’s often helpful to keep the conversion steps small and separate.
  prefs: []
  type: TYPE_NORMAL
- en: Premature optimization to create a single, composite function is often a nightmare
    when changes are required.
  prefs: []
  type: TYPE_NORMAL
- en: For display purposes, the date, day-of-week, and time-of-day can be extracted
    from a single `datetime` instance. There’s no need to keep many date-related fields
    around as part of the `TidePrediction` object.
  prefs: []
  type: TYPE_NORMAL
- en: The tide prediction is provided in two separate units of measure. For the purposes
    of this example, we retained the two separate values. Pragmatically, the height
    in feet is the height in cm multiplied by ![-1-- 30.48](img/file42.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: For some applications, where the value for height in feet is rarely used, a
    property might make more sense than a computed value. For other applications,
    where the two heights are both used widely, having both values computed may improve
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.3 Deliverables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This project has the following deliverables:'
  prefs: []
  type: TYPE_NORMAL
- en: Documentation in the `docs` folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acceptance tests in the `tests/features` and `tests/steps` folders.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit tests for the application modules in the `tests` folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application to clean some acquired data and apply simple conversions to a few
    fields. Later projects will add more complex validation rules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many of these deliverables are described in previous chapters. Specifically,
    [*Chapter** 9*](ch013.xhtml#x1-2080009), [*Project 3.1: Data Cleaning Base Application*](ch013.xhtml#x1-2080009)
    covers the basics of the deliverables for this project.'
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests for validation functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The unique validators used by a Pydantic class need test cases. For the example
    shown, the validator function is used to convert two strings into a date.
  prefs: []
  type: TYPE_NORMAL
- en: 'Boundary Value Analysis suggests there are three equivalence classes for date
    conversions:'
  prefs: []
  type: TYPE_NORMAL
- en: Syntactically invalid dates. The punctuation or the number of digits is wrong.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Syntactically valid, but calendrical invalid dates. The 30th of February, for
    example, is invalid, even when formatted properly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Valid dates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The above list of classes leads to a minimum of three test cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some developers like to explore each of the fields within a date, providing
    5 distinct values: the lower limit (usually 1), the upper limit (e.g., 12 or 31),
    just below the limit (e.g., 0), just above the upper limit (e.g., 13 or 32), and
    a value that’s in the range and otherwise valid. These additional test cases,
    however, are really testing the `strptime()` method of the `datetime` class. These
    cases are duplicate tests of the `datetime` module. These cases are *not* needed,
    since the `datetime` module already has plenty of test cases for calendrically
    invalid date strings.'
  prefs: []
  type: TYPE_NORMAL
- en: Don’t test the behavior of modules outside the application. Those modules have
    their own test cases.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll look at a project to validate nominal data. This
    can be more complicated than validating ordinal or cardinal data.
  prefs: []
  type: TYPE_NORMAL
- en: '10.2 Project 3.3: Validate text fields (and numeric coded fields)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For nominal data, we’ll use **pydantic**’s technique of applying a validator
    function to the value of a field. In cases where the field contains a code consisting
    only of digits, there can be some ambiguity as to whether or not the value is
    a cardinal number. Some software may treat any sequence of digits as a number,
    dropping leading zeroes. This can lead to a need to use a validator to recover
    a sensible value for fields that are strings of digits, but not cardinal values.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.1 Description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This project’s intent is to perform data validation, cleaning, and standardization.
    This project will expand on the features of the **Pydantic** package to do somewhat
    more sophisticated data validation and conversion.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll continue working with a data set like [https://tidesandcurrents.noaa.gov/tide_predictions.html](https://tidesandcurrents.noaa.gov/tide_predictions.html).
    The tide predictions around the US include dates, but the date is decomposed into
    three fields, and our data cleaning application needs to combine them.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a specific example, see [https://tidesandcurrents.noaa.gov/noaatideannual.html?id=8725769](https://tidesandcurrents.noaa.gov/noaatideannual.html?id=8725769).
    Note that the downloaded `.txt` file is really a tab-delimited CSV file with a
    complex header. This will require some sophisticated acquisition processing similar
    to the examples shown in [*Chapter** 3*](ch007.xhtml#x1-560003), [*Project 1.1:
    Data Acquisition Base* *Application*](ch007.xhtml#x1-560003).'
  prefs: []
  type: TYPE_NORMAL
- en: For data with a relatively small domain of unique values, a Python `enum` class
    is a very handy way to define the allowed collection of values. Using an enumeration
    permits simple, strict validation by `pydantic`.
  prefs: []
  type: TYPE_NORMAL
- en: Some data — like account numbers, as one example — have a large domain of values
    that may be in a state of flux. Using an `enum` class would mean transforming
    the valid set of account numbers into an enumerated type before attempting to
    work with any data. This may not be particularly helpful, since there’s rarely
    a compelling need to confirm that an account number is valid; this is often a
    stipulation that is made about the data.
  prefs: []
  type: TYPE_NORMAL
- en: For fields like account numbers, there can be a need to validate potential values
    without an enumeration of all allowed values. This means the application must
    rely on patterns of the text to determine if the value is valid, or if the value
    needs to be cleaned to make it valid. For example, there may be a required number
    of digits, or check digits embedded within the code. In the case of credit card
    numbers, the last digit of a credit card number is used as part of confirmation
    that the overall number is valid. For more information, see [https://www.creditcardvalidator.org/articles/luhn-algorithm](https://www.creditcardvalidator.org/articles/luhn-algorithm).
  prefs: []
  type: TYPE_NORMAL
- en: After considering some of the additional validations that need to be performed,
    we’ll take a look at a design approach for adding more complicated validations
    to the cleaning application.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.2 Approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For reference to the general approach to this application, see [*Chapter** 9*](ch013.xhtml#x1-2080009),
    [*Project* *3.1: Data Cleaning Base Application*](ch013.xhtml#x1-2080009), specifically
    [*Approach*](ch013.xhtml#x1-2150002).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The model can be defined using the **pydantic** package. This package offers
    two paths to validating string values against a domain of valid values. These
    alternatives are:'
  prefs: []
  type: TYPE_NORMAL
- en: Define an enumeration with all valid values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define a regular expression for the string field. This has the advantage of
    defining very large domains of valid values, including *potentially* infinite
    domains of values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Enumeration is an elegant solution that defines the list of values as a class.
    As shown earlier, it might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This will define a domain of two string values, ”L” and ”H”. This map provides
    easier-to-understand names, `Low` and `High`. This class can be used by **pydantic**
    to validate a string value.
  prefs: []
  type: TYPE_NORMAL
- en: An example of a case when we need to apply a `BeforeValidator` annotated type
    might be some tide data with lower-case ”h” and ”l” instead of proper upper-case
    ”H” or ”L”. This allows the validator to clean the data **prior** to the built-in
    data conversion.
  prefs: []
  type: TYPE_NORMAL
- en: 'We might use an annotated type. It looked like this in the preceding example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The annotated type hint describes the base type, `HighLow`, and a validation
    rule to be applied before the **pydantic** conversion. In this case, it’s a lambda
    to convert the text to upper case. We’ve emphasized the validation of enumerated
    values using an explicit enumeration because it is an important technique for
    establishing the complete set of allowed codes for a given attribute. The enumerated
    type’s class definition is often a handy place to record notes and other information
    about the coded values.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve looked at the various aspects of the approach, we can turn our
    attention to the deliverables for this project.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.3 Deliverables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This project has the following deliverables:'
  prefs: []
  type: TYPE_NORMAL
- en: Documentation in the `docs` folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acceptance tests in the `tests/features` and `tests/steps` folders.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit tests for the application modules in the `tests` folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application to clean source data in a number of fields.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many of these deliverables are described in previous chapters. Specifically,
    [*Chapter** 9*](ch013.xhtml#x1-2080009), [*Project 3.1: Data Cleaning Base Application*](ch013.xhtml#x1-2080009)
    covers the basics of the deliverables for this project.'
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests for validation functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The unique validators used by a pydantic class need test cases. For the example
    shown, the validator function is used to validate the state of the tide. This
    is a small domain of enumerated values. There are three core kinds of test cases:'
  prefs: []
  type: TYPE_NORMAL
- en: Valid codes like `’H’` or `’L’`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Codes that can be reliably cleaned. For example, lower-case codes `’h’` and
    `’l’` are unambiguous. A data inspection notebook may reveal non-code values like
    `’High’` or `’Low’`, also. These can be reliably cleaned.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invalid codes like `’’`, or `’9’`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The domain of values that can be cleaned properly is something that is subject
    to a great deal of change. It’s common to find problems and use an inspection
    notebook to uncover a new encoding when upstream applications change. This will
    lead to additional test cases, and then additional validation processing to make
    the test cases pass.
  prefs: []
  type: TYPE_NORMAL
- en: In the next project, we’ll look at the situation where data must be validated
    against an externally defined set of values.
  prefs: []
  type: TYPE_NORMAL
- en: '10.3 Project 3.4: Validate references among separate data sources'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [*Chapter** 9*](ch013.xhtml#x1-2080009), [*Project 3.1: Data Cleaning Base
    Application*](ch013.xhtml#x1-2080009) we relied on the foundational behavior of
    Pydantic to convert fields from source text to Python types. This next project
    will look at a more complicated validation rule.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.1 Description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This project’s intent is to perform data validation, cleaning, and standardization.
    This project will expand on the features of the `pydantic` package to do somewhat
    more sophisticated data validations and conversions.
  prefs: []
  type: TYPE_NORMAL
- en: Data sets in [https://data.census.gov](https://data.census.gov) have Z**IP Code
    Tabulation Areas** (**ZCTAs**). For certain regions, these US postal codes can
    (and should) have leading zeroes. In some variations on this data, however, the
    ZIP codes get treated as numbers and the leading zeroes get lost.
  prefs: []
  type: TYPE_NORMAL
- en: Data sets at [https://data.census.gov](https://data.census.gov) have information
    about the city of Boston, Massachusets, which has numerous US postal codes with
    leading zeroes. The Food Establishment Inspections available at [https://data.boston.gov/group/permitting](https://data.boston.gov/group/permitting)
    provides insight into Boston-area restaurants. In addition to postal codes (which
    are nominal data), this data involves numerous fields that contain nominal data
    as well as ordinal data.
  prefs: []
  type: TYPE_NORMAL
- en: For data with a relatively small domain of unique values, a Python `enum` class
    is a very handy way to define the allowed collection of values. Using an enumeration
    permits simple, strict validation by **Pydantic**.
  prefs: []
  type: TYPE_NORMAL
- en: Some data — like account numbers, as one example — have a large domain of values
    that may be in a state of flux. Using an `enum` class would mean transforming
    the valid set of account numbers into an enum before attempting to work with any
    data. This may not be particularly helpful, since there’s rarely a compelling
    need to confirm that an account number is valid; this is often a simple stipulation
    that is made about the data.
  prefs: []
  type: TYPE_NORMAL
- en: This leads to a need to validate potential values without an enumeration of
    the allowed values. This means the application must rely on patterns of the text
    to determine if the value is valid, or if the value needs to be cleaned to make
    it valid.
  prefs: []
  type: TYPE_NORMAL
- en: 'When an application cleans postal code data, there are two distinct parts to
    the cleaning:'
  prefs: []
  type: TYPE_NORMAL
- en: Clean the postal code to have the proper format. For US ZIP codes, it’s generally
    5 digits. Some codes are 5 digits, a hyphen, and 4 more digits.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the code with some master list to be sure it’s a meaningful code that
    references an actual post office or location.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It’s important to keep these separate since the first step is covered by the
    previous project, and doesn’t involve anything terribly complicated. The second
    step involves some additional processing to compare a given record against a master
    list of allowed values.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.2 Approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For reference to the general approach to this application, see [*Chapter** 9*](ch013.xhtml#x1-2080009),
    [*Project* *3.1: Data Cleaning Base Application*](ch013.xhtml#x1-2080009), specifically
    [*Approach*](ch013.xhtml#x1-2150002).'
  prefs: []
  type: TYPE_NORMAL
- en: When we have nominal values that must refer to external data, we can call these
    “foreign keys.” They’re references to an external collection of entities for which
    the values are primary keys. An example of this is a postal code. There is a defined
    list of valid postal codes; the code is a primary key in this collection. In our
    sample data, the postal code is a foreign key reference to the defining collection
    of postal codes.
  prefs: []
  type: TYPE_NORMAL
- en: Other examples include country codes, US state codes, and US phone system area
    codes. We can write a regular expression to describe the potential domain of key
    values. For US state codes, for example, we can use the regular expression `r’\w\w’`
    to describe state codes as having two letters. We could narrow this domain slightly
    using `r’[A-Z]{2}’` to require the state code use upper-case letters only. There
    are only 50 state codes, plus a few territories and districts; limiting this further
    would make for a very long regular expression.
  prefs: []
  type: TYPE_NORMAL
- en: The confounding factor here is when the primary keys need to be loaded from
    an external source — for example, a database. In this case, the simple `@validator`
    method has a dependency on external data. Further, this data must be loaded prior
    to any data cleaning activities.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have two choices for gathering the set of valid key values:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an `Enum` class with a list of valid values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define a `@classmethod` to initialize the pydantic class with valid values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, [https://data.opendatasoft.com](https://data.opendatasoft.com)
    has a useful list of US zip codes. See the URL [https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/georef-united-states-of-america-zc-point@public/exports/csv](https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/georef-united-states-of-america-zc-point@public/exports/csv)
    for US Zip Codes Points, United States of America. This is a file that can be
    downloaded and transformed into an enum or used to initialize a class. The `Enum`
    class creation is a matter of creating a list of two tuples with the label and
    the value for the enumeration. The `Enum` definition can be built with code like
    the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This will create an enumerated class, `ZipCode`, from the approximately 33,000
    ZIP codes in the downloaded source file. The enumerated labels will be Python
    attribute names similar to `ZIP_75846`. The values for these labels will be the
    US postal codes, for example, `’75846’`. The `":0>5s"` string format will force
    in leading zeroes where needed.
  prefs: []
  type: TYPE_NORMAL
- en: The `zip_code_values()` function saves us from writing 30,000 lines of code
    to define the enumeration class, `ZipCode`. Instead, this function reads 30,000
    values, creating a list of pairs used to create an `Enum` subclass.
  prefs: []
  type: TYPE_NORMAL
- en: The odd encoding of `utf_8_sig` is necessary because the source file has a leading
    **byte-order mark** (**BOM**). This is unusual butpermitted by Unicode standards.
    Other data sources for ZIP codes may not include this odd artifact. The encoding
    gracefully ignores the BOM bytes.
  prefs: []
  type: TYPE_NORMAL
- en: The unusual encoding of `utf_8_sig` is a special case because this file happens
    to be in an odd format.
  prefs: []
  type: TYPE_NORMAL
- en: There are a large number of encodings for text. While UTF-8 is popular, it is
    not universal.
  prefs: []
  type: TYPE_NORMAL
- en: When unusual characters appear, it’s important to find the source of the data
    and ask what encoding they used.
  prefs: []
  type: TYPE_NORMAL
- en: In general, it’s impossible to uncover the encoding given a sample file. There
    are a large number of valid byte code mappings that overlap between ASCII, CP1252,
    and UTF-8.
  prefs: []
  type: TYPE_NORMAL
- en: This design requires the associated data file. One potential improvement is
    to create a Python module from the source data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the **Pydantic** functional validators uses a similar algorithm to the
    one shown above. The validation initialization is used to build an object that
    retains a set of valid values. We’ll start with the goal of a small model using
    annotated types. The model looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The model relies on the `ValidZip` type. This type has two validation rules:
    before any conversion, a `zip_format_valid()` function is applied, and after conversion,
    a `zip_lookup_valid()` function is used.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve only defined a single field, `zip`, in this **Pydantic** class. This will
    let us focus on the validation-by-lookup design. A more robust example, perhaps
    based on the Boston health inspections shown above, would have a number of additional
    fields reflecting the source data to be analyzed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The before validator function, `zip_format_valid()`, compares the ZIP code
    to a regular expression to ensure that it is valid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `zip_format_valid()` can be expanded to use an f-string like `f"{zip:0>5s}`
    to reformat a ZIP code that’s missing the leading zeroes. We’ll leave this for
    you to integrate into this function.
  prefs: []
  type: TYPE_NORMAL
- en: The after validator function is a callable object. It’s an instance of a class
    that defines the `__call__()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the core class definition, and the creation of the instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This will define the `zip_lookup_valid` callable object. Initially, there’s
    new value for the internal `self.zip_set` attribute. This must be built using
    a function that evaluates `zip_lookup_valid.load(source)`. This will populate
    the set of valid values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve called this function `prepare_validator()` and it looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This idea of a complex validation follows the SOLID design principle. It separates
    the essential work of the `SomethingWithZip` class from the `ValidZip` type definition.
  prefs: []
  type: TYPE_NORMAL
- en: Further, the `ValidZip` type depends on a separate class, `ZipLookupValidator`,
    which handles the complications of loading data. This separation makes it somewhat
    easier to change validation files, or change the format of the data used for validation
    without breaking the `SomethingWithZip` class and the applications that use it.
    Further, it provides a reusable type, `ValidZip`. This can be used for multiple
    fields of a model, or multiple models.
  prefs: []
  type: TYPE_NORMAL
- en: Having looked at the technical approach, we’ll shift to looking at the deliverables
    for this project.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.3 Deliverables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This project has the following deliverables:'
  prefs: []
  type: TYPE_NORMAL
- en: Documentation in the `docs` folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acceptance tests in the `tests/features` and `tests/steps` folders.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit tests for the application modules in the `tests` folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application to clean and validate data against external sources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many of these deliverables are described in previous chapters. Specifically,
    [*Chapter** 9*](ch013.xhtml#x1-2080009), [*Project 3.1: Data Cleaning Base Application*](ch013.xhtml#x1-2080009)
    covers the basics of the deliverables for this project.'
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests for data gathering and validation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The unique validators used by a **Pydantic** class need test cases. For the
    example shown, the validator function is used to validate US ZIP codes. There
    are three core kinds of test cases:'
  prefs: []
  type: TYPE_NORMAL
- en: Valid ZIP codes with five digits that are found in the ZIP code database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Syntactically valid ZIP codes with five digits that are **not** found in the
    ZIP code database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Syntactically invalid ZIP that don’t have five digits, or can’t — with the addition
    of leading zeroes — be made into valid codes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '10.4 Project 3.5: Standardize data to common codes and ranges'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another aspect of cleaning data is the transformation of raw data values into
    standardized values. For example, codes in use have evolved over time, and older
    data codes should be standardized to match new data codes. The notion of standardizing
    values can be a sensitive topic if critical information is treated as an outlier
    and rejected or improperly standardized.
  prefs: []
  type: TYPE_NORMAL
- en: We can also consider imputing new values to fill in for missing values as a
    kind of standardization technique. This can be a necessary step when dealing with
    missing data or data that’s likely to represent some measurement error, not the
    underlying phenomenon being analyzed.
  prefs: []
  type: TYPE_NORMAL
- en: This kind of transformation often requires careful, thoughtful justification.
    We’ll show some programming examples. The deeper questions of handling missing
    data, imputing values, handling outliers, and other standardization operations
    are outside the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: See [https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779)
    for an overview of some ways to deal with missing or invalid data.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.1 Description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creating standardized values is at the edge of data cleaning and validation.
    These values can be described as ”derived” values, computed from existing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are numerous kinds of standardizations; we’ll look at two:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute a standardized value, or Z-score, for cardinal data. For a normal distribution,
    the Z-scores have a mean of 0, and a standard deviation of 1\. It permits comparing
    values measured on different scales.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collapse nominal values into a single standardized value. For example, replacing
    a number of historical product codes with a single, current product code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first of these, computing a Z-score, rarely raises questions about the statistical
    validity of the standardized value. The computation, *Z* = ![x−σμ-](img/file45.jpg),
    is well understood and has known statistical properties.
  prefs: []
  type: TYPE_NORMAL
- en: The second standardization, replacing nominal values with a standardized code,
    can be troubling. This kind of substitution may simply correct errors in the historical
    record. It may also obscure an important relationship. It’s not unusual for a
    data inspection notebook to reveal outliers or erroneous values in a data set
    that needs to be standardized.
  prefs: []
  type: TYPE_NORMAL
- en: Enterprise software may have unrepaired bugs. Some business records can have
    unusual code values that map to other code values.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the codes in use may shift over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some records may have values that reflect two eras: pre-repair and post-repair.
    Worse, of course, there may have been several attempts at a repair, leading to
    more nuanced timelines.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this project, we need some relatively simple data. The Ancombe’s Quartet
    data will do nicely as examples from which derived Z-scores can be computed. For
    more information, see [*Chapter** 3*](ch007.xhtml#x1-560003), [*Project 1.1: Data
    Acquisition Base* *Application*](ch007.xhtml#x1-560003).'
  prefs: []
  type: TYPE_NORMAL
- en: The objective is to compute standardized values for the two values that comprise
    the samples in the Anscombe’s Quartet series. When the data has a normal distribution,
    these derived, standardized Z-scores will have a mean of zero and a standard deviation
    of one. When the data does not have a normal distribution, these values will diverge
    from the expected values.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.2 Approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For reference to the general approach to this application, see [*Chapter** 9*](ch013.xhtml#x1-2080009),
    [*Project* *3.1: Data Cleaning Base Application*](ch013.xhtml#x1-2080009), specifically
    [*Approach*](ch013.xhtml#x1-2150002).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To replace values with preferred standardized values, we’ve seen how to clean
    bad data in previous projects. See, for example, [*Project 3.3: Validate text
    fields (and* *numeric coded fields)*](#x1-2350002).'
  prefs: []
  type: TYPE_NORMAL
- en: For Z-score standardization, we’ll be computing a derived value. This requires
    knowing the mean, *μ*, and standard deviation, *σ*, for a variable from which
    the Z-score can be computed.
  prefs: []
  type: TYPE_NORMAL
- en: 'This computation of a derived value suggests there are the following two variations
    on the analytical data model class definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: An “initial” version, which lacks the Z-score values. These objects are incomplete
    and require further computation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A “final” version, where the Z-score values have been computed. These objects
    are complete.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two common approaches to handling this distinction between incomplete
    and complete objects:'
  prefs: []
  type: TYPE_NORMAL
- en: The two classes are distinct. The complete version is a subclass of the incomplete
    version, with additional fields defined.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The derived values are marked as optional. The incomplete version starts with
    `None` values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first design is a more conventional object-oriented approach. The formality
    of a distinct type to clearly mark the state of the data is a significant advantage.
    The extra class definition, however, can be seen as clutter, since the incomplete
    version is transient data that doesn’t create enduring value. The incomplete records
    live long enough to compute the complete version, and the file can then be deleted.
  prefs: []
  type: TYPE_NORMAL
- en: The second design is sometimes used for functional programming. It saves the
    subclass definition, which can be seen as a slight simplification.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: These two class definitions show one way to formalize the distinction between
    the initially cleaned, validated, and converted data, and the complete sample
    with the standardized Z-scores present for both of the variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be handled as three separate operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Clean and convert the initial data, writing a temporary file of the `InitialSample`
    instances.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the temporary file, computing the means and standard deviations of the
    variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the temporary file again, building the final samples from the `InitialSample`
    instances and the computed intermediate values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A sensible optimization is to combine the first two steps: clean and convert
    the data, accumulating values that can be used to compute the mean and standard
    deviation. This is helpful because the `statistics` module expects a sequence
    of objects that might not fit in memory. The mean, which involves a sum and a
    count, is relatively simple. The standard deviation requires accumulating a sum
    and a sum of squares.'
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑ mx = ---x n ](img/file46.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The mean of *x*, *m*[x], is the sum of the *x* values divided by the count of
    *x* values, shown as *n*.
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∘ ∑-------(∑-x)2- ---x2 −---n-- sx = n − 1 ](img/file47.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The standard deviation of *x*, *s*[x], uses the sum of *x*², the sum of *x*,
    and the number of values, *n*.
  prefs: []
  type: TYPE_NORMAL
- en: This formula for the standard deviation has some numeric stability issues, and
    there are variations that are better designs. See [https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance).
  prefs: []
  type: TYPE_NORMAL
- en: We’ll define a class that accumulates the values for mean and variance. From
    this, we can compute the standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This `variance` class performs an incremental computation of mean, standard
    deviation, and variance. Each individual value is presented by the `add()` method.
    After all of the data has been presented, the properties can be used to return
    the summary statistics.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s used as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This provides a way to compute the summary statistics without using a lot of
    memory. It permits the optimization of computing the statistics the first time
    the data is seen. And, it reflects a well-designed algorithm that is numerically
    stable.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve explored the technical approach, it’s time to look at the deliverables
    that must be created for this project.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.3 Deliverables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This project has the following deliverables:'
  prefs: []
  type: TYPE_NORMAL
- en: Documentation in the `docs` folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acceptance tests in the `tests/features` and `tests/steps` folders.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit tests for the application modules in the `tests` folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application to clean the acquired data and compute derived standardized Z-scores.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many of these deliverables are described in previous chapters. Specifically,
    [*Chapter** 9*](ch013.xhtml#x1-2080009), [*Project 3.1: Data Cleaning Base Application*](ch013.xhtml#x1-2080009)
    covers the basics of the deliverables for this project.'
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests for standardizing functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are two parts of the standardizing process that require unit tests. The
    first is the incremental computation of mean, variance, and standard deviation.
    This must be compared against results computed by the `statistics` module to assure
    that the results are correct. The `pytest.approx` object (or the `math.isclose()`
    function) are useful for asserting the incremental computation matches the expected
    values from the standard library module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, of course, the construction of the final sample, including the
    standardized Z-scores, needs to be tested. The test case is generally quite simple:
    a single value with a given x, y, mean of x, mean of y, the standard deviation
    of x, and the standard deviation of y need to be converted from the incomplete
    form to the complete form. The computation of the derived values is simple enough
    that the expected results can be computed by hand to check the results.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to test this class, even though it seems very simple. Experience
    suggests that these seemingly simple classes are places where a `+` replaces a
    `-` and the distinction isn’t noticed by people inspecting the code. This kind
    of small mistake is best found with a unit test.
  prefs: []
  type: TYPE_NORMAL
- en: Acceptance test
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The acceptance test suite for this standardization processing will involve a
    main program that creates two output files. This suggests the after-scenario cleanup
    needs to ensure the intermediate file is properly removed by the application.
  prefs: []
  type: TYPE_NORMAL
- en: The cleaning application could use the `tempfile` module to create a file that
    will be deleted when closed. This is quite reliable, but it can be difficult to
    debug very obscure problems if the file that reveals the problems is automatically
    deleted. This doesn’t require any additional acceptance test Then step to be sure
    the file is removed, since we don’t need to test the `tempfile` module.
  prefs: []
  type: TYPE_NORMAL
- en: The cleaning application can also create a temporary file in the current working
    directory. This can be unlinked for normal operation, but left in place for debugging
    purposes. This will require at least two scenarios to be sure the file is removed
    normally, and be sure the file is retained to support debugging.
  prefs: []
  type: TYPE_NORMAL
- en: The final choice of implementation — and the related test scenarios — is left
    to you.
  prefs: []
  type: TYPE_NORMAL
- en: '10.5 Project 3.6: Integration to create an acquisition pipeline'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*User experience*](ch013.xhtml#x1-2100001), we looked at the two-step user
    experience. One command is used to acquire data. After this, a second command
    is used to clean the data. An alternative user experience is a single shell pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.1 Description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The previous projects in this chapter have decomposed the cleaning operation
    into two distinct steps. There’s another, very desirable user experience alternative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we’d like the following to work, also:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The idea is to have the **acquire** application write a sequence of NDJSON objects
    to standard output. The **clean** application will read the sequence of NDJSON
    objects from standard input. The two applications will run concurrently, passing
    data from process to process.
  prefs: []
  type: TYPE_NORMAL
- en: For very large data sets, this can reduce the processing time. Because of the
    overhead in serializing Python objects to JSON text and deserializing Python objects
    from the text, the pipeline will not run in half the time of the two steps executed
    serially.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple extractions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the case of CSV extraction of Anscombe Quartet data, we have an **acquire**
    application that’s capable of creating four files concurrently. This doesn’t fit
    well with the shell pipeline. We have two architectural choices for handling this.
  prefs: []
  type: TYPE_NORMAL
- en: 'One choice is to implement a ”fan-out” operation: the **acquire** program fans
    out data to four separate clean applications. This is difficult to express as
    a collection of shell pipelines. To implement this, a parent application uses
    `concurrent.futures`, queues, and processing pools. Additionally, the **acquire**
    program would need to write to shared queue objects, and the **clean** program
    would read from a shared queue.'
  prefs: []
  type: TYPE_NORMAL
- en: The alternative is to process only one of the Anscombe series at a time. Introducing
    a `-s`` Series_1Pair` argument lets the user name a class that can extract a single
    series from the source data. Processing a single series at a time permits a pipeline
    that can be readily described as a shell command.
  prefs: []
  type: TYPE_NORMAL
- en: This concept is often necessary to disentangle enterprise data. It’s common
    for enterprise applications — which often evolve organically — to have values
    from distinct problem domains as parts of a common record.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll turn to the technical approach in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.2 Approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For reference to the general approach to this application, see [*Chapter** 9*](ch013.xhtml#x1-2080009),
    [*Project* *3.1: Data Cleaning Base Application*](ch013.xhtml#x1-2080009), specifically
    [*Approach*](ch013.xhtml#x1-2150002).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Writing the standard output (and reading from standard input) suggests that
    these applications will have two distinct operating modes:'
  prefs: []
  type: TYPE_NORMAL
- en: Opening a named file for output or input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using an existing, open, unnamed file — often a pipe created by the shell —
    for output or input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This suggests that the bulk of the design for an application needs to focus
    on open file-like objects. These are often described by the type hint of `TextIO`:
    they are files that can read (or write) text.'
  prefs: []
  type: TYPE_NORMAL
- en: The top-level `main()` function must be designed either to open a named file,
    or to provide `sys.stdout` or `sys.stdin` as argument values. The various combinations
    of files are provided to a function that will do the more useful work.
  prefs: []
  type: TYPE_NORMAL
- en: 'This pattern looks like the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The `process()` function is either given a file opened by a context manager,
    or the function is given the already open `sys.stdout`.
  prefs: []
  type: TYPE_NORMAL
- en: The ability for a Python application to be part of a shell pipeline is a significant
    help in creating larger, more sophisticated composite processes. This higher-level
    design effort is sometimes called “Programming In The Large.”
  prefs: []
  type: TYPE_NORMAL
- en: Being able to read and write from pipelines was a core design feature of the
    Unix operating system and continues to be central to all of the various GNU/Linux
    variants.
  prefs: []
  type: TYPE_NORMAL
- en: This pipeline-aware design has the advantage of being slightly easier to unit
    test. The `process()` function’s output argument value can be an `io.StringIO`
    object. When using a `StringIO` object, the file processing is simulated entirely
    in memory, leading to faster, and possibly simpler, tests.
  prefs: []
  type: TYPE_NORMAL
- en: This project sets the stage for a future project. See [*Chapter** 12*](ch016.xhtml#x1-27600012),
    [*Project 3.8:* *Integrated Data Acquisition Web Service*](ch016.xhtml#x1-27600012)
    for a web service that can leverage this pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Consider packages to help create a pipeline
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A Python application to create a shell pipeline can involve a fair amount of
    programming to create two subprocesses that share a common buffer. This is handled
    elegantly by the shell.
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative is [https://cgarciae.github.io/pypeln/](https://cgarciae.github.io/pypeln/).
    The **PypeLn** package is an example of a package that wraps the `subprocess`
    module to make it easier for a parent application to create a pipeline that executes
    the two child applications: **acquire** and **clean**.'
  prefs: []
  type: TYPE_NORMAL
- en: Using a higher-level Python application to start the acquire-to-clean pipeline
    avoids the potential pitfalls of shell programming. It permits Python programs
    with excellent logging and debugging capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve seen the technical approach, it’s appropriate to review the deliverables.
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.3 Deliverables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This project has the following deliverables:'
  prefs: []
  type: TYPE_NORMAL
- en: Documentation in the `docs` folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acceptance tests in the `tests/features` and `tests/steps` folders.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit tests for the application modules in the `tests` folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Revised applications that can be processed as a pipeline of two concurrent processes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many of these deliverables are described in previous chapters. Specifically,
    [*Chapter** 9*](ch013.xhtml#x1-2080009), [*Project 3.1: Data Cleaning Base Application*](ch013.xhtml#x1-2080009)
    covers the basics of the deliverables for this project.'
  prefs: []
  type: TYPE_NORMAL
- en: Acceptance test
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The acceptance test suite needs to confirm the two applications can be used
    as stand-alone commands, as well as used in a pipeline. One technique for confirming
    the pipeline behavior is to use shell programs like `cat` to provide input that
    mocks the input from another application.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the `When` step may execute the following kind of command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The **clean** application is executed in a context where it is part of an overall
    pipeline. The head of the pipeline is not the **acquire** application; we’ve used
    the `cat`` some_mock_file.ndj` command as a useful mock for the other application’s
    output. This technique permits a lot of flexibility to test applications in a
    variety of shell contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Using a pipeline can permit some helpful debugging because it disentangles two
    complicated programs into two smaller programs. The programs can be built, tested,
    and debugged in isolation.
  prefs: []
  type: TYPE_NORMAL
- en: 10.6 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This chapter expanded in several ways on the project in [*Chapter** 9*](ch013.xhtml#x1-2080009),
    [*Project 3.1:* *Data Cleaning Base Application*](ch013.xhtml#x1-2080009). The
    following additional processing features were added:'
  prefs: []
  type: TYPE_NORMAL
- en: Pythonic approaches to validation and conversion of cardinal values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approaches to validation and conversion of nominal and ordinal values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques for uncovering key relationships and validating data that must properly
    reference a foreign key.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline architectures using the shell pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10.7 Extras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here are some ideas for you to add to these projects.
  prefs: []
  type: TYPE_NORMAL
- en: 10.7.1 Hypothesis testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The computations for mean, variance, standard deviation, and standardized Z-scores
    involve floating-point values. In some cases, the ordinary truncation errors of
    float values can introduce significant numeric instability. For the most part,
    the choice of a proper algorithm can ensure results are useful.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to basic algorithm design, additional testing is sometimes helpful.
    For numeric algorithms, the **Hypothesis** package is particularly helpful. See
    [https://hypothesis.readthedocs.io/en/latest/](https://hypothesis.readthedocs.io/en/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking specifically at [*Project 3.5: Standardize data to common codes and*
    *ranges*](#x1-2450004), the [*Approach*](#x1-2470002) section suggests a way to
    compute the variance. This class definition is an excellent example of a design
    that can be tested effectively by the Hypothesis module to confirm that the results
    of providing a sequence of three known values produces the expected results for
    the count, sum, mean, variance, and standard deviation.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.7.2 Rejecting bad data via filtering (instead of logging)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the examples throughout this chapter, there’s been no in-depth mention of
    what to do with data that raises an exception because it cannot be processed.
    There are three common choices:'
  prefs: []
  type: TYPE_NORMAL
- en: Allow the exception to stop processing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log each problem row as it is encountered, discarding it from the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the faulty data to a separate output file so it can be examined with a
    data inspection notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first option is rather drastic. This is useful in some data cleaning applications
    where there’s a reasonable expectation of very clean, properly curated data. In
    some enterprise applications, this is a sensible assumption, and invalid data
    is the cause for crashing the application and sorting out the problems.
  prefs: []
  type: TYPE_NORMAL
- en: The second option has the advantage of simplicity. A `try:`/`except:` block
    can be used to write log entries for faulty data. If the volume of problems is
    small, then locating the problems in the log and resolving them may be appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: The third option is often used when there is a large volume of questionable
    or bad data. The rows are written to a file for further study.
  prefs: []
  type: TYPE_NORMAL
- en: 'You are encouraged to implement this third strategy: create a separate output
    file for rejected samples. This means creating acceptance tests for files that
    will lead to the rejection of at least one faulty row.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.7.3 Disjoint subentities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An even more complicated data validation problem occurs when the source documents
    don’t reflect a single resulting dataclass. This often happens when disjoint subtypes
    are merged into a single data set. This kind of data is a union of the disjoint
    types. The data must involve a ”discriminator” field that shows which type of
    object is being described.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we may have a few fields with date, time, and document ID that
    are common to all samples. In addition to those fields, a `document_type` field
    provides a set of codes to discriminate between the different kinds of invoices
    and different kinds of payments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, a conversion function involves two stages of conversions:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the subtype. This may involve converting the common fields and the
    discriminator field. The work will be delegated to a subtype-specific conversion
    for the rest of the work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert each subtype. This may involve a family of functions associated with
    each of the discriminator values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This leads to a function design as shown in the activity diagram in [*Figure
    10.1*](#10.1).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1: Subentity validation ](img/file48.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Subentity validation'
  prefs: []
  type: TYPE_NORMAL
- en: 10.7.4 Create a fan-out cleaning pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are two common alternatives for concurrent processing of a **acquire**
    and **clean** application:'
  prefs: []
  type: TYPE_NORMAL
- en: A shell pipeline that connects the **acquire** application to the **clean**
    application. These two subprocesses run concurrently. Each ND JSON line written
    by the **acquire** application is immediately available for processing by the
    **clean** application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pool of workers, managed by `concurrent.futures`. Each ND JSON line created
    by the **acquire** application is placed in a queue for one of the workers to
    consume.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The shell pipeline is shown in [*Figure 10.2*](#10.2).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2: Components of a shell pipeline ](img/file49.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Components of a shell pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: The shell creates two child process with a shared buffer between them. For the
    acquire child process, the shared buffer is `sys.stdout`. For the clean child
    process, the shared buffer is `sys.stdin`. As the two applications run, each byte
    written is available to be read.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve included explicit references to the Python runtime in these diagrams.
    This can help clarify how our application is part of the overall Python environment.
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline creation is an elegant feature of the shell, and can be used to
    create complex sequences of concurrent processing. This is a handy way to think
    of decomposing a large collection of transformations into a number of concurrent
    transformations.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, the pipeline model isn’t ideal. This is often the case when we
    need asymmetric collections of workers. For example, when one process is dramatically
    faster than another, it helps to have multiple copies of the slow processes to
    keep up with the faster process. This is handled politely by the `concurrent.futures`
    package, which lets an application create a ”pool” of workers.
  prefs: []
  type: TYPE_NORMAL
- en: The pool can be threads or processes, depending on the nature of the work. For
    the most part, CPU cores tend to be used better by process pools, because OS scheduling
    is often process-focused. The Python **Global Interpreter Lock** (**GIL**) often
    prohibits compute-intensive thread pools from making effective use of CPU resources.
  prefs: []
  type: TYPE_NORMAL
- en: For huge data sets, worker-pool architecture can provide some performance improvements.
    There is overhead in serializing and deserializing the Python objects to pass
    the values from process to process. This overhead imposes some limitations on
    the benefits of multiprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: The components that implement a worker process pool are shown in [*Figure 10.3*](#10.3).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3: Components of a worker pool ](img/file50.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Components of a worker pool'
  prefs: []
  type: TYPE_NORMAL
- en: This design is a significant alteration to the relationship between the `acquire.py`
    and `clean.py` applications. When the `acquire.py` application creates the process
    pool, it uses class and function definitions available within the same parent
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'This suggests the `clean.py` module needs to have a function that processes
    exactly one source document. This function may be as simple as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This function uses the analysis model definition, `SeriesSample`, to perform
    the validation, cleaning, and conversion of the acquired data. This can raise
    exceptions, which need to be logged.
  prefs: []
  type: TYPE_NORMAL
- en: The child processes are created with copies of the parent application’s logging
    configuration. The `multiprocessing.get_logger()` function will retrieve the logger
    that was initialized into the process when the pool of worker processes was created.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `acquire.py` application can use a higher-order `map()` function to allocate
    requests to the workers in an executor pool. The general approach is shown in
    the following incomplete code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This works by allocating a number of resources, starting with the target file
    to be written, then the pool of processes to write clean data records to the file,
    and finally, the source for the original, raw data samples. Each of these has
    a context manager to be sure the resources are properly released when all of the
    processing has finished.
  prefs: []
  type: TYPE_NORMAL
- en: We use the `ProcessPoolExecutor` object as a context manager to make sure the
    subprocesses are properly cleaned up when the source data has been fully consumed
    by the `map()` function, and all of the results retrieved from the `Future` objects
    that were created.
  prefs: []
  type: TYPE_NORMAL
- en: The `get_series()` function is an iterator that provides the builds the acquire
    version of each `SeriesSample` object. This will use an appropriately configured
    `Extractor` object to read a source and extract a series from it.
  prefs: []
  type: TYPE_NORMAL
- en: Since generators are lazy, nothing really happens until the values of the `acquire_document_iter`
    variable are consumed. The `executor.map()` will consume the source, providing
    each document to the pool of workers to create a `Future` object that reflects
    the work being done by a separate subprocess. When the work by the subprocess
    finishes, the `Future` object will have the result and be ready for another request.
  prefs: []
  type: TYPE_NORMAL
- en: When the `persist_samples()` functions consume the values from the `clean_samples`
    iterator, each of the `Future` objects will yield their result. These result objects
    are the values computed by the `clean.clean_sample()` function. The sequence of
    results is written to the target file.
  prefs: []
  type: TYPE_NORMAL
- en: The `concurrent.futures` process pool `map()` algorithm will preserve the original
    order. The process pool offers alternative methods that can make results ready
    as soon as they’re computed. This can reorder the results; something that may
    or may not be relevant for subsequent processing.
  prefs: []
  type: TYPE_NORMAL
