- en: Chapter 10
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章
- en: Data Cleaning Features
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗功能
- en: There are a number of techniques for validating and converting data to native
    Python objects for subsequent analysis. This chapter guides you through three
    of these techniques, each appropriate for different kinds of data. The chapter
    moves on to the idea of standardization to transform unusual or atypical values
    into a more useful form. The chapter concludes with the integration of acquisition
    and cleansing into a composite pipeline.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多验证数据并将其转换为后续分析的原生Python对象的技术。本章将指导您了解这三种技术，每种技术适用于不同类型的数据。本章接着讨论标准化思想，将异常或不典型值转换为更有用的形式。本章最后将获取和清洗集成到一个复合管道中。
- en: 'This chapter will expand on the project in [*Chapter** 9*](ch013.xhtml#x1-2080009),
    [*Project 3.1: Data Cleaning* *Base Application*](ch013.xhtml#x1-2080009). The
    following additional skills will be emphasized:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将在[*第9章*](ch013.xhtml#x1-2080009)、[*项目3.1：数据清洗基础应用程序*](ch013.xhtml#x1-2080009)的基础上扩展项目。以下附加技能将被强调：
- en: CLI application extension and refactoring to add features.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLI应用程序扩展和重构以添加功能。
- en: Pythonic approaches to validation and conversion.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证和转换的Python方法。
- en: Techniques for uncovering key relationships.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现关键关系的技巧。
- en: Pipeline architectures. This can be seen as a first step toward a processing
    **DAG** (**Directed Acyclic Graph**) in which various stages are connected.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道架构。这可以被视为向处理**DAG**（**有向无环图**）迈出的第一步，其中各个阶段相互连接。
- en: We’ll start with a description of the first project to expand on the previous
    chapters on processing. This will include some new **Pydantic** features to work
    with more complex data source fields.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个描述开始，以扩展之前章节中关于处理的内容。这将包括一些新的**Pydantic**功能，以处理更复杂的数据源字段。
- en: '10.1 Project 3.2: Validate and convert source fields'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 项目3.2：验证和转换源字段
- en: 'In [*Chapter** 9*](ch013.xhtml#x1-2080009), [*Project 3.1: Data Cleaning Base
    Application*](ch013.xhtml#x1-2080009) we relied on the foundational behavior of
    the **Pydantic** package to convert numeric fields from the source text to Python
    types like `int`, `float`, and `Decimal`. In this chapter, we’ll use a dataset
    that includes date strings so we can explore some more complex conversion rules.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第9章*](ch013.xhtml#x1-2080009)、[*项目3.1：数据清洗基础应用程序*](ch013.xhtml#x1-2080009)中，我们依赖于**Pydantic**包的基础行为，将源文本中的数值字段转换为Python类型，如`int`、`float`和`Decimal`。在本章中，我们将使用包含日期字符串的数据集，以便我们可以探索一些更复杂的转换规则。
- en: This will follow the design pattern from the earlier project. It will use a
    distinct data set, however, and some unique data model definitions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这将遵循早期项目的设计模式。它将使用一个不同的数据集，以及一些独特的数据模型定义。
- en: 10.1.1 Description
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1 描述
- en: This project’s intent is to perform data validation, cleaning, and standardization.
    This project will expand on the features of the `pydantic` package to do somewhat
    more sophisticated data validations and conversions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的目的是执行数据验证、清洗和标准化。本项目将扩展`pydantic`包的功能，以进行更复杂的数据验证和转换。
- en: This new data cleaning application can be designed around a data set like [https://tidesandcurrents.noaa.gov/tide_predictions.html](https://tidesandcurrents.noaa.gov/tide_predictions.html).
    The tide predictions around the US include dates, but the fields are decomposed,
    and our data cleaning application needs to combine them.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新的数据清洗应用程序可以设计成一个数据集，例如[https://tidesandcurrents.noaa.gov/tide_predictions.html](https://tidesandcurrents.noaa.gov/tide_predictions.html)。美国周围的潮汐预测包括日期，但字段被分解，我们的数据清洗应用程序需要将它们合并。
- en: 'For a specific example, see [https://tidesandcurrents.noaa.gov/noaatideannual.html?id=8725769](https://tidesandcurrents.noaa.gov/noaatideannual.html?id=8725769).
    Note that the downloaded `.txt` file is a tab-delimited CSV file with a very complicated
    multi-line header. This will require some sophisticated acquisition processing
    similar to the examples shown in [*Chapter** 3*](ch007.xhtml#x1-560003), [*Project
    1.1: Data* *Acquisition Base Application*](ch007.xhtml#x1-560003).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 具体示例请见[https://tidesandcurrents.noaa.gov/noaatideannual.html?id=8725769](https://tidesandcurrents.noaa.gov/noaatideannual.html?id=8725769)。注意，下载的`.txt`文件是一个带有非常复杂的多行标题的制表符分隔的CSV文件。这需要一些类似于在[*第3章*](ch007.xhtml#x1-560003)、[*项目1.1：数据获取基础应用程序*](ch007.xhtml#x1-560003)中展示的复杂获取处理。
- en: 'An alternative example is the CO2 PPM — Trends in Atmospheric Carbon Dioxide
    data set, available at [https://datahub.io/core/co2-ppm](https://datahub.io/core/co2-ppm).
    This has dates that are provided in two forms: as a `year-month-day` string and
    as a decimal number. We can better understand this data if we can reproduce the
    decimal number value.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是CO2 PPM——大气二氧化碳趋势数据集，可在[https://datahub.io/core/co2-ppm](https://datahub.io/core/co2-ppm)找到。这个数据集提供了两种日期形式：一种是`年-月-日`字符串，另一种是十进制数。如果我们能够重现十进制数值，我们就能更好地理解这些数据。
- en: 'The second example data set is [https://datahub.io/core/co2-ppm/r/0.html](https://datahub.io/core/co2-ppm/r/0.html)
    This is an HTML file, requiring some acquisition processing similar to the examples
    from [*Chapter** 4*](ch008.xhtml#x1-780004), [*Data Acquisition Features: Web
    APIs and* *Scraping*](ch008.xhtml#x1-780004).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个示例数据集是[https://datahub.io/core/co2-ppm/r/0.html](https://datahub.io/core/co2-ppm/r/0.html)。这是一个HTML文件，需要一些类似于[*第4章*](ch008.xhtml#x1-780004)中示例的获取处理。
- en: 'The use case for this cleaning application is identical to the description
    shown in [*Chapter** 9*](ch013.xhtml#x1-2080009), [*Project 3.1: Data Cleaning
    Base Application*](ch013.xhtml#x1-2080009). The acquired data — pure text, extracted
    from the source files — will be cleaned to create **Pydantic** models with fields
    of useful Python internal types.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个清理应用程序的使用案例与[*第9章*](ch013.xhtml#x1-2080009)，[*项目3.1：数据清理基础应用程序*](ch013.xhtml#x1-2080009)中显示的描述相同。获取的数据——纯文本，从源文件中提取——将被清理以创建具有有用Python内部字段的**Pydantic**模型。
- en: We’ll take a quick look at the tide table data on the [https://tidesandcurrents.noaa.gov](https://tidesandcurrents.noaa.gov)
    website.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将快速查看[https://tidesandcurrents.noaa.gov](https://tidesandcurrents.noaa.gov)网站上的潮汐表数据。
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The data to be acquired has two interesting structural problems:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取的数据有两个有趣的结构问题：
- en: There’s a 19-line preamble containing some useful metadata. Lines 2 to 18 have
    a format of a label and a value, for example, `State:`` FL`.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '有一个包含一些有用元数据的19行序言。第2到18行具有标签和值的格式，例如，`State: FL`。'
- en: The data is tab-delimited CSV data. There appear to be six column titles. However,
    looking at the tab characters, there are eight columns of header data followed
    by nine columns of data.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据是制表符分隔的CSV数据。看起来有六个列标题。然而，查看制表符，有八个标题列后面跟着九个数据列。
- en: 'The acquired data should fit the dataclass definition shown in the following
    fragment of a class definition:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 获取的数据应该符合以下类定义片段中的数据类定义：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The example omits the details of the `from_row()` method. If a CSV reader is
    used, this method needs to pick out columns from the CSV-format file, skipping
    over the generally empty columns. If regular expressions are used to parse the
    source lines, this method will use the groups from the match object.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 示例省略了`from_row()`方法的细节。如果使用CSV读取器，此方法需要从CSV格式文件中挑选列，跳过通常为空的列。如果使用正则表达式解析源行，此方法将使用匹配对象的组。
- en: Since this looks like many previous projects, we’ll look at the distinct technical
    approach next.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这看起来像许多以前的项目，我们接下来将探讨独特的技术方法。
- en: 10.1.2 Approach
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.2 方法
- en: 'The core processing of the data cleaning application should be — except for
    a few module changes — very similar to the earlier examples. For reference, see
    [*Chapter** 9*](ch013.xhtml#x1-2080009), [*Project 3.1: Data Cleaning Base Application*](ch013.xhtml#x1-2080009),
    specifically [*Approach*](ch013.xhtml#x1-2150002). This suggests that the `clean`
    module should have minimal changes from the earlier version.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清理应用程序的核心处理应该与早期示例非常相似——除了少数模块更改。为了参考，请参阅[*第9章*](ch013.xhtml#x1-2080009)，[*项目3.1：数据清理基础应用程序*](ch013.xhtml#x1-2080009)，特别是[*方法*](ch013.xhtml#x1-2150002)。这表明`clean`模块应该与早期版本有最小的更改。
- en: The principle differences should be two different implementations of the `acquire_model`
    and the `analysis_model`. For the tide data example, a class is shown in the [*Description*](#x1-2310001)
    section that can be used for the acquire model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 主要差异应该是对`acquire_model`和`analysis_model`的两种不同实现。对于潮汐数据示例，在[*描述*](#x1-2310001)部分展示了一个可用于获取模型的类。
- en: It’s important to maintain a clear distinction between the acquired data, which
    is often text, and the data that will be used for later analysis, which can be
    a mixture of more useful Python object types.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在获取的数据（通常是文本）和用于后续分析的数据（可以是更多有用的Python对象类型混合）之间保持清晰的区分是很重要的。
- en: The two-step conversion from source data to the interim acquired data format,
    and from the acquired data format to the clean data format can — sometimes — be
    optimized to a single conversion.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从源数据到中间获取数据格式的两步转换，以及从获取数据格式到清洁数据格式的转换，有时可以优化为单一转换。
- en: An optimization to combine processing into a single step can also make debugging
    more difficult.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 将处理合并为单一步骤的优化也可能使调试更加困难。
- en: 'We’ll show one approach to defining the enumerated set of values for the state
    of the tide. In the source data, codes of `’H’` and `’L’` are used. The following
    class will define this enumeration of values:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将展示一种定义潮汐状态值枚举集的方法。在源数据中，使用`’H’`和`’L’`代码。以下类将定义这个值的枚举：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We’ll rely on the enumerated type and two other annotated types to define a
    complete record. We’ll return to the annotated types after showing the record
    as a whole first. A complete tide prediction record looks as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将依靠枚举类型和另外两种注解类型来定义一个完整的记录。在展示整个记录之后，我们将回到注解类型。一个完整的潮汐预测记录看起来如下：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This shows how the source columns’ `date` and `time` are combined into a single
    text value prior to validation. This is done by the `from_acquire_dataclass()`
    method, so it happens before invoking the `TidePrediction` constructor.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了在验证之前，源列的`date`和`time`是如何合并成一个单一文本值的。这是通过`from_acquire_dataclass()`方法完成的，因此它发生在调用`TidePrediction`构造函数之前。
- en: 'The `TideCleanDateTime` and `TideCleanHighLow` type hints will leverage annotated
    types to define validation rules for each of these attributes. Here are the two
    definitions:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`TideCleanDateTime`和`TideCleanHighLow`类型提示将利用注解类型为这些属性中的每一个定义验证规则。以下是两个定义：'
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `TideCleanDateTime` type uses the `clean_date()` function to clean up the
    `date` string prior to any attempt at conversion. Similarly, the `TideCleanHighLow`
    type uses a lambda to transform the value to upper case before validation against
    the `HighLow` enumerated type.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`TideCleanDateTime`类型使用`clean_date()`函数在转换之前清理`date`字符串。同样，`TideCleanHighLow`类型使用lambda将值转换为大写字母，然后在验证`HighLow`枚举类型之前。'
- en: The `clean_date()` function works by applying the one (and only) expected date
    format to the string value. This is not designed to be flexible or permissive.
    It’s designed to confirm the data is an exact match against expectations.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`clean_date()`函数通过将一个（且仅一个）预期的日期格式应用于字符串值来工作。这不是为了灵活或宽容而设计的。它是为了确认数据与预期完全匹配。'
- en: 'The function looks like this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 函数看起来是这样的：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If the data doesn’t match the expected format, the `strptime()` function will
    raise a `ValueError` exception. This will be incorporated into a `pydantic.ValidationError`
    exception that enumerates all of the errors encountered. The `match` statement
    will pass non-string values through to the **pydantic** handler for validation;
    we don’t need to handle any other types.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据与预期格式不匹配，`strptime()`函数将引发`ValueError`异常。这将被纳入`pydantic.ValidationError`异常中，该异常列举了遇到的全部错误。`match`语句将非字符串值传递给**pydantic**处理程序进行验证；我们不需要处理任何其他类型。
- en: 'This model can also be used for analysis of clean data. (See [*Chapter** 13*](ch017.xhtml#x1-29700013),
    [*Project 4.1: Visual Analysis Techniques*](ch017.xhtml#x1-29700013).) In this
    case, the data will already be a valid `datetime.datetime` object, and no conversion
    will need to be performed. The use of a type hint of `str`` |`` datetime.datetime`
    emphasizes the two types of values this method will be applied to.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型也可以用于清洁数据的分析。（参见[*第13章**](ch017.xhtml#x1-29700013)，[*项目4.1：可视化分析技术*](ch017.xhtml#x1-29700013)。）在这种情况下，数据将已经是有效的`datetime.datetime`对象，不需要进行转换。使用类型提示`str`` |`` datetime.datetime`强调了此方法将应用于两种类型的值。
- en: This two-part ”combine and convert” operation is broken into two steps to fit
    into the **Pydantic** design pattern. The separation follows the principle of
    minimizing complex initialization processing and creating class definitions that
    are more declarative and less active.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个“合并和转换”的两步操作被分解为两步，以适应**Pydantic**设计模式。这种分离遵循最小化复杂初始化处理和创建更具声明性、更少主动性的类定义的原则。
- en: It’s often helpful to keep the conversion steps small and separate.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 保持转换步骤小而独立通常是有帮助的。
- en: Premature optimization to create a single, composite function is often a nightmare
    when changes are required.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在需要更改时，过早地优化以创建一个单一的复合函数通常是一个噩梦。
- en: For display purposes, the date, day-of-week, and time-of-day can be extracted
    from a single `datetime` instance. There’s no need to keep many date-related fields
    around as part of the `TidePrediction` object.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了显示目的，日期、星期几和时间可以从中提取单个`datetime`实例。没有必要将许多与日期相关的字段作为`TidePrediction`对象的一部分保留。
- en: The tide prediction is provided in two separate units of measure. For the purposes
    of this example, we retained the two separate values. Pragmatically, the height
    in feet is the height in cm multiplied by ![-1-- 30.48](img/file42.jpg).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 潮汐预测提供了两个不同的度量单位。在此示例中，我们保留了两个单独的值。从实用主义的角度来看，英尺的高度是厘米高度乘以![负1-- 30.48](img/file42.jpg)。
- en: For some applications, where the value for height in feet is rarely used, a
    property might make more sense than a computed value. For other applications,
    where the two heights are both used widely, having both values computed may improve
    performance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些应用，当英尺高度值很少使用时，属性可能比计算值更有意义。对于其他应用，当两种高度都广泛使用时，同时计算这两个值可能会提高性能。
- en: 10.1.3 Deliverables
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.3 可交付成果
- en: 'This project has the following deliverables:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此项目有以下可交付成果：
- en: Documentation in the `docs` folder.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docs`文件夹中的文档。'
- en: Acceptance tests in the `tests/features` and `tests/steps` folders.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`tests/features`和`tests/steps`文件夹中的验收测试。
- en: Unit tests for the application modules in the `tests` folder.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对`tests`文件夹中的应用模块进行单元测试。
- en: Application to clean some acquired data and apply simple conversions to a few
    fields. Later projects will add more complex validation rules.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用以清理一些获取的数据，并对几个字段进行简单的转换。后续的项目将添加更复杂的验证规则。
- en: 'Many of these deliverables are described in previous chapters. Specifically,
    [*Chapter** 9*](ch013.xhtml#x1-2080009), [*Project 3.1: Data Cleaning Base Application*](ch013.xhtml#x1-2080009)
    covers the basics of the deliverables for this project.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中许多可交付成果在之前的章节中已有描述。具体来说，[*第9章**](ch013.xhtml#x1-2080009)，[*项目 3.1：数据清洗基础应用*](ch013.xhtml#x1-2080009)涵盖了此项目可交付成果的基础。
- en: Unit tests for validation functions
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 验证函数的单元测试
- en: The unique validators used by a Pydantic class need test cases. For the example
    shown, the validator function is used to convert two strings into a date.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Pydantic类使用的独特验证器需要测试用例。对于示例中所示，验证器函数用于将两个字符串转换为日期。
- en: 'Boundary Value Analysis suggests there are three equivalence classes for date
    conversions:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 边界值分析表明，日期转换有三个等价类：
- en: Syntactically invalid dates. The punctuation or the number of digits is wrong.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语法无效的日期。标点符号或数字的位数是错误的。
- en: Syntactically valid, but calendrical invalid dates. The 30th of February, for
    example, is invalid, even when formatted properly.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语法有效，但日历无效的日期。例如，2月30日是无效的，即使格式正确。
- en: Valid dates.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有效的日期。
- en: The above list of classes leads to a minimum of three test cases.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 上述类列表导致至少有三个测试用例。
- en: 'Some developers like to explore each of the fields within a date, providing
    5 distinct values: the lower limit (usually 1), the upper limit (e.g., 12 or 31),
    just below the limit (e.g., 0), just above the upper limit (e.g., 13 or 32), and
    a value that’s in the range and otherwise valid. These additional test cases,
    however, are really testing the `strptime()` method of the `datetime` class. These
    cases are duplicate tests of the `datetime` module. These cases are *not* needed,
    since the `datetime` module already has plenty of test cases for calendrically
    invalid date strings.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一些开发者喜欢探索日期内的每个字段，提供5个不同的值：下限（通常是1），上限（例如，12或31），略低于上限（例如，0），略高于上限（例如，13或32），以及一个在范围内且有效的值。然而，这些额外的测试用例实际上是在测试`datetime`类的`strptime()`方法。这些案例是对`datetime`模块的重复测试。这些案例是不必要的，因为`datetime`模块已经有了足够多的针对日历无效日期字符串的测试用例。
- en: Don’t test the behavior of modules outside the application. Those modules have
    their own test cases.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 不要测试应用程序外模块的行为。这些模块有自己的测试用例。
- en: In the next section, we’ll look at a project to validate nominal data. This
    can be more complicated than validating ordinal or cardinal data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨一个验证名义数据的项目。这可能比验证序数或基数数据更复杂。
- en: '10.2 Project 3.3: Validate text fields (and numeric coded fields)'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 项目 3.3：验证文本字段（以及数值编码字段）
- en: For nominal data, we’ll use **pydantic**’s technique of applying a validator
    function to the value of a field. In cases where the field contains a code consisting
    only of digits, there can be some ambiguity as to whether or not the value is
    a cardinal number. Some software may treat any sequence of digits as a number,
    dropping leading zeroes. This can lead to a need to use a validator to recover
    a sensible value for fields that are strings of digits, but not cardinal values.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于名义数据，我们将使用**pydantic**将验证器函数应用于字段值的技巧。在字段包含仅由数字组成的代码的情况下，可能会有些模糊，即值是否为基数。一些软件可能将任何数字序列视为数字，忽略前导零。这可能导致需要使用验证器来恢复字段（字符串形式的数字，但不是基数）的合理值。
- en: 10.2.1 Description
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.1 描述
- en: This project’s intent is to perform data validation, cleaning, and standardization.
    This project will expand on the features of the **Pydantic** package to do somewhat
    more sophisticated data validation and conversion.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的目的是执行数据验证、清洗和标准化。本项目将扩展**Pydantic**包的功能，以进行更复杂的数据验证和转换。
- en: We’ll continue working with a data set like [https://tidesandcurrents.noaa.gov/tide_predictions.html](https://tidesandcurrents.noaa.gov/tide_predictions.html).
    The tide predictions around the US include dates, but the date is decomposed into
    three fields, and our data cleaning application needs to combine them.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用类似[https://tidesandcurrents.noaa.gov/tide_predictions.html](https://tidesandcurrents.noaa.gov/tide_predictions.html)的数据集。美国的潮汐预测包括日期，但日期被分解为三个字段，我们的数据清洗应用程序需要将它们合并。
- en: 'For a specific example, see [https://tidesandcurrents.noaa.gov/noaatideannual.html?id=8725769](https://tidesandcurrents.noaa.gov/noaatideannual.html?id=8725769).
    Note that the downloaded `.txt` file is really a tab-delimited CSV file with a
    complex header. This will require some sophisticated acquisition processing similar
    to the examples shown in [*Chapter** 3*](ch007.xhtml#x1-560003), [*Project 1.1:
    Data Acquisition Base* *Application*](ch007.xhtml#x1-560003).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个具体的例子，请参阅[https://tidesandcurrents.noaa.gov/noaatideannual.html?id=8725769](https://tidesandcurrents.noaa.gov/noaatideannual.html?id=8725769)。请注意，下载的`.txt`文件实际上是一个带有复杂标题的制表符分隔的CSV文件。这需要一些类似于[第3章](ch007.xhtml#x1-560003)中所示示例的复杂获取处理。
- en: For data with a relatively small domain of unique values, a Python `enum` class
    is a very handy way to define the allowed collection of values. Using an enumeration
    permits simple, strict validation by `pydantic`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有相对较小唯一值域的数据，Python的`enum`类是一个非常方便的方式来定义允许的值集合。使用枚举允许`pydantic`进行简单、严格的验证。
- en: Some data — like account numbers, as one example — have a large domain of values
    that may be in a state of flux. Using an `enum` class would mean transforming
    the valid set of account numbers into an enumerated type before attempting to
    work with any data. This may not be particularly helpful, since there’s rarely
    a compelling need to confirm that an account number is valid; this is often a
    stipulation that is made about the data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据——例如账户号码，仅举一例——具有可能处于变化状态的大量值域。使用`enum`类意味着在尝试处理任何数据之前，将有效的账户号码集合转换为枚举类型。这可能并不特别有用，因为很少有必要确认账户号码的有效性；这通常是关于数据的一个规定。
- en: For fields like account numbers, there can be a need to validate potential values
    without an enumeration of all allowed values. This means the application must
    rely on patterns of the text to determine if the value is valid, or if the value
    needs to be cleaned to make it valid. For example, there may be a required number
    of digits, or check digits embedded within the code. In the case of credit card
    numbers, the last digit of a credit card number is used as part of confirmation
    that the overall number is valid. For more information, see [https://www.creditcardvalidator.org/articles/luhn-algorithm](https://www.creditcardvalidator.org/articles/luhn-algorithm).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像账户号码这样的字段，可能需要验证潜在值，而不必列举所有允许的值。这意味着应用程序必须依赖于文本的模式来确定值是否有效，或者值是否需要清理才能使其有效。例如，可能需要一定数量的数字，或者代码中嵌入的校验位。在信用卡号码的情况下，信用卡号码的最后一位数字用作确认整体号码有效的一部分。更多信息，请参阅[https://www.creditcardvalidator.org/articles/luhn-algorithm](https://www.creditcardvalidator.org/articles/luhn-algorithm)。
- en: After considering some of the additional validations that need to be performed,
    we’ll take a look at a design approach for adding more complicated validations
    to the cleaning application.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑了一些需要执行的其他验证之后，我们将探讨为清洗应用程序添加更复杂验证的设计方法。
- en: 10.2.2 Approach
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.2 方法
- en: 'For reference to the general approach to this application, see [*Chapter** 9*](ch013.xhtml#x1-2080009),
    [*Project* *3.1: Data Cleaning Base Application*](ch013.xhtml#x1-2080009), specifically
    [*Approach*](ch013.xhtml#x1-2150002).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对该应用的一般方法的参考，请参阅[*第9章*](ch013.xhtml#x1-2080009)，[*项目* *3.1：数据清洗基础应用*](ch013.xhtml#x1-2080009)，特别是[*方法*](ch013.xhtml#x1-2150002)。
- en: 'The model can be defined using the **pydantic** package. This package offers
    two paths to validating string values against a domain of valid values. These
    alternatives are:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可以使用**pydantic**包定义。此包提供了两种验证字符串值与有效值域的方法。这些替代方案是：
- en: Define an enumeration with all valid values.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义包含所有有效值的枚举。
- en: Define a regular expression for the string field. This has the advantage of
    defining very large domains of valid values, including *potentially* infinite
    domains of values.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为字符串字段定义一个正则表达式。这具有定义非常大的有效值域的优势，包括*可能*无限的值域。
- en: 'Enumeration is an elegant solution that defines the list of values as a class.
    As shown earlier, it might look like this:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 枚举是一个优雅的解决方案，它将值列表定义为类。如前所述，它可能看起来像这样：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This will define a domain of two string values, ”L” and ”H”. This map provides
    easier-to-understand names, `Low` and `High`. This class can be used by **pydantic**
    to validate a string value.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这将定义一个包含两个字符串值“L”和“H”的域。此映射提供了更容易理解的名称，`Low`和`High`。此类可以被**pydantic**用于验证字符串值。
- en: An example of a case when we need to apply a `BeforeValidator` annotated type
    might be some tide data with lower-case ”h” and ”l” instead of proper upper-case
    ”H” or ”L”. This allows the validator to clean the data **prior** to the built-in
    data conversion.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们需要应用一个带有`BeforeValidator`注解的类型时，一个例子可能是某些带有小写“h”和“l”而不是正确的大写“H”或“L”的海潮数据。这允许验证器在内置数据转换之前清理数据**之前**。
- en: 'We might use an annotated type. It looked like this in the preceding example:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能使用一个注解类型。在先前的示例中看起来是这样的：
- en: '[PRE7]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The annotated type hint describes the base type, `HighLow`, and a validation
    rule to be applied before the **pydantic** conversion. In this case, it’s a lambda
    to convert the text to upper case. We’ve emphasized the validation of enumerated
    values using an explicit enumeration because it is an important technique for
    establishing the complete set of allowed codes for a given attribute. The enumerated
    type’s class definition is often a handy place to record notes and other information
    about the coded values.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注解类型提示描述了基类型`HighLow`以及在进行**pydantic**转换之前要应用的验证规则。在这种情况下，它是一个将文本转换为上标的lambda函数。我们强调了使用显式枚举进行枚举值的验证，因为这对于建立给定属性允许的代码的完整集合是一个重要的技术。枚举类型的类定义通常是记录关于编码值的笔记和其他信息的便捷位置。
- en: Now that we’ve looked at the various aspects of the approach, we can turn our
    attention to the deliverables for this project.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了该方法的各种方面，我们可以将注意力转向本项目的可交付成果。
- en: 10.2.3 Deliverables
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.3 可交付成果
- en: 'This project has the following deliverables:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目有以下可交付成果：
- en: Documentation in the `docs` folder.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docs`文件夹中的文档。'
- en: Acceptance tests in the `tests/features` and `tests/steps` folders.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tests/features`和`tests/steps`文件夹中的验收测试。'
- en: Unit tests for the application modules in the `tests` folder.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tests`文件夹中的应用模块的单元测试。'
- en: Application to clean source data in a number of fields.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用到多个领域的源数据清洗。
- en: 'Many of these deliverables are described in previous chapters. Specifically,
    [*Chapter** 9*](ch013.xhtml#x1-2080009), [*Project 3.1: Data Cleaning Base Application*](ch013.xhtml#x1-2080009)
    covers the basics of the deliverables for this project.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 其中许多可交付成果在之前的章节中已有描述。具体来说，[*第9章*](ch013.xhtml#x1-2080009)，[*项目3.1：数据清洗基础应用*](ch013.xhtml#x1-2080009)涵盖了本项目可交付成果的基础内容。
- en: Unit tests for validation functions
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 验证函数的单元测试
- en: 'The unique validators used by a pydantic class need test cases. For the example
    shown, the validator function is used to validate the state of the tide. This
    is a small domain of enumerated values. There are three core kinds of test cases:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: pydantic类使用的唯一验证器需要测试用例。对于前面显示的示例，验证器函数用于验证潮汐的状态。这是一个小的枚举值域。有三个核心类型的测试用例：
- en: Valid codes like `’H’` or `’L’`.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有效的代码，如`’H’`或`’L’`。
- en: Codes that can be reliably cleaned. For example, lower-case codes `’h’` and
    `’l’` are unambiguous. A data inspection notebook may reveal non-code values like
    `’High’` or `’Low’`, also. These can be reliably cleaned.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可靠清理的代码。例如，小写代码`’h’`和`’l’`是不含糊的。数据检查笔记本也可能揭示出非代码值，如`’High’`或`’Low’`，这些也可以可靠地清理。
- en: Invalid codes like `’’`, or `’9’`.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无效的代码如 `’’` 或 `’9’`。
- en: The domain of values that can be cleaned properly is something that is subject
    to a great deal of change. It’s common to find problems and use an inspection
    notebook to uncover a new encoding when upstream applications change. This will
    lead to additional test cases, and then additional validation processing to make
    the test cases pass.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 能够被正确清理的值域是会经历很大变化的。当上游应用发生变化时，发现问题和使用检查笔记来揭示新的编码是很常见的。这将导致额外的测试用例，然后是额外的验证处理以确保测试用例通过。
- en: In the next project, we’ll look at the situation where data must be validated
    against an externally defined set of values.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个项目中，我们将探讨数据必须与外部定义的值集进行验证的情况。
- en: '10.3 Project 3.4: Validate references among separate data sources'
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 项目 3.4：验证不同数据源之间的引用
- en: 'In [*Chapter** 9*](ch013.xhtml#x1-2080009), [*Project 3.1: Data Cleaning Base
    Application*](ch013.xhtml#x1-2080009) we relied on the foundational behavior of
    Pydantic to convert fields from source text to Python types. This next project
    will look at a more complicated validation rule.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 9 章*](ch013.xhtml#x1-2080009)，[*项目 3.1：数据清理基础应用*](ch013.xhtml#x1-2080009)
    中，我们依赖于 Pydantic 的基本行为将源文本字段转换为 Python 类型。下一个项目将探讨更复杂的验证规则。
- en: 10.3.1 Description
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.1 描述
- en: This project’s intent is to perform data validation, cleaning, and standardization.
    This project will expand on the features of the `pydantic` package to do somewhat
    more sophisticated data validations and conversions.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的目的是执行数据验证、清理和标准化。本项目将扩展 `pydantic` 包的功能，以进行更复杂的数据验证和转换。
- en: Data sets in [https://data.census.gov](https://data.census.gov) have Z**IP Code
    Tabulation Areas** (**ZCTAs**). For certain regions, these US postal codes can
    (and should) have leading zeroes. In some variations on this data, however, the
    ZIP codes get treated as numbers and the leading zeroes get lost.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://data.census.gov](https://data.census.gov) 上的数据集包含 Z**IP Code Tabulation
    Areas**（**ZCTAs**）。对于某些地区，这些美国邮政编码可以（并且应该）有前导零。然而，在某些数据变体中，ZIP 码被当作数字处理，前导零丢失了。'
- en: Data sets at [https://data.census.gov](https://data.census.gov) have information
    about the city of Boston, Massachusets, which has numerous US postal codes with
    leading zeroes. The Food Establishment Inspections available at [https://data.boston.gov/group/permitting](https://data.boston.gov/group/permitting)
    provides insight into Boston-area restaurants. In addition to postal codes (which
    are nominal data), this data involves numerous fields that contain nominal data
    as well as ordinal data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://data.census.gov](https://data.census.gov) 上的数据集包含关于马萨诸塞州波士顿市的信息，该市有多个带前导零的美国邮政编码。在
    [https://data.boston.gov/group/permitting](https://data.boston.gov/group/permitting)
    可用的食品经营场所检查提供了波士顿地区餐馆的见解。除了邮政编码（这是名义数据）之外，这些数据还涉及许多包含名义数据和有序数据的字段。'
- en: For data with a relatively small domain of unique values, a Python `enum` class
    is a very handy way to define the allowed collection of values. Using an enumeration
    permits simple, strict validation by **Pydantic**.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有相对较小唯一值域的数据，Python 的 `enum` 类是一个非常方便的方式来定义允许的值集合。使用枚举允许 Pydantic 进行简单、严格的验证。
- en: Some data — like account numbers, as one example — have a large domain of values
    that may be in a state of flux. Using an `enum` class would mean transforming
    the valid set of account numbers into an enum before attempting to work with any
    data. This may not be particularly helpful, since there’s rarely a compelling
    need to confirm that an account number is valid; this is often a simple stipulation
    that is made about the data.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据——例如账户号码——具有可能处于变化状态的大值域。使用 `enum` 类意味着在尝试处理任何数据之前将有效的账户号码集合转换为枚举。这可能并不特别有用，因为很少有必要确认账户号码的有效性；这通常是对数据的一个简单规定。
- en: This leads to a need to validate potential values without an enumeration of
    the allowed values. This means the application must rely on patterns of the text
    to determine if the value is valid, or if the value needs to be cleaned to make
    it valid.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致需要验证潜在值，而不需要列出允许的值。这意味着应用程序必须依赖于文本的模式来确定值是否有效，或者值是否需要被清理以使其有效。
- en: 'When an application cleans postal code data, there are two distinct parts to
    the cleaning:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用程序清理邮政编码数据时，清理有两个不同的部分：
- en: Clean the postal code to have the proper format. For US ZIP codes, it’s generally
    5 digits. Some codes are 5 digits, a hyphen, and 4 more digits.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清理邮政编码，使其具有正确的格式。对于美国ZIP代码，通常是5位数字。有些代码是5位数字，一个连字符，然后是4位数字。
- en: Compare the code with some master list to be sure it’s a meaningful code that
    references an actual post office or location.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将代码与一些主列表进行比较，以确保它是一个有意义的代码，引用了实际的邮局或位置。
- en: It’s important to keep these separate since the first step is covered by the
    previous project, and doesn’t involve anything terribly complicated. The second
    step involves some additional processing to compare a given record against a master
    list of allowed values.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是将这些内容分开，因为第一步已经被上一个项目涵盖，并且不涉及任何特别复杂的事情。第二步涉及到一些额外的处理，用于将给定的记录与允许值的总列表进行比较。
- en: 10.3.2 Approach
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.2 方法
- en: 'For reference to the general approach to this application, see [*Chapter** 9*](ch013.xhtml#x1-2080009),
    [*Project* *3.1: Data Cleaning Base Application*](ch013.xhtml#x1-2080009), specifically
    [*Approach*](ch013.xhtml#x1-2150002).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此应用的通用方法，请参阅[*第9章*](ch013.xhtml#x1-2080009)、[*项目3.1：数据清洗基础应用*](ch013.xhtml#x1-2080009)，特别是[*方法*](ch013.xhtml#x1-2150002)。
- en: When we have nominal values that must refer to external data, we can call these
    “foreign keys.” They’re references to an external collection of entities for which
    the values are primary keys. An example of this is a postal code. There is a defined
    list of valid postal codes; the code is a primary key in this collection. In our
    sample data, the postal code is a foreign key reference to the defining collection
    of postal codes.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们必须引用外部数据的名义值时，我们可以称这些为“外键”。它们是对一个外部实体集合的引用，其中这些值是主键。例如，邮政编码就是一个这样的例子。存在一个有效的邮政编码列表；在这个集合中，编码是主键。在我们的样本数据中，邮政编码是对定义集合的外键引用。
- en: Other examples include country codes, US state codes, and US phone system area
    codes. We can write a regular expression to describe the potential domain of key
    values. For US state codes, for example, we can use the regular expression `r’\w\w’`
    to describe state codes as having two letters. We could narrow this domain slightly
    using `r’[A-Z]{2}’` to require the state code use upper-case letters only. There
    are only 50 state codes, plus a few territories and districts; limiting this further
    would make for a very long regular expression.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 其他例子包括国家代码、美国州代码和美国电话系统区域代码。我们可以编写一个正则表达式来描述键值的潜在域。例如，对于美国州代码，我们可以使用正则表达式`r’\w\w’`来描述州代码由两个字母组成。我们可以通过使用`r’[A-Z]{2}’`来稍微缩小这个域，以要求州代码只使用大写字母。只有50个州代码，加上一些领土和地区；进一步限制会使正则表达式变得非常长。
- en: The confounding factor here is when the primary keys need to be loaded from
    an external source — for example, a database. In this case, the simple `@validator`
    method has a dependency on external data. Further, this data must be loaded prior
    to any data cleaning activities.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题在于当主键需要从外部源加载时——例如，数据库。在这种情况下，简单的`@validator`方法依赖于外部数据。此外，这些数据必须在任何数据清理活动之前加载。
- en: 'We have two choices for gathering the set of valid key values:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两种方法来收集有效键值集：
- en: Create an `Enum` class with a list of valid values.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个包含有效值的`Enum`类。
- en: Define a `@classmethod` to initialize the pydantic class with valid values.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个`@classmethod`来初始化pydantic类中的有效值。
- en: 'For example, [https://data.opendatasoft.com](https://data.opendatasoft.com)
    has a useful list of US zip codes. See the URL [https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/georef-united-states-of-america-zc-point@public/exports/csv](https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/georef-united-states-of-america-zc-point@public/exports/csv)
    for US Zip Codes Points, United States of America. This is a file that can be
    downloaded and transformed into an enum or used to initialize a class. The `Enum`
    class creation is a matter of creating a list of two tuples with the label and
    the value for the enumeration. The `Enum` definition can be built with code like
    the following example:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[https://data.opendatasoft.com](https://data.opendatasoft.com) 有一个有用的美国邮政编码列表。查看美国邮政编码点的URL
    [https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/georef-united-states-of-america-zc-point@public/exports/csv](https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/georef-united-states-of-america-zc-point@public/exports/csv)，这是可以下载并转换为枚举或用于初始化类的文件。创建`Enum`类的操作只是创建一个包含枚举标签和值的元组列表。`Enum`定义可以通过以下示例代码构建：
- en: '[PRE8]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This will create an enumerated class, `ZipCode`, from the approximately 33,000
    ZIP codes in the downloaded source file. The enumerated labels will be Python
    attribute names similar to `ZIP_75846`. The values for these labels will be the
    US postal codes, for example, `’75846’`. The `":0>5s"` string format will force
    in leading zeroes where needed.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: The `zip_code_values()` function saves us from writing 30,000 lines of code
    to define the enumeration class, `ZipCode`. Instead, this function reads 30,000
    values, creating a list of pairs used to create an `Enum` subclass.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: The odd encoding of `utf_8_sig` is necessary because the source file has a leading
    **byte-order mark** (**BOM**). This is unusual butpermitted by Unicode standards.
    Other data sources for ZIP codes may not include this odd artifact. The encoding
    gracefully ignores the BOM bytes.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: The unusual encoding of `utf_8_sig` is a special case because this file happens
    to be in an odd format.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: There are a large number of encodings for text. While UTF-8 is popular, it is
    not universal.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: When unusual characters appear, it’s important to find the source of the data
    and ask what encoding they used.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: In general, it’s impossible to uncover the encoding given a sample file. There
    are a large number of valid byte code mappings that overlap between ASCII, CP1252,
    and UTF-8.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: This design requires the associated data file. One potential improvement is
    to create a Python module from the source data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the **Pydantic** functional validators uses a similar algorithm to the
    one shown above. The validation initialization is used to build an object that
    retains a set of valid values. We’ll start with the goal of a small model using
    annotated types. The model looks like this:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The model relies on the `ValidZip` type. This type has two validation rules:
    before any conversion, a `zip_format_valid()` function is applied, and after conversion,
    a `zip_lookup_valid()` function is used.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: We’ve only defined a single field, `zip`, in this **Pydantic** class. This will
    let us focus on the validation-by-lookup design. A more robust example, perhaps
    based on the Boston health inspections shown above, would have a number of additional
    fields reflecting the source data to be analyzed.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'The before validator function, `zip_format_valid()`, compares the ZIP code
    to a regular expression to ensure that it is valid:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `zip_format_valid()` can be expanded to use an f-string like `f"{zip:0>5s}`
    to reformat a ZIP code that’s missing the leading zeroes. We’ll leave this for
    you to integrate into this function.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: The after validator function is a callable object. It’s an instance of a class
    that defines the `__call__()` method.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the core class definition, and the creation of the instance:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This will define the `zip_lookup_valid` callable object. Initially, there’s
    new value for the internal `self.zip_set` attribute. This must be built using
    a function that evaluates `zip_lookup_valid.load(source)`. This will populate
    the set of valid values.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这将定义 `zip_lookup_valid` 可调用对象。最初，内部 `self.zip_set` 属性没有新值。这必须使用一个评估 `zip_lookup_valid.load(source)`
    的函数来构建。这将填充有效值的集合。
- en: 'We’ve called this function `prepare_validator()` and it looks like this:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个函数命名为 `prepare_validator()`，其代码如下：
- en: '[PRE12]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This idea of a complex validation follows the SOLID design principle. It separates
    the essential work of the `SomethingWithZip` class from the `ValidZip` type definition.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这种复杂验证的想法遵循SOLID设计原则。它将 `SomethingWithZip` 类的基本工作与 `ValidZip` 类型定义分开。
- en: Further, the `ValidZip` type depends on a separate class, `ZipLookupValidator`,
    which handles the complications of loading data. This separation makes it somewhat
    easier to change validation files, or change the format of the data used for validation
    without breaking the `SomethingWithZip` class and the applications that use it.
    Further, it provides a reusable type, `ValidZip`. This can be used for multiple
    fields of a model, or multiple models.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`ValidZip` 类型依赖于一个单独的类，`ZipLookupValidator`，它处理加载数据的复杂性。这种分离使得更改验证文件或更改用于验证的数据格式变得相对容易，而不会破坏
    `SomethingWithZip` 类及其使用它的应用程序。此外，它提供了一个可重用的类型，`ValidZip`。这可以用于模型的多字段或多个模型。
- en: Having looked at the technical approach, we’ll shift to looking at the deliverables
    for this project.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了技术方法之后，我们将转向查看本项目的可交付成果。
- en: 10.3.3 Deliverables
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.3 可交付成果
- en: 'This project has the following deliverables:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目有以下可交付成果：
- en: Documentation in the `docs` folder.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docs` 文件夹中的文档。'
- en: Acceptance tests in the `tests/features` and `tests/steps` folders.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `tests/features` 和 `tests/steps` 文件夹中的验收测试。
- en: Unit tests for the application modules in the `tests` folder.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `tests` 文件夹中的应用模块单元测试。
- en: Application to clean and validate data against external sources.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于对外部数据进行清洗和验证的应用程序。
- en: 'Many of these deliverables are described in previous chapters. Specifically,
    [*Chapter** 9*](ch013.xhtml#x1-2080009), [*Project 3.1: Data Cleaning Base Application*](ch013.xhtml#x1-2080009)
    covers the basics of the deliverables for this project.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 许多这些可交付成果在之前的章节中已有描述。具体来说，[*第9章*](ch013.xhtml#x1-2080009)，[*项目3.1：数据清洗基础应用*](ch013.xhtml#x1-2080009)涵盖了本项目可交付成果的基础。
- en: Unit tests for data gathering and validation
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据收集和验证的单元测试。
- en: 'The unique validators used by a **Pydantic** class need test cases. For the
    example shown, the validator function is used to validate US ZIP codes. There
    are three core kinds of test cases:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pydantic** 类使用的独特验证器需要测试用例。对于所展示的示例，验证器函数用于验证美国ZIP代码。有三种核心类型的测试用例：'
- en: Valid ZIP codes with five digits that are found in the ZIP code database.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在ZIP代码数据库中找到的五位数有效ZIP代码。
- en: Syntactically valid ZIP codes with five digits that are **not** found in the
    ZIP code database.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语法上有效的五位数ZIP代码，这些代码在ZIP代码数据库中**没有**找到。
- en: Syntactically invalid ZIP that don’t have five digits, or can’t — with the addition
    of leading zeroes — be made into valid codes.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语法上无效的ZIP代码，它们没有五个数字，或者无法——通过添加前导零——变成有效代码。
- en: '10.4 Project 3.5: Standardize data to common codes and ranges'
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.4 项目3.5：将数据标准化为常用代码和范围
- en: Another aspect of cleaning data is the transformation of raw data values into
    standardized values. For example, codes in use have evolved over time, and older
    data codes should be standardized to match new data codes. The notion of standardizing
    values can be a sensitive topic if critical information is treated as an outlier
    and rejected or improperly standardized.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗的另一个方面是将原始数据值转换为标准化值。例如，使用的代码随着时间的推移而演变，旧的数据代码应该标准化以匹配新的数据代码。如果关键信息被视为异常值并被拒绝或错误地标准化，那么标准化值的观念可能是一个敏感话题。
- en: We can also consider imputing new values to fill in for missing values as a
    kind of standardization technique. This can be a necessary step when dealing with
    missing data or data that’s likely to represent some measurement error, not the
    underlying phenomenon being analyzed.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以考虑使用新的值来填补缺失值，作为一种标准化技术。当处理缺失数据或可能代表某些测量误差而非分析的基本现象的数据时，这可能是一个必要的步骤。
- en: This kind of transformation often requires careful, thoughtful justification.
    We’ll show some programming examples. The deeper questions of handling missing
    data, imputing values, handling outliers, and other standardization operations
    are outside the scope of this book.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这种转换通常需要仔细、深思熟虑的论证。我们将展示一些编程示例。处理缺失数据、插补值、处理异常值和其他标准化操作等更深层次的问题超出了本书的范围。
- en: See [https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779)
    for an overview of some ways to deal with missing or invalid data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779)以了解处理缺失或无效数据的一些方法概述。
- en: 10.4.1 Description
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.1 描述
- en: Creating standardized values is at the edge of data cleaning and validation.
    These values can be described as ”derived” values, computed from existing values.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 创建标准化值处于数据清理和验证的边缘。这些值可以描述为“派生”值，从现有值计算得出。
- en: 'There are numerous kinds of standardizations; we’ll look at two:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化种类繁多；我们将探讨两种：
- en: Compute a standardized value, or Z-score, for cardinal data. For a normal distribution,
    the Z-scores have a mean of 0, and a standard deviation of 1\. It permits comparing
    values measured on different scales.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为基数数据计算一个标准化值，或Z分数。对于正态分布，Z分数的平均值为0，标准差为1。它允许比较不同尺度上测量的值。
- en: Collapse nominal values into a single standardized value. For example, replacing
    a number of historical product codes with a single, current product code.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将名义值合并为单个标准化值。例如，用单个当前产品代码替换多个历史产品代码。
- en: The first of these, computing a Z-score, rarely raises questions about the statistical
    validity of the standardized value. The computation, *Z* = ![x−σμ-](img/file45.jpg),
    is well understood and has known statistical properties.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法中的第一种，计算Z分数，很少会引起关于标准化值统计有效性的疑问。计算公式，*Z* = ![x−σμ-](img/file45.jpg)，是众所周知的，并且具有已知的统计特性。
- en: The second standardization, replacing nominal values with a standardized code,
    can be troubling. This kind of substitution may simply correct errors in the historical
    record. It may also obscure an important relationship. It’s not unusual for a
    data inspection notebook to reveal outliers or erroneous values in a data set
    that needs to be standardized.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种标准化，用标准化代码替换名义值，可能会引起麻烦。这种替换可能只是纠正了历史记录中的错误。它也可能掩盖了一个重要的关系。在需要标准化的数据集中，数据检查笔记本揭示异常值或错误值并不罕见。
- en: Enterprise software may have unrepaired bugs. Some business records can have
    unusual code values that map to other code values.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 企业软件可能存在未修复的bug。一些业务记录可能具有不寻常的代码值，这些值映射到其他代码值。
- en: Of course, the codes in use may shift over time.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，使用的代码可能会随时间而变化。
- en: 'Some records may have values that reflect two eras: pre-repair and post-repair.
    Worse, of course, there may have been several attempts at a repair, leading to
    more nuanced timelines.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 一些记录可能包含反映两个时代的值：修复前和修复后。更糟糕的是，当然，可能已经尝试了多次修复，导致更复杂的时序。
- en: 'For this project, we need some relatively simple data. The Ancombe’s Quartet
    data will do nicely as examples from which derived Z-scores can be computed. For
    more information, see [*Chapter** 3*](ch007.xhtml#x1-560003), [*Project 1.1: Data
    Acquisition Base* *Application*](ch007.xhtml#x1-560003).'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我们需要一些相对简单的数据。Ancombe的四重奏数据将很好地作为示例，从中可以计算派生的Z分数。更多信息，请参阅[*第3章*](ch007.xhtml#x1-560003)，[*项目1.1：数据采集基础*
    *应用*](ch007.xhtml#x1-560003)。
- en: The objective is to compute standardized values for the two values that comprise
    the samples in the Anscombe’s Quartet series. When the data has a normal distribution,
    these derived, standardized Z-scores will have a mean of zero and a standard deviation
    of one. When the data does not have a normal distribution, these values will diverge
    from the expected values.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是计算Anscombe的四重奏系列样本中包含的两个值的标准化值。当数据呈正态分布时，这些派生的标准化Z分数将具有零平均值和标准差为1。当数据不是正态分布时，这些值将偏离预期值。
- en: 10.4.2 Approach
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.2 方法
- en: 'For reference to the general approach to this application, see [*Chapter** 9*](ch013.xhtml#x1-2080009),
    [*Project* *3.1: Data Cleaning Base Application*](ch013.xhtml#x1-2080009), specifically
    [*Approach*](ch013.xhtml#x1-2150002).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于参考此应用的通用方法，请参阅[*第9章**](ch013.xhtml#x1-2080009)，[*项目* *3.1：数据清理基础应用*](ch013.xhtml#x1-2080009)，特别是[*方法*](ch013.xhtml#x1-2150002)。
- en: 'To replace values with preferred standardized values, we’ve seen how to clean
    bad data in previous projects. See, for example, [*Project 3.3: Validate text
    fields (and* *numeric coded fields)*](#x1-2350002).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了用首选的标准化值替换值，我们已经在以前的项目中看到了如何清理不良数据。例如，参见[*项目3.3：验证文本字段（以及* *数值编码字段)*](#x1-2350002)。
- en: For Z-score standardization, we’ll be computing a derived value. This requires
    knowing the mean, *μ*, and standard deviation, *σ*, for a variable from which
    the Z-score can be computed.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Z分数标准化，我们将计算一个导出值。这需要知道一个变量的平均值，*μ*，和标准差，*σ*，以便可以计算Z分数。
- en: 'This computation of a derived value suggests there are the following two variations
    on the analytical data model class definitions:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这种导出值的计算表明，在分析数据模型类定义上有以下两种变体：
- en: An “initial” version, which lacks the Z-score values. These objects are incomplete
    and require further computation.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个“初始”版本，缺乏Z分数值。这些对象是不完整的，需要进一步计算。
- en: A “final” version, where the Z-score values have been computed. These objects
    are complete.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个“最终”版本，其中已经计算了Z分数值。这些对象是完整的。
- en: 'There are two common approaches to handling this distinction between incomplete
    and complete objects:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 处理不完整和完整对象之间区别的两种常见方法：
- en: The two classes are distinct. The complete version is a subclass of the incomplete
    version, with additional fields defined.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这两个类是不同的。完整版本是不完整版本的子类，定义了额外的字段。
- en: The derived values are marked as optional. The incomplete version starts with
    `None` values.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导出的值被标记为可选。不完整的版本以`None`值开始。
- en: The first design is a more conventional object-oriented approach. The formality
    of a distinct type to clearly mark the state of the data is a significant advantage.
    The extra class definition, however, can be seen as clutter, since the incomplete
    version is transient data that doesn’t create enduring value. The incomplete records
    live long enough to compute the complete version, and the file can then be deleted.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种设计是一种更传统的面向对象的方法。使用一个明确标记数据状态的独立类型的形式化是一个显著的优势。然而，额外的类定义可能被视为杂乱，因为不完整的版本是暂时性的数据，不会创造持久的价值。不完整的记录足以计算完整的版本，然后可以删除该文件。
- en: The second design is sometimes used for functional programming. It saves the
    subclass definition, which can be seen as a slight simplification.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种设计有时用于函数式编程。它节省了子类定义，这可以看作是一种轻微的简化。
- en: '[PRE13]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: These two class definitions show one way to formalize the distinction between
    the initially cleaned, validated, and converted data, and the complete sample
    with the standardized Z-scores present for both of the variables.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个类定义展示了如何将最初清理、验证和转换的数据与包含标准化Z分数的完整样本之间的区别形式化。
- en: 'This can be handled as three separate operations:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以被视为三个单独的操作：
- en: Clean and convert the initial data, writing a temporary file of the `InitialSample`
    instances.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清理和转换初始数据，写入包含`InitialSample`实例的临时文件。
- en: Read the temporary file, computing the means and standard deviations of the
    variables.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取临时文件，计算变量的平均值和标准差。
- en: Read the temporary file again, building the final samples from the `InitialSample`
    instances and the computed intermediate values.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次读取临时文件，从`InitialSample`实例和计算出的中间值构建最终样本。
- en: 'A sensible optimization is to combine the first two steps: clean and convert
    the data, accumulating values that can be used to compute the mean and standard
    deviation. This is helpful because the `statistics` module expects a sequence
    of objects that might not fit in memory. The mean, which involves a sum and a
    count, is relatively simple. The standard deviation requires accumulating a sum
    and a sum of squares.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 合理的优化是将前两个步骤结合起来：清理和转换数据，累积可用于计算平均值和标准差的价值。这样做是有帮助的，因为`statistics`模块期望的是一个可能不适合内存的对象序列。平均值涉及求和和计数，相对简单。标准差需要累积总和以及平方和。
- en: '![ ∑ mx = ---x n ](img/file46.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑ mx = ---x n ](img/file46.jpg)'
- en: The mean of *x*, *m*[x], is the sum of the *x* values divided by the count of
    *x* values, shown as *n*.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '*x*的均值，*m*[x]，是*x*值的总和除以*x*值的计数，表示为*n*。'
- en: '![ ∘ ∑-------(∑-x)2- ---x2 −---n-- sx = n − 1 ](img/file47.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![ ∘ ∑-------(∑-x)² ---x² ---n-- sx = n − 1 ](img/file47.jpg)'
- en: The standard deviation of *x*, *s*[x], uses the sum of *x*², the sum of *x*,
    and the number of values, *n*.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '*x* 的标准差 *s*[x] 使用 *x*² 的和、*x* 的和以及数值的数量，*n*。'
- en: This formula for the standard deviation has some numeric stability issues, and
    there are variations that are better designs. See [https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这个标准差的公式存在一些数值稳定性问题，并且有更好的设计变体。参见 [https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance)。
- en: We’ll define a class that accumulates the values for mean and variance. From
    this, we can compute the standard deviation.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个类来累积均值和方差的值。从这个类中，我们可以计算标准差。
- en: '[PRE14]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This `variance` class performs an incremental computation of mean, standard
    deviation, and variance. Each individual value is presented by the `add()` method.
    After all of the data has been presented, the properties can be used to return
    the summary statistics.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 `variance` 类执行均值、标准差和方差的增量计算。每个单独的值通过 `add()` 方法表示。在所有数据都表示之后，可以使用属性来返回汇总统计信息。
- en: 'It’s used as shown in the following snippet:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下代码片段所示：
- en: '[PRE15]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This provides a way to compute the summary statistics without using a lot of
    memory. It permits the optimization of computing the statistics the first time
    the data is seen. And, it reflects a well-designed algorithm that is numerically
    stable.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了一种在不使用大量内存的情况下计算汇总统计的方法。它允许在第一次看到数据时优化计算统计信息。此外，它反映了一个设计良好的算法，该算法在数值上是稳定的。
- en: Now that we’ve explored the technical approach, it’s time to look at the deliverables
    that must be created for this project.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了技术方法，是时候看看本项目必须创建的可交付成果了。
- en: 10.4.3 Deliverables
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.3 可交付成果
- en: 'This project has the following deliverables:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目有以下可交付成果：
- en: Documentation in the `docs` folder.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docs` 文件夹中的文档。'
- en: Acceptance tests in the `tests/features` and `tests/steps` folders.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tests/features` 和 `tests/steps` 文件夹中的验收测试。'
- en: Unit tests for the application modules in the `tests` folder.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tests` 文件夹中的应用模块单元测试。'
- en: Application to clean the acquired data and compute derived standardized Z-scores.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用以清理获取的数据并计算派生的标准化 Z 分数。
- en: 'Many of these deliverables are described in previous chapters. Specifically,
    [*Chapter** 9*](ch013.xhtml#x1-2080009), [*Project 3.1: Data Cleaning Base Application*](ch013.xhtml#x1-2080009)
    covers the basics of the deliverables for this project.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 许多这些可交付成果在之前的章节中已有描述。具体来说，[*第 9 章*](ch013.xhtml#x1-2080009)，[*项目 3.1：数据清洗基础应用*](ch013.xhtml#x1-2080009)
    讲述了本项目可交付成果的基础。
- en: Unit tests for standardizing functions
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标准化函数的单元测试
- en: There are two parts of the standardizing process that require unit tests. The
    first is the incremental computation of mean, variance, and standard deviation.
    This must be compared against results computed by the `statistics` module to assure
    that the results are correct. The `pytest.approx` object (or the `math.isclose()`
    function) are useful for asserting the incremental computation matches the expected
    values from the standard library module.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化过程有两个部分需要单元测试。第一部分是均值、方差和标准差的增量计算。这必须与 `statistics` 模块计算的结果进行比较，以确保结果正确。`pytest.approx`
    对象（或 `math.isclose()` 函数）对于断言增量计算与标准库模块的预期值匹配非常有用。
- en: 'Additionally, of course, the construction of the final sample, including the
    standardized Z-scores, needs to be tested. The test case is generally quite simple:
    a single value with a given x, y, mean of x, mean of y, the standard deviation
    of x, and the standard deviation of y need to be converted from the incomplete
    form to the complete form. The computation of the derived values is simple enough
    that the expected results can be computed by hand to check the results.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当然，包括标准化 Z 分数在内的最终样本的构建也需要进行测试。测试用例通常很简单：给定 x、y、x 的均值、y 的均值、x 的标准差和 y 的标准差的单个值需要从不完全形式转换为完整形式。派生值的计算足够简单，以至于可以通过手工计算预期结果来检查结果。
- en: It’s important to test this class, even though it seems very simple. Experience
    suggests that these seemingly simple classes are places where a `+` replaces a
    `-` and the distinction isn’t noticed by people inspecting the code. This kind
    of small mistake is best found with a unit test.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这个类看起来非常简单，测试这个类也很重要。经验表明，这些看似简单的类是`+`替换`-`的地方，而且这种区别并没有被审查代码的人注意到。这种小错误最好通过单元测试来发现。
- en: Acceptance test
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 接受测试
- en: The acceptance test suite for this standardization processing will involve a
    main program that creates two output files. This suggests the after-scenario cleanup
    needs to ensure the intermediate file is properly removed by the application.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种标准化处理的接受测试套件将涉及一个主程序，该程序创建两个输出文件。这表明在场景之后清理需要确保中间文件被应用程序正确删除。
- en: The cleaning application could use the `tempfile` module to create a file that
    will be deleted when closed. This is quite reliable, but it can be difficult to
    debug very obscure problems if the file that reveals the problems is automatically
    deleted. This doesn’t require any additional acceptance test Then step to be sure
    the file is removed, since we don’t need to test the `tempfile` module.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 清理应用程序可以使用`tempfile`模块创建一个在关闭时会被删除的文件。这相当可靠，但如果揭示问题的文件被自动删除，调试非常难以捉摸的问题可能会很困难。这不需要任何额外的接受测试步骤来确保文件被删除，因为我们不需要测试`tempfile`模块。
- en: The cleaning application can also create a temporary file in the current working
    directory. This can be unlinked for normal operation, but left in place for debugging
    purposes. This will require at least two scenarios to be sure the file is removed
    normally, and be sure the file is retained to support debugging.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 清理应用程序还可以在当前工作目录中创建一个临时文件。这可以在正常操作中解除链接，但为了调试目的而保留。这将需要至少两个场景来确保文件被正常删除，并确保文件被保留以支持调试。
- en: The final choice of implementation — and the related test scenarios — is left
    to you.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的实现选择——以及相关的测试场景——留给你决定。
- en: '10.5 Project 3.6: Integration to create an acquisition pipeline'
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.5 项目 3.6：集成创建获取管道
- en: In [*User experience*](ch013.xhtml#x1-2100001), we looked at the two-step user
    experience. One command is used to acquire data. After this, a second command
    is used to clean the data. An alternative user experience is a single shell pipeline.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*用户体验*](ch013.xhtml#x1-2100001)中，我们探讨了双步用户体验。一个命令用于获取数据。之后，使用第二个命令来清理数据。另一种用户体验是单一的shell管道。
- en: 10.5.1 Description
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5.1 描述
- en: The previous projects in this chapter have decomposed the cleaning operation
    into two distinct steps. There’s another, very desirable user experience alternative.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 本章前面的项目已经将清理操作分解为两个不同的步骤。还有一个非常理想的用户体验替代方案。
- en: 'Specifically, we’d like the following to work, also:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们希望以下内容也能正常工作：
- en: '[PRE16]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The idea is to have the **acquire** application write a sequence of NDJSON objects
    to standard output. The **clean** application will read the sequence of NDJSON
    objects from standard input. The two applications will run concurrently, passing
    data from process to process.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 想法是让**获取**应用程序将一系列NDJSON对象写入标准输出。**清理**应用程序将读取标准输入的NDJSON对象序列。这两个应用程序将并发运行，从进程到进程传递数据。
- en: For very large data sets, this can reduce the processing time. Because of the
    overhead in serializing Python objects to JSON text and deserializing Python objects
    from the text, the pipeline will not run in half the time of the two steps executed
    serially.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非常大的数据集，这可以减少处理时间。由于将Python对象序列化为JSON文本和从文本反序列化Python对象的开销，管道的运行时间不会是两个步骤串行执行时间的一半。
- en: Multiple extractions
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多次提取
- en: In the case of CSV extraction of Anscombe Quartet data, we have an **acquire**
    application that’s capable of creating four files concurrently. This doesn’t fit
    well with the shell pipeline. We have two architectural choices for handling this.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在CSV提取Anscombe四重奏数据的情况下，我们有一个**获取**应用程序，能够同时创建四个文件。这并不适合shell管道。我们有两个架构选择来处理这种情况。
- en: 'One choice is to implement a ”fan-out” operation: the **acquire** program fans
    out data to four separate clean applications. This is difficult to express as
    a collection of shell pipelines. To implement this, a parent application uses
    `concurrent.futures`, queues, and processing pools. Additionally, the **acquire**
    program would need to write to shared queue objects, and the **clean** program
    would read from a shared queue.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 一个选择是实现一个“扇出”操作：**acquire** 程序将数据扇出到四个单独的清洁应用程序。这很难用 shell 管道集合来表示。为了实现这一点，父应用程序使用
    `concurrent.futures`、队列和处理池。此外，**acquire** 程序需要写入共享队列对象，而 **clean** 程序则从共享队列中读取。
- en: The alternative is to process only one of the Anscombe series at a time. Introducing
    a `-s`` Series_1Pair` argument lets the user name a class that can extract a single
    series from the source data. Processing a single series at a time permits a pipeline
    that can be readily described as a shell command.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是同时只处理 Anscombe 系列中的一个。引入 `-s` `Series_1Pair` 参数允许用户命名一个可以从源数据中提取单个序列的类。同时处理单个序列允许将管道描述为
    shell 命令。
- en: This concept is often necessary to disentangle enterprise data. It’s common
    for enterprise applications — which often evolve organically — to have values
    from distinct problem domains as parts of a common record.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念通常对于梳理企业数据是必要的。企业应用程序——通常是有机演化的——作为公共记录的一部分，常常包含来自不同问题域的值。
- en: We’ll turn to the technical approach in the next section.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节转向技术方法。
- en: 10.5.2 Approach
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5.2 方法
- en: 'For reference to the general approach to this application, see [*Chapter** 9*](ch013.xhtml#x1-2080009),
    [*Project* *3.1: Data Cleaning Base Application*](ch013.xhtml#x1-2080009), specifically
    [*Approach*](ch013.xhtml#x1-2150002).'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此应用程序的一般方法的参考，请参阅[*第 9 章*](ch013.xhtml#x1-2080009)、[*项目 3.1：数据清理基础应用程序*](ch013.xhtml#x1-2080009)，特别是[*方法*](ch013.xhtml#x1-2150002)。
- en: 'Writing the standard output (and reading from standard input) suggests that
    these applications will have two distinct operating modes:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 写入标准输出（从标准输入读取）表明，这些应用程序将有两种不同的操作模式：
- en: Opening a named file for output or input.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打开一个命名的文件进行输出或输入。
- en: Using an existing, open, unnamed file — often a pipe created by the shell —
    for output or input.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用现有的、已打开的、未命名的文件——通常是由 shell 创建的管道——进行输出或输入。
- en: 'This suggests that the bulk of the design for an application needs to focus
    on open file-like objects. These are often described by the type hint of `TextIO`:
    they are files that can read (or write) text.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，应用程序设计的大部分需要集中在打开的文件-like 对象上。这些通常由 `TextIO` 的类型提示描述：它们是可以读取（或写入）文本的文件。
- en: The top-level `main()` function must be designed either to open a named file,
    or to provide `sys.stdout` or `sys.stdin` as argument values. The various combinations
    of files are provided to a function that will do the more useful work.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 顶层 `main()` 函数必须设计为打开一个命名的文件，或者提供 `sys.stdout` 或 `sys.stdin` 作为参数值。各种文件组合被提供给一个将执行更有用工作的函数。
- en: 'This pattern looks like the following snippet:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式看起来像以下片段：
- en: '[PRE17]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `process()` function is either given a file opened by a context manager,
    or the function is given the already open `sys.stdout`.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '`process()` 函数要么由上下文管理器打开的文件提供，要么函数被提供已经打开的 `sys.stdout`。'
- en: The ability for a Python application to be part of a shell pipeline is a significant
    help in creating larger, more sophisticated composite processes. This higher-level
    design effort is sometimes called “Programming In The Large.”
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Python 应用程序能够成为 shell 管道的一部分，这对于创建更大、更复杂的复合过程非常有帮助。这种高级设计努力有时被称为“大型编程”。
- en: Being able to read and write from pipelines was a core design feature of the
    Unix operating system and continues to be central to all of the various GNU/Linux
    variants.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 能够从管道中读取和写入是 Unix 操作系统的核心设计特性，并且继续是所有各种 GNU/Linux 变体的核心。
- en: This pipeline-aware design has the advantage of being slightly easier to unit
    test. The `process()` function’s output argument value can be an `io.StringIO`
    object. When using a `StringIO` object, the file processing is simulated entirely
    in memory, leading to faster, and possibly simpler, tests.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这种管道感知设计具有稍微容易进行单元测试的优势。`process()` 函数的输出参数值可以是一个 `io.StringIO` 对象。当使用 `StringIO`
    对象时，文件处理完全在内存中模拟，从而实现更快，可能更简单的测试。
- en: This project sets the stage for a future project. See [*Chapter** 12*](ch016.xhtml#x1-27600012),
    [*Project 3.8:* *Integrated Data Acquisition Web Service*](ch016.xhtml#x1-27600012)
    for a web service that can leverage this pipeline.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Consider packages to help create a pipeline
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A Python application to create a shell pipeline can involve a fair amount of
    programming to create two subprocesses that share a common buffer. This is handled
    elegantly by the shell.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative is [https://cgarciae.github.io/pypeln/](https://cgarciae.github.io/pypeln/).
    The **PypeLn** package is an example of a package that wraps the `subprocess`
    module to make it easier for a parent application to create a pipeline that executes
    the two child applications: **acquire** and **clean**.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Using a higher-level Python application to start the acquire-to-clean pipeline
    avoids the potential pitfalls of shell programming. It permits Python programs
    with excellent logging and debugging capabilities.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve seen the technical approach, it’s appropriate to review the deliverables.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.3 Deliverables
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This project has the following deliverables:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Documentation in the `docs` folder.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acceptance tests in the `tests/features` and `tests/steps` folders.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit tests for the application modules in the `tests` folder.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Revised applications that can be processed as a pipeline of two concurrent processes.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many of these deliverables are described in previous chapters. Specifically,
    [*Chapter** 9*](ch013.xhtml#x1-2080009), [*Project 3.1: Data Cleaning Base Application*](ch013.xhtml#x1-2080009)
    covers the basics of the deliverables for this project.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Acceptance test
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The acceptance test suite needs to confirm the two applications can be used
    as stand-alone commands, as well as used in a pipeline. One technique for confirming
    the pipeline behavior is to use shell programs like `cat` to provide input that
    mocks the input from another application.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the `When` step may execute the following kind of command:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The **clean** application is executed in a context where it is part of an overall
    pipeline. The head of the pipeline is not the **acquire** application; we’ve used
    the `cat`` some_mock_file.ndj` command as a useful mock for the other application’s
    output. This technique permits a lot of flexibility to test applications in a
    variety of shell contexts.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Using a pipeline can permit some helpful debugging because it disentangles two
    complicated programs into two smaller programs. The programs can be built, tested,
    and debugged in isolation.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 10.6 Summary
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This chapter expanded in several ways on the project in [*Chapter** 9*](ch013.xhtml#x1-2080009),
    [*Project 3.1:* *Data Cleaning Base Application*](ch013.xhtml#x1-2080009). The
    following additional processing features were added:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Pythonic approaches to validation and conversion of cardinal values.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approaches to validation and conversion of nominal and ordinal values.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques for uncovering key relationships and validating data that must properly
    reference a foreign key.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline architectures using the shell pipeline.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10.7 Extras
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here are some ideas for you to add to these projects.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 10.7.1 Hypothesis testing
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The computations for mean, variance, standard deviation, and standardized Z-scores
    involve floating-point values. In some cases, the ordinary truncation errors of
    float values can introduce significant numeric instability. For the most part,
    the choice of a proper algorithm can ensure results are useful.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: In addition to basic algorithm design, additional testing is sometimes helpful.
    For numeric algorithms, the **Hypothesis** package is particularly helpful. See
    [https://hypothesis.readthedocs.io/en/latest/](https://hypothesis.readthedocs.io/en/latest/).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking specifically at [*Project 3.5: Standardize data to common codes and*
    *ranges*](#x1-2450004), the [*Approach*](#x1-2470002) section suggests a way to
    compute the variance. This class definition is an excellent example of a design
    that can be tested effectively by the Hypothesis module to confirm that the results
    of providing a sequence of three known values produces the expected results for
    the count, sum, mean, variance, and standard deviation.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 10.7.2 Rejecting bad data via filtering (instead of logging)
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the examples throughout this chapter, there’s been no in-depth mention of
    what to do with data that raises an exception because it cannot be processed.
    There are three common choices:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Allow the exception to stop processing.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log each problem row as it is encountered, discarding it from the output.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the faulty data to a separate output file so it can be examined with a
    data inspection notebook.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first option is rather drastic. This is useful in some data cleaning applications
    where there’s a reasonable expectation of very clean, properly curated data. In
    some enterprise applications, this is a sensible assumption, and invalid data
    is the cause for crashing the application and sorting out the problems.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: The second option has the advantage of simplicity. A `try:`/`except:` block
    can be used to write log entries for faulty data. If the volume of problems is
    small, then locating the problems in the log and resolving them may be appropriate.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: The third option is often used when there is a large volume of questionable
    or bad data. The rows are written to a file for further study.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'You are encouraged to implement this third strategy: create a separate output
    file for rejected samples. This means creating acceptance tests for files that
    will lead to the rejection of at least one faulty row.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 10.7.3 Disjoint subentities
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An even more complicated data validation problem occurs when the source documents
    don’t reflect a single resulting dataclass. This often happens when disjoint subtypes
    are merged into a single data set. This kind of data is a union of the disjoint
    types. The data must involve a ”discriminator” field that shows which type of
    object is being described.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: For example, we may have a few fields with date, time, and document ID that
    are common to all samples. In addition to those fields, a `document_type` field
    provides a set of codes to discriminate between the different kinds of invoices
    and different kinds of payments.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, a conversion function involves two stages of conversions:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Identify the subtype. This may involve converting the common fields and the
    discriminator field. The work will be delegated to a subtype-specific conversion
    for the rest of the work.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert each subtype. This may involve a family of functions associated with
    each of the discriminator values.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This leads to a function design as shown in the activity diagram in [*Figure
    10.1*](#10.1).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1: Subentity validation ](img/file48.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Subentity validation'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 10.7.4 Create a fan-out cleaning pipeline
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are two common alternatives for concurrent processing of a **acquire**
    and **clean** application:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: A shell pipeline that connects the **acquire** application to the **clean**
    application. These two subprocesses run concurrently. Each ND JSON line written
    by the **acquire** application is immediately available for processing by the
    **clean** application.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pool of workers, managed by `concurrent.futures`. Each ND JSON line created
    by the **acquire** application is placed in a queue for one of the workers to
    consume.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The shell pipeline is shown in [*Figure 10.2*](#10.2).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2: Components of a shell pipeline ](img/file49.jpg)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Components of a shell pipeline'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: The shell creates two child process with a shared buffer between them. For the
    acquire child process, the shared buffer is `sys.stdout`. For the clean child
    process, the shared buffer is `sys.stdin`. As the two applications run, each byte
    written is available to be read.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: We’ve included explicit references to the Python runtime in these diagrams.
    This can help clarify how our application is part of the overall Python environment.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline creation is an elegant feature of the shell, and can be used to
    create complex sequences of concurrent processing. This is a handy way to think
    of decomposing a large collection of transformations into a number of concurrent
    transformations.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, the pipeline model isn’t ideal. This is often the case when we
    need asymmetric collections of workers. For example, when one process is dramatically
    faster than another, it helps to have multiple copies of the slow processes to
    keep up with the faster process. This is handled politely by the `concurrent.futures`
    package, which lets an application create a ”pool” of workers.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: The pool can be threads or processes, depending on the nature of the work. For
    the most part, CPU cores tend to be used better by process pools, because OS scheduling
    is often process-focused. The Python **Global Interpreter Lock** (**GIL**) often
    prohibits compute-intensive thread pools from making effective use of CPU resources.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: For huge data sets, worker-pool architecture can provide some performance improvements.
    There is overhead in serializing and deserializing the Python objects to pass
    the values from process to process. This overhead imposes some limitations on
    the benefits of multiprocessing.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: The components that implement a worker process pool are shown in [*Figure 10.3*](#10.3).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3: Components of a worker pool ](img/file50.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Components of a worker pool'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: This design is a significant alteration to the relationship between the `acquire.py`
    and `clean.py` applications. When the `acquire.py` application creates the process
    pool, it uses class and function definitions available within the same parent
    process.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'This suggests the `clean.py` module needs to have a function that processes
    exactly one source document. This function may be as simple as the following:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This function uses the analysis model definition, `SeriesSample`, to perform
    the validation, cleaning, and conversion of the acquired data. This can raise
    exceptions, which need to be logged.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: The child processes are created with copies of the parent application’s logging
    configuration. The `multiprocessing.get_logger()` function will retrieve the logger
    that was initialized into the process when the pool of worker processes was created.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'The `acquire.py` application can use a higher-order `map()` function to allocate
    requests to the workers in an executor pool. The general approach is shown in
    the following incomplete code snippet:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This works by allocating a number of resources, starting with the target file
    to be written, then the pool of processes to write clean data records to the file,
    and finally, the source for the original, raw data samples. Each of these has
    a context manager to be sure the resources are properly released when all of the
    processing has finished.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: We use the `ProcessPoolExecutor` object as a context manager to make sure the
    subprocesses are properly cleaned up when the source data has been fully consumed
    by the `map()` function, and all of the results retrieved from the `Future` objects
    that were created.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: The `get_series()` function is an iterator that provides the builds the acquire
    version of each `SeriesSample` object. This will use an appropriately configured
    `Extractor` object to read a source and extract a series from it.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Since generators are lazy, nothing really happens until the values of the `acquire_document_iter`
    variable are consumed. The `executor.map()` will consume the source, providing
    each document to the pool of workers to create a `Future` object that reflects
    the work being done by a separate subprocess. When the work by the subprocess
    finishes, the `Future` object will have the result and be ready for another request.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 由于生成器是惰性的，直到消耗了 `acquire_document_iter` 变量的值，实际上并没有发生任何事情。`executor.map()` 会消耗源数据，将每个文档提供给工作池以创建一个反映由单独子进程执行的工作的
    `Future` 对象。当子进程的工作完成时，`Future` 对象将包含结果并准备好接受另一个请求。
- en: When the `persist_samples()` functions consume the values from the `clean_samples`
    iterator, each of the `Future` objects will yield their result. These result objects
    are the values computed by the `clean.clean_sample()` function. The sequence of
    results is written to the target file.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 当 `persist_samples()` 函数从 `clean_samples` 迭代器中消耗值时，每个 `Future` 对象都会产生它们的结果。这些结果对象是由
    `clean.clean_sample()` 函数计算出的值。结果序列被写入目标文件。
- en: The `concurrent.futures` process pool `map()` algorithm will preserve the original
    order. The process pool offers alternative methods that can make results ready
    as soon as they’re computed. This can reorder the results; something that may
    or may not be relevant for subsequent processing.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '`concurrent.futures` 进程池的 `map()` 算法将保留原始顺序。进程池提供了其他方法，可以在计算完成后立即使结果就绪。这可能会重新排序结果；这可能是或可能不是后续处理相关的。'
