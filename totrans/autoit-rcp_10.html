<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch10"/>Chapter 10. Data Analysis and Visualizations</h1></div></div></div><p>You have so much of data and its just lying around? Ever wondered how you could easily analyze data and generate insights? Curious about the data analysis process? Well, you are at the right place!</p><p>In this chapter, we will cover the following recipes:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Reading, selecting, and interpreting data with visualizations</li><li class="listitem" style="list-style-type: disc">Generating insights with data filtering and aggregation</li><li class="listitem" style="list-style-type: disc">Automating social media analysis for businesses</li></ul></div><div><div><div><div><h1 class="title"><a id="ch10lvl1sec79"/>Introduction</h1></div></div></div><div><blockquote class="blockquote"><p>
<em>In God we trust. All others must bring data
                                                                 -W. Edwards Demming, Statistician</em>
</p></blockquote></div><p>Today, businesses heavily rely on data to get insights into what customers need, what channels they will use to buy, and so on. This way, businesses can take informed decisions about launching a new product or coming up with new offers. But how do businesses achieve this? What does decision making actually involve?</p><p>Data-based decision making refers to the process of data inspection, scrubbing or cleaning, data transformation, and generating models on top of data for the purpose of generating insights, discovering useful information, and drawing conclusions. For instance, an e-commerce company would use this process to analyze consumer buying patterns and suggest appropriate time slots for coming up with promotional offers for a select group of products. In fact, businesses analyze static or real-time data for multiple purposes, such as generating trends, building forecast models, or simply to extract structured information from raw data. Data analysis has multiple facets and approaches and can be briefly categorized under business intelligence, predictive analytics, and text mining.</p><p>
<strong>Business intelligence</strong> (<strong>BI</strong>) is capable of handling large amounts of structured and, sometimes, unstructured data to allow for the easy interpretation of these large volumes of data. Identifying new opportunities based on insights into data can provide businesses with a competitive advantage and stability.</p><p>
<strong>Predictive analytics</strong> encompasses the application of various statistical models, such as machine learning, to analyze historical data and current trends, in order to come up with predictions for future or unknown events. It involves generating models and capturing relationships among data features for risk assessment and decision making.</p><p>
<strong>Text analytics</strong> is the process of deriving quality information from structured or unstructured text data. Text analytics involves linguistic, statistical, and contextual techniques to extract and classify information for businesses.</p><p>However, data-based decisions are not easy and cannot be taken in a jiffy. Data-based decision making is a step-by-step process involving multiple operations. Let's understand the complete process in detail in the next section.</p><div><div><div><div><h2 class="title"><a id="ch10lvl2sec226"/>Steps to data-based decision making</h2></div></div></div><p>At a high level, the process can be categorized into the following phases. Of course, you can customize the process to suit your objectives:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Define hypothesis and data requirements</strong>: Before you start the process, you should be clear with your business goals--they should be <strong>Specific, Measurable, Acceptable, Relevant, and Timely</strong> (<strong>SMART</strong>). You don't want to start collecting data without being clear about the problem you're solving. As far as possible, come up with clear problem statements, such as "What has been the trend for mobile sales in the consumer space for the last three quarters?" Or something futuristic such as "Will I be able to sell electronic goods this winter with a profit margin of 150%?" Can you come up with a statement like this for your company?</li><li class="listitem" style="list-style-type: disc"><strong>Data source</strong>: You should also be clear about the source of your data. Are you relying on your own company database to perform the data analysis? Are you also relying on any third-party market research or trends to base your analysis on? If you are using third-party data, how do you plan to extract the data from the source (possibly through an API) and put it in your data store?</li><li class="listitem" style="list-style-type: disc"><strong>Data collection</strong>: Now that you're clear about what you want to generate insights into, the next step is to collect data in the required format. For instance, if you want data on the trend of mobile sales, you should collect data for the factors that influence mobile sales, such as new product introductions (product), offers (price), payment options, and date/time of purchase, among other relevant factors. Also, you should have an agreeable or a standard way of storing data; for instance, I may store the mobile sales per unit in USD and not in EUR, or I may store the sales in days and not in hours. Identifying a representative sample is really useful in such cases. Representative samples accurately reflect the entire population and definitely help in analysis.</li><li class="listitem" style="list-style-type: disc"><strong>Data transformation</strong>: Now you know where to collect the data from and in what format, it's time to decide where you want to load the data. It could be a plain old CSV or an SQL database; you need to know beforehand so that you can organize the data in the best way and get ready for analysis. This step can be referred to as transformation, as it involves extracting data from the source data system to a destination data system. In large scales, data is stored in a data warehouse system.</li><li class="listitem" style="list-style-type: disc"><strong>Data cleansing</strong>: Once the data is processed and organized, it's time to look at the data sanity. Transformed data may be incompatible, contain duplicates, or may at least contain measurement, sampling, and data entry errors. Data cleansing involves the removal of inaccurate data, adding default values for missing data, removing outliers, and resolving any other data inconsistency issues. You really have to be careful while dumping outliers; you should decide on the ways you want to remove them--is it a simple deletion of records or imputing them with the mean/mode of the other observations? You're the best decision maker in this case.</li><li class="listitem" style="list-style-type: disc"><strong>Data analysis</strong>: Once we have the data cleansed and ready for use, it's time for deeper analysis. You can analyze the data for business intelligence, or generate predictive models, using statistical techniques such as Logistic Regression. You could also perform text analysis on top of it to generate insights and arrive at decisions.</li><li class="listitem" style="list-style-type: disc"><strong>Data visualization</strong>: Once the analysis is done, it can be reported in many formats so that the analysis can be effectively communicated to the audience. Data visualization uses information display, such as tables and charts, to help communicate key messages contained in the data. Visualizations also help users to interpret their assumptions and generate meaningful information from the analysis.</li><li class="listitem" style="list-style-type: disc"><strong>Data interpretation and feedback</strong>: This phase helps you answer three main questions. Does the analysis answer the questions you began with? Does it help you validate your assumptions, that is, accept or reject your hypothesis? Do you need more data to improve your models or conclusions? It's not complete until your conclusions don't flow back into the system. The feedback loop makes sure that the predictive models are enriched and trained well for future use.</li></ul></div><p>OK, that's a good start! I think you must have got a fair idea of the complete process: data collection to generating insights. You will realize that a few of these steps, such as defining objectives, data collection, and transforming data, are custom to the market context and the problem being solved.</p><p>In this chapter, we will focus on a few generic aspects, such as collecting real-time data, reading data, performing data analysis, and data visualization. We will take a look at the popular Python modules that will help us read the data efficiently and analyze the data to generate insights. You will also learn about the Python modules that help interpret data and generate visualizations (charts).</p><p>At the end of this chapter, we will also look at a typical business process that can be automated with the knowledge we built with the recipes covered in the chapter. This chapter will help you start your journey as a data scientist, but doesn't cover extensive topics, such as statistical techniques or predictive modeling.</p><p>During the course of this chapter, we will use the following Python modules:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">pandas</code> (<a class="ulink" href="http://pandas.pydata.org/pandas-docs/version/0.15.2/tutorials.html">http://pandas.pydata.org/pandas-docs/version/0.15.2/tutorials.html</a>)</li><li class="listitem" style="list-style-type: disc"><code class="literal">numpy</code> (<a class="ulink" href="http://www.numpy.org/">http://www.numpy.org/</a>)</li><li class="listitem" style="list-style-type: disc"><code class="literal">matplotlib</code> (<a class="ulink" href="http://matplotlib.org/">http://matplotlib.org/</a>)</li><li class="listitem" style="list-style-type: disc"><code class="literal">seaborn</code> (<a class="ulink" href="https://pypi.python.org/pypi/seaborn/">https://pypi.python.org/pypi/seaborn/</a>)</li></ul></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch10lvl1sec80"/>Reading, selecting, and interpreting data with visualizations</h1></div></div></div><p>In this recipe, we will have the help of a known dataset. We will use TechCrunch's Continental USA CSV file that contains the listing of 1,460 company funding rounds. This is how it looks. It contains data points, such as the company name, number of employees, funding date, amounts raised, and type of funding (series A or angel funding):</p><p>
</p><div><img alt="Reading, selecting, and interpreting data with visualizations" src="img/image_10_001.jpg"/></div><p>
</p><div><ol class="orderedlist arabic"><li class="listitem">Now, let's install the modules that we will use to read and select the data from this CSV file. Before doing that, we will set up a virtual environment and activate it:<pre class="programlisting">
<strong>chetans-MacBookPro:ch11 Chetan$ virtualenv analyze</strong>
<strong>New python executable in analyze/bin/python2.7</strong>
<strong>Also creating executable in analyze/bin/python</strong>
<strong>Installing setuptools, pip, wheel...done.</strong>
<strong>chetans-MacBookPro:ch11 Chetan$ source analyze/bin/activate</strong>
<strong>(analyze)chetans-MacBookPro:ch11 Chetan$</strong>
</pre></li><li class="listitem">OK, cool! Now, let's install <code class="literal">pandas</code>. We will use <code class="literal">pandas</code> to read our CSV file and select the data to perform analysis. We install <code class="literal">pandas</code> with our favorite utility, <code class="literal">python-pip</code>. The following are the installation logs for <code class="literal">pandas</code> on my Mac OSX:<pre class="programlisting">
<strong>(analyze)chetans-MacBookPro:ch11 Chetan$ pip install pandas</strong>
<strong>Collecting pandas</strong>
<strong>Collecting pytz&gt;=2011k (from pandas)</strong>
<strong>      Using cached pytz-2016.7-py2.py3-none-any.whl</strong>
<strong>Collecting python-dateutil (from pandas)</strong>
<strong>      Using cached python_dateutil-2.6.0-py2.py3-none-any.whl</strong>
<strong>Collecting numpy&gt;=1.7.0 (from pandas)</strong>
<strong>Collecting six&gt;=1.5 (from python-dateutil-&gt;pandas)</strong>
<strong>      Using cached six-1.10.0-py2.py3-none-any.whl</strong>
<strong>Installing collected packages: pytz, six, &#13;
        python-dateutil, numpy, pandas</strong>
<strong>Successfully installed numpy-1.11.2 pandas-0.19.1 &#13;
        python-dateutil-2.6.0 pytz-2016.7 six-1.10.0</strong>
</pre><div><div><h3 class="title"><a id="note28"/>Note</h3><p>Installing the <code class="literal">pandas</code> module also installs the <code class="literal">numpy</code> module for me. In fact, I had installed these modules on my machine earlier as well; hence, a lot of these modules get picked up from cache. On your machine, the installation logs may differ.</p></div></div></li><li class="listitem">Next, let's install <code class="literal">matplotlib</code> and <code class="literal">seaborn</code>; libraries that will be used by us for visualizations. The following are the installation logs on my machine, first for <code class="literal">matplotlib</code>:<pre class="programlisting">
<strong>(analyze)chetans-MacBookPro:ch11 Chetan$ pip install matplotlib</strong>
<strong>Collecting matplotlib</strong>
<strong>Requirement already satisfied (use --upgrade to upgrade):&#13;
          numpy&gt;=1.6 in ./analyze/lib/python2.7/site-packages &#13;
          (from matplotlib)</strong>
<strong>Requirement already satisfied (use --upgrade to upgrade):&#13;
          pytz in ./analyze/lib/python2.7/site-packages &#13;
          (from matplotlib)</strong>
<strong>Requirement already satisfied (use --upgrade to upgrade):&#13;
          python-dateutil in ./analyze/lib/python2.7/site-packages&#13;
          (from matplotlib)</strong>
<strong>Collecting cycler (from matplotlib)</strong>
<strong>      Using cached cycler-0.10.0-py2.py3-none-any.whl</strong>
<strong>Collecting pyparsing!=2.0.0,!=2.0.4,!=2.1.2,&gt;=1.5.6&#13;
        (from matplotlib)</strong>
<strong>      Using cached pyparsing-2.1.10-py2.py3-none-any.whl</strong>
<strong>Requirement already satisfied (use --upgrade to upgrade):&#13;
        six&gt;=1.5 in ./analyze/lib/python2.7/site-packages &#13;
        (from python-dateutil-&gt;matplotlib)</strong>
<strong>Installing collected packages: cycler, pyparsing, &#13;
        matplotlib</strong>
<strong>Successfully installed cycler-0.10.0 &#13;
        matplotlib-1.5.3 pyparsing-2.1.10</strong>
</pre><p>As you can see these modules are installed on my machine, so the installation logs may differ when you install these modules for the first time on your machine. Here are the logs for seaborn:</p><pre class="programlisting">
<strong>(analyze)chetans-MacBookPro:ch11 Chetan$ pip install seaborn</strong>
<strong>Collecting seaborn</strong>
<strong>Collecting scipy (from seaborn)</strong>
<strong>Requirement already satisfied (use --upgrade to upgrade):&#13;
        numpy&gt;=1.7.1 in ./analyze/lib/python2.7/site-packages&#13;
        (from scipy-&gt;seaborn)</strong>
<strong>Installing collected packages: scipy, seaborn</strong>
<strong>Successfully installed scipy-0.18.1 seaborn-0.7.1</strong>
</pre></li></ol></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec227"/>How to do it...</h2></div></div></div><div><ol class="orderedlist arabic"><li class="listitem">First, let's just download the CSV file from <a class="ulink" href="http://samplecsvs.s3.amazonaws.com/TechCrunchcontinentalUSA.csv">https://support.spatialkey.com/spatialkey-sample-csv-data/</a>. The direct download link for the TechCrunch file is <a class="ulink" href="http://samplecsvs.s3.amazonaws.com/TechCrunchcontinentalUSA.csv">http://samplecsvs.s3.amazonaws.com/TechCrunchcontinentalUSA.csv</a>. You can download this file with the <code class="literal">wget</code> command as follows:<pre class="programlisting">
<strong>(analyze)chetans-MacBookPro:ch11 Chetan$ &#13;
          wget http://samplecsvs.s3.amazonaws.com/&#13;
          TechCrunchcontinentalUSA.csv</strong>
<strong>--2016-11-20 16:01:57--&#13;
          http://samplecsvs.s3.amazonaws.com/&#13;
          TechCrunchcontinentalUSA.csv</strong>
<strong>Resolving samplecsvs.s3.amazonaws.com... 54.231.97.224</strong>
<strong>Connecting to samplecsvs.s3.amazonaws.com&#13;
          |54.231.97.224|:80... connected.</strong>
<strong>HTTP request sent, awaiting response... 200 OK</strong>
<strong>Length: 93536 (91K) [application/x-csv]</strong>
<strong>Saving to: 'TechCrunchcontinentalUSA.csv'</strong>
<strong>TechCrunchcontinentalUSA.csv              100%&#13;
        [=======================================================&#13;
         =========================================&gt;]&#13;
                                     91.34K  20.3KB/s   in 4.5s   </strong>
<strong>2016-11-20 16:02:03 (20.3 KB/s) - &#13;
        'TechCrunchcontinentalUSA.csv' saved [93536/93536]</strong>
</pre></li><li class="listitem">Now, let's go ahead and write our first piece of Python code to read the CSV file. We read the CSV file and print the first five rows:<pre class="programlisting">      import pandas as pd&#13;
      &#13;
      pd.set_option('display.line_width', 5000)&#13;
      pd.set_option('display.max_columns', 60)&#13;
      &#13;
      df = pd.read_csv('TechCrunchcontinentalUSA.csv')&#13;
      print "First five rows:\n", df[:5]&#13;
</pre><p>In the preceding code example, we read the first five records of the CSV file:</p><p>
</p><div><img alt="How to do it..." src="img/image_10_002.jpg"/></div><p>
</p></li><li class="listitem">The <code class="literal">pandas</code> module reads the file's contents and converts them to a data frame of rows and columns. Now, if you look at the output of the preceding code, you will notice that an index column gets added to the file contents. With <code class="literal">pandas</code>, it's easy to parse the date, tell if the dates in our CSV file have the date first or month first (UK or US format), and make the date column as the index column:<pre class="programlisting">        import pandas as pd &#13;
 &#13;
        pd.set_option('display.line_width', 5000) &#13;
        pd.set_option('display.max_columns', 60) &#13;
 &#13;
        df = pd.read_csv('TechCrunchcontinentalUSA.csv',&#13;
                  index_col='fundedDate', \ &#13;
                  parse_dates=['fundedDate'], dayfirst=True,) &#13;
        print "Top five rows:\n", df[:5] &#13;
</pre><p>If you run the preceding code snippet, you should be able to see the index column, <strong>fundedDate</strong>, as shown in the following screenshot:</p><p>
</p><div><img alt="How to do it..." src="img/image_10_003.jpg"/></div><p>
</p></li><li class="listitem">Neat! Now, we're able to read the data, but how about selecting some data so that we can perform some analysis on top of it. Let's select the column that depicts the amount of funding raised by the companies (the <strong>raisedAmt</strong> column):<pre class="programlisting">        import pandas as pd &#13;
 &#13;
        pd.set_option('display.line_width', 5000) &#13;
        pd.set_option('display.max_columns', 60) &#13;
        df = pd.read_csv('TechCrunchcontinentalUSA.csv', &#13;
                  index_col='fundedDate', \ &#13;
                  parse_dates=['fundedDate'], dayfirst=True,) &#13;
 &#13;
        raised = df['raisedAmt'][:5] &#13;
        print "Funding Raised by Companies over time:\n", raised &#13;
</pre><p>Note that in the following screenshot, we have printed the top five records of the companies that have raised funding:</p><p>
</p><div><img alt="How to do it..." src="img/image_10_004.jpg"/></div><p>
</p></li><li class="listitem">OK, cool! So we can select the column of our choice and get the data we wanted to analyze. Let's see if we can generate some nice visualizations for it. The following recipe generates a line chart for the funding rounds reported for all the years (<em>x</em> axis), based on the amount raised (<em>y</em> axis):<pre class="programlisting">        import pandas as pd &#13;
        from matplotlib import pyplot as plt &#13;
        import seaborn as sns &#13;
 &#13;
        plt.style.use('default') &#13;
        pd.set_option('display.line_width', 5000) &#13;
        pd.set_option('display.max_columns', 60) &#13;
 &#13;
        df = pd.read_csv('TechCrunchcontinentalUSA.csv') &#13;
        print "First five rows:\n", df[:5] &#13;
 &#13;
        df = pd.read_csv('TechCrunchcontinentalUSA.csv', &#13;
                  index_col='fundedDate', \ &#13;
                  parse_dates=['fundedDate'], dayfirst=True,) &#13;
        print "Top five rows:\n", df[:5] &#13;
        raised = df['raisedAmt'][:5] &#13;
        print "Funding Raised by Companies over time:\n", raised &#13;
 &#13;
        sns.set_style("darkgrid") &#13;
        sns_plot = df['raisedAmt'].plot() &#13;
        plt.ylabel("Amount Raised in USD");&#13;
        plt.xlabel("Funding Year") &#13;
        plt.savefig('amountRaisedOverTime.pdf') &#13;
</pre><p>In the following screenshot, see how the rate of funding (or the rate of reporting) increased, and with that, the amounts raised also saw a steady increase!</p><p>
</p><div><img alt="How to do it..." src="img/image_10_005.jpg"/></div><p>
</p></li><li class="listitem">Fantastic! I know you have already started to like what we're doing here. Let's move forward and see whether we can select multiple columns from the CSV file. In the following example, we get the data for 50 rows, with the column names being <strong>company</strong>, <strong>category</strong>, and <strong>fundedDate</strong>:<pre class="programlisting">      import pandas as pd &#13;
      from matplotlib import pyplot as plt &#13;
      plt.style.use('default') &#13;
      pd.set_option('display.line_width', 5000) &#13;
      pd.set_option('display.max_columns', 60) &#13;
 &#13;
      fundings = pd.read_csv('TechcrunchcontinentalUSA.csv') &#13;
      print "Type of funding:\n", fundings[:5]['round'] &#13;
 &#13;
      # Selecting multiple columns &#13;
      print "Selected company, category and date of &#13;
             funding:\n",\ &#13;
      fundings[['company', 'category', &#13;
            'fundedDate']][600:650] &#13;
</pre><p>The output of the preceding code snippet is as follows:</p><p>
</p><div><img alt="How to do it..." src="img/image_10_006.jpg"/></div><p>
</p></li><li class="listitem">OK, great! Now let's select one of these columns and perform some analysis on top of it. In the following code example, we select the <strong>category</strong> column that gives us the categories of all the reported funding rounds. We then process the selected column to get the most common category of the company that got funded:<pre class="programlisting">        import pandas as pd &#13;
        from matplotlib import pyplot as plt &#13;
        plt.style.use('default') &#13;
 &#13;
        pd.set_option('display.line_width', 5000) &#13;
        pd.set_option('display.max_columns', 60) &#13;
 &#13;
        fundings = pd.read_csv('TechcrunchcontinentalUSA.csv') &#13;
        print "Type of funding:\n", fundings[:5]['round'] &#13;
 &#13;
        # Selecting multiple columns &#13;
        print "Selected company, category and date of funding:\n",\ &#13;
            fundings[['company', 'category', 'fundedDate']][600:650] &#13;
 &#13;
        # Most common category of company that got funded &#13;
        counts = fundings['category'].value_counts() &#13;
        print "Count of common categories of company &#13;
               that raised funds:\n", \ &#13;
               counts &#13;
</pre><p>The output of the preceding code snippet is:</p><pre class="programlisting">
<strong>Count of common categories of company that raised funds:</strong>
<strong>web           1208</strong>
<strong>software       102</strong>
<strong>mobile          48</strong>
<strong>hardware        39</strong>
<strong>other           16</strong>
<strong>cleantech       14</strong>
<strong>consulting       5</strong>
<strong>biotech          4</strong>
<strong>Name: category, dtype: int64</strong>
</pre></li><li class="listitem">Data and numbers give a lot of information, but the impact can actually only be seen through visualizations. Let's see whether we can plot the above data as a horizontal bar chart. The following recipe does the job for us:<pre class="programlisting">        import pandas as pd &#13;
        from matplotlib import pyplot as plt &#13;
        plt.style.use('default') &#13;
        pd.set_option('display.line_width', 5000) &#13;
        pd.set_option('display.max_columns', 60) &#13;
 &#13;
        fundings = pd.read_csv('TechcrunchcontinentalUSA.csv') &#13;
        print "Type of funding:\n", fundings[:5]['round'] &#13;
 &#13;
        # Selecting multiple columns &#13;
        print "Selected company, category and date of funding:\n",\ &#13;
            fundings[['company', 'category', 'fundedDate']][600:650] &#13;
 &#13;
        # Most common category of company that got funded &#13;
        counts = fundings['category'].value_counts() &#13;
        print "Count of common categoris of company &#13;
               that raised funds:\n", \ &#13;
               counts &#13;
        counts.plot(kind='barh') &#13;
        plt.xlabel("Count of categories") &#13;
 &#13;
        plt.savefig('categoriesFunded.pdf') &#13;
</pre><p>On the <em>y</em> axis, we have the category of the company that got funded, and the <em>x</em> axis is the total count of companies in a given category. Also, we save the plotted chart in a PDF file named <code class="literal">categoriesFunded.pdf</code>:</p><p>
</p><div><img alt="How to do it..." src="img/image_10_007.jpg"/></div><p>
</p></li></ol></div><p>Whoa! So many web companies got funded? Awesome! I too should start a web company--it increases the chances of getting funded.</p></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec228"/>How it works...</h2></div></div></div><p>In this section we dealt with the two major aspects of data analysis. First, we covered how to read the dataset from a CSV file and select the appropriate data (rows or columns) from our dataset.</p><p>In the first code snippet, we used the help of the <code class="literal">pandas</code> module to read the CSV file. In <code class="literal">pandas</code>, we have the <code class="literal">read_csv(csv_file)</code> method that takes the CSV file path as an argument. The <code class="literal">pandas</code> module reads the file and stores the file contents as data frames. DataFrame is a two-dimensional, labeled-in-memory data structure with columns of potentially different types. It mimics the structure of a spreadsheet or an SQL table or a dictionary of series objects. It has a nice set of methods and attributes to select, index, and filter data. In our first recipe, we read the CSV file and generated a DataFrame object <code class="literal">df</code>. Using <code class="literal">df</code> we selected the first five rows of our CSV file with <code class="literal">df[:5]</code>. See how easy it becomes to select the rows of a CSV file with <code class="literal">pandas</code>.</p><p>We can do a few more things with the <code class="literal">read_csv()</code> method. By default, <code class="literal">pandas</code> adds another index column to our dataset, but we can specify which column from the CSV file should be used for indexing our data. We achieved this by passing the <code class="literal">index_col</code> parameter to the <code class="literal">read_csv()</code> method. We also converted the string dates present in the <strong>fundedDate</strong> column of the CSV file to a datetime format with the <code class="literal">parse_dates</code> parameter and said that the date is in a format where the day is the first part of the date with the <code class="literal">dayfirst</code> parameter.</p><p>After getting the DataFrame and using <strong>fundedDate</strong> as index, we used <code class="literal">df['raisedAmt'][:5]</code> to select the <strong>raisedAmt</strong> column and print the first five rows. We then used the <code class="literal">seaborn</code> library to set the style of our plot with <code class="literal">sns.set_style("darkgrid")</code> and generated the bar chart with the <code class="literal">plot()</code> method. The <code class="literal">seaborn</code> library is used for generating nice visualizations and is implemented on <code class="literal">matplotlib</code>.</p><p>Using the <code class="literal">matplotlib</code> library, we created an object, <code class="literal">plt</code>, which was then used to label our chart with the <code class="literal">ylabel()</code> and <code class="literal">xlabel()</code> methods. The <code class="literal">plt</code> object was also used to finally store the resulting chart in the PDF format with the <code class="literal">savefig()</code> method.</p><p>In the second example, we selected multiple columns with <code class="literal">fundings[['company', 'category' and 'fundedDate']]</code>. We selected three columns from a CSV file in one line of code. We then plotted a horizontal bar chart with the <code class="literal">plot()</code> method and specified the type of chart with <code class="literal">kind=barh</code>. Finally, we made use of the <code class="literal">matplotlib</code> library to label the <em>x</em> axis with the <code class="literal">xlabel()</code> method and saved the chart with the <code class="literal">savefig()</code> method. As you can see, we didn't have to use the <code class="literal">seaborn</code> library to plot the chart; we could simply do it with <code class="literal">matplotlib</code>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch10lvl1sec81"/>Generating insights using data filtering and aggregation</h1></div></div></div><p>Reading CSV files and selecting multiple columns is easy with <code class="literal">pandas</code>. In this section, we will take a look at how to slice and dice data, essentially filtering data with <code class="literal">pandas</code>.</p><div><div><div><div><h2 class="title"><a id="ch10lvl2sec229"/>Getting ready</h2></div></div></div><p>In this section, we will use the same set of libraries (the following ones) that we used in the previous recipe:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">pandas</code> for filtering and data analysis</li><li class="listitem" style="list-style-type: disc"><code class="literal">matplotlib</code> and <code class="literal">seaborn</code> for plotting charts and saving the data in a PDF file</li></ul></div></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec230"/>How to do it...</h2></div></div></div><div><ol class="orderedlist arabic"><li class="listitem">Let's start by importing the required libraries and reading the CSV file with the <code class="literal">read_csv()</code> method. The following code carries out the operations:<pre class="programlisting">        import pandas as pd &#13;
&#13;
        from matplotlib import pyplot as plt &#13;
        import seaborn as sns &#13;
  &#13;
        plt.style.use('default') &#13;
 &#13;
        pd.set_option('display.line_width', 5000) &#13;
        pd.set_option('display.max_columns', 60) &#13;
 &#13;
        fundings = pd.read_csv(&#13;
                   'TechcrunchcontinentalUSA.csv', &#13;
                   index_col='fundedDate', \ &#13;
                   parse_dates=['fundedDate'], dayfirst=True) &#13;
</pre></li><li class="listitem">Now, let's filter on the data frame and use multiple columns to filter data. Say we filter funding records on the funding category, state, and the selected city in the state. This can be achieved with the following piece of code:<pre class="programlisting">        import pandas as pd &#13;
        from matplotlib import pyplot as plt &#13;
        import seaborn as sns &#13;
 &#13;
        plt.style.use('default') &#13;
 &#13;
        pd.set_option('display.line_width', 5000) &#13;
        pd.set_option('display.max_columns', 60) &#13;
 &#13;
       funding = pd.read_csv(&#13;
                  'TechcrunchcontinentalUSA.csv', &#13;
                  index_col='fundedDate', \ &#13;
                  parse_dates=['fundedDate'], dayfirst=True) &#13;
 &#13;
       #Web fundings in CA &#13;
       web_funding = funding['category'] == 'web' &#13;
       in_CA = funding['state'] == 'CA' &#13;
       in_city = funding['city'].isin(['Palo Alto',&#13;
                         'San Francisco', 'San Mateo', &#13;
                         'Los Angeles', 'Redwood City']) &#13;
</pre><p>The preceding piece of code returns all the funding records in the State of California (CA) for the cities Palo Alto, San Francisco, San Mateo, Los Angeles, and Redwood City for the web companies. The following is a partial screenshot of the output:</p><p>
</p><div><img alt="How to do it..." src="img/image_10_008.jpg"/></div><p>
</p></li><li class="listitem">OK cool, now let's see if we can get the count of funding for the companies in the web category by city names. The following code will get us the details we need:<pre class="programlisting">        web_funding = funding[web_funding &amp; in_CA &amp; in_city] &#13;
        web_counts = web_funding['city'].value_counts() &#13;
        print "Funding rounds for companies in 'web' &#13;
                   category by cities in CA:\n", web_counts &#13;
</pre><p>The output of the preceding piece of code is the number of funding rounds received by the companies in the <strong>web</strong> category in the selected cities:</p><p>
</p><div><img alt="How to do it..." src="img/image_10_009.jpg"/></div><p>
</p><p>Wow! The preceding analysis was quite useful; you got to know that the web companies in San Francisco city have received the funding 195 times (from the data we have). Looks like if you are a web company in San Francisco, all your funding worries are over. Well, that sounds logical and simple.</p></li><li class="listitem">But wait, isn't this information incomplete? How about gathering data for companies in all categories including <strong>web</strong> and then representing the data for the companies in the <strong>web</strong> category as a percentage of all the categories? This way, we will know whether you should have your company in 'San Francisco' or in any other city. OK then, let's get the count of funding rounds for companies in all categories (including <strong>web</strong>), for all the selected cities in CA. The following code will get us the information we need:<pre class="programlisting">        total_funding = funding[in_CA &amp; in_city] &#13;
        total_counts = total_funding['city'].value_counts() &#13;
        print "Funding rounds for companies in 'all' &#13;
               categories by cities in CA:\n",total_counts &#13;
</pre><p>Here is the output of the preceding code snippet:</p><p>
</p><div><img alt="How to do it..." src="img/image_10_010.jpg"/></div><p>
</p></li><li class="listitem">Nice! Now, let's get the data for the companies in the <strong>web</strong> category as a percentage of companies in all the categories for the selected cities of CA. We can get this data by simply dividing the data for the <strong>web</strong> categories by all the categories and then multiplying by 100 to represent it as a percentage. The following code snippet will help us in this case:<pre class="programlisting">        sns.set_style("darkgrid") &#13;
        sns_plot = (web_counts*100/total_counts.astype(&#13;
                              float)).plot(kind='barh') &#13;
</pre></li><li class="listitem">Now, let's plot this data as a horizontal bar chart with the following code:<pre class="programlisting">        plt.xlabel("(Funding Rounds in Web Category) / (&#13;
              Funding Rounds in All Categories) * (100)") &#13;
        plt.savefig('webFundedByCity.pdf') &#13;
</pre><p>The following screenshot helps us compare the funding rounds for web companies with respect to the funding rounds for companies in all other categories and for the cities in California:</p><p>
</p><div><img alt="How to do it..." src="img/image_10_011.jpg"/></div><p>
</p><p>After the analysis what did you figure? Do you still want to set up your company in San Francisco? If you are a web company in Los Angeles, even though the funding rounds are limited, there is a higher chance (0.925) of your company getting funded than when you're in San Francisco (0.855), at least from the data points we have.</p></li></ol></div><p>Now, let's take a look at another example. Say we want to analyze our data to see what months have historically received more funding than others. Additionally, can we also relate this with the rounds of funding [such as series A or angel funding]? Interesting thought! But how do we get there? The <code class="literal">pandas</code> module has support for grouping the data and data aggregation, which will come to our rescue for this analysis. Let's solve this problem step by step:</p><div><ol class="orderedlist arabic"><li class="listitem">First, let's read the CSV file and select two columns: <strong>raisedAmt</strong> and <strong>rounds</strong>. Let's also add another column, <code class="literal">month</code>, as the index column to this data frame. The following code gets the data frame ready for further analysis:<pre class="programlisting">        import pandas as pd &#13;
        from matplotlib import pyplot as plt &#13;
        import seaborn as sns &#13;
 &#13;
        plt.style.use('default') &#13;
 &#13;
        pd.set_option('display.line_width', 5000) &#13;
        pd.set_option('display.max_columns', 60) &#13;
 &#13;
        df = pd.read_csv('TechCrunchcontinentalUSA.csv',&#13;
                     index_col='fundedDate', \ &#13;
                     parse_dates=['fundedDate'], dayfirst=True,) &#13;
 &#13;
        funds = df[['raisedAmt', 'round']] &#13;
        funds['month'] = funds.index.month &#13;
        print "Funding Rounds with Month Index:\n", funds &#13;
</pre></li><li class="listitem">Now we need to get the data for the funds raised based on the month. The following code does exactly what we need:<pre class="programlisting">        funding_by_month = funds.groupby('month').aggregate('sum') &#13;
        funding_by_month.index = ['Jan', 'Feb', 'Mar',&#13;
                                  'Apr', 'May', 'June', 'July', \ &#13;
                                  'Aug', 'Sept', 'Oct', 'Nov', 'Dec'] &#13;
        print "Funding Rounds Grouped By Month:\n", funding_by_month &#13;
</pre></li><li class="listitem">Now, if we plot the data for this analysis, we will see how the funding fluctuates on a monthly basis for all the years of data we have:<p>
</p><div><img alt="How to do it..." src="img/image_10_012.jpg"/></div><p>
</p><p>Cool, looks like it's better to ask for funding in January. Maybe, the investors are in a good mood after the Christmas and New Year vacation; what do you think?</p></li><li class="listitem">Now, if you want to analyze and build a correlation between the month of year, amount raised, and the round of funding, we can get the data with the following piece of code:<pre class="programlisting">        funds['month'] = funds.index.month &#13;
        funding_by_stage = funds.groupby(['month', &#13;
                           'round']).aggregate('sum') &#13;
        print funding_by_stage &#13;
</pre><p>The output of the preceding code snippet is a data frame arranged as shown in the following screenshot. The data is grouped by <strong>month</strong> and <strong>round</strong> columns of funding, and <strong>raisedAmt</strong> is aggregated accordingly:</p><p>
</p><div><img alt="How to do it..." src="img/image_10_013.jpg"/></div><p>
</p></li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec231"/>How it works...</h2></div></div></div><p>For the first problem, we load the data from the CSV file as a data frame with the <code class="literal">read_csv()</code> method. Then, we filter the data based on multiple factors, where the state is <code class="literal">CA</code> and the company category is <code class="literal">web</code> and the cities are <code class="literal">Palo Alto</code>, <code class="literal">San Francisco</code>, <code class="literal">San Mateo</code>, <code class="literal">Los Angeles</code>, and <code class="literal">Redwood City</code>. Filtering is a column-based operation and is pretty straight forward; it gets you the relevant data frame after applying the criteria.</p><p>Then, we calculated the count of funding rounds, grouped by cities for the <strong>web</strong> companies with the <code class="literal">value_counts()</code> method. We did the same exercise for the funding rounds for companies in all the categories, including <strong>web</strong>.</p><p>Finally, we simply divided the data and got the data for the <strong>web</strong> companies as a percentage of the data for all the categories. The <code class="literal">pandas</code> module handled this operation for us seamlessly. It used both the data points for the same city for analysis without us even worrying about it.</p><p>Finally, we plotted the horizontal bar chart with the <code class="literal">plot()</code> method, depicted the percentages for each city individually, and got the insight we were looking for.</p><p>In the second example, we first got the data frame by selecting multiple columns: <strong>raisedAmt</strong> and <strong>round</strong>. We also added a new column to the <strong>month</strong> DataFrame and treated it as an index column.</p><p>Then, we grouped the data based on <strong>month</strong> with the help of the <strong>groupby()</strong> method. We then summed up the funding amounts to get the amount of funds raised based on <strong>month</strong>. To get the total funds, we used the <strong>aggregation()</strong> method and added the data points to get the required information.</p><p>Also, to build the correlation between the funds raised with respect to <strong>month</strong> and <strong>round</strong>, we grouped the data frame by <strong>month</strong> and <strong>round</strong> and again applied the aggregation on <strong>raisedAmt</strong>.</p></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec232"/>There's more...</h2></div></div></div><p>In the preceding two recipes, we learned about data analysis and visualization, and we extensively used the <code class="literal">pandas</code> Python module in our recipes. The <code class="literal">pandas</code> module is a very comprehensive library and has capabilities such as working with time series, advanced indexing techniques, merging and concatenating objects, and working with different datasets (JSON and Excel), amongst others. I highly encourage you to go through the <code class="literal">pandas</code> API Reference to learn more about this awesome library.</p><p>In the next recipe, let's see whether we can apply the knowledge we have gained so far in this chapter by helping Judy automate her task.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch10lvl1sec82"/>Automating social media analysis for businesses</h1></div></div></div><p>Judy is a columnist at a leading magazine in London. As a writer, she is always interested in recent topics. She collects data and analyzes it to come up with insights that would be interesting to her readers.</p><p>Currently, Judy is interested in the battle between Apple's iPhone and Samsung's Note and wants to publish an article in her magazine. She plans to collect the data by talking to people on the streets and reading blog posts, but she knows she will get an awful lot of information from social media. She is aware of the fact that people take to Twitter these days to express their pleasure or disappointment about using a product and also refer products to their friends on social media. However, she is worried about the fact that she has to go through this huge volume of social media data for her article.</p><p>You are a data scientist and Judy's colleague. Can you help Judy with her needs? It is an opportunity for you to show off your data skills!</p><p>Let's begin by analyzing Judy's problems. To begin with, Judy needs to collect data from an ever-growing social media platforms such as Twitter. Secondly, she needs to analyze this data to generate interesting insights. So, we should be able to build a system that caters to both her problems. Also, you may want to build a system that only solves her current needs, but she should be able to use it for any projects in the future.</p><div><div><div><div><h2 class="title"><a id="ch10lvl2sec233"/>Getting ready</h2></div></div></div><p>Let's install all the modules that we'll need to work on this problem. We already have <code class="literal">pandas</code>, <code class="literal">matplotlib</code>, and <code class="literal">seaborn</code> installed. For this problem, we will also install <code class="literal">tweepy</code>, a module to work with Twitter data. Let's install tweepy with our very own <code class="literal">python-pip</code>:</p><pre class="programlisting">
<strong>(analyze)chetans-MacBookPro:ch11 Chetan$ pip install tweepy</strong>
<strong>You are using pip version 7.1.0, however version 9.0.1 is available.</strong>
<strong>You should consider upgrading via the 'pip install --upgrade pip' command.</strong>
<strong>Collecting tweepy</strong>
<strong>  Downloading tweepy-3.5.0-py2.py3-none-any.whl</strong>
<strong>Collecting requests&gt;=2.4.3 (from tweepy)</strong>
<strong>  Downloading requests-2.12.1-py2.py3-none-any.whl (574kB)</strong>
<strong>    100% |████████████████████████████████| 577kB 161kB/s </strong>
<strong>Requirement already satisfied (use --upgrade to upgrade): six&gt;=1.7.3 in ./analyze/lib/python2.7/site-packages (from tweepy)</strong>
<strong>Collecting requests-oauthlib&gt;=0.4.1 (from tweepy)</strong>
<strong>  Downloading requests_oauthlib-0.7.0-py2.py3-none-any.whl</strong>
<strong>Collecting oauthlib&gt;=0.6.2 (from requests-oauthlib&gt;=0.4.1-&gt;tweepy)</strong>
<strong>  Downloading oauthlib-2.0.0.tar.gz (122kB)</strong>
<strong>    100% |████████████████████████████████| 122kB 345kB/s </strong>
<strong>Building wheels for collected packages: oauthlib</strong>
<strong>  Running setup.py bdist_wheel for oauthlib</strong>
<strong>  Stored in directory: /Users/chetan/Library/Caches/pip/wheels/e4/e1/92/68af4b20ac26182fbd623647af92118fc4cdbdb2c613030a67</strong>
<strong>Successfully built oauthlib</strong>
<strong>Installing collected packages: requests, oauthlib, requests-oauthlib, tweepy</strong>
<strong>Successfully installed oauthlib-2.0.0 requests-2.12.1 requests-oauthlib-0.7.0 tweepy-3.5.0</strong>
</pre></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec234"/>How to do it...</h2></div></div></div><p>OK nice, we're now armed with all the modules. So, let's get started by collecting data from Twitter. Twitter has this amazing set, Streaming APIs, which helps developers collect tweets in real time. We will use this library for our data collection needs too.</p><div><ol class="orderedlist arabic"><li class="listitem">The following code uses the Twitter Streaming APIs to collect data and store it in a text file with each tweet stored in the JSON format. We look for tweets that have two keywords, <code class="literal">iPhone 7</code> and <code class="literal">Note 5</code>. For this chapter, I ran the code for around 5 minutes, but for Judy we may have to run it for hours, or even days, to collect the maximum data to generate accurate insights:<pre class="programlisting">        from tweepy import Stream&#13;
        from tweepy import OAuthHandler&#13;
        from tweepy.streaming import StreamListener&#13;
        import json&#13;
    &#13;
        #consumer key, consumer secret, access token, access secret.&#13;
        ckey="&lt;&gt;"&#13;
        csecret="&lt;&gt;"&#13;
        atoken="&lt;&gt;"&#13;
        asecret="&lt;&gt;"&#13;
    &#13;
        tweets_data_path = 'twitter_data.txt'&#13;
        f = open(tweets_data_path, "w")&#13;
    &#13;
        class listener(StreamListener):&#13;
    &#13;
            def on_data(self, data):&#13;
                print data&#13;
                f.write(data)&#13;
                #all_data = json.loads(data)&#13;
                #tweet = all_data["text"]&#13;
                #lang = all_data["lang"]&#13;
                #username = all_data["user"]["screen_name"]&#13;
                #print "username:%s, tweet:%s, &#13;
                     language:%s" %(username, tweet, lang)&#13;
                return True&#13;
    &#13;
            def on_error(self, status):&#13;
                print "Error:", status&#13;
    &#13;
        auth = OAuthHandler(ckey, csecret)&#13;
        auth.set_access_token(atoken, asecret)&#13;
    &#13;
        twitterStream = Stream(auth, listener())&#13;
        twitterStream.filter(track=["iPhone 7","Note 5"])&#13;
        f.close()&#13;
</pre><p>Okay, now that we have the data from Twitter flowing in, let's write a code to analyze this data and see whether we can find any interesting stuff that can be shared with Judy for her article.</p></li><li class="listitem">Apple iPhone and Samsung Note are such popular products on a global level that people talk about these products from all around the world. It'd be really interesting to find the different languages used by consumers to talk about these products on Twitter. The following code does exactly what we wanted to do with Twitter data. It goes through the stored tweets, figures out the languages of all the tweets, and groups them to plot the top four languages:<pre class="programlisting">        import json&#13;
        import pandas as pd&#13;
        import matplotlib.pyplot as plt&#13;
        import seaborn as sns&#13;
    &#13;
        tweets = []&#13;
        fh = open("twitter_data.txt", "r")&#13;
        for data in fh:&#13;
            try:&#13;
                tweets.append(json.loads(data))&#13;
            except:&#13;
                continue&#13;
    &#13;
        tweet_df = pd.DataFrame()&#13;
        tweet_df['lang'] = map(lambda x: x['lang'], tweets)&#13;
        tweets_by_lang = tweet_df['lang'].value_counts()&#13;
    &#13;
        fig, axis = plt.subplots()&#13;
        sns.set_style("darkgrid")&#13;
        axis.set_xlabel('Languages', fontsize=15)&#13;
        axis.set_ylabel('Tweets' , fontsize=15)&#13;
        clrs = ['green', 'blue', 'red', 'black']&#13;
        sns_plot = tweets_by_lang[:4].plot(ax=axis, kind='bar', color=clrs)&#13;
        plt.savefig('language.pdf')&#13;
</pre><p>If we run the preceding code snippet, it plots the bar chart for the top languages people have used to tweet about iPhone 7 and Note 5.</p><p>
</p><div><img alt="How to do it..." src="img/image_10_014.jpg"/></div><p>
</p><p>Awesome! I think Judy will love this analysis. Even though the top language is English (<strong>en</strong>), as expected, it is quite interesting to see that the other three languages are Italian (<strong>it</strong>), Spanish (<strong>es</strong>), and Portuguese (<strong>pt</strong>). This will be a cool feed for her article.</p><p>The results that you will get from this exercise will depend on when you run the data collection program. For instance, if you run it for a short duration between 2 and 8 a.m. GMT time, you will see more tweets in Chinese or Japanese as it is daytime in these countries. By the way, wouldn't it be interesting to analyze what are the best times of the day when people tweet? You may find some correlation.</p></li><li class="listitem">Let's go further and do some more cool stuff with this data. How about performing text-based analysis on the tweets to get consumer sentiments about these products? By sentiment, I mean, is a tweet cursing these products, appreciating a product's feature, or just a passing comment? But wait; is it possible to get this kind of data? Yes, absolutely. The following code uses Python's NTLK-based APIs to perform sentiment analysis on tweets (text) to determine its polarity: positive, negative or neutral sentiment. It then groups this data to represent it with a bar chart and saves it in a PDF file:<pre class="programlisting">        import json &#13;
        import pandas as pd &#13;
        import matplotlib.pyplot as plt &#13;
        import seaborn as sns &#13;
        import requests &#13;
 &#13;
        tweets = [] &#13;
        fh = open("twitter_data.txt", "r") &#13;
        for data in fh: &#13;
            try: &#13;
                tweets.append(json.loads(data)) &#13;
            except: &#13;
                continue &#13;
 &#13;
        probablities = pd.DataFrame() &#13;
        prob = [] &#13;
        for tweet in tweets: &#13;
            text = tweet['text'] &#13;
            r = requests.post(url="http://text-processing.com/api/sentiment/", &#13;
                              data={"text":text},) &#13;
            print r.text &#13;
            if r.status_code == 200: &#13;
                ans = json.loads(r.text) &#13;
                prob.append(ans["label"]) &#13;
        probablities['data'] = map(lambda x: x, prob) &#13;
        p_df = probablities['data'].value_counts() &#13;
 &#13;
        fig, axis = plt.subplots() &#13;
        sns.set_style("darkgrid") &#13;
        axis.set_xlabel('Sentiments', fontsize=15) &#13;
        axis.set_ylabel('Tweets' , fontsize=15) &#13;
        clrs = ['green', 'yellow', 'red'] &#13;
        sns_plot = p_df.plot(ax=axis, kind='bar', color=clrs) &#13;
        plt.savefig('sentiments.pdf') &#13;
</pre><p>If you run the preceding piece of code, you will get a bar chart with the sentiment data for all the stored tweets. Just by looking at the graph, you will know that the consumers have generally spoken positively about both products:</p><p>
</p><div><img alt="How to do it..." src="img/image_10_015.jpg"/></div><p>
</p><p>There are a few complaints about them, hence the sentiments with negative polarity, but then there are neutral comments that could just be product referrals or comments on the products. Neat!</p></li></ol></div><p>While this is nice, don't you think Judy might be interested to know how many of these negative tweets are about the iPhone or Note? Well, I will leave that to you; I'm sure you will figure that out for Judy.</p></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec235"/>How it works...</h2></div></div></div><p>For the first problem, we first collected the data required for analysis. Twitter's Streaming APIs set helps us collect this information in real time. To use Streaming APIs, you need to register for a developer app with Twitter and collect your consumer key, consumer secret, auth token, and auth secret. It is a straightforward process and can be easily looked up on <a class="ulink" href="https://dev.twitter.com">https://dev.twitter.com</a>. So, in the data collection example (the first example in this recipe), we instantiated the <code class="literal">OAuthHandler</code> class to get Twitter's authorization object, <code class="literal">auth</code>, and then used it to set the authorization token and secret with the <code class="literal">set_access_token()</code> method. The <code class="literal">Stream</code> class of Twitter's Streaming APIs is bound to the <code class="literal">listener</code> class and returns the <code class="literal">twitterStream</code> object. The <code class="literal">listener</code> class inherits the <code class="literal">StreamListener</code> class, which will monitor incoming tweets and take actions on the arriving tweets in the <code class="literal">on_data()</code> method. The filtering of tweets is done with the <code class="literal">twitterStream.filter()</code> method.</p><p>Now, we know that the incoming tweets are available in the <code class="literal">on_data()</code> method; we hooked on to it to store the tweets in the <code class="literal">twitter_data.txt</code> file. For this, we opened the file in the write (<code class="literal">w</code>) mode and used the <code class="literal">write()</code> method to write the tweet to the file in the JSON format. With this, we finished the first recipe and collected the data required by Judy. Now, it's time to perform the analysis.</p><p>For the first insight on getting the languages, we started by opening the <code class="literal">twitter_data.txt</code> file in the read(<code class="literal">r</code>) mode. We read through all the tweets (the JSON format) and appended them to the <code class="literal">tweets</code> array. Using <code class="literal">pandas</code>, we created an empty DataFrame object, <code class="literal">tweet_df</code>, with <code class="literal">pd.DataFrame()</code>. With Python's <code class="literal">map()</code> method, we operated on the <code class="literal">tweets</code> array and added a new column, <code class="literal">lang</code>, to our empty DataFrame. The <code class="literal">value_counts()</code> method was then used to get the count of all the languages of the tweets under analysis and was stored in the variable, <code class="literal">tweets_by_lang</code>.</p><p>The other part of the code is as usual, where we created a <code class="literal">plt</code> object from matplotlib and used the <code class="literal">plot()</code> method to generate a bar chart using the <code class="literal">seaborn</code> library. We set the axis labels with the <code class="literal">set_xlabel()</code> and <code class="literal">set_ylabel()</code> methods and used the colors <code class="literal">green</code>, <code class="literal">blue</code>, <code class="literal">red</code>, and <code class="literal">black</code> to manifest the different languages. Finally, we saved the plot in a PDF file with the <code class="literal">savefig()</code> method.</p><p>For the second insight involving sentiment analysis, we started by reading through all the tweets from <code class="literal">twitter_data.txt</code> and storing them to the <code class="literal">tweets</code> array. We then created an empty data frame, probabilities, processed all the tweets for sentiment analysis, and stored the analysis in the <code class="literal">prob</code> array. We then added a column, <code class="literal">text</code>, to our empty data frame using the <code class="literal">map()</code> method on the <code class="literal">prob</code> array.</p><p>Our data frame, <code class="literal">probabilities['text']</code>, now contains sentiments for all the tweets we analyzed. Following the regular set of operations, we got the set of values for <code class="literal">positive</code>, <code class="literal">negative</code>, and <code class="literal">neutral</code> sentiments for the analyzed tweets and plotted them as a bar chart.</p><p>If you look at all the examples in this recipe, we have divided the task of data collection and analysis as separate programs. If our visualizations are massive, we can even separate them out. This makes sure that Judy can use the data collection Python program to gather information on another set of keywords for her articles in the future.</p><p>She can also run the analysis on the data sets she has by making small changes to the analysis and visualization parameters from our program. So, for all her articles in the future, we have managed to automate the data collection, analysis, and visualization process for her.</p><p>I'm already seeing a smile on Judy's face. I hope you enjoyed this chapter. I'm confident that the knowledge you gained in this chapter will get you started into the world of data and visualizations.</p></div><div><div><div><div><h2 class="title"><a id="ch10lvl2sec236"/>There's more...</h2></div></div></div><p>We have just scratched the surface of text-based sentiment analysis in the preceding recipe. Sentiment analysis involves text classifications, tokenization, semantic reasoning, and much more interesting stuff. I highly encourage you to go through a book on NLTK to learn more about working with text in Python at <a class="ulink" href="http://www.nltk.org/book/">http://www.nltk.org/book/</a>.</p></div></div></body></html>