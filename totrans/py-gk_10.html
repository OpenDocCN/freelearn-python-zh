<html><head></head><body>
		<div><h1 id="_idParaDest-169"><em class="italic"><a id="_idTextAnchor207"/>Chapter 7</em>: Multiprocessing, Multithreading, and Asynchronous Programming</h1>
			<p>We can write efficient and optimized code for faster execution time, but there is always a limit to the amount of resources available for the processes running our programs. However, we can still improve application execution time by executing certain tasks in parallel on the same machine or across different machines. This chapter will cover parallel processing or concurrency in Python for the applications running on a single machine. We will cover parallel processing using multiple machines in the next chapter. In this chapter, we focus on the built-in support available in Python for the implementation of parallel processing. We will start with the multithreading in Python followed by discussing the multiprocessing. After that, we will discuss how we can design responsive systems using asynchronous programming. For each of the approaches, we will design and discuss a case study of implementing a concurrent application to download files from a Google Drive directory.</p>
			<p>We will cover the following topics in this chapter:</p>
			<ul>
				<li>Understanding multithreading in Python and its limitations</li>
				<li>Going beyond a single CPU – implementing multiprocessing</li>
				<li>Using asynchronous programming for responsive systems</li>
			</ul>
			<p>After completing this chapter, you will be aware of the different options for building multithreaded or multiprocessing applications using built-in Python libraries. These skills will help you to build not only more efficient applications but also build applications for large-scale users.</p>
			<h1 id="_idParaDest-170"><a id="_idTextAnchor208"/>Technical requirements</h1>
			<p>The following are the technical requirements for this chapter:</p>
			<ul>
				<li>Python 3 (3.7 or later)</li>
				<li>A Google Drive account</li>
				<li>API key enabled for your Google Drive account</li>
			</ul>
			<p>Sample code for this chapter can be found at <a href="https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter07">https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter07</a>.</p>
			<p>We will start our discussion with multithreading concepts in Python.</p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor209"/>Understanding multithreading in Python and its limitations</h1>
			<p>A thread is a basic unit of <a id="_idIndexMarker736"/>execution within an operating system process, and it consists of its own program counter, a stack, and a set of registers. An application process can be built using multiple threads that can run simultaneously and share the same memory.</p>
			<p>For multithreading in a program, all the threads of a process share common code and other resources, such as data and system files. For each thread, all its related information is stored as a data structure inside the operating system kernel, and this data structure<a id="_idIndexMarker737"/> is called the <strong class="bold">Thread Control Block</strong> (<strong class="bold">TCB</strong>). The TCB has the following main components:</p>
			<ul>
				<li><strong class="bold">Program Counter (PC)</strong>: This is used to <a id="_idIndexMarker738"/>track the execution flow of the program.</li>
				<li><strong class="bold">System Registers (REG)</strong>: These<a id="_idIndexMarker739"/> registers are used to hold variable data.</li>
				<li><strong class="bold">Stack</strong>: The stack is an <a id="_idIndexMarker740"/>array of registers that manages the execution history.</li>
			</ul>
			<p>The anatomy of a thread is exhibited in <em class="italic">Figure 7.1</em>, with three threads. Each thread has its own PC, a stack, and REG, but shares code and other resources with other threads:</p>
			<div><div><img src="img/B17189_07_01.jpg" alt="Figure 7.1 – Multiple threads in a process&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – Multiple threads in a process</p>
			<p>The TCB also contains a thread identifier, the state of the thread (such as running, waiting, or stopped), and a pointer to the process it belongs to. Multithreading is an operating system concept. It is a feature offered through the system kernel. The operating system facilitates the execution of multiple threads concurrently in the same process context, allowing them to share the process memory. This means the operating system has full control of which thread will be activated, rather than the application. We need to underline this point for a later discussion comparing different concurrency options.</p>
			<p>When threads are run on <a id="_idIndexMarker741"/>a single-CPU machine, the operating system actually switches the CPU from one thread to the other such that the threads appear to be running concurrently. Is there any advantage to running multiple threads on a single-CPU machine? The answer is yes and no, and it depends on the nature of the application. For applications running using only the local memory, there may not be any advantage; in fact, it is likely to exhibit lower performance due to the overhead of switching threads on a single CPU. But for applications that depend on other resources, the execution can be faster because of the better utilization of the CPU: when one thread is waiting for another resource, another thread can utilize the CPU.</p>
			<p>When executing multiple threads on multiprocessors or multiple CPU cores, it is possible to execute them concurrently. Next, we will discuss the limitations of multithreaded programming in Python.</p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor210"/>What is a Python blind spot?</h2>
			<p>From a programming <a id="_idIndexMarker742"/>perspective, multithreading is an approach to running different parts of an application concurrently. Python uses multiple kernel threads that can run the Python user threads. But the Python implementation (<em class="italic">CPython</em>) allows threads to access the Python objects through one global lock, which is called the <strong class="bold">Global Interpreter Lock (GIL)</strong>. In simple words, the GIL is a mutex that allows only one thread to use the Python<a id="_idIndexMarker743"/> interpreter at a time and blocks all other threads. This is necessary to protect the reference count that is managed for each object in Python from garbage collection. Without such protection, the reference count can get corrupted if it's updated by multiple threads at the same time. The reason for this limitation is to protect the internal interpreter data structures and third-party <em class="italic">C</em> code that is not thread safe.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">This GIL limitation does not exist in Jython and IronPython, which are other implementations of Python.</p>
			<p>This Python limitation may give us the impression that there is no advantage to writing multithreaded programs in Python. This is not true. We still can write code in Python that runs concurrently or in parallel, and we will see it in our case study. Multithreading can be beneficial in the following cases:</p>
			<ul>
				<li><strong class="bold">I/O bound tasks</strong>: When <a id="_idIndexMarker744"/>working with multiple I/O operations, there is always room to improve performance by running tasks using more than one thread. When one thread is waiting for a response from an I/O resource, it will release the GIL and let the other threads work. The original thread will wake up as soon as the response arrives from the I/O resource.</li>
				<li><strong class="bold">Responsive GUI application</strong>: For <a id="_idIndexMarker745"/>interactive GUI applications, it is necessary to have a design pattern to display the progress of tasks running in the background (for example, downloading a file) and also to allow a user to work on other GUI features while one or more tasks are running in the background. This is all possible by using separate threads for the actions initiated by a user through the GUI.</li>
				<li><strong class="bold">Multiuser applications</strong>: Threads are also a prerequisite for building multiuser applications. A web server and a<a id="_idIndexMarker746"/> file server are examples of such applications. As soon as a new request arrives in the main thread of such an application, a new thread is created to serve the request while the main thread at the back listens for a new request.</li>
			</ul>
			<p>Before discussing a case study of a multithreaded application, it is important to introduce the key components of multithreaded programming in Python.</p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor211"/>Learning the key components of multithreaded programming in Python</h2>
			<p>Multithreading in<a id="_idIndexMarker747"/> Python allows us to run different components of a program concurrently. To create multiple threads of an application, we will use the Python <code>threading</code> module, and the main components of this module are described next.   </p>
			<p>We will start by discussing the <code>threading</code> module in Python first.</p>
			<h3>The threading module</h3>
			<p>The <code>threading</code> module comes as a <a id="_idIndexMarker748"/>standard module and provides simple and easy-to-use methods for building multiple threads of a program. Under the hood, this module uses the lower level <code>_thread</code> module, which was a popular choice of multithreading in the early version of Python.</p>
			<p>To create a new thread, we will create an object of the <code>Thread</code> class that can take a function (to be executed) name as the <code>target</code> attribute and arguments to be passed to the function as the <code>args</code> attribute. A thread can be given a name that can be set at the time it is created using the <code>name</code> argument with the constructor.</p>
			<p>After creating an object of the <code>Thread</code> class, we need to start the thread by using the <code>start</code> method. To make the main program or thread wait until the newly created thread object(s) finishes, we<a id="_idIndexMarker749"/> need to use the <code>join</code> method. The <code>join</code> method makes sure that the main thread (a calling thread) waits until the thread on which the <code>join</code> method is called completes its execution.</p>
			<p>To explain the process of creating, starting, and waiting to finish the execution of a thread, we will create a simple program with three threads. A complete code example of such a program is shown next:</p>
			<pre># <strong class="bold">thread1</strong>.py to create simple threads with function
from threading import current_thread, Thread as Thread
from time import sleep
def <strong class="bold">print_hello</strong>():
    sleep(2)
    print("{}: Hello".format(current_thread().name))
def <strong class="bold">print_message</strong>(msg):
    sleep(1)
    print("{}: {}".format(current_thread().name, msg))
# create threads
t1 = <strong class="bold">Thread(target=print_hello, name="Th 1")</strong>
t2 = Thread(target=print_hello, name="Th 2")
t3 = <strong class="bold">Thread(target=print_message, args=["Good morning"], </strong>
<strong class="bold">        name="Th 3")</strong>
# start the threads
<strong class="bold">t1.start</strong>()
t2.start()
t3.start()
# wait till all are done
<strong class="bold">t1.join</strong>()
t2.join()
t3.join()</pre>
			<p>In this program, we implemented the following:</p>
			<ul>
				<li>We created two simple functions, <code>print_hello</code> and <code>print_message</code>, that are to be used by the<a id="_idIndexMarker750"/> threads. We used the <code>sleep</code> function from the <code>time</code> module in both functions to make sure that the two functions finish their execution time at different times.</li>
				<li>We created three <code>Thread</code> objects. Two of the three objects will execute one function (<code>print_hello</code>) to illustrate the code sharing by the threads, and the third thread object will use the second function (<code>print_message</code>), which takes one argument as well.</li>
				<li>We started all three threads one by one using the <code>start</code> method.</li>
				<li>We waited for each thread to finish by using the <code>join</code> method.</li>
			</ul>
			<p>The <code>Thread</code> objects can be stored in a list to simplify the <code>start</code> and <code>join</code> operations using a <code>for</code> loop. The console output of this program will look like this:</p>
			<pre>Th 3: Good morning
Th 2: Hello
Th 1: Hello</pre>
			<p>Thread 1 and thread 2 have more sleep time than thread 3, so thread 3 will always finish first. Thread 1 and thread 2 can finish in any order depending on who gets hold of the processor first.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">By default, the <code>join</code> method blocks the caller thread indefinitely. But we can use a timeout (in seconds) as an argument to the <code>join</code> method. This will make the caller thread block only for the timeout period.</p>
			<p>We will review a<a id="_idIndexMarker751"/> few more concepts before discussing a more complex case study.</p>
			<h3>Daemon threads</h3>
			<p>In a normal application, our<a id="_idIndexMarker752"/> main program implicitly waits until all other threads finish their execution. However, sometimes we need to run some threads in the background so that they run without blocking the main program from terminating itself. These threads are known as <strong class="bold">daemon threads</strong>. These<a id="_idIndexMarker753"/> threads stay active as long as the main program (with non-daemon threads) is running, and it is fine to terminate the daemon threads once the non-daemon threads exit. The use of daemon threads is popular in situations where it is not an issue if a thread dies in the middle of its execution without losing or corrupting any data.</p>
			<p>A thread can be declared a daemon thread by using one of the following two approaches:</p>
			<ul>
				<li>Pass the <code>daemon</code> attribute set to <code>True</code> with the constructor (<code>daemon = True</code>).</li>
				<li>Set the <code>daemon</code> attribute to <code>True</code> on the thread instance (<code>thread.daemon = True</code>).</li>
			</ul>
			<p>If a thread is set as a daemon thread, we start the thread and forget about it. The thread will be automatically killed when the program that called it quits.</p>
			<p>The next code shows the use of both <a id="_idIndexMarker754"/>daemon and non-daemon threads:</p>
			<pre>#<strong class="bold">thread2</strong>.py to create daemon and non-daemon threads
from threading import current_thread, Thread as Thread
from time import sleep
def <strong class="bold">daeom_func</strong>():
    #print(threading.current_thread().isDaemon())
    sleep(3)
    print("{}: Hello from daemon".format           (current_thread().name))
def <strong class="bold">nondaeom_func</strong>():
    #print(threading.current_thread().isDaemon())
    sleep(1)
    print("{}: Hello from non-daemon".format(        current_thread().name))
#creating threads
t1 = Thread(target=daeom_func, name="Daemon Thread",     <strong class="bold">daemon=True</strong>)
t2 = Thread(target=nondaeom_func, name="Non-Daemon Thread")
# start the threads
t1.start()
t2.start()
print("Exiting the main program")</pre>
			<p>In this code example, we created one daemon and one non-daemon thread. The daemon thread (<code>daeom_func</code>) is executing a function that has a sleep time of <code>3</code> seconds, whereas the non-daemon thread is executing a function (<code>nondaeom_func</code>) that has a sleep time of 1 second. The sleep<a id="_idIndexMarker755"/> time of the two functions is set to make sure the non-daemon thread finishes its execution first. The console output of this program is as follows:</p>
			<pre>Exiting the main program
Non-Daemon Thread: Hello from non-daemon </pre>
			<p>Since we did not use a <code>join</code> method in any thread, the main thread exits first, and then the non-daemon thread finishes a bit later with a print message. But there is no print message from the daemon thread. This is because the daemon thread is terminated as soon as the non-daemon thread finishes its execution. If we change the sleep time in the <code>nondaeom_func</code> function to <code>5</code>, the console output will be as follows:</p>
			<pre>Exiting the main program
Daemon Thread: Hello from daemon
Non-Daemon Thread: Hello from non-daemon</pre>
			<p>By delaying the execution of the non-daemon thread, we make sure the daemon thread finished its execution and does not get terminated abruptly.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">If we use a <code>join</code> on the daemon thread, the main thread will be forced to wait for the daemon thread to finish its execution.</p>
			<p>Next, we will investigate how to synchronize the threads in Python.</p>
			<h3>Synchronizing threads</h3>
			<p><strong class="bold">Thread synchronization</strong> is a mechanism to ensure that the two or more threads do not execute a shared block<a id="_idIndexMarker756"/> of code at the same time. The block of code that is typically accessing shared data or shared<a id="_idIndexMarker757"/> resources is also known as the <strong class="bold">critical section</strong>. This concept can be made clearer through the following figure:</p>
			<p class="figure-caption">   </p>
			<div><div><img src="img/B17189_07_02.jpg" alt="Figure 7.2 – Two threads accessing a critical section of a program&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – Two threads accessing a critical section of a program</p>
			<p>Multiple threads accessing the critical section at the same time may try to access or change the data at the same <a id="_idIndexMarker758"/>time, which may result in unpredictable results on the data. This situation is called a <strong class="bold">race condition</strong>.</p>
			<p>To illustrate the concept of the race condition, we will implement a simple program with two threads, and each thread increments a shared variable 1 million times. We chose a high number for the increment to make sure that we can observe the outcome of the race condition. The race condition may also be observed by using a lower value for the increment cycle on a slower CPU. In this program, we will create two threads that are using the same function (<code>inc</code> in this case) as the target. The code for accessing the shared variable and incrementing it by 1 occurs in the critical section, and the two threads are accessing it without any protection. The complete code example is as follows:</p>
			<pre># <strong class="bold">thread3a</strong>.py when no thread synchronization used
from threading import Thread as Thread
def inc():
    global x
    for _ in range(<strong class="bold">1000000</strong>):
        <strong class="bold">x+=1</strong>
#global variabale
x = 0
# creating threads
t1 = Thread(target=inc, name="Th 1")
t2 = Thread(target=inc, name="Th 2")
# start the threads
t1.start()
t2.start()
#wait for the threads
t1.join()
t2.join()
print("final value of x :", x)</pre>
			<p>The expected value of <code>x</code> at the end <a id="_idIndexMarker759"/>of the execution is <em class="italic">2,000,000</em>, which will not be observed in the console output. Every time we execute this program, we will get a different value of <code>x</code> that's a lot lower than 2,000,000. This is because of the race condition between the two threads. Let's look at a scenario where threads <code>Th 1</code> and <code>Th 2</code> are running the critical section (<code>x+=1</code>) at the same time. Both threads will ask for the current value of <code>x</code>. If we assume the current value of <code>x</code> is <code>100</code>, both threads will read it as <code>100</code> and increment it to a new value of <code>101</code>. The two threads will write back to the memory the new value of <code>101</code>. This is a one-time increment and, in reality, the two threads should increment the variable independently of each other and the final value of <code>x</code> should be <code>102</code>. How can we achieve this? This is where thread synchronization comes to the rescue.</p>
			<p>Thread synchronization can be achieved by using a <code>Lock</code> class from the <code>threading</code> module. The lock is<a id="_idIndexMarker760"/> implemented using a <code>Lock</code> class provides two methods, <code>acquire</code> and <code>release</code>, which are described next:</p>
			<ul>
				<li>The <code>acquire</code> method is used to<a id="_idIndexMarker762"/> acquire a lock. A lock can be <code>unlocked</code>), then the lock is provided to the requesting thread to proceed. In the case of a non-blocking acquire request, the thread execution is not blocked. If the lock is available (<code>unlocked</code>), then the lock is provided (and <code>locked</code>) to the requesting thread to proceed, otherwise the requesting thread gets <code>False</code> as a response.</li>
				<li>The <code>release</code> method is used to release a lock, which means it resets the lock to an <code>unlocked</code> state. If there is any thread blocking and waiting for the lock, it will allow one of the threads to proceed.</li>
			</ul>
			<p>The <code>thread3a.py</code> code example is revised with the use of a lock around the increment statement on the shared variable <code>x</code>. In this revised example, we created a lock at the main thread level and then passed it to the <code>inc</code> function to acquire and release a lock around the shared variable. The complete revised code example is as follows:</p>
			<pre># <strong class="bold">thread3b</strong>.py when thread synchronization is used
from threading import Lock, Thread as Thread
def inc_with_lock (lock):
    global x
    for _ in range(1000000):
        <strong class="bold">lock.acquire()</strong>
        x+=1
        <strong class="bold">lock.release()</strong>
x = 0
<strong class="bold">mylock = Lock()</strong>
# creating threads
t1 = Thread(target= inc_with_lock, args=(<strong class="bold">mylock</strong>,), name="Th     1")
t2 = Thread(target= inc_with_lock, args=(<strong class="bold">mylock</strong>,), name="Th     2")
# start the threads
t1.start()
t2.start()
#wait for the threads
t1.join()
t2.join()
print("final value of x :", x)</pre>
			<p>After using the <code>Lock</code> object, the value of <code>x</code> is always <code>2000000</code>. The <code>Lock</code> object made sure that only one<a id="_idIndexMarker764"/> thread increments the shared variable at a time. The advantage of thread synchronization is that you can use system resources with enhanced performance and predictable results.</p>
			<p>However, locks have to be used carefully because improper use of locks can result in a deadlock situation. Suppose a thread acquires a lock on resource A and is waiting to acquire a lock on resource B. But another thread already holds a lock on resource B and is looking to acquire a lock resource A. The two threads will wait for each other to release the locks, but it will never happen. To avoid deadlock situations, the multithreading and multiprocessing libraries come with mechanisms such as adding a timeout for a resource to hold a lock, or using a context manager to acquire locks. </p>
			<h3>Using a synchronized queue</h3>
			<p>The <code>Queue</code> module in <a id="_idIndexMarker765"/>Python implements multi-producer and multi-consumer queues. Queues are very useful in multithread applications when the information has to be exchanged between different threads safely. The beauty of the synchronized queue is that they come with all the required locking mechanisms, and there is no need to use additional locking semantics.</p>
			<p>There are three types of queues in the <code>Queue</code> module:</p>
			<ul>
				<li><strong class="bold">FIFO</strong>: In the FIFO queue, the task <a id="_idIndexMarker766"/>added first is retrieved first.</li>
				<li><strong class="bold">LIFO</strong>: In the LIFO queue, the<a id="_idIndexMarker767"/> last task added is retrieved first.</li>
				<li><strong class="bold">Priority queue</strong>: In this queue, the entries are <a id="_idIndexMarker768"/>sorted and the entry with the lowest value is retrieved first.</li>
			</ul>
			<p>These queues use locks to protect access to the queue entries from competing threads. The use of a queue with a multithreaded program is best illustrated with a code example. In the next example, we will create a FIFO queue with dummy tasks in it. To process the tasks from the queue, we will implement a custom thread class by inheriting the <code>Thread</code> class. This is another way of implementing a thread.</p>
			<p>To implement a custom thread class, we need to override the <code>init</code> and <code>run</code> methods. In the <code>init</code> method, it is required to call the <code>init</code> method of the superclass (the <code>Thread</code> class). The <code>run</code> method is the execution part of the thread class. The complete code example is as follows:</p>
			<pre># <strong class="bold">thread5</strong>.py with queue and custom Thread class
from queue import Queue
from threading import Thread as Thread
from time import sleep
class <strong class="bold">MyWorker (Thread)</strong>:
   def __init__(self, name, q):
      <strong class="bold">threading.Thread.__init__(self)</strong>
      self.name = name
      self.queue = q
   def <strong class="bold">run</strong>(self):
      while True:
          item = self.queue.get()
          sleep(1)
          try:
              print ("{}: {}".format(self.name, item))
          finally:
            self.queue.task_done()
#filling the queue
<strong class="bold">myqueue = Queue()</strong>
for i in range (10):
    <strong class="bold">myqueue.put</strong>("Task {}".format(i+1))
# creating threads
for i in range (5):
    worker = MyWorker("Th {}".format(i+1), myqueue)
    worker.daemon = True
    worker.start()
myqueue.join()</pre>
			<p>In this code example, we <a id="_idIndexMarker769"/>created five worker threads using the custom thread class (<code>MyThread</code>). These five worker threads access the queue to get the task item from it. After getting the task item, the threads sleep for 1 second and then print the thread name and the task name. For each <code>get</code> call for an item of a queue, a subsequent call of <code>task_done()</code> indicates that the processing of the task has been completed.</p>
			<p>It is important to note that we used the <code>join</code> method on the <code>myqueue</code> object and not on the threads. The <code>join</code> method on the queue blocks the main thread until all items in the queue have been processed and completed (<code>task_done</code> is called for them). This is a recommended way to block the main thread when a queue object is used to hold the tasks' data for threads.</p>
			<p>Next, we will implement an application to download files from Google Drive using the <code>Thread</code> class, the <code>Queue</code> class, and a couple of third-party libraries.</p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor212"/>Case study – a multithreaded application to download files from Google Drive</h2>
			<p>We have discussed in the previous<a id="_idIndexMarker770"/> section that multithreaded applications in Python stand out well when different threads are working on input and output tasks. That is why we selected to implement an application that downloads files from a shared directory of Google Drive. To implement this application, we will need the following:</p>
			<ul>
				<li><strong class="bold">Google Drive</strong>: A Google Drive account (a free basic account is fine) with one directory marked as shared.</li>
				<li><strong class="bold">API key</strong>: An API key to access Google APIs is required. The API key needs to be enabled to use the Google APIs for Google Drive. The API can be enabled by following the guidelines on the Google Developers site (<a href="https://developers.google.com/drive/api/v3/enable-drive-api">https://developers.google.com/drive/api/v3/enable-drive-api</a>).</li>
				<li><code>pip</code> tool.</li>
				<li><code>pip</code> tool as well. There are other libraries available that offer the same functionality. We selected the <code>gdown</code> library for its ease of use.</li>
			</ul>
			<p>To use the <code>getfilelistpy</code> module, we need to create a resource data structure. This data structure will include a folder identifier as <code>id</code> (this will be Google Drive folder ID in our case), the API security key (<code>api_key</code>) for accessing the Google Drive folder, and a list of file attributes (<code>fields</code>) to be fetched when we get a list of files. We build the resource data structure as follows:</p>
			<pre>resource = {
    "api_key": "AIzaSyDYKmm85kebxddKrGns4z0",
    "id": "0B8TxHW2Ci6dbckVwTRtTl3RUU",
    "fields": "files(name, id, webContentLink)",
}
'''API key and id used in the examples are not original, so should be replaced as per your account and shared directory id''' </pre>
			<p>We limit the file attributes to the <code>file id</code>, <code>name</code>, and its <code>web link</code> (URL) only. Next, we need to add each file item into a queue as a task for threads. The queue will be used by multiple worker threads to download the files in parallel.</p>
			<p>To make the application more flexible in terms of the number of workers we can use, we build a pool of worker threads. The size of the pool is controlled by a global variable that is set at the beginning of the program. We created worker threads as per the size of the thread pool. Each worker thread in the pool has access to the queue, which has a list of files. Like the previous code<a id="_idIndexMarker771"/> example, each worker thread will take one file item from the queue at a time, download the file, and mark the file item as complete using the <code>task_done</code> method. An example code for defining a resource data structure and for defining a class for the worker thread is as follows:</p>
			<pre>#threads_casestudy.py
from queue import Queue
from threading import Thread
import time
from getfilelistpy import getfilelist
import gdown
THREAD_POOL_SIZE = 1
resource = {
    "api_key": "AIzaSyDYKmm85kea2bxddKrGns4z0",
    "id": "0B8TxHW2Ci6dbckVweTRtTl3RUU ",
    "fields": "files(name,id,webContentLink)",
}
class DownlaodWorker(Thread):
    def __init__(self, name, queue):
        Thread.__init__(self)
        self.name = name
        self.queue = queue
    def run(self):
        while True:
            # Get the file id and name from the queue
            item1 = self.queue.get()
            try:
                <strong class="bold">gdown.download</strong>( item1['webContentLink'], 
                    './files/{}'.format(item1['name']), 
                    quiet=False)
            finally:
                self.queue.task_done()</pre>
			<p>We get the files' metadata from a Google Drive directory using the resource data structure as follows:</p>
			<pre>def get_files(resource):
        #global files_list
        res = getfilelist.GetFileList(resource)
        files_list = res['fileList'][0]
        return files_list</pre>
			<p>In the <code>main</code> function, we create a <code>Queue</code> object to insert file metadata into the queue. The <code>Queue</code> object is handed <a id="_idIndexMarker772"/>over to a pool of worker threads for downloading the files. The worker threads will download the files, as discussed earlier. We use the <code>time</code> class to measure the time it takes to complete the download of all the files from the Google Drive directory. The code for the <code>main</code> function is as follows:</p>
			<pre>def main():
    start_time = time.monotonic()
    files = <strong class="bold">get_files(resource)</strong>
    #add files info into the queue
    queue = Queue()
    for item in files['files']:
        queue.put(item)
    for i in range (THREAD_POOL_SIZE):
        worker = DownlaodWorker("Thread {}".format(i+1), 
                queue)
        worker.daemon = True
        worker.start()
    queue.join()
    end_time = time.monotonic()
    print('Time taken to download: {} seconds'.
          format( end_time - start_time))
main()</pre>
			<p>For this application, we have 10 files in the Google Drive directory, varying in size from 500 KB to 3 MB. We ran the application with 1, 5, and 10 worker threads. The total time taken to download the 10 files with 1 thread was approximately 20 seconds. This is almost equivalent to <a id="_idIndexMarker773"/>writing a code without any threads. In fact, we have written a code to download the same files without any threads and made it available with this book's source code as an example. The time it took to download 10 files with a non-threaded application was approximately 19 seconds.</p>
			<p>When we changed the number of worker threads to 5, the time taken to download the 10 files reduced significantly to approximately 6 seconds on our MacBook machine (Intel Core i5 with 16 GB RAM). If you run the same program on your computer, the time may be different, but there will definitely be an improvement if we increase the number of worker threads. With 10 threads, we observed the execution time to be around 4 seconds. This observation shows that there is an improvement in the execution time for I/O bound tasks by using multithreading regardless of the GIL limitation it has.</p>
			<p>This concludes our <a id="_idIndexMarker774"/>discussion of how to implement threads in Python and how to benefit from different locking mechanisms using the <code>Lock</code> class and the <code>Queue</code> class. Next, we will discuss multiprocessing programming in Python.</p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor213"/>Going beyond a single CPU – implementing multiprocessing</h1>
			<p>We have seen the complexity of multithreaded programming and its limitations. The question is whether the complexity of multithreading is worth the effort. It may be worth it for I/O-related tasks but not for general application use cases, especially when an alternative approach exists. The alternative approach is to use multiprocessing because separate Python processes are not constrained by the GIL and execution can happen in parallel. This is especially beneficial when applications run on multicore processors and involve intensive CPU-demanding tasks. In reality, the use of multiprocessing is the only option in Python's built-in libraries to utilize multiple processor cores.</p>
			<p><strong class="bold">Graphics Processing Units</strong> (<strong class="bold">GPUs</strong>) provide a <a id="_idIndexMarker775"/>greater number of cores than regular CPUs and are considered more suitable for data processing tasks, especially when executing them in parallel. The only caveat is that in order to execute a data processing program on a GPU, we have to transfer the data from the main memory to the GPU's memory. This additional step of data transfer will be compensated when we are processing a large dataset. But there will be little or no benefit if our dataset is small. Using GPUs for big data processing, especially for training machine learning models, is becoming a popular option. NVIDIA has introduced a GPU for parallel processing called CUDA, which is well supported through external libraries in Python.</p>
			<p>Each process has a data <a id="_idIndexMarker776"/>structure called the <strong class="bold">Process Control Block</strong> (<strong class="bold">PCB</strong>) at the operating system level. Like the TCB, the PCB has a <strong class="bold">Process ID</strong> (<strong class="bold">PID</strong>) for process<a id="_idIndexMarker777"/> identification, stores the state of the process (such as running or waiting), and has a program counter, CPU registers, CPU scheduling information, and many more attributes.</p>
			<p>In the case of multiple processes for CPUs, there is no sharing of memory natively. This means there is a lower chance of data corruption. If the two processes have to share the data, they need to use some interprocess communication mechanism. Python supports interprocess communication through its primitives. In the next subsections, we will first discuss the fundamentals of creating processes in Python and then discuss how to achieve interprocess communication.</p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor214"/>Creating multiple processes </h2>
			<p>For multiprocessing programming, Python provides a <code>multiprocessing</code> package that is very similar to the <a id="_idIndexMarker778"/>multithreading package. The <code>multiprocessing</code> package includes two approaches to implement multiprocessing, which are using the <code>Process</code> object and the <code>Pool</code> object. We will discuss each of these approaches one by one.</p>
			<h3>Using the Process object</h3>
			<p>The processes <a id="_idIndexMarker779"/>can be spawned by creating a <code>Process</code> object and then using its <code>start</code> method similar to the <code>start</code> method for starting a <code>Thread</code> object. In fact, the <code>Process</code> object offers the same API as the <code>Thread</code> object. A simple code example for creating multiple child processes is as follows:</p>
			<pre># <strong class="bold">process1</strong>.py to create simple processes with function
import os
from multiprocessing import Process, current_process as cp
from time import sleep
def <strong class="bold">print_hello</strong>():
    sleep(2)
    print("{}-{}: Hello".format(os.getpid(), cp().name))
def <strong class="bold">print_message</strong>(msg):
    sleep(1)
    print("{}-{}: {}".format(os.getpid(), cp().name, msg))
def main():
    processes = []
    # creating process
    processes.append(<strong class="bold">Process</strong>(target=print_hello, name="Process       1"))
    processes.append(<strong class="bold">Process</strong>(target=print_hello, name="Process       2"))
    processes.append(<strong class="bold">Process</strong>(target=print_message,      args=["Good morning"], name="Process 3"))
    # start the process
    for p in processes:
        <strong class="bold">p.start</strong>()
    # wait till all are done
    for p in processes:
        <strong class="bold">p.join()</strong>
    print("Exiting the main process")
if __name__ == '__main__':
    main()</pre>
			<p>As already mentioned, the methods used for the <code>Process</code> object are pretty much the same as those used for the <code>Thread</code> object. The explanation of this example is the same as for the example code in the <a id="_idIndexMarker780"/>multithreading code examples.</p>
			<h3>Using the Pool object</h3>
			<p>The <code>Pool</code> object offers a <a id="_idIndexMarker781"/>convenient way (using its <code>map</code> method) of creating processes, assigning functions to each new process, and distributing input parameters across the processes. We selected the code example with a pool size of <code>3</code> but provided input parameters for five processes. The reason for setting the pool size to <code>3</code> is to make sure a maximum of three child processes are active at a time, regardless of how many parameters we pass with the <code>map</code> method of the <code>Pool</code> object. The additional parameters will be handed over to the same child processes as soon they finish their current execution. Here is a code example with a pool size of <code>3</code>:</p>
			<pre># <strong class="bold">process2</strong>.py to create processes using a pool
import os
from multiprocessing import Process, Pool, current_process     as cp
from time import sleep
def <strong class="bold">print_message</strong>(msg):
    sleep(1)
    print("{}-{}: {}".format(os.getpid(), cp().name, msg))
def main():
    # creating process from a pool
    with <strong class="bold">Pool</strong>(3) as proc:
        <strong class="bold">proc.map</strong>(print_message, ["Orange", "Apple", "Banana",
                                 "Grapes","Pears"])
    print("Exiting the main process")
if __name__ == '__main__':
    main()</pre>
			<p>The magic of distributing input parameters to a function that is tied to a set of pool processes is done by the <code>map</code> method. The <code>map</code> method waits until all functions complete their execution, and that is why there is no need to use a <code>join</code> method if the processes are created using the <code>Pool</code> object.</p>
			<p>A few differences between using the <code>Process</code> object versus the <code>Pool</code> object are shown in the following table:</p>
			<div><div><img src="img/B17189_07_Table_1.jpg" alt="Table 7.1 – Comparison of using the Pool object and the Process object&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 7.1 – Comparison of using the Pool object and the Process object</p>
			<p>Next, we will discuss how to<a id="_idIndexMarker782"/> exchange data between processes. </p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor215"/>Sharing data between processes</h2>
			<p>There are two<a id="_idIndexMarker783"/> approaches in the multiprocessing package to share data between processes. These are <strong class="bold">shared memory</strong> and <strong class="bold">server process</strong>. They are described next. </p>
			<h3>Using shared ctype objects (shared memory)</h3>
			<p>In this case, a <em class="italic">shared memory</em> block is<a id="_idIndexMarker784"/> created, and the processes have access to this shared memory block. The shared memory is created as soon we initiate one of the <code>ctype</code> datatypes available in the <code>multiprocessing</code> package. The datatypes are <code>Array</code> and <code>Value</code>. The <code>Array</code> datatype is a <code>ctype</code> array and the <code>Value</code> datatype is a generic <code>ctype</code> object, both of which are allocated from the shared memory. To create a <code>ctype</code> array, we will use a statement like the following:</p>
			<pre>mylist = multiprocessing.Array('i', 5)</pre>
			<p>This will create an array of the <code>integer</code> datatype with a size of <code>5</code>. <code>i</code> is one of the typecodes, and it stands for integer. We can use the <code>d</code> typecode for float datatypes. We can also initialize the array by providing the sequence as a second argument (instead of the size) as follows:</p>
			<pre>mylist = multiprocessing.Array('i', [1,2,3,4,5])</pre>
			<p>To create a <code>Value</code> <code>ctype</code> object, we will use a statement similar to the following:</p>
			<pre>obj = multiprocessing.Value('i')</pre>
			<p>This will create an object of the <code>integer</code> datatype because the typecode is set to <code>i</code>. The value of this object can <a id="_idIndexMarker785"/>be accessed or set by using the <code>value</code> attribute.</p>
			<p>Both these <code>ctype</code> objects have <code>Lock</code> as an optional argument, which is set to <code>True</code> by default. This argument when set to <code>True</code> is used to create a new recursive lock object that provides synchronized access to the value of the objects. If it is set to <code>False</code>, there will be no protection and it will not be a safe process. If your process is only accessing the shared memory for reading purposes, it is fine to set the <code>Lock</code> to <code>False</code>. We leave this <code>Lock</code> argument as the default (<code>True</code>) in our next code examples.</p>
			<p>To illustrate the use of these <code>ctype</code> objects from the shared memory, we will create a default list with three numeric values, a <code>ctype</code> array of size <code>3</code> to hold the incremented values of the original array, and a <code>ctype</code> object to hold the sum of the incremented array. These objects will be created by a parent process in shared memory and will be accessed and updated by a child process from the shared memory. This interaction of the parent and the child processes with the shared memory is shown in the following figure:</p>
			<div><div><img src="img/B17189_07_03.jpg" alt="Figure 7.3 – Use of shared memory by a parent and a child process&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – Use of shared memory by a parent and a child process</p>
			<p>A complete code<a id="_idIndexMarker786"/> example of using the shared memory is shown next:</p>
			<pre># <strong class="bold">process3</strong>.py to use shared memory ctype objects
import multiprocessing
from multiprocessing import Process, Pool, current_process   as cp
def <strong class="bold">inc_sum_list(list, inc_list, sum):</strong>
    sum.value = 0
    for index, num in enumerate(list):
        inc_list[index] = num + 1
        sum.value = sum.value + inc_list[index]
def main():
    mylist = [2, 5, 7]
    inc_list = <strong class="bold">multiprocessing.Array('i', 3)</strong>
    sum = <strong class="bold">multiprocessing.Value('i')</strong>
    p = Process(target=inc_sum_list,                args=(mylist, inc_list, sum))
    p.start()
    p.join()
    print("incremented list: ", list(inc_list))
    print("sum of inc list: ", sum.value)
    print("Exiting the main process")
if __name__ == '__main__':
    main()</pre>
			<p>The shared datatypes (<code>inc_list</code> and <code>sum</code> in this case) are accessed by both the parent process and the child process. It is important to mention that using the shared memory is not a recommended option because it requires synchronization and locking mechanisms (similar to what we did for multithreading) when the same shared memory objects are accessed by<a id="_idIndexMarker787"/> multiple processes and the <code>Lock</code> argument is set to <code>False</code>. </p>
			<p>The next approach of sharing data between processes is using the server process. </p>
			<h3>Using the server process</h3>
			<p>In this case, a server process<a id="_idIndexMarker788"/> is started as soon as a Python program starts. This new process is used to create and manage the new child processes requested by a parent process. This server process can hold Python objects that other processes can access using proxies.</p>
			<p>To implement the server process and share the objects between the processes, the <code>multiprocessing</code> package provides a <code>Manager</code> object. The <code>Manager</code> object supports different data types such as the following:</p>
			<ul>
				<li>Lists</li>
				<li>Dictionaries</li>
				<li>Locks</li>
				<li>Rlocks</li>
				<li>Queues</li>
				<li>Values</li>
				<li>Arrays</li>
			</ul>
			<p>The code example we selected for illustrating the server process creates a <code>dictionary</code> object using the <code>Manager</code> object, then passes the dictionary object to different child processes to insert more data and to print out the dictionary contents. We will create three child processes for our example: two for inserting data into the dictionary object and one for getting the dictionary contents as the console output. The interaction between<a id="_idIndexMarker789"/> the parent process, the server process, and the three child processes is shown in <em class="italic">Figure 7.4</em>. The parent process creates the server process as soon as a new process request is executed using the <em class="italic">Manager</em> context. The child processes are created and managed by the server process. The shared data is available within the server process and is accessible by all processes, including the parent process:</p>
			<div><div><img src="img/B17189_07_04.jpg" alt="Figure 7.4 – Use of server process for sharing data between processes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4 – Use of server process for sharing data between processes</p>
			<p>The complete code example is shown next:</p>
			<pre># <strong class="bold">process4</strong>.py to use shared memory using the server process
import multiprocessing
from multiprocessing import Process, Manager
def <strong class="bold">insert_data</strong> (dict1, code, subject):
    dict1[code] =  subject
def <strong class="bold">output</strong>(dict1):
    print("Dictionary data: ", dict1)
def main():
    with multiprocessing.<strong class="bold">Manager</strong>() as mgr:
        # create a dictionary in the server process
        mydict = <strong class="bold">mgr.dict</strong>({100: "Maths", 200: "Science"})
        p1 = Process(target=<strong class="bold">insert_data</strong>, args=(mydict, 300,           "English"))
        p2 = Process(target=<strong class="bold">insert_data</strong>, args=(mydict, 400,           "French"))
        p3 = Process(target=<strong class="bold">output</strong>, args=(mydict,))
        p1.start()
        p2.start()
        p1.join()
        p2.join()
        p3.start()
        p3.join()
    print("Exiting the main process")
if __name__ == '__main__':
    main()</pre>
			<p>The server process approach offers more flexibility than the shared memory approach because it supports a large variety of object types. However, this comes at the cost of slower performance <a id="_idIndexMarker790"/>compared to the shared memory approach. </p>
			<p>In the next section, we will explore the options of direct communication between the processes.</p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor216"/>Exchanging objects between processes</h2>
			<p>In the previous section, we <a id="_idIndexMarker791"/>studied how to share data between the processes through an external memory block or a new process. In this section, we will investigate exchanging of data between processes using Python objects. The <code>multiprocessing</code> module provides two options for this purpose. These are using the <code>Queue</code> object and the <code>Pipe</code> object.</p>
			<h3>Using the Queue object</h3>
			<p>The <code>Queue</code> object is <a id="_idIndexMarker792"/>available from the <code>multiprocessing</code> package and is nearly the same as the synchronized queue object (<code>queue.Queue</code>) that we used for multithreading. This <code>Queue</code> object is process-safe and does not require any additional protection. A code example to illustrate the use of the multiprocessing <code>Queue</code> object for data exchange is shown next:</p>
			<pre># <strong class="bold">process5</strong>.py to use queue to exchange data
import multiprocessing
from multiprocessing import Process, Queue
def <strong class="bold">copy_data</strong> (list, myqueue):
    for num in list:
        myqueue.put(num)
def <strong class="bold">output</strong>(myqueue):
    while not myqueue.empty():
        print(myqueue.get())
def main():
    mylist = [2, 5, 7]
    myqueue = Queue()
    p1 = Process(target=copy_data, args=(mylist, myqueue))
    p2 = Process(target=output, args=(myqueue,))
    p1.start()
    p1.join()
    p2.start()
    p2.join()
    print("Queue is empty: ",myqueue.empty())
    print("Exiting the main process")
if __name__ == '__main__':
    main()</pre>
			<p>In this code example, we created a <a id="_idIndexMarker793"/>standard <code>list</code> object and a multiprocessing <code>Queue</code> object. The <code>list</code> and <code>Queue</code> objects are passed to a new process, which is attached to a function called <code>copy_data</code>. This function will copy the data from the <code>list</code> object to the <code>Queue</code> object. A new process is initiated to print the contents of the <code>Queue</code> object. Note that the data in the <code>Queue</code> object is set by the previous process and the data will be available to the new process. This is a convenient way to exchange data without adding the complexity of shared memory or the server process.</p>
			<h3>Using the Pipe object</h3>
			<p>The <code>Pipe</code> object is like a pipe <a id="_idIndexMarker794"/>between two processes for exchanging data. This is why this object is especially useful when two-way communication is required. When we create a <code>Pipe</code> object, it provides two connection objects, which are the two ends of the <code>Pipe</code> object. Each connection object provides a <code>send</code> and a <code>recv</code> method to send and receive data.</p>
			<p>To illustrate the concept and use of the <code>Pipe</code> object, we will create two functions that will be attached to two separate processes:</p>
			<ul>
				<li>The first function is for sending the message through a <code>Pipe</code> object connection. We will send a few data messages and finish the communication with a <code>BYE</code> message.</li>
				<li>The second function is to receive the message using the other connection object of the <code>Pipe</code> object. This <a id="_idIndexMarker795"/>function will run in an infinite loop until it receives a <code>BYE</code> message.</li>
			</ul>
			<p>The two functions (or processes) are provided with the two connection objects of a pipe. The complete code is as follows:</p>
			<pre># <strong class="bold">process6</strong>.py to use Pipe to exchange data
from multiprocessing import Process, Pipe
def <strong class="bold">mysender</strong> (s_conn):
    <strong class="bold">s_conn.send</strong>({100, "Maths"})
    s_conn.send({200, "Science"})
    s_conn.send("BYE")
def <strong class="bold">myreceiver</strong>(r_conn):
    while True:
        msg = <strong class="bold">r_conn.recv</strong>()
        if msg == "BYE":
            break
        print("Received message : ", msg)
def main():
    sender_conn, receiver_conn= Pipe()
    p1 = Process(target=mysender, args=(sender_conn, ))
    p2 = Process(target=myreceiver, args=(receiver_conn,))
    p1.start()
    p2.start()
    p1.join()
    p2.join()
    print("Exiting the main process")
if __name__ == '__main__':
    main()</pre>
			<p>It is important to mention<a id="_idIndexMarker796"/> that the data in a <code>Pipe</code> object can easily be corrupted if the two processes try to read from or write to it using the same connection object at the same time. That is why multiprocessing queues are the preferred option: because they provide proper synchronization between the processes.</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor217"/>Synchronization between processes</h2>
			<p>Synchronization between processes <a id="_idIndexMarker797"/>makes sure that two or more processes do not access the same resources or program code at the same time, which is also called the <code>Lock</code> object, similar to what we used in the case of multithreading.</p>
			<p>We illustrated the use of <code>queues</code> and <code>ctype</code> datatypes with <code>Lock</code> set to <code>True</code>, which is process safe. In the next code example, we will illustrate the use of the <code>Lock</code> object to make sure one process gets access to<a id="_idIndexMarker799"/> the console output at a time. We created the processes using the <code>Pool</code> object and to pass the same <code>Lock</code> object to all processes, we used the <code>Lock</code> from the <code>Manager</code> object and not the one from the multiprocessing package. We also used the <code>partial</code> function to tie the <code>Lock</code> object to each process, along with a list to be distributed to each process function. Here is the complete code example:</p>
			<pre># <strong class="bold">process7</strong>.py to show synchronization and locking
from functools import partial
from multiprocessing import Pool, Manager
def printme (lock, msg):
    lock.acquire()
    try:
        print(msg)
    finally:
        lock.release()
def main():
    with Pool(3) as proc:
        lock = Manager().Lock()
        func = partial(printme,lock)
        proc.map(func, ["Orange", "Apple", "Banana",
                                 "Grapes","Pears"])
    print("Exiting the main process")
if __name__ == '__main__':
    main()</pre>
			<p>If we do not use the <code>Lock</code> object, the output from the different processes can be mixed up.</p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor218"/>Case study – a multiprocessor application to download files from Google Drive</h2>
			<p>In this section, we will implement the<a id="_idIndexMarker800"/> same case study as we did in the <em class="italic">Case study – a multithreaded application to download files from Google Drive</em> section, but using processors instead. The prerequisites and goals are the same as described for the case study of the multithreaded application.</p>
			<p>For this application, we used the same code that we built for the multithreaded application except that we used processes instead of threads. Another difference is that we used the <code>JoinableQueue</code> object from the <code>multiprocessing</code> module to achieve the same functionality as we were getting from the regular <code>Queue</code> object. Code for defining a resource data structure and for a function to download files from Google Drive is as follows:</p>
			<pre>#<strong class="bold">processes_casestudy</strong>.py
import time
from multiprocessing import Process, JoinableQueue
from getfilelistpy import getfilelist
import gdown
PROCESSES_POOL_SIZE = 5
resource = {
    "api_key": "AIzaSyDYKmm85keqnk4bF1Da2bxddKrGns4z0",
    "id": "0B8TxHW2Ci6dbckVwetTlV3RUU",
    "fields": "files(name,id,webContentLink)",
}
def <strong class="bold">mydownloader</strong>( queue):
    while True:
        # Get the file id and name from the queue
        item1 =  queue.get()
        try:
            <strong class="bold">gdown.download</strong>(item1['webContentLink'],
                           './files/{}'.format(item1['name']),
                           quiet=False)
        finally:
            queue.task_done()</pre>
			<p>We get the files' metadata, such as the name and HTTP link, from a Google Drive directory using the resource<a id="_idIndexMarker801"/> data structure as follows:</p>
			<pre>def get_files(resource):
    res = getfilelist.GetFileList(resource)
    files_list = res['fileList'][0]
    return files_list</pre>
			<p>In our <code>main</code> function, we create a <code>JoinableQueue</code> object and insert the files' metadata into the queue. The queue will be handed over to a pool of processes to download the files. The processes will download the files. We used the <code>time</code> class to measure the time it takes to download all the files from the Google Drive directory. The code for the <code>main</code> function is as follows:</p>
			<pre>def main ():
    files = get_files(resource)
    #add files info into the queue
    <strong class="bold">myqueue = JoinableQueue()</strong>
    for item in files['files']:
        myqueue.put(item)
    processes = []
    for id in range(PROCESSES_POOL_SIZE):
        p = <strong class="bold">Process</strong>(target=<strong class="bold">mydownloader</strong>, args=(myqueue,))
        p.daemon = True
        p.start()
    start_time = time.monotonic()
    <strong class="bold">myqueue.join()</strong>
    total_exec_time = time.monotonic() - start_time
    print(f'Time taken to download: {total_exec_time:.2f}         seconds')
if __name__ == '__main__':
    main()</pre>
			<p>We ran this application by varying the different number of processes, such as <code>3</code>, <code>5</code>, <code>7</code>, and <code>10</code>. We found that the time it took to download the same files (as for the case study of multithreading) is slightly <a id="_idIndexMarker802"/>better than with the multithreaded application. The execution time will vary from machine to machine, but on our machine (MacBook Pro: Intel Core i5 with 16 GB RAM), it took around 5 seconds with 5 processes and 3 seconds with 10 processes running in parallel. This improvement of 1 second over the multithreaded application is in line with the expected results as multiprocessing provides true concurrency.</p>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor219"/>Using asynchronous programming for responsive systems</h1>
			<p>With multiprocessing <a id="_idIndexMarker803"/>and multithreaded programming, we were mostly dealing with synchronous programming, where we request something and wait for the response to be received before we move to the next block <a id="_idIndexMarker804"/>of code. If any context switching is applied, it is provided by the operating system. Asynchronous programming in Python is different mainly in the following two aspects:</p>
			<ul>
				<li>The tasks are to be created for asynchronous execution. This means the parent caller does not have to wait for the response from another process. The process will respond to the caller once it finishes the execution.</li>
				<li>The operating system is no longer managing the context switching between the processes and the threads. The asynchronous program will be given only a single thread in a process, but we can do multiple things with it. In this style of execution, every process or task voluntarily releases control whenever it is idle or waiting for another resource<a id="_idIndexMarker805"/> to make sure that the other tasks get a turn. This concept is called <strong class="bold">cooperative multitasking</strong>.</li>
			</ul>
			<p>Cooperative multitasking is an effective tool for achieving concurrency at the application level. In cooperative multitasking, we do not build processes or threads, but tasks, which comprises <code>yield</code>) while keeping the stack of objects under control before it is resumed.</p>
			<p>For a system based on cooperative multitasking, there is always a question of when to release the control back to a scheduler or to an event loop. The most commonly used logic is to use the I/O operation as the event to release the control because there is always a waiting time involved whenever we are doing an I/O operation.</p>
			<p>But hold on, is it not the same logic we used for multithreading? We found that multithreading improves application performance when dealing with I/O operations. But there is a difference here. In the case of multithreading, the operating system is managing the context switching between the threads, and it can preempt any running thread for any reason and<a id="_idIndexMarker810"/> give control to another thread. But in asynchronous programming or cooperative multitasking, the tasks or coroutines are not visible to the operating systems and cannot be preempted. The coroutines in fact cannot be preempted by the main event loop. But this does not mean that the operating system cannot preempt the whole Python process. The main Python process is still competing for resources with other applications and processes at the operating system level.</p>
			<p>In the next section, we will discuss a few building blocks of asynchronous programming in Python, which is <a id="_idIndexMarker811"/>provided by the <code>asyncio</code> module, and we will conclude with a comprehensive case study.</p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor220"/>Understanding the asyncio module</h2>
			<p>The <code>asyncio</code> module is available in <a id="_idIndexMarker812"/>Python 3.5 or later to write concurrent programs using the <code>async/await</code> syntax. But it is recommended to use Python 3.7 or later to build any serious <code>asyncio</code> application. The library is<a id="_idIndexMarker813"/> rich with features and supports creating and running Python coroutines, performing network I/O operations, distributing tasks to queues, and synchronizing concurrent code.</p>
			<p>We will start with how to write and execute coroutines and tasks.</p>
			<h3>Coroutines and tasks</h3>
			<p>Coroutines are the functions that <a id="_idIndexMarker814"/>are to be executed asynchronously. A simple<a id="_idIndexMarker815"/> example of sending a string to the console output using a coroutine is as follows:</p>
			<pre>#<strong class="bold">asyncio1</strong>.py to build a basic coroutine
import asyncio
import time
async def <strong class="bold">say</strong>(delay, msg):
    <strong class="bold">await</strong> asyncio.sleep(delay)
    print(msg)
print("Started at ", time.strftime("%X"))
asyncio.<strong class="bold">run</strong>(say(1,"Good"))
asyncio.run(say(2, "Morning"))
print("Stopped at ", time.strftime("%X"))</pre>
			<p>In this code example, it is important <a id="_idIndexMarker816"/>to note the following:</p>
			<ul>
				<li>The coroutine takes <code>delay</code> and <code>msg</code> arguments. The <code>delay</code> argument is used to add a delay<a id="_idIndexMarker817"/> before sending the <code>msg</code> string to the console output.</li>
				<li>We used the <code>asyncio.sleep</code> function instead of the traditional <code>time.sleep</code> function. If the <code>time.sleep</code> function is used, control will not be given back to the event loop. That is why it is important to use the compatible <code>asyncio.sleep</code> function.</li>
				<li>The coroutine is executed twice with two different values of the <code>delay</code> argument by using the <code>run</code> method. The <code>run</code> method will not execute the coroutines concurrently.</li>
			</ul>
			<p>The console output of this program will be as follows. This shows that the coroutines are executed one after the other as the total delay added is 3 seconds:</p>
			<pre>Started at 15:59:55
Good
Morning
Stopped at 15:59:58</pre>
			<p>To run the coroutines in parallel, we need to use the <code>create_task</code> function from the <code>asyncio</code> module. This function creates a task that can be used to schedule coroutines to run concurrently.</p>
			<p>The next code example is a revised version of <code>asyncio1.py</code>, in which we wrapped the coroutine (<code>say</code> in our case) into a task using the <code>create_task</code> function. In this revised version, we created<a id="_idIndexMarker818"/> two tasks that are wrapping the <code>say</code> coroutine. We <a id="_idIndexMarker819"/>waited for the two tasks to be completed using the <code>await</code> keyword:</p>
			<pre>#<strong class="bold">asyncio2</strong>.py to build and run coroutines in parallel
import asyncio
import time
async def <strong class="bold">say</strong>(delay, msg):
    await asyncio.sleep(delay)
    print(msg)
async def main ():
    task1 = <strong class="bold">asyncio.create_task</strong>( say(1, 'Good'))
    task2 = asyncio.create_task( say(1, 'Morning'))
    print("Started at ", time.strftime("%X"))
    <strong class="bold">await task1</strong>
    await task2
    print("Stopped at ", time.strftime("%X"))
asyncio.run(main())</pre>
			<p>The console output of this program is as follows:</p>
			<pre>Started at 16:04:40
Good
Morning
Stopped at  16:04:41</pre>
			<p>This console output <a id="_idIndexMarker820"/>shows that the two tasks were completed in 1 second, which <a id="_idIndexMarker821"/>is proof that the tasks are executed in parallel.</p>
			<h3>Using awaitable objects</h3>
			<p>An object is<a id="_idIndexMarker822"/> awaitable if we can apply the <code>await</code> statement to it. The majority of <code>asyncio</code> functions and modules inside it are designed to work with awaitable objects. But most Python objects and third-party libraries are not built for asynchronous programming. It is important to select compatible libraries that provide awaitable objects to use when building asynchronous applications.</p>
			<p>Awaitable objects are split<a id="_idIndexMarker823"/> mainly into three types: coroutines, tasks, and <code>Future</code> is a low-level object that is like a callback mechanism used to process the result coming from the <code>async</code>/<code>await</code>. The <code>Future</code> objects are typically not exposed for user-level programming.</p>
			<h3>Running tasks concurrently</h3>
			<p>If we have to run<a id="_idIndexMarker824"/> multiple tasks in parallel, we can use the <code>await</code> keyword as we did in the previous example. But there is a better way of doing this by using the <code>gather</code> function. This function will run the awaitable objects in the sequence provided. If any of the awaitable objects is a coroutine, it will be scheduled as a task. We will see the use of the <code>gather</code> function in the next section with a code example.</p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor221"/>Distributing tasks using queues</h2>
			<p>The <code>Queue</code> object in the <code>asyncio</code> package is similar to the <code>Queue</code> module but it is not thread safe. The <code>aysncio</code> module provides a variety of queue implementations, such as FIFO queues, priority queues, and LIFO queues. The queues in the <code>asyncio</code> module can be used to distribute the <a id="_idIndexMarker825"/>workloads to the tasks.</p>
			<p>To illustrate the use of a queue with tasks, we will write a small program that will simulate the execution time of a real function by sleeping for a random amount of time. The random sleeping time is calculated for 10 such executions and added to a <code>Queue</code> object as working items by the main process. The <code>Queue</code> object is passed to a pool of three tasks. Each task in the pool<a id="_idIndexMarker826"/> executes the assigned coroutine, which consumes the execution time as per the queue entry available to it. The complete code is shown next:</p>
			<pre>#<strong class="bold">asyncio3</strong>.py to distribute work via queue
import asyncio
import random
import time
async def <strong class="bold">executer</strong>(name, queue):
    while True:
        exec_time = await queue.get()
        await asyncio.sleep(exec_time)
        queue.task_done()
        #print(f'{name} has taken  {exec_time:.2f} seconds')
async def main ():
    <strong class="bold">myqueue = asyncio.Queue()</strong>
    calc_exuection_time = 0
    for _ in range(10):
        sleep_for = random.uniform(0.4, 0.8)
        calc_exuection_time += sleep_for
        <strong class="bold">myqueue.put_nowait</strong>(sleep_for)
    tasks = []
    for id in range(3):
        task = <strong class="bold">asyncio.create_task</strong>(<strong class="bold">executer</strong>(f'Task-{id+1}',                 myqueue))
        tasks.append(task)
    start_time = time.monotonic()
    await myqueue.join()
    total_exec_time = time.monotonic() - start_time
    for task in tasks:
        task.cancel()
    await asyncio.gather(*tasks, return_exceptions=True)
    print(f"Calculated execution time         {calc_exuection_time:0.2f}")
    print(f"Actual execution time {total_exec_time:0.2f}")
asyncio.run(main())</pre>
			<p>We used the <code>put_no_wait</code> function of the <code>Queue</code> object because it is a non-blocking operation. The console output of this <a id="_idIndexMarker827"/>program is as follows:</p>
			<pre>Calculated execution time 5.58
Actual execution time 2.05</pre>
			<p>This clearly shows that<a id="_idIndexMarker828"/> the tasks are executed in parallel, and the execution is three times better than if tasks are run sequentially.</p>
			<p>So far, we have covered the fundamental concepts of the <code>asyncio</code> package in Python. Before concluding this topic, we will revisit the case study we did for the multithreading section by implementing it using the <code>asyncio</code> tasks.</p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor222"/>Case study – asyncio application to download files from Google Drive</h2>
			<p>We will implement the <a id="_idIndexMarker829"/>same case study as we did in the <em class="italic">Case study – a multithreaded application to download files from Google Drive</em> section, but using the <code>asyncio</code> module with <code>async</code>, <code>await</code>, and <code>async queue</code>. The prerequisites for this case study are the same except that we use the <code>aiohttp</code> and <code>aiofiles</code> library instead of the <code>gdown</code> library. The reason is simple: the <code>gdown</code> library is not built as an async module. There is no benefit of using it with async programming. This is an important point to consider whenever selecting libraries to be used with async applications.</p>
			<p>For this application, we built a coroutine, <code>mydownloader</code>, to download a file from Google Drive using the <code>aiohttp</code> and <code>aiofiles</code> modules. This is shown in the following code, and the code that is different from the previous case studies is highlighted:</p>
			<pre>#<strong class="bold">asyncio_casestudy</strong>.py
import asyncio
import time
import aiofiles, aiohttp
from getfilelistpy import getfilelist
TASK_POOL_SIZE = 5
resource = {
    "api_key": "AIzaSyDYKmm85keqnk4bF1DpYa2dKrGns4z0",
    "id": "0B8TxHW2Ci6dbckVwetTlV3RUU",
    "fields": "files(name, id, webContentLink)",
}
async def <strong class="bold">mydownloader</strong>(name, queue):
    while True:
        # Get the file id and name from the queue
        item = await queue.get()
        try:
            <strong class="bold">async with aiohttp.ClientSession()</strong> as sess:
                <strong class="bold">async with sess.get</strong>(item['webContentLink']) 
                    as resp:
                    if resp.status == 200:
                       f = <strong class="bold">await aiofiles.open</strong>('./files/{}'                         .format(
                            item['name']), mode='wb')
                        <strong class="bold">await f.write</strong>(await resp.read())
                        <strong class="bold">await f.close()</strong>
        finally:
            print(f"{name}: Download completed for 
                        ",item['name'])
            queue.task_done()</pre>
			<p>The process to get the list of files from a shared Google Drive folder is the same as we used in the previous case<a id="_idIndexMarker830"/> study for multithreading and multiprocessing. In this case study, we created a pool of tasks (configurable) based on the <code>mydownloader</code> coroutine. These tasks are then scheduled to run together, and our parent process waits for all tasks to complete their execution. A code to get a list of files from Google Drive and then download the files using <code>asyncio</code> tasks is as follows:</p>
			<pre>def get_files(resource):
    res = getfilelist.GetFileList(resource)
    files_list = res['fileList'][0]
    return files_list
async def main ():
    files = get_files(resource)
    #add files info into the queue
    myqueue = asyncio.Queue()
    for item in files['files']:
        myqueue.put_nowait(item)
    tasks = []
    for id in range(TASK_POOL_SIZE):
        task = <strong class="bold">asyncio.create_task</strong>(
            <strong class="bold">mydownloade</strong>r(f'Task-{id+1}', myqueue))
        tasks.append(task)
    start_time = time.monotonic()
    <strong class="bold">await myqueue.join()</strong>
    total_exec_time = time.monotonic() - start_time
    for task in tasks:
        task.cancel()
    <strong class="bold">await asyncio.gather</strong>(*tasks, return_exceptions=True)
    print(f'Time taken to download: {total_exec_time:.2f}         seconds')
asyncio.run(main())</pre>
			<p>We ran this application by<a id="_idIndexMarker831"/> varying the number of tasks, such as 3, 5, 7, and 10. We found that the time it took to download the files with the <code>asyncio</code> tasks is lower than when we downloaded the same files using the multithreading approach or the multiprocessing approach. The exact details of the time taken with the multithreading approach and the multiprocessing approach are available in the <em class="italic">Case study – a multithreaded application to download files from Google Drive</em> and <em class="italic">Case study – a multiprocessor application to download files from Google Drive</em> sections.</p>
			<p>The execution time can vary from machine to machine, but on our machine (MacBook Pro: Intel Core i5 with 16 GB RAM), it took around 4 seconds with 5 tasks and 2 seconds with 10 tasks running in parallel. This is a significant improvement compared to the numbers we observed for the multithreading and multiprocessing case studies. This is in line with expected results, as <code>asyncio</code> provides a better concurrency framework when it comes to I/O-related tasks, but it has to be implemented using the right set of programming objects.</p>
			<p>This concludes our discussion of asynchronous programming. This section provided all the core ingredients to build an asynchronous application using the <code>asyncio</code> package.</p>
			<h1 id="_idParaDest-185"><a id="_idTextAnchor223"/>Summary</h1>
			<p>In this chapter, we discussed different options of concurrent programming in Python using the standard libraries. We started with multithreading with an introduction to the core concepts of concurrent programming. We introduced the challenges with multithreading, such as the GIL, which allows only one thread at a time to access Python objects. The concepts of locking and synchronization were explored with practical examples of Python code. We also discussed the types of task that multithreaded programming is more effective for using a case study.</p>
			<p>We studied how to achieve concurrency using multiple processes in Python. With multiprocessing programming, we learned how to share data between processes using shared memory and the server process, and also how to exchange objects safely between processes using the <code>Queue</code> object and the <code>Pipe</code> object. In the end, we built the same case study as we did for the multithreading example, but using processes instead. Then, we introduced a completely different approach to achieving concurrency by using asynchronous programming. This was a complete shift in concept, and we started it by looking at the high-level concepts of the <code>async</code> and <code>await</code> keywords and how to build tasks, or coroutines, using the <code>asyncio</code> package. We concluded the chapter with the same case study we examined for multiprocessing and multithreading but using asynchronous programming.</p>
			<p>This chapter has provided a lot of hands-on examples of how to implement concurrent applications in Python. This knowledge is important for anyone who wants to build multithreaded or asynchronous applications using the standard libraries available in Python.</p>
			<p>In the next chapter, we will explore using third-party libraries to build concurrent applications in Python.</p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor224"/>Questions</h1>
			<ol>
				<li>What coordinates Python threads? Is it a Python interpreter?</li>
				<li>What is the GIL in Python?</li>
				<li>When should you use daemon threads?</li>
				<li>For a system with limited memory, should we use a <code>Process</code> object or <code>Pool</code> object to create processes?</li>
				<li>What are Futures in the <code>asyncio</code> package?</li>
				<li>What is an event loop in asynchronous programming?</li>
				<li>How do you write an asynchronous coroutine or function in Python?</li>
			</ol>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor225"/>Further reading</h1>
			<ul>
				<li><em class="italic">Learning Concurrency in Python</em> by Elliot Forbes</li>
				<li><em class="italic">Expert Python Programming</em> by Michal Jaworski and Tarek Ziade</li>
				<li><em class="italic">Python 3 Object-Oriented Programming,</em> <em class="italic">Second Edition</em> by Dusty Phillips</li>
				<li><em class="italic">Mastering Concurrency</em> <em class="italic">in Python</em> by Quan Nguyen</li>
				<li><em class="italic">Python Concurrency with asyncio</em> by Mathew Fowler</li>
			</ul>
			<h1 id="_idParaDest-188"><a id="_idTextAnchor226"/>Answers</h1>
			<ol>
				<li value="1">The threads and processes are coordinated by the operating system kernel.</li>
				<li>Python's GIL is a locking mechanism used by Python to allow only one thread to execute at a time.</li>
				<li>Daemon threads are used when it is not an issue for a thread to be terminated once its main thread terminates.</li>
				<li>The <code>Pool</code> object keeps only the active processes in memory, so it is a better choice.</li>
				<li>Futures are like a callback mechanism that is used to process the result coming from async/await calls.</li>
				<li>An event loop object keeps track of tasks and handles the flow of control between them.</li>
				<li>We can write an asynchronous coroutine by starting with <code>async def</code>.</li>
			</ol>
		</div>
	</body></html>