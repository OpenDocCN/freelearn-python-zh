<html><head></head><body>
		<div><div></div>
		</div>
		<div><h1 id="_idParaDest-165"><a id="_idTextAnchor379"/>9. Building More Features</h1>
		</div>
		<div><h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Use caching to improve API performance and efficiently get the latest information</li>
				<li class="bullets">Add the cache function to the Smilecook application using the Flask-Caching package</li>
				<li class="bullets">Implement rate-limiting functionality to an API</li>
				<li class="bullets">Use IP address to perform rate limiting</li>
			</ul>
			<p>In this chapter, we will cover caching to improve performance and get accustomed to using the rate-limiting function.</p>
		</div>
		<div><h2 id="_idParaDest-166"><a id="_idTextAnchor380"/>Introduction</h2>
			<p>We added pagination, searching, and ordering functions to our Smilecook application in our last chapter so that users can navigate to their recipes much easier. This also helps to reduce the server burden and improve performance. We have explained how making our APIs snappy is important in today's world.</p>
			<p>In this chapter, we will be further improving our API performance from another aspect. We will be adding in the <code>cache</code> function, which will temporarily save data to the application memory. This will allow us to save the time required to query the database every time. This can greatly improve API performance and reduce server burden. There is a Flask extension package, Flask-Caching, that can help us in implementing the caching function. We will first talk about the theory behind caching, and through practical exercises, we show you how to implement this function in our Smilecook application.</p>
			<p>Besides caching, we will implement a rate-limiting function. That will prevent certain high-usage users from jeopardizing the whole system by limiting their usage. Ensuring fair usage of our APIs is crucial to guarantee service quality. We will be using a Flask extension package, <code>Flask-Limiter</code>, for that.</p>
			<p>These two caching and rate-limiting functions are very common and powerful in real-world scenarios. Let's learn about how they work.</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor381"/>Caching</h2>
			<p><strong class="keyword">Caching</strong> means storing data in a temporary space (a cache) so that it can be retrieved faster in subsequent requests. The temporary space can be application memory, server hard disk space, or something else. The whole purpose of caching is to lighten the workload by avoiding any heavy processes for querying the data again. For example, in our Smilecook application, if we reckon that the recipes from a popular author will always get queried by the users, we can cache these recipes. So, the next time that users ask for these recipes, we can just send back the recipes in the cache instead of querying against our database. You can see caching everywhere. Almost all applications have caching implemented nowadays. Even in our local browsers, we save website results on the local hard disk to achieve faster access next time.</p>
			<p>For server-level caching, most of the time, the cache is stored in the same web server as the application. But technically speaking, it can be stored in another server as well, such as <strong class="keyword">Redis</strong> (<strong class="keyword">Remote Dictionary Server</strong>) or <strong class="keyword">Memcached</strong> (a high-performance distributed cached memory). They are all in-memory data storage systems that allow key-value storage as well as storing data. For simple applications and easy implementation, we can also use a single global dictionary as a cache (simple cache).</p>
			<h3 id="_idParaDest-168">Benefit of <a id="_idTextAnchor382"/>Caching</h3>
			<p>Through caching, not only can we reduce the volume of data to be transferred, but we can also improve the overall performance. This is done by reducing the bandwidth required, reducing the server loading time, and more. Take our Smilecook application as an example: if we have a low traffic, caching may not be a lot of help, because the cache will pretty much expire before the next query comes in. But imagine that we have high traffic, say, 10,000 requests per minute, coming in asking for recipes. If these recipes are all cached and the cache has not expired, we will be able to simply return the recipes in the cache to the client frontend. In this scenario, we would be saving 10,000 database queries, which could be a substantial cost-saving measure.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor383"/>Flask-Caching</h2>
			<p><code>cache</code> as a dictionary object that contains key-value pairs. The key here is used to specify the resource to <code>cache</code>, whereas the value is used to store the actual data to be cached. Take the resource for retrieving all the recipes as an example. The flow contains the following stages:</p>
			<ol>
				<li>Request the get <code>/recipes</code> resource.</li>
				<li>Use the key to search for the existing cache (Flask-Caching will be using <code>request.path</code> and <code>hashed_args</code> to be the key value, for example, <code>recipesbcd8b0c2eb1fce714eab6cef0d771acc</code>).</li>
				<li>If the recipes were previously cached, return the cached data.</li>
				<li>If no cache for these recipes exists, follow the standard flow to get the recipes from the database.</li>
				<li>Save the result (the recipe data) in the cache.</li>
				<li>Return the recipe data.</li>
			</ol>
			<p>The process is better illustrated through the following figure:</p>
			<div><div><img src="img/C15309_09_01.jpg" alt="Figure 9.1: Flask-Caching process flow chart&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 9.1: Flask-Caching process flow chart</h6>
			<p>By following this flow, you can see that data that is cached can be served before we query against the database.</p>
			<p>I hope you have a better understanding of the theory behind caching. Let's roll up our sleeves and work on bringing this feature and our Smilecook application together, through the coming exercises.</p>
			<h3 id="_idParaDest-170">E<a id="_idTextAnchor384"/>xercise 56: Implementing Caching Functionality Using Flask-Caching</h3>
			<p>In this exercise, we will be installing the Flask-Caching package. Then, we will implement the <code>cache</code> function in <code>RecipeListResource</code>. We will also add two decorators, <code>@app.before_request</code> and <code>@app.after_request</code>, to print application logs for easier testing:</p>
			<ol>
				<li value="1">Add the Flask-Caching package and version in <code>requirements.txt</code>:<pre>Flask-Caching==1.7.2</pre></li>
				<li>R<a id="_idTextAnchor385"/>un the <code>pip</code> command to install the package:<pre>pip install -r requirements.txt</pre><p>Once we have run the <code>install</code> command, we should see the following result:</p><pre>Installing collected packages: Flask-Caching
Successfully installed Flask-Caching-1.7.2</pre></li>
				<li>Import <code>Cache</code> in <code>extensions.py</code> and instantiate it:<pre>from flask_caching import Cache
cache = Cache()</pre></li>
				<li>Import <code>cache</code> from <code>extensions</code> in <code>app.py</code>:<pre>from extensions import db, jwt, image_set, cache</pre></li>
				<li>In <code>app.py</code>, put in <code>cache.init_app(app)</code> under the <code>register_extensions</code> function. Pass in the <code>app</code> object to initialize the caching function:<pre>def register_extensions(app):
    db.app = app
    db.init_app(app)
    migrate = Migrate(app, db)
    jwt.init_app(app)
    configure_uploads(app, image_set)
    patch_request_class(app, 10 * 1024 * 1024)
    cache.init_app(app)</pre></li>
				<li>Add the caching-related configuration in <code>config.py</code>:<pre>CACHE_TYPE = 'simple' 
CACHE_DEFAULT_TIMEOUT = 10 * 60</pre><p>The default <code>CACHE_TYPE</code> is <code>Null</code>, meaning there is no cache. Here, we set <code>CACHE_TYPE</code> as <code>simple</code>, which means we are going to use the <code>SimpleCache</code> strategy. The default expiration time is <code>10 * 60</code> seconds, which is 10 minutes.</p></li>
				<li>I<a id="_idTextAnchor386"/>mport <code>cache</code> from <code>extensions</code> in <code>resources/recipe.py</code>:<pre>from extensions import image_set, cache</pre></li>
				<li>I<a id="_idTextAnchor387"/>n <code>resources/recipe.py</code>, put the <code>cache</code> decorator in the <code>get</code> method of <code>RecipeListResource</code>:<pre>class RecipeListResource(Resource):
    @use_kwargs({'q': fields.Str(missing=''),
                                'page': fields.Int(missing=1),
                                'per_page': fields.Int(missing=20),
                                'sort': fields.Str(missing='created_at'),
                                'order': fields.Str(missing='desc')})
    @cache.cached(timeout=60, query_string=True)
    def get(self, q, page, per_page, sort, order):</pre><p>We are setting the cache expiration time (<code>timeout</code>) to be <code>60</code> seconds here. <code>query_string = True</code> means it allows the passing in of arguments.</p></li>
				<li>For testing, print a line of <code>Querying database</code> in the <code>RecipeListResource.get</code> method:<pre>    def get(self, q, page, per_page, sort, order):
        print('Querying database...')</pre></li>
				<li><a id="_idTextAnchor388"/>For testing, in <code>app.py</code>, add in the following decorator definition at the bottom of the <code>register_extensions(app)</code> function:<pre>@app.before_request
    def before_request():
        print('\n==================== BEFORE REQUEST ====================\n')
        print(cache.cache._cache.keys())
        print('\n=======================================================\n')
    @app.after_request
    def after_request(response):
        print('\n==================== AFTER REQUEST ====================\n')
        print(cache.cache._cache.keys())
        print('\n=======================================================\n')
        return response</pre></li>
			</ol>
			<p>We have already completed our first caching function on <code>RecipeListResource</code>. That should reduce the frequency of having to get recipes from the database. Let's test it out in our next exercise to make sure it works.</p>
			<h3 id="_idParaDest-171">E<a id="_idTextAnchor389"/>xercise 57: Testing the Caching Function with Postman</h3>
			<p>In this exercise, we will be using Postman to test the caching function. And we will verify whether it works or not in the PyCharm console:</p>
			<ol>
				<li value="1">First, get all the recipe details back. Click on <strong class="bold">GET</strong> <strong class="bold">RecipeList</strong>.</li>
				<li>Then, send the request. The result is shown in the following screenshot:<div><img src="img/C15309_09_02.jpg" alt="Figure 9.2: Getting all recipe details&#13;&#10;"/></div><h6>Figure 9.2: Getting all recipe details</h6></li>
				<li>Check the application log in the PyCharm console.<div><img src="img/C15309_09_03.jpg" alt="Figure 9.3: Checking the application log&#13;&#10;"/></div><h6>Figure 9.3: Checking the application log</h6><p>In the console, we can see that before the request, the cache is empty. After the database query, the data is cached and returned to the frontend client.</p></li>
				<li>Get all the recipe details back again one more time and check the result in the PyCharm console:<div><img src="img/C15309_09_04.jpg" alt="Figure 9.4: Getting all the recipe details again&#13;&#10;"/></div></li>
			</ol>
			<h6>Figure 9.4: Getting all the recipe details again</h6>
			<p>Because this is the second time that we are requesting the data, we get it from the cache rather than the database; the previous result was cached. We can see from the PyCharm console that the result was cached and no query to the database was fired.</p>
			<p>So, we have completed the implementation and testing of the caching function here. Since we are just caching one record here, the performance gain may not be obvious. But imagine we were getting thousands of requests of the same kind in a short period of time; this caching functionality can greatly reduce the workload of our database.</p>
			<h4>Note<a id="_idTextAnchor390"/></h4>
			<p class="callout">If we want to see the data in the cache, we can use this line of code: <code>print(cache.cache._cache.items())</code>, to check the key-value stored there. There we can see that the value in the cache is the JSON data that we return to the client frontend.</p>
			<h3 id="_idParaDest-172">Clea<a id="_idTextAnchor391"/>ring the Cache when Data Updates</h3>
			<p>When data is updated, the data that was cached before becomes stale immediately. For example, if the cover image of a recipe is updated, the old cover image is removed. But in the cache, there would still be the URL of the old cover image, which would no longer work. Therefore, we need a mechanism for clearing the old cache and storing the URL of the new cover image to our cache instead.</p>
			<h3 id="_idParaDest-173">Acti<a id="_idTextAnchor392"/>vity 16: Getting Cache Data after Updating Recipe Details</h3>
			<p>When we get all the recipe details, they will be stored in the cache and can be used directly in the next request. In this activity, we will check to see what will happen when we try to get recipe details after updating the recipe data:</p>
			<ol>
				<li value="1">First, get all the recipe details back.</li>
				<li>Update one of the recipe details.</li>
				<li>Get all the recipe details back again and check the recipe details.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 340.</p></li>
			</ol>
			<p>In our next exercise, we shall find all the resources that are involved in updating data. We shall add a step to clear the cache after data is updated.</p>
			<h3 id="_idParaDest-174">Exer<a id="_idTextAnchor393"/>cise 58: Implementing Cache-Clearing Functionality</h3>
			<p>In this exercise, we will try to clear the cache when recipe data is updated. There are quite a few resources involved here. We shall tackle them one by one:</p>
			<ol>
				<li value="1">Import cache from extensions in utils.py:<pre>from extensions import image_set, cache</pre></li>
				<li>Create a new function under <code>utils.py</code> that is for clearing the cache. The function should clear the cache with a specific prefix:<pre>def clear_cache(key_prefix):
    keys = [key for key in cache.cache._cache.keys() if key.startswith(key_prefix)]
    cache.delete_many(*keys)</pre><p>Here<a id="_idTextAnchor394"/>, the code is to use the <code>for</code> loop for <code>key</code> in <code>cache.cache._cache.keys()</code> to iterate all the keys in the cache. If the key is prefixed with the passed-in prefix, it will be placed on the <code>keys</code> list. Then, we will be using the <code>cache.delete_many</code> method to clear the cache. The single star, <code>*</code>, in the preceding code, is for unpacking the list into positional arguments.</p></li>
				<li>Impo<a id="_idTextAnchor395"/>rt the <code>clear_cache</code> function in <code>resources/recipe.py</code>:<pre>from utils import clear_cache</pre></li>
				<li>Invoke <code>clear_cache('/recipes')</code> in the resources that update recipe data. In the <code>RecipeResource.patch</code>, <code>RecipeResource.delete</code>, <code>RecipePublishResource.put</code>, <code>RecipePublishResource.delete</code>, and <code>RecipeCoverUploadResource.put</code> methods, add in <code>clear_cache('/recipes')</code> before <code>return</code>:<pre>clear_cache('/recipes')</pre><p>So, here, if done properly, the old cache data will be cleared when the data is updated. Next time, when this updated data is requested, it will be stored in the cache again.</p></li>
				<li>Import the <code>generate_token</code>, <code>verify_token</code>, <code>save_image</code>, <code>clear_cache</code> function in <code>resources/user.py</code>:<pre>from utils import generate_token, verify_token, save_image, clear_cache</pre></li>
				<li>Invoke <code>clear_cache('/recipes')</code> in <code>UserAvatarUploadResource.put</code> to clear the cache when data is updated:<pre>clear_cache('/recipes')</pre><p>When the user updates their avatar image, that will change the <code>avatar_url</code> attribute. Therefore, we will need to clear the stale cache there as well.</p></li>
			</ol>
			<p>After this exercise, I believe that you will have a much better understanding of how the whole flow of caching works. We build the caching function here to improve performance, but at the same time, we want to make sure that the cache is refreshed to ensure data quality.</p>
			<h3 id="_idParaDest-175">Exer<a id="_idTextAnchor396"/>cise 59: Verifying the Cache-Clearing Function</h3>
			<p>In our previous exercise, we added the step to clear the cache to the resources that are involved in data updates. In this activity, we will verify the cache-clearing function that we have implemented. We can test it by updating the data and seeing whether the API returns the updated data:</p>
			<ol>
				<li value="1">Get all the recipe data back. Click on <strong class="bold">RecipeList</strong> and send the request. The result is shown in the following screenshot:<div><img src="img/C15309_09_05.jpg" alt="Figure 9.5: Get all the recipe data back and send the request&#13;&#10;"/></div><h6>Figure 9.5: Get all the recipe data back and send the request</h6></li>
				<li>Check the PyCharm console for the application log:<div><img src="img/C15309_09_06.jpg" alt="Figure 9.6: Checking the PyCharm console for the application log&#13;&#10;"/></div><h6>Figure 9.6: Checking the PyCharm console for the application log</h6><p>We can see that the cache is empty before the request. Then, after querying the database, the new data is cached.</p></li>
				<li>Log in to your account. Click on the <strong class="bold">Collections</strong> tab and select the <strong class="bold">POST Token</strong> request.</li>
				<li>Send the request. The result is shown in the following screenshot:<div><img src="img/C15309_09_07.jpg" alt="Figure 9.7: Selecting the POST Token request and sending the request&#13;&#10;"/></div><h6>Figure 9.7: Selecting the POST Token request and sending the request</h6></li>
				<li>Modify a recipe record using the <code>PATCH</code> method. First, select the <code>PATCH Recipe</code> request. Now, select the <code>Bearer {token}</code>; the token should be the access token.</li>
				<li>Select the <code>num_of_servings</code> to <code>10</code> and <code>cook_time</code> to <code>100</code>. Please check the following:<pre>{ 
    "num_of_servings": 10, 
    "cook_time": 100 
} </pre></li>
				<li>Send the request. The result is shown in the following screenshot:<div><img src="img/C15309_09_08.jpg" alt="Figure 9.8: Modifying the recipe record using the PATCH method&#13;&#10;"/></div><h6>Figure 9.8: Modifying the recipe record using the PATCH method</h6></li>
				<li>Check the PyCharm console for the application log:<div><img src="img/C15309_09_09.jpg" alt="Figure 9.9: Checking the application log&#13;&#10;"/></div></li>
			</ol>
			<h6>Figure 9.9: Checking the application log</h6>
			<p>We can see that the cache is there before the request. But after the recipe record is updated, the cache becomes stale and is removed. </p>
			<p>So, in this exercise, we have completed the testing of the cache-clearing function. This will ensure that we are getting the latest data.</p>
			<h4>Note</h4>
			<p class="callout">The <a id="_idTextAnchor397"/>printing of the application log is for testing only. Before we go on, we need to comment on the <code>print</code> command in <code>before_request</code> and <code>after_request</code>. We can do that by using <em class="italics">command + /</em> on a Mac, or <em class="italics">Ctrl + /</em> on a Windows machine.</p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor398"/>API Rate Limiting</h2>
			<p>When we provide an API service, we need to ensure fair usage for every user so that the system resources are effectively and fairly serving all. We want to make sure that the majority of users are getting good server performance; therefore, we need to apply restrictions. By limiting a small number of high-traffic users, we can make sure that the majority of users are satisfied.</p>
			<p>The way to do that is to set a limit per user. For example, we can limit the number of requests per user to be no more than 100 per second. This number will be enough for the normal usage of our API. If there is any particular case where a user is firing 100+ requests per second, the excess requests will not be handled. This is to reserve system resources (such as CPU processing and bandwidth resources) for other users.</p>
			<p>To achieve this, we introduce the concept of rate-limiting. By limiting the "rate" of our API service per user, we guarantee that the majority of our users are able to enjoy the service performance they deserve.</p>
			<h3 id="_idParaDest-177"><a id="_idTextAnchor399"/>HTTP Headers and Response Codes</h3>
			<p>We can use HTTP headers to display rate limit information. The following attributes in the HTTP headers can tell us the number of requests (the rate) allowed, the remaining quota, and when the limit will be reset:</p>
			<ul>
				<li><strong class="bold">X-RateLimit-Limit</strong>: Shows the rate limit of this API endpoint</li>
				<li><strong class="bold">X-RateLimit-Remaining</strong>: Shows the number of remaining requests allowed before the next reset</li>
				<li><strong class="bold">X-RateLimit-Reset</strong>: When the rate limit will be reset (in UTC epoch time)</li>
				<li><strong class="bold">Retry-After</strong>: The number of seconds before the next reset</li>
			</ul>
			<p>When a user starts to violate the rate limit, the API will return the HTTP status code <code>Too Many Requests</code>, with the error message in the response body:</p>
			<pre>{ 
    "errors": "Too Many Requests" 
}</pre>
			<p>To implement this rate limit function, we can use the Flask extension package Flask-Limiter. The Flask-Limiter package can help us easily add the rate limit function to our APIs.</p>
			<h2 id="_idParaDest-178">Flask-Lim<a id="_idTextAnchor400"/>iter</h2>
			<p><code>RATELIMIT_HEADERS_ENABLED</code> configuration. We, therefore, don't need to code the HTTP header information ourselves. Besides that, it also supports a configurable backend for storage with current implementations for Redis, in-memory, Memcached, and others.</p>
			<p>We can even set multiple limits; we just need to delimit them using a delimiter. For example, we can set the limit to be <code>100</code> requests per minute and <code>1000</code> requests per hour, at the same time.</p>
			<p>Use the following syntax to set up the rate limit for our API endpoint:</p>
			<pre>[count] [per|/] [n (optional)] [second|minute|hour|day|month|year]</pre>
			<p>Here are some examples:</p>
			<pre>100 per minute
100/minute
100/minute;1000/hour;5000/day</pre>
			<p>Now we understand how rate limits work. We will work on a practical exercise together to add this useful functionality to our Smilecook application.</p>
			<h3 id="_idParaDest-179">Exercise <a id="_idTextAnchor401"/>60: Implementing API Rate-Limiting Functionality</h3>
			<p>In this exercise, we will implement API rate-limiting functionality using <code>Flask-Limiter</code>. We will install and set <code>Flask-Limiter</code>, and then add the limit in rate-limit to <code>RecipeListResource</code>:</p>
			<ol>
				<li value="1">Add <code>Flask-Limiter</code> version <code>1.0.1</code> to <code>requirements.txt</code>:<pre>Flask-Limiter==1.0.1</pre></li>
				<li>Install t<a id="_idTextAnchor402"/>he package using the <code>pip install</code> command:<pre>pip install -r requirements.txt</pre><p>You should be seeing the following installation result:</p><pre>Installing collected packages: limits, Flask-Limiter
  Running setup.py install for limits ... done
  Running setup.py install for Flask-Limiter ... done
Successfully installed Flask-Limiter-1.0.1 limits-1.3</pre></li>
				<li>Import <code>Limiter</code> and <code>get_remote_address</code> in <code>extensions.py</code> and instantiate a <code>limiter</code> object:<pre>from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
limiter = Limiter(key_func=get_remote_address)</pre><p>The <code>get_remote_address</code> function will return the IP address for the current request. If the IP address is not found, it will return <code>127.0.0.1</code>, which means the localhost. Here, our strategy is to limit the rate per IP address.</p></li>
				<li>In <code>app.py</code>, import <code>limiter</code> from <code>extensions</code>:<pre>from extensions import db, jwt, image_set, cache, limiter</pre></li>
				<li>In <code>app.py</code>, initialize the <code>limiter</code> object under <code>register_extensions</code>. Pass in the <code>app</code> object to the <code>limiter.init_app</code> method:<pre>    limiter.init_app(app)</pre></li>
				<li>In <code>config.py</code>, set <code>RATELIMIT_HEADERS_ENABLED</code> to <code>True</code>:<pre>RATELIMIT_HEADERS_ENABLED = True</pre><p>This wil<a id="_idTextAnchor403"/>l allow Flask-Limiter to put in rate limit-related information in the HTTP header, including <strong class="bold">X-RateLimit-Limit</strong>, <strong class="bold">X-RateLimit-Remaining</strong>, <strong class="bold">X-RateLimit-Reset</strong>, and <strong class="bold">Retry-After</strong>.</p></li>
				<li>In <code>resources/recipe.py</code>, import <code>limiter</code> from <code>extensions</code>:<pre>from extensions import image_set, cache,  limiter</pre></li>
				<li>In <code>RecipeListResource</code>, put the <code>limiter.limit</code> function in the <code>decorators</code> attribute:<pre>class RecipeListResource(Resource):
    decorators = [limiter.limit('2 per minute', methods=['GET'], error_message='Too Many Requests')]</pre><p>We are setting the number of requests to be only two per minute. The HTTP method is <code>GET</code> and the error message is <code>Too Many Requests</code>.</p></li>
				<li>Click <strong class="bold">Ru<a id="_idTextAnchor404"/>n</strong> to start the Flask application; then, we are ready to test it:<div><img src="img/C15309_09_10.jpg" alt="Figure 9.10: Start the Flask application and then test it&#13;&#10;"/></div></li>
			</ol>
			<h6>Figure 9.10: Start the Flask application and then test it</h6>
			<p>Now that this exercise is complete, our API has rate-limiting functionality. In the next exercise, we have to test our rate limit function.</p>
			<h3 id="_idParaDest-180"><a id="_idTextAnchor405"/>Exercise 61: Verifying the Rate-Limit Function</h3>
			<p>In the last exercise, we set the API for getting all recipe details, which can only be obtained twice per minute. So, in this exercise, we will see whether the result is what we expected:</p>
			<ol>
				<li value="1">Get all the recipe data back. Click on GET <code>RecipeList</code> and send the request.</li>
				<li>Select the <code>60</code> seconds later.</p></li>
				<li>Get all the recipe data back again and send the request twice more.</li>
				<li>Select the <strong class="bold">Body</strong> in the HTTP response. The result is shown in the following screenshot:<div><img src="img/C15309_09_12.jpg" alt="Figure 9.12: Getting all the recipe data back again and sending the request twice&#13;&#10;"/></div></li>
			</ol>
			<h6>Figure 9.12: Getting all the recipe data back again and sending the request twice</h6>
			<p>We can see that at the third request, we will receive the error HTTP status code <strong class="bold">429 TOO MANY REQUESTS</strong>. That means the rate limit is working.</p>
			<p>In this exercise, we have completed the rate limit function. By restricting a small number of abusive users, we ensure that the majority of users can enjoy high servicing performance.</p>
			<h3 id="_idParaDest-181">Exercise 62:<a id="_idTextAnchor406"/> Adding a Whitelist</h3>
			<p>We want to ease the rate limit for our developers, the testers of the API, because they may indeed need to fire frequent requests to the APIs for testing. What should we do in this case? In this exercise, we shall see how we can use Flask-Limiter to satisfy this requirement.</p>
			<p>We would like to set up an IP whitelist that can allow certain IP addresses to use the API without any rate limit:</p>
			<ol>
				<li value="1">In <code>app.py</code>, import <code>request</code>:<pre>from flask import Flask, request</pre></li>
				<li>In <code>app.py</code>, use the <code>@limiter.request_filter</code> decorator and set up the whitelist function. Put <code>127.0.0.1</code> (localhost) in the whitelist:<pre>    @limiter.request_filter
    def ip_whitelist():
        return request.remote_addr == '127.0.0.1'</pre></li>
				<li>Run <code>app.py</code>:<div><img src="img/C15309_09_13.jpg" alt="Figure 9.13: Running the app.py file&#13;&#10;"/></div><h6>Figure 9.13: Running the app.py file</h6></li>
				<li>Test the application by firing a <code>GET</code> all recipe request, and check the HTTP header for the rate limit. Click on <code>GET RecipeList</code> and send the request. Select the <strong class="bold">Header</strong> tab in <strong class="bold">Response</strong>. The result is shown in the following screenshot:<div><img src="img/C15309_09_14.jpg" alt="Figure 9.14: Checking the HTTP header for rate limit&#13;&#10;"/></div></li>
			</ol>
			<h6>Figure 9.14: Checking the HTTP header for rate limit</h6>
			<p>We can see that the rate-limit restriction is gone. In this exercise, you have seen that the rate-limiting function can be flexible. It can be enacted or withdrawn depending on different situations.</p>
			<h3 id="_idParaDest-182">Activity 17: A<a id="_idTextAnchor407"/>dding Multiple Rate-Limit Restrictions</h3>
			<p>In this activity, we are going to add multiple rate-limit restrictions to the same resource. But remember, we added a whitelist to the previous exercise. We need to comment out that code, so we can test it:</p>
			<ol>
				<li value="1">In <code>UserRecipeListResource</code>, add the rate limit. The limit is <code>3</code> times per minute, <code>30</code> times per hour, and <code>300</code> times a day.</li>
				<li>Comment out the whitelist code.</li>
				<li>Test the rate limit function using Postman.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 343.</p></li>
			</ol>
			<p>Congratulations! Now that you have completed this activity, you know how to flexibly use the rate-limiting function.</p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor408"/>Summary</h2>
			<p>In this chapter, we have learned about and implemented caching and rate-limiting functions in our Smilecook API. Such functions make our APIs even more efficient. Our Smilecook application is saving the cache in application memory, though, which means the cache will be gone after a server reboot. To address this, we can work with Redis or Memcached in the future, which can persist the cache even after a server reboot. They also support sharing the cache with multiple servers. This is something we encourage you to explore outside of this book. The most important thing at the moment is for you to learn all the fundamental concepts covered in this book. So, later, if you want to extend to more advanced implementation, it shouldn't be too hard for you.</p>
			<p>In the next and final chapter, we will build the Smilecook frontend client for you to work with the backend API. Through this frontend client, we will understand the whole picture better. You will see how the frontend and backend interact. Finally, we will deploy the whole application to the Heroku cloud platform, which means our Smilecook application will be used by everybody.</p>
		</div>
	</body></html>