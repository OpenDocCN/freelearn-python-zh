<html><head></head><body>
<div><div><div><h1 id="_idParaDest-261"><em class="italic"><a id="_idTextAnchor244"/>Chapter 14</em>: Race Conditions</h1>
			<p>In this chapter, we will discuss the concept of <strong class="bold">race conditions</strong> and their potential causes in the context of concurrency. The definition of a critical section, which is a concept highly relevant to race conditions and concurrent programming, will also be covered. We will use some example code in Python to simulate race conditions and the solutions that are commonly used to address them. Finally, real-life applications that commonly deal with race conditions will also be discussed.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>The concept of race conditions</li>
				<li>Simulating race conditions in Python</li>
				<li>Locks as a solution to race conditions</li>
				<li>Race conditions in real life</li>
			</ul>
			<p>This chapter, similar to the previous two chapters, offers a closer look at what could go wrong in concurrent programming and exposes us to a wide range of approaches in terms of how to avoid and prevent that. In this chapter, our focus will be on race conditions.</p>
			<h1 id="_idParaDest-262"><a id="_idTextAnchor245"/>Technical requirements</h1>
			<p>The code for this chapter can be found in the following GitHub repository: <a href="https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter14">https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter14</a>.</p>
			<h1 id="_idParaDest-263"><a id="_idTextAnchor246"/>The concept of race conditions</h1>
			<p>Typically, a <strong class="bold">race condition</strong> is defined as a phenomenon during which the output<a id="_idIndexMarker1053"/> of a system is both indeterminate and dependent on the scheduling algorithm and the order in which tasks are scheduled and executed. When data becomes mishandled or corrupted during this process, a race condition becomes a bug in the system. Given the nature of this problem, it is quite common for a race condition to occur in concurrent systems, which emphasizes the importance of scheduling and coordinating independent tasks.</p>
			<p>A race condition can occur in both an electronic hardware system and a software application; in this chapter, we will only discuss race conditions in the context of software development – specifically, <strong class="bold">concurrent software applications</strong>. This section will cover the theoretical foundations of race conditions and their root causes along with the concept of <em class="italic">critical sections</em>.</p>
			<h2 id="_idParaDest-264"><a id="_idTextAnchor247"/>Critical sections</h2>
			<p><strong class="bold">Critical sections</strong> indicate shared resources that are accessed<a id="_idIndexMarker1054"/> by multiple processes or threads in a concurrent application, These can lead to unexpected, and even erroneous, behavior. We have learned<a id="_idIndexMarker1055"/> that there are multiple methods to protect the integrity of the data contained in these resources, and we call these protected sections <em class="italic">critical sections</em>.</p>
			<p>As you can imagine, the data in these critical sections, when interacted with and altered concurrently or in parallel, can become mishandled or corrupted. This is especially true when the threads and processes interacting with it are poorly coordinated and scheduled. Therefore, the logical conclusion is to not allow multiple agents to go into a critical section<a id="_idIndexMarker1056"/> at the same time. We call this concept <strong class="bold">mutual exclusion</strong>.</p>
			<p>In the next subsection, we will discuss the relationship between critical sections and the causes of race conditions.</p>
			<h2 id="_idParaDest-265"><a id="_idTextAnchor248"/>How race conditions occur</h2>
			<p>Let's consider a simple concurrent<a id="_idIndexMarker1057"/> program in order to understand what can give rise to a race condition:</p>
			<ol>
				<li>Suppose that the program has a shared resource and two separate threads (<em class="italic">thread 1</em> and <em class="italic">thread 2</em>) that will access and interact with that resource. Specifically, the shared resource is a number and, as per their respective execution instructions, each thread is to read in that number, increment it by 1, and finally, update the value of the shared resource with the incremented number.</li>
				<li>Next, suppose that the shared number is originally 2, and then thread 1 accesses and interacts with the number; the shared resource will become 3.</li>
				<li>After thread 1 successfully alters and exits the resource, thread 2 begins to execute its instructions, and the shared resource that is a number is updated to 4. Throughout this process, the number that was originally 2 was incremented twice (each time by a separate thread) and held a value of 4 at the end. In this case, the shared number was not mishandled or corrupted.</li>
				<li>Then, imagine a scenario in which the shared number is still 2 at the beginning, yet both of the threads access the number at the same time. Now, each of the threads reads in the number 2 from the shared resource; they each increment the number 2 to 3 individually, and then write the number 3 back to the shared resource. Even though the shared resource was accessed and interacted with twice by a thread, it only held a value of 3 at the end of the process.</li>
			</ol>
			<p>This is an example of a race<a id="_idIndexMarker1058"/> condition occurring in a concurrent program: since the second thread to access a shared resource does it before the first thread finishes its execution (in other words, before writing the new value to the shared resource), the second thread fails to take in the updated resource value. This leads to the fact that when the second thread writes to the resource, the value that is processed and updated by the first thread is overwritten. At the end of the execution of the two threads, the shared resource has, technically, only been updated by the second thread.</p>
			<p>The following diagram further illustrates the contrast between a correct data handling process and a situation involving a race condition:</p>
			<div><div><img src="img/_B17499Figure_14.1.jpg" alt="Figure 14.1 – Mishandling shared data " width="1332" height="988"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.1 – Mishandling shared data</p>
			<p>Intuitively, we can see that a race condition results in the mishandling and corruption of data. In the preceding example, we can see that a race condition only occurs with two separate threads accessing a common resource, causing the shared resource to be updated incorrectly and hold an incorrect value at the end of the program. We understand that most real-life concurrent applications contain significantly more threads and processes along<a id="_idIndexMarker1059"/> with additional shared resources, and the more threads/processes that interact with the shared resource, the more likely it is that a race condition will occur.</p>
			<p>Before we discuss a solution that we can implement to solve the problem of race conditions, let's try to simulate the problem in Python.</p>
			<h1 id="_idParaDest-266"><a id="_idTextAnchor249"/>Simulating race conditions in Python</h1>
			<p>If you have already downloaded<a id="_idIndexMarker1060"/> the code for this book from the GitHub<a id="_idIndexMarker1061"/> page, go ahead and navigate to the <code>Chapter14</code> folder.</p>
			<p>To simulate a race condition, first, we need a common resource. In this case, it's a counter variable along with multiple threads that can access it simultaneously. Let's take a look at the <code>Chapter14/example1.py</code> file—specifically, the <code>update()</code> function, as follows:</p>
			<pre>import random
import time
def update():
    global counter
    current_counter = counter # reading in shared resource
    time.sleep(random.randint(0, 1)) # simulating heavy 
    calculations
    counter = current_counter + 1 # updating shared 
    resource</pre>
			<p>The goal of the preceding <code>update()</code> function is to increment a global variable called <code>counter</code>, and it is to be called by a separate thread in our script. Inside the function, we are interacting with a shared resource—in this case, <code>counter</code>. Then, we assign the value of <code>counter</code> to another local variable, called <code>current_counter</code> (this is to simulate the process of reading data from more complex data structures for the shared resources).</p>
			<p>Next, we will pause the execution of the function by using the <code>time.sleep()</code> method. The length of the period during which the program will be paused is pseudo-randomly chosen between <code>0</code> and <code>1</code>, generated by the function call, <code>random.randint(0, 1)</code>. So, the program will either pause for one second or not at all. Finally, we assign the newly computed value of <code>current_counter</code> (which is its one-increment) to the originally shared resource (the <code>counter</code> variable).</p>
			<p>Now, we can move on to our main program:</p>
			<pre>import threading
counter = 0
threads = [threading.Thread(target=update) for i in \
  range(20)]
for thread in threads:
    thread.start()
for thread in threads:
    thread.join()
print(f'Final counter: {counter}.')
print('Finished.')</pre>
			<p>Here, we are initializing the <code>counter</code> global variable with a set of <code>threading.Thread</code> objects in order to execute the <code>update()</code> function concurrently; we are initializing 20 <code>thread</code> objects to increment our shared counter 20 times. After starting<a id="_idIndexMarker1062"/> and joining all of the threads that we have, we can finally<a id="_idIndexMarker1063"/> print out the end value of our shared <code>counter</code> variable.</p>
			<p>Theoretically, a well-designed concurrent program will successfully increment the shared counter 20 times in total, and since its original value is <code>0</code>, the end value of the counter should be <code>20</code> at the end of the program. However, as you run this script, the <code>counter</code> variable that you obtain will most likely not hold an end value of <code>20</code>. The following is my output, obtained from running the script:</p>
			<pre>&gt; python3 example1.py
Final counter: 9.
Finished.</pre>
			<p>The preceding output indicates that the counter was only successfully incremented nine times. This is a direct result of the race condition that our concurrent program has. This race condition occurs when a specific thread spends time reading in and processing the data from the shared resource (specifically, for one second using the <code>time.sleep()</code> method), and another thread reads in the current value of the <code>counter</code> variable, which, at this point, has not been updated by the first thread since it has not completed its execution.</p>
			<p>Interestingly, if a thread<a id="_idIndexMarker1064"/> does not spend any time processing<a id="_idIndexMarker1065"/> the data (in other words, when <code>0</code> is chosen by the pseudo-random <code>random.randint()</code> method), the value of the shared resource can potentially be updated just in time for the next thread to read and process it. This phenomenon is illustrated by the fact that the end value of the counter varies within different runs of the program. For example, the following is the output that I obtained after running the script three times. The output from the first run is as follows:</p>
			<pre>&gt; python3 example1.py
Final counter: 9.
Finished.</pre>
			<p>The output from the second run is as follows:</p>
			<pre>&gt; python3 example1.py
Final counter: 12.
Finished.</pre>
			<p>The output from the third run is as follows:</p>
			<pre>&gt; python3 example1.py
Final counter: 5.
Finished.</pre>
			<p>Again, the final value of the counter is dependent on the number of threads that spend one second pausing and the number of threads not pausing at all. Since these two numbers are, in turn, dependent on the <code>random.randint()</code> method, the final value of the counter changes between<a id="_idIndexMarker1066"/> different runs of the program. We will still have a race condition<a id="_idIndexMarker1067"/> in our program, except for when we can ensure that the final value of the counter is always <code>20</code> (that is, the counter is being successfully incremented 20 times in total).</p>
			<p>In the next section, we will discuss the most common solution to race conditions: locks.</p>
			<h1 id="_idParaDest-267"><a id="_idTextAnchor250"/>Locks as a solution to race conditions</h1>
			<p>Intuitively, since the race conditions that we observed arose when multiple threads or processes accessed and wrote to a shared resource simultaneously, the key idea behind solving race conditions<a id="_idIndexMarker1068"/> is isolating the executions of different threads/processes, especially when interacting with a shared resource. Specifically, we need<a id="_idIndexMarker1069"/> to make sure that a thread/process can only access the shared resource after any other threads/processes interacting with the resource have finished their interactions with that resource.</p>
			<p>With locks, we can turn a shared resource inside a concurrent program into a critical section, whose integrity of data is guaranteed to be protected. We will see this in action next.</p>
			<h2 id="_idParaDest-268"><a id="_idTextAnchor251"/>The effectiveness of locks</h2>
			<p>A critical section guarantees<a id="_idIndexMarker1070"/> the mutual exclusion of a shared resource and cannot be accessed concurrently by multiple processes or threads; this will prevent any protected data from being updated or altered with conflicting information, resulting from race conditions.</p>
			<p>In the following diagram, <code>var</code>—by a <strong class="bold">mutex</strong> (<strong class="bold">mutual exclusion</strong>) lock. This is because <strong class="bold">Thread A</strong> is already accessing the resource:</p>
			<div><div><img src="img/_B17499Figure_14.2.jpg" alt="Figure 14.2 – Locks prevent simultaneous access to a critical section " width="1053" height="292"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.2 – Locks prevent simultaneous access to a critical section</p>
			<p>Now, we will specify that, in order to gain access to a critical section in a concurrent program, a thread or process needs to acquire a lock object that is associated with the critical section; similarly, that thread or process also needs to release that lock upon leaving the critical section. This setup will effectively prevent multiple accesses to the critical section and will, therefore, prevent race conditions. The following diagram illustrates the execution flow of multiple threads interacting with multiple critical sections, with the implementation of locks in place:</p>
			<div><div><img src="img/_B17499Figure_14.3.jpg" alt="Figure 14.3 – Locks and critical sections in multiple threads " width="1650" height="306"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.3 – Locks and critical sections in multiple threads</p>
			<p>As you can see in the preceding<a id="_idIndexMarker1072"/> diagram, threads <strong class="bold">T1</strong> and <strong class="bold">T2</strong> both interact with three critical sections in their respective execution instructions: <strong class="bold">CS1</strong>, <strong class="bold">CS2</strong>, and <strong class="bold">CS3</strong>. Here, <strong class="bold">T1</strong> and <strong class="bold">T2</strong> attempt to access <strong class="bold">CS1</strong> at almost the same time. Additionally, since <strong class="bold">CS1</strong> is protected with lock <strong class="bold">L1</strong>, only <strong class="bold">T1</strong> can acquire lock <strong class="bold">L1</strong> and, hence, access/interact with the critical section. In contrast, <strong class="bold">T2</strong> has to spend time waiting for <strong class="bold">T1</strong> to exit out of the critical section and release the lock before accessing the section itself. Similarly, for the critical sections, <strong class="bold">CS2</strong> and <strong class="bold">CS3</strong>, although both threads require access to a critical section at the same time, only one can process it, while the other has to wait to acquire the lock associated with the critical section.</p>
			<p>Now, let's implement this solution using <em class="italic">Python</em>.</p>
			<h2 id="_idParaDest-269"><a id="_idTextAnchor252"/>Implementation in Python</h2>
			<p>Navigate<a id="_idIndexMarker1073"/> to the <code>Chapter14/example2.py</code> file and consider<a id="_idIndexMarker1074"/> our corrected <code>update()</code> function, as follows:</p>
			<pre>import random
import time
def update():
    global counter
    with count_lock:
        current_counter = counter # reading in shared 
        resource
        time.sleep(random.randint(0, 1)) # simulating heavy 
        calculations
        counter = current_counter + 1</pre>
			<p>Here, you can see that all of the execution instructions of a thread specified in the <code>update()</code> function are under the context manager of a lock object named <code>count_lock</code>. So, every time a thread is called to run the function, it will first have to acquire the lock object before any instructions can be executed. In our main program, we simply create the lock object in addition to what we already have, as follows:</p>
			<pre>import threading
counter = 0
count_lock = threading.Lock()
threads = [threading.Thread(target=update) for i in \
  range(20)]
for thread in threads:
    thread.start()
for thread in threads:
    thread.join()
print(f'Final counter: {counter}.')
print('Finished.')</pre>
			<p>Run the program. Your output should look similar to the following:</p>
			<pre>&gt; python3 example2.py
Final counter: 20.
Finished.</pre>
			<p>Here, you can see that the counter was successfully incremented 20 times and held the correct value at the end of the program. Furthermore, no matter how many times the script is executed, the final<a id="_idIndexMarker1075"/> value of the counter will always be <strong class="bold">20</strong>. This is the advantage<a id="_idIndexMarker1076"/> of using locks to implement critical sections in your concurrent programs.</p>
			<h2 id="_idParaDest-270"><a id="_idTextAnchor253"/>The downside of locks</h2>
			<p>In <a href="B17499_12_Final_SS_ePub.xhtml#_idTextAnchor215"><em class="italic">Chapter 12</em></a>, <em class="italic">Deadlocks</em>, we covered an interesting<a id="_idIndexMarker1077"/> phenomenon in which the use of locks can lead to undesirable results. Specifically, we discovered that with enough locks implemented in a concurrent program, the whole program can become sequential. Let's analyze this concept with our current program. Consider the <code>Chapter14/example3.py</code> file, as follows:</p>
			<pre>import threading
import random; random.seed(0)
import time
def update(pause_period):
    global counter
    with count_lock:
        current_counter = counter # reading in shared 
        resource
        time.sleep(pause_period) # simulating heavy 
        calculations
        counter = current_counter + 1 # updating shared 
        resource
pause_periods = [random.randint(0, 1) for i in range(20)]
###############################################################
counter = 0
count_lock = threading.Lock()
start = time.perf_counter()
for i in range(20):
    update(pause_periods[i])
print('--Sequential version--')
print(f'Final counter: {counter}.')
print(f'Took {time.perf_counter() - start : .2f} seconds.')
###############################################################
counter = 0
threads = [threading.Thread(target=update, \
  args=(pause_periods[i],)) for i in range(20)]
start = time.perf_counter()
for thread in threads:
    thread.start()
for thread in threads:
    thread.join()
print('--Concurrent version--')
print(f'Final counter: {counter}.')
print(f'Took {time.perf_counter() - start : .2f} seconds.')
###############################################################
print('Finished.')</pre>
			<p>The goal of this script is to compare the speed of our current concurrent program with its sequential version. Here, we are still using the same <code>update()</code> function, with locks, and we are running it 20 times, both sequentially and concurrently, as we did earlier. Additionally, we are creating a list of determined periods for pausing:</p>
			<pre>pause_periods = [random.randint(0, 1) for i in range(20)]</pre>
			<p>This is so that these periods are consistent between when we simulate the sequential version and when we simulate the concurrent version (for this reason, the <code>update()</code> function now takes<a id="_idIndexMarker1078"/> in a parameter that specifies the period of pausing each time it is called). </p>
			<p>This is the setup we need to simulate the <em class="italic">sequentiality</em> of a program with many locks, which we will see firsthand in the next subsection.</p>
			<h3 id="_idParaDest-271">Turning a concurrent program into a sequential program</h3>
			<p>Here, we simply call the <code>update()</code> function inside a <code>for</code> loop, with 20 iterations, keeping track of the time it takes<a id="_idIndexMarker1079"/> for the loop to finish. Note that, even though this is to simulate the sequential version<a id="_idIndexMarker1080"/> of the program, the <code>update()</code> function still needs the lock object to be created beforehand, so we are initializing it here:</p>
			<pre>counter = 0
count_lock = threading.Lock()
start = time.perf_counter()
for i in range(20):
    update(pause_periods[i])
print('--Sequential version--')
print(f'Final counter: {counter}.')
print(f'Took {time.perf_counter() - start : .2f} seconds.')</pre>
			<p>The last step is to reset the counter and run the concurrent version of the program that we already implemented. Again, we need to pass in the corresponding pause period while initializing each of the threads that run the <code>update()</code> function. Additionally, we are keeping track of the time it takes for this concurrent version of the program to run:</p>
			<pre>counter = 0
threads = [threading.Thread(target=update, \
args=(pause_periods[i],)) for i in range(20)]
start = time.perf_counter()
for thread in threads:
    thread.start()
for thread in threads:
    thread.join()
print('--Concurrent version--')
print(f'Final counter: {counter}.')
print(f'Took {time.perf_counter() - start : .2f} seconds.')</pre>
			<p>Now, after you have run the script, you will observe that both the sequential version and the concurrent version of our program took the same amount of time to run. Specifically, the following<a id="_idIndexMarker1081"/> is the output that I obtained; in this case, they both<a id="_idIndexMarker1082"/> took approximately 12 seconds. The actual time that your program takes might be different, but the speed of the two versions should still be equal:</p>
			<pre>&gt; python3 example3.py
--Sequential version--
Final counter: 20.
Took 12.03 seconds.
--Concurrent version--
Final counter: 20.
Took 12.03 seconds.
Finished.</pre>
			<p>So, our concurrent program is taking just as much time as its sequential version, which negates one of the biggest purposes of implementing concurrency in a program: <em class="italic">improving speed</em>. But why would concurrent and traditional sequential applications with the same sets of instructions and elements also have the same speed? Should the concurrent program always produce a faster speed than the sequential one?</p>
			<p>Recall that, in our program, the critical section is being protected by a lock object, and no multiple threads can access it at the same time. Since the execution of the program (incrementing the counter 20 times) depends on a thread accessing the critical section, the placement of the lock object<a id="_idIndexMarker1083"/> in the critical section means that only one thread can be executing at any given time. With this specification, the executions of any two threads cannot overlap with each other, and no additional speed can be gained from this implementation of concurrency.</p>
			<p>This is the phenomenon<a id="_idIndexMarker1084"/> that we encountered when analyzing the problem of deadlock: if enough locks are placed in a concurrent program, that program will become entirely sequential. This is the reason why locks are sometimes undesirable solutions to problems in concurrent programming. However, this situation only happens if all of the executions of the concurrent program are dependent upon interacting with the critical section. Most of the time, reading and manipulating the data of a shared resource is only a portion of the entire program; therefore, concurrency still provides the intended additional speed for our program.</p>
			<p>An additional aspect of locks is the fact that they do not actually lock anything. We will discuss this point in more detail next.</p>
			<h3 id="_idParaDest-272">Locks do not lock anything</h3>
			<p>The only way that a lock object is utilized, with respect to a specific shared resource, is for the threads<a id="_idIndexMarker1085"/> and processes interacting with that resource to also interact with the lock. In other words, if those threads and processes choose to not check with the lock before accessing and altering the shared resource, the lock object itself cannot stop them from doing so.</p>
			<p>In our examples, you have discovered that to implement the acquiring/releasing process of a lock object, the instructions of a thread or process need to be wrapped around by a lock context manager; this specification is dependent on the implementation of <em class="italic">the thread/process execution logic</em> and not the resource. That is because the lock objects that we have seen are not in any way connected to the resources that they are supposed to protect. So, if the thread/process execution logic does not require any interaction with the lock object associated with the shared resource, that thread or process can simply gain access to the resource without difficulty, potentially resulting in the <em class="italic">mismanipulation</em> and corruption of data.</p>
			<p>This is not only true in the scope of having multiple threads and processes in a single concurrent program. Let's suppose that we have a concurrent system consisting of multiple components that all interact and manipulate the data of a resource shared across the system, and this resource is associated with a lock object; it follows that, if any of these components fail to interact with that lock, it can simply bypass the protection implemented by the lock and access the shared resource. More importantly, this characteristic of locks also has implications regarding the security of a concurrent program. If an outside, malicious agent is connected to the system (for instance, a malicious client interacting with a server) and intends to corrupt the data shared across the system, that agent can be instructed to simply ignore the lock object and access that data in an intrusive way.</p>
			<p>The view that locks don't lock anything was popularized by Raymond Hettinger, a Python core developer who worked on the implementation of various elements in Python concurrent programming. It is argued that using lock objects alone does not guarantee a secure implementation of concurrent data structures and systems. Locks need to be concretely linked to the resources that they are supposed to protect, and nothing should be able to access a resource without first acquiring the lock that is associated with it. Alternatively, other concurrent synchronization tools, such as atomic message queues can provide a solution to this problem.</p>
			<p>You have now learned about the concept of race conditions, how they are caused in concurrent systems, and how to effectively prevent them. In the next section, we will provide an overarching view of how race conditions can occur in real-life examples, within the various subfields of computer science.</p>
			<h1 id="_idParaDest-273"><a id="_idTextAnchor254"/>Race conditions in real life</h1>
			<p>In particular, we will discuss the topics of security, file management, and networking. Race conditions don't simply exist<a id="_idIndexMarker1086"/> in simple, minimal code examples about global counters. They are present in many important tasks such as security, file management, and networking. In this section, we will briefly discuss what some of these examples might look like from a theoretical perspective.</p>
			<h2 id="_idParaDest-274"><a id="_idTextAnchor255"/>Security</h2>
			<p>Concurrent programming can<a id="_idIndexMarker1087"/> have significant implications in terms of the security of the system in question. Recall that a race condition arises between the process of reading and altering the data of a resource; a race condition in an authenticating system can cause the corruption of data between the <strong class="bold">time of check</strong> (when the credentials of an agent are checked) and the <strong class="bold">time of use</strong> (when the agent can utilize the resource). This problem is also known as a <strong class="bold">Time-Of-Check-To-Time-Of-Use</strong> (<strong class="bold">TOCTTOU</strong>) bug, which is undoubtedly detrimental<a id="_idIndexMarker1088"/> to security systems.</p>
			<p>The careless protection of shared resources when handling race conditions, as we briefly touched upon in the last section, can provide external agents with access to those supposedly<a id="_idIndexMarker1089"/> protected resources. Those agents can then change the data of the resources to create <strong class="bold">privilege escalation</strong> (simply put, to give themselves illegal access to more shared resources), or they can simply corrupt the data, causing the whole system to malfunction.</p>
			<p>Interestingly, race conditions can also be used to implement computer security. As race conditions result from the uncoordinated access of multiple threads/processes to a shared resource, the specification in which a race condition occurs is significantly random. For example, in our Python example, you learned that, when simulating a race condition, the final value of the counter varies between different executions of the program; this is (partly) because of the unpredictable nature of the situation in which multiple threads are running and accessing the shared resources. (I say partly since the randomness also results from the random pausing periods that we generate in each execution of the program.) So, race conditions are sometimes intentionally provoked, and the information obtained when the race condition occurs can be used to generate digital fingerprints for security processes—again, this information is significantly random and is, therefore, valuable for security purposes.</p>
			<h2 id="_idParaDest-275"><a id="_idTextAnchor256"/>Operating systems</h2>
			<p>Race conditions can occur in the context of file and memory management in an operating system, when two<a id="_idIndexMarker1090"/> separate programs attempt to access the same resource, such as memory space. Imagine a situation where two processes from different programs have been running for a significant amount of time, and even though they were originally initialized apart from each other in terms of memory space, enough data has been accumulated and the stack of execution of one process now collides with that of the other process. This can lead to the two processes sharing the same portion of memory space and, ultimately, can result in unpredictable consequences.</p>
			<p>Another aspect of the complexity of race conditions is illustrated by version 7 of Unix's operating system — specifically, in the <code>mkdir</code> command. Typically, the <code>mkdir</code> command is used to create a new directory in the Unix operating system; this is done by calling the <code>mknod</code> command to create the actual directory and the <code>chown</code> command to specify the owner of that directory. Because there are two separate commands to be run and a definite gap exists between when the first command is finished and the second is called, this can cause a race condition.</p>
			<p>During the gap between the two commands, if someone deletes the new directory created by the <code>mknod</code> command and links the reference to another file, when the <code>chown</code> command is run, the ownership of that file will change. The following diagram further illustrates this exploitation:</p>
			<div><div><img src="img/_B17499Figure_14.4.jpg" alt="Figure 14.4 – The mkdir race condition " width="993" height="345"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.4 – The mkdir race condition</p>
			<p>By exploiting this vulnerability, someone can, theoretically, change the ownership of any file in an operating system so that someone can create a new directory.</p>
			<h2 id="_idParaDest-276"><a id="_idTextAnchor257"/>Networking</h2>
			<p>In networking, race conditions can take the form of giving<a id="_idIndexMarker1091"/> multiple users unique privileges in a network. Specifically, let's say a given server should only have exactly one user with admin privileges. If two users, who are both eligible to become the server admin, request access to those privileges at the same time, then it is possible for both of them to gain that access. This is because, at the point when both of the user requests are received by the server, neither of the users have been granted admin privileges yet, and the server thinks that admin privileges can still be given out.</p>
			<p>This form of a race condition is quite common when a network is highly optimized for parallel processing (for example, non-blocking sockets), without any careful consideration of the resources shared across the network. </p>
			<p>Overall, race conditions can manifest themselves in many important tasks in computer science and engineering, such as security, operating systems, and as we just saw, networking. This requires the concurrency engineer to be extra vigilant about the correctness of their programs.</p>
			<h1 id="_idParaDest-277"><a id="_idTextAnchor258"/>Summary</h1>
			<p>A race condition occurs when two or more threads/processes access and alter a shared resource simultaneously, resulting in mishandled and corrupted data. Race conditions also have significant implications in real-life applications, such as security, operating systems, and networking.</p>
			<p>In this chapter, we learned how to isolate the execution of different threads/processes to tackle many forms of race conditions. We have examined how to use locks to turn a shared resource into a critical section to protect the integrity of its data. Additionally, we have discussed a number of practical disadvantages when it comes to using locks.</p>
			<p>In the next chapter, we will consider one of the biggest problems in Python concurrent programming: the infamous <strong class="bold">Global Interpreter Lock</strong> (<strong class="bold">GIL</strong>). You will learn about the basic idea behind the GIL, its purposes, and how to effectively work with it in concurrent Python applications.</p>
			<h1 id="_idParaDest-278"><a id="_idTextAnchor259"/>Questions</h1>
			<ol>
				<li value="1">What is a critical section?</li>
				<li>What is a race condition, and why is it undesirable in a concurrent program?</li>
				<li>What is the underlying cause of a race condition?</li>
				<li>How can locks solve the problem of race conditions?</li>
				<li>Why are locks sometimes undesirable in a concurrent program?</li>
				<li>What is the significance of race conditions in real-life systems and applications?</li>
			</ol>
			<h1 id="_idParaDest-279"><a id="_idTextAnchor260"/>Further reading</h1>
			<p>For more information, please refer to the following resources:</p>
			<ul>
				<li><em class="italic">Parallel Programming with Python</em>, by Jan Palach, Packt Publishing Ltd, 2014.</li>
				<li><em class="italic">Python Parallel Programming Cookbook</em>, by Giancarlo Zaccone, Packt Publishing Ltd, 2015.</li>
				<li><em class="italic">Race Conditions and Critical Sections</em> (tutorials.jenkov.com/java-concurrency/race-conditions-and-critical-sections), by Jakob Jenkov.</li>
				<li><em class="italic">Race conditions, files, and security flaws; or the tortoise and the hare redux</em>, by Matt Bishop, Technical Report CSE-95-98 (1995).</li>
				<li><em class="italic">Computer and Information Security, </em><a href="B17499_11_Final_SS_ePub.xhtml#_idTextAnchor195"><em class="italic">Chapter 11</em></a><em class="italic">, Software Flaws and Malware 1 Illustration</em> (slideplayer.com/slide/10319860/).</li>
			</ul>
		</div>
	</div>
</div>
</body></html>