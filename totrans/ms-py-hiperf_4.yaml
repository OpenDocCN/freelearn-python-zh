- en: Chapter 4. Optimize Everything
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The path to mastering performance in Python has just started. Profiling only
    takes us half way there. Measuring how our program is using the resources at its
    disposal only tells us where the problem is, not how to fix it. In the previous
    chapters, we saw some practical examples when going over the profilers. We did
    some optimization, but we never really explained a lot about it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the process of optimization, and to do that,
    we need to start with the basics. We''ll keep it inside the language for now:
    no external tools, just Python and the right way to use it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Memoization / lookup tables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usage of default arguments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List comprehension
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ctypes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: String concatenation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other tips and tricks of Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memoization / lookup tables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is one of the most common techniques used to improve the performance of
    a piece of code (namely a function). We can save the results of expensive function
    calls associated with a specific set of input values and return the saved result
    (instead of redoing the whole computation) when the function is called with the
    remembered input. It might be confused with caching, since this is one type of
    memoization, although this term also refers to other types of optimization (such
    as HTTP caching, buffering, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: This methodology is very powerful because in practice, it'll turn what should
    have been a potentially very expensive call into a *O(1)* function call (for more
    information about this, refer to [Chapter 1](ch01.html "Chapter 1. Profiling 101"),
    *Profiling 101*) if the implementation is right. Normally, the parameters are
    used to create a unique key, which is then used on a dictionary to either save
    the result or obtain it if it's been already saved.
  prefs: []
  type: TYPE_NORMAL
- en: There is, of course, a trade-off to this technique. If we're going to remember
    the returned values of a memoized function, then we'll be exchanging memory space
    for speed. This is a very acceptable trade-off, unless the saved data becomes
    more than what the system can handle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Classic use cases for this optimization are function calls that repeat the
    input parameters often. This will assure that most of the time, the memoized results
    are returned. If there are many function calls, but with different parameters,
    we''ll only store results and spend our memory without any real benefit, as shown
    in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Memoization / lookup tables](img/B02088_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You can clearly see how the blue bar (**Fixed params, memoized**) is clearly
    the fastest use case, while the others are all similar due to their nature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code that generates values for the preceding chart. To generate
    some sort of time-consuming function, the code will call either the `twoParams`
    function or the `twoParamsMemoized` function several hundred times under different
    conditions, and it will log the execution time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The main insight to take from the preceding chart is that, just like with every
    aspect of programming, there is no silver bullet algorithm that will work for
    all cases. Memoization is clearly a very basic way of optimizing code, but clearly,
    it won't optimize anything given the right circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: As for the code, there is not much to it. It is a very simple, non real-world
    example of the point I was trying to send across. The `performTest` function will
    take care of running a series of 10 tests for every use case and measure the total
    time each use case takes. Notice that we're not really using profilers at this
    point. We're just measuring time in a very basic and ad-hoc way, which works for
    us.
  prefs: []
  type: TYPE_NORMAL
- en: The input for both functions is simply a set of numbers on which they will run
    some math functions, just for the sake of doing something.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other interesting bit about the arguments is that, since the first argument
    is a list of numbers, we can''t just use the `args` parameter as a key inside
    the `Memoized` class'' methods. This is why we have the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This line will concatenate all the numbers from the first parameter into a single
    value, which will act as the key. The second parameter is not used here because
    it's always random, which would imply that the key will never be the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another variation of the preceding method is to precalculate all values from
    the function during initialization (assuming we have a limited number of inputs,
    of course) initialization and then refer to the lookup table during execution.
    This approach has several preconditions:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of input values must be finite; otherwise it's impossible to precalculate
    everything
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lookup table with all of its values, must fit into memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just like before, the input must be repeated, at least once, so the optimization
    both makes sense and is worth the extra effort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are different approaches when it comes to architecting the lookup table,
    all offering different types of optimizations. It all depends on the type of application
    and solution that you're trying to optimize. Here is a set of examples.
  prefs: []
  type: TYPE_NORMAL
- en: Performing a lookup on a list or linked list
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This solution works by iterating over an unsorted list and checking the key
    against each element, with the associated value as the result we're looking for.
  prefs: []
  type: TYPE_NORMAL
- en: This is obviously a very slow method of implementation, with a Big O notation
    of *O(n)* for both the average and worst case scenarios. Still, given the right
    circumstances, it could prove to be faster than calling the actual function every
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this case, using a linked list would improve the performance of the algorithm
    over using a simple list. However, it would still depend heavily on the type of
    linked list it is (doubly linked list, simple linked list with direct access to
    the first and last elements, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: Simple lookup on a dictionary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This method works using a one-dimensional dictionary lookup, indexed by a key
    consisting of the input parameters (enough of them create a unique key). In particular
    cases (like we covered earlier), this is probably one of the fastest lookups,
    even faster than binary search in some cases with a constant execution time (Big
    O notation of *O(1)*).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that this approach is efficient as long as the key-generation algorithm
    is capable of generating unique keys every time. Otherwise, the performance could
    degrade over time due to the many collisions on the dictionaries.
  prefs: []
  type: TYPE_NORMAL
- en: Binary search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This particular method is only possible if the list is sorted. This could potentially
    be an option depending on the values to sort. Yet sorting them would require extra
    effort that would hurt the performance of the entire effort. However, it presents
    very good results, even in long lists (average Big O notation of *O(log n)*).
    It works by determining in which half of the list the value is and repeating until
    either the value is found or the algorithm is able to determine that the value
    is not in the list.
  prefs: []
  type: TYPE_NORMAL
- en: To put all of this into perspective, looking at the `Memoized` class mentioned
    earlier, it implements a simple lookup on a dictionary. However, this would be
    the place to implement either of the other algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Use cases for lookup tables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are some classic example use cases for this type of optimization, but
    the most common one is probably the optimization of trigonometric functions. Based
    on the computing time, these functions are really slow. When used repeatedly,
    they can cause some serious damage to your program's performance.
  prefs: []
  type: TYPE_NORMAL
- en: This is why it is normally recommended to precalculate the values of these functions.
    For functions that deal with an infinite domain universe of possible input values,
    this task becomes impossible. So, the developer is forced to sacrifice accuracy
    for performance by precalculating a discrete subdomain of the possible input values
    (that is, going from floating points down to integer numbers).
  prefs: []
  type: TYPE_NORMAL
- en: This approach might not be ideal in some cases, since some systems require both
    performance and accuracy. So, the solution is to meet in the middle and use some
    form of interpolation to calculate the required value, based on the ones that
    have been precalculated. It will provide better accuracy. Even though it won't
    be as performant as using the lookup table directly, it should prove to be faster
    than doing the trigonometric calculation every time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at some examples of this; for instance, for the following trigonometric
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We'll take a look at how simple precalculation won't be accurate enough and
    how some form of interpolation will result in a better level of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code will precalculate the values for the function in a range
    from `-1000` to `1000` (only integer values). Then it''ll try to do the same calculation
    (only for a smaller range) for floating point numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The results from the preceding code will help demonstrate how the simple lookup
    table approach is not accurate enough (see the following chart). However, it compensates
    for this with speed, as the original function takes 0.001526 seconds to run while
    the lookup table only takes 0.000717 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: '![Use cases for lookup tables](img/B02088_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding chart shows how the lack of interpolation hurts the accuracy.
    You can see how, even though both plots are quite similar, the results from the
    lookup table execution aren''t as accurate as the `trig` function used directly.
    So, now, let''s take another look at the same problem. However, this time, we''ll
    add some basic interpolation (we''ll limit the rage of values from `-PI` to `PI`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As you might've noticed in the previous chart, the resulting plot is periodic
    (especially because we've limited the range from `-PI` to `PI`). So, we'll focus
    on a particular range of values that will generate one single segment of the plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the preceding script also shows that the interpolation solution
    is still faster than the original trigonometric function, although not as fast
    as it was earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Interpolation solution | Original function |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.000118 seconds | 0.000343 seconds |'
  prefs: []
  type: TYPE_TB
- en: 'The following chart is a bit different from the previous one, especially because
    it shows (in green) the error percentage between the interpolated value and the
    original one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Use cases for lookup tables](img/B02088_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The biggest error we have is around 12 percent (which represents the peaks we
    see on the chart). However, it's for the smallest values, such as -0.000852248551417
    versus -0.000798905501416\. This is a case where the error percentage needs to
    be contextualized to see if it really matters. In our case, since the values related
    to that error are so small, we can ignore that error in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are other use cases for lookup tables, such as in image processing. However,
    for the sake of this book, the preceding example should be enough to demonstrate
    their benefits and the trade-off implied in their usage.
  prefs: []
  type: TYPE_NORMAL
- en: Usage of default arguments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another optimization technique, one that is contrary to memoization, is not
    particularly generic. Instead, it is directly tied to how the Python interpreter
    works.
  prefs: []
  type: TYPE_NORMAL
- en: Default arguments can be used to determine values once at function creation
    time, instead of at run time.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This can only be done for functions or objects that will not be changed during
    program execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example of how this optimization can be applied. The following
    code shows two versions of the same function, which does some random trigonometric
    calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This optimization can be problematic if not correctly documented. Since it uses
    attributes to precompute terms that should not change during the program's execution,
    it could lead to the creation of a confusing API.
  prefs: []
  type: TYPE_NORMAL
- en: 'With a quick and simple test, we can double-check the performance gain from
    this optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code measures the time it takes for the script to finish each
    of the versions of the function to run its code for a range of `1000`. It saves
    those measurements, and finally, it creates an average for each case. The result
    is displayed in the following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Usage of default arguments](img/B02088_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It clearly isn't an amazing optimization. However, it does shave off some microseconds
    from our execution time, so we'll keep it in mind. Just remember that this optimization
    could cause problems if you're working as part of an OS developer team.
  prefs: []
  type: TYPE_NORMAL
- en: List comprehension and generators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: List comprehension is a special construct provided by Python to generate lists
    by writing in the way a mathematician would, by describing its content instead
    of writing about the way the content should be generated (with a classic `for`
    loop).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see an example of this to better understand how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now, list comprehension is not meant to replace `for` loops altogether. They
    are a great help when dealing with loops that, like the earlier one, are creating
    a list. However, they aren't particularly recommended for those `for` loops that
    you write because of their side effects. This means you're not creating a list.
    You're most likely calling a function inside it or doing some other calculation
    that does not translate into a list. For these cases, a list comprehension expression
    would actually hurt readability.
  prefs: []
  type: TYPE_NORMAL
- en: To understand why these expressions are more performant than regular `for` loops,
    we need to do some disassembling and read a bit of bytecode. We can do this because,
    even though Python is an interpreted language, it is still being translated into
    bytecode by the compiler. This bytecode is the one that is interpreted. So, using
    the `dis` module, we can turn that bytecode into something that humans can read,
    and analyze its execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the code then:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'That code will output two things:'
  prefs: []
  type: TYPE_NORMAL
- en: The time each piece of code takes to run
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The set of instructions generated by the interpreter, thanks to the `dis` module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a screenshot of how that output would look (in your system, the time
    might change, but the rest should be pretty similar):'
  prefs: []
  type: TYPE_NORMAL
- en: '![List comprehension and generators](img/B02088_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'First things first; the output proves that the list comprehension version of
    the code is, indeed, faster. Now, let''s take a closer look at both lists of instructions,
    side by side, to try to understand them better:'
  prefs: []
  type: TYPE_NORMAL
- en: '| The for loop instructions | Comments | The list comprehension instructions
    | Comments |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `BUILD_LIST` |   | `BUILD_LIST` |   |'
  prefs: []
  type: TYPE_TB
- en: '| `STORE_NAME` | The definition of our "multiples_of_two" list |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| `SETUP_LOOP` |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| `LOAD_NAME` | Range function | `LOAD_NAME` | Range function |'
  prefs: []
  type: TYPE_TB
- en: '| `LOAD_CONST` | 100 (the attribute for the range) | `LOAD_CONST` | 100 (the
    attribute for the range) |'
  prefs: []
  type: TYPE_TB
- en: '| `CALL_FUNCTION` | Calls range | `CALL_FUNCTION` | Calls range |'
  prefs: []
  type: TYPE_TB
- en: '| `GET_ITER` |   | `GET_ITER` |   |'
  prefs: []
  type: TYPE_TB
- en: '| `FOR_ITER` |   | `FOR_ITER` |   |'
  prefs: []
  type: TYPE_TB
- en: '| `STORE_NAME` | Our temp variable x | `STORE_NAME` | Our temp variable `x`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `LOAD_NAME` |   | `LOAD_NAME` |   |'
  prefs: []
  type: TYPE_TB
- en: '| `LOAD_CONST` | *X % 2 == 0* | `LOAD_CONST` | *X % 2 == 0* |'
  prefs: []
  type: TYPE_TB
- en: '| `BINARY_MODULO` | `BINARY_MODULO` |'
  prefs: []
  type: TYPE_TB
- en: '| `LOAD_CONST` | `LOAD_CONST` |'
  prefs: []
  type: TYPE_TB
- en: '| `COMPARE_OP` | `COMPARE_OP` |'
  prefs: []
  type: TYPE_TB
- en: '| `POP_JUMP_IF_FALSE` |   | `POP_JUMP_IF_FALSE` |   |'
  prefs: []
  type: TYPE_TB
- en: '| `LOAD_NAME` |   | `LOAD_NAME` |   |'
  prefs: []
  type: TYPE_TB
- en: '| `LOAD_ATTR` | Lookup for the append method | `LIST_APPEND` | Appends the
    value to the list |'
  prefs: []
  type: TYPE_TB
- en: '| `LOAD_NAME` | Loads the value of *X* |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| `CALL_FUNCTION` | Appends the actual value to the list |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| `POP_TOP` |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| `JUMP_ABSOLUTE` |   | `JUMP_ABSOLUTE` |   |'
  prefs: []
  type: TYPE_TB
- en: '| `JUMP_ABSOLUTE` |   | `STORE_NAME` |   |'
  prefs: []
  type: TYPE_TB
- en: '| `POP_BLOCK` |   | `LOAD_CONST` |   |'
  prefs: []
  type: TYPE_TB
- en: '| `LOAD_CONST` |   | `RETURN_VALUE` |   |'
  prefs: []
  type: TYPE_TB
- en: '| `RETURN_VALUE` |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: From the preceding table, you can see how the `for` loop generates a longer
    list of instructions. The instructions generated by the comprehension code almost
    looks like a subset of the ones generated by the `for` loop, with the major difference
    of how the values are added. For the `for` loop, they are added one by one, using
    three instructions (`LOAD_ATTR`, `LOAD_NAME`, and `CALL_FUNCTION`). On the other
    hand, for the list comprehension column, this is done with one single, optimized
    instruction (`LIST_APPEND`).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the reason why when generating a list, the `for` loop should not be
    your weapon of choice. This is because the list comprehension you're writing is
    more efficient and sometimes, even writes more readable code.
  prefs: []
  type: TYPE_NORMAL
- en: That being said, remember to not abuse these expressions by replacing every
    `for` loop, even the ones that do other things (side effects). In these cases,
    list comprehension expressions are not optimized and will take longer than a regular
    `for` loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, there is one more related consideration to take into account: when
    generating big lists, comprehension expressions might not be the best solution.
    This is because they still need to generate every single value. So, if you''re
    generating a list of 100k items, there is a better way. You can use generator
    expressions. Instead of returning a list, they return a generator object, which
    has a similar API to what lists have. However, every time you request a new item,
    that item will be dynamically generated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main difference between a generator object and a list object is that the
    first one doesn''t support random access. So, you can''t really use the brackets
    notation for anything. However, you can use the generator object to iterate over
    your list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Another key difference between lists and generator objects is that you can only
    iterate once over the latter, while you can do the same as many times as you like
    over a list. This is a key difference because it will limit the usage of your
    efficiently generated list. So, take it into account when deciding to go with
    a list comprehension expression or a generator expression.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach might add a little overhead when accessing the values, but it''ll
    be faster when creating the list. Here is a comparison of both list comprehension
    and generator expressions when creating lists of different lengths:'
  prefs: []
  type: TYPE_NORMAL
- en: '![List comprehension and generators](img/B02088_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The chart clearly shows that the performance of the generator expressions is
    better than the list comprehension expressions for lengthier lists. For smaller
    lists, the list comprehension ones are better.
  prefs: []
  type: TYPE_NORMAL
- en: ctypes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ctypes library allows the developer to reach under the hood of Python and
    tap into the power of the C language. This is only possible with the official
    interpreter (CPython) because it is written in C. Other versions of it, such as
    PyPy or Jython, do not provide access to this library.
  prefs: []
  type: TYPE_NORMAL
- en: This interface to C can be used to do many things, since you literally have
    the ability to load pre-compiled code and use it from C. This means you have access
    to libraries such as `kernel32.dll` and `msvcrt.dll` for Windows systems, and
    `libc.so.6` for Linux systems.
  prefs: []
  type: TYPE_NORMAL
- en: For our particular case, we'll focus on ways to optimize our code, showing how
    to load a custom C library and how to load a system library to take advantage
    of its optimized code. For full details on how to use this library, refer to the
    official documentation at [https://docs.python.org/2/library/ctypes.html](https://docs.python.org/2/library/ctypes.html).
  prefs: []
  type: TYPE_NORMAL
- en: Loading your own custom C library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, no matter how many optimization techniques we use on our code, they
    won't be enough to help us achieve the best possible time. In these cases, we
    can always write the sensitive code outside our program, in C, compile it into
    a library, and import it into our Python code.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at an example of how we can do this and what type of performance
    boost we are expecting.
  prefs: []
  type: TYPE_NORMAL
- en: The problem to solve is a very simple one, something really basic. We'll write
    the code to generate a list of prime numbers, from a list of 1 million integers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python code for that could be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is simple enough. Yes, we could easily improve it by changing
    the list comprehension expression for a generator. However, for the sake of showing
    the improvement from the C code, let's not do that. Now, the C code is taking
    4.5 seconds on average to run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now write the `check_prime` function in C, and let''s export it into
    a shared library (`.so` file):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To generate the library file, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can edit our Python script to run both versions of the function and
    compare the times, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Full Python version | C version |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 4.49 seconds | 1.04 seconds |'
  prefs: []
  type: TYPE_TB
- en: The performance boost is pretty good. It has gone from 4.5 seconds down to just
    1 second!
  prefs: []
  type: TYPE_NORMAL
- en: Loading a system library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At times, there is no need to code your C function. The system's libraries probably
    have it for you already. All you have to do is import that library and use the
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see another simple example to demonstrate the concept.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following line generates a list of 1 million random numbers, taking 0.9
    seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the full code that runs both lines and prints out the times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: String concatenation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python strings deserve a separate portion of this chapter because they're not
    like strings in other languages. In Python, strings are immutable, which means
    that once you create one you can't really change its value.
  prefs: []
  type: TYPE_NORMAL
- en: This is a relatively confusing affirmation, since we're used to doing things
    such as concatenation or replacement on string variables. However, what the average
    Python developer doesn't realize is that there is a lot more going on behind the
    curtains than they think.
  prefs: []
  type: TYPE_NORMAL
- en: Since string objects are immutable, every time we do anything to change its
    content, we're actually creating a whole new string with new content and pointing
    our variable to that new string. So, we must be careful when working with strings
    to make sure we actually want to do that.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a very simple way to check the preceding scenario. The following code
    will create a set of variables with the same string (we''ll write the string every
    time). Then, using the `id` function (which, in CPython, returns the memory address
    where the value the variable points to is stored), we''ll compare them to each
    other. If strings were mutable, then all objects would be different, and thus,
    the returned values should be different. Let''s look at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As the comments on the code state, the output will be `True`, `True`, and `False`,
    thus showing how the system is actually reusing the `This is a string` string
    every time we write it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image tries to represent the same idea in a more graphical way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![String concatenation](img/B02088_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Although we wrote the string twice, internally, both variables are referencing
    the same block of memory (containing the actual string). If we assign another
    value to one of them, we would not be changing the string content. We would just
    be pointing our variable to another memory address.
  prefs: []
  type: TYPE_NORMAL
- en: '![String concatenation](img/B02088_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The same thing happens in the preceding case, where we have a variable `b` pointing
    directly to variable `a`. Still, if we try to modify `b`, we would just be creating
    a new string once again.
  prefs: []
  type: TYPE_NORMAL
- en: '![String concatenation](img/B02088_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Finally, what happens if we change the value of both our variables from our
    example? What happens to the `hello world` string stored in memory? Well, if there
    are no other references to it, the **Garbage Collector** will eventually take
    care of it, releasing that memory.
  prefs: []
  type: TYPE_NORMAL
- en: That being said, immutable objects are not all that bad. They are actually good
    for performance if used right, since they can be used as dictionary keys, for
    instance, or even shared between different variable bindings (since the same block
    of memory is used every time you reference the same string). This means that the
    string `hey there` will be the same exact object every time you use that string,
    no matter what variable it is stored in (like we saw earlier).
  prefs: []
  type: TYPE_NORMAL
- en: 'With this in mind, think about what would happen for some common cases, such
    as the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will generate a new string for `full_doc` for every item
    in the `word_list` list. This is not really efficient memory usage, is it? This
    is a very common case when we''re trying to recreate a string from different parts.
    There is a better, more memory efficient way of doing it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The alternative is easier to read, faster to write, and more efficient, both
    memory and time wise. The following code shows the time each option takes. With
    the right command, we can also see that the `for` loop alternative uses a bit
    more memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'With the following commands we can execute the script and measure the memory
    used, using the Linux utility `time`:'
  prefs: []
  type: TYPE_NORMAL
- en: '#for the for-loop version:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '#for the join version:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output from the for-loop version command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from the join version command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The join version clearly takes considerably less time, and the peak memory consumption
    (measured by the time command) is also less.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other use case we want to consider when working with strings in Python
    is a different type of concatenation; it is used when you''re only dealing with
    a few variables, such as the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You''ll end up creating a set of substrings every time the system computes
    a new concatenation. So a better and more efficient way of doing this is using
    variable interpolation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, it is even better to create substrings using the `locals` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Other tips and tricks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The tips mentioned earlier are some of the most common techniques to optimize
    a program. Some of them are Python specific (such as string concatenation or using
    ctypes) and others are more generic (such as memoization and lookup tables).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are still a few more minor tips and tricks specific to Python, which
    we will cover here. They might not yield a significant boost of speed, but they
    will shed some more light into the inner workings of the language:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Membership testing**: When trying to figure out if a value is inside a list
    (we use the word "list" generically here, not specifically referencing the type
    `list`), something such as "a in b", we would have better results using sets and
    dictionaries (with a lookup time of *O(1)*) than lists or tuples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Don''t reinvent the wheel**: Python comes with built-in core blocks that
    are written in optimized C. There is no need to use hand-built alternatives, since
    the latter will most likely be slower. Datatypes such as `lists`, `tuples`, `sets`,
    and `dictionaries`, and modules such as `array`, `itertools`, and `collections.deque`
    are recommended. Built-in functions also apply here. They''ll always be faster
    to do something such as `map(operator.add, list1, list2)` will always be faster
    than `map(lambda x, y: x+y, list1, list2)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Don''t forget about deque**: When needing a fixed length array or a variable
    length stack, lists perform well. However, when dealing with the `pop(0)` or `insert(0,
    your_list)` operation, try to use `collections.deque`, since it offers fast (*O(1)*)
    appends and pops up on either end of the list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sometimes is better not to def**: Calling a function adds a lot of overhead
    (as we already saw earlier). So, sometimes, in time-critical loops especially,
    inlining the code of a function, instead of calling that function, will be more
    performant. There is a big trade-off with this tip, since it could also considerably
    hurt things such as readability and maintainability. So this should only be done
    if, in fact, the boost on performance is absolutely required. The following simple
    example shows how a simple lookup operation ends up adding a considerable amount
    of time:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**When possible, sort by the key**: When doing a custom sort on a list, try
    not to sort using a comparison function. Instead, when possible, sort by the key.
    This is because the key function will be called only once per item, whereas the
    comparison function will be called several times per item during the process.
    Let''s see a quick example comparing both methods:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**1 is better than True**: Python 2.3 `while 1` gets optimized into a single
    jump, while `while True` does not, thus taking several jumps to complete. This
    implies that writing `while 1` is more efficient than `while True`, although just
    like inlining the code, this tip comes with a big trade-off.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiple assignments are slow but...**: Multiple assignments (`a,b = "hello
    there", 123`) are generally slower than single assignments. However, again, when
    doing variable swaps, it becomes faster than doing it the regular way (because
    we skip the usage and assignment of the temporal variable):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Chained comparisons are good**: When comparing three variables with each
    other, instead of doing *x < y* and *y < z*, you can use *x < y < z*. This should
    prove easier to read (more natural) and faster to run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Using namedtuples instead of regular objects**: When creating simple objects
    to store data, using the regular class notation, the instances contain a dictionary
    for attribute storage. This storage can become wasteful for objects with few attributes.
    If you need to create a large number of those objects, then that waste of memory
    adds up. For such cases, you can use `namedtuples`. This is a new `tuple` subclass,
    which can be easily constructed and is optimized for the task. For details on
    `namedtuples`, check the official documentation at [https://docs.python.org/2/library/collections.html#collections.namedtuple](https://docs.python.org/2/library/collections.html#collections.namedtuple).
    The following code creates 1 million objects, both using regular classes and `namedtuples`,
    and displays the time for each action:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered several optimization techniques. Some of them are
    meant to provide big boosts on speed, and/or save memory. Some of them are just
    meant to provide minor speed improvements. Most of this chapter covered Python-specific
    techniques, but some of them can be translated into other languages as well.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will go over optimization techniques. In particular,
    we'll cover multi-threading and multiprocessing, and you'll learn when to apply
    each one.
  prefs: []
  type: TYPE_NORMAL
