<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Principles of Algorithm Design</h1>
            </header>

            <article>
                
<p>Why do we want to study algorithm design? There are of course many reasons, and our motivation for learning something is very much dependent on our own circumstances. There are without doubt important professional reasons for being interested in algorithm design. Algorithms are the foundations of all computing. We think of a computer as being a piece of hardware, a hard drive, memory chips, processors, and so on. However, the essential component, the thing that, if missing, would render modern technology impossible, is algorithms.</p>
<p>The theoretical foundation of algorithms, in the form of the Turing machine, was established several decades before digital logic circuits could actually implement such a machine. The Turing machine is essentially a mathematical model that, using a predefined set of rules, translates a set of inputs into a set of outputs. The first implementations of Turing machines were mechanical and the next generation may likely see digital logic circuits replaced by quantum circuits or something similar. Regardless of the platform, algorithms play a central predominant role.</p>
<p>Another aspect is the effect algorithms have in technological innovation. As an obvious example, consider the page rank search algorithm, a variation of which the Google search engine is based on. Using this and similar algorithms allows researchers, scientists, technicians, and others to quickly search through vast amounts of information extremely quickly. This has a massive effect on the rate at which new research can be carried out, new discoveries made, and new innovative technologies developed.</p>
<p>The study of algorithms is also important because it trains us to think very specifically about certain problems. It can serve to increase our mental and problem solving abilities by helping us isolate the components of a problem and define relationships between these components. In summary, there are four broad reasons for studying algorithms:</p>
<ol>
<li>They are essential for computer science and <em>intelligent</em> systems.</li>
<li>They are important in many other domains (computational biology, economics, ecology, communications, ecology, physics, and so on).</li>
<li>They play a role in technology innovation.</li>
<li>They improve problem solving and analytical thinking.</li>
</ol>
<p>Algorithms, in their simplest form, are just a sequence of actions, a list of instructions. It may just be a linear construct of the form do <em>x</em>, then do <em>y</em>, then do <em>z</em>, then finish. However, to make things more useful we add clauses to the effect of, <em>x</em> then do <em>y</em>, in Python the <kbd>if-else</kbd> statements. Here, the future course of action is dependent on some conditions; say the state of a data structure. To this we also add the operation, iteration, the while, and for statements. Expanding our algorithmic literacy further we add recursion. Recursion can often achieve the same result as iteration, however, they are fundamentally different. A recursive function calls itself, applying the same function to progressively smaller inputs. The input of any recursive step is the output of the previous recursive step.</p>
<p>Essentially, we can say that algorithms are composed of the following four elements:</p>
<ul>
<li>Sequential operations</li>
<li>Actions based on the state of a data structure</li>
<li>Iteration, repeating an action a number of times</li>
<li>Recursion, calling itself on a subset of inputs</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Algorithm design paradigms</h1>
            </header>

            <article>
                
<p>In general, we can discern three broad approaches to algorithm design. They are:</p>
<ul>
<li>Divide and conquer</li>
<li>Greedy algorithms</li>
<li>Dynamic programming</li>
</ul>
<p>As the name suggests, the divide and conquer paradigm involves breaking a problem into smaller sub problems, and then in some way combining the results to obtain a global solution. This is a very common and natural problem solving technique, and is, arguably, the most commonly used approach to algorithm design.</p>
<p>Greedy algorithms often involve optimization and combinatorial problems; the classic example is applying it to the traveling salesperson problem, where a greedy approach always chooses the closest destination first. This shortest path strategy involves finding the best solution to a local problem in the hope that this will lead to a global solution.</p>
<p>The dynamic programming approach is useful when our sub problems overlap. This is different from divide and conquer. Rather than break our problem into independent sub problems, with dynamic programming, intermediate results are cached and can be used in subsequent operations. Like divide and conquer it uses recursion; however, dynamic programming allows us to compare results at different stages. This can have a performance advantage over divide and conquer for some problems because it is often quicker to retrieve a previously calculated result from memory rather than having to recalculate it.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Recursion and backtracking</h1>
            </header>

            <article>
                
<p>Recursion is particularly useful for divide and conquer problems; however, it can be difficult to understand exactly what is happening, since each recursive call is itself spinning off other recursive calls. At the core of a recursive function are two types of cases: base cases, which tell the recursion when to terminate, and recursive cases that call the function they are in. A simple problem that naturally lends itself to a recursive solution is calculating factorials. The recursive factorial algorithm defines two cases: the base case when <em>n</em> is zero, and the recursive case when <em>n</em> is greater than zero. A typical implementation is the following:</p>
<pre>
    def factorial(n):<br/>        #test for a base case<br/>        if n==0:<br/>            return 1<br/>            # make a calculation and a recursive call<br/>            f= n*factorial(n-1)<br/>        print(f)<br/>        return(f)<br/>        factorial(4)
</pre>
<p>This code prints out the digits 1, 2, 4, 24. To calculate 4 requires four recursive calls plus the initial parent call. On each recursion, a copy of the methods variables is stored in memory. Once the method returns it is removed from memory. The following is a way we can visualize this process:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="182" src="assets/image_03_001.png" width="511"/></div>
<p>It may not necessarily be clear if recursion or iteration is a better solution to a particular problem; after all they both repeat a series of operations and both are very well suited to divide and conquer approaches to algorithm design. Iteration churns away until the problem is done. Recursion breaks the problem down into smaller and smaller chunks and then combines the results. Iteration is often easier for programmers, because control stays local to a loop, whereas recursion can more closely represent mathematical concepts such as factorials. Recursive calls are stored in memory, whereas iterations are not. This creates a trade off between processor cycles and memory usage, so choosing which one to use may depend on whether the task is processor or memory intensive. The following table outlines the key differences between recursion and iteration:</p>
<table class="table">
<tbody>
<tr>
<td>
<p><strong>Recursion</strong></p>
</td>
<td>
<p><strong>Iteration</strong></p>
</td>
</tr>
<tr>
<td>
<p>Terminates when a base case is reached</p>
</td>
<td>
<p>Terminates when a defined condition is met</p>
</td>
</tr>
<tr>
<td>
<p>Each recursive call requires space in memory</p>
</td>
<td>
<p>Each iteration is not stored in memory</p>
</td>
</tr>
<tr>
<td>
<p>An infinite recursion results in a stack overflow error</p>
</td>
<td>
<p>An infinite iteration will run while the hardware is powered</p>
</td>
</tr>
<tr>
<td>
<p>Some problems are naturally better suited to recursive solutions</p>
</td>
<td>
<p>Iterative solutions may not always be obvious</p>
</td>
</tr>
</tbody>
</table>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Backtracking</h1>
            </header>

            <article>
                
<p>Backtracking is a form of recursion that is particularly useful for types of problems such as traversing tree structures, where we are presented with a number of options at each node, from which we must choose one. Subsequently we are presented with a different set of options, and depending on the series of choices made either a goal state or a dead end is reached. If it is the latter, we must backtrack to a previous node and traverse a different branch. Backtracking is a divide and conquer method for exhaustive search. Importantly backtracking <strong>prunes</strong> branches that cannot give a result.</p>
<p>An example of back tracking is given in the following example. Here, we have used a recursive approach to generating all the possible permutations of a given string, <em>s</em>, of a given length <em>n</em>:</p>
<pre>
    def bitStr(n, s):            <br/><br/>         if n == 1: return s <br/>         return [ digit + bits for digit in bitStr(1,s)for bits in bitStr(n - 1,s)] <br/><br/>    print (bitStr(3,'abc'))     
</pre>
<p>This generates the following output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_03_002.png"/></div>
<p>Notice the double list compression and the two recursive calls within this comprehension. This recursively concatenates each element of the initial sequence, returned when <kbd><em>n</em> = 1</kbd>, with each element of the string generated in the previous recursive call. In this sense it is <em>backtracking</em> to uncover previously ingenerated combinations. The final string that is returned is all <em>n</em> letter combinations of the initial string.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Divide and conquer - long multiplication</h1>
            </header>

            <article>
                
<p>For recursion to be more than just a clever trick, we need to understand how to compare it to other approaches, such as iteration, and to understand when its use will lead to a faster algorithm. An iterative algorithm that we are all familiar with is the procedure we learned in primary math classes, used to multiply two large numbers. That is, long multiplication. If you remember, long multiplication involved iterative multiplying and carry operations followed by a shifting and addition operation.</p>
<p>Our aim here is to examine ways to measure how efficient this procedure is and attempt to answer the question; is this the most efficient procedure we can use for multiplying two large numbers together?</p>
<p>In the following figure, we can see that multiplying two 4 digit numbers together requires 16 multiplication operations, and we can generalize to say that an <em>n</em> digit number requires, approximately, <em>n<sup>2</sup></em> multiplication operations:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="155" src="assets/image_03_003.png" width="367"/></div>
<p>This method of analyzing algorithms, in terms of the number of computational primitives such as multiplication and addition, is important because it gives us a way to understand the relationship between the time it takes to complete a certain computation and the size of the input to that computation. In particular, we want to know what happens when the input, the number of digits, n, is very large. This topic, called asymptotic analysis, or time complexity, is essential to our study of algorithms and we will revisit it often during this chapter and the rest of this book.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Can we do better? A recursive approach</h1>
            </header>

            <article>
                
<p>It turns out that in the case of long multiplication the answer is yes, there are in fact several algorithms for multiplying large numbers that require less operations. One of the most well-known alternatives to long multiplication is the <strong>Karatsuba algorithm</strong>, first published in 1962. This takes a fundamentally different approach: rather than iteratively multiplying single digit numbers, it recursively carries out multiplication operations on progressively smaller inputs. Recursive programs call themselves on smaller subsets of the input. The first step in building a recursive algorithm is to decompose a large number into several smaller numbers. The most natural way to do this is to simply split the number in to two halves, the first half of most significant digits, and a second half of least significant digits. For example, our four-digit number, 2345, becomes a pair of two-digit numbers, 23 and 45. We can write a more general decomposition of any 2 <em>n</em> digit numbers, <em>x</em> and <em>y</em> using the following, where <em>m</em> is any positive integer less than <em>n</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_03_004.png"/></div>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_03_005.png"/></div>
<p>So now we can rewrite our multiplication problem <em>x</em>, <em>y</em> as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_03_006.png"/></div>
<p>When we expand and gather like terms we get the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_03_007.png"/></div>
<p>More conveniently, we can write it like this:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_03_003.jpg"/></div>
<p>Where:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_03_004.jpg"/></div>
<p>It should be pointed out that this suggests a recursive approach to multiplying two numbers since this procedure does itself involve multiplication. Specifically, the products <em>ac</em>, <em>ad</em>, <em>bc</em>, and <em>bd</em> all involve numbers smaller than the input number and so it is conceivable that we could apply the same operation as a partial solution to the overall problem. This algorithm, so far, consists of four recursive multiplication steps and it is not immediately clear if it will be faster than the classic long multiplication approach.</p>
<p>What we have discussed so far in regards to the recursive approach to multiplication, has been well known to mathematicians since the late 19<sup>th</sup> century. The Karatsuba algorithm improves on this is by making the following observation. We really only need to know three quantities: <em>z<sub>2</sub></em>= <em>ac</em> ; <em>z<sub>1</sub>=ad +bc</em>, and <em>z<sub>0</sub></em>= <em>bd</em> to solve equation 3.1. We need to know the values of <em>a, b, c, d</em> only in so far as they contribute to the overall sum and products involved in calculating the quantities <em>z<sub>2</sub></em>, <em>z<sub>1</sub></em>, and <em>z<sub>0</sub></em>. This suggests the possibility that perhaps we can reduce the number of recursive steps. It turns out that this is indeed the situation.</p>
<p>Since the products <em>ac</em> and <em>bd</em> are already in their simplest form, it seems unlikely that we can eliminate these calculations. We can however make the following observation:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_03_001.jpg"/></div>
<p>When we subtract the quantities <em>ac</em> and <em>bd,</em> which we have calculated in the previous recursive step, we get the quantity we need, namely (<em>ad</em> + <em>bc</em>):</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_03_002.jpg"/></div>
<p>This shows that we can indeed compute the sum of <em>ad + bc</em> without separately computing each of the individual quantities. In summary, we can improve on equation 3.1 by reducing from four recursive steps to three. These three steps are as follows:</p>
<ol>
<li>Recursively calculate <em>ac.</em></li>
<li>Recursively calculate <em>bd.</em></li>
<li>Recursively calculate (<em>a</em> +<em>b</em>)(<em>c</em> + <em>d</em>) and subtract <em>ac</em> and <em>bd.</em></li>
</ol>
<p>The following example shows a Python implementation of the Karatsuba algorithm:</p>
<pre>
    from math import log10  <br/>    def karatsuba(x,y): <br/><br/>        # The base case for recursion <br/>        if x &lt; 10 or y &lt; 10: <br/>            return x*y     <br/><br/>        #sets n, the number of digits in the highest input number <br/>        n = max(int(log10(x)+1), int(log10(y)+1)) <br/><br/>        # rounds up n/2     <br/>        n_2 = int(math.ceil(n / 2.0)) <br/>        #adds 1 if n is uneven <br/>        n = n if n % 2 == 0 else n + 1 <br/><br/>        #splits the input numbers      <br/>        a, b = divmod(x, 10**n_2) <br/>        c, d = divmod(y, 10**n_2) <br/><br/>        #applies the three recursive steps <br/>        ac = karatsuba(a,c) <br/>        bd = karatsuba(b,d) <br/>        ad_bc = karatsuba((a+b),(c+d)) - ac - bd <br/><br/>        #performs the multiplication     <br/>        return (((10**n)*ac) + bd + ((10**n_2)*(ad_bc))) 
</pre>
<p>To satisfy ourselves that this does indeed work, we can run the following test function:</p>
<pre>
    import random <br/>    def test(): <br/>            for i in range(1000): <br/>                x = random.randint(1,10**5) <br/>                y = random.randint(1,10**5) <br/>                expected = x * y <br/>                result = karatsuba(x, y) <br/>                if result != expected: <br/>                    return("failed")                 <br/>            return('ok')   
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Runtime analysis</h1>
            </header>

            <article>
                
<p>It should be becoming clear that an important aspect to algorithm design is gauging the efficiency both in terms of space (memory) and time (number of operations). This second measure, called runtime performance, is the subject of this section. It should be mentioned that an identical metric is used to measure an algorithm's memory performance. There are a number of ways we could, conceivably, measure run time and probably the most obvious is simply to measure the time the algorithm takes to complete. The major problem with this approach is that the time it takes for an algorithm to run is very much dependent on the hardware it is run on. A platform-independent way to gauge an algorithm's runtime is to count the number of operations involved. However, this is also problematic in that there is no definitive way to quantify an operation. This is dependent on the programming language, the coding style, and how we decide to count operations. We can use this idea, though, of counting operations, if we combine it with the expectation that as the size of the input increases the runtime will increase in a specific way. That is, there is a mathematical relationship between <em>n</em>, the size of the input, and the time it takes for the algorithm to run.</p>
<p>Much of the discussion that follows will be framed by the following three guiding principles. The rational and importance of these principles should become clearer as we proceed. These principles are as follows:</p>
<ul>
<li>Worst case analysis. Make no assumptions on the input data.</li>
<li>Ignore or suppress constant factors and lower order terms. At large inputs higher order terms dominate.</li>
<li>Focus on problems with large input sizes.</li>
</ul>
<p>Worst case analysis is useful because it gives us a tight upper bound that our algorithm is guaranteed not to exceed. Ignoring small constant factors, and lower order terms is really just about ignoring the things that, at large values of the input size, <em>n</em>, do not contribute, in a large degree, to the overall run time. Not only does it make our work mathematically easier, it also allows us to focus on the things that are having the most impact on performance.</p>
<p>We saw with the Karatsuba algorithm that the number of multiplication operations increased to the square of the size, <em>n</em>, of the input. If we have a four-digit number the number of multiplication operations is 16; an eight-digit number requires 64 operations. Typically, though, we are not really interested in the behavior of an algorithm at small values of <em>n</em>, so we most often ignore factors that increase at slower rates, say linearly with <em>n</em>. This is because at high values of <em>n</em>, the operations that increase the fastest as we increase <em>n</em>, will dominate.</p>
<p>We will explain this in more detail with an example, the merge sort algorithm. Sorting is the subject of <a href="0728804d-a35d-4cad-a773-68062f359f9b.xhtml">Chapter 10</a>, <span><em>Sorting</em>,</span> however, as a precursor and as a useful way to learn about runtime performance, we will introduce merge sort here.</p>
<p>The merge sort algorithm is a classic algorithm developed over 60 years ago. It is still used widely in many of the most popular sorting libraries. It is relatively simple and efficient. It is a recursive algorithm that uses a divide and conquer approach. This involves breaking the problem into smaller sub problems, recursively solving them, and then somehow combining the results. Merge sort is one of the most obvious demonstrations of the divide and conquer paradigm.</p>
<p>The merge sort algorithm consists of three simple steps:</p>
<ol>
<li>Recursively sort the left half of the input array.</li>
<li>Recursively sort the right half of the input array.</li>
<li>Merge two sorted sub arrays into one.</li>
</ol>
<p>A typical problem is sorting a list of numbers into a numerical order. Merge sort works by splitting the input into two halves and working on each half in parallel. We can illustrate this process schematically with the following diagram:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="224" src="assets/image_03_012.png" width="301"/></div>
<p>Here is the Python code for the merge sort algorithm:</p>
<pre>
    def mergeSort(A): <br/>        #base case if the input array is one or zero just return. <br/>        if len(A) &gt; 1: <br/>            # splitting input array <br/>            print('splitting ', A ) <br/>            mid = len(A)//2 <br/>            left = A[:mid] <br/>            right = A[mid:] <br/>            #recursive calls to mergeSort for left and right sub arrays                 <br/>            mergeSort(left) <br/>            mergeSort(right) <br/>            #initalizes pointers for left (i) right (j) and output array (k)  <br/>    # 3 initalization operations <br/>            i = j = k = 0         <br/>            #Traverse and merges the sorted arrays <br/>            while i &lt;len(left) and j&lt;len(right): <br/>    # if left &lt; right comparison operation  <br/>                if left[i] &lt; right[j]: <br/>    # if left &lt; right Assignment operation <br/>                    A[k]=left[i] <br/>                    i=i+1 <br/>                else: <br/>    #if right &lt;= left assignment <br/>                    A[k]= right[j] <br/>                    j=j+1 <br/>                k=k+1 <br/><br/>            while i&lt;len(left): <br/>    #Assignment operation <br/>                A[k]=left[i] <br/>                i=i+1 <br/>                k=k+1 <br/><br/>            while j&lt;len(right): <br/>    #Assignment operation <br/>                A[k]=right[j] <br/>                j=j+1 <br/>                k=k+1 <br/>        print('merging ', A) <br/>        return(A)   
</pre>
<p>We run this program for the following results:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="160" src="assets/image_03_013.png" width="262"/></div>
<p>The problem that we are interested in is how we determine the running time performance, that is, what is the rate of growth in the time it takes for the algorithm to complete relative to the size of <em>n</em>. To understand this a bit better, we can map each recursive call onto a tree structure. Each node in the tree is a recursive call working on progressively smaller sub problems:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="224" src="assets/image_03_014.png" width="365"/></div>
<p>Each invocation of merge-sort subsequently creates two recursive calls, so we can represent this with a binary tree. Each of the child nodes receives a sub set of the input. Ultimately we want to know the total time it takes for the algorithm to complete relative to the size of <em>n</em>. To begin with we can calculate the amount of work and the number of operations at each level of the tree.</p>
<p>Focusing on the runtime analysis, at level 1, the problem is split into two <em>n</em>/2 sub problems, at level 2 there is four <em>n</em>/4 sub problems, and so on. The question is when does the recursion bottom out, that is, when does it reach its base case. This is simply when the array is either zero or one.</p>
<p>The number of recursive levels is exactly the number of times you need to divide <em>n</em> by 2 until you get a number that is at most 1. This is precisely the definition of log2. Since we are counting the initial recursive call as level 0, the total number of levels is log<sub>2</sub><em>n</em> + 1.</p>
<p>Let's just pause to refine our definitions. So far we have been describing the number of elements in our input by the letter <em>n</em>. This refers to the number of elements in the first level of the recursion, that is, the length of the initial input. We are going to need to differentiate between the size of the input at subsequent recursive levels. For this we will use the letter <em>m</em> or specifically <em>m<sub>j</sub></em> for the length of the input at recursive level <em>j.</em></p>
<p>Also there are a few details we have overlooked, and I am sure you are beginning to wonder about. For example, what happens when <em>m</em>/2 is not an integer, or when we have duplicates in our input array. It turns out that this does not have an important impact on our analysis here; we will revisit some of the finer details of the merge sort algorithm in <a href="f1986aa3-0229-4480-8496-d25a2a9dae46.xhtml">Chapter 12</a>, <em>Design Techniques and Strategies</em>.</p>
<p>The advantage of using a recursion tree to analyze algorithms is that we can calculate the work done at each level of the recursion. How to define this work is simply as the total number of operations and this of course is related to the size of the input. It is important to measure and compare the performance of algorithms in a platform independent way. The actual run time will of course be dependent on the hardware on which it is run. Counting the number of operations is important because it gives us a metric that is directly related to an algorithm's performance, independent of the platform.</p>
<p>In general, since each invocation of merge sort is making two recursive calls, the number of calls is doubling at each level. At the same time each of these calls is working on an input that is half of its parents. We can formalize this and say that:</p>
<div class="packt_infobox">For level j , where <em>j</em> is an integer 0, 1, 2 ... log<sub>2</sub><em>n</em>, there are two <sup>j</sup> sub problems each of size <em>n</em>/2<sup>j</sup>.</div>
<p>To calculate the total number of operations, we need to know the number of operations encompassed by a single merge of two sub arrays. Let's count the number of operations in the previous Python code. What we are interested in is all the code after the two recursive calls have been made. Firstly, we have the three assignment operations. This is followed by three while loops. In the first loop we have an if else statement and within each of are two operations, a comparison followed by an assignment. Since there are only one of these sets of operations within the if else statements, we can count this block of code as two operations carried out <em>m</em> times. This is followed by two while loops with an assignment operation each. This makes a total of 4<em>m</em> + 3 operations for each recursion of merge sort.</p>
<p>Since <em>m</em> must be at least 1, the upper bound for the number of operations is 7<em>m</em>. It has to be said that this has no pretense at being an exact number. We could of course decide to count operations in a different way. We have not counted the increment operations or any of the housekeeping operations; however, this is not so important as we are more concerned with the rate of growth of the runtime with respect to <em>n</em> at high values of <em>n</em>.</p>
<p>This may seem a little daunting since each call of a recursive call itself spins off more recursive calls, and seemingly explodes exponentially. The key fact that makes this manageable is that as the number of recursive calls doubles, the size of each sub problem halves. These two opposing forces cancel out nicely as we can demonstrate.</p>
<p>To calculate the maximum number of operations at each level of the recursion tree we simply multiply the number of sub problems by the number of operations in each sub problem as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_03_005.jpg"/></div>
<p>Importantly this shows that, because the 2<sup>j</sup> cancels out the number of operations at each level is independent of the level. This gives us an upper bound to the number of operations carried out on each level, in this example, 7<em>n</em>. It should be pointed out that this includes the number of operations performed by each recursive call on that level, not the recursive calls made on subsequent levels. This shows that the work done, as the number of recursive calls doubles with each level, is exactly counter balanced by the fact that the input size for each sub problem is halved.</p>
<p>To find the total number of operations for a complete merge sort we simply multiply the number of operations on each level by the number of levels. This gives us the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_03_006.jpg"/></div>
<p>When we expand this out, we get the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_03_007.jpg"/></div>
<p>The key point to take from this is that there is a logarithmic component to the relationship between the size of the input and the total running time. If you remember from school mathematics, the distinguishing characteristic of the logarithm function is that it flattens off very quickly. As an input variable, <em>x</em>, increases in size, the output variable, <em>y</em> increases by smaller and smaller amounts. For example, compare the log function to a linear function:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="222" src="assets/image_03_018.png" width="327"/></div>
<p>In the previous example, multiplying the <em>n</em>log<sub>2</sub><em>n</em> component and comparing it to <em>n</em><sup>2</sup> .</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="253" src="assets/image_03_019.png" width="355"/></div>
<p>Notice how for very low values of <em>n</em>, the time to complete, <em>t</em> , is actually lower for an algorithm that runs in n<sup>2</sup> time. However, for values above about 40, the log function begins to dominate, flattening the output until at the comparatively moderate size <em>n</em> = 100, the performance is more than twice than that of an algorithm running in <em>n</em><sup>2</sup> time. Notice also that the disappearance of the constant factor, + 7 is irrelevant at high values of <em>n</em>.</p>
<p>The code used to generate these graphs is as follows:</p>
<pre>
    import matplotlib.pyplot as plt <br/>    import math <br/>    x=list(range(1,100)) <br/>    l =[]; l2=[]; a = 1 <br/>    plt.plot(x , [y * y for y in x] ) <br/>    plt.plot(x, [(7 *y )* math.log(y, 2) for y in x]) <br/>    plt.show() 
</pre>
<p>You will need to install the matplotlib library, if it is not installed already, for this to work. Details can be found at the following address; I encourage you to experiment with this list comprehension expression used to generate the plots. For example, adding the following plot statement:</p>
<pre>
    plt.plot(x, [(6 *y )* math.log(y, 2) for y in x]) 
</pre>
<p>Gives the following output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="225" src="assets/image_03_020.png" width="349"/></div>
<p>The preceding graph shows the difference between counting six operations or seven operations. We can see how the two cases diverge, and this is important when we are talking about the specifics of an application. However, what we are more interested in here is a way to characterize growth rates. We are not so much concerned with the absolute values, but how these values change as we increase <em>n</em>. In this way we can see that the two lower curves have similar growth rates, when compared to the top (<em>x</em><sup>2</sup>) curve. We say that these two lower curves have the same <strong>complexity class</strong>. This is a way to understand and describe different runtime behaviors. We will formalize this performance metric in the next section.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Asymptotic analysis</h1>
            </header>

            <article>
                
<p>There are essentially three things that characterize an algorithm's runtime performance. They are:</p>
<ul>
<li>Worst case - Use an input that gives the slowest performance</li>
<li>Best case - Use an input that give, the best results</li>
<li>Average case - Assumes the input is random</li>
</ul>
<p>To calculate each of these, we need to know the upper and lower bounds. We have seen a way to represent an algorithm's runtime using mathematical expressions, essentially adding and multiplying operations. To use asymptotic analyses, we simply create two expressions, one each for the best and worst cases.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Big O notation</h1>
            </header>

            <article>
                
<p>The letter "O" in big <em>O</em> notation stands for order, in recognition that rates of growth are defined as the order of a function. We say that one function <em>T</em>(<em>n</em>) is a big O of another function, <em>F</em>(<em>n</em>), and we define this as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_03_008.jpg"/></div>
<p>The function, <em>g</em>(<em>n</em>), of the input size, <em>n</em>, is based on the observation that for all sufficiently large values of <em>n</em>, <em>g</em>(<em>n</em>) is bounded above by a constant multiple of <em>f</em>(<em>n</em>). The objective is to find the smallest rate of growth that is less than or equal to <em>f</em>(<em>n</em>). We only care what happens at higher values of <em>n</em>. The variable <em>n<sub>0</sub></em>represents the threshold below which the rate of growth is not important, The function T(n) represents the <strong>tight upper bound</strong> F(n). In the following plot we see that <em>T</em>(<em>n</em>) = <em>n<sup>2</sup></em> + 500 = <em>O</em>(<em>n<sup>2</sup></em>) with <em>C</em> = 2 and <em>n<sub>0</sub></em> is approximately 23:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="256" src="assets/image_03_022.png" width="344"/></div>
<p>You will also see the notation <em>f</em>(<em>n</em>) = <em>O</em>(<em>g</em>(<em>n</em>)). This describes the fact that <em>O</em>(<em>g</em>(<em>n</em>)) is really a set of functions that include all functions with the same or smaller rates of growth than <em>f</em>(n). For example, <em>O</em>(<em>n<sup>2</sup></em>) also includes the functions <em>O</em>(<em>n</em>), <em>O</em>(<em>n</em>log<em>n</em>), and so on.</p>
<p>In the following table, we list the most common growth rates in order from lowest to highest. We sometimes call these growth rates the <strong>time complexity</strong> of a function, or the complexity class of a function:</p>
<table class="table">
<tbody>
<tr>
<td>
<p><strong>Complexity Class</strong></p>
</td>
<td>
<p><strong>Name</strong></p>
</td>
<td>
<p><strong>Example operations</strong></p>
</td>
</tr>
<tr>
<td>
<p>O(1)</p>
</td>
<td>
<p>Constant</p>
</td>
<td>
<p>append, get item, set item.</p>
</td>
</tr>
<tr>
<td>
<p>O(log<em>n</em>)</p>
</td>
<td>
<p>Logarithmic</p>
</td>
<td>
<p>Finding an element in a sorted array.</p>
</td>
</tr>
<tr>
<td>
<p>O(n)</p>
</td>
<td>
<p>Linear</p>
</td>
<td>
<p>copy, insert, delete, iteration.</p>
</td>
</tr>
<tr>
<td>
<p><em>n</em>Log<em>n</em></p>
</td>
<td>
<p>Linear-Logarithmic</p>
</td>
<td>
<p>Sort a list, merge - sort.</p>
</td>
</tr>
<tr>
<td>
<p><em>n<sup>2</sup></em></p>
</td>
<td>
<p>Quadratic</p>
</td>
<td>
<p>Find the shortest path between two nodes in a graph. Nested loops.</p>
</td>
</tr>
<tr>
<td>
<p><em>n<sup>3</sup></em></p>
</td>
<td>
<p>Cubic</p>
</td>
<td>
<p>Matrix multiplication.</p>
</td>
</tr>
<tr>
<td>
<p>2<em><sup>n</sup></em></p>
</td>
<td>
<p>Exponential</p>
</td>
<td>
<p>'Towers of Hanoi' problem, backtracking.</p>
</td>
</tr>
</tbody>
</table>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Composing complexity classes</h1>
            </header>

            <article>
                
<p>Normally, we need to find the total running time of a number of basic operations. It turns out that we can combine the complexity classes of simple operations to find the complexity class of more complex, combined operations. The goal is to analyze the combined statements in a function or method to understand the total time complexity of executing several operations. The simplest way to combine two complexity classes is to add them. This occurs when we have two sequential operations. For example, consider the two operations of inserting an element into a list and then sorting that list. We can see that inserting an item occurs in O(<em>n</em>) time and sorting is O(<em>n</em>log<em>n</em>) time. We can write the total time complexity as O(<em>n</em> + <em>n</em>log<em>n</em>), that is, we bring the two functions inside the O(...). We are only interested in the highest order term, so this leaves us with just O(<em>n</em>log<em>n</em>).</p>
<p>If we repeat an operation, for example, in a while loop, then we multiply the complexity class by the number of times the operation is carried out. If an operation with time complexity O(<em>f</em>(<em>n</em>)) is repeated O(<em>n</em>) times then we multiply the two complexities:</p>
<p>O(<em>f</em>(<em>n</em>) * O(<em>n</em>)) = O(<em>nf</em>(<em>n</em>)).</p>
<p>For example, suppose the function f(...) has a time complexity of O(<em>n</em><sup>2</sup>) and it is executed <em>n</em> times in a while loop as follows:</p>
<pre>
    for i n range(n): <br/>        f(...) 
</pre>
<p>The time complexity of this loop then becomes O(<em>n</em><sup>2</sup>) * O(<em>n</em>) = O(<em>n * n<sup>2</sup></em>) = O(<em>n<sup>3</sup></em>). Here we are simply multiplying the time complexity of the operation with the number of times this operation executes. The running time of a loop is at most the running time of the statements inside the loop multiplied by the number of iterations. A single nested loop, that is, one loop nested inside another loop, will run in <em>n</em><sup>2</sup> time assuming both loops run <em>n</em> times. For example:</p>
<pre>
    for i in range(0,n):  <br/>        for j in range(0,n) <br/>            #statements 
</pre>
<p>Each statement is a constant, c, executed <em>n</em><em>n</em> times, so we can express the running time as ; <em>c</em><em>n</em> <em>n</em> = <em>cn</em><sup>2</sup> = O(<em>n</em>2).</p>
<p>For consecutive statements within nested loops we add the time complexities of each statement and multiply by the number of times the statement executed. For example:</p>
<pre>
    n = 500    #c0   <br/>    #executes n times <br/>    for i in range(0,n): <br/>        print(i)    #c1 <br/>    #executes n times <br/>    for i in range(0,n): <br/>        #executes n times <br/>        for j in range(0,n): <br/>        print(j)   #c2 
</pre>
<p>This can be written as <em>c</em><sub>0</sub> +<em>c</em><sub>1</sub><em>n</em> + <em>cn</em><sup>2</sup> = O(<em>n</em><sup>2</sup>).</p>
<p>We can define (base 2) logarithmic complexity, reducing the size of the problem by ½, in constant time. For example, consider the following snippet:</p>
<pre>
    i = 1 <br/>    while i &lt;= n: <br/>        i=i * 2 <br/>        print(i) 
</pre>
<p>Notice that <kbd>i</kbd> is doubling on each iteration, if we run this with <em>n</em> = 10 we see that it prints out four numbers; 2, 4, 8, and 16. If we double <em>n</em> we see it prints out five numbers. With each subsequent doubling of n the number of iterations is only increased by 1. If we assume <em>k</em> iterations, we can write this as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/03_009.png"/></div>
<p>From this we can conclude that the total time = <strong>O</strong>(<em>log(n)</em>).</p>
<p>Although Big O is the most used notation involved in asymptotic analysis, there are two other related notations that should be briefly mentioned. They are Omega notation and Theta notation.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Omega notation (Ω)</h1>
            </header>

            <article>
                
<p>In a similar way that Big O notation describes the upper bound, Omega notation describes a <strong>tight lower bound</strong>. The definition is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-232 image-border" height="54" src="assets/03_010.png" width="383"/></div>
<p>The objective is to give the largest rate of growth that is equal to or less than the given algorithms, T(<em>n</em>), rate of growth.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Theta notation (ϴ)</h1>
            </header>

            <article>
                
<p>It is often the case where both the upper and lower bounds of a given function are the same and the purpose of Theta notation is to determine if this is the case. The definition is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-233 image-border" height="52" src="assets/03_011.png" width="410"/></div>
<p>Although Omega and Theta notations are required to completely describe growth rates, the most practically useful is Big O notation and this is the one you will see most often.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Amortized analysis</h1>
            </header>

            <article>
                
<p>Often we are not so interested in the time complexity of individual operations, but rather the time averaged running time of sequences of operations. This is called amortized analysis. It is different from average case analysis, which we will discuss shortly, in that it makes no assumptions regarding the data distribution of input values. It does, however, take into account the state change of data structures. For example, if a list is sorted it should make any subsequent find operations quicker. Amortized analysis can take into account the state change of data structures because it analyzes sequences of operations, rather then simply aggregating single operations.</p>
<p>Amortized analysis finds an upper bound on runtime by imposing an artificial cost on each operation in a sequence of operations, and then combining each of these costs. The artificial cost of a sequence takes in to account that the initial expensive operations can make subsequent operations cheaper.</p>
<p>When we have a small number of expensive operations, such as sorting, and lots of cheaper operations such as lookups, standard worst case analysis can lead to overly pessimistic results, since it assumes that each lookup must compare each element in the list until a match is found. We should take into account that once we sort the list we can make subsequent find operations cheaper.</p>
<p>So far in our runtime analysis we have assumed that the input data was completely random and have only looked at the effect the size of the input has on the runtime. There are two other common approaches to algorithm analysis; they are:</p>
<ul>
<li>Average case analysis</li>
<li>Benchmarking</li>
</ul>
<p>Average case analysis finds the average running time based on some assumptions regarding the relative frequencies of various input values. Using real-world data, or data that replicates the distribution of real-world data, is many times on a particular data distribution and the average running time is calculated.</p>
<p>Benchmarking is simply having an agreed set of typical inputs that are used to measure performance. Both benchmarking and average time analysis rely on having some domain knowledge. We need to know what the typical or expected datasets are. Ultimately we will try to find ways to improve performance by fine-tuning to a very specific application setting.</p>
<p>Let's look at a straightforward way to benchmark an algorithm's runtime performance. This can be done by simply timing how long the algorithm takes to complete given various input sizes. As we mentioned earlier, this way of measuring runtime performance is dependent on the hardware that it is run on. Obviously faster processors will give better results, however, the relative growth rates as we increase the input size will retain characteristics of the algorithm itself rather than the hardware it is run on. The absolute time values will differ between hardware (and software) platforms; however, their relative growth will still be bound by the time complexity of the algorithm.</p>
<p>Let's take a simple example of a nested loop. It should be fairly obvious that the time complexity of this algorithm is O(n<sup>2</sup>) since for each n iterations in the outer loop there are also n iterations in the inter loop. For example, our simple nested for loop consists of a simple statement executed on the inner loop:</p>
<pre>
    def nest(n): <br/>        for i in range(n): <br/>            for j in range(n): <br/>                i+j 
</pre>
<p>The following code is a simple test function that runs the nest function with increasing values of <kbd>n</kbd>. With each iteration we calculate the time this function takes to complete using the <kbd>timeit.timeit</kbd> function. The <kbd>timeit</kbd> function, in this example, takes three arguments, a string representation of the function to be timed, a setup function that imports the nest function, and an <kbd>int</kbd> parameter that indicates the number of times to execute the main statement. Since we are interested in the time the nest function takes to complete relative to the input size, <kbd>n</kbd>, it is sufficient, for our purposes, to call the nest function once on each iteration. The following function returns a list of the calculated runtimes for each value of n:</p>
<pre>
    import timeit  <br/>    def test2(n): <br/>        ls=[] <br/>        for n in range(n): <br/>            t=timeit.timeit("nest(" + str(n) +")", setup="from __main__ import nest", number = 1) <br/>            ls.append(t) <br/>        return ls    
</pre>
<p>In the following code we run the test2 function and graph the results, together with the appropriately scaled n<sup>2</sup> function for comparison, represented by the dashed line:</p>
<pre>
    import matplotlib.pyplot as plt <br/>    n=1000 <br/>    plt.plot(test2(n)) <br/>    plt.plot([x*x/10000000 for x in range(n)]) 
</pre>
<p>This gives the following results:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" height="230" src="assets/image_03_026.png" width="350"/></div>
<p>As we can see, this gives us pretty much what we expect. It should be remembered that this represents both the performance of the algorithm itself as well as the behavior of underlying software and hardware platforms, as indicated by both the variability in the measured runtime and the relative magnitude of the runtime. Obviously a faster processor will result in faster runtimes, and also performance will be affected by other running processes, memory constraints, clock speed, and so on.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we have taken a general overview of algorithm design. Importantly, we saw a platform independent way to measure an algorithm's performance. We looked at some different approaches to algorithmic problems. We looked at a way to recursively multiply large numbers and also a recursive approach for merge sort. We saw how to use backtracking for exhaustive search and generating strings. We also introduced the idea of benchmarking and a simple platform-dependent way to measure runtime. In the following chapters, we will revisit many of these ideas with reference to specific data structures. In the next chapter, we will discuss linked lists and other pointer structures.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>