<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer126">
			<h1 id="_idParaDest-110"><em class="italic"><a id="_idTextAnchor101"/>Chapter 6</em>: Automatic Differentiation and Accelerated Linear Algebra for Machine Learning</h1>
			<p>With the recent explosion of data and data generating systems, machine learning has grown to be an exciting field, both in research and industry. However, implementing a machine learning model might prove to be a difficult endeavor. Specifically, common tasks in machine learning, such as deriving the loss function and its derivative, using gradient descent to find the optimal combination of model parameters, or using the kernel method for nonlinear data, demand clever implementations to make predictive models efficient. </p>
			<p>In this chapter, we will discuss the JAX library, the premier high-performance machine learning tool in Python. We will explore some of its most powerful features, such as automatic differentiation, JIT compilation, and automatic vectorization. These features streamline the tasks that are central to machine learning mentioned previously, making training a predictive model as simple and accessible as possible. All these discussions will revolve around a hands-on example of a binary classification problem.</p>
			<p>By the end of the chapter, you will be able to use JAX to power your machine learning applications and explore the more advanced features that it offers.</p>
			<p>The list of topics covered in this chapter is as follows:</p>
			<ul>
				<li>A crash course in machine learning</li>
				<li>Getting JAX up and running</li>
				<li>Automatic differentiation for loss minimization</li>
				<li>Just-In-Time compilation for improved efficiency</li>
				<li>Automatic vectorization for efficient kernels</li>
			</ul>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor102"/>A crash course in machine learning</h1>
			<p>To fully appreciate the functionalities that JAX offers, let's first talk about the principal components of a typical workflow of training a machine learning model. If you are <a id="_idIndexMarker419"/>already familiar with the basics, feel free to skip to the next section, where we begin discussing JAX.</p>
			<p>In machine learning, we set out to solve the problem of predicting an unknown target value of interest of a data point by considering its observable features. The goal is to design a predictive model that processes the observable features and outputs an estimate of what the target value might be. For example, image recognition models analyze the pixel values of an image to predict which object the image depicts, while a model processing weather data could predict the probability of rainy weather for tomorrow by accounting for temperature, wind, and humidity.</p>
			<p>In general, a machine learning model could be viewed as a general mathematical function that takes in the observable features, typically referred to as <em class="italic">X</em>, and produces a prediction about the target value, <em class="italic">Y</em>. Implicitly assumed by the model is the fact that there is a certain mathematical relationship between the features (input) and the target values (output). Machine learning models in turn aim to learn about this relationship by studying a large number of examples. As such, data is crucial in any machine learning workflow.</p>
			<p>A <strong class="bold">dataset</strong> is a <a id="_idIndexMarker420"/>collection of <em class="italic">labeled</em> examples – data points whose observable features, <em class="italic">X</em>, and target values, <em class="italic">Y</em>, are both available. It is through these labeled points that a model will attempt to uncover the relationship between <em class="italic">X</em> and <em class="italic">Y</em>; this is known as <em class="italic">training</em> the model. After training, the learned model can then take in the features of <em class="italic">unlabeled</em> data points, whose target values are unknown to us, and output its predictions. These predictions are what the model believes the target values of the unlabeled data points to be, given the observable features that they have.</p>
			<p>In this section, we will talk about how training is facilitated for many common machine learning models. Specifically, it consists of three core components—the model's parameters, the model's loss function, and the minimization of that loss function—which we will consider one by one.</p>
			<p>We will ground our discussions with an explicit example problem. Assume that we have a number of points in a two-dimensional space (which is simply the <em class="italic">xy</em> plane), each of which belongs to either the positive class or the negative class. The following is <a id="_idIndexMarker421"/>an example where the yellow points are the positives, and the purple ones are the negatives:</p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/Figure_6.1_17499.jpg" alt="Figure 6.1 – An example binary classification problem " width="664" height="248"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – An example binary classification problem</p>
			<p>In this case, the observable features are the <em class="italic">x</em> and <em class="italic">y</em> coordinates of the points, so <em class="italic">X</em> consists of two features, while the target value <em class="italic">y</em> is their class membership, a positive or a negative. Our goal is to train a machine learning model on a set of labeled data and make predictions on unlabeled points. For example, after training, our model should be able to predict whether point (2, 1) is positive or negative.</p>
			<p>It is quite clear to us what the separation between the positives and the negatives is, where the lower-right corner corresponds to the positive class, and the upper-left <a id="_idIndexMarker422"/>corner corresponds to the negative class. Point (2, 1) specifically is likely a positive (a yellow point). But how does a machine learning model do this automatically, without humans having to hardcode such a classification rule? We will see this in the next section.</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor103"/>Model parameters</h2>
			<p>A predictive model starts out with a formula that attempts to explain the relationship <a id="_idIndexMarker423"/>between the observable features and the target value. We will consider the class of linear models (one of the most common and simplest types of machine learning models), which assume that the target value is a linear combination of the features (hence the model's name)</p>
			<p class="figure"><img src="image/Formula_6.1_B17499.png" alt="" width="440" height="49"/></p>
			<p>where <img src="image/Formula_6.2_B17499.png" alt="" width="40" height="32"/> and <img src="image/Formula_6.3_B17499.png" alt="" width="38" height="31"/> are the <em class="italic">x</em> and <em class="italic">y</em> coordinates of the points, and the numbers <img src="image/Formula_6.4_B17499.png" alt="" width="41" height="31"/> are the model's <em class="italic">parameters</em>.</p>
			<p>We do not know the values of these parameters, and each combination of values for these parameters defines a unique relationship between <em class="italic">x</em> and <em class="italic">y</em>, a unique hypothesis. It is the values of these parameters that we need to identify during the training phase, such that the aforementioned relationship fits the labeled data that we have access to.</p>
			<p>In our classification model, we constrain the target values <em class="italic">y</em> to be either positive (+1) or negative (-1), but <em class="italic">y</em> as defined by the linear relationship discussed earlier can take on any real value. As such, it is customary to transform a real-valued <em class="italic">y</em> to be either +1 or -1 by simply taking the sign of the value. In other words, the actual model we are working with is:</p>
			<p class="figure"><img src="image/Formula_6.5_B17499.png" alt="" width="550" height="46"/></p>
			<p>Performing <a id="_idIndexMarker424"/>predictions is at the heart of a common machine learning task. Given the linear relationship between features <em class="italic">X</em> and the target value <em class="italic">y</em>, predicting the target value <img src="image/Formula_6.6_B17499.png" alt="" width="34" height="30"/> of a new data point with features <img src="image/Formula_6.7_B17499.png" alt="" width="60" height="34"/> and <img src="image/Formula_6.8_B17499.png" alt="" width="57" height="33"/> simply involves applying the mathematical model we have:</p>
			<p class="figure"><img src="image/Formula_6.9_B17499.png" alt="" width="606" height="50"/></p>
			<p>The only missing piece of our puzzle is how to identify the correct values for our parameters <img src="image/Formula_6.10_B17499.png" alt="" width="45" height="34"/>. Again, we need to determine the values for the model's parameters to fit our training data. But how can we exactly quantify how well a specific value combination for the parameters explains a given labeled dataset? This question leads to the next component of a machine learning workflow: the model's loss function.</p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor104"/>Loss function</h2>
			<p>A model's loss function quantifies a property about the model that we'd like to minimize. This could, in general, be the cost incurred when the model is trained a certain way. Within our context, the loss function is specifically the amount of predictive <a id="_idIndexMarker425"/>error that the model makes on the training dataset. This quantity approximates the error made on future unseen data, which is what we really care about. (In different settings, you might encounter <strong class="bold">utility functions</strong>, which, most of the time, are simply the negative of corresponding <a id="_idIndexMarker426"/>loss functions, which are to be maximized.) The lower the loss, the better our model is likely to perform (barring technical problems such as overfitting).</p>
			<p>In a classification problem like ours, there are many ways to design an appropriate loss function. For example, the 0-1 loss simply counts how many instances there are in which the model makes an incorrect prediction, while the binary cross-entropy loss is a more popular choice as the function is smoother than 0-1 and thus easy to optimize (we will come back to this point later).</p>
			<p>For our example, we will use a version of the support-vector loss function, which is formalized as:</p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/Formula_6.11_B17499.jpg" alt="" width="1650" height="158"/>
				</div>
			</div>
			<p>where <em class="italic">i</em>, indicating the index of unique examples in the training set, ranges from 1 to <em class="italic">n</em>, the training set size. <img src="image/Formula_6.12_B17499.png" alt="" width="40" height="36"/> is the true label of the <em class="italic">i</em>-th example, while <img src="image/Formula_6.13_B17499.png" alt="" width="48" height="46"/> is the corresponding label predicted by the learning model.</p>
			<p>This non-negative function calculates the average of the <img src="image/Formula_6.14_B17499.png" alt="" width="320" height="44"/> term across all training examples. It is easy to see that for each example <em class="italic">i</em>, if <img src="image/Formula_6.15_B17499.png" alt="" width="39" height="35"/> and <img src="image/Formula_6.16_B17499.png" alt="" width="44" height="43"/> are similar to each other, the max term will evaluate to a small value. On the other hand, if <img src="image/Formula_6.17_B17499.png" alt="" width="38" height="34"/> and <img src="image/Formula_6.18_B17499.png" alt="" width="43" height="41"/> have opposite signs (in other words, if the model misclassifies the example), the max term will increase. As such, this function is appropriate to quantify the error that our predictive model is making for a given parameter combination.</p>
			<p>Given this loss function, the problem of training our linear model reduces to finding the <a id="_idIndexMarker427"/>optimal parameter <img src="image/Formula_6.19_B17499.png" alt="" width="45" height="34"/> that minimizes the loss (preferably at 0). Of course, the search space of <img src="image/Formula_6.20_B17499.png" alt="" width="45" height="34"/> is very large—each parameter could be any real-valued number—so exhaustively trying out all possible combinations of values for <img src="image/Formula_6.21_B17499.png" alt="" width="45" height="34"/> is out of the question. This leads us to the final subsection: intelligently minimizing a loss function.</p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor105"/>Loss minimization</h2>
			<p>Without <a id="_idIndexMarker428"/>going into too much technical detail, you can mathematically find the minimum of a given function by utilizing its <em class="italic">derivative</em> information. Derivates of a function denote the rate of change in the function value with respect to the rate of change of its input. By analyzing the value of the derivative, or more commonly referred to in machine learning as the <em class="italic">gradient</em>, we could algorithmically identify input locations at which the objective function is likely to be low-valued.</p>
			<p>However, derivatives are only informative in helping us locate the function minimum if the function itself is <em class="italic">smooth</em>. Smoothness is a mathematical property that states that if the input value of a function only changes by a little, the function value (output) should not change by too much. Notable examples of non-smooth functions include the step function and the absolute value function, but most <a id="_idIndexMarker429"/>functions that you will encounter are likely to be smooth. The smoothness property explains our preference for the binary cross-entropy loss over the 0-1 loss, the latter of which is a non-smooth function. The support vector-style loss function we will be using is also smooth.</p>
			<p>The overall takeaway is that if we have access to the derivate of a smooth function, finding its minimum will be easy to do. But how do we do this exactly? Many gradient-based optimization routines have been studied, but we will consider arguably the most common method: <em class="italic">gradient descent</em>. The high-level idea is to inspect the gradient of the loss function at a given location and move in the direction <em class="italic">opposite</em> to that gradient (in other words, descending the gradient, hence the name), illustrated next:</p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/Figure_6.2_B17499.jpg" alt="Figure 6.2 – Illustration of gradient descent " width="1650" height="584"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2 – Illustration of gradient descent</p>
			<p>Due to the definition of the gradient, the function value <em class="italic">f(x')</em> evaluated at the point <em class="italic">x'</em> that is immediately in the opposite direction of the gradient with respect to a given point <em class="italic">x</em> is lower than <em class="italic">f(x)</em>. As such, by using the gradient of the loss function <a id="_idIndexMarker430"/>as a guide and incrementally moving in the opposite direction, we can theoretically arrive at the function minimum. One important consideration is that, if we move our evaluation by a large amount, even in the correct direction, we could end up missing the true minimum. To address this, we typically adjust our step to be only a fraction of the loss gradient.</p>
			<p>Concretely, denote <img src="image/Formula_6.22_B17499.png" alt="" width="50" height="33"/> as the gradient of the loss function <em class="italic">L</em> with respect to the <em class="italic">i</em>-th parameter <img src="image/Formula_6.23_B17499.png" alt="" width="42" height="32"/>, so the gradient descent <em class="italic">update rule</em> is to adjust the value of <img src="image/Formula_6.24_B17499.png" alt="" width="40" height="30"/> as:</p>
			<p class="figure-caption"><img src="image/Formula_6.25_B17499.png" alt="" width="307" height="44"/></p>
			<p>where <em class="italic">γ</em> is a small number (much lower than 1) that acts as a step size. By repeatedly applying this update rule, we could incrementally adjust the values of the model's parameters to lower the model's loss, thereby improving its predictive performance.</p>
			<p>And with that, we have finished sketching out the main components of a typical machine <a id="_idIndexMarker431"/>learning workflow. As a summary, we work with a predictive model (in our case, a linear one) that has a certain number of adjusting parameters that seek to explain the data we have. Each parameter combination leads to a different hypothesis whose validity is quantified by the loss function. Finally, by taking the derivative of the loss function and using gradient descent, we have a way to incrementally update the model's parameters to achieve better performance.</p>
			<p>We will now begin the main topic of this chapter: the JAX library.</p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor106"/>Getting JAX up and running</h1>
			<p>As briefly mentioned, JAX is a combination of different tools for developing accelerated, high-performance computations with a focus on machine learning applications. Remember <a id="_idIndexMarker432"/>from the last chapter that the NumPy library offers optimized computation for numerical operations such as finding the min/max or taking the sum of the average along an axis. We can think of JAX as the NumPy equivalent for machine learning, where common tasks in machine learning could be done in highly optimized code. These, as we will see, include automatic differentiation, accelerated linear algebra using a Just-In-Time compiler, and efficient vectorization and parallelization of code, among other things.</p>
			<p>JAX offers these <a id="_idIndexMarker433"/>functionalities through what's known as <strong class="bold">functional transformations</strong>. In the simplest sense, a functional transformation in JAX <a id="_idIndexMarker434"/>converts a function, typically one that we build ourselves, to an optimized version where different functionalities are facilitated. This is done via a simple API, as we will see later.</p>
			<p>But first, we will talk about installing the library on a local system.</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor107"/>Installing JAX</h2>
			<p>A barebones <a id="_idIndexMarker435"/>version of JAX could be simply installed using the regular Python package manager <strong class="source-inline">pip</strong>:</p>
			<p class="source-code">pip install --upgrade pip</p>
			<p class="source-code">pip install --upgrade jax jaxlib</p>
			<p>These commands will install a CPU-only version of JAX, which is recommended if you are planning to try it out on a local system such as a laptop. The full version of JAX, with support for NVIDIA GPUs (commonly used by deep learning models), could also be installed via a more involved procedure, outlined in their documentation: <a href="https://jax.readthedocs.io/en/latest/developer.html">https://jax.readthedocs.io/en/latest/developer.html</a>.</p>
			<p>Furthermore, note that JAX can only be installed on Linux (Ubuntu 16.04 or above) or macOS (10.12 or above). To run JAX on a Windows machine, you would need the Windows Subsystem for Linux, which lets you run a GNU/Linux environment on Windows. If you typically run your machine learning jobs on a cluster, you should talk to your system administrator about finding the correct way to install JAX.</p>
			<p>If you are an independent machine learning practitioner, you might find these technical requirements to install the full version of JAX intimidating. Luckily, there is a free platform that we could use to see JAX in action: <em class="italic">Google Colab</em>.</p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor108"/>Using Google Colab</h2>
			<p>Generally, Google Colab is a free Jupyter notebook platform that is integrated with a high-performance backend and machine learning tools on which you could build <a id="_idIndexMarker436"/>prototypes of machine learning models with ease.</p>
			<p>There are two main advantages to using Google Colab. First, the platform comes with most machine learning-related libraries (for example, TensorFlow, PyTorch, and scikit-learn) and <a id="_idIndexMarker437"/>tools preinstalled, including a <strong class="bold">GPU</strong> (<strong class="bold">graphics processing unit</strong>) and a <strong class="bold">TPU</strong> (<strong class="bold">tensor processing unit</strong>), free of charge! This allows independent machine learning researchers and students <a id="_idIndexMarker438"/>to try out expensive hardware on their machine learning pipelines at no cost. Second, Google Colab allows collaborative editing between different users, enabling streamlined group work when building models.</p>
			<p>To utilize this platform, all you need is a Google/Gmail account when signing in at <a href="https://colab.research.google.com/">https://colab.research.google.com/</a>. Using this platform is generally simple and intuitive, but you can also refer to several readings included at the end of this chapter for further content relating to Google Colab. Note that the code presented in the remainder of this chapter is run on Google Colab, which means you could go to <strong class="bold">[INSERT LINK]</strong> and simply run the code yourself to follow along with our later discussions.</p>
			<p>And with that, we are ready to begin talking about the functionalities that JAX offers.</p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor109"/>Automatic differentiation for loss minimization</h1>
			<p>Recall from our previous discussion that to fit a predictive model to a training dataset, we first <a id="_idIndexMarker439"/>analyze an appropriate <a id="_idIndexMarker440"/>loss function, derive the gradient of this loss, and then adjust the parameters of the model in the opposite direction of the gradient to achieve a lower loss. This procedure is only possible if we have access to the derivative of the loss function.</p>
			<p>Earlier machine learning models were able to do this because researchers derived the derivatives of common loss functions by hand using calculus, which were then hardcoded into the training algorithm so that a loss function could be minimized. Unfortunately, taking the derivative of a function could be difficult to do at times, especially if the loss function being used is not well behaved. In the past, you would have to choose a different, more mathematically convenient loss function to make your model run even if the new function <a id="_idIndexMarker441"/>was less appropriate, potentially sacrificing some of the expressiveness of the original function.</p>
			<p>Recent advances in computational algebra have resulted in the technique called <strong class="bold">automatic differentiation</strong>. Exactly as it sounds, this technique allows for an automated <a id="_idIndexMarker442"/>process to numerically evaluate the derivative of a given function, expressed in computer code, without relying on a human <a id="_idIndexMarker443"/>to provide the closed-form solution for that derivative. On the highest level, automatic differentiation traces the order in which the considered function is computed and which arithmetic operations (for example, addition, multiplication, and exponentiation) were involved, and then applies the chain rule to obtain the numerical value of the derivative in an automated manner. By intelligently taking advantage of the information present in the computational calculation of the function itself, automatic differentiation can compute its derivative efficiently and accurately.</p>
			<p>Recall that in our running toy problem, we would like to minimize the loss function:</p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/Formula_6.26_B17499.jpg" alt="" width="1650" height="171"/>
				</div>
			</div>
			<p>where <img src="image/Formula_6.27_B17499.png" alt="" width="46" height="44"/> is the prediction made by our linear predictive model for the <em class="italic">i</em>-th data point. Of course, we could derive the gradient of this loss function when implementing gradient descent (which would require us to brush up on our calculus!), but with automatic differentiation, we will not have to.</p>
			<p>The technique is widely used in deep learning tools such as TensorFlow and PyTorch, where computing the gradient of a loss function is a central task. We could <a id="_idIndexMarker444"/>use the API for automatic <a id="_idIndexMarker445"/>differentiation from these libraries to compute the loss gradient, but by using JAX's API, we will be able to utilize other powerful functionalities that we will get to later. For now, let's build our model and the gradient descent algorithm using JAX's automatic differentiation tool.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor110"/>Making the dataset</h2>
			<p>First, we need to generate the toy dataset shown in <em class="italic">Figure 6.1</em>. To do this, we will make <a id="_idIndexMarker446"/>use of the <strong class="source-inline">make_blobs</strong> function from the <strong class="source-inline">datasets</strong> module in scikit-learn. If you are running your code locally, you would need to install scikit-learn using the <strong class="source-inline">pip</strong> command:</p>
			<p class="source-code">pip install sklearn</p>
			<p>Otherwise, if you are using Google Colab, scikit-learn, as well as all the other external libraries used in this chapter's code, already comes preinstalled, so you only need to import it into your program.</p>
			<p>The <strong class="source-inline">make_blobs</strong> function, as the name suggests, generates blobs (in other words, clusters) of data points. Its API is simple and readable:</p>
			<p class="source-code">X, y = make_blobs(</p>
			<p class="source-code">    n_samples=500, n_features=2, centers=2, \</p>
			<p class="source-code">      cluster_std=0.5, random_state=0</p>
			<p class="source-code">)</p>
			<p>Here we are creating a two-dimensional (specified by <strong class="source-inline">n_features</strong>) dataset of 500 points (specified by <strong class="source-inline">n_samples</strong>). These data points should belong to one of two classes (specified by <strong class="source-inline">centers</strong>). The <strong class="source-inline">cluster_std</strong> argument controls how tightly the points belonging to the same class cluster together; in this case, we are setting it to <strong class="source-inline">0.5</strong>. At this point, we have a dataset of 500 points, each of which has two features and belongs to either the positive class or the negative class. The variable <strong class="source-inline">X</strong> is a NumPy array that contains the features, having a shape of <strong class="source-inline">(500, 2)</strong>, while <strong class="source-inline">y</strong> is a 500-long, one-dimensional, binary array containing the labels.</p>
			<p>By default, the negative class has the label of <strong class="source-inline">0</strong> in <strong class="source-inline">y</strong>. As a data preprocessing step, we will <a id="_idIndexMarker447"/>convert all the instances of <strong class="source-inline">0</strong> in <strong class="source-inline">y</strong> to <strong class="source-inline">-1</strong> using NumPy's convenient conditional indexing syntax:</p>
			<p class="source-code">y[y == 0] = -1</p>
			<p>We are also adding to <strong class="source-inline">X</strong> a third feature column that only contains instances of <strong class="source-inline">1</strong>. This column is a constant and corresponds to the free coefficient <img src="image/Formula_6.28.1_B17499.png" alt="" width="46" height="30"/> in our model:</p>
			<p class="figure-caption"><img src="image/Formula_6.28_B17499.png" alt="" width="578" height="48"/></p>
			<p>On the other hand, <img src="image/Formula_6.29_B17499.png" alt="" width="48" height="32"/> and <img src="image/Formula_6.30_B17499.png" alt="" width="49" height="32"/> are the coefficients for the first and second features in <strong class="source-inline">X</strong>.</p>
			<p>We have now generated our toy example. If you now call Matplotlib to generate a scatter plot on <strong class="source-inline">X</strong>, for example, via the following code:</p>
			<p class="source-code">plt.scatter(X[:, 0], X[:, 1], c=y);</p>
			<p>You will obtain the same plot as in <em class="italic">Figure 6.1</em>. And with that, we are ready to begin building our predictive model.</p>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor111"/>Building a linear model</h2>
			<p>Remember two core components of a machine learning pipeline: the model and the loss <a id="_idIndexMarker448"/>function. To build our model, we first need to write functions that correspond to these components. First, the model comes in the form of the <strong class="source-inline">predict</strong> function:</p>
			<p class="source-code">import jax.numpy as jnp</p>
			<p class="source-code">def predict(w, X):</p>
			<p class="source-code">    return jnp.dot(X, w)</p>
			<p>This function takes in the model's parameters <strong class="source-inline">w</strong>, implicitly assumed to be in the form of a NumPy array, and the features of the training data points <strong class="source-inline">X</strong>, a NumPy two-dimensional matrix. Here we are using the <strong class="source-inline">dot</strong> function from the NumPy module of JAX, aliased <strong class="source-inline">jnp</strong> in the code, to take the dot product of <strong class="source-inline">w</strong> and <strong class="source-inline">X</strong>. If you are unfamiliar with the dot product, it is simply an algebraic operation that is a shorthand for the linear combination <img src="image/Formula_6.31_B17499.png" alt="" width="340" height="41"/> that corresponds to our model. Overall, this function encodes for our model that the prediction we make about the label of a data point is (the sign of) the dot product of the parameters and the data features.</p>
			<p>While not a focus of our discussion, it is important at this point to note that the <strong class="source-inline">numpy</strong> module of JAX provides nearly identical APIs for mathematical operations as NumPy. The <strong class="source-inline">dot</strong> function is simply one example; others could be found on their documentation site: <a href="https://jax.readthedocs.io/en/latest/jax.numpy.html">https://jax.readthedocs.io/en/latest/jax.numpy.html</a>. This is to <a id="_idIndexMarker449"/>say that if you have accumulated a significant amount of code in NumPy but would like to convert it to JAX, replacing your import statement of <strong class="source-inline">import numpy as np</strong> with <strong class="source-inline">import jax.numpy as np</strong> would, in most cases, do the trick; no complicated, painstaking conversion is necessary.</p>
			<p>So, we have implemented the prediction function of our model. Our next task is to write the loss function</p>
			<p class="figure-caption"><img src="image/Formula_6.32_B17499.png" alt="" width="588" height="156"/></p>
			<p>The loss function can be written with the following code:</p>
			<p class="source-code">def loss(w):</p>
			<p class="source-code">    preds = predict(w, X)</p>
			<p class="source-code">    return jnp.mean(jnp.clip(1 - jnp.multiply(y, preds), \</p>
			<p class="source-code">      a_min=0))</p>
			<p>Let's take <a id="_idIndexMarker450"/>a moment to break this code down. This <strong class="source-inline">loss</strong> function is a function of <strong class="source-inline">w</strong>, since, if <strong class="source-inline">w</strong> changes, the loss will change accordingly.</p>
			<p>Now, let's focus on the term inside the summation symbol in the previous formula: it is the max between 0 and <img src="image/Formula_6.33_B17499.png" alt="" width="171" height="43"/>, where <img src="image/Formula_6.34_B17499.png" alt="" width="38" height="35"/> is the true label of a data point and <img src="image/Formula_6.35_B17499.png" alt="" width="41" height="40"/> is the corresponding prediction made by the model. As such, we compute this quantity with <strong class="source-inline">jnp.clip(1 - jnp.multiply(y, preds), a_min=0)</strong>, before which we call the <strong class="source-inline">predict</strong> function to obtain the values for <img src="image/Formula_6.36_B17499.png" alt="" width="44" height="43"/>, stored in the <strong class="source-inline">preds</strong> variable.</p>
			<p>If you are familiar with the NumPy equivalent, you might have noticed that these functions that we are using, <strong class="source-inline">clip</strong> and <strong class="source-inline">multiply</strong>, have an identical interface in NumPy; again, JAX tries to make the transition for NumPy users as seamless as possible. Finally, we compute the mean of this max term across all training examples with the <strong class="source-inline">mean</strong> function.</p>
			<p>Believe it or not, we have successfully implemented our linear model! With a specific value for <strong class="source-inline">w</strong>, we will be able to predict what the label of a data point is using the <strong class="source-inline">predict</strong> function and compute our loss with the <strong class="source-inline">loss</strong> function. At this point, we can try out different values for <strong class="source-inline">w</strong>; as a common practice, we will initialize <strong class="source-inline">w</strong> to be random numbers:</p>
			<p class="source-code">np.random.seed(0)</p>
			<p class="source-code">w = np.random.randn(3)</p>
			<p>The last <a id="_idIndexMarker451"/>step of our training procedure is to find the best value for <strong class="source-inline">w</strong> that minimizes the loss, which we will implement next.</p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor112"/>Gradient descent with automatic differentiation</h2>
			<p>Recall that in the gradient descent algorithm, we compute the gradient of the loss function <a id="_idIndexMarker452"/>for the current value of <strong class="source-inline">w</strong> and then adjust <strong class="source-inline">w</strong> by subtracting a fraction of the gradient (effectively moving in the opposite direction of the gradient).</p>
			<p>Again, in the past, we would need to derive the gradient in closed form and implement it in our code. With automatic differentiation, we simply need the following:</p>
			<p class="source-code">from jax import grad</p>
			<p class="source-code">loss_grad = grad(loss)</p>
			<p><strong class="source-inline">loss</strong> is the function we implemented in the last subsection. Here we are using the <strong class="source-inline">grad</strong> function from JAX to obtain the <em class="italic">gradient function</em> of <strong class="source-inline">loss</strong>. This is to say that if we were to call <strong class="source-inline">loss_grad</strong>, the gradient function, on a given value of <strong class="source-inline">w</strong> using <strong class="source-inline">loss_grad(w)</strong>, we would obtain the gradient of the <strong class="source-inline">loss</strong> function at <strong class="source-inline">w</strong>, in the same way <strong class="source-inline">loss(w)</strong> gives us the loss itself at <strong class="source-inline">w</strong>.</p>
			<p>This is a so-called <em class="italic">function transformation</em>, which takes in a function in Python (<strong class="source-inline">loss</strong> in our case) and returns a related function (<strong class="source-inline">loss_grad</strong>) that could then be called on actual inputs to compute quantities that we are interested in. Function transformations are a powerful interface that makes it easy to work with functional logic and are the main way in which you use JAX. We will see that all other <a id="_idIndexMarker453"/>functionalities offered by JAX discussed in this chapter are function transformations.</p>
			<p>With our gradient function in hand, we now implement gradient descent using a simple <strong class="source-inline">for</strong> loop:</p>
			<p class="source-code">n_iters = 200</p>
			<p class="source-code">lr = 0.01</p>
			<p class="source-code">loss_grad = grad(loss)</p>
			<p class="source-code">for i in range(n_iters):</p>
			<p class="source-code">    grads = loss_grad(w)</p>
			<p class="source-code">    w = w - lr * grads</p>
			<p class="source-code">    tmp_loss = loss(w)</p>
			<p class="source-code">    if tmp_loss == 0:</p>
			<p class="source-code">        break</p>
			<p>If you are looking at the Google Colab notebook, you might see some other book-keeping code that is used to show a real-time progress bar, but the preceding code is the main logic of gradient descent.</p>
			<p>At each iteration of the <strong class="source-inline">for</strong> loop, we compute the gradient of the loss function at the current value of <strong class="source-inline">w</strong>, and then use that gradient to adjust the value of <strong class="source-inline">w</strong>: <strong class="source-inline">w = w - lr * grads</strong>. How big of a step we will take in the opposite direction of the gradient is controlled by the <strong class="source-inline">lr</strong> variable, set to <strong class="source-inline">0.01</strong>. This variable is typically referred to as the <strong class="bold">learning rate</strong> in the language of machine learning.</p>
			<p>After the adjustment, we will check to see whether this new value of <strong class="source-inline">w</strong> does, in fact, minimize our loss at 0, in which case we simply exit the <strong class="source-inline">for</strong> loop with the <strong class="source-inline">break</strong> statement. If not, we repeat this process until some termination condition is satisfied; in our case, we simply say that we only do this for at most 200 times (specified by the <strong class="source-inline">n_iters</strong> variable).</p>
			<p>When <a id="_idIndexMarker454"/>we run the code, we will observe that as the <strong class="source-inline">for</strong> loop progresses, the value of the loss decreases for each adjustment. At the end of the loop, our loss decreases to a small number: roughly 0.013. Using Matplotlib, we could quickly sketch out the progression of this decrease with <strong class="source-inline">plt.plot(losses)</strong>, which produces the following plot:</p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/Figure_6.3_17499.jpg" alt="Figure 6.3 – The decrease in loss of the linear model " width="648" height="248"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3 – The decrease in loss of the linear model</p>
			<p>We see that with the random guess for the value of <strong class="source-inline">w</strong> at the beginning, we receive a loss of roughly 2.5, but using gradient descent, we have reduced it to almost zero. At this point, we can be confident that the value of <strong class="source-inline">w</strong> that we currently have after gradient descent fits our data well.</p>
			<p>This concludes the training of our linear predictive model. A customary task that follows training a model is to examine the predictions made by the model across a whole feature space. This process helps ensure that our model is learning appropriately. When working in a low-dimensional space such as ours, we can even visualize this space.</p>
			<p>Specifically, our decision boundary is a straight line that (hopefully) separates the two <a id="_idIndexMarker455"/>classes that we'd like to perform classification: <img src="image/Formula_6.37_B17499.png" alt="" width="431" height="45"/>, where <img src="image/Formula_6.38_B17499.png" alt="" width="39" height="31"/> and <img src="image/Formula_6.39_B17499.png" alt="" width="38" height="30"/> are coordinates of points within our two-dimensional space. As such, we can draw this line by first generating a fine-grid array for the <em class="italic">x</em> coordinates <img src="image/Formula_6.40_B17499.png" alt="" width="52" height="33"/>:</p>
			<p class="source-code">xs = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)</p>
			<p>We can derive the <em class="italic">y</em> coordinates <img src="image/Formula_6.41_B17499.png" alt="" width="59" height="37"/> using the equation for the line we had previously:</p>
			<p class="source-code">plt.plot(xs, (xs * w[0] + w[2]) / - w[1])</p>
			<p>Plotting this line together with the scattered points corresponding to our training data, we obtain the following plot:</p>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/Figure_6.4_17499.jpg" alt="Figure 6.4 – The decision boundary of the learned model " width="628" height="248"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4 – The decision boundary of the learned model</p>
			<p>We see that our decision boundary nicely separates the two classes that we have. Now, when we'd like to make a prediction regarding which class an unseen point belongs to, we simply put its features through our <strong class="source-inline">predict</strong> function and look at the sign of the output.</p>
			<p>And that <a id="_idIndexMarker456"/>is all there is to it! Of course, when we work with more complicated models, other more specialized considerations need to be made. However, the high-level process remains unchanged: designing a model and its loss function and then using gradient descent (or some other loss minimization strategy) to find the optimal combination of parameters. We have seen that using the automatic differentiation module in JAX, we were able to do this easily with minimal code while avoiding the hairy math that is usually involved with deriving the gradient of a loss function.</p>
			<p>With that <a id="_idIndexMarker457"/>said, the benefits that JAX offers don't stop there. In our next section, we will see how to make our current code more efficient by using JAX's internal <strong class="bold">Just-In-Time</strong> (<strong class="bold">JIT</strong>) <strong class="bold">compiler</strong>.</p>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor113"/>Just-In-Time compilation for improved efficiency</h1>
			<p>As we have learned from the last chapter, JIT compilation allows a piece of code that is <a id="_idIndexMarker458"/>expected to run many times to be executed more efficiently. This process is specifically useful in machine learning where functions such as the loss or the gradient of the loss of a model need to be computed many times during the loss minimization phase. We hence expect that by leveraging a JIT compiler, we can make our machine learning models train faster.</p>
			<p>You might think that to do this, we would need to hook one of the JIT compilers we considered in the last chapter into JAX. However, JAX comes with its own JIT compiler, which requires minimal code to integrate in an existing program. We will see how to use it by modifying the training loop we made in the last section.</p>
			<p>First, we reset the <img src="image/Formula_6.10_B17499.png" alt="" width="45" height="34"/> parameters of our models:</p>
			<p class="source-code">np.random.seed(0)</p>
			<p class="source-code">w = np.random.randn(3)</p>
			<p>Now, the way we will integrate the JIT compiler into our program is to point it to the gradient of our loss function. Remember that we can reap the benefits of JIT compilation by applying it to a piece of code that we expect to execute many times, which we have mentioned is exactly the case for the loss gradient. Recall that we were able to derive this gradient in the last section using the <strong class="source-inline">grad</strong> function transformation:</p>
			<p class="source-code">loss_grad = grad(loss)</p>
			<p>To obtain the JIT-compiled version of this function, we can follow a very similar process, in which we transform this loss gradient function using <strong class="source-inline">jit</strong>:</p>
			<p class="source-code">from jax import jit</p>
			<p class="source-code">loss_grad = jit(grad(loss))</p>
			<p>Amazingly, that is all we need to do. Now, if we run the same training loop we currently have, with this one line changed, we will obtain the same result, including the learning curve and the decision boundary we plotted earlier.</p>
			<p>However, upon inspecting the running time of the training loop, we will notice that the training loop is now much more efficient: within our Google Colab notebook, the speed goes from roughly 77 iterations to 175 iterations per second! Note that although these numbers might vary across different runs of the code, the improvement should stay apparent. While this speedup might not be meaningful <a id="_idIndexMarker459"/>to us and our simple linear model, it will prove useful for more heavyweight models such as neural networks, with millions to billions of parameters, which take weeks and months to train.</p>
			<p>Furthermore, this speedup was achieved with a single line of code changed using the function transformation, <strong class="source-inline">jit</strong>. This means that however complicated the functions you are working with are, if they are JAX-compatible, you can simply pass them through <strong class="source-inline">jit</strong> to enjoy the benefits of JIT compilation.</p>
			<p>At this point, we have explored two powerful techniques to accelerate our machine learning pipeline: automatic differentiation and JIT compilation. In the next section, we will look at a final valuable feature of JAX – automatic vectorization.</p>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor114"/>Automatic vectorization for efficient kernels</h1>
			<p>You might remember from our discussions on NumPy that the library is efficient at applying <a id="_idIndexMarker460"/>numerical operations to all elements in an array or the elements along specific axes. By exploiting the fact that the same operation is to be applied to multiple elements, the library optimizes low-level code that performs the operation, making the computation much more efficient than doing the same thing via an iterative loop. This <a id="_idIndexMarker461"/>process is called <strong class="bold">vectorization</strong>.</p>
			<p>When working with machine learning models, we would like to go through a procedure of vectorizing a specific function, rather than looping through an array or a matrix, to gain performance speedup. Vectorization is typically not easy to do and might involve clever tricks to rewrite the function that we'd like to vectorize into another form that admits vectorization easily.</p>
			<p>JAX addresses this concern by providing a function transformation that automatically vectorizes a given function, even if the function is only designed to take in <a id="_idIndexMarker462"/>single-valued variables (which means traditionally, an iterative loop would be necessary). In this section, we will see how to use this feature by going through the process of kernelizing our predictive model. First, we need to briefly discuss what kernels are as well as their place in machine learning.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor115"/>Data that is not linearly separable</h2>
			<p>Remember that the predictive model we currently have assumes that the target we'd like <a id="_idIndexMarker463"/>to predict for can be expressed as a linear combination of the data features; the model is thus a linear one. This assumption is quite restrictive in practice, as data can present highly nonlinear and still meaningful patterns.</p>
			<p>For example, here we use the <strong class="source-inline">make_moons</strong> function from the same <strong class="source-inline">datasets</strong> module of scikit-learn to generate another toy dataset and visualize it, again using a scatter plot:</p>
			<p class="source-code">from sklearn.datasets import make_moons</p>
			<p class="source-code">X, y = make_moons(n_samples=200, noise=0.1, random_state=0)</p>
			<p class="source-code">X = np.hstack((X, np.ones_like(y).reshape(-1, 1)))</p>
			<p class="source-code">y[y == 0] = -1</p>
			<p class="source-code">plt.scatter(X[:, 0], X[:, 1], c=y);</p>
			<p>This will generate the following plot:</p>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/Figure_6.5_17499.jpg" alt="Figure 6.5 – A linearly non-separable dataset " width="617" height="252"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5 – A linearly non-separable dataset</p>
			<p>Once again, this is a binary classification problem where we need to distinguish between <a id="_idIndexMarker464"/>the yellow and the dark blue points. As you can see, these data points are not linearly separable since, unlike our previous dataset, a straight line that perfectly separates the two classes here does not exist. In fact, if we were to try to fit a linear model to this data, we would obtain a model with a relatively high loss (roughly 0.7) and a decision boundary that is clearly unsuitable, as shown here:</p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/Figure_6.6_17499.jpg" alt="Figure 6.6 – Fitting a linear model on nonlinear data " width="634" height="248"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6 – Fitting a linear model on nonlinear data</p>
			<p>How, then, could <a id="_idIndexMarker465"/>we modify our current model so that it can handle nonlinear data? A typical solution in these kinds of situations is the kernel method in machine learning, which we will briefly discuss next.</p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor116"/>The kernel method in machine learning</h2>
			<p>Without going into much technical detail, the kernel method (sometimes referred <a id="_idIndexMarker466"/>to as the <strong class="bold">kernel trick</strong>) refers <a id="_idIndexMarker467"/>to the process of transforming a low-dimensional dataset, such as ours, that is nonlinear to higher dimensions, with the hope that in higher dimensions, linear hyperplanes that separate the data will exist.</p>
			<p>To take a data point to higher dimensions, additional features are created from the features already included in the original data. In our running example, we are given the <em class="italic">x</em> and <em class="italic">y</em> coordinates of the data points; hence, our data is two-dimensional. A common way to compose more features in this case is to compute polynomials of these coordinates up to a degree, <em class="italic">d</em>. For instance, polynomials that are up to degree 2 for features <em class="italic">x</em> and <em class="italic">y</em> include <em class="italic">xy</em> and <img src="image/Formula_6.42_B17499.png" alt="" width="40" height="35"/>. The larger <em class="italic">d</em> is, the more expressiveness we will gain from these new features we are creating, but we will also incur a higher computational cost.</p>
			<p>Notice that these polynomial features are nonlinear, which motivates their use in helping us find a model that separates our nonlinear data. With these new, nonlinear features in hand, we will then fit our predictive model on this bigger, more expressive dataset we just engineered. Note that while the predictive model <a id="_idIndexMarker468"/>remains in the same form as the linear parameters <img src="image/Formula_6.43_B17499.png" alt="" width="39" height="29"/>, the data features that the model learns from are nonlinear, so the resulting decision boundary will also be nonlinear.</p>
			<p>I mentioned earlier that the more expressive we want the features we are synthetically creating to be, the more computational costs we will incur. Here, kernels are a way to manipulate the data points so that we can interact with them in higher dimensions without having to explicitly compute the high-dimensional features. In essence, a kernel is simply a function that takes in a pair of data points and returns the inner product of some transformation of the feature vectors. This is just to say that it returns a matrix that represents the two data points in higher dimensions.</p>
			<p>In this chapter, we will use one of the most popular kernels in machine learning: the radial basis function, or RBF, kernel, which is defined as follows:</p>
			<p class="figure-caption"><img src="image/Formula_6.44_B17499.png" alt="" width="485" height="65"/></p>
			<p>where <em class="italic">x</em> and <em class="italic">x'</em> are the low-dimensional feature vectors of the two data points, and <em class="italic">l</em> is commonly referred to as the length scale, a tunable parameter of the kernel. There are many benefits to using the RBF kernel that we won't be going into in much detail here, but on the highest level, RBF can be regarded as an infinite-dimensional kernel, which means that it will provide us with a high level of expressiveness in the features we are computing.</p>
			<p>The new model using this kernel method, modified from our previous linear model, now makes the following assumption for each pair of feature vector <em class="italic">x</em> and label <em class="italic">y</em>:</p>
			<p class="figure-caption"><img src="image/Formula_6.45_B17499.png" alt="" width="1021" height="54"/></p>
			<p>where <img src="image/Formula_6.46_B17499.png" alt="" width="36" height="34"/> is the <em class="italic">i</em>-th data point in the training set and <img src="image/Formula_6.47_B17499.png" alt="" width="31" height="27"/> are the model parameters <a id="_idIndexMarker469"/>to be optimized. While this assumption is written to contrast the linear assumption that we had earlier:</p>
			<p class="figure-caption"><img src="image/Formula_6.48_B17499.png" alt="" width="478" height="46"/></p>
			<p><img src="image/Formula_6.49_B17499.png" alt="" width="33" height="30"/> now refers to the entire feature vector of a data point, not the <em class="italic">i</em>-th feature of a data point. Further, we now have <em class="italic">n</em> terms of <img src="image/Formula_6.50_B17499.png" alt="" width="32" height="28"/>, as opposed to 2 (or <em class="italic">d</em>) terms of <img src="image/Formula_6.51_B17499.png" alt="" width="38" height="28"/> as before.</p>
			<p>We see the role of the kernel here is to transform the feature of <em class="italic">x</em> by computing the inner product of its features and those of each of the data points in the training set. The output of the kernel is then used as the new features to be multiplied by the model parameters <img src="image/Formula_6.52_B17499.png" alt="" width="35" height="30"/>, whose values will be optimized as part of the training process.</p>
			<p>Let's <a id="_idIndexMarker470"/>now implement this kernel in our code:</p>
			<p class="source-code">lengthscale = 0.3</p>
			<p class="source-code">def rbf_kernel(x, z):</p>
			<p class="source-code">    return jnp.exp(- jnp.linalg.norm(x - z) ** 2  \</p>
			<p class="source-code">      / lengthscale)</p>
			<p>Here we are setting the length scale at 0.3, but feel free to play around with this value and observe its effect on the learned model afterward. The <strong class="source-inline">rbf_kernel</strong> function takes in <strong class="source-inline">x</strong> and <strong class="source-inline">z</strong>, two given data points. The code inside the function is self-explanatory: we compute the squared norm of the difference between <strong class="source-inline">x</strong> and <strong class="source-inline">z</strong> (<strong class="source-inline">linalg.norm</strong> in JAX follows the same API as in NumPy), divide it by <strong class="source-inline">lengthscale</strong>, and use its negative as the input of the natural exponential function, <strong class="source-inline">exp</strong>.</p>
			<p>Now, we need to implement the new version of the <strong class="source-inline">predict</strong> function for our kernelized model, which we will see how to do in the next subsection.</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor117"/>Automatic vectorization for kernelized models</h2>
			<p>You <a id="_idIndexMarker471"/>might remember that the job of the <strong class="source-inline">prediction</strong> function is to implement the assumption our model is making, which in this case is:</p>
			<p class="figure-caption"><img src="image/Formula_6.53_B17499.png" alt="" width="1041" height="57"/></p>
			<p>The challenge here is in the sum <img src="image/Formula_6.54_B17499.png" alt="" width="709" height="47"/>, where <img src="image/Formula_6.55_B17499.png" alt="" width="38" height="31"/>, <img src="image/Formula_6.56_B17499.png" alt="" width="38" height="30"/>, …, <img src="image/Formula_6.57_B17499.png" alt="" width="42" height="30"/> are the individual data points in our training set. In a naïve implementation, we would iterate through each data point <img src="image/Formula_6.58_B17499.png" alt="" width="34" height="31"/>, compute <img src="image/Formula_6.59_B17499.png" alt="" width="128" height="38"/>, multiply it by <img src="image/Formula_6.60_B17499.png" alt="" width="32" height="29"/>, and finally, add these terms together. Computing the inner product <a id="_idIndexMarker472"/>of two vectors is generally a computationally expensive operation, so repeating this computation <em class="italic">n</em> times would be prohibitively costly.</p>
			<p>To make this kernelized model more practical, we'd like to <em class="italic">vectorize</em> this procedure. Normally, this would require reimplementing our kernel function using various tricks to enable vectorization. However, with JAX, we can effortlessly obtain the vectorized version of the <strong class="source-inline">rbf_kernel</strong> function using the <strong class="source-inline">vmap</strong> transformation:</p>
			<p class="source-code">from jax import vmap</p>
			<p class="source-code">kernel = rbf_kernel</p>
			<p class="source-code">vec_kernel = jit(vmap(vmap(kernel, (0, None)), (None, 0)))</p>
			<p>Don't let the last line of code intimidate you. Here we are simply getting the vectorized form of our kernel function along the first axis (remember the kernel has two inputs) with <strong class="source-inline">vmap(kernel, (0, None)</strong>, and then vectorizing that very vectorized-along-the-first-axis kernel along the second axis with <strong class="source-inline">vmap(vmap(kernel, (0, None)), (None, 0))</strong>. Finally, we derive the JIT-compiled version of the function with <strong class="source-inline">jit</strong>.</p>
			<p>Aside from being a concise one-liner, this code perfectly illustrates why function transformation (JAX's design choice) is so useful: we can compose different function transformations in a nested way, repeatedly calling a transformation on the output of another transformation; in this case, we have the JIT-compiled function of a twice-vectorized function.</p>
			<p>With <a id="_idIndexMarker473"/>this vectorized kernel function composed, we are now ready to implement the corresponding <strong class="source-inline">predict</strong> and <strong class="source-inline">loss</strong> functions:</p>
			<p class="source-code">def predict(alphas, X_test):</p>
			<p class="source-code">    return jnp.dot(vec_kernel(X, X_test), alphas)</p>
			<p class="source-code">def loss(alphas):</p>
			<p class="source-code">    preds = predict(alphas, X)</p>
			<p class="source-code">    return jnp.mean(jnp.clip(1 - jnp.multiply(y, preds), \</p>
			<p class="source-code">      a_min=0))</p>
			<p>Of note here is the <strong class="source-inline">predict</strong> function, where we are calling <strong class="source-inline">vec_kernel(X, X_test)</strong> to compute the vector of <img src="image/Formula_6.61_B17499.png" alt="" width="607" height="48"/> via vectorization. Remember that without <strong class="source-inline">vmap</strong>, we would need to either iterate through the individual data points (for example, using a <strong class="source-inline">for</strong> loop), which is specifically more inefficient, or rewrite our <strong class="source-inline">rbf_kernel</strong> function so that the function itself facilitates the vectorization. In the end, we simply compute the dot product between the returned output and the vector of the parameters we'd like to optimize, <strong class="source-inline">alphas</strong>.</p>
			<p>Our loss function stays the same. We now can apply the same training procedure as <a id="_idIndexMarker474"/>we have before, this time on the <strong class="source-inline">alphas</strong> variable:</p>
			<p class="source-code">np.random.seed(0)</p>
			<p class="source-code">alphas = np.random.randn(y.size)</p>
			<p class="source-code">for i in range(n_iters):</p>
			<p class="source-code">    grads = loss_grad(alphas)</p>
			<p class="source-code">    alphas = alphas - lr * grads</p>
			<p class="source-code">    tmp_loss = loss(alphas)</p>
			<p class="source-code">    if tmp_loss == 0:</p>
			<p class="source-code">        break</p>
			<p>We can also plot out the progression of the loss throughout this training:</p>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<img src="image/Figure_6.7_17499.jpg" alt="Figure 6.7 – The decrease in loss of the kernelized model " width="630" height="248"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.7 – The decrease in loss of the kernelized model</p>
			<p>We see a steady decrease in loss, indicating that the model is fitting its parameters to the data. In the end, our loss is well below <strong class="source-inline">0.7</strong>, which was the loss of <a id="_idIndexMarker475"/>the purely linear model we previously had.</p>
			<p>Finally, we'd like to visualize what our model has learned. This is a bit more involved as we no longer have a nice linear equation that we can translate into a line as before. Instead, we visualize the predictions made by the model itself. In the following figure, we show these predictions made on a fine grid that spans across our training data points, where the colors show the predicted classes, and the hues show the confidence in the predictions:</p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="image/Figure_6.8_17499.jpg" alt="Figure 6.8 – Predictions made by the learned kernelized model " width="724" height="223"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.8 – Predictions made by the learned kernelized model</p>
			<p>We see that the model was able to identify the nonlinear pattern in our data, indicated <a id="_idIndexMarker476"/>by the yellow and dark blue blobs lying right in the center of the moons. As such, we have successfully modified our predictive model to learn from nonlinear data, which was done by using a kernel to create nonlinear features and JAX's automatic vectorization transformation to accelerate this computation.</p>
			<p>Here, the speed we achieve is roughly 160 iterations per second. To once again see how big a speedup the JIT compiler offers our program, we remove the <strong class="source-inline">jit()</strong> function calls from the implementation of our model and rerun the code with the following changes:</p>
			<p class="source-code">kernel = rbf_kernel</p>
			<p class="source-code">vec_kernel = vmap(vmap(kernel, (0, <strong class="bold">None</strong>)), (<strong class="bold">None</strong>, 0))</p>
			<p class="source-code">loss_grad = grad(loss)</p>
			<p>This time, the speedup is even more impressive: without JIT, the speed drops to 47 iterations per second.</p>
			<p>This concludes our discussion on JAX and some of its main features. However, there are <a id="_idIndexMarker477"/>other more advanced tools included in JAX that we didn't cover but could prove useful in your machine learning applications such as asynchronous dispatch, parallelization, or computing convolutions. Information on these features may be found on the documentation page: <a href="https://jax.readthedocs.io/en/latest/">https://jax.readthedocs.io/en/latest/</a>.</p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor118"/>Summary</h1>
			<p>JAX is a Python- and NumPy-friendly library that offers high-performance tools that are specific to machine learning tasks. JAX centers its API around function transformations, allowing users, in one line of code, to pass in generic Python functions and receive transformed versions of the functions that would otherwise either be expensive to compute or require more advanced implementations. The syntax of function transformations also enables flexible and complex compositions of functions, which are common in machine learning.</p>
			<p>Throughout this chapter, we have seen how to utilize JAX to compute the gradient of machine learning loss functions using automatic differentiation, JIT-compile our code for further optimization, and vectorize kernel functions via a binary classification example. However, these tasks are present in most use cases, and you will be able to seamlessly apply what we have discussed here to your own machine learning needs.</p>
			<p>At this point, we have reached the end of the first part of this book, in which we discuss Python-native and various other techniques to accelerate our Python applications. In the second part of the book, we examine parallel and concurrent programming, the techniques of which allow us to distribute computational loads across multiple threads and processes, making our applications more efficient on a linear scale.</p>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor119"/>Questions</h1>
			<ol>
				<li>What are the main components of a machine learning pipeline?</li>
				<li>How is the loss function of a machine learning model typically minimized and how does JAX help with this process?</li>
				<li>How can the predictive model used in this chapter handle nonlinear data and how does JAX help with this process?</li>
			</ol>
			<h1 id="_idParaDest-129"><a id="_idTextAnchor120"/>Further reading</h1>
			<ul>
				<li>Linear models in machine learning: <a href="https://blog.dataiku.com/top-machine-learning-algorithms-how-they-work-in-plain-english-1">https://blog.dataiku.com/top-machine-learning-algorithms-how-they-work-in-plain-english-1</a></li>
				<li>The JAX ecosystem: <a href="https://moocaholic.medium.com/jax-a13e83f49897">https://moocaholic.medium.com/jax-a13e83f49897</a></li>
				<li>A brief tutorial that further explores JAX: <a href="https://colinraffel.com/blog/you-don-t-know-jax.html">https://colinraffel.com/blog/you-don-t-know-jax.html</a></li>
			</ul>
		</div>
	</div>
</div>
</body></html>