- en: Chapter 2. The Profilers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we covered the basics of profiling and understood its
    importance. You learned how it will help the development process if we incorporate
    the practice of profiling into the cycle of development. We also went over some
    good profiling practices.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we covered some theory about the different execution times our program
    can have. In this chapter, we'll use the first part (the part about profiling).
    Then, with the help of two specific Python profilers (`cProfile` and `line_profilers`),
    we'll start putting into practice some theory that you have learned.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Some basic information about each profiler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to download and install each profiler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use cases examples with different options
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differences and similarities between both profilers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Getting to know our new best friends: the profilers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After all the theory and generic examples from the previous chapter, it is
    time for some real Python. So, let''s begin with two of the most known and used
    Python profilers: `cProfile` and `line_profiler`. They will help us profile our
    code in two different ways.'
  prefs: []
  type: TYPE_NORMAL
- en: On one hand, we have `cProfile` ([https://docs.python.org/2/library/profile.html#module-cProfile](https://docs.python.org/2/library/profile.html#module-cProfile)),
    It comes by default with Python since version 2.5 and is the recommended profiler
    for most use cases. At least that is what the official Python documentation says
    about it. On the other hand, we have `line_profiler` ([https://github.com/rkern/line_profiler](https://github.com/rkern/line_profiler)),
    which is not an official part of the Python programming language, but it's a well-known
    profiler out there.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go over both of them in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: cProfile
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like I've already mentioned, `cProfile` comes by default with the standard Python
    interpreter (`cPython`) since version 2.5\. Other versions, such as PyPy, don't
    have it. It is a deterministic profiler. It provides a set of APIs that allow
    the developers to gather information about the execution of Python programs, more
    specifically, about the CPU time used by each function. It also provides other
    details, such as the number of times a function was called.
  prefs: []
  type: TYPE_NORMAL
- en: It exclusively measures CPU time and pays no attention to memory consumption
    and other memory related stats. Nonetheless, it is a great starter point, since
    most of the times, if we're trying to optimize code, this type of analysis will
    provide an immediate set of optimization candidates.
  prefs: []
  type: TYPE_NORMAL
- en: There is no need for installation, since it's part of the language already.
    To use it, all you have to do is to import the `cProfile` package.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A deterministic profiler is just another name for an event-based profiler (check
    out the previous chapter for more details). This means that that this profiler
    will be aware of every function call, return statement, and other events during
    the execution of our code. It will also measure everything that happens during
    that time (unlike the statistical profiler we saw in the previous chapter).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a very simple example taken from Python''s documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code outputs the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'From this output, the following information can be gathered:'
  prefs: []
  type: TYPE_NORMAL
- en: The first line tells us that 197 function calls were monitored, and out of them,
    192 were primitive calls, which means no recursion was involved.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ncalls` reports the number of calls to the function. If there are two numbers
    in this column, it means there was recursion. The second one is the number of
    primitive calls, and the first one is the total number of calls. This number can
    be helpful to identify the possible bugs (unexpected high numbers) or possible
    inline expansion points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tottime` is the total time spent inside the function (excluding the time spent
    doing subcalls to other functions). This particular information can help the developer
    find long running loops that could be optimized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`percall` is simply the quotient of `tottime` divided by `ncalls`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cumtime` is the cumulative time spent inside the function including the time
    spent in subfunctions (this includes recursive calls as well). This number could
    help identify higher level errors, such as those in the selection of the algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`percall` is the quotient of `cumtime` divided by primitive calls.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filename:lineno(function)` provides the file name, line number, and function
    name of the analyzed function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A note about limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is no such thing as the invisible profiler. This means that even in the
    case of `cProfile`, which has a very small overhead, it still adds an overhead
    to our code. On every event that is triggered, there is some lag between the time
    that the event actually happens and that time that the profiler gets to query
    the state of the internal clock. At the same time, there is some lag between the
    moment the program counter leaves the profiler's code and goes back into the user's
    code to continue with the execution.
  prefs: []
  type: TYPE_NORMAL
- en: Adding to the fact, that as any piece of data inside a computer, the internal
    clock has a set precision, and any measurement that is smaller than that precision
    will be lost. That being said, the developer needs to have a special consideration
    when profiling code with a high number of recursive calls or, in particular cases,
    when a function calls many other functions, since that error can accumulate and
    begin to be significant.
  prefs: []
  type: TYPE_NORMAL
- en: The API provided
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `cProfile` profiler provides a set of methods that will help the developer
    gather statistics in different contexts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This classic method used in the preceding example gathers statistics about
    the execution of the command. After that, it calls the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If no file name is given, it''ll create a new instance of `stats` (more on
    this class in a minute). Here is the preceding same example, but using the extra
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run the preceding code, you''ll notice that nothing gets printed out.
    However, if you inspect the content of the folder, you''ll notice a new file,
    called `stats`. If you try to open that file, you won''t be able to understand
    its meaning because it was saved using a binary format. In a few minutes, we''ll
    see how to read that information and manipulate it to create our own reports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This method is very similar to the preceding one. The only difference is that
    it also receives the `globals` and `locals` dictionaries for the command-line
    string. After that, it executes the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: It gathers profiling statistics just like `run` does. Let's see an example of
    the main difference between `run` and `runctx`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s stick to `run` and write the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'What we would actually get when running the code is the following error message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `re` module is not found by the `run` method because as we saw earlier that
    `run` calls the `exec` function with the `__main__.__dict__` as parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s use `runctx` in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the output would change into a valid one as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `Profile(timer=None, timeunit=0.0, subcalls=True, builtins=True)` method
    returns a class, providing more control to the developer during the profiling
    process than `run` and `runctx` do.
  prefs: []
  type: TYPE_NORMAL
- en: The `timer` parameter is a custom function that can be used to measure time
    in a different way than the one provided by default. It must be a function returning
    a number representing the current time. If the developer needs a custom function,
    it should be as fast as possible to lower overhead and avoid problems of calibration
    (please refer to *A note about limitations* section a few pages back).
  prefs: []
  type: TYPE_NORMAL
- en: If the number returned by the timer is an integer, the `timeunit` parameter
    specifies the multiplier that represents the duration of each unit of time. For
    example, if the returned value is in milliseconds, then `timeunit` would be `.001`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s also take a look at the methods provided by the returned class:'
  prefs: []
  type: TYPE_NORMAL
- en: '`enable()`: This starts collecting profiling data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`disable()`: This stops collecting profiling data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`create_stats()`: This stops collecting data and records the information gathered
    as the current profile'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`print_stats(sort=-1)`: This creates a `stats` object and prints the result
    into `STDOUT`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dump_stats(filename)`: This writes the content of the current profile into
    a file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`run(cmd)`: This is same as the `run` function we saw earlier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`runctx(cmd, globals, locals)`: This is same as the `runctx` function we saw
    earlier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`runcall(func, *args, **kwargs)`: This gathers profiling information about
    the function called'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s see the preceding example, using the following method this time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: There are more lines involved to get the profiling going, but it is clearly
    less invasive to the original code. That is an advantage when trying to profile
    code that's already been written and tested. This way, we can add and remove our
    profiling code without having to modify the original code.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an even less invasive alternative, which involves not adding code
    at all, but using some specific command-line parameters when running the script
    instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note that this will profile the entire code, so if you were actually just profiling
    a specific portion of your script, the preceding approach would not return the
    same results.
  prefs: []
  type: TYPE_NORMAL
- en: Now, before going into more detailed and interesting examples, let's first look
    at the `Stats` class and understand what it can do for us.
  prefs: []
  type: TYPE_NORMAL
- en: The Stats class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `pstats` module provides the developer with the `Stats` class, which, in
    turn, allows them to read and manipulate the content of the `stats` file (the
    file into which we saved the profiling information using one of the methods described
    earlier).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following code loads the `stats` file and prints out the sorted
    statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that the `Stats` class constructor is able to receive a `cProfile.Profile`
    instance instead of the file name as the source of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a closer look at the methods provided by the `pstats.Stats` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '`strip_dirs()`: This removes all the leading paths'' information from the file
    names in the report. This method modifies the `stats` instance, so any instance
    that has this method executed will be considered to have its items in a random
    order. If two entries are considered to be the same (same line on the same file
    name having the same function name), then those entries would be accumulated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add(*filenames)`: This method loads more information into `stats` from the
    files referenced in the file names. It''s worth mentioning that just like with
    only one file, the `stats` entries that reference the same function (file name,
    and line and function name) will be accumulated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dump_stats(filename)`: Just like in the `cProfile.Profile` class, this method
    saves the data loaded into the `Stats` class inside a file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sort_stats(*keys)`: This method is present since version 2.3, and it modifies
    the `stats` object by sorting its entries by the given criteria. When more than
    one criteria is given, then the additional ones are used only when there is equality
    in the previous ones. For instance, if `sort_stats (''name'', ''file'')` is used,
    it would sort all entries by function name, and when that name is the same, it
    would sort those entries by file name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The method is smart enough to understand abbreviations as long as they''re
    unambiguous, so be careful there. The full list of the currently supported sorting
    criteria is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Criteria | Meaning | Ascending/Descending |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `calls` | Total number of calls | Descending |'
  prefs: []
  type: TYPE_TB
- en: '| `cumulative` | Cumulative time | Descending |'
  prefs: []
  type: TYPE_TB
- en: '| `cumtime` | Cumulative time | Descending |'
  prefs: []
  type: TYPE_TB
- en: '| `file` | File name | Ascending |'
  prefs: []
  type: TYPE_TB
- en: '| `filename` | File name | Ascending |'
  prefs: []
  type: TYPE_TB
- en: '| `module` | File name | Ascending |'
  prefs: []
  type: TYPE_TB
- en: '| `ncalls` | Total number of calls | Descending |'
  prefs: []
  type: TYPE_TB
- en: '| `pcalls` | Primitive call count | Descending |'
  prefs: []
  type: TYPE_TB
- en: '| `line` | Line number | Ascending |'
  prefs: []
  type: TYPE_TB
- en: '| `name` | Function name | Ascending |'
  prefs: []
  type: TYPE_TB
- en: '| `nfl` | Composite of name/file/line | Descending |'
  prefs: []
  type: TYPE_TB
- en: '| `stdname` | Standard name | Ascending |'
  prefs: []
  type: TYPE_TB
- en: '| `time` | Internal time | Descending |'
  prefs: []
  type: TYPE_TB
- en: '| `tottime` | Internal time | Descending |'
  prefs: []
  type: TYPE_TB
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**A note on nfl versus stdname**'
  prefs: []
  type: TYPE_NORMAL
- en: The main difference between these two sort types is that the latter is a sort
    of the printed name. This means the line numbers will be sorted as strings (which
    means that for 4, 20, and 30 the sorting will be 20, 30, 4). The `nfl` sort does
    a numeric comparison of the line number fields.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, for backward compatibility reasons, some numeric values are accepted,
    instead of the ones in the preceding table. They are `-1`, `0`, `1`, and `2`,
    and they're translated into `stdname`, `calls`, `time`, and `cumulative`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '`reverse_order()`: This method reverses the default order of the sort key selected
    (so, if the key is by the default ascending order, it would be in the descending
    order now).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`print_stats(*restrictions)`: This method takes care of printing out the stats
    into `STDOUT`. The optional argument is meant to restrict the output of this function.
    It can either be an integer value, a decimal value, or a string. They are explained
    here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`integer`: This will limit the number of lines printed'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Decimal between 0.0 and 1.0 (inclusive)`: This will select the percentage
    of the lines'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`String`: This is a regular expression to match against the standard name'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![The Stats class](img/B02088_02_01.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'The preceding screenshot shows the output we get from calling the `print_stats`
    method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: If more than one parameter is passed, then they are applied sequentially. As
    we've seen in the preceding lines of code, the output of this profiler can be
    quite long. However, if we sort it properly, then we can summarize that output
    using this parameter and still get relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: The `print_callers(*restrictions)` function works with the same input and restriction
    rules than the previous one, but the output is a bit different. For every function
    called during the execution of our program, it'll show the number of times each
    call was made, the total and cumulative times, and a combination of filename,
    and the line and function names.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a quick example of how using `cProfile.Profile` and `Stats`
    can render the list of caller functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice how we''re combining the `pstats.Stats` class with the `cProfile.Profile`
    class. They''re working together to gather and show the information in the way
    we need it. Now, look at the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Stats class](img/B02088_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The `print_callees(*restrictions)` method prints a list of functions that call
    other functions. The format of the data shown and the restrictions are same as
    the preceding example.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may encounter a block like the one shown in the following screenshot as
    part of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Stats class](img/B02088_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This output means that the functions on the right-hand side were called by the
    same function on the left-hand side.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we've seen the basics of how to use `cProfile` and `Stats`, let's dig
    into some more interesting and practical examples.
  prefs: []
  type: TYPE_NORMAL
- en: Fibonacci again
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's go back to the Fibonacci example, since a basic recursive Fibonacci sequence
    calculator has a lot of room for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first look at the unprofiled, unoptimized code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This code will output the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fibonacci again](img/B02088_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The output is printed correctly, but look at the highlighted sections in the
    preceding screenshot. These sections are explained here:'
  prefs: []
  type: TYPE_NORMAL
- en: There are 57.356 function calls during those 0.114 seconds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Out of those, only 66 were primitive calls (not called by recursion)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line 3 of our code, 57.270 (57.291—21) were recursion-induced function calls
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we all know, the act of calling another function adds an overhead to our
    time. Since it looks like (for the `cumtime` column) that most of the execution
    time is spent inside this function, we can safely assume that if we speed this
    up, the entire script's time would be affected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s apply a simple decorator to the `fib` function that will allow
    us to cache the previously calculated values (a technique also known as memoization,
    about which you''ll read in the upcoming chapters) so that we don''t have to call
    fib more than once per value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s run the code again and look at the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fibonacci again](img/B02088_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We went from around 57k total calls to only 145 and from 0.114 seconds to 0.001\.
    That's an amazing improvement! However, we have more primitive calls, but we also
    have significantly less recursive calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s continue with another possible optimization. Our example works quite
    fast for a single call, but let''s try to do several runs in a row and get the
    combined stats for that execution. Perhaps, we''ll get something interesting back.
    To do this, we need to use the stats module. Let''s see an example for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve pushed the envelope here. Getting the Fibonacci sequence for 1000 might
    be too much to ask, especially from a recursive implementation. Indeed, we ran
    out of recursion depth. This is mainly due to the fact that `cPython` has a guard
    to prevent a stack overflow error generated by the amount of recursive calls (ideally,
    a tail recursion optimization would solve this, but `cPython` does not provide
    it). So, we just found another issue. Let''s try to fix it and reanalyze the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The preceding lines of code print a huge list of really big numbers, but these
    lines prove that we made it. We can now compute the Fibonacci sequence for the
    number 1000\. Now, let's analyze it and see what we find.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the new profiling code, but requiring the iterative version of the Fibonacci
    implementation, we will get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This, in turn, will yield the following result into the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fibonacci again](img/B02088_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Our new code is taking 0.187 seconds to calculate the Fibonacci sequence of
    1000 five times. It's not a bad number, but we know we can improve it by caching
    the results, just like we did earlier. *As you can see, we have 5005 calls to
    the* `fib` *function. If we cache it, we would have a lot less function calls,
    which would mean less execution time*.
  prefs: []
  type: TYPE_NORMAL
- en: 'With very little effort, we can improve that time by caching the calls to the
    `fib` function, which, according the preceding report, is called 5005 times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get something like the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fibonacci again](img/B02088_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Simply by caching the call to `fib`, we went from 0.187 seconds to 0.006 seconds.
    This is an amazing improvement. Well done!
  prefs: []
  type: TYPE_NORMAL
- en: Tweet stats
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's look at another example, something a bit more conceptually complex, since
    calculating the Fibonacci sequence is not really an everyday use case. Let's do
    something a bit more interesting. These days, Twitter allows you to download your
    complete list of tweets in the form of a CSV file. We'll use this file to generate
    some statistics from our feed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the data provided, we''ll calculate the following statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: The percentage of messages that are actual replies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The percentage of tweets that were made from the website ([https://twitter.com](https://twitter.com))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The percentage of tweets that were made from a mobile phone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output form our script will look like the one shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tweet stats](img/B02088_02_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To keep things simple, we'll take care of parsing the CSV file and doing these
    basic calculations. We won't use any third-party modules; that way, we'll be in
    total control of the code and its analysis. This means leaving out obvious things,
    such as using the CSV module from Python.
  prefs: []
  type: TYPE_NORMAL
- en: Other bad practices shown earlier, such as the `inc_stat` function or the fact
    that we're loading the entire file into memory before processing it, will remind
    you that this is just an example to show basic improvements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the initial code of the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: To be fair, the code doesn't do anything too complicated. It loads the content
    of the file, splits it into lines, and then it splits each line into different
    fields. Finally, it counts things. One might think that with this explanation,
    there is nothing much to optimize, but we're about to see that there is always
    room for some optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Another important thing to note is that the CSV file we'll be processing has
    almost 150 MB of tweets data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the script that imports that code, uses it, and generates a profiling
    report:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output we get from this execution is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tweet stats](img/B02088_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'There are three main areas of interest in the preceding screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: Total execution time
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cumulative times of individual function calls
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Total number of calls for individual functions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Our aim is to lower the total execution time. For that, we will pay special
    attention to the cumulative times of individual functions and the total number
    of calls for individual functions. We can infer the following conclusions for
    the last two points:'
  prefs: []
  type: TYPE_NORMAL
- en: The `build_twit_stats` function is the one that takes the most time. However,
    as you can see in the preceding lines of code, it just calls all other functions,
    so it makes sense. We can focus on `read_data` since it's the second most time-consuming
    function. This is interesting, because it means that our bottleneck is not when
    we calculate the stats, but when we load the data for it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the third line of the code, we also see exactly our bottleneck inside `read_data`.
    We perform too many `split` commands and they add up.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also see that the fourth most time-consuming function is `get_stats`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, let's tackle these issues and see if we get better results. The biggest
    bottleneck we had was the way we were loading data. We were loading it all into
    memory first and then iterating over it to calculate our stats. We can improve
    this by reading the file line by line and calculating the stats after each one.
    Let's see how that code would look.
  prefs: []
  type: TYPE_NORMAL
- en: 'The new `read_data` method looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We had to add some logic to take into account multiline tweets, which are also
    saved as multiline records on our CSV file. We changed our `get_stats` function
    into `get_line_stats`, which simplifies its logic since it only calculates the
    values for the current record:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The two final improvements were to remove the calls to `inc_stat`, since, thanks
    to the dictionary we're using, the call is unnecessary. We also replaced the usage
    of the find method using the more proficient `in` operator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run the code again and see the changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tweet stats](img/B02088_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We went from 2 seconds to 1.6; that was a considerable improvement. The `read_data`
    function is still up there with the most time-consuming functions, but that's
    just because it now also calls the `get_line_stats` function. We can also improve
    on this, since even though the `get_line_stats` function does very little, we're
    incurring in a lookup time by calling it so often inside the loop. We could inline
    this function and see if that helps.
  prefs: []
  type: TYPE_NORMAL
- en: 'The new code would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, with the new changes, the report will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tweet stats](img/B02088_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There is a notable improvement between the first screenshot and the preceding
    one. We got the time down to barely above 1.4 seconds from 2 seconds. The number
    of function calls is considerably lower as well (it went from around 3 million
    calls to 1.7 million), which in turn should help lower the time spent doing lookups
    and calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an added bonus, we will improve the readability of our code by simplifying
    it. Here is the final code all together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This is it for our review of `cProfile`. With it, we managed to profile our
    scripts, getting per-function numbers and total function calls. It helped us improve
    on the overall view of the system. We'll now look at a different profiler, which
    will give us per-line details that `cProfile` is not capable of providing.
  prefs: []
  type: TYPE_NORMAL
- en: line_profiler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This profiler is different from `cProfile`. It helps you profile a function
    line by line instead of doing a deterministic profiling, like the other one does.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install this profiler, you can use the pip ([https://pypi.python.org/pypi/pip](https://pypi.python.org/pypi/pip))
    command-line tool, with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you run into any trouble, such as missing files during installation, make
    sure you have all development dependencies installed. In the case of Ubuntu, you
    can ensure that all the dependencies are installed by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`$ sudo apt-get install python-dev libxml2-dev libxslt-dev`'
  prefs: []
  type: TYPE_NORMAL
- en: This profiler is trying to fill in a breach left by `cProfile` and others like
    it. Other profilers cover CPU time on function calls. Most of the time, this is
    more than enough to catch the problems and fix them (we saw that earlier). However,
    sometimes, the problem or bottleneck is related to one specific line inside the
    function and that is where `line_profiler` comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: The author recommends us to use the `kernprof` utility, so we'll look at examples
    of it. Kernprof will create an instance of the profiler and insert it into the
    `__builtins__` namespace with the name, profile. The profiler was designed to
    be used as a decorator, so you can just decorate any function you want, and it
    will time the execution for each line of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how we''ll execute the profiler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The decorated function is ready to be profiled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, `kernprof` will save the results into a file called `script_to_profile.py.lprof`,
    but you can tell it to display the results right away using the `-v` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a simple example output to help you understand what you''ll be looking
    at:'
  prefs: []
  type: TYPE_NORMAL
- en: '![line_profiler](img/B02088_02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The output contains every line of the function, next to the timing information.
    There are six columns of information, and this is what they mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Line #`: This is the line number inside the file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Hits`: This is the number of times this line is executed during the profiling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Time`: This is the total execution time of that line, specified in timer''s
    unit. In the header information before the table with the results, you''ll notice
    a field called `Timer unit`, that number is the conversion factor to seconds (to
    calculate the actual time, you''ll have to do time `x` timer''s unit). It might
    be different on different systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Per hit`: The average amount of time spent executing that line of code. This
    is also specified in timer''s units.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`% Time`: The percentage of time spent executing that line, relative to the
    total time spent executing the entire function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you''re building another tool that leverages `line_profiler`, there are
    two ways to let it know which functions to profile: using the constructor and
    using the `add_function` method.'
  prefs: []
  type: TYPE_NORMAL
- en: The `line_profiler` also provides the same `run`, `runctx`, `runcall`, `enable`,
    and `disable` methods that `cProfile.Profile` provides. However, the last two
    aren't safe when nesting, so be careful. After profiling, you can dump the `stats`
    into a file using the `dump_stats(filename)` method, or you can print them using
    the `print_stats([stream])` method. It'll print the results into `sys.stdout`
    or whatever other stream you pass it as parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of the same function from earlier. This time, the function
    is being profiled using the `line_profiler` API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: kernprof
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `kernprof` is the profiling utility that comes bundled with `line_profiler`
    and allows us to abstract most of the profiling code from our own source code.
    This means we can use it to profile our application, like we saw earlier. `kernprof`
    will do several things for us:'
  prefs: []
  type: TYPE_NORMAL
- en: It'll work with `cProfile`, `lsprof`, and even the profile module, depending
    on which one is available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It'll find our script properly. If the script is not inside the current folder,
    it'll even check the `PATH` variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It'll instantiate and insert the profiler into the `__builtins__` namespace
    with the name `profile`. This will allow us to use the profiler inside our code.
    In the case of `line_profiler`, we can even use it as a decorator without having
    to worry about importing anything.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output files with the profiling `stats` can be viewed using the `pstats.Stats`
    class or even from the command line as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Or in the case of `lprof` files:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Some things to consider about kernprof
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a couple of things to take into consideration when reading the output
    from kernprof. In some cases, the output might be confusing, or the numbers might
    not add up. Here are the answers to some of the most common questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Line-by-line time doesn''t add up to total time when the profile function
    calls another one**: When profiling a function that gets called by another profiled
    function, sometimes, it might happen that the numbers don''t add up. This is because
    `kernprof` is only recording the time spent inside the function and tries to avoid
    measuring any overhead added by the profiler itself, as shown in the following
    screenshot:![Some things to consider about kernprof](img/B02088_02_13.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding screenshot shows an example of this. The `printI` function takes
    `0.010539` seconds according to the profiler. However, inside the `test` function,
    the total amount of time spent seems to be `19567` timer's units, which amounts
    to `0.019567` seconds.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**List comprehension lines have a lot more hits than they should inside the
    report**: This is basically because the report is adding one hit per iteration
    inside the expression. Here is an example of this:![Some things to consider about
    kernprof](img/B02088_02_14.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can see how the actual expression line has `102` hits, `2` for each time
    the `printExpression` function is called, and the other 100 due to the range used.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we've seen the basics of how to use `line_profiler` and `kernprof`,
    let's get our hands dirty with more interesting examples.
  prefs: []
  type: TYPE_NORMAL
- en: Back to Fibonacci
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Yes, let's again profile our original Fibonacci code. It'll be good to compare
    the output from both profilers to see how they work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first look at the output from this new profiler:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Back to Fibonacci](img/B02088_02_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Out of all the numbers in the report, we can rest assured that timing is not
    an issue. Inside the `fib` function, none of the lines take too long (nor should
    they). Inside `fib_seq`, only one does, but that's because of the recursion shown
    inside `fib`.
  prefs: []
  type: TYPE_NORMAL
- en: So, our problem (as we already know) is the recursion and the number of times
    we're executing the `fib` function (57, 291 times to be exact). Every time we
    make a function call, the interpreter has to do a lookup by name and then execute
    the function. Every time we call the `fib` function, two more calls are made.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that comes to mind is to somehow lower the number of recursive
    calls. We can rewrite it into an iterative version or do a quick fix by adding
    the cached decorator, like we did earlier. We can see the results in the following
    report:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Back to Fibonacci](img/B02088_02_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The number of hits for the `fib` function went from 57, 291 hits to `21`. This
    is another proof that the cached decorator is a great optimization in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Inverted index
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Instead of repeating the second example from within a different profiler, let''s
    look at another problem: creating an inverted index ([http://en.wikipedia.org/wiki/inverted_index](http://en.wikipedia.org/wiki/inverted_index)).'
  prefs: []
  type: TYPE_NORMAL
- en: An inverted index is a resource used by many search engines to find words in
    several files at the same time. The way they work is by pre-scanning all files,
    splitting their content into words, and then saving the relations between those
    words and the files (some even save the position of the word too). This way, when
    a search is made on a specific word, the searching time is `O(1)` (constant).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'So now, if we were to look for the word `file`, we know it''s in both files
    (at different positions). Let''s see the code that calculates this index (again,
    the point of the following code is to show classic improvement opportunities,
    so stick with us until we see the optimized version of the code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is as simple as it gets. It gets the job done for simple
    `.txt` files, and that is what we want right now. It'll load all `.txt` files
    inside the files folder, split their content into words, and calculate the offset
    of those words inside the document. Finally, it'll save all this information into
    a file called `index-file.txt`.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's begin profiling and see what we get. Since we don't really know exactly
    which are the heavy-duty functions and which ones are the light ones, let's add
    the `@profile` decorator to all of them and run the profiler.
  prefs: []
  type: TYPE_NORMAL
- en: getOffsetUpToWord
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `getOffsetUpToWord` function looks like a great candidate for optimization,
    since it gets called quite a few times during execution. Let's keep the decorator
    on it for now.
  prefs: []
  type: TYPE_NORMAL
- en: '![getOffsetUpToWord](img/B02088_02_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: getWords
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `getWords` function does a lot of processing. It even has two nested `for`
    loops, so we'll keep the decorator on as well.
  prefs: []
  type: TYPE_NORMAL
- en: '![getWords](img/B02088_02_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: list2dict
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `list2dict` function takes care of grabbing a list of arrays with two elements
    and returning a dictionary, using the first element of the array items as key
    and the second one as values. We'll leave the `@profile` decorator on for now.
  prefs: []
  type: TYPE_NORMAL
- en: '![list2dict](img/B02088_02_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: readFileContent
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `readFileContent` function has two lines, and the significant one simply
    calls the `split` method on the content of the file. There is not a lot to improve
    here, so we'll discard it and focus on the other ones.
  prefs: []
  type: TYPE_NORMAL
- en: '![readFileContent](img/B02088_02_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: saveIndex
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `saveIndex` function writes the results of the processing to a file, using
    a specific format. We might be able to get some better numbers here too.
  prefs: []
  type: TYPE_NORMAL
- en: '![saveIndex](img/B02088_02_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: __start__
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Finally, the main method, `__start__`, takes care of calling the other functions
    and doesn't do much heavy lifting, so we'll also discard it.
  prefs: []
  type: TYPE_NORMAL
- en: '![__start__](img/B02088_02_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, let's summarize. We originally had six functions, out of which we discarded
    two, because they were too trivial or just didn't do anything relevant. Now, we
    have a total of four functions to review and optimize.
  prefs: []
  type: TYPE_NORMAL
- en: getOffsetUpToWord
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let's first look at the `getOffsetUpToWord` function, which has a lot of lines
    for something as simple as adding up the length of the words leading up to the
    current index. There is probably a more Pythonic way to go about it, so let's
    try it out.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function originally comprised 1.4 seconds of the total execution time,
    so let''s try to lower that number by simplifying the code. The adding up of the
    length of the words can be translated into a reduce expression, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This simplification removes the need for extra time doing variable assignments
    and lookups. It might not seem like much. However, if we run the profiler again
    with this new code, the time would go down to 0.9 seconds. There is still an obvious
    drawback to that implementation: the lambda function. We''re dynamically creating
    a function every time we call `getOffsetUpToWord`. We''re calling it 313,868 times,
    so it would be a good idea to have this function already created. We can just
    add a reference to it in the reduce expression, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![getOffsetUpToWord](img/B02088_02_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: With this minor improvement, the execution time goes down to 0.8 seconds. In
    the preceding screenshot, we can see that there are still a lot of unwanted hits
    (and therefore time) spent in the first two lines of the function. This check
    is unnecessary because the reduce function already defaults to 0\. Finally, the
    assignment to the length variable can be removed, and we can return directly the
    sum of the length, the index, and the integer 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, we''re left with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The total execution time for this function went from 1.4 to an amazing 0.67
    seconds.
  prefs: []
  type: TYPE_NORMAL
- en: getWords
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s now move on to the next one: the `getWords` function. It is a pretty
    slow one. According to the screenshot, the execution of this function adds up
    to 4 seconds. That''s not good. Let''s see what we can do about it. First things
    first, the most expensive (time-consuming) line in this function is the one that
    calls the `getOffsetUpToWord` function. Since we already optimized that one, the
    total time of this function is now 2.2 seconds (down from 4 seconds).'
  prefs: []
  type: TYPE_NORMAL
- en: That's a pretty decent side effect optimization, but we can still do a bit more
    for this function. We're using a normal dictionary for the `wordIndexDict` variable,
    so we have to check whether a key is set before actually using it. Doing that
    check inside this function takes up about 0.2 seconds. It is not a lot, but an
    optimization nonetheless. To remove that check, we can use the `defaultdict` class.
    It is a subclass of the `dict` class, which adds an extra functionality. It sets
    a default value for when a key doesn't exist. This will remove the need for those
    0.2 seconds inside the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another trivial but helpful optimization is the assignment of results to variables.
    It might seem like a small thing, but doing it 313,868 times will no doubt hurt
    our timing. So, take a look at these lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'These lines can be changed into a single line of code, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: With that, we shaved off another 0.2 seconds. Finally, we're doing a strip operation
    on every line and then on every word. We can simplify this by calling the `replace`
    method several times for the entire content when loading the file. This will take
    care of cleaning up the text we'll be processing and remove added time for lookups
    and method calls inside the `getWords` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The new code looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'It only takes 1.57 seconds to run. There is one extra optimization that we
    might want to look at. It fits this particular case, because the `getOffsetUpToWord`
    function is only used in one place. Since this function got reduced to a one-liner,
    we can just put the one-liner in place of the function call. This one-liner will
    subtract the lookup time and give us a whopping 1.07 seconds (that''s a 0.50 seconds
    reduction!). This is how the latest version of the function looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![getWords](img/B02088_02_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If you'll call the function from several places, this might be an optimization
    that is not worth having, since it'll hurt the code maintainability. Code maintainability
    is also an important aspect when developing. It should be a deciding factor when
    trying to figure out when to stop with the optimization process.
  prefs: []
  type: TYPE_NORMAL
- en: list2dict
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Moving on, for the `list2dict` function, we can''t really do much, but we can
    clean it up to get a more readable code and shave of about 0.1 seconds. Again,
    we''re not doing this strictly for the speed gain, but for the readability gain.
    We have a chance to use the `defaultdict` class again and remove the check for
    a key so that the new code looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code has less lines, is easier to read, and more easy to understand.
  prefs: []
  type: TYPE_NORMAL
- en: saveIndex
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Finally, let's take a look at the `saveIndex` function. According to our initial
    report, this function took 0.23 seconds to preprocess and save the data into the
    index file. That's a pretty good number already, but we can do a bit better by
    taking a second look at all the string concatenations we have.
  prefs: []
  type: TYPE_NORMAL
- en: Before saving the data, for every word we generate a string by concatenating
    several pieces together. In that same loop, we will also reset the `indexLine`
    and `glue` variables. These actions will add up to a lot of time, so we might
    want to change our strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the preceding code, we changed the entire `for` loop. Now,
    instead of adding the new string to the `indexLine` variable, we appended it into
    a list. We also removed the map call, which was making sure we were dealing with
    strings during the `join` call. That `map` was moved into the `list2dict` function,
    casting the indexes to the string directly while appending them.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we used the `+` operator to concatenate strings instead of doing string
    expansion, which is a more expensive operation. In the end, this function went
    down from 0.23 seconds to 0.13, giving us a 0.10-second gain in speed.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To sum things up, we''ve seen two major profilers used with Python: `cProfile`,
    which comes bundled with the language, and `line_profiler`, which gives us the
    chance to look at each line of code independently. We also covered some examples
    of analysis and optimization using them.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at a set of visual tools that will help us
    in our job by displaying the same data we covered in this chapter, but in a graphic
    manner.
  prefs: []
  type: TYPE_NORMAL
