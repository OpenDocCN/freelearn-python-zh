<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<meta charset="utf-8"/>
<meta content="pandoc" name="generator"/>
<title>ch010.xhtml</title>

<!-- kobo-style -->
<style id="koboSpanStyle" type="text/css" xmlns="http://www.w3.org/1999/xhtml">.koboSpan { -webkit-text-combine: inherit; }</style>
</head>
<body epub:type="bodymatter">

<h1 data-number="10">Chapter 6<br/>
Project 2.1: Data Inspection Notebook</h1>
<p>We often need to do an ad hoc inspection of source data. In particular, the very first time we acquire new data, we need to see the file to be sure it meets expectations. Additionally, debugging and problem-solving also benefit from ad hoc data inspections. This chapter will guide you through using a Jupyter notebook to survey data and find the structure and domains of the attributes.</p>
<p>The previous chapters have focused on a simple dataset where the data types look like obvious floating-point values. For such a trivial dataset, the inspection isn’t going to be very complicated.</p>
<p>It can help to start with a trivial dataset and focus on the tools and how they work together. For this reason, we’ll continue using relatively small datasets to let you learn about the tools without having the burden of <strong>also</strong> trying to understand the data.</p>
<p>This chapter’s projects cover how to create and use a Jupyter notebook for data inspection. This permits tremendous flexibility, something often required when looking at new data for the first time. It’s also essential when diagnosing problems with data that has — unexpectedly — changed.</p>
<p>A Jupyter notebook is inherently interactive and saves us from having to design and build an interactive application. Instead, we need to be disciplined in using a notebook only to examine data, never to apply changes.</p>
<p>This chapter has one project, to build an inspection notebook. We’ll start with a description of the notebook’s purpose. </p>

<h2 data-number="10.1">6.1  Description</h2>
<p>When confronted with raw data acquired from a source application, database, or web API, it’s prudent to inspect the data to be sure it really can be used for the desired analysis. It’s common to find that data doesn’t precisely match the given descriptions. It’s also possible to discover that the metadata is out of date or incomplete.</p>
<p>The foundational principle behind this project is the following:</p>
<p><strong>We don’t always know what the actual data looks like.</strong></p>
<p>Data may have errors because source applications have bugs. There could be ”undocumented features,” which are similar to bugs but have better explanations. There may have been actions made by users that have introduced new codes or status flags. For example, an application may have a ”comments” field on an accounts-payable record, and accounting clerks may have invented their own set of coded values, which they put in the last few characters of this field. This defines a manual process outside the enterprise software. It’s an essential business process that contains valuable data; it’s not part of any software.</p>
<p>The general process for building a useful Jupyter notebook often proceeds through the following phases:</p>
<ol>
<li><div><p>Start with a simple display of selected rows.</p>
</div></li>
<li><div><p>Then, show ranges for what appear to be numerical fields.</p>
</div></li>
<li><div><p>Later, in a separate analysis notebook, we can find central tendency (mean, median, and standard deviation) values after they’ve been cleaned.</p>
</div></li>
</ol>
<p>Using a notebook moves us away from the previous chapters’ focus on CLI applications. This is necessary because a notebook is interactive. It is designed to allow exploration with few constraints.</p>
<p>The <strong>User Experience </strong>(<strong>UX</strong>) has two general steps to it:</p>
<ol>
<li><div><p>Run a data acquisition application. This is one of the CLI commands for projects in any of the previous chapters.</p>
</div></li>
<li><div><p>Start a Jupyter Lab server. This is a second CLI command to start the server. The <code>jupyter</code><code> lab</code> command will launch a browser session. The rest of the work is done through the browser:</p>
<ol>
<li><div><p>Create a notebook by clicking the notebook icon.</p>
</div></li>
<li><div><p>Load data by entering some Python code into a cell.</p>
</div></li>
<li><div><p>Determine if the data is useful by creating cells to show the data and show properties of the data.</p>
</div></li>
</ol>
</div></li>
</ol>
<p>For more information on Jupyter, see <a class="url" href="https://www.packtpub.com/product/learning-jupyter/9781785884870">https://www.packtpub.com/product/learning-jupyter/9781785884870</a>. </p>

<h3 data-number="10.1.1">6.1.1  About the source data</h3>
<p>An essential ingredient here is that all of the data acquisition projects <strong>must</strong> produce output in a consistent format. We’ve suggested using NDJSON (sometimes called JSON NL) as a format for preserving the raw data. See <a href="ch007.xhtml#x1-560003"><em>Chapter</em><em> 3</em></a>, <a href="ch007.xhtml#x1-560003"><em>Project 1.1: Data Acquisition Base Application</em></a>, for more information on the file format.</p>
<p>It’s imperative to review the previous projects’ acceptance test suites to be sure there is a test to confirm the output files have the correct, consistent format.</p>
<p>To recap the data flow, we’ve done the following:</p>
<ul>
<li><p>Read from some source. This includes files, RESTful APIs, HTML pages, and SQL databases.</p></li>
<li><p>Preserved the raw data in an essentially textual form, stripping away any data type information that may have been imposed by a SQL database or RESTful JSON document.</p></li>
</ul>
<p>The inspection step will look at the text versions of values in these files. Later projects, starting with <a href="ch013.xhtml#x1-2080009"><em>Chapter</em><em> 9</em></a>, <a href="ch013.xhtml#x1-2080009"><em>Project 3.1: Data Cleaning Base Application</em></a>, will look at converting data from text into something more useful for analytic work.</p>
<p>An inspection notebook will often be required to do some data cleanup in order to show data problems. This will be enough cleanup to understand the data and no more. Later projects will expand the cleanup to cover all of the data problems.</p>
<p>In many data acquisition projects, it’s unwise to attempt any data conversion before an initial inspection. This is because the data is highly variable and poorly documented. A disciplined, three-step approach separates acquisition and inspection from attempts at data conversion and processing.</p>
<p>We may find a wide variety of unexpected things in a data source. For example, a CSV file may have an unexpected header, leading to a row of bad data. Or, a CSV file may — sometimes — lack headers, forcing the acquire application to supply default headers. A file that’s described as CSV may not have delimiters, but may have fixed-size text fields padded with spaces. There may be empty rows that can be ignored. There may be empty rows that delimit the useful data and separate it from footnotes or other non-data in the file. A ZIP archive may contain a surprising collection of irrelevant files in addition to the desired data file.</p>
<p>Perhaps one of the worst problems is trying to process files that are not prepared using a widely used character encoding such as UTF-8. Files encoded with CP-1252 encoding may have a few odd-looking characters when the decoder assumes it’s UTF-8 encoding. Python’s <code>codecs</code> module provides a number of forms of alternative file encoding to handle this kind of problem. This problem seems rare; some organizations will note the encoding for text to prevent problems.</p>
<p>Inspection notebooks often start as <code>print()</code> functions in the data acquisition process to show what the data is. The idea here is to extend this concept a little and use an interactive notebook instead of <code>print()</code> to get a look at the data and see that it meets expectations.</p>
<div><div><p>Not all managers agree with taking time to build an inspection notebook. Often, this is a conflict between assumptions and reality with the following potential outcomes:</p>
<ul>
<li><p>A manager can assume there will be no surprises in the data; the data will be entirely as specified in a data contract or other schema definition.</p>
<ul>
<li><p>When the data doesn’t match expectations, a data inspection notebook will be a helpful part of the debugging effort.</p></li>
<li><p>In the unlikely event the data does match expectations, the data inspection notebook can be used to show that the data is valid.</p></li>
</ul></li>
<li><p>A manager can assume the data is unlikely to be correct. In this case, the data inspection notebook will be seen as useful for uncovering the inevitable problems.</p></li>
</ul>
<p>Notebooks often start as <code>print()</code> or logger output to confirm the data is useful. This debugging output can be migrated to an informal notebook — at a low cost — and evolve into something more complete and focused on inspection and data quality assurance.</p>
</div>
</div>
<p>This initial project won’t build a complicated notebook. The intent is to provide an <strong>interactive </strong>display of data, allowing exploration and investigation. In the next section, we’ll outline an approach to this project, and to working with notebooks in general. </p>



<h2 data-number="10.2">6.2  Approach</h2>
<p>We’ll take some guidance from the C4 model ( <a class="url" href="https://c4model.com">https://c4model.com</a>) when looking at our approach.</p>
<ul>
<li><p><strong>Context</strong>: For this project, the context diagram has two use cases: acquire and inspect</p></li>
<li><p><strong>Containers</strong>: There’s one container for the various applications: the user’s personal computer</p></li>
<li><p><strong>Components</strong>: There are two significantly different collections of software components: the acquisition program and inspection notebooks</p></li>
<li><p><strong>Code</strong>: We’ll touch on this to provide some suggested directions</p></li>
</ul>
<p>A context diagram for this application is shown in <a href="#6.1"><em>Figure 6.1</em></a>.</p>
<figure class="IMG---Figure">
<img alt="Figure 6.1: Context Diagram " src="img/file31.jpg"/>
<figcaption class="IMG---Caption">Figure 6.1: Context Diagram </figcaption>
</figure>
<p>The data analyst will use the CLI to run the data acquisition program. Then, the analyst will use the CLI to start a Jupyter Lab server. Using a browser, the analyst can then use Jupyter Lab to inspect the data.</p>
<p>The components fall into two overall categories. The component diagram is shown in <a href="#6.2"><em>Figure </em><em>6.2</em></a>.</p>
<figure class="IMG---Figure">
<img alt="Figure 6.2: Component diagram " src="img/file32.jpg"/>
<figcaption class="IMG---Caption">Figure 6.2: Component diagram </figcaption>
</figure>
<p>The diagram shows the interfaces seen by the data analyst, the <code>terminal</code> and the <code>browser</code>. These are shown with the <em>boundary </em>icon from the <strong>Unified Modeling</strong> <strong>Language </strong>(<strong>UML</strong>).</p>
<p>The <code>Acquisition</code> group of components contains the various modules and the overall acquire application. This is run from the command line to acquire the raw data from an appropriate source. The <code>db_extract</code> module is associated with an external SQL database. The <code>api_download</code> module is associated with an external RESTful API. Additional sources and processing modules could be added to this part of the diagram.</p>
<p>The processing performed by the <code>Acquisition</code> group of components creates the data files shown in the <code>Storage</code> group. This group depicts the raw data files acquired by the <code>acquire</code> application. These files will be refined and processed by further analytic applications.</p>
<p>The <code>Inspection</code> group shows the <code>jupyter</code> component. This is the entire Jupyter Lab application, summarized as a single icon. The <code>notebook</code> component is the notebook we’ll build in this application. This notebook depends on Jupyter Lab.</p>
<p>The <code>browser</code> is shown with the boundary icon. The intention is to characterize the notebook interaction via the browser as the user experience.</p>
<p>The <code>notebook</code> component will use a number of built-in Python modules. This notebook’s cells can be decomposed into two smaller kinds of components:</p>
<ul>
<li><p>Functions to gather data from acquisition files.</p></li>
<li><p>Functions to show the raw data. The <code>collections.Counter</code> class is very handy for this.</p></li>
</ul>
<p>You will need to locate (and install) a version of Jupyter Lab for this project. This needs to be added to the <code>requirements-dev.txt</code> file so other developers know to install it.</p>
<p>When using <code>conda</code> to manage virtual environments, the command might look like the following:</p>
<div><div><pre class="console">% conda install jupyterlab</pre>
</div>
</div>
<p>When using other tools to manage virtual environments, the command might look like the following:</p>
<div><div><pre class="console">% python -m pip install jupyterlab</pre>
</div>
</div>
<p>Once the <code>jupyter</code> products are installed, it must be started from the command line. This command will start the server and launch a browser window:</p>
<div><div><pre class="console">% jupyter lab</pre>
</div>
</div>
<p>For information on using Jupyter Lab, see <a class="url" href="https://jupyterlab.readthedocs.io/en/latest/">https://jupyterlab.readthedocs.io/en/latest/</a>.</p>
<p>If you’re not familiar with Jupyter, now is the time to use tutorials and learn the basics before moving on with this project.</p>
<div><div><p>Many of the notebook examples will include <code>import</code> statements.</p>
<p>Developers new to working with a Jupyter notebook should not take this as advice to repeat <code>import</code> statements in multiple cells throughout a notebook.</p>
<p>In a practical notebook, the imports can be collected together, often in a separate cell to introduce all of the needed packages.</p>
<p>In some enterprises, a startup script is used to provide a common set of imports for a number of closely related notebooks.</p>
<p>We’ll return to more flexible ways to handle Python libraries from notebooks in <a href="ch017.xhtml#x1-29700013"><em>Chapter</em><em> 13</em></a>, <a href="ch017.xhtml#x1-29700013"><em>Project 4.1: Visual Analysis Techniques</em></a>.</p>
</div>
</div>
<p>There are two other important considerations for this project: the ability to write automated tests for a notebook and the interaction of Python modules and notebooks. We’ll look at these topics in separate sections. </p>

<h3 data-number="10.2.1">6.2.1  Notebook test cases for the functions</h3>
<p>It’s common to require unit test cases for a Python package. To be sure the test cases are meaningful, some enterprises insist the test cases exercise 100% of the lines of code in the module. For some industries, all logic paths must be tested. For more information, see <a href="ch005.xhtml#x1-170001"><em>Chapter</em><em> 1</em></a>, <a href="ch005.xhtml#x1-170001"><em>Project Zero: A Template for Other</em> <em>Projects</em></a>.</p>
<p>For notebooks, automated testing can be a shade more complicated than it is for a Python module or package. The complication is that notebooks can contain arbitrary code that is not designed with testability in mind.</p>
<p>In order to have a disciplined, repeatable approach to creating notebooks, it’s helpful to develop a notebook in a series of stages, evolving toward a notebook that supports automated testing.</p>
<p>A notebook is software, and without test cases, any software is untrustworthy. In rare cases, the notebook’s code is simple enough that we can inspect it to develop some sense of its overall fitness for purpose. In most cases, complex computations, functions, and class definitions require a test case to demonstrate the code can be trusted to work properly.</p>
<p>The stages of notebook evolution often work as follows:</p>
<ol>
<li><div><p>At stage zero, a notebook is often started with arbitrary Python code in cells and few or no function or class definitions. This is a great way to start development because the interactive nature of the notebook provides immediate results. Some cells will have errors or bad ideas in them. The order for processing the cells is not simply top to bottom. This code is difficult (or impossible) to validate with any automated testing.</p>
</div></li>
<li><div><p>Stage one will transform cell expressions into function and class definitions. This version of the notebook can also have cells with examples using the functions and classes. The order is closer to strictly top-to-bottom; there are fewer cells with known errors. The presence of examples serves as a basis for validating the notebook’s processing, but an automated test isn’t available.</p>
</div></li>
<li><div><p>Stage two has more robust tests, using formal <code>assert</code> statements or <code>doctest</code> comments to define a repeatable test procedure. Rerunning the notebook from the beginning after any change will validate the notebook by executing the <code>assert</code> statements. All the cells are valid and the notebook processing is strictly top to bottom.</p>
</div></li>
<li><div><p>When there is more complicated or reusable processing, it may be helpful to refactor the function and class definitions out of the notebook and into a module. The module will have a unit test module or may be tested via <strong>doctest </strong>examples. This new module will be imported by the notebook; the notebook is used more for the presentation of results than the development of new ideas.</p>
</div></li>
</ol>
<p>One easy road to automated testing is to include <strong>doctest </strong>examples inside function and class definitions. For example, we might have a notebook cell that contains something like the following function definition:</p>
<div><div><pre class="source-code">def min_x(series: Series) -&gt; float:
    """
    &gt;&gt;&gt; s = [
    ...     {’x’: ’3’, ’y’: ’4’},
    ...     {’x’: ’2’, ’y’: ’3’},
    ...     {’x’: ’5’, ’y’: ’6’}]
    &gt;&gt;&gt; min_x(s)
    2
    """
    return min(int(s[’x’]) for s in series.samples)</pre>
</div>
</div>
<p>The lines in the function’s docstring marked with <code>&gt;&gt;&gt;</code> are spotted by the <strong>doctest</strong> tool. The lines are evaluated and the results are compared with the example from the docstring.</p>
<p>The last cell in the notebook can execute the <code>doctest.testmod()</code> function. This will examine all of the class and function definitions in the notebook, locate their <strong>doctest </strong>examples, and confirm the actual results match the expectations.</p>
<p>For additional tools to help with notebook testing, see <a class="url" href="https://testbook.readthedocs.io/en/latest/">https://testbook.readthedocs.io/en/latest/</a>.</p>
<p>This evolution from a place for recording good ideas to an engineered solution is not trivially linear. There are often exploration and learning opportunities that lead to changes and shifts in focus. Using a notebook as a tool for tracking ideas, both good and bad, is common.</p>
<p>A notebook is also a tool for presenting a final, clear picture of whether or not the data is what the users expect. In this second use case, separating function and class definitions becomes more important. We’ll look at this briefly in the next section. </p>


<h3 data-number="10.2.2">6.2.2  Common code in a separate module</h3>
<p>As we noted earlier, a notebook lets an idea evolve through several forms.</p>
<p>We might have a cell with the following</p>
<div><div><pre class="source-code">x_values = []
for s in source_data[1:]:
    x_values.append(float(s[’x’]))
min(x_values)</pre>
</div>
</div>
<p>Note that this computation skips the first value in the series. This is because the source data has a header line that’s read by the <code>csv.reader()</code> function. Switching to <code>csv.DictReader()</code> can politely skip this line, but also changes the result structure from a list of strings into a dictionary.</p>
<p>This computation of the minimum value can be restated as a function definition. Since it does three things — drops the first line, extracts the <code>’x’</code> attribute, and converts it into a float — it might be better to decompose it into three functions. It can be refactored again to include <strong>doctest</strong> examples in each function. See <a href="#x1-1500001"><em>Notebook test cases for the functions</em></a> for the example.</p>
<p>Later, this function can be cut from the notebook cell and pasted into a separate module. We’ll assume the overall function was named <code>min_x()</code>. We might add this to a module named <code>series_stats.py</code>. The notebook can then import and use the function, leaving the definition as a sidebar detail:</p>
<div><div><pre class="source-code">from series_stats import min_x</pre>
</div>
</div>
<p>When refactoring a notebook to a reusable module, it’s important to use <strong>cut</strong> <strong>and paste</strong>, not copy and paste. A copy of the function will lead to questions if one of the copies is changed to improve performance or fix a problem and the other copy is left untouched. This is sometimes called the <strong>Don’t Repeat</strong> <strong>Yourself </strong>(<strong>DRY</strong>) principle.</p>
<p>When working with external modules that are still under development, any changes to a module will require stopping the notebook kernel and rerunning the notebook from the very beginning to remove and reload the function definitions. This can become awkward. There are some extensions to iPython that can be used to reload modules, or even auto-reload modules when the source module changes.</p>
<p>An alternative is to wait until a function or class seems mature and unlikely to change before refactoring the notebook to create a separate module. Often, this decision is made as part of creating a final presentation notebook to display useful results.</p>
<p>We can now look at the specific list of deliverables for this project. </p>



<h2 data-number="10.3">6.3  Deliverables</h2>
<p>This project has the following deliverables:</p>
<ul>
<li><p>A <code>pyproject.toml</code> file that identifies the tools used. For this book, we used <code>jupyterlab==3.5.3</code>. Note that while the book was being prepared for publication, version 4.0 was released. This ongoing evolution of components makes it important for you to find the latest version, not the version quoted here.</p></li>
<li><p>Documentation in the <code>docs</code> folder.</p></li>
<li><p>Unit tests for any new application modules in the <code>tests</code> folder.</p></li>
<li><p>Any new application modules in the <code>src</code> folder with code to be used by the inspection notebook.</p></li>
<li><p>A notebook to inspect the raw data acquired from any of the sources.</p></li>
</ul>
<p>The project directory structure suggested in <a href="ch005.xhtml#x1-170001"><em>Chapter</em><em> 1</em></a>, <a href="ch005.xhtml#x1-170001"><em>Project Zero: A Template</em> <em>for Other Projects</em></a> mentions a <code>notebooks</code> directory. See <a href="ch005.xhtml#x1-260003"><em>List of deliverables</em></a> for more information. Previous chapters haven’t used any notebooks, so this directory might not have been created in the first place. For this project, the <code>snotebooks</code> directory is needed.</p>
<p>Let’s look at a few of these deliverables in a little more detail. </p>

<h3 data-number="10.3.1">6.3.1  Notebook .ipynb file</h3>
<p>The notebook can (and should) be a mixture of Markdown cells providing notes and context, and computation cells showing the data.</p>
<p>Readers who have followed the projects up to this point will likely have a directory with NDJSON files that need to be read to construct useful Python objects. One good approach is to define a function to read lines from a file, and use <code>json.loads()</code> to transform the line of text into a small dictionary of useful data.</p>
<p>There’s no compelling reason to use the <code>model</code> module’s class definitions for this inspection. The class definitions can help to make the data somewhat more accessible.</p>
<p>The inspection process starts with cells that name the files, creating <code>Path</code> objects.</p>
<p>A function code like the following example might be helpful:</p>
<div><div><pre class="source-code">import csv
from collections.abc import Iterator
import json
from typing import TextIO

def samples_iter(source: TextIO) -&gt; Iterator[dict[str, str]]:
    yield from (json.loads(line) for line in source)</pre>
</div>
</div>
<p>This function will iterate over the acquired data. In many cases, we can use the iterator to scan through a large collection of samples, picking individual attribute values or some subset of the samples.</p>
<p>We can use the following statement to create a list-of-dictionary structure from the given path:</p>
<div><div><pre class="source-code">from pathlib import Path
source_path = Path("/path/to/quartet/Series_1.ndjson")
with source_path.open() as source_file:
    source_data = list(samples_iter(source_file))</pre>
</div>
</div>
<p>We can start with these basics in a few cells of the notebook. Given this foundation, further cells can explore the available data.</p>

<h4 class="likesubsubsectionHead" data-number="10.3.1.1">Cells and functions to analyze data</h4>
<p>For this initial inspection project, the analysis requirements are small. The example datasets from the previous chapters are artificial data, designed to demonstrate the need to use graphical techniques for exploratory data analysis.</p>
<p>For other datasets, however, there may be a variety of odd or unusual problems.</p>
<p>For example, the <strong>CO</strong><strong>2</strong> <strong>PPM — Trends in Atmospheric Carbon</strong> <strong>Dioxide </strong>dataset, available at <a class="url" href="https://datahub.io/core/co2-ppm">https://datahub.io/core/co2-ppm</a>, has a number of ”missing value” codes in the data. Here are two examples:</p>
<ul>
<li><p>The CO2 average values sometimes have values of −99<em>.</em>99 as a placeholder for a time when a measurement wasn’t available. In these cases, a statistical process used data from adjacent months to interpolate the missing value.</p></li>
<li><p>Additionally, the number of days of valid data for a month’s summary wasn’t recorded, and a −1 value is used.</p></li>
</ul>
<p>This dataset requires a bit more care to be sure of the values in each column and what the columns mean.</p>
<p>Capturing the domain of values in a given column is helpful here. The <code>collections</code> module has a <code>Counter</code> object that’s ideal for understanding the data in a specific column.</p>
<p>A cell can use a three-step computation to see the domain of values:</p>
<ol>
<li><div><p>Use the <code>samples_iter()</code> function to yield the source documents.</p>
</div></li>
<li><div><p>Create a generator with sample attribute values.</p>
</div></li>
<li><div><p>Create a <code>Counter</code> to summarize the values.</p>
</div></li>
</ol>
<p>This can lead to a cell in the notebook with the following statements:</p>
<div><div><pre class="source-code">from collections import Counter

values_x = (sample[’x’] for sample in source_data)
domain_x = Counter(values_x)</pre>
</div>
</div>
<p>The next cell in the notebook can display the value of the <code>domain_x</code> value. If the <code>csv.reader()</code> function is used, it will reveal the header along with the domain of values. If the <code>csv.DictReader()</code> class is used, this collection will not include the header. This permits a tidy exploration of the various attributes in the collection of samples.</p>
<p>An inspection notebook is not the place to attempt more sophisticated data analysis. Computing means or medians should only be done on cleaned data. We’ll return to this in <a href="ch019.xhtml#x1-32500015"><em>Chapter</em><em> 15</em></a>, <a href="ch019.xhtml#x1-32500015"><em>Project 5.1: Modeling Base</em> <em>Application</em></a>.</p>


<h4 class="likesubsubsectionHead" data-number="10.3.1.2">Cells with Markdown to explain things</h4>
<p>It’s very helpful to include cells using Markdown to provide information, insights, and lessons learned about the data.</p>
<p>For information on the markdown language, see the Daring Fireball website: <a class="url" href="https://daringfireball.net/projects/markdown/basics">https://daringfireball.net/projects/markdown/basics</a>.</p>
<p>As noted earlier in this chapter, there are two general flavors of notebooks:</p>
<ul>
<li><p><strong>Exploratory</strong>: These notebooks are a series of blog posts about the data and the process of exploring and inspecting the data. Cells may not all work because they’re works in process.</p></li>
<li><p><strong>Presentation</strong>: These notebooks are a more polished, final report on data or problems. The paths that lead to dead ends should be pruned into summaries of the lessons learned.</p></li>
</ul>
<p>A bright line separates these two flavors of notebooks. The distinguishing factor is the reproducibility of the notebook. A notebook that’s useful for presentations can be run from beginning to end without manual intervention to fix problems or skip over cells with syntax errors or other problems. Otherwise, the notebook is part of an exploration. It’s often necessary to copy and edit an exploratory notebook to create a derived notebook focused on presentation.</p>
<p>Generally, a notebook designed for a presentation uses Markdown cells to create a narrative flow that looks like any chapter of a book or article in a journal. We’ll return to more formal reporting in <a href="ch018.xhtml#x1-31300014"><em>Chapter</em><em> 14</em></a>, <a href="ch018.xhtml#x1-31300014"><em>Project 4.2: Creating</em> <em>Reports</em></a>.</p>


<h4 class="likesubsubsectionHead" data-number="10.3.1.3">Cells with test cases</h4>
<p>Earlier, we introduced a <code>samples_iter()</code> function that lacked any unit tests or examples. It’s considerably more helpful to provide a <strong>doctest </strong>string within a notebook:</p>
<div><div><pre class="source-code">def samples_iter(source: TextIO) -&gt; Iterator[dict[str, str]]:
    """
    # Build NDJSON file with two lines
    &gt;&gt;&gt; import json
    &gt;&gt;&gt; from io import StringIO
    &gt;&gt;&gt; source_data = [
    ...     {’x’: 0, ’y’: 42},
    ...     {’x’: 1, ’y’: 99},
    ... ]
    &gt;&gt;&gt; source_text = [json.dumps(sample) for sample in source_data]
    &gt;&gt;&gt; ndjson_file = StringIO(’\\n’.join(source_text))

    # Parse the file
    &gt;&gt;&gt; list(samples_iter(ndjson_file))
    [{’x’: 0, ’y’: 42}, {’x’: 1, ’y’: 99}]
    """
    yield from (json.loads(line) for line in source)</pre>
</div>
</div>
<p>This function’s docstrings include an extensive test case. The test case builds an NDJSON document from a list of two dictionaries. The test case then applies the <code>samples_iter()</code> function to parse the NDJSON file and recover the original two samples.</p>
<p>To execute this test, the notebook needs a cell to examine the docstrings in all of the functions and classes defined in the notebook:</p>
<div><div><pre class="source-code">import doctest
doctest.testmod()</pre>
</div>
</div>
<p>This works because the global context for a notebook is treated like a module with a default name of <code>__main__</code>. This module will be examined by the <code>textmod()</code> function to find docstrings that look like they contain doc test examples.</p>
<p>Having the last cell run the <strong>doctest </strong>tool makes it easy to run the notebook, scroll to the end, and confirm the tests have all passed. This is an excellent form of validation. </p>



<h3 data-number="10.3.2">6.3.2  Executing a notebook’s test suite</h3>
<p>A Jupyter notebook is inherently interactive. This makes an automated acceptance test of a notebook potentially challenging.</p>
<p>Fortunately, there’s a command that executes a notebook to confirm it works all the way through without problems.</p>
<p>We can use the following command to execute a notebook to confirm that all the cells will execute without any errors:</p>
<div><div><pre class="console">% jupyter execute notebooks/example_2.ipynb</pre>
</div>
</div>
<p>A notebook may ingest a great deal of data, making it very time-consuming to test the notebook as a whole. This can lead to using a cell to read a configuration file and using this information to use a subset of data for test purposes. </p>



<h2 data-number="10.4">6.4  Summary</h2>
<p>This chapter’s project covered the basics of creating and using a Jupyter Lab notebook for data inspection. This permits tremendous flexibility, something often required when looking at new data for the first time.</p>
<p>We also looked at adding <strong>doctest </strong>examples to functions and running the <strong>doctest </strong>tool in the last cell of a notebook. This lets us validate that the code in the notebook is very likely to work properly.</p>
<p>Now that we’ve got an initial inspection notebook, we can start to consider the specific kinds of data being acquired. In the next chapter, we’ll add features to this notebook. </p>


<h2 data-number="10.5">6.5  Extras</h2>
<p>Here are some ideas for you to add to this project. </p>

<h3 data-number="10.5.1">6.5.1  Use pandas to examine data</h3>
<p>A common tool for interactive data exploration is the <code>pandas</code> package.</p>
<p>See <a class="url" href="https://pandas.pydata.org">https://pandas.pydata.org</a> for more information.</p>
<p>Also, see <a class="url" href="https://www.packtpub.com/product/learning-pandas/9781783985128">https://www.packtpub.com/product/learning-pandas/9781783985128</a> for resources for learning more about pandas.</p>
<p>The value of using pandas for examining text may be limited. The real value of pandas is for doing more sophisticated statistical and graphical analysis of the data.</p>
<p>We encourage you to load NDJSON documents using pandas and do some preliminary investigation of the data values. </p>



</body>
</html>
