- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Asynchronous Programming with Thread and Queue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many times, code that works flawlessly in the simplicity of a test environment
    encounters problems in the real world; unfortunately, this seems to be the case
    for the ABQ Data Entry application. While your network functions ran instantaneously
    in your localhost-only test environment, the lab's slow VPN uplink has exposed
    some shortcomings in your programming. Users report that the application freezes
    or becomes unresponsive when network transactions are taking place. Although it
    does work, it looks unprofessional and is an annoyance to users.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve this problem, we''re going to need to apply asynchronous programming
    techniques, which we''ll learn about in the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: In *The Tkinter event queue*, we'll learn how to manipulate Tkinter's event
    processing to improve the responsiveness of the application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *Running code in the background with threads*, we'll explore writing multi-threaded
    applications using Python's `threading` module.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *Passing messages using a queue*, you'll learn how to use `Queue` objects
    to implement inter-thread communication.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *Using locks to protect shared resources*, we'll utilize a `Lock` object
    to keep threads from overwriting one another.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Tkinter's event queue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed in *Chapter 11*, *Creating Automated Tests with unittest*, many
    tasks in Tkinter, such as drawing and updating widgets, are done asynchronously
    rather than taking immediate action when called in code. More specifically, the
    actions you perform in Tkinter, such as clicking a button, triggering a key bind
    or trace, or resizing a window, place an **event** in the event queue. On each
    iteration of the main loop, Tkinter pulls all outstanding events from the queue
    and processes them one at a time. For each event, Tkinter executes any **tasks**
    (that is, callbacks or internal operations like redrawing widgets) bound to the
    event before proceeding to the next event in the queue.
  prefs: []
  type: TYPE_NORMAL
- en: Tasks are roughly prioritized by Tkinter as either **regular** or **do-when-idle**
    (often referred to as **idle tasks**). During event processing, regular tasks
    are processed first, followed by idle tasks when all the regular tasks are finished.
    Most drawing or widget-updating tasks are classified as idle tasks, while actions
    like callback functions are, by default, regular priority.
  prefs: []
  type: TYPE_NORMAL
- en: Event queue control
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most of the time, we get the behavior we need from Tkinter by relying on higher-level
    constructs like `command` callbacks and `bind()`. However, there are situations
    where we might want to directly interact with the event queue and manually control
    how events are processed. We've already seen some of the functionality available
    to do this, but let's take a deeper look at them here.
  prefs: []
  type: TYPE_NORMAL
- en: The update() methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Chapter 11*, *Creating Automated Tests with unittest*, you learned about
    the `update()` and `update_idletasks()` methods. To review, these methods will
    cause Tkinter to execute any tasks for events currently in the queue; `update()`
    runs tasks for all events currently waiting in the queue until it's entirely clear,
    while `update_idletasks()` only runs the idle tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Since idle tasks are generally smaller and safer operations, it's recommended
    to use `update_idletasks()` unless you find it doesn't do the job.
  prefs: []
  type: TYPE_NORMAL
- en: Note that `update()` and `update_idletasks()` will cause the processing of *all*
    outstanding events for *all* widgets, regardless of what widget the method is
    called on. There is no way to only process events for a particular widget or Tkinter
    object.
  prefs: []
  type: TYPE_NORMAL
- en: The after() methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to allowing us to control the processing of the queue, Tkinter
    widgets have two methods for adding arbitrary code to the event queue on a delay:
    `after()` and `after_idle()`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic use of `after()` looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we're setting the `root.quit()` method to run after 1 second
    (1,000 milliseconds). What actually happens here is that an event bound to `root.quit`
    is added to the event queue, but with the condition that it shouldn't be executed
    until at least 1,000 milliseconds from the moment when `after()` is called. During
    that time period, any other events in the queue will be processed first. As a
    result, while the command will not be executed *sooner* than 1,000 milliseconds,
    it will very likely be executed *later*, depending on what else is being processed
    already in the event queue.
  prefs: []
  type: TYPE_NORMAL
- en: The `after_idle()` method also adds a task to the event queue, but rather than
    giving it an explicit delay it simply adds it as an idle task, ensuring that it
    will be run after any regular tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In both methods, any additional arguments after the callback reference are
    simply passed to the callback as positional arguments; for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we're passing the arguments `'hello'`, `'Python'`, and `'programmers'`
    to a `print()` call. This statement will schedule the statement `print('hello',
    'Python', 'programmers!')` to be run as soon as possible after 1 second has elapsed.
  prefs: []
  type: TYPE_NORMAL
- en: Note that `after()` and `after_idle()` cannot take keyword arguments for the
    passed callable, only positional arguments.
  prefs: []
  type: TYPE_NORMAL
- en: Code scheduled with `after()` can also be un-scheduled using the `after_cancel()`
    method. The method takes a task ID number, which is returned when we call `after()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we could amend our previous example like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this script, we save the return value of `after()`, which gives us the ID
    of the scheduled task. Then, in the callback for our button, we call `after_cancel()`,
    passing in the ID value. Clicking the button before the 3 seconds is up results
    in the `root.quit` task being canceled and the application remaining open.
  prefs: []
  type: TYPE_NORMAL
- en: Common uses of event queue control
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *Chapter 11*, *Creating Automated Tests with unittest*, we made good use
    of queue control methods to make sure our tests ran quickly and efficiently without
    having to wait on human interaction. There are a few different ways we can use
    these methods in actual applications though, which we'll look at here.
  prefs: []
  type: TYPE_NORMAL
- en: Smoothing out display changes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In an application with dynamic GUI changes, the smoothness of these changes
    may suffer a bit as the windows resize in response to elements appearing and reappearing.
    For example, in the ABQ application, you may notice a smaller application window
    appearing just after login, which gets quickly resized as the GUI is built. This
    is not a major issue, but it detracts from the overall presentation of the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can correct this by delaying the `deiconify()` call after login using `after()`.
    Inside `Application.__init__()`, let''s alter that line as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now, instead of immediately restoring the application window after login, we
    have delayed the restore by a quarter of a second. While barely perceptible to
    the user, it gives Tkinter enough time to build and redraw the GUI before displaying
    the window, smoothing out the operation.
  prefs: []
  type: TYPE_NORMAL
- en: Use delayed code sparingly, and don't rely on it in situations where the delayed
    code's stability or security depends on some other process finishing first. This
    can lead to a **race condition**, in which some unforeseen circumstance like a
    slow disk or network connection can cause your delay to be insufficient to properly
    order the execution of code. In the case of our application, our delay is merely
    a cosmetic fix; nothing disastrous will happen if the application window is restored
    before it finishes drawing.
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating GUI freezes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because callback tasks are prioritized over screen-updating tasks, a callback
    task that blocks code execution for an extended period of time can cause the program
    to seem frozen or stuck at an awkward point while the redrawing tasks wait for
    it to complete. One way to address this is to use the `after()` and `update()`
    methods to control the event queue processing manually. To see how this works,
    we'll build a simple application that uses these methods to keep the UI responsive
    during a long-running task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with this simple, but slow, application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This application uses `time.sleep()` to simulate some heavy processing task
    done in multiple phases. The GUI presents the user with a button, which launches
    the processes, and a status indicator to show progress.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the user clicks the button, the status indicator is *supposed* to do the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Show **Starting process** for 2 seconds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Show **Phase 1**, **Phase 2**, through **Phase 4** for 2 seconds each.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, it should read **Complete**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you try it, though, you'll see it does no such thing. Instead, it freezes
    up the moment the button goes down and does not unfreeze until all the phases
    are complete and the status reads **Complete**. Why does this happen?
  prefs: []
  type: TYPE_NORMAL
- en: When the button-click event is processed by the main loop, the `run_process()`
    callback takes priority over any drawing tasks (since those are idle tasks) and
    is immediately executed, blocking the main loop until it returns. When the callback
    calls `self.status.set()`, the `status` variable's write events are placed in
    the queue (where they will eventually trigger a redraw event on the `Label` widget).
    However, processing of the queue is currently halted, waiting on the `run_process()`
    method to return. When it finally does return, all the updates to `status` that
    were waiting in the event queue are executed in a fraction of a second.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make this a bit better, let''s schedule `run_process()` using `after()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This time, the loop part of `run_process()` is split off into a separate method
    called `_run_phases()`. The `run_process()` method itself just sets the starting
    status, then schedules `_run_phases()` to run 50 milliseconds later. This delay
    gives Tkinter time to finish up any drawing tasks and to update the status before
    kicking off the long blocking loop. The exact amount of time isn't critical in
    this case, just so long as it's sufficient for Tkinter to finish drawing operations,
    but short enough that users aren't likely to notice it; 50 milliseconds seems
    to do the job just fine.
  prefs: []
  type: TYPE_NORMAL
- en: We still aren't seeing individual phase status messages with this version, though;
    it goes directly from **Starting process** to **Complete** because the `_run_phases()`
    method is still blocking the event loop when it eventually runs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To fix this, we can use `update_idletasks()` in the loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'By forcing Tkinter to run the remaining idle tasks in the queue before starting
    the long blocking method, our GUI kept is up to date. Unfortunately, there are
    some shortcomings to this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, the individual tasks still block the application while they're running.
    No matter how we break them up, the application will still be frozen while the
    individual units of the process are executing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, this approach is problematic for separation of concerns. In a real
    application, our processing phases are likely going to be running in a backend
    or model class of some kind. Those classes should not be manipulating GUI widgets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While these queue control methods can be useful for managing GUI-layer processes,
    it''s clear we need a better solution for working with slow background processes
    like the ABQ network upload functions. For those, we''ll need to use something
    more powerful: threads.'
  prefs: []
  type: TYPE_NORMAL
- en: Running code in the background with threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All of the code we have written up to this point in the book can be described
    as **single-threaded**; that is, every statement is executed one at a time, the
    prior statement finishing before the next one is begun. Even asynchronous elements
    such as our Tkinter event queue, though they may change the order in which tasks
    are executed, still execute only one task at a time. This means that a long-running
    procedure like a slow network transaction or file read will unavoidably freeze
    up our application while it runs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see this in action, run the `sample_rest_service.py` script included with
    the example code for *Chapter 14* (make sure you run the *Chapter 14* version,
    not the *Chapter 13* version!). Now run ABQ Data Entry, make sure you''ve got
    some data in the database for today, and run the REST upload. The upload should
    take about 20 seconds, during which time the service script should be printing
    status messages like these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Meanwhile, though, our GUI application is frozen. You'll find you cannot interact
    with any of the controls, and moving or resizing it may result in a blank gray
    window. Only when the upload process completes will your application become responsive
    again.
  prefs: []
  type: TYPE_NORMAL
- en: To truly get around this problem, we need to create a **multi-threaded** application,
    in which multiple pieces of code can be run concurrently without needing to wait
    for one another. In Python, we can do this using the `threading` module.
  prefs: []
  type: TYPE_NORMAL
- en: The threading module
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multi-threaded application programming can be quite challenging to grasp fully,
    but the standard library's `threading` module makes working with threads about
    as simple as it can be.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate the basic use of `threading`, let''s first create an intentionally
    slow function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This function takes a string and prints it at a rate of one word per second.
    This will simulate a long-running, computationally expensive process and give
    us some feedback that it's still running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a Tkinter GUI frontend for this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This simple application has a text entry and a button; when the button is pushed,
    the text in the entry is sent to the `print_slowly()` function. Run this code,
    then enter or paste a long sentence into the `Entry` widget.
  prefs: []
  type: TYPE_NORMAL
- en: When you click the button, you'll see that the entire application freezes up
    as the words are printed to the console. That's because it's all running in a
    single execution thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s add the threading code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This time, we've imported the `Thread` class and created a new callback called
    `print_threaded()`. This callback uses a `Thread` object to run `print_slowly()`
    in its own execution thread.
  prefs: []
  type: TYPE_NORMAL
- en: A `Thread` object takes a `target` argument that points to the callable which
    will be run in the new execution thread. It can also take an `args` tuple, which
    contains arguments to be passed into the `target` argument, and a `kwargs` dictionary,
    which will also be expanded in the `target` function's argument list.
  prefs: []
  type: TYPE_NORMAL
- en: To execute the `Thread` object, we call its `start()` method. This method does
    not block, so the `print_threaded()` callback immediately returns, allowing Tkinter
    to resume its event loop while `thread` executes in the background.
  prefs: []
  type: TYPE_NORMAL
- en: If you try this code, you'll see that the GUI no longer freezes while the sentence
    is printed. No matter how long the sentence, the GUI remains responsive the whole
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Tkinter and thread safety
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Threading introduces a great deal of complication into a code base, and not
    all code is written to behave properly in a multi-threaded environment.
  prefs: []
  type: TYPE_NORMAL
- en: We refer to code that is written with threading in mind as being **thread-safe**.
  prefs: []
  type: TYPE_NORMAL
- en: It's often repeated that Tkinter is not thread-safe; this isn't entirely true.
    Assuming your Tcl/Tk binaries have been compiled with thread support (which those
    included with the official Python distributions for Linux, Windows, and macOS
    have been), Tkinter should work fine in a multi-threaded program. However, the
    Python documentation warns us that there are still some edge cases where Tkinter
    calls across threads do not behave properly.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to avoid these issues is to keep our Tkinter code within a single
    thread and restrict our use of threads to non-Tkinter code (such as our model
    classes).
  prefs: []
  type: TYPE_NORMAL
- en: More information about Tkinter and threading can be found at [https://docs.python.org/3/library/tkinter.html#threading-model](https://docs.python.org/3/library/tkinter.html#threading-model).
  prefs: []
  type: TYPE_NORMAL
- en: Converting our network functions to threaded execution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Passing a function to a `Thread` object's `target` argument is one way of running
    code in a thread; a more flexible and powerful approach is to subclass the `Thread`
    class and override its `run()` method with the code you want to execute. To demonstrate
    this approach, let's update the corporate REST upload feature we created for ABQ
    Data Entry in *Chapter 13*, *Connecting to the Cloud*, so that it runs the slow
    upload operation in a separate thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, open up `models.py` and let''s import the `Thread` class, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Rather than having a `CorporateRestModel` method execute the upload, we're going
    to create a class based on `Thread` whose instances will be able to execute the
    upload operation in a separate thread. We'll call it `ThreadedUploader`.
  prefs: []
  type: TYPE_NORMAL
- en: To execute its upload, the `ThreadedUploader` instance will need an endpoint
    URL and a local file path; we can simply pass those to the object in its initializer.
    It will also need access to an authenticated session; that presents more of a
    problem. We might be able to get away with passing our authenticated `Session`
    object to the thread, but at the time of writing there is a great deal of uncertainty
    as to whether `Session` objects are thread-safe, so it's best to avoid sharing
    them between threads.
  prefs: []
  type: TYPE_NORMAL
- en: However, we don't really need the whole `Session` object, just the authentication
    token or session cookie.
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out that when we authenticate to the REST server, a cookie called
    `session` is placed in our cookie jar, which we can see by inspecting the `Session.cookies`
    object from a terminal, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `cookies` attribute is a `requests.CookieJar` object, which behaves in many
    ways like a dictionary. Each cookie has a unique name, which can be used to retrieve
    the cookie itself. In this case, our session cookie is called `session`.
  prefs: []
  type: TYPE_NORMAL
- en: Since the cookie itself is just a string, we can safely pass it to another thread.
    Once there, we'll create a new `Session` object and give it the cookie, after
    which it can authenticate requests.
  prefs: []
  type: TYPE_NORMAL
- en: Immutable objects, including strings, integers, and floats, are always thread-safe.
    Since immutable objects can't be altered after creation, we don't have to worry
    that two threads will try to change the object at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start our new uploader class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The initializer method starts by calling the superclass initializer to set up
    the `Thread` object, then assigns the passed `files_url` and `filepath` strings
    to instance attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a new `Session` object and add the passed cookie value to the
    cookie jar by assigning it to the `session` key (the same key used in the original
    session''s cookie jar). Now we have all the information we need to execute an
    upload process. The actual process to be executed in the thread is implemented
    in its `run()` method, which we''ll add next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note that this code is essentially the code from the model's `upload()` method,
    except that the function arguments have been changed to instance properties.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's head over to our model and see how we can use this class.
  prefs: []
  type: TYPE_NORMAL
- en: The Python documentation recommends that you *only* override `run()` and `__init__()`
    when subclassing `Thread`. Other methods should be left alone for proper operation.
  prefs: []
  type: TYPE_NORMAL
- en: Using the threaded uploader
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we''ve created a threaded uploader, we just need to make `CorporateRestModel`
    use it. Find your model class and let''s rewrite the `upload_file()` method as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here, we start by extracting the session cookie from our `Session` object, then
    pass it along with the URL and the file path to the `ThreadedUploader` initializer.
    Finally, we call the thread's `start()` method to begin execution of the upload.
  prefs: []
  type: TYPE_NORMAL
- en: Now, give your REST upload another try and you'll see that the application doesn't
    freeze up. Great job! However, it doesn't quite behave how we'd like it to yet...
  prefs: []
  type: TYPE_NORMAL
- en: Remember, you override the `run()` method, but call the `start()` method. Mixing
    these up will cause your code to either do nothing or block like a normal single-threaded
    call.
  prefs: []
  type: TYPE_NORMAL
- en: Passing messages using a queue
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We've solved the problem of the program freezing up, but now we have some new
    problems. The most obvious problem is that our callback immediately shows a message
    box claiming that we've successfully uploaded the file, even though you can see
    from the server output that the process is still ongoing in the background. A
    subtler and far worse problem is that we aren't alerted to errors. If you try
    terminating the test service while the upload is running (so that the callback
    should fail), it will still immediately claim that the upload succeeded, even
    though you can see on the terminal that exceptions are being raised. What's going
    on here?
  prefs: []
  type: TYPE_NORMAL
- en: The first problem here is that the `Thread.start()` method doesn't block code
    execution. This is what we wanted, of course, but it now means our success dialog
    isn't waiting until the upload process is complete before it displays. As soon
    as the new thread is launched, the execution of code in the main thread continues
    in parallel with the new thread, immediately showing the success dialog.
  prefs: []
  type: TYPE_NORMAL
- en: The second problem is that code running in its own thread cannot pass exceptions
    caused in the thread's `run()` method back to the main thread. Those exceptions
    are raised within the new thread, and can only be caught in the new thread. As
    far as our main thread is concerned, the code in the `try` block executed just
    fine. In fact, the upload operation can't communicate failures *or* successes.
  prefs: []
  type: TYPE_NORMAL
- en: In order to solve these problems, we need a way for the GUI and model threads
    to communicate, so that the upload thread can send error or progress messages
    back to the main thread to be handled appropriately. We can do this using a **queue**.
  prefs: []
  type: TYPE_NORMAL
- en: The Queue object
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Python''s `queue.Queue` class provides a **first-in first-out** (**FIFO**)
    data structure. Python objects can be placed into a `Queue` object using the `put()`
    method, and retrieved using the `get()` method; to see how this works, execute
    this in the Python shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This may not seem terribly exciting; after all, you can do essentially the same
    thing with a `list` object. What makes `Queue` useful, though, is that it is thread-safe.
    One thread can place messages on the queue, and another can retrieve them and
    respond appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: By default, the queue's `get()` method will block execution until an item is
    received. This behavior can be altered by passing `False` as its first argument,
    or using the `get_nowait()` method. In no-wait mode, the method will return immediately,
    raising an exception if the queue is empty.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how this works, execute the following in the shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also check whether the queue is empty using the `empty()` or `qsize()`
    methods; for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, `empty()` returns a Boolean indicating if the queue is empty,
    and `qsize()` returns the number of items in the queue. `Queue` has several other
    methods that are useful in more advanced multi-threading situations, but `get()`,
    `put()`, and `empty()` will be sufficient to solve our problems.
  prefs: []
  type: TYPE_NORMAL
- en: Using queues to communicate between threads
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before editing our application code, let's create a simple example application
    to make sure we understand how to use `Queue` to communicate between threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with a long-running thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `Backend` object is a subclass of `Thread` that takes a `Queue` object as
    an argument and saves it as an instance property. Its `run()` method simulates
    a long-running four-phase process using `print()` and `sleep()`. At the beginning,
    at the end, and before each phase, we use `queue.put()` to place a status message
    into the queue module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we''ll create a frontend for this process in Tkinter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This simple application contains a `Label` object bound to a `status` control
    variable, a `Button` widget bound to a callback method, `go()`, and a `Queue`
    object stored as an instance variable. The idea is that, when we click the **Run
    process** button, the `go()` method will run our `Backend` class and the queued
    messages will be displayed in the label by way of the `status` control variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create the `go()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `go()` method creates an instance of the `Backend` class, passing in the
    application's `Queue` object, and starts it. Because both threads now have a reference
    to `queue`, we can use it to communicate between them. We've already seen how
    `Backend` places status messages on the queue, so how should `App()` retrieve
    them?
  prefs: []
  type: TYPE_NORMAL
- en: 'Maybe we could start a loop, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: That won't work, of course, because the loop will block; the Tkinter event loop
    would be stuck executing `go()`, freezing up the GUI and defeating the purpose
    of using a second thread. Instead, we need a way to periodically poll the `queue`
    object for status messages and update the status whenever one is received.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by writing a method that can check the queue and respond appropriately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Using the `Queue.empty()` method, we first find out if the queue is empty or
    not. If it is, we don't want to do anything, because `get()` will, by default,
    block until it receives a message, and we don't want to block execution. If the
    `queue` object contains items, we'll want to get those items and send them to
    our `status` variable. We're doing this in a `while` loop so that we only leave
    the function when the queue is empty.
  prefs: []
  type: TYPE_NORMAL
- en: This only performs one check, of course; we want to keep polling the queue module
    until the thread sends a `done` message. Thus, if our status is not `done`, we
    need to schedule another queue check.
  prefs: []
  type: TYPE_NORMAL
- en: 'That can be done with a call to `after()` at the end of `check_queue()`, like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now `check_queue()` will do its job, then schedule itself to run again every
    `100` milliseconds until the status is `done`. All that remains is to kick off
    the process at the end of `go()`, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: If you run this application, you'll see that we get status messages in (relatively)
    real time. Unlike the single-threaded application we created earlier in the chapter,
    there is no freezing, even while the tasks are running.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a communication queue to our threaded uploader
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s apply our knowledge of queues to fix the problems with the `ThreadedUploader`
    class. To begin, we''ll update the initializer signature so that we can pass in
    a `Queue` object, then store the object as an instance attribute, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Just as we did in our example application, we''ll create the `Queue` object
    in the `CorporateRestModel` object so that both the uploader and the model have
    a reference to it. In addition, we''ll save the queue as a public attribute of
    the model so that the application object can also reference it. To do that, we''ll
    first need to import `Queue` into `models.py`, so add this import at the top:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, back down in the `CorporateRestModel` initializer, create a `Queue` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to update the `upload_file()` method so that it passes the queue
    into the `ThreadedUploader` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Now the GUI can access the queue from `rest_model.queue`, and we can use that
    connection to send messages from our upload thread back to the GUI. Before we
    can use that connection, however, we need to develop a communications protocol.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a communications protocol
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have established a channel for inter-thread communication, we have
    to decide how our two threads will communicate. In other words, what exactly will
    our uploader thread place on the queue, and how should our application thread
    respond to it? We could just throw anything into the queue and keep writing `if`
    statements on the app-side to deal with whatever shows up, but a better approach
    is to standardize communications by defining a simple protocol.
  prefs: []
  type: TYPE_NORMAL
- en: Our uploader thread will mainly be sending status-related information back to
    the application so that it can display updates about what's happening in message
    boxes or on the status bar. We will create a message format that we can use to
    determine what the thread is doing and communicate that to the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'The message structure will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Field | Description |'
  prefs: []
  type: TYPE_TB
- en: '| `status` | One word indicating the type of message, such as info or error
    |'
  prefs: []
  type: TYPE_TB
- en: '| `subject` | A short sentence summarizing the message |'
  prefs: []
  type: TYPE_TB
- en: '| `body` | A longer string with details about the message |'
  prefs: []
  type: TYPE_TB
- en: We could create a structure like this using dictionary or a class, but simple
    collections of named fields like this are a great use-case for **named tuples**.
    The `collections.namedtuple()` function allows us to quickly create mini-classes
    that contain only named properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a `namedtuple` class looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This is equivalent to writing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The `namedtuple()` method is much faster to create than a class, and unlike
    a dictionary it enforces uniformity—that is, every `MyClass` object must have
    a `prop1` and a `prop2` attribute, whereas a dictionary is never required to have
    any particular keys.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the top of the `models.py` file, let''s import `namedtuple` and use it to
    define a class called `Message`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we''ve created the `Message` class, making a new `Message` object
    is just like making an instance of any other class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Let's implement the use of these `Message` objects in our queue.
  prefs: []
  type: TYPE_NORMAL
- en: Sending messages from the uploader
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have established a protocol, it''s time to put it to use. Locate
    the `ThreadedUploader` class, and let''s update the `run()` method to send messages,
    starting with an informational message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Our first message is just an informational message indicating that the upload
    is starting. Next, we''ll begin the upload and return some messages indicating
    the success or failure of the operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: As before, we begin the upload process by opening the file and making our `PUT`
    request to the web service. This time, though, we run `raise_for_status()` in
    a `try` block. If we catch an exception from the operation, we put a message with
    a status of `error` on the queue along with the text of the exception. If we succeed,
    we place a success message on the queue.
  prefs: []
  type: TYPE_NORMAL
- en: That's all that our `ThreadedUploader` needs to do; now we need to head over
    the GUI to implement a response to these messages.
  prefs: []
  type: TYPE_NORMAL
- en: Handling queue messages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Back in the `Application` object, we need to add some code to monitor the queue
    and take appropriate actions when a message is sent from the thread. As we did
    in our queue demo application, we'll create a method that uses the Tkinter event
    loop to periodically poll the queue and handle any messages sent from the model's
    queue object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the `Application._check_queue()` method like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The method accepts a `Queue` object, and starts by checking to see whether it
    has any items. If so, it retrieves one. Once we have one, we need to examine it
    and determine what to do with it based on the `status` value.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s handle a `done` status; add this code under the `if` block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: When our upload finishes successfully, we want to show a message box and set
    the status, then return without doing anything else.
  prefs: []
  type: TYPE_NORMAL
- en: The `Message` object's `status`, `subject`, and `body` attributes map nicely
    to the `title`, `message`, and `detail` arguments of the message box, so we've
    just passed those directly to it. We also show the subject of the message in the
    application's status bar by setting the `status` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll handle `error` messages from the queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Once again, we show a message box, this time using `showerror()`. We also want
    to exit the method, since the thread has presumably quit and we don't need to
    schedule the next queue check.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s handle the `info` statuses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Informational messages don't really warrant a modal message box, so we're just
    sending them to the status bar.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last thing we need to do in this method is make sure it gets called again
    if the thread is still going. Since `done` and `error` messages cause the method
    to return, if we''ve reached this point in the function the thread is still running
    and we should continue to poll it. So, we''ll add a call to `after()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'With `_check_queue()` written, we just need to eliminate the exception handling
    around `rest_model.upload_file()` at the end of `_upload_to_corporate_rest()`
    and call `_check_queue()` instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This call doesn't need to be scheduled with `after()` since there will most
    likely not be a message on the first call, causing `_check_queue()` to just schedule
    its next call and return.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've finished that update, launch the test server and the application
    and try the REST upload again. Watch the status bar and you'll see the progress
    getting displayed, ending with a message box when the process completes. Try it
    with the HTTP server turned off, and you should see an error message pop up right
    away.
  prefs: []
  type: TYPE_NORMAL
- en: Using locks to protect shared resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While it's great that our application no longer freezes up during slow file
    uploads, it raises a potential problem. Suppose a user tries to start a second
    REST upload while the first is ongoing? Go ahead and try this; launch the sample
    HTTP server and the application, and try to launch two REST uploads in quick succession,
    so that the second begins before the first finishes. Note the output from the
    REST server; depending on your timing, you may see confusing log messages with
    percentages going up and down as both threads upload files at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, our sample REST server only simulates a slow link with `sleep()`;
    the actual file upload happens so fast it's unlikely to cause a problem. In a
    situation with a genuinely slow network, concurrent uploads could be more problematic.
    While it's possible that the receiving server is robust enough to sensibly handle
    two threads trying to upload the same file, it's best if we avoid that situation
    in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: What we need is some kind of flag that is shared between threads which can indicate
    if a thread is currently uploading so that others will know not to do so. We can
    do this using the `threading` module's `Lock` object.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Lock object
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A lock is a very simple object with two states: **acquired** and **released**.
    When a `Lock` object is in the released state, any thread may call its `acquire()`
    method to put it in the acquired state. Once a thread has acquired the lock, the
    `acquire()` method will block until the lock has been released by calling its
    `release()` method. That means that if another thread calls `acquire()`, its execution
    will wait until the lock is released by the first thread.'
  prefs: []
  type: TYPE_NORMAL
- en: To see how this works, look at the `basic_threading_demo.py` script we created
    earlier in this chapter. Run that script from a terminal prompt, enter a sentence
    into the `Entry` widget, and click the **Run threaded** button.
  prefs: []
  type: TYPE_NORMAL
- en: As we noted earlier, the sentence prints out at one word per second to the terminal
    output. But now, click the **Run threaded** button twice in quick succession.
    Notice that the output is a jumble of repeated words as the two threads simultaneously
    output text to the terminal. You can just imagine the havoc multiple threads could
    wreak upon a file or network session in a situation like this.
  prefs: []
  type: TYPE_NORMAL
- en: 'To correct this, let''s create a lock. First, import `Lock` from the `threading`
    module and create an instance of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, inside the `print_slowly()` function, let''s add calls to `acquire()`
    and `release()` around the method, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Save this file as `basic_threading_demo_with_lock.py` and run it again. Now,
    when you click the **Run threaded** button multiple times, each run waits for
    the previous one to release the lock before beginning. In this way, we can force
    threads to wait for each other while still maintaining a responsive application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Lock` object can also be used as a context manager so that `acquire()`
    is called on entering the block and `release()` upon exiting. Thus, we could rewrite
    the preceding example like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Using a Lock object to prevent concurrent uploads
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s apply our understanding of the `Lock` object to prevent concurrent uploading
    to the corporate REST server. To begin, we need to import `Lock` into `models.py`,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll create a `Lock` object as a class attribute of the `ThreadedUploader`
    class, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Recall from *Chapter 4*, *Organizing Our Code with Classes*, that objects assigned
    to class attributes are shared by all instances of the class. Therefore, by creating
    the lock as a class attribute, any `ThreadedUploader` thread will have access
    to the lock.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, inside the `run()` method, we need to utilize our lock. The cleanest approach
    is to use it as a context manager, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Whether the `put()` call returns or raises an exception, the context manager
    will ensure that `release()` is called when the block exits so that other calls
    to `run()` can acquire the lock.
  prefs: []
  type: TYPE_NORMAL
- en: After adding this code, run the test HTTP server and the application again and
    try launching two REST uploads in quick succession. Now you should see that the
    second upload doesn't start until the first has completed.
  prefs: []
  type: TYPE_NORMAL
- en: Threading and the GIL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whenever we discuss threading in Python, it's important to understand Python's
    **Global Interpreter Lock** (**GIL**) and how it affects threading.
  prefs: []
  type: TYPE_NORMAL
- en: The GIL is a lock mechanism that protects Python's memory management by preventing
    more than one thread from executing Python commands at the same time. Similar
    to the lock we implemented in our `ThreadedUploader` class, the GIL can be thought
    of like a token that can be held by only one thread at a time; whichever thread
    holds the token may execute Python instructions, and the rest have to wait.
  prefs: []
  type: TYPE_NORMAL
- en: 'It may seem like this defeats the idea of multi-threading on Python, However,
    there are two factors that mitigate the impact of the GIL:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the GIL only limits the execution of *Python* code; many libraries execute
    code in other languages. For example, Tkinter executes TCL code, and `psycopg2`
    executes compiled C code. Non-Python code like this can run in a separate thread
    while Python code runs in another.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, **Input/Output** (**I/O**) operations like disk access or network requests
    can run concurrently with Python code. For instance, when we make an HTTP request
    using `requests`, the GIL is released while waiting for the server to respond.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The only situation where the GIL really limits the utility of multi-threading
    is when we have computationally expensive Python code. Slow operations in typical
    data-oriented applications like ABQ are likely to be I/O-based operations, and
    for heavy-computation situations we can use non-Python libraries like `numpy`.
    Even so, it's good to be aware of the GIL and know that it may impact the effectiveness
    of a multi-threaded design.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to use asynchronous and multi-threaded programming
    techniques to remove unresponsive behavior from your program. You learned how
    to work with and control Tkinter's event queue using the `after()` and `update()`
    methods, and how to apply these methods to solve problems in your application.
    You also learned how to use Python's `threading` module to run processes in the
    background, and how to utilize `Queue` objects to communicate between threads.
    Finally, you learned to use the `Lock` object to prevent shared resources from
    getting corrupted.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we''re going to explore the most powerful widget in Tkinter:
    the Canvas. We''ll learn how to draw images and animate them, and create useful
    and informative charts.'
  prefs: []
  type: TYPE_NORMAL
