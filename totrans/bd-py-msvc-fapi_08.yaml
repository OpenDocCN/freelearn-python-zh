- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating Coroutines, Events, and Message-Driven Transactions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The FastAPI framework is an asynchronous framework that runs over the asyncio
    platform, which utilizes the ASGI protocol. It is well known for its 100% support
    for asynchronous endpoints and non-blocking tasks. This chapter will focus on
    how we create highly scalable applications with asynchronous tasks and event-driven
    and message-driven transactions.
  prefs: []
  type: TYPE_NORMAL
- en: We learned in [*Chapter 2*](B17975_02.xhtml#_idTextAnchor033)*, Exploring the
    Core Features*, that *Async/Await* or asynchronous programming is a design pattern
    that enables other services or transactions to run outside the main thread. The
    framework uses the `async` keyword to create asynchronous processes that will
    run on top of other thread pools and will be *awaited*, instead of invoking them
    directly. The number of external threads is defined during the Uvicorn server
    startup through the `--worker` option.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will delve into the framework and scrutinize the various
    components of the FastAPI Framework that can run asynchronously using multiple
    threads. The following highlights will help us understand how asynchronous FastAPI
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing coroutines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating asynchronous background tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Celery tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building message-driven transactions using RabbitMQ
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building publish/subscribe messaging using Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying reactive programming in tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing asynchronous **Server-Sent Events** (**SSE**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an asynchronous WebSocket
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will cover asynchronous features, software specifications, and
    the components of a *newsstand management system* prototype. The discussions will
    use this online newspaper management system prototype as a specimen to understand,
    explore, and implement asynchronous transactions that will manage the *newspaper
    content*, *subscription*, *billing*, *user profiles*, *customers*, and other business-related
    transactions. The code has all been uploaded to [https://github.com/PacktPublishing/Building-Python-Microservices-with-FastAPI](https://github.com/PacktPublishing/Building-Python-Microservices-with-FastAPI)
    under the `ch08` project.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing coroutines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the FastAPI framework, a *thread pool* is always present to execute both
    synchronous API and non-API transactions for every request. For ideal cases where
    both the transactions have minimal performance overhead with *CPU-bound* and *I/O-bound*
    transactions, the overall performance of using the FastAPI framework is still
    better than those frameworks that use non-ASGI-based platforms. However, when
    contention occurs due to high CPU-bound traffic or heavy CPU workloads, the performance
    of FastAPI starts to wane due to *thread switching*.
  prefs: []
  type: TYPE_NORMAL
- en: Thread switching is a context switch from one thread to another within the same
    process. So, if we have several transactions with varying workloads running in
    the background and on the browser, FastAPI will run these transactions in the
    thread pool with several context switches. This scenario will cause contention
    and degradation to lighter workloads. To avoid performance issues, we apply *coroutine
    switching* instead of threads.
  prefs: []
  type: TYPE_NORMAL
- en: Applying coroutine switching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The FastAPI framework works at the optimum speed with a mechanism called *coroutine
    switching*. This approach allows transaction-tuned tasks to work cooperatively
    by allowing other running processes to pause so that the thread can execute and
    finish more urgent tasks, and resume "awaited" transactions without preempting
    the thread. These coroutine switches are programmer-defined components and not
    kernel-related or memory-related features. In FastAPI, there are two ways of implementing
    coroutines: (a) applying the `@asyncio.coroutine` decorator, and (b) using the
    `async`/`await` construct.'
  prefs: []
  type: TYPE_NORMAL
- en: Applying @asyncio.coroutine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`asyncio` is a Python extension that implements the Python concurrency paradigm
    using a single-threaded and single-process model and provides API classes and
    methods for running and managing coroutines. This extension provides an `@asyncio.coroutine`
    decorator that transforms API and native services into *generator-based coroutines*.
    However, this is an old approach and can only be used in FastAPI that uses Python
    3.9 and below. The following is a login service transaction of our *newsstand
    management system* prototype implemented as a coroutine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`build_user_list()` is a native service that converts all login records into
    the `str` format. It is decorated with the `@asyncio.coroutine` decorator to transform
    the transaction into an asynchronous task or coroutine. A coroutine can invoke
    another coroutine function or method using only the `yield from` clause. This
    construct pauses the coroutine and passes the control of the thread to the coroutine
    function invoked. By the way, the `asyncio.sleep()` method is one of the most
    widely used asynchronous utilities of the `asyncio` module, which can pause a
    process for a few seconds, but is not the ideal one. On the other hand, the following
    code is an API service implemented as a coroutine that can minimize contention
    and performance degradation in client-side executions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `list_login()` API service retrieves all the login details of the application’s
    users through a coroutine CRUD transaction implemented in *GINO ORM*. The API
    service again uses the `yield from` clause to run and execute the `get_all_login()`
    coroutine function.
  prefs: []
  type: TYPE_NORMAL
- en: 'A coroutine function can invoke and await multiple coroutines concurrently
    using the `asyncio.gather()` utility. This `asyncio` method manages a list of
    coroutines and waits until all its coroutines have completed their tasks. Then,
    it will return a list of results from the corresponding coroutines. The following
    code is an API that retrieves login records through an asynchronous CRUD transaction
    and then invokes `count_login()` and `build_user_list()` concurrently to process
    these records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`list_login_records()` uses `asyncio.gather()` to run the `count_login()` and
    `build_user_list()` tasks and later extract their corresponding returned values
    for processing.'
  prefs: []
  type: TYPE_NORMAL
- en: Using the async/await construct
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another way of implementing a coroutine is using `async`/`await` constructs.
    As with the previous approach, this syntax creates a task that can pause anytime
    during its operation before it reaches the end. But the kind of coroutine that
    this approach produces is called a *native coroutine*, which is not iterable in
    the way that the generator type is. The `async`/`await` syntax also allows the
    creation of other asynchronous components such as the `async with` context managers
    and `async for` iterators. The following code is the `count_login()` task previously
    invoked in the generator-based coroutine service, `list_login_records()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `count_login()` native service is a native coroutine because of the `async`
    keyword placed before its method definition. It only uses `await` to invoke other
    coroutines. The `await` keyword suspends the execution of the current coroutine
    and passes the control of the thread to the invoked coroutine function. After
    the invoked coroutine finishes its process, the thread control will yield back
    to the caller coroutine. Using the `yield from` construct instead of `await` will
    raise an error because our coroutine here is not generator-based. The following
    is an API service implemented as a native coroutine that manages data entry for
    the new administrator profiles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Both generator-based and native coroutines are monitored and managed by an *event
    loop*, which represents an infinite loop inside a thread. Technically, it is an
    object found in the *thread*, and each *thread* in the thread pool can only have
    one event loop, which contains a list of helper objects called *tasks*. Each task,
    pre-generated or manually created, executes one coroutine. For instance, when
    the previous `add_admin()` API service invokes the `insert_admin()` coroutine
    transaction, the event loop will suspend `add_admin()` and tag its task as an
    *awaited* task. Afterward, the event loop will assign a task to run the `insert_admin()`
    transaction. Once the task has completed its execution, it will yield the control
    back to `add_admin()`. The thread that manages the FastAPI application is not
    interrupted during these shifts of execution since it is the event loop and its
    tasks that participate in the *coroutine switching* mechanism. Let us now use
    these coroutines to build our application
  prefs: []
  type: TYPE_NORMAL
- en: Designing asynchronous transactions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a few programming paradigms that we can follow when creating coroutines
    for our application. Utilizing more coroutine switching in the process can help
    improve the software performance. In our *newsstand* application, there is an
    endpoint, `/admin/login/list/enc`, in the `admin.py` router that returns a list
    of encrypted user details. In its API service, shown in the following code, each
    record is managed by an `extract_enc_admin_profile()` transaction call instead
    of passing the whole data record to a single call, thus allowing the concurrent
    executions of tasks. This strategy is better than running the bulk of transactions
    in a thread without *context switches*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the `extract_enc_admin_profile()` coroutine, shown in the following code,
    implements a chaining design pattern, where it calls the other smaller coroutines
    through a chain. Simplifying and breaking down the monolithic and complex processes
    into smaller but more robust coroutines will improve the application’s performance
    by utilizing more context switches. In this API, `extract_enc_admin_profile()`
    creates three context switches in a chain, better than thread switches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, the following implementation is the smaller subroutines
    awaited and executed by `extract_enc_admin_profile()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: These three subroutines will give the main coroutine the encrypted `str` that
    contains the details of an administrator profile. All these encrypted strings
    will be collated by the API service using the `asyncio.gather()` utility.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another programming approach to utilizing the coroutine switching is the use
    of pipelines created by `asyncio.Queue`. In this programming design, the queue
    structure is the common point between two tasks: (a) the task that will place
    a value to the queue called the *producer*, and (b) the task that will fetch the
    item from the queue, the *consumer*. We can implement a *one producer/one consumer*
    interaction or a *multiple producers/multiple consumers* setup with this approach.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code highlights the `process_billing()` native service that builds
    a *producer/consumer* transaction flow. The `extract_billing()` coroutine is the
    producer that retrieves the billing records from the database and passes each
    record one at a time to the queue. `build_billing_sheet()`, on the other hand,
    is the consumer that fetches the record from the queue structure and generates
    the billing sheet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In this programming design, the `build_billing()` coroutine will explicitly
    wait for the record queued by `extract_billing()`. This setup is possible due
    to the `asyncio.create_task()` utility, which directly assigns and schedules a
    task to each coroutine.
  prefs: []
  type: TYPE_NORMAL
- en: 'The queue is the only method parameter common to the coroutines because it
    is their common point. The `join()`of `asyncio.Queue` ensures that all the items
    passed to the pipeline by `extract_billing()` are fetched and processed by `build_billing_sheet()`.
    It also blocks the external controls that would affect the coroutine interactions.
    The following code shows how to create `asyncio.Queue` and schedule a task for
    execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: By the way, always pass `cancel()to` the task right after its coroutine has
    completed the process. On the other hand, we can also apply other ways so that
    the performance of our coroutines can improve.
  prefs: []
  type: TYPE_NORMAL
- en: Using the HTTP/2 protocol
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Coroutine execution can be faster in applications running on the *HTTP/2* protocol.
    We can replace the *Uvicorn* server with *Hypercorn*, which now supports ASGI-based
    frameworks such as FastAPI. But first, we need to install `hypercorn` using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'For *HTTP/2* to work, we need to create an SSL certificate. Using OpenSSL,
    our app has two *PEM* files for our *newsstand* prototype: (a) the private encryption
    (`key.pem`) and (b) the certificate information (`cert.pem`.) We place these files
    in the main project folder before executing the following `hypercorn` command
    to run our FastAPI application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now, let us explore other FastAPI tasks that can also use coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: Creating asynchronous background tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 2*](B17975_02.xhtml#_idTextAnchor033)*, Exploring the Core Features*,
    we first showcased the `BackgroundTasks` injectable API class, but we didn’t mention
    creating asynchronous background tasks. In this discussion, we will be focusing
    on creating asynchronous background tasks using the `asyncio` module and coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: Using the coroutines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The framework supports the creation and execution of asynchronous background
    processes using the `async`/`await` structure. The following native service is
    an asynchronous transaction that generates a billing sheet in CSV format in the
    background:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This `generate_billing_sheet()` coroutine service will be executed as a background
    task in the following API service, `save_vendor_billing()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now, nothing has changed when it comes to defining background processes. We
    usually inject `BackgroundTasks` into the API service method and apply `add_task()`
    to provide task schedules, assignments, and execution for a specific process.
    But since the approach is now to utilize coroutines, the background task will
    use the event loop instead of waiting for the current thread to finish its jobs.
  prefs: []
  type: TYPE_NORMAL
- en: If the background process requires arguments, we pass these arguments to `add_task()`
    right after its *first parameter*. For instance, the arguments for the `billing_date`
    and `query_list` parameters of `generate_billing_sheet()` should be placed after
    the `generate_billing_sheet` injection into `add_task()`. Moreover, the `billing_date`
    value should be passed before the `result` argument because `add_task()` still
    follows the order of parameter declaration in `generate_billing_sheet()` to avoid
    a type mismatch.
  prefs: []
  type: TYPE_NORMAL
- en: All asynchronous background tasks will continuously execute and will not be
    *awaited* even if their coroutine API service has already returned a response
    to the client.
  prefs: []
  type: TYPE_NORMAL
- en: Creating multiple tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`BackgroundTasks` allows the creation of multiple asynchronous transactions
    that will execute concurrently in the background. In the `save_vendor_billing()`
    service, there is another task created for a new transaction called the `create_total_payables_year()`
    transaction, which requires the same arguments as `generate_billing_sheet()`.
    Again, this newly created task will be utilizing the event loop instead of the
    thread.'
  prefs: []
  type: TYPE_NORMAL
- en: The application always encounters performance issues when the background processes
    have high-CPU workloads. Also, tasks generated by `BackgroundTasks` are not capable
    of returning values from the transactions. Let us look for another solution where
    tasks can manage high workloads and execute processes with returned values.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Celery tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Celery* is a non-blocking task queue that runs on a distributed system. It
    can manage asynchronous background processes that are huge and heavy with CPU
    workloads. It is a third-party tool, so we need to install it first through `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'It schedules and runs tasks concurrently on a single server or distributed
    environment. But it requires a message transport to send and receive messages,
    such as *Redis*, an in-memory database that can be used as a message broker for
    messages in strings, dictionaries, lists, sets, bitmaps, and stream types. Also,
    we can install Redis on Linux, macOS, and Windows. Now, after the installation,
    run its `redis-server.exe` command to start the server. In Windows, the Redis
    service is set to run by default after installation, which causes a *TCP bind
    listener* error. So, we need to stop it before running the startup command. *Figure
    8.1* shows Windows **Task Manager** with the Redis service giving a **Stopped**
    status:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Stopping the Redis service'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.01_B17975.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 – Stopping the Redis service
  prefs: []
  type: TYPE_NORMAL
- en: 'After stopping the service, we should now see Redis running as shown in *Figure
    8.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – A running Redis server'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.02_B17975.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 – A running Redis server
  prefs: []
  type: TYPE_NORMAL
- en: Creating and configuring the Celery instance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before creating Celery tasks, we need a Celery instance placed in a dedicated
    module of our application. The *newsstand* prototype has the Celery instance in
    the `/services/billing.py` module, and the following is part of the code that
    shows the process of Celery instantiation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To create the Celery instance, we need the following details:'
  prefs: []
  type: TYPE_NORMAL
- en: The name of the current module containing the Celery instance (the first argument)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The URL of Redis as our message broker (`broker`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The backend result where the results of tasks are stored and monitored (`backend`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The list of other modules used in the message body or by the Celery task (`include`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the instantiation, we need to set the appropriate serializer and content
    types to process the incoming and outgoing message body of the tasks involved,
    if there are any. To allow the passing of full Python objects with non-JSON-able
    values, we need to include `pickle` as a supported content type, then declare
    a default task and result serializer to the object stream. However, using a `pickle`
    serializer poses some security issues because it tends to expose some transaction
    data. To avoid compromising the app, apply sanitation to message objects, such
    as removing sensitive values or credentials, before pursuing the messaging operation.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the serialization options, other important properties such as `task_create_missing_queues`,
    `task_ignore_result`, and error-related configuration should also be part of the
    `CeleryConfig` class. Now, we declare all these details in a custom class, which
    we will inject into the `config_from_object()` method of the Celery instance.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we can create a Celery logger through its `get_task_logger()`with
    the name of the current task.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main goal of the *Celery instance* is to annotate Python methods to become
    tasks. The Celery instance has a `task()` decorator that we can apply to all callable
    procedures we want to define as asynchronous tasks. Part of the `task()` decorator
    is the task’s `name`, an optional unique name composed of the *package*, *module
    name(s)*, and the *method name of the transaction*. It has other attributes that
    can add more refinement to the task definition, such as the `auto_retry` list,
    which registers `Exception` classes that may cause execution retries when emitted,
    and `max_tries`, which limits the number of retry executions of a task. By the
    way, Celery 5.2.3 and below can only define tasks from *non-coroutine methods*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `services.billing.tasks.create_total_payables_year_celery` task shown here
    adds all the payable amounts per date and returns the total amount:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The given task has only five (`5`) retries to recover when it encounters either
    `ValueError` or `TypeError` at runtime. Also, it is a function that returns a
    computed amount, which is impossible to create when using `BackgroundTasks`. All
    functional tasks use the *Redis* database as the temporary storage for their returned
    values, which is the reason there is a backend parameter in the Celery constructor.
  prefs: []
  type: TYPE_NORMAL
- en: Calling the task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'FastAPI services can call these tasks using the `apply_async()`or `delay()`function.
    The latter is the easier option since it is preconfigured and only needs the parameters
    for the transaction to get the result. The `apply_async()` function is a better
    option since it accepts more details that can optimize the task execution. These
    details are `queue`, `time_limit`, `retry`, `ignore_result`, `expires`, and some
    `kwargs` of arguments. But both these functions return an `AsyncResult` object,
    which returns resources such as the task’s `state`, the `wait()` function to help
    the task finish its operation, and the `get()` function to return its computed
    value or an exception. The following code is a coroutine API service that calls
    the `services.billing.tasks.create_total_payables_year_celery` task using the
    `apply_async` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Setting `task_create_missing_queues` to `True` at the `CeleryConfig` setup is
    always recommended because it automatically creates the task `queue`, default
    or not, once the worker server starts. The worker server places all the loaded
    tasks in a task `queue` for execution, monitoring, and result retrieval. Thus,
    we should always define a task `queue` in the `apply_async()` function’s argument
    before extracting `AsyncResult`.
  prefs: []
  type: TYPE_NORMAL
- en: The `AsyncResult` object has a `get()` method that releases the returned value
    of the task from the `AsyncResult` instance, with or without a timeout. In the
    `compute_payables_yearly()` service, the amount payable in `AsyncResult` is retrieved
    by the `get()` function with a timeout of 5 seconds. Let us now deploy and run
    our tasks using the Celery server
  prefs: []
  type: TYPE_NORMAL
- en: Starting the worker server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Running the Celery worker creates a single process that handles and manages
    all the queued tasks. The worker needs to know in which module the Celery instance
    is created, together with the tasks to establish the server process. In our prototype,
    the `services.billing` module is where we place our Celery application. Thus,
    the complete command to start the worker is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Here, `-A` specifies the module of our Celery object and tasks. The `-Q` option
    indicates that the worker will be using a *low-*, *normal-*, *or high-priority*
    queue. But first, we need to set `task_create_missing_queues` to `True` in the
    Celery setup. We also need to indicate the number of threads that the worker needs
    for task execution by adding the `-c` option. The `-P` option specifies the type
    of *thread pool* that the worker will be utilizing. By default, the Celery worker
    uses the `prefork pool` applicable to most CPU-bound transactions. Other options
    are *solo*, *eventlet*, and *gevent*, but our setup will be utilizing *solo*,
    the most suitable choice for running CPU-intensive tasks in a microservice environment.
    On the other hand, the `-l` option enables the logger we set using `get_task_logger()`
    during the setup. Now, there are also ways to monitor our running tasks and one
    of those options is to use the Flower tool.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring the tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Flower* is Celery’s monitoring tool that observes and monitors all tasks executions
    by generating a real-time audit on a web-based platform. But first, we need to
    install it using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'And then, we run the following `celery` command with the `flower` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To view the audit, we run `http://localhost:5555/tasks` on a browser. *Figure
    8.3* shows a *Flower* snapshot of an execution log incurred by the `services.billing.tasks.create_total_payables_year_celery`
    task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – The Flower monitoring tool'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.03_B17975.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 – The Flower monitoring tool
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have used Redis as our in-memory backend database for task results
    and a message broker. Let us now use another asynchronous message broker that
    can replace Redis, *RabbitMQ*.
  prefs: []
  type: TYPE_NORMAL
- en: Building message-driven transactions using RabbitMQ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RabbitMQ is a lightweight asynchronous message broker that supports multiple
    messaging protocols such as *AMQP*, *STOM*, *WebSocket*, and *MQTT*. It requires
    *erlang* before it works properly in Windows, Linux, or macOS. Its installer can
    be downloaded from [https://www.rabbitmq.com/download.html](https://www.rabbitmq.com/download.html).
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Celery instance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of using Redis as the broker, RabbitMQ is a better replacement as a
    message broker that will mediate messages between the client and the Celery worker
    threads. For multiple tasks, RabbitMQ can command the Celery worker to work on
    these tasks one at a time. The RabbitMQ broker is good for huge messages and it
    saves these messages to disk memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, we need to set up a new Celery instance that will utilize the RabbitMQ
    message broker using its *guest* account. We will use the AMQP protocol as the
    mechanism for a producer/consumer type of messaging setup. Here is the snippet
    that will replace the previous Celery configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Redis will still be the backend resource, as indicated in Celery’s `backend_result`,
    since it is still simple and easy to control and manage when message traffic increases.
    Let us now use the RabbitMQ to create and manage message-driven transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring AMQP messaging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can configure the RabbitMQ management dashboard to monitor the messages
    handled by RabbitMQ. After the setup, we can log in to the dashboard using the
    account details to set the broker. *Figure 8.4* shows a screenshot of RabbitMQ’s
    analytics of a situation where the API services called the `services.billing.tasks.create_total_payables_year_celery`
    task several times:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – The RabbitMQ management tool'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.04_B17975.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4 – The RabbitMQ management tool
  prefs: []
  type: TYPE_NORMAL
- en: If the RabbitMQ dashboard fails to capture the behavior of the tasks, the *Flower*
    tool will always be an option for gathering the details about the arguments, `kwargs`,
    UUID, state, and processing date of the tasks. And if RabbitMQ is not the right
    messaging tool, we can always resort to *Apache Kafka*.
  prefs: []
  type: TYPE_NORMAL
- en: Building publish/subscribe messaging using Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with RabbitMQ, *Apache Kafka* is an asynchronous messaging tool used by applications
    to send and store messages between producers and consumers. However, it is faster
    than RabbitMQ because it uses *topics* with partitions where producers can append
    various types of messages across these minute folder-like structures. In this
    architecture, the consumers can consume all these messages in a parallel mode,
    unlike in queue-based messaging, which enables producers to send multiple messages
    to a queue that can only allow message consumption sequentially. Within this publish/subscribe
    architecture, Kafka can handle an exchange of large quantities of data per second
    in continuous and real-time mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three Python extensions that we can use to integrate the FastAPI
    services with Kafka, namely the `kafka-python`, `confluent-kafka`, and `pykafka`
    extensions. Our online *newsstand* prototype will use `kafka-python`, so we need
    to install it using the `pip` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Among the three extensions, it is only with `kafka-python` that we can channel
    and apply Java API libraries to Python for the implementation of a client. We
    can download Kafka from [https://kafka.apache.org/downloads](https://kafka.apache.org/downloads).
  prefs: []
  type: TYPE_NORMAL
- en: Running the Kafka broker and server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kafka has a *ZooKeeper* server that manages and synchronizes the exchange of
    messages within Kafka’s distributed system. The ZooKeeper server runs as the broker
    that monitors and maintains the Kafka nodes and topics. The following command
    starts the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can start the Kafka server by running the following console command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: By default, the server will run on localhost at port `9092`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the topic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When the two servers have started, we can now create a topic called `newstopic`
    through the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `newstopic` topic has three (`3`) partitions that will hold all the appended
    messages of our FastAPI services. These are also the points where the consumers
    will simultaneously access all the published messages.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the publisher
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After creating the topic, we can now implement a producer that publishes messages
    to the Kafka cluster. The `kafka-python` extension has a `KafkaProducer` class
    that instantiates a single thread-safe producer for all the running FastAPI threads.
    The following is an API service that sends a newspaper messenger record to the
    Kafka `newstopic` topic for the consumer to access and process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The coroutine API service, `send_messenger_details()`, asks for details about
    a newspaper messenger and stores them in a `BaseModel` object. And then, it sends
    the dictionary of profile details to the cluster in byte format. Now, one of the
    options to consume Kafka tasks is to run its built-in `kafka-console-consumer.bat`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: Running a consumer on a console
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Running the following command from the console is one way to consume the current
    messages from the `newstopic` topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This command creates a consumer that will connect to the Kafka cluster to read
    in real time the current messages from `newtopic` sent by the producer. *Figure
    8.5* shows the capture of the consumer while it is running on the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – The Kafka consumer'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.05_B17975.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.5 – The Kafka consumer
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want the consumer to read all the messages sent by the producer starting
    from the point where the Kafka server and broker began running, we need to add
    the `--from-beginning` option to the command. The following will read all the
    messages from `newstopic` and continuously capture incoming messages in real time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Another way of implementing a consumer using the FastAPI framework is through
    SSE. Typical API service implementation will not work with the Kafka consumer
    requirement since we need a continuously running service that subscribes to `newstopic`
    for real-time data. So, let us now explore how we create SSE in the FastAPI framework
    and how it will consume Kafka messages.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing asynchronous Server-Sent Events (SSE)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SSE is a server push mechanism that sends data to the browser without reloading
    the page. Once subscribed, it generates event-driven streams in real time for
    various purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating SSE in the FastAPI framework only requires the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The `EventSourceResponse` class from the `sse_starlette.see` module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An event generator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Above all, the framework also allows non-blocking implementation of the whole
    server push mechanism using coroutines that can run even on *HTTP/2*. The following
    is a coroutine API service that implements a Kafka consumer using SSE’s open and
    lightweight protocol:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '`send_message_stream()` is a coroutine API service that implements the whole
    SSE. It returns a special response generated by an `EventSourceResponse` function.
    While the HTTP stream is open, it continuously retrieves data from its source
    and converts any internal events into SSE signals until the connection is closed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, event generator functions create internal events, which
    can also be asynchronous. `send_message_stream()`, for instance, has a nested
    generator function, `event_provider()`, which consumes the last message sent by
    the producer service using the `consumer.poll()` method. If the message is valid,
    the generator converts the message retrieved into a `dict` object and inserts
    all its details into the database through `MessengerRepository`. Then, it yields
    all the internal details for the `EventSourceResponse` function to convert into
    SSE signals. *Figure 8.6* shows the data streams generated by `send_message_stream()`rendered
    from the browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – The SSE data streams'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.06_B17975.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.6 – The SSE data streams
  prefs: []
  type: TYPE_NORMAL
- en: Another way to implement a Kafka consumer is through *WebSocket*. But this time,
    we will focus on the general procedure of how to create an asynchronous WebSocket
    application using the FastAPI framework.
  prefs: []
  type: TYPE_NORMAL
- en: Building an asynchronous WebSocket
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike in SSE, connection in WebSocket is always *bi-directional*, which means
    the server and client communicate with each other using a long TCP socket connection.
    The communication is always in real time and it doesn’t require the client or
    the server to reply to every event sent.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the asynchronous WebSocket endpoint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The FastAPI framework allows the implementation of an asynchronous WebSocket
    that can also run on the *HTTP/2* protocol. The following is an example of an
    asynchronous WebSocket created using the coroutine block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: First, we decorate a coroutine function with `@router.websocket()` when using
    APIRouter, or `@api.websocket()` when using the FastAPI decorator to declare a
    WebSocket component. The decorator must also define a unique endpoint URL for
    the WebSocket. Then, the WebSocket function must have an injected `WebSocket`
    as its first method argument. It can also include other parameters such as query
    and header parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The `WebSocket` injectable has four ways for sending messages, namely `send()`,
    `send_text()`, `send_json()`, and `send_bytes()`. Applying `send()` will always
    manage every message as plain text by default. The previous `customer_list_ws()`coroutine
    is a WebSocket that sends every customer record in JSON format.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, there are also four methods the WebSocket injectable can
    provide, and these are the `receive()`, `receive_text()`, `receive_json()`, and
    `receive_bytes()` methods. The `receive()` method expects the message to be in
    plain-text format by default. Now, our `customer_list_ws()` endpoint expects a
    JSON reply from a client because it invokes the `receive_json()` method after
    its send message operation.
  prefs: []
  type: TYPE_NORMAL
- en: The WebSocket endpoint must close the connection right after its transaction
    is done.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the WebSocket client
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are many ways to create a WebSocket client but this chapter will focus
    on utilizing a coroutine API service that will perform a handshake with the asynchronous
    `customer_list_ws()` endpoint once called on a browser or a `curl` command. Here
    is the code of our WebSocket client implemented using the `websockets` library
    that runs on top of the `asyncio` framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: After a successful handshake is created by the `websockets.connect()` method,
    `customer_list_ws_client()` will have a loop running continuously to fetch all
    incoming consumer details from the WebSocket endpoint. The message received will
    be converted into its dictionary needed by other processes. Now, our client also
    sends an acknowledgment notification message back to the WebSocket coroutine with
    JSON data containing the *customer ID* of the profile. The loop will stop once
    the WebSocket endpoint closes its connection.
  prefs: []
  type: TYPE_NORMAL
- en: Let us now explore other asynchronous programming features that can work with
    the FastAPI framework.
  prefs: []
  type: TYPE_NORMAL
- en: Applying reactive programming in tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reactive programming is a paradigm that involves the generation of streams that
    undergo a series of operations to propagate some changes during the process. Python
    has an *RxPY* library that offers several methods that we can apply to these streams
    asynchronously to extract the terminal result as desired by the subscribers.
  prefs: []
  type: TYPE_NORMAL
- en: In the reactive programming paradigm, all intermediate operators working along
    the streams will execute to propagate some changes if there is an `Observable`
    instance beforehand and an `Observer` that subscribes to this instance. The main
    goal of this paradigm is to achieve the desired result at the end of the propagation
    process using functional programming.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Observable data using coroutines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It all starts with the implementation of a coroutine function that will emit
    these streams of data based on a business process. The following is an `Observable`
    function that emits publication details in `str` format for those publications
    that did well in sales:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'An `Observable` function can be synchronous or asynchronous. Our target is
    to create an asynchronous one such as `process_list()`. The coroutine function
    should have the following callback methods to qualify as an `Observable` function:'
  prefs: []
  type: TYPE_NORMAL
- en: An `on_next()` method that emits items given a certain condition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An `on_completed()` method that is executed once when the function has completed
    the operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An `on_error()` method that is called when an error occurs on `Observable`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our `process_list()` emits the details of the publication that gained some
    profit. Then, we create an `asyncio` task for the call of the `process_list()`
    coroutine. We created a nested function, `evaluate_profit()`, which returns the
    `Disposable` task required by RxPY’s `create()` method for the production of the
    `Observable` stream. The cancellation of this task happens when the `Observable`
    stream is all consumed. The following is the complete implementation for the execution
    of the asynchronous `Observable` function and the use of the `create()` method
    to generate streams of data from this `Observable` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The subscriber created by `create_observable()`is our application’s `list_sales_by_quota()`
    API service. It needs to get the current event loop running for the method to
    generate the observable. Afterward, it invokes the `subscribe()` method to send
    a subscription to the stream and extract the needed result. The Observable’s `subscribe()`
    method is invoked for a client to subscribe to the stream and observe the occurring
    propagations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The `list_sales_by_quote()` coroutine service shows us how to subscribe to
    an `Observable`. A subscriber should utilize the following callback methods:'
  prefs: []
  type: TYPE_NORMAL
- en: An `on_next()` method to consume all the items from the stream
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An `on_completed()` method to indicate the end of the subscription
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An `on_error()` method to flag an error during the subscription process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And since the `Observable` processes run asynchronously, the scheduler is an
    optional argument that provides the right manager to schedule and run these processes.
    The API service used `AsyncIOScheduler` as the appropriate schedule for the subscription.
    But there are other shortcuts to generating Observables that do not use a custom
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Creating background process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As when we create continuously running Observables, we use the `interval()`
    function instead of using a custom `Observable` function. Some observables are
    designed to end successfully, but some are created to run continuously in the
    background. The following Observable runs in the background periodically to provide
    some updates on the total amount received from newspaper subscriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The `interval()` method creates a stream of data periodically in seconds. But
    this Observable imposes some propagations on its stream because of the execution
    of the `pipe()` method. The Observable’s `pipe()` method creates a pipeline of
    reactive operators called the intermediate operators. This pipeline can consist
    of a chain of operators running one at a time to change items from the streams.
    It seems that this series of operations creates multiple subscriptions on the
    subscriber. So, `fetch_records()` has a `map()` operator in its pipeline to extract
    the result from the `compute_subcription()` method. It uses `merge_all()` at the
    end of the pipeline to merge and flatten all substreams created into one final
    stream, the stream expected by the subscriber. Now, we can also generate Observable
    data from files or API response.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing API resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another way of creating an Observable is using the `from_()` method, which extracts
    resources from files, databases, or API endpoints. The Observable function retrieves
    its data from a JSON document generated by an API endpoint from our application.
    The assumption is that we are running the application using `hypercorn`, which
    uses *HTTP/2*, and so we need to bypass the TLS certificate by setting the `verify`
    parameter of `httpx.AsyncClient()` to `False`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code highlights the `from_()` in the `fetch_subscription()` operation,
    which creates an Observable that emits streams of `str` data from the `https://localhost:8000/ch08/subscription/list/all`
    endpoint. These reactive operators of the Observable, namely `filter()`, `map()`,
    and `merge_all()`, are used to propagate the needed contexts along the stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The `filter()` method is another pipeline operator that returns Boolean values
    from a validation rule. It executes the following `filter_within_dates()` to verify
    whether the record retrieved from the JSON document is within the date range specified
    by the subscriber:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, the following `convert_str()` is a coroutine function executed
    by the `map()` operator to generate a concise profile detail of the newspaper
    subscribers derived from the JSON data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Running these two functions modifies the original emitted data stream from
    JSON to a date-filtered stream of `str` data. The coroutine `list_dated_subscription()`API
    service, on the other hand, subscribes to `fetch_subscription()` to extract the
    newspaper subscriptions within the `min_date` and `max_date` range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Although the FastAPI framework does not yet fully support reactive programming,
    we can still create coroutines that can work with various RxPY utilities. Now,
    we will explore how coroutines are not only for background processes but also
    for FastAPI event handlers.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The FastAPI framework has special functions called *event handlers* that execute
    before the application starts up and during shutdown. These events are activated
    every time the `uvicorn` or `hypercorn` server reloads. Event handlers can also
    be coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the startup event
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The *startup event* is an event handler that the server executes when it starts
    up. We decorate the function with the `@app.on_event("startup")` decorator to
    create a startup event. Applications may require a startup event to centralize
    some transactions, such as the initial configuration of some components or the
    set up of data-related resources. The following example is the application startup
    event that opens a database connection for the GINO repository transactions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: This `initialize()` event is defined in our application’s `main.py` file so
    that GINO can only create the connection once every server reload or restart.
  prefs: []
  type: TYPE_NORMAL
- en: Defining shutdown events
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Meanwhile, the *shutdown event* cleans up unwanted memory, destroys unwanted
    connections, and logs the reason for shutting down the application. The following
    is the shutdown event of our application that closes the GINO database connection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: We can define startup and shutdown events in APIRouter but be sure this will
    not cause transaction overlapping or collision with other routers. Moreover, event
    handlers do not work in mounted sub-applications.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The use of coroutines is one of the factors that makes the FastAPI microservice
    application fast, aside from its use of an ASGI-based server. This chapter has
    proven that using coroutines to implement API services will improve the performance
    better than utilizing more threads in the thread pool. Since the framework runs
    on an asyncio platform, we can utilize asyncio utilities to design various design
    patterns to manage the CPU-bound and I/O-bound services.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter used Celery and Redis for creating and managing asynchronous background
    tasks for behind-the-scenes transactions such as logging, system monitoring, time-sliced
    computations, and batch jobs. We learned that RabbitMQ and Apache Kafka provided
    an integrated solution for building asynchronous and loosely coupled communication
    between FastAPI components, especially for the message-passing part of these interactions.
    Most importantly, coroutines were applied to create these asynchronous and non-blocking
    background processes and message-passing solutions to enhance performance. Reactive
    programming was also introduced in this chapter through the RxPy extension module.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter, in general, concludes that the FastAPI framework is ready to build
    a microservice application that has a *reliable*, *asynchronous*, *message-driven*,
    *real-time message-passing*, and *distributed core system*. The next chapter will
    highlight other FastAPI features that provide integrations with UI-related tools
    and frameworks, API documentation using OpenAPI Specification, session handling,
    and circumventing CORS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: Infrastructure-Related Issues, Numerical and Symbolic Computations,
    and Testing Microservices'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final part of the book, we will discuss other essential microservice
    features, such as distributed tracing and logging, service registries, virtual
    environments, and API metrics. Serverless deployment using Docker and Docker Compose
    with NGINX as a reverse proxy will also be covered. Furthermore, we will look
    at FastAPI as a framework for building scientific applications using numerical
    algorithms from the `numpy`, `scipy`, `sympy`, and `pandas` modules to model,
    analyze, and visualize the mathematical and statistical solutions of its API services.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part comprises the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B17975_09.xhtml#_idTextAnchor266)*, Utilizing Other Advanced
    Features*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B17975_10.xhtml#_idTextAnchor292)*, Solving Numerical, Symbolic,
    and Graphical Problems*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B17975_11.xhtml#_idTextAnchor321)*, Adding Other Microservice
    Features*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
