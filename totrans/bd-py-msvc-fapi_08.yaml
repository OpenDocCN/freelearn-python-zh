- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Creating Coroutines, Events, and Message-Driven Transactions
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建协程、事件和消息驱动的交易
- en: The FastAPI framework is an asynchronous framework that runs over the asyncio
    platform, which utilizes the ASGI protocol. It is well known for its 100% support
    for asynchronous endpoints and non-blocking tasks. This chapter will focus on
    how we create highly scalable applications with asynchronous tasks and event-driven
    and message-driven transactions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: FastAPI 框架是一个基于 asyncio 平台的异步框架，它利用了 ASGI 协议。它因其对异步端点和非阻塞任务100%的支持而闻名。本章将重点介绍我们如何通过异步任务和事件驱动以及消息驱动的交易来创建高度可扩展的应用程序。
- en: We learned in [*Chapter 2*](B17975_02.xhtml#_idTextAnchor033)*, Exploring the
    Core Features*, that *Async/Await* or asynchronous programming is a design pattern
    that enables other services or transactions to run outside the main thread. The
    framework uses the `async` keyword to create asynchronous processes that will
    run on top of other thread pools and will be *awaited*, instead of invoking them
    directly. The number of external threads is defined during the Uvicorn server
    startup through the `--worker` option.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [*第 2 章*](B17975_02.xhtml#_idTextAnchor033) *探索核心功能* 中了解到，*Async/Await*
    或异步编程是一种设计模式，它允许其他服务或交易在主线程之外运行。该框架使用 `async` 关键字创建将在其他线程池之上运行的异步进程，并将被 *await*，而不是直接调用它们。外部线程的数量在
    Uvicorn 服务器启动期间通过 `--worker` 选项定义。
- en: 'In this chapter, we will delve into the framework and scrutinize the various
    components of the FastAPI Framework that can run asynchronously using multiple
    threads. The following highlights will help us understand how asynchronous FastAPI
    is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入研究框架并仔细审查 FastAPI 框架的各个组件，这些组件可以使用多个线程异步运行。以下要点将帮助我们理解异步 FastAPI：
- en: Implementing coroutines
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现协程
- en: Creating asynchronous background tasks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建异步后台任务
- en: Understanding Celery tasks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Celery 任务
- en: Building message-driven transactions using RabbitMQ
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 RabbitMQ 构建消息驱动的交易
- en: Building publish/subscribe messaging using Kafka
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Kafka 构建发布/订阅消息
- en: Applying reactive programming in tasks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在任务中应用响应式编程
- en: Customizing events
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义事件
- en: Implementing asynchronous **Server-Sent Events** (**SSE**)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现异步 **服务器端事件**（**SSE**）
- en: Building an asynchronous WebSocket
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建异步 WebSocket
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter will cover asynchronous features, software specifications, and
    the components of a *newsstand management system* prototype. The discussions will
    use this online newspaper management system prototype as a specimen to understand,
    explore, and implement asynchronous transactions that will manage the *newspaper
    content*, *subscription*, *billing*, *user profiles*, *customers*, and other business-related
    transactions. The code has all been uploaded to [https://github.com/PacktPublishing/Building-Python-Microservices-with-FastAPI](https://github.com/PacktPublishing/Building-Python-Microservices-with-FastAPI)
    under the `ch08` project.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖异步功能、软件规范以及 *新闻亭管理系统* 原型的组件。讨论将使用这个在线报纸管理系统原型作为样本来理解、探索和实现异步交易，这些交易将管理
    *报纸内容*、*订阅*、*计费*、*用户资料*、*客户* 以及其他与业务相关的交易。所有代码都已上传到 [https://github.com/PacktPublishing/Building-Python-Microservices-with-FastAPI](https://github.com/PacktPublishing/Building-Python-Microservices-with-FastAPI)
    下的 `ch08` 项目。
- en: Implementing coroutines
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现协程
- en: In the FastAPI framework, a *thread pool* is always present to execute both
    synchronous API and non-API transactions for every request. For ideal cases where
    both the transactions have minimal performance overhead with *CPU-bound* and *I/O-bound*
    transactions, the overall performance of using the FastAPI framework is still
    better than those frameworks that use non-ASGI-based platforms. However, when
    contention occurs due to high CPU-bound traffic or heavy CPU workloads, the performance
    of FastAPI starts to wane due to *thread switching*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在 FastAPI 框架中，始终存在一个 *线程池* 来执行每个请求的同步 API 和非 API 交易。对于理想情况，即这两种交易都具有最小的性能开销，无论是
    *CPU 密集型* 还是 *I/O 密集型* 交易，使用 FastAPI 框架的整体性能仍然优于那些使用非 ASGI 基础平台的框架。然而，当由于高 CPU
    密集型流量或重 CPU 工作负载导致竞争时，由于 *线程切换*，FastAPI 的性能开始下降。
- en: Thread switching is a context switch from one thread to another within the same
    process. So, if we have several transactions with varying workloads running in
    the background and on the browser, FastAPI will run these transactions in the
    thread pool with several context switches. This scenario will cause contention
    and degradation to lighter workloads. To avoid performance issues, we apply *coroutine
    switching* instead of threads.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 线程切换是在同一进程内从一个线程切换到另一个线程的上下文切换。因此，如果我们有多个具有不同工作负载的事务在后台和浏览器上运行，FastAPI 将在线程池中运行这些事务，并执行多个上下文切换。这种情况将导致对较轻工作负载的竞争和性能下降。为了避免性能问题，我们采用
    *协程切换* 而不是线程。
- en: Applying coroutine switching
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用协程切换
- en: 'The FastAPI framework works at the optimum speed with a mechanism called *coroutine
    switching*. This approach allows transaction-tuned tasks to work cooperatively
    by allowing other running processes to pause so that the thread can execute and
    finish more urgent tasks, and resume "awaited" transactions without preempting
    the thread. These coroutine switches are programmer-defined components and not
    kernel-related or memory-related features. In FastAPI, there are two ways of implementing
    coroutines: (a) applying the `@asyncio.coroutine` decorator, and (b) using the
    `async`/`await` construct.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: FastAPI 框架通过称为 *协程切换* 的机制以最佳速度运行。这种方法允许事务调优的任务通过允许其他运行进程暂停，以便线程可以执行并完成更紧急的任务，并在不抢占线程的情况下恢复
    "awaited" 事务。这些协程切换是程序员定义的组件，与内核或内存相关的功能无关。在 FastAPI 中，有两种实现协程的方法：(a) 应用 `@asyncio.coroutine`
    装饰器，和 (b) 使用 `async`/`await` 构造。
- en: Applying @asyncio.coroutine
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用 @asyncio.coroutine
- en: '`asyncio` is a Python extension that implements the Python concurrency paradigm
    using a single-threaded and single-process model and provides API classes and
    methods for running and managing coroutines. This extension provides an `@asyncio.coroutine`
    decorator that transforms API and native services into *generator-based coroutines*.
    However, this is an old approach and can only be used in FastAPI that uses Python
    3.9 and below. The following is a login service transaction of our *newsstand
    management system* prototype implemented as a coroutine:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`asyncio` 是一个 Python 扩展，它使用单线程和单进程模型实现 Python 并发范式，并提供用于运行和管理协程的 API 类和方法。此扩展提供了一个
    `@asyncio.coroutine` 装饰器，将 API 和原生服务转换为基于生成器的协程。然而，这是一个旧的方法，只能在使用 Python 3.9 及以下版本的
    FastAPI 中使用。以下是我们 *新闻亭管理系统* 原型中实现为协程的登录服务事务：'
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`build_user_list()` is a native service that converts all login records into
    the `str` format. It is decorated with the `@asyncio.coroutine` decorator to transform
    the transaction into an asynchronous task or coroutine. A coroutine can invoke
    another coroutine function or method using only the `yield from` clause. This
    construct pauses the coroutine and passes the control of the thread to the coroutine
    function invoked. By the way, the `asyncio.sleep()` method is one of the most
    widely used asynchronous utilities of the `asyncio` module, which can pause a
    process for a few seconds, but is not the ideal one. On the other hand, the following
    code is an API service implemented as a coroutine that can minimize contention
    and performance degradation in client-side executions:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`build_user_list()` 是一个原生服务，它将所有登录记录转换为 `str` 格式。它使用 `@asyncio.coroutine` 装饰器将事务转换为异步任务或协程。协程可以使用
    `yield from` 子句调用另一个协程函数或方法。顺便说一句，`asyncio.sleep()` 方法是 `asyncio` 模块中最广泛使用的异步实用工具之一，它可以使进程暂停几秒钟，但并不是理想的。另一方面，以下代码是一个作为协程实现的
    API 服务，它可以最小化客户端执行中的竞争和性能下降：'
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `list_login()` API service retrieves all the login details of the application’s
    users through a coroutine CRUD transaction implemented in *GINO ORM*. The API
    service again uses the `yield from` clause to run and execute the `get_all_login()`
    coroutine function.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`list_login()` API 服务通过在 *GINO ORM* 中实现的协程 CRUD 事务检索应用程序用户的全部登录详情。API 服务再次使用
    `yield from` 子句来运行和执行 `get_all_login()` 协程函数。'
- en: 'A coroutine function can invoke and await multiple coroutines concurrently
    using the `asyncio.gather()` utility. This `asyncio` method manages a list of
    coroutines and waits until all its coroutines have completed their tasks. Then,
    it will return a list of results from the corresponding coroutines. The following
    code is an API that retrieves login records through an asynchronous CRUD transaction
    and then invokes `count_login()` and `build_user_list()` concurrently to process
    these records:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 协程函数可以使用 `asyncio.gather()` 工具并发调用和等待多个协程。这个 `asyncio` 方法管理一个协程列表，并等待直到所有协程完成其任务。然后，它将返回对应协程的结果列表。以下是一个通过异步
    CRUD 事务检索登录记录的 API，然后并发调用 `count_login()` 和 `build_user_list()` 来处理这些记录：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`list_login_records()` uses `asyncio.gather()` to run the `count_login()` and
    `build_user_list()` tasks and later extract their corresponding returned values
    for processing.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`list_login_records()` 使用 `asyncio.gather()` 来运行 `count_login()` 和 `build_user_list()`
    任务，并在之后提取它们对应的返回值进行处理。'
- en: Using the async/await construct
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 async/await 构造
- en: 'Another way of implementing a coroutine is using `async`/`await` constructs.
    As with the previous approach, this syntax creates a task that can pause anytime
    during its operation before it reaches the end. But the kind of coroutine that
    this approach produces is called a *native coroutine*, which is not iterable in
    the way that the generator type is. The `async`/`await` syntax also allows the
    creation of other asynchronous components such as the `async with` context managers
    and `async for` iterators. The following code is the `count_login()` task previously
    invoked in the generator-based coroutine service, `list_login_records()`:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 实现协程的另一种方法是使用 `async`/`await` 构造。与之前的方法一样，这种语法创建了一个任务，在执行过程中可以随时暂停，直到到达末尾。但这种方法产生的协程被称为
    *原生协程*，它不能像生成器类型那样迭代。`async`/`await` 语法还允许创建其他异步组件，例如 `async with` 上下文管理器和 `async
    for` 迭代器。以下代码是之前在基于生成器的协程服务 `list_login_records()` 中调用的 `count_login()` 任务：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `count_login()` native service is a native coroutine because of the `async`
    keyword placed before its method definition. It only uses `await` to invoke other
    coroutines. The `await` keyword suspends the execution of the current coroutine
    and passes the control of the thread to the invoked coroutine function. After
    the invoked coroutine finishes its process, the thread control will yield back
    to the caller coroutine. Using the `yield from` construct instead of `await` will
    raise an error because our coroutine here is not generator-based. The following
    is an API service implemented as a native coroutine that manages data entry for
    the new administrator profiles:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`count_login()` 原生服务是一个原生协程，因为它在方法定义之前放置了 `async` 关键字。它只使用 `await` 来调用其他协程。`await`
    关键字暂停当前协程的执行，并将线程控制权传递给被调用的协程函数。在被调用的协程完成其处理后，线程控制权将返回给调用协程。使用 `yield from` 构造而不是
    `await` 会引发错误，因为我们的协程不是基于生成器的。以下是一个作为原生协程实现的 API 服务，用于管理新管理员资料的录入：'
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Both generator-based and native coroutines are monitored and managed by an *event
    loop*, which represents an infinite loop inside a thread. Technically, it is an
    object found in the *thread*, and each *thread* in the thread pool can only have
    one event loop, which contains a list of helper objects called *tasks*. Each task,
    pre-generated or manually created, executes one coroutine. For instance, when
    the previous `add_admin()` API service invokes the `insert_admin()` coroutine
    transaction, the event loop will suspend `add_admin()` and tag its task as an
    *awaited* task. Afterward, the event loop will assign a task to run the `insert_admin()`
    transaction. Once the task has completed its execution, it will yield the control
    back to `add_admin()`. The thread that manages the FastAPI application is not
    interrupted during these shifts of execution since it is the event loop and its
    tasks that participate in the *coroutine switching* mechanism. Let us now use
    these coroutines to build our application
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于生成器和原生的协程都由一个 *事件循环* 监控和管理，它代表线程内部的一个无限循环。技术上，它是在 *线程* 中找到的一个对象，线程池中的每个 *线程*
    只能有一个事件循环，其中包含一个称为 *任务* 的辅助对象列表。每个任务，无论是预先生成的还是手动创建的，都会执行一个协程。例如，当先前的 `add_admin()`
    API 服务调用 `insert_admin()` 协程事务时，事件循环将挂起 `add_admin()` 并将其任务标记为 *等待* 任务。之后，事件循环将分配一个任务来运行
    `insert_admin()` 事务。一旦任务完成执行，它将控制权交还给 `add_admin()`。在执行转换期间，管理 FastAPI 应用程序的线程不会被中断，因为事件循环及其任务参与了
    *协程切换* 机制。现在让我们使用这些协程来构建我们的应用程序
- en: Designing asynchronous transactions
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计异步事务
- en: 'There are a few programming paradigms that we can follow when creating coroutines
    for our application. Utilizing more coroutine switching in the process can help
    improve the software performance. In our *newsstand* application, there is an
    endpoint, `/admin/login/list/enc`, in the `admin.py` router that returns a list
    of encrypted user details. In its API service, shown in the following code, each
    record is managed by an `extract_enc_admin_profile()` transaction call instead
    of passing the whole data record to a single call, thus allowing the concurrent
    executions of tasks. This strategy is better than running the bulk of transactions
    in a thread without *context switches*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在为我们的应用程序创建协程时，我们可以遵循几种编程范式。在过程中利用更多的协程切换可以帮助提高软件性能。在我们的 *newsstand* 应用程序中，`admin.py`
    路由器中有一个端点 `/admin/login/list/enc`，它返回一个加密的用户详情列表。在其API服务中，如下所示代码所示，每条记录都由一个 `extract_enc_admin_profile()`
    事务调用管理，而不是将整个数据记录传递给单个调用，从而允许任务的并发执行。这种策略比在没有 *上下文切换* 的情况下运行大量事务在线程中更好：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, the `extract_enc_admin_profile()` coroutine, shown in the following code,
    implements a chaining design pattern, where it calls the other smaller coroutines
    through a chain. Simplifying and breaking down the monolithic and complex processes
    into smaller but more robust coroutines will improve the application’s performance
    by utilizing more context switches. In this API, `extract_enc_admin_profile()`
    creates three context switches in a chain, better than thread switches:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`extract_enc_admin_profile()` 协程，如下所示代码所示，实现了一个链式设计模式，其中它通过链调用其他较小的协程。将单体和复杂的过程简化并分解成更小但更健壮的协程，通过利用更多的上下文切换来提高应用程序的性能。在这个API中，`extract_enc_admin_profile()`
    在链中创建了三个上下文切换，比线程切换更优：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'On the other hand, the following implementation is the smaller subroutines
    awaited and executed by `extract_enc_admin_profile()`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，以下实现是 `extract_enc_admin_profile()` 等待和执行的较小子程序：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: These three subroutines will give the main coroutine the encrypted `str` that
    contains the details of an administrator profile. All these encrypted strings
    will be collated by the API service using the `asyncio.gather()` utility.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个子程序将给主协程提供包含管理员配置文件详情的加密 `str`。所有这些加密字符串都将由API服务使用 `asyncio.gather()` 工具汇总。
- en: 'Another programming approach to utilizing the coroutine switching is the use
    of pipelines created by `asyncio.Queue`. In this programming design, the queue
    structure is the common point between two tasks: (a) the task that will place
    a value to the queue called the *producer*, and (b) the task that will fetch the
    item from the queue, the *consumer*. We can implement a *one producer/one consumer*
    interaction or a *multiple producers/multiple consumers* setup with this approach.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code highlights the `process_billing()` native service that builds
    a *producer/consumer* transaction flow. The `extract_billing()` coroutine is the
    producer that retrieves the billing records from the database and passes each
    record one at a time to the queue. `build_billing_sheet()`, on the other hand,
    is the consumer that fetches the record from the queue structure and generates
    the billing sheet:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this programming design, the `build_billing()` coroutine will explicitly
    wait for the record queued by `extract_billing()`. This setup is possible due
    to the `asyncio.create_task()` utility, which directly assigns and schedules a
    task to each coroutine.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'The queue is the only method parameter common to the coroutines because it
    is their common point. The `join()`of `asyncio.Queue` ensures that all the items
    passed to the pipeline by `extract_billing()` are fetched and processed by `build_billing_sheet()`.
    It also blocks the external controls that would affect the coroutine interactions.
    The following code shows how to create `asyncio.Queue` and schedule a task for
    execution:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: By the way, always pass `cancel()to` the task right after its coroutine has
    completed the process. On the other hand, we can also apply other ways so that
    the performance of our coroutines can improve.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Using the HTTP/2 protocol
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Coroutine execution can be faster in applications running on the *HTTP/2* protocol.
    We can replace the *Uvicorn* server with *Hypercorn*, which now supports ASGI-based
    frameworks such as FastAPI. But first, we need to install `hypercorn` using `pip`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'For *HTTP/2* to work, we need to create an SSL certificate. Using OpenSSL,
    our app has two *PEM* files for our *newsstand* prototype: (a) the private encryption
    (`key.pem`) and (b) the certificate information (`cert.pem`.) We place these files
    in the main project folder before executing the following `hypercorn` command
    to run our FastAPI application:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now, let us explore other FastAPI tasks that can also use coroutines.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Creating asynchronous background tasks
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 2*](B17975_02.xhtml#_idTextAnchor033)*, Exploring the Core Features*,
    we first showcased the `BackgroundTasks` injectable API class, but we didn’t mention
    creating asynchronous background tasks. In this discussion, we will be focusing
    on creating asynchronous background tasks using the `asyncio` module and coroutines.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Using the coroutines
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The framework supports the creation and execution of asynchronous background
    processes using the `async`/`await` structure. The following native service is
    an asynchronous transaction that generates a billing sheet in CSV format in the
    background:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 框架支持使用 `async`/`await` 结构创建和执行异步后台进程。以下原生服务是一个异步事务，在后台以 CSV 格式生成账单表：
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This `generate_billing_sheet()` coroutine service will be executed as a background
    task in the following API service, `save_vendor_billing()`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 `generate_billing_sheet()` 协程服务将在以下 API 服务 `save_vendor_billing()` 中作为后台任务执行：
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now, nothing has changed when it comes to defining background processes. We
    usually inject `BackgroundTasks` into the API service method and apply `add_task()`
    to provide task schedules, assignments, and execution for a specific process.
    But since the approach is now to utilize coroutines, the background task will
    use the event loop instead of waiting for the current thread to finish its jobs.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在定义后台进程方面，并没有什么变化。我们通常将 `BackgroundTasks` 注入 API 服务方法，并使用 `add_task()` 提供任务调度、分配和执行。但由于现在的方法是利用协程，后台任务将使用事件循环而不是等待当前线程完成其工作。
- en: If the background process requires arguments, we pass these arguments to `add_task()`
    right after its *first parameter*. For instance, the arguments for the `billing_date`
    and `query_list` parameters of `generate_billing_sheet()` should be placed after
    the `generate_billing_sheet` injection into `add_task()`. Moreover, the `billing_date`
    value should be passed before the `result` argument because `add_task()` still
    follows the order of parameter declaration in `generate_billing_sheet()` to avoid
    a type mismatch.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果后台进程需要参数，我们将在 `add_task()` 的 *第一个参数* 之后传递这些参数。例如，`generate_billing_sheet()`
    的 `billing_date` 和 `query_list` 参数的参数应该在将 `generate_billing_sheet` 注入 `add_task()`
    之后放置。此外，`billing_date` 的值应该在 `result` 参数之前传递，因为 `add_task()` 仍然遵循 `generate_billing_sheet()`
    中参数声明的顺序，以避免类型不匹配。
- en: All asynchronous background tasks will continuously execute and will not be
    *awaited* even if their coroutine API service has already returned a response
    to the client.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所有异步后台任务将持续执行，即使它们的协程 API 服务已经向客户端返回了响应，也不会被 *await*。
- en: Creating multiple tasks
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建多个任务
- en: '`BackgroundTasks` allows the creation of multiple asynchronous transactions
    that will execute concurrently in the background. In the `save_vendor_billing()`
    service, there is another task created for a new transaction called the `create_total_payables_year()`
    transaction, which requires the same arguments as `generate_billing_sheet()`.
    Again, this newly created task will be utilizing the event loop instead of the
    thread.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`BackgroundTasks` 允许创建多个异步事务，这些事务将在后台并发执行。在 `save_vendor_billing()` 服务中，为一个新的交易创建了一个名为
    `create_total_payables_year()` 的事务，它需要与 `generate_billing_sheet()` 相同的参数。再次强调，这个新创建的任务将使用事件循环而不是线程。'
- en: The application always encounters performance issues when the background processes
    have high-CPU workloads. Also, tasks generated by `BackgroundTasks` are not capable
    of returning values from the transactions. Let us look for another solution where
    tasks can manage high workloads and execute processes with returned values.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当后台进程具有高 CPU 工作负载时，应用程序总是遇到性能问题。此外，由 `BackgroundTasks` 生成的任务无法从事务中返回值。让我们寻找另一种解决方案，其中任务可以管理高工作量并执行带有返回值的进程。
- en: Understanding Celery tasks
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Celery 任务
- en: '*Celery* is a non-blocking task queue that runs on a distributed system. It
    can manage asynchronous background processes that are huge and heavy with CPU
    workloads. It is a third-party tool, so we need to install it first through `pip`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*Celery* 是一个在分布式系统上运行的非阻塞任务队列。它可以管理具有大量和重 CPU 工作负载的异步后台进程。它是一个第三方工具，因此我们需要首先通过
    `pip` 安装它：'
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'It schedules and runs tasks concurrently on a single server or distributed
    environment. But it requires a message transport to send and receive messages,
    such as *Redis*, an in-memory database that can be used as a message broker for
    messages in strings, dictionaries, lists, sets, bitmaps, and stream types. Also,
    we can install Redis on Linux, macOS, and Windows. Now, after the installation,
    run its `redis-server.exe` command to start the server. In Windows, the Redis
    service is set to run by default after installation, which causes a *TCP bind
    listener* error. So, we need to stop it before running the startup command. *Figure
    8.1* shows Windows **Task Manager** with the Redis service giving a **Stopped**
    status:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 它在单个服务器或分布式环境中并发地安排和运行任务。但它需要一个消息传输来发送和接收消息，例如*Redis*，一个可以用于字符串、字典、列表、集合、位图和流类型的消息代理的内存数据库。我们还可以在Linux、macOS和Windows上安装Redis。现在，安装后，运行其`redis-server.exe`命令以启动服务器。在Windows中，Redis服务默认设置为安装后运行，这会导致*TCP绑定监听器*错误。因此，在运行启动命令之前，我们需要停止它。*图8.1*显示了Redis服务在Windows**任务管理器**中处于**停止**状态：
- en: '![Figure 8.1 – Stopping the Redis service'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.1 – 停止Redis服务'
- en: '](img/Figure_8.01_B17975.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_8.01_B17975.jpg]'
- en: Figure 8.1 – Stopping the Redis service
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 停止Redis服务
- en: 'After stopping the service, we should now see Redis running as shown in *Figure
    8.2*:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 停止服务后，我们现在应该看到Redis正在运行，如图*图8.2*所示：
- en: '![Figure 8.2 – A running Redis server'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.2 – 运行中的Redis服务器'
- en: '](img/Figure_8.02_B17975.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_8.02_B17975.jpg]'
- en: Figure 8.2 – A running Redis server
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 运行中的Redis服务器
- en: Creating and configuring the Celery instance
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建和配置Celery实例
- en: 'Before creating Celery tasks, we need a Celery instance placed in a dedicated
    module of our application. The *newsstand* prototype has the Celery instance in
    the `/services/billing.py` module, and the following is part of the code that
    shows the process of Celery instantiation:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建Celery任务之前，我们需要将Celery实例放置在我们应用程序的专用模块中。*newsstand*原型将Celery实例放在`/services/billing.py`模块中，以下代码展示了Celery实例化的过程：
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To create the Celery instance, we need the following details:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建Celery实例，我们需要以下详细信息：
- en: The name of the current module containing the Celery instance (the first argument)
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含Celery实例的当前模块的名称（第一个参数）
- en: The URL of Redis as our message broker (`broker`)
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为我们的消息代理的Redis的URL（`broker`）
- en: The backend result where the results of tasks are stored and monitored (`backend`)
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储和监控任务结果的后端（`backend`）
- en: The list of other modules used in the message body or by the Celery task (`include`)
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在消息体或Celery任务中使用的其他模块的列表（`include`）
- en: After the instantiation, we need to set the appropriate serializer and content
    types to process the incoming and outgoing message body of the tasks involved,
    if there are any. To allow the passing of full Python objects with non-JSON-able
    values, we need to include `pickle` as a supported content type, then declare
    a default task and result serializer to the object stream. However, using a `pickle`
    serializer poses some security issues because it tends to expose some transaction
    data. To avoid compromising the app, apply sanitation to message objects, such
    as removing sensitive values or credentials, before pursuing the messaging operation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化后，如果有的话，我们需要设置适当的序列化和内容类型来处理涉及的任务的传入和传出消息体。为了允许传递具有非JSON可序列化值的完整Python对象，我们需要将`pickle`作为支持的内容类型包括在内，然后向对象流声明默认的任务和结果序列化器。然而，使用`pickle`序列化器会带来一些安全问题，因为它倾向于暴露一些事务数据。为了避免损害应用程序，在执行消息操作之前，对消息对象进行清理，例如删除敏感值或凭据。
- en: Apart from the serialization options, other important properties such as `task_create_missing_queues`,
    `task_ignore_result`, and error-related configuration should also be part of the
    `CeleryConfig` class. Now, we declare all these details in a custom class, which
    we will inject into the `config_from_object()` method of the Celery instance.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 除了序列化选项之外，其他重要的属性，如`task_create_missing_queues`、`task_ignore_result`和与错误相关的配置，也应该成为`CeleryConfig`类的一部分。现在，我们在一个自定义类中声明所有这些细节，然后将其注入到Celery实例的`config_from_object()`方法中。
- en: Additionally, we can create a Celery logger through its `get_task_logger()`with
    the name of the current task.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以通过其`get_task_logger()`方法创建一个Celery日志记录器，其名称为当前任务。
- en: Creating the task
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建任务
- en: The main goal of the *Celery instance* is to annotate Python methods to become
    tasks. The Celery instance has a `task()` decorator that we can apply to all callable
    procedures we want to define as asynchronous tasks. Part of the `task()` decorator
    is the task’s `name`, an optional unique name composed of the *package*, *module
    name(s)*, and the *method name of the transaction*. It has other attributes that
    can add more refinement to the task definition, such as the `auto_retry` list,
    which registers `Exception` classes that may cause execution retries when emitted,
    and `max_tries`, which limits the number of retry executions of a task. By the
    way, Celery 5.2.3 and below can only define tasks from *non-coroutine methods*.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*Celery实例*的主要目标是注释Python方法以成为任务。Celery实例有一个`task()`装饰器，我们可以将其应用于我们想要定义为异步任务的所有可调用过程。`task()`装饰器的一部分是任务的`name`，这是一个可选的唯一名称，由*package*、*module
    name(s)*和*transaction*的*method name*组成。它还有其他属性，可以添加更多细化到任务定义中，例如`auto_retry`列表，它注册了可能引起执行重试的`Exception`类，以及`max_tries`，它限制了任务的重试执行次数。顺便说一下，Celery
    5.2.3及以下版本只能从*non-coroutine methods*定义任务。'
- en: 'The `services.billing.tasks.create_total_payables_year_celery` task shown here
    adds all the payable amounts per date and returns the total amount:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这里显示的`services.billing.tasks.create_total_payables_year_celery`任务将每天的应付金额相加，并返回总额：
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The given task has only five (`5`) retries to recover when it encounters either
    `ValueError` or `TypeError` at runtime. Also, it is a function that returns a
    computed amount, which is impossible to create when using `BackgroundTasks`. All
    functional tasks use the *Redis* database as the temporary storage for their returned
    values, which is the reason there is a backend parameter in the Celery constructor.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 给定的任务在运行时遇到`ValueError`或`TypeError`时只有五次(`5`)重试来恢复。此外，它是一个返回计算金额的函数，当使用`BackgroundTasks`时，这是不可能创建的。所有功能任务都使用*Redis*数据库作为它们返回值的临时存储，这也是为什么在Celery构造函数中有后端参数的原因。
- en: Calling the task
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调用任务
- en: 'FastAPI services can call these tasks using the `apply_async()`or `delay()`function.
    The latter is the easier option since it is preconfigured and only needs the parameters
    for the transaction to get the result. The `apply_async()` function is a better
    option since it accepts more details that can optimize the task execution. These
    details are `queue`, `time_limit`, `retry`, `ignore_result`, `expires`, and some
    `kwargs` of arguments. But both these functions return an `AsyncResult` object,
    which returns resources such as the task’s `state`, the `wait()` function to help
    the task finish its operation, and the `get()` function to return its computed
    value or an exception. The following code is a coroutine API service that calls
    the `services.billing.tasks.create_total_payables_year_celery` task using the
    `apply_async` method:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: FastAPI服务可以使用`apply_async()`或`delay()`函数调用这些任务。后者是一个更简单的选项，因为它预先配置好，只需要提供事务的参数即可获取结果。`apply_async()`函数是一个更好的选项，因为它接受更多细节，可以优化任务执行。这些细节包括`queue`、`time_limit`、`retry`、`ignore_result`、`expires`以及一些`kwargs`参数。但这两个函数都返回一个`AsyncResult`对象，该对象返回资源，如任务的`state`、帮助任务完成操作的`wait()`函数以及返回其计算值或异常的`get()`函数。以下是一个协程API服务，它使用`apply_async`方法调用`services.billing.tasks.create_total_payables_year_celery`任务：
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Setting `task_create_missing_queues` to `True` at the `CeleryConfig` setup is
    always recommended because it automatically creates the task `queue`, default
    or not, once the worker server starts. The worker server places all the loaded
    tasks in a task `queue` for execution, monitoring, and result retrieval. Thus,
    we should always define a task `queue` in the `apply_async()` function’s argument
    before extracting `AsyncResult`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在`CeleryConfig`设置中将`task_create_missing_queues`设置为`True`总是推荐的做法，因为它会在工作服务器启动时自动创建任务`queue`，无论是默认的还是有其他名称。工作服务器将所有加载的任务放置在任务`queue`中以便执行、监控和检索结果。因此，在提取`AsyncResult`之前，我们应该始终在`apply_async()`函数的参数中定义一个任务`queue`。
- en: The `AsyncResult` object has a `get()` method that releases the returned value
    of the task from the `AsyncResult` instance, with or without a timeout. In the
    `compute_payables_yearly()` service, the amount payable in `AsyncResult` is retrieved
    by the `get()` function with a timeout of 5 seconds. Let us now deploy and run
    our tasks using the Celery server
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`AsyncResult`对象有一个`get()`方法，它从`AsyncResult`实例中释放任务返回的值，无论是否有超时。在`compute_payables_yearly()`服务中，使用5秒的超时通过`get()`函数检索`AsyncResult`中的应付金额。现在让我们使用Celery服务器部署和运行我们的任务。'
- en: Starting the worker server
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动工作服务器
- en: 'Running the Celery worker creates a single process that handles and manages
    all the queued tasks. The worker needs to know in which module the Celery instance
    is created, together with the tasks to establish the server process. In our prototype,
    the `services.billing` module is where we place our Celery application. Thus,
    the complete command to start the worker is the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, `-A` specifies the module of our Celery object and tasks. The `-Q` option
    indicates that the worker will be using a *low-*, *normal-*, *or high-priority*
    queue. But first, we need to set `task_create_missing_queues` to `True` in the
    Celery setup. We also need to indicate the number of threads that the worker needs
    for task execution by adding the `-c` option. The `-P` option specifies the type
    of *thread pool* that the worker will be utilizing. By default, the Celery worker
    uses the `prefork pool` applicable to most CPU-bound transactions. Other options
    are *solo*, *eventlet*, and *gevent*, but our setup will be utilizing *solo*,
    the most suitable choice for running CPU-intensive tasks in a microservice environment.
    On the other hand, the `-l` option enables the logger we set using `get_task_logger()`
    during the setup. Now, there are also ways to monitor our running tasks and one
    of those options is to use the Flower tool.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring the tasks
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Flower* is Celery’s monitoring tool that observes and monitors all tasks executions
    by generating a real-time audit on a web-based platform. But first, we need to
    install it using `pip`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'And then, we run the following `celery` command with the `flower` option:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To view the audit, we run `http://localhost:5555/tasks` on a browser. *Figure
    8.3* shows a *Flower* snapshot of an execution log incurred by the `services.billing.tasks.create_total_payables_year_celery`
    task:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – The Flower monitoring tool'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.03_B17975.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 – The Flower monitoring tool
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have used Redis as our in-memory backend database for task results
    and a message broker. Let us now use another asynchronous message broker that
    can replace Redis, *RabbitMQ*.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Building message-driven transactions using RabbitMQ
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RabbitMQ is a lightweight asynchronous message broker that supports multiple
    messaging protocols such as *AMQP*, *STOM*, *WebSocket*, and *MQTT*. It requires
    *erlang* before it works properly in Windows, Linux, or macOS. Its installer can
    be downloaded from [https://www.rabbitmq.com/download.html](https://www.rabbitmq.com/download.html).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Celery instance
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of using Redis as the broker, RabbitMQ is a better replacement as a
    message broker that will mediate messages between the client and the Celery worker
    threads. For multiple tasks, RabbitMQ can command the Celery worker to work on
    these tasks one at a time. The RabbitMQ broker is good for huge messages and it
    saves these messages to disk memory.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, we need to set up a new Celery instance that will utilize the RabbitMQ
    message broker using its *guest* account. We will use the AMQP protocol as the
    mechanism for a producer/consumer type of messaging setup. Here is the snippet
    that will replace the previous Celery configuration:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Redis will still be the backend resource, as indicated in Celery’s `backend_result`,
    since it is still simple and easy to control and manage when message traffic increases.
    Let us now use the RabbitMQ to create and manage message-driven transactions.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring AMQP messaging
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can configure the RabbitMQ management dashboard to monitor the messages
    handled by RabbitMQ. After the setup, we can log in to the dashboard using the
    account details to set the broker. *Figure 8.4* shows a screenshot of RabbitMQ’s
    analytics of a situation where the API services called the `services.billing.tasks.create_total_payables_year_celery`
    task several times:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – The RabbitMQ management tool'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.04_B17975.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4 – The RabbitMQ management tool
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: If the RabbitMQ dashboard fails to capture the behavior of the tasks, the *Flower*
    tool will always be an option for gathering the details about the arguments, `kwargs`,
    UUID, state, and processing date of the tasks. And if RabbitMQ is not the right
    messaging tool, we can always resort to *Apache Kafka*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Building publish/subscribe messaging using Kafka
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with RabbitMQ, *Apache Kafka* is an asynchronous messaging tool used by applications
    to send and store messages between producers and consumers. However, it is faster
    than RabbitMQ because it uses *topics* with partitions where producers can append
    various types of messages across these minute folder-like structures. In this
    architecture, the consumers can consume all these messages in a parallel mode,
    unlike in queue-based messaging, which enables producers to send multiple messages
    to a queue that can only allow message consumption sequentially. Within this publish/subscribe
    architecture, Kafka can handle an exchange of large quantities of data per second
    in continuous and real-time mode.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three Python extensions that we can use to integrate the FastAPI
    services with Kafka, namely the `kafka-python`, `confluent-kafka`, and `pykafka`
    extensions. Our online *newsstand* prototype will use `kafka-python`, so we need
    to install it using the `pip` command:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Among the three extensions, it is only with `kafka-python` that we can channel
    and apply Java API libraries to Python for the implementation of a client. We
    can download Kafka from [https://kafka.apache.org/downloads](https://kafka.apache.org/downloads).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Running the Kafka broker and server
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kafka has a *ZooKeeper* server that manages and synchronizes the exchange of
    messages within Kafka’s distributed system. The ZooKeeper server runs as the broker
    that monitors and maintains the Kafka nodes and topics. The following command
    starts the server:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we can start the Kafka server by running the following console command:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: By default, the server will run on localhost at port `9092`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Creating the topic
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When the two servers have started, we can now create a topic called `newstopic`
    through the following command:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The `newstopic` topic has three (`3`) partitions that will hold all the appended
    messages of our FastAPI services. These are also the points where the consumers
    will simultaneously access all the published messages.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the publisher
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After creating the topic, we can now implement a producer that publishes messages
    to the Kafka cluster. The `kafka-python` extension has a `KafkaProducer` class
    that instantiates a single thread-safe producer for all the running FastAPI threads.
    The following is an API service that sends a newspaper messenger record to the
    Kafka `newstopic` topic for the consumer to access and process:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The coroutine API service, `send_messenger_details()`, asks for details about
    a newspaper messenger and stores them in a `BaseModel` object. And then, it sends
    the dictionary of profile details to the cluster in byte format. Now, one of the
    options to consume Kafka tasks is to run its built-in `kafka-console-consumer.bat`
    command.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Running a consumer on a console
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Running the following command from the console is one way to consume the current
    messages from the `newstopic` topic:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This command creates a consumer that will connect to the Kafka cluster to read
    in real time the current messages from `newtopic` sent by the producer. *Figure
    8.5* shows the capture of the consumer while it is running on the console:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – The Kafka consumer'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.05_B17975.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.5 – The Kafka consumer
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want the consumer to read all the messages sent by the producer starting
    from the point where the Kafka server and broker began running, we need to add
    the `--from-beginning` option to the command. The following will read all the
    messages from `newstopic` and continuously capture incoming messages in real time:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Another way of implementing a consumer using the FastAPI framework is through
    SSE. Typical API service implementation will not work with the Kafka consumer
    requirement since we need a continuously running service that subscribes to `newstopic`
    for real-time data. So, let us now explore how we create SSE in the FastAPI framework
    and how it will consume Kafka messages.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Implementing asynchronous Server-Sent Events (SSE)
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SSE is a server push mechanism that sends data to the browser without reloading
    the page. Once subscribed, it generates event-driven streams in real time for
    various purposes.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating SSE in the FastAPI framework only requires the following:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The `EventSourceResponse` class from the `sse_starlette.see` module
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An event generator
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Above all, the framework also allows non-blocking implementation of the whole
    server push mechanism using coroutines that can run even on *HTTP/2*. The following
    is a coroutine API service that implements a Kafka consumer using SSE’s open and
    lightweight protocol:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '`send_message_stream()` is a coroutine API service that implements the whole
    SSE. It returns a special response generated by an `EventSourceResponse` function.
    While the HTTP stream is open, it continuously retrieves data from its source
    and converts any internal events into SSE signals until the connection is closed.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, event generator functions create internal events, which
    can also be asynchronous. `send_message_stream()`, for instance, has a nested
    generator function, `event_provider()`, which consumes the last message sent by
    the producer service using the `consumer.poll()` method. If the message is valid,
    the generator converts the message retrieved into a `dict` object and inserts
    all its details into the database through `MessengerRepository`. Then, it yields
    all the internal details for the `EventSourceResponse` function to convert into
    SSE signals. *Figure 8.6* shows the data streams generated by `send_message_stream()`rendered
    from the browser:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – The SSE data streams'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.06_B17975.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.6 – The SSE data streams
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Another way to implement a Kafka consumer is through *WebSocket*. But this time,
    we will focus on the general procedure of how to create an asynchronous WebSocket
    application using the FastAPI framework.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Building an asynchronous WebSocket
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike in SSE, connection in WebSocket is always *bi-directional*, which means
    the server and client communicate with each other using a long TCP socket connection.
    The communication is always in real time and it doesn’t require the client or
    the server to reply to every event sent.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the asynchronous WebSocket endpoint
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The FastAPI framework allows the implementation of an asynchronous WebSocket
    that can also run on the *HTTP/2* protocol. The following is an example of an
    asynchronous WebSocket created using the coroutine block:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: First, we decorate a coroutine function with `@router.websocket()` when using
    APIRouter, or `@api.websocket()` when using the FastAPI decorator to declare a
    WebSocket component. The decorator must also define a unique endpoint URL for
    the WebSocket. Then, the WebSocket function must have an injected `WebSocket`
    as its first method argument. It can also include other parameters such as query
    and header parameters.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: The `WebSocket` injectable has four ways for sending messages, namely `send()`,
    `send_text()`, `send_json()`, and `send_bytes()`. Applying `send()` will always
    manage every message as plain text by default. The previous `customer_list_ws()`coroutine
    is a WebSocket that sends every customer record in JSON format.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, there are also four methods the WebSocket injectable can
    provide, and these are the `receive()`, `receive_text()`, `receive_json()`, and
    `receive_bytes()` methods. The `receive()` method expects the message to be in
    plain-text format by default. Now, our `customer_list_ws()` endpoint expects a
    JSON reply from a client because it invokes the `receive_json()` method after
    its send message operation.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: The WebSocket endpoint must close the connection right after its transaction
    is done.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the WebSocket client
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are many ways to create a WebSocket client but this chapter will focus
    on utilizing a coroutine API service that will perform a handshake with the asynchronous
    `customer_list_ws()` endpoint once called on a browser or a `curl` command. Here
    is the code of our WebSocket client implemented using the `websockets` library
    that runs on top of the `asyncio` framework:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: After a successful handshake is created by the `websockets.connect()` method,
    `customer_list_ws_client()` will have a loop running continuously to fetch all
    incoming consumer details from the WebSocket endpoint. The message received will
    be converted into its dictionary needed by other processes. Now, our client also
    sends an acknowledgment notification message back to the WebSocket coroutine with
    JSON data containing the *customer ID* of the profile. The loop will stop once
    the WebSocket endpoint closes its connection.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Let us now explore other asynchronous programming features that can work with
    the FastAPI framework.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Applying reactive programming in tasks
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reactive programming is a paradigm that involves the generation of streams that
    undergo a series of operations to propagate some changes during the process. Python
    has an *RxPY* library that offers several methods that we can apply to these streams
    asynchronously to extract the terminal result as desired by the subscribers.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: In the reactive programming paradigm, all intermediate operators working along
    the streams will execute to propagate some changes if there is an `Observable`
    instance beforehand and an `Observer` that subscribes to this instance. The main
    goal of this paradigm is to achieve the desired result at the end of the propagation
    process using functional programming.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Observable data using coroutines
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It all starts with the implementation of a coroutine function that will emit
    these streams of data based on a business process. The following is an `Observable`
    function that emits publication details in `str` format for those publications
    that did well in sales:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'An `Observable` function can be synchronous or asynchronous. Our target is
    to create an asynchronous one such as `process_list()`. The coroutine function
    should have the following callback methods to qualify as an `Observable` function:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: An `on_next()` method that emits items given a certain condition
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An `on_completed()` method that is executed once when the function has completed
    the operation
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An `on_error()` method that is called when an error occurs on `Observable`
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our `process_list()` emits the details of the publication that gained some
    profit. Then, we create an `asyncio` task for the call of the `process_list()`
    coroutine. We created a nested function, `evaluate_profit()`, which returns the
    `Disposable` task required by RxPY’s `create()` method for the production of the
    `Observable` stream. The cancellation of this task happens when the `Observable`
    stream is all consumed. The following is the complete implementation for the execution
    of the asynchronous `Observable` function and the use of the `create()` method
    to generate streams of data from this `Observable` function:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The subscriber created by `create_observable()`is our application’s `list_sales_by_quota()`
    API service. It needs to get the current event loop running for the method to
    generate the observable. Afterward, it invokes the `subscribe()` method to send
    a subscription to the stream and extract the needed result. The Observable’s `subscribe()`
    method is invoked for a client to subscribe to the stream and observe the occurring
    propagations:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The `list_sales_by_quote()` coroutine service shows us how to subscribe to
    an `Observable`. A subscriber should utilize the following callback methods:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: An `on_next()` method to consume all the items from the stream
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An `on_completed()` method to indicate the end of the subscription
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An `on_error()` method to flag an error during the subscription process
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And since the `Observable` processes run asynchronously, the scheduler is an
    optional argument that provides the right manager to schedule and run these processes.
    The API service used `AsyncIOScheduler` as the appropriate schedule for the subscription.
    But there are other shortcuts to generating Observables that do not use a custom
    function.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Creating background process
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As when we create continuously running Observables, we use the `interval()`
    function instead of using a custom `Observable` function. Some observables are
    designed to end successfully, but some are created to run continuously in the
    background. The following Observable runs in the background periodically to provide
    some updates on the total amount received from newspaper subscriptions:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The `interval()` method creates a stream of data periodically in seconds. But
    this Observable imposes some propagations on its stream because of the execution
    of the `pipe()` method. The Observable’s `pipe()` method creates a pipeline of
    reactive operators called the intermediate operators. This pipeline can consist
    of a chain of operators running one at a time to change items from the streams.
    It seems that this series of operations creates multiple subscriptions on the
    subscriber. So, `fetch_records()` has a `map()` operator in its pipeline to extract
    the result from the `compute_subcription()` method. It uses `merge_all()` at the
    end of the pipeline to merge and flatten all substreams created into one final
    stream, the stream expected by the subscriber. Now, we can also generate Observable
    data from files or API response.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Accessing API resources
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another way of creating an Observable is using the `from_()` method, which extracts
    resources from files, databases, or API endpoints. The Observable function retrieves
    its data from a JSON document generated by an API endpoint from our application.
    The assumption is that we are running the application using `hypercorn`, which
    uses *HTTP/2*, and so we need to bypass the TLS certificate by setting the `verify`
    parameter of `httpx.AsyncClient()` to `False`.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code highlights the `from_()` in the `fetch_subscription()` operation,
    which creates an Observable that emits streams of `str` data from the `https://localhost:8000/ch08/subscription/list/all`
    endpoint. These reactive operators of the Observable, namely `filter()`, `map()`,
    and `merge_all()`, are used to propagate the needed contexts along the stream:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The `filter()` method is another pipeline operator that returns Boolean values
    from a validation rule. It executes the following `filter_within_dates()` to verify
    whether the record retrieved from the JSON document is within the date range specified
    by the subscriber:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'On the other hand, the following `convert_str()` is a coroutine function executed
    by the `map()` operator to generate a concise profile detail of the newspaper
    subscribers derived from the JSON data:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Running these two functions modifies the original emitted data stream from
    JSON to a date-filtered stream of `str` data. The coroutine `list_dated_subscription()`API
    service, on the other hand, subscribes to `fetch_subscription()` to extract the
    newspaper subscriptions within the `min_date` and `max_date` range:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Although the FastAPI framework does not yet fully support reactive programming,
    we can still create coroutines that can work with various RxPY utilities. Now,
    we will explore how coroutines are not only for background processes but also
    for FastAPI event handlers.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Customizing events
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The FastAPI framework has special functions called *event handlers* that execute
    before the application starts up and during shutdown. These events are activated
    every time the `uvicorn` or `hypercorn` server reloads. Event handlers can also
    be coroutines.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Defining the startup event
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The *startup event* is an event handler that the server executes when it starts
    up. We decorate the function with the `@app.on_event("startup")` decorator to
    create a startup event. Applications may require a startup event to centralize
    some transactions, such as the initial configuration of some components or the
    set up of data-related resources. The following example is the application startup
    event that opens a database connection for the GINO repository transactions:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This `initialize()` event is defined in our application’s `main.py` file so
    that GINO can only create the connection once every server reload or restart.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Defining shutdown events
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Meanwhile, the *shutdown event* cleans up unwanted memory, destroys unwanted
    connections, and logs the reason for shutting down the application. The following
    is the shutdown event of our application that closes the GINO database connection:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We can define startup and shutdown events in APIRouter but be sure this will
    not cause transaction overlapping or collision with other routers. Moreover, event
    handlers do not work in mounted sub-applications.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The use of coroutines is one of the factors that makes the FastAPI microservice
    application fast, aside from its use of an ASGI-based server. This chapter has
    proven that using coroutines to implement API services will improve the performance
    better than utilizing more threads in the thread pool. Since the framework runs
    on an asyncio platform, we can utilize asyncio utilities to design various design
    patterns to manage the CPU-bound and I/O-bound services.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: This chapter used Celery and Redis for creating and managing asynchronous background
    tasks for behind-the-scenes transactions such as logging, system monitoring, time-sliced
    computations, and batch jobs. We learned that RabbitMQ and Apache Kafka provided
    an integrated solution for building asynchronous and loosely coupled communication
    between FastAPI components, especially for the message-passing part of these interactions.
    Most importantly, coroutines were applied to create these asynchronous and non-blocking
    background processes and message-passing solutions to enhance performance. Reactive
    programming was also introduced in this chapter through the RxPy extension module.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: This chapter, in general, concludes that the FastAPI framework is ready to build
    a microservice application that has a *reliable*, *asynchronous*, *message-driven*,
    *real-time message-passing*, and *distributed core system*. The next chapter will
    highlight other FastAPI features that provide integrations with UI-related tools
    and frameworks, API documentation using OpenAPI Specification, session handling,
    and circumventing CORS.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: Infrastructure-Related Issues, Numerical and Symbolic Computations,
    and Testing Microservices'
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final part of the book, we will discuss other essential microservice
    features, such as distributed tracing and logging, service registries, virtual
    environments, and API metrics. Serverless deployment using Docker and Docker Compose
    with NGINX as a reverse proxy will also be covered. Furthermore, we will look
    at FastAPI as a framework for building scientific applications using numerical
    algorithms from the `numpy`, `scipy`, `sympy`, and `pandas` modules to model,
    analyze, and visualize the mathematical and statistical solutions of its API services.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'This part comprises the following chapters:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B17975_09.xhtml#_idTextAnchor266)*, Utilizing Other Advanced
    Features*'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B17975_10.xhtml#_idTextAnchor292)*, Solving Numerical, Symbolic,
    and Graphical Problems*'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B17975_11.xhtml#_idTextAnchor321)*, Adding Other Microservice
    Features*'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
