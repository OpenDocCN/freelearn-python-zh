- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Network Data Analysis with Elastic Stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In *Chapter 7*, *Network Monitoring with Python – Part 1*, and *Chapter 8*,
    *Network Monitoring with Python – Part 2*, we discussed the various ways to monitor
    a network. In the two chapters, we looked at two different approaches for network
    data collection: we can either retrieve data from network devices such as SNMP,
    or we can listen for the data sent by network devices using flow-based exports.
    After the data is collected, we will need to store the data in a database, then
    analyze the data to gain insights to decide what the data means. Most of the time,
    the analyzed results are displayed in a graph, whether a line graph, bar graph,
    or pie chart. We can use individual tools such as PySNMP, Matplotlib, and Pygal
    for each step, or we can leverage all-in-one tools such as Cacti or ntop for monitoring.
    The tools introduced in those two chapters gave us basic monitoring and understanding
    of the network.'
  prefs: []
  type: TYPE_NORMAL
- en: We then moved on to *Chapter 9*, *Building Network Web Services with Python*,
    to build API services to abstract our network from higher-level tools. In *Chapter
    11*, *AWS Cloud Networking*, and *Chapter 12*, *Azure Cloud Networking*, we extended
    our on-premises network to the cloud using AWS and Azure. We have covered much
    ground in these chapters and have a solid set of tools to help us make our network
    programmable.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with this chapter, we will build on our toolsets from previous chapters
    and look at other tools and projects that I have found useful in my journey once
    I was comfortable with the tools covered in previous chapters. In this chapter,
    we will take a look at an open source project, Elastic Stack ([https://www.elastic.co](https://www.elastic.co)),
    that can help us with analyzing and monitoring our network beyond what we have
    seen before.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the Elastic (or ELK) Stack?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elastic Stack installation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data ingestion with Logstash
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data ingestion with Beats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Search with Elasticsearch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data visualization with Kibana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s begin by answering the question: what exactly is the Elastic Stack?'
  prefs: []
  type: TYPE_NORMAL
- en: What is the Elastic Stack?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Elastic Stack is also known as the “ELK” Stack. So, what is it? Let’s see
    what the developers have to say in their own words ([https://www.elastic.co/what-is/elk-stack](https://www.elastic.co/what-is/elk-stack)):'
  prefs: []
  type: TYPE_NORMAL
- en: '”ELK” is the acronym for three open source projects: Elasticsearch, Logstash,
    and Kibana. Elasticsearch is a search and analytics engine. Logstash is a serverside
    data processing pipeline that ingests data from multiple sources simultaneously,
    transforms it, and then sends it to a “stash” like Elasticsearch. Kibana lets
    users visualize data with charts and graphs in Elasticsearch. The Elastic Stack
    is the next evolution of the ELK Stack.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B18403_13_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.1: Elastic Stack (source: https://www.elastic.co/what-is/elk-stack)'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the statement, the Elastic Stack is a collection of different
    projects working together to cover the whole spectrum of data collection, storage,
    retrieval, analytics, and visualization. What is nice about the stack is that
    it is tightly integrated, but each component can also be used separately. If we
    dislike Kibana for visualization, we can easily plug in Grafana for the graphs.
    What if we have other data ingestion tools that we want to use? No problem, we
    can use the RESTful API to post our data to Elasticsearch. At the center of the
    stack is Elasticsearch, an open source, distributed search engine. The other projects
    were created to enhance and support the search function. This might sound a bit
    confusing at first, but as we look deeper at the components of the project, it
    will become clearer.
  prefs: []
  type: TYPE_NORMAL
- en: Why did they change the name of ELK Stack to Elastic Stack? In 2015, Elastic
    introduced a family of lightweight, single-purpose data shippers called Beats.
    They were an instant hit and continue to be very popular, but the creators could
    not come up with a good acronym for the “B” and decided to just rename the whole
    stack to Elastic Stack.
  prefs: []
  type: TYPE_NORMAL
- en: We will focus on the network monitoring and data analysis aspects of the Elastic
    Stack. Still, the stack has many use cases, including risk management, e-commerce
    personalization, security analysis, fraud detection, and more. It is being used
    by various organizations, from web companies such as Cisco, Box, and Adobe, to
    government agencies such as NASA JPL, the United States Census Bureau, and others
    ([https://www.elastic.co/customers/](https://www.elastic.co/customers/)).
  prefs: []
  type: TYPE_NORMAL
- en: When we talk about Elastic, we are referring to the company behind the Elastic
    Stack. The tools are open source and the company makes money by selling support,
    hosted solutions, and consulting around open source projects. The company stock
    is publicly traded on the New York Stock Exchange with the ESTC symbol.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a better idea of what the ELK Stack is, let’s take a look at
    the lab topology for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Lab topology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the network lab, we will reuse the network topology we used in *Chapter
    8*, *Network Monitoring with Python – Part 2\.* The network gear will have the
    management interfaces in the `192.168.2.0/24` management network with the interconnections
    in the `10.0.0.0/8` network and the subnets in `/30s`.
  prefs: []
  type: TYPE_NORMAL
- en: Where can we install the ELK Stack in the lab? In production, we should run
    the ELK Stack in a dedicated cluster. In our lab, however, we can quickly spin
    up a testing instance via Docker containers. If a refresher of Docker is needed,
    please refer to *Chapter 5*, *Docker Containers for Network Engineers*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a graphical representation of our network lab topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B18403_13_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.2: Lab Topology'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Device** | **Management IP** | **Loopback IP** |'
  prefs: []
  type: TYPE_TB
- en: '| r1 | `192.168.2.218` | `192.168.0.1` |'
  prefs: []
  type: TYPE_TB
- en: '| r2 | `192.168.2.219` | `192.168.0.2` |'
  prefs: []
  type: TYPE_TB
- en: '| r3 | `192.168.2.220` | `192.168.0.3` |'
  prefs: []
  type: TYPE_TB
- en: '| r5 | `192.168.2.221` | `192.168.0.4` |'
  prefs: []
  type: TYPE_TB
- en: '| r6 | `192.168.2.222` | `192.168.0.5` |'
  prefs: []
  type: TYPE_TB
- en: 'The Ubuntu hosts information is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Device Name** | **External Link Eth0** | **Internal IP Eth1** |'
  prefs: []
  type: TYPE_TB
- en: '| Client | `192.168.2.211` | `10.0.0.9` |'
  prefs: []
  type: TYPE_TB
- en: '| Server | `192.168.2.212` | `10.0.0.5` |'
  prefs: []
  type: TYPE_TB
- en: 'To run multiple containers, we should allocate at least 4 GB RAM or more to
    the host. Let’s start Docker Engine, if not done already, then pull the image
    from Docker Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When the Docker container is run, the generated default Elastic user password
    and Kibana enrollment token are output to the terminal; please take a note of
    them as we will need them later. You might need to scroll up the screen a bit
    to find them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the Elasticsearch container runs, we can test out the instance by browsing
    to `https://<your ip>:9200`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B18403_13_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.3: Elasticsearch Initial Result'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then pull and run the Kibana container image from a separate terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Once Kibana boots up, we can access it via port 5601:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, chat or text message  Description
    automatically generated](img/B18403_13_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.4: Kibana Start Page'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice it is asking for the enrolment token we jotted down before. We can paste
    that in and click on **Configure Elastic**. It will prompt us for a token, which
    is now displayed on the Kibana terminal. Once that is authenticated, Kibana will
    start to configure Elastic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application  Description automatically generated](img/B18403_13_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.5: Configurating Elastic'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we should be able to access the Kibana interface at `http://<ip>:5601`.
    We do not need any integration at this point; we will pick **Explore on my own**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18403_13_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.6:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be presented with an option to load some sample data. This is a great
    way to get our feet wet with the tool, so let’s import this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application, website  Description automatically
    generated](img/B18403_13_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.7: Kibana Home Page'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will choose **Try sample data** and add the sample eCommerce orders, sample
    flight data, and sample web logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18403_13_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.8: Adding Sample Data'
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, we now have Elasticsearch and Kibana running as containers with
    forwarded ports on the management host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Great! We are almost done. The last piece of the puzzle is Logstash. Since
    we will be working with different Logstash configuration files, modules, and plugins,
    we will install it on the management host with a package instead of a Docker container.
    Logstash requires Java to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can download the Logstash bundled package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We will modify a few fields in the Logstash configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We will not start Logstash just yet. We will wait until we have installed the
    network-related plugins and created the necessary configuration file later in
    the chapter to start the Logstash process.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a moment to look at deploying the ELK Stack as a hosted service in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Elastic Stack as a service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Elasticsearch is a popular service available as a hosted option by both Elastic.co
    and other cloud providers. Elastic Cloud ([https://www.elastic.co/cloud/](https://www.elastic.co/cloud/))
    does not have an infrastructure of its own, but it offers the option to spin up
    deployments on AWS, Google Cloud Platform, or Azure. Because Elastic Cloud is
    built on other public cloud VM offerings, the cost will be a bit more than getting
    it directly from a cloud provider, such as AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Timeline  Description automatically generated](img/B18403_13_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.9: Elastic Cloud Offerings'
  prefs: []
  type: TYPE_NORMAL
- en: AWS offers the hosted OpenSearch product ([https://aws.amazon.com/opensearch-service/](https://aws.amazon.com/opensearch-service/))
    tightly integrated with the existing AWS offerings. For example, AWS CloudWatch
    Logs can be streamed directly to the AWS OpenSearch instance ([https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_OpenSearch_Stream.html)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: From my own experience, as attractive as the Elastic Stack is for its advantages,
    it is a project that I feel is easy to get started but hard to scale without a
    steep learning curve. The learning curve is even steeper when we do not deal with
    Elasticsearch on a daily basis. If you, like me, want to take advantage of the
    features Elastic Stack offers but do not want to become a full-time Elastic engineer,
    I would highly recommend using one of the hosted options for production.
  prefs: []
  type: TYPE_NORMAL
- en: Which hosted provider to choose depends on your preference of cloud provider
    lockdown and if you want to use the latest features. Since Elastic Cloud is built
    by the folks behind the Elastic Stack project, they tend to offer the latest features
    faster than AWS. On the other hand, if your infrastructure is fully built in the
    AWS cloud, having a tightly integrated OpenSearch instance saves you the time
    and effort required to maintain a separate cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at an end-to-end example from data ingestion to visualization in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: First End-to-End example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most common pieces of feedback from people new to Elastic Stack is
    the amount of detail you need to understand to get started. To get the first usable
    record in the Elastic Stack, the user needs to build a cluster, allocate master
    and data nodes, ingest the data, create the index, and manage it via the web or
    command-line interface. Over the years, Elastic Stack has simplified the installation
    process, improved its documentation, and created sample datasets for new users
    to get familiar with the tools before using the stack in production.
  prefs: []
  type: TYPE_NORMAL
- en: Running the components in Docker containers helps with some of the pain of installation
    but increases the complexity of maintenance. It is a balancing act to choose between
    running them in a virtual machine vs. containers.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dig deeper into the different components of the Elastic Stack, it
    is helpful to look at an example that spans Logstash, Elasticsearch, and Kibana.
    By going over this end-to-end example, we will become familiar with the function
    that each component provides. When we look at each component in more detail later
    in the chapter, we can compartmentalize where the particular component fits into
    the overall picture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by putting our log data into Logstash. We will configure each of
    the routers to export the log data to the Logstash server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'On our Elastic Stack host, with all of the components installed, we will create
    a simple Logstash configuration that listens on UDP port `5144` and outputs the
    data to the console in JSON format as well as the Elasticsearch host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The configuration file consists of only an input section and an output section
    without modifying the data. The type, `syslog-ios`, is a name we picked to identify
    this index. In the `output` section, we configure the index name with variables
    representing today’s date. We can run the Logstash process directly from the binary
    directory in the foreground:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, Elasticsearch allows automatic index generation when data is sent
    to it. We can generate some log data on the router by resetting the interface,
    reloading BGP, or simply going into the configuration mode and exiting out. Once
    there are some new logs generated, we will see the `cisco-syslog-<date>` index
    being created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can do a quick `curl` to see the index created on Elasticsearch.
    The `curl` command use the `insecure` flag to accommodate the self-signed certificate.
    The URL is in the “`https://<username>:<password>@<ip><port>/<path`>” format.
    `"_cat/indices/cisco*"` shows the category of indices, then match the indices
    name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use Kibana to create the index by going to **Menu -> Management
    -> Stack Management**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B18403_13_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.10: Stack Management'
  prefs: []
  type: TYPE_NORMAL
- en: 'Under **Data -> Index Management**, we can see the newly created **cisco-syslog**
    index:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B18403_13_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.11: Index Management'
  prefs: []
  type: TYPE_NORMAL
- en: We can now move to **Stack Management -> Kibana -> Data Views** to create a
    data view.
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B18403_13_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.12: Create New Data Views Step 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the index is already in Elasticsearch, we will only need to match the
    index name. Remember that our index name is a variable based on time; we can use
    a star wildcard (`*`) to match all the current and future indices starting with
    **cisco-syslog**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B18403_13_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.13: Create New Data Views Step 2'
  prefs: []
  type: TYPE_NORMAL
- en: Our index is time-based, that is, we have a field that can be used as a timestamp,
    and we can search based on time. We should specify the field that we designated
    as the timestamp. In our case, Elasticsearch was already smart enough to pick
    a field from our syslog for the timestamp; we just need to choose it in the second
    step from the drop-down menu.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the index pattern is created, we can use the **Menu -> Discover** (under
    **Analytics**) tab to look at the entries. Make sure you pick the right indices
    and the time range:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application  Description automatically generated](img/B18403_13_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.14: Elasticsearch Index Document Discovery'
  prefs: []
  type: TYPE_NORMAL
- en: After we have collected some more log information, we can stop the Logstash
    process by using *Ctrl + C* on the Logstash process. This first example shows
    how we can leverage the Elastic Stack pipeline from data ingestion, storage, and
    visualization. The data ingestion used in Logstash (or Beats) is a continuous
    data stream that automatically flows into Elasticsearch. The Kibana visualization
    tool provides a way for us to analyze the data in Elasticsearch in a more intuitive
    way, then create a permanent visualization once we are happy with the result.
    There are more visualization graphs we can create with Kibana, which we will see
    more examples of later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Even with just one example, we can see that the most important part of the workflow
    is Elasticsearch. It is the simple RESTful interface, storage scalability, automatic
    indexing, and quick search result that gives the stack the power to adapt to our
    network analysis needs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at how we can use Python to interact with
    Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch with a Python client
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can interact with Elasticsearch via its HTTP RESTful API using a Python
    library. For instance, in the following example, we will use the `requests` library
    to perform a `GET` operation to retrieve information from the Elasticsearch host.
    For example, we know that `HTTP GET` for the following URL endpoint can retrieve
    the current indices starting with `kibana`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the `requests` library to make a similar function in a Python script,
    `Chapter13_1.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing the script will give us a list of indices starting with `kibana`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use the Python Elasticsearch client, [https://elasticsearch-py.readthedocs.io/en/master/](https://elasticsearch-py.readthedocs.io/en/master/).
    It is designed as a thin wrapper around Elasticsearch’s RESTful API to allow for
    maximum flexibility. Let’s install it and run a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The example, `Chapter13_2`, simply connects to the Elasticsearch cluster and
    does a search for anything that matches the indices that start with `kibana`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, the result will return the first 10,000 entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Using the simple script, the advantage of the client library is not obvious.
    However, the client library is very helpful when we create a more complex search
    operation, such as a scroll where we need to use the returned token per query
    to continue executing the subsequent queries until all the results are returned.
    The client can also help with more complicated administrative tasks, such as when
    we need to re-index an existing index. We will see more examples using the client
    library in the remainder of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at more data ingestion examples from our Cisco
    device syslogs.
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion with Logstash
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last example, we used Logstash to ingest log data from network devices.
    Let’s build on that example and add a few more configuration changes in `network_config/config_2.cfg`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the input section, we will listen on two UDP ports, `5144` and `5145`. When
    the logs are received, we will tag the log entries with either `syslog-core` or
    `syslog-edge`. We will also add a filter section to the configuration to specifically
    match the `syslog-edge` type and apply a regular expression section, `Grok`, for
    the message section. In this case, we will match everything and add an extra field,
    `received_at`, with the value of the timestamp.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on Grok, take a look at the following documentation: [https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html](https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will change `r5` and `r6` to send syslog information to UDP port `5145`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'When we start the Logstash server, we will see that both ports are now listening:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'By separating out the entries using different types, we can specifically search
    for the types in the Kibana **Discover** dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B18403_13_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.15: Syslog Index'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we expand on the entry with the `syslog-edge` type, we can see the new field
    that we added:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application  Description automatically generated](img/B18403_13_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.16: Syslog Timestamp'
  prefs: []
  type: TYPE_NORMAL
- en: The Logstash configuration file provides many options in the input, filter,
    and output. In particular, the **Filter** section provides ways for us to enhance
    the data by selectively matching the data and further processing it before outputting
    it to Elasticsearch. Logstash can be extended with modules; each module provides
    a quick end-to-end solution for ingesting data and visualizations with purpose-built
    dashboards.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on the Logstash modules, take a look at the following
    document: [https://www.elastic.co/guide/en/logstash/8.4/logstash-modules.html](https://www.elastic.co/guide/en/logstash/8.4/logstash-modules.html)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: Elastic Beats are similar to Logstash modules. They are single-purpose data
    shippers, usually installed as an agent, that collect data on the host and send
    the output data either directly to Elasticsearch or Logstash for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: There are hundreds of different downloadable Beats, such as Filebeat, Metricbeat,
    Packetbeat, Heartbeat, and so on. In the next section, we will see how we can
    use Filebeat to ingest Syslog data into Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion with Beats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As good as Logstash is, the data ingestion process can get complicated and hard
    to scale. If we expand on our network log example, we can see that even with just
    network logs, it can get complicated trying to parse different log formats from
    IOS routers, NXOS routers, ASA firewalls, Meraki wireless controllers, and more.
    What if we need to ingest log data from Apache web logs, server host health, and
    security information? What about data formats such as NetFlow, SNMP, and counters?
    The more data we need to aggregate, the more complicated it can get.
  prefs: []
  type: TYPE_NORMAL
- en: 'While we cannot completely get away from aggregation and the complexity of
    data ingestion, the current trend is to move toward a more lightweight, single-purpose
    agent that sits as close to the data source as possible. For example, we can have
    a data collection agent installed directly on our Apache server specialized in
    collecting web log data; or we can have a host that only collects, aggregates,
    and organizes Cisco IOS logs. Elastic Stack collectively calls these lightweight
    data shippers Beats: [https://www.elastic.co/products/beats](https://www.elastic.co/products/beats).'
  prefs: []
  type: TYPE_NORMAL
- en: Filebeat is a version of Elastic Beats software intended for forwarding and
    centralizing log data. It looks for the log file we specified in the configuration
    to be harvested; once it has finished processing, it will send the new log data
    to an underlying process that aggregates the events and outputs to Elasticsearch.
    In this section, we will look at using Filebeat with the Cisco modules to collect
    network log data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s install Filebeat and set up the Elasticsearch host with the bundled visualization
    template and index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The directory layout can be confusing because they are installed in various
    `/usr`, `/etc/`, and `/var` locations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B18403_13_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.17: Elastic Filebeat File Locations (source: https://www.elastic.co/guide/en/beats/filebeat/8.4/directory-layout.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will make a few changes to the configuration file, `/etc/filebeat/filebeat.yml`,
    for the location of Elasticsearch and Kibana:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Filebeat can be used to set up the index templates and example Kibana dashboards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s enable the `cisco` module for Filebeat:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s configure the `cisco` module for `syslog` first. The file is located
    under `/etc/filebeat/modules.d/cisco.yml`. In our case, I am also specifying a
    custom log file location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can start, stop, and check the status of the Filebeat service using the
    common Ubuntu Linux command `service Filebeat` [`start`|`stop`|`status`]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Modify or add UDP port `514` for syslog on our devices. We should be able to
    see the syslog information under the **filebeat-*** index search:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B18403_13_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.18: Elastic Filebeat Index'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we compare that to the previous syslog example, we can see that there are
    a lot more fields and meta information associated with each record, such as `agent.version`,
    `event.code`, and `event.severity`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B18403_13_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.19: Elastic Filebeat Cisco Log'
  prefs: []
  type: TYPE_NORMAL
- en: Why do the extra fields matter? Among other advantages, the fields make search
    aggregation easier, and this, in turn, allows us to graph the results better.
    We will see graphing examples in the upcoming section where we discuss Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the `cisco` module, there are modules for Palo Alto Networks, AWS, Google
    Cloud, MongoDB, and many more. The most up-to-date module list can be viewed at
    [https://www.elastic.co/guide/en/beats/filebeat/8.4/filebeat-modules.html](https://www.elastic.co/guide/en/beats/filebeat/8.4/filebeat-modules.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'What if we want to monitor NetFlow data? No problem, there is a module for
    that! We will run through the same process with the Cisco module by enabling the
    module and setting up the dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, configure the module configuration file, `/etc/filebeat/modules.d/netflow.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We will configure the devices to send the NetFlow data to port `2055`. If you
    need a refresher, please read the relevant configuration in *Chapter 8*, *Network
    Monitoring with Python – Part 2*. We should be able to see the new `netflow` data
    input type:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, website  Description automatically generated](img/B18403_13_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.20: Elastic NetFlow Input'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that each module came pre-bundled with visualization templates? Not
    to jump ahead too much into visualization, but if we click on the **visualization**
    tab on the left panel, then search for **netflow**, we can see a few visualizations
    that were created for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18403_13_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.21: Kibana Visualization'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **Conversation Partners [Filebeat Netflow]** option, which will
    give us a nice table of the top talkers that we can reorder by each of the fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B18403_13_22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.22: Kibana Table'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will focus on the Elasticsearch part of the ELK Stack.
  prefs: []
  type: TYPE_NORMAL
- en: Search with Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need more data in Elasticsearch to make the search and graph more interesting.
    I would recommend reloading a few of the lab devices to have the log entries for
    interface resets, BGP and OSPF establishments, as well as device boot-up messages.
    Otherwise, feel free to use the sample data we imported at the beginning of this
    chapter for this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look back at the `Chapter13_2.py` script example, when we searched, there
    were two pieces of information that could potentially change from each query:
    the index and query body. What I typically like to do is to break that information
    into input variables that I can dynamically change at runtime to separate the
    logic of the search and the script itself. Let’s make a file called `query_body_1.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We will create a script, `Chapter13_3.py`, that uses `argparse` to take the
    user input at the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then use the two input values to construct the search the same way we
    have done before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the `help` option to see what arguments should be supplied with
    the script. Here are the results when we use the same query against the two different
    indices we created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: When developing our search, it usually takes a few tries before we get the result
    we are looking for. One of the tools Kibana provides is a developer console that
    allows us to play around with the search criteria and view the search results
    on the same page. The tool is under the menu section *Management for Dev Tools*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in the following figure, we execute the same search we have done
    now and we’re able to see the returned JSON result. This is one of my favorite
    tools on the Kibana interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text  Description automatically generated](img/B18403_13_23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.23: Kibana Dev Tools'
  prefs: []
  type: TYPE_NORMAL
- en: 'Much of the network data is based on time, such as the log and NetFlow data
    we have collected. The values are taken at a snapshot in time, and we will likely
    group the value in a time scope. For example, we might want to know, “who are
    the NetFlow top talkers in the last 7 days?” or “which device has the most BGP
    reset messages in the last hour?” Most of these questions have to do with aggregation
    and time scope. Let’s look at a query that limits the time range, `query_body_2.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a Boolean query, [https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-bool-query.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-bool-query.html),
    which means it can take a combination of other queries. In our query, we use the
    filter to limit the time range to be the last 10 minutes. We copy the `Chapter13_3.py`
    script to `Chapter13_4.py` and modify the output to grab the number of hits as
    well as a loop over the actual returned results list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing the script will show that we only have `23` hits in the last 10 minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can add another filter option in the query to limit the source IP via `query_body_3.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The result will be limited by both the source IP of r1’s loopback IP in the
    last 10 minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s modify the search body one more time to add an aggregation, [https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket.html),
    that takes a sum of all the network bytes from our previous search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The result will be different every time we run the script `Chapter13_5.py`.
    The current result is about 1 MB for me when I run the script consecutively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, building a search query is an iterative process; you typically
    start with a wide net and gradually narrow the criteria to fine-tune the results.
    In the beginning, you will probably spend a lot of time reading the documentation
    and searching for the exact syntax and filters. As you gain more experience under
    your belt, the search syntax will become easier. Going back to the previous visualization
    we saw from the `netflow` module setup for the NetFlow top talker, we can use
    the inspection tool to see the **Request** body:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application  Description automatically generated](img/B18403_13_24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.24: Kibana Request'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can put that into a query JSON file, `query_body_5.json`, and execute it
    with the `Chapter13_6.py` file. We will receive the raw data that the graph was
    based on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next section, let’s take a deeper look at the visualization part of
    the Elastic Stack: Kibana.'
  prefs: []
  type: TYPE_NORMAL
- en: Data visualization with Kibana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have used Kibana to discover data, manage indices in Elasticsearch,
    use developer tools to develop queries, and use a few other features. We also
    saw the pre-populated visualization charts from NetFlow, which gave us the top
    talker pair from our data. In this section, we will walk through the steps of
    creating our own graphs. We will start by creating a pie chart.
  prefs: []
  type: TYPE_NORMAL
- en: 'A pie chart is great at visualizing a portion of a component in relation to
    the whole. Let’s create a pie chart based on the Filebeat index that graphs the
    top 10 source IP addresses based on the number of record counts. We will select
    **Dashboard -> Create dashboard -> Create visualization -> Pie**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B18403_13_25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.25: Kibana Pie Chart'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we will type **netflow** in the search bar to pick our **[Filebeat NetFlow]**
    indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B18403_13_26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.26: Kibana Pie Chart Source'
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, we are given the total count of all the records in the default
    time range. The time range can be dynamically changed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18403_13_27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.27: Kibana Time Range'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can assign a custom label for the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Icon  Description automatically generated](img/B18403_13_28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.28: Kibana Chart Label'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s click on the **Add** option to add more buckets. We will choose to split
    the slices, pick the terms for aggregation, and select the **source.ip** field
    from the drop-down menu. We will leave the **order** **Descending** but increase
    S**ize** to **10**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The change will only be applied when you click the **Apply** button at the
    top. It is a common mistake to expect the change to happen in real time when using
    a modern website and not by clicking on the **Apply** button:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B18403_13_29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.29: Kibana Play Button'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can click on **Options** at the top to turn off **Donut** and turn on **Show
    labels**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B18403_13_30.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.30: Kibana Chart Options'
  prefs: []
  type: TYPE_NORMAL
- en: 'The final graph is a nice pie chart showing the top IP sources based on the
    number of document counts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, pie chart  Description automatically generated](img/B18403_13_31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.31: Kibana Pie Chart'
  prefs: []
  type: TYPE_NORMAL
- en: 'As with Elasticsearch, the Kibana graph is also an iterative process that typically
    takes a few tries to get right. What if we split the result into different charts
    instead of slices on the same chart? Yeah, that is not very visually appealing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B18403_13_32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.32: Kibana Split Chart'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s stick to splitting things into slices on the same pie chart and change
    the time range to **Last 1 hour**, then save the chart so that we can come back
    to it later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we can also share the graph either in an embedded URL (if Kibana
    is accessible from a shared location) or a snapshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, pie chart  Description automatically generated](img/B18403_13_33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.33: Kibana Save Chart'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also do more with the metrics operations. For example, we can pick the
    data table chart type and repeat our previous bucket breakdown with the source
    IP. But we can also add a second metric by adding up the total number of network
    bytes per bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B18403_13_34.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.34: Kibana Metrics'
  prefs: []
  type: TYPE_NORMAL
- en: 'The result is a table showing both the number of document counts as well as
    the sum of the network bytes. This can be downloaded in CSV format for local storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, table  Description automatically generated with
    medium confidence](img/B18403_13_35.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.35: Kibana Tables'
  prefs: []
  type: TYPE_NORMAL
- en: Kibana is a very powerful visualization tool in the Elastic Stack. We are just
    scratching the surface of its visualization capabilities. Besides many other graph
    options to better tell the story of your data, we can also group multiple visualizations
    onto a dashboard to be displayed. We can also use Timelion ([https://www.elastic.co/guide/en/kibana/8.4/timelion.html](https://www.elastic.co/guide/en/kibana/8.4/timelion.html))
    to group independent data sources for a single visualization or use Canvas ([https://www.elastic.co/guide/en/kibana/current/canvas.html](https://www.elastic.co/guide/en/kibana/current/canvas.html))
    as a presentation tool based on data in Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Kibana is typically used at the end of the workflow to present our data meaningfully.
    We have covered the basic workflow from data ingestion to storage, retrieval,
    and visualization in the span of a chapter. It still amazes me that we can accomplish
    so much in a short period with the aid of an integrated, open source stack such
    as Elastic Stack.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we used the Elastic Stack to ingest, analyze, and visualize
    network data. We used Logstash and Beats to ingest the network syslog and NetFlow
    data. Then we used Elasticsearch to index and categorize the data for easier retrieval.
    Finally, we used Kibana to visualize the data. We used Python to interact with
    the stack and help us gain more insights into our data. Together, Logstash, Beats,
    Elasticsearch, and Kibana present a powerful all-in-one project that can help
    us understand our data better.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at using Git for network development with
    Python.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book community
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To join our community for this book – where you can share feedback, ask questions
    to the author, and learn about new releases – follow the QR code below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/networkautomationcommunity](https://packt.link/networkautomationcommunity)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code2903617220506617062.png)'
  prefs: []
  type: TYPE_IMG
