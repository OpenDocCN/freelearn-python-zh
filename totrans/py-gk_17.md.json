["```py\n#iris_data_analysis.py\nfrom pandas import read_csv\nfrom matplotlib import pyplot\ndata_file = \"iris/iris.data\"\niris_names = ['sepal-length', 'sepal-width', 'petal-  length', 'petal-width', 'class']\ndf = read_csv(data_file, names=iris_names)\nprint(df.shape)\nprint(df.head(20))\nprint(df.describe())\nprint(df.groupby('class').size())\n# box and whisker plots\ndf.plot(kind='box', subplots=True, layout=(3,3),   sharex=False, sharey=False)\npyplot.show()\n# check the histograms\ndf.hist()\npyplot.show()\n```", "```py\n       sepal-length  sepal-width  petal-length  petal-width\ncount    150.000000   150.000000    150.000000   150.000000\nmean       5.843333     3.054000      3.758667     1.198667\nstd        0.828066     0.433594      1.764420     0.763161\nmin        4.300000     2.000000      1.000000     0.100000\n25%        5.100000     2.800000      1.600000     0.300000\n50%        5.800000     3.000000      4.350000     1.300000\n75%        6.400000     3.300000      5.100000     1.800000\nmax        7.900000     4.400000      6.900000     2.500000\n```", "```py\nIris-setosa        50\nIris-versicolor    50\nIris-virginica     50\n```", "```py\n    #train_test_split function, we split the full dataset into a features dataset (typically called X, which should be uppercase in machine learning nomenclature) and the expected output dataset (called y, which should be lowercase in machine learning nomenclature). These two datasets (X and y) are split by the train_test_split function as per our test_size (20%, in our case). We also allow the data to be shuffled before splitting it. The output of this operation will give us four datasets (X_train, y_train, X_test, and y_test) for training and testing purposes. \n    ```", "```py\n    fit method. In the next statement, we made predictions based on the testing data (X_test). These predictions will be used to evaluate the performance of our trained model. \n    ```", "```py\n    #iris_build_svm_model.py (#3)\n    # predictions evaluation\n    print(accuracy_score(y_test, predictions))\n    print(classification_report(y_test, predictions))\n    ```", "```py\n0.9666666\n    Iris-setosa       1.00      1.00      1.00        11\nIris-versicolor       1.00      0.92      0.96        13\n Iris-virginica       0.86      1.00      0.92         6\n       accuracy                           0.97        30\n      macro avg       0.95      0.97      0.96        30\n   weighted avg       0.97      0.97      0.97        30\n```", "```py\n#iris_eval_svc_model.py (part 1 of 2)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import   GridSearchCV,RandomizedSearchCV\nfrom sklearn.datasets import load_iris\nfrom sklearn.svm import SVC\niris= load_iris()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test=train_test_split   (X,y,test_size=0.2)\nparams = {\"C\":[0.001, 0.01, 1, 5, 10, 100],              \"gamma\": [0.001, 0.01, 0.1, 1, 10, 100]}\nmodel=SVC()\ngrid_cv=GridSearchCV(model, params,  cv=5)\ngrid_cv.fit(X_train,y_train)\nprint(f\"GridSearch- best parameter:{grid_cv.best_params_}\")\nprint(f\"GridSearch- accuracy: {grid_cv.best_score_}\")\nprint(classification_report(y_test,   grid_cv.best_estimator_.predict( X_test)))\n```", "```py\nGridSearch- best parameter: {'C': 5, 'gamma': 0.1}\nGridSearch- accuracy: 0.9833333333333334\n```", "```py\n#iris_eval_svc_model.py (part 2 of 2)\nrand_cv=RandomizedSearchCV(model, params, n_iter = 5, cv=5)\nrand_cv.fit(x_train,y_train)\nprint(f\" RandomizedSearch - best parameter:   {rand_cv.best_params_}\")\nprint(f\" RandomizedSearch - accuracy:   {rand_cv.best_score_}\")\n```", "```py\nRandomizedSearch- best parameter: {'gamma': 10, 'C': 5}\nRandomizedSearch- accuracy: 0.9333333333333333\n```", "```py\n#iris_save_load_predict_model.py \n#model is creating using the code in #iris_build_svm_model.py\n#saving the model to a file\nwith open(\"model.pkl\", 'wb') as file:\n   pickle.dump(model, file)\n#loading the model from a file (in another application\nwith open(\"model.pkl\", 'rb') as file:\n   loaded_model = pickle.load(file)\n   x_new = [[5.6, 2.6, 3.9, 1.2]]\n   y_new = loaded_model.predict(x_new)\n   print(\"X=%s, Predicted=%s\" % (x_new[0], y_new[0]))\n```", "```py\n#iris_save_load_predict_gridmodel.py \n#grid_cv is created and trained using the code in the \n  #iris_eval_svm_model.py\njoblib.dump(grid_cv.best_estimator_, \"model.joblib\")\nloaded_model = joblib.load(\"model.joblib\")\nx_new = [[5.6, 2.5, 3.9, 1.1]]\ny_new = loaded_model.predict(x_new)\nprint(\"X=%s, Predicted=%s\" % (x_new[0], y_new[0]))\n```", "```py\n    gsutil mb gs://<bucket name>\n    gsutil mb gs://muasif-svc-model #Example bucket created\n    ```", "```py\n    gsutil model with an extension such as pkl, joblib, or bst, depending on the library we used to package the model.We can now initiate a workflow to create a model object on the AI Platform by using the following command. Note that the name of the model must include only alphanumeric and underscore characters:\n\n    ```", "```py\n\n    ```", "```py\n    gcloud ai-platform versions model attribute will point to the name of the model we created in the previous step. b) The `origin` attribute will point to the storage bucket location where the model file is residing. We will only provide the directory's location, not the path to the file.c) The `framework` attribute is used to select which ML library to use. GCP offers scikit-learn, TensorFlow, and XGBoost.d) `runtime-version` is for the scikit-learn library in our case.e) `python-version` is selected as 3.7, which is the highest version offered that's by GCP AI Platform at the time of writing this book.f) The `region` attribute is set as per the region that was selected for the model.g) The `machine-type` attribute is optional and is used to indicate what type of compute node to use for model deployment. If not provided, the `n1-standard-2` machine type is used.The `versions create` command may take a few minutes to deploy a new version. Once it is done, we will get an output similar to the following:\n\n    ```", "```py\n\n    ```", "```py\n    gcloud ai-platform versions describe v1 –  model=my_iris_model\n    ```", "```py\n    [5.6, 2.5, 3.9, 1.1]\n    [3.2, 1.4, 3.0, 1.8]\n    ```", "```py\n    gcloud ai-platform predict --model my_iris_model --version v1 --json-instances input.json\n    ```", "```py\n    Using endpoint [https://us-central1-ml.googleapis.com/]\n    ['Iris-versicolor', 'Iris-virginica']\n    ```"]