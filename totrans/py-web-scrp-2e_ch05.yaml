- en: Dynamic Content
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: According to a 2006 study by the United Nations, 73 percent of leading websites
    rely on JavaScript for important functionalities (refer to [http://www.un.org/esa/socdev/enable/documents/execsumnomensa.doc](http://www.un.org/esa/socdev/enable/documents/execsumnomensa.doc)).
    The growth and popularity of model-view-controller (or MVC) frameworks within
    JavaScript such as React, AngularJS, Ember, Node and many more have only increased
    the importance of JavaScript as the primary engine for web page content.
  prefs: []
  type: TYPE_NORMAL
- en: The use of JavaScript can vary from simple form events to single page apps that
    download the entire page content after loading. One consequence of this architecture
    is the content may not available in the original HTML, and the scraping techniques
    we've covered so far will not extract the important information on the site.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover two approaches to scraping data from dynamic JavaScript
    websites. These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Reverse engineering JavaScript
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rendering JavaScript
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example dynamic web page
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at an example dynamic web page. The example website has a search
    form, which is available at [http://example.webscraping.com/search](http://example.webscraping.com/search),
    which is used to locate countries. Let''s say we want to find all the countries
    that begin with the letter A:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4364OS_05_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we right-click on these results to inspect them with our browser tools (as
    covered in [Chapter 2](py-web-scrp-2e_ch02.html), *Scraping the Data*), we would
    find the results are stored within a `div` element with ID `"results"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4364OS_05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s try to extract these results using the `lxml` module, which was also
    covered in [Chapter 2](py-web-scrp-2e_ch02.html), *Scraping the Data*, and the
    `Downloader` class from [Chapter 3](py-web-scrp-2e_ch03.html), *Caching Downloads*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The example scraper here has failed to extract results. Examining the source
    code of this web page (by using the right-click View Page Source option instead
    of using the browser tools) can help you understand why. Here, we find the `div`
    element we are trying to scrape is empty:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Our browser tools give us a view of the current state of the web page. In this
    case, it means the web page has used JavaScript to load search results dynamically.
    In the next section, we will use another feature of our browser tools to understand
    how these results are loaded.
  prefs: []
  type: TYPE_NORMAL
- en: What is AJAX?
  prefs: []
  type: TYPE_NORMAL
- en: '**AJAX** stands for **Asynchronous JavaScript and XML** and was coined in 2005
    to describe the features available across web browsers that make dynamic web applications
    possible. Most importantly, the JavaScript `XMLHttpRequest` object, which was
    originally implemented by Microsoft for ActiveX, became available in many common
    web browsers. This allowed JavaScript to make HTTP requests to a remote server
    and receive responses, which meant that a web application could send and receive
    data.  The previous way to communicate between client and server was to refresh
    the entire web page, which resulted in a poor user experience and wasted bandwidth
    when only a small amount of data needed to be transmitted.'
  prefs: []
  type: TYPE_NORMAL
- en: Google's Gmail and Maps sites were early examples of the dynamic web applications
    and helped make AJAX mainstream.
  prefs: []
  type: TYPE_NORMAL
- en: Reverse engineering a dynamic web page
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we tried to scrape data from a web page the same way as introduced
    in [Chapter 2](py-web-scrp-2e_ch02.html), *Scraping the Data*. This method did
    not work because the data is loaded dynamically using JavaScript. To scrape this
    data, we need to understand how the web page loads the data, a process which can
    be described as reverse engineering. Continuing the example from the preceding
    section, in our browser tools, if we click on the Network tab and then perform
    a search, we will see all of the requests made for a given page. There are a lot!
    If we scroll up through the requests, we see mainly photos (from loading country
    flags), and then we notice one with an interesting name: `search.json` with a
    path of `/ajax`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/network_view.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we click on that URL using Chrome, we can see more details (there is similar
    functionality for this in all major browsers, so your view may vary; however the
    main features should function similarly). Once we click on the URL of interest,
    we can see more details, including a preview which shows us the response in parsed
    form. Here, similar to the Inspect Element view in our Elements tab, we use the
    carrots to expand the preview and see that each country of our results is included
    in JSON form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/preview_ajax.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also open the URL directly by right-clicking and opening the URL in
    a new tab. When you do so, you will see it as a simple JSON response. This AJAX
    data is not only accessible from within the Network tab or via a browser, but
    can also be downloaded directly, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As we can see from the previous code, the `requests` library allows us to access
    JSON responses as a Python dictionary by using the `json` method. We could also
    download the raw string response and load it using Python's `json.loads` method.
  prefs: []
  type: TYPE_NORMAL
- en: Our code gives us a simple way to scrape countries containing the letter `A`.
    To find the details of the countries requires calling the AJAX search with each
    letter of the alphabet. For each letter, the search results are split into pages,
    and the number of pages is indicated by `page_size` in the response.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, we cannot save all results returned because the same countries
    will be returned in multiple searches-for example, `Fiji` matches searches for
    `f`, `i`, and `j`. These duplicates are filtered here by storing results in a
    set before writing them to a text file-the set data structure ensures unique elements.
  prefs: []
  type: TYPE_NORMAL
- en: Here is an example implementation that scrapes all of the countries by searching
    for each letter of the alphabet and then iterating the resulting pages of the
    JSON responses. The results are then stored in a simple text file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run the code, you will see progressive output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Once the script is completed, the `countries.txt` file in the relative folder `../data/`
    will show a sorted list of the country names. You may also note the page length
    can be set using the `PAGE_SIZE` global variable. You may want to try toggling
    this to increase or decrease the number of requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'This AJAX scraper provides a simpler way to extract the country details than
    the traditional page-by-page scraping approach covered in [Chapter 2](py-web-scrp-2e_ch02.html),
    *Scraping the Data*. This is a common experience: AJAX-dependent websites initially
    look more complex, however their structure encourages separating the data and
    presentation layers, which can actually make our job of extracting data easier.
    If you find a site with an open Application Programming Interface (or API) like
    this example site, you can simply scrape the API rather than using CSS selectors
    and XPath to load data from HTML.'
  prefs: []
  type: TYPE_NORMAL
- en: Edge cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The AJAX search script is quite simple, but it can be simplified further by
    utilizing possible edge cases. So far, we have queried each letter, which means
    26 separate queries, and there are duplicate results between these queries. It
    would be ideal if a single search query could be used to match all results. We
    will try experimenting with different characters to see if this is possible. This
    is what happens if the search term is left empty:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, this did not work-there are no results. Next we will check if
    ''*'' will match all results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Still no luck. Then we check `''.''`, which is a regular expression to match
    any character:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Perfect! The server must be matching results using regular expressions. So,
    now searching each letter can be replaced with a single search for the dot character.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we can set the page size in the AJAX URLs using the `page_size`
    query string value. The web site search interface has options for setting this
    to 4, 10, and 20, with the default set to 10\. So, the number of pages to download
    could be halved by increasing the page size to the maximum.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now, what if a much higher page size is used, a size higher than what the web
    interface select box supports?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Apparently, the server does not check whether the page size parameter matches
    the options allowed in the interface and now returns all the results in a single
    page. Many web applications do not check the page size parameter in their AJAX
    backend because they expect all API requests to only come via the web interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have crafted a URL to download the data for all countries in a single
    request. Here is the updated and much simpler implementation which saves the data
    to a CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Rendering a dynamic web page
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the example search web page, we were able to quickly reverse engineer how
    the API worked and how to use it to retrieve the results in one request. However,
    websites can be very complex and difficult to understand, even with advanced browser
    tools. For example, if the website has been built with **Google Web Toolkit**
    (**GWT**), the resulting JavaScript code will be machine-generated and minified.
    This generated JavaScript code can be cleaned with a tool such as `JS beautifier`,
    but the result will be verbose and the original variable names will be lost, so
    it is difficult to understand and reverse engineer.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, higher level frameworks like `React.js` and other Node.js-based
    tools can further abstract already complex JavaScript logic and obfuscate data
    and variable names and add more layers of API request security (by requiring cookies,
    browser sessions and timestamps or using other anti-scraper technologies).
  prefs: []
  type: TYPE_NORMAL
- en: With enough effort, any website can be reverse engineered. However, this effort
    can be avoided by instead using a browser rendering engine, which is the part
    of the web browser that parses HTML, applies the CSS formatting, and executes
    JavaScript to display a web page. In this section, the WebKit rendering engine
    will be used, which has a convenient Python interface through the Qt framework.
  prefs: []
  type: TYPE_NORMAL
- en: What is WebKit?
  prefs: []
  type: TYPE_NORMAL
- en: The code for WebKit started life as the KHTML project in 1998, which was the
    rendering engine for the Konqueror web browser. It was then forked by Apple as
    WebKit in 2001 for use in their Safari web browser. Google used WebKit up to Chrome
    Version 27 before forking their version from WebKit called **Blink** in 2013\.
    Opera originally used their internal rendering engine called **Presto** from 2003
    to 2012 before briefly switching to WebKit, and then followed Chrome to Blink.
    Other popular browser rendering engines are **Trident,** used by Internet Explorer,
    and **Gecko** by Firefox.
  prefs: []
  type: TYPE_NORMAL
- en: PyQt or PySide
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two available Python bindings to the Qt framework, `PyQt` and `PySide`.
    `PyQt` was first released in 1998 but requires a license for commercial projects.
    Due to this licensing problem, the company developing Qt, then Nokia and now Digia,
    later developed Python bindings in 2009 called `PySide` and released it under
    the more permissive LGPL license.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are minor differences between the two bindings but the examples developed
    here will work with either. The following snippet can be used to import whichever
    Qt binding is installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, if `PySide` is not available, an `ImportError` exception will be raised
    and `PyQt` will be imported. If `PyQt` is also unavailable, another `ImportError`
    will be raised and the script will exit.
  prefs: []
  type: TYPE_NORMAL
- en: The instructions to download and install each of the Python bindings for Qt
    are available at [http://qt-project.org/wiki/Setting_up_PySide](http://qt-project.org/wiki/Setting_up_PySide)
    and [http://pyqt.sourceforge.net/Docs/PyQt4/installation.html](http://pyqt.sourceforge.net/Docs/PyQt4/installation.html).
    Depending on the version of Python 3 you are using, there might not be availability
    yet for the library, but releases are somewhat frequent so you can always check
    back soon.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging with Qt
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whether you are using PySide or PyQt, you will likely run into sites where you
    need to debug the application or script. We have already covered one way to do
    so, by utilizing the `QWebView` GUI `show()` method to "see" what is being rendered
    on the page you've loaded. You can also use the `page().mainFrame().toHtml()`
    chain (easily referenced when using the `BrowserRender` class via the `html` method
    to pull the HTML at any point, write it to a file and save and then open it in
    your browser.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, there are several useful Python debuggers, such as `pdb` which
    you can integrate into your script and then use breakpoints to step through the
    code where the error, issue or bug is expressed. There are several different ways
    to set this up and specific to whichever library and Qt version you have installed,
    so we recommend searching for the exact setup you have and reviewing implementation
    to allow setting breakpoints or trace.
  prefs: []
  type: TYPE_NORMAL
- en: Executing JavaScript
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To confirm your WebKit installation can execute JavaScript, there is a simple
    example available at [http://example.webscraping.com/dynamic](http://example.webscraping.com/dynamic).
  prefs: []
  type: TYPE_NORMAL
- en: 'This web page simply uses JavaScript to write `Hello World` to a `div` element.
    Here is the source code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'With the traditional approach of downloading the original HTML and parsing
    the result, the `div` element will be empty, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an initial example with WebKit, which needs to follow the `PyQt` or
    `PySide` imports shown in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'There is quite a lot going on here, so we will step through the code line by
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: The first line instantiates the `QApplication` object that the Qt framework
    requires before other Qt objects can be initialized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, a `QWebView` object is created, which is a widget for the web documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `QEventLoop` object is created, which is used to create a local event loop.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `loadFinished` callback of the `QwebView` object is linked to the `quit`
    method of `QEventLoop` so when a web page finishes loading, the event loop will
    stop.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The URL to load is then passed to `QWebView`. `PyQt` requires this URL string
    to be wrapped in a `QUrl` object, but for `PySide`, this is optional.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `QWebView` loads asynchronously, so execution immediately passes to the
    next line while the web page is loading-however, we want to wait until this web
    page is loaded, so `loop.exec_()` is called to start the event loop.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the web page completes loading, the event loop will exit and code execution
    continues. The resulting HTML from the loaded web page is extracted using the
    `toHTML` method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final line shows the JavaScript has been successfully executed and the `div`
    element contains `Hello World`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The classes and methods used here are all excellently documented in the C++
    Qt framework website at [http://qt-project.org/doc/qt-4.8/](http://qt-project.org/doc/qt-4.8/).
    `PyQt` and `PySide` have their own documentation, however, the descriptions and
    formatting for the original C++ version is superior, and, generally Python developers
    use it instead.
  prefs: []
  type: TYPE_NORMAL
- en: Website interaction with WebKit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The search web page we have been examining requires the user to modify and submit
    a search form, and then click on the page links. However, so far, our browser
    renderer can only execute JavaScript and access the resulting HTML. Scraping the
    search page requires extending the browser renderer to support these interactions.
    Fortunately, Qt has an excellent API to select and manipulate the HTML elements,
    which makes implementation straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an alternative version to the earlier AJAX search example, which sets
    the search term to `''.''` and page size to `''1000''` and loads all results in
    a single query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The first few lines instantiate the Qt objects required to render a web page,
    the same as in the previous `Hello World` example. Next, the `QWebView` GUI `show()`
    method is called so that the render window is displayed, which is useful for debugging.
    Then, a reference to the frame is created to make the following lines shorter.
  prefs: []
  type: TYPE_NORMAL
- en: The `QWebFrame` class has many useful methods to interact with web pages. The
    three lines containing `findFirstElement` use the CSS selectors to locate an element
    in the frame, and set the search parameters. Then, the form is submitted with
    the `evaluateJavaScript()` method which simulates the click event. This method
    is very convenient because it allows insertion and execution of any JavaScript
    code we submit, including calling JavaScript methods defined in the web page directly.
    Then, the final line enters the application event loop so we can review what is happening
    in the form. Without this, the script would exit immediately.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is displayed when this script is run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/pyqt_search.png)'
  prefs: []
  type: TYPE_IMG
- en: The final line of code we ran `app._exec()` is a blocking call and will prevent
    any more lines of code in this particular thread from executing. Having a view
    of how your code is functioning by using `webkit.show()` is a great way to debug
    your application and determine what is really happening on the web page.
  prefs: []
  type: TYPE_NORMAL
- en: To stop the running application, you can simply close the Qt window (or the
    Python interpreter).
  prefs: []
  type: TYPE_NORMAL
- en: Waiting for results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The final part of implementing our WebKit crawler is scraping the search results,
    which turns out to be the most difficult part because it isn''t obvious when the
    AJAX event is complete and the country data is loaded. There are three possible
    approaches to deal with this conundrum:'
  prefs: []
  type: TYPE_NORMAL
- en: Wait a set amount of time and hope the AJAX event is complete
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Override Qt's network manager to track when URL requests are complete
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poll the web page for the expected content to appear
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first option is the simplest to implement but it''s inefficient, since if
    a safe timeout is set, usually the script spends too much time waiting. Also,
    when the network is slower than usual, a fixed timeout could fail. The second
    option is more efficient but cannot be applied when there are client-side delays;
    for example, if the download is complete, but a button needs to be pressed before
    content is displayed. The third option is more reliable and straightforward to
    implement; though there is the minor drawback of wasting CPU cycles when checking
    whether the content has loaded. Here is an implementation for the third option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here, the code will remain in the `while` loop until the country links are present
    in the `results` div. For each loop, `app.processEvents()` is called to give the
    Qt event loop time to perform tasks, such as responding to click events and updating
    the GUI. We could additionally add a `sleep` for a short period of seconds in
    this loop to give the CPU intermittent breaks.
  prefs: []
  type: TYPE_NORMAL
- en: A full example of the code so far can be found at [https://github.com/kjam/wswp/blob/master/code/chp5/pyqt_search.py](https://github.com/kjam/wswp/blob/master/code/chp5/pyqt_search.py).
  prefs: []
  type: TYPE_NORMAL
- en: The Render class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To help make this functionality easier to use in future, here are the methods
    used and packaged into a class, whose source code is also available at [https://github.com/kjam/wswp/blob/master/code/chp5/browser_render.py](https://github.com/kjam/wswp/blob/master/code/chp5/browser_render.py):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: You may have noticed the `download()` and `wait_load()` methods have some additional
    code involving a timer. This timer tracks how long is spent waiting and cancels
    the event loop if the deadline is reached. Otherwise, when a network problem is
    encountered, the event loop would run indefinitely.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how to scrape the search page using this new class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Selenium
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the WebKit library used in the previous section, we have full control
    to customize the browser renderer to behave as we need it to. If this level of
    flexibility is not needed, a good and easier-to-install alternative is Selenium,
    which provides an API to automate several popular web browsers. Selenium can be
    installed using `pip` with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To demonstrate how Selenium works, we will rewrite the previous search example
    in Selenium. The first step is to create a connection to the web browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: When this command is run, an empty browser window will pop up. If you received
    an error instead, you likely need to install `geckodriver` ([https://github.com/mozilla/geckodriver/releases](py-web-scrp-2e_ch04.html))
    and ensure it is available via your `PATH` variables.
  prefs: []
  type: TYPE_NORMAL
- en: Using a browser you can see and interact with (rather than a Qt widget) is handy
    because with each command, the browser window can be checked to see if the script worked
    as expected. Here, we used Firefox, but Selenium also provides interfaces to other
    common web browsers, such as Chrome and Internet Explorer. Note that you can only
    use a Selenium interface for a web browser that is installed on your system.
  prefs: []
  type: TYPE_NORMAL
- en: To see if your system's browser is supported and what other dependencies or
    drivers you may need to install to use Selenium, check the Selenium documentation
    on supported platforms: [http://www.seleniumhq.org/about/platforms.jsp](http://www.seleniumhq.org/about/platforms.jsp).
  prefs: []
  type: TYPE_NORMAL
- en: 'To load a web page in the chosen web browser, the `get()` method is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, to set which element to select, the ID of the search textbox can be used.
    Selenium also supports selecting elements with a CSS selector or XPath. When the
    search textbox is found, we can enter content with the `send_keys()` method, which
    simulates typing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To return all results in a single search, we want to set the page size to 1000\.
    However, this is not straightforward because Selenium is designed to interact
    with the browser, rather than to modify the web page content. To get around this
    limitation, we can use JavaScript to set the select box content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the form inputs are ready, so the search button can be clicked on to perform
    the search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to wait for the AJAX request to complete before loading the results,
    which was the hardest part of the script in the previous WebKit implementation.
    Fortunately, Selenium provides a simple solution to this problem by setting a
    timeout with the `implicitly_wait()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Here, a delay of 30 seconds was used. Now, if we search for elements that are
    not yet available, Selenium will wait up to 30 seconds before raising an exception.
    Selenium also allows for more detailed polling control using explicit waits (which
    are well-documented at [http://www.seleniumhq.org/docs/04_webdriver_advanced.jsp](http://www.seleniumhq.org/docs/04_webdriver_advanced.jsp)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To select the country links, we use the same CSS selector that we used in the
    WebKit example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the text of each link can be extracted to create a list of countries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the browser can be shut down by calling the `close()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The source code for this example is available at [https://github.com/kjam/wswp/blob/master/code/chp5/selenium_search.py](https://github.com/kjam/wswp/blob/master/code/chp5/selenium_search.py).
    For further details about Selenium, the Python bindings are documented at [https://selenium-python.readthedocs.org/](https://selenium-python.readthedocs.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Selenium and Headless Browsers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although it's convenient and fairly easy to install and use Selenium with common
    browsers; this can present problems when running these scripts on servers. For
    servers, it's more common to use headless browsers. They also tend to be faster
    and more configurable than fully-functional web browsers.
  prefs: []
  type: TYPE_NORMAL
- en: The most popular headless browser at the time of this publication is PhantomJS.
    It runs via its own JavaScript-based webkit engine. PhantomJS can be installed
    easily on most servers, and can be installed locally by following the latest download
    instructions ([http://phantomjs.org/download.html](http://phantomjs.org/download.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using PhantomJS with Selenium merely requires a different initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The first difference you notice is no browser window is opened, but there is
    a PhantomJS instance running. To test our code, we can visit a page and take a
    screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now if you open that saved PNG file, you can see what the PhantomJS browser
    has rendered:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/python_website.png)'
  prefs: []
  type: TYPE_IMG
- en: We notice it is a long window. We could change this by using `maximize_window`
    or setting a window size with `set_window_size`, both of which are documented
    in the [Selenium Python documentation on the WebDriver API](http://selenium-python.readthedocs.io/api.html).
  prefs: []
  type: TYPE_NORMAL
- en: Screenshot options are great for debugging any Selenium issues you have, even
    if you are using Selenium with a real browser -- since there are times the script
    may fail to work due to a slow-loading page or changes in the page structure or
    JavaScript on the site. Having a screenshot of the page exactly as it was at the
    time of the error can be very helpful. Additionally, you can use the driver's
    `page_source` attribute to save or inspect the current page source.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason to utilize a browser-based parser like Selenium is it makes it
    more difficult to act like a scraper. Some sites use scraper-avoidance techniques
    like Honeypots, where the site might include a hidden toxic link on a page, which
    will get your scraper banned if your script clicks it. For these types of problems,
    Selenium acts as a great scraper because of its browser-based architecture. If
    you cannot click or see a link in the browser, you also cannot interact with it
    via Selenium. Additionally, your headers will include whichever browser you are
    using and you'll have access to normal browser features like cookies, sessions
    as well as loading images and interactive elements, which are sometimes required
    to load particular forms or pages. If your scraper must interact with the page
    and seem "human-like", Selenium is a great choice.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covered two approaches to scraping data from dynamic web pages.
    It started with reverse engineering a dynamic web page using browser tools, and
    then moved on to using a browser renderer to trigger JavaScript events for us.
    We first used WebKit to build our own custom browser, and then reimplemented this
    scraper with the high-level Selenium framework.
  prefs: []
  type: TYPE_NORMAL
- en: A browser renderer can save the time needed to understand how the backend of
    a website works; however, there are some disadvantages. Rendering a web page adds
    overhead and is much slower than just downloading the HTML or using API calls.
    Additionally, solutions using a browser renderer often require polling the web
    page to check whether the resulting HTML has loaded, which is brittle and can
    fail when the network is slow.
  prefs: []
  type: TYPE_NORMAL
- en: I typically use a browser renderer for short-term solutions where the long-term
    performance and reliability is less important; for long-term solutions, I attempt to
    reverse engineer the website. Of course, some sites may require "human-like" interactions
    or have closed APIs, meaning a browser rendered implementation will likely be
    the only way to acquire content.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover how to interact with forms and cookies to
    log into a website and edit content.
  prefs: []
  type: TYPE_NORMAL
