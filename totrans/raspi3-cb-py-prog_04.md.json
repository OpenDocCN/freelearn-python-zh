["```py\nfrom sklearn.naive_bayes import GaussianNB\nimport numpy as np\nimport matplotlib.pyplot as plt\n```", "```py\nin_file = 'data_multivar.txt'\na = []\nb = []\nwith open(in_file, 'r') as f:\n  for line in f.readlines():\n    data = [float(x) for x in line.split(',')]\n    a.append(data[:-1])\n    b.append(data[-1])\na = np.array(a)\nb = np.array(b)\n```", "```py\nclassification_gaussiannb = GaussianNB()\nclassification_gaussiannb.fit(a, b)\nb_pred = classification_gaussiannb.predict(a)\n```", "```py\ncorrectness = 100.0 * (b == b_pred).sum() / a.shape[0]\nprint \"correctness of the classification =\", round(correctness, 2), \"%\"\n```", "```py\ndef plot_classification(classification_gaussiannb, a , b):\n  a_min, a_max = min(a[:, 0]) - 1.0, max(a[:, 0]) + 1.0\n  b_min, b_max = min(a[:, 1]) - 1.0, max(a[:, 1]) + 1.0\n  step_size = 0.01\n  a_values, b_values = np.meshgrid(np.arange(a_min, a_max,   step_size), np.arange(b_min, b_max, step_size))\n  mesh_output1 = classification_gaussiannb.predict(np.c_[a_values.ravel(), b_values.ravel()])\n  mesh_output2 = mesh_output1.reshape(a_values.shape)\n  plt.figure()\n  plt.pcolormesh(a_values, b_values, mesh_output2, cmap=plt.cm.gray)\n  plt.scatter(a[:, 0], a[:, 1], c=b , s=80, edgecolors='black', linewidth=1,cmap=plt.cm.Paired)\n```", "```py\nplt.xlim(a_values.min(), a_values.max())\nplt.ylim(b_values.min(), b_values.max())\n*# specify the ticks on the X and Y axes* plt.xticks((np.arange(int(min(a[:, 0])-1), int(max(a[:, 0])+1), 1.0)))\nplt.yticks((np.arange(int(min(a[:, 1])-1), int(max(a[:, 1])+1), 1.0)))\nplt.show()\nplot_classification(classification_gaussiannb, a, b)\n```", "```py\nimport numpy as np\nfrom sklearn import linear_model\nimport matplotlib.pyplot as plt\na = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\nb = np.array([1, 1, 1, 2, 2, 2])\n```", "```py\nclassification = linear_model.LogisticRegression(solver='liblinear', C=100)\nclassification.fit(a, b)\n```", "```py\ndef plot_classification(classification, a , b):\n  a_min, a_max = min(a[:, 0]) - 1.0, max(a[:, 0]) + 1.0\n  b_min, b_max = min(a[:, 1]) - 1.0, max(a[:, 1]) + 1.0 step_size = 0.01\n  a_values, b_values = np.meshgrid(np.arange(a_min, a_max, step_size), np.arange(b_min, b_max, step_size))\n  mesh_output1 = classification.predict(np.c_[a_values.ravel(), b_values.ravel()])\n  mesh_output2 = mesh_output1.reshape(a_values.shape)\n  plt.figure()\n  plt.pcolormesh(a_values, b_values, mesh_output2, cmap=plt.cm.gray)\n  plt.scatter(a[:, 0], a[:, 1], c=b , s=80, edgecolors='black',linewidth=1,cmap=plt.cm.Paired)\n # specify the boundaries of the figure  plt.xlim(a_values.min(), a_values.max())\n  plt.ylim(b_values.min(), b_values.max())\n # specify the ticks on the X and Y axes  plt.xticks((np.arange(int(min(a[:, 0])-1), int(max(a[:, 0])+1), 1.0)))\n  plt.yticks((np.arange(int(min(a[:, 1])-1), int(max(a[:, 1])+1), 1.0)))\n  plt.show()\n  plot_classification(classification, a, b)\n```", "```py\nfrom sklearn import cross_validation\nfrom sklearn.naive_bayes import GaussianNB\nimport numpy as np\nimport matplotlib.pyplot as plt\nin_file = 'data_multivar.txt'\na = []\nb = []\nwith open(in_file, 'r') as f:\n  for line in f.readlines():\n    data = [float(x) for x in line.split(',')]\n    a.append(data[:-1])\n    b.append(data[-1])\na = np.array(a)\nb = np.array(b)\n```", "```py\na_training, a_testing, b_training, b_testing = cross_validation.train_test_split(a, b, test_size=0.25, random_state=5)\nclassification_gaussiannb_new = GaussianNB()\nclassification_gaussiannb_new.fit(a_training, b_training)\n```", "```py\nb_test_pred = classification_gaussiannb_new.predict(a_testing)\n```", "```py\ncorrectness = 100.0 * (b_testing == b_test_pred).sum() / a_testing.shape[0]\nprint \"correctness of the classification =\", round(correctness, 2), \"%\"\n```", "```py\ndef plot_classification(classification_gaussiannb_new, a_testing , b_testing):\n  a_min, a_max = min(a_testing[:, 0]) - 1.0, max(a_testing[:, 0]) + 1.0\n  b_min, b_max = min(a_testing[:, 1]) - 1.0, max(a_testing[:, 1]) + 1.0\n  step_size = 0.01\n  a_values, b_values = np.meshgrid(np.arange(a_min, a_max, step_size), np.arange(b_min, b_max, step_size))\n  mesh_output = classification_gaussiannb_new.predict(np.c_[a_values.ravel(), b_values.ravel()])\n  mesh_output = mesh_output.reshape(a_values.shape)\n  plt.figure()\n  plt.pcolormesh(a_values, b_values, mesh_output, cmap=plt.cm.gray)\n  plt.scatter(a_testing[:, 0], a_testing[:, 1], c=b_testing , s=80, edgecolors='black', linewidth=1,cmap=plt.cm.Paired)\n # specify the boundaries of the figure  plt.xlim(a_values.min(), a_values.max())\n  plt.ylim(b_values.min(), b_values.max())\n  # specify the ticks on the X and Y axes\n  plt.xticks((np.arange(int(min(a_testing[:, 0])-1), int(max(a_testing[:, 0])+1), 1.0)))\n  plt.yticks((np.arange(int(min(a_testing[:, 1])-1), int(max(a_testing[:, 1])+1), 1.0)))\n  plt.show()\nplot_classification(classification_gaussiannb_new, a_testing, b_testing)\n```", "```py\nfrom sklearn import cross_validation\nfrom sklearn.naive_bayes import GaussianNB\nimport numpy as np\nin_file = 'cross_validation_multivar.txt'\na = []\nb = []\nwith open(in_file, 'r') as f:\n  for line in f.readlines():\n    data = [float(x) for x in line.split(',')]\n    a.append(data[:-1])\n    b.append(data[-1])\na = np.array(a)\nb = np.array(b)\nclassification_gaussiannb = GaussianNB()\n```", "```py\nnum_of_validations = 5\naccuracy = cross_validation.cross_val_score(classification_gaussiannb, a, b, scoring='accuracy', cv=num_of_validations)\nprint \"Accuracy: \" + str(round(100* accuracy.mean(), 2)) + \"%\"\nf1 = cross_validation.cross_val_score(classification_gaussiannb, a, b, scoring='f1_weighted', cv=num_of_validations)\nprint \"f1: \" + str(round(100*f1.mean(), 2)) + \"%\"\nprecision = cross_validation.cross_val_score(classification_gaussiannb,a, b, scoring='precision_weighted', cv=num_of_validations)\nprint \"Precision: \" + str(round(100*precision.mean(), 2)) + \"%\"\nrecall = cross_validation.cross_val_score(classification_gaussiannb, a, b, scoring='recall_weighted', cv=num_of_validations)\nprint \"Recall: \" + str(round(100*recall.mean(), 2)) + \"%\"\n```", "```py\nimport nltk.classify.util\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.corpus import movie_reviews\n```", "```py\ndef collect_features(word_list):\n  word = []\n  return dict ([(word, True) for word in word_list])\n```", "```py\nif __name__=='__main__':\n  plus_filenum = movie_reviews.fileids('pos')\n  minus_filenum = movie_reviews.fileids('neg')\n```", "```py\n  feature_pluspts = [(collect_features(movie_reviews.words(fileids=[f])),\n'Positive') for f in plus_filenum]\n  feature_minuspts = [(collect_features(movie_reviews.words(fileids=[f])),\n'Negative') for f in minus_filenum]\n```", "```py\n  threshold_fact = 0.8\n  threshold_pluspts = int(threshold_fact * len(feature_pluspts))\n  threshold_minuspts = int(threshold_fact * len(feature_minuspts))\n```", "```py\n  feature_training = feature_pluspts[:threshold_pluspts] + feature_minuspts[:threshold_minuspts]\n  feature_testing = feature_pluspts[threshold_pluspts:] + feature_minuspts[threshold_minuspts:]\n  print \"nNumber of training datapoints:\", len(feature_training)\n  print \"Number of test datapoints:\", len(feature_testing)\n```", "```py\n  # Train a Naive Bayes classifiers\n  classifiers = NaiveBayesClassifier.train(feature_training)\n  print \"nAccuracy of the classifiers:\",nltk.classify.util.accuracy(classifiers,feature_testing)\n  print \"nTop 10 most informative words:\"\n  for item in classifiers.most_informative_features()[:10]:print item[0]\n # Sample input reviews  in_reviews = [\n  \"The Movie was amazing\",\n  \"the movie was dull. I would never recommend it to anyone.\",\n  \"The cinematography is pretty great in the movie\",\n  \"The direction was horrible and the story was all over the place\"\n  ]\n  print \"nPredictions:\"\n  for review in in_reviews:\n    print \"nReview:\", review\n  probdist = classifiers.prob_classify(collect_features(review.split()))\n  predict_sentiment = probdist.max()\n  print \"Predicted sentiment:\", predict_sentiment\n  print \"Probability:\", round(probdist.prob(predict_sentiment), 2)\n```", "```py\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom gensim import models, corpora\nfrom nltk.corpus import stopwords\n```", "```py\ndef load_words(in_file):\n  element = []\n  with open(in_file, 'r') as f:\n    for line in f.readlines():\n      element.append(line[:-1])\n  return element\n```", "```py\nclassPreprocedure(object):\n  def __init__(self):\n # Create a regular expression tokenizer    self.tokenizer = RegexpTokenizer(r'w+')\n```", "```py\n    self.english_stop_words= stopwords.words('english')\n```", "```py\n    self.snowball_stemmer = SnowballStemmer('english')  \n```", "```py\n  def procedure(self, in_data):\n# Tokenize the string\n    token = self.tokenizer.tokenize(in_data.lower())\n```", "```py\n    tokenized_stopwords = [x for x in token if not x in self.english_stop_words]\n```", "```py\n    token_stemming = [self.snowball_stemmer.stem(x) for x in tokenized_stopwords]\n```", "```py\n    return token_stemming\n```", "```py\nif __name__=='__main__':\n # File containing input data  in_file = 'data_topic_modeling.txt'\n # Load words  element = load_words(in_file)\n```", "```py\n  preprocedure = Preprocedure()\n```", "```py\n  processed_tokens = [preprocedure.procedure(x) for x in element]\n```", "```py\n  dict_tokens = corpora.Dictionary(processed_tokens)\n  corpus = [dict_tokens.doc2bow(text) for text in processed_tokens]\n```", "```py\n  num_of_topics = 2\n  num_of_words = 4\n  ldamodel = models.ldamodel.LdaModel(corpus,num_topics=num_of_topics, id2word=dict_tokens, passes=25)\n  print \"Most contributing words to the topics:\"\n  for item in ldamodel.print_topics(num_topics=num_of_topics, num_words=num_of_words):\n    print \"nTopic\", item[0], \"==>\", item[1]\n```"]