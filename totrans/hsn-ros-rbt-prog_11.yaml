- en: Virtual SLAM and Navigation Using Gazebo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will be introduced to the concepts and components of robot
    navigation. Using **SLAM** (short for **Simultaneous Localization and Mapping**)
    techniques, you will be able to execute autonomous navigation with GoPiGo3. This
    chapter deals with advanced topics in simulation. Hence, it is essential that
    you have understood the concepts of the previous chapter, where we gave you the
    basics to interact with a virtual robot in Gazebo.
  prefs: []
  type: TYPE_NORMAL
- en: SLAM is a technique used in robotics to explore and map an unknown environment
    while estimating the pose of the robot itself. As it moves all around, it will
    be acquiring structured information of the surroundings by processing the raw
    data coming from its sensors.
  prefs: []
  type: TYPE_NORMAL
- en: You will explore this concept with a practical approach using the digital twin
    of GoPiGo3, neatly understanding why a SLAM implementation is required for proper
    navigation. The simulation will be run in Gazebo, the ROS native simulation tool
    with a physics engine that offers realistic results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic simulation using Gazebo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Components in navigation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robot perception and SLAM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practicing SLAM and navigation with GoPiGo3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By covering these topics, you will get more familiar with the Gazebo environment.
    You will understand the concepts of navigation and SLAM and how they relate to
    each other. With a very practical approach, you will learn to run SLAM and navigation
    tasks in Gazebo with a virtual model of a robot.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To summarize and clarify the purposes of the steps that we''ll take in this
    chapter dealing with the virtual robot, and in the next chapter regarding the
    physical GoPiGo3, the following list shows all these sensors and actuators we
    are going to work with, as well as the sections of the previous chapters that
    have dealt with each one:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distance sensor**: In [Chapter 6](0b20bdff-f1dc-42e8-ae83-fc290da31381.xhtml), *Programming
    in ROS – Commands and Tools*, the *Case study 1: publishing and reading the distance
    sensor*section taught you how to use the distance sensor under ROS with the physical
    robot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Line follower**. See the following list for assembly and unit-testing instructions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IMU sensor**. See the following list for assembly and unit-testing instructions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pi camera**: In [Chapter 6](0b20bdff-f1dc-42e8-ae83-fc290da31381.xhtml), *Programming
    in ROS – Commands and Tools*, the *Case Study 1: Publishing and reading the distance
    sensor*section taught you how to use the Pi camera under ROS with the physical
    robot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Drive motors and encoders**: In the previous chapter, the *Case study 3:
    Remote control using the keyboard *section taught you first how to use these items
    in ROS with the physical robot, and then how to implement a differential drive
    controller under the Gazebo simulator in ROS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For all of these, you have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Assembly instructions, which can be found in the *Deep dive into the electromechanics*section of
    [Chapter 1](9bb411d1-934c-4497-aad4-7ad770d3783c.xhtml), *Assembling the Robot*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit testing instructions, which can found in the *Unit testing of sensors and
    drives*section of [Chapter 2](7a2b1b82-c666-42df-9f10-9777eabe82df.xhtml), *Unit
    Testing of GoPiGo3*, where the provided software taught you how to deal with unit
    tests using Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For optimal and easy-to-understand coverage of the topic of SLAM, we will implement
    a 360º-coverage **Laser Distance Sensor** (**LDS**)in the virtual robot. There
    are low-cost versions of this sensor technology, such as **EAI YDLIDAR X4** (available
    at[https://www.aliexpress.com/item/32908156152.html](https://es.aliexpress.com/item/32908156152.html)),
    which is the one we will make use of in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will make use of the code located in the `Chapter8_Virtual_SLAM`
    folder at [https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter8_Virtual_SLAM](https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter8_Virtual_SLAM).
    Copy its files to the ROS workspace to have them available, and leave the rest
    outside of the `src` folder. This way, you will have a cleaner ROS environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The code contains two new ROS packages as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`gopigo3_description`, which contains the URDF model plus the SDF (Gazebo tags)
    for a complete, dynamic simulation. This package provides the `gopigo3_rviz.launch` launch
    file to interactively visualize the model in RViz.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`virtual_slam` contains the virtual robot simulation itself, plus the launch
    files needed to run SLAM in Gazebo.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, rebuild the workspace so that it is known to your ROS installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Check that the packages have been correctly installed by selecting them and
    listing the files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Then you need to make some installation and configuration to run the exercises,
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: ROS navigation packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following steps provide the installation instructions for ROS Kinetic,
    the version running in Ubuntu 16.04:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s prepare your machine with the required ROS packages needed for
    the navigation stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In **ROS Kinetic**, you can install `slam_gmapping` from binaries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This installs the `gmapping` and `openslam_gmapping` packages. If working with
    ROS Melodic (that is, you are in Ubuntu 18.04):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the corresponding versions for Melodic:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally the `slam_gmapping` package, that the time of writing is already
    available in its binary version:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ROS master running on the local computer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since, in this chapter, you will only be using your local machine, you need
    to reconfigure the ROS master URI so that it does not point to the robot but to
    your local computer. Then, open your local `.bashrc` file and comment out the
    line at the end that specifies the URL where the ROS master can be found:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Close all Terminals, open a new one, and check the `ROS_MASTER_URI` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You should find that the environment variable has reverted to the default server
    (`localhost`) and default port (`11311`). Now, we are ready to switch to the virtual
    robot.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic simulation using Gazebo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you performed a very basic version of navigation, where the
    feedback to the robot about its environment always came from you as a human operator.
    For example, you saw that GoPiGo3 is advancing to an obstacle, so you made it
    turn left or right to avoid it.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section takes you one step forward in remote control by providing feedback
    not only from your human vision, but also from robotic sensors. More precisely,
    GoPiGo3 will provide data from the Pi camera and from its distance sensor. The
    goal is that you can teleoperate it more precisely by getting as high-quality
    sensor data as possible. You may be able to guess at least two common scenarios
    in the real world where this kind of manual teleoperation is key for the execution
    of a planned task:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Surgical robot teleoperation**: Where an expert surgeon can carry out a surgical
    operation without being present in the operating room where the patient is being
    attended to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Teleoperated rescue robots**: This used in accidents where human cannot access
    the location on their own, such as a ravine between mountains in the occurrence
    of a flood, or disasters where direct human presence is to be avoided, for example
    in a nuclear disaster where the level of radioactivity is so high that an exposed
    human could absorb a dose of deadly radiation in a few minutes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having these keys in mind, you should understand this section not only as a
    prior learning step before entering into autonomous navigation, but also as a
    motivational introduction to a common way of working with teleoperated robots
    in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Adding sensors to the GoPiGo3 model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up to now, you should have equipped your virtual robot with a differential drive
    controller that provides the capability to convert velocity commands into rotations
    of the left and right wheels. We need to complete the model with some sort of
    perception of the environment. For this, we will add controllers for two common
    sensors, a two-dimensional camera and an LDS. The first corresponds to the Pi
    camera of your physical robot, while the second is the unidirectional distance
    sensor of the GoPiGo3 kit.
  prefs: []
  type: TYPE_NORMAL
- en: Camera model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can add the solid of the camera as usual with `<visual>` tags, but since
    it is a commercial device, you can a get better look by using a realistic three-dimensional
    CAD model supplied by the manufacturer or made by someone else in the open source
    community. The URDF definition is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see two blocks in the preceding snippet: the `<link>` element to specify
    the solid, and the `<joint>` block to attach the camera to the robot chassis.
    Since the camera is rigidly attach to the body, we specify the `type="fixed">` to
    model such a characteristic.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding the `<link>` element, we introduce the `<mesh>` tag to import the
    geometry from a CAD DAE filetype, marked in bold in the preceding snippet. The
    following screenshot shows the CAD model of the camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e760d9a-31b7-489b-888f-bdc204d1a3f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then we add the camera technical features using a `<gazebo>` tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `<update_rate>` tag specifies that the sensor is read at a frequency of 30
    Hz, that is, it takes 30 images per second. Finally, we add the Gazebo plugin
    that emulates the behavior of the camera. The following snippet is what substitutes
    the commented line that referred to `plugin "camera_controller"`in the preceding
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The controller for the camera is in the `libgazebo_ros_camera.so` file, so what
    you provide within this block are the technical specifications of the camera you
    are using. Setting `<updateRate>` to `0.0` means that Gazebo should take the refreshment
    rate from the preceding `<sensor>` tag, that is, 30 Hz. As specified (see fields
    in bold letters), camera images will be published in the `/gopigo/camera1/image_raw` topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Launch the ROS visualization tool to check that the model is properly built.
    Since **RViz** only represents its visual features—it does not include any physical
    simulation engine—it is a much lighter environment than Gazebo and you have available
    all the options to check every aspect of the appearance of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This launch file is very similar to the one you used in [Chapter 4](742e6846-70e4-4bd4-8576-f3e4f445df3f.xhtml),
    *Creating the Virtual Two-Wheeled ROS Robot*. The following screenshot shows the
    result you should see:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/752416bd-2335-4e05-ad11-39114c43716d.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, you will do a practical exercise to see how the camera
    works with Gazebo.
  prefs: []
  type: TYPE_NORMAL
- en: Simulating the camera
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Follow these steps for the simulation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first place the robot in Gazebo the same way we did in the previous
    chapter and enable remote control with the keyboard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`key_teleop` allows you to remotely control the GoPiGo3 with the arrow keys
    of your keyboard.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, launch a node from the `image_view` package that comes preinstalled with
    ROS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We are remapping the `image` topic so that the node takes its data from the
    camera node topic, `/gopigo/camera1/image_raw`. This topic is defined in the preceding snippet
    of the camera controller plugin with the combination of the `<imageTopicName>`
    and
  prefs: []
  type: TYPE_NORMAL
- en: '`<cameraInfoTopicName>` tags. Teleoperate the robot with the arrow keys and
    you will see the subjective view in the image window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d34617c-f8e6-4f0f-ae1a-e178edec74d1.png)'
  prefs: []
  type: TYPE_IMG
- en: The background window corresponds to Gazebo (launched from Terminal `T1`) and
    there you can see the virtual robot looking at the traffic cones. The subjective
    view is shown in the left window (`T2`), provided by the Pi camera image live
    feed using the `image_view` package. Finally, the left-bottom window (`T3`) is
    the one you need to select to be able to move the robot with the arrow keys of
    the keyboard. We have used them to place the robot in front of the traffic cones,
    as shown in the preceding screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, let''s obtain the ROS graph with the well-known command, `rqt_graph`,
    and have a look at how the topic remapping for the image is handled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93620544-f345-4af9-8041-866578548eea.png)'
  prefs: []
  type: TYPE_IMG
- en: Thanks to the mapping argument, `image:=/gopigo/camera1/image_raw`, the `image`
    topic of the `image_view` package remains implicit and just the `/gopigo/camera1/image_raw`
    is visible.
  prefs: []
  type: TYPE_NORMAL
- en: Are you aware how quick and easy it is to deliver a robot behavior when you
    are using prebuilt ROS modules and your custom robot definition? In the next section,
    we will cover these same steps for the second sensor.
  prefs: []
  type: TYPE_NORMAL
- en: Distance sensor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We add the solid model of this sensor under the `<visual>` tag by following
    the same procedure we covered for the camera. The URDF definition is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see two blocks in the preceding snippet: the `<link>` element to specify
    the solid, and the `<joint>` block to attach the sensor body to the robot chassis.
    Since the distance sensor is rigidly attach to the robot chassis, we specify the `type="fixed">` to
    model this characteristic. The solid model that we are using is shown in the following
    screenshot. In this case, we use a CAD model in STL format and reference it from
    the `<mesh>` tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da1def45-c0f9-4659-9a4a-a0f00ab1e0aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will base the sensor itself in another solid, since, if you do this with
    the solid in the preceding screenshot, you will see in Gazebo that the distance
    rays are blocked by the solid, and so the sensor will always produce a zero value
    for distance. So, we are going to explain to you a trick with which you can separate
    the solid model of the sensor from the sensing point, located at the origin of
    the link frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This snippet creates a box of 10 cm x 10 cm and place it in the coordinates
    specified by the `<joint>` tag.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we add the sensor technical features using a `<gazebo>` tag, which you
    can see refers to the `distance_sensor` link defined in the preceding snippet
    (not `distance_sensor_solid`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The `<update_rate>` tag specifies that the sensor is read at a frequency of
    10 Hz, and the `<range>` tag sets measured distance values between 10 cm and 3
    m at 1 cm resolution.
  prefs: []
  type: TYPE_NORMAL
- en: The `<visualize>**true**</visualize>` tag block allows you to see in Gazebo
    the laser ray of the distance sensor covering the `<range>` limits explained here;
    that is, its detection coverage reaches up to 3 meters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we add the Gazebo plugin that emulates the behavior of the distance
    sensor. The following snippet is what substitutes the commented line referring to `plugin
    "gazebo_ros_ir"`in the preceding code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The controller for the distance sensor is in the `libgazebo_ros_range.so` file,
    so what you provide within this block are the technical specifications of the
    sensor you are using. Setting the `<updateRate>` tag to `0.0` means that Gazebo
    should take the refreshment rate from the preceding `<sensor>` tag, that is, 10
    Hz. As specified (see fields in bold letters), range values will be published
    in the `/sensor/ir_front` topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Launch the ROS visualization tool to check that the model is properly built.
    Since **RViz** only represents its visual features, it is a much lighter environment
    than Gazebo and you have available all the options to check every aspect of the
    appearance of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following screenshot, you can see the result together with the camera
    that we included earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9d6f056-2e71-49b1-8202-463f99b4f90f.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, you will do a practical exercise to see how it works with
    the distance sensor under Gazebo.
  prefs: []
  type: TYPE_NORMAL
- en: Simulating the distance sensor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This test includes both the distance sensor and the two-dimensional camera.
    Run the example by using four Terminals, as indicated in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is a composed view of the result you should obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a86a6a72-eed5-4b33-931f-f7bc71c827da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding screenshot, you can find the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: The central window is the Gazebo one, where you can see GoPiGo3, an obstacle,
    and the rays of the distance sensor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The top-left gray window is the one we need to have selected so that arrow-key
    pushes are received as `/cmd_vel` topic messages for remote control.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bottom-left black window shows in real time the messages transmitted to
    the topic of the distance sensor, that is, `/gopigo/distance_sensor`. The current
    distance to the obstacle is found in the `range` field, with a value of 1.13 m.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The right window shows the live view seen by the robot thanks to its two-dimensional camera,
    received in the`/gopigo/camera1/image_raw` topic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can manually drive from one side of the scene to the other without crashing
    into any of the furniture. You plan—as a human—the optimal trajectory, and execute
    it to bring the robot to the destination goal while avoiding the obstacles. What
    you have done yourself previously is what the robot now has to do itself, performing
    as well as possible. This task is known as **navigation** and is what we are going
    to cover in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Components in navigation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Navigation is the movement of a robot from the current position to a target
    location following a planned trajectory. This ability in a robot means that it
    is capable of determining its position at any point along the trajectory, as well
    as to setting up a plan of action given a representation of the environment, such
    as a map. We should also add the ability to avoid dynamic obstacles or others
    that were not present when the map was built for the first time.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four components to consider when building the navigation ability:'
  prefs: []
  type: TYPE_NORMAL
- en: A map of the environment, preexisting and given to the robot as an input, or
    built by its own means using the sensory data that it collects with its sensors.
    This whole process, that is, data acquisition plus interpretation, constitutes
    what we call the capability of robot perception. One well-known technique that
    takes advantage of robot perception is known as SLAM, as discussed earlier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time pose, understood as the ability of a robot to locate itself in terms
    of position and rotation (together referred to as pose) with respect to a fixed
    frame of reference in the environment. The typical technique in robotics for obtaining
    pose is known as dead reckoning, in which the current pose is estimated relative
    to the previous one plus internal odometry data—coming from the rotary encoders
    of the motors—and IMU sensor data to reduce the error of these calculations. Both
    of them are present in the GoPiGo3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robot perception, which arises from the combination of sensor data plus its
    interpretation, making the robot aware of the objects and obstacles that are around.
    In the GoPiGo3, the sensors that contribute to perception are the distance sensor,
    the Pi camera, and the LDS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Path planning and execution, which includes the calculation of the optimal path
    and its execution so that the robot can achieve the target location. Since the
    map does not include all the details of the environment and there can be dynamic
    obstacles, the path planning should also be dynamic. Its algorithm will be better
    as it will be able to adapt to the varying conditions in the environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will cover the costmap, a key concept on top of which navigation is
    based.
  prefs: []
  type: TYPE_NORMAL
- en: Costmaps for safe navigation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The costmap for robot navigation arises from the combination of the robot's
    pose, estimated from the odometry data (encoders) and the IMU sensor, the perception
    of objects and obstacles in the environment using the distance sensor and LDS,
    and the **occupancy grid map** (**OGM**) obtained from the SLAM technique.
  prefs: []
  type: TYPE_NORMAL
- en: These sources of information provide as output a joint measurement of obstacle
    areas, probable collisions, and the movable area for the robot. There is a global
    costmap and a local one. The global one accounts for the navigation path using
    the fixed map obtained through SLAM, while the local version allows the robot
    to deal with the fine-grained details of its immediate environment to move around
    obstacles and avoid collisions.
  prefs: []
  type: TYPE_NORMAL
- en: The costmap, be it local or global, is measured in a range of 8 bits, that is,
    a value from 0 to 255 in each cell of the grid occupancy map. A zero value means
    a free area, and 255 is an occupied area. Values near 255 account for collision
    areas, while intermediate values range from low collision probabilities (0-127) to
    high (128-252).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will finally deal with SLAM, the technique that is at
    the core of robot navigation. As a starting point, we will complete the setup
    of the GoPiGo3 perception capability with the integration of an LDS.
  prefs: []
  type: TYPE_NORMAL
- en: Robot perception and SLAM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most straightforward way to implement robot navigation in ROS is by using
    an LDS that provides 360° coverage, allowing the robot to be aware of all the
    objects and obstacles around it.
  prefs: []
  type: TYPE_NORMAL
- en: In the introduction to this chapter, we identified the **EAI YDLIDAR X4 **as
    a low-cost option that can be integrated with our physical robot. That will be
    covered in the next chapter, while in the present one we will develop its virtual
    model to be integrated in Gazebo.
  prefs: []
  type: TYPE_NORMAL
- en: The next subsection extends the virtual GoPiGo3 that we've worked on in this
    chapter to include this very model of LDS. Afterward, we will deploy a quick SLAM
    example to get an overview of what this functionality can provide to robot navigation.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a Laser Distance Sensor (LDS)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The process to add the sensor is similar to what we did for the distance sensor
    in the previous section. Follow these steps to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We add the solid model of this sensor under the `<visual>` tag by following
    the same procedure we covered for the previous sensors. The URDF definition is
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see two `<visual>` blocks within the `<link>` element in the preceding snippet:
    `sensor_body` is the LDS itself, and `support` creates the physical interface
    between the sensor and the robot chassis. The solid model that we are using for
    the sensor body is the one shown in the following screenshot, which consists of
    a CAD model in STL format referenced from the `<mesh>` tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47254f3d-453d-4cfe-b185-c5d91356bac0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we add a `<joint>` element of `<type="fixed">` to attach the sensor assembly
    to the robot chassis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we add the sensor technical features using a `<gazebo>` tag that you can
    see refers to the `distance_sensor` link defined in the preceding snippet (not `distance_sensor_solid`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The `<range>` tag sets measured distance values between 12 cm and 10 m, as can
    be found in the technical specification of the EAI YDLIDAR X4\. Pay special attention
    to the `<visualize>true</visualize>` tag, since, with a sensor like this, with
    360º vision, the screen will be filled with rays to show the angle range that
    it covers. It is recommended to set this to `false` once you have visually checked
    that the sensor is working properly.
  prefs: []
  type: TYPE_NORMAL
- en: The `<visualize>true</visualize>` tag block has the same meaning and effect
    for the distance sensor, as explained in the previous section when we built its
    model, in the *Distance sensor* subsection. The only difference is that the LDS
    covers all angles with 360º coverage, tracing as many rays as the number of samples
    specified inside the `<samples>` tag.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `<update_rate>` tag specifies that the sensor is read at a frequency of
    5 Hz, but the specification of the LDS is 5,000 Hz. Why don''t we put the actual
    value? This is for CPU usage reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Bear in mind that, if we set the reading frequency at its actual physical capability,
    it will take 5,000 samples per second, and each sample is a vector of 720 points.
  prefs: []
  type: TYPE_NORMAL
- en: Since LDS covers all possible directions, to get 720 rays evenly spaced at 0.5º,
    you have put one more sample, that is, 721, since 0º and 360º are actually the
    same angle.
  prefs: []
  type: TYPE_NORMAL
- en: Each point will be characterized by two float values (64 bits), so each sample
    needs 720 x 2 x 64 = 92160 bits = 11 Kb. Since there would be 5,000 samples, we
    would need a bandwidth of 53 Mb/s. That's a huge value to be managed by a Raspberry
    Pi CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Since the robot will move at low speed, there is no need to have such a high-frequency reading,
    so we can limit it to only 5 Hz, which will have no impact on the robot behavior.
    This will require only 55 Kb/s of bandwidth, 1,000 times lower than what the sensor
    can provide.
  prefs: []
  type: TYPE_NORMAL
- en: This is a clear example of why you should not directly introduce the specifications
    of sensors within Gazebo, since it can impact the performance of the simulation.
    You need to critically analyze each sensor and decide what parameters to set in
    its virtual controller so that it reproduces the actual behavior well, while not
    unnecessarily overloading the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to add the Gazebo plugin that emulates the behavior of the
    distance sensor. The following snippet is what substitutes the commented line
    referring to `plugin "gazebo_ros_lds_lfcd_controller"`in the preceding code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The controller for the distance sensor is in the `libgazebo_ros_laser_range.so`
    file, so what you provide within this block are the technical specifications of
    the sensor for which you want to override the values provided in the `<sensor>`
    tag in the preceding snippet. As specified (see fields in bold letters), the range
    values will be published in the `/gopigo/scan` topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, launch the ROS visualization tool to check that the model is properly
    built. Since RViz only represents its visual features, it is a much lighter environment
    than Gazebo and you have available all the options to check every aspect of the
    appearance of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following screenshot, you can see the result together with the camera
    that we included earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/191d5439-d7ed-4e6e-8f3c-bd7346cdfbfc.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next subsection, you will do a practical exercise to see how it works
    the laser distance sensor under Gazebo.
  prefs: []
  type: TYPE_NORMAL
- en: Simulating the LDS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After including the LDS model in the virtual robot, we can proceed to see how
    it works by running the simulation in Gazebo:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following commands in separate Terminals to see the sensor in action:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In `T3`, you will see a large feed of data, since each `LaserScan` message contains
    720 points to cover the 360° view around the sensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test this sensor, it is better to use a Python script that makes the robot
    wander in the environment while avoiding the obstacles. To do this, we have implemented
    the following rules in our script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If there is no obstacle, move forward at a reference speed of 0.8 m/s.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: If the range provided by the distance sensor is lower than 2 meters, go back
    and rotate counter-clockwise until avoiding the obstacle.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the distance sensor throws unidirectional measurements, we should check
    the measurements from the LDS to find if there are obstacles to the sides, and
    the threshold should be lower than 1.6 meters. If obstacles are detected, go back
    and rotate counter-clockwise faster to avoid the obstacle and not get stuck on
    it.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This simple algorithm is implemented in the `wanderAround.py` script, and can
    be found under the `./virtual_slam/scripts/wanderAround.py` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, give it a try, and enjoy watching how the GoPiGo3 goes from one side of
    the world to the other while avoiding obstacles. The sequence to run is the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the robot wandering around:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75cee70f-3dd8-4cac-b221-d52f1228a2f7.png)'
  prefs: []
  type: TYPE_IMG
- en: To finish this section, we will briefly cover the key concepts of the SLAM theory
    so that you know what's under the hood when we proceed in the last section of
    the chapter, covering the practical part of this implementation of robot navigation.
  prefs: []
  type: TYPE_NORMAL
- en: SLAM concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SLAM allows the robot to build a map of the environment using the following
    two sources of information:'
  prefs: []
  type: TYPE_NORMAL
- en: Robot pose estimation, coming from the internal odometry (rotary encoders) and
    IMU sensor data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distance to objects, obstacles and walls, coming from distance sensors, the
    LDS in particular
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In its most basic version, a map includes two-dimensional information, while
    in more advanced applications using industrial-grade LIDAR sensors, a richer map
    is built using three-dimensional information from LIDAR and/or from three-dimensional
    cameras. For the purpose of our learning path, we will deal with the two-dimensional
    OGM, also very common in ROS projects.
  prefs: []
  type: TYPE_NORMAL
- en: Occupancy Grid Map (OGM)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Take the example of a square room with four static obstacles inside. The following
    diagram shows the map generated using SLAM in ROS (you will later learn how to
    generate it yourself):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e521a1a-e44d-40dd-a401-bda67cd033c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In such a two-dimensional map, the free areas and occupied areas are drawn
    in different intensities of gray in 8-bit format (0-255 range, as was already
    mentioned earlier when describing the costmaps). Then, the occupancy probability
    for each cell is obtained as the difference between 255 and the intensity value,
    divided by 255\. This means the following:'
  prefs: []
  type: TYPE_NORMAL
- en: White areas (255 value) give a 0% probability; that is, there is no obstacle
    in them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Black areas (0 value) give a 100% probability; that is, they are occupied.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This probability distribution allows a costmap to be built that helps the robot
    to determine which trajectory to select to achieve the target location. When published
    to ROS, the occupancy probabilities translate into integer values between 0 (0% probability,
    that is, free space) and 100 (100%, that is, occupied space). A value of -1 is
    assigned to unknown areas. Map information is stored using two files:'
  prefs: []
  type: TYPE_NORMAL
- en: A `.pgm` format file, known as portable graymap format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A `.yaml` file containing the configuration of the map. See the following example
    of its content:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The most interesting parameters are the last two:'
  prefs: []
  type: TYPE_NORMAL
- en: '`occupied_thresh = 0.65` means that a cell is considered as occupied if its
    probability is above 65%.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`free_thresh = 0.196` establishes the threshold value below which the cell
    is considered free, that is, 19.6%.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the size in pixels of the image, it is straightforward to infer the physical
    dimension of the cells in the map. This value is indicated by the `resolution`
    parameter, that is, 0.01 meter/pixel.
  prefs: []
  type: TYPE_NORMAL
- en: The SLAM process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Building the map using a Gazebo simulation involves employing the following
    workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: Launch the robot model within a modeled environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch the mapping ROS package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch a special visualization in RViz that lets us see the areas the robot
    is scanning as it moves.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Teleoperate the robot to make it cover as much as possible of the surface of
    the virtual environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the exploration is finished, save the map, generating the two files in
    the formats indicated in the preceding section, that is, `.pgm` and `.yaml`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Having finished this information acquisition phase, we are ready for the robot
    to try and successfully complete a navigation task.
  prefs: []
  type: TYPE_NORMAL
- en: The navigation process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once your robot has generated a map, it will use it to plan a path to a given
    target destination. The process of executing such a plan is called navigation,
    and involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Launch the robot model within the modeled environment. This step is the same
    as the first step in the SLAM process described earlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide the costmap that the robot built before. Bear in mind that the map is
    a characteristic of the environment, not of the robot. Hence, you can build the
    map with one robot and use the same map in navigation for any other robot you
    put in the same environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the navigation algorithm. We will use the **Adaptive Monte Carlo Localization**
    (**AMCL**) algorithm, the most common choice for effective navigation. It is out
    of the scope of the book to describe such algorithms, but useful references are
    provided in the further reading section at the end of the chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch a RViz visualization that will let you visualize the robot in the environment
    and easily mark the target pose (position and orientation) that it should achieve.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let the robot navigate autonomously to the target location. At this point, you
    can relax and enjoy watching how the GoPiGo3 drives to the indicated position
    while avoiding the obstacles and minimizing the distance it has to cover.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Should you want the robot to navigate to another location, you just have to
    indicate it in RViz once it has reached the previous target. Now it is time to
    see the preceding two processes—SLAM and navigation—in action. That is the scope
    of the last section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Practising SLAM and navigation with the GoPiGo3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like it was mentioned at the end of the previous section, we are going to run
    an end-to-end example of SLAM and navigation with GoPiGo3\. The first process
    deals with building a map of the environment using SLAM. Let's retrace the steps
    listed in the preceding section and see how to execute each of them in ROS.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the environment to build a map using SLAM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s follow these steps to build the map of a simple Gazebo world called `stage_2.world`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Launch the robot model within a modeled environment by running the following
    line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This command launches Gazebo and places the GoPiGo3 model in the middle of
    it, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f637221-af4b-4b01-98af-6f5bcfd49e48.png)'
  prefs: []
  type: TYPE_IMG
- en: The environment consists of a square space with four static obstacles. The two-dimensional map
    we used in the *Occupancy Grid Map (OGM)* subsection of the previous section corresponds
    to this Gazebo world, whose filename is `stage_2.world`.
  prefs: []
  type: TYPE_NORMAL
- en: You can see that this world is by far simpler than the one we used in the first
    part of the chapter (there is an even simpler environment without the obstacles,
    named `stage_1.world`). We use this to illustrate the navigation concepts with
    a minimal setup for better understanding.
  prefs: []
  type: TYPE_NORMAL
- en: It is left as an exercise for the reader to repeat this process with the Gazebo
    world from the first *Dynamic simulation using Gazebo* section. To do so, just
    omit the `world` argument so that it takes the default specified within the launch
    file. The command to execute this simulation is `$ roslaunch virtual_slam gopigo3_world.launch`
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, take into account that we can specify any other environment we want
    to use in Gazebo by setting the `world` parameter to the filename of the one selected
    (available worlds are located inside the `./virtual_slam/worlds` folder of the
    code of this chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Launch the SLAM mapping ROS package, including an RViz visualization that superimposes
    the virtual model of the robot with the actual scan data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The appearance of the RViz window is shown in the following screenshot, where
    you can see together the virtual robot and the scan data (green points) in real
    time. The light-gray-colored areas are what the robot is actually perceiving with
    its LDS sensor, while the non colored areas (shadow spaces behind the obstacles)
    are not yet known by the GoPiGo3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/028c1384-3570-4e8f-891a-9e7a56253871.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next step, we will explore the full environment to build the map.
  prefs: []
  type: TYPE_NORMAL
- en: 'Teleoperate the robot to make it cover as much as possible of the surface of
    the current Gazebo world. Let''s do this as usual with the teleoperation package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'As you move the robot, the LDS sensor will acquire scan data from the unknown
    areas, and you will receive feedback in the RViz window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf884e89-b642-45b0-9a26-11396d67ef2a.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, you can see that, after wandering in the environment,
    only the bottom-left part is not scanned. Then move the robot to that location,
    and, as soon as you have all the space filled with a homogeneous color (light
    gray), proceed to step 4 in order to save the map.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you''ve finished the exploration, save the map, generating two files of
    the formats indicated in the preceding *SLAM process* subsection, that is, `.pgm` and `.yaml`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: You will get two files in the root folder of your workspace: `map_stage_2.pgm`
    and `map_stage_2.yaml`.
  prefs: []
  type: TYPE_NORMAL
- en: The appearance of the generated map is shown in the preceding *Occupancy Grid
    Map (OGM)* subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Provided with the map, we are ready to perform robot navigation with the GoPiGo3.
  prefs: []
  type: TYPE_NORMAL
- en: Driving along a planned trajectory using navigation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, close all open Terminals. Then, as in the SLAM process, let''s proceed
    step by step to perform some navigation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Launch the robot model within the modeled environment. This step is the same
    as the first step in the SLAM process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Set up the navigation algorithm and launch RViz. We will use AMCL, the most
    common choice for effective navigation. It is out of the scope of the book to
    describe such algorithm, but you are provided with useful references in the* Further
    reading *section at the end of the chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In this step, we also provide the costmap that the robot built before. To do
    this, you just have to reference the `.yaml` map file you created before. Make
    sure that the corresponding `.pgm` file has the same name and is placed in the
    same location. This point is specified in the `roslaunch` command through the
    `map_file` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The RViz window, shown in the following screenshot, lets you visualize the
    robot in the environment and mark the target pose (position and orientation) that
    it should achieve:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/50a5ea86-a012-47f8-b2f8-f2dec49c6eaa.png)'
  prefs: []
  type: TYPE_IMG
- en: Find the 2D Nav Goal button at the top-right of the RViz window. You will use
    it to mark the target location to which the robot should navigate.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, you have to tell the robot that this is the initial pose by pressing
    the **2D Pose Estimate **button. Then, mark it on screen (in this particular case,
    it isn't necessary, since the initial pose is the same as the one the robot had
    when it started to build the map in the preceding subsection).
  prefs: []
  type: TYPE_NORMAL
- en: 'Afterward, you can press **2D Nav Goal** button and set the target to the *bottom-left
    corner* by clicking the left mouse button. Release the mouse when the arrow has
    the desired orientation. After releasing, the robot will compute the path to follow
    and start navigating autonomously:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0bb18a70-d0d2-40ba-8653-1d4807050717.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot shows the first instant of the navigation plan execution.
    The orientation of the red arrow tells the GoPiGo3 in what direction it should
    stay facing once it has arrived at the target, and the curved line going from
    the robot to the target is the planned path. Since it has a map of the environment
    available, the robot is able to plan a path that avoids the obstacles. Wait a
    few seconds and you will see how the robot reaches the target without any external
    help.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the robot has arrived, press the **2D Nav Goal** button again and then
    mark the top-right corner. The following screenshot shows the first instant of
    the execution of the next navigation plan:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d094c479-4c53-4f8a-be7b-78af194f3844.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see the new planned path and how this takes into account the presence
    of obstacles along the way to avoid collisions. The blue square around the robot
    represents the local window for obstacle avoidance planning. This is used by the
    **Dynamic Window Approach** (**DWA**) method, which generates a local path that
    efficiently evades the obstacles. The DWA method performs the calculations taking
    into account the robot's dynamics, in particular, its limited velocity and acceleration.
  prefs: []
  type: TYPE_NORMAL
- en: The AMCL algorithm for robot navigation generates the global path to reach the
    target based on the provided map, while the DWA method calculates the local path that
    accounts for the local conditions the robot may find near it. The latter provides
    the capability to deal both with obstacles present in the map, and also dynamic
    ones, such as people crossing the robot's path, for example. The *global path*
    and *local path* combine together to produce *highly autonomous robot navigation*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the GoPiGo3 in the final instant before reaching
    the goal. Appreciate how, at this point, the DWA window also includes the target:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26c3de76-b819-42f1-a2bc-fd0c6d5d478f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, you should frame robot navigation within a sequence of tasks that
    the robot has to complete, one after the other, in order to achieve the goal that
    has been set by the user. Taking the examples of the navigation paths that we
    have used already in this chapter for explanation purposes, imagine a scenario
    in which the GoPiGo3 has to pick up an object in location *A* (the left-bottom
    corner) and deliver it to location *B* (the upper-right corner). In this case,
    the sequence of tasks would be as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to location *A*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Pick up the piece at location *A*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to location *B*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Drop off the piece at location *B*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Conceptually, it is easy, right? But in this chapter, we have only covered the
    basics to accomplish tasks 1 and 3\. Later, in [Chapter 10](3bf944de-e0f8-4e78-a38b-47796c91185b.xhtml),
    *Applying Machine Learning in Robotics*, you will be given the technical background
    on **object recognition** so that you can also program tasks 2 and 4\. More precisely,
    it will be in the *A methodology to programmatically apply ML in Robotics *section where
    we will provide you with this insight.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has introduced you to the master task of robot navigation. SLAM
    and navigation are complex matters and active research topics in robotics. So,
    this chapter has given you a taste of how to implement it so that you can quickly
    understand its mechanics without entering into details of the algorithms and the
    mathematics behind.
  prefs: []
  type: TYPE_NORMAL
- en: We expect to have aroused your curiosity on this topic. Now you are prepared
    to carry out the same task in the real world with the physical GoPiGo3\. In the
    next chapter, you will perform the navigation and SLAM tasks with the physical
    robot.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Where are the sensor specifications included within a Gazebo SDF file?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) Outside of a `<gazebo>` tag
  prefs: []
  type: TYPE_NORMAL
- en: B) Within a `<joint>` tag
  prefs: []
  type: TYPE_NORMAL
- en: C) Within a `<sensor>` tag
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the controller specification of a sensor in Gazebo, what is the most
    relevant parameter in terms of CPU usage while running the simulation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) The scan distance, because the larger the sensor range is, the more bandwidth
    consumption the CPU performs.
  prefs: []
  type: TYPE_NORMAL
- en: B) The angular scan, since the greater the angular resolution, the more bandwidth
    consumption is required to store the readings in the RAM.
  prefs: []
  type: TYPE_NORMAL
- en: C) The maximum sensor frequency, because they are so high in real sensors that
    they easily can overload the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Where are the sensor mechanical properties included within a Gazebo description
    of the robot?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) Outside of a `<gazebo>` tag
  prefs: []
  type: TYPE_NORMAL
- en: B) Within a `<joint>` tag
  prefs: []
  type: TYPE_NORMAL
- en: C) Within a `<sensor>` tag
  prefs: []
  type: TYPE_NORMAL
- en: What does the SLAM technique provide to a robot?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) A method to avoid moving obstacles in the environment
  prefs: []
  type: TYPE_NORMAL
- en: B) A method to build a map of the environment
  prefs: []
  type: TYPE_NORMAL
- en: C) A method to avoid static and moving obstacles in the environment
  prefs: []
  type: TYPE_NORMAL
- en: How do you operationally specify a navigation goal to a robot?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) Tell it the target location and orientation
  prefs: []
  type: TYPE_NORMAL
- en: B) Set a target location in a two-dimensional map of the environment
  prefs: []
  type: TYPE_NORMAL
- en: C) Mark the borders of the area where the robot is expected to navigate to
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To delve deeper into the concepts explained in this chapter, you can check
    out the following references:'
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive Monte Carlo Localization (AMCL), at [http://roboticsknowledgebase.com/wiki/state-estimation/adaptive-monte-carlo-localization/](http://roboticsknowledgebase.com/wiki/state-estimation/adaptive-monte-carlo-localization/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Particle Filters in Robotics*, Proceedings of Uncertainty in AI (UAI), Thrun
    S. (2002), at [http://robots.stanford.edu/papers/thrun.pf-in-robotics-uai02.pdf](http://robots.stanford.edu/papers/thrun.pf-in-robotics-uai02.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SLAM for Dummies*, A Tutorial Approach to Simultaneous Localization and Mapping,
    Riisgaard S, at [http://zyzx.haust.edu.cn/moocresource/data/081503/U/802/pdfs/soren_project.pdf](http://zyzx.haust.edu.cn/moocresource/data/081503/U/802/pdfs/soren_project.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Robot Perception for Indoor Navigation*, Endres, F. (2015), Albert-Ludwigs-Universitat
    Freiburg, at[ https://d-nb.info/1119716993/34](https://d-nb.info/1119716993/34)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
