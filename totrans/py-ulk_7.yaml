- en: Chapter 7. Optimization Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn how to optimize our Python code to get better
    responsive programs. But, before we dive into this, I would like to stress that
    do not optimize until it is necessary. A better-readable program has a better
    life and maintainability than a tersely-optimized program. First, we will take
    a look at simple optimization tricks to keep a program optimized. We should have
    knowledge about them so that we can apply easy optimizations from the start. Then,
    we will look at profiling to find bottlenecks in the current program and apply
    optimizations where we need them. As a last resort, we can compile in the C language
    and provide functionality as an extension to Python. Here is the gist of topics
    that we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Writing optimized code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling to find bottlenecks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using fast libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using C speeds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing optimized code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Key 1: Easy optimizations for code.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We should pay close attention to not use loops inside loops, giving us quadratic
    behavior. We can use built-ins, such as map, ZIP, and reduce, instead of using
    loops if possible. For example, in the following code, the one with map is faster
    because the looping is implicit and done at C level. By plotting their run times
    respectively on graph as `test 1` and `test 2`, we see that it is nearly constant
    for PyPy but reduces a lot for CPython, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image is a graphical representation of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Writing optimized code](img/B04885_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Generators should be used, when the result that is consumed is averagely smaller
    than the total result consumed. In other words, the result that is generated in
    the end may not be used. They also serve to conserve memory because no temporary
    result is stored but generated on demand. In the following example, `sqrt_5` creates
    a generator, while `sqrt_6` creates a list. The `use_combo` instance breaks out
    of the loop of iteration after a given number of iterations. Test 1 runs `use_combo(sqrt_5,range(10),5)`
    and all results are consumed from iterator, whereas test 2 is for the `use_combo(sqrt_6,range(10),5)`
    generator. Test 1 should take more time than test 2 as it creates results for
    all ranges of inputs. Tests 3, and 4 are run with a range of `25`, and tests 5,
    and 6 are run with a range of `100`. As it can be seen, the time consumption variation
    increases with no of elements in the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image is the graphical representation of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Writing optimized code](img/B04885_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When we are inside a loop and reference an outside namespace variable, it is
    first searched in local, then nonlocal, followed by global, and then built-in
    scopes. If the number of repetitions are more, then such overheads add up. We
    can reduce namespace lookup by making such global/built-in objects available in
    the local namespace. For example, in the following code snippet, `sqrt_7(test2)`
    will be faster `than sqrt_1(test1)` because of the same reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image is the graphical representation of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Writing optimized code](img/B04885_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The cost of subclassing is not much and subclassing doesn''t make method calls
    slower even if common sense says that it will take lot of time to look a method
    up on the inheritance hierarchy. Let''s take the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, if we call `get_sqrt` on the `Actual(case1)` class, we need to search
    it seven levels deep in its base classes, whereas for the `Actual2(case2)` class
    it is present on the class itself. The following graph is our plot for both scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Writing optimized code](img/B04885_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Also, if we are using too many checks in the program logic for return codes
    or error conditions, we should see how many such checks are really needed. We
    can write the program logic without using any checks and then get the errors in
    the exception handling logic. This makes the code easy to understand. As in the
    following example, the `getf_1` function uses checks to filter out error conditions,
    but too many checks are making code hard to understand. The other `get_f2` function
    is the same application logic or algorithm with exception handling. For test 1
    `(get_f1)` and test 2 `(get_f2)`, no file is present, so all exceptions are raised.
    In this scenario, the exception handling logic, that is test 2, takes more time.
    For test 3 `(get_f1)` and test 4 `(get_f2)`, the file and key are present; hence,
    no error is raised. In this case, test 4 takes less time, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image is the graphical representation of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Writing optimized code](img/B04885_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Function calling has overheads and if the performance bottlenecks can be removed
    by reducing function calls, we should do so. Typically, functions call in loops.
    In the following example, when we wrote logic inline, it took less time. Also,
    for PyPy such effects are less in general as most called functions in loops are
    generally called with the same type of arguments; hence, they get compiled. Any
    further call to these functions is like calling a C language function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image is the graphical representation of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Writing optimized code](img/B04885_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Profiling to find bottlenecks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Key 2: Identifying application performance bottlenecks.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We should not rely on our intuition on how to optimize application. There are
    two major ways for logic slowdown; one is CPU time taken, and the second is the
    wait for results from some other entity. By profiling, we can find out such cases
    in which we can tweak logic, and language syntax to get better performance on
    the same hardware. The following code is a `showtime` decorator that I use to
    calculate the time taken to call a function. It is simple and effective to get
    rapid answers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'When profiling a single large function that does a lot of stuff, we may need
    to know on what particular line we are spending the most time. This query can
    be answered using the `line_profiler` module. You can get it with `pip install
    line_profiler`. It shows the time that is spent per line. To get results, we should
    decorate the function with a special profile decorator that will be used by `line_profiler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Another way of profiling is using the `kernprof` program that is supplied with
    the `line_profiler` module. We have to decorate the function to be a profiler
    by the `@profile` decorator and run the program, as shown in the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for this will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Memory profilers are a very good tool to estimate memory consumption in a program.
    To profile a function, simply decorate it with profile and run the program like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'To get details on the command line, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use it to debug long-running programs. The following code is for
    a simple socket server. It adds lists to the global lists variable, which never
    gets deleted. Saving contents in `simple_serv.py` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, run the program via profiler as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Put some bogus hits to the server. I used the `netcat` utility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Kill the server after some time and plot the memory consumed over time with
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We get a good graph showing us memory consumption over time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Profiling to find bottlenecks](img/B04885_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Other than getting program memory consumption, we may be interested in objects
    carrying spaces. Objgraph ([https://pypi.python.org/pypi/objgraph](https://pypi.python.org/pypi/objgraph))
    is able to graph object links for your programs. Guppy ([https://pypi.python.org/pypi/guppy/](https://pypi.python.org/pypi/guppy/))
    is another package that has heapy, which is a heap analysis tool. It is very helpful
    to see the number of objects on heap for a running program. As of this writing,
    it was only available for Python 2\. For analysis of a long-running process, Dowser
    ([https://pypi.python.org/pypi/dowser](https://pypi.python.org/pypi/dowser)) is
    also a good choice. We can use Dowser to see the memory consumption to run Celery
    or a WSGI server. Django-Dowser is good and provides the same functionality as
    an app, but as the name suggests, it only works with Django.
  prefs: []
  type: TYPE_NORMAL
- en: Using fast libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Key 3: Use easy drop-in faster libraries.**'
  prefs: []
  type: TYPE_NORMAL
- en: There are libraries out there that can help a lot in optimizing code, rather
    than writing some optimized routines yourself. For example, if we have a list
    that needs to be fast at FIFO, we may use the `blist` package. We can use C versions
    of libraries, such as `cStringIO` (faster StringIO), `ujson` (faster JSON handling),
    `numpy` (math, and vectors), and `lxml` (XML handling). Most of the libraries
    that are listed here are just a Python wrapper over C libraries. You only need
    to search once for your problem domain. Other than this, we can make a C, or C++
    library interface with Python very easily, which is also our next topic.
  prefs: []
  type: TYPE_NORMAL
- en: Using C speeds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Key 4: Running at C speeds.**'
  prefs: []
  type: TYPE_NORMAL
- en: SWIG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SWIG is an interface compiler that connects programs written in C, and C++
    with scripting languages. We can use SWIG to call C, C++ compiled in Python. Let''s
    say that we have a factorial computing library in C, with source code in the `fact.c`
    file and the corresponding `fact.h` header file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The source code in `fact.c` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The source code in `fact.h` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to write an interface file for SWIG, which tells it what it needs
    to be exposed to Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, module indicates the module name for the Python library, and `SWIG_FILE_WITH_INIT`
    indicates that the resulting C code should be built with a Python extension. The
    content in `{% %}` is used in the C wrap code that is generated. We have three
    files, `fact.c`, `fact.h`, and `fact.i`, in directory. We run SWIG to generate
    `wrapper_code` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `-O` option is used for optimizations and `-py3` is for Python 3 specific
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 'This generates `fact.py` and `fact_wrap.c`. The `fact.py` is a Python module
    and `fact_wrap.c` is the glue code between C and Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, I have to include my `python.h` path to compile it. This will generate
    `fact.o` and `fact_wrap.o`. Now, the last part is to create a dynamic linked library,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `_fact.so` file is used by the `fact.py` to run C functions. Now, we can
    use the fact module in our Python programs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: CFFI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **C Foreign Function Interface** (**CFFI**) for Python is one tool that
    looks the best to me because of the easy setup and interface. It works on an ABI
    and API level.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using our factorial C programs here as well, we first create a shared library
    for the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have a `_fact.so` shared library object in our current directory. To
    load this in the Python environment, we can perform this action which is very
    straightforward. We should have header files for the library so that we can use
    declarations. Install the CFFI package from distribution or pip that is needed
    for this, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We can reduce import times for the module if we do not call `cdef` in the import
    modules. We can write another `setup_fact_ffi.py` module that gives us a `fact_ffi.py`
    module with the compiled information. Hence, the load times decrease a lot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We now can use this module to get `ffi` and load our shared library as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Until this point, as we were using a precompiled shared library, we didn't need
    a compiler. Let's suppose that there is this small C function that you need in
    Python, and you do not want to write another .c file for it, then this is how
    it can be done. You can also extend it to shared libraries as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define a `build_ffi.py` file, which will compile and create a module
    for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run Python `fact_build.py`, this will create a `_fact_cffi.cpython-34m.so`
    module. To use it, we have to import it and use the `lib` variable to access the
    module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Cython
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cython is like a superset of Python in which we can optionally give static declarations.
    The source code is compiled to C/C++ extension modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'We write our old factorial program in `fact_cpy.pyx` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Here `cpdef` is the function declaration for CPython that creates a Python function
    and conversion logic for arguments, and a C function that actually executes. `cpdef`
    is defining the data type for the res variable, which helps in speedup.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have to create a `setup.py` file to compile this code into an extension
    module (we can directly use it by using `pyximport` but we will leave that for
    now). The contents for the `setup.py` file will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now to build the module, all we have to do is type in the following command,
    and we get a `fact_cpy.cpython-34m.so` file in the current directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this in Python is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw various techniques that are used to optimize and profile
    code. I will, again, point out that we should always focus on first writing the
    correct program, then writing test cases for it, and then optimizing it. We should
    write code with optimizations that we know at that time or without optimization
    the first time, and we should hunt for them only if we need them from a business
    perspective. Compiling a C module can give a good speedup for CPU-intensive tasks.
    Also, we can give up GIL in C modules, which can also help us in increasing performance.
    But, all of this was on single system. Now, in the next chapter, we will see how
    we can improve performance when the tricks that were discussed in this chapter
    are not sufficient for a real-life scenario.
  prefs: []
  type: TYPE_NORMAL
