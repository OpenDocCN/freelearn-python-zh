<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer041">
<h1 class="chapter-number" id="_idParaDest-146"><a id="_idTextAnchor159"/>6</h1>
<h1 id="_idParaDest-147"><a id="_idTextAnchor160"/>Deploying and Monitoring Your Application</h1>
<p>In the previous chapter, we built the frontend of our app, thereby completing it as a useable tool. However, while we are able to use it locally, no other users would be able to. Therefore, in this chapter, we’ll deploy our application and make it available via a public domain name, <a href="http://tozo.dev">tozo.dev</a>. We’ll also ensure that we are monitoring the app so that we can quickly fix any issues.</p>
<p>So, in this chapter, you’ll learn how to build the infrastructure in AWS for any Docker containerized app that needs a database; this infrastructure will be able to scale to very high loads without <a id="_idIndexMarker372"/>major changes. You’ll also learn how to set up a <strong class="bold">domain name system</strong> (<strong class="bold">DNS</strong>) and HTTPS for your domain, both of which are applicable to any website or application. Finally, you’ll learn the importance of monitoring and how to do so easily.</p>
<p>For our app to be accessible via a public domain name, it will need to be running on a system that is always accessible via the internet. This could be any system, including our local computer. However, the system needs to be continuously running and accessible via a stable IP address. For this reason, it is much better to pay for a dedicated system managed by AWS.</p>
<p class="callout-heading">AWS costs</p>
<p class="callout">The AWS infrastructure built in this chapter will cost approximately $20 per month to run without the free tier. It will be cheaper (but not free) if you are able to use the free tier. Alternatively, AWS has a number of startup credit programs you may be eligible for. </p>
<p class="callout">If you want to stop paying, you will need to remove the infrastructure, which can be done by deleting the <strong class="source-inline">resource</strong> definitions and running <strong class="source-inline">terraform apply</strong>.</p>
<p>Once we have paid for a remote system, we could configure it to run our app directly, as we have our local system. However, we will use a containerized infrastructure as it is easier to configure the container to run our app than to configure the remote system. </p>
<p>So, in this chapter, we will cover the following topics:</p>
<ul>
<li>Making the app production-ready</li>
<li>Deploying to AWS</li>
<li>Serving on a domain</li>
<li>Sending production emails</li>
<li>Monitoring production </li>
</ul>
<h1 id="_idParaDest-148"><a id="_idTextAnchor161"/>Technical requirements</h1>
<p>To follow the development in this chapter using the companion repository, <a href="https://github.com/pgjones/tozo">https://github.com/pgjones/tozo</a>, see the commits between the <strong class="source-inline">r1-ch6-start</strong> and <strong class="source-inline">r1-ch6-end</strong> tags.</p>
<h1 id="_idParaDest-149"><a id="_idTextAnchor162"/>Making the app production-ready</h1>
<p>As our production <a id="_idIndexMarker373"/>infrastructure will run containers, we need to containerize our app. To do so, we’ll need to decide how to serve the frontend and backend, and how to build the container image.</p>
<h2 id="_idParaDest-150"><a id="_idTextAnchor163"/>Serving the frontend </h2>
<p>So far in <a id="_idIndexMarker374"/>development, we’ve used <strong class="source-inline">npm run start</strong> to run a server that serves the frontend code. This is called <strong class="bold">server-side rendering</strong> (<strong class="bold">SSR</strong>), and <a id="_idIndexMarker375"/>we could continue to do this in production. However, it is much easier to utilize <strong class="bold">client-side rendering</strong> (<strong class="bold">CSR</strong>), as this does <a id="_idIndexMarker376"/>not require a dedicated frontend server. CSR works by building a bundle of frontend files that can be served by any server (rather than a dedicated frontend server), and we’ll use the backend server.</p>
<p>To build the frontend bundle, we can use the <strong class="source-inline">npm run build</strong> command. This command creates a single HTML file (<em class="italic">frontend/build/index.xhtml</em>) and multiple static files (<strong class="source-inline">css</strong>, <strong class="source-inline">js</strong>, and <strong class="source-inline">media</strong>) in the following structure:</p>
<pre class="source-code">
tozo
└── frontend
    └── build
        └── static
            ├── css
            ├── js
            └── media</pre>
<p>The static files, consisting of the files within the <em class="italic">frontend/build/static</em> folder, can be served by <a id="_idIndexMarker377"/>moving the files and structure to the <em class="italic">backend/src/backend/static</em> folder. Our backend will then serve these files automatically with paths matching the folder structure.</p>
<p>The remaining part of the bundle (i.e., the HTML file) will need to be served for any request that matches a page in the app. To do this, we first need a serving blueprint, which is created by adding the following to <em class="italic">backend/src/backend/blueprints/serving.py</em>:</p>
<pre class="source-code">
from quart import Blueprint
blueprint = Blueprint("serving", __name__)</pre>
<p>The blueprint then needs to be registered with the app, by adding the following to <em class="italic">backend/src/backend/run.py</em>:</p>
<pre class="source-code">
from backend.blueprints.serving import blueprint as serving_blueprint
app.register_blueprint(serving_blueprint)</pre>
<p>As the backend has no knowledge regarding which paths match the pages on the frontend, it will have to serve the frontend for any paths that do not match backend API paths. This is done in Quart by using a <strong class="source-inline">&lt;path:path&gt;</strong> URL variable; so, add the following into <em class="italic">backend/src/backend/blueprints/serving.py</em>:</p>
<pre class="source-code">
from quart import render_template, ResponseReturnValue
from quart_rate_limiter import rate_exempt
 
@blueprint.get("/")
@blueprint.get("/&lt;path:path&gt;")
@rate_exempt
async def index(path: str | None = None) -&gt; ResponseReturnValue:
    return await render_template("index.xhtml")</pre>
<p>Finally, <em class="italic">frontend/build/index.xhtml</em> will <a id="_idIndexMarker378"/>need to be copied to <em class="italic">backend/src/backend/templates/index.xhtml</em> for the production app, as we will do when containerizing the app.</p>
<p>As it is now possible to serve the frontend from the backend server, we can now focus on using a production-ready backend server.</p>
<h2 id="_idParaDest-151"><a id="_idTextAnchor164"/>Serving the backend</h2>
<p>So far in <a id="_idIndexMarker379"/>development, we’ve used <strong class="source-inline">pdm run start</strong> to run and serve the backend. This, however, is unsuitable for production as it starts a Hypercorn server configured for development (for example, it configures the server to output debugging information). </p>
<p class="callout-heading">Hypercorn</p>
<p class="callout">Quart is a framework that requires a server to work. So far in development, we’ve been using Hypercorn as <a id="_idIndexMarker380"/>configured for development. Hypercorn is a Python server that supports HTTP/1, HTTP/2, and HTTP/3 in a performant manner and is recommended by Quart. </p>
<p>We will configure Hypercorn for production usage using the following placed in <em class="italic">hypercorn.toml</em>:</p>
<pre class="source-code">
accesslog = "-"
access_log_format = "%(t)s %(h)s %(f)s - %(S)s '%(r)s' %(s)s %(b)s %(D)s"
bind = "0.0.0.0:8080"
errorlog = "-"</pre>
<p>The <strong class="source-inline">accesslog</strong> and <strong class="source-inline">errorlog</strong> configuration ensure that Hypercorn logs every request and error while it runs, which will help us understand what the server is doing. The <strong class="source-inline">bind</strong> configures Hypercorn to listen on the <strong class="source-inline">8080</strong> port, which we’ll direct network traffic to when we set up the production infrastructure in the next section. </p>
<p>The server <a id="_idIndexMarker381"/>can then be started in production via the following command:</p>
<p class="source-code">pdm run hypercorn --config hypercorn.toml backend.run:app</p>
<p>Now we know how to serve the backend in a production environment, we need to focus on how we install everything we need to do so.</p>
<h2 id="_idParaDest-152"><a id="_idTextAnchor165"/>Containerizing the app</h2>
<p>To run the app in production, we need all the app’s dependencies and the app’s code installed <a id="_idIndexMarker382"/>in the container. We will achieve this by building a container image with the dependencies installed and the code included.</p>
<p>To build the image, we’ll use a Dockerfile as it is the clearest way to build an image. Specifically, we will use a multistage Dockerfile, with the first stage building the frontend, and the final stage installing and running the backend server.</p>
<p class="callout-heading">Docker terms</p>
<p class="callout">A <strong class="bold">Dockerfile</strong> is used <a id="_idIndexMarker383"/>with Docker to build a container image. The Dockerfile is an ordered list of commands, with each command producing a layer of the final image, and with each layer building upon the previous. The final image will need to include everything required to run the code contained within it. A running instance of the image <a id="_idIndexMarker384"/>is known as a <strong class="bold">container</strong>.</p>
<h3>Building the frontend stage</h3>
<p>To build <a id="_idIndexMarker385"/>the frontend, we will need a system with NodeJS installed. As this is a common requirement, there are NodeJS base images we can use. Therefore, we can start by adding the following to <em class="italic">Dockerfile</em> to create a NodeJS-based stage called <strong class="source-inline">frontend</strong>:</p>
<p class="source-code">FROM node:18-bullseye-slim as frontend </p>
<p>Next, we need to create a working directory and install the frontend dependencies within it:</p>
<pre class="source-code">
WORKDIR /frontend/
COPY frontend/package.json frontend/package-lock.json /frontend/
RUN npm install</pre>
<p>This is best done before the code is copied into the image as the dependencies change less often than the code. </p>
<p class="callout-heading">Dockerfile caching</p>
<p class="callout">The Dockerfile is a sequence of commands with each command forming a layer in the final image. These layers are built in the order given in the Dockerfile and a change to any layer requires all the subsequent layers to be rebuilt with earlier layers being cached. Hence, it is best to put layers that rarely change before those that change often.</p>
<p>Finally, we can copy the frontend code we’ve written for our app into the image and build it with the following code:</p>
<pre class="source-code">
COPY frontend /frontend/
RUN npm run build</pre>
<p>We now have a complete frontend stage containing the built frontend. We’ll make use of this in the production image.</p>
<h3>Building the production image</h3>
<p>The production image will be built as the second stage of the <em class="italic">Dockerfile</em>. This stage can also start from an existing base image, as systems with Python installed are also a common <a id="_idIndexMarker386"/>requirement. To do so the following should be added to the <em class="italic">Dockerfile</em>:</p>
<p class="source-code">FROM python:3.10.1-slim-bullseye</p>
<p>Next, we need to add an <strong class="source-inline">init</strong> system to ensure that signals are correctly sent to our backend server as it runs in the Docker container. <strong class="source-inline">dumb-init</strong> is a popular solution and one I’ve used many times before. <strong class="source-inline">dumb-init</strong> is installed and configured with the following additions:</p>
<pre class="source-code">
RUN apt-get update &amp;&amp; apt install dumb-init 
ENTRYPOINT ["/usr/bin/dumb-init", "--"]</pre>
<p>We can then configure Hypercorn to start when the image is run:</p>
<pre class="source-code">
EXPOSE 8080
RUN mkdir -p /app
WORKDIR /app
COPY hypercorn.toml /app/
CMD ["pdm", "run", "hypercorn", "--config", "hypercorn.toml", "backend.run:app"]</pre>
<p>Next, we need to install the backend dependencies, which first requires that we install <strong class="source-inline">pdm</strong> and configure Python to work with it:</p>
<pre class="source-code">
RUN python -m venv /ve
ENV PATH=/ve/bin:${PATH}
RUN pip install --no-cache-dir pdm</pre>
<p>This allows us to install the backend dependencies using <strong class="source-inline">pdm</strong>:</p>
<pre class="source-code">
COPY backend/pdm.lock backend/pyproject.toml /app/
RUN pdm install --prod --no-lock --no-editable </pre>
<p>Now, we can include the built frontend from the frontend stage:</p>
<pre class="source-code">
COPY --from=frontend /frontend/build/index.xhtml \
    /app/backend/templates/ 
COPY --from=frontend /frontend/build/static/. /app/backend/static/</pre>
<p>Finally, we can copy the backend code into the image:</p>
<p class="source-code">COPY backend/src/ /app/</p>
<p>This <a id="_idIndexMarker387"/>gives us a complete image ready to use in production. </p>
<p>To make the image more secure, we can alter the user that will run the server. By default, this is the <strong class="source-inline">root</strong> user that comes with admin privileges and access, whereas changing to <strong class="source-inline">nobody</strong> removes these privileges. We can do this by adding the following:</p>
<p class="source-code">USER nobody</p>
<p>As we’ve defined how to build a Docker image, we can now focus on building and deploying it.</p>
<h1 id="_idParaDest-153"><a id="_idTextAnchor166"/>Deploying to AWS</h1>
<p>To deploy our app, we need to build an infrastructure that runs containers and a database. The containers <a id="_idIndexMarker388"/>must be reachable from the public internet, and the database from the containers. This infrastructure is easily buildable with <strong class="bold">AWS</strong>, which we’ll use. However, in this book, we’ll use AWS services that have equivalents on other cloud providers if you wish to use a different provider.</p>
<p>To start, we need <a id="_idIndexMarker389"/>to create an AWS account (through this link: <a href="http://aws.amazon.com">aws.amazon.com</a>) using an email, password, and your card details. This account will be the root <a id="_idIndexMarker390"/>or superuser account; therefore, we will create an additional <strong class="bold">identity and access management</strong> (<strong class="bold">IAM</strong>) subaccount for Terraform to use. The IAM user is created via the <strong class="bold">Add users</strong> button on the IAM <strong class="bold">Users</strong> dashboard shown in <em class="italic">Figure 6.1</em>: </p>
<div>
<div class="IMG---Figure" id="_idContainer035">
<img alt="Figure 6.1: The IAM dashboard (with the Add users button) " height="733" src="image/B18727_06_01.jpg" width="1319"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1: The IAM dashboard (with the Add users button)</p>
<p>I will name <a id="_idIndexMarker391"/>the user <strong class="source-inline">terraform</strong> to indicate what it is used for. It should have programmatic access only and have the <strong class="source-inline">AdministratorAccess</strong> policy attached. Once created, an access key ID and secret access key will be shown; both need to be added as follows to <em class="italic">infrastructure/secrets.auto.tfvars</em>:</p>
<pre class="source-code">
aws_access_key = "abcd"
aws_secret_key = "abcd"</pre>
<p>I am using <strong class="source-inline">abcd</strong> as examples, which you need to replace with your own values.</p>
<p>With your credentials in place, we can start configuring Terraform to work with AWS. Firstly, add the AWS provider to Terraform by adding the following to the existing Terraform <strong class="source-inline">required_providers</strong> section in <em class="italic">infrastructure/main.tf</em>:</p>
<pre class="source-code">
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "&gt;=3.35.0"
    }
  }
}</pre>
<p>After making this change, <strong class="source-inline">terraform init</strong> will need to be run for the change to take effect.</p>
<p>We can then <a id="_idIndexMarker392"/>configure the provider, which requires choosing a region to use. As I’m based in London, UK, I’ll be using <strong class="source-inline">eu-west-2</strong>, however, I recommend that you use whichever region is closest to your customers. This is done by adding the following to <em class="italic">infrastructure/aws.tf</em>:</p>
<pre class="source-code">
variable "aws_access_key" {
  sensitive = true
}
 
variable "aws_secret_key" {
  sensitive = true
}
 
provider "aws" {
  access_key = var.aws_access_key
  secret_key = var.aws_secret_key
  region     = "eu-west-2"
}</pre>
<p>We can now use Terraform to manage the AWS infrastructure, which means we can focus on what we want that infrastructure to be.</p>
<h2 id="_idParaDest-154"><a id="_idTextAnchor167"/>Designing the production system</h2>
<p>In <a href="B18727_02.xhtml#_idTextAnchor053"><em class="italic">Chapter 2</em></a>, <em class="italic">Creating a Reusable Backend with Quart</em>, we decided to build a three-tier architecture <a id="_idIndexMarker393"/>where there is a backend API that communicates with the frontend and with a database. This means that in AWS, we need to be running the database, the backend in a container, and a load balancer to listen to incoming requests from the frontend. To do so, we can use the services and setup shown in <em class="italic">Figure 6.2</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer036">
<img alt="Figure 6.2: The intended AWS architecture " height="512" src="image/Figure_6.2_NEW.jpg" width="565"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2: The intended AWS architecture</p>
<p>This architecture uses the following AWS services:</p>
<ul>
<li><strong class="bold">Relational Database Service</strong> (<strong class="bold">RDS</strong>) to <a id="_idIndexMarker394"/>run a PostgreSQ database</li>
<li><strong class="bold">Elastic Container Service</strong> (<strong class="bold">ECS</strong>) to <a id="_idIndexMarker395"/>run the app container</li>
<li><strong class="bold">Application Load Balancer</strong> (<strong class="bold">ALB</strong>) to <a id="_idIndexMarker396"/>accept connections from the internet (frontend)</li>
</ul>
<p>In addition, we’ll <a id="_idIndexMarker397"/>use the <strong class="bold">Fargate</strong> variant of ECS as this means that we won’t need to manage the systems running the containers.</p>
<p>By using these managed services, we can pay AWS to do most of the work of managing the servers, allowing <a id="_idIndexMarker398"/>us to focus on our app instead. We can now set up the networking to support this architecture.</p>
<h2 id="_idParaDest-155"><a id="_idTextAnchor168"/>Setting up the networking</h2>
<p>To build <a id="_idIndexMarker399"/>our architecture, we have to start at the foundation, which is the network. We need to define how the systems can communicate with one <a id="_idIndexMarker400"/>another. In <em class="italic">Figure 6.3</em>, you can see that we are aiming for a single <strong class="bold">virtual private cloud</strong> (<strong class="bold">VPC</strong>) with public and private subnets. </p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer037">
<img alt="Figure 6.3: The intended network setup " height="796" src="image/B18727_06_03.jpg" width="449"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3: The intended network setup</p>
<p>Crucially, the private subnets can only communicate with the public subnets, but not the internet directly. This means that we can place the database in the private subnets, and the app and ALB in the public subnets, thereby adding an additional layer of security that prevents unauthorized database access.</p>
<p class="callout-heading">VPC</p>
<p class="callout">A VPC is a virtual network containing resources. We’ll use a single VPC for all our resources.</p>
<p>To build <a id="_idIndexMarker401"/>the network, we first need to create an AWS VPC for our systems by adding the following to <em class="italic">infrastructure/aws_network.tf</em>:</p>
<pre class="source-code">
resource "aws_vpc" "vpc" {
  cidr_block         = "10.0.0.0/16"
  enable_dns_support = true
}</pre>
<p class="callout-heading">CIDR notation</p>
<p class="callout">AWS uses <strong class="bold">CIDR</strong> block <a id="_idIndexMarker402"/>notation to specify the range of valid IPs associated with parts of the network. This notation works by specifying an IP address followed by a number called the netmask (after the <strong class="source-inline">/</strong>). IPv4 addresses consist of 4 bytes (each byte is 8 bits) with each byte written as a number separated by dots (<strong class="source-inline">.</strong>). The netmask number indicates how many leading bits of the trial address must match the given address to be considered part of the given range. The following examples show CIDR ranges:</p>
<p class="callout"> - <strong class="source-inline">10.0.0.0/16</strong> indicates that the first 16 bits (or the first two bytes) must match within this range (i.e., any address starting with <strong class="source-inline">10.0</strong> is in the range) </p>
<p class="callout"> - <strong class="source-inline">10.0.0.64/26</strong> indicates that the first 26 bits or the first 3 bytes and then the first 2 bits of the final byte must match (i.e., any address between <strong class="source-inline">10.0.0.64</strong> and <strong class="source-inline">10.0.0.128</strong> (excluding <strong class="source-inline">10.0.0.128</strong>)</p>
<p class="callout"> - <strong class="source-inline">0.0.0.0/0</strong> means that any IP address matches</p>
<p>With this VPC setup, all the IP addresses we will use will be in the <strong class="source-inline">10.0.0.0/16</strong> CIDR block and hence will begin with <strong class="source-inline">10.0</strong>. This block is a conventional choice for AWS VPCs. </p>
<p>We can now <a id="_idIndexMarker403"/>divide the VPC into subnets or subnetworks, as this allows us to restrict which subnets can communicate with each other and the public internet. Firstly, we’ll divide the VPC into public subnets in the CIDR block <strong class="source-inline">10.0.0.0/24</strong> and private subnets in <strong class="source-inline">10.0.1.0/24</strong>. I’ve chosen these as blocks as it makes the distinction very clear that any IP that starts with <strong class="source-inline">10.0.0</strong> will be public, and <strong class="source-inline">10.0.1</strong> will be private.</p>
<p>As an AWS region is split into availability zones, we’ll create a public and a private subnet for each zone, with up to a total of four subnets. Four is the best number as it is represented by 2 bits and hence makes the CIDR ranges easier to express. The netmask for these subnets is therefore 26, as it is 24 plus the 2 bits required. This is done by adding the following to <em class="italic">infrastructure/aws_network.tf</em>: </p>
<pre class="source-code">
data "aws_availability_zones" "available" {}
 
resource "aws_subnet" "public" {
  availability_zone = data.aws_availability_zones.available.names[count.index]
  cidr_block        = "10.0.0.${64 * count.index}/26"
  count             = min(4, length(data.aws_availability_zones.available.names))
  vpc_id            = aws_vpc.vpc.id
}
 
resource "aws_subnet" "private" {
  availability_zone = data.aws_availability_zones.available.names[count.index]
  cidr_block        = "10.0.1.${64 * count.index}/26"
  count             = min(4, length(data.aws_availability_zones.available.names))
  vpc_id            = aws_vpc.vpc.id
}</pre>
<p class="callout-heading">Availability zones</p>
<p class="callout">AWS Regions are split into multiple (usually three) <strong class="bold">availability zones</strong> (often called <strong class="bold">AZs</strong>). Each zone <a id="_idIndexMarker404"/>is a physical data center separated from the others such that if there was a failure of one zone (e.g., a fire), it would not affect the others. Placing our systems in multiple zones, therefore, gives more robustness against failures.</p>
<p>As the <em class="italic">public</em> name suggests, we want systems in the public subnets to be able to communicate <a id="_idIndexMarker405"/>with the internet. This means that we need to add an internet gateway to the VPC and allow network traffic to route between it and the public subnets. This is done by adding the following to <em class="italic">infrastructure/aws_network.tf</em>:</p>
<pre class="source-code">
resource "aws_internet_gateway" "internet_gateway" {
  vpc_id = aws_vpc.vpc.id
}
resource "aws_route_table" "public" {
  vpc_id = aws_vpc.vpc.id
 
  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.internet_gateway.id
  }
}
 
resource "aws_route_table_association" "public_gateway" {
  count          = length(aws_subnet.public)
  subnet_id      = aws_subnet.public[count.index].id
  route_table_id = aws_route_table.public.id
}</pre>
<p>Finally, in terms of networking, we need a load balancer to accept connections from the internet and <a id="_idIndexMarker406"/>route them to the app containers. To begin, let’s add a security group for the load balancer that allows inbound (ingress) connections on ports <strong class="source-inline">80</strong> and <strong class="source-inline">443</strong> and any outbound (egress) connection; we do this in <em class="italic">infrastructure/aws_network.tf</em>:</p>
<pre class="source-code">
resource "aws_security_group" "lb" {
  vpc_id = aws_vpc.vpc.id
 
  ingress {
    protocol    = "tcp"
    from_port   = 80
    to_port     = 80
    cidr_blocks = ["0.0.0.0/0"]
  }
 
  ingress {
    protocol    = "tcp"
    from_port   = 443
    to_port     = 443
    cidr_blocks = ["0.0.0.0/0"]
  }
 
  egress {
    protocol    = "-1"
    from_port   = 0
    to_port     = 0
    cidr_blocks = ["0.0.0.0/0"]
  }
}</pre>
<p class="callout-heading">Protocols and ports</p>
<p class="callout">By default, websites <a id="_idIndexMarker407"/>serve requests using TCP (the protocol) on port <strong class="source-inline">80</strong> for HTTP and port <strong class="source-inline">443</strong> for HTTPS. The ports can be changed, but this isn’t recommended as most users won’t understand how to do the matching change in their browser. </p>
<p class="callout">The next version of HTTP, HTTP/3, will use QUIC over UDP as the protocol, with potentially any port the server defines. This technology is in its infancy at the moment though, and hence won’t be used in this book.</p>
<p>The load balancer itself can now be added by adding the following to <em class="italic">infrastructure/aws_network.tf</em>:</p>
<pre class="source-code">
resource "aws_lb" "tozo" {
  name               = "alb"
  subnets            = aws_subnet.public.*.id
  load_balancer_type = "application"
  security_groups    = [aws_security_group.lb.id]
}
 
resource "aws_lb_target_group" "tozo" {
  port        = 8080
  protocol    = "HTTP"
  vpc_id      = aws_vpc.vpc.id
  target_type = "ip"
 
  health_check {
    path = "/control/ping/"
  }
  lifecycle {
    create_before_destroy = true
  }
 
  stickiness {
    enabled = true
    type    = "lb_cookie"
  }
}</pre>
<p class="callout-heading">Load balancing</p>
<p class="callout">A load balancer <a id="_idIndexMarker408"/>will distribute requests across the target group in an attempt to balance the load experienced by each target in the target group. Therefore, it is possible to use multiple machines to serve the requests behind a single load balancer.</p>
<p>With the <a id="_idIndexMarker409"/>load balancer in place and ready, we can now start adding systems to the network, starting with the database. </p>
<h2 id="_idParaDest-156"><a id="_idTextAnchor169"/>Adding a database</h2>
<p>We can now add a PostgreSQL database to the private subnets, and then via a security group, we <a id="_idIndexMarker410"/>can ensure that the database can only communicate with systems in the public subnets. This makes it harder for an attacker to gain access to the database as they are unable to access it directly. So, to do this, the following should be added to <em class="italic">infrastructure/aws_network.tf</em>:</p>
<pre class="source-code">
resource "aws_db_subnet_group" "default" {
  subnet_ids = aws_subnet.private.*.id
}
 
resource "aws_security_group" "database" {
  vpc_id = aws_vpc.vpc.id
 
  ingress {
    from_port   = 5432
    to_port     = 5432
    protocol    = "TCP"
    cidr_blocks = aws_subnet.public.*.cidr_block
  }
 
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = aws_subnet.public.*.cidr_block
  }
}</pre>
<p>The database itself is created using the <strong class="source-inline">aws_db_instance</strong> Terraform resource, which requires quite a lot of configuration variables to be defined. What is given in the following <a id="_idIndexMarker411"/>code is a safe set of variables to run a database that counts in the AWS free tier. The following should be added to <em class="italic">infrastructure/aws.tf</em>:</p>
<pre class="source-code">
variable "db_password" {
  sensitive = true
} 
resource "aws_db_instance" "tozo" {
  apply_immediately       = true
  allocated_storage       = 20
  backup_retention_period = 5
  db_subnet_group_name    = aws_db_subnet_group.default.name
  deletion_protection     = true
  engine                  = "postgres"
  engine_version          = "14"
  instance_class          = "db.t3.micro"
  db_name                 = "tozo"
  username                = "tozo"
  password                = var.db_password
  vpc_security_group_ids  = [aws_security_group.database.id]
}</pre>
<p>The <strong class="source-inline">db_password</strong> should be added to <em class="italic">infrastructure/secrets.auto.tfvars</em> with a value ideally created by a password generator on a very strong setting (this password will never need to be memorized or typed).</p>
<p>As your app <a id="_idIndexMarker412"/>usage grows, I recommend that you change the value of <strong class="source-inline">instance_class</strong> to a larger machine, enable <strong class="source-inline">multi_az</strong> to ensure robustness in the case of an availability zone failure, and enable <strong class="source-inline">storage_encrypted</strong>.</p>
<p class="callout-heading">AWS web interface</p>
<p class="callout">In this book, we are intentionally defining all the infrastructure as code and ignoring the AWS web interface. This is best as it ensures that we can always restore the infrastructure to a known working state (by running <strong class="source-inline">terraform apply</strong>) and as it means we have an auditable history of changes. However, it is still very useful to use the web interface to inspect the infrastructure and check everything is as expected.</p>
<p>After running <strong class="source-inline">terraform apply</strong>, you should see a database running in RDS, which means we can create a cluster to run the app in.</p>
<h2 id="_idParaDest-157"><a id="_idTextAnchor170"/>Running the cluster</h2>
<p>We will use <a id="_idIndexMarker413"/>an ECS cluster to run our Docker images in, and furthermore, we will run the ECS cluster with Fargate as this means we won’t have to manage the servers or the cluster itself. While Fargate is not part of the AWS free tier and will sadly cost more, it is worth it to avoid having to manage things ourselves. </p>
<p>Before we can set ECS up though, we first need a repository to place the Docker images in and where <a id="_idIndexMarker414"/>ECS will pull and run the images from. We can use <a id="_idIndexMarker415"/>the <strong class="bold">elastic container register</strong> (<strong class="bold">ECR</strong>) for this by adding the following to <em class="italic">infrastructure/aws_cluster.tf</em>:</p>
<pre class="source-code">
resource "aws_ecr_repository" "tozo" {
  name = "tozo"
}
 
resource "aws_ecr_lifecycle_policy" "tozo" {
  repository = aws_ecr_repository.tozo.name
 
  policy = jsonencode({
    rules = [
      {
        rulePriority = 1
        description  = "Keep prod and latest tagged images"
        selection = {
          tagStatus     = "tagged"
          tagPrefixList = ["prod", "latest"]
          countType     = "imageCountMoreThan"
          countNumber   = 9999
        }
        action = {
          type = "expire"
        }
      },
      {
        rulePriority = 2
        description  = "Expire images older than 7 days"
        selection = {
          tagStatus   = "any"
          countType   = "sinceImagePushed"
          countUnit   = "days"
          countNumber = 7
        }
        action = {
          type = "expire"
        }
      }
    ]
  })
}</pre>
<p>Alongside creating the repository itself, this ensures that old images are deleted, which is crucial <a id="_idIndexMarker416"/>to reducing storage costs over time. Images tagged with <strong class="source-inline">prod</strong> are kept, as these are applied to the image that should be running (<strong class="source-inline">latest</strong> is added by Docker to the most recently built image).</p>
<p class="callout-heading">Docker image tagging</p>
<p class="callout">When a <a id="_idIndexMarker417"/>Docker image is built, it can be given tags to identify it. By default, it will be tagged as <strong class="source-inline">latest</strong> until a newer image is built and takes the tag. It is therefore best to tag images in a useful way to know what they represent.</p>
<p>We can now create the ECS cluster, which requires a task definition and then a service to run the task in the cluster. Starting with the task, we need an IAM role to execute, which we’ll call <strong class="source-inline">ecs_task_execution</strong>, and an IAM role for the task to exist, which we’ll <a id="_idIndexMarker418"/>call <strong class="source-inline">ecs_task</strong>. These are created by adding the following to <em class="italic">infrastructure/aws_cluster.tf</em>:</p>
<pre class="source-code">
resource "aws_iam_role" "ecs_task_execution" {
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Principal = {
          Service = "ecs-tasks.amazonaws.com"
        }
        Effect = "Allow"
        Sid    = ""
      }
    ]
  })
}
resource "aws_iam_role_policy_attachment" "ecs-task" { 
  role       = aws_iam_role.ecs_task_execution.name 
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy" 
}
resource "aws_iam_role" "ecs_task" {
  assume_role_policy = jsonencode({ 
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Principal = {
          Service = "ecs-tasks.amazonaws.com"
        }
        Effect = "Allow"
        Sid    = ""
      }
    ]
  })
} </pre>
<p>The policy attachment is used to attach an existing execution policy to the IAM role. </p>
<p>With the <a id="_idIndexMarker419"/>roles created, we can now define the ECS task itself. This needs to include all of the environment variables required for the code to run correctly in production. Therefore, an <strong class="source-inline">app_secret_key</strong> variable should be created in the same way as for <strong class="source-inline">db_password</strong> and added to the <em class="italic">infrastructure/secrets.auto.tfvars</em> file first. Then, the following can be added to <em class="italic">infrastructure/aws_cluster.tf</em>:</p>
<pre class="source-code">
variable "app_secret_key" { 
  sensitive = true 
} 
resource "aws_ecs_task_definition" "tozo" {
  family                   = "app"
  network_mode             = "awsvpc"
  requires_compatibilities = ["FARGATE"]
  cpu                      = 256
  memory                   = 512
  execution_role_arn       = aws_iam_role.ecs_task_execution.arn
  task_role_arn            = aws_iam_role.ecs_task.arn
  container_definitions = jsonencode([{
    name      = "tozo"
    image     = "${aws_ecr_repository.tozo.repository_      url}:latest"
    essential = true
    environment = [
      {
        name  = "TOZO_BASE_URL"
        value = "https://tozo.dev"
      },
      {
        name  = "TOZO_SECRET_KEY"
        value = var.app_secret_key
      },
      {
        name  = "TOZO_QUART_DB_DATABASE_URL"
        value = "postgresql://tozo:${var.db_password}@${aws_db_          instance.tozo.endpoint}/tozo"
      },
      {
        name  = "TOZO_QUART_AUTH_COOKIE_SECURE"
        value = "true"
      },
      {
        name  = "TOZO_QUART_AUTH_COOKIE_SAMESITE"
        value = "Strict"
      }
    ]
    portMappings = [{
      protocol      = "tcp"
      containerPort = 8080
      hostPort      = 8080
    }]
  }])
}</pre>
<p>Like with the database, as you gain customers and the app scales up, the <strong class="source-inline">cpu</strong> and <strong class="source-inline">memory</strong> values can be increased to meet the demand.</p>
<p>We have <a id="_idIndexMarker420"/>now created the task the service will run; however, before we can create the service, we need to allow connections between the load balancer and the running containers (which are exposing port <strong class="source-inline">8080</strong>), by adding the following to <em class="italic">infrastructure/aws_network.tf</em>:</p>
<pre class="source-code">
resource "aws_security_group" "ecs_task" {
  vpc_id = aws_vpc.vpc.id
 
  ingress {
    protocol        = "tcp"
    from_port       = 8080
    to_port         = 8080
    security_groups = [aws_security_group.lb.id]
  }
 
  egress {
    protocol    = "-1"
    from_port   = 0
    to_port     = 0
    cidr_blocks = ["0.0.0.0/0"]
  }
}</pre>
<p>This finally <a id="_idIndexMarker421"/>allows the service and cluster to be defined by using the following code in <em class="italic">infrastructure/aws_cluster.tf</em>:</p>
<pre class="source-code">
resource "aws_ecs_cluster" "production" {
  name = "production"
}
resource "aws_ecs_service" "tozo" {
  name            = "tozo"
  cluster         = aws_ecs_cluster.production.id
  task_definition = aws_ecs_task_definition.tozo.arn
  desired_count   = 1
  launch_type     = "FARGATE"
 
  network_configuration {
    security_groups  = [aws_security_group.ecs_task.id]
    subnets          = aws_subnet.public.*.id
    assign_public_ip = true
  } 
  load_balancer {
    target_group_arn = aws_lb_target_group.tozo.arn
    container_name   = "tozo"
    container_port   = 8080
  } 
  lifecycle {
    ignore_changes = [task_definition, desired_count]
  }
}</pre>
<p>The <strong class="source-inline">desired_count</strong> refers to the number of running containers and should be increased <a id="_idIndexMarker422"/>as your app handles more requests; a minimum of three should mean that there are containers running in different availability zones and hence is more robust.</p>
<p class="callout-heading">Autoscaling</p>
<p class="callout">As the traffic <a id="_idIndexMarker423"/>to your app grows, you can scale the infrastructure by allocating larger machines and by increasing <strong class="source-inline">desired_count</strong>. You should be able to scale to very heavy traffic this way (and many congratulations to you when you do). However, if your traffic is periodic (for example, you have more traffic during the day than the night), then using autoscaling can save costs. Autoscaling is where more resources are allocated automatically as the traffic increases.</p>
<p>We now have the cluster ready to go; all we need now is for the Docker images to be built and placed into the repository.</p>
<h2 id="_idParaDest-158"><a id="_idTextAnchor171"/>Adding continuous deployment</h2>
<p>With everything ready to run, we can now deploy changes by building the container image, uploading it to the ECR registry, and informing ECS to deploy the new image. This is something <a id="_idIndexMarker424"/>that is best done whenever a change is made to the main branch of the GitHub repository. We can do this using a GitHub action, much like in the <em class="italic">Adopting a collaborative development process using GitHub</em> section in <a href="B18727_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a><em class="italic">, Setting Up Our System for Development</em>.</p>
<p>To start, we need to create an IAM user that has permission to push Docker images to the ECR registry and to inform ECS to deploy a new image. This user will also need an access key, as we’ll use this to authenticate the <strong class="source-inline">push</strong> and <strong class="source-inline">deploy</strong> commands. The following code creates this user and should be placed in <em class="italic">infrastructure/aws.tf</em>:</p>
<pre class="source-code">
resource "aws_iam_user" "cd_bot" {
  name = "cd-bot"
  path = "/"
}
 
resource "aws_iam_user_policy" "cd_bot" {
  name = "cd-bot-policy"
  user = aws_iam_user.cd_bot.name
 
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action   = "ecr:*"
        Effect   = "Allow"
        Resource = aws_ecr_repository.tozo.arn
      },
      {
        Action   = "ecr:GetAuthorizationToken"
        Effect   = "Allow"
        Resource = "*"
      },
      {
        Action   = "ecs:UpdateService"
        Effect   = "Allow"
        Resource = aws_ecs_service.tozo.id
      }
    ]
  })
}
 
resource "aws_iam_access_key" "cd_bot" {
  user = aws_iam_user.cd_bot.name
}</pre>
<p>As the <a id="_idIndexMarker425"/>continuous deployment will run as a GitHub action, we need to make this access key and the repository URL available as a <strong class="source-inline">github_actions_secret</strong>; this is done by adding the following to <em class="italic">infrastructure/github.tf</em>:</p>
<pre class="source-code">
resource "github_actions_secret" "debt_aws_access_key" {
  repository      = github_repository.tozo.name
  secret_name     = "AWS_ACCESS_KEY_ID"
  plaintext_value = aws_iam_access_key.cd_bot.id
}
 
resource "github_actions_secret" "debt_aws_secret_key" {
  repository      = github_repository.tozo.name
  secret_name     = "AWS_SECRET_ACCESS_KEY"
  plaintext_value = aws_iam_access_key.cd_bot.secret
}
 
resource "github_actions_secret" "debt_aws_repository_url" {
  repository      = github_repository.tozo.name
  secret_name     = "AWS_REPOSITORY_URL"
  plaintext_value = aws_ecr_repository.tozo.repository_url
}</pre>
<p>These <a id="_idIndexMarker426"/>secrets can now be used in the continuous deployment action. This action consists of two jobs: </p>
<ul>
<li>The first job builds the Docker image and pushes it to the ECR registry</li>
<li>The second instructs ECS to deploy it (by replacing the currently running image)</li>
</ul>
<p>Starting with the first job, the following should be added to <em class="italic">.github/workflows/cd.yml</em>:</p>
<pre class="source-code">
name: CD
 
on:
  push:
    branches: [ main ]
  workflow_dispatch:
 
jobs:
  push:
    runs-on: ubuntu-latest
    env:
      AWS_REPOSITORY_URL: ${{ secrets.AWS_REPOSITORY_URL }}
 
    steps:
      - uses: actions/checkout@v3
 
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{secrets.AWS_SECRET_ACCESS_            KEY}}
          aws-region: eu-west-2
 
      - name: Login to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v1
 
      - name: Fetch a cached image
        continue-on-error: true
        run: docker pull $AWS_REPOSITORY_URL:latest
      - name: Build the image
        run: | 
          docker build \
            --cache-from $AWS_REPOSITORY_URL:latest \ 
            -t $AWS_REPOSITORY_URL:latest \
            -t $AWS_REPOSITORY_URL:$GITHUB_SHA .
      - name: Push the images
        run: docker push --all-tags $AWS_REPOSITORY_URL</pre>
<p>To save on build time, the last built image, tagged as <strong class="source-inline">latest</strong>, is pulled and used as a cache. The built image is then identified by being tagged with the commit hash.</p>
<p>We can <a id="_idIndexMarker427"/>now add a <strong class="source-inline">deploy</strong> job that should instruct ECS to deploy the image built for this commit. This is done by adding a <strong class="source-inline">prod</strong> tag to the image already tagged with the commit hash and then informing ECS to run it. This is done by adding the following to <em class="italic">.github/workflows/cd.yml</em>:</p>
<pre class="source-code">
  deploy:
    needs: push
    runs-on: ubuntu-latest
    env:
      AWS_REPOSITORY_URL: ${{ secrets.AWS_REPOSITORY_URL }} 
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{secrets.AWS_SECRET_ACCESS_KEY}}
          aws-region: eu-west-2
 
      - name: Inform ECS to deploy a new image
        run: |
          MANIFEST=$(aws ecr batch-get-image --region eu-west-2 --repository-name tozo --image-ids imageTag=$GITHUB_SHA --query 'images[].imageManifest' --output text)
          aws ecr put-image --region eu-west-2 --repository-name tozo --image-tag prod --image-manifest "$MANIFEST" || true
          aws ecs update-service --cluster production --service tozo --region eu-west-2 --force-new-deployment</pre>
<p>This job is <a id="_idIndexMarker428"/>idempotent and rerunning it will deploy the specific commit it is associated with. This <a id="_idIndexMarker429"/>means it can be rerun to <strong class="bold">roll back</strong> a deployment as needed.</p>
<p class="callout-heading">Deployment issues and rollbacks</p>
<p class="callout">Not every deployment will go well, and the failure could be during the deployment or after deployment. If the deployment itself fails, ECS will automatically keep the previous deployment running. If the failure is after deployment, you can roll back to a safe previous version by rerunning an old <strong class="source-inline">deploy</strong> job.</p>
<p>Now, on every change to the main branch, you should see that change automatically goes live in the production environment. In addition, you can rerun an old <strong class="source-inline">deploy</strong> job if there is a bug or issue with the running job. This is a very productive way of developing an app.</p>
<p>While we can visit the app via the ALB URL, our users will expect to use a nice domain name, which is what we’ll focus on next.</p>
<h1 id="_idParaDest-159"><a id="_idTextAnchor172"/>Serving on a domain</h1>
<p>We’ll want a memorable domain name for users to find and identify our app, which means we’ll need <a id="_idIndexMarker430"/>to buy one from a domain name registrar. I like to use Gandi (<a href="http://gandi.net">gandi.net</a>) or AWS <a id="_idIndexMarker431"/>as they are trustworthy, however, I like to separate the domain name from the hosting provider in case something goes wrong; for that reason, I’ll be using Gandi in this book and have used it to register <a href="http://tozo.dev">tozo.dev</a> for the next few years, as shown in <em class="italic">Figure 6.4</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer038">
<img alt="Figure 6.4: The Gandi home page for registering a domain " height="672" src="image/B18727_06_04.jpg" width="1110"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4: The Gandi home page for registering a domain</p>
<p>The domain name registrar will allow for the relevant DNS records for a domain name to be specified; to do so with Gandi, we need to add the <strong class="source-inline">gandi</strong> provider to <strong class="source-inline">terraform</strong> by adding the following highlighted code to the existing <strong class="source-inline">terraform</strong> section in <em class="italic">infrastructure/main.tf</em>:</p>
<pre class="source-code">
terraform {
  required_providers {
<strong class="bold">    gandi = {</strong>
<strong class="bold">      source = "go-gandi/gandi"</strong>
<strong class="bold">      version = "~&gt; 2.0.0"</strong>
<strong class="bold">    }</strong>
  }
}</pre>
<p class="callout-heading">DNS</p>
<p class="callout">While the domain name is memorable for humans, the browser will need a corresponding IP address in order to make the request. This is the purpose of DNS, which will resolve a domain name into the correct IP address. This is done automatically by the browser, but if you’d like to try it manually, you can use the <strong class="source-inline">dig</strong> tool (e.g., <strong class="source-inline">dig tozo.dev</strong>).A single domain will have multiple DNS records. So far, we’ve discussed the <strong class="source-inline">A</strong> record, which contains the IPv4 address for the domain. There is also an <strong class="source-inline">AAA</strong> record for an IPv6 address, an <strong class="source-inline">ALIAS</strong> record that points to another domain’s <strong class="source-inline">A</strong> or <strong class="source-inline">AAA</strong> record, an <strong class="source-inline">MX</strong> record for mail server information (which we’ll use in the <em class="italic">Sending production emails</em> section of this chapter), a <strong class="source-inline">CNAME</strong> record to alias a subdomain to another domain name, and various others.</p>
<p>Once <a id="_idIndexMarker432"/>initialized via <strong class="source-inline">terraform init</strong>, we can start to use <strong class="source-inline">terraform apply</strong> to make these changes. First, we need to retrieve a production API key from Gandi, which is found in the <strong class="bold">Security</strong> section as shown in <em class="italic">Figure 6.5</em>: </p>
<div>
<div class="IMG---Figure" id="_idContainer039">
<img alt="Figure 6.5: The Gandi Security section; note the Production API key section " height="1240" src="image/B18727_06_05.jpg" width="1300"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5: The Gandi Security section; note the Production API key section</p>
<p>The API key <a id="_idIndexMarker433"/>needs to be added as follows to <em class="italic">infrastructure/secrets.auto.tfvars</em> (your key will differ from my <strong class="source-inline">abcd</strong> example):</p>
<p class="source-code">gandi_api_key = "abcd"</p>
<p>Then, the key is used to configure the <strong class="source-inline">gandi</strong> provider by adding the following to <em class="italic">infrastructure/dns.tf</em>:</p>
<pre class="source-code">
variable "gandi_api_key" {
  sensitive = true
}
 
provider "gandi" {
  key = var.gandi_api_key
}</pre>
<p>The <strong class="source-inline">gandi</strong> provider is now <a id="_idIndexMarker434"/>set up and can be used to set DNS records. We need two records: an <strong class="source-inline">ALIAS</strong> record for the domain, and a <strong class="source-inline">CNAME</strong> record for the <a href="http://www.tozo.dev">www.tozo.dev</a> subdomain. The following should be added to <em class="italic">infrastructure/dns.tf</em>:</p>
<pre class="source-code">
data "gandi_domain" "tozo_dev" {
  name = "tozo.dev"
}
 
resource "gandi_livedns_record" "tozo_dev_ALIAS" {
  zone   = data.gandi_domain.tozo_dev.id
  name   = "@"
  type   = "ALIAS"
  ttl    = 3600
  values = ["${aws_lb.tozo.dns_name}."]
}
resource "gandi_livedns_record" "tozo_dev_www" {
  zone   = data.gandi_domain.tozo_dev.id
  name   = "www"
  type   = "CNAME"
  ttl    = 3600
  values = ["tozo.dev."]
}</pre>
<p>With the DNS records in place, we can now focus on adding HTTPS (SSL).</p>
<h2 id="_idParaDest-160"><a id="_idTextAnchor173"/>Securing the connection</h2>
<p>It is best practice to ensure that communication between the user and the app is encrypted; however, this becomes essential when the communication consists of sensitive <a id="_idIndexMarker435"/>information, such as the user’s password. As such, we’ll only use encrypted communication for our app.</p>
<p>To secure this connection, we can utilize HTTPS using SSL (or TLS), which is widely supported and easy to use. To do so, we need to be issued an encryption certificate that browsers will recognize. Fortunately, Let’s Encrypt will issue us a certificate for free. Let’s Encrypt is usable with Terraform via the <strong class="source-inline">acme</strong> provider, which is activated by adding the following highlighted code to the existing <strong class="source-inline">terraform</strong> section in <em class="italic">infrastructure/main.tf</em> and then running <strong class="source-inline">terraform init</strong>:</p>
<pre class="source-code">
terraform {
  required_providers {
<strong class="bold">    acme = {</strong>
<strong class="bold">      source  = "vancluever/acme"</strong>
<strong class="bold">      version = "~&gt; 2.0"</strong>
<strong class="bold">    }</strong>
  }
}</pre>
<p class="callout-heading">Certificate authorities</p>
<p class="callout">To enable HTTPS, we could create our own self-signed certificate; this would work, but browsers will display a warning. This warning will state that the browser does not trust that the given certificate belongs to the domain. To avoid this warning, we need a recognized certificate authority to sign our certificate. To do so, the certificate authority must confirm that the owner of the domain is the one asking for the certificate. There are many other certificate authorities that charge for this service, but Let’s Encrypt does it for free!</p>
<p>To acquire a <a id="_idIndexMarker436"/>certificate for a domain name, we’ll need to prove to Let’s Encrypt that we control the domain name. We can do this via the <strong class="source-inline">acme</strong> provider by adding the following to <em class="italic">infrastructure/certs.tf</em>:</p>
<pre class="source-code">
provider "acme" {
  server_url = "https://acme-v02.api.letsencrypt.org/directory"
}
 
resource "tls_private_key" "private_key" {
  algorithm = "RSA"
}
 
resource "acme_registration" "me" {
  account_key_pem = tls_private_key.private_key.private_key_pem
  email_address   = "pgjones@tozo.dev"
}
 
resource "acme_certificate" "tozo_dev" {
  account_key_pem = acme_registration.me.account_key_pem
  common_name     = "tozo.dev"
 
  dns_challenge {
    provider = "gandiv5"
 
    config = {
      GANDIV5_API_KEY = var.gandi_api_key
    }
  }
}
resource "aws_acm_certificate" "tozo_dev" {
  private_key       = acme_certificate.tozo_dev.private_key_pem
  certificate_body  = acme_certificate.tozo_dev.certificate_pem
  certificate_chain = "${acme_certificate.tozo_dev.certificate_pem}${acme_certificate.tozo_dev.issuer_pem}"
 
  lifecycle {
    create_before_destroy = true
  }
}</pre>
<p>Remember to <a id="_idIndexMarker437"/>change the email address, so that reminders and updates from Let’s Encrypt go to you rather than to my email!</p>
<p>The certificates we’ve just created can now be added to the ALB, as doing so will enable users to connect to the ALB, and hence our app, via HTTPS. To ensure only HTTPS is used, let’s redirect any visitors that connect via HTTP (port <strong class="source-inline">80</strong>) to do so via HTTPS (port <strong class="source-inline">443</strong>) by adding the following to <em class="italic">infrastructure/aws_network.tf</em>:</p>
<pre class="source-code">
resource "aws_lb_listener" "http" {
  load_balancer_arn = aws_lb.tozo.arn
  port              = "80"
  protocol          = "HTTP"
 
  default_action {
    type = "redirect"
 
    redirect {
      port        = "443"
      protocol    = "HTTPS"
      status_code = "HTTP_301"
    }
  }
}</pre>
<p>We can <a id="_idIndexMarker438"/>then accept HTTPS connections and forward them to the target group containing our running app by adding the following code to <em class="italic">infrastruture/aws_network.tf</em>:</p>
<pre class="source-code">
resource "aws_lb_listener" "https" {
  load_balancer_arn = aws_lb.tozo.arn
  port              = 443
  protocol          = "HTTPS"
  ssl_policy        = "ELBSecurityPolicy-2016-08"
  certificate_arn   = aws_acm_certificate.tozo_dev.arn
 
  default_action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.tozo.arn
  }
}</pre>
<p>With these changes, you can run the following:</p>
<pre class="source-code">
terraform init
terraform apply</pre>
<p>This should <a id="_idIndexMarker439"/>create all the infrastructure. You will then need to push your local code to the GitHub repository for the CD job to run and deploy the app. Once that completes, you should be able to visit <a href="http://tozo.dev">tozo.dev</a> (or whatever your domain is) and see the running app. We can now focus on how we can send emails, such as a welcome email, to the app’s users.</p>
<h1 id="_idParaDest-161"><a id="_idTextAnchor174"/>Sending production emails</h1>
<p>In the <em class="italic">Sending emails</em> section of <a href="B18727_02.xhtml#_idTextAnchor053"><em class="italic">Chapter 2</em></a><em class="italic">, Creating a Reusable Backend with Quart,</em> we configured our app to send emails via Postmark if a <strong class="source-inline">POSTMARK_TOKEN</strong> configuration value was present. We can now set up production so that there is a <strong class="source-inline">POSTMARK_TOKEN</strong> in the app’s configuration.</p>
<p>To do so, we first <a id="_idIndexMarker440"/>need approval from Postmark; this is done to ensure that we don’t intend to misuse their service. As we are using Postmark for transactional emails (e.g., password reset tokens), we should get permission. This is gained via the request approval button or by talking directly to their support.</p>
<p>With permission granted, we can add the relevant DNS records to prove to Postmark that we control the <a href="http://tozo.dev">tozo.dev</a> domain. These are available from your Postmark account and should be added as follows to <em class="italic">infrastructure/dns.tf</em>:</p>
<pre class="source-code">
resource "gandi_livedns_record" "tozo_dev_DKIM" {
  zone   = data.gandi_domain.tozo_dev.id
  name   = "20210807103031pm._domainkey"
  type   = "TXT"
  ttl    = 10800
  values = ["k=rsa;p=<strong class="bold">abcd</strong>"]
}
 
resource "gandi_livedns_record" "tozo_dev_CNAME" {
  zone   = data.gandi_domain.tozo_dev.id
  name   = "pm-bounces"
  type   = "CNAME"
  ttl    = 10800
  values = ["pm.mtasv.net."]
}</pre>
<p>Note the <a id="_idIndexMarker441"/>highlighted <strong class="source-inline">abcd</strong> DKIM value is a placeholder and should be replaced with your own value.</p>
<p>The Postmark token we need is also available in your account and should be added to <em class="italic">infrastructure/secrets.auto.tfvars</em> (your key will differ from my <strong class="source-inline">abcd</strong> example):</p>
<p class="source-code">postmark_token = "abcd"</p>
<p>To make this token available to our app, we need it to be an environment variable in the running container. This is achieved by adding the following to the existing <strong class="source-inline">aws_ecs_task_definition</strong> section in <em class="italic">infrastructure/aws_cluster.tf</em>:</p>
<pre class="source-code">
<strong class="bold">variable "postmark_token" {</strong>
<strong class="bold">  sensitive = true</strong>
<strong class="bold">}</strong>
resource "aws_ecs_task_definition" "tozo" {
  container_definitions = jsonencode([{
    environment = [
<strong class="bold">      {</strong>
<strong class="bold">        name  = "TOZO_POSTMARK_TOKEN"</strong>
<strong class="bold">        value = var.postmark_token</strong>
<strong class="bold">      }</strong>
    ]
  }])
}</pre>
<p>The highlighted lines should be added to the file. Note that the environment variable name is <strong class="source-inline">TOZO_POSTMARK_TOKEN</strong> as only environment variables prefixed with <strong class="source-inline">TOZO_</strong> are loaded into the app’s configuration. See the <em class="italic">Creating a basic Quart app</em> section in <a href="B18727_02.xhtml#_idTextAnchor053"><em class="italic">Chapter 2</em></a><em class="italic">, Creating a Reusable Backend with Quart</em>.</p>
<p>Our app should <a id="_idIndexMarker442"/>now send the welcome, reset password, and other emails using Postmark. We can monitor this by logging into Postmark and checking the activity. Next, we can focus on monitoring the app itself.</p>
<h1 id="_idParaDest-162"><a id="_idTextAnchor175"/>Monitoring production</h1>
<p>Now that our app is running in production, we need to keep it working. This means we need to <a id="_idIndexMarker443"/>monitor for issues, notably errors and slow performance, as both lead to a poor user experience. To do so, I find it easiest to use Sentry (<a href="http://sentry.io">sentry.io</a>), which can monitor errors and performance in the frontend and backend code.</p>
<h2 id="_idParaDest-163"><a id="_idTextAnchor176"/>Monitoring the backend</h2>
<p>To monitor <a id="_idIndexMarker444"/>the backend, we should create a new project in Sentry and call it <strong class="source-inline">backend</strong>. This is where we’ll see any errors and can monitor the <a id="_idIndexMarker445"/>performance. The project will have its own <strong class="bold">data source name</strong> (<strong class="bold">DSN</strong>) value, which we’ll need to provide to the app in production. The DSN is found on the project’s configuration page on <a href="http://sentry.io">sentry.io</a>.</p>
<p>To make the DSN available to our app, we need it to be an environment variable in the running container. This is achieved by adding the following to the existing <strong class="source-inline">aws_ecs_task_definition</strong> section in <em class="italic">infrastructure/aws_cluster.tf</em>:</p>
<pre class="source-code">
resource "aws_ecs_task_definition" "tozo" {
  container_definitions = jsonencode([{
    environment = [
      {
        name  = "SENTRY_DSN"
        value = "<strong class="bold">https://examplePublicKey@o0.ingest.sentry.io/0</strong>"
      }
    ]
  }])
}</pre>
<p>The highlighted <a id="_idIndexMarker446"/>value will be different for your setup, as the value used here is Sentry’s example DSN.</p>
<p>We next need to install <strong class="source-inline">sentry-sdk</strong> by running the following in the <em class="italic">backend</em> folder:</p>
<p class="source-code">pdm add sentry-sdk</p>
<p>This allows us to activate the Sentry monitoring for Quart using Sentry’s <strong class="source-inline">QuartIntegration</strong>; we can do this by adding the following to <em class="italic">backend/src/backend/run.py</em>:</p>
<pre class="source-code">
import sentry_sdk
from sentry_sdk.integrations.quart import QuartIntegration
if "SENTRY_DSN" in os.environ:
    sentry_sdk.init(
        dsn=os.environ["SENTRY_DSN"],
        integrations=[QuartIntegration()],
        traces_sample_rate=0.2,
    )
<strong class="bold">app = Quart(__name__)</strong></pre>
<p>It is important that <strong class="source-inline">sentry_sdk.init</strong> is before <strong class="source-inline">app = Quart(__name__)</strong>, as highlighted <a id="_idIndexMarker447"/>in the previous code.</p>
<p class="callout-heading">Expected performance</p>
<p class="callout">As a rule of thumb, if an action takes more than 100 milliseconds to return a response, the user will notice the slowdown and have a bad experience. Therefore, I aim to have routes completed within 40 milliseconds, as this gives time for the network transmission and any UI updates to take place within the 100 millisecond target. There is an exception though, which is that any route that hashes the password should take in excess of 100 milliseconds – otherwise, the hash is too weak and liable to be broken.</p>
<p>This is all we need to monitor the backend, so now we can do the same for the frontend. </p>
<h2 id="_idParaDest-164"><a id="_idTextAnchor177"/>Monitoring the frontend</h2>
<p>To monitor <a id="_idIndexMarker448"/>the frontend, we first need to create a <strong class="source-inline">frontend</strong> project in Sentry. Next, we need to install the Sentry SDK by running the following in the <em class="italic">frontend</em> folder:</p>
<p class="source-code">npm install @sentry/react @sentry/tracing</p>
<p>This allows us to activate the Sentry monitoring using Sentry’s browser integration by adding the following to <em class="italic">frontend/src/index.tsx</em>: </p>
<pre class="source-code">
import * as Sentry from "@sentry/react";
import { BrowserTracing } from "@sentry/tracing";
if (process.env.NODE_ENV === "production") {
  Sentry.init({
    dsn: "<strong class="bold">https://examplePublicKey@o0.ingest.sentry.io/0</strong>",
    integrations: [new BrowserTracing()],
    tracesSampleRate: 0.2,
  });
}</pre>
<p>The highlighted DSN value provided is an example, and yours is available in the project settings on <a href="http://sentry.io">sentry.io</a>. As this value isn’t sensitive, it is safe for us to place it directly in the frontend code. </p>
<p>To work <a id="_idIndexMarker449"/>correctly, it is important that <strong class="source-inline">Sentry.init</strong> is before the following:</p>
<pre class="source-code">
const root = ReactDOM.createRoot(
  document.getElementById("root") as HTMLElement,
);</pre>
<p>And that is all we need to monitor the frontend. Next, we can show the user a friendly error page when an error occurs.</p>
<h2 id="_idParaDest-165"><a id="_idTextAnchor178"/>Displaying an error page</h2>
<p>It is likely, despite our best efforts, that users will encounter bugs and errors as they use the app. When <a id="_idIndexMarker450"/>this happens, we should show the user a helpful error page that acknowledges the issue and encourages the user to try again, as shown in <em class="italic">Figure 6.6</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer040">
<img alt="Figure 6.6: The Error page " height="188" src="image/B18727_06_06.jpg" width="893"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6: The Error page</p>
<p>This page is implemented by adding the following code to <em class="italic">frontend/src/pages/Error.tsx</em>:</p>
<pre class="source-code">
import Alert from "@mui/material/Alert";
import AlertTitle from "@mui/material/AlertTitle";
import Container from "@mui/material/Container";
import Link from "@mui/material/Link";
 
const Error = () =&gt; (
  &lt;Container maxWidth="sm"&gt;
    &lt;Alert severity="error" sx={{ marginTop: 2 }}&gt;
      &lt;AlertTitle&gt;Error&lt;/AlertTitle&gt;
        Sorry, something has gone wrong. 
        Please try reloading the page or click{" "}      
        &lt;Link href="/"&gt;here&lt;/Link&gt;.
    &lt;/Alert&gt;
  &lt;/Container&gt;
);
export default Error;</pre>
<p class="callout-heading">Error tolerance</p>
<p class="callout">In my experience, users are very tolerant of bugs that are acknowledged and fixed quickly, with the inconvenience being quickly forgotten. However, bugs that are not acknowledged or affect the user multiple times are not forgiven and result in the user using a different app. This is why it is vital to monitor the app for errors and fix them first, before adding any new features.</p>
<p>To display <a id="_idIndexMarker451"/>this error page when an error occurs, we can use Sentry’s <strong class="source-inline">ErrorBoundary</strong> by making the following changes to <em class="italic">frontend/src/index.tsx</em>:</p>
<pre class="source-code">
import Error from "src/pages/Error";
root.render(
  &lt;React.StrictMode&gt;
    &lt;Sentry.ErrorBoundary fallback={&lt;Error /&gt;}&gt;
      &lt;App /&gt;
    &lt;/Sentry.ErrorBoundary&gt;
  &lt;/React.StrictMode&gt;,
);</pre>
<p>To check <a id="_idIndexMarker452"/>that everything is set up and works correctly, we can create a route that errors when visited by adding the following to <em class="italic">frontend/src/Router.tsx</em>:</p>
<pre class="source-code">
const ThrowError = () =&gt; {throw new Error("Test Error")};
 
const Router = () =&gt; (
  &lt;BrowserRouter&gt;
<strong class="bold">    ...</strong>
    &lt;Routes&gt;
<strong class="bold">      ...</strong>
      &lt;Route
        element={&lt;ThrowError /&gt;}
        path="/test-error/"
      /&gt;
    &lt;/Routes&gt;
  &lt;/BrowserRouter&gt;
);</pre>
<p>In the code block, <strong class="source-inline">...</strong> represents code that has been omitted for brevity.</p>
<p>Now, any visit to <strong class="source-inline">/test-error/</strong> will result in an error and the error page being displayed.</p>
<p>With a friendly error page and Sentry installed, we are able to monitor for errors and performance issues.</p>
<h1 id="_idParaDest-166"><a id="_idTextAnchor179"/>Summary</h1>
<p>In this chapter, we’ve deployed our app to the cloud and served it on our own memorable domain name, thereby allowing any user to use our app. We also learned how to monitor it for any issues, and so are ready to fix bugs as quickly as possible.</p>
<p>The infrastructure we’ve built in this chapter can be used for any containerized app that needs a database and will scale to very high loads. </p>
<p>In the next chapter, we’ll add some advanced features to our app and turn it into a progressive web app.</p>
</div>
</div></body></html>