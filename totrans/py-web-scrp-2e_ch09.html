<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Putting It All Together</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Putting It All Together</h1>
            </header>

            <article>
                
<p>This book has so far introduced scraping techniques using a custom website, which helped us focus on learning particular skills. In this chapter, we will analyze a variety of real-world websites to show how the techniques we've learned in the book can be applied. First, we'll use Google to show a real-world search form, then Facebook for a JavaScript-dependent website and API, Gap for a typical online store, and finally, BMW for a map interface. Since these are live websites, there is a risk they will change by the time you read this. However, this is fine because the purpose of this chapter's examples is to show you how the techniques learned so far can be applied, rather than to show you how to scrape any particular website. If you choose to run an example, first check whether the website structure has changed since these examples were made and whether their current terms and conditions prohibit scraping.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Scraping a Google search result web page</li>
<li>Investigating the Facebook API</li>
<li>Using multiple threads with the Gap website</li>
<li>Reverse engineering the BMW dealer locator page</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Google search engine</h1>
            </header>

            <article>
                
<p>To investigate using our knowledge of CSS selectors, we will scrape Google search results. According to the Alexa data used in <a href="py-web-scrp-2e_ch04.html" target="_blank"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Concurrent Downloading</em>, google.com is the world's most popular website, and conveniently, its structure is simple and straightforward to scrape.</p>
<div class="packt_infobox"><br/>
<span class="packt_screen">International Google</span> may redirect to a country-specific version, depending on your location. In these examples, Google is set to the Romanian&#160;version, so your results may look slightly different.</div>
<p>Here is the Google search homepage loaded with browser tools&#160;to inspect the form:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="images/google_search-1.png"/></div>
<p>We can see here that the search query is stored in an input with name <kbd>q</kbd>, and then the form is submitted to the path <kbd>/search</kbd> set by the <kbd>action</kbd> attribute. We can test this by doing a test search to submit the form, which would then be redirected to a URL, such as&#160;<a href="https://www.google.ro/?gws_rd=cr,ssl&amp;ei=TuXYWJXqBsGsswHO8YiQAQ#q=test&amp;*" target="_blank">https://www.google.ro/?gws_rd=cr,ssl&amp;ei=TuXYWJXqBsGsswHO8YiQAQ#q=test&amp;*</a>. The exact URL will depend on your browser and location. Also if you have Google Instant enabled, AJAX will be used to load the search results dynamically rather than submitting the form. This URL has many parameters, but the only one required is <kbd>q</kbd> for the query.</p>
<p>The URL <a href="https://www.google.com/search?q=test"><span class="URLPACKT">https://www.google.com/search?q=test</span></a>&#160;shows we can use this URL to produce a search result, as shown in this screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="images/image_09_002.jpg"/></div>
<p>The structure of the search results can be examined with your browser tools, as shown here:</p>
<div class="packt_figure"><img class="image-border" src="images/google_results.png"/></div>
<p>Here, we see that the search results are structured as links whose parent element is a <kbd>&lt;h3&gt;</kbd> tag with class "<kbd>r</kbd>".</p>
<p>To scrape the search results, we will use a CSS selector, which was introduced in <a href="py-web-scrp-2e_ch02.html" target="_blank"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Scraping the Data</em>:</p>
<pre><strong>&gt;&gt;&gt; from lxml.html import fromstring</strong><br/><strong>&gt;&gt;&gt; import requests </strong><br/><strong>&gt;&gt;&gt; html = requests.get('https://www.google.com/search?q=test') </strong><br/><strong>&gt;&gt;&gt; tree = fromstring(html.content) </strong><br/><strong>&gt;&gt;&gt; results = tree.cssselect('h3.r a') </strong><br/><strong>&gt;&gt;&gt; results </strong><br/><strong>[&lt;Element a at 0x7f3d9affeaf8&gt;, </strong><br/><strong> &lt;Element a at 0x7f3d9affe890&gt;, </strong><br/><strong> &lt;Element a at 0x7f3d9affe8e8&gt;, </strong><br/><strong> &lt;Element a at 0x7f3d9affeaa0&gt;, </strong><br/><strong> &lt;Element a at 0x7f3d9b1a9e68&gt;, </strong><br/><strong> &lt;Element a at 0x7f3d9b1a9c58&gt;, </strong><br/><strong> &lt;Element a at 0x7f3d9b1a9ec0&gt;, </strong><br/><strong> &lt;Element a at 0x7f3d9b1a9f18&gt;, </strong><br/><strong> &lt;Element a at 0x7f3d9b1a9f70&gt;, </strong><br/><strong> &lt;Element a at 0x7f3d9b1a9fc8&gt;] </strong>
</pre>
<p>So far, we downloaded the Google search results and used <kbd>lxml</kbd> to extract the links. In the preceding screenshot, the link includes a bunch of extra parameters alongside the actual website URL, which are used for tracking clicks.</p>
<p>Here is the first link we find on the page:</p>
<pre><strong>&gt;&gt;&gt; link = results[0].get('href') </strong><br/><strong>&gt;&gt;&gt; link </strong><br/><strong>'/url?q=http://www.speedtest.net/&amp;sa=U&amp;ved=0ahUKEwiCqMHNuvbSAhXD6gTMAA&amp;usg=AFQjCNGXsvN-v4izEgZFzfkIvg'</strong> 
</pre>
<p>The content we want here is <kbd>http://www.speedtest.net/</kbd>, which can be parsed from the query string using the <kbd>urlparse</kbd> module:</p>
<pre><strong>&gt;&gt;&gt; from urllib.parse import parse_qs, urlparse </strong><br/><strong>&gt;&gt;&gt; qs = urlparse(link).query </strong><br/><strong>&gt;&gt;&gt; parsed_qs = parse_qs(qs)</strong><br/><strong>&gt;&gt;&gt; parsed_qs</strong><br/><strong>{'q': ['http://www.speedtest.net/'], </strong><br/><strong> 'sa': ['U'], </strong><br/><strong> 'ved': ['0ahUKEwiCqMHNuvbSAhXD6gTMAA'], </strong><br/><strong> 'usg': ['AFQjCNGXsvN-v4izEgZFzfkIvg']}</strong><br/><strong>&gt;&gt;&gt; parsed_qs.get('q', []) </strong><br/><strong>['http://www.speedtest.net/']</strong>
</pre>
<p>This query string parsing can be applied to extract all links.</p>
<pre><strong>&gt;&gt;&gt; links = [] </strong><br/><strong>&gt;&gt;&gt; for result in results: </strong><br/><strong>...     link = result.get('href') </strong><br/><strong>...     qs = urlparse(link).query </strong><br/><strong>...     links.extend(parse_qs(qs).get('q', [])) </strong><br/><strong>... </strong><br/><strong>&gt;&gt;&gt; links </strong><br/><strong>['http://www.speedtest.net/', </strong><br/><strong>'test', </strong><br/><strong>'https://www.test.com/', </strong><br/><strong>'https://ro.wikipedia.org/wiki/Test', </strong><br/><strong>'https://en.wikipedia.org/wiki/Test', </strong><br/><strong>'https://www.sri.ro/verificati-va-aptitudinile-1', </strong><br/><strong>'https://www.sie.ro/AgentiaDeSpionaj/test-inteligenta.html', 'http://www.hindustantimes.com/cricket/india-vs-australia-live-cricket-score-4th-test-dharamsala-day-3/story-8K124GMEBoiKOgiAaaB5bN.html', </strong><br/><strong>'https://sports.ndtv.com/india-vs-australia-2017/live-cricket-score-india-vs-australia-4th-test-day-3-dharamsala-1673771', </strong><br/><strong>'http://pearsonpte.com/test-format/']</strong> 
</pre>
<p>Success! The links from the first page of this Google search have been successfully scraped. The full source for this example is available at <a href="https://github.com/kjam/wswp/blob/master/code/chp9/scrape_google.py">https://github.com/kjam/wswp/blob/master/code/chp9/scrape_google.py</a>.</p>
<p>One difficulty with Google is that a CAPTCHA image will be shown if your IP appears suspicious, for example, when downloading too fast:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="282" width="337" class="image-border" src="images/4364OS_09_04.png"/></div>
<p>This CAPTCHA image could be solved using the techniques covered in <span class="ChapterrefPACKT">Chapter 7</span>, <em>Solving CAPTCHA</em>, though it would be preferable to avoid suspicion and download slowly, or use proxies if a faster download rate is required. Overloading Google can get your IP or even set of IPs banned from Google domains for a series of hours or day; so ensure you are courteous to others' (and your own) use of the site so your home or office doesn't get blacklisted.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Facebook</h1>
            </header>

            <article>
                
<p>To demonstrate using a browser and API, we will investigate Facebook's site. Currently, Facebook is the world's largest social network in terms of monthly active users, and therefore, its user data is extremely valuable.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">The website</h1>
            </header>

            <article>
                
<p>Here is an example Facebook page for Packt Publishing at<a href="https://www.facebook.com/PacktPub"><span class="URLPACKT">https://www.facebook.com/PacktPub</span></a>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="images/B05679_09_05.png"/></div>
<p>Viewing the source of this page, you would find that the first few posts are available, and that later posts are loaded with AJAX when the browser scrolls. Facebook also has a mobile interface, which, as mentioned in <a href="py-web-scrp-2e_ch01.html" target="_blank"><span class="ChapterrefPACKT">Chapter 1</span></a>, <em>Introduction to Web Scraping</em>, is often easier to scrape. The same page using the mobile interface is available at <a href="https://m.facebook.com/PacktPub"><span class="URLPACKT">https://m.facebook.com/PacktPub</span></a>:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="images/image_09_004.jpg"/></div>
<p>If we interacted with the mobile website and then checked our browser tools, we would find that this interface uses a similar structure for the AJAX events, so it isn't easier to scrape. These AJAX events can be reverse engineered; however, different types of Facebook pages use different AJAX calls, and from my past experience, Facebook often changes the structure of these calls; so, scraping them will require ongoing maintenance. Therefore, as discussed in <span class="ChapterrefPACKT">Chapter 5</span>, <em>Dynamic Content</em>, unless performance is crucial, it would be preferable to use a browser rendering engine to execute the JavaScript events for us and give us access to the resulting HTML.</p>
<p>Here is an example snippet using Selenium to automate logging in to Facebook and then redirecting to the given page URL:</p>
<pre>from selenium import webdriver<br/><br/><br/>def get_driver():<br/>    try:<br/>        return webdriver.PhantomJS()<br/>    except:<br/>        return webdriver.Firefox()<br/><br/>def facebook(username, password, url):<br/>    driver = get_driver()<br/>    driver.get('https://facebook.com')<br/>    driver.find_element_by_id('email').send_keys(username)<br/>    driver.find_element_by_id('pass').send_keys(password)<br/>    driver.find_element_by_id('loginbutton').submit()<br/>    driver.implicitly_wait(30)<br/>    # wait until the search box is available,<br/>    # which means it has successfully logged in<br/>    search = driver.find_element_by_name('q')<br/>    # now logged in so can go to the page of interest<br/>    driver.get(url)<br/>    # add code to scrape data of interest here ...
</pre>
<p>This function can then be called to load the Facebook page of interest and scrape the resulting generated HTML, using a valid Facebook e-mail and password.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Facebook API</h1>
            </header>

            <article>
                
<p>As mentioned in <a href="py-web-scrp-2e_ch01.html" target="_blank"><span class="ChapterrefPACKT">Chapter 1</span></a>, <em>Introduction to Web Scraping</em>, scraping a website is a last resort when the data is not available in a structured format. Facebook does offer APIs for a vast majority&#160;of the public or private (via your user account) data, so we should check whether these APIs provide access to what we are after before building an intensive browser scraper.</p>
<p>The first thing to do is determine what data is available via the API. To figure this out, we should first reference the API documentation. The developer documentation available at&#160;<a href="https://developers.facebook.com/docs/" target="_blank">https://developers.facebook.com/docs/</a> shows all different types of APIs, including the Graph API, which is the one containing the information we desire. If you need to build other interactions with Facebook (via the API or SDK), the documentation is regularly updated and easy to use.</p>
<p>Also available via the documentation links is the in-browser Graph API Explorer, located&#160;at&#160;<a href="https://developers.facebook.com/tools/explorer/">https://developers.facebook.com/tools/explorer/</a>. As shown in the following screenshot, the Explorer is a great place to test queries and their results:</p>
<p><img class="image-border" src="images/graph_explorer.png"/></p>
<p>Here, I can search the API to retrieve the PacktPub Facebook Page ID. This Graph Explorer can also be used to generate access tokens, which we will use to navigate the API.</p>
<p>To utilize the Graph API with Python, we need to use special access tokens with slightly more advanced requests. Luckily, there is already a well-maintained library for us, called <kbd>facebook-sdk</kbd> (<a href="https://facebook-sdk.readthedocs.io" target="_blank">https://facebook-sdk.readthedocs.io</a>). We can easily install it using pip:</p>
<pre><strong>pip install facebook-sdk</strong>
</pre>
<p>Here is an example of using Facebook's Graph API to extract data from the Packt Publishing page:</p>
<pre><strong>In [1]: from facebook import GraphAPI</strong><br/><br/><strong>In [2]: access_token = '....'  # insert your actual token here</strong><br/><br/><strong>In [3]: graph = GraphAPI(access_token=access_token, version='2.7')</strong><br/><br/><strong>In [4]: graph.get_object('PacktPub')</strong><br/><strong>Out[4]: {'id': '204603129458', 'name': 'Packt'}</strong>
</pre>
<p>We see the same results as from the browser-based Graph Explorer. We can request more information about the page by passing some extra details we would like to extract. To determine which details, we can see all available fields for pages in the Graph documentation&#160;<a href="https://developers.facebook.com/docs/graph-api/reference/page/">https://developers.facebook.com/docs/graph-api/reference/page/</a>. Using the keyword argument&#160;<kbd>fields</kbd>, we can extract these extra available fields from the API:</p>
<pre><strong>In [5]: graph.get_object('PacktPub', fields='about,events,feed,picture')</strong><br/><strong>Out[5]: </strong><br/><strong>{'about': 'Packt provides software learning resources, from eBooks to video courses, to everyone from web developers to data scientists.',</strong><br/><strong> 'feed': {'data': [{'created_time': '2017-03-27T10:30:00+0000',</strong><br/><strong> 'id': '204603129458_10155195603119459',</strong><br/><strong> 'message': "We've teamed up with CBR Online to give you a chance to win 5 tech eBooks - enter by March 31! http://bit.ly/2mTvmeA"},</strong><br/><strong>...</strong><br/><strong> 'id': '204603129458',</strong><br/><strong> 'picture': {'data': {'is_silhouette': False,</strong><br/><strong> 'url': 'https://scontent.xx.fbcdn.net/v/t1.0-1/p50x50/14681705_10154660327349459_72357248532027065_n.png?oh=d0a26e6c8a00cf7e6ce957ed2065e430&amp;oe=59660265'}}}</strong><br/><br/>
</pre>
<p>We can see that this response is a well-formatted Python dictionary,&#160;which we can easily parse.</p>
<p>The Graph API provides many other calls to access user data, which&#160;are documented on Facebook's developer page at <a href="https://developers.facebook.com/docs/graph-api"><span class="URLPACKT">https://developers.facebook.com/docs/graph-api</span></a>. Depending on the data you need, you may also want to create a Facebook developer application, which can give you a longer usable access token.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Gap</h1>
            </header>

            <article>
                
<p>To demonstrate using a Sitemap to investigate content, we will use the Gap website.</p>
<p>Gap has a well structured website with a <kbd>Sitemap</kbd> to help web crawlers locate their updated content. If we use the techniques from <a href="py-web-scrp-2e_ch01.html" target="_blank"><span class="ChapterrefPACKT">Chapter 1</span></a>, <em>Introduction to Web Scraping</em>, to investigate a website, we would find their <kbd>robots.txt</kbd> file at <a href="http://www.gap.com/robots.txt" target="_blank"><span class="URLPACKT">http://www.gap.com/robots.txt</span></a>, which contains a link to this Sitemap:</p>
<pre>Sitemap: http://www.gap.com/products/sitemap_index.xml 
</pre>
<p>Here are the contents of the linked <kbd>Sitemap</kbd> file:</p>
<pre>&lt;?xml version="1.0" encoding="UTF-8"?&gt; <br/>&lt;sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"&gt; <br/>    &lt;sitemap&gt; <br/>        &lt;loc&gt;http://www.gap.com/products/sitemap_1.xml&lt;/loc&gt; <br/>        &lt;lastmod&gt;<span>2017-03-24</span>&lt;/lastmod&gt; <br/>    &lt;/sitemap&gt; <br/>    &lt;sitemap&gt; <br/>        &lt;loc&gt;http://www.gap.com/products/sitemap_2.xml&lt;/loc&gt; <br/>        &lt;lastmod&gt;<span>2017-03-24</span>&lt;/lastmod&gt; <br/>    &lt;/sitemap&gt; <br/>&lt;/sitemapindex&gt; 
</pre>
<p>As shown here, this <kbd>Sitemap</kbd> link is just an index and contains links to other <kbd>Sitemap</kbd> files. These other <kbd>Sitemap</kbd> files then contain links to thousands of product categories, such as <a href="http://www.gap.com/products/womens-jogger-pants.jsp" target="_blank">http://www.gap.com/products/womens-jogger-pants.jsp</a>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="images/gap_pants.png"/></div>
<p>There is a lot of content to crawl here, so we will use the threaded crawler developed in <a href="py-web-scrp-2e_ch04.html" target="_blank"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Concurrent Downloading</em>. You may recall that this crawler supports a URL pattern to match on the page. We can also define a <kbd>scraper_callback</kbd> keyword argument variable, which will allow us to parse more links.&#160;</p>
<p>Here is an example callback to crawl the Gap <kbd>Sitemap</kbd> link:</p>
<pre>from lxml import etree <br/>from threaded_crawler import threaded_crawler <br/><br/>def scrape_callback(url, html): <br/>    if url.endswith('.xml'): <br/>        # Parse the sitemap XML file <br/>        tree = etree.fromstring(html) <br/>        links = [e[0].text for e in tree] <br/>        return links <br/>    else: <br/>        # Add scraping code here <br/>        pass 
</pre>
<p>This callback first checks the downloaded URL extension. If the extension is <kbd>.xml</kbd>, the downloaded URL is for a <kbd>Sitemap</kbd> file, and the <kbd>lxml</kbd><kbd>etree</kbd> module is used to parse the XML and extract the links from it. Otherwise, this is a category URL, although this example does not implement scraping the category. Now we can use this callback with the threaded crawler to crawl <kbd>gap.com</kbd>:</p>
<pre><strong>In [1]: from chp9.gap_scraper_callback import scrape_callback</strong><br/><br/><strong>In [2]: from chp4.threaded_crawler import threaded_crawler</strong><br/><br/><strong>In [3]: sitemap = 'http://www.gap.com/products/sitemap_index.xml'</strong><br/><br/><strong>In [4]: threaded_crawler(sitemap, '[gap.com]*', scraper_callback=scrape_callback)</strong><br/><strong>10</strong><br/><strong>[&lt;Thread(Thread-517, started daemon 140145732585216)&gt;]</strong><br/><strong>Exception in thread Thread-517:</strong><br/><strong>Traceback (most recent call last):</strong><br/><strong>...</strong><br/><strong> File "src/lxml/parser.pxi", line 1843, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:118282)</strong><br/><strong>ValueError: Unicode strings with encoding declaration are not supported. Please use bytes input or XML fragments without declaration.</strong>
</pre>
<p>Unfortunately,&#160;<kbd>lxml</kbd> expects to load content from bytes or XML fragments, and we have instead stored the Unicode response (so we could parse using regular expressions and easily save to disk in Chapter 3, <em>Caching Downloads</em>&#160;and Chapter 4,&#160;<em>Concurrent Downloading</em>). However, we do have access to the URL in this function. Although it is inefficient, we could load the page again; if we only do this for XML pages, it should keep the number of requests down and therefore not add too much load time. Of course, if we are using caching this also makes it more efficient.</p>
<p>Let's try rewriting the callback function:</p>
<pre>import requests<br/><br/>def scrape_callback(url, html): <br/>    if url.endswith('.xml'): <br/>        # Parse the sitemap XML file <br/>        resp = requests.get(url)<br/>        tree = etree.fromstring(resp.content) <br/>        links = [e[0].text for e in tree] <br/>        return links <br/>    else: <br/>        # Add scraping code here <br/>        pass
</pre>
<p>Now, if we try running it again, we see&#160;success:</p>
<pre><strong>In [4]: threaded_crawler(sitemap, '[gap.com]*', scraper_callback=scrape_callback)</strong><br/><strong>10</strong><br/><strong>[&lt;Thread(Thread-51, started daemon 139775751223040)&gt;]</strong><br/><strong>Downloading: http://www.gap.com/products/sitemap_index.xml </strong><br/><strong>Downloading: http://www.gap.com/products/sitemap_2.xml </strong><br/><strong>Downloading: http://www.gap.com/products/gap-canada-français-index.jsp </strong><br/><strong>Downloading: http://www.gap.co.uk/products/index.jsp </strong><br/><strong>Skipping http://www.gap.co.uk/products/low-impact-sport-bras-women-C1077315.jsp due to depth Skipping http://www.gap.co.uk/products/sport-bras-women-C1077300.jsp due to depth </strong><br/><strong>Skipping http://www.gap.co.uk/products/long-sleeved-tees-tanks-women-C1077314.jsp due to depth Skipping http://www.gap.co.uk/products/short-sleeved-tees-tanks-women-C1077312.jsp due to depth ...</strong>
</pre>
<p>As expected, the <kbd>Sitemap</kbd> files were first downloaded and then the clothing categories. You'll find throughout your web scraping projects that you may need to modify and adapt your code and classes so they fit with new problems. This is just one of the many exciting challenges of scraping content from the Internet.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">BMW</h1>
            </header>

            <article>
                
<p>To investigate how to reverse engineer a new website, we will take a look at the BMW site. The BMW website has a search tool to find local dealerships, available at <a href="https://www.bmw.de/de/home.html?entryType=dlo"><span class="URLPACKT">https://www.bmw.de/de/home.html?entryType=dlo</span></a>:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="images/image_09_006.jpg"/></div>
<p>This tool takes a location and then displays the points near it on a map, such as this search for <kbd>Berlin</kbd>:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="images/image_09_007.jpg"/></div>
<p>Using browser developer tools such as the Network tab, we find that the search triggers this AJAX request:</p>
<pre><a href="https://c2b-services.bmw.com/c2b-localsearch/services/api/v3/clients/BMWDIGITAL_DLO/DE/pois?country=DE&amp;category=BM&amp;maxResults=149&amp;language=en&amp;name=berlin&amp;lat=52.507203343605816&amp;lng=13.426278585701539&amp;callback=callback&amp;_1490620507900=">https://c2b-services.bmw.com/c2b-localsearch/services/api/v3/ <br/>    clients/BMWDIGITAL_DLO/DE/ <br/>        pois?country=DE&amp;category=BM&amp;maxResults=99&amp;language=en&amp; <br/>            lat=52.507537768880056&amp;lng=13.425269635701511 <br/></a>
</pre>
<p>Here, the <kbd>maxResults</kbd> parameter is set to <kbd>99</kbd>. However, we can increase this to download all locations in a single query, a technique covered in <a href="py-web-scrp-2e_ch01.html" target="_blank"><span class="ChapterrefPACKT">Chapter 1</span></a>, <em>Introduction to Web Scraping</em>. Here is the result when <kbd>maxResults</kbd> is increased to <kbd>1000</kbd>:</p>
<pre><strong>&gt;&gt;&gt; import requests</strong><br/><strong>&gt;&gt;&gt; url = 'https://c2b-services.bmw.com/c2b-localsearch/services/api/v3/clients/BMWDIGITAL_DLO/DE/pois?country=DE&amp;category=BM&amp;maxResults=%d&amp;language=en&amp;         lat=52.507537768880056&amp;lng=13.425269635701511' </strong><br/><strong>&gt;&gt;&gt; jsonp = requests.get(url % 1000) </strong><br/><strong>&gt;&gt;&gt; jsonp.content </strong><br/><strong>'callback({"status":{ </strong><br/><strong>... </strong><br/><strong>})'</strong> 
</pre>
<p>This AJAX request provides the data in <strong>JSONP</strong> format, which stands for <strong>JSON with padding</strong>. The padding is usually a function to call, with the pure JSON data as an argument, in this case the <kbd>callback</kbd> function call. The padding is not easily understood by parsing libraries, so we need to remove it to properly parse the data.</p>
<p>To parse this data with Python's <kbd>json</kbd> module, we need to first strip this padding, which we can do with slicing:</p>
<pre><strong>&gt;&gt;&gt; import json </strong><br/><strong>&gt;&gt;&gt; pure_json = jsonp.text[jsonp.text.index('(') + 1 : jsonp.text.rindex(')')] </strong><br/><strong>&gt;&gt;&gt; dealers = json.loads(pure_json) </strong><br/><strong>&gt;&gt;&gt; dealers.keys() </strong><br/><strong>dict_keys(['status', 'translation', 'metadata', 'data', 'count'])</strong><br/><strong>&gt;&gt;&gt; dealers['count'] </strong><br/><strong>715</strong> 
</pre>
<p>We now have all the German BMW dealers loaded in a JSON object-currently, 715 of them. Here is the data for the first dealer:</p>
<pre><strong>&gt;&gt;&gt; dealers['data']['pois'][0] </strong><br/><strong>{'attributes': {'businessTypeCodes': ['NO', 'PR'],</strong><br/><strong> 'distributionBranches': ['T', 'F', 'G'],</strong><br/><strong> 'distributionCode': 'NL',</strong><br/><strong> 'distributionPartnerId': '00081',</strong><br/><strong> 'facebookPlace': '',</strong><br/><strong> 'fax': '+49 (30) 200992110',</strong><br/><strong> 'homepage': 'http://bmw-partner.bmw.de/niederlassung-berlin-weissensee',</strong><br/><strong> 'mail': 'nl.berlin@bmw.de',</strong><br/><strong> 'outletId': '3',</strong><br/><strong> 'outletTypes': ['FU'],</strong><br/><strong> 'phone': '+49 (30) 200990',</strong><br/><strong> 'requestServices': ['RFO', 'RID', 'TDA'],</strong><br/><strong> 'services': ['EB', 'PHEV']},</strong><br/><strong> 'category': 'BMW',</strong><br/><strong> 'city': 'Berlin',</strong><br/><strong> 'country': 'Germany',</strong><br/><strong> 'countryCode': 'DE',</strong><br/><strong> 'dist': 6.662869863289401,</strong><br/><strong> 'key': '00081_3',</strong><br/><strong> 'lat': 52.562568863415,</strong><br/><strong> 'lng': 13.463589476607,</strong><br/><strong> 'name': 'BMW AG Niederlassung Berlin Filiale Weißensee',</strong><br/><strong> 'oh': None,</strong><br/><strong> 'postalCode': '13088',</strong><br/><strong> 'postbox': None,</strong><br/><strong> 'state': None,</strong><br/><strong> 'street': 'Gehringstr. 20'}</strong>
</pre>
<p>We can now save the data of interest. Here is a snippet to write the name and latitude and longitude of these dealers to a spreadsheet:</p>
<pre>with open('../../data/bmw.csv', 'w') as fp: <br/>    writer = csv.writer(fp) <br/>    writer.writerow(['Name', 'Latitude', 'Longitude']) <br/>    for dealer in dealers['data']['pois']: <br/>        name = dealer['name'] <br/>        lat, lng = dealer['lat'], dealer['lng'] <br/>        writer.writerow([name, lat, lng]) 
</pre>
<p>After running this example, the contents of the <kbd>bmw.csv</kbd> spreadsheet will look similar to this:</p>
<pre>Name,Latitude,Longitude <br/>BMW AG Niederlassung Berlin Filiale Weissensee,52.562568863415,13.463589476607 <br/>Autohaus Graubaum GmbH,52.4528925,13.521265 <br/>Autohaus Reier GmbH &amp; Co. KG,52.56473,13.32521 <br/>... 
</pre>
<p>The full source code for scraping this data from BMW is available at <a href="https://github.com/kjam/wswp/blob/master/code/chp9/bmw_scraper.py">https://github.com/kjam/wswp/blob/master/code/chp9/bmw_scraper.py</a>.</p>
<div class="packt_infobox"><span class="packt_screen">Translating foreign content<br/></span>You may have noticed that the first screenshot for BMW was in German, but the second was in English. This is because the text for the second was translated using the Google Translate browser extension. This is a useful technique when trying to understand how to navigate a website in a foreign language. When the BMW website is translated, the website still works as usual. Be aware, though, as Google Translate will break some websites, for example, if the content of a select box is translated and a form depends on the original value.<br/>
Google Translate is available as the <kbd>Google Translate</kbd> extension for Chrome, the <kbd>Google Translator</kbd> add-on for Firefox, and can be installed as the<kbd>Google Toolbar</kbd> for Internet Explorer. Alternatively, <a href="http://translate.google.com"><span class="URLPACKT">http://translate.google.com</span></a> can be used for translations; however,&#160;this is only useful for raw text as the formatting is not preserved.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>This chapter analyzed a variety of prominent websites and demonstrated how the techniques covered in this book can be applied to them. We used&#160;CSS selectors to scrape Google results, tested a browser renderer and an API for&#160;Facebook pages, used a <kbd>Sitemap</kbd> to crawl Gap, and took advantage of an AJAX call to scrape all BMW dealers from a map.</p>
<p>You can now apply the techniques covered in this book to scrape websites that contain data of interest to you. As demonstrated by this chapter, the tools and methods you have learned throughout the book can help you scrape many different sites and content from the Internet.&#160;<span>I hope this begins a long and fruitful career in extracting content from the Web and automating data extraction with Python!</span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>
</body>
</html>