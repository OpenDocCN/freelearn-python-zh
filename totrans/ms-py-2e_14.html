<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer244" class="Basic-Text-Frame">&#13;
    <h1 class="chapterNumber">14</h1>&#13;
    <h1 id="_idParaDest-388" class="chapterTitle">Multiprocessing – When a Single CPU Core Is Not Enough</h1>&#13;
    <p class="normal">In the previous chapter, we discussed <code class="inlineCode">asyncio</code>, which can use the <code class="inlineCode">threading</code> and <code class="inlineCode">multiprocessing</code> modules but mainly uses single-thread/single-process parallelization. In this chapter, we will see how we can directly use multiple threads or processes to speed up our code and what caveats to keep in mind. This chapter can actually be seen as an extension to the list of performance tips.</p>&#13;
    <p class="normal">The <code class="inlineCode">threading</code> module makes it possible to run code in parallel in a single process. This makes <code class="inlineCode">threading</code> very useful for I/O-related tasks such as reading/writing files or network communication, but a useless option for slow and heavy calculations, which is where the <code class="inlineCode">multiprocessing</code> module shines.</p>&#13;
    <p class="normal">With the <code class="inlineCode">multiprocessing</code> module, you can run code in multiple processes, which means you can run code on multiple CPU cores, multiple processors, or even on multiple computers. This is an easy way to work around the <strong class="keyWord">Global Interpreter Lock</strong> (<strong class="keyWord">GIL</strong>) that was discussed in <em class="chapterRef">Chapter 12</em>, <em class="italic">Performance – Tracking and Reducing Your Memory and CPU Usage.</em></p>&#13;
    <p class="normal">The <code class="inlineCode">multiprocessing</code> module has a fairly easy-to-use interface with many convenience features, but the <code class="inlineCode">threading</code> module is rather basic and requires you to manually create and manage the threads. For this, we also have the <code class="inlineCode">concurrent.futures</code> module, which offers a simple way of executing a list of tasks either through threads or processes. This interface is also partially comparable to the <code class="inlineCode">asyncio</code> features we saw in the previous chapter.</p>&#13;
    <p class="normal">To summarize, this chapter covers:</p>&#13;
    <ul>&#13;
      <li class="bulletList">The Global Interpreter Lock (GIL)</li>&#13;
      <li class="bulletList">Multithreading versus multiprocessing</li>&#13;
      <li class="bulletList">Locking, deadlocks, and thread safety</li>&#13;
      <li class="bulletList">Data sharing and synchronization between processes</li>&#13;
      <li class="bulletList">Choosing between multithreading, multiprocessing, and single-threading</li>&#13;
      <li class="bulletList">Hyper-threading versus physical cores</li>&#13;
      <li class="bulletList">Remote multiprocessing with <code class="inlineCode">multiprocessing</code> and <code class="inlineCode">ipyparallel</code></li>&#13;
    </ul>&#13;
    <h1 id="_idParaDest-389" class="heading-1">The Global Interpreter Lock (GIL)</h1>&#13;
    <p class="normal">The GIL <a id="_idIndexMarker1117"/>has been mentioned in this book several times already, but we have not really covered it in detail and it really does need a bit more explanation before we continue with this chapter.</p>&#13;
    <p class="normal">In short, the name already explains what it does. It is a global lock for the Python interpreter so it can only execute a single statement <a id="_idIndexMarker1118"/>at once. A <strong class="keyWord">lock</strong> or <strong class="keyWord">mutex</strong> (<strong class="keyWord">mutual exclusion</strong>) in parallel<a id="_idIndexMarker1119"/> computing is a synchronization primitive that can block parallel execution. With a lock, you can make sure that nobody can touch your variable while you are working on it.</p>&#13;
    <p class="normal">Python offers several types of synchronization primitives, such as <code class="inlineCode">threading.Lock</code> and <code class="inlineCode">threading.Semaphore</code>. These are covered in more detail in the <em class="italic">Sharing data between threads and processes</em> section of this chapter.</p>&#13;
    <p class="normal">That means that even with the <code class="inlineCode">threading</code> module, you are still only executing a single Python statement at the same time. So, when it comes to pure Python code, your multithreaded solutions will <strong class="keyWord">always</strong> be slower than single-threaded solutions because <code class="inlineCode">threading</code> introduces some synchronization overhead while not offering any benefit for that scenario.</p>&#13;
    <p class="normal">Let’s continue with some more in-depth information about the GIL.</p>&#13;
    <h2 id="_idParaDest-390" class="heading-2">The use of multiple threads</h2>&#13;
    <p class="normal">Since the GIL only allows a <a id="_idIndexMarker1120"/>single Python statement to be executed at the same time, what point does threading have? The effectiveness greatly depends on your goal. Similar to the <code class="inlineCode">asyncio</code> examples in <em class="chapterRef">Chapter 13</em>, <code class="inlineCode">threading</code> can give you a lot of benefit if you are waiting for external resources.</p>&#13;
    <p class="normal">For example, if you are trying to fetch a webpage, open a file (remember that the <code class="inlineCode">aiofiles</code> module actually uses threads), or if you want<a id="_idIndexMarker1121"/> to execute something periodically, <code class="inlineCode">threading</code> can work great.</p>&#13;
    <div class="packt_tip">&#13;
      <p class="normal">When writing a new application, I would generally recommend that you make it ready for <code class="inlineCode">asyncio</code> if there is even a small chance of becoming I/O-limited in the future. Rewriting for <code class="inlineCode">asyncio</code> at a later time can be a huge amount of work.</p>&#13;
    </div>&#13;
    <p class="normal">There are several advantages<a id="_idIndexMarker1122"/> to <code class="inlineCode">asyncio</code> over <code class="inlineCode">threading</code>:</p>&#13;
    <ul>&#13;
      <li class="bulletList"><code class="inlineCode">asyncio</code> is generally faster than threading because you don’t have any thread synchronization overhead.</li>&#13;
      <li class="bulletList">Since <code class="inlineCode">asyncio</code> is normally single-threaded, you don’t have to worry about thread safety (more about thread safety later in the chapter).</li>&#13;
    </ul>&#13;
    <h2 id="_idParaDest-391" class="heading-2">Why do we need the GIL?</h2>&#13;
    <p class="normal">The GIL is <a id="_idIndexMarker1123"/>currently an essential part of the CPython interpreter because it makes sure that memory management is always consistent.</p>&#13;
    <p class="normal">To explain how this works, we need to know a bit about how the CPython interpreter manages its memory.</p>&#13;
    <p class="normal">Within CPython, the memory management system and garbage collection system rely on reference counting. This means that CPython counts how many names you have linked to a value. If you have a line of Python like this: <code class="inlineCode">a = SomeObject()</code>, that means that this instance of <code class="inlineCode">SomeObject</code> has 1 reference, namely, <code class="inlineCode">a</code>. If we were to do <code class="inlineCode">b = a</code>, the reference count would increase to 2. When the reference count reaches 0, the variable will be deleted by the garbage collector when it runs. </p>&#13;
    <p class="normal">You can check the number of references using <code class="inlineCode">sys.getrefcount(variable)</code>. You should note that the call to <code class="inlineCode">sys.getrefcount()</code> increases your reference count by 1, so if it returns 2, the actual number is 1.</p>&#13;
    <p class="normal">As the GIL makes sure that only a single Python statement can be executed simultaneously, you can never have issues where multiple bits of code manipulate memory at the same time, or where memory is being released to the system that is not actually free.</p>&#13;
    <p class="normal">If the reference counter is not correctly managed, this could easily result in memory leaks or a crashing Python interpreter. Remember the segmentation faults we saw in <em class="chapterRef">Chapter 11</em>, <em class="italic">Debugging – Solving the Bugs?</em> That is what could easily happen without the GIL, and it would instantly kill your Python interpreter.</p>&#13;
    <h2 id="_idParaDest-392" class="heading-2">Why do we still have the GIL?</h2>&#13;
    <p class="normal">When Python was<a id="_idIndexMarker1124"/> initially created, many operating systems didn’t even have a concept of threading, and all common processors only had a single core. Long story short, there are two main reasons for the GIL:</p>&#13;
    <ul>&#13;
      <li class="bulletList">There was initially no point in creating a complex solution that would handle threading</li>&#13;
      <li class="bulletList">The GIL is a really simple solution for a very complex problem</li>&#13;
    </ul>&#13;
    <p class="normal">Luckily, it seems that is not the end of the discussion. Recently (May 2021), Guido van Rossum came out of retirement and he has plans to address the GIL limitations by creating sub-interpreters for threads. How this will work out in practice remains to be seen, of course, but the ambitious plan is to make CPython 3.15 about 5 times faster than CPython 3.10, which would be an amazing performance increase.</p>&#13;
    <p class="normal">Now that we know when the GIL limits CPython threads, let’s look at how we can create and use multiple threads and processes.</p>&#13;
    <h1 id="_idParaDest-393" class="heading-1">Multiple threads and processes</h1>&#13;
    <p class="normal">The <code class="inlineCode">multiprocessing</code> module was introduced in Python 2.6, and it has been a game changer when it comes to working with<a id="_idIndexMarker1125"/> multiple processes in Python. Specifically, it has made it rather easy to work around the limitations of the GIL because each process has its own GIL.</p>&#13;
    <p class="normal">The usage of the <code class="inlineCode">multiprocessing</code> module is largely similar to the <code class="inlineCode">threading</code> module, but it has several really useful extra features that make much more sense with multiple processes. Alternatively, you can also use it with <code class="inlineCode">concurrent.futures.ProcessPoolExecutor</code>, which has an interface nearly identical to <code class="inlineCode">concurrent.futures.ThreadPoolExecutor</code>.</p>&#13;
    <p class="normal">These similarities mean that in many cases you can simply swap out the modules and your code will keep running as expected. Don’t be fooled, however; while threads can still use the same memory objects and only have thread safety and deadlocks to worry about, multiple processes also have these issues and introduce several other issues when it comes to sharing memory, objects, and results.</p>&#13;
    <p class="normal">In either case, dealing with parallel code comes with caveats. This is also why code that uses multiple threads<a id="_idIndexMarker1126"/> or<a id="_idIndexMarker1127"/> processes has the reputation of being difficult to work with. Many of these issues are not as daunting as they might seem; if you follow a few rules, that is.</p>&#13;
    <div class="note">&#13;
      <p class="normal">Before we continue with the example code, you should be aware that it is critically important to have your code in an <code class="inlineCode">if</code> <code class="inlineCode">__name__ == '__main__'</code> block when using <code class="inlineCode">multiprocessing</code>. When the <code class="inlineCode">multiprocessing</code> module launches the extra Python processes, it will execute the same Python script, so without using this block you will end up with an infinite loop of starting processes.</p>&#13;
    </div>&#13;
    <p class="normal">Within this section, we are going to cover:</p>&#13;
    <ul>&#13;
      <li class="bulletList">Basic examples using <code class="inlineCode">threading</code>, <code class="inlineCode">multiprocessing</code>, and <code class="inlineCode">concurrent.futures</code></li>&#13;
      <li class="bulletList">Cleanly exiting threads and processes</li>&#13;
      <li class="bulletList">Batch processing</li>&#13;
      <li class="bulletList">Sharing memory between processes</li>&#13;
      <li class="bulletList">Thread safety</li>&#13;
      <li class="bulletList">Deadlocks</li>&#13;
      <li class="bulletList">Thread-local variables</li>&#13;
    </ul>&#13;
    <p class="normal">Several of these, such as race conditions and locking, are not exclusive to threading and could be interesting for <code class="inlineCode">multiprocessing</code> as well.</p>&#13;
    <h2 id="_idParaDest-394" class="heading-2">Basic examples</h2>&#13;
    <p class="normal">To create threads and processes, we have several options:</p>&#13;
    <ul>&#13;
      <li class="bulletList"><code class="inlineCode">concurrent.futures</code>: An<a id="_idIndexMarker1128"/> easy-to-use interface for running functions in either threads or processes, similar to <code class="inlineCode">asyncio</code></li>&#13;
      <li class="bulletList"><code class="inlineCode">threading</code>: An <a id="_idIndexMarker1129"/>interface for creating threads directly</li>&#13;
      <li class="bulletList"><code class="inlineCode">multiprocessing</code>: An<a id="_idIndexMarker1130"/> interface with many utility and convenience functions to create and manage multiple Python processes</li>&#13;
    </ul>&#13;
    <p class="normal">Let’s look at an example of each one.</p>&#13;
    <h3 id="_idParaDest-395" class="heading-3">concurrent.futures</h3>&#13;
    <p class="normal">Let’s start with a basic <a id="_idIndexMarker1131"/>example of the <code class="inlineCode">concurrent.futures</code> module. In this example, we run two timer jobs that run and print in parallel:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> time&#13;
<span class="hljs-keyword">import</span> concurrent.futures&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">timer</span>(<span class="hljs-params">name, steps, interval=</span><span class="hljs-number">0.1</span>):&#13;
    <span class="hljs-string">'''timer function that sleeps 'steps * interval' '''</span>&#13;
    <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(steps):&#13;
        <span class="hljs-built_in">print</span>(name, step)&#13;
        time.sleep(interval)&#13;
&#13;
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    <span class="hljs-comment"># Replace with concurrent.futures.ProcessPoolExecutor for</span>&#13;
    <span class="hljs-comment"># multiple processes instead of threads</span>&#13;
    <span class="hljs-keyword">with</span> concurrent.futures.ThreadPoolExecutor() <span class="hljs-keyword">as</span> executor:&#13;
        <span class="hljs-comment"># Submit the function to the executor with some arguments</span>&#13;
        executor.submit(timer, steps=<span class="hljs-number">3</span>, name=<span class="hljs-string">'a'</span>)&#13;
        <span class="hljs-comment"># Sleep a tiny bit to keep the output order consistent</span>&#13;
        time.sleep(<span class="hljs-number">0.1</span>)&#13;
        executor.submit(timer, steps=<span class="hljs-number">3</span>, name=<span class="hljs-string">'b'</span>)&#13;
</code></pre>&#13;
    <p class="normal">Before we execute the code, let’s see what we did here. First, we created a <code class="inlineCode">timer</code> function that runs <code class="inlineCode">time.sleep(interval)</code> and does that <code class="inlineCode">steps</code> times. Before sleeping, it prints the <code class="inlineCode">name</code> and the current <code class="inlineCode">step</code> so we can easily see what is happening.</p>&#13;
    <p class="normal">Then, we create an <code class="inlineCode">executor</code> using <code class="inlineCode">concurrent.futures.ThreadPoolExecutor</code> to execute the functions.</p>&#13;
    <p class="normal">Lastly, we submit the functions we want to execute with their respective arguments to start both of the threads. In between starting them, we sleep a very short time so our output in this example is consistent. If we were to execute the code without the <code class="inlineCode">time.sleep(0.1)</code>, the output order would be random because sometimes <code class="inlineCode">a</code> would be faster and other times <code class="inlineCode">b</code> would be faster.</p>&#13;
    <p class="normal">The main reason for including the tiny sleep is testing. All of the code in this book is available on GitHub (<a href="https://github.com/mastering-python/code_2"><span class="url">https://github.com/mastering-python/code_2</span></a>) and is automatically tested.</p>&#13;
    <p class="normal">Now when executing this script, we get the following:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> python3 T_00_concurrent_futures.py&#13;
a 0&#13;
b 0&#13;
a 1&#13;
b 1&#13;
a 2&#13;
b 2&#13;
</code></pre>&#13;
    <p class="normal">As expected, they run<a id="_idIndexMarker1132"/> right next to each other, but due to the tiny <code class="inlineCode">time.sleep(0.1)</code> we added, the results are consistently interleaved. In this case we started the <code class="inlineCode">ThreadPoolExecutor</code> with the default arguments, which results in threads without specific names and an automatically calculated thread count.</p>&#13;
    <p class="normal">The thread count depends on the Python version. Up to Python 3.8, the number of workers was equal to the number of hyper-threaded CPU cores in the machine multiplied by 5. So, if your machine has 2 cores with hyper-threading enabled, it would result in 4 cores * 5 = 20 threads. With a 64-core machine, that would result in 320 threads, which would probably incur more synchronization overhead than benefits.</p>&#13;
    <p class="normal">For Python 3.8 and above, this has been changed to <code class="inlineCode">min(32, cores + 4)</code>, which should be enough to always have at least 5 threads for I/O operations but not so much that it uses large amounts of resources on machines with many cores. For the same <code class="inlineCode">64</code>-core machine, this would still be capped at <code class="inlineCode">32</code> threads.</p>&#13;
    <p class="normal">In the case of the <code class="inlineCode">ProcessPoolExecutor</code>, the number of processor cores including hyper-threading will be used. That means that if your processor has 4 cores with hyper-threading enabled, you will get a default of 8 processes.</p>&#13;
    <p class="normal">Naturally, the traditional <code class="inlineCode">threading</code> module is still a good option and offers a bit more control while still having an easy-to-use interface.</p>&#13;
    <p class="normal">Before Python 3, the <code class="inlineCode">thread</code> module was also available as a low-level API to threads. This module is still available but renamed to <code class="inlineCode">_thread</code>. Internally, both <code class="inlineCode">concurrent.futures.ThreadPoolExecutor</code> and <code class="inlineCode">threading</code> are still using it, but you should generally have no need to access it directly.</p>&#13;
    <h3 id="_idParaDest-396" class="heading-3">threading</h3>&#13;
    <p class="normal">Now we will look at how to<a id="_idIndexMarker1133"/> recreate the <code class="inlineCode">concurrent.futures</code> example using the <code class="inlineCode">threading</code> module:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> time&#13;
<span class="hljs-keyword">import</span> threading&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">timer</span>(<span class="hljs-params">name, steps, interval=</span><span class="hljs-number">0.1</span>):&#13;
    <span class="hljs-string">'''timer function that sleeps 'steps * interval' '''</span>&#13;
    <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(steps):&#13;
        <span class="hljs-built_in">print</span>(name, step)&#13;
        time.sleep(interval)&#13;
&#13;
<span class="hljs-comment"># Create the threads declaratively</span>&#13;
a = threading.Thread(target=timer, kwargs=<span class="hljs-built_in">dict</span>(name=<span class="hljs-string">'a'</span>, steps=<span class="hljs-number">3</span>))&#13;
b = threading.Thread(target=timer, kwargs=<span class="hljs-built_in">dict</span>(name=<span class="hljs-string">'b'</span>, steps=<span class="hljs-number">3</span>))&#13;
&#13;
<span class="hljs-comment"># Start the threads</span>&#13;
a.start()&#13;
<span class="hljs-comment"># Sleep a tiny bit to keep the output order consistent</span>&#13;
time.sleep(<span class="hljs-number">0.1</span>)&#13;
b.start()&#13;
</code></pre>&#13;
    <p class="normal">The <code class="inlineCode">timer</code> function is identical to the previous example, so no difference there. The execution, however, is a bit different.</p>&#13;
    <p class="normal">In this case we create<a id="_idIndexMarker1134"/> the threads by instantiating <code class="inlineCode">threading.Thread()</code> directly, but inheriting <code class="inlineCode">threading.Thread</code> is also an option, as we will see in the next example. The arguments to the <code class="inlineCode">target</code> function can be given by passing an <code class="inlineCode">args</code> and/or <code class="inlineCode">kwargs</code> argument, but these are optional if you have no need for them or if you have prefilled them using <code class="inlineCode">functools.partial</code>.</p>&#13;
    <p class="normal">With the earlier example, we created a <code class="inlineCode">ThreadPoolExecutor()</code> that creates a bunch of threads and runs the functions on those threads. With this example, we are explicitly creating the threads to run a single function and exit as soon as the function is done. This is mostly useful for long-running backgrounded threads as this method requires setting up and tearing down a thread for each function. Generally, the overhead of starting a thread is very little, but it depends on your Python interpreter (CPython, PyPy, and so on) and your operating system.</p>&#13;
    <p class="normal">Now for the same example, but inheriting <code class="inlineCode">threading.Thread</code> instead of a declarative call to <code class="inlineCode">threading.Thread()</code>:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> time&#13;
<span class="hljs-keyword">import</span> threading&#13;
&#13;
<span class="hljs-keyword">class</span> <span class="hljs-title">Timer</span>(threading.Thread):&#13;
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, name, steps, interval=</span><span class="hljs-number">0.1</span>):&#13;
        self.steps = steps&#13;
        self.interval = interval&#13;
        <span class="hljs-comment"># Small gotcha: threading.Thread has a built-in name</span>&#13;
        <span class="hljs-comment"># parameter so be careful not to manually override it</span>&#13;
        <span class="hljs-built_in">super</span>().__init__(name=name)&#13;
&#13;
    <span class="hljs-keyword">def</span> <span class="hljs-title">run</span>(<span class="hljs-params">self</span>):&#13;
        <span class="hljs-string">'''timer function that sleeps 'steps * interval' '''</span>&#13;
        <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.steps):&#13;
            <span class="hljs-built_in">print</span>(self.name, step)&#13;
            time.sleep(self.interval)&#13;
a = Timer(name=<span class="hljs-string">'a'</span>, steps=<span class="hljs-number">3</span>)&#13;
b = Timer(name=<span class="hljs-string">'b'</span>, steps=<span class="hljs-number">3</span>)&#13;
&#13;
<span class="hljs-comment"># Start the threads</span>&#13;
a.start()&#13;
<span class="hljs-comment"># Sleep a tiny bit to keep the output order consistent</span>&#13;
time.sleep(<span class="hljs-number">0.1</span>)&#13;
b.start()&#13;
</code></pre>&#13;
    <p class="normal">The code is roughly the<a id="_idIndexMarker1135"/> same as the procedural version where we called <code class="inlineCode">threading.Thread()</code> directly, but there are two critical differences that you need to be aware of:</p>&#13;
    <ul>&#13;
      <li class="bulletList"><code class="inlineCode">name</code> is a reserved attribute for <code class="inlineCode">threading.Thread</code>. On Linux/Unix machines your process manager (for instance, <code class="inlineCode">top</code>) can display this name instead of <code class="inlineCode">/usr/bin/python3</code>.</li>&#13;
      <li class="bulletList">The default target function is <code class="inlineCode">run()</code>. Be careful to override the <code class="inlineCode">run()</code> method instead of the <code class="inlineCode">start()</code> method, otherwise your code will <em class="italic">not</em> execute in a separate thread but will execute like a regular function call when you call <code class="inlineCode">start()</code> instead.</li>&#13;
    </ul>&#13;
    <p class="normal">The procedural and class-based versions use the exact same API internally and are equally powerful, so choosing between them comes down to personal preference only.</p>&#13;
    <h3 id="_idParaDest-397" class="heading-3">multiprocessing</h3>&#13;
    <p class="normal">Lastly, we can recreate the earlier<a id="_idIndexMarker1136"/> timer scripts using <code class="inlineCode">multiprocessing</code> as well. First with the procedural call to <code class="inlineCode">multiprocessing.Process()</code>:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> time&#13;
<span class="hljs-keyword">import</span> multiprocessing&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">timer</span>(<span class="hljs-params">name, steps, interval=</span><span class="hljs-number">0.1</span>):&#13;
    <span class="hljs-string">'''timer function that sleeps 'steps * interval' '''</span>&#13;
    <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(steps):&#13;
        <span class="hljs-built_in">print</span>(name, step)&#13;
        time.sleep(interval)&#13;
&#13;
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    <span class="hljs-comment"># Create the processes declaratively</span>&#13;
    a = multiprocessing.Process(target=timer, kwargs=<span class="hljs-built_in">dict</span>(name=<span class="hljs-string">'a'</span>, steps=<span class="hljs-number">3</span>))&#13;
    b = multiprocessing.Process(target=timer, kwargs=<span class="hljs-built_in">dict</span>(name=<span class="hljs-string">'b'</span>, steps=<span class="hljs-number">3</span>))&#13;
&#13;
    <span class="hljs-comment"># Start the processes</span>&#13;
    a.start()&#13;
    <span class="hljs-comment"># Sleep a tiny bit to keep the output order consistent</span>&#13;
    time.sleep(<span class="hljs-number">0.1</span>)&#13;
    b.start()&#13;
</code></pre>&#13;
    <p class="normal">The code looks effectively the<a id="_idIndexMarker1137"/> same with a few minor changes. Instead of <code class="inlineCode">threading.Thread</code> we used <code class="inlineCode">multiprocessing.Process</code>, and we have to run the code from an <code class="inlineCode">if __name__ == '__main__'</code> block. Beyond that, both the code and execution are the same in this simple example.</p>&#13;
    <p class="normal">Lastly, for completeness, let’s look at the class-based version as well:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> time&#13;
<span class="hljs-keyword">import</span> multiprocessing&#13;
&#13;
<span class="hljs-keyword">class</span> <span class="hljs-title">Timer</span>(multiprocessing.Process):&#13;
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, name, steps, interval=</span><span class="hljs-number">0.1</span>):&#13;
        self.steps = steps&#13;
        self.interval = interval&#13;
        <span class="hljs-comment"># Similar to threading.Thread, multiprocessing.Process</span>&#13;
        <span class="hljs-comment"># also supports the name parameter but you are not</span>&#13;
        <span class="hljs-comment"># required to use it here.</span>&#13;
        <span class="hljs-built_in">super</span>().__init__(name=name)&#13;
&#13;
    <span class="hljs-keyword">def</span> <span class="hljs-title">run</span>(<span class="hljs-params">self</span>):&#13;
        <span class="hljs-string">'''timer function that sleeps 'steps * interval' '''</span>&#13;
        <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.steps):&#13;
            <span class="hljs-built_in">print</span>(self.name, step)&#13;
            time.sleep(self.interval)&#13;
&#13;
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    a = Timer(name=<span class="hljs-string">'a'</span>, steps=<span class="hljs-number">3</span>)&#13;
    b = Timer(name=<span class="hljs-string">'</span><span class="hljs-string">b'</span>, steps=<span class="hljs-number">3</span>)&#13;
&#13;
    <span class="hljs-comment"># Start the process</span>&#13;
    a.start()&#13;
    <span class="hljs-comment"># Sleep a tiny bit to keep the output order consistent</span>&#13;
    time.sleep(<span class="hljs-number">0.1</span>)&#13;
    b.start()&#13;
</code></pre>&#13;
    <p class="normal">Once again, we are<a id="_idIndexMarker1138"/> required to use the <code class="inlineCode">if __name__ == '__main__'</code> block. But beyond that, the code is virtually identical to the <code class="inlineCode">threading</code> version. As was the case with <code class="inlineCode">threading</code>, choosing between the procedural and class-based style depends only on your personal preference.</p>&#13;
    <p class="normal">Now that we know how to start threads and processes, let’s look at how we can cleanly shut them down again.</p>&#13;
    <h2 id="_idParaDest-398" class="heading-2">Cleanly exiting long-running threads and processes</h2>&#13;
    <p class="normal">The <code class="inlineCode">threading</code> module is <a id="_idIndexMarker1139"/>mostly useful for long-running threads that handle an external resource. Some example scenarios:</p>&#13;
    <ul>&#13;
      <li class="bulletList">When creating a server and you want to keep listening for new connections</li>&#13;
      <li class="bulletList">When connecting to HTTP WebSockets and you need the connection to stay open</li>&#13;
      <li class="bulletList">When you need to periodically save your changes</li>&#13;
    </ul>&#13;
    <p class="normal">Naturally, these scenarios can also use <code class="inlineCode">multiprocessing</code>, but <code class="inlineCode">threading</code> is often more convenient, as we will see later.</p>&#13;
    <p class="normal">At some point you might need to <a id="_idIndexMarker1140"/>shut the thread down from <strong class="keyWord">outside</strong> of the thread; during the exit of your main script, for example. Waiting for a thread that is exiting by itself is trivial; the only thing you need to do is <code class="inlineCode">future.result()</code> or <code class="inlineCode">some_thread.join(timeout=...)</code> and you are done. The harder part is telling a thread to shut itself down and run the cleanup while it’s still doing something.</p>&#13;
    <p class="normal">The only real solution for this issue, which applies if you are lucky, is a simple <code class="inlineCode">while</code> loop that keeps running until you give a stop signal like this:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> time&#13;
<span class="hljs-keyword">import</span> threading&#13;
&#13;
<span class="hljs-keyword">class</span> <span class="hljs-title">Forever</span>(threading.Thread):&#13;
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):&#13;
        self.stop = threading.Event()&#13;
        <span class="hljs-built_in">super</span>().__init__()&#13;
&#13;
    <span class="hljs-keyword">def</span> <span class="hljs-title">run</span>(<span class="hljs-params">self</span>):&#13;
        <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> self.stop.is_set():&#13;
            <span class="hljs-comment"># Do whatever you need to do here</span>&#13;
            time.sleep(<span class="hljs-number">0.1</span>)&#13;
&#13;
thread = Forever()&#13;
thread.start()&#13;
<span class="hljs-comment"># Do whatever you need to do here</span>&#13;
thread.stop.<span class="hljs-built_in">set</span>()&#13;
thread.join()&#13;
</code></pre>&#13;
    <p class="normal">This code uses <code class="inlineCode">threading.Event()</code> as a flag to tell the thread to exit when needed. While you can use a <code class="inlineCode">bool</code> instead of <code class="inlineCode">threading.Event()</code> with the current CPython interpreter, there is no guarantee for this to work with future Python versions and/or other types of <a id="_idIndexMarker1141"/>interpreters. The reason this is currently safe for CPython is that, due to the GIL, all Python operations are effectively single-threaded. That’s why threads are useful for waiting for external resources, but have a negative effect on the performance of your Python code.</p>&#13;
    <p class="normal">Additionally, if you were<a id="_idIndexMarker1142"/> to translate this code to multiprocessing, you could simply replace <code class="inlineCode">threading.Event()</code> with <code class="inlineCode">multiprocessing.Event()</code> and it should keep working with no other changes, assuming you are not interacting with external variables. With multiple Python processes, you are no longer protected by the single GIL so you need to be more careful when modifying variables. More about this topic in the <em class="italic">Sharing data between threads and processes</em> section later in this chapter. </p>&#13;
    <p class="normal">Now that we have the <code class="inlineCode">stop</code> event, we can run <code class="inlineCode">stop.set()</code> so the thread knows when to exit and will do so after the maximum of 0.1 seconds’ sleep.</p>&#13;
    <p class="normal">This is the ideal scenario: to have a loop where the loop condition is checked regularly and the loop interval is your maximum thread shutdown delay. What happens if the thread is busy doing some operation and doesn’t check the <code class="inlineCode">while</code> condition? As you might suspect, setting the <code class="inlineCode">stop</code> event is useless in those scenarios and you need a more powerful method to exit the thread.</p>&#13;
    <p class="normal">To handle this scenario, you have a few options:</p>&#13;
    <ul>&#13;
      <li class="bulletList">Avoid the issue entirely by using <code class="inlineCode">asyncio</code> or <code class="inlineCode">multiprocessing</code> instead. In terms of performance, <code class="inlineCode">asyncio</code> is your best option by far, but <code class="inlineCode">multiprocessing</code> can work as well if your code is suitable.</li>&#13;
      <li class="bulletList">Make the thread a daemon thread by setting <code class="inlineCode">your_thread.daemon = True</code> <em class="italic">before</em> starting the thread. This will automatically kill the thread when the main process exits so it is not a graceful shutdown. You can still add a teardown using the <code class="inlineCode">atexit</code> module.</li>&#13;
      <li class="bulletList">Kill the thread from the outside by either telling your operating system to send a terminate/kill signal or by raising an exception within the thread from the main thread. You might be tempted to go for this method, but I would strongly recommend against it. Not only is it unreliable, but it can cause your entire Python interpreter to crash, so it really is not an option you should ever consider.</li>&#13;
    </ul>&#13;
    <p class="normal">We have <a id="_idIndexMarker1143"/>already seen <a id="_idIndexMarker1144"/>how to use <code class="inlineCode">asyncio</code> in the previous chapter, so let’s look at how we can terminate with <code class="inlineCode">multiprocessing</code>. Before we start, however, you should note that the same limitations that apply to <code class="inlineCode">threading</code> also largely apply to <code class="inlineCode">multiprocessing</code>. While <code class="inlineCode">multiprocessing</code> does have a built-in solution for terminating processes as opposed to threading, it is still not a clean method and it won’t (reliably) run your exit handlers, <code class="inlineCode">finally</code> clauses, and so on. This means you should <em class="italic">always</em> try an event first, but use <code class="inlineCode">multiprocessing.Event</code> instead of <code class="inlineCode">threading.Event</code>, of course.</p>&#13;
    <p class="normal">To illustrate how we can forcefully terminate or kill a thread (while risking memory corruption):</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> time&#13;
<span class="hljs-keyword">import</span> multiprocessing&#13;
&#13;
<span class="hljs-keyword">class</span> <span class="hljs-title">Forever</span>(multiprocessing.Process):&#13;
    <span class="hljs-keyword">def</span> <span class="hljs-title">run</span>(<span class="hljs-params">self</span>):&#13;
        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:&#13;
            <span class="hljs-comment"># Do whatever you need to do here</span>&#13;
            time.sleep(<span class="hljs-number">0.1</span>)&#13;
&#13;
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    process = Forever()&#13;
    process.start()&#13;
&#13;
    <span class="hljs-comment"># Kill our "unkillable" process</span>&#13;
    process.terminate()&#13;
    <span class="hljs-comment"># Wait for 10 seconds to properly exit      </span>&#13;
    process.join(<span class="hljs-number">10</span>)&#13;
&#13;
    <span class="hljs-comment"># If it still didn't exit, kill it</span>&#13;
    <span class="hljs-keyword">if</span> process.exitcode <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:&#13;
        process.kill()&#13;
</code></pre>&#13;
    <p class="normal">In this example, we first <a id="_idIndexMarker1145"/>try a regular <code class="inlineCode">terminate()</code>, which sends a <code class="inlineCode">SIGTERM</code> signal on <a id="_idIndexMarker1146"/>Unix machines and <code class="inlineCode">TerminateProcess()</code> on Windows. If that does not work, we try again with a <code class="inlineCode">kill()</code>, which sends a <code class="inlineCode">SIGKILL</code> on Unix and does not currently have a Windows equivalent, so on Windows the <code class="inlineCode">kill()</code> and <code class="inlineCode">terminate()</code> methods behave the same way and both effectively kill the process without teardown.</p>&#13;
    <h2 id="_idParaDest-399" class="heading-2">Batch processing using concurrent.futures</h2>&#13;
    <p class="normal">Starting threads or processes<a id="_idIndexMarker1147"/> in a fire-and-forget fashion is easy <a id="_idIndexMarker1148"/>enough, as we have seen in the prior examples. However, often, you want to spin up several threads or processes and wait until they have all finished.</p>&#13;
    <p class="normal">This is a case where <code class="inlineCode">concurrent.futures</code> and <code class="inlineCode">multiprocessing</code> really shine. They allow you to call <code class="inlineCode">executor.map()</code> or <code class="inlineCode">pool.map()</code> very similarly to how we saw in <em class="chapterRef">Chapter 5</em>, <em class="italic">Functional Programming – Readability Versus Brevity</em>. Effectively, you only need to create a list of items to process, call the <code class="inlineCode">[executor/pool].map()</code> function, and you are done. You could build something similar with the <code class="inlineCode">threading</code> module if you are looking for a fun challenge, but there is little use for it otherwise.</p>&#13;
    <p class="normal">To give our system a test, let’s get some information about a hostname that should use the system DNS resolving system. Since that queries an external resource, we should expect nice results when using threading, right? Well... let’s give it a try and have a look:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> timeit&#13;
<span class="hljs-keyword">import</span> socket&#13;
<span class="hljs-keyword">import</span> concurrent.futures&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">getaddrinfo</span>(<span class="hljs-params">*args</span>):&#13;
    <span class="hljs-comment"># Call getaddrinfo but ignore the given parameter</span>&#13;
    socket.getaddrinfo(<span class="hljs-string">'localhost'</span>, <span class="hljs-literal">None</span>)&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">benchmark</span>(<span class="hljs-params">threads, n=</span><span class="hljs-number">1000</span>):&#13;
    <span class="hljs-keyword">if</span> threads &gt; <span class="hljs-number">1</span>:&#13;
        <span class="hljs-comment"># Create the executor</span>&#13;
        <span class="hljs-keyword">with</span> concurrent.futures.ThreadPoolExecutor(threads) \&#13;
                <span class="hljs-keyword">as</span> executor:&#13;
            executor.<span class="hljs-built_in">map</span>(getaddrinfo, <span class="hljs-built_in">range</span>(n))&#13;
&#13;
    <span class="hljs-keyword">else</span>:&#13;
        <span class="hljs-comment"># Make sure to use 'list'. Otherwise the generator will</span>&#13;
        <span class="hljs-comment"># not execute because it is lazy</span>&#13;
        <span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(getaddrinfo, <span class="hljs-built_in">range</span>(n)))&#13;
&#13;
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    <span class="hljs-keyword">for</span> threads <span class="hljs-keyword">in</span> (<span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">50</span>, <span class="hljs-number">100</span>):&#13;
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f'Testing with </span><span class="hljs-subst">{threads}</span><span class="hljs-string"> threads and n=</span><span class="hljs-subst">{</span><span class="hljs-number">10</span><span class="hljs-subst">}</span><span class="hljs-string"> took: '</span>,&#13;
              end=<span class="hljs-string">''</span>)&#13;
        <span class="hljs-built_in">print</span>(<span class="hljs-string">'{:.1f}'</span>.<span class="hljs-built_in">format</span>(timeit.timeit(&#13;
            <span class="hljs-string">f'benchmark(</span><span class="hljs-subst">{threads}</span><span class="hljs-string">)'</span>,&#13;
            setup=<span class="hljs-string">'from __main__ import benchmark'</span>,&#13;
            number=<span class="hljs-number">10</span>,&#13;
        )))&#13;
</code></pre>&#13;
    <p class="normal">Let’s analyze this<a id="_idIndexMarker1149"/> code. First, we have the <code class="inlineCode">getaddrinfo()</code> function, which attempts to fetch some info about a hostname through your operating<a id="_idIndexMarker1150"/> system, an external resource that could benefit from multiple threads.</p>&#13;
    <p class="normal">Second, we have the <code class="inlineCode">benchmark()</code> function, which uses multiple threads for the <code class="inlineCode">map()</code> if <code class="inlineCode">threads</code> is set to a number above 1. If not, it goes for the regular <code class="inlineCode">map()</code>.</p>&#13;
    <p class="normal">Lastly, we execute the benchmarks for <code class="inlineCode">1</code>, <code class="inlineCode">10</code>, <code class="inlineCode">50</code>, and <code class="inlineCode">100</code> threads where <code class="inlineCode">1</code> is the regular non-threaded approach. So how much can threads help us here? This test strongly depends on your computer, operating system, network, etc., so your results may be different, but this is what happened on my OS X machine using CPython 3.10:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> python3 T_07_thread_batch_processing.py&#13;
Testing with 1 threads and n=10 took: 2.1&#13;
Testing with 10 threads and n=10 took: 1.9&#13;
Testing with 50 threads and n=10 took: 1.9&#13;
Testing with 100 threads and n=10 took: 13.9&#13;
</code></pre>&#13;
    <p class="normal">Did you expect those results? While <code class="inlineCode">1</code> thread is indeed slower than <code class="inlineCode">10</code> threads and <code class="inlineCode">50</code> threads, at <code class="inlineCode">100</code> we are clearly seeing the diminishing returns and the overhead of having <code class="inlineCode">100</code> threads. Also, the benefit of using multiple threads is rather limited here due to <code class="inlineCode">socket.getaddrinfo()</code> being pretty fast.</p>&#13;
    <p class="normal">If we were to read a whole bunch of files from a slow networked filesystem or if we were to use it to fetch multiple webpages in parallel, we would see a much larger difference. That immediately shows the downside of threading: it only gives you a benefit if the external resource is slow enough to warrant the synchronization overhead. With a fast external resource, you are likely to experience slowdowns instead because the GIL becomes the bottleneck. CPython can only execute a single statement at once so that can quickly become problematic.</p>&#13;
    <p class="normal">When it comes to <a id="_idIndexMarker1151"/>performance, you should always run a <a id="_idIndexMarker1152"/>benchmark to see what works best for your case, especially when it comes to thread count. As you saw in the earlier example, more is not always better and the 100-thread version is many times slower than even the single-threaded version.</p>&#13;
    <p class="normal">So, what if we try the same using processes instead of threads? For brevity, we will skip the actual code since we effectively only need to swap out <code class="inlineCode">concurrent.futures.ThreadPoolExecutor()</code> with <code class="inlineCode">concurrent.futures.ProcessPoolExecutor()</code> and we are done. The tested code can be found on GitHub if you are interested. When we execute that code, we get these results:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> python3 T_08_process_batch_processing.py&#13;
Testing with 1 processes and n=10 took: 2.1&#13;
Testing with 10 processes and n=10 took: 3.2&#13;
Testing with 50 processes and n=10 took: 8.3&#13;
Testing with 100 processes and n=10 took: 15.0&#13;
</code></pre>&#13;
    <p class="normal">As you can see, we got universally slower results when using multiple processes. While multiprocessing can offer a lot of benefit when the GIL or a single CPU core is the limit, the overhead can cost you performance in other scenarios.</p>&#13;
    <h2 id="_idParaDest-400" class="heading-2">Batch processing using multiprocessing</h2>&#13;
    <p class="normal">In the previous section, we <a id="_idIndexMarker1153"/>saw how we can use <code class="inlineCode">concurrent.futures</code> to do <a id="_idIndexMarker1154"/>batch processing. You might be wondering why we would want to use <code class="inlineCode">multiprocessing</code> directly if <code class="inlineCode">concurrent.futures</code> can handle it for us. The reason is rather simple: <code class="inlineCode">concurrent.futures</code> is an easy-to-use and very simple interface to both <code class="inlineCode">threading</code> and <code class="inlineCode">multiprocessing</code>, but <code class="inlineCode">multiprocessing</code> offers several advanced options that can be very convenient and can even help your performance in some scenarios. </p>&#13;
    <p class="normal">In the previous examples we only saw <code class="inlineCode">multiprocessing.Process</code>, which is the process analog to <code class="inlineCode">threading.Thread</code>. In this case, however, we will be using <code class="inlineCode">multiprocessing.Pool</code>, which creates a process pool very similar to the <code class="inlineCode">concurrent.futures</code> executors but offers several additional features:</p>&#13;
    <ul>&#13;
      <li class="bulletList"><code class="inlineCode">map_async(func, iterable, [..., callback, ...])</code>&#13;
        <p class="normal">The <code class="inlineCode">map_async()</code> method is similar to the <code class="inlineCode">map()</code> method in <code class="inlineCode">concurrent.futures</code>, but instead of blocking it returns a list of <code class="inlineCode">AsyncResult</code> objects so you can fetch the results when you need them.</p>&#13;
      </li>&#13;
      <li class="bulletList"><code class="inlineCode">imap(func, iterable[, chunksize])</code>&#13;
        <p class="normal">The <code class="inlineCode">imap()</code> method is effectively the generator version of <code class="inlineCode">map()</code>. It works in roughly the same way, but it doesn’t preload the items from the iterable so you can safely process large iterables if needed. This can be <em class="italic">much</em> faster if you need to process many items.</p>&#13;
      </li>&#13;
      <li class="bulletList"><code class="inlineCode">imap_unordered(func, iterable[, chunksize])</code>&#13;
        <p class="normal">The <code class="inlineCode">imap_unordered()</code> method is effectively the same as <code class="inlineCode">imap()</code> except that it returns the results as soon as they are processed, which can improve performance even further. If the order of your results is of no importance to you, give it a try as it can make your code even faster.</p>&#13;
      </li>&#13;
      <li class="bulletList"><code class="inlineCode">starmap(func, iterable[, chunksize])</code>&#13;
        <p class="normal">The <code class="inlineCode">starmap()</code> method is very similar to the <code class="inlineCode">map()</code> method, but supports multiple arguments by passing them like <code class="inlineCode">*args</code>. If you were to run <code class="inlineCode">starmap(function, [(1, 2), (3, 4)])</code>, the <code class="inlineCode">starmap()</code> method would call <code class="inlineCode">function(1, 2)</code> and <code class="inlineCode">function(3, 4)</code>. This can be really useful in conjunction with <code class="inlineCode">zip()</code> to combine several lists of arguments.</p>&#13;
      </li>&#13;
      <li class="bulletList"><code class="inlineCode">starmap_async(func, iterable, [..., callback, ...])</code>&#13;
        <p class="normal">As you can imagine, <code class="inlineCode">starmap_async()</code> is effectively the non-blocking <code class="inlineCode">starmap()</code> method, but it returns a list of <code class="inlineCode">AsyncResult</code> objects so you can fetch them at your convenience.</p>&#13;
      </li>&#13;
    </ul>&#13;
    <p class="normal">The <a id="_idIndexMarker1155"/>usage of <code class="inlineCode">multiprocessing.Pool()</code> is largely<a id="_idIndexMarker1156"/> analogous to <code class="inlineCode">concurrent.future.SomeExecutor()</code> beyond the extra methods mentioned above. Depending on your scenario, it can be slower, a similar speed, or faster than <code class="inlineCode">concurrent.futures</code>, so always make sure to benchmark for your specific use case. This little bit of benchmark code should give you a nice starting point:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> timeit&#13;
<span class="hljs-keyword">import</span> functools&#13;
<span class="hljs-keyword">import</span> multiprocessing&#13;
<span class="hljs-keyword">import</span> concurrent.futures&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">triangle_number</span>(<span class="hljs-params">n</span>):&#13;
    total = <span class="hljs-number">0</span>&#13;
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n + <span class="hljs-number">1</span>):&#13;
        total += i&#13;
&#13;
    <span class="hljs-keyword">return</span> total&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">bench_mp</span>(<span class="hljs-params">n, count, chunksize</span>):&#13;
    <span class="hljs-keyword">with</span> multiprocessing.Pool() <span class="hljs-keyword">as</span> pool:&#13;
        <span class="hljs-comment"># Generate a generator like [n, n, n, ..., n, n]</span>&#13;
        iterable = (n <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(count))&#13;
        <span class="hljs-built_in">list</span>(pool.imap_unordered(triangle_number, iterable,&#13;
                                 chunksize=chunksize))&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">bench_ft</span>(<span class="hljs-params">n, count, chunksize</span>):&#13;
    <span class="hljs-keyword">with</span> concurrent.futures.ProcessPoolExecutor() <span class="hljs-keyword">as</span> executor:&#13;
        <span class="hljs-comment"># Generate a generator like [n, n, n, ..., n, n]</span>&#13;
        iterable = (n <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(count))&#13;
        <span class="hljs-built_in">list</span>(executor.<span class="hljs-built_in">map</span>(triangle_number, iterable,&#13;
                          chunksize=chunksize))&#13;
&#13;
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    timer = functools.partial(timeit.timeit, number=<span class="hljs-number">5</span>)&#13;
&#13;
    n = <span class="hljs-number">1000</span>&#13;
    chunksize = <span class="hljs-number">50</span>&#13;
    <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> (<span class="hljs-number">100</span>, <span class="hljs-number">1000</span>, <span class="hljs-number">10000</span>):&#13;
        <span class="hljs-comment"># Using &lt;6 formatting for consistent alignment</span>&#13;
        args = <span class="hljs-string">', '</span>.join((&#13;
            <span class="hljs-string">f'n=</span><span class="hljs-subst">{n:&lt;</span><span class="hljs-number">6</span><span class="hljs-subst">}</span><span class="hljs-string">'</span>,&#13;
            <span class="hljs-string">f'count=</span><span class="hljs-subst">{count:&lt;</span><span class="hljs-number">6</span><span class="hljs-subst">}</span><span class="hljs-string">'</span>,&#13;
            <span class="hljs-string">f'chunksize=</span><span class="hljs-subst">{chunksize:&lt;</span><span class="hljs-number">6</span><span class="hljs-subst">}</span><span class="hljs-string">'</span>,&#13;
        ))&#13;
        time_mp = timer(&#13;
            <span class="hljs-string">f'bench_mp(</span><span class="hljs-subst">{args}</span><span class="hljs-string">)'</span>,&#13;
            setup=<span class="hljs-string">'from __main__ import bench_mp'</span>,&#13;
        )&#13;
        time_ft = timer(&#13;
            <span class="hljs-string">f'bench_ft(</span><span class="hljs-subst">{args}</span><span class="hljs-string">)'</span>,&#13;
            setup=<span class="hljs-string">'from __main__ import bench_ft'</span>,&#13;
        )&#13;
&#13;
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f'</span><span class="hljs-subst">{args}</span><span class="hljs-string"> mp: </span><span class="hljs-subst">{time_mp:</span><span class="hljs-number">.2</span><span class="hljs-subst">f}</span><span class="hljs-string">, ft: </span><span class="hljs-subst">{time_ft:</span><span class="hljs-number">.2</span><span class="hljs-subst">f}</span><span class="hljs-string">'</span>)&#13;
</code></pre>&#13;
    <p class="normal">On my machine, this gives the following results:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$</span> python3 T_09_multiprocessing_pool.py&#13;
n=1000  , count=100   , chunksize=50     mp: 0.71, ft: 0.42&#13;
n=1000  , count=1000  , chunksize=50     mp: 0.76, ft: 0.96&#13;
n=1000  , count=10000 , chunksize=50     mp: 1.12, ft: 1.40&#13;
</code></pre>&#13;
    <p class="normal">Before I <a id="_idIndexMarker1157"/>benchmarked this, I was not expecting <code class="inlineCode">concurrent.futures</code> to be that much faster in some cases and that much slower in other <a id="_idIndexMarker1158"/>cases. Analyzing these results, you can see that processing 1,000 items with <code class="inlineCode">concurrent.futures</code> took more time than processing 10,000 items with multiprocessing in this particular case. Similarly, for 100 items the <code class="inlineCode">multiprocessing</code> module was nearly twice as slow. Naturally, every run yields different results and there is not a single option that will perform well for every scenario, but it is something to keep in mind.</p>&#13;
    <p class="normal">Now that we know how to run our code in multiple threads or processes, let’s look at how we can safely share data between the threads/processes.</p>&#13;
    <h1 id="_idParaDest-401" class="heading-1">Sharing data between threads and processes</h1>&#13;
    <p class="normal">Data sharing is <a id="_idIndexMarker1159"/>really the most difficult part about multiprocessing, multithreading, and distributed programming in general: which data to pass along, which data to share, and which data to skip. The theory is really simple, however: whenever possible, don’t transfer any data, don’t share any data, and keep everything local. This is essentially the <strong class="keyWord">functional programming</strong> paradigm, which is why functional programming mixes really well with multiprocessing. In practice, regrettably, this is simply not always possible. The <code class="inlineCode">multiprocessing</code> library has several options to share data, but internally they break down to two different options:</p>&#13;
    <ul>&#13;
      <li class="bulletList"><strong class="keyWord">Shared memory</strong>: This is by <a id="_idIndexMarker1160"/>far the fastest solution since it has very little overhead, but it can only be used for immutable types and is restricted to a select few types and custom objects that are created through <code class="inlineCode">multiprocessing.sharedctypes</code>. This is a fantastic solution if you only need to store primitive types such as <code class="inlineCode">int</code>, <code class="inlineCode">float</code>, <code class="inlineCode">bool</code>, <code class="inlineCode">str</code>, <code class="inlineCode">bytes</code>, and/or fixed-sized lists or dicts (where the children are primitives).</li>&#13;
      <li class="bulletList"><code class="inlineCode">multiprocessing.Manager</code>: The <code class="inlineCode">Manager</code> classes offer a host of different options for storing and synchronizing <a id="_idIndexMarker1161"/>data, such as locks, semaphores, queues, lists, dicts, and several others. If it can be pickled, it can work with a manager.</li>&#13;
    </ul>&#13;
    <p class="normal">For threading, the solution is even easier: all memory is shared so, by default, all objects are available from every thread. There is an exception called a thread-local variable, which we will see later.</p>&#13;
    <p class="normal">Sharing memory <a id="_idIndexMarker1162"/>brings its own caveats, however, as we will see in the <em class="italic">Thread safety</em> section in the case of <code class="inlineCode">threading</code>. Since multiple threads and/or processes can write to the same piece of memory at the same time, this is an inherently risky operation. At best, your changes can become lost due to conflicting writes; at worst, your memory could become corrupted, which could even result in a crashing interpreter. Luckily, Python is pretty good at protecting you, so if you are not doing anything too exotic you do not have to worry about crashing your interpreter.</p>&#13;
    <h2 id="_idParaDest-402" class="heading-2">Shared memory between processes</h2>&#13;
    <p class="normal">Python offers several different <a id="_idIndexMarker1163"/>structures to make memory sharing between processes a safe operation:</p>&#13;
    <ul>&#13;
      <li class="bulletList"><code class="inlineCode">multiprocessing.Value </code></li>&#13;
      <li class="bulletList"><code class="inlineCode">multiprocessing.Array </code></li>&#13;
      <li class="bulletList"><code class="inlineCode">multiprocessing.shared_memory.SharedMemory </code></li>&#13;
      <li class="bulletList"><code class="inlineCode">multiprocessing.shared_memory.ShareableList </code></li>&#13;
    </ul>&#13;
    <p class="normal">Let’s dive into a few of these types to demonstrate how to use them.</p>&#13;
    <p class="normal">For sharing primitive values, you can use <code class="inlineCode">multiprocessing.Value</code> and <code class="inlineCode">multiprocessing.Array</code>. These are essentially the same, but with <code class="inlineCode">Array</code> you can store multiple values whereas <code class="inlineCode">Value</code> is just a single value. As arguments, these expect a typecode identical to how the <code class="inlineCode">array</code> module works in Python, which means they map to C types. This results in <code class="inlineCode">d</code> as a double (floating point) number, <code class="inlineCode">i</code> as a signed integer, <code class="inlineCode">b</code> as a signed char, etc. </p>&#13;
    <div class="note">&#13;
      <p class="normal">For more options, look at the documentation for<a id="_idIndexMarker1164"/> the <code class="inlineCode">array</code> module: <a href="https://docs.python.org/3/library/array.html"><span class="url">https://docs.python.org/3/library/array.html</span></a>.</p>&#13;
    </div>&#13;
    <p class="normal">For more advanced types, you can take a look at the <code class="inlineCode">multiprocessing.sharedctypes</code> module, which is also where the <code class="inlineCode">Value</code> and <code class="inlineCode">Array</code> classes originate from.</p>&#13;
    <p class="normal">Both <code class="inlineCode">multiprocessing.Value</code> and <code class="inlineCode">multiprocessing.Array</code> are not difficult to use, but they do not feel very<a id="_idIndexMarker1165"/> Pythonic to me:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> multiprocessing&#13;
&#13;
some_int = multiprocessing.Value(<span class="hljs-string">'i'</span>, <span class="hljs-number">123</span>)&#13;
<span class="hljs-keyword">with</span> some_int.get_lock():&#13;
    some_int.value += <span class="hljs-number">10</span>&#13;
<span class="hljs-built_in">print</span>(some_int.value)&#13;
&#13;
some_double_array = multiprocessing.Array(<span class="hljs-string">'d'</span>, [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])&#13;
<span class="hljs-keyword">with</span> some_double_array.get_lock():&#13;
    some_double_array[<span class="hljs-number">0</span>] += <span class="hljs-number">2.5</span>&#13;
<span class="hljs-built_in">print</span>(some_double_array[:])&#13;
</code></pre>&#13;
    <p class="normal">If you need to share memory and performance is important to you, feel free to use them. If possible, however, I would avoid them (or sharing memory at all if possible) as the usage is clunky at best.</p>&#13;
    <p class="normal">The <code class="inlineCode">multiprocessing.shared_memory.SharedMemory</code> object is similar to the <code class="inlineCode">Array</code> but it is a lower-level structure. It offers you an interface to read/write to an optionally <strong class="keyWord">named</strong> block of memory so you can access it from other processes by name as well. Additionally, when you are done using it you <em class="italic">must</em> call <code class="inlineCode">unlink()</code> to release the memory:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> multiprocessing <span class="hljs-keyword">import</span> shared_memory&#13;
&#13;
<span class="hljs-comment"># From process A we could write something</span>&#13;
name = <span class="hljs-string">'share_a'</span>&#13;
share_a = shared_memory.SharedMemory(name, create=<span class="hljs-literal">True</span>, size=<span class="hljs-number">4</span>)&#13;
share_a.buf[<span class="hljs-number">0</span>] = <span class="hljs-number">10</span>&#13;
&#13;
<span class="hljs-comment"># From a different process, or the same one, we can access the data</span>&#13;
share_a = shared_memory.SharedMemory(name)&#13;
<span class="hljs-built_in">print</span>(share_a.buf[<span class="hljs-number">0</span>])&#13;
&#13;
<span class="hljs-comment"># Make sure to clean up after. And only once!</span>&#13;
share_a.unlink()&#13;
</code></pre>&#13;
    <p class="normal">As we can see in this <a id="_idIndexMarker1166"/>example, the first call had a <code class="inlineCode">create=True</code> parameter to ask the operating system for memory. Only after that (and before calling <code class="inlineCode">unlink()</code>) can we reference the block from other (or the same) processes.</p>&#13;
    <p class="normal">Once again it is not the most Pythonic interface, but it can be effective for sharing memory. Since the name is optional and automatically generated otherwise, you could omit it from the creation of the shared memory block and read it back from <code class="inlineCode">share_a.name</code>. Also, like the <code class="inlineCode">Array</code> and <code class="inlineCode">Value</code> objects, this too has a fixed size and cannot be grown without replacing it.</p>&#13;
    <p class="normal">Lastly, we have the <code class="inlineCode">multiprocessing.shared_memory.ShareableList</code> object. While this object is slightly more convenient than <code class="inlineCode">Array</code> and <code class="inlineCode">SharedMemory</code> since it allows you to be flexible with types (i.e. <code class="inlineCode">item[0]</code> could be a <code class="inlineCode">str</code> and <code class="inlineCode">item[1]</code> could be an <code class="inlineCode">int</code>), it is still a hard-to-use interface and it does not allow you to resize it. While you can change the type for the items, you cannot resize the object, so swapping out a number with a larger string will not work. At least its usage is more Pythonic than the other options:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> multiprocessing <span class="hljs-keyword">import</span> shared_memory&#13;
&#13;
shared_list = shared_memory.ShareableList([<span class="hljs-string">'Hi'</span>, <span class="hljs-number">1</span>, <span class="hljs-literal">False</span>, <span class="hljs-literal">None</span>])&#13;
<span class="hljs-comment"># Changing type from str to bool here</span>&#13;
shared_list[<span class="hljs-number">0</span>] = <span class="hljs-literal">True</span>&#13;
<span class="hljs-comment"># Don't forget to unlink()</span>&#13;
shared_list.shm.unlink()&#13;
</code></pre>&#13;
    <p class="normal">Seeing all of these options for sharing memory between processes, should you be using them? Yes, if you need high performance, that is.</p>&#13;
    <p class="normal">It should be a good indication of why it is best to keep memory local with parallel processing, however. Sharing memory between processes is a complicated problem to solve. Even with these methods, which are the fastest and least complicated available, it is a bit of a pain already.</p>&#13;
    <p class="normal">So, how much performance<a id="_idIndexMarker1167"/> impact does memory sharing have? Let’s run a few benchmarks to see the difference between sharing a variable and returning it for post-processing. First, the version that does not use shared memory as a performance base:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> multiprocessing&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">triangle_number_local</span>(<span class="hljs-params">n</span>):&#13;
    total = <span class="hljs-number">0</span>&#13;
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n + <span class="hljs-number">1</span>):&#13;
        total += i&#13;
&#13;
    <span class="hljs-keyword">return</span> total&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">bench_local</span>(<span class="hljs-params">n, count</span>):&#13;
    <span class="hljs-keyword">with</span> multiprocessing.Pool() <span class="hljs-keyword">as</span> pool:&#13;
        results = pool.imap_unordered(&#13;
            triangle_number_local,&#13;
            (n <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(count)),&#13;
        )&#13;
        <span class="hljs-built_in">print</span>(<span class="hljs-string">'Sum:'</span>, <span class="hljs-built_in">sum</span>(results))&#13;
</code></pre>&#13;
    <p class="normal">The <code class="inlineCode">triangle_number_local()</code> function calculates the sum of all numbers up to and including <code class="inlineCode">n</code> and returns it, similar to a factorial function but with addition instead. </p>&#13;
    <p class="normal">The <code class="inlineCode">bench_local()</code> function calls the <code class="inlineCode">triangle_number_local()</code> function <code class="inlineCode">count</code> times and stores the results. After that, we <code class="inlineCode">sum()</code> those results to verify the output.</p>&#13;
    <p class="normal">Now let’s look at the version using shared memory:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> multiprocessing&#13;
&#13;
<span class="hljs-keyword">class</span> <span class="hljs-title">Shared</span>:&#13;
    <span class="hljs-keyword">pass</span>&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">initializer</span>(<span class="hljs-params">shared_value</span>):&#13;
    Shared.value = shared_value&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">triangle_number_shared</span>(<span class="hljs-params">n</span>):&#13;
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n + <span class="hljs-number">1</span>):&#13;
        <span class="hljs-keyword">with</span> Shared.value.get_lock():&#13;
            Shared.value.value += i&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">bench_shared</span>(<span class="hljs-params">n, count</span>):&#13;
    shared_value = multiprocessing.Value(<span class="hljs-string">'i'</span>, <span class="hljs-number">0</span>)&#13;
&#13;
    <span class="hljs-comment"># We need to explicitly share the shared_value. On Unix you</span>&#13;
    <span class="hljs-comment"># can work around this by forking the process, on Windows it</span>&#13;
    <span class="hljs-comment"># would not work otherwise</span>&#13;
    pool = multiprocessing.Pool(&#13;
        initializer=initializer,&#13;
        initargs=(shared_value,),&#13;
    )&#13;
&#13;
    iterable = (n <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(count))&#13;
    <span class="hljs-built_in">list</span>(pool.imap_unordered(triangle_number_shared, iterable))&#13;
    <span class="hljs-built_in">print</span>(<span class="hljs-string">'Sum:'</span>, shared_value.value)&#13;
&#13;
    pool.close()&#13;
</code></pre>&#13;
    <p class="normal">In this case we have<a id="_idIndexMarker1168"/> created a <code class="inlineCode">Shared</code> class as a namespace to store the shared variable, but a <code class="inlineCode">global</code> variable would also be an option.</p>&#13;
    <p class="normal">To make sure the shared variable is available, we need to send it along to all workers in the <code class="inlineCode">pool</code> using an <code class="inlineCode">initializer</code> method argument.</p>&#13;
    <p class="normal">Additionally, as the <code class="inlineCode">+=</code> operation is not atomic (not a single operation, since it does <em class="italic">fetch, add, set</em>), we need to make sure to lock the variable using the <code class="inlineCode">get_lock()</code> method. </p>&#13;
    <p class="normal">The <em class="italic">Thread safety</em> section later in this chapter goes into more detail about when locking is and is not needed.</p>&#13;
    <p class="normal">To run the benchmarks, we use the following code:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> timeit&#13;
&#13;
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    n = <span class="hljs-number">1000</span>&#13;
    count = <span class="hljs-number">100</span>&#13;
    number = <span class="hljs-number">5</span>&#13;
&#13;
    <span class="hljs-keyword">for</span> function <span class="hljs-keyword">in</span> <span class="hljs-string">'</span><span class="hljs-string">bench_local'</span>, <span class="hljs-string">'bench_shared'</span>:&#13;
        statement = <span class="hljs-string">f'</span><span class="hljs-subst">{function}</span><span class="hljs-string">(n=</span><span class="hljs-subst">{n}</span><span class="hljs-string">, count=</span><span class="hljs-subst">{count}</span><span class="hljs-string">)'</span>&#13;
        result = timeit.timeit(&#13;
            statement, number=number,&#13;
            setup=<span class="hljs-string">f'from __main__ import </span><span class="hljs-subst">{function}</span><span class="hljs-string">'</span>,&#13;
        )&#13;
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f'</span><span class="hljs-subst">{statement}</span><span class="hljs-string">: </span><span class="hljs-subst">{result:</span><span class="hljs-number">.3</span><span class="hljs-subst">f}</span><span class="hljs-string">'</span>)&#13;
</code></pre>&#13;
    <p class="normal">Now when executing this, we see the reason for not sharing memory if possible:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con">bench_local(n=1000, count=100): 0.598&#13;
bench_shared(n=1000, count=100): 4.157&#13;
</code></pre>&#13;
    <p class="normal">The code using shared memory is roughly 8 times slower, which makes sense because my machine has 8 cores. Since the shared memory example spends most of its time with locking/unlocking (which can only be done by one process at the same time), we have effectively made the code run on a single core again.</p>&#13;
    <p class="normal">I should point out that this<a id="_idIndexMarker1169"/> is pretty much the worst-case scenario for shared memory. Since all the functions do is write to the shared variable, most of the time is spent locking and unlocking the variables. If you were to do actual processing in the function and only write the results, it would be much better already.</p>&#13;
    <p class="normal">You might be curious about how we could rewrite this example the right way while still using shared variables. In this case it is rather easy, but this largely depends on your specific use case and this might not work for you:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">triangle_number_shared_efficient</span>(<span class="hljs-params">n</span>):&#13;
    total = <span class="hljs-number">0</span>&#13;
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n + <span class="hljs-number">1</span>):&#13;
        total += i&#13;
&#13;
    <span class="hljs-keyword">with</span> Shared.value.get_lock():&#13;
        Shared.value.value += total&#13;
</code></pre>&#13;
    <p class="normal">This code runs almost as fast as the <code class="inlineCode">bench_local()</code> function. As a rule of thumb, just remember to reduce the number of locks and writes as much as possible. </p>&#13;
    <p class="normal">Sharing data between processes using managers</p>&#13;
    <p class="normal">Now that we have seen <a id="_idIndexMarker1170"/>how we can directly share memory <a id="_idIndexMarker1171"/>to get the best performance possible, let’s look at a far more convenient and much more flexible solution: the <code class="inlineCode">multiprocessing.Manager</code> class.</p>&#13;
    <p class="normal">Whereas shared memory restricted us to primitive types, with a <code class="inlineCode">Manager</code> we can share anything that can be pickled in a very easy way if we are willing to sacrifice a little bit of performance. The mechanism it uses is very different, though; it connects through a network connection. The huge advantage of this method is that you can even use this across multiple devices (which we will see later in this chapter).</p>&#13;
    <p class="normal">The <code class="inlineCode">Manager</code> itself is not an object you will use much, though you will probably use the objects provided by the <code class="inlineCode">Manager</code>. The list is plentiful so we will only cover a few in detail, but you can always look at the Python documentation for the current list of options: <a href="https://docs.python.org/3/library/multiprocessing.html#managers"><span class="url">https://docs.python.org/3/library/multiprocessing.html#managers</span></a>.</p>&#13;
    <p class="normal">One of the most convenient options for sharing data with <code class="inlineCode">multiprocessing</code> is the <code class="inlineCode">multiprocessing.Namespace</code> object. The <code class="inlineCode">Namespace</code> object behaves very similarly to a regular object, with the difference being that it can be accessed as a shared memory object from all processes. As<a id="_idIndexMarker1172"/> long as your objects can be<a id="_idIndexMarker1173"/> pickled, you can use them as attributes of a <code class="inlineCode">Namespace</code> instance. To illustrate the usage of the <code class="inlineCode">Namespace</code>:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> multiprocessing&#13;
&#13;
manager = multiprocessing.Manager()&#13;
namespace = manager.Namespace()&#13;
&#13;
namespace.spam = <span class="hljs-number">123</span>&#13;
namespace.eggs = <span class="hljs-number">456</span>&#13;
</code></pre>&#13;
    <p class="normal">As you can see in this example, you can simply set the attributes of <code class="inlineCode">namespace</code> as you would expect from regular objects, but they are shared between all processes. Since the locking now happens through network sockets, the overhead is even larger than with shared memory, so only write data when you must. Directly translating the earlier shared memory example to use a <code class="inlineCode">Namespace</code> and explicit <code class="inlineCode">Lock</code> (a <code class="inlineCode">Namespace</code> does not have a <code class="inlineCode">get_lock()</code> method) yields the following code:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">triangle_number_namespace</span>(<span class="hljs-params">namespace, lock, n</span>):&#13;
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n + <span class="hljs-number">1</span>):&#13;
        <span class="hljs-keyword">with</span> lock:&#13;
            namespace.total += i&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">bench_manager</span>(<span class="hljs-params">n, count</span>):&#13;
    manager = multiprocessing.Manager()&#13;
    namespace = manager.Namespace()&#13;
    namespace.total = <span class="hljs-number">0</span>&#13;
    lock = manager.Lock()&#13;
    <span class="hljs-keyword">with</span> multiprocessing.Pool() <span class="hljs-keyword">as</span> pool:&#13;
        <span class="hljs-built_in">list</span>(pool.starmap(&#13;
            triangle_number_namespace,&#13;
            ((namespace, lock, n) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(count)),&#13;
        ))&#13;
        <span class="hljs-built_in">print</span>(<span class="hljs-string">'Sum:'</span>, namespace.total)&#13;
</code></pre>&#13;
    <p class="normal">As with the shared memory example, this is a really inefficient case because we are locking for each iteration of the loop, and it really shows. While the local version took about 0.6 seconds and the shared memory version took about 4 seconds, this version takes a whopping 90 seconds for effectively the same operation.</p>&#13;
    <p class="normal">Once again, we can easily speed it up by reducing the time spent in the synchronized/locked code:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">triangle_number_namespace_efficient</span>(<span class="hljs-params">namespace, lock, n</span>):&#13;
    total = <span class="hljs-number">0</span>&#13;
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n + <span class="hljs-number">1</span>):&#13;
        total += i&#13;
&#13;
    <span class="hljs-keyword">with</span> lock:&#13;
        namespace.total += i&#13;
</code></pre>&#13;
    <p class="normal">When benchmarking this<a id="_idIndexMarker1174"/> version with the same <a id="_idIndexMarker1175"/>benchmark code as before, we can see that it is still much slower than the 0.6 seconds we got with the local version:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con">bench_local(n=1000, count=100): 0.637&#13;
bench_manager(n=1000, count=100): 1.476&#13;
</code></pre>&#13;
    <p class="normal">That being said, at least this is much more acceptable than the 90 seconds we would get otherwise.</p>&#13;
    <p class="normal">Why are these locks so incredibly slow? For a proper lock to be set, all the parties need to agree that the data is locked, which is a process that takes time. That simple fact slows down execution much more than most people would expect. The server/process that runs the <code class="inlineCode">Manager</code> needs to confirm to the client that it has the lock; only once that has been done can the client read, write, and release the lock again.</p>&#13;
    <p class="normal">On a regular hard disk setup, database servers aren’t able to handle more than about 10 transactions per second <em class="italic">on the same row</em> due to locking and disk latency. Using lazy file syncing, SSDs, and a battery-backed RAID cache, that performance can be increased to handle, perhaps, 100 transactions per second on the same row. These are simple hardware limitations; because you have multiple processes trying to write to a single target, you need to synchronize the actions between the processes, and that takes a lot of time.</p>&#13;
    <p class="normal">Even with the fastest <a id="_idIndexMarker1176"/>hardware available, synchronization <a id="_idIndexMarker1177"/>can lock all the processes and produce enormous slowdowns, so if at all possible, try to avoid sharing data between multiple processes. Put simply, if all the processes are constantly reading and writing from/to the same object, it is generally faster to use a single process instead because the locking will effectively restrict you to a single process anyway.</p>&#13;
    <p class="normal">Redis, one of the fastest data storage systems available, was fully single-threaded for over a decade until 2020 because the locking overhead was not worth the benefit. Even the current threaded version is effectively a collection of single-threaded servers with their own memory space to avoid locking.</p>&#13;
    <h2 id="_idParaDest-403" class="heading-2">Thread safety</h2>&#13;
    <p class="normal">When working with threads or<a id="_idIndexMarker1178"/> processes, you need to be aware that you might not be the only one modifying a variable at some point in time. There are many scenarios where this will not be an issue and often you are lucky and it won’t affect you, but when it does it can cause bugs that are extremely difficult to debug.</p>&#13;
    <p class="normal">As an example, imagine having two bits of code incrementing a number at the same time and imagine what could go wrong. Initially, let’s assume the value is 10. With multiple threads, this could result in the following sequence:</p>&#13;
    <ol class="numberedList" style="list-style-type: decimal;">&#13;
      <li class="numberedList" value="1">Two threads fetch the number to local memory to increment. It is currently 10 for both.</li>&#13;
      <li class="numberedList">Both threads increment the number in their local memory to 11.</li>&#13;
      <li class="numberedList">Both threads write the number back from local memory (which is 11 for both) to the global one, so the global number is now 11.</li>&#13;
    </ol>&#13;
    <p class="normal">Since both threads fetched the number at the same time, one overwrote the increment of the other with its own increment. So instead of incrementing twice, you now have a variable that was only incremented once.</p>&#13;
    <p class="normal">In many cases, the current GIL implementation in CPython will protect you from these issues when using <code class="inlineCode">threading</code>, but you should never take that protection for granted and make sure to protect your variables if multiple threads might update your variable at the same time.</p>&#13;
    <p class="normal">Perhaps an actual code example <a id="_idIndexMarker1179"/>might make the scenario a bit clearer:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> time&#13;
<span class="hljs-keyword">import</span> concurrent.futures&#13;
&#13;
counter = <span class="hljs-number">10</span>&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">increment</span>(<span class="hljs-params">name</span>):&#13;
    <span class="hljs-keyword">global</span> counter&#13;
    current_value = counter&#13;
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f'</span><span class="hljs-subst">{name}</span><span class="hljs-string"> value before increment: </span><span class="hljs-subst">{current_value}</span><span class="hljs-string">'</span>)&#13;
    counter = current_value + <span class="hljs-number">1</span>&#13;
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f'</span><span class="hljs-subst">{name}</span><span class="hljs-string"> value after increment: </span><span class="hljs-subst">{counter}</span><span class="hljs-string">'</span>)&#13;
&#13;
<span class="hljs-built_in">print</span>(<span class="hljs-string">f'Before thread start: </span><span class="hljs-subst">{counter}</span><span class="hljs-string">'</span>)&#13;
&#13;
<span class="hljs-keyword">with</span> concurrent.futures.ThreadPoolExecutor() <span class="hljs-keyword">as</span> executor:&#13;
    executor.<span class="hljs-built_in">map</span>(increment, <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>))&#13;
<span class="hljs-built_in">print</span>(<span class="hljs-string">f'After thread finish: </span><span class="hljs-subst">{counter}</span><span class="hljs-string">'</span>)&#13;
</code></pre>&#13;
    <p class="normal">As you can see, the <code class="inlineCode">increment</code> function stores <code class="inlineCode">counter</code> in a temporary variable, prints it, and writes to <code class="inlineCode">counter</code> after adding 1 to it. This example is admittedly a bit contrived because you would normally do <code class="inlineCode">counter += 1</code> instead, which reduces the odds of unexpected behaviour, but even in that case you have no guarantee that your results are correct.</p>&#13;
    <p class="normal">To illustrate the output of this script:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python3 T_12_thread_safety.py&#13;
Before thread start: 10&#13;
0 value before increment: 10&#13;
0 value after increment: 11&#13;
1 value before increment: 11&#13;
1 value after increment: 12&#13;
2 value before increment: 11&#13;
2 value after increment: 12&#13;
4 value before increment: 12&#13;
4 value after increment: 13&#13;
3 value before increment: 12&#13;
3 value after increment: 13&#13;
After thread finish: 13&#13;
</code></pre>&#13;
    <p class="normal">Why did we end up with 13 at the end? Pure luck really. Some of my attempts resulted in 15, some in 11, and others in 14. That’s what makes thread safety issues so incredibly hard to debug; in a complicated codebase, it can be really hard to figure out what is causing the bug and you cannot <a id="_idIndexMarker1180"/>reliably reproduce the issue.</p>&#13;
    <div class="packt_tip">&#13;
      <p class="normal">When experiencing strange and hard-to-explain errors in a system using multiple threads/processes, make sure to see if they also occur when running single-threaded. Mistakes like these are easily made and can easily be introduced by third-party code that was not meant to be thread-safe.</p>&#13;
    </div>&#13;
    <p class="normal">To make your code thread-safe, you have a few different options:</p>&#13;
    <ul>&#13;
      <li class="bulletList">This might seem obvious, but if you don’t update shared variables from multiple threads/processes in parallel then there is nothing to worry about.</li>&#13;
      <li class="bulletList">Use atomic operations when modifying your variables. An atomic operation is one that executes in a single instruction so no conflicts could ever arise. For example, incrementing a number could be an atomic operation where the fetching, incrementing, and updating happens in a single instruction.Within Python, an increment is usually done with <code class="inlineCode">counter += 1</code> which is actually a shorthand for <code class="inlineCode">counter = counter + 1</code>. Can you see the issue here? Instead of incrementing <code class="inlineCode">counter</code> internally, Python will write a new value to the variable <code class="inlineCode">counter</code>, which means it is not an atomic operation.</li>&#13;
      <li class="bulletList">Use locks to protect your variables.</li>&#13;
    </ul>&#13;
    <p class="normal">Knowing these options for thread-safe code, you might be wondering which operations are thread-safe and which aren’t. Luckily, Python does have some documentation about the issue, and I would strongly recommend looking at it as this is prone to change in the future: <a href="https://docs.python.org/3/faq/library.html#what-kinds-of-global-value-mutation-are-thread-safe"><span class="url">https://docs.python.org/3/faq/library.html#what-kinds-of-global-value-mutation-are-thread-safe</span></a>.</p>&#13;
    <p class="normal">For the current CPython versions (at least CPython 3.10 and below) where the GIL is protecting us, we can assume these operations to be atomic and therefore thread-safe:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code">L.append(x)&#13;
L1.extend(L2)&#13;
x = L[i]&#13;
x = L.pop()&#13;
L1[i:j] = L2&#13;
L.sort()&#13;
x = y&#13;
x.field = y&#13;
D[x] = y&#13;
D1.update(D2)&#13;
D.keys()&#13;
</code></pre>&#13;
    <p class="normal">These are not atomic and not thread-safe:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code">i = i+<span class="hljs-number">1</span>&#13;
L.append(L[-<span class="hljs-number">1</span>])&#13;
L[i] = L[j]&#13;
D[x] = D[x] + <span class="hljs-number">1</span>&#13;
</code></pre>&#13;
    <p class="normal">What could we do to<a id="_idIndexMarker1181"/> make <code class="inlineCode">i = i + 1</code> thread-safe? The most obvious solution is to use our own lock, similar to the GIL:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># This lock needs to be the same object for all threads</span>&#13;
lock = threading.Lock()&#13;
i = <span class="hljs-number">0</span>&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">increment</span>():&#13;
    <span class="hljs-keyword">global</span> i&#13;
    <span class="hljs-keyword">with</span> lock():&#13;
        i += <span class="hljs-number">1</span>&#13;
</code></pre>&#13;
    <p class="normal">As you can see, with a lock we can protect the updates for a variable easily. I should note that even though we used a <code class="inlineCode">global</code> variable in this case, the same limitation applies for the attributes of class instances and other variables as well.</p>&#13;
    <p class="normal">Naturally, this all applies to <code class="inlineCode">multiprocessing</code> as well, with the subtle difference that variables are not shared by default with multiple processes, so you need to do something to explicitly cause an issue. Having said that, the earlier shared memory and <code class="inlineCode">Manager</code> examples break immediately if you remove the locks from them.</p>&#13;
    <h2 id="_idParaDest-404" class="heading-2">Deadlocks</h2>&#13;
    <p class="normal">Now that you know <a id="_idIndexMarker1182"/>how to update your variables in a thread-safe manner, you might be hoping that we are done with threading limitations. Unfortunately, the opposite is true. The locks we used to make our variable updates thread-safe can actually introduce another issue that can be even more devious to solve: <strong class="keyWord">deadlocks</strong>.</p>&#13;
    <p class="normal">A deadlock can occur<a id="_idIndexMarker1183"/> when threads or processes are holding a lock while waiting for another thread/process to release a lock. In some cases, you can even have a thread/process that is waiting for itself. To illustrate, let’s assume that we have locks <code class="inlineCode">a</code> and <code class="inlineCode">b</code> and two different threads. Now the following occurs:</p>&#13;
    <ol class="numberedList" style="list-style-type: decimal;">&#13;
      <li class="numberedList" value="1">Thread 0 locks <code class="inlineCode">a</code></li>&#13;
      <li class="numberedList">Thread 1 locks <code class="inlineCode">b</code></li>&#13;
      <li class="numberedList">Thread 0 waits for lock <code class="inlineCode">b</code></li>&#13;
      <li class="numberedList">Thread 1 waits for lock <code class="inlineCode">a</code></li>&#13;
    </ol>&#13;
    <p class="normal">Now thread 1 is waiting for thread 0 to finish, and vice versa. Neither will ever finish because they are waiting for each other.</p>&#13;
    <p class="normal">To illustrate this scenario:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> time&#13;
<span class="hljs-keyword">import</span> threading&#13;
&#13;
a = threading.Lock()&#13;
b = threading.Lock()&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">thread_0</span>():&#13;
    <span class="hljs-built_in">print</span>(<span class="hljs-string">'thread 0 locking a'</span>)&#13;
    <span class="hljs-keyword">with</span> a:&#13;
        time.sleep(<span class="hljs-number">0.1</span>)&#13;
        <span class="hljs-built_in">print</span>(<span class="hljs-string">'thread 0 locking b'</span>)&#13;
        <span class="hljs-keyword">with</span> b:&#13;
            <span class="hljs-built_in">print</span>(<span class="hljs-string">'thread 0 everything locked'</span>)&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">thread_1</span>():&#13;
    <span class="hljs-built_in">print</span>(<span class="hljs-string">'thread 1 locking b'</span>)&#13;
    <span class="hljs-keyword">with</span> b:&#13;
        time.sleep(<span class="hljs-number">0.1</span>)&#13;
        <span class="hljs-built_in">print</span>(<span class="hljs-string">'thread 1 locking a'</span>)&#13;
        <span class="hljs-keyword">with</span> a:&#13;
            <span class="hljs-built_in">print</span>(<span class="hljs-string">'</span><span class="hljs-string">thread 1 everything locked'</span>)&#13;
&#13;
threading.Thread(target=thread_0).start()&#13;
threading.Thread(target=thread_1).start()&#13;
</code></pre>&#13;
    <p class="normal">The code is relatively straightforward but warrants at least some explanation. As previously discussed, the <code class="inlineCode">thread_0</code> function locks <code class="inlineCode">a</code> first and <code class="inlineCode">b</code> after and <code class="inlineCode">thread_1</code> does this in the reverse order. This is what causes the deadlock; they will each wait for the other to finish. To be sure we actually reach the deadlock in this example, we have a small sleep to make sure <code class="inlineCode">thread_0</code> does not finish before <code class="inlineCode">thread_1</code> starts. In real-world scenarios, you would have some code in that bit that would take time as well. </p>&#13;
    <p class="normal">How can we resolve locking issues like these? Locking strategies and resolving these issues could easily fill a chapter by themselves and there are several different types of locking problems and solutions. You could even have a livelock problem where both threads are attempting to resolve the deadlock problem at the same time with the same method, causing them to also wait for each other but with constantly changing locks.</p>&#13;
    <p class="normal">An easy way to visualize a livelock is to think of a narrow part of a road where two cars are approaching from opposite sides. Both cars would attempt to drive at the same time and both would back off when they notice that the other car is moving. Repeat that and you have a livelock.</p>&#13;
    <p class="normal">In general, there are several strategies that you can<a id="_idIndexMarker1184"/> employ to avoid deadlocks:</p>&#13;
    <ul>&#13;
      <li class="bulletList">Deadlocks can only occur when you have multiple locks, so if your code only ever acquires a single lock at the same time, no problems can occur.</li>&#13;
      <li class="bulletList">Try to keep the lock section small so there is less chance of accidentally adding another lock within that block. This can also help performance because a lock can make your parallel code essentially single-threaded again.</li>&#13;
      <li class="bulletList">This is probably the most important tip for fixing deadlocks. <em class="italic">Always have a consistent locking order.</em> If you always lock in the same order, you can never have deadlocks. Let’s explain how this helps:With the earlier example and the two locks <code class="inlineCode">a</code> and <code class="inlineCode">b</code>, the problem occurred because thread 0 was waiting for <code class="inlineCode">b</code> and thread 1 was waiting for <code class="inlineCode">a</code>.If they both had attempted to lock <code class="inlineCode">a</code> first and <code class="inlineCode">b</code> after, we never would have reached the deadlock state because one of the threads would lock <code class="inlineCode">a</code> and that would cause the other thread to stall long before <code class="inlineCode">b</code> could ever be locked.</li>&#13;
    </ul>&#13;
    <h2 id="_idParaDest-405" class="heading-2">Thread-local variables</h2>&#13;
    <p class="normal">We have seen how to lock <a id="_idIndexMarker1185"/>variables so only a single thread can modify a variable simultaneously. We have also seen how we can prevent deadlocks while using locks. What if we want to give a thread a separate global variable? That is where <code class="inlineCode">threading.local</code> comes in: it gives you a context specifically for your current thread. This can be useful for database connections, for example; you probably want to give each thread its own database connection, but having to pass around the connection is inconvenient, so a global variable or connection manager is a much more convenient option.</p>&#13;
    <p class="normal">This section does not apply to <code class="inlineCode">multiprocessing</code>, since variables are not automatically shared between processes. A forked process can inherit the variables from the parent, however, so care must be taken to explicitly initialize non-shared resources.</p>&#13;
    <p class="normal">Let’s illustrate the usage of thread-local variables with a small example:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> threading&#13;
<span class="hljs-keyword">import</span> concurrent.futures&#13;
&#13;
context = threading.local()&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">init_counter</span>():&#13;
    context.counter = <span class="hljs-number">10</span>&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">increment</span>(<span class="hljs-params">name</span>):&#13;
    current_value = context.counter&#13;
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f'</span><span class="hljs-subst">{name}</span><span class="hljs-string"> value before increment: </span><span class="hljs-subst">{current_value}</span><span class="hljs-string">'</span>)&#13;
    context.counter = current_value + <span class="hljs-number">1</span>&#13;
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f'</span><span class="hljs-subst">{name}</span><span class="hljs-string"> value after increment: </span><span class="hljs-subst">{context.counter}</span><span class="hljs-string">'</span>)&#13;
&#13;
init_counter()&#13;
<span class="hljs-built_in">print</span>(<span class="hljs-string">f'Before thread start: </span><span class="hljs-subst">{context.counter}</span><span class="hljs-string">'</span>)&#13;
&#13;
<span class="hljs-keyword">with</span> concurrent.futures.ThreadPoolExecutor(&#13;
        initializer=init_counter) <span class="hljs-keyword">as</span> executor:&#13;
    executor.<span class="hljs-built_in">map</span>(increment, <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>))&#13;
&#13;
<span class="hljs-built_in">print</span>(<span class="hljs-string">f'After thread finish: </span><span class="hljs-subst">{context.counter}</span><span class="hljs-string">'</span>)&#13;
</code></pre>&#13;
    <p class="normal">This example is largely the same as the thread-safety example, but instead of having a global <code class="inlineCode">counter</code> variable, we are now using <code class="inlineCode">threading.local()</code> as a context to set the <code class="inlineCode">counter</code> variable to. We are also using an extra feature of the <code class="inlineCode">concurrent.futures.ThreadPoolExecutor</code> here, the <code class="inlineCode">initializer</code> function. Since a thread-local variable only exists within that thread and is not automatically copied to other threads, all threads (including the main thread) need to have <code class="inlineCode">counter</code> set separately. Without setting it, we would get an <code class="inlineCode">AttributeError</code>.</p>&#13;
    <p class="normal">When running the code, we can see that all threads are independently updating their variables instead of the completely mixed version we saw in the thread-safety example:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python3 T_15_thread_local.py&#13;
Before thread start: 10&#13;
0 value before increment: 10&#13;
0 value after increment: 11&#13;
1 value before increment: 10&#13;
2 value before increment: 11&#13;
1 value after increment: 11&#13;
3 value before increment: 10&#13;
3 value after increment: 11&#13;
2 value after increment: 12&#13;
4 value before increment: 10&#13;
4 value after increment: 11&#13;
After thread finish: 10&#13;
</code></pre>&#13;
    <p class="normal">If possible, I would <a id="_idIndexMarker1186"/>always recommend returning variables from a thread or appending them to a post-processing queue and never updating a global variable or global state because it is faster and less error-prone. Using thread-local variables can really help you in these cases to make sure you have only one instance of a connection or collection class.</p>&#13;
    <p class="normal">Now that we know how to share (or stop sharing) variables, it is time to learn about the advantages and disadvantages of using threads as opposed to processes. We should have a basic grasp of memory management with threads and processes now. With all of these options, what should we choose and why?</p>&#13;
    <h1 id="_idParaDest-406" class="heading-1">Processes, threads, or a single thread?</h1>&#13;
    <p class="normal">Now that we know how to use <code class="inlineCode">multiprocessing</code>, <code class="inlineCode">threading</code> and <code class="inlineCode">concurrent.futures</code>, which should you choose for your case?</p>&#13;
    <p class="normal">Since <code class="inlineCode">concurrent.futures</code> implements both <code class="inlineCode">threading,</code> and <code class="inlineCode">multiprocessing</code>, you can mentally exchange <code class="inlineCode">threading</code> in this section with <code class="inlineCode">concurrent.futures.ThreadPoolExecutor</code>. The same goes for <code class="inlineCode">multiprocessing</code> and <code class="inlineCode">concurrent.futures.ProcessPoolExecutor</code>, of course.</p>&#13;
    <p class="normal">When we consider the choice between single-threaded, multithreaded, and multiprocess, there are multiple factors that we can consider.</p>&#13;
    <p class="normal">The first and most important question you should ask yourself is whether you really need to use <code class="inlineCode">threading</code> or <code class="inlineCode">multiprocessing</code>. Often, code is fast enough and you should ask yourself if the<a id="_idIndexMarker1187"/> cost of dealing with the potential side effects of memory sharing and such is worth it. Not only does writing code become more complicated when parallel <a id="_idIndexMarker1188"/>processing is involved, but the complexity of debugging is multiplied as well.</p>&#13;
    <p class="normal">Second, you should ask yourself what is limiting your performance. If the limitation is external I/O, then it could be useful to use <code class="inlineCode">asyncio</code> or <code class="inlineCode">threading</code> to handle that, but it is still no guarantee.</p>&#13;
    <p class="normal">For example, if you are reading a bunch of files from a slow hard disk, threading might not even help you. If the hard disk is the limiting factor, it will not become faster no matter what you try. So before you rewrite your entire codebase to function with <code class="inlineCode">threading</code>, make sure to test if your solution has any chance of working.</p>&#13;
    <p class="normal">Assuming that your I/O bottleneck can be alleviated, then you still have the choice of <code class="inlineCode">asyncio</code> versus <code class="inlineCode">threading</code>. Since <code class="inlineCode">asyncio</code> is the fastest of the available options, I would opt for that solution if it works with your codebase, but using <code class="inlineCode">threading</code> is not a bad option either, of course. </p>&#13;
    <p class="normal">If the GIL is your bottleneck due to heavy calculations from your Python code, then <code class="inlineCode">multiprocessing</code> can help you a lot. But even in those cases, <code class="inlineCode">multiprocessing</code> is not your only option; for many slow processes, it can also help to employ fast libraries such as <code class="inlineCode">numpy</code>. </p>&#13;
    <p class="normal">I am a great fan of the <code class="inlineCode">multiprocessing</code> library and it is one of the easiest implementations of multiprocess code that I have seen so far, but it still comes with several caveats such as more difficult memory management and deadlocks, as we have seen. So always consider if you actually need the solution and if your problem is suitable for multiprocessing. If a large portion of code is written using functional programming it can be really easy to implement; if you need to interact with a lot of external resources, such as databases, it can be really difficult to implement.</p>&#13;
    <h2 id="_idParaDest-407" class="heading-2">threading versus concurrent.futures</h2>&#13;
    <p class="normal">When given the choice, should you<a id="_idIndexMarker1189"/> use <code class="inlineCode">threading</code> or <code class="inlineCode">concurrent.futures</code>? In my opinion, it <a id="_idIndexMarker1190"/>depends on what you are trying to do.</p>&#13;
    <p class="normal">The advantages of <code class="inlineCode">threading</code> over <code class="inlineCode">concurrent.futures</code> are:</p>&#13;
    <ul>&#13;
      <li class="bulletList">We can specify the name of the thread explicitly, which can be seen in the task manager on many operating systems.</li>&#13;
      <li class="bulletList">We can explicitly create and start a long-running thread for a function instead of relying on the availability within a thread pool.</li>&#13;
    </ul>&#13;
    <p class="normal">If your scenario allows you to choose, I believe you should use <code class="inlineCode">concurrent.futures</code> instead of <code class="inlineCode">threading</code> for the following reasons:</p>&#13;
    <ul>&#13;
      <li class="bulletList">With <code class="inlineCode">concurrent.futures</code> you can switch between threads and processes by using <code class="inlineCode">concurrent.futures.ProcessPoolExecutor</code> instead of <code class="inlineCode">concurrent.futures.ThreadPoolExecutor</code>.</li>&#13;
      <li class="bulletList">With <code class="inlineCode">concurrent.futures</code> you have the <code class="inlineCode">map()</code> method to easily batch-process a list of items without having the (potential) overhead of setting up and shutting down the thread.</li>&#13;
      <li class="bulletList">The <code class="inlineCode">concurrent.futures.Future</code> objects as returned by the <code class="inlineCode">concurrent.futures</code> methods allow for fine-grained control of the results and the handling.</li>&#13;
    </ul>&#13;
    <h2 id="_idParaDest-408" class="heading-2">multiprocessing versus concurrent.futures</h2>&#13;
    <p class="normal">When it comes to <a id="_idIndexMarker1191"/>multiprocessing, I think the <code class="inlineCode">concurrent.futures</code> interface adds <a id="_idIndexMarker1192"/>much less benefit than it does in the case of threading, especially since <code class="inlineCode">multiprocessing.Pool</code> essentially offers you a nearly identical interface to <code class="inlineCode">concurrent.futures.ProcessPoolExecutor</code>.</p>&#13;
    <p class="normal">The advantages of <code class="inlineCode">multiprocessing</code> over <code class="inlineCode">concurrent.futures</code> are:</p>&#13;
    <ul>&#13;
      <li class="bulletList">Many advanced mapping methods such as <code class="inlineCode">imap_unordered</code> and <code class="inlineCode">starmap</code>.</li>&#13;
      <li class="bulletList">More control over the pool (i.e. <code class="inlineCode">terminate()</code>, <code class="inlineCode">close()</code>).</li>&#13;
      <li class="bulletList">It can be used across multiple machines.</li>&#13;
      <li class="bulletList">You can manually specify the startup method (<code class="inlineCode">fork</code>, <code class="inlineCode">spawn</code>, or <code class="inlineCode">forkserver</code>), which gives you control over how variables are copied from the parent process.</li>&#13;
      <li class="bulletList">You can choose the Python interpreter. Using <code class="inlineCode">multiprocessing.set_executable()</code>, you could run a Python 3.10 pool while running Python 3.9 for the main process.</li>&#13;
    </ul>&#13;
    <p class="normal">The <a id="_idIndexMarker1193"/>advantages <a id="_idIndexMarker1194"/>of <code class="inlineCode">concurrent.futures</code> over <code class="inlineCode">multiprocessing</code> are:</p>&#13;
    <ul>&#13;
      <li class="bulletList">You can easily switch to the <code class="inlineCode">concurrent.futures.ThreadPoolExecutor.</code></li>&#13;
      <li class="bulletList">The returned <code class="inlineCode">Future</code> objects allow for more fine-grained control over the result handling when compared to the <code class="inlineCode">AsyncResult</code> objects <code class="inlineCode">multiprocessing</code> uses.</li>&#13;
    </ul>&#13;
    <p class="normal">Personally, I prefer <code class="inlineCode">multiprocessing</code> if you have no need for compatibility with <code class="inlineCode">threads</code> because of the advanced mapping methods.</p>&#13;
    <h1 id="_idParaDest-409" class="heading-1">Hyper-threading versus physical CPU cores</h1>&#13;
    <p class="normal">Hyper-threading is a technology <a id="_idIndexMarker1195"/>that offers extra virtual CPU cores to your physical cores. The idea is that, because these virtual CPU cores have separate caches and other resources, you can more efficiently switch between multiple tasks. If you task-switch between two heavy processes, the <a id="_idIndexMarker1196"/>CPU won’t have to unload/reload all caches. When it comes to actual CPU instruction processing, however, it will not help you.</p>&#13;
    <p class="normal">When you truly maximize CPU usage, it is generally better to only use the physical processor count. To demonstrate how this affects the performance, we will run a simple test with several process counts. Since my processor has <code class="inlineCode">8</code> cores (<code class="inlineCode">16</code> if you include hyper-threading), we will run it with <code class="inlineCode">1</code>, <code class="inlineCode">2</code>, <code class="inlineCode">4</code>, <code class="inlineCode">8</code>, <code class="inlineCode">16</code>, and <code class="inlineCode">32</code> processes to demonstrate how it affects the performance:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> timeit&#13;
<span class="hljs-keyword">import</span> multiprocessing&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">busy_wait</span>(<span class="hljs-params">n</span>):&#13;
    <span class="hljs-keyword">while</span> n &gt; <span class="hljs-number">0</span>:&#13;
        n -= <span class="hljs-number">1</span>&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">benchmark</span>(<span class="hljs-params">n, processes, tasks</span>):&#13;
    <span class="hljs-keyword">with</span> multiprocessing.Pool(processes=processes) <span class="hljs-keyword">as</span> pool:&#13;
        <span class="hljs-comment"># Execute the busy_wait function 'tasks' times with</span>&#13;
        <span class="hljs-comment"># parameter n</span>&#13;
        pool.<span class="hljs-built_in">map</span>(busy_wait, [n <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tasks)])&#13;
    <span class="hljs-comment"># Create the executor</span>&#13;
&#13;
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    n = <span class="hljs-number">100000</span>&#13;
    tasks = <span class="hljs-number">128</span>&#13;
    <span class="hljs-keyword">for</span> exponent <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">6</span>):&#13;
        processes = <span class="hljs-built_in">int</span>(<span class="hljs-number">2</span> ** exponent)&#13;
        statement = <span class="hljs-string">f'benchmark(</span><span class="hljs-subst">{n}</span><span class="hljs-string">, </span><span class="hljs-subst">{processes}</span><span class="hljs-string">, </span><span class="hljs-subst">{tasks}</span><span class="hljs-string">)'</span>&#13;
        result = timeit.timeit(&#13;
            statement,&#13;
            number=<span class="hljs-number">5</span>,&#13;
            setup=<span class="hljs-string">'</span><span class="hljs-string">from __main__ import benchmark'</span>,&#13;
        )&#13;
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f'</span><span class="hljs-subst">{statement}</span><span class="hljs-string">: </span><span class="hljs-subst">{result:</span><span class="hljs-number">.3</span><span class="hljs-subst">f}</span><span class="hljs-string">'</span>) &#13;
</code></pre>&#13;
    <p class="normal">To keep the <a id="_idIndexMarker1197"/>processor busy, we are using a <code class="inlineCode">while</code> loop<a id="_idIndexMarker1198"/> from <code class="inlineCode">n</code> to <code class="inlineCode">0</code> in the <code class="inlineCode">busy_wait()</code> function. For the benchmarking, we are using a <code class="inlineCode">multiprocessing.Pool()</code> instance with the given number of processes and running <code class="inlineCode">busy_wait(100000)</code> 128 times:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python3 T_16_hyper_threading.py&#13;
benchmark(100000, 1): 3.400&#13;
benchmark(100000, 2): 1.894&#13;
benchmark(100000, 4): 1.208&#13;
benchmark(100000, 8): 0.998&#13;
benchmark(100000, 16): 1.124&#13;
benchmark(100000, 32): 1.787&#13;
</code></pre>&#13;
    <p class="normal">As you can see, with my <code class="inlineCode">8</code>-<code class="inlineCode">c</code>ore CPU with hyper-threading enabled, the version with <code class="inlineCode">8</code> threads is obviously the fastest. Even though the operating system task manager shows <code class="inlineCode">16</code> cores, it is not always faster to utilize more than the <code class="inlineCode">8</code> physical cores. Additionally, due to the boosting behavior of modern processors, you can see that using <code class="inlineCode">8</code> processors is only <code class="inlineCode">3.4</code> times faster than the single-threaded variant, as opposed to the expected <code class="inlineCode">8</code>-<code class="inlineCode">t</code>imes speedup.</p>&#13;
    <p class="normal">This illustrates the problem with hyper-threading when heavily loading the processor with instructions. As soon as the single processes actually use 100% of a CPU core, the task switching between the processes actually reduces performance. Since there are only <code class="inlineCode">8</code> physical cores, the other processes have to fight to get something done on the processor cores. Don’t forget that other processes on the system and the operating system itself will also consume a bit of processing power.</p>&#13;
    <p class="normal">If you are truly pressed for performance with a CPU-bound problem then matching the physical CPU cores is often the best solution, but if locking is a bottleneck, then a single thread can be faster than any multithreaded solution due to CPU boosting behavior.</p>&#13;
    <p class="normal">If you do not expect to<a id="_idIndexMarker1199"/> maximize all cores all the time, then I recommend not passing the <code class="inlineCode">processes</code> parameter to <code class="inlineCode">multiprocessing.Pool()</code>, which causes it to default to <code class="inlineCode">os.cpu_count()</code>, which returns all cores including hyper-threaded ones.</p>&#13;
    <p class="normal">It all depends on your <a id="_idIndexMarker1200"/>use case, however, and the only way to know for certain is to test for your specific scenario. As a rule of thumb, I recommend the following:</p>&#13;
    <ul>&#13;
      <li class="bulletList">Disk I/O bound? A single process is most likely your best bet.</li>&#13;
      <li class="bulletList">CPU bound? The number of physical CPU cores is your best bet.</li>&#13;
      <li class="bulletList">Network I/O bound? Start with the defaults and tune if needed. This is one of the few cases where 128 threads on an 8-core processor can still be useful.</li>&#13;
      <li class="bulletList">No obvious bound but many (hundreds of) parallel processes are needed? Perhaps you should try <code class="inlineCode">asyncio</code> instead of <code class="inlineCode">multiprocessing</code>.</li>&#13;
    </ul>&#13;
    <p class="normal">Note that the creation of multiple processes is not free in terms of memory and open files; while you could have a nearly unlimited number of coroutines, this is not the case for processes. Depending on your operating system configuration, you could reach the maximum open files limit long before you even reach 100 processes, and even if you reach those numbers, CPU scheduling will be your bottleneck instead.</p>&#13;
    <p class="normal">So what should we do if our CPU cores are not enough? Simple: use more CPU cores. Where do we get those? Multiple computers! It is time to graduate to distributed computing.</p>&#13;
    <h1 id="_idParaDest-410" class="heading-1">Remote processes</h1>&#13;
    <p class="normal">So far, we have only executed <a id="_idIndexMarker1201"/>our scripts on multiple local processors, but we can actually expand this much further. Using the <code class="inlineCode">multiprocessing</code> library, it’s actually very easy to execute jobs on remote servers, but the documentation is currently still a bit cryptic. There are actually a few ways of executing processes in a distributed way, but the most obvious one isn’t the easiest one. The <code class="inlineCode">multiprocessing.connection</code> module has both the <code class="inlineCode">Client</code> and <code class="inlineCode">Listener</code> classes, which facilitate secure communication between the clients and servers in a simple way. </p>&#13;
    <p class="normal">Communication is not the same as process management and queue management, however; those features require some extra effort. The <code class="inlineCode">multiprocessing</code> library is still a bit bare in this regard, but it’s most certainly possible given a few different processes.</p>&#13;
    <h2 id="_idParaDest-411" class="heading-2">Distributed processing using multiprocessing</h2>&#13;
    <p class="normal">We will start with a <a id="_idIndexMarker1202"/>module containing a few constants <a id="_idIndexMarker1203"/>that should be shared between all clients and the server, so the secret password and the hostname of the server are available to all. In addition to that, we will add our prime calculation functions, which we will be using later. The imports in the following modules will expect this file to be stored as <code class="inlineCode">T_17_remote_multiprocessing/constants.py,</code> but feel free to call it anything you like as long as the imports and references keep working:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code">host = <span class="hljs-string">'localhost'</span>&#13;
port = <span class="hljs-number">12345</span>&#13;
password = <span class="hljs-string">b'some secret password'</span>&#13;
</code></pre>&#13;
    <p class="normal">Next up, we define the functions that need to be available to both the server and the client. We will store this as <code class="inlineCode">T_17_remote_multiprocessing/functions.py</code>:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">primes</span>(<span class="hljs-params">n</span>):&#13;
    <span class="hljs-keyword">for</span> i, prime <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(prime_generator()):&#13;
        <span class="hljs-keyword">if</span> i == n:&#13;
            <span class="hljs-keyword">return</span> prime&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">prime_generator</span>():&#13;
    n = <span class="hljs-number">2</span>&#13;
    primes = <span class="hljs-built_in">set</span>()&#13;
    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:&#13;
        <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> primes:&#13;
            <span class="hljs-keyword">if</span> n % p == <span class="hljs-number">0</span>:&#13;
                <span class="hljs-keyword">break</span>&#13;
        <span class="hljs-keyword">else</span>:&#13;
            primes.add(n)&#13;
            <span class="hljs-keyword">yield</span> n&#13;
        n += <span class="hljs-number">1</span>&#13;
</code></pre>&#13;
    <p class="normal">Now it’s time to create the actual server that links the functions and the job queue. We will store this as <code class="inlineCode">T_17_remote_multiprocessing/server.py</code>:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> multiprocessing&#13;
<span class="hljs-keyword">from</span> multiprocessing <span class="hljs-keyword">import</span> managers&#13;
&#13;
<span class="hljs-keyword">import</span> constants&#13;
<span class="hljs-keyword">import</span> functions&#13;
&#13;
queue = multiprocessing.Queue()&#13;
manager = managers.BaseManager(address=(<span class="hljs-string">''</span>, constants.port),&#13;
                               authkey=constants.password)&#13;
&#13;
manager.register(<span class="hljs-string">'queue'</span>, <span class="hljs-built_in">callable</span>=<span class="hljs-keyword">lambda</span>: queue)&#13;
manager.register(<span class="hljs-string">'primes'</span>, <span class="hljs-built_in">callable</span>=functions.primes)&#13;
&#13;
server = manager.get_server()&#13;
server.serve_forever()&#13;
</code></pre>&#13;
    <p class="normal">After creating the<a id="_idIndexMarker1204"/> server, we need to have a client script<a id="_idIndexMarker1205"/> that sends the jobs. You could use a single script for both sending and processing, but to keep things sensible we will use separate scripts. </p>&#13;
    <p class="normal">The following script will add <code class="inlineCode">0</code> to <code class="inlineCode">999</code> to the queue for processing. We will store this as <code class="inlineCode">T_17_remote_multiprocessing/submitter.py</code>:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> multiprocessing <span class="hljs-keyword">import</span> managers&#13;
&#13;
<span class="hljs-keyword">import</span> constants&#13;
&#13;
manager = managers.BaseManager(&#13;
    address=(constants.host, constants.port),&#13;
    authkey=constants.password)&#13;
manager.register(<span class="hljs-string">'queue'</span>)&#13;
manager.connect()&#13;
&#13;
queue = manager.queue()&#13;
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>):&#13;
    queue.put(i)&#13;
</code></pre>&#13;
    <p class="normal">Lastly, we need to<a id="_idIndexMarker1206"/> create a client to actually process the <a id="_idIndexMarker1207"/>queue. We will store this as <code class="inlineCode">T_17_remote_multiprocessing/client.py</code>:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> multiprocessing <span class="hljs-keyword">import</span> managers&#13;
&#13;
<span class="hljs-keyword">import</span> functions&#13;
&#13;
manager = managers.BaseManager(&#13;
    address=(functions.host, functions.port),&#13;
    authkey=functions.password)&#13;
manager.register(<span class="hljs-string">'queue'</span>)&#13;
manager.register(<span class="hljs-string">'primes'</span>)&#13;
manager.connect()&#13;
&#13;
queue = manager.queue()&#13;
<span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> queue.empty():&#13;
    <span class="hljs-built_in">print</span>(manager.primes(queue.get()))&#13;
</code></pre>&#13;
    <p class="normal">From the preceding code, you can see how we pass along functions; the manager allows the registering of functions and classes that can be called from the clients as well. With that, we pass along a queue from the multiprocessing class, which is safe for both multithreading and multiprocessing. </p>&#13;
    <p class="normal">Now we need to start the processes themselves. First, the server that keeps on running:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python3 T_17_remote_multiprocessing/server.py&#13;
</code></pre>&#13;
    <p class="normal">After that, run the producer to generate the prime generation requests:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python3 T_17_remote_multiprocessing/submitter.py&#13;
</code></pre>&#13;
    <p class="normal">Now we can run multiple clients on multiple machines to get the first 1,000 primes. Since these clients now print the first 1,000 primes, the output is a bit too lengthy to show here, but you can simply run this in parallel multiple times or on multiple machines to generate your output:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python3 T_17_remote_multiprocessing/client.py&#13;
</code></pre>&#13;
    <p class="normal">Instead of printing, you can use queues or pipes to send the output to a different process if you’d like. As you can see, though, it’s still a bit of work to process things in parallel and it requires some code synchronization to work. There are a few alternatives available, such as <strong class="keyWord">Redis</strong>, <strong class="keyWord">ØMQ</strong>, <strong class="keyWord">Celery</strong>, <strong class="keyWord">Dask</strong>, and <strong class="keyWord">IPython Parallel</strong>. Which of these is the best and most <a id="_idIndexMarker1208"/>suitable depends on your use case. If <a id="_idIndexMarker1209"/>you are simply looking for processing tasks on multiple CPUs, then <code class="inlineCode">multiprocessing</code>, Dask, and IPython Parallel are probably your best choices. If you are looking for background processing and/or easy offloading to multiple machines, then ØMQ and Celery are better choices.</p>&#13;
    <h2 id="_idParaDest-412" class="heading-2">Distributed processing using Dask</h2>&#13;
    <p class="normal">The Dask library <a id="_idIndexMarker1210"/>is quickly becoming the standard for distributed <a id="_idIndexMarker1211"/>Python execution. It has very tight integration with many scientific Python libraries such as NumPy and Pandas, making parallel execution in many cases completely transparent. These libraries are covered in detail in <em class="chapterRef">Chapter 15</em>, <em class="italic">Scientific Python and Plotting</em>.</p>&#13;
    <p class="normal">The Dask library provides an easy parallel interface that can execute single-threaded, use multiple threads, use multiple processes, and even use multiple machines. As long as you keep the data-sharing limitations of multiple threads, processes, and machines in mind, you can easily switch between them to see which performs best for your use case.</p>&#13;
    <h3 id="_idParaDest-413" class="heading-3">Installing Dask</h3>&#13;
    <p class="normal">The Dask library consists of <a id="_idIndexMarker1212"/>multiple packages and you might not need all of them. Broadly speaking, the <code class="inlineCode">Dask</code> package is only the core, and we can choose from several extras, which can be installed through <code class="inlineCode">pip install dask[extra]</code>:</p>&#13;
    <ul>&#13;
      <li class="bulletList"><code class="inlineCode">array</code>: Adds an array interface similar to <code class="inlineCode">numpy.ndarray</code>. Internally, these structures consist of multiple <code class="inlineCode">numpy.ndarray</code> instances spread across your Dask cluster for easy parallel processing.</li>&#13;
      <li class="bulletList"><code class="inlineCode">dataframe</code>: Similar to the array interface, this is a collection of <code class="inlineCode">pandas.DataFrame</code> objects.</li>&#13;
      <li class="bulletList"><code class="inlineCode">diagnostics</code>: Adds profilers, progress bars, and even a fully interactive dashboard with live information about the currently running jobs. </li>&#13;
      <li class="bulletList"><code class="inlineCode">distributed</code>: Packages needed for running Dask across multiple systems instead of locally only.</li>&#13;
      <li class="bulletList"><code class="inlineCode">complete</code>: All of the above extras.</li>&#13;
    </ul>&#13;
    <p class="normal">For the demonstrations in this chapter, we will need to install at least the <code class="inlineCode">distributed</code> extra, so you need to run either:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>pip3 install -U <span class="hljs-con-string">"dask[distributed]"</span>&#13;
</code></pre>&#13;
    <p class="normal">Or:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>pip3 install -U <span class="hljs-con-string">"dask[complete]"</span>&#13;
</code></pre>&#13;
    <p class="normal">If you are playing around with Jupyter notebooks, the progress bars in the <code class="inlineCode">diagnostics</code> extra also have Jupyter support, which can be useful.</p>&#13;
    <h3 id="_idParaDest-414" class="heading-3">Basic example</h3>&#13;
    <p class="normal">Let’s start with a basic<a id="_idIndexMarker1213"/> example of executing some code via Dask without explicitly setting up a cluster. To illustrate how this can help performance, we will be using a <code class="inlineCode">busy-wait</code> loop to maximize CPU load.In this case, we will be using the <code class="inlineCode">dask.distributed</code> submodule, which has an interface quite similar to <code class="inlineCode">concurrent.futures</code>:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> sys&#13;
<span class="hljs-keyword">import</span> datetime&#13;
&#13;
<span class="hljs-keyword">from</span> dask <span class="hljs-keyword">import</span> distributed&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">busy_wait</span>(<span class="hljs-params">n</span>):&#13;
    <span class="hljs-keyword">while</span> n &gt; <span class="hljs-number">0</span>:&#13;
        n -= <span class="hljs-number">1</span>&#13;
&#13;
<span class="hljs-keyword">def</span> <span class="hljs-title">benchmark_dask</span>(<span class="hljs-params">client</span>):&#13;
    start = datetime.datetime.now()&#13;
&#13;
    <span class="hljs-comment"># Run up to 1 million</span>&#13;
    n = <span class="hljs-number">1000000</span>&#13;
    tasks = <span class="hljs-built_in">int</span>(sys.argv[<span class="hljs-number">1</span>])  <span class="hljs-comment"># Get number of tasks from argv</span>&#13;
&#13;
    <span class="hljs-comment"># Submit the tasks to Dask</span>&#13;
    futures = client.<span class="hljs-built_in">map</span>(busy_wait, [n] * tasks, pure=<span class="hljs-literal">False</span>)&#13;
    <span class="hljs-comment"># Gather the results; this blocks until the results are ready</span>&#13;
    client.gather(futures)&#13;
&#13;
    duration = datetime.datetime.now() - start&#13;
    per_second = <span class="hljs-built_in">int</span>(tasks / duration.total_seconds())&#13;
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f'</span><span class="hljs-subst">{tasks}</span><span class="hljs-string"> tasks at </span><span class="hljs-subst">{per_second}</span><span class="hljs-string"> per '</span>&#13;
          <span class="hljs-string">f'second, total time: </span><span class="hljs-subst">{duration}</span><span class="hljs-string">'</span>)&#13;
&#13;
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'</span><span class="hljs-string">__main__'</span>:&#13;
    benchmark_dask(distributed.Client())&#13;
</code></pre>&#13;
    <p class="normal">The code is mostly straightforward, but there are a few small caveats to look at. First of all, when submitting the task to Dask, you need to tell Dask that it is an impure function.</p>&#13;
    <div class="note">&#13;
      <p class="normal">If you recall from <em class="chapterRef">Chapter 5</em>, <em class="italic">Functional Programming – Readability Versus Brevity</em>, a pure function in functional programming is one that has no side effects; its output is consistent and only depends on the input. A function returning a random value is impure because repeated calls return different results. </p>&#13;
    </div>&#13;
    <p class="normal">Dask will automatically <a id="_idIndexMarker1214"/>cache the results in the case of pure functions. If you have two identical calls, Dask will only execute the function once.</p>&#13;
    <p class="normal">To queue the tasks, we need to use a function such as <code class="inlineCode">client.map()</code> or <code class="inlineCode">client.submit()</code>. These work in a very similar way to <code class="inlineCode">executor.submit()</code> in the case of <code class="inlineCode">concurrent.futures</code>.</p>&#13;
    <p class="normal">Lastly, we need to fetch the results from the futures. This can be done by calling <code class="inlineCode">future.result()</code>, or in batch by using <code class="inlineCode">client.gather(futures)</code>. Once again, very similar to <code class="inlineCode">concurrent.futures</code>.</p>&#13;
    <p class="normal">To make the code a bit more flexible, we made the number of tasks configurable so it runs in a reasonable amount of time on your system. If you have a much slower or much faster system, you will want to adjust this to get useful results.</p>&#13;
    <p class="normal">When we execute the script, we get the following results:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python3 T_18_dask.py 128&#13;
128 tasks at 71 per second, total time: 0:00:01.781836&#13;
</code></pre>&#13;
    <p class="normal">That is how easily you can execute some code across all of your CPU cores. Naturally, we can also test in single-threaded or distributed mode; the only part we need to vary is how we initialize <code class="inlineCode">distributed.Client()</code>.</p>&#13;
    <h3 id="_idParaDest-415" class="heading-3">Running a single thread</h3>&#13;
    <p class="normal">Let’s run the same code<a id="_idIndexMarker1215"/> but in single-threaded mode:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    benchmark_dask(distributed.Client())&#13;
</code></pre>&#13;
    <p class="normal">Now if we run it, we can see that Dask was definitely using multiple processes before:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python3 T_19_dask_single.py 128&#13;
128 tasks at 20 per second, total time: 0:00:06.142977&#13;
</code></pre>&#13;
    <p class="normal">This can be useful for debugging thread-safety issues. If the issues still persist in single-threaded mode, thread safety is probably not your issue.</p>&#13;
    <h3 id="_idParaDest-416" class="heading-3">Distributed execution across multiple machines</h3>&#13;
    <p class="normal">For a much more impressive<a id="_idIndexMarker1216"/> feat, let’s run the code on multiple machines at the same time. To run Dask on multiple systems simultaneously, there are many deployment options available:</p>&#13;
    <ul>&#13;
      <li class="bulletList">Manual setup using the <code class="inlineCode">dask-scheduler</code> and <code class="inlineCode">dask-worker</code> commands</li>&#13;
      <li class="bulletList">Automatic deployment over SSH using the <code class="inlineCode">dask-ssh</code> command</li>&#13;
      <li class="bulletList">Deployment straight to an existing compute cluster running Kubernetes, Hadoop, and others</li>&#13;
      <li class="bulletList">Deployment to cloud providers such as Amazon, Google, and Microsoft Azure</li>&#13;
    </ul>&#13;
    <p class="normal">In this case, we are going to use <code class="inlineCode">dask-scheduler</code> since it’s the solution that you can run on pretty much any machine that can run Python.</p>&#13;
    <p class="normal">Note that you can encounter errors if the Dask versions and dependencies are not in sync, so updating to the latest version before starting is a good idea.</p>&#13;
    <p class="normal">First, we start the <code class="inlineCode">dask-scheduler</code>:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>dask-scheduler&#13;
[...]&#13;
distributed.scheduler - INFO - Scheduler at:  tcp://10.1.2.3:8786&#13;
distributed.scheduler - INFO - dashboard at:                :8787&#13;
</code></pre>&#13;
    <p class="normal">Once you have the <code class="inlineCode">dask-scheduler</code> running, it will also host the dashboard mentioned above, which shows the current status: <code class="inlineCode">http://localhost:8787/status</code>.</p>&#13;
    <p class="normal">Now we can run the <code class="inlineCode">dask-worker</code> processes on all machines that need to participate:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>dask-worker --nprocs auto tcp://10.1.2.3:8786&#13;
</code></pre>&#13;
    <p class="normal">With the <code class="inlineCode">--nprocs</code> parameter, you can set the number of processes to start. With <code class="inlineCode">auto</code>, it is set to the number of CPU cores including hyper-threading. When set to a positive number, it will start that exact number of processes; when set to a negative number the number is added to the number of CPU cores.</p>&#13;
    <p class="normal">Your dashboard screen and the console should show all of the connected clients now. It’s time to run our script again, but distributed this time:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&#13;
    benchmark_dask(distributed.Client(<span class="hljs-string">'localhost:8786'</span>))&#13;
</code></pre>&#13;
    <p class="normal">That’s the only thing we need to do: configure where the scheduler is running. Note that we could also connect from other machines using the IP or hostname instead.</p>&#13;
    <p class="normal">Let’s run it and <a id="_idIndexMarker1217"/>see if it became any faster:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python3 T_20_dask_distributed.py 2048&#13;
[...]&#13;
2048 tasks at 405 per second, total time: 0:00:05.049570&#13;
</code></pre>&#13;
    <p class="normal">Wow, that’s quite a difference! Instead of the <code class="inlineCode">20</code> per second we could do in single-threaded mode or the <code class="inlineCode">71</code> per second we could do in multiple-process mode, we can now process <code class="inlineCode">405</code> of these tasks per second. As you can see, it also took very little effort to set up.</p>&#13;
    <p class="normal">The Dask library has many more options to increase efficiency, limit memory, prioritize work, and more. We didn’t even cover the combining of tasks by chaining them or running a <code class="inlineCode">reduce</code> on bundled results. I can strongly recommend considering Dask if your code could benefit from running on multiple systems at the same time.</p>&#13;
    <h2 id="_idParaDest-417" class="heading-2">Distributed processing using ipyparallel</h2>&#13;
    <p class="normal">The IPython Parallel <a id="_idIndexMarker1218"/>module, similar to Dask, makes it <a id="_idIndexMarker1219"/>possible to process code on multiple computers at the same time. It should be noted that you can run Dask on top of <code class="inlineCode">ipyparallel</code>. The library supports more features than you are likely to need, but the basic usage is important to know just in case you need to do heavy calculations that can benefit from multiple computers. </p>&#13;
    <p class="normal">First, let’s start by installing the latest <code class="inlineCode">ipyparallel</code> package and all the IPython components:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>pip3 install -U "ipython[all]" ipyparallel&#13;
</code></pre>&#13;
    <p class="normal">Especially on Windows, it might be easier to install IPython using Anaconda instead, as it includes binaries for many science, math, engineering, and data analysis packages. To get a consistent installation, the Anaconda installer is also available for OS X and Linux systems.</p>&#13;
    <p class="normal">Secondly, we<a id="_idIndexMarker1220"/> need a cluster configuration. Technically, this is optional, but since we are going to create a distributed IPython cluster, it is much more convenient to configure everything using a specific profile:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>ipython profile create --parallel --profile=mastering_python&#13;
[ProfileCreate] Generating default config file: '~/.ipython/profile_mastering_python/ipython_config.py'&#13;
[ProfileCreate] Generating default config file: '~/.ipython/profile_mastering_python/ipython_kernel_config.py'&#13;
[ProfileCreate] Generating default config file: '~/.ipython/profile_mastering_python/ipcontroller_config.py'&#13;
[ProfileCreate] Generating default config file: '~/.ipython/profile_mastering_python/ipengine_config.py'&#13;
[ProfileCreate] Generating default config file: '~/.ipython/profile_mastering_python/ipcluster_config.py'&#13;
</code></pre>&#13;
    <p class="normal">These<a id="_idIndexMarker1221"/> configuration files contain a huge number of options, so I recommend searching for a specific section instead of walking through them. A quick listing gave me about 2,500 lines of configuration in total for these five files. The filenames already provide hints about the purpose of the configuration files, but we’ll walk through the files explaining their purpose and some of the most important settings.</p>&#13;
    <h3 id="_idParaDest-418" class="heading-3">ipython_config.py</h3>&#13;
    <p class="normal">This is the generic <a id="_idIndexMarker1222"/>IPython configuration file; you can customize pretty much everything about your IPython shell here. It defines how your shell should look, which modules should be loaded by default, whether or not to load a GUI, and quite a bit more. For the purpose of this chapter, it’s not all that important, but it’s definitely worth a look if you’re going to use IPython more often. One of the things you can configure here is the automatic loading of extensions, such as <code class="inlineCode">line_profiler</code> and <code class="inlineCode">memory_profiler</code> discussed in <em class="chapterRef">Chapter 12</em>, <em class="italic">Performance – Tracking and Reducing Your Memory and CPU Usage</em>. For example:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code">c.InteractiveShellApp.extensions = [&#13;
    <span class="hljs-string">'line_profiler'</span>,&#13;
    <span class="hljs-string">'memory_profiler'</span>,&#13;
]&#13;
</code></pre>&#13;
    <h3 id="_idParaDest-419" class="heading-3">ipython_kernel_config.py</h3>&#13;
    <p class="normal">This file configures your <a id="_idIndexMarker1223"/>IPython kernel and allows you to overwrite/extend <code class="inlineCode">ipython_config.py</code>. To understand its purpose, it’s important to know what an IPython kernel is. The kernel, in this context, is the program that runs and introspects the code. By default, this is <code class="inlineCode">IPyKernel</code>, which is a regular Python interpreter, but there are also other options such as <code class="inlineCode">IRuby</code> or <code class="inlineCode">IJavascript</code> to run Ruby or JavaScript respectively.</p>&#13;
    <p class="normal">One of the more useful options is the possibility of configuring the listening port(s) and IP addresses for the kernel. By default, the ports are all set to use a random number, but it is important to note that if someone else has access to the same machine while you are running your kernel, they will be able to connect to your IPython kernel, which can be dangerous on shared machines.</p>&#13;
    <h3 id="_idParaDest-420" class="heading-3">ipcontroller_config.py</h3>&#13;
    <p class="normal"><code class="inlineCode">ipcontroller</code> is the master process<a id="_idIndexMarker1224"/> of your IPython cluster. It controls the engines and the distribution of tasks and takes care of tasks such as logging.</p>&#13;
    <p class="normal">The most important parameter in terms of performance is the <code class="inlineCode">TaskScheduler</code> setting. By default, the <code class="inlineCode">c.TaskScheduler.scheme_name</code> setting is set to use the Python LRU scheduler, but depending on your workload, others such as <code class="inlineCode">leastload</code> and <code class="inlineCode">weighted</code> might be better. If you have to process so many tasks on such a large cluster that the scheduler becomes the bottleneck, there is also the <code class="inlineCode">plainrandom</code> scheduler, which works surprisingly well if all your machines have similar specs and the tasks have similar durations.</p>&#13;
    <p class="normal">For the purpose of our test, we will set the IP of the controller to <code class="inlineCode">*</code>, which means that <em class="italic">all</em> IP addresses will be accepted and that every network connection will be accepted. If you are in an unsafe environment/network and/or don’t have any firewalls that allow you to selectively enable certain IP addresses, then this method is <em class="italic">not</em> recommended! In such cases, I recommend launching through more secure options, such as <code class="inlineCode">SSHEngineSetLauncher</code> or <code class="inlineCode">WindowsHPCEngineSetLauncher</code>, instead.</p>&#13;
    <p class="normal">Assuming your <a id="_idIndexMarker1225"/>network is indeed safe, set the factory IP to all the local addresses:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code">c.HubFactory.client_ip = <span class="hljs-string">'*'</span>&#13;
c.RegistrationFactory.ip = <span class="hljs-string">'*'</span>&#13;
</code></pre>&#13;
    <p class="normal">Now start the controller:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>ipcontroller --profile=mastering_python&#13;
[IPControllerApp] Hub listening on tcp://*:58412 for registration.&#13;
[IPControllerApp] Hub listening on tcp://127.0.0.1:58412 for registration.&#13;
...&#13;
 [IPControllerApp] writing connection info to ~/.ipython/profile_mastering_python/security/ipcontroller-client.json&#13;
[IPControllerApp] writing connection info to ~/.ipython/profile_mastering_python/security/ipcontroller-engine.json&#13;
...&#13;
</code></pre>&#13;
    <p class="normal">Pay attention to the files that were written to the security directory of the profile directory. They contain the authentication information that is used by <code class="inlineCode">ipengine</code> to find and connect to the <code class="inlineCode">ipcontroller</code>, such as the encryption keys and port information.</p>&#13;
    <h3 id="_idParaDest-421" class="heading-3">ipengine_config.py</h3>&#13;
    <p class="normal"><code class="inlineCode">ipengine</code> is the actual<a id="_idIndexMarker1226"/> worker process. These processes run the actual calculations, so to speed up the processing you will need these on as many machines as you have available. You probably won’t need to change this file, but it can be useful if you want to configure centralized logging or need to change the working directory. Generally, you don’t want to start the <code class="inlineCode">ipengine</code> process manually since you will most likely want to launch multiple processes per computer. That’s where our next command comes in, the <code class="inlineCode">ipcluster</code> command.</p>&#13;
    <h3 id="_idParaDest-422" class="heading-3">ipcluster_config.py</h3>&#13;
    <p class="normal">The <code class="inlineCode">ipcluster</code> command is<a id="_idIndexMarker1227"/> actually just an easy shorthand to start a combination of <code class="inlineCode">ipcontroller</code> and <code class="inlineCode">ipengine</code> at the same time. For a simple local processing cluster, I recommend using this, but when starting a distributed cluster, it can be useful to have the control that the separate use of <code class="inlineCode">ipcontroller</code> and <code class="inlineCode">ipengine</code> offers. In most cases the command offers enough options, so you might have no need for the separate commands.</p>&#13;
    <p class="normal">The most important configuration option is <code class="inlineCode">c.IPClusterEngines.engine_launcher_class</code>, as this controls the communication method between the engines and the controller. Along with that, it is also the most important component for secure communication between the processes. By default it’s set to <code class="inlineCode">ipyparallel.apps.launcher.LocalControllerLauncher</code>, which is designed for local processes, but <code class="inlineCode">ipyparallel.apps.launcher.SSHEngineSetLauncher</code> is also an option if you want to use SSH to communicate with the clients. Alternatively, there is <code class="inlineCode">ipyparallel.apps.launcher.WindowsHPCEngineSetLauncher</code> for Windows HPC.</p>&#13;
    <p class="normal">Before we can create the cluster on all machines, we need to transfer the configuration files. Your options are to transfer all the files or to simply transfer the files in your IPython profile’s <code class="inlineCode">security</code> directory.</p>&#13;
    <p class="normal">Now it’s time to start the cluster. Since we already started the <code class="inlineCode">ipcontroller</code> separately, we only need to start the engines. On the local machine, we simply need to start it, but the other machines don’t have the configuration yet. One option is copying the entire IPython profile directory, but the only file that really needs copying is <code class="inlineCode">security/ipcontroller-engine.json</code>; after creating the profile using the profile creation command, that is. So unless you are going to copy the entire IPython profile directory, you need to execute the profile creation command again:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>ipython profile create --parallel --profile=mastering_python&#13;
</code></pre>&#13;
    <p class="normal">After that, simply copy the <code class="inlineCode">ipcontroller-engine.json</code> file and you’re done. Now we can start the actual engines:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>ipcluster engines --profile=mastering_python -n 4&#13;
[IPClusterEngines] IPython cluster: started&#13;
[IPClusterEngines] Starting engines with [daemon=False]&#13;
[IPClusterEngines] Starting 4 Engines with LocalEngineSetLauncher&#13;
</code></pre>&#13;
    <p class="normal">Note that the <code class="inlineCode">4</code> here was chosen for a quad-core processor, but any number would do. The default will use the number of logical processor cores, but depending on the workload it might be better to match the number of physical processor cores instead.</p>&#13;
    <p class="normal">Now we can run some parallel code from our IPython shell. To demonstrate the performance difference, we will use a simple sum of all the numbers from 0 to 10,000,000. Not an extremely heavy task, but when performed 10 times in succession, a regular Python interpreter takes a while:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con">In [1]: %timeit for _ in range(10): sum(range(10000000))&#13;
1 loops, best of 3: 2.27 s per loop&#13;
</code></pre>&#13;
    <p class="normal">This time however, to illustrate the difference, we will run it 100 times to demonstrate how fast a distributed cluster is. Note that this is with only a three-machine cluster, but it’s still quite a bit faster:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con">In [1]: import ipyparallel&#13;
&#13;
In [2]: client = ipyparallel.Client(profile='mastering_python')&#13;
In [3]: view = client.load_balanced_view()&#13;
In [4]: %timeit view.map(lambda _: sum(range(10000000)), range(100)).wait()&#13;
1 loop, best of 3: 909 ms per loop&#13;
</code></pre>&#13;
    <p class="normal">More fun, however, is <a id="_idIndexMarker1228"/>the definition of parallel functions in <code class="inlineCode">ipyparallel</code>. With just a simple decorator, a function is marked as parallel:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con">In [1]: import ipyparallel&#13;
&#13;
In [2]: client = ipyparallel.Client(profile='mastering_python')&#13;
In [3]: view = client.load_balanced_view()&#13;
In [4]: @view.parallel()&#13;
   ...: def loop():&#13;
   ...:     return sum(range(10000000))&#13;
   ...:&#13;
In [5]: loop.map(range(10))&#13;
Out[5]: &lt;AsyncMapResult: loop&gt;&#13;
</code></pre>&#13;
    <p class="normal">The <code class="inlineCode">ipyparallel</code> library offers many more useful features, but that is outside the scope of this book. Even though <code class="inlineCode">ipyparallel</code> is a separate entity from the rest of Jupyter/IPython, it does integrate well, which makes combining them easy enough.</p>&#13;
    <p class="normal">Exercises</p>&#13;
    <p class="normal">While preparing for multiple threads and/or multiple processes is less invasive than preparing for <code class="inlineCode">asyncio</code> is, it still requires a bit of thought if you have to pass or share variables. So, this is really a question of how difficult you want to make it for yourself.</p>&#13;
    <p class="normal">See if you can make an echo server and client as separate processes.Even though we did not cover <code class="inlineCode">multiprocessing.Pipe()</code>, I trust you can work with it regardless. It can be created through <code class="inlineCode">a, b = multiprocessing.Pipe()</code> and you can use it with <code class="inlineCode">[a/b].send()</code> and <code class="inlineCode">[a/b].recv()</code>.</p>&#13;
    <ul>&#13;
      <li class="bulletList">Read all files in a directory and sum the size of the files by reading each file using <code class="inlineCode">threading</code> and <code class="inlineCode">multiprocessing</code>, or <code class="inlineCode">concurrent.futures</code> if you want an easier exercise. If you want an extra challenge, walk through the directories recursively by letting the thread/process queue new items while running.</li>&#13;
      <li class="bulletList">Create a pool of workers that keeps waiting for items to be queued through <code class="inlineCode">multiprocessing.Queue()</code>. Bonus points if you make it a safe RPC (remote procedure call) type operation.</li>&#13;
      <li class="bulletList">Apply your functional programming skills and calculate something in a parallel way. Perhaps parallel sorting?</li>&#13;
    </ul>&#13;
    <p class="normal">All of these exercises are unfortunately still easy compared to what you can experience in the wild. If you really want a challenge, start applying these techniques (especially memory sharing) to your existing or new projects and hope (or not) that you run into a real challenge. </p>&#13;
    <div class="note">&#13;
      <p class="normal">Example answers for these exercises can be found on GitHub: <a href="Chapter_14.xhtml"><span class="url">https://github.com/mastering-python/exercises</span></a>. You are encouraged to submit your own solutions and learn about alternative solutions from others.</p>&#13;
    </div>&#13;
    <h1 id="_idParaDest-423" class="heading-1">Summary</h1>&#13;
    <p class="normal">We have covered many different topics in this chapter, so let’s summarize them:</p>&#13;
    <ul>&#13;
      <li class="bulletList">What the Python GIL is, why we need it, and how we can work around it</li>&#13;
      <li class="bulletList">When to use threads, when to use processes, and when to use <code class="inlineCode">asyncio</code></li>&#13;
      <li class="bulletList">Running code in parallel threads using <code class="inlineCode">threading</code> and <code class="inlineCode">concurrent.futures</code></li>&#13;
      <li class="bulletList">Running code in parallel processes using <code class="inlineCode">multiprocessing</code> and <code class="inlineCode">concurrent.futures</code></li>&#13;
      <li class="bulletList">Running code distributed across multiple machines</li>&#13;
      <li class="bulletList">Sharing data between threads and processes</li>&#13;
      <li class="bulletList">Thread safety</li>&#13;
      <li class="bulletList">Deadlocks</li>&#13;
    </ul>&#13;
    <p class="normal">The most important lesson you can learn from this chapter is that the synchronization of data between threads and processes is <em class="italic">really slow</em>. Whenever possible, you should only send data to the function and return once it is done, with nothing in between. Even in that case, if you can send less data, send less data. If possible, keep your calculations and data local.</p>&#13;
    <p class="normal">In the next chapter, we will learn about scientific Python libraries and plotting. These libraries can help you perform difficult calculations and data processing in record time. These libraries are mostly highly optimized for performance and go great together with multiprocessing or the Dask library.</p>&#13;
    <h1 class="heading-1">Join our community on Discord</h1>&#13;
    <p class="normal">Join our community’s Discord space for discussions with the author and other readers: <a href="https://discord.gg/QMzJenHuJf"><span class="url">https://discord.gg/QMzJenHuJf</span></a></p>&#13;
    <p class="normal"><img src="Images/QR_Code156081100001293319171.png" alt="" width="177" height="177"/></p>&#13;
  </div>&#13;
</div></body></html>