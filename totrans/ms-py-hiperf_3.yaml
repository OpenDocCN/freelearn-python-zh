- en: Chapter 3. Going Visual – GUIs to Help Understand Profiler Output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we already covered profiling in the previous chapter, the process we
    went through was like walking in the dark, or at least, in a place with very little
    light. We kept looking at numbers. Basically, we kept trying to decrease the number
    of hits, number of seconds, or other similar numbers. However, it was hard to
    understand how those numbers related to each other based on the representation
    we had of them.
  prefs: []
  type: TYPE_NORMAL
- en: We couldn't easily see the big blueprint of our system, based off of that output.
    If our systems would've been even bigger, seeing that blueprint would've been
    even harder.
  prefs: []
  type: TYPE_NORMAL
- en: Simply because we're human beings and not computers ourselves, we work better
    when we have some sort of visual aid. In this particular case, our work would
    benefit if we could better understand how everything is related. To do this, we
    have tools that provide visual representations of the numbers we saw in the previous
    chapter. These tools will provide us with much needed help. In turn, we'll be
    able to locate and fix the bottlenecks of our systems much faster. As an added
    bonus, we'll have a better understanding of our system.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover two tools that fall into this category:'
  prefs: []
  type: TYPE_NORMAL
- en: '**KCacheGrind / pyprof2calltree**: This combo will provide the ability to transform
    the output of `cProfile` into the format required by KCacheGrind, which in turn
    will help us visualize the information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RunSnakeRun** ([http://www.vrplumber.com/programming/runsnakerun/](http://www.vrplumber.com/programming/runsnakerun/)):
    This tool will also allow us to visualize and analyze the output from `cProfile`.
    It provides square maps and sortable lists to help us in our task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each one, we'll go over the basics of installation and UI explanation. Then,
    we'll grab the examples from [Chapter 2](ch02.html "Chapter 2. The Profilers"),
    *The Profilers*, and reanalyze them based on the output from these tools.
  prefs: []
  type: TYPE_NORMAL
- en: KCacheGrind – pyprof2calltree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first GUI tool we will see is KCacheGrind. It is a data visualization tool
    designed to parse and display different formats of profiling data. For our case,
    we will display the output from `cProfile`. However, to do this, we'll also need
    the help from the command-line tool called `pyprof2calltree`.
  prefs: []
  type: TYPE_NORMAL
- en: This tool is a rebranding of a very popular one called `lsprofcalltree.py` ([https://people.gnome.org/~johan/lsprofcalltree.py](https://people.gnome.org/~johan/lsprofcalltree.py)).
    It tries to behave more like the `kcachegrind-converter` ([https://packages.debian.org/en/stable/kcachegrind-converters](https://packages.debian.org/en/stable/kcachegrind-converters))
    package from Debian. We'll use the tool to transform the output from `cProfile`
    into something KCacheGrind can understand.
  prefs: []
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To install `pyprof2calltree`, you''ll first need to install the `pip` command-line
    utility. Then, just use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that all installation steps and instructions are meant for the Ubuntu 14.04
    Linux distribution, unless otherwise noted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for KCacheGrind, the installation is a bit different. The visualizer is
    part of the KDE desktop environment, so if you already have it installed, chances
    are that you already have KCacheGrind also. However, if you don''t have it (maybe
    you''re a Gnome user), you can just use your package manager and install it. For
    instance, in Ubuntu, you''d use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With this command, you'll probably have to install a lot of packages not directly
    related to the utility, but to KDE. So, the installation might take some time
    depending on your Internet connection.
  prefs: []
  type: TYPE_NORMAL
- en: For Windows and OS X users, there is the option of installing the QCacheGrind
    branch of KCacheGrind, which is already precompiled and can be installed as a
    binary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Windows users can download it from [http://sourceforge.net/projects/qcachegrindwin/](http://sourceforge.net/projects/qcachegrindwin/),
    and OS X users can install it using brew:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two ways to use `pyprof2calltree`: one is from the command line,
    passing in arguments, and the other one is directly from the **read–eval–print
    loop**(**REPL**) (or even from our own scripts being profiled).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first one (command-line version) comes in very handy when we already have
    the profiling results stored somewhere. So, with this tool, we can simply run
    the following command and get the output when needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'There are some optional parameters, which can help us in different cases. Two
    of them are explained here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-k`: If we want to run KCacheGrind on the output data right away, this option
    will do it for us'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-r`: If we don''t have the profiling data already saved in a file, we can
    use this parameter to pass in the Python script we''ll use to collect the said
    data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, if you want to use it from the REPL instead, you can simply import either
    (or both) the `convert` function or the `visualize` function from the `pyprof2calltree`
    package. The first one will save the data into a file, and the second one will
    launch KCacheGrind with the output from the profiler.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This code will call KCacheGrind. It''ll show something like what you see in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Usage](img/B02088_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, you can see the list on the left-hand side (**1**)
    showing some of the numbers we saw in the previous chapter. On the right-hand
    side (**2**), we've selected one of the tabs, specifically the **Callee Map**
    tab. It shows a set of boxes, representing the hierarchy of function calls from
    the one selected on the left-hand side all the way down.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the list from the left-hand side, there are two columns that we''ll want
    to pay special attention to:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Incl. (from Inclusive time) column**: This shows an indicator of how long
    each function takes in aggregate. This means it adds up the time its code takes
    plus the time that other functions called by it take. If a function has a high
    number in this column, it doesn''t necessarily mean that the function takes too
    long. It could mean that the functions called by it do.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Self column**: This shows the time spent inside a particular function, without
    taking into account the ones called by it. So, if a function has a high **Self**
    value, then it probably means that a lot of time is spent inside it, and it''s
    a good place to start looking for optimization paths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another useful view is **Call Graph**, which can be found on the lower-right
    box once a function is selected on the list. It''ll show a representation of the
    functions that will help explain how each one calls the next one (and how many
    times). Here is an example from the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Usage](img/B02088_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A profiling example – TweetStats
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's now go back to the examples of [Chapter 2](ch02.html "Chapter 2. The Profilers"),
    *The Profilers*, and tackle them using the `pyprof2calltree`/`kcachegrind` combo.
  prefs: []
  type: TYPE_NORMAL
- en: Let's avoid the Fibonacci examples, since they're quite simple and we've been
    over them already. So, let's jump directly to the code from the TweetStats module.
    It would read a list of tweets and get some statistics from it. We're not modifying
    the code. So, for reference, just take a look at it in [Chapter 2](ch02.html "Chapter 2. The
    Profilers"), *The Profilers*.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the script using the class and printing the actual stats, we''re modifying
    it to save the stats instead. This is a very simple change as you can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, with the stats saved into the `tweet-stats.prof` file, we can use the
    following command to transform it and start the visualizer all at once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This, in turn, will show us something like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A profiling example – TweetStats](img/B02088_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Again, with the **Callee Map** selected for the first function call, we can
    see the entire map of our script. It clearly shows where the bottlenecks are (biggest
    blocks on the right-hand side): `read_data`, the `split` method, and the `get_data`
    function on the far right of the map.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the `get_stats` section of the map, we can see how there are two functions
    that make up for part of the size: `inc_stat` and `find` from string. We know
    the first one from seeing the code. This function does very little, so it''s entire
    size will only be due to lookup times accumulated (we''re calling it around 760k
    times after all). The same thing happens for the `find` method. We''re calling
    it way too many times, so the lookup time accumulates and starts to be of notice.
    So, let''s apply a set of very simple improvements to this function. Let''s remove
    the `inc_stat` function and inline it''s behavior. Let''s also change the `find`
    method line and use the in operator. The result will look like the one shown in
    this screenshot: :'
  prefs: []
  type: TYPE_NORMAL
- en: '![A profiling example – TweetStats](img/B02088_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: That other side of the map changed drastically. Now, we can see that the `get_stats`
    function no longer calls other functions, so the lookup times were removed. It
    now only represents 9.45 percent of the total execution time, down from 23.73
    percent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yes, the preceding conclusions are the same ones we arrived at in the previous
    chapter, but we did so using a different method. Let''s then keep doing the same
    optimization we did earlier and see how the map changes again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A profiling example – TweetStats](img/B02088_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, we see that by selecting the `build_twitt_stats`
    function (in the list on the left-hand side), the functions that get called are
    simply methods of the string objects.
  prefs: []
  type: TYPE_NORMAL
- en: Sadly, KCacheGrind isn't showing us the total time of execution. However, the
    map clearly shows that we've simplified and optimized our code anyway.
  prefs: []
  type: TYPE_NORMAL
- en: A profiling example – Inverted Index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Again, let''s get another example from [Chapter 2](ch02.html "Chapter 2. The
    Profilers"), *The Profilers*: the inverted index. Let''s update its code in order
    to generate the stats data and save it into a file so that we can later analyze
    it with KCacheGrind.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The only thing we need to change is the last line of the file, instead of just
    calling the `__start__` function. We have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'So now, executing the script will save the data into the `inverted-index-stats.prof`
    file. Later, we can use the following command to start up KCacheGrind:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what we will see first:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A profiling example – Inverted Index](img/B02088_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s first do a resort of the functions on the left-hand side by the second
    column (**Self**). So, we can look at the functions that take the longest to execute
    because of their code (not because of how long the functions it calls take). We
    will get the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A profiling example – Inverted Index](img/B02088_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, according to the preceding list, the two most problematic functions right
    now are `getWords` and `list2dict`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first one can be improved in several ways, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The `wordIndexDict` attribute can be changed to be of the `defaultdict` type,
    which will remove the `if` statement checking for an existing index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The strip statements can be removed from the `readFileContent` function, simplifying
    our code here
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A lot of assignments can be removed, so avoid spending milliseconds in them,
    since we can use the values directly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, our new `getWords` function looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we run the stats again, the map and the numbers look a bit different:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A profiling example – Inverted Index](img/B02088_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, our function is now using less time, both overall (**Incl.** column) and
    inside it (**Self** column). However, there is still another detail we might want
    to look into before leaving this function alone. The `getWords` function is calling
    `getOffsetUpToWord` a total of **141,295** times, the lookup time spent in there
    alone, should be enough to merit a review. So, let's see what we can do.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve already solved this issue in the earlier chapter. We saw that we can
    reduce the entire `getOffsetUpToWord` function to a one-liner, which we can later
    write directly inside the `getWords` function to avoid lookup time. With this
    in mind, let see what our new map looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A profiling example – Inverted Index](img/B02088_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, we have increased the overall time, but that's nothing to worry about.
    It is due to the fact that now we have one function less to spread the timing
    between, so the number changed for all other functions. However, the one we really
    care about (the **Self** time) went down, by about 4 percent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding screenshot also shows the **Call Graph** view, which helps us
    see that even though we made an improvement, the `reduce` function is still being
    called over **100,000** times. If you look at the code of the `getWords` function,
    you would notice we don''t really need the `reduce` function. This is because
    on every call, we''re adding up all the numbers we added on the previous call
    plus one more, so we can simplify this in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'With this final touch to the functions, the numbers change once again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A profiling example – Inverted Index](img/B02088_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The inclusive amount of time of the function was lowered significantly, so overall,
    this function now takes less time to execute (which was our goal). The internal
    time (**Self** column) went down, which is a good thing. This is because it also
    means that we're doing the same in less time (specially because we know that we're
    not calling any other function).
  prefs: []
  type: TYPE_NORMAL
- en: RunSnakeRun
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RunSnakeRun is yet another GUI tool to help us visualize the profiling output
    and, in turn, help us make sense of it. This particular project is a simplified
    version of KCacheGrind. Whereas the latter is also useful for C and C++ developers,
    RunSnakeRun is specifically designed and written for Python developers.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier, with KCacheGrind, if we wanted to plot the output of `cProfile`, we
    needed an extra tool (`pyprof2calltree`). This time we won't. RunSnakeRun knows
    how to interpret it and display it, so all we need to do is call it and pass in
    the path to the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The features provided by this tool are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sortable data grid views with fields, such as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: function name
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: number of total calls
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: cumulative time
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: filename and line number
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Function-specific information, such as all callers of this function and all
    callee's of this function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Square map of the call tree with size proportional to the amount of time spent
    inside each function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to install this tool, you have to make sure that several dependencies
    are covered, mainly the following ones:'
  prefs: []
  type: TYPE_NORMAL
- en: Python profiler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: wxPython (2.8 or above) ([http://www.wxpython.org/](http://www.wxpython.org/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python (of course!) 2.5 or above, but lower than 3.x
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You'll also need to have `pip` ([https://pypi.python.org/pypi/pip](https://pypi.python.org/pypi/pip))
    installed in order to run the installation command.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, make sure you have all these installed before moving forward. If you''re
    in a Debian-based distribution of Linux (say Ubuntu), you can use the following
    line to make sure you have everything you need (provided you already have Python
    installed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Windows and OS X users will need to find the correct precompiled binaries for
    their current OS version for each of the dependencies mentioned earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, you can just run this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: After that, you should be ready to go.
  prefs: []
  type: TYPE_NORMAL
- en: Usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, to quickly show you how to use it, let''s go back to previous last example:
    `inverted-index.py`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s execute that script using the `cProfile` profiler as a parameter and
    save that output into a file. Then, we can just call `runsnake` and pass it the
    file path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Usage](img/B02088_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding screenshot, you can see the three main areas of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: The sortable list, which contains all the numbers returned by `cProfile`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function-specific info section, which has several tabs of interest, such
    as the **Callees**, **Callers** and **Source Code** tabs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The square map section, which graphically represents the call tree of the execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A nice little feature that the GUI has is that it'll highlight the related box
    on the right-hand side if you hover your mouse over a function in the list from
    the left-hand side. The same thing will happen if you hover over a box on the
    right-hand side; its corresponding entry in the list will be highlighted.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling examples – the lowest common multiplier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's take a look at a very basic, non-practical example of a function in need
    of serious optimization and what it would look like using this GUI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our example function takes care of finding the lowest common multiplier between
    two numbers. It''s a pretty basic example: one you can find all over the Internet.
    However, it''s also a good place to start getting a feel of this UI.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The function''s code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: I'm pretty sure you can spot every single possible optimization just by looking
    at it, but stay with me. Let's profile this bad boy and load up the resulting
    output on RunSnakeRun.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to run it, use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To start the GUI, use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Profiling examples – the lowest common multiplier](img/B02088_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: One thing we didn't mention earlier, but that is a nice add-on to the square
    map, is the fact that next to each box's name, we can see how much time it takes
    to run that function.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, at first sight, we can spot several issues already:'
  prefs: []
  type: TYPE_NORMAL
- en: We see that both `max` and `min` functions only take up to 0,228 seconds out
    of the total 0,621 seconds that our function takes to run. So, there is more to
    our function than simply max and min.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also see that both `max` and `min` functions are called **943,446** times
    each. No matter how small the lookup time is, if you call a function almost 1
    million times it's going to add up.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s perform some obvious fixes to our code and see how it looks again, through
    the *eyes of the snake*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get something like what''s shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Profiling examples – the lowest common multiplier](img/B02088_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, neither `min` nor `max` even register on the square map. This is because
    we're just only calling them once, and the function went from 0.6 seconds to 0.1
    second. This is the power of not doing unnecessary function lookups for you folks.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's take a look at another, more complex, and thus, interesting function
    in dire need of optimization.
  prefs: []
  type: TYPE_NORMAL
- en: A profiling example – search using the inverted index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the previous chapter, we've been over the code of the inverted index from
    all possible angles. This is great, since we've analyzed it from several perspectives
    and using different approaches. However, it would make no sense to also look at
    it using RunSnakeRun, since this tool is very similar to the one we just tried
    (KCacheGrind).
  prefs: []
  type: TYPE_NORMAL
- en: 'So instead, let''s use the output of the inverted search script and code ourselves,
    a search script that will use that output. We will initially shoot for a simple
    search function that will only look for one single word in the index. The steps
    are quite straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the index in memory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Search for the word and grab the indexing information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parse the indexing information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each index entry, read the corresponding file and grab the surrounding string
    as a result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s the initial version of our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the code, just run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output we will get is similar to the following screenshot (given we have
    a few books inside the `files` folder):'
  prefs: []
  type: TYPE_NORMAL
- en: '![A profiling example – search using the inverted index](img/B02088_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The output could be improved by highlighting the search term or showing some
    of the previous words for more context. However, we'll run with it for the time
    being.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see how our code looks when we open the `search.prof` file inside
    `RunSnakeRun`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A profiling example – search using the inverted index](img/B02088_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: That's a lot of boxes, especially comparing it to our previous example of the
    lowest common multiplier. However, let's see what insight can be gathered from
    it at first sight.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two most time-consuming functions are `loadIndex` and `list2dict`, closely
    followed by `readFileContent`. We can see this on the left-side column:'
  prefs: []
  type: TYPE_NORMAL
- en: All these functions are actually spending most of their time inside other functions
    they call. So, their cumulative time is high, but their local time is considerably
    lower.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we sort by local time on the list, we would see that the top five functions
    are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `read` method from the file object
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The `loadIndex` function
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The `list2dict` function
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The `findAll` method of the regular expression object
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: And the `readFileContent` function
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, let''s first take a look at the `loadIndex` function. Even though most
    of its time is spent inside the `list2dict` function, we still have one minor
    optimization to do, which will simplify its code and significantly reduce its
    local time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This simple change took the local time of the function from 0.03s down to 0.00002s.
    Even though it wasn't already a big pain, we both increased its readability and
    improved its time. So, overall, we did well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, based on the last analysis, we knew that most of the time spent inside
    this function was actually spent inside another one called by it. So, now that
    we basically decreased its local time to almost nothing, we need to focus on our
    next target: `list2dict`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, first, let''s see how the picture has changed with the simple improvement
    we did earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A profiling example – search using the inverted index](img/B02088_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's move on to `list2dict`. This function is the one in charge of parsing
    every line of the index file into something we can use later. It will parse every
    line of the index file, more specifically, into a hash table (or dictionary) indexed
    by a word, which will make our search be of O(1) in average (read back to [Chapter
    1](ch01.html "Chapter 1. Profiling 101"), *Profiling 101,* if you don't remember
    what this means) when we search. The values of the dictionary are the path to
    the actual files and the different offsets where the word is.
  prefs: []
  type: TYPE_NORMAL
- en: 'From our analysis, we can see that though we spend some time inside the function
    itself, most of the complexity is inside the regular expression methods. Regular
    expressions are great for many reasons, but sometimes, we tend to overuse them
    in cases where using simple `split` and `replace` functions would do. So, let''s
    see how we can parse our data, get the same output without the regular expressions,
    and see if we can do it in less `time:def list2dict(l)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The code looks cleaner already. There are no regular expressions anywhere (which
    will help readability sometimes, since not everyone is an expert in reading regular
    expressions). We have less lines of code. We removed the `join` line, and we even
    got rid of the nasty `del` line, which was not necessary.
  prefs: []
  type: TYPE_NORMAL
- en: We, however, added a list comprehension line, but this is just a simple `replace`
    method on every item of the list in one line, that's all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what our map looks like now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A profiling example – search using the inverted index](img/B02088_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Well, there is definitely a change there. If you compare the last two screenshots,
    you would notice the box for the `list2dict` function has moved to the right.
    This means it now takes less time than the `readFileContent` function. Our function''s
    box is also simpler now. The only things inside it are the `split` and the `replace`
    methods. Finally, in case there was any doubt, let''s look at the numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: Local time went down from 0.024s to 0.019s. It makes sense that the local time
    didn't decrease that much, because we're still doing all the work inside the function.
    This decrease is mainly due to the absence of the `del` line and the `join` line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The total cumulative time decreased considerably. It went down from 0.094s to
    0.031s, due to the lack of complex functions (regular expressions) used for the
    job.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We took the total cumulative time of the function down to a third of what is
    was. So, it was a good optimization, especially considering that if we had larger
    indexes, then the time would be much bigger.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last assumption is not always true. It depends greatly on the type of algorithm
    being used. However, in our case, since we're looping over all the lines of the
    index file, we can safely assume it is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a quick look at the numbers from the first analysis of the code
    and the last one so that we can see if there is actually an improvement on the
    overall time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A profiling example – search using the inverted index](img/B02088_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Finally, as you can see, we went from around 0.2 seconds of execution with the
    original code all the way down to 0.072 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the final version of the code, all put together with the earlier improvements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To summarize, in this chapter, we covered two of the most popular and common
    tools used by Python developers trying to make sense of the numbers returned by
    profilers such as `cProfile`. We analyzed the old code under this new light. We
    even got to analyze some new code.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll start talking about optimization in more detail.
    We will cover some of the things we've already seen in practice and some recommendations
    of good practices when profiling and optimizing code.
  prefs: []
  type: TYPE_NORMAL
