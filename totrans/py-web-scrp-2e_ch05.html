<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Dynamic Content</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Dynamic Content</h1>
            </header>

            <article>
                
<p>According to a&#160;2006 study by the United Nations, 73 percent of leading websites rely on JavaScript for important functionalities (refer to <a href="http://www.un.org/esa/socdev/enable/documents/execsumnomensa.doc" target="_blank"><span class="URLPACKT">http://www.un.org/esa/socdev/enable/documents/execsumnomensa.doc</span></a>). The growth and popularity of model-view-controller (or MVC) frameworks within JavaScript such as React, AngularJS, Ember, Node and many more have only increased the importance of JavaScript as the primary engine for web page content.</p>
<p>The use of JavaScript can vary from simple form events to single page apps that download the entire page content after loading. One&#160;consequence of this architecture is the content may not available in the original HTML, and the scraping techniques we've covered so far will not extract the important information on the site.</p>
<p>This chapter will cover two approaches to scraping data from dynamic JavaScript websites. These are as follows:</p>
<ul>
<li>Reverse engineering JavaScript</li>
<li>Rendering JavaScript</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">An example dynamic web page</h1>
            </header>

            <article>
                
<p>Let's look at an example dynamic web page. The example website has a search form, which is available at <a href="http://example.webscraping.com/search" target="_blank"><span class="URLPACKT">http://example.webscraping.com/search</span></a>, which is used to locate countries. Let's say we want to find all the countries that begin with the letter A:</p>
<div class="CDPAlignCenter CDPAlign"><img height="430" width="384" class="image-border" src="images/4364OS_05_02.png"/></div>
<p>If we right-click on these results to inspect them with our browser tools&#160;(as covered in <a href="py-web-scrp-2e_ch02.html" target="_blank"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Scraping the Data</em>), we would find the results are stored within a <kbd>div</kbd> element with ID <kbd>"results"</kbd>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="507" width="459" class="image-border" src="images/4364OS_05_03.png"/></div>
<p>Let's try to extract these results using the <kbd>lxml</kbd> module, which was also covered in <a href="py-web-scrp-2e_ch02.html" target="_blank"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Scraping the Data</em>, and the <kbd>Downloader</kbd> class from <a href="py-web-scrp-2e_ch03.html" target="_blank"><span class="ChapterrefPACKT">Chapter 3</span></a>, <em>Caching Downloads</em>:</p>
<pre><strong>&gt;&gt;&gt; from lxml.html import fromstring</strong><br/><strong>&gt;&gt;&gt; from downloader import Downloader </strong><br/><strong>&gt;&gt;&gt; D = Downloader()  </strong><br/><strong>&gt;&gt;&gt; html = D('http://example.webscraping.com/search') </strong><br/><strong>&gt;&gt;&gt; tree = fromstring(html) </strong><br/><strong>&gt;&gt;&gt; tree.cssselect('div#results a') </strong><br/><strong>[]</strong> 
</pre>
<p>The example scraper here has failed to extract results. Examining the source code of this web page (by using the right-click View Page Source option instead of&#160;using the browser tools) can help you understand why. Here, we find the <kbd>div</kbd> element we are trying to scrape is empty:</p>
<pre>&lt;div id="results"&gt; <br/>&lt;/div&gt; 
</pre>
<p>Our browser tools&#160;give us a view of the current state of the web page. In this case, it means the web page has used JavaScript to load search results dynamically. In the next section, we will use another feature of our browser tools&#160;to understand how these results are loaded.</p>
<div class="packt_infobox"><span class="packt_screen">What is AJAX?<br/></span><strong>AJAX</strong> stands for <strong>Asynchronous JavaScript and XML</strong> and was coined in 2005 to describe the features available across web browsers that make dynamic web applications possible. Most importantly, the JavaScript <kbd>XMLHttpRequest</kbd> object, which was originally implemented by Microsoft for ActiveX, became&#160;available in many common web browsers. This allowed JavaScript to make HTTP requests to a remote server and receive responses, which meant that a web application could send and receive data. &#160;The previous&#160;way to communicate between client and server was to refresh the entire web page, which resulted in a poor user experience and wasted bandwidth when only a small amount of data needed to be transmitted.<br/>
Google's Gmail and Maps sites were early examples of the dynamic web applications and helped make&#160;AJAX mainstream.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Reverse engineering a dynamic web page</h1>
            </header>

            <article>
                
<p>So far, we tried to scrape data from a web page the same way as introduced in <a href="py-web-scrp-2e_ch02.html" target="_blank"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Scraping the Data</em>. This method&#160;did not work because the data is loaded dynamically using&#160;JavaScript. To scrape this data, we need to understand how the web page loads the data, a process which can be described&#160;as reverse engineering. Continuing the example from the preceding section, in our browser tools, if we click on the <span class="packt_screen">Network</span>&#160;tab and then perform a search, we will see all of the requests made for a given page. There are a lot! If we scroll up through the requests, we see mainly photos (from loading country flags), and then we notice one with an interesting name: <kbd>search.json</kbd> with a path of&#160;<kbd>/ajax</kbd>:</p>
<p><img class="image-border" src="images/network_view.png"/></p>
<p>If we click on that URL using Chrome, we can see more details (there is similar functionality for this in all major browsers, so your view may vary; however the main features should function similarly). Once we click on the URL of interest, we can see more details, including a preview which shows us the response in parsed form. Here, similar to the Inspect Element view in our Elements tab, we use the carrots to expand the preview and see that each country of our results is included in JSON form:</p>
<p><img class="image-border" src="images/preview_ajax.png"/></p>
<p>We can also open the URL directly by right-clicking and opening the URL in a new tab. When you do so, you will see it as a simple JSON response. This AJAX data is not only accessible from within the Network tab or via a browser, but can also be downloaded directly, as follows:</p>
<pre><strong>&gt;&gt;&gt; import requests</strong><br/><strong>&gt;&gt;&gt; resp = requests.get('http://example.webscraping.com/ajax/search.json?page=0&amp;page_size=10&amp;search_term=a') </strong><br/><strong>&gt;&gt;&gt; resp.json()</strong><br/><strong>{'error': '', </strong><br/><strong> 'num_pages': 22, </strong><br/><strong> 'records': [{'country': 'Afghanistan', </strong><br/><strong>    'id': 1261, </strong><br/><strong>    'pretty_link': '&lt;div&gt;&lt;a href="/view/Afghanistan-1"&gt;&lt;img        src="/places/static/images/flags/af.png" /&gt;Afghanistan&lt;/a&gt;&lt;/div&gt;'}, </strong><br/><strong>  ...] </strong><br/><strong>}</strong> 
</pre>
<p>As we can see from the previous code, the <kbd>requests</kbd> library allows us to access JSON responses as a Python dictionary by using the&#160;<kbd>json</kbd> method. We could also download the raw string response and load it using&#160;Python's&#160;<kbd>json.loads</kbd> method.&#160;</p>
<p>Our code gives us&#160;a simple way to scrape countries containing the letter <kbd>A</kbd>. To find the details of the&#160;countries requires calling the AJAX search with each letter of the alphabet. For each letter, the search results are split into pages, and the number of pages is indicated by <kbd>page_size</kbd> in the response.</p>
<p>Unfortunately, we cannot&#160;save all results returned because the same countries will be returned in multiple searches-for example, <kbd>Fiji</kbd> matches searches for <kbd>f</kbd>, <kbd>i</kbd>, and <kbd>j</kbd>. These duplicates are filtered here by storing results in a set before writing them to a text file-the set data structure ensures unique&#160;elements.</p>
<p>Here is an example implementation that scrapes all of the countries by searching for each letter of the alphabet and then iterating the resulting pages of the JSON responses. The results are then stored in a&#160;simple text file.</p>
<pre>import requests<br/>import string<br/><br/>PAGE_SIZE = 10<br/><br/>template_url = 'http://example.webscraping.com/ajax/' + <br/>    'search.json?page={}&amp;page_size={}&amp;search_term={}'<br/><br/>countries = set()<br/><br/>for letter in string.ascii_lowercase:<br/>    print('Searching with %s' % letter)<br/>    page = 0<br/>    while True:<br/>        resp = requests.get(template_url.format(page, PAGE_SIZE, letter))<br/>        data = resp.json()<br/>        print('adding %d more records from page %d' %<br/>              (len(data.get('records')), page))<br/>        for record in data.get('records'):<br/>            countries.add(record['country'])<br/>        page += 1<br/>        if page &gt;= data['num_pages']:<br/>            break<br/><br/>with open('../data/countries.txt', 'w') as countries_file:<br/>    countries_file.write('n'.join(sorted(countries)))
</pre>
<p>When you run the code, you will see progressive output:</p>
<pre><strong>$ python chp5/json_scraper.py</strong><br/><strong>Searching with a</strong><br/><strong>adding 10 more records from page 0</strong><br/><strong>adding 10 more records from page 1</strong><br/><strong>...</strong>
</pre>
<p>Once the script is completed, the&#160;<kbd>countries.txt</kbd> file in the relative folder&#160;<kbd>../data/</kbd> will show a sorted list of the country names. You may also note the page length can be set using the&#160;<kbd>PAGE_SIZE</kbd> global variable. You may want to try toggling this to increase or decrease the number of requests.</p>
<p>This AJAX scraper provides a simpler way to extract the country details than the traditional page-by-page scraping approach covered in <a href="py-web-scrp-2e_ch02.html" target="_blank"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Scraping the Data</em>. This is a common experience: AJAX-dependent websites initially look more complex, however their structure encourages separating the data and presentation layers, which can actually make our job of extracting data easier. If you find a site with an open Application Programming Interface (or API) like this example site, you can simply scrape the API rather than using CSS selectors and XPath to load data from HTML.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Edge cases</h1>
            </header>

            <article>
                
<p>The AJAX search script is quite simple, but it can be simplified further by utilizing possible&#160;edge cases. So far, we have queried each letter, which means 26 separate queries, and there are duplicate results between these queries. It would be ideal if a single search query could be used to match all results. We will try experimenting with different characters to see if this is possible. This is what happens if the search term is left empty:</p>
<pre><strong>&gt;&gt;&gt; url = 'http://example.webscraping.com/ajax/search.json?page=0&amp;page_size=10&amp;search_term=' </strong><br/><strong>&gt;&gt;&gt; requests.get(url).json()['num_pages'] </strong><br/><strong>0</strong> 
</pre>
<p>Unfortunately, this did not work-there are no results. Next we will check if '*' will match all results:</p>
<pre><strong>&gt;&gt;&gt; requests.get(url + '*').json()['num_pages'] </strong><br/><strong>0</strong> 
</pre>
<p>Still no luck. Then we check <kbd>'.'</kbd>, which is a regular expression to match any character:</p>
<pre><strong>&gt;&gt;&gt; requests.get(url + '.').json()['num_pages'] </strong><br/><strong>26</strong>
</pre>
<p>Perfect!&#160;The server must be matching results using&#160;regular expressions. So, now searching each letter can be replaced with a single search for the dot character.</p>
<p>Furthermore, we can&#160;set the page size in the AJAX URLs using the <kbd>page_size</kbd> query string value. The web site search interface has options for setting this to 4, 10, and 20, with the default set to 10. So, the number of pages to download could be halved by increasing the page size to the maximum.</p>
<pre><strong>&gt;&gt;&gt; url = 'http://example.webscraping.com/ajax/search.json?page=0&amp;page_size=20&amp;search_term=.' </strong><br/><strong>&gt;&gt;&gt; requests.get(url).json()['num_pages'] </strong><br/><strong>13</strong> 
</pre>
<p>Now, what if a much higher page size is used, a size higher than what the web interface select box supports?</p>
<pre><strong>&gt;&gt;&gt; url = 'http://example.webscraping.com/ajax/search.json?page=0&amp;page_size=1000&amp;search_term=.' </strong><br/><strong>&gt;&gt;&gt; requests.get(url).json()['num_pages'] </strong><br/><strong>1</strong> 
</pre>
<p>Apparently, the server does not check whether the page size parameter matches the options allowed in the interface and now returns all the results in a single page. Many web applications do not check the page size parameter in their AJAX backend because they expect all API requests to only come via&#160;the web interface.</p>
<p>Now, we have crafted a URL to download the data for all countries in a single request. Here is the updated and much simpler implementation which saves the data to a CSV file:</p>
<pre>from csv import DictWriter<br/>import requests<br/><br/>PAGE_SIZE = 1000<br/><br/>template_url = 'http://example.webscraping.com/ajax/' + <br/> 'search.json?page=0&amp;page_size={}&amp;search_term=.'<br/><br/>resp = requests.get(template_url.format(PAGE_SIZE))<br/>data = resp.json()<br/>records = data.get('records')<br/><br/>with open('../data/countries.csv', 'w') as countries_file:<br/>   wrtr = DictWriter(countries_file, fieldnames=records[0].keys())<br/>   wrtr.writeheader()<br/>   wrtr.writerows(records)
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Rendering a dynamic web page</h1>
            </header>

            <article>
                
<p>For the example search web page, we were able to&#160;quickly reverse engineer how the API worked and how to use it to retrieve the results in one request. However, websites can&#160;be very complex and difficult to understand, even with advanced browser tools. For example, if the website has been built with <strong>Google Web Toolkit</strong> (<strong>GWT</strong>), the resulting JavaScript code will be machine-generated and minified. This generated JavaScript code can be cleaned with a tool such as <kbd>JS beautifier</kbd>, but the result will be verbose and the original variable names will be lost, so it is difficult to understand and reverse engineer.</p>
<p>Additionally, higher level frameworks like <kbd>React.js</kbd> and other Node.js-based tools can further abstract already complex JavaScript logic and obfuscate data and variable names and add more layers of&#160;API request security (by requiring cookies, browser sessions and timestamps or using other anti-scraper technologies).</p>
<p>With enough effort, any website can be reverse engineered. However, this effort can be avoided by instead using a browser rendering engine, which is the part of the web browser that parses HTML, applies the CSS formatting, and executes JavaScript to display a web page. In this section, the WebKit rendering engine will be used, which has a convenient Python interface through the Qt framework.</p>
<div class="packt_infobox"><span class="packt_screen">What is WebKit?<br/></span>The code for WebKit started life as the KHTML project in 1998, which was the rendering engine for the Konqueror web browser. It was then forked by Apple as WebKit in 2001 for use in their Safari web browser. Google used WebKit up to Chrome Version 27 before forking their version from WebKit called <strong>Blink</strong> in 2013. Opera originally used their internal rendering engine called <strong>Presto</strong> from 2003 to 2012 before briefly switching to WebKit, and then followed Chrome to Blink. Other popular browser rendering engines are <strong>Trident,</strong> used by Internet Explorer, and <strong>Gecko</strong> by Firefox.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">PyQt or PySide</h1>
            </header>

            <article>
                
<p>There are two available Python bindings to the Qt framework, <kbd>PyQt</kbd> and <kbd>PySide</kbd>. <kbd>PyQt</kbd> was first released in 1998 but requires a license for commercial projects. Due to this licensing problem, the company developing Qt, then Nokia and now Digia, later developed Python bindings in 2009 called <kbd>PySide</kbd> and released it under the more permissive LGPL license.</p>
<p>There are minor differences between the two bindings but the examples developed here will work with either. The following snippet can be used to import whichever Qt binding is installed:</p>
<pre>try: <br/>    from PySide.QtGui import * <br/>    from PySide.QtCore import * <br/>    from PySide.QtWebKit import * <br/>except ImportError: <br/>    from PyQt4.QtGui import * <br/>    from PyQt4.QtCore import * <br/>    from PyQt4.QtWebKit import * 
</pre>
<p>Here, if <kbd>PySide</kbd> is not available, an <kbd>ImportError</kbd> exception will be raised and <kbd>PyQt</kbd> will be imported. If <kbd>PyQt</kbd> is also unavailable, another <kbd>ImportError</kbd> will be raised and the script will exit.</p>
<div class="packt_infobox">The instructions to download and install each of the Python bindings for Qt are available at <a href="http://qt-project.org/wiki/Setting_up_PySide" target="_blank"><span class="URLPACKT">http://qt-project.org/wiki/Setting_up_PySide</span></a> and <a href="http://pyqt.sourceforge.net/Docs/PyQt4/installation.html" target="_blank"><span class="URLPACKT">http://pyqt.sourceforge.net/Docs/PyQt4/installation.html</span></a>. Depending on the version of Python 3 you are using, there might not be availability yet for the library, but releases are somewhat frequent so you can always check back soon.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Debugging with Qt</h1>
            </header>

            <article>
                
<p>Whether you are using PySide or PyQt, you will likely run into sites where you need to debug the application or script. We have already covered one way to do so, by utilizing t<span>he</span> <kbd>QWebView</kbd> <span>GUI</span> <kbd>show()</kbd> <span>method to "see" what is being rendered on the page you've loaded. You can also use the&#160;</span><kbd>page().mainFrame().toHtml()</kbd> chain (easily referenced when using the&#160;<kbd>BrowserRender</kbd> class via the&#160;<kbd>html</kbd> method to pull the HTML at any point, write it to a file and save and then open it in your browser.</p>
<p>In addition, there are several useful Python debuggers, such as&#160;<kbd>pdb</kbd> which you can integrate into your script and then use breakpoints to step through the code where the error, issue or bug is expressed. There are several different ways to set this up and specific to whichever library and Qt version you have installed, so we recommend searching for the exact setup you have and reviewing implementation to allow setting breakpoints or trace.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Executing JavaScript</h1>
            </header>

            <article>
                
<p>To confirm your WebKit installation can execute JavaScript, there is a simple example available at <a href="http://example.webscraping.com/dynamic" target="_blank"><span class="URLPACKT">http://example.webscraping.com/dynamic</span></a>.</p>
<p>This web page simply uses JavaScript to write <kbd>Hello World</kbd> to a <kbd>div</kbd> element. Here is the source code:</p>
<pre>&lt;html&gt; <br/>    &lt;body&gt; <br/>        &lt;div id="result"&gt;&lt;/div&gt; <br/>        &lt;script&gt; <br/>        document.getElementById("result").innerText = 'Hello World'; <br/>        &lt;/script&gt; <br/>    &lt;/body&gt; <br/>&lt;/html&gt; 
</pre>
<p>With the traditional approach of downloading the original HTML and parsing the result, the <kbd>div</kbd> element will be empty, as follows:</p>
<pre><strong>&gt;&gt;&gt; import lxml.html</strong><br/><strong>&gt;&gt;&gt; from chp3.downloader import Downloader</strong><br/><strong>&gt;&gt;&gt; D = Downloader()</strong><br/><strong>&gt;&gt;&gt; url = 'http://example.webscraping.com/dynamic' </strong><br/><strong>&gt;&gt;&gt; html = D(url) </strong><br/><strong>&gt;&gt;&gt; tree = lxml.html.fromstring(html) </strong><br/><strong>&gt;&gt;&gt; tree.cssselect('#result')[0].text_content() </strong><br/><strong>''</strong> 
</pre>
<p>Here is an initial example with WebKit, which needs to follow the <kbd>PyQt</kbd> or <kbd>PySide</kbd> imports shown in the previous section:</p>
<pre><strong>&gt;&gt;&gt; app = QApplication([]) </strong><br/><strong>&gt;&gt;&gt; webview = QWebView() </strong><br/><strong>&gt;&gt;&gt; loop = QEventLoop() </strong><br/><strong>&gt;&gt;&gt; webview.loadFinished.connect(loop.quit) </strong><br/><strong>&gt;&gt;&gt; webview.load(QUrl(url)) </strong><br/><strong>&gt;&gt;&gt; loop.exec_() </strong><br/><strong>&gt;&gt;&gt; html = webview.page().mainFrame().toHtml() </strong><br/><strong>&gt;&gt;&gt; tree = lxml.html.fromstring(html) </strong><br/><strong>&gt;&gt;&gt; tree.cssselect('#result')[0].text_content() </strong><br/><strong>'Hello World'</strong> 
</pre>
<p>There is quite a lot going on here, so we will step through the code line by line:</p>
<ul>
<li>The first line instantiates the <kbd>QApplication</kbd> object that the Qt framework requires before other Qt objects can be initialized.</li>
<li>Next, a <kbd>QWebView</kbd> object is created, which is a widget&#160;for the web documents.</li>
<li>A <kbd>QEventLoop</kbd> object is created, which is&#160;used to create a local event loop.</li>
<li>The <kbd>loadFinished</kbd> callback of the <kbd>QwebView</kbd> object is linked to the <kbd>quit</kbd> method of <kbd>QEventLoop</kbd> so when a web page finishes loading, the event loop will stop.</li>
<li>The URL to load is then passed to <kbd>QWebView</kbd>. <kbd>PyQt</kbd> requires this URL string to be&#160;wrapped in&#160;a <kbd>QUrl</kbd> object, but&#160;for <kbd>PySide</kbd>, this is optional.</li>
<li>The <kbd>QWebView</kbd> loads asynchronously, so execution immediately passes to the next line while the web page is loading-however, we want to wait until this web page is loaded, so <kbd>loop.exec_()</kbd> is called to start the event loop.</li>
<li>When the web page completes loading, the event loop will exit and code execution continues.&#160;The resulting HTML from the loaded web page is extracted using the <kbd>toHTML</kbd> method.</li>
<li>The final line shows the JavaScript has been successfully executed and the <kbd>div</kbd> element contains <kbd>Hello World</kbd>.</li>
</ul>
<p>The classes and methods used here are all excellently documented in the C++ Qt framework website at <a href="http://qt-project.org/doc/qt-4.8/" target="_blank"><span class="URLPACKT">http://qt-project.org/doc/qt-4.8/</span></a>. <kbd>PyQt</kbd> and <kbd>PySide</kbd> have their own documentation, however, the descriptions and formatting for the original C++ version is superior, and, generally Python developers use it instead.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Website interaction with WebKit</h1>
            </header>

            <article>
                
<p>The search web page we have been examining requires the user to modify and submit a search form, and then click on the page links. However, so far, our browser renderer can only execute JavaScript and access the resulting HTML. Scraping&#160;the search page requires extending the browser renderer to support these interactions. Fortunately, Qt has an excellent API to select and manipulate the HTML elements, which makes implementation&#160;straightforward.</p>
<p>Here is an alternative version to the earlier AJAX search example, which sets the search term to <kbd>'.'</kbd> and page size to <kbd>'1000'</kbd>&#160;and loads all results in a single query:</p>
<pre>app = QApplication([]) <br/>webview = QWebView() <br/>loop = QEventLoop() <br/>webview.loadFinished.connect(loop.quit) <br/>webview.load(QUrl('http://example.webscraping.com/search')) <br/>loop.exec_() <br/>webview.show() <br/>frame = webview.page().mainFrame() <br/>frame.findFirstElement('#search_term'). <br/>       setAttribute('value', '.') <br/>frame.findFirstElement('#page_size option:checked'). <br/>       setPlainText('1000') <br/>frame.findFirstElement('#search'). <br/>       evaluateJavaScript('this.click()') <br/>app.exec_() 
</pre>
<p>The first few lines instantiate the Qt objects required to render a web page, the same as in the previous <kbd>Hello World</kbd> example. Next, the <kbd>QWebView</kbd> GUI <kbd>show()</kbd> method is called so that the render window is displayed, which is useful for debugging. Then, a reference to the frame is created to make the following lines shorter.</p>
<p>The <kbd>QWebFrame</kbd> class has many useful methods to interact with web pages. The three lines containing <kbd>findFirstElement</kbd>&#160;use the CSS selectors to locate an element in the frame, and set the search parameters. Then, the form is submitted with the <kbd>evaluateJavaScript()</kbd> method which simulates the click event. This method is very convenient because it allows insertion and execution of any JavaScript code we submit, including calling JavaScript methods defined in the web page directly. Then, the final line enters the application event loop so we can review what is&#160;happening in&#160;the form. Without this, the script would exit immediately.</p>
<p>This is displayed when this script is run:</p>
<div class="CDPAlignCenter CDPAlign"><img height="417" width="555" class="image-border" src="images/pyqt_search.png"/></div>
<p>The final line of code we ran&#160;<kbd>app._exec()</kbd> is a blocking call and will prevent any more lines of code in this particular thread from executing. Having a view of how your code is functioning by using <kbd>webkit.show()</kbd> is a great way to debug your application and determine what is really happening on the web page.&#160;</p>
<p>To stop the running application, you can simply close the Qt window (or the Python interpreter).&#160;</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Waiting for results</h1>
            </header>

            <article>
                
<p>The final part of implementing our WebKit crawler is scraping the search results, which turns out to be the most difficult part because it isn't obvious when the AJAX event is complete and the country data is loaded. There are three possible approaches to deal with this conundrum:</p>
<ul>
<li>Wait a set amount of time and hope the AJAX event is complete</li>
<li>Override Qt's network manager to track when URL requests are complete</li>
<li>Poll the web page for the expected content to appear</li>
</ul>
<p>The first option is the simplest to implement but it's inefficient, since&#160;if a safe timeout is set, usually the script spends too much time waiting. Also, when the network is slower than usual, a fixed timeout could fail. The second option is more efficient but cannot be applied when there are client-side delays; for example, if the download is complete, but a button needs to be pressed before content is displayed. The third option is more&#160;reliable and straightforward to implement; though there is the minor drawback of wasting CPU cycles when checking whether the content has loaded. Here is an implementation for the third option:</p>
<pre><strong>&gt;&gt;&gt; elements = None </strong><br/><strong>&gt;&gt;&gt; while not elements: </strong><br/><strong>...     app.processEvents() </strong><br/><strong>...     elements = frame.findAllElements('#results a') </strong><br/><strong>... </strong><br/><strong>&gt;&gt;&gt; countries = [e.toPlainText().strip() for e in elements] </strong><br/><strong>&gt;&gt;&gt; print(countries)</strong><br/><strong>['Afghanistan', 'Aland Islands', ... , 'Zambia', 'Zimbabwe']</strong> 
</pre>
<p>Here, the code will remain in the&#160;<kbd>while</kbd>&#160;loop until the country links are present in the <kbd>results</kbd> div. For each loop, <kbd>app.processEvents()</kbd> is called to give the Qt event loop time to perform tasks, such as responding to click events and updating the GUI. We could additionally add a&#160;<kbd>sleep</kbd> for a short period of seconds in this loop to give the CPU intermittent breaks.</p>
<p>A full example of the code so far can be found at <a href="https://github.com/kjam/wswp/blob/master/code/chp5/pyqt_search.py">https://github.com/kjam/wswp/blob/master/code/chp5/pyqt_search.py</a>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">The Render class</h1>
            </header>

            <article>
                
<p>To help make this functionality easier to use in future, here are the methods used and packaged into a class, whose source code is also available at <a href="https://github.com/kjam/wswp/blob/master/code/chp5/browser_render.py"><span class="URLPACKT">https://github.com/kjam/wswp/blob/master/code/chp5/browser_render.py</span></a>:</p>
<pre>import time <br/><br/>class BrowserRender(QWebView): <br/>    def __init__(self, show=True): <br/>        self.app = QApplication(sys.argv) <br/>        QWebView.__init__(self) <br/>        if show: <br/>            self.show() # show the browser <br/><br/>    def download(self, url, timeout=60): <br/>        """Wait for download to complete and return result""" <br/>        loop = QEventLoop() <br/>        timer = QTimer() <br/>        timer.setSingleShot(True) <br/>        timer.timeout.connect(loop.quit) <br/>        self.loadFinished.connect(loop.quit) <br/>        self.load(QUrl(url)) <br/>        timer.start(timeout * 1000) <br/>        loop.exec_() # delay here until download finished <br/>        if timer.isActive(): <br/>            # downloaded successfully <br/>            timer.stop() <br/>            return self.html() <br/>        else: <br/>            # timed out <br/>            print 'Request timed out: ' + url <br/><br/>    def html(self): <br/>        """Shortcut to return the current HTML""" <br/>        return self.page().mainFrame().toHtml() <br/><br/>    def find(self, pattern): <br/>        """Find all elements that match the pattern""" <br/>        return self.page().mainFrame().findAllElements(pattern) <br/><br/>    def attr(self, pattern, name, value): <br/>        """Set attribute for matching elements""" <br/>        for e in self.find(pattern): <br/>            e.setAttribute(name, value) <br/><br/>    def text(self, pattern, value): <br/>        """Set attribute for matching elements""" <br/>        for e in self.find(pattern): <br/>            e.setPlainText(value) <br/><br/>    def click(self, pattern): <br/>        """Click matching elements""" <br/>        for e in self.find(pattern): <br/>            e.evaluateJavaScript("this.click()") <br/><br/>    def wait_load(self, pattern, timeout=60): <br/>        """Wait until pattern is found and return matches""" <br/>        deadline = time.time() + timeout <br/>        while time.time() &lt; deadline: <br/>            self.app.processEvents() <br/>            matches = self.find(pattern) <br/>            if matches: <br/>                return matches <br/>        print('Wait load timed out') 
</pre>
<p>You may have noticed the <kbd>download()</kbd> and <kbd>wait_load()</kbd> methods have some additional code involving a timer. This timer tracks how long is&#160;spent waiting and cancels the event loop if&#160;the deadline is reached. Otherwise, when a network problem is encountered, the event loop would run indefinitely.</p>
<p>Here is how to scrape the search page using this new class:</p>
<pre><strong>&gt;&gt;&gt; br = BrowserRender() </strong><br/><strong>&gt;&gt;&gt; br.download('http://example.webscraping.com/search') </strong><br/><strong>&gt;&gt;&gt; br.attr('#search_term', 'value', '.') </strong><br/><strong>&gt;&gt;&gt; br.text('#page_size option:checked', '1000') </strong><br/><strong>&gt;&gt;&gt; br.click('#search') </strong><br/><strong>&gt;&gt;&gt; elements = br.wait_load('#results a') </strong><br/><strong>&gt;&gt;&gt; countries = [e.toPlainText().strip() for e in elements] </strong><br/><strong>&gt;&gt;&gt; print countries</strong><br/><strong>['Afghanistan', 'Aland Islands', ... , 'Zambia', 'Zimbabwe']</strong> 
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Selenium</h1>
            </header>

            <article>
                
<p>With the WebKit library used in the previous&#160;section, we have full control to customize the browser renderer to behave as we need it to. If this level of flexibility is not needed, a good and easier-to-install alternative is Selenium, which provides an API to automate several popular web browsers. Selenium can be installed using <kbd>pip</kbd> with the following command:</p>
<pre><strong>pip install selenium</strong>
</pre>
<p>To demonstrate how Selenium works, we will rewrite the previous search example in Selenium. The first step is to create a connection to the web browser:</p>
<pre><strong>&gt;&gt;&gt; from selenium import webdriver </strong><br/><strong>&gt;&gt;&gt; driver = webdriver.Firefox()</strong> 
</pre>
<p>When this command is run, an empty browser window will pop up. If you received an error instead, you likely need to install <kbd>geckodriver</kbd> (<a href="py-web-scrp-2e_ch04.html" target="_blank">https://github.com/mozilla/geckodriver/releases</a>) and ensure it is available&#160;via your&#160;<kbd>PATH</kbd> variables.</p>
<p>Using a browser you can see and interact with (rather than a Qt widget) is handy because with each command, the browser window can be checked to see if the script&#160;worked as expected. Here, we used Firefox, but Selenium also provides interfaces to other common web browsers, such as Chrome and Internet Explorer. Note that you can only use a Selenium interface for a web browser that is installed on your system.</p>
<div class="packt_infobox">To see if your system's browser is supported and what other dependencies or drivers you may need to install&#160;to&#160;use Selenium, check the Selenium documentation on supported platforms:&#160;<a href="http://www.seleniumhq.org/about/platforms.jsp" target="_blank">http://www.seleniumhq.org/about/platforms.jsp</a>.&#160;</div>
<p>To load a web page in the chosen web browser, the <kbd>get()</kbd> method is called:</p>
<pre><strong>&gt;&gt;&gt; driver.get('http://example.webscraping.com/search')</strong> 
</pre>
<p>Then, to set which element to select, the ID of the search textbox can be used. Selenium also supports selecting elements with a CSS selector or XPath. When the search textbox is found, we can enter content with the <kbd>send_keys()</kbd> method, which simulates typing:</p>
<pre><strong>&gt;&gt;&gt; driver.find_element_by_id('search_term').send_keys('.')</strong> 
</pre>
<p>To return all results in a single search, we want to set the page size to 1000. However, this is not straightforward because Selenium is designed to interact with the browser, rather than to modify the web page content. To get around this limitation, we can use JavaScript to set the select box content:</p>
<pre><strong>&gt;&gt;&gt; js = "document.getElementById('page_size').options[1].text = '1000';" </strong><br/><strong>&gt;&gt;&gt; driver.execute_script(js)</strong>
</pre>
<p>Now the form inputs are ready, so the search button can be clicked on to perform the search:</p>
<pre><strong>&gt;&gt;&gt; driver.find_element_by_id('search').click()</strong> 
</pre>
<p>We need to wait for the AJAX request to complete before loading the results, which was the hardest part of the script in the previous WebKit implementation. Fortunately, Selenium provides a simple solution to this problem by setting a timeout with the <kbd>implicitly_wait()</kbd> method:</p>
<pre><strong>&gt;&gt;&gt; driver.implicitly_wait(30)</strong> 
</pre>
<p>Here, a delay of 30 seconds was used. Now, if we search for elements that&#160;are not yet available, Selenium will wait up to 30 seconds before raising an exception. Selenium also allows for more detailed polling control using explicit waits (which are well-documented at <a href="http://www.seleniumhq.org/docs/04_webdriver_advanced.jsp" target="_blank">http://www.seleniumhq.org/docs/04_webdriver_advanced.jsp</a>).</p>
<p>To select the country links, we use the same CSS selector that we used in the WebKit example:</p>
<pre><strong>&gt;&gt;&gt; links = driver.find_elements_by_css_selector('#results a')</strong> 
</pre>
<p>Then, the text of each link can be extracted to create a list of countries:</p>
<pre><strong>&gt;&gt;&gt; countries = [link.text for link in links] </strong><br/><strong>&gt;&gt;&gt; print(countries)</strong><br/><strong>['Afghanistan', 'Aland Islands', ... , 'Zambia', 'Zimbabwe']</strong> 
</pre>
<p>Finally, the browser can be shut down by calling the <kbd>close()</kbd> method:</p>
<pre><strong>&gt;&gt;&gt; driver.close()</strong> 
</pre>
<p>The source code for this example is available at <a href="https://github.com/kjam/wswp/blob/master/code/chp5/selenium_search.py"><span class="URLPACKT">https://github.com/kjam/wswp/blob/master/code/chp5/selenium_search.py</span></a>. For further details about Selenium, the Python bindings are documented at <a href="https://selenium-python.readthedocs.org/" target="_blank"><span class="URLPACKT">https://selenium-python.readthedocs.org/</span></a>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Selenium and Headless Browsers</h1>
            </header>

            <article>
                
<p>Although it's convenient and fairly easy to install and use Selenium with common browsers; this can present problems when running these scripts on servers. For servers, it's more common to use headless browsers. They also tend to be faster and more configurable than fully-functional&#160;web browsers.</p>
<p>The most popular headless browser at the time of this publication is PhantomJS. It runs via its own JavaScript-based webkit engine. PhantomJS can be installed easily on most servers, and can be installed locally by&#160;following the latest download instructions (<a href="http://phantomjs.org/download.html" target="_blank">http://phantomjs.org/download.html</a>).&#160;</p>
<p>Using PhantomJS with Selenium merely requires a different initialization:</p>
<pre><strong>&gt;&gt;&gt; from selenium import webdriver</strong><br/><strong>&gt;&gt;&gt; driver = webdriver.PhantomJS()  # note: you should use the phantomjs executable path here  </strong><br/><strong>                                    # if you see an error (e.g. PhantomJS('/Downloads/pjs'))</strong>
</pre>
<p>The first difference you notice is no browser window is opened, but there is a PhantomJS instance running. To test our code, we can visit a page and take a screenshot.</p>
<pre><strong>&gt;&gt;&gt; driver.get('http://python.org')</strong><br/><strong>&gt;&gt;&gt; driver.save_screenshot('../data/python_website.png')</strong><br/><strong>True</strong>
</pre>
<p>Now if you open that saved PNG file, you can see what the PhantomJS browser has rendered:</p>
<div class="CDPAlignCenter CDPAlign"><img height="617" width="35" class="image-border" src="images/python_website.png"/></div>
<p>We notice it is a long window. We could change this by using <kbd>maximize_window</kbd> or setting a window size with <kbd>set_window_size</kbd>, both of which are documented in the <a href="http://selenium-python.readthedocs.io/api.html">Selenium Python documentation on the WebDriver API</a>.</p>
<p>Screenshot options are great for debugging any Selenium issues you have, even if you are using Selenium with a real browser -- since&#160;there are times the script may fail to work due to a slow-loading page or changes in the page structure or JavaScript on the site. Having a screenshot of the page exactly as it was at the time of the error can be very helpful. Additionally, you can use the driver's <kbd>page_source</kbd> attribute to save or inspect the current page source.&#160;</p>
<p>Another reason to utilize a browser-based parser like Selenium is it makes it more difficult to act like a scraper.&#160;Some sites&#160;use scraper-avoidance techniques like Honeypots, where the site might include a hidden&#160;toxic&#160;link on a page, which will get your scraper banned if your script clicks it. For these types of problems, Selenium acts as a great scraper because of its browser-based architecture. If you cannot click or see a link in the browser, you also cannot interact with it via Selenium. Additionally, your headers will include whichever browser you are using and you'll have access to normal browser features like cookies, sessions as well as loading images and interactive elements, which are sometimes required to load particular forms or pages. If your scraper must interact with the page and seem "human-like", Selenium is a great choice.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>This chapter covered two approaches to scraping data from dynamic web pages. It started with reverse engineering a dynamic web page using&#160;browser tools, and then moved on to using a browser renderer to trigger JavaScript events for us. We first used WebKit to build our own custom browser, and then reimplemented this scraper with the high-level Selenium framework.</p>
<p>A browser renderer can save the time needed to understand how the backend of a website works; however, there are some disadvantages. Rendering a web page adds overhead and is much slower than just downloading the HTML or using API calls. Additionally, solutions using a browser renderer often require polling the web page to check whether the resulting HTML has loaded, which is brittle and can fail when the network is slow.</p>
<p>I typically use a browser renderer for short-term solutions where the long-term performance and reliability is less important; for long-term solutions, I attempt&#160;to reverse engineer the website. Of course, some sites may require "human-like" interactions or have closed APIs, meaning a browser rendered implementation will likely&#160;be the only way to acquire content.</p>
<p>In the next chapter, we will cover how to interact with forms and cookies to log into a website and edit content.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>
</body>
</html>