- en: '*Chapter 1*: Benchmarking and Profiling'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recognizing the slow parts of your program is the single most important task
    when it comes to speeding up your code. In most cases, the code that causes the
    application to slow down is a very small fraction of the program. By identifying
    these critical sections, you can focus on the parts that need the most improvement
    without wasting time in micro-optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '**Profiling** is a technique that allows us to pinpoint the most resource-intensive
    parts of an application. A **profiler** is a program that runs an application
    and monitors how long each function takes to execute, thus detecting the functions
    on which your application spends most of its time.'
  prefs: []
  type: TYPE_NORMAL
- en: Python provides several tools to help us find these bottlenecks and measure
    important performance metrics. In this chapter, we will learn how to use the standard
    `cProfile` module and the `line_profiler` third-party package. We will also learn
    how to profile the memory consumption of an application through the `memory_profiler`
    tool. Another useful tool that we will cover is **KCachegrind**, which can be
    used to graphically display the data produced by various profilers.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, **benchmarks** are small scripts used to assess the total execution
    time of your application. We will learn how to write benchmarks and use them to
    accurately time your programs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics we will cover in this chapter are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Designing your application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing tests and benchmarks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing better tests and benchmarks with `pytest-benchmark`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding bottlenecks with `cProfile`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing our code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the `dis` module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling memory usage with `memory_profiler`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of the chapter, you will have gained a solid understanding of how
    to optimize a Python program and will be armed with practical tools that facilitate
    the optimization process.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To follow the content of this chapter, you should have a basic understanding
    of Python programming and be familiar with core concepts such as variables, classes,
    and functions. You should also be comfortable with working with the command line
    to run Python programs. Finally, the code for this chapter can be found in the
    following GitHub repository: [https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter01](https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter01).'
  prefs: []
  type: TYPE_NORMAL
- en: Designing your application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the early development stages, the design of a program can change quickly
    and may require large rewrites and reorganizations of the code base. By testing
    different prototypes without the burden of optimization, you are free to devote
    your time and energy to ensure that the program produces correct results and that
    the design is flexible. After all, who needs an application that runs fast but
    gives the wrong answer?
  prefs: []
  type: TYPE_NORMAL
- en: 'The mantras that you should remember when optimizing your code are outlined
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Make it run**: We have to get the software in a working state and ensure
    that it produces the correct results. This exploratory phase serves to better
    understand the application and to spot major design issues in the early stages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Make it right**: We want to ensure that the design of the program is solid.
    Refactoring should be done before attempting any performance optimization. This
    really helps separate the application into independent and cohesive units that
    are easy to maintain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Make it fast**: Once our program is working and well structured, we can focus
    on performance optimization. We may also want to optimize memory usage if that
    constitutes an issue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will write and profile a *particle simulator* test application.
    A **simulator** is a program that considers some particles and simulates their
    movement over time according to a set of laws that we impose. These particles
    can be abstract entities or correspond to physical objects—for example, billiard
    balls moving on a table, molecules in a gas, stars moving through space, smoke
    particles, fluids in a chamber, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Building a particle simulator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Computer simulations are useful in fields such as physics, chemistry, astronomy,
    and many other disciplines. The applications used to simulate systems are particularly
    performance-intensive, and scientists and engineers spend an inordinate amount
    of time optimizing their code. In order to study realistic systems, it is often
    necessary to simulate a very high number of bodies, and every small increase in
    performance counts.
  prefs: []
  type: TYPE_NORMAL
- en: In our first example, we will simulate a system containing particles that constantly
    rotate around a central point at various speeds, just like the hands of a clock.
  prefs: []
  type: TYPE_NORMAL
- en: 'The necessary information to run our simulation will be the starting positions
    of the particles, the speed, and the rotation direction. From these elements,
    we have to calculate the position of the particle in the next instant of time.
    An example system is shown in the following diagram. The origin of the system
    is the `(0, 0)` point, the position is indicated by the *x*, *y* vector, and the
    velocity is indicated by the *vx*, *vy* vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – An example of a particle system ](img/Figure_1.1_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 – An example of a particle system
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic feature of a circular motion is that the particles always move perpendicular
    to the direction connecting the particle and the center. To move the particle,
    we simply change the position by taking a series of very small steps (which correspond
    to advancing the system for a small interval of time) in the direction of motion,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – Movement of a particle ](img/Figure_1.2_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – Movement of a particle
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by designing the application in an `Particle` class that stores
    the particle positions, `x` and `y`, and their angular velocity, `ang_vel`, as
    illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that we accept positive and negative numbers for all the parameters (the
    sign of `ang_vel` will simply determine the direction of rotation).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another class, called `ParticleSimulator`, will encapsulate the laws of motion
    and will be responsible for changing the positions of the particles over time.
    The `__init__` method will store a list of `Particle` instances, and the `evolve`
    method will change the particle positions according to our laws. The code is illustrated
    in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We want the particles to rotate around the position corresponding to the `x=0`
    and `y=0` coordinates, at a constant speed. The direction of the particles will
    always be perpendicular to the direction from the center (refer to *Figure 1.1*
    in this chapter). To find the direction of the movement along the *x* and *y*
    axes (corresponding to the Python `v_x` and `v_y` variables), it is sufficient
    to use these formulae:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If we let one of our particles move, after a certain time *t*, it will reach
    another position following a circular path. We can approximate a circular trajectory
    by dividing the time interval, *t*, into tiny time steps, *dt*, where the particle
    moves in a straight line tangentially to the circle. (Note that higher-order curves
    could be used rather than straight lines for better accuracy, but we will stick
    with lines as the simplest approximation.) The final result is just an approximation
    of a circular motion.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to avoid a strong divergence, such as the one illustrated in the following
    diagram, it is necessary to take very small time steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – Undesired divergence in particle motion due to large time steps
    ](img/Figure_1.3_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 – Undesired divergence in particle motion due to large time steps
  prefs: []
  type: TYPE_NORMAL
- en: 'In a more schematic way, we have to carry out the following steps to calculate
    the particle position at time *t*:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the direction of motion (`v_x` and `v_y`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the displacement (`d_x` and `d_y`), which is the product of the time
    step, angular velocity, and direction of motion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *Steps 1* and *2* enough times to cover the total time *t*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code snippet shows the full `ParticleSimulator` implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: And with that, we have finished building the foundation of our particle simulator.
    Next, we will see it in action by visualizing the simulated particles.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the simulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use the `matplotlib` library here to visualize our particles. This library
    is not included in the Python standard library, but it can be easily installed
    using the `pip install matplotlib` command.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can use the Anaconda Python distribution ([https://store.continuum.io/cshop/anaconda/](https://store.continuum.io/cshop/anaconda/)),
    which includes `matplotlib` and most of the other third-party packages used in
    this book. Anaconda is free and is available for Linux, Windows, and Mac.
  prefs: []
  type: TYPE_NORMAL
- en: To make an interactive visualization, we will use the `matplotlib.pyplot.plot`
    function to display the particles as points and the `matplotlib.animation.FuncAnimation`
    class to animate the evolution of the particles over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `visualize` function takes a `ParticleSimulator` particle instance as an
    argument and displays the trajectory in an animated plot. The steps necessary
    to display the particle trajectory using the `matplotlib` tools are outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up the axes and use the `plot` function to display the particles. The `plot`
    function takes a list of *x* and *y* coordinates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write an initialization function, `init`, and a function, `animate`, that updates
    the *x* and *y* coordinates using the `line.set_data` method. Note that in `init`,
    we need to return the line data in the form of `line`, due to syntactic reasons.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a `FuncAnimation` instance by passing the `init` and `animate` functions
    and the `interval` parameters, which specify the update interval, and `blit`,
    which improves the update rate of the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the animation with `plt.show()`, as illustrated in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To test this code, we define a small function, `test_visualize`, that animates
    a system of three particles rotating in different directions. Note in the following
    code snippet that the third particle completes a round three times faster than
    the others:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `test_visualize` function is helpful to graphically understand the system
    time evolution. Simply close the animation window when you'd like to terminate
    the program. With this program in hand, in the following section, we will write
    more test functions to properly verify program correctness and measure performance.
  prefs: []
  type: TYPE_NORMAL
- en: Writing tests and benchmarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a working simulator, we can start measuring our performance
    and tune up our code so that the simulator can handle as many particles as possible.
    As a first step, we will write a test and a benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: We need a test that checks whether the results produced by the simulation are
    correct or not. Optimizing a program commonly requires employing multiple strategies;
    as we rewrite our code multiple times, bugs may easily be introduced. A solid
    test suite ensures that the implementation is correct at every iteration so that
    we are free to go wild and try different things with the confidence that, if the
    test suite passes, the code will still work as expected. More specifically, what
    we are implementing here are called unit tests, which aim to verify the intended
    logic of the program regardless of the implementation details, which may change
    during optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our test will take three particles, simulate them for `0.1` time units, and
    compare the results with those from a reference implementation. A good way to
    organize your tests is using a separate function for each different aspect (or
    unit) of your application. Since our current functionality is included in the
    `evolve` method, our function will be named `test_evolve`. The following code
    snippet shows the `test_evolve` implementation. Note that, in this case, we compare
    floating-point numbers up to a certain precision through the `fequal` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `assert` statements will raise an error if the included conditions are not
    satisfied. Upon running the `test_evolve` function, if you notice no error or
    output printed out, that means all the conditions are met.
  prefs: []
  type: TYPE_NORMAL
- en: A test ensures the correctness of our functionality but gives little information
    about its running time. A **benchmark** is a simple and representative use case
    that can be run to assess the running time of an application. Benchmarks are very
    useful to keep score of how fast our program is with each new version that we
    implement.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write a representative benchmark by instantiating a thousand `Particle`
    objects with random coordinates and angular velocity and feeding them to a `ParticleSimulator`
    class. We then let the system evolve for `0.1` time units. The code is illustrated
    in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: With the benchmark program implemented, we now need to run it and keep track
    of the time needed for the benchmark to complete execution, which we will see
    next. (Note that when you run these tests and benchmarks on your own system, you
    are likely to see different numbers listed in the text, which is completely normal
    and dependent on your system configurations and Python version.)
  prefs: []
  type: TYPE_NORMAL
- en: Timing your benchmark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A very simple way to time a benchmark is through the Unix `time` command. Using
    the `time` command, as follows, you can easily measure the execution time of an
    arbitrary process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `time` command is not available for Windows. To install Unix tools such
    as `time` on Windows, you can use the `cygwin` shell, downloadable from the official
    website ([http://www.cygwin.com/](http://www.cygwin.com/)). Alternatively, you
    can use similar PowerShell commands, such as `Measure-Command` ([https://msdn.microsoft.com/en-us/powershell/reference/5.1/microsoft.powershell.utility/measure-command](https://msdn.microsoft.com/en-us/powershell/reference/5.1/microsoft.powershell.utility/measure-command)),
    to measure execution time.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, `time` displays three metrics, as outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`real`: The actual time spent running the process from start to finish, as
    if it were measured by a human with a stopwatch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`user`: The cumulative time spent by all the **central processing units** (**CPUs**)
    during the computation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sys`: The cumulative time spent by all the CPUs during system-related tasks,
    such as memory allocation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Sometimes, `user` and `sys` might be greater than `real`, as multiple processors
    may work in parallel.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`time` also offers richer formatting options. For an overview, you can explore
    its manual (using the `man time` command). If you want a summary of all the metrics
    available, you can use the `-v` option.'
  prefs: []
  type: TYPE_NORMAL
- en: The Unix `time` command is one of the simplest and most direct ways to benchmark
    a program. For an accurate measurement, the benchmark should be designed to have
    a long enough execution time (in the order of seconds) so that the setup and teardown
    of the process are small compared to the execution time of the application. The
    `user` metric is suitable as a monitor for the CPU performance, while the `real`
    metric also includes the time spent on other processes while waiting for **input/output**
    (**I/O**) operations.
  prefs: []
  type: TYPE_NORMAL
- en: Another convenient way to time Python scripts is the `timeit` module. This module
    runs a snippet of code in a loop for *n* times and measures the total execution
    time. Then, it repeats the same operation *r* times (by default, the value of
    *r* is `3`) and records the time of the best run. Due to this timing scheme, `timeit`
    is an appropriate tool to accurately time small statements in isolation.
  prefs: []
  type: TYPE_NORMAL
- en: The `timeit` module can be used as a Python package, from the command line or
    from *IPython*.
  prefs: []
  type: TYPE_NORMAL
- en: IPython is a Python shell design that improves the interactivity of the Python
    interpreter. It boosts tab completion and many utilities to time, profile, and
    debug your code. We will use this shell to try out snippets throughout the book.
    The IPython shell accepts `%` symbol—that enhance the shell with special behaviors.
    Commands that start with `%%` are called **cell magics**, which can be applied
    on multiline snippets (termed as **cells**).
  prefs: []
  type: TYPE_NORMAL
- en: IPython is available on most Linux distributions through `pip` and is included
    in Anaconda. You can use IPython as a regular Python shell (`ipython`), but it
    is also available in a Qt-based version (`ipython qtconsole`) and as a powerful
    browser-based interface (`jupyter notebook`).
  prefs: []
  type: TYPE_NORMAL
- en: 'In IPython and `-n` and `-r` options. If not specified, they will be automatically
    inferred by `timeit`. When invoking `timeit` from the command line, you can also
    pass some setup code, through the `-s` option, which will execute before the benchmark.
    In the following snippet, the `IPython` command line and Python module version
    of `timeit` are demonstrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note that while the command line and IPython interfaces automatically infer
    a reasonable number of loops `n`, the Python interface requires you to explicitly
    specify a value through the `number` argument.
  prefs: []
  type: TYPE_NORMAL
- en: Writing better tests and benchmarks with pytest-benchmark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Unix `time` command is a versatile tool that can be used to assess the running
    time of small programs on a variety of platforms. For larger Python applications
    and libraries, a more comprehensive solution that deals with both testing and
    benchmarking is `pytest`, in combination with its `pytest-benchmark` plugin.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will write a simple benchmark for our application using
    the `pytest` testing framework. For those who are, the `pytest` documentation,
    which can be found at [http://doc.pytest.org/en/latest/](http://doc.pytest.org/en/latest/),
    is the best resource to learn more about the framework and its uses.
  prefs: []
  type: TYPE_NORMAL
- en: You can install `pytest` from the console using the `pip install pytest` command.
    The benchmarking plugin can be installed, similarly, by issuing the `pip install
    pytest-benchmark` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'A testing framework is a set of tools that simplifies writing, executing, and
    debugging tests, and provides rich reports and summaries of the test results.
    When using the `pytest` framework, it is recommended to place tests separately
    from the application code. In the following example, we create a `test_simul.py`
    file that contains the `test_evolve` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pytest` executable can be used from the command line to discover and run
    tests contained in Python modules. To execute a specific test, we can use the
    `pytest path/to/module.py::function_name` syntax. To execute `test_evolve`, we
    can type the following command in a console to obtain simple but informative output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have a test in place, it is possible for you to execute your test as
    a benchmark using the `pytest-benchmark` plugin. If we change our `test` function
    so that it accepts an argument named `benchmark`, the `pytest` framework will
    automatically pass the `benchmark` resource as an argument (in `pytest` terminology,
    these resources are called `benchmark` resource can be called by passing the function
    that we intend to benchmark as the first argument, followed by the additional
    arguments. In the following snippet, we illustrate the edits necessary to benchmark
    the `ParticleSimulator.evolve` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the benchmark, it is sufficient to rerun the `pytest test_simul.py::test_evolve`
    command. The resulting output will contain detailed timing information regarding
    the `test_evolve` function, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – Output from pytest ](img/Figure_1.4_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 – Output from pytest
  prefs: []
  type: TYPE_NORMAL
- en: For each test collected, `pytest-benchmark` will execute the `benchmark` function
    several times and provide a statistic summary of its running time. The preceding
    output shown is very interesting as it shows how running times vary between runs.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the benchmark in `test_evolve` was run `34` times (`Rounds`
    column), its timings ranged between `29` and `41` `Min` and `Max`), and the `Average`
    and `Median` times were fairly similar at about `30` ms, which is actually very
    close to the best timing obtained. This example demonstrates how there can be
    substantial performance variability between runs and that, as opposed to taking
    timings with one-shot tools such as `time`, it is a good idea to run the program
    multiple times and record a representative value, such as the minimum or the median.
  prefs: []
  type: TYPE_NORMAL
- en: '`pytest-benchmark` has many more features and options that can be used to take
    accurate timings and analyze the results. For more information, consult the documentation
    at [http://pytest-benchmark.readthedocs.io/en/stable/usage.html](http://pytest-benchmark.readthedocs.io/en/stable/usage.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Finding bottlenecks with cProfile
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After assessing the correctness and timing of the execution time of the program,
    we are ready to identify the parts of the code that need to be tuned for performance.
    We typically aim to identify parts that are small compared to the size of the
    program.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two profiling modules are available through the Python standard library, as
    outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The profile module**: This module is written in pure Python and adds significant
    overhead to the program execution. Its presence in the standard library is due
    to its vast platform support and the ease with which it can be extended.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`profile`. It is written in C, has a small overhead, and is suitable as a general-purpose
    profiler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `cProfile` module can be used in three different ways, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: From the command line
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a Python module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With IPython
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cProfile` does not require any change in the source code and can be executed
    directly on an existing Python script or function. You can use `cProfile` from
    the command line in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print a long output containing several profiling metrics of all of
    the functions called in the application. You can use the `-s` option to sort the
    output by a specific metric. In the following snippet, the output is sorted by
    the `tottime` metric, which will be described here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The data produced by `cProfile` can be saved in an output file by passing the
    `-o` option. The format that `cProfile` uses is readable by the `stats` module
    and other tools. The usage of the`-o` option is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The usage of `cProfile` as a Python module requires invoking the `cProfile.run`
    function in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also wrap a section of code between method calls of a `cProfile.Profile`
    object, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`cProfile` can also be used interactively with IPython. The `%prun` magic command
    lets you profile an individual function call, as illustrated in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – Using cProfile within IPython ](img/Figure_1.5_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – Using cProfile within IPython
  prefs: []
  type: TYPE_NORMAL
- en: 'The `cProfile` output is divided into five columns, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ncalls`: The number of times the function was called.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tottime`: The total time spent in the function without taking into account
    the calls to other functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cumtime`: The time spent in the function including other function calls.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`percall`: The time spent for a single call of the function—this can be obtained
    by dividing the total or cumulative time by the number of calls.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filename:lineno`: The filename and corresponding line numbers. This information
    is not available when calling C extension modules.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most important metric is `tottime`, the actual time spent in the function
    body excluding subcalls, which tells us exactly where the bottleneck is.
  prefs: []
  type: TYPE_NORMAL
- en: Unsurprisingly, the largest portion of time is spent in the `evolve` function.
    We can imagine that the loop is the section of the code that needs performance
    tuning. `cProfile` only provides information at the function level and does not
    tell us which specific statements are responsible for the bottleneck. Fortunately,
    as we will see in the next section, the `line_profiler` tool is capable of providing
    line-by-line information of the time spent in the function.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the `cProfile` text output can be daunting for big programs with a
    lot of calls and subcalls. Some visual tools aid the task by improving navigation
    with an interactive, graphical interface.
  prefs: []
  type: TYPE_NORMAL
- en: Graphically analyzing profiling results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: KCachegrind is a `cProfile`.
  prefs: []
  type: TYPE_NORMAL
- en: KCachegrind is available in the Ubuntu 16.04 official repositories. The Qt port,
    QCacheGrind, can be downloaded for Windows from [http://sourceforge.net/projects/qcachegrindwin/](http://sourceforge.net/projects/qcachegrindwin/).
    Mac users can compile QCacheGrind using MacPorts ([http://www.macports.org/](http://www.macports.org/))
    by following the instructions present in the blog post at [http://blogs.perl.org/users/rurban/2013/04/install-kachegrind-on-macosx-with-ports.html](http://blogs.perl.org/users/rurban/2013/04/install-kachegrind-on-macosx-with-ports.html).
  prefs: []
  type: TYPE_NORMAL
- en: KCachegrind can't directly read the output files produced by `cProfile`. Luckily,
    the `pyprof2calltree` third-party Python module is able to convert the `cProfile`
    output file into a format readable by KCachegrind.
  prefs: []
  type: TYPE_NORMAL
- en: You can install `pyprof2calltree` from the `pip install pyprof2calltree` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'To best show the KCachegrind features, we will use another example with a more
    diversified structure. We define a recursive function, `factorial`, and two other
    functions that use `factorial`, named `taylor_exp` and `taylor_sin`. They represent
    the polynomial coefficients of the Taylor approximations of `exp(x)` and `sin(x)`
    and are illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To access profile information, we first need to generate a `cProfile` output
    file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can convert the output file with `pyprof2calltree` and launch KCachegrind
    by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – Profiling output generated by pyprof2calltree and displayed
    by KCachegrind ](img/Figure_1.6_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 – Profiling output generated by pyprof2calltree and displayed by
    KCachegrind
  prefs: []
  type: TYPE_NORMAL
- en: 'The screenshot shows the KCachegrind UI. On the left, we have an output fairly
    similar to `cProfile`. The actual column names are slightly different: `cProfile`
    module''s `cumtime` value and `tottime`. The values are given in percentages by
    clicking on the **Relative** button on the menu bar. By clicking on the column
    headers, you can sort them by the corresponding property.'
  prefs: []
  type: TYPE_NORMAL
- en: On the top right, a click on the `factorial` function. The one on the left corresponds
    to the calls made by `taylor_exp` and the one on the right to the calls made by
    `taylor_sin`.
  prefs: []
  type: TYPE_NORMAL
- en: On the bottom right, you can display another diagram, a `taylor_exp` calls `factorial`
    `taylor_sin` calls `factorial` `factorial` calls itself **187250** times.
  prefs: []
  type: TYPE_NORMAL
- en: You can navigate to the `taylor_exp` will cause the graph to change, showing
    only the contribution of `taylor_exp` to the total cost.
  prefs: []
  type: TYPE_NORMAL
- en: '`.dot` diagram representing a call graph.'
  prefs: []
  type: TYPE_NORMAL
- en: Profiling line by line with line_profiler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we know which function we have to optimize, we can use the `line_profiler`
    module that provides information on how time is spent in a line-by-line fashion.
    This is very useful in situations where it's difficult to determine which statements
    are costly. The `line_profiler` module is a third-party module that is available
    on PyPI and can be installed by following the instructions at [https://github.com/rkern/line_profiler](https://github.com/rkern/line_profiler).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use `line_profiler`, we need to apply a `@profile` decorator to
    the functions we intend to monitor. Note that you don''t have to import the `profile`
    function from another module as it gets injected into the global namespace when
    running the `kernprof.py` profiling script. To produce profiling output for our
    program, we need to add the `@profile` decorator to the `evolve` function, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `kernprof.py` script will produce an output file and print the result of
    the profiling on the standard output. We should run the script with two options,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-l` to use the `line_profiler` function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-v` to immediately print the results on screen'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The usage of `kernprof.py` is illustrated in the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also possible to run the profiler in an IPython shell for interactive
    editing. You should first load the `line_profiler` extension that will provide
    the `lprun` magic command. Using that command, you can avoid adding the `@profile`
    decorator, as illustrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7 – Using line_profiler within IPython ](img/Figure_1.7_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 – Using line_profiler within IPython
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is quite intuitive and is divided into six columns, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Line #`: The number of the line that was run'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Hits`: The number of times that line was run'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Time`: The execution time of the line in microseconds (`Time`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Per Hit`: Time/hits'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`% Time`: Fraction of the total time spent executing that line'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Line Contents`: The content of the line'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By looking at the `for` loop body with a cost of around 10-20 percent each.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing our code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have identified where exactly our application is spending most of
    its time, we can make some changes and assess the resulting improvement in performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different ways to tune up our pure Python code. The way that typically
    produces the most significant results is to improve the *algorithms* used. In
    this case, instead of calculating the velocity and adding small steps, it will
    be more efficient (and correct, as it is not an approximation) to express the
    equations of motion in terms of radius, `r`, and angle, `alpha`, (instead of `x`
    and `y`), and then calculate the points on a circle using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Another optimization method lies in minimizing the number of instructions. For
    example, we can precalculate the `timestep * p.ang_vel` factor that doesn't change
    with time. We can exchange the loop order (first, we iterate on particles, then
    we iterate on time steps) and put the calculation of the factor outside the loop
    on the particles.
  prefs: []
  type: TYPE_NORMAL
- en: 'The line-by-line profiling also showed that even simple assignment operations
    can take a considerable amount of time. For example, the following statement takes
    more than 10 percent of the total time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can improve the performance of the loop by reducing the number of assignment
    operations performed. To do that, we can avoid intermediate variables by rewriting
    the expression into a single, slightly more complex statement (note that the right-hand
    side gets evaluated completely before being assigned to the variables), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This leads to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'After applying the changes, we should verify that the result is still the same
    by running our test. We can then compare the execution times using our benchmark,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we obtained only a modest increment in speed by making a pure
    Python micro-optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Using the dis module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will dig into the Python internals to estimate the performance
    of individual statements. In the CPython interpreter, Python code is first converted
    to an intermediate representation, the **bytecode**, and then executed by the
    Python interpreter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To inspect how the code is converted to bytecode, we can use the `dis` Python
    module (`dis` stands for `dis.dis` function on the `ParticleSimulator.evolve`
    method, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print, for each line in the function, a list of bytecode instructions.
    For example, the `v_x = (-p.y)/norm` statement is expanded in the following set
    of instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '`LOAD_FAST` loads a reference of the `p` variable onto the stack and `LOAD_ATTR`
    loads the `y` attribute of the item present on top of the stack. The other instructions,
    `UNARY_NEGATIVE` and `BINARY_TRUE_DIVIDE`, simply do arithmetic operations on
    top-of-stack items. Finally, the result is stored in `v_x` (`STORE_FAST`).'
  prefs: []
  type: TYPE_NORMAL
- en: By analyzing the `dis` output, we can see that the first version of the loop
    produces `51` bytecode instructions, while the second gets converted into `35`
    instructions.
  prefs: []
  type: TYPE_NORMAL
- en: The `dis` module helps discover how the statements get converted and serves
    mainly as an exploration and learning tool of the Python bytecode representation.
    For a more comprehensive introduction and discussion on the Python bytecode, refer
    to the *Further reading* section at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: To improve our performance even further, we can keep trying to figure out other
    approaches to reduce the number of instructions. It's clear, however, that this
    approach is ultimately limited by the speed of the Python interpreter, and it
    is probably not the right tool for the job. In the following chapters, we will
    see how to speed up interpreter-limited calculations by executing fast specialized
    versions written in a lower-level language (such as C or Fortran).
  prefs: []
  type: TYPE_NORMAL
- en: Profiling memory usage with memory_profiler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In some cases, high memory usage constitutes an issue. For example, if we want
    to handle a huge number of particles, we will incur a memory overhead due to the
    creation of many `Particle` instances.
  prefs: []
  type: TYPE_NORMAL
- en: The `memory_profiler` module summarizes, in a way similar to `line_profiler`,
    the memory usage of a process.
  prefs: []
  type: TYPE_NORMAL
- en: The `memory_profiler` package is also available on PyPI. You should also install
    the `psutil` module ([https://github.com/giampaolo/psutil](https://github.com/giampaolo/psutil))
    as an optional dependency that will make `memory_profiler` considerably faster.
  prefs: []
  type: TYPE_NORMAL
- en: Just as with `line_profiler`, `memory_profiler` also requires the instrumentation
    of the source code by placing a `@profile` decorator on the function we intend
    to monitor. In our case, we want to analyze the `benchmark` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can slightly change `benchmark` to instantiate a considerable amount (`100000`)
    of `Particle` instances and decrease the simulation time, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use `memory_profiler` from an IPython shell through the `%mprun` magic
    command, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8 – Output from memory_profiler ](img/Figure_1.8_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 – Output from memory_profiler
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to run `memory_profiler` from the shell using the `mprof run`
    command after adding the `@profile` decorator.
  prefs: []
  type: TYPE_NORMAL
- en: From the `Increment` column, we can see that 100,000 `Particle` objects take
    `23.7 MiB` of memory.
  prefs: []
  type: TYPE_NORMAL
- en: 1 **mebibyte** (**MiB**) is equivalent to 1,048,576 bytes. It is different from
    1 **megabyte** (**MB**), which is equivalent to 1,000,000 bytes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `__slots__` on the `Particle` class to reduce its memory footprint.
    This feature saves some memory by avoiding storing the variables of the instance
    in an internal dictionary. This strategy, however, has a small limitation—it prevents
    the addition of attributes other than the ones specified in `__slots__`. You can
    see this feature in use in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now rerun our benchmark to assess the change in memory consumption.
    The result is displayed in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.9 – Improvement in memory consumption ](img/Figure_1.9_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.9 – Improvement in memory consumption
  prefs: []
  type: TYPE_NORMAL
- en: By rewriting the `Particle` class using `__slots__`, we can save about 10 MiB
    of memory.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the basic principles of optimization and applied
    those principles to a test application. When optimizing an application, the first
    thing to do is test and identify the bottlenecks in the application. We saw how
    to write and time a benchmark using the `time` Unix command, the Python `timeit`
    module, and the full-fledged `pytest-benchmark` package. We learned how to profile
    our application using `cProfile`, `line_profiler`, and `memory_profiler`, and
    how to analyze and navigate the profiling data graphically with KCachegrind.
  prefs: []
  type: TYPE_NORMAL
- en: Speed is undoubtedly an important component of any modern software. The techniques
    we have learned in this chapter will allow you to systematically tackle the problem
    of making your Python programs more efficient from different angles. Further,
    we have seen that these tasks can take advantage of Python built-in/native packages
    and do not require any special external tools.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore how to improve performance using algorithms
    and data structures available in the Python standard library. We will cover scaling
    and sample usage of several data structures, and learn techniques such as caching
    and memorization. We will also introduce Big O notation, which is a common computer
    science tool to analyze the running time of algorithms and data structures.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Arrange the following three items in order of importance when building a software
    application: correctness (the program does what it is supposed to do), efficiency
    (the program is optimized in speed and memory management), and functionality (the
    program runs).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How could `assert` statements be used in Python to check for the correctness
    of a program?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a benchmark in the context of optimizing a software program?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How could `timeit` magic commands be used in Python to estimate the speed of
    a piece of code?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: List three different types of information that are recorded and returned by
    `cProfile` (included as output columns) in the context of profiling a program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On a high level, what is the role of the `dis` module in optimization?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `exercise.py` file, we write a simple function, `close()`, that checks
    whether a pair of particles are close to each other (with 1e-5 tolerance). In
    `benchmark()`, we randomly initialize two particles and call `close()` after running
    the simulation. Make a guess of what takes most of the execution time in `close()`,
    and profile the function via `benchmark()` using `cProfile`; does the result confirm
    your guess?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Profilers in Python: [https://docs.python.org/3/library/profile.html](https://docs.python.org/3/library/profile.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using PyCharm''s profiler: [https://www.jetbrains.com/help/pycharm/profiler.html](https://www.jetbrains.com/help/pycharm/profiler.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A beginner-friendly introduction to bytecode in Python: [https://opensource.com/article/18/4/introduction-python-bytecode](https://opensource.com/article/18/4/introduction-python-bytecode)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
