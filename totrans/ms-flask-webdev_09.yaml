- en: Creating Asynchronous Tasks with Celery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While creating web apps, it is vital to keep the time taken to process a request
    below or around 50 ms. On web applications or web services that have a medium
    to high rate of requests per second, response time becomes even more paramount.
    Think of requests such as a flow of liquid that needs to be handled at least as
    quickly as its flow rate, or else it will overflow. Any extra processing on the
    server that can be avoided, should be avoided. However, it is quite common to
    have requirements to operations in a web app that take longer than a couple of
    seconds, especially when complex database operations or image processing are involved.
  prefs: []
  type: TYPE_NORMAL
- en: In building an application that is able to scale horizontally, it should be
    possible to decouple all the heavy processing procedures from the web server's
    layer, and couple them to a worker's layer that can independently scale itself.
  prefs: []
  type: TYPE_NORMAL
- en: To protect our user experience and site reliability, a task queue named Celery
    will be used to move these operations out of the Flask process.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Using Docker to run RabbitMQ and Redis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Celery and Flask integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning to identify processes that should run outside the web server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and calling several types of tasks from simple asynchronous to complex
    workflows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Celery as a scheduler with beats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Celery?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Celery** is an asynchronous task queue written in Python. Celery runs multiple
    tasks, which are user-defined functions, concurrently, through the Python multiprocessing
    library. Celery receives messages that tell it to start a task from a **broker**,
    which is usually called a message queue, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bcdd964a-f516-4b59-b8cd-3e4392304658.png)'
  prefs: []
  type: TYPE_IMG
- en: A **message queue** is a system specifically designed to send data between producer
    processes and consumer processes. **Producer processes** are any programs that
    create messages to be sent to the queue, and **consumer processes** are any programs
    that take the messages out of the queue. Messages sent from a producer are stored
    in a **First In, First Out** (**FIFO**) queue, where the oldest items are retrieved
    first. Messages are stored until a consumer receives the message, after which
    the message is deleted. Message queues provide real-time messaging without relying
    on polling, which means continuously checking the status of a process. As messages
    are sent from producers, consumers are listening on their connection to the message
    queue for new messages; the consumer is not constantly contacting the queue. This
    difference is like the difference between **AJAX** and **WebSockets**, in that AJAX
    requires constant contact with the server, while WebSockets are just a continuous
    bidirectional communication stream.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to replace the message queue with a traditional database. Celery
    even comes with built-in support for SQLAlchemy to allow this. However, using
    a database as a broker for Celery is highly discouraged. Using a database in place
    of a message queue requires the consumer to constantly poll the database for updates.
    Also, because Celery uses multiprocessing for concurrency, the number of connections
    making lots of reads goes up quickly. Under medium loads, using a database requires
    the producer to make lots of writes to the database at the same time as the consumer
    is reading.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to use a message queue as a broker and a database to store
    the results of the tasks. In the preceding diagram, the message queue was used
    for sending task requests and task results. However, using a database to store
    the end result of the task allows the final product to be stored indefinitely,
    whereas the message queue will throw out the data as soon as the producer receives
    the data, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03a68834-f6ae-4b8f-aa49-36634765da94.png)'
  prefs: []
  type: TYPE_IMG
- en: This database is often a key/value NoSQL store to help handle the load. This
    is useful if you plan on doing analytics on previously run tasks, but otherwise
    it's safer to just stick with the message queue.
  prefs: []
  type: TYPE_NORMAL
- en: There is even an option to drop the results of tasks entirely, and not have
    the results returned at all. This has the downside that the producer has no way
    of knowing if a task was successful or not, but often, this is permissible in
    smaller projects.
  prefs: []
  type: TYPE_NORMAL
- en: For our stack, we will use RabbitMQ as the message broker. RabbitMQ runs on
    all major operating systems and is very simple to be set up and run. Celery also
    supports RabbitMQ without any extra libraries, and is the recommended message
    queue in the Celery documentation.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, there is no way to use RabbitMQ with Celery in Python
    3\. You can use Redis, however, instead of RabbitMQ. The only difference will
    be the connection strings. For more information, see [http://docs.celeryproject.org/en/latest/getting-started/brokers/redis.html](http://docs.celeryproject.org/en/latest/getting-started/brokers/redis.html).
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Celery and RabbitMQ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To install Celery on our `virtualenv`, we need to add it to our `requirements.txt` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As always, use the provided `init.sh` script, or use the procedure explained here to
    create and install all dependencies on a Python virtual environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also need a Flask extension to help handle the initialization of Celery:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The Flask documentation states that Flask extensions for Celery are unnecessary.
    However, getting the Celery server to work with Flask's application context, when
    your app is organized with an application factory, is significant. So, we will
    use `Flask-Celery-Helper` to do the heavy lifting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, RabbitMQ needs to be up and running. To do this easily, we will use a
    Docker container. Make sure you have Docker installed and properly set up; if
    not, then check out [Chapter 1](2d7573ed-1b2f-48df-9fac-9423d3f1cd51.xhtml), *Getting
    Started*, for instructions. First, we will need a very simple Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This is all it takes to build and run a RabbitMQ Docker image with the management
    interface. We are using a Docker Hub image that is available for download at [https://hub.docker.com/_/rabbitmq/](https://hub.docker.com/_/rabbitmq/).
    Visit the Hub page for further configuration details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s build our image issue the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `-t` flag is used to tag our image with a friendly name; in this case, `blog-rmq`.
    Then run the newly created image in the background using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `-d` flag is to run the container in the background (daemon). The `-p` flag
    is for port mapping between the container and our host/desktop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check if it''s properly running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Let's check out the RabbitMQ management interface. In your browser, navigate
    to `http://localhost:15672` and log in using the configured credentials set up
    on the Dockerfile. In this case, our username is `rabbitmq`, and our password
    is also `rabbitmq`.
  prefs: []
  type: TYPE_NORMAL
- en: If you need more information, RabbitMQ maintains a detailed list of installation
    and configuration instructions for each operating system at [https://www.rabbitmq.com/download.html](https://www.rabbitmq.com/download.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'After RabbitMQ is installed, go to a Terminal window and run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Creating tasks in Celery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As stated before, Celery tasks are just user-defined functions that perform
    some operations. But before any tasks can be written, our Celery object needs
    to be created. This is the object that the Celery server will import to handle
    running and scheduling all of the tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a bare minimum, Celery needs one configuration variable to run, and that
    is the connection to the message broker. The connection is defined the same as
    the SQLAlchemy connection; that is, as a URL. The backend, which stores our tasks''
    results, is also defined as a URL, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `__init__.py` file, the `Celery` class from `Flask-Celery-Helper` will
    be initialized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'So, in order for our Celery process to work with the database and any other
    Flask extensions, it needs to work within our application context. In order to
    do so, Celery will need to create a new instance of our application for each process.
    Like most Celery apps, we need a Celery factory to create an application instance
    and register our Celery instance on it. In a new file, named `celery_runner.py`,
    in the top-level directory—the same location where `manage.py` resides—we have
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `make_celery` function wraps every call to each Celery task in a Python
    `with` block. This makes sure that every call to any Flask extension will work
    as it is working with our app. Also, make sure not to name the Flask app instance
    `app`, as Celery tries to import any object named `app` or `celery` as the Celery
    application instance. So naming your Flask object `app` will cause Celery to try
    to use it as a Celery object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can write our first task. It will be a simple task to start with; one
    that just returns any string passed to it. We have a new file in the blog module
    directory, named `tasks.py`. In this file, find the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the final piece of the puzzle is to run the Celery process, which is called
    a **worker**, in a new Terminal window. Again, this is the process that will be
    listening to our message broker for commands to start new tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `loglevel` flag is there, so you will see the confirmation that a task was
    received, and its output was available, in the Terminal window.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can send commands to our Celery worker. Open a Flask shell session,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The function can be called as if it were any other function, and doing so will
    execute the function in the current process. However, calling the `delay` method
    on the task will send a message to the worker process to execute the function
    with the given arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Terminal window that is running the Celery worker, you should see something
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As with any asynchronous task, the `ready` method can be used to tell if the
    task has successfully been completed. If `True`, the `get` method can be used
    to retrieve the result of the tasks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `get` method causes the current process to wait until the `ready` function
    returns `True` to retrieve the result. So, calling `get` immediately after calling
    the task essentially makes the task synchronous. Because of this, it's rather
    rare for tasks to actually return a value to the producer. The vast majority of
    tasks perform some operation and then exit.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a task is run on the Celery worker, the state of the task can be accessed
    via the `state` attribute. This allows for a more fine-grained understanding of
    what the task is currently doing in the worker process. The available states are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FAILURE`: The task failed, and all of the retries failed as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PENDING`: The task has not yet been received by the worker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RECEIVED`: The task has been received by the worker, but is not yet processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RETRY`: The task failed and is waiting to be retried.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`REVOKED`: The task was stopped.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`STARTED`: The worker has started processing the task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SUCCESS`: The task was completed successfully.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In Celery, if a task fails, then the task can recall itself with the `retry`
    method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `bind` parameter in the decorator function tells Celery to pass a reference
    to the task object as the first parameter in the function. Using the `self` parameter,
    the `retry` method can be called, which will rerun the task with the same parameters.
    There are several other parameters that can be passed to the function decorator
    to change the behavior of the task:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_retries`: This is the maximum number of times the task can be retried
    before it is declared as failed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`default_retry_delay`: This is the time in seconds to wait before running the
    task again. It''s a good idea to keep this at around a minute or so if you expect
    that the conditions that led to the task failing are transitory; for example,
    network errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rate_limit`: This specifies the total number of unique calls to this task
    that are allowed to run in a given interval. If the value is an integer, then
    it represents the total number of calls that this task that is allowed to run
    per second. The value can also be a string in the form of *x/m*,for *x* number
    of tasks per minute, or *x/h*, for *x* number of tasks per hour. For example,
    passing in *5/m* will only allow this task to be called five times a minute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`time_limit`: If this is specified, then the task will be killed if it runs
    longer than the specified number of seconds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ignore_result`: If the task''s return value isn''t used, then don''t send
    it back.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's a good idea to specify all of these for each task to avoid any chance that
    a task will not be run.
  prefs: []
  type: TYPE_NORMAL
- en: Running Celery tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `delay` method is a shorthand version of the `apply_async` method, which
    is called in this format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'However, the `args` keyword can be implicit, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling `apply_async` allows you to define some extra functionality in the
    task call that you cannot specify in the `delay` method. First, the `countdown`
    option specifies the amount of time in seconds that the worker, upon receiving
    the task, should wait before running it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `countdown` is not a guarantee that the task will be run after `600` seconds.
    The `countdown` option only says that the task is up for processing after *x*
    number of seconds. If all of the worker processes are busy with the other tasks,
    then it will not be run immediately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another keyword argument that `apply_async` gives is the `eta` argument. `eta`
    is passed through a Python `datetime` object that specifies exactly when the task
    should be run. Again, `eta` is not reliable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Celery workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Celery provides many ways to group multiple, dependent tasks together, or to
    execute many tasks in parallel. These methods take a large amount of influence
    from language features found in functional programming languages. However, to
    understand how this works, we first need to understand signatures. Consider the
    following task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see a **signature** in action to understand it. Open up a Flask shell
    and enter the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Calling the signature (sometimes referred to as a **subtask**) of a task creates
    a function that can be passed to the other functions to be executed. Executing
    the signature, like the third to last line in the preceding example, executes
    the function in the current process, and not in the worker.
  prefs: []
  type: TYPE_NORMAL
- en: Partials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first application of task signatures is functional programming style partials.
    **Partials** are functions, which originally take many arguments, but an operation
    is applied to the original function to return a new function, so the first *n*
    arguments are always the same. Consider the following example, where we have a
    `multiply` function that is not a task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a fictional API, but is very close to the Celery version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The output in the worker window should show `16`. Basically, we created a new
    function, saved to partial, that will always multiply its input by four.
  prefs: []
  type: TYPE_NORMAL
- en: Callbacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once a task is completed, it is very common to run another task, based on the
    output of the previous task. To achieve this, the `apply_async` function has a
    `link` method, used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The worker output should show that both the `multiply` task and the `log` task
    returned `16`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have a function that does not take input, or your callback does not
    need the result of the original method, then the task signature must be marked
    as immutable with the `si` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '**Callbacks** can be used to solve real-world problems. If we wanted to send
    a welcome email every time a task created a new user, then we could produce that
    effect with the following call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Partials and callbacks can be combined to produce some powerful effects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: It's important to note that, if this call were saved and the `get` method was
    called on it, the result would be `16`, rather than `64`. This is because the
    `get` method does not return the results for callback methods. This will be solved
    with later methods.
  prefs: []
  type: TYPE_NORMAL
- en: Group
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `group` function takes a list of signatures and creates a callable function
    to execute all of the signatures in parallel, then returns a list of all of the
    results as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Chain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `chain` function takes task signatures and passes the value of each result
    to the next value in the chain, returning one result, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Chains and partials can be taken a bit further. Chains can be used to create
    new functions when using partials, and chains can be nested as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Chord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `chord` function creates a signature that will execute a `group` of signatures
    and pass the final result to a callback:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Just like the link argument, the callback is not returned with the `get` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `chain` syntax with a group and a callback automatically creates
    a chord signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Running tasks periodically
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Celery also has the ability to call tasks periodically. For those familiar with
    *******nix** operating systems, this system is a lot like the command-line utility
    `cron`, but it has the added benefit of being defined in our source code rather
    than on some system file. As such, it will be much easier to update our code when
    it is ready for publishing to production—a stage that we will reach in [Chapter
    13](380101ac-fb85-4e2e-b664-8d6de77928f4.xhtml), *Deploying Flask Apps*. In addition,
    all of the tasks are run within the application context, whereas a Python script
    called by `cron` would not be.
  prefs: []
  type: TYPE_NORMAL
- en: 'To add periodic tasks, add the following to the `DevConfig` configuration object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This `configuration` variable defines that the `log` task should be run every
    30 seconds, with the `args` tuple passed as the parameters. Any `timedelta` object
    can be used to define the interval to run the task on.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the periodic tasks, another specialised worker, named a `beat` worker,
    is needed. In another Terminal window, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: If you now watch the Terminal output for the main `Celery` worker, you should
    now see a log event every 30 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: What if your task needs to run on much more specific intervals; say, for example,
    every Tuesday in June at 3 am and 5 pm? For very specific intervals, there is
    the Celery `crontab` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate how the `crontab` object represents intervals, consider the following
    examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The object has the following arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`minute`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hour`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`day_of_week`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`day_of_month`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`month_of_year`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these arguments can take various inputs. With plain integers, they operate
    much like the `timedelta` object, but can also take strings and lists. When passed
    a list, the task will execute on every moment that is in the list. When passed
    a string in the form of **/x*, the task will execute every moment that the modulo
    operation returns zero. Also, the two forms can be combined to form a comma-separated
    string of integers and divisions.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Celery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When our code is pushed to the server, our `Celery` worker will not be run
    in the Terminal window—rather, it will be run as a background task. Because of
    this, Celery provides many command-line arguments to monitor the status of your
    `Celery` worker and tasks. These commands take the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The main tasks to view the status of your workers are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`status`: This prints the running workers and if they are up.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`result`: When passed a task ID, this shows the return value and final status
    of the task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`purge`: Using this, all messages in the broker will be deleted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inspect active`: This lists all active tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inspect scheduled`: This lists all tasks that have been scheduled with the
    `eta` argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inspect registered`: This lists all of the tasks waiting to be processed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inspect stats`: This returns a dictionary full of statics on the currently
    running workers and the broker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web-based monitoring with Flower
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Flower** is a web-based, real-time management tool for Celery. In Flower,
    all active, queued, and completed tasks can be monitored. Flower also provides
    graphs and statics on how long each task has been sitting in the queue versus
    how long its execution took, and the arguments to each of those tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install `flower`, use the `pip` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'To run it, just treat `flower` as a Celery command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, open your browser to `http://localhost:5555`. It''s best to familiarize
    yourself with the interface while tasks are running, so go to the command line
    and type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Your worker process will now start processing 10,000 tasks. Browse around the
    different pages while the tasks are running to see how `flower` interacts with
    your worker while it''s really churning, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd0af794-6345-440f-a8b6-f02b99026662.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating a reminder app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's get into some real-world example applications of Celery. Suppose another
    page on our site now requires a reminders feature. Users can create reminders
    that will send an email to a specified location at a specified time. We will need
    a model, a task, and a way to call our task automatically every time a model is
    created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the following basic SQLAlchemy model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need a task that will send an email to the location in the model. In
    our `blog/tasks.py` file, look up the following task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Note that our task takes a primary key, rather than a model. This is a hedge
    against a race condition, as a passed model could be stale by the time the worker
    finally gets around to processing it. You will also have to replace the placeholder
    emails and login details with your own login info.
  prefs: []
  type: TYPE_NORMAL
- en: How do we have our task called when the user creates a reminder model? We will
    use an SQLAlchemy feature, named `events`. SQLAlchemy allows us to register callbacks
    on our models that will be called when specific changes are made to our models.
    Our task will use the `after_insert` event, which is called after new data is
    entered into the database, whether the model is brand new or being updated.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need a callback in `blog/tasks.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, in `blog/__init__.py`, we will register our callback on our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Now, every time a model is saved, a task is registered that will send an email
    to our user.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a weekly digest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Say our blog has a lot of people who don't use RSS, and prefer mailing lists.
    We need some way to create a list of new posts at the end of every week to increase
    our site's traffic. To solve this problem, we will create a digest task that will
    be called by a beat worker at 10 am, every Saturday.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, in `blog/tasks.py`, let''s create our task as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also need to add a periodic schedule to our configuration object in
    `config.py` to manage our task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to configure our SMTP server so that we are able to send emails.
    This can be done using Gmail or your corporate email credentials. Add your chosen
    account information to the configuration object in `config.py` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need our email template. Unfortunately, HTML in email clients is
    terribly outdated. Every single email client has different rendering bugs and
    quirks, and the only way to find them is to open your email in all the clients.
    Many email clients don''t even support CSS, and those that do support a very small
    amount of selectors and attributes. In order to compensate, we have to use the
    web development methods of 10 years ago; that is, designing tables with inline
    styles. Here is our `digest.html` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Now, at the end of every week, our digest task will be called, and will send
    an email to all the users present in our mailing list.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Celery is a very powerful task queue that allows programmers to defer the processing
    of slower tasks to another process. Now that you understand how to move complex
    tasks out of the Flask process, we will take a look at a collection of Flask extensions
    that simplify some common tasks seen in Flask apps.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to leverage some great community-built
    Flask extensions to improve performance, debug, and even quickly create an administration
    back office.
  prefs: []
  type: TYPE_NORMAL
