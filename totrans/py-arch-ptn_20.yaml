- en: '16'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ongoing Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Just as software itself is never truly complete, software architecture is never
    a finished piece of work. There are always changes, adjustments, and tweaks that
    need to be performed in order to improve the system: adding new features; improving
    performance; fixing security problems. While good architecture requires us to
    understand deeply how to design a system, the reality of the ongoing process is
    more about making changes and improvements.'
  prefs: []
  type: TYPE_NORMAL
- en: We will talk in this chapter about some of those aspects, as well as dealing
    with some of the techniques and ideas around making changes in a real working
    system, keeping in mind that the process can always be improved further by reflecting
    on how the process is performed and following some guidelines to ensure that the
    system can be changed continuously while at the same time maintaining service
    to customers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting the architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduled downtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incidents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Versioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backward compatibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature flags
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Teamwork aspects of changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start by taking a look at why to make changes in the architecture of a
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting the architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While for most of this book we've been talking about system design, which is
    the basic function of an architect, it is most likely that the bulk of their day-to-day
    job will be more focused on redesigns.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is always an endless task, as working software systems are always under
    revision and expansion. Some of the reasons why it may be necessary to adjust
    the architecture of a system are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: To provide certain features or characteristics previously not available – for
    example, adding an event-driven system to run asynchronous tasks, allowing us
    to avoid the request-response pattern that was previously all that was available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because there are bottlenecks or limitations with the current architecture.
    For example, only a single database is present in the system and there's a limit
    on the number of queries that can run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As systems grow, it may be necessary to divide parts to allow better control
    over them – for example, dividing a monolith into microservices, as we saw in
    *Chapter 8*, *Advanced Event-Driven Structures*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To increase the security of the system – for example, removing or encoding stored
    information that might be sensitive, like emails addresses and other **personally
    identifiable information** (**PII**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Big API changes, like introducing a new version of an API either internally
    or externally. For example, adding a new endpoint that works better for other
    internal systems to perform some action, where the calling services should be
    migrated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changes in the storage system, including all the different ideas that we discussed
    in *Chapter 3*, *Data Modeling* when talking about distributed databases. This
    could also include adding or replacing existing storage systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To adapt technologies that are obsolete. This can happen in legacy systems that
    have a critical component that is no longer supported, or a fundamental security
    problem. For example, replacing an old module with another that is capable of
    using new security processes because the old one is not maintained anymore and
    relies on old encryption methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rewrites using new languages or technology. This can be done to consolidate
    technologies if at some point a system was created using a different language,
    and, after a while, it is decided to bring it in line with the most used language
    to allow better maintenance. This scenario is typical in organizations that experienced
    growth, and at some point, a team decided to use their favorite language to create
    a service. After some time, this may cause problems by complicating maintenance
    as expertise in this language may be lacking. This can be even worse if the original
    developer has left the organization. It could be better to adjust or rewrite the
    service by integrating it into an existing one or replace it with an equivalent
    one in the preferred language.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other kinds of technical debt – for example, refactors that can clean the code
    and make it more readable, or to allow for changing names of components to be
    more precise, among other things.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just some examples, but the truth is that all systems require constant
    updating and adjusting, as software is rarely a finished task.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge is not only to design these changes to achieve the expected results,
    but also to move from the starting point to the destination with minimal interruption
    to the system. These days the expectation is that online systems are only very
    rarely interrupted, setting a high bar for any change.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this, changes need to be taken in small steps, taking extra care
    to ensure that the system is available at all points.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduled downtime
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While ideally there should be no interruption in the system as a result of the
    changes made, sometimes it's simply not possible to perform big changes without
    interrupting the system.
  prefs: []
  type: TYPE_NORMAL
- en: When and whether it's sensible to have downtime may depend greatly depending
    on the system. For example, in its first years of operation, the popular website
    Stack Overflow ([https://stackoverflow.com/](https://stackoverflow.com/)) had
    frequent downtime, initially even every day, where the webpage returned a "*down
    for maintenance*" page during the morning hours in Europe. That changed eventually,
    and now it's rare to see that kind of message.
  prefs: []
  type: TYPE_NORMAL
- en: But that was acceptable in the early stages of the project as the bulk of their
    users used the site in line with North American hours and it was (and still is)
    a free website.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling downtime is always an option, but it's a costly one, so it needs
    to be designed in a way that minimizes the impact on the operations. If the system
    is an established 24x7 service that's critical for customers, or produces income
    for the business while up (like a store, for example), any downtime will have
    a pretty hefty price tag.
  prefs: []
  type: TYPE_NORMAL
- en: In other cases, like a small new service with very little traffic, customers
    will either be more understanding or there'll even be a good chance that they
    will be unaffected.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduled downtime should be communicated beforehand to affected customers.
    This communication can take multiple forms, and will greatly depend on the kind
    of service. For example, a public web store may announce downtime with a banner
    on their page during the week informing that it won't be available on Sunday morning,
    but scheduling downtime for a banking operation may require months of advance
    notice and negotiation over when is the best time.
  prefs: []
  type: TYPE_NORMAL
- en: If possible, is a good practice to define maintenance windows to properly set
    clear expectations about times when the service will or might have a high risk
    of some sort of interruption.
  prefs: []
  type: TYPE_NORMAL
- en: Maintenance window
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Maintenance windows are periods where it is communicated beforehand that maintenance
    might happen. The idea is to guarantee the stability of the system outside of
    maintenance windows while allocating clear times where maintenance might happen.
  prefs: []
  type: TYPE_NORMAL
- en: A maintenance window could perhaps be at weekends or nights in the most active
    timezone for the system. During the busiest hours of activity the service remains
    uninterrupted, and maintenance is only carried over when it can't wait, like when
    preventing or fixing a critical incident.
  prefs: []
  type: TYPE_NORMAL
- en: Maintenance windows are different than scheduled downtime. While in some cases
    it will happen, not every maintenance window needs to involve downtime – there
    is simply the possibility that it might happen.
  prefs: []
  type: TYPE_NORMAL
- en: Not every maintenance window needs to be defined equally – some may be safer
    than others and capable of doing more extensive maintenance. For example, weekends
    may be reserved for scheduled downtime, but nights during the working week may
    see regular deployments.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s important to communicate maintenance windows in advance, for example
    designing a table like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Days | Time | Type of maintenance window | Risk | Comments |'
  prefs: []
  type: TYPE_TB
- en: '| Monday to Thursday | 08:00 – 12:00 UTC | Regular maintenance | Low risk |
    Regular deployments considered low risk. No impact to service. |'
  prefs: []
  type: TYPE_TB
- en: '| Saturday | 08:00 – 18:00 UTC | Serious maintenance | High risk | Adjustments
    considered risky. While the expectation is that the service will be fully available,
    there is a chance that it will be interrupted at some point during the window.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Saturday | 08:00 – 18:00 UTC | Notified Scheduled downtime | Service unavailable
    | One month''s notice given. Essential maintenance that requires the service to
    be unavailable. |'
  prefs: []
  type: TYPE_TB
- en: An important detail about maintenance windows is that they should be big enough
    to allow ample time for the maintenance to be done. Be sure to be generous with
    time, as it's better to set expectations with a large maintenance window that
    can be used safely for any eventuality, rather than a short one that often needs
    to be extended.
  prefs: []
  type: TYPE_NORMAL
- en: While scheduled downtime and maintenance windows will help frame the times where
    the service is active and what times are riskier for the user, it's still possible
    that some problem arises and causes a problem in the system.
  prefs: []
  type: TYPE_NORMAL
- en: Incidents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unfortunately, at some point in its life, the system won't behave as it should.
    It will produce an error so important that it needs to be taken care of immediately.
  prefs: []
  type: TYPE_NORMAL
- en: An incident is defined as a problem that disrupts the service so much that it
    requires an emergency response.
  prefs: []
  type: TYPE_NORMAL
- en: This doesn't necessarily mean that the full service is totally interrupted –
    it could be a noticeable degradation of the external service, or even a problem
    in one internal service that reduces the quality of service overall. For example,
    if an asynchronous task handler is failing 50% of the time, external customers
    may only see that their tasks take longer, but that is probably important enough
    to take corrective action.
  prefs: []
  type: TYPE_NORMAL
- en: During incidents, using all monitoring tools available is critical to find the
    problem as soon as possible and be able to correct it. Reaction times should be
    as fast as possible while keeping the risk of corrective actions as low as possible.
    A balance needs to be struck here, and depending on the nature of the incident,
    riskier actions can be taken, for example when the system is completely down,
    as recovering the system will be more important.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recovery during incidents will normally be limited by two factors:'
  prefs: []
  type: TYPE_NORMAL
- en: How good the monitoring tools are at detecting and understanding problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How fast a change can be introduced in the system, related to how quick it is
    to change a parameter or to deploy new code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first of the above points is the *understand* part and the second is the
    *solve* part (though it may be necessary to make changes to get a better understanding
    of the problem, as we saw in *Chapter 14*, *Profiling*).
  prefs: []
  type: TYPE_NORMAL
- en: We cover both of these aspects in the book, with the observability tools examined
    in *Chapter 11*, *Package Management*, and *Chapter 12*, *Logging*. We also may
    need to use the techniques described in *Chapter 14*, *Profiling*.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing changes to the system is tightly related to the **Continuous Integration**
    (**CI**) techniques that we discussed in *Chapter 4*, *The Data Layer*. A fast
    CI pipeline can make a big difference in how long it takes new code to be ready
    to deploy.
  prefs: []
  type: TYPE_NORMAL
- en: This is why these two elements, the observability and the time required to make
    a change, are so important. In normal situations, taking a long time to deploy
    or to make a change is normally just a minor annoyance, but in a critical situation,
    it could hinder the fixes that can help the health of the system to recover.
  prefs: []
  type: TYPE_NORMAL
- en: The reaction to an incident is a complicated process that requires flexibility
    and improvisation, which improve with experience. But there needs to be as well
    a continuous process of improving the uptime of the system and understanding the
    weakest part of the system, to avoid the problems or minimize them.
  prefs: []
  type: TYPE_NORMAL
- en: Postmortem analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Postmortem analysis*, also called a post-incident review, is an analysis done
    after a problem has impacted the service. Its objective is to understand what
    failed, why, and take corrective measures to ensure that the problem doesn''t
    happen again, or at least that it has a reduced impact.'
  prefs: []
  type: TYPE_NORMAL
- en: Typically, a postmortem starts with the people involved in the correction of
    the problem filling in a template form. Having a template predefined helps to
    shape the discussion and focus on the remediation to carry out.
  prefs: []
  type: TYPE_NORMAL
- en: There are plenty of postmortem templates available online that you can search
    through to see if there's a particular one that you like, or just to get ideas.
    As with any other part of the process, it should be improved and refined as it
    goes along. Remember to create and tweak your own template.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic template should start with all the main details of **what** happened,
    followed by **why** it happened, and finally, the most important part: what are
    the **next actions** to correct the problem?'
  prefs: []
  type: TYPE_NORMAL
- en: Remember that a postmortem analysis happens after the incident is over. While
    it could be good to take some notes while is happening, the focus during an incident
    is to fix it first. Focus on the most important thing first.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a simple template could be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Incident report
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**. A brief description of what happened.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Example:* The service went down between 08:30 and 9:45 UTC on the 5^(th) of
    November.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Impact**. Describe the impact of the problem. What was the external problem?
    How external users were affected?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Example:* All user requests were returning 500 errors.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Detection**. A description of how it was detected initially. Could it have
    been detected earlier?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Example:* The monitoring system alerted about the problem at 8:35 UTC, after
    5 minutes of 100% error requests.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Response**. Actions taken to correct the problem.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Example:* John cleaned the disk space in the database server and restarted
    the database.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Timeline**. A timeline of events to understand how the incident developed
    and how long each phase took.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Example:*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 8:30 Start of the problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 8:35 An alert in the monitoring system was triggered. John started looking into
    the problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 8:37 It is detected that the database is unresponsive and cannot be restarted.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 9:05 After investigation, John discovered that the database disk was full.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 9:30 The logs in the database server had filled up the server disk space, causing
    the database server to crash.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 9:40 Old logs are removed from the server, freeing disk space. The database
    is restarted.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 9:45 Service is restored.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Root cause**. A description of the identified root cause of the problem that,
    if fixed, will completely remove this problem.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Detecting the root cause is not necessarily easy, as sometimes a chain of events
    will be involved. To help find the root cause, you can use the *five whys* technique.
    Start describing the impact and ask why it happened. Then ask why this happened,
    and so on. Keep iterating until you have asked "why?" five times, and the resulting
    one will be the root cause. Don't take this to mean that you *must* ask "why?"
    exactly five times, but keep going until you can get a solid answer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Take into account that the investigation can go further than the steps taken
    to recover the service during the incident, where a quick fix may have been enough
    to get out of the woods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Example:*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The server returned errors. *Why?*
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Because* the database had crashed. *Why?*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Because* the database server ran out of space. *Why?*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Because* the space was fully filled with logs. *Why?*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Because* the log space on the disk was not limited and could grow indefinitely.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Lessons learned**. Things that could be improved in the process, as well
    as any other element that went well and could be useful to know, like the usage
    of a certain tool or metric that was useful when analyzing the problem.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Example*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The amount of disk space that logs use should be limited in all cases.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The disk space itself is not being monitored or alerted before it completely
    runs out.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The alerting system is too slow and requires a high level of errors before alerting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Next actions**.The most important part of the process. Describe what actions
    should be performed to eliminate or, if that''s not possible, mitigate the problem.
    Be sure that these actions have clear owners and are followed up.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If there's a ticketing system, these actions should be transformed into tickets
    and be prioritized accordingly to be sure that the proper team implements them.
  prefs: []
  type: TYPE_NORMAL
- en: Not only should the root cause be addressed, but also any possible improvements
    detected in the lessons learned part.
  prefs: []
  type: TYPE_NORMAL
- en: '*Example*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Action: Enable log rotation to limit the amount of space that logs can take
    up in all servers, starting with the database. Assigned to the operations team.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Action: Monitor and alert on the disk space to raise an alert if the disk space
    has less than 20% of the total available space, to allow faster reactions. Assigned
    to the operations team.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Action: Tweak the error alert to change it to alert when there''s only one
    minute of 30% or more requests returning errors. Assigned to the operations team.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the template doesn't have to be filled out in one go. Typically, the
    template will be filled in as much as possible, and a postmortem meeting will
    be held, when the incident can be analyzed and the template totally filled in,
    including the *Next action* part, which, again, is the most important part of
    the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that it's crucial that postmortem processes are focused on improving
    the system and not on assigning blame for the problem. The objective of the process
    is to detect weak spots and to try to make sure that problems are not repeated.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, an equivalent process to try to foresee problems has been put
    in place, especially before an important event.
  prefs: []
  type: TYPE_NORMAL
- en: Premortem analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *premortem analysis* is an exercise to try to analyze what could go wrong
    before an important event. The event could be some milestone, launch event, or
    something similar that is expected to significantly change the conditions of the
    system.
  prefs: []
  type: TYPE_NORMAL
- en: The word "*premortem*" is quite a funny neologism that comes from the usage
    of "*postmortem*" as a way to refer to an analysis done after the fact, making
    an analogy with an autopsy. Though hopefully, nothing is dead yet!.
  prefs: []
  type: TYPE_NORMAL
- en: It can also be called a *preparation analysis*.
  prefs: []
  type: TYPE_NORMAL
- en: For example, there could be a marketing campaign launch that is expected to
    double or triple the amount of traffic that had previously been normal.
  prefs: []
  type: TYPE_NORMAL
- en: 'The premortem analysis is the reverse of a postmortem. You set your mindset
    in the future and ask: *What went wrong?* *What is the worst-case scenario?* From
    there, you verify your assumptions about your system and prepare for them.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider an analysis for the above example of tripling the amount of traffic
    on the system. Can we simulate the conditions to verify that our system is ready
    for it? Which elements of the system do we think are less robust?
  prefs: []
  type: TYPE_NORMAL
- en: All that can lead to planning for the different scenarios and running tests
    to ensure that the system will be ready for the event.
  prefs: []
  type: TYPE_NORMAL
- en: When doing any premortem analysis, be sure to have enough time to perform the
    necessary actions and tests to prepare the system. As usual, actions will have
    to be prioritised to be sure that time is well spent. But keep in mind that this
    preparation can be an endless task, and as time will be limited, it needs to be
    focused on the most important or sensitive parts of the system. Be sure to use
    as many data-driven actions as possible and focus the analysis on real data and
    not hunches.
  prefs: []
  type: TYPE_NORMAL
- en: Load testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A key element of preparation in these cases is *load testing*.
  prefs: []
  type: TYPE_NORMAL
- en: Load testing is creating a simulated load that goes to an increased level of
    traffic. It can be done in an explorative way, i.e., let's find out what the limits
    of our system are; or in a confirmative way, i.e., let's double-check that we
    can reach this level of traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Load testing is typically done not in production environments, but in staging
    ones, replicating the configuration and hardware in production, though it is normal
    to create a final load test verifying that the configuration in the production
    environment is the correct one.
  prefs: []
  type: TYPE_NORMAL
- en: An interesting part of load testing analysis in cloud environments is to ensure
    that any autoscaling in the system works correctly, so it provisions more hardware
    automatically when receiving greater load, and deletes it when it's not necessary.
    Caution is required here, as a full load test to the maximum capacity of the cluster
    can be expensive each time it's run.
  prefs: []
  type: TYPE_NORMAL
- en: The basic element of a load test is to simulate a typical user performing actions
    on the system. For example, a typical user can log in, check a few pages, add
    some information, and then log out. We can replicate this behavior using automated
    tools that work on our external interface.
  prefs: []
  type: TYPE_NORMAL
- en: A good way of using these tools is reusing any kind of automated testing that
    can be created, and using it as well as the basis for the simulation. This makes
    the integration or system test framework the unit to enable load testing.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we can multiply that unit simulating the behavior for a single user multiple
    times to simulate the effect of `N` users, producing enough load to test our system.
  prefs: []
  type: TYPE_NORMAL
- en: For simplicity, it's better to use a single simulation that works as a combination
    of typical behaviors of users instead of trying to generate multiple smaller simulations
    trying to replicate different users.
  prefs: []
  type: TYPE_NORMAL
- en: As we said before, the usage of some system test that exercises the main parts
    of the system works very well in these cases, once you double-check that the behavior
    is compatible with the typical case in the system.
  prefs: []
  type: TYPE_NORMAL
- en: If necessary, or to perform tweaks, logs can be analyzed to generate an adequate
    profile of the typical interfaces exercised by the users. Remember to relay in
    data when possible. Load tests, though, are sometimes needed when there is no
    solid data, as they are done typically when new features are introduced, so estimations
    have to be used.
  prefs: []
  type: TYPE_NORMAL
- en: Remember to monitor the results of each simulation, and errors in particular.
    This will help detect possible problems. Load tests also exercise the monitoring
    of the system, so it's a good exercise in detecting weak points and improving
    on them.
  prefs: []
  type: TYPE_NORMAL
- en: The more intensive load tests are, the more problems they'll be able to capture.
    Then we can avoid those problems once real traffic is in play.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that creating the load can also suffer from its own bottlenecks.
    To multiply the simulations, it may be necessary to use multiple servers and ensure
    that the network is capable of supporting the traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Multiplying the simulation can be done directly by starting the process multiple
    times. This procedure, though simple, is quite effective and can be controlled
    with simple scripts. It also has the flexibility that the simulation can be any
    kind of process, including readjusted system tests using any existing software.
    This speeds up the preparation of the load test and builds trust that the simulation
    is accurate, as it reuses existing software that has been tested previously.
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to use specific tools aimed at common use cases like HTTP
    interfaces, for example, Locust ([https://locust.io/](https://locust.io/)). This
    tool allows us to create a web session, simulating a user accessing the system.
    The great advantages of Locust are that it already has a reporting system embedded
    and can be scaled with minimal preparation. However, it requires the creation
    of a new session explicitly for the load test and is only capable of working with
    web interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: Load tests should also be aimed at creating some headroom in the production
    cluster so they verify that the load is always under control, even in cases when
    it's growing, instead of finding bottlenecks during regular operations, which
    may produce incidents.
  prefs: []
  type: TYPE_NORMAL
- en: Versioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When making changes to any service, a system needs to be in place to track the
    different changes. That way, we can understand what gets deployed when and what
    has changed from last week.
  prefs: []
  type: TYPE_NORMAL
- en: This information is really powerful when you're facing an incident. One of the
    riskiest moments in a system is when there's a new deployment, as new code can
    create new problems. It's not unusual that an incident is produced due to the
    release of a new version.
  prefs: []
  type: TYPE_NORMAL
- en: '*Versioning* means assigning a unique code version to each service or system.
    It makes it easy to understand what software has been deployed and track down
    what has been changed from one version to another.'
  prefs: []
  type: TYPE_NORMAL
- en: Version numbers are normally assigned in the source control system at specific
    points to precisely track the code at that particular point. The point of having
    a defined version is to have a precise definition of the code under that unique
    version number. A version number that is applicable to multiple iterations of
    the code is useless.
  prefs: []
  type: TYPE_NORMAL
- en: Version numbers are about communicating the differences in code when talking
    about different snapshots of the same project. Their main objective is to communicate
    and allow us to understand how software evolves, not only within the team, but
    externally as well.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, versions were highly related to packaged software and different
    versions of the software that were sold in boxes, making them marketing versions.
    When the internal version was required, a *build number* was used, which was a
    consecutive number based on the number of times the software had been compiled.
  prefs: []
  type: TYPE_NORMAL
- en: Versions can not only be applied to whole software, but also to elements of
    it, as API version, library versions, etc. In the same way, different versions
    can be used effectively for the same software, such as for creating an internal
    version for the technical team but an external version for marketing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: For example, some software could be sold as Awesome Software `v4`, have an API
    `v2`, and internally be described as build number `v4.356`.
  prefs: []
  type: TYPE_NORMAL
- en: In modern software, where the releases are frequent and the version needs to
    change often, this simple method is not adequate, and instead different version
    schemas are created. The most common is *semantic versioning*.
  prefs: []
  type: TYPE_NORMAL
- en: We talked about semantic versioning in *Chapter 2*,*API Design*, but the topic
    is important enough to be repeated. Note that the same concept can be used both
    for APIs and code releases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Semantic versioning uses two or three numbers, separated by dots. An optional
    `v` prefix can be added to clarify that it refers to a version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The first number (`X`) is called the major version. The second (`Y`) is the
    minor version, and the last number (`Z`) is the patch version. These numbers are
    increased as new versions are generated:'
  prefs: []
  type: TYPE_NORMAL
- en: An increase in the major version indicates that the software is not compatible
    with previously existing software.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An increase in the minor version means that this version contains new features,
    but they don't break compatibility with older versions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, an increase of the patch version only covers bugfixes and other improvements
    like security patches. It fixes problems, but doesn't change the compatibility
    of the system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep in mind that increasing a major version number can also mark changes that
    would ordinarily appear in minor version updates, too. A change in the major version
    number will likely bring new features as well as major overhauls.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good example of this kind of versioning is the Python interpreter itself:'
  prefs: []
  type: TYPE_NORMAL
- en: Python 3 was an increase in the major version, and as such, code from Python
    2 required changes to be run under Python 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.9 introduced new features compared with Python 3.8, for example, the
    new union operators for dictionaries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.9.7 adds bugfixes and improvements over the previous patch version
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic versioning is very popular and it's particularly useful when dealing
    with APIs and with libraries that are going to be used externally. It provides
    a clear expectation, from just the version number, on what to expect from a new
    change, and allows clarity at the time of adding new features.
  prefs: []
  type: TYPE_NORMAL
- en: This kind of versioning, though, may be too restrictive for certain projects,
    and in particular, for internal interfaces. As it operates with small iterations
    that maintain compatibility along the way, only deprecating features after they
    are old, it works more like a window that is always evolving. Therefore, it's
    difficult to introduce a meaningful specific version.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the Linux kernel decided to move away from semantic versioning
    for this reason, deciding that instead new major versions will be small and not
    change things, and won''t carry any particular meaning: [http://lkml.iu.edu/hypermail/linux/kernel/1804.1/06654.html](http://lkml.iu.edu/hypermail/linux/kernel/1804.1/06654.html).'
  prefs: []
  type: TYPE_NORMAL
- en: When working with internal APIs, especially with microservices or internal libraries
    that change very often and are consumed by other parts of the organization, it
    is better to relax the rules and, while using something similar to semantic versioning,
    just using it as a general tool to increase version numbers in a consistent manner
    to provide an understanding of how the code changes, but without necessarily having
    to force changes in major or minor versions.
  prefs: []
  type: TYPE_NORMAL
- en: When communicating through external APIs, though, version numbers do not only
    carry a technical meaning, but also a marketing one. Using semantic versioning
    gives a strong assurance of the capacities of the API.
  prefs: []
  type: TYPE_NORMAL
- en: As versioning is so important, a good idea is to allow services to self-report
    their version number via a specific endpoint like `/api/version` or another easily
    accessed way to be sure that it's clear and can be checked by other dependant
    services.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that it can be possible to create a general version of a whole
    system, even if internally its different components have their own independent
    versions. In cases like online services, though, that can be tricky or pointless.
    Instead, the focus should be on maintaining backward compatibility.
  prefs: []
  type: TYPE_NORMAL
- en: Backward compatibility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The key aspect of changing architecture in a running system is the necessity
    of always keeping backward compatibility in its interfaces and APIs.
  prefs: []
  type: TYPE_NORMAL
- en: We also talked about backward compatibility in regard to databases changes in
    *Chapter 3*, *Data Modeling*. Here we will talk about interfaces, but it follows
    the same ideas.
  prefs: []
  type: TYPE_NORMAL
- en: Backward compatibility means that systems keep their old interfaces working
    as expected, so any calling system won't be affected by the change. This allows
    them to be upgraded at any point, without interrupting the service.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that backward compatibility needs to apply externally, as customers
    rely on a stable working interface, but also internally where multiple services
    interact with each other. If the system is complex and has multiple parts, the
    APIs connecting them should be backward compatible. This is particularly important
    in microservices architectures to allow the independent deployment of microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'This concept is quite simple, but it has implications on how changes need to
    be designed and implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: Changes should always *be additive*. That means that they *add* options, and
    don't remove them. This makes any existing calls to the system keep using the
    existing features and options and doesn't disrupt them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing options should be done with extreme care, and only after verifying
    that they are not used anymore. To be able to detect that, we need to adjust the
    monitoring so we have real data that can clearly provide solid data to allow us
    to determine this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With external interfaces, it may be almost impossible to remove any option or
    endpoint, especially on APIs. Customers don't want to change their existing systems
    to adjust to any changes unless there's a good reason, and even in that case it
    will take a lot of work to adequately communicate it. We will talk later in this
    chapter about this situation.
  prefs: []
  type: TYPE_NORMAL
- en: Web interfaces allow greater flexibility for changes as they are used manually
    by humans.
  prefs: []
  type: TYPE_NORMAL
- en: Even additive changes in externally accessible APIs are difficult. External
    customers tend to remember the API as it is, so it can be difficult to change
    the format of existing calls, even if it's just adding a new field.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This depends on the format used. Adding a new field in a JSON object is safer
    than changing a SOAP definition, which needs to be defined beforehand. This is
    one of the reasons why JSON is so popular – because it's flexible in the definition
    of the objects returned.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Nonetheless, for external APIs it could be safer to add new endpoints if necessary.
    API changes are normally done in stages, creating a new version of the API and
    trying to encourage customers to change to the new and better API. These migrations
    can be long and arduous, as external users will require clear advantages to be
    persuaded to adopt the change on their end.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A good example of how painful a change in APIs can be is the migration from
    Python 2 to Python 3\. Python 3 has been available since 2008, but took a long
    time to get any kind of traction, because programs written in Python 2 needed
    to be changed. The migration has been quite lengthy, even to the point that the
    last Python 2 interpreter (Python 2.7) was supported for ten years, from its first
    release in 2010 until 2020\. Even with that long process, there's still code in
    legacy systems working with Python 2\. This shows the difficulty of moving from
    one API to another if no backward compatibility is respected.
  prefs: []
  type: TYPE_NORMAL
- en: Existing tests, both unit and integration tests, are the best way to ensure
    that the API is backward compatible. In essence, any new feature should pass the
    tests without a problem, as the old behavior won't change. Good test coverage
    of the API functionality is the best way to maintain compatibility.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing changes in external interfaces is more complicated and normally
    requires the definition of stricter APIs and a slower pace of change. Internal
    interfaces allow greater flexibility, as their changes can be communicated across
    the organization in an incremental way that will allow adaptation without interrupting
    the service at any point.
  prefs: []
  type: TYPE_NORMAL
- en: Incremental changes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Incremental changes to the system, slowing mutating and adjusting the APIs,
    can be released in sequence with multiple services involved. But the changes need
    to be applied in sequence and keep backward compatibility in mind.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s say that we have two services: service A generates an interface
    displaying students taking exams, and calls service B to obtain the list of examinees.
    This is done by calling an internal endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: There's a new feature that needs to be introduced in service A that requires
    extra information from the examinees, and requires us to know the number of times
    that each examinee has attempted a particular exam to sort them adequately by
    that parameter. With the current information, that's impossible, but service B
    can be tweaked to return that information.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, the API needs to be *extended*, so it returns that information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Only after this change is properly done and deployed can service A use it.
    This process happens in the following stages:'
  prefs: []
  type: TYPE_NORMAL
- en: Initial stage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deployment of service B with `/examinees (v2)`. Note how service A will just
    ignore the extra field and keep working normally.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deployment of service A reading and using the new parameter `exam_tries`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All of the steps are stable. The service works without a problem throughout
    each one, so there's detachment between the different services.
  prefs: []
  type: TYPE_NORMAL
- en: This detachment is important because if there's a problem with a deployment,
    it can be reversed and only affects a single service, quickly reverting to the
    previous stable situation until the issue can be fixed. The worst situation is
    to have two changes in services that need to happen at the same time, as a failure
    in one will affect the other and reversing the situation may not be easy. Even
    worse, the problem could be in the interaction between them, and in that situation
    it won't be clear which one is responsible, because it could be both. It is important
    to keep to small individual steps where each step is solid and reliable.
  prefs: []
  type: TYPE_NORMAL
- en: 'This way of operating allows us to implement greater changes, for example,
    renaming a field. Let''s say that we don''t like the `examinee_id` field and want
    to change it for a more appropriate `student_id`. The process will go like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Update the returned object to include a new field called `student_id`, replicating
    the previous value in service B:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Update and deploy service A to use `student_id` instead of `examinee_id`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do the same in other services that possibly call service B.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use monitoring tools and logs to verify this!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Remove the old field from service B and deploy the service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Remove the old field from service B and deploy the service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This step is technically optional, though it would be good for maintenance reasons
    to remove cruft from the API. But the reality of the day-to-day work means that
    it's likely that it will stay there, just not being accessed anymore. A good balance
    needs to be found between the convenience of leaving it be and maintaining a clean
    and updated API.
  prefs: []
  type: TYPE_NORMAL
- en: This illustrates how we can deploy changes without interrupting the service
    in terms of *what* is being deployed. But, how can we ensure that the services
    are always available while deploying a new version?
  prefs: []
  type: TYPE_NORMAL
- en: Deploying without interruption
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To allow continuous releases without service interruption, we need to take the
    backward-compatible changes and deploy them while the service is still responding.
  prefs: []
  type: TYPE_NORMAL
- en: To do so, the best ally is the load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: We talked about load balancers in *Chapter 5*, *The Twelve-Factor App Methodology*,
    and *Chapter 8*, *Advanced Event-Driven Structures*. They are really useful!
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of a successful smooth deployment requires several instances of
    the service to be updated, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We are going to assume that we are using cloud instances or containers that
    can be created and destroyed easily. Keep in mind that you can treat them as workers
    under nginx or any other kind of web server acting as a load balancer inside a
    single server. This is how the `nginx reload` command works.
  prefs: []
  type: TYPE_NORMAL
- en: This is the initial stage, where all the instances have version 1 of the service
    to be updated:![Diagram
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17580_16_01.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 16.1: Starting point'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A new instance with service 2 is created. Note that it's not yet been added
    to the load balancer.![Diagram
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17580_16_02.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 16.2: New server created'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The new version is added to the load balancer. Right now, the requests can be
    directed to version 1 or version 2\. If we followed the principles of backward
    compatibility, though, this should not cause any problems.![Diagram
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17580_16_03.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 16.3: New server included in the load balancer'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To keep the number of instances constant, an old instance needs to be removed.
    A careful approach here means starting by disabling the old instance in the load
    balancer, so no new requests will be addressed. After the service finishes all
    the already-ongoing requests (remember, no new requests will be sent to this instance),
    the instance is effectively disabled and can be removed totally from the load
    balancer.![Diagram
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17580_16_04.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 16.4: Removal of an old server from the load balancer'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The old instance can be destroyed/recycled.![Diagram
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17580_16_05.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 16.5: Old server has been totally removed'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The process can be repeated until all instances are at version 2.![Diagram
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17580_16_06.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 16.6: Final stage with all new servers'
  prefs: []
  type: TYPE_NORMAL
- en: There are tools that allow us to do this process automatically. For example,
    Kubernetes will perform this automatically when rolling out changes to containers.
    We also saw that web services like nginx or Apache will do as well. But the same
    process can also be applied manually or through developing custom tools when an
    unusual use case demands it.
  prefs: []
  type: TYPE_NORMAL
- en: Feature flags
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea of feature flags is to hide functionality that is still not ready to
    be released under a configuration change. Following the principles of small increments
    and quick iteration makes it impossible to create big changes, like a new user
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: To complicate things further, these big changes will likely happen in parallel
    with others. There's no chance of delaying the whole release process for 6 months
    or more until the new user interface is working correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a separate branch that's long-lived is also not a great solution, as
    merging this branch becomes a nightmare. Long-living branches are complex to manage
    and always difficult to work with.
  prefs: []
  type: TYPE_NORMAL
- en: A better solution is to create a configuration parameter that activates or deactivates
    this feature. The feature can then be tested in a particular environment, while
    all the development continues at the same pace.
  prefs: []
  type: TYPE_NORMAL
- en: That means that other changes, like bug fixes or performance improvements, are
    still happening and being deployed. And the work done on the big new feature is
    merged into the main branch as often as usual. This means that the developed parts
    of the big new feature are also being released to the production environment,
    but they are not active yet.
  prefs: []
  type: TYPE_NORMAL
- en: Tests need to ensure that both options – the feature active and deactivated
    – work correctly, but working in small increments makes this relatively easy.
  prefs: []
  type: TYPE_NORMAL
- en: The feature will be then developed in small increments until it's ready for
    release. The final step is to simply enable it through a configuration change.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the feature may be active for certain users or environments. This
    is how beta features are tested: they rely on some users being able to access
    the feature before it is fully released. The test users could be internal to the
    organization initially, like QA teams, managers, product owners, etc., so they
    can provide feedback on the feature, but using production data.'
  prefs: []
  type: TYPE_NORMAL
- en: This technique allows us to grow in confidence and release big features without
    sacrificing small incremental approaches to it.
  prefs: []
  type: TYPE_NORMAL
- en: Teamwork aspects of changes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Software architecture is not only about technology, but a part of it is highly
    dependent on communication and human aspects.
  prefs: []
  type: TYPE_NORMAL
- en: The process of implementing changes in a system has some human elements affecting
    teamwork that need to be taken into consideration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that the work of a software architect typically lies in managing
    communication with multiple teams, which requires care and soft skills in both
    actively listening to teams and explaining or even negotiating design changes.
    Depending on the size of the organization, that could be challenging as different
    teams may have wildly different cultures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pace and acceptance of technical changes in an organization are tightly
    related to the organization's culture (or subcultures). Changes in organizations'
    ways of working typically occur much more slowly, although organizations that
    can quickly change technologies tend to be faster in adjusting to organization-wide
    changes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the same way, technology changes require support and training, even if it's
    purely within the organization. When requiring some big technology change, be
    sure to have a point of contact where the team can go to resolve doubts and questions.A
    lot of the questions can be solved by explaining *why* that change is required
    and working from there.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember when we talked about Conway's Law of software architecture in *Chapter
    1*,*Introduction to software architecture*, about how the communication structure
    and architectural structure are related. A change in one will likely affect the
    other, which means that big enough architectural changes will lead to organizational
    restructuring, which has its own challenges.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the same time, changes may have *winners* and *losers* in the affected teams.
    One engineer could feel threatened because they won't be able to use their favorite
    programming language. In the same way, their partner will be excited because now
    the opportunity to use their favorite piece of tech is amazing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This problem can be particularly poignant in team shuffling when people are moving
    around or when creating new teams. An important factor in the pace of development
    is to have an efficient team and making changes to teams has an impact on their
    communication and effectiveness. This impact needs to be analyzed and taken into
    consideration.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Maintenance needs to be introduced routinely as part of the day-to-day operations
    of the organization. Regular maintenance should include all security updates,
    but also tasks like upgrading OS versions, dependencies, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A general plan to deal with this kind of routine maintenance will provide clarity
    and clear expectations. For example*: the OS version will be upgraded within three
    to six months of a new LTS version being released*. This produces predictability,
    gives clear objectives to follow, and produces continuous improvement of the system.In
    the same way, automatic tools that detect security vulnerabilities make it easy
    for the team to know when it''s time to upgrade dependencies either in the code
    or in the underlying system.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the same way, the repayment of technical debt needs to be introduced as a habit
    to be sure that the system is healthy. Technical debt is typically detected by
    the teams themselves, as they'll have the best understanding of it, and is manifested
    with a progressively slower pace of code changes. If technical debt is not addressed,
    it will become more and more complicated to work with, making the development
    process more difficult and risking burnout by developers. Be sure to budget time
    to tackle it before it gets out of control.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a general consideration, just keep in mind that changes in architecture need
    to be carried out by members of the team, and that information needs to be communicated
    and executed correctly. As with any other task where communication is an important
    component, this presents its own challenges and problems, as communicating with
    people, especially with several people, is arguably one of the most difficult
    tasks in software development. Any software architecture designer needs to be
    aware of this and allocate enough time to be sure to, on one hand, communicate
    the plan adequately, and on the other, receive feedback and adjust accordingly
    to get the best results.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we described the different aspects and challenges of keeping
    a system running while developing and changing it, including its architecture.
  prefs: []
  type: TYPE_NORMAL
- en: We started by describing different ways that architecture can require adjustments
    and changes. We then moved on to talk about how to manage changes, including the
    option of having some designated time where the system won't be available, and
    introduced the concept of maintenance windows to clearly communicate expectations
    of stability and change.
  prefs: []
  type: TYPE_NORMAL
- en: We next went over the different incidents that can happen when problems arise,
    and the system struggles. We went over the necessary continuous process of improvement
    and reflection after an incident of this kind happens, and also looked at preparation
    processes that can be used before a significant event where the risk increases,
    for example, because of a marketing push expected to increase the load of the
    system.
  prefs: []
  type: TYPE_NORMAL
- en: To deal with this, we next introduced load testing and how it can be used to
    verify the system's capacity for accepting a defined load, making sure that it's
    ready to support the expected traffic. We talked as well about the necessity of
    creating a versioning system that clearly communicates what version of the software
    is currently deployed.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we talked about the critical aspect of backward compatibility and how
    is crucial in ensuring small, fast increments that are the key to continuous improvement
    and advancement. We also talked about how feature flags can help mix this process
    of releasing bigger features that need to be activated as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we described different aspects of how changes in a system and architecture
    can affect human collaboration and communication and how that needs to be taken
    into account while performing changes to the system, in particular changes that
    may affect the structure of the teams, which, as we've seen, will tend to replicate
    the structure of the software.
  prefs: []
  type: TYPE_NORMAL
