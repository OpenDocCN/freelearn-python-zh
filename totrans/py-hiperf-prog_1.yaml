- en: Chapter 1. Benchmarking and Profiling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recognizing the slow parts of your program is the single most important task
    when it comes to speeding up your code. In most cases, the bottlenecks account
    for a very small fraction of the program. By specifically addressing those critical
    spots you can focus on the parts that need improvement without wasting time in
    micro-optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: '**Profiling** is the technique that allows us to pinpoint the bottlenecks.
    A **profiler** is a program that runs the code and observes how long each function
    takes to run, detecting the slow parts of the program. Python provides several
    tools to help us find those bottlenecks and navigate the performance metrics.
    In this chapter, we will learn how to use the standard `cProfile` module, `line_profiler`
    and `memory_profiler`. We will also learn how to interpret the profiling results
    using the program **KCachegrind**.'
  prefs: []
  type: TYPE_NORMAL
- en: You may also want to assess the total execution time of your program and see
    how it is affected by your changes. We will learn how to write benchmarks and
    how to accurately time your programs.
  prefs: []
  type: TYPE_NORMAL
- en: Designing your application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When you are designing a performance-intensive program, the very first step
    is to write your code without having optimization in mind; quoting *Donald Knuth*:'
  prefs: []
  type: TYPE_NORMAL
- en: Premature optimization is the root of all evil.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the early development stages, the design of the program can change quickly,
    requiring you to rewrite and reorganize big chunks of code. By testing different
    prototypes without bothering about optimizations, you learn more about your program,
    and this will help you make better design decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mantras that you should remember when optimizing your code, are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Make it run**: We have to get the software in a working state, and be sure
    that it produces the correct results. This phase serves to explore the problem
    that we are trying to solve and to spot major design issues in the early stages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Make it right**: We want to make sure that the design of the program is solid.
    Refactoring should be done before attempting any performance optimization. This
    really helps separate the application into independent and cohesive units that
    are easier to maintain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Make it fast**: Once our program is working and has a good design we want
    to optimize the parts of the program that are not fast enough. We may also want
    to optimize memory usage if that constitutes an issue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section we will profile a test application—**a particle simulator**.
    The simulator is a program that takes some particles and evolves them over time
    according to a set of laws that we will establish. Those particles can either
    be abstract entities or correspond to physical objects. They can be, for example,
    billiard balls moving on a table, molecules in gas, stars moving through space,
    smoke particles, fluids in a chamber, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Those simulations are useful in fields such as Physics, Chemistry, and Astronomy,
    and the programs used to simulate physical systems are typically performance-intensive.
    In order to study realistic systems it's often necessary to simulate the highest
    possible number of bodies.
  prefs: []
  type: TYPE_NORMAL
- en: In our first example, we will simulate a system containing particles that constantly
    rotate around a central point at various speeds, like the hands of a clock.
  prefs: []
  type: TYPE_NORMAL
- en: The necessary information to run our simulation will be the starting positions
    of the particles, the speed, and the rotation direction. From these elements,
    we have to calculate the position of the particle in the next instant of time.
  prefs: []
  type: TYPE_NORMAL
- en: '![Designing your application](img/8458OS_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The basic feature of a circular motion is that the particles always move perpendicularly
    to the direction connecting the particle and the center, as shown in the preceding
    image. To move the particle we simply change the position by taking a series of
    very small steps in the direction of motion, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Designing your application](img/8458OS_01_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We will start by designing the application in an object-oriented way. According
    to our requirements, it is natural to have a generic `Particle` class that simply
    stores the particle position (*x*, *y*) and its angular speed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Another class, called `ParticleSimulator` will encapsulate our laws of motion
    and will be responsible for changing the positions of the particles over time.
    The `__init__` method will store a list of `Particle` instances and the `evolve`
    method will change the particle positions according to our laws.
  prefs: []
  type: TYPE_NORMAL
- en: We want the particles to rotate around the point (*x*, *y*), which, here, is
    equal to (0, 0), at constant speed. The direction of the particles will always
    be perpendicular to the direction from the center (refer to the first figure of
    this chapter). To find this vector
  prefs: []
  type: TYPE_NORMAL
- en: '![Designing your application](img/8458OS_01_041.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '(corresponding to the Python variables `v_x` and `v_y`) it is sufficient to
    use these formulae:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Designing your application](img/8458OS_01_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we let one of our particles move, after a certain time *dt*, it will follow
    a circular path, reaching another position. To let the particle follow that trajectory
    we have to divide the time interval *dt* into very small time steps where the
    particle moves tangentially to the circle. The final result, is just an approximation
    of a circular motion and, in fact, it''s similar to a polygon. The time steps
    should be very small, otherwise the particle trajectory will diverge quickly,
    as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Designing your application](img/8458OS_01_03(1).jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In a more schematic way, to calculate the particle position at time *dt* we
    have to carry out the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the direction of motion: `v_x`, `v_y`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the displacement (`d_x`, `d_y`) which is the product of time and speed
    and follows the direction of motion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1 and 2 for enough time steps to cover the total time *dt*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code shows the full `ParticleSimulator` implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the `matplotlib` library to visualize our particles. This library
    is not included in the Python standard library. To install it, you can follow
    the instructions included in the official documentation at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://matplotlib.org/users/installing.html](http://matplotlib.org/users/installing.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Alternatively, you can use the Anaconda Python distribution ([https://store.continuum.io/cshop/anaconda/](https://store.continuum.io/cshop/anaconda/))
    that includes `matplotlib` and most of the other third-party packages used in
    this book. Anaconda is free and available for Linux, Windows, and Mac.
  prefs: []
  type: TYPE_NORMAL
- en: The `plot` function included in `matplotlib` can display our particles as points
    on a Cartesian grid and the `FuncAnimation` class can animate the evolution of
    our particles over time.
  prefs: []
  type: TYPE_NORMAL
- en: The `visualize` function accomplishes this by taking the particle simulator
    and displaying the trajectory in an animated plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `visualize` function is structured as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Setup the axes and display the particles as points using the plot function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write an initialization function (`init`) and an update function (`animate`)
    that changes the *x*, *y* coordinates of the data points using the `line.set_data`
    method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a `FuncAnimation` instance passing the functions and some parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the animation with `plt.show()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The complete implementation of the visualize function is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we define a small test function—`test_visualize`—that animates a system
    of three particles rotating in different directions. Note that the third particle
    completes a round three times faster than the others:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Writing tests and benchmarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a working simulator, we can start measuring our performance
    and tuning-up our code, so that our simulator can handle as many particles as
    possible. The first step in this process is to write a test and a benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: We need a test that checks whether the results produced by the simulation are
    correct or not. In the optimization process we will rewrite the code to try different
    solutions; by doing so we may easily introduce bugs. Maintaining a solid test
    suite is essential to avoid wasting time on broken code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our test will take three particle and let the system evolve for 0.1 time units.
    We then compare our results, up to a certain precision, with those from a reference
    implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We also want to write a benchmark that can measure the performance of our application.
    This will provide an indication of how much we have improved over the previous
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our benchmark we instantiate 100 `Particle` objects with random coordinates
    and angular velocity, and feed them to a `ParticleSimulator` class. We then let
    the system evolve for 0.1 time units:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Timing your benchmark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can easily measure the execution time of any process from the command line
    by using the Unix `time` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `time` command is not available for Windows, but can be found in the `cygwin`
    shell that you can download from the official website [http://www.cygwin.com/](http://www.cygwin.com/).
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, `time` shows three metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '`real`: The actual time spent in running the process from start to finish,
    as if it was measured by a human with a stopwatch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`user`: The cumulative time spent by all the CPUs during the computation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sys`: The cumulative time spent by all the CPUs during system-related tasks
    such as memory allocation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice that sometimes `user` + `sys` might be greater than `real`, as multiple
    processors may work in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`time` also offers several formatting options; for an overview you can explore
    its manual (by using the `man time` command). If you want a summary of all the
    metrics available, you can use the `-v` option.'
  prefs: []
  type: TYPE_NORMAL
- en: The Unix `time` command is a good way to benchmark your program. To achieve
    a more accurate measurement, the benchmark should run long enough (in the order
    of seconds) so that the setup and tear-down of the process become small, compared
    to the execution time. The `user` metric is suitable as a monitor for the CPU
    performance, as the `real` metric includes also the time spent in other processes
    or waiting for I/O operations.
  prefs: []
  type: TYPE_NORMAL
- en: Another useful program to time Python scripts is the `timeit` module. This module
    runs a snippet of code in a loop for *n* times and measures the time taken. Then,
    it repeats this operation *r* times (by default the value of *r* is 3) and takes
    the best of those runs. Because of this procedure, `timeit` is suitable to accurately
    time small statements in isolation.
  prefs: []
  type: TYPE_NORMAL
- en: The `timeit` module can be used as a Python module, from the command line, or
    from **IPython**.
  prefs: []
  type: TYPE_NORMAL
- en: IPython is a Python shell designed for interactive usage. It boosts tab completion
    and many utilities to time, profile, and debug your code. We will make use of
    this shell to try out snippets throughout the book. The IPython shell accepts
    **magic commands**—statements that start with a `%` symbol—that enhance the shell
    with special behaviors. Commands that start with `%%` are called **cell magics**,
    and these commands can be applied on multi-line snippets (called **cells**).
  prefs: []
  type: TYPE_NORMAL
- en: 'IPython is available on most Linux distributions and is included in Anaconda.
    You can follow the installation instructions in the official documentation at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://ipython.org/install.html](http://ipython.org/install.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can use IPython as a regular Python shell (`ipython`) but it is also available
    in a Qt-based version (`ipython qtconsole`) and as a powerful browser-based interface
    (`ipython notebook`).
  prefs: []
  type: TYPE_NORMAL
- en: In IPython and command line interfaces it is possible to specify the number
    of loops or repetitions with the options `-n` and `-r`, otherwise they will be
    determined automatically. When invoking `timeit` from the command line, you can
    also give a setup code that will run before executing the statement in a loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code we show how to use timeit from IPython, from the command
    line and as a Python module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Notice that while the command line and IPython interfaces are automatically
    determining a reasonable value for `n`, the Python interface requires you to explicitly
    pass it as the `number` argument.
  prefs: []
  type: TYPE_NORMAL
- en: Finding bottlenecks with cProfile
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After assessing the execution time of the program we are ready to identify the
    parts of the code that need performance tuning. Those parts are typically quite
    small, compared to the size of the program.
  prefs: []
  type: TYPE_NORMAL
- en: 'Historically, there are three different profiling modules in Python''s standard
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The** `profile` **module**: This module is written in pure Python and adds
    a significant overhead to the program execution. Its presence in the standard
    library is due mainly to its extendibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The** `hotshot` **module**: A C module designed to minimize the profiling
    overhead. Its use is not recommended by the Python community and it is not available
    in Python 3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The** `cProfile` **module**: The main profiling module, with an interface
    similar to `profile`. It has a small overhead and it is suitable as a general
    purpose profiler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will see how to use the cProfile module in two different ways:'
  prefs: []
  type: TYPE_NORMAL
- en: From the command line
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From IPython
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to use `cProfile`, no change in the code is required, it can be executed
    directly on an existing Python script or function.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use `cProfile` from the command line in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print a long output containing several profiling metrics. You can
    use the option `-s` to sort the output by a certain metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'You can save an output file in a format readable by the `stats` module and
    other tools by passing the `-o` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also profile interactively from IPython. The `%prun` magic command
    lets you profile a function using `cProfile`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `cProfile` output is divided into five columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ncalls`: The number of times the function was called.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tottime`: The total time spent in the function without taking into account
    the calls to other functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cumtime`: The time spent in the function including other function calls.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`percall`: The time spent for a single call of the function—it can be obtained
    by dividing the total or cumulative time by the number of calls.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filename:lineno`: The filename and corresponding line number. This information
    is not present when calling C extensions modules.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most important metric is `tottime`, the actual time spent in the function
    body excluding sub-calls. In our case, the largest portion of time is spent in
    the `evolve` function. We can imagine that the loop is the section of the code
    that needs performance tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing data in a textual way can be daunting for big programs with a lot
    of calls and sub-calls. Some graphic tools aid the task by improving the navigation
    with an interactive interface.
  prefs: []
  type: TYPE_NORMAL
- en: KCachegrind is a GUI (Graphical User Interface) useful to analyze the profiling
    output of different programs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'KCachegrind is available in Ubuntu 13.10 official repositories. The Qt port,
    QCacheGrind can be downloaded for Windows from the following web page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://sourceforge.net/projects/qcachegrindwin/](http://sourceforge.net/projects/qcachegrindwin/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mac users can compile QCacheGrind using Mac Ports ([http://www.macports.org/](http://www.macports.org/))
    by following the instructions present in the blog post at this link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://blogs.perl.org/users/rurban/2013/04/install-kachegrind-on-macosx-with-ports.html](http://blogs.perl.org/users/rurban/2013/04/install-kachegrind-on-macosx-with-ports.html)'
  prefs: []
  type: TYPE_NORMAL
- en: KCachegrind can't read directly the output files produced by `cProfile`. Luckily,
    the `pyprof2calltree` third-party Python module is able to convert the `cProfile`
    output file into a format readable by KCachegrind.
  prefs: []
  type: TYPE_NORMAL
- en: You can install `pyprof2calltree` from source ([https://pypi.python.org/pypi/pyprof2calltree/](https://pypi.python.org/pypi/pyprof2calltree/))
    or from the Python Package Index ([https://pypi.python.org/](https://pypi.python.org/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To best show the KCachegrind features we will use another example with a more
    diversified structure. We define a recursive function `factorial`, and two other
    functions that use `factorial`, and they are `taylor_exp` and `taylor_sin`. They
    represent the polynomial coefficients of the Taylor approximations of `exp(x)`
    and `sin(x)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to first generate the `cProfile` output file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can convert the output file with `pyprof2calltree` and launch KCachegrind:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![Finding bottlenecks with cProfile](img/8458OS_01_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding image is a screenshot of the KCachegrind user interface. On the
    left we have an output fairly similar to `cProfile`. The actual column names are
    slightly different: **Incl.** translates to `cProfile` module''s `cumtime`; **Self**
    translates to `tottime`. The values are given in percentages by clicking on the
    **Relative** button on the menu bar.By clicking on the column headers you can
    sort by the corresponding property.'
  prefs: []
  type: TYPE_NORMAL
- en: On the top right, a click on the **Callee Map** tab contains a diagram of the
    function costs. In the diagram, each function is represented by a rectangle and
    the time percentage spent by the function is proportional to the area of the rectangle.
    Rectangles can contain sub-rectangles that represent sub-calls to other functions.
    In this case, we can easily see that there are two rectangles for the `factorial`
    function. The one on the left corresponds to the calls made by `taylor_exp` and
    the one on the right to the calls made by `taylor_sin`.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the bottom right, you can display another diagram—the call **graph**—by
    clicking on the **Call Graph** tab. A call graph is a graphical representation
    of the calling relationship between the functions: each square represents a function
    and the arrows imply a calling relationship. For example, `taylor_exp` calls <listcomp>
    (a list comprehension) which calls `factorial` **500** times `taylor_sin` calls
    factorial **250** times. KCachegrind also detects recursive calls: `factorial`
    calls itself **187250** times.'
  prefs: []
  type: TYPE_NORMAL
- en: You can navigate to the **Call Graph** or the **Caller Map** tabs by double-clicking
    on the rectangles; the interface will update accordingly showing that the timing
    properties are relative to the selected function. For example, double-clicking
    on `taylor_exp` will cause the graph to change, showing only the `taylor_exp`
    contribution to the total cost.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Gprof2Dot** ([https://code.google.com/p/jrfonseca/wiki/Gprof2Dot](https://code.google.com/p/jrfonseca/wiki/Gprof2Dot))
    is another popular tool used to produce call graphs. Starting from output files
    produced by one of the supported profilers, it will generate a `.dot` diagram
    representing the call graph.'
  prefs: []
  type: TYPE_NORMAL
- en: Profile line by line with line_profiler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we know which function we have to optimize, we can use the `line_profiler`
    module that shows us how time is spent in a line-by-line fashion. This is very
    useful in situations where it''s difficult to determine which statements are costly.
    The `line_profiler` module is a third-party module that is available on the Python
    Package Index and can be installed by following the instructions on its website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://pythonhosted.org/line_profiler/](http://pythonhosted.org/line_profiler/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use `line_profiler`,we need to apply a `@profile` decorator to
    the functions we intend to monitor. Notice that you don''t have to import the
    `profile` function from another module, as it gets injected in the global namespace
    when running the profiling script `kernprof.py`. To produce profiling output for
    our program we need to add the `@profile` decorator to the `evolve` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The script `kernprof.py` will produce an output file and will print on standard
    output the result of the profiling. We should run the script with two options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-l` to use the `line_profiler` function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-v` to immediately print the results on screen'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It is also possible to run the profiler in an IPython shell for interactive
    editing. You should first load the `line_profiler` extension that will provide
    the magic command `lprun`. By using that command you can avoid adding the `@profile`
    decorator.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is quite intuitive and is divided into columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Line number**: The number of the line that was run'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hits**: The number of times that line was run'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time**: The execution time of the line in microseconds (Time)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Per Hit**: Time divided by hits'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**% Time**: Fraction of the total time spent executing that line'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Line Contents**: the source of the corresponding line'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By looking at the percentage column we can have a pretty good idea of where
    the time is spent. In this case, there are a few statements in the `for` loop
    body with a cost of around 10-20 percent each.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing our code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have identified exactly how the time is spent, we can modify the
    code and assess the change in performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few different ways to tune up our pure Python code. The way that
    usually produces the most remarkable results is to change the *algorithm*. In
    this case, instead of calculating the velocity and adding small steps, it would
    be more efficient (and correct, as it is not an approximation) to express the
    equations of motion in terms of radius `r` and angle `alpha` (instead of `x` and
    `y`), and then calculate the points on a circle using the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Another way lies in minimizing the number of instructions. For example, we can
    pre-calculate the factor `timestep * p.ang_speed` that doesn't change with time.
    We can exchange the loop order (first we iterate on particles, then we iterate
    on time steps) and put the calculation of the factor outside of the loop on the
    particles.
  prefs: []
  type: TYPE_NORMAL
- en: 'The line by line profiling showed also that even simple assignment operations
    can take a considerable amount of time. For example, the following statement takes
    more than 10 percent of the total time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, a way to optimize the loop is reducing the number of assignment
    operations. To do that, we can avoid intermediate variables by sacrificing readability
    and rewriting the expression in a single and slightly more complex statement (notice
    that the right-hand side gets evaluated completely before being assigned to the
    variables):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This leads to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'After applying the changes we should make sure that the result is still the
    same, by running our test. We can then compare the execution times using our benchmark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: By acting on pure Python we obtained just a modest increment in speed.
  prefs: []
  type: TYPE_NORMAL
- en: The dis module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, it's not easy to evaluate how many operations a Python statement
    will take. In this section, we will explore Python internals to estimate the performance
    of Python statements. Python code gets converted to an intermediate representation—called
    **bytecode**—that gets executed by the Python virtual machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'To help inspect how the code gets converted into bytecode we can use the Python
    module `dis` (disassemble). Its usage is really simple, it is sufficient to call
    the function `dis.dis` on the `ParticleSimulator.evolve` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate, for each line, a list of bytecode instructions. For example,
    the statement `v_x = (-p.y)/norm` is expanded in the following set of instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`LOAD_FAST` loads a reference of the variable `p` onto the stack, `LOAD_ATTR`
    loads the `y` attribute of the item present on top of the stack. The other instructions
    (`UNARY_NEGATIVE` and `BINARY_TRUE_DIVIDE`) simply do arithmetic operations on
    top-of-stack items. Finally, the result is stored in `v_x` (`STORE_FAST`).'
  prefs: []
  type: TYPE_NORMAL
- en: By analyzing the complete `dis` output we can see that the first version of
    the loop produces 51 bytecode instructions, while the second gets converted into
    35 instructions.
  prefs: []
  type: TYPE_NORMAL
- en: The `dis` module helps discover how the statements get converted and serve mainly
    as an exploration and learning tool of the Python bytecode representation.
  prefs: []
  type: TYPE_NORMAL
- en: To improve our performance even further, we could keep trying to figure out
    other approaches to reduce the amount of instructions. It's clear however, that
    this approach has some limits and it is probably not the right tool for the job.
    In the next chapter, we will see how to speed up those kinds of calculations with
    the help of NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling memory usage with memory_profiler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In some cases, memory usage constitutes an issue. For example, if we want to
    handle a huge number of particles we will have a memory overhead due to the creation
    of many `Particle` instances.
  prefs: []
  type: TYPE_NORMAL
- en: The module `memory_profiler` summarizes, in a way similar to `line_profiler`,
    the memory usage of the process.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `memory_profiler` package is also available on the Python Package Index.
    You should also install the `psutil` module ([https://code.google.com/p/psutil/](https://code.google.com/p/psutil/))
    as an optional dependency, it will make `memory_profiler` run considerably faster.
  prefs: []
  type: TYPE_NORMAL
- en: Just like `line_profiler`, `memory_profiler` also requires the instrumentation
    of the source code, by putting a `@profile` decorator on the function we intend
    to monitor. In our case, we want to analyze the function `benchmark`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can slightly change `benchmark` to instantiate a considerable amount (100000)
    of `Particle` instances and decrease the simulation time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use `memory_profiler` from an IPython shell through the magic command
    `%mprun`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is possible to run `memory_profiler` from the shell using the `mprof run`
    command after adding the `@profile` decorator.
  prefs: []
  type: TYPE_NORMAL
- en: From the output we can see that 100000 `Particle` objects take 25.7 **MiB**
    of memory.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 1 MiB (mebibyte) is equivalent to 1024² = 1,048,576 bytes. It is different from
    1 MB (*megabyte*), which is equivalent to 1000² = 1,000,000 bytes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `__slots__` on the `Particle` class to reduce its memory footprint.
    This feature saves some memory by avoiding storing the variables of the instance
    in an internal dictionary. This optimization has a drawback: it prevents the addition
    of attributes other than the ones specified in `__slots__` (to use this feature
    in Python 2 you should make sure that you are using new-style classes):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: By rewriting the `Particle` class using `__slots__` we can save 11 MiB of memory.
  prefs: []
  type: TYPE_NORMAL
- en: Performance tuning tips for pure Python code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a rule of thumb, when optimizing pure Python code, you should look at what
    is available in the standard library. The standard library contains clever algorithms
    for the most common data structures such as lists, dicts, and sets. Furthermore,
    a lot of standard library modules are implemented in C and have fast processing
    times. However, it's important to always time the different solutions—the outcomes
    are often unpredictable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `collections` module provides extra data containers that can efficiently
    handle some common operations. For example, you can use `deque` in place of a
    list when you need to pop items from the start and append new items at the end.
    The `collections` module also includes a `Counter` class that can be used to count
    repeated elements in an iterable object. Beware, that `Counter` can be slower
    than the equivalent code written with a standard loop over a dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You can put the code in a file named `purepy.py` and time it through IPython:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'In general, list comprehension and generators should be preferred in place
    of explicit loops. Even if the speedup over a standard loop is modest, this is
    a good practice because it improves readability. We can see in the following example,
    that both list comprehension and generator expressions are faster than an explicit
    loop when combined with the function `sum`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can add those functions to `purepy.py` and test with IPython:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The `bisect` module can help with fast insertion and retrieval of elements,
    while maintaining a sorted list.
  prefs: []
  type: TYPE_NORMAL
- en: Raw optimization of pure Python code is not very effective, unless there is
    a substantial algorithmic advantage. The second-best way to speed up your code
    is to use external libraries specifically designed for the purpose, such as `numpy`,
    or to write extensions modules in a more "down to the metal" language such as
    C with the help of **Cython**.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the basic principles of optimization and we applied
    those principles to our test application. The most important thing is identifying
    the bottlenecks in the application before editing the code. We saw how to write
    and time a benchmark using the `time` Unix command and the Python `timeit` module.
    We learned how to profile our application using `cProfile`, `line_profiler`, and
    `memory_profiler`, and how to analyze and navigate graphically the profiling data
    with KCachegrind. We surveyed some of the strategies to optimize pure Python code
    by leveraging the tools available in the standard library.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how to use `numpy` to dramatically speedup
    computations in an easy and convenient way.
  prefs: []
  type: TYPE_NORMAL
