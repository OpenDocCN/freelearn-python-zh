<html><head></head><body>
		<div id="_idContainer084">
			<h1 id="_idParaDest-288" class="chapter-number"><a id="_idTextAnchor292"/>10</h1>
			<h1 id="_idParaDest-289"><a id="_idTextAnchor293"/>Solving Numerical, Symbolic, and Graphical Problems</h1>
			<p>Microservice architecture is not only used to build fine-grained, optimized, and scalable applications in the banking, insurance, production, human resources, and manufacturing industries. It is also used to develop scientific and computation-related research and scientific software prototypes for<a id="_idIndexMarker826"/> applications such as <strong class="bold">laboratory information management systems</strong> (<strong class="bold">LIMSs</strong>), weather<a id="_idIndexMarker827"/> forecasting systems, <strong class="bold">geographical information systems</strong> (<strong class="bold">GISs</strong>), and healthcare systems.</p>
			<p>FastAPI is one of the best choices in building these granular services since they usually involve highly computational tasks, workflows, and reports. This chapter will highlight some transactions not yet covered in the previous chapters, such as symbolic computations using <strong class="source-inline">sympy</strong>, solving linear systems using <strong class="source-inline">numpy</strong>, plotting mathematical models using <strong class="source-inline">matplotlib</strong>, and generating data archives using <strong class="source-inline">pandas</strong>. This chapter will also show you how FastAPI is flexible when solving workflow-related transactions by simulating some Business Process Modeling Notation (BPMN) tasks. For developing big data applications, a portion of this chapter will showcase GraphQL queries for big data applications and Neo4j graph databases for graph-related projects with the framework.</p>
			<p>The main objective of this chapter is to introduce the FastAPI framework as a tool for providing microservice solutions for scientific research and computational sciences. </p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Setting up the projects</li>
				<li>Implementing the symbolic computations</li>
				<li>Creating arrays and DataFrames</li>
				<li>Performing statistical analysis</li>
				<li>Generating CSV and XLSX reports</li>
				<li>Plotting data models</li>
				<li>Simulating a BPMN workflow</li>
				<li>Using GraphQL queries and mutations</li>
				<li>Utilizing the Neo4j graph database</li>
			</ul>
			<h1 id="_idParaDest-290"><a id="_idTextAnchor294"/>Technical requirements</h1>
			<p>This chapter provides the base skeleton of a <strong class="bold">periodic census and computational system</strong> that enhances fast data collection procedures in different areas of a specific country. Although unfinished, the prototype provides FastAPI implementations that highlight important topics of this chapter, such as creating and plotting mathematical models, gathering answers from respondents, providing questionnaires, creating workflow templates, and utilizing a graph database. The code for this chapter can be found at <a href="https://github.com/PacktPublishing/Building-Python-Microservices-with-FastAPI">https://github.com/PacktPublishing/Building-Python-Microservices-with-FastAPI</a> in the <strong class="source-inline">ch10</strong> project. </p>
			<h1 id="_idParaDest-291"><a id="_idTextAnchor295"/>Setting up the projects</h1>
			<p>The <a id="_idIndexMarker828"/>PCCS project has two versions: <strong class="source-inline">ch10-relational</strong>, which uses a PostgreSQL database with Piccolo ORM as the data mapper, and <strong class="source-inline">ch10-mongo</strong>, which saves data as MongoDB documents using Beanie ODM.</p>
			<h2 id="_idParaDest-292"><a id="_idTextAnchor296"/>Using the Piccolo ORM </h2>
			<p><strong class="source-inline">ch10-relational</strong> uses a fast <a id="_idIndexMarker829"/>Piccolo ORM that can support both sync and async CRUD transactions. This ORM was not introduced in <a href="B17975_05.xhtml#_idTextAnchor107"><em class="italic">Chapter 5</em></a><em class="italic">, Connecting to a Relational Database</em>, because it is more appropriate for computational, data science-related, and big data applications. The Piccolo ORM is different from other ORMs because it scaffolds a project containing the initial project structure and templates for customization. But before creating the project, we need to install the <strong class="source-inline">piccolo</strong> module using <strong class="source-inline">pip</strong>:</p>
			<p class="source-code">pip install piccolo</p>
			<p>Afterward, install the <strong class="source-inline">piccolo-admin</strong> module, which provides helper classes for the GUI administrator page of its projects:</p>
			<p class="source-code">pip install piccolo-admin</p>
			<p>Now, we can create a project inside a newly created root project folder by running <strong class="source-inline">piccolo asgi new</strong>, a CLI command that scaffolds the Piccolo project directory. The process will ask for the API framework and application server to utilize, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/Figure_10.01_B17975.jpg" alt="Figure 10.1 – Scaffolding a Piccolo ORM project&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Scaffolding a Piccolo ORM project</p>
			<p>You must <a id="_idIndexMarker830"/>use FastAPI for the application framework and <strong class="source-inline">uvicorn</strong> is the recommended ASGI server. Now, we can add Piccolo applications inside the project by running the <strong class="source-inline">piccolo app new</strong> command inside the project folder. The following screenshot shows the main project directory, where we execute the CLI command to create a Piccolo application:</p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/Figure_10.02_B17975.jpg" alt="Figure 10.2 – Piccolo project directory&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Piccolo project directory</p>
			<p>The scaffolded <a id="_idIndexMarker831"/>project always has a default application called <strong class="source-inline">home</strong>, but it can be modified or even deleted. Once removed, the Piccolo platform allows you to replace <strong class="source-inline">home</strong> by adding a new application to the project by running the <strong class="source-inline">piccolo app new</strong> command inside the project folder, as shown in the preceding screenshot. A Piccolo application contains the ORM models, BaseModel, services, repository classes, and API methods. Each application has an auto-generated <strong class="source-inline">piccolo_app.py</strong> module where we need to configure an <strong class="source-inline">APP_CONFIG</strong> variable to register all the ORM details. The following is the configuration of our project’s survey application:</p>
			<pre class="source-code">
<strong class="bold">APP_CONFIG = AppConfig(</strong>
    app_name="survey",
    migrations_folder_path=os.path.join(
        CURRENT_DIRECTORY, "piccolo_migrations"
    ),
    <strong class="bold">table_classes=[Answers, Education, Question, Choices, </strong>
       <strong class="bold">Profile, Login, Location, Occupation, Respondent],</strong>
    migration_dependencies=[],
    commands=[],
<strong class="bold">)</strong></pre>
			<p>For the ORM platform to recognize the new Piccolo application, its <strong class="source-inline">piccolo_app.py</strong> must be added to <strong class="source-inline">APP_REGISTRY</strong> of the main project’s <strong class="source-inline">piccolo_conf.py</strong> module. The following is the content of the <strong class="source-inline">piccolo_conf.py</strong> file of our <strong class="source-inline">ch10-piccolo</strong> project:</p>
			<pre class="source-code">
<strong class="bold">from piccolo.engine.postgres import PostgresEngine</strong>
<strong class="bold">from piccolo.conf.apps import AppRegistry</strong>
<strong class="bold">DB = PostgresEngine(</strong>
    <strong class="bold">config={</strong>
        <strong class="bold">"database": "pccs",</strong>
        <strong class="bold">"user": "postgres",</strong>
        <strong class="bold">"password": "admin2255",</strong>
        <strong class="bold">"host": "localhost",</strong>
        <strong class="bold">"port": 5433,</strong>
    <strong class="bold">}</strong>
<strong class="bold">)</strong>
<strong class="bold">APP_REGISTRY = AppRegistry(</strong>
    <strong class="bold">apps</strong>=["<strong class="bold">survey.piccolo_app</strong>", 
          "piccolo_admin.piccolo_app"]
)</pre>
			<p>The <strong class="source-inline">piccolo_conf.py</strong> file is <a id="_idIndexMarker832"/>also the module where we establish the PostgreSQL database connection. Aside from PostgreSQL, the Piccolo ORM also supports SQLite databases.</p>
			<h3>Creating the data models</h3>
			<p>Like in Django ORM, Piccolo ORM<a id="_idIndexMarker833"/> has migration commands to generate the database tables based on model classes. But first, we need to create model classes by utilizing its <strong class="source-inline">Table</strong> API class. It also has helper classes to establish column mappings and foreign key relationships. The following are some data model classes that comprise our database <strong class="source-inline">pccs</strong>:</p>
			<pre class="source-code">
<strong class="bold">from piccolo.columns import ForeignKey, Integer, Varchar,</strong>
       <strong class="bold">Text, Date, Boolean, Float</strong>
<strong class="bold">from piccolo.table import Table</strong>
class Login(<strong class="bold">Table</strong>):
    <strong class="bold">username = Varchar(unique=True)</strong>
    <strong class="bold">password = Varchar()</strong>
class Education(<strong class="bold">Table</strong>):
    <strong class="bold">name = Varchar()</strong>
class Profile(<strong class="bold">Table</strong>):
    <strong class="bold">fname = Varchar()</strong>
    <strong class="bold">lname = Varchar()</strong>
    <strong class="bold">age = Integer()</strong>
    <strong class="bold">position = Varchar()</strong>
    <strong class="bold">login_id = ForeignKey(Login, unique=True)</strong>
    <strong class="bold">official_id = Integer()</strong>
    <strong class="bold">date_employed = Date()</strong></pre>
			<p>After creating<a id="_idIndexMarker834"/> the model classes, we can update the database by creating the migrations files. Migration is a way of updating the database of a project. In the Piccolo platform, we can run the <strong class="source-inline">piccolo migrations new &lt;app_name&gt;</strong> command to generate files in the <strong class="source-inline">piccolo_migrations</strong> folder. These are called migration files and they contain migration scripts. But to save time, we will include the <strong class="source-inline">--auto</strong> option for the command to let the ORM check the recently executed migration files and auto-generate the migration script containing the newly reflected schema updates. Check the newly created migration file first before running the <strong class="source-inline">piccolo migrations forward &lt;app_name&gt;</strong> command to execute the migration script. This last command will auto-create all the tables in the database based on the model classes.</p>
			<h3>Implementing the repository layer</h3>
			<p>Creating the <a id="_idIndexMarker835"/>repository layer comes after performing all the necessary migrations. Piccolo’s CRUD operations are like those in the Peewee ORM. It is swift, short, and easy to implement. The following code shows an implementation of the <strong class="source-inline">insert_respondent()</strong> transaction, which adds a new respondent profile:</p>
			<pre class="source-code">
<strong class="bold">from survey.tables import Respondent</strong>
from typing import Dict, List, Any
<strong class="bold">class RespondentRepository</strong>:
    <strong class="bold">async def insert_respondent(self, </strong>
             <strong class="bold">details:Dict[str, Any]) -&gt; bool: </strong>
        try:
            <strong class="bold">respondent = Respondent(**details)</strong>
            <strong class="bold">await respondent.save()</strong>
        except Exception as e: 
            return False 
        return True</pre>
			<p>Like Peewee, Piccolo’s model classes can persist records, as shown by <strong class="source-inline">insert_respondent()</strong>, which implements an asynchronous <strong class="source-inline">INSERT</strong> transaction. On the other hand, <strong class="source-inline">get_all_respondent()</strong> retrieves all respondent profiles and has the same approach as Peewee, as shown here:  </p>
			<pre class="source-code">
    <strong class="bold">async def get_all_respondent(self):</strong>
        return <strong class="bold">await Respondent.select()</strong>
                  <strong class="bold">.order_by(Respondent.id)</strong></pre>
			<p>The remaining Peewee-like <strong class="source-inline">DELETE</strong> and <strong class="source-inline">UPDATE</strong> respondent transactions are created in the project’s <strong class="source-inline">/survey/repository/respondent.py</strong> module. </p>
			<h2 id="_idParaDest-293"><a id="_idTextAnchor297"/>The Beanie ODM</h2>
			<p>The <a id="_idIndexMarker836"/>second version of the PCCS project, <strong class="source-inline">ch10-mongo</strong>, utilizes a MongoDB datastore and uses the Beanie ODM to implement its asynchronous CRUD transactions. We covered Beanie in <a href="B17975_06.xhtml#_idTextAnchor155"><em class="italic">Chapter 6</em></a><em class="italic">, Using a Non-Relational Database</em>. Now, let us learn how to apply FastAPI in symbolic computations. We will be using the <strong class="source-inline">ch10-piccolo</strong> project for this.</p>
			<h1 id="_idParaDest-294"><a id="_idTextAnchor298"/>Implementing symbolic computations</h1>
			<p><strong class="bold">Symbolic computation</strong> is a<a id="_idIndexMarker837"/> mathematical approach to solving problems using symbols or mathematical variables. It uses mathematical equations or expressions formulated using symbolic variables to solve linear and nonlinear systems, rational expressions, logarithmic expressions, and other complex real-world models. To perform symbolic computation in Python, you must install the <strong class="source-inline">sympy</strong> module using the <strong class="source-inline">pip</strong> command:</p>
			<p class="source-code">pip install sympy</p>
			<p>Let us now start creating our first symbolic expressions.</p>
			<h2 id="_idParaDest-295"><a id="_idTextAnchor299"/>Creating symbolic expressions</h2>
			<p>One way of<a id="_idIndexMarker838"/> implementing the FastAPI endpoint that performs symbolic computation is to create a service that accepts a mathematical model or equation as a string and converts that string into a <strong class="source-inline">sympy</strong> symbolic expression. The following <strong class="source-inline">substitute_eqn()</strong> processes an equation in <strong class="source-inline">str</strong> format and converts it into valid linear or nonlinear bivariate equations with the <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> variables. It also accepts values for <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> to derive the solution of the expression:</p>
			<pre class="source-code">
<strong class="bold">from sympy import symbols, sympify</strong>
<strong class="bold">@router.post("/sym/equation")</strong>
async def substitute_bivar_eqn(<strong class="bold">eqn: str, xval:int, </strong>
               <strong class="bold">yval:int</strong>):
    try:
        <strong class="bold">x, y = symbols('x, y')</strong>
        <strong class="bold">expr = sympify(eqn)</strong>
        return <strong class="bold">str(expr.subs({x: xval, y: yval}))</strong>
    except:
        return JSONResponse(content={"message": 
            "invalid equations"}, status_code=500)</pre>
			<p>Before converting a string equation into a <strong class="source-inline">sympy</strong> expression, we need to define the <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> variables as <strong class="source-inline">Symbols</strong> objects using the <strong class="source-inline">symbols()</strong> utility. This method accepts a string of comma-delimited variable names and returns a tuple of symbols equivalent to the variables. After creating all the needed <strong class="source-inline">Symbols()</strong> objects, we can convert our equation into <strong class="source-inline">sympy</strong> expressions by using any of the following <strong class="source-inline">sympy</strong> methods:</p>
			<ul>
				<li><strong class="source-inline">sympify()</strong>: This uses <strong class="source-inline">eval()</strong> to convert the string equation into a valid <strong class="source-inline">sympy</strong> expression with all Python types converted into their <strong class="source-inline">sympy</strong> equivalents</li>
				<li><strong class="source-inline">parse_expr()</strong>: A full-fledged expression parser that transforms and modifies the tokens of the expression and converts them into their <strong class="source-inline">sympy</strong> equivalents</li>
			</ul>
			<p>Since the <strong class="source-inline">substitute_bivar_eqn()</strong> service utilizes the <strong class="source-inline">sympify()</strong> method, the string expression needs to be sanitized from unwanted code before sympifying to avoid any compromise. </p>
			<p>On the other <a id="_idIndexMarker839"/>hand, the <strong class="source-inline">sympy</strong> expression object has a <strong class="source-inline">subs()</strong> method to substitute values to derive the solution. Its resulting object must be converted into <strong class="source-inline">str</strong> format for <strong class="source-inline">Response</strong> to render the data. Otherwise, <strong class="source-inline">Response</strong> will raise <strong class="source-inline">ValueError</strong>, regarding the result as non-iterable. </p>
			<h2 id="_idParaDest-296"><a id="_idTextAnchor300"/>Solving linear expressions</h2>
			<p>The <strong class="source-inline">sympy</strong> module allows you to <a id="_idIndexMarker840"/>implement services that solve multivariate systems of linear equations. The following API service highlights an implementation that accepts two bivariate linear models in string format with their respective solutions:</p>
			<pre class="source-code">
<strong class="bold">from sympy import Eq, symbols, Poly, solve, sympify</strong>
<strong class="bold">@router.get("/sym/linear")</strong>
<strong class="bold">async def solve_linear_bivar_eqns(eqn1:str, </strong>
            <strong class="bold">sol1: int, eqn2:str, sol2: int):</strong>
    x, y = symbols('x, y')
    
    expr1 = <strong class="bold">parse_expr(eqn1, locals())</strong>
    expr2 = <strong class="bold">parse_expr(eqn2, locals())</strong>
    
    <strong class="bold">if Poly(expr1, x).is_linear and </strong>
                 <strong class="bold">Poly(expr1, x).is_linear:</strong>
        <strong class="bold">eq1 = Eq(expr1, sol1)</strong>
        <strong class="bold">eq2 = Eq(expr2, sol2)</strong>
        <strong class="bold">sol = solve([eq1, eq2], [x, y])</strong>
        return <strong class="bold">str(sol)</strong>
    else:
        return <strong class="bold">None</strong></pre>
			<p>The <strong class="source-inline">solve_linear_bivar_eqns()</strong> service accepts two bivariate linear equations and their respective outputs (or intercepts) and aims to establish a system of linear equations. First, it registers the <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> variables as <strong class="source-inline">sympy</strong> objects and then uses the <strong class="source-inline">parser_expr()</strong> method to transform the string expressions into their <strong class="source-inline">sympy</strong> equivalents. Afterward, the <a id="_idIndexMarker841"/>service needs to establish linear equality of these equations using the <strong class="source-inline">Eq()</strong> solver, which maps each <strong class="source-inline">sympy</strong> expression to its solution. Then, the API service passes all these linear equations to the <strong class="source-inline">solve()</strong> method to derive the <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> values. The result of <strong class="source-inline">solve()</strong> also needs to be rendered as a string, like in the substitution.</p>
			<p>Aside from the <strong class="source-inline">solve()</strong> method, the <a id="_idIndexMarker842"/>API also uses the <strong class="source-inline">Poly()</strong> utility to create a polynomial object from an expression to be able to access essential properties of an equation, such as <strong class="source-inline">is_linear()</strong>.</p>
			<h2 id="_idParaDest-297"><a id="_idTextAnchor301"/>Solving non-linear expressions</h2>
			<p>The <a id="_idIndexMarker843"/>previous <strong class="source-inline">solve_linear_bivar_eqns()</strong> can be reused to solve non-linear systems. The tweak is to shift the validation from filtering the linear equations to any non-linear equations. The following script highlights this code change:</p>
			<pre class="source-code">
<strong class="bold">@router.get("/sym/nonlinear")</strong>
async def solve_nonlinear_bivar_eqns(eqn1:str, sol1: int, 
           eqn2:str, sol2: int):
    … … … … … …
    … … … … … …    
    <strong class="bold">if not Poly(expr1, x, y).is_linear or </strong>
              <strong class="bold">not Poly(expr1, x, y).is_linear:</strong>
    … … … … … …
    … … … … … …
        return str(sol)
    else:
        return None</pre>
			<h2 id="_idParaDest-298"><a id="_idTextAnchor302"/>Solving linear and non-linear inequalities</h2>
			<p>The <strong class="source-inline">sympy</strong> module <a id="_idIndexMarker844"/>supports solving solutions for both linear and non-linear inequalities but on univariate equations only. The following is an API service that accepts a univariate string expression with its output or intercepts, and extracts the solution using the <strong class="source-inline">solve()</strong> method:</p>
			<pre class="source-code">
<strong class="bold">@router.get("/sym/inequality")</strong>
async def solve_univar_inequality(eqn:str, sol:int):
    x= symbols('x')
    <strong class="bold">expr1 = Ge(parse_expr(eqn, locals()), sol)</strong>
    <strong class="bold">sol = solve([expr1], [x])</strong>
    return str(sol)</pre>
			<p>The <strong class="source-inline">sympy</strong> module has <strong class="source-inline">Gt()</strong> or <strong class="source-inline">StrictGreaterThan</strong>, <strong class="source-inline">Lt()</strong> or <strong class="source-inline">StrictLessThan</strong>, <strong class="source-inline">Ge()</strong> or <strong class="source-inline">GreaterThan</strong>, and <strong class="source-inline">Le()</strong> or <strong class="source-inline">LessThan</strong> solvers, which we can use to create inequality. But first, we need to convert the <strong class="source-inline">str</strong> expression into a <strong class="source-inline">Symbols()</strong> object using the <strong class="source-inline">parser_expr()</strong> method before passing them to these solvers. The preceding service uses the <strong class="source-inline">GreaterThan</strong> solver, which creates an equation where the left-hand side of the expression is generally larger than the left. </p>
			<p>Most applications designed and developed for mathematical modeling and data science use <strong class="source-inline">sympy</strong> to create complex mathematical models symbolically, plot data directly from the <strong class="source-inline">sympy</strong> equation, or<a id="_idIndexMarker845"/> generate results based on datasets or live data. Now, let us proceed to the next group of API services, which deals with data analysis and manipulation using <strong class="source-inline">numpy</strong>, <strong class="source-inline">scipy</strong>, and <strong class="source-inline">pandas</strong>.</p>
			<h1 id="_idParaDest-299"><a id="_idTextAnchor303"/>Creating arrays and DataFrames</h1>
			<p>When numerical algorithms require some arrays to store data, a module <a id="_idIndexMarker846"/>called <strong class="bold">NumPy</strong>, short for <strong class="bold">Numerical Python</strong>, is a good resource for utility functions, objects, and classes that are used to create, transform, and manipulate arrays.</p>
			<p>The module is<a id="_idIndexMarker847"/> best known for its n-dimensional<em class="italic"> </em>arrays or ndarrays, which consume less memory storage than the typical <a id="_idIndexMarker848"/>Python lists. An <strong class="source-inline">ndarray</strong> incurs less overhead when performing data manipulation than executing the list operations in totality. Moreover, <strong class="source-inline">ndarray</strong> is strictly heterogeneous, unlike Python’s list collections.</p>
			<p>But before we start our NumPy-FastAPI service implementation, we need to install the <strong class="source-inline">numpy</strong> module using the <strong class="source-inline">pip</strong> command:</p>
			<p class="source-code">pip install numpy</p>
			<p>Our first API service will process some survey data and return it in <strong class="source-inline">ndarray</strong> form. The following <strong class="source-inline">get_respondent_answers()</strong> API retrieves a list of survey data from PostgreSQL <a id="_idIndexMarker849"/>through <a id="_idIndexMarker850"/>Piccolo and transforms the list of data into an <strong class="source-inline">ndarray</strong>:</p>
			<pre class="source-code">
from survey.repository.answers import AnswerRepository
from survey.repository.location import LocationRepository
<strong class="bold">import ujson</strong>
<strong class="bold">import numpy as np</strong>
<strong class="bold">@router.get("/answer/respondent")</strong>
async def get_respondent_answers(qid:int):
    <strong class="bold">repo_loc = LocationRepository()</strong>
    <strong class="bold">repo_answers = AnswerRepository()</strong>
    locations = await repo_loc.get_all_location()
    data = []
    for loc in locations:
        loc_q = await repo_answers
            .get_answers_per_q(loc["id"], qid)
        if not len(loc_q) == 0:
            loc_data = [ weights[qid-1]
              [str(item["answer_choice"])] 
                for item in loc_q]
            data.append(loc_data)
    <strong class="bold">arr = np.array(data)</strong>
    return <strong class="bold">ujson.loads(ujson.dumps(arr.tolist()))</strong> </pre>
			<p>Depending on the size of the data retrieved, it would be faster if we apply the <strong class="source-inline">ujson</strong> or <strong class="source-inline">orjson</strong> serializers and de-serializers to convert <strong class="source-inline">ndarray</strong> into JSON data. Even though <strong class="source-inline">numpy</strong> has<a id="_idIndexMarker851"/> data types such as <strong class="source-inline">uint</strong>, <strong class="source-inline">single</strong>, <strong class="source-inline">double</strong>, <strong class="source-inline">short</strong>, <strong class="source-inline">byte</strong>, and <strong class="source-inline">long</strong>, JSON <a id="_idIndexMarker852"/>serializers can still manage to convert them into their standard Python equivalents. Our given API sample prefers <strong class="source-inline">ujson</strong> utilities to convert the array into a JSON-able response.</p>
			<p>Aside from NumPy, <strong class="source-inline">pandas</strong> is another<a id="_idIndexMarker853"/> popular module that’s used in data analysis, manipulation, transformation, and retrieval. But to use pandas, we need to install NumPy, followed by the <strong class="source-inline">pandas</strong>, <strong class="source-inline">matplotlib</strong>, and <strong class="source-inline">openpxyl</strong> modules:</p>
			<p class="source-code">pip install pandas matplotlib openpxyl</p>
			<p>Let us now discuss about the ndarray in numpy module.</p>
			<h2 id="_idParaDest-300"><a id="_idTextAnchor304"/>Applying NumPy’s linear system operations</h2>
			<p>Data manipulation<a id="_idIndexMarker854"/> in an <strong class="source-inline">ndarray</strong> is easier and faster, unlike in a list collection, which requires list comprehension and loops. The vectors and matrices created by <strong class="source-inline">numpy</strong> have operations to manipulate their items, such as scalar multiplication, matrix multiplication, transposition, vectorization, and reshaping. The following API service shows how the product between a scalar gradient and an array of survey data is derived using the <strong class="source-inline">numpy</strong> module:</p>
			<pre class="source-code">
<strong class="bold">@router.get("/answer/increase/{gradient}")</strong>
async def answers_weight_multiply(<strong class="bold">gradient:int</strong>, qid:int):
    repo_loc = LocationRepository()
    repo_answers = AnswerRepository()
    locations = await repo_loc.get_all_location()
    data = []
    for loc in locations:
        loc_q = await repo_answers
            .get_answers_per_q(loc["id"], qid)
        if not len(loc_q) == 0:
            loc_data = [ weights[qid-1]
             [str(item["answer_choice"])] 
                 for item in loc_q]
            data.append(loc_data)
    arr = np.array(list(itertools.chain(*data)))
    <strong class="bold">arr = arr * gradient</strong>
    return ujson.loads(ujson.dumps(arr.tolist()))</pre>
			<p>As shown in the previous scripts, all <strong class="source-inline">ndarray</strong> instances resulting from any <strong class="source-inline">numpy</strong> operations can be serialized as JSON-able components using various JSON serializers. There are other linear algebraic <a id="_idIndexMarker855"/>operations that <strong class="source-inline">numpy</strong> can implement without sacrificing the performance of the microservice application. Let us take a look now on panda's DataFrame.</p>
			<h2 id="_idParaDest-301"><a id="_idTextAnchor305"/>Applying the pandas module</h2>
			<p>In this module, datasets are created as a <strong class="source-inline">DataFrame</strong> object, similar to in Julia and R. It contains rows and columns of data. FastAPI can render these DataFrames using any JSON serializers. The following API service retrieves all survey results from all survey locations and creates a DataFrame from these datasets:</p>
			<pre class="source-code">
import ujson
import numpy as np
import pandas as pd
<strong class="bold">@router.get("/answer/all")</strong>
async def get_all_answers():
    repo_loc = LocationRepository()
    repo_answers = AnswerRepository()
    locations = await repo_loc.get_all_location()
    temp = []
    data = []
    for loc in locations:
        for qid in range(1, 13):
            loc_q1 = await repo_answers
               .get_answers_per_q(loc["id"], qid)
            if not len(loc_q1) == 0:
                loc_data = [ weights[qid-1]
                   [str(item["answer_choice"])] 
                      for item in loc_q1]
                temp.append(loc_data)
        temp = list(itertools.chain(*temp))
        if not len(temp) == 0:
            data.append(temp)
        temp = list()
    <strong class="bold">arr = np.array(data)</strong>
    <strong class="bold">return ujson.loads(pd.DataFrame(arr)</strong>
           <strong class="bold">.to_json(orient='split'))</strong></pre>
			<p>The <strong class="source-inline">DataFrame</strong> object has a <strong class="source-inline">to_json()</strong> utility method, which returns a JSON object with an option to format the resulting JSON according to the desired type. On another note, <strong class="source-inline">pandas</strong> can also generate time series, a one-dimensional array depicting a column of a DataFrame. Both<a id="_idIndexMarker856"/> DataFrames and time series have built-in methods that are useful for adding, removing, updating, and saving the datasets to CSV and XLSX files. But before we discuss pandas’ data transformation processes, let us look at another module that works with <strong class="source-inline">numpy</strong> in many statistical computations, differentiation, integration, and linear optimizations: the <strong class="source-inline">scipy</strong> module.</p>
			<h1 id="_idParaDest-302"><a id="_idTextAnchor306"/>Performing statistical analysis</h1>
			<p>The <strong class="source-inline">scipy</strong> module<a id="_idIndexMarker857"/> uses <strong class="source-inline">numpy</strong> as its base module, which is why installing <strong class="source-inline">scipy</strong> requires <strong class="source-inline">numpy</strong> to be installed first. We can use the <strong class="source-inline">pip</strong> command to install the module:</p>
			<p class="source-code">pip install scipy</p>
			<p>Our application uses the module to derive the declarative statistics of the survey data. The following <strong class="source-inline">get_respondent_answers_stats()</strong> API service computes the mean, variance, skewness, and kurtosis of the dataset using the <strong class="source-inline">describe()</strong> method from <strong class="source-inline">scipy</strong>:</p>
			<pre class="source-code">
<strong class="bold">from scipy import stats</strong>
<strong class="bold">def ConvertPythonInt(o):</strong>
    if isinstance(o, np.int32): return int(o)  
    raise TypeError
<strong class="bold">@router.get("/answer/stats")</strong>
async def get_respondent_answers_stats(qid:int):
    repo_loc = LocationRepository()
    repo_answers = AnswerRepository()
    locations = await repo_loc.get_all_location()
    data = []
    for loc in locations:
        loc_q = await repo_answers
           .get_answers_per_q(loc["id"], qid)
             if not len(loc_q) == 0:
                 loc_data = [ weights[qid-1]
                   [str(item["answer_choice"])] 
                       for item in loc_q]
            data.append(loc_data)
    <strong class="bold">result = stats.describe(list(itertools.chain(*data)))</strong>
    return <strong class="bold">json.dumps(result._asdict(), </strong>
                 <strong class="bold"> default=ConvertPythonInt)</strong></pre>
			<p>The <strong class="source-inline">describe()</strong> method <a id="_idIndexMarker858"/>returns a <strong class="source-inline">DescribeResult</strong> object, which contains all the computed results. To render all the statistics as part of <strong class="source-inline">Response</strong>, we can invoke the <strong class="source-inline">as_dict()</strong> method of the <strong class="source-inline">DescribeResult</strong> object and serialize it using the JSON serializer.</p>
			<p>Our API sample also uses<a id="_idIndexMarker859"/> additional utilities such as the <strong class="source-inline">chain()</strong> method from <strong class="source-inline">itertools</strong> to flatten the list of data and a custom converter, <strong class="source-inline">ConvertPythonInt</strong>, to convert NumPy’s <strong class="source-inline">int32</strong> types into Python <strong class="source-inline">int</strong> types. Now, let us explore how to save data to CSV and XLSX files using the <strong class="source-inline">pandas</strong> module.</p>
			<h1 id="_idParaDest-303"><a id="_idTextAnchor307"/>Generating CSV and XLSX reports</h1>
			<p>The <strong class="source-inline">DataFrame</strong> object has built-in <strong class="source-inline">to_csv()</strong> and <strong class="source-inline">to_excel()</strong> methods that save its data in CSV or<a id="_idIndexMarker860"/> XLSX files, respectively. But the main goal is to create an API service<a id="_idIndexMarker861"/> that will return these files as responses. The following implementation shows how a FastAPI service can return a CSV file containing a list of respondent profiles:</p>
			<pre class="source-code">
<strong class="bold">from fastapi.responses import StreamingResponse</strong>
<strong class="bold">import pandas as pd</strong>
from io import StringIO
from survey.repository.respondent import 
        RespondentRepository
<strong class="bold">@router.get("/respondents/csv", response_description='csv')</strong>
async def create_respondent_report_csv():
    repo = RespondentRepository()
    <strong class="bold">result = await repo.get_all_respondent()</strong>
    
    ids = [ item["id"] for item in result ]
    fnames = [ f'{item["fname"]}' for item in result ]
    lnames = [ f'{item["lname"]}' for item in result ]
    ages = [ item["age"] for item in result ]
    genders = [ f'{item["gender"]}' for item in result ]
    maritals = [ f'{item["marital"]}' for item in result ]
   
    dict = {'Id': ids, 'First Name': fnames, 
            'Last Name': lnames, 'Age': ages, 
            'Gender': genders, 'Married?': maritals} 
  
    <strong class="bold">df = pd.DataFrame(dict)</strong>
    <strong class="bold">outFileAsStr = StringIO()</strong>
    <strong class="bold">df.to_csv(outFileAsStr, index = False)</strong>
    return <strong class="bold">StreamingResponse(</strong>
        <strong class="bold">iter([outFileAsStr.getvalue()]),</strong>
        <strong class="bold">media_type='text/csv',</strong>
        <strong class="bold">headers={</strong>
            <strong class="bold">'Content-Disposition': </strong>
              <strong class="bold">'attachment;filename=list_respondents.csv',</strong>
            <strong class="bold">'Access-Control-Expose-Headers': </strong>
               <strong class="bold">'Content-Disposition'</strong>
        }
    )</pre>
			<p>We need to create a <strong class="source-inline">dict()</strong> containing columns of data from the repository to create a <strong class="source-inline">DataFrame</strong> object. From the given script, we store each data column in a separate <strong class="source-inline">list()</strong>, add all the lists in <strong class="source-inline">dict()</strong> with keys as column header names, and pass <strong class="source-inline">dict()</strong> as a parameter to the constructor of <strong class="source-inline">DataFrame</strong>. </p>
			<p>After creating the <strong class="source-inline">DataFrame</strong> object, invoke the <strong class="source-inline">to_csv()</strong> method to convert its columnar dataset into a text stream, <strong class="source-inline">io.StringIO</strong>, which supports Unicode characters. Finally, we must render the <strong class="source-inline">StringIO</strong> object through FastAPI’s <strong class="source-inline">StreamResponse</strong> with the <strong class="source-inline">Content-Disposition</strong> header set to rename the default filename of the CSV object.</p>
			<p>Instead of using the<a id="_idIndexMarker862"/> pandas <strong class="source-inline">ExcelWriter</strong>, our Online Survey application opted for<a id="_idIndexMarker863"/> another way of saving <strong class="source-inline">DataFrame</strong> through the <strong class="source-inline">xlsxwriter</strong> module. This module has a <strong class="source-inline">Workbook</strong> class, which creates a workbook containing worksheets where we can plot all column data per row. The following API service uses this module to render XLSX content:</p>
			<pre class="source-code">
<strong class="bold">import xlsxwriter</strong>
<strong class="bold">from io import BytesIO</strong>
<strong class="bold">@router.get("/respondents/xlsx", </strong>
          <strong class="bold">response_description='xlsx')</strong>
async def create_respondent_report_xlsx():
    repo = RespondentRepository()
    <strong class="bold">result = await repo.get_all_respondent()</strong>
    <strong class="bold">output = BytesIO()</strong>
    <strong class="bold">workbook = xlsxwriter.Workbook(output)</strong>
    <strong class="bold">worksheet = workbook.add_worksheet()</strong>
    worksheet.write(0, 0, 'ID')
    worksheet.write(0, 1, 'First Name')
    worksheet.write(0, 2, 'Last Name')
    worksheet.write(0, 3, 'Age')
    worksheet.write(0, 4, 'Gender')
    worksheet.write(0, 5, 'Married?')
    row = 1
    for respondent in result:
        worksheet.write(row, 0, respondent["id"])
        … … … … … …
        worksheet.write(row, 5, respondent["marital"])
        row += 1
    workbook.close()
    output.seek(0)
    <strong class="bold">headers = {</strong>
        <strong class="bold">'Content-Disposition': 'attachment; </strong>
             <strong class="bold">filename="list_respondents.xlsx"'</strong>
    <strong class="bold">}</strong>
    <strong class="bold">return StreamingResponse(output, headers=headers)</strong></pre>
			<p>The given <strong class="source-inline">create_respondent_report_xlsx()</strong> service retrieves all the respondent records<a id="_idIndexMarker864"/> from the database and plots each profile record per row in the <a id="_idIndexMarker865"/>worksheet from the newly created <strong class="source-inline">Workbook</strong>. Instead of writing to a file, <strong class="source-inline">Workbook</strong> will store its content in a byte stream, <strong class="source-inline">io.ByteIO</strong>, which will be rendered by <strong class="source-inline">StreamResponse</strong>.</p>
			<p>The <strong class="source-inline">pandas</strong> module can also help FastAPI services read CSV and XLSX files for rendition or data analysis. It has a <strong class="source-inline">read_csv()</strong> that reads data from a CSV file and converts it into JSON content. The <strong class="source-inline">io.StringIO</strong> stream object will contain the full content, including its Unicode characters. The following service retrieves the content of a valid CSV file and returns JSON data:</p>
			<pre class="source-code">
<strong class="bold">@router.post("/upload/csv")</strong>
async def upload_csv(<strong class="bold">file: UploadFile = File(...)</strong>):
    <strong class="bold">df = pd.read_csv(StringIO(str(file.file.read(), </strong>
            <strong class="bold">'utf-8')), encoding='utf-16')</strong>
    return orjson.loads(df.to_json(orient='split'))</pre>
			<p>There are two ways to handle <strong class="source-inline">multipart</strong> file uploads in FastAPI:</p>
			<ul>
				<li>Use <strong class="source-inline">bytes</strong> to contain the file</li>
				<li>Use <strong class="source-inline">UploadFile</strong> to wrap the file object</li>
			</ul>
			<p><a href="B17975_09.xhtml#_idTextAnchor266"><em class="italic">Chapter 9</em></a><em class="italic">, Utilizing Other Advanced Features</em>, introduced the <strong class="source-inline">UploadFile</strong> class for capturing uploaded files because it supports more Pydantic features and has built-in operations that can work with coroutines. It can handle large file uploads without raising an change to - exception when the uploading process reaches the memory limit, unlike using the <strong class="source-inline">bytes</strong> type for file content storage. Thus, the given <strong class="source-inline">read-csv()</strong> service uses <strong class="source-inline">UploadFile</strong> to capture any <a id="_idIndexMarker866"/>CSV files for data analysis with <strong class="source-inline">orjson</strong> as its <a id="_idIndexMarker867"/>JSON serializer.</p>
			<p>Another way to handle file upload transactions is through Jinja2 form templates. We can use <strong class="source-inline">TemplateResponse</strong> to pursue file uploading and render the file content using the Jinja2 templating language. The following service reads a CSV file using <strong class="source-inline">read_csv()</strong> and serializes it into HTML table-formatted content:</p>
			<pre class="source-code">
<strong class="bold">@router.get("/upload/survey/form", </strong>
          <strong class="bold">response_class = HTMLResponse)</strong>
def upload_survey_form(request:Request):
    <strong class="bold">return templates.TemplateResponse("upload_survey.html",</strong>
             <strong class="bold">{"request": request})</strong>
<strong class="bold">@router.post("/upload/survey/form")</strong>
async def submit_survey_form(request: Request, 
              <strong class="bold">file: UploadFile = File(...)</strong>):
    <strong class="bold">df = pd.read_csv(StringIO(str(file.file.read(), </strong>
               <strong class="bold">'utf-8')), encoding='utf-8')</strong>
    <strong class="bold">return templates.TemplateResponse('render_survey.html', </strong>
         <strong class="bold">{'request': request, 'data': df.to_html()})</strong></pre>
			<p>Aside from <strong class="source-inline">to_json()</strong> and <strong class="source-inline">to_html()</strong>, the <strong class="source-inline">TextFileReader</strong> object also has other converters that can help FastAPI render various content types, including <strong class="source-inline">to_latex()</strong>, <strong class="source-inline">to_excel()</strong>, <strong class="source-inline">to_hdf()</strong>, <strong class="source-inline">to_dict()</strong>, <strong class="source-inline">to_pickle()</strong>, and <strong class="source-inline">to_xarray()</strong>. Moreover, the <strong class="source-inline">pandas</strong> module has a <strong class="source-inline">read_excel()</strong> that can read XLSX content and <a id="_idIndexMarker868"/>convert<a id="_idIndexMarker869"/> it into any rendition type, just like its <strong class="source-inline">read_csv()</strong> counterpart.</p>
			<p>Now, let us explore how FastAPI services can plot charts and graphs and output their graphical result through <strong class="source-inline">Response</strong>.</p>
			<h1 id="_idParaDest-304"><a id="_idTextAnchor308"/>Plotting data models</h1>
			<p>With the help <a id="_idIndexMarker870"/>of the <strong class="source-inline">numpy</strong> and <strong class="source-inline">pandas</strong> modules, FastAPI services can generate and render different types of graphs and charts using the <strong class="source-inline">matplotlib</strong> utilities. Like in the previous discussions, we will utilize an <strong class="source-inline">io.ByteIO</strong> stream and <strong class="source-inline">StreamResponse</strong> to generate graphical results for the API endpoints. The following API service retrieves survey data from the repository, computes the mean for each data strata, and returns a line graph of the data in PNG format:</p>
			<pre class="source-code">
<strong class="bold">from io import BytesIO</strong>
<strong class="bold">import matplotlib.pyplot as plt</strong>
from survey.repository.answers import AnswerRepository
from survey.repository.location import LocationRepository
<strong class="bold">@router.get("/answers/line")</strong>
async def plot_answers_mean():
    x = [1, 2, 3, 4, 5, 6, 7]
    repo_loc = LocationRepository()
    repo_answers = AnswerRepository()
    locations = await repo_loc.get_all_location()
    temp = []
    data = []
    for loc in locations:
        for qid in range(1, 13):
            loc_q1 = await repo_answers
               .get_answers_per_q(loc["id"], qid)
            if not len(loc_q1) == 0:
                loc_data = [ weights[qid-1]
                  [str(item["answer_choice"])] 
                     for item in loc_q1]
                temp.append(loc_data)
        temp = list(itertools.chain(*temp))
        if not len(temp) == 0:
            data.append(temp)
        temp = list()
    <strong class="bold">y = list(map(np.mean, data))</strong>
    <strong class="bold">filtered_image = BytesIO()</strong>
    <strong class="bold">plt.figure()</strong>
    
    plt.plot(x, y)
 
    plt.xlabel('Question Mean Score')
    plt.ylabel('State/Province')
    plt.title('Linear Plot of Poverty Status')
 
    <strong class="bold">plt.savefig(filtered_image, format='png')</strong>
    <strong class="bold">filtered_image.seek(0)</strong>
   
    <strong class="bold">return StreamingResponse(filtered_image, </strong>
                <strong class="bold">media_type="image/png")</strong></pre>
			<p>The <strong class="source-inline">plot_answers_mean()</strong> service utilizes the <strong class="source-inline">plot()</strong> method of the <strong class="source-inline">matplotlib</strong> module<a id="_idIndexMarker871"/> to plot the app’s mean survey results per location on a line<em class="italic"> </em>graph. Instead of saving the file to the filesystem, the service stores the image in the <strong class="source-inline">io.ByteIO</strong> stream using the module’s <strong class="source-inline">savefig()</strong> method. The stream is rendered using <strong class="source-inline">StreamResponse</strong>, like in the previous samples. The following figure shows the rendered stream image in PNG format through <strong class="source-inline">StreamResponse</strong>:</p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/Figure_10.03_B17975.jpg" alt="Figure 10.3 – Line graph from StreamResponse&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – Line graph from StreamResponse</p>
			<p>The other API <a id="_idIndexMarker872"/>services of our app, such as <strong class="source-inline">plot_sparse_data()</strong>, create a bar chart image in JPEG format of some simulated or derived data:</p>
			<pre class="source-code">
<strong class="bold">@router.get("/sparse/bar")</strong>
async def plot_sparse_data():
   <strong class="bold">df = pd.DataFrame(np.random.randint(10, size=(10, 4)),</strong>
      <strong class="bold">columns=["Area 1", "Area 2", "Area 3", "Area 4"])</strong>
   <strong class="bold">filtered_image = BytesIO()</strong>
   <strong class="bold">plt.figure()</strong>
   df.sum().plot(kind='barh', color=['red', 'green', 
          'blue', 'indigo', 'violet'])
   plt.title("Respondents in Survey Areas")
   plt.xlabel("Sample Size")
   plt.ylabel("State")
   <strong class="bold">plt.savefig(filtered_image, format='png')</strong>
   
   <strong class="bold">filtered_image.seek(0)</strong>
   <strong class="bold">return StreamingResponse(filtered_image, </strong>
           <strong class="bold">media_type="image/jpeg")</strong></pre>
			<p>The<a id="_idIndexMarker873"/> approach is the same as our line graph rendition. With the same strategy, the following service creates a pie chart that shows the percentage of male and female respondents that were surveyed:</p>
			<pre class="source-code">
<strong class="bold">@router.get("/respondents/gender")</strong>
async def plot_pie_gender():
    repo = RespondentRepository()
    count_male = await repo.list_gender('M')
    count_female = await repo.list_gender('F')
    gender = [len(count_male), len(count_female)]
    filtered_image = BytesIO()
    my_labels = 'Male','Female'
    plt.pie(gender,labels=my_labels,autopct='%1.1f%%')
    plt.title('Gender of Respondents')
    plt.axis('equal')
    plt.savefig(filtered_image, format='png')
    filtered_image.seek(0)
   
    return StreamingResponse(filtered_image, 
               media_type="image/png")</pre>
			<p>The <a id="_idIndexMarker874"/>responses generated by the <strong class="source-inline">plot_sparse_data()</strong> and <strong class="source-inline">plot_pie_gender()</strong> services are as follows:</p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/Figure_10.04_B17975.jpg" alt="Figure 10.4 – The bar and pie charts generated by StreamResponse&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – The bar and pie charts generated by StreamResponse</p>
			<p>This section will introduce an approach to creating API endpoints that produce graphical results using <strong class="source-inline">matplotlib</strong>. But there are other descriptive, complex, and stunning graphs and charts that you can create in less time using <strong class="source-inline">numpy</strong>, <strong class="source-inline">pandas</strong>, <strong class="source-inline">matplotlib</strong>, and the FastAPI framework. These extensions can even solve complex mathematical and <a id="_idIndexMarker875"/>data science-related problems, given the right hardware resources.</p>
			<p>Now, let us shift our focus to the other project, <strong class="source-inline">ch10-mongo</strong>, to tackle topics regarding workflows, GraphQL, and Neo4j graph database transactions and how FastAPI can utilize them.</p>
			<h1 id="_idParaDest-305"><a id="_idTextAnchor309"/>Simulating a BPMN workflow</h1>
			<p>Although the FastAPI <a id="_idIndexMarker876"/>framework has no built-in utilities to support its workflows, it is flexible and fluid enough to be integrated into other workflow tools such as Camunda and Apache Airflow through extension modules, middleware, and other customizations. But this section will only focus on the raw solution of simulating BPMN workflows using Celery, which can be extended to a more flexible, real-time, and <a id="_idIndexMarker877"/>enterprise-grade approach such as Airflow integration.</p>
			<h2 id="_idParaDest-306"><a id="_idTextAnchor310"/>Designing the BPMN workflow</h2>
			<p>The <strong class="source-inline">ch10-mongo</strong> project <a id="_idIndexMarker878"/>has implemented the following BPMN workflow design using Celery:</p>
			<ul>
				<li>A sequence of service tasks that derives the percentage of the survey data result, as shown in the following diagram:</li>
			</ul>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/Figure_10.05_B17975.jpg" alt="Figure 10.5 – Percentage computation workflow design&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – Percentage computation workflow design</p>
			<ul>
				<li>A group of batch operations that saves data to CSV and XLSX files, as shown in the following diagram: </li>
			</ul>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/Figure_10.06_B17975.jpg" alt="Figure 10.6 – Data archiving workflow design&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – Data archiving workflow design</p>
			<ul>
				<li>A group of chained <a id="_idIndexMarker879"/>tasks that operates on each location's data independently, as shown in the following diagram:</li>
			</ul>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/Figure_10.07_B17975.jpg" alt="Figure 10.7 – Workflow design for stratified survey data analysis&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – Workflow design for stratified survey data analysis</p>
			<p>There are many ways to<a id="_idIndexMarker880"/> implement the given design, but the most immediate solution is to utilize the Celery setup that we used in <a href="B17975_07.xhtml#_idTextAnchor190"><em class="italic">Chapter 7</em></a><em class="italic">, Securing the REST APIs</em>.</p>
			<h2 id="_idParaDest-307"><a id="_idTextAnchor311"/>Implementing the workflow</h2>
			<p>Celery’s <strong class="source-inline">chain()</strong> method<a id="_idIndexMarker881"/> implements a workflow of linked task executions, as depicted in <em class="italic">Figure 10.5</em>, where every parent task returns the result to the first parameter of next task. The chained workflow works if each task runs successfully without encountering any exceptions at runtime. The following is the API service in <strong class="source-inline">/api/survey_workflow.py</strong> that implements the chained workflow:</p>
			<pre class="source-code">
<strong class="bold">@router.post("/survey/compute/avg")</strong>
async def chained_workflow(surveydata: SurveyDataResult):
    survey_dict = surveydata.dict(exclude_unset=True)
    result = <strong class="bold">chain(compute_sum_results</strong>
        <strong class="bold">.s(survey_dict['results']).set(queue='default'), </strong>
            <strong class="bold">compute_avg_results.s(len(survey_dict))</strong>
             <strong class="bold">.set(queue='default'), derive_percentile.s()</strong>
             <strong class="bold">.set(queue='default')).apply_async()</strong>
    return {'message' : result.get(timeout = 10) }</pre>
			<p><strong class="source-inline">compute_sum_results()</strong>, <strong class="source-inline">compute_avg_results()</strong>, and <strong class="source-inline">derive_percentile()</strong> are bound tasks. Bound tasks are Celery tasks that are implemented to have the first method parameter allocated to the task instance itself, thus the <strong class="source-inline">self</strong> keyword appearing in its parameter list. Their task implementation always has the <strong class="source-inline">@celery.task(bind=True)</strong> decorator. The Celery task manager prefers bound tasks when applying workflow primitive signatures to create workflows. The following code shows<a id="_idIndexMarker882"/> the bound tasks that are used in the chained workflow design: </p>
			<pre class="source-code">
<strong class="bold">@celery.task(bind=True)</strong>
def compute_sum_results(self, results:Dict[str, int]):
    scores = []
    for key, val in results.items():
        scores.append(val)
    return sum(scores)</pre>
			<p><strong class="source-inline">compute_sum_results()</strong> computes the total survey result per state, while <strong class="source-inline">compute_avg_results()</strong>consumes the sum computed by <strong class="source-inline">compute_sum_results()</strong> to derive the mean value:</p>
			<pre class="source-code">
<strong class="bold">@celery.task(bind=True)</strong>
def compute_avg_results(self, value, len):
    return (value/len)</pre>
			<p>On the other hand, <strong class="source-inline">derive_percentile()</strong> consumes the mean values produced by <strong class="source-inline">compute_avg_results()</strong> to return a percentage value:</p>
			<pre class="source-code">
<strong class="bold">@celery.task(bind=True)</strong>
def derive_percentile(self, avg):
    percentage = f"{avg:.0%}"
    return percentage</pre>
			<p>The given <strong class="source-inline">derive_percentile()</strong> consumes the mean values produced by <strong class="source-inline">compute_avg_results()</strong> to return a percentage value.</p>
			<p>To implement the gateway approach, Celery has a <strong class="source-inline">group()</strong> primitive signature, which is used to implement parallel task executions, as depicted in <em class="italic">Figure 10.6</em>. The following API shows the implementation of the workflow structure with parallel executions: </p>
			<pre class="source-code">
<strong class="bold">@router.post("/survey/save")</strong>
async def grouped_workflow(surveydata: SurveyDataResult):
    survey_dict = surveydata.dict(exclude_unset=True)
    <strong class="bold">result = group([save_result_xlsx</strong>
       <strong class="bold">.s(survey_dict['results']).set(queue='default'), </strong>
         <strong class="bold">save_result_csv.s(len(survey_dict))</strong>
          <strong class="bold">.set(queue='default')]).apply_async()</strong>
    return {'message' : result.get(timeout = 10) } </pre>
			<p>The workflow shown<a id="_idIndexMarker883"/> in <em class="italic">Figure 10.7</em> depicts a mix of grouped and chained workflows. It is common for many real-world microservice applications to solve workflow-related problems with a mixture of different Celery signatures, including <strong class="source-inline">chord()</strong>, <strong class="source-inline">map()</strong>, and <strong class="source-inline">starmap()</strong>. The following script implements a workflow with mixed signatures: </p>
			<pre class="source-code">
<strong class="bold">@router.post("/process/surveys")</strong>
async def process_surveys(surveys: List[SurveyDataResult]):
    surveys_dict = [s.dict(exclude_unset=True) 
         for s in surveys]
    <strong class="bold">result = group([chain(compute_sum_results</strong>
       <strong class="bold">.s(survey['results']).set(queue='default'), </strong>
         <strong class="bold">compute_avg_results.s(len(survey['results']))</strong>
         <strong class="bold">.set(queue='default'), derive_percentile.s()</strong>
         <strong class="bold">.set(queue='default')) for survey in </strong>
                <strong class="bold">surveys_dict]).apply_async()</strong>
    return {'message': result.get(timeout = 10) }</pre>
			<p>The Celery signature plays an essential role in building workflows. A <strong class="source-inline">signature()</strong> method or <strong class="source-inline">s()</strong> that appears in the construct manages the execution of the task, which includes accepting the initial task parameter value(s) and utilizing the queues that the Celery worker uses to load tasks. As discussed in <a href="B17975_07.xhtml#_idTextAnchor190"><em class="italic">Chapter 7</em></a><em class="italic">, Securing the REST APIs</em>, <strong class="source-inline">apply_async()</strong> triggers the <a id="_idIndexMarker884"/>whole workflow execution and retrieves the result. </p>
			<p>Aside from workflows, the FastAPI framework can also use the GraphQL platform to build CRUD transactions, especially when dealing with a large amount of data in a microservice architecture.</p>
			<h1 id="_idParaDest-308"><a id="_idTextAnchor312"/>Using GraphQL queries and mutations</h1>
			<p>GraphQL<a id="_idIndexMarker885"/> is an API standard that implements REST and CRUD transactions at the same time. It is a high-performing platform that’s used in building REST API<a id="_idIndexMarker886"/> endpoints that only <a id="_idIndexMarker887"/>need a few steps to set up. Its objective is to create endpoints for data manipulation and query transactions.</p>
			<h2 id="_idParaDest-309"><a id="_idTextAnchor313"/>Setting up the GraphQL platform</h2>
			<p>Python extensions<a id="_idIndexMarker888"/> such as Strawberry, Ariadne, Tartiflette, and Graphene support GraphQL-FastAPI integration. This chapter introduces the use of the new Ariadne 3.x to build CRUD transactions for this <strong class="source-inline">ch10-mongo</strong> project with MongoDB as the repository.</p>
			<p>First, we need to install the latest <strong class="source-inline">graphene</strong> extension using the <strong class="source-inline">pip</strong> command:</p>
			<p class="source-code">pip install graphene</p>
			<p>Among the GraphQL libraries, Graphene<a id="_idIndexMarker889"/> is the easiest to set up, with fewer decorators and methods to override. It easily integrates with the FastAPI framework without requiring additional middleware and too much auto-wiring. </p>
			<h2 id="_idParaDest-310"><a id="_idTextAnchor314"/>Creating the record insertion, update, and deletion</h2>
			<p>Data manipulation operations are always part of GraphQL’s mutation mechanism. This is a GraphQL feature that modifies the server-side state of the application and returns arbitrary data as a sign of a successful change in the state. The following is an implementation of a GraphQL<a id="_idIndexMarker890"/> mutation that inserts, deletes, and updates records:</p>
			<pre class="source-code">
<strong class="bold">from models.data.pccs_graphql import LoginData</strong>
<strong class="bold">from graphene import String, Int, Mutation, Field</strong>
from repository.login import LoginRepository
<strong class="bold">class CreateLoginData(Mutation):</strong>
    <strong class="bold">class Arguments:</strong>
      id = Int(required=True)
      username = String(required=True)
      password = String(required=True)
    <strong class="bold">ok = Boolean()</strong>
    <strong class="bold">loginData = Field(lambda: LoginData)</strong>
    async def mutate(root, info, <strong class="bold">id, username, password</strong>):
        login_dict = {"id": id, "username": username, 
                   "password": password}
        login_json = dumps(login_dict, default=json_serial)
        repo = LoginRepository()
        result = await repo.add_login(loads(login_json))
        if not result == None:
          ok = True
        else: 
          ok = False
        <strong class="bold">return CreateLoginData(loginData=result, ok=ok)</strong></pre>
			<p><strong class="source-inline">CreateLoginData</strong> is a mutation that adds a new login record to the data store. The inner class, <strong class="source-inline">Arguments</strong>, indicates the record fields that will comprise the new login record for insertion. These arguments must appear in the overridden <strong class="source-inline">mutate()</strong> method to capture the values of these fields. This method will also call the ORM, which will persist the newly created record.</p>
			<p>After a successful insert transaction, <strong class="source-inline">mutate()</strong> must return the class variables defined inside a mutation class such as <strong class="source-inline">ok</strong> and the <strong class="source-inline">loginData</strong> object. These returned values must be part of the mutation instance. </p>
			<p>Updating a login attribute <a id="_idIndexMarker891"/>has a similar implementation to <strong class="source-inline">CreateLoginData</strong> except the arguments need to be exposed. The following is a mutation class that updates the <strong class="source-inline">password</strong> field of a login record that’s been retrieved using its <strong class="source-inline">username</strong>:</p>
			<pre class="source-code">
<strong class="bold">class ChangeLoginPassword(Mutation):</strong>
    <strong class="bold">class Arguments:</strong>
      username = String(required=True)
      password = String(required=True)
    <strong class="bold">ok = Boolean()</strong>
    <strong class="bold">loginData = Field(lambda: LoginData)</strong>
    async def mutate(root, info, <strong class="bold">username, password</strong>):       
        repo = LoginRepository()
        result = await repo.change_password(username, 
                  password)
        
        if not result == None:
          ok = True
        else: 
          ok = False
        <strong class="bold">return CreateLoginData(loginData=result, ok=ok)</strong></pre>
			<p>Similarly, the <a id="_idIndexMarker892"/>delete mutation class retrieves a record through an <strong class="source-inline">id</strong> and deletes it from the data store:</p>
			<pre class="source-code">
<strong class="bold">class DeleteLoginData(Mutation):</strong>
    <strong class="bold">class Arguments:</strong>
      id = Int(required=True)
      
    <strong class="bold">ok = Boolean()</strong>
    <strong class="bold">loginData = Field(lambda: LoginData)</strong>
    async def mutate(root, info, id):       
        repo = LoginRepository()
        result = await repo.delete_login(id)
        if not result == None:
          ok = True
        else: 
          ok = False
        <strong class="bold">return DeleteLoginData(loginData=result, ok=ok)</strong></pre>
			<p>Now, we can store all our mutation classes in an <strong class="source-inline">ObjectType</strong> class that exposes these transactions to the client. We assign field names to each <strong class="source-inline">Field</strong> instance of the given mutation classes. These field names will serve as the query names of the transactions. The following code shows the <strong class="source-inline">ObjectType</strong> class, which defines our <strong class="source-inline">CreateLoginData</strong>, <strong class="source-inline">ChangeLoginPassword</strong>, and <strong class="source-inline">DeleteLoginData</strong> mutations:</p>
			<pre class="source-code">
<strong class="bold">class LoginMutations(ObjectType):</strong>
    create_login = CreateLoginData.Field()
    edit_login = ChangeLoginPassword.Field()
    delete_login = DeleteLoginData.Field()</pre>
			<h2 id="_idParaDest-311"><a id="_idTextAnchor315"/>Implementing the query transactions</h2>
			<p>GraphQL query<a id="_idIndexMarker893"/> transactions are implementations of the <strong class="source-inline">ObjectType</strong> base class. Here, <strong class="source-inline">LoginQuery</strong> retrieves all login records from the data store:</p>
			<pre class="source-code">
<strong class="bold">class LoginQuery(ObjectType):</strong>
    <strong class="bold">login_list = None</strong>
    <strong class="bold">get_login = Field(List(LoginData))</strong>
  
    async def resolve_<strong class="bold">get_login</strong>(self, info):
      repo = LoginRepository()
      <strong class="bold">login_list = await repo.get_all_login()</strong>
      return <strong class="bold">login_list</strong></pre>
			<p>The class must have a query field name, such as <strong class="source-inline">get_login</strong>, that will serve as its query name during query execution. The field name must be part of the <strong class="source-inline">resolve_*()</strong> method name for it to be registered under the <strong class="source-inline">ObjectType</strong> class. A class variable, such as <strong class="source-inline">login_list</strong>, must be declared for it to contain all the retrieved records.</p>
			<h2 id="_idParaDest-312"><a id="_idTextAnchor316"/>Running the CRUD transactions</h2>
			<p>We need a <a id="_idIndexMarker894"/>GraphQL schema to integrate the GraphQL components and register the mutation and query classes for the FastAPI framework before running the GraphQL transactions. The following script shows the instantiation of GraphQL’s <strong class="source-inline">Schema</strong> class with <strong class="source-inline">LoginQuery</strong> and <strong class="source-inline">LoginMutations</strong>:</p>
			<pre class="source-code">
<strong class="bold">from graphene import Schema </strong>
schema = Schema(query=<strong class="bold">LoginQuery</strong>, mutation=<strong class="bold">LoginMutations</strong>,
    <strong class="bold">auto_camelcase=False</strong>)</pre>
			<p>We set the <strong class="source-inline">auto_camelcase</strong> property of the <strong class="source-inline">Schema</strong> instance to <strong class="source-inline">False</strong> to maintain the use of the original field names with an underscore and avoid the camel case notation approach.</p>
			<p>Afterward, we<a id="_idIndexMarker895"/> use the schema instance to create the <strong class="source-inline">GraphQLApp()</strong> instance. GraphQLApp is equivalent to an application that needs mounting to the FastAPI framework. We can use the <strong class="source-inline">mount()</strong> utility of FastAPI to integrate the <strong class="source-inline">GraphQLApp()</strong> instance with its URL pattern and the chosen GraphQL browser tool to run the API transactions. The following code shows how to integrate the GraphQL applications with Playground as the browser tool to run the APIs:</p>
			<pre class="source-code">
<strong class="bold">from starlette_graphene3 import GraphQLApp,     </strong>
          <strong class="bold">make_playground_handler</strong>
app = FastAPI()
app.mount("/ch10/graphql/login", 
       GraphQLApp(survey_graphene_login.schema, 
          on_get=make_playground_handler()) )
app.mount("/ch10/graphql/profile", 
       GraphQLApp(survey_graphene_profile.schema, 
          on_get=make_playground_handler()) )</pre>
			<p>We can use the left-hand side panel to insert a new record through a JSON script containing the field name of the <strong class="source-inline">CreateLoginData</strong> mutation, which is <strong class="source-inline">create_login</strong>, along with passing the necessary record data, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/Figure_10.08_B17975.jpg" alt="Figure 10.8 – Running the create_login mutation&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.8 – Running the create_login mutation</p>
			<p>To perform query transactions, we must create a JSON script with a field name of <strong class="source-inline">LoginQuery</strong>, which is <strong class="source-inline">get_login</strong>, together with the record fields needed to be retrieved. The following screenshot shows how to run the <strong class="source-inline">LoginQuery</strong> transaction:</p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/Figure_10.09_B17975.jpg" alt="Figure 10.9 – Running the get_login query transaction&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.9 – Running the get_login query transaction</p>
			<p>GraphQL<a id="_idIndexMarker896"/> can help consolidate all the CRUD transactions from different microservices with easy setup and configuration. It can serve as an API Gateway where all GraphQLApps from multiple microservices are mounted to create a single façade application. Now, let us integrate FastAPI into a graph database. </p>
			<h1 id="_idParaDest-313"><a id="_idTextAnchor317"/>Utilizing the Neo4j graph database</h1>
			<p>For an application that<a id="_idIndexMarker897"/> requires storage that emphasizes relationships among data records, a graph database is an appropriate storage method to use. One of the platforms that use graph databases is Neo4j. FastAPI can easily integrate with Neo4j, but we need to install the <strong class="source-inline">Neo4j</strong> module using the <strong class="source-inline">pip</strong> command:</p>
			<p class="source-code">pip install neo4j</p>
			<p>Neo4j is a NoSQL<a id="_idIndexMarker898"/> database with a flexible and powerful data model that can manage and connect different enterprise-related data based on related attributes. It has a semi-structured database architecture with simple ACID properties and a non-JOIN policy that make its operations fast and easy to execute.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">ACID, which stands for atomicity, consistency, isolation, and durability, describes a database transaction as a group of operations that performs as a single unit with correctness and consistency.</p>
			<h2 id="_idParaDest-314"><a id="_idTextAnchor318"/>Setting the Neo4j database</h2>
			<p>The <strong class="source-inline">neo4j</strong> module<a id="_idIndexMarker899"/> includes <strong class="source-inline">neo4j-driver</strong>, which is needed to establish a connection with the graph database. It needs a URI that contains the <strong class="source-inline">bolt</strong> protocol, server address, and port. The default database port to use is <strong class="source-inline">7687</strong>. The following script shows how to create Neo4j database connectivity:</p>
			<pre class="source-code">
<strong class="bold">from neo4j import GraphDatabase</strong>
<strong class="bold">uri = "bolt://127.0.0.1:7687"</strong>
driver = GraphDatabase.driver(uri, auth=("neo4j", 
      "admin2255"))</pre>
			<h2 id="_idParaDest-315"><a id="_idTextAnchor319"/>Creating the CRUD transactions</h2>
			<p>Neo4j has a <a id="_idIndexMarker900"/>declarative graph query language called Cypher that allows CRUD transactions of the graph database. These Cypher scripts need to be encoded as <strong class="source-inline">str</strong> SQL commands to be executed by its query runner. The following API service adds a new database record to the graph database:</p>
			<pre class="source-code">
<strong class="bold">@router.post("/neo4j/location/add")</strong>
def create_survey_loc(node_name: str, 
        node_req_atts: LocationReq):
    node_attributes_dict = 
          node_req_atts.dict(exclude_unset=True)
    <strong class="bold">node_attributes = '{' + ', '.join(f'{key}:\'{value}\''</strong>
        <strong class="bold">for (key, value) in node_attributes_dict.items()) </strong>
              <strong class="bold">+ '}'</strong>
    <strong class="bold">query = f"CREATE ({node_name}:Location  </strong>
         <strong class="bold">{node_attributes})"</strong>
    try:
        <strong class="bold">with driver.session() as session:</strong>
            <strong class="bold">session.run(query=query)</strong>
        return JSONResponse(content={"message":
         "add node location successful"}, status_code=201)
    except Exception as e:
        print(e)
        return JSONResponse(content={"message": "add node 
            location unsuccessful"}, status_code=500)</pre>
			<p><strong class="source-inline">create_survey_loc()</strong> adds new survey location details to the Neo4j database. A record is considered <a id="_idIndexMarker901"/>a node in the graph database with a name and attributes equivalent to the record fields in the relational databases. We use the connection object to create a session that has a <strong class="source-inline">run()</strong> method to execute Cypher scripts.</p>
			<p>The command to add a new node is <strong class="source-inline">CREATE</strong>, while the syntax to update, delete, and retrieve nodes can be added with the <strong class="source-inline">MATCH</strong> command. The following <strong class="source-inline">update_node_loc()</strong> service searches for a particular node based on the node’s name and performs the <strong class="source-inline">SET</strong> command to update the given fields:</p>
			<pre class="source-code">
<strong class="bold">@router.patch("/neo4j/update/location/{id}")</strong>
async def update_node_loc(id:int, 
           node_req_atts: LocationReq):
    node_attributes_dict = 
         node_req_atts.dict(exclude_unset=True)
    node_attributes = '{' + ', '.join(f'{key}:\'{value}\'' 
       for (key, value) in 
            node_attributes_dict.items()) + '}'
    <strong class="bold">query = f"""</strong>
        <strong class="bold">MATCH (location:Location)</strong>
        <strong class="bold">WHERE ID(location) = {id}</strong>
        <strong class="bold">SET location += {node_attributes}"""</strong>
    try:
        <strong class="bold">with driver.session() as session:</strong>
            <strong class="bold">session.run(query=query)</strong>
        return JSONResponse(content={"message": 
          "update location successful"}, status_code=201)
    except Exception as e:
        print(e)
        return JSONResponse(content={"message": "update 
           location  unsuccessful"}, status_code=500)</pre>
			<p>Likewise, the<a id="_idIndexMarker902"/> delete transaction uses the <strong class="source-inline">MATCH</strong> command to search for the node to be deleted. The following service implements <strong class="source-inline">Location</strong> node deletion:</p>
			<pre class="source-code">
<strong class="bold">@router.delete("/neo4j/delete/location/{node}")</strong>
def delete_location_node(node:str):
    node_attributes = '{' + f"name:'{node}'" + '}'
    <strong class="bold">query = f"""</strong>
        <strong class="bold">MATCH (n:Location {node_attributes})</strong>
        <strong class="bold">DETACH DELETE n</strong>
    <strong class="bold">"""</strong>
    try:
        <strong class="bold">with driver.session() as session:</strong>
            <strong class="bold">session.run(query=query)</strong>
        return JSONResponse(content={"message": 
          "delete location node successful"}, 
             status_code=201)
    except:
        return JSONResponse(content={"message": 
           "delete location node unsuccessful"}, 
               status_code=500)</pre>
			<p>When retrieving <a id="_idIndexMarker903"/>nodes, the following service retrieves all the nodes from the database:</p>
			<pre class="source-code">
<strong class="bold">@router.get("/neo4j/nodes/all")</strong>
async def list_all_nodes():
    <strong class="bold">query = f"""</strong>
        <strong class="bold">MATCH (node)</strong>
        <strong class="bold">RETURN node"""</strong>
    try:
        <strong class="bold">with driver.session() as session:</strong>
            <strong class="bold">result = session.run(query=query)</strong>
            <strong class="bold">nodes = result.data()</strong>
        <strong class="bold">return nodes</strong>
    except Exception as e:
        return JSONResponse(content={"message": "listing
            all nodes unsuccessful"}, status_code=500)</pre>
			<p>The following service only retrieves a single node based on the node’s <strong class="source-inline">id</strong>:</p>
			<pre class="source-code">
<strong class="bold">@router.get("/neo4j/location/{id}")</strong>
async def get_location(id:int):
    <strong class="bold">query = f"""</strong>
        <strong class="bold">MATCH (node:Location)</strong>
        <strong class="bold">WHERE ID(node) = {id}</strong>
        <strong class="bold">RETURN node"""</strong>
    try:
        <strong class="bold">with driver.session() as session:</strong>
            <strong class="bold">result = session.run(query=query)</strong>
            <strong class="bold">nodes = result.data()</strong>
        return nodes
    except Exception as e:
        return JSONResponse(content={"message": "get 
          location node unsuccessful"}, status_code=500)</pre>
			<p>Our implementation <a id="_idIndexMarker904"/>will not be complete if we have no API endpoint that will link nodes based on attributes. Nodes are linked to each other based on relationship names and attributes that are updatable and removable. The following API endpoint creates a node relationship between the <strong class="source-inline">Location</strong> nodes and <strong class="source-inline">Respondent</strong> nodes:</p>
			<pre class="source-code">
<strong class="bold">@router.post("/neo4j/link/respondent/loc")</strong>
def link_respondent_loc(respondent_node: str, 
    loc_node: str, node_req_atts:LinkRespondentLoc):
    node_attributes_dict = 
         node_req_atts.dict(exclude_unset=True)
   
    <strong class="bold">node_attributes = '{' + ', '.join(f'{key}:\'{value}\'' </strong>
       <strong class="bold">for (key, value) in </strong>
          <strong class="bold">node_attributes_dict.items()) + '}'</strong>
  
    <strong class="bold">query = f"""</strong>
        <strong class="bold">MATCH (respondent:Respondent), (loc:Location)</strong>
        <strong class="bold">WHERE respondent.name = '{respondent_node}' AND </strong>
            <strong class="bold">loc.name = '{loc_node}'</strong>
        <strong class="bold">CREATE (respondent) -[relationship:LIVES_IN </strong>
              <strong class="bold">{node_attributes}]-&gt;(loc)"""</strong>
    try:
        <strong class="bold">with driver.session() as session:</strong>
            <strong class="bold">session.run(query=query)</strong>
        return JSONResponse(content={"message": "add … 
            relationship successful"}, status_code=201)
    except:
        return JSONResponse(content={"message": "add 
          respondent-loc relationship unsuccessful"}, 
                 status_code=500)</pre>
			<p>The FastAPI <a id="_idIndexMarker905"/>framework can easily integrate into any database platform. The previous chapters have proven that FastAPI can deal with relational database transactions with ORM and document-based NoSQL transactions with ODM, while this chapter has proven the same for the Neo4j graph database due to its easy configurations.</p>
			<h1 id="_idParaDest-316"><a id="_idTextAnchor320"/>Summary</h1>
			<p>This chapter introduced the scientific side of FastAPI by showing that API services can provide numerical computation, symbolic formulation, and graphical interpretation of data via the <strong class="source-inline">numpy</strong>, <strong class="source-inline">pandas</strong>, <strong class="source-inline">sympy</strong>, and <strong class="source-inline">matplotlib</strong> modules. This chapter also helped us understand how far we can integrate FastAPI with new technology and design strategies to provide new ideas for the microservice architecture, such as using GraphQL to manage CRUD transactions and Neo4j for real-time and node-based data management. We also introduced the basic approach that FastAPI can apply to solve various BPMN workflows using Celery tasks. With this, we have started to understand the power and flexibility of the framework in building microservice applications. </p>
			<p>The next chapter will cover the last set of topics to complete our deep dive into FastAPI. We will cover some deployment strategies, Django and Flask integrations, and other microservice design patterns that haven’t been discussed in the previous chapters.</p>
		</div>
		<div>
			<div id="_idContainer085">
			</div>
		</div>
	</body></html>