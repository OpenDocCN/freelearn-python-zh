<html><head></head><body>
		<div><h1 id="_idParaDest-288" class="chapter-number"><a id="_idTextAnchor292"/>10</h1>
			<h1 id="_idParaDest-289"><a id="_idTextAnchor293"/>Solving Numerical, Symbolic, and Graphical Problems</h1>
			<p>Microservice architecture is not only used to build fine-grained, optimized, and scalable applications in the banking, insurance, production, human resources, and manufacturing industries. It is also used to develop scientific and computation-related research and scientific software prototypes for<a id="_idIndexMarker826"/> applications such as <strong class="bold">laboratory information management systems</strong> (<strong class="bold">LIMSs</strong>), weather<a id="_idIndexMarker827"/> forecasting systems, <strong class="bold">geographical information systems</strong> (<strong class="bold">GISs</strong>), and healthcare systems.</p>
			<p>FastAPI is one of the best choices in building these granular services since they usually involve highly computational tasks, workflows, and reports. This chapter will highlight some transactions not yet covered in the previous chapters, such as symbolic computations using <code>sympy</code>, solving linear systems using <code>numpy</code>, plotting mathematical models using <code>matplotlib</code>, and generating data archives using <code>pandas</code>. This chapter will also show you how FastAPI is flexible when solving workflow-related transactions by simulating some Business Process Modeling Notation (BPMN) tasks. For developing big data applications, a portion of this chapter will showcase GraphQL queries for big data applications and Neo4j graph databases for graph-related projects with the framework.</p>
			<p>The main objective of this chapter is to introduce the FastAPI framework as a tool for providing microservice solutions for scientific research and computational sciences. </p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Setting up the projects</li>
				<li>Implementing the symbolic computations</li>
				<li>Creating arrays and DataFrames</li>
				<li>Performing statistical analysis</li>
				<li>Generating CSV and XLSX reports</li>
				<li>Plotting data models</li>
				<li>Simulating a BPMN workflow</li>
				<li>Using GraphQL queries and mutations</li>
				<li>Utilizing the Neo4j graph database</li>
			</ul>
			<h1 id="_idParaDest-290"><a id="_idTextAnchor294"/>Technical requirements</h1>
			<p>This chapter provides the base skeleton of a <code>ch10</code> project. </p>
			<h1 id="_idParaDest-291"><a id="_idTextAnchor295"/>Setting up the projects</h1>
			<p>The <a id="_idIndexMarker828"/>PCCS project has two versions: <code>ch10-relational</code>, which uses a PostgreSQL database with Piccolo ORM as the data mapper, and <code>ch10-mongo</code>, which saves data as MongoDB documents using Beanie ODM.</p>
			<h2 id="_idParaDest-292"><a id="_idTextAnchor296"/>Using the Piccolo ORM </h2>
			<p><code>ch10-relational</code> uses a fast <a id="_idIndexMarker829"/>Piccolo ORM that can support both sync and async CRUD transactions. This ORM was not introduced in <a href="B17975_05.xhtml#_idTextAnchor107"><em class="italic">Chapter 5</em></a><em class="italic">, Connecting to a Relational Database</em>, because it is more appropriate for computational, data science-related, and big data applications. The Piccolo ORM is different from other ORMs because it scaffolds a project containing the initial project structure and templates for customization. But before creating the project, we need to install the <code>piccolo</code> module using <code>pip</code>:</p>
			<pre>pip install piccolo</pre>
			<p>Afterward, install the <code>piccolo-admin</code> module, which provides helper classes for the GUI administrator page of its projects:</p>
			<pre>pip install piccolo-admin</pre>
			<p>Now, we can create a project inside a newly created root project folder by running <code>piccolo asgi new</code>, a CLI command that scaffolds the Piccolo project directory. The process will ask for the API framework and application server to utilize, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_10.01_B17975.jpg" alt="Figure 10.1 – Scaffolding a Piccolo ORM project&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Scaffolding a Piccolo ORM project</p>
			<p>You must <a id="_idIndexMarker830"/>use FastAPI for the application framework and <code>uvicorn</code> is the recommended ASGI server. Now, we can add Piccolo applications inside the project by running the <code>piccolo app new</code> command inside the project folder. The following screenshot shows the main project directory, where we execute the CLI command to create a Piccolo application:</p>
			<div><div><img src="img/Figure_10.02_B17975.jpg" alt="Figure 10.2 – Piccolo project directory&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Piccolo project directory</p>
			<p>The scaffolded <a id="_idIndexMarker831"/>project always has a default application called <code>home</code>, but it can be modified or even deleted. Once removed, the Piccolo platform allows you to replace <code>home</code> by adding a new application to the project by running the <code>piccolo app new</code> command inside the project folder, as shown in the preceding screenshot. A Piccolo application contains the ORM models, BaseModel, services, repository classes, and API methods. Each application has an auto-generated <code>piccolo_app.py</code> module where we need to configure an <code>APP_CONFIG</code> variable to register all the ORM details. The following is the configuration of our project’s survey application:</p>
			<pre class="source-code">
<strong class="bold">APP_CONFIG = AppConfig(</strong>
    app_name="survey",
    migrations_folder_path=os.path.join(
        CURRENT_DIRECTORY, "piccolo_migrations"
    ),
    <strong class="bold">table_classes=[Answers, Education, Question, Choices, </strong>
       <strong class="bold">Profile, Login, Location, Occupation, Respondent],</strong>
    migration_dependencies=[],
    commands=[],
<strong class="bold">)</strong></pre>
			<p>For the ORM platform to recognize the new Piccolo application, its <code>piccolo_app.py</code> must be added to <code>APP_REGISTRY</code> of the main project’s <code>piccolo_conf.py</code> module. The following is the content of the <code>piccolo_conf.py</code> file of our <code>ch10-piccolo</code> project:</p>
			<pre class="source-code">
<strong class="bold">from piccolo.engine.postgres import PostgresEngine</strong>
<strong class="bold">from piccolo.conf.apps import AppRegistry</strong>
<strong class="bold">DB = PostgresEngine(</strong>
    <strong class="bold">config={</strong>
        <strong class="bold">"database": "pccs",</strong>
        <strong class="bold">"user": "postgres",</strong>
        <strong class="bold">"password": "admin2255",</strong>
        <strong class="bold">"host": "localhost",</strong>
        <strong class="bold">"port": 5433,</strong>
    <strong class="bold">}</strong>
<strong class="bold">)</strong>
<strong class="bold">APP_REGISTRY = AppRegistry(</strong>
    <strong class="bold">apps</strong>=["<strong class="bold">survey.piccolo_app</strong>", 
          "piccolo_admin.piccolo_app"]
)</pre>
			<p>The <code>piccolo_conf.py</code> file is <a id="_idIndexMarker832"/>also the module where we establish the PostgreSQL database connection. Aside from PostgreSQL, the Piccolo ORM also supports SQLite databases.</p>
			<h3>Creating the data models</h3>
			<p>Like in Django ORM, Piccolo ORM<a id="_idIndexMarker833"/> has migration commands to generate the database tables based on model classes. But first, we need to create model classes by utilizing its <code>Table</code> API class. It also has helper classes to establish column mappings and foreign key relationships. The following are some data model classes that comprise our database <code>pccs</code>:</p>
			<pre class="source-code">
<strong class="bold">from piccolo.columns import ForeignKey, Integer, Varchar,</strong>
       <strong class="bold">Text, Date, Boolean, Float</strong>
<strong class="bold">from piccolo.table import Table</strong>
class Login(<strong class="bold">Table</strong>):
    <strong class="bold">username = Varchar(unique=True)</strong>
    <strong class="bold">password = Varchar()</strong>
class Education(<strong class="bold">Table</strong>):
    <strong class="bold">name = Varchar()</strong>
class Profile(<strong class="bold">Table</strong>):
    <strong class="bold">fname = Varchar()</strong>
    <strong class="bold">lname = Varchar()</strong>
    <strong class="bold">age = Integer()</strong>
    <strong class="bold">position = Varchar()</strong>
    <strong class="bold">login_id = ForeignKey(Login, unique=True)</strong>
    <strong class="bold">official_id = Integer()</strong>
    <strong class="bold">date_employed = Date()</strong></pre>
			<p>After creating<a id="_idIndexMarker834"/> the model classes, we can update the database by creating the migrations files. Migration is a way of updating the database of a project. In the Piccolo platform, we can run the <code>piccolo migrations new &lt;app_name&gt;</code> command to generate files in the <code>piccolo_migrations</code> folder. These are called migration files and they contain migration scripts. But to save time, we will include the <code>--auto</code> option for the command to let the ORM check the recently executed migration files and auto-generate the migration script containing the newly reflected schema updates. Check the newly created migration file first before running the <code>piccolo migrations forward &lt;app_name&gt;</code> command to execute the migration script. This last command will auto-create all the tables in the database based on the model classes.</p>
			<h3>Implementing the repository layer</h3>
			<p>Creating the <a id="_idIndexMarker835"/>repository layer comes after performing all the necessary migrations. Piccolo’s CRUD operations are like those in the Peewee ORM. It is swift, short, and easy to implement. The following code shows an implementation of the <code>insert_respondent()</code> transaction, which adds a new respondent profile:</p>
			<pre class="source-code">
<strong class="bold">from survey.tables import Respondent</strong>
from typing import Dict, List, Any
<strong class="bold">class RespondentRepository</strong>:
    <strong class="bold">async def insert_respondent(self, </strong>
             <strong class="bold">details:Dict[str, Any]) -&gt; bool: </strong>
        try:
            <strong class="bold">respondent = Respondent(**details)</strong>
            <strong class="bold">await respondent.save()</strong>
        except Exception as e: 
            return False 
        return True</pre>
			<p>Like Peewee, Piccolo’s model classes can persist records, as shown by <code>insert_respondent()</code>, which implements an asynchronous <code>INSERT</code> transaction. On the other hand, <code>get_all_respondent()</code> retrieves all respondent profiles and has the same approach as Peewee, as shown here:  </p>
			<pre class="source-code">
    <strong class="bold">async def get_all_respondent(self):</strong>
        return <strong class="bold">await Respondent.select()</strong>
                  <strong class="bold">.order_by(Respondent.id)</strong></pre>
			<p>The remaining Peewee-like <code>DELETE</code> and <code>UPDATE</code> respondent transactions are created in the project’s <code>/survey/repository/respondent.py</code> module. </p>
			<h2 id="_idParaDest-293"><a id="_idTextAnchor297"/>The Beanie ODM</h2>
			<p>The <a id="_idIndexMarker836"/>second version of the PCCS project, <code>ch10-mongo</code>, utilizes a MongoDB datastore and uses the Beanie ODM to implement its asynchronous CRUD transactions. We covered Beanie in <a href="B17975_06.xhtml#_idTextAnchor155"><em class="italic">Chapter 6</em></a><em class="italic">, Using a Non-Relational Database</em>. Now, let us learn how to apply FastAPI in symbolic computations. We will be using the <code>ch10-piccolo</code> project for this.</p>
			<h1 id="_idParaDest-294"><a id="_idTextAnchor298"/>Implementing symbolic computations</h1>
			<p><code>sympy</code> module using the <code>pip</code> command:</p>
			<pre>pip install sympy</pre>
			<p>Let us now start creating our first symbolic expressions.</p>
			<h2 id="_idParaDest-295"><a id="_idTextAnchor299"/>Creating symbolic expressions</h2>
			<p>One way of<a id="_idIndexMarker838"/> implementing the FastAPI endpoint that performs symbolic computation is to create a service that accepts a mathematical model or equation as a string and converts that string into a <code>sympy</code> symbolic expression. The following <code>substitute_eqn()</code> processes an equation in <code>str</code> format and converts it into valid linear or nonlinear bivariate equations with the <code>x</code> and <code>y</code> variables. It also accepts values for <code>x</code> and <code>y</code> to derive the solution of the expression:</p>
			<pre class="source-code">
<strong class="bold">from sympy import symbols, sympify</strong>
<strong class="bold">@router.post("/sym/equation")</strong>
async def substitute_bivar_eqn(<strong class="bold">eqn: str, xval:int, </strong>
               <strong class="bold">yval:int</strong>):
    try:
        <strong class="bold">x, y = symbols('x, y')</strong>
        <strong class="bold">expr = sympify(eqn)</strong>
        return <strong class="bold">str(expr.subs({x: xval, y: yval}))</strong>
    except:
        return JSONResponse(content={"message": 
            "invalid equations"}, status_code=500)</pre>
			<p>Before converting a string equation into a <code>sympy</code> expression, we need to define the <code>x</code> and <code>y</code> variables as <code>Symbols</code> objects using the <code>symbols()</code> utility. This method accepts a string of comma-delimited variable names and returns a tuple of symbols equivalent to the variables. After creating all the needed <code>Symbols()</code> objects, we can convert our equation into <code>sympy</code> expressions by using any of the following <code>sympy</code> methods:</p>
			<ul>
				<li><code>sympify()</code>: This uses <code>eval()</code> to convert the string equation into a valid <code>sympy</code> expression with all Python types converted into their <code>sympy</code> equivalents</li>
				<li><code>parse_expr()</code>: A full-fledged expression parser that transforms and modifies the tokens of the expression and converts them into their <code>sympy</code> equivalents</li>
			</ul>
			<p>Since the <code>substitute_bivar_eqn()</code> service utilizes the <code>sympify()</code> method, the string expression needs to be sanitized from unwanted code before sympifying to avoid any compromise. </p>
			<p>On the other <a id="_idIndexMarker839"/>hand, the <code>sympy</code> expression object has a <code>subs()</code> method to substitute values to derive the solution. Its resulting object must be converted into <code>str</code> format for <code>Response</code> to render the data. Otherwise, <code>Response</code> will raise <code>ValueError</code>, regarding the result as non-iterable. </p>
			<h2 id="_idParaDest-296"><a id="_idTextAnchor300"/>Solving linear expressions</h2>
			<p>The <code>sympy</code> module allows you to <a id="_idIndexMarker840"/>implement services that solve multivariate systems of linear equations. The following API service highlights an implementation that accepts two bivariate linear models in string format with their respective solutions:</p>
			<pre class="source-code">
<strong class="bold">from sympy import Eq, symbols, Poly, solve, sympify</strong>
<strong class="bold">@router.get("/sym/linear")</strong>
<strong class="bold">async def solve_linear_bivar_eqns(eqn1:str, </strong>
            <strong class="bold">sol1: int, eqn2:str, sol2: int):</strong>
    x, y = symbols('x, y')
    
    expr1 = <strong class="bold">parse_expr(eqn1, locals())</strong>
    expr2 = <strong class="bold">parse_expr(eqn2, locals())</strong>
    
    <strong class="bold">if Poly(expr1, x).is_linear and </strong>
                 <strong class="bold">Poly(expr1, x).is_linear:</strong>
        <strong class="bold">eq1 = Eq(expr1, sol1)</strong>
        <strong class="bold">eq2 = Eq(expr2, sol2)</strong>
        <strong class="bold">sol = solve([eq1, eq2], [x, y])</strong>
        return <strong class="bold">str(sol)</strong>
    else:
        return <strong class="bold">None</strong></pre>
			<p>The <code>solve_linear_bivar_eqns()</code> service accepts two bivariate linear equations and their respective outputs (or intercepts) and aims to establish a system of linear equations. First, it registers the <code>x</code> and <code>y</code> variables as <code>sympy</code> objects and then uses the <code>parser_expr()</code> method to transform the string expressions into their <code>sympy</code> equivalents. Afterward, the <a id="_idIndexMarker841"/>service needs to establish linear equality of these equations using the <code>Eq()</code> solver, which maps each <code>sympy</code> expression to its solution. Then, the API service passes all these linear equations to the <code>solve()</code> method to derive the <code>x</code> and <code>y</code> values. The result of <code>solve()</code> also needs to be rendered as a string, like in the substitution.</p>
			<p>Aside from the <code>solve()</code> method, the <a id="_idIndexMarker842"/>API also uses the <code>Poly()</code> utility to create a polynomial object from an expression to be able to access essential properties of an equation, such as <code>is_linear()</code>.</p>
			<h2 id="_idParaDest-297"><a id="_idTextAnchor301"/>Solving non-linear expressions</h2>
			<p>The <a id="_idIndexMarker843"/>previous <code>solve_linear_bivar_eqns()</code> can be reused to solve non-linear systems. The tweak is to shift the validation from filtering the linear equations to any non-linear equations. The following script highlights this code change:</p>
			<pre class="source-code">
<strong class="bold">@router.get("/sym/nonlinear")</strong>
async def solve_nonlinear_bivar_eqns(eqn1:str, sol1: int, 
           eqn2:str, sol2: int):
    … … … … … …
    … … … … … …    
    <strong class="bold">if not Poly(expr1, x, y).is_linear or </strong>
              <strong class="bold">not Poly(expr1, x, y).is_linear:</strong>
    … … … … … …
    … … … … … …
        return str(sol)
    else:
        return None</pre>
			<h2 id="_idParaDest-298"><a id="_idTextAnchor302"/>Solving linear and non-linear inequalities</h2>
			<p>The <code>sympy</code> module <a id="_idIndexMarker844"/>supports solving solutions for both linear and non-linear inequalities but on univariate equations only. The following is an API service that accepts a univariate string expression with its output or intercepts, and extracts the solution using the <code>solve()</code> method:</p>
			<pre class="source-code">
<strong class="bold">@router.get("/sym/inequality")</strong>
async def solve_univar_inequality(eqn:str, sol:int):
    x= symbols('x')
    <strong class="bold">expr1 = Ge(parse_expr(eqn, locals()), sol)</strong>
    <strong class="bold">sol = solve([expr1], [x])</strong>
    return str(sol)</pre>
			<p>The <code>sympy</code> module has <code>Gt()</code> or <code>StrictGreaterThan</code>, <code>Lt()</code> or <code>StrictLessThan</code>, <code>Ge()</code> or <code>GreaterThan</code>, and <code>Le()</code> or <code>LessThan</code> solvers, which we can use to create inequality. But first, we need to convert the <code>str</code> expression into a <code>Symbols()</code> object using the <code>parser_expr()</code> method before passing them to these solvers. The preceding service uses the <code>GreaterThan</code> solver, which creates an equation where the left-hand side of the expression is generally larger than the left. </p>
			<p>Most applications designed and developed for mathematical modeling and data science use <code>sympy</code> to create complex mathematical models symbolically, plot data directly from the <code>sympy</code> equation, or<a id="_idIndexMarker845"/> generate results based on datasets or live data. Now, let us proceed to the next group of API services, which deals with data analysis and manipulation using <code>numpy</code>, <code>scipy</code>, and <code>pandas</code>.</p>
			<h1 id="_idParaDest-299"><a id="_idTextAnchor303"/>Creating arrays and DataFrames</h1>
			<p>When numerical algorithms require some arrays to store data, a module <a id="_idIndexMarker846"/>called <strong class="bold">NumPy</strong>, short for <strong class="bold">Numerical Python</strong>, is a good resource for utility functions, objects, and classes that are used to create, transform, and manipulate arrays.</p>
			<p>The module is<a id="_idIndexMarker847"/> best known for its n-dimensional<em class="italic"> </em>arrays or ndarrays, which consume less memory storage than the typical <a id="_idIndexMarker848"/>Python lists. An <code>ndarray</code> incurs less overhead when performing data manipulation than executing the list operations in totality. Moreover, <code>ndarray</code> is strictly heterogeneous, unlike Python’s list collections.</p>
			<p>But before we start our NumPy-FastAPI service implementation, we need to install the <code>numpy</code> module using the <code>pip</code> command:</p>
			<pre>pip install numpy</pre>
			<p>Our first API service will process some survey data and return it in <code>ndarray</code> form. The following <code>get_respondent_answers()</code> API retrieves a list of survey data from PostgreSQL <a id="_idIndexMarker849"/>through <a id="_idIndexMarker850"/>Piccolo and transforms the list of data into an <code>ndarray</code>:</p>
			<pre class="source-code">
from survey.repository.answers import AnswerRepository
from survey.repository.location import LocationRepository
<strong class="bold">import ujson</strong>
<strong class="bold">import numpy as np</strong>
<strong class="bold">@router.get("/answer/respondent")</strong>
async def get_respondent_answers(qid:int):
    <strong class="bold">repo_loc = LocationRepository()</strong>
    <strong class="bold">repo_answers = AnswerRepository()</strong>
    locations = await repo_loc.get_all_location()
    data = []
    for loc in locations:
        loc_q = await repo_answers
            .get_answers_per_q(loc["id"], qid)
        if not len(loc_q) == 0:
            loc_data = [ weights[qid-1]
              [str(item["answer_choice"])] 
                for item in loc_q]
            data.append(loc_data)
    <strong class="bold">arr = np.array(data)</strong>
    return <strong class="bold">ujson.loads(ujson.dumps(arr.tolist()))</strong> </pre>
			<p>Depending on the size of the data retrieved, it would be faster if we apply the <code>ujson</code> or <code>orjson</code> serializers and de-serializers to convert <code>ndarray</code> into JSON data. Even though <code>numpy</code> has<a id="_idIndexMarker851"/> data types such as <code>uint</code>, <code>single</code>, <code>double</code>, <code>short</code>, <code>byte</code>, and <code>long</code>, JSON <a id="_idIndexMarker852"/>serializers can still manage to convert them into their standard Python equivalents. Our given API sample prefers <code>ujson</code> utilities to convert the array into a JSON-able response.</p>
			<p>Aside from NumPy, <code>pandas</code> is another<a id="_idIndexMarker853"/> popular module that’s used in data analysis, manipulation, transformation, and retrieval. But to use pandas, we need to install NumPy, followed by the <code>pandas</code>, <code>matplotlib</code>, and <code>openpxyl</code> modules:</p>
			<pre>pip install pandas matplotlib openpxyl</pre>
			<p>Let us now discuss about the ndarray in numpy module.</p>
			<h2 id="_idParaDest-300"><a id="_idTextAnchor304"/>Applying NumPy’s linear system operations</h2>
			<p>Data manipulation<a id="_idIndexMarker854"/> in an <code>ndarray</code> is easier and faster, unlike in a list collection, which requires list comprehension and loops. The vectors and matrices created by <code>numpy</code> have operations to manipulate their items, such as scalar multiplication, matrix multiplication, transposition, vectorization, and reshaping. The following API service shows how the product between a scalar gradient and an array of survey data is derived using the <code>numpy</code> module:</p>
			<pre class="source-code">
<strong class="bold">@router.get("/answer/increase/{gradient}")</strong>
async def answers_weight_multiply(<strong class="bold">gradient:int</strong>, qid:int):
    repo_loc = LocationRepository()
    repo_answers = AnswerRepository()
    locations = await repo_loc.get_all_location()
    data = []
    for loc in locations:
        loc_q = await repo_answers
            .get_answers_per_q(loc["id"], qid)
        if not len(loc_q) == 0:
            loc_data = [ weights[qid-1]
             [str(item["answer_choice"])] 
                 for item in loc_q]
            data.append(loc_data)
    arr = np.array(list(itertools.chain(*data)))
    <strong class="bold">arr = arr * gradient</strong>
    return ujson.loads(ujson.dumps(arr.tolist()))</pre>
			<p>As shown in the previous scripts, all <code>ndarray</code> instances resulting from any <code>numpy</code> operations can be serialized as JSON-able components using various JSON serializers. There are other linear algebraic <a id="_idIndexMarker855"/>operations that <code>numpy</code> can implement without sacrificing the performance of the microservice application. Let us take a look now on panda's DataFrame.</p>
			<h2 id="_idParaDest-301"><a id="_idTextAnchor305"/>Applying the pandas module</h2>
			<p>In this module, datasets are created as a <code>DataFrame</code> object, similar to in Julia and R. It contains rows and columns of data. FastAPI can render these DataFrames using any JSON serializers. The following API service retrieves all survey results from all survey locations and creates a DataFrame from these datasets:</p>
			<pre class="source-code">
import ujson
import numpy as np
import pandas as pd
<strong class="bold">@router.get("/answer/all")</strong>
async def get_all_answers():
    repo_loc = LocationRepository()
    repo_answers = AnswerRepository()
    locations = await repo_loc.get_all_location()
    temp = []
    data = []
    for loc in locations:
        for qid in range(1, 13):
            loc_q1 = await repo_answers
               .get_answers_per_q(loc["id"], qid)
            if not len(loc_q1) == 0:
                loc_data = [ weights[qid-1]
                   [str(item["answer_choice"])] 
                      for item in loc_q1]
                temp.append(loc_data)
        temp = list(itertools.chain(*temp))
        if not len(temp) == 0:
            data.append(temp)
        temp = list()
    <strong class="bold">arr = np.array(data)</strong>
    <strong class="bold">return ujson.loads(pd.DataFrame(arr)</strong>
           <strong class="bold">.to_json(orient='split'))</strong></pre>
			<p>The <code>DataFrame</code> object has a <code>to_json()</code> utility method, which returns a JSON object with an option to format the resulting JSON according to the desired type. On another note, <code>pandas</code> can also generate time series, a one-dimensional array depicting a column of a DataFrame. Both<a id="_idIndexMarker856"/> DataFrames and time series have built-in methods that are useful for adding, removing, updating, and saving the datasets to CSV and XLSX files. But before we discuss pandas’ data transformation processes, let us look at another module that works with <code>numpy</code> in many statistical computations, differentiation, integration, and linear optimizations: the <code>scipy</code> module.</p>
			<h1 id="_idParaDest-302"><a id="_idTextAnchor306"/>Performing statistical analysis</h1>
			<p>The <code>scipy</code> module<a id="_idIndexMarker857"/> uses <code>numpy</code> as its base module, which is why installing <code>scipy</code> requires <code>numpy</code> to be installed first. We can use the <code>pip</code> command to install the module:</p>
			<pre>pip install scipy</pre>
			<p>Our application uses the module to derive the declarative statistics of the survey data. The following <code>get_respondent_answers_stats()</code> API service computes the mean, variance, skewness, and kurtosis of the dataset using the <code>describe()</code> method from <code>scipy</code>:</p>
			<pre class="source-code">
<strong class="bold">from scipy import stats</strong>
<strong class="bold">def ConvertPythonInt(o):</strong>
    if isinstance(o, np.int32): return int(o)  
    raise TypeError
<strong class="bold">@router.get("/answer/stats")</strong>
async def get_respondent_answers_stats(qid:int):
    repo_loc = LocationRepository()
    repo_answers = AnswerRepository()
    locations = await repo_loc.get_all_location()
    data = []
    for loc in locations:
        loc_q = await repo_answers
           .get_answers_per_q(loc["id"], qid)
             if not len(loc_q) == 0:
                 loc_data = [ weights[qid-1]
                   [str(item["answer_choice"])] 
                       for item in loc_q]
            data.append(loc_data)
    <strong class="bold">result = stats.describe(list(itertools.chain(*data)))</strong>
    return <strong class="bold">json.dumps(result._asdict(), </strong>
                 <strong class="bold"> default=ConvertPythonInt)</strong></pre>
			<p>The <code>describe()</code> method <a id="_idIndexMarker858"/>returns a <code>DescribeResult</code> object, which contains all the computed results. To render all the statistics as part of <code>Response</code>, we can invoke the <code>as_dict()</code> method of the <code>DescribeResult</code> object and serialize it using the JSON serializer.</p>
			<p>Our API sample also uses<a id="_idIndexMarker859"/> additional utilities such as the <code>chain()</code> method from <code>itertools</code> to flatten the list of data and a custom converter, <code>ConvertPythonInt</code>, to convert NumPy’s <code>int32</code> types into Python <code>int</code> types. Now, let us explore how to save data to CSV and XLSX files using the <code>pandas</code> module.</p>
			<h1 id="_idParaDest-303"><a id="_idTextAnchor307"/>Generating CSV and XLSX reports</h1>
			<p>The <code>DataFrame</code> object has built-in <code>to_csv()</code> and <code>to_excel()</code> methods that save its data in CSV or<a id="_idIndexMarker860"/> XLSX files, respectively. But the main goal is to create an API service<a id="_idIndexMarker861"/> that will return these files as responses. The following implementation shows how a FastAPI service can return a CSV file containing a list of respondent profiles:</p>
			<pre class="source-code">
<strong class="bold">from fastapi.responses import StreamingResponse</strong>
<strong class="bold">import pandas as pd</strong>
from io import StringIO
from survey.repository.respondent import 
        RespondentRepository
<strong class="bold">@router.get("/respondents/csv", response_description='csv')</strong>
async def create_respondent_report_csv():
    repo = RespondentRepository()
    <strong class="bold">result = await repo.get_all_respondent()</strong>
    
    ids = [ item["id"] for item in result ]
    fnames = [ f'{item["fname"]}' for item in result ]
    lnames = [ f'{item["lname"]}' for item in result ]
    ages = [ item["age"] for item in result ]
    genders = [ f'{item["gender"]}' for item in result ]
    maritals = [ f'{item["marital"]}' for item in result ]
   
    dict = {'Id': ids, 'First Name': fnames, 
            'Last Name': lnames, 'Age': ages, 
            'Gender': genders, 'Married?': maritals} 
  
    <strong class="bold">df = pd.DataFrame(dict)</strong>
    <strong class="bold">outFileAsStr = StringIO()</strong>
    <strong class="bold">df.to_csv(outFileAsStr, index = False)</strong>
    return <strong class="bold">StreamingResponse(</strong>
        <strong class="bold">iter([outFileAsStr.getvalue()]),</strong>
        <strong class="bold">media_type='text/csv',</strong>
        <strong class="bold">headers={</strong>
            <strong class="bold">'Content-Disposition': </strong>
              <strong class="bold">'attachment;filename=list_respondents.csv',</strong>
            <strong class="bold">'Access-Control-Expose-Headers': </strong>
               <strong class="bold">'Content-Disposition'</strong>
        }
    )</pre>
			<p>We need to create a <code>dict()</code> containing columns of data from the repository to create a <code>DataFrame</code> object. From the given script, we store each data column in a separate <code>list()</code>, add all the lists in <code>dict()</code> with keys as column header names, and pass <code>dict()</code> as a parameter to the constructor of <code>DataFrame</code>. </p>
			<p>After creating the <code>DataFrame</code> object, invoke the <code>to_csv()</code> method to convert its columnar dataset into a text stream, <code>io.StringIO</code>, which supports Unicode characters. Finally, we must render the <code>StringIO</code> object through FastAPI’s <code>StreamResponse</code> with the <code>Content-Disposition</code> header set to rename the default filename of the CSV object.</p>
			<p>Instead of using the<a id="_idIndexMarker862"/> pandas <code>ExcelWriter</code>, our Online Survey application opted for<a id="_idIndexMarker863"/> another way of saving <code>DataFrame</code> through the <code>xlsxwriter</code> module. This module has a <code>Workbook</code> class, which creates a workbook containing worksheets where we can plot all column data per row. The following API service uses this module to render XLSX content:</p>
			<pre class="source-code">
<strong class="bold">import xlsxwriter</strong>
<strong class="bold">from io import BytesIO</strong>
<strong class="bold">@router.get("/respondents/xlsx", </strong>
          <strong class="bold">response_description='xlsx')</strong>
async def create_respondent_report_xlsx():
    repo = RespondentRepository()
    <strong class="bold">result = await repo.get_all_respondent()</strong>
    <strong class="bold">output = BytesIO()</strong>
    <strong class="bold">workbook = xlsxwriter.Workbook(output)</strong>
    <strong class="bold">worksheet = workbook.add_worksheet()</strong>
    worksheet.write(0, 0, 'ID')
    worksheet.write(0, 1, 'First Name')
    worksheet.write(0, 2, 'Last Name')
    worksheet.write(0, 3, 'Age')
    worksheet.write(0, 4, 'Gender')
    worksheet.write(0, 5, 'Married?')
    row = 1
    for respondent in result:
        worksheet.write(row, 0, respondent["id"])
        … … … … … …
        worksheet.write(row, 5, respondent["marital"])
        row += 1
    workbook.close()
    output.seek(0)
    <strong class="bold">headers = {</strong>
        <strong class="bold">'Content-Disposition': 'attachment; </strong>
             <strong class="bold">filename="list_respondents.xlsx"'</strong>
    <strong class="bold">}</strong>
    <strong class="bold">return StreamingResponse(output, headers=headers)</strong></pre>
			<p>The given <code>create_respondent_report_xlsx()</code> service retrieves all the respondent records<a id="_idIndexMarker864"/> from the database and plots each profile record per row in the <a id="_idIndexMarker865"/>worksheet from the newly created <code>Workbook</code>. Instead of writing to a file, <code>Workbook</code> will store its content in a byte stream, <code>io.ByteIO</code>, which will be rendered by <code>StreamResponse</code>.</p>
			<p>The <code>pandas</code> module can also help FastAPI services read CSV and XLSX files for rendition or data analysis. It has a <code>read_csv()</code> that reads data from a CSV file and converts it into JSON content. The <code>io.StringIO</code> stream object will contain the full content, including its Unicode characters. The following service retrieves the content of a valid CSV file and returns JSON data:</p>
			<pre class="source-code">
<strong class="bold">@router.post("/upload/csv")</strong>
async def upload_csv(<strong class="bold">file: UploadFile = File(...)</strong>):
    <strong class="bold">df = pd.read_csv(StringIO(str(file.file.read(), </strong>
            <strong class="bold">'utf-8')), encoding='utf-16')</strong>
    return orjson.loads(df.to_json(orient='split'))</pre>
			<p>There are two ways to handle <code>multipart</code> file uploads in FastAPI:</p>
			<ul>
				<li>Use <code>bytes</code> to contain the file</li>
				<li>Use <code>UploadFile</code> to wrap the file object</li>
			</ul>
			<p><a href="B17975_09.xhtml#_idTextAnchor266"><em class="italic">Chapter 9</em></a><em class="italic">, Utilizing Other Advanced Features</em>, introduced the <code>UploadFile</code> class for capturing uploaded files because it supports more Pydantic features and has built-in operations that can work with coroutines. It can handle large file uploads without raising an change to - exception when the uploading process reaches the memory limit, unlike using the <code>bytes</code> type for file content storage. Thus, the given <code>read-csv()</code> service uses <code>UploadFile</code> to capture any <a id="_idIndexMarker866"/>CSV files for data analysis with <code>orjson</code> as its <a id="_idIndexMarker867"/>JSON serializer.</p>
			<p>Another way to handle file upload transactions is through Jinja2 form templates. We can use <code>TemplateResponse</code> to pursue file uploading and render the file content using the Jinja2 templating language. The following service reads a CSV file using <code>read_csv()</code> and serializes it into HTML table-formatted content:</p>
			<pre class="source-code">
<strong class="bold">@router.get("/upload/survey/form", </strong>
          <strong class="bold">response_class = HTMLResponse)</strong>
def upload_survey_form(request:Request):
    <strong class="bold">return templates.TemplateResponse("upload_survey.html",</strong>
             <strong class="bold">{"request": request})</strong>
<strong class="bold">@router.post("/upload/survey/form")</strong>
async def submit_survey_form(request: Request, 
              <strong class="bold">file: UploadFile = File(...)</strong>):
    <strong class="bold">df = pd.read_csv(StringIO(str(file.file.read(), </strong>
               <strong class="bold">'utf-8')), encoding='utf-8')</strong>
    <strong class="bold">return templates.TemplateResponse('render_survey.html', </strong>
         <strong class="bold">{'request': request, 'data': df.to_html()})</strong></pre>
			<p>Aside from <code>to_json()</code> and <code>to_html()</code>, the <code>TextFileReader</code> object also has other converters that can help FastAPI render various content types, including <code>to_latex()</code>, <code>to_excel()</code>, <code>to_hdf()</code>, <code>to_dict()</code>, <code>to_pickle()</code>, and <code>to_xarray()</code>. Moreover, the <code>pandas</code> module has a <code>read_excel()</code> that can read XLSX content and <a id="_idIndexMarker868"/>convert<a id="_idIndexMarker869"/> it into any rendition type, just like its <code>read_csv()</code> counterpart.</p>
			<p>Now, let us explore how FastAPI services can plot charts and graphs and output their graphical result through <code>Response</code>.</p>
			<h1 id="_idParaDest-304"><a id="_idTextAnchor308"/>Plotting data models</h1>
			<p>With the help <a id="_idIndexMarker870"/>of the <code>numpy</code> and <code>pandas</code> modules, FastAPI services can generate and render different types of graphs and charts using the <code>matplotlib</code> utilities. Like in the previous discussions, we will utilize an <code>io.ByteIO</code> stream and <code>StreamResponse</code> to generate graphical results for the API endpoints. The following API service retrieves survey data from the repository, computes the mean for each data strata, and returns a line graph of the data in PNG format:</p>
			<pre class="source-code">
<strong class="bold">from io import BytesIO</strong>
<strong class="bold">import matplotlib.pyplot as plt</strong>
from survey.repository.answers import AnswerRepository
from survey.repository.location import LocationRepository
<strong class="bold">@router.get("/answers/line")</strong>
async def plot_answers_mean():
    x = [1, 2, 3, 4, 5, 6, 7]
    repo_loc = LocationRepository()
    repo_answers = AnswerRepository()
    locations = await repo_loc.get_all_location()
    temp = []
    data = []
    for loc in locations:
        for qid in range(1, 13):
            loc_q1 = await repo_answers
               .get_answers_per_q(loc["id"], qid)
            if not len(loc_q1) == 0:
                loc_data = [ weights[qid-1]
                  [str(item["answer_choice"])] 
                     for item in loc_q1]
                temp.append(loc_data)
        temp = list(itertools.chain(*temp))
        if not len(temp) == 0:
            data.append(temp)
        temp = list()
    <strong class="bold">y = list(map(np.mean, data))</strong>
    <strong class="bold">filtered_image = BytesIO()</strong>
    <strong class="bold">plt.figure()</strong>
    
    plt.plot(x, y)
 
    plt.xlabel('Question Mean Score')
    plt.ylabel('State/Province')
    plt.title('Linear Plot of Poverty Status')
 
    <strong class="bold">plt.savefig(filtered_image, format='png')</strong>
    <strong class="bold">filtered_image.seek(0)</strong>
   
    <strong class="bold">return StreamingResponse(filtered_image, </strong>
                <strong class="bold">media_type="image/png")</strong></pre>
			<p>The <code>plot_answers_mean()</code> service utilizes the <code>plot()</code> method of the <code>matplotlib</code> module<a id="_idIndexMarker871"/> to plot the app’s mean survey results per location on a line<em class="italic"> </em>graph. Instead of saving the file to the filesystem, the service stores the image in the <code>io.ByteIO</code> stream using the module’s <code>savefig()</code> method. The stream is rendered using <code>StreamResponse</code>, like in the previous samples. The following figure shows the rendered stream image in PNG format through <code>StreamResponse</code>:</p>
			<div><div><img src="img/Figure_10.03_B17975.jpg" alt="Figure 10.3 – Line graph from StreamResponse&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – Line graph from StreamResponse</p>
			<p>The other API <a id="_idIndexMarker872"/>services of our app, such as <code>plot_sparse_data()</code>, create a bar chart image in JPEG format of some simulated or derived data:</p>
			<pre class="source-code">
<strong class="bold">@router.get("/sparse/bar")</strong>
async def plot_sparse_data():
   <strong class="bold">df = pd.DataFrame(np.random.randint(10, size=(10, 4)),</strong>
      <strong class="bold">columns=["Area 1", "Area 2", "Area 3", "Area 4"])</strong>
   <strong class="bold">filtered_image = BytesIO()</strong>
   <strong class="bold">plt.figure()</strong>
   df.sum().plot(kind='barh', color=['red', 'green', 
          'blue', 'indigo', 'violet'])
   plt.title("Respondents in Survey Areas")
   plt.xlabel("Sample Size")
   plt.ylabel("State")
   <strong class="bold">plt.savefig(filtered_image, format='png')</strong>
   
   <strong class="bold">filtered_image.seek(0)</strong>
   <strong class="bold">return StreamingResponse(filtered_image, </strong>
           <strong class="bold">media_type="image/jpeg")</strong></pre>
			<p>The<a id="_idIndexMarker873"/> approach is the same as our line graph rendition. With the same strategy, the following service creates a pie chart that shows the percentage of male and female respondents that were surveyed:</p>
			<pre class="source-code">
<strong class="bold">@router.get("/respondents/gender")</strong>
async def plot_pie_gender():
    repo = RespondentRepository()
    count_male = await repo.list_gender('M')
    count_female = await repo.list_gender('F')
    gender = [len(count_male), len(count_female)]
    filtered_image = BytesIO()
    my_labels = 'Male','Female'
    plt.pie(gender,labels=my_labels,autopct='%1.1f%%')
    plt.title('Gender of Respondents')
    plt.axis('equal')
    plt.savefig(filtered_image, format='png')
    filtered_image.seek(0)
   
    return StreamingResponse(filtered_image, 
               media_type="image/png")</pre>
			<p>The <a id="_idIndexMarker874"/>responses generated by the <code>plot_sparse_data()</code> and <code>plot_pie_gender()</code> services are as follows:</p>
			<div><div><img src="img/Figure_10.04_B17975.jpg" alt="Figure 10.4 – The bar and pie charts generated by StreamResponse&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – The bar and pie charts generated by StreamResponse</p>
			<p>This section will introduce an approach to creating API endpoints that produce graphical results using <code>matplotlib</code>. But there are other descriptive, complex, and stunning graphs and charts that you can create in less time using <code>numpy</code>, <code>pandas</code>, <code>matplotlib</code>, and the FastAPI framework. These extensions can even solve complex mathematical and <a id="_idIndexMarker875"/>data science-related problems, given the right hardware resources.</p>
			<p>Now, let us shift our focus to the other project, <code>ch10-mongo</code>, to tackle topics regarding workflows, GraphQL, and Neo4j graph database transactions and how FastAPI can utilize them.</p>
			<h1 id="_idParaDest-305"><a id="_idTextAnchor309"/>Simulating a BPMN workflow</h1>
			<p>Although the FastAPI <a id="_idIndexMarker876"/>framework has no built-in utilities to support its workflows, it is flexible and fluid enough to be integrated into other workflow tools such as Camunda and Apache Airflow through extension modules, middleware, and other customizations. But this section will only focus on the raw solution of simulating BPMN workflows using Celery, which can be extended to a more flexible, real-time, and <a id="_idIndexMarker877"/>enterprise-grade approach such as Airflow integration.</p>
			<h2 id="_idParaDest-306"><a id="_idTextAnchor310"/>Designing the BPMN workflow</h2>
			<p>The <code>ch10-mongo</code> project <a id="_idIndexMarker878"/>has implemented the following BPMN workflow design using Celery:</p>
			<ul>
				<li>A sequence of service tasks that derives the percentage of the survey data result, as shown in the following diagram:</li>
			</ul>
			<div><div><img src="img/Figure_10.05_B17975.jpg" alt="Figure 10.5 – Percentage computation workflow design&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – Percentage computation workflow design</p>
			<ul>
				<li>A group of batch operations that saves data to CSV and XLSX files, as shown in the following diagram: </li>
			</ul>
			<div><div><img src="img/Figure_10.06_B17975.jpg" alt="Figure 10.6 – Data archiving workflow design&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – Data archiving workflow design</p>
			<ul>
				<li>A group of chained <a id="_idIndexMarker879"/>tasks that operates on each location's data independently, as shown in the following diagram:</li>
			</ul>
			<div><div><img src="img/Figure_10.07_B17975.jpg" alt="Figure 10.7 – Workflow design for stratified survey data analysis&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – Workflow design for stratified survey data analysis</p>
			<p>There are many ways to<a id="_idIndexMarker880"/> implement the given design, but the most immediate solution is to utilize the Celery setup that we used in <a href="B17975_07.xhtml#_idTextAnchor190"><em class="italic">Chapter 7</em></a><em class="italic">, Securing the REST APIs</em>.</p>
			<h2 id="_idParaDest-307"><a id="_idTextAnchor311"/>Implementing the workflow</h2>
			<p>Celery’s <code>chain()</code> method<a id="_idIndexMarker881"/> implements a workflow of linked task executions, as depicted in <em class="italic">Figure 10.5</em>, where every parent task returns the result to the first parameter of next task. The chained workflow works if each task runs successfully without encountering any exceptions at runtime. The following is the API service in <code>/api/survey_workflow.py</code> that implements the chained workflow:</p>
			<pre class="source-code">
<strong class="bold">@router.post("/survey/compute/avg")</strong>
async def chained_workflow(surveydata: SurveyDataResult):
    survey_dict = surveydata.dict(exclude_unset=True)
    result = <strong class="bold">chain(compute_sum_results</strong>
        <strong class="bold">.s(survey_dict['results']).set(queue='default'), </strong>
            <strong class="bold">compute_avg_results.s(len(survey_dict))</strong>
             <strong class="bold">.set(queue='default'), derive_percentile.s()</strong>
             <strong class="bold">.set(queue='default')).apply_async()</strong>
    return {'message' : result.get(timeout = 10) }</pre>
			<p><code>compute_sum_results()</code>, <code>compute_avg_results()</code>, and <code>derive_percentile()</code> are bound tasks. Bound tasks are Celery tasks that are implemented to have the first method parameter allocated to the task instance itself, thus the <code>self</code> keyword appearing in its parameter list. Their task implementation always has the <code>@celery.task(bind=True)</code> decorator. The Celery task manager prefers bound tasks when applying workflow primitive signatures to create workflows. The following code shows<a id="_idIndexMarker882"/> the bound tasks that are used in the chained workflow design: </p>
			<pre class="source-code">
<strong class="bold">@celery.task(bind=True)</strong>
def compute_sum_results(self, results:Dict[str, int]):
    scores = []
    for key, val in results.items():
        scores.append(val)
    return sum(scores)</pre>
			<p><code>compute_sum_results()</code> computes the total survey result per state, while <code>compute_avg_results()</code>consumes the sum computed by <code>compute_sum_results()</code> to derive the mean value:</p>
			<pre class="source-code">
<strong class="bold">@celery.task(bind=True)</strong>
def compute_avg_results(self, value, len):
    return (value/len)</pre>
			<p>On the other hand, <code>derive_percentile()</code> consumes the mean values produced by <code>compute_avg_results()</code> to return a percentage value:</p>
			<pre class="source-code">
<strong class="bold">@celery.task(bind=True)</strong>
def derive_percentile(self, avg):
    percentage = f"{avg:.0%}"
    return percentage</pre>
			<p>The given <code>derive_percentile()</code> consumes the mean values produced by <code>compute_avg_results()</code> to return a percentage value.</p>
			<p>To implement the gateway approach, Celery has a <code>group()</code> primitive signature, which is used to implement parallel task executions, as depicted in <em class="italic">Figure 10.6</em>. The following API shows the implementation of the workflow structure with parallel executions: </p>
			<pre class="source-code">
<strong class="bold">@router.post("/survey/save")</strong>
async def grouped_workflow(surveydata: SurveyDataResult):
    survey_dict = surveydata.dict(exclude_unset=True)
    <strong class="bold">result = group([save_result_xlsx</strong>
       <strong class="bold">.s(survey_dict['results']).set(queue='default'), </strong>
         <strong class="bold">save_result_csv.s(len(survey_dict))</strong>
          <strong class="bold">.set(queue='default')]).apply_async()</strong>
    return {'message' : result.get(timeout = 10) } </pre>
			<p>The workflow shown<a id="_idIndexMarker883"/> in <em class="italic">Figure 10.7</em> depicts a mix of grouped and chained workflows. It is common for many real-world microservice applications to solve workflow-related problems with a mixture of different Celery signatures, including <code>chord()</code>, <code>map()</code>, and <code>starmap()</code>. The following script implements a workflow with mixed signatures: </p>
			<pre class="source-code">
<strong class="bold">@router.post("/process/surveys")</strong>
async def process_surveys(surveys: List[SurveyDataResult]):
    surveys_dict = [s.dict(exclude_unset=True) 
         for s in surveys]
    <strong class="bold">result = group([chain(compute_sum_results</strong>
       <strong class="bold">.s(survey['results']).set(queue='default'), </strong>
         <strong class="bold">compute_avg_results.s(len(survey['results']))</strong>
         <strong class="bold">.set(queue='default'), derive_percentile.s()</strong>
         <strong class="bold">.set(queue='default')) for survey in </strong>
                <strong class="bold">surveys_dict]).apply_async()</strong>
    return {'message': result.get(timeout = 10) }</pre>
			<p>The Celery signature plays an essential role in building workflows. A <code>signature()</code> method or <code>s()</code> that appears in the construct manages the execution of the task, which includes accepting the initial task parameter value(s) and utilizing the queues that the Celery worker uses to load tasks. As discussed in <a href="B17975_07.xhtml#_idTextAnchor190"><em class="italic">Chapter 7</em></a><em class="italic">, Securing the REST APIs</em>, <code>apply_async()</code> triggers the <a id="_idIndexMarker884"/>whole workflow execution and retrieves the result. </p>
			<p>Aside from workflows, the FastAPI framework can also use the GraphQL platform to build CRUD transactions, especially when dealing with a large amount of data in a microservice architecture.</p>
			<h1 id="_idParaDest-308"><a id="_idTextAnchor312"/>Using GraphQL queries and mutations</h1>
			<p>GraphQL<a id="_idIndexMarker885"/> is an API standard that implements REST and CRUD transactions at the same time. It is a high-performing platform that’s used in building REST API<a id="_idIndexMarker886"/> endpoints that only <a id="_idIndexMarker887"/>need a few steps to set up. Its objective is to create endpoints for data manipulation and query transactions.</p>
			<h2 id="_idParaDest-309"><a id="_idTextAnchor313"/>Setting up the GraphQL platform</h2>
			<p>Python extensions<a id="_idIndexMarker888"/> such as Strawberry, Ariadne, Tartiflette, and Graphene support GraphQL-FastAPI integration. This chapter introduces the use of the new Ariadne 3.x to build CRUD transactions for this <code>ch10-mongo</code> project with MongoDB as the repository.</p>
			<p>First, we need to install the latest <code>graphene</code> extension using the <code>pip</code> command:</p>
			<pre>pip install graphene</pre>
			<p>Among the GraphQL libraries, Graphene<a id="_idIndexMarker889"/> is the easiest to set up, with fewer decorators and methods to override. It easily integrates with the FastAPI framework without requiring additional middleware and too much auto-wiring. </p>
			<h2 id="_idParaDest-310"><a id="_idTextAnchor314"/>Creating the record insertion, update, and deletion</h2>
			<p>Data manipulation operations are always part of GraphQL’s mutation mechanism. This is a GraphQL feature that modifies the server-side state of the application and returns arbitrary data as a sign of a successful change in the state. The following is an implementation of a GraphQL<a id="_idIndexMarker890"/> mutation that inserts, deletes, and updates records:</p>
			<pre class="source-code">
<strong class="bold">from models.data.pccs_graphql import LoginData</strong>
<strong class="bold">from graphene import String, Int, Mutation, Field</strong>
from repository.login import LoginRepository
<strong class="bold">class CreateLoginData(Mutation):</strong>
    <strong class="bold">class Arguments:</strong>
      id = Int(required=True)
      username = String(required=True)
      password = String(required=True)
    <strong class="bold">ok = Boolean()</strong>
    <strong class="bold">loginData = Field(lambda: LoginData)</strong>
    async def mutate(root, info, <strong class="bold">id, username, password</strong>):
        login_dict = {"id": id, "username": username, 
                   "password": password}
        login_json = dumps(login_dict, default=json_serial)
        repo = LoginRepository()
        result = await repo.add_login(loads(login_json))
        if not result == None:
          ok = True
        else: 
          ok = False
        <strong class="bold">return CreateLoginData(loginData=result, ok=ok)</strong></pre>
			<p><code>CreateLoginData</code> is a mutation that adds a new login record to the data store. The inner class, <code>Arguments</code>, indicates the record fields that will comprise the new login record for insertion. These arguments must appear in the overridden <code>mutate()</code> method to capture the values of these fields. This method will also call the ORM, which will persist the newly created record.</p>
			<p>After a successful insert transaction, <code>mutate()</code> must return the class variables defined inside a mutation class such as <code>ok</code> and the <code>loginData</code> object. These returned values must be part of the mutation instance. </p>
			<p>Updating a login attribute <a id="_idIndexMarker891"/>has a similar implementation to <code>CreateLoginData</code> except the arguments need to be exposed. The following is a mutation class that updates the <code>password</code> field of a login record that’s been retrieved using its <code>username</code>:</p>
			<pre class="source-code">
<strong class="bold">class ChangeLoginPassword(Mutation):</strong>
    <strong class="bold">class Arguments:</strong>
      username = String(required=True)
      password = String(required=True)
    <strong class="bold">ok = Boolean()</strong>
    <strong class="bold">loginData = Field(lambda: LoginData)</strong>
    async def mutate(root, info, <strong class="bold">username, password</strong>):       
        repo = LoginRepository()
        result = await repo.change_password(username, 
                  password)
        
        if not result == None:
          ok = True
        else: 
          ok = False
        <strong class="bold">return CreateLoginData(loginData=result, ok=ok)</strong></pre>
			<p>Similarly, the <a id="_idIndexMarker892"/>delete mutation class retrieves a record through an <code>id</code> and deletes it from the data store:</p>
			<pre class="source-code">
<strong class="bold">class DeleteLoginData(Mutation):</strong>
    <strong class="bold">class Arguments:</strong>
      id = Int(required=True)
      
    <strong class="bold">ok = Boolean()</strong>
    <strong class="bold">loginData = Field(lambda: LoginData)</strong>
    async def mutate(root, info, id):       
        repo = LoginRepository()
        result = await repo.delete_login(id)
        if not result == None:
          ok = True
        else: 
          ok = False
        <strong class="bold">return DeleteLoginData(loginData=result, ok=ok)</strong></pre>
			<p>Now, we can store all our mutation classes in an <code>ObjectType</code> class that exposes these transactions to the client. We assign field names to each <code>Field</code> instance of the given mutation classes. These field names will serve as the query names of the transactions. The following code shows the <code>ObjectType</code> class, which defines our <code>CreateLoginData</code>, <code>ChangeLoginPassword</code>, and <code>DeleteLoginData</code> mutations:</p>
			<pre class="source-code">
<strong class="bold">class LoginMutations(ObjectType):</strong>
    create_login = CreateLoginData.Field()
    edit_login = ChangeLoginPassword.Field()
    delete_login = DeleteLoginData.Field()</pre>
			<h2 id="_idParaDest-311"><a id="_idTextAnchor315"/>Implementing the query transactions</h2>
			<p>GraphQL query<a id="_idIndexMarker893"/> transactions are implementations of the <code>ObjectType</code> base class. Here, <code>LoginQuery</code> retrieves all login records from the data store:</p>
			<pre class="source-code">
<strong class="bold">class LoginQuery(ObjectType):</strong>
    <strong class="bold">login_list = None</strong>
    <strong class="bold">get_login = Field(List(LoginData))</strong>
  
    async def resolve_<strong class="bold">get_login</strong>(self, info):
      repo = LoginRepository()
      <strong class="bold">login_list = await repo.get_all_login()</strong>
      return <strong class="bold">login_list</strong></pre>
			<p>The class must have a query field name, such as <code>get_login</code>, that will serve as its query name during query execution. The field name must be part of the <code>resolve_*()</code> method name for it to be registered under the <code>ObjectType</code> class. A class variable, such as <code>login_list</code>, must be declared for it to contain all the retrieved records.</p>
			<h2 id="_idParaDest-312"><a id="_idTextAnchor316"/>Running the CRUD transactions</h2>
			<p>We need a <a id="_idIndexMarker894"/>GraphQL schema to integrate the GraphQL components and register the mutation and query classes for the FastAPI framework before running the GraphQL transactions. The following script shows the instantiation of GraphQL’s <code>Schema</code> class with <code>LoginQuery</code> and <code>LoginMutations</code>:</p>
			<pre class="source-code">
<strong class="bold">from graphene import Schema </strong>
schema = Schema(query=<strong class="bold">LoginQuery</strong>, mutation=<strong class="bold">LoginMutations</strong>,
    <strong class="bold">auto_camelcase=False</strong>)</pre>
			<p>We set the <code>auto_camelcase</code> property of the <code>Schema</code> instance to <code>False</code> to maintain the use of the original field names with an underscore and avoid the camel case notation approach.</p>
			<p>Afterward, we<a id="_idIndexMarker895"/> use the schema instance to create the <code>GraphQLApp()</code> instance. GraphQLApp is equivalent to an application that needs mounting to the FastAPI framework. We can use the <code>mount()</code> utility of FastAPI to integrate the <code>GraphQLApp()</code> instance with its URL pattern and the chosen GraphQL browser tool to run the API transactions. The following code shows how to integrate the GraphQL applications with Playground as the browser tool to run the APIs:</p>
			<pre class="source-code">
<strong class="bold">from starlette_graphene3 import GraphQLApp,     </strong>
          <strong class="bold">make_playground_handler</strong>
app = FastAPI()
app.mount("/ch10/graphql/login", 
       GraphQLApp(survey_graphene_login.schema, 
          on_get=make_playground_handler()) )
app.mount("/ch10/graphql/profile", 
       GraphQLApp(survey_graphene_profile.schema, 
          on_get=make_playground_handler()) )</pre>
			<p>We can use the left-hand side panel to insert a new record through a JSON script containing the field name of the <code>CreateLoginData</code> mutation, which is <code>create_login</code>, along with passing the necessary record data, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_10.08_B17975.jpg" alt="Figure 10.8 – Running the create_login mutation&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.8 – Running the create_login mutation</p>
			<p>To perform query transactions, we must create a JSON script with a field name of <code>LoginQuery</code>, which is <code>get_login</code>, together with the record fields needed to be retrieved. The following screenshot shows how to run the <code>LoginQuery</code> transaction:</p>
			<div><div><img src="img/Figure_10.09_B17975.jpg" alt="Figure 10.9 – Running the get_login query transaction&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.9 – Running the get_login query transaction</p>
			<p>GraphQL<a id="_idIndexMarker896"/> can help consolidate all the CRUD transactions from different microservices with easy setup and configuration. It can serve as an API Gateway where all GraphQLApps from multiple microservices are mounted to create a single façade application. Now, let us integrate FastAPI into a graph database. </p>
			<h1 id="_idParaDest-313"><a id="_idTextAnchor317"/>Utilizing the Neo4j graph database</h1>
			<p>For an application that<a id="_idIndexMarker897"/> requires storage that emphasizes relationships among data records, a graph database is an appropriate storage method to use. One of the platforms that use graph databases is Neo4j. FastAPI can easily integrate with Neo4j, but we need to install the <code>Neo4j</code> module using the <code>pip</code> command:</p>
			<pre>pip install neo4j</pre>
			<p>Neo4j is a NoSQL<a id="_idIndexMarker898"/> database with a flexible and powerful data model that can manage and connect different enterprise-related data based on related attributes. It has a semi-structured database architecture with simple ACID properties and a non-JOIN policy that make its operations fast and easy to execute.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">ACID, which stands for atomicity, consistency, isolation, and durability, describes a database transaction as a group of operations that performs as a single unit with correctness and consistency.</p>
			<h2 id="_idParaDest-314"><a id="_idTextAnchor318"/>Setting the Neo4j database</h2>
			<p>The <code>neo4j</code> module<a id="_idIndexMarker899"/> includes <code>neo4j-driver</code>, which is needed to establish a connection with the graph database. It needs a URI that contains the <code>bolt</code> protocol, server address, and port. The default database port to use is <code>7687</code>. The following script shows how to create Neo4j database connectivity:</p>
			<pre class="source-code">
<strong class="bold">from neo4j import GraphDatabase</strong>
<strong class="bold">uri = "bolt://127.0.0.1:7687"</strong>
driver = GraphDatabase.driver(uri, auth=("neo4j", 
      "admin2255"))</pre>
			<h2 id="_idParaDest-315"><a id="_idTextAnchor319"/>Creating the CRUD transactions</h2>
			<p>Neo4j has a <a id="_idIndexMarker900"/>declarative graph query language called Cypher that allows CRUD transactions of the graph database. These Cypher scripts need to be encoded as <code>str</code> SQL commands to be executed by its query runner. The following API service adds a new database record to the graph database:</p>
			<pre class="source-code">
<strong class="bold">@router.post("/neo4j/location/add")</strong>
def create_survey_loc(node_name: str, 
        node_req_atts: LocationReq):
    node_attributes_dict = 
          node_req_atts.dict(exclude_unset=True)
    <strong class="bold">node_attributes = '{' + ', '.join(f'{key}:\'{value}\''</strong>
        <strong class="bold">for (key, value) in node_attributes_dict.items()) </strong>
              <strong class="bold">+ '}'</strong>
    <strong class="bold">query = f"CREATE ({node_name}:Location  </strong>
         <strong class="bold">{node_attributes})"</strong>
    try:
        <strong class="bold">with driver.session() as session:</strong>
            <strong class="bold">session.run(query=query)</strong>
        return JSONResponse(content={"message":
         "add node location successful"}, status_code=201)
    except Exception as e:
        print(e)
        return JSONResponse(content={"message": "add node 
            location unsuccessful"}, status_code=500)</pre>
			<p><code>create_survey_loc()</code> adds new survey location details to the Neo4j database. A record is considered <a id="_idIndexMarker901"/>a node in the graph database with a name and attributes equivalent to the record fields in the relational databases. We use the connection object to create a session that has a <code>run()</code> method to execute Cypher scripts.</p>
			<p>The command to add a new node is <code>CREATE</code>, while the syntax to update, delete, and retrieve nodes can be added with the <code>MATCH</code> command. The following <code>update_node_loc()</code> service searches for a particular node based on the node’s name and performs the <code>SET</code> command to update the given fields:</p>
			<pre class="source-code">
<strong class="bold">@router.patch("/neo4j/update/location/{id}")</strong>
async def update_node_loc(id:int, 
           node_req_atts: LocationReq):
    node_attributes_dict = 
         node_req_atts.dict(exclude_unset=True)
    node_attributes = '{' + ', '.join(f'{key}:\'{value}\'' 
       for (key, value) in 
            node_attributes_dict.items()) + '}'
    <strong class="bold">query = f"""</strong>
        <strong class="bold">MATCH (location:Location)</strong>
        <strong class="bold">WHERE ID(location) = {id}</strong>
        <strong class="bold">SET location += {node_attributes}"""</strong>
    try:
        <strong class="bold">with driver.session() as session:</strong>
            <strong class="bold">session.run(query=query)</strong>
        return JSONResponse(content={"message": 
          "update location successful"}, status_code=201)
    except Exception as e:
        print(e)
        return JSONResponse(content={"message": "update 
           location  unsuccessful"}, status_code=500)</pre>
			<p>Likewise, the<a id="_idIndexMarker902"/> delete transaction uses the <code>MATCH</code> command to search for the node to be deleted. The following service implements <code>Location</code> node deletion:</p>
			<pre class="source-code">
<strong class="bold">@router.delete("/neo4j/delete/location/{node}")</strong>
def delete_location_node(node:str):
    node_attributes = '{' + f"name:'{node}'" + '}'
    <strong class="bold">query = f"""</strong>
        <strong class="bold">MATCH (n:Location {node_attributes})</strong>
        <strong class="bold">DETACH DELETE n</strong>
    <strong class="bold">"""</strong>
    try:
        <strong class="bold">with driver.session() as session:</strong>
            <strong class="bold">session.run(query=query)</strong>
        return JSONResponse(content={"message": 
          "delete location node successful"}, 
             status_code=201)
    except:
        return JSONResponse(content={"message": 
           "delete location node unsuccessful"}, 
               status_code=500)</pre>
			<p>When retrieving <a id="_idIndexMarker903"/>nodes, the following service retrieves all the nodes from the database:</p>
			<pre class="source-code">
<strong class="bold">@router.get("/neo4j/nodes/all")</strong>
async def list_all_nodes():
    <strong class="bold">query = f"""</strong>
        <strong class="bold">MATCH (node)</strong>
        <strong class="bold">RETURN node"""</strong>
    try:
        <strong class="bold">with driver.session() as session:</strong>
            <strong class="bold">result = session.run(query=query)</strong>
            <strong class="bold">nodes = result.data()</strong>
        <strong class="bold">return nodes</strong>
    except Exception as e:
        return JSONResponse(content={"message": "listing
            all nodes unsuccessful"}, status_code=500)</pre>
			<p>The following service only retrieves a single node based on the node’s <code>id</code>:</p>
			<pre class="source-code">
<strong class="bold">@router.get("/neo4j/location/{id}")</strong>
async def get_location(id:int):
    <strong class="bold">query = f"""</strong>
        <strong class="bold">MATCH (node:Location)</strong>
        <strong class="bold">WHERE ID(node) = {id}</strong>
        <strong class="bold">RETURN node"""</strong>
    try:
        <strong class="bold">with driver.session() as session:</strong>
            <strong class="bold">result = session.run(query=query)</strong>
            <strong class="bold">nodes = result.data()</strong>
        return nodes
    except Exception as e:
        return JSONResponse(content={"message": "get 
          location node unsuccessful"}, status_code=500)</pre>
			<p>Our implementation <a id="_idIndexMarker904"/>will not be complete if we have no API endpoint that will link nodes based on attributes. Nodes are linked to each other based on relationship names and attributes that are updatable and removable. The following API endpoint creates a node relationship between the <code>Location</code> nodes and <code>Respondent</code> nodes:</p>
			<pre class="source-code">
<strong class="bold">@router.post("/neo4j/link/respondent/loc")</strong>
def link_respondent_loc(respondent_node: str, 
    loc_node: str, node_req_atts:LinkRespondentLoc):
    node_attributes_dict = 
         node_req_atts.dict(exclude_unset=True)
   
    <strong class="bold">node_attributes = '{' + ', '.join(f'{key}:\'{value}\'' </strong>
       <strong class="bold">for (key, value) in </strong>
          <strong class="bold">node_attributes_dict.items()) + '}'</strong>
  
    <strong class="bold">query = f"""</strong>
        <strong class="bold">MATCH (respondent:Respondent), (loc:Location)</strong>
        <strong class="bold">WHERE respondent.name = '{respondent_node}' AND </strong>
            <strong class="bold">loc.name = '{loc_node}'</strong>
        <strong class="bold">CREATE (respondent) -[relationship:LIVES_IN </strong>
              <strong class="bold">{node_attributes}]-&gt;(loc)"""</strong>
    try:
        <strong class="bold">with driver.session() as session:</strong>
            <strong class="bold">session.run(query=query)</strong>
        return JSONResponse(content={"message": "add … 
            relationship successful"}, status_code=201)
    except:
        return JSONResponse(content={"message": "add 
          respondent-loc relationship unsuccessful"}, 
                 status_code=500)</pre>
			<p>The FastAPI <a id="_idIndexMarker905"/>framework can easily integrate into any database platform. The previous chapters have proven that FastAPI can deal with relational database transactions with ORM and document-based NoSQL transactions with ODM, while this chapter has proven the same for the Neo4j graph database due to its easy configurations.</p>
			<h1 id="_idParaDest-316"><a id="_idTextAnchor320"/>Summary</h1>
			<p>This chapter introduced the scientific side of FastAPI by showing that API services can provide numerical computation, symbolic formulation, and graphical interpretation of data via the <code>numpy</code>, <code>pandas</code>, <code>sympy</code>, and <code>matplotlib</code> modules. This chapter also helped us understand how far we can integrate FastAPI with new technology and design strategies to provide new ideas for the microservice architecture, such as using GraphQL to manage CRUD transactions and Neo4j for real-time and node-based data management. We also introduced the basic approach that FastAPI can apply to solve various BPMN workflows using Celery tasks. With this, we have started to understand the power and flexibility of the framework in building microservice applications. </p>
			<p>The next chapter will cover the last set of topics to complete our deep dive into FastAPI. We will cover some deployment strategies, Django and Flask integrations, and other microservice design patterns that haven’t been discussed in the previous chapters.</p>
		</div>
		<div><div></div>
		</div>
	</body></html>