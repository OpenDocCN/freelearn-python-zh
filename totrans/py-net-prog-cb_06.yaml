- en: Chapter 6. Screen-scraping and Other Practical Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Searching for business addresses using the Google Maps API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Searching for geographic coordinates using the Google Maps URL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Searching for an article in Wikipedia
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Searching for Google stock quote
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Searching for a source code repository at GitHub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading news feed from BBC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crawling links present in a web page
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter shows some of the interesting Python scripts that you can write
    to extract useful information from the web, for example, searching for a business
    address, stock quote for a particular company or the latest news from a news agency
    website. These scripts demonstrate how Python can extract simple information in
    simpler ways without communicating with complex APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Following these recipes, you should be able to write code for complex scenarios,
    for example, find the details about a business, including location, news, stock
    quote, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Searching for business addresses using the Google Maps API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You would like to search for the address of a well-known business in your area.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use the Python geocoding library `pygeocoder` to search for a local
    business. You need to install this library from **PyPI** with `pip` or `easy_install`,
    by entering `$ pip install pygeocoder` or `$ easy_install pygeocoder`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us find the address of Argos Ltd., a well-known UK retailer using a few
    lines of Python code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 6.1 gives a simple geocoding example to search for a business address,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This recipe will print the address of Argos Ltd., as shown. The output may
    vary slightly based on the output from your installed geocoding library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe relies on the Python third-party geocoder library.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe defines a simple function `search_business()` that takes the business
    name as an input and passes that to the `geocode()` function. The `geocode()`
    function can return zero or more search results depending on your search term.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, the `geocode()` function has got the business name Argos Ltd.,
    London, as the search query. In return, it gives the address of Argos Ltd., which
    is 110-114 King Street, London, Greater London W6 0QP, UK.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `pygeocoder` library is powerful and has many interesting and useful features
    for geocoding. You may find more details on the developer's website at [https://bitbucket.org/xster/pygeocoder/wiki/Home](https://bitbucket.org/xster/pygeocoder/wiki/Home).
  prefs: []
  type: TYPE_NORMAL
- en: Searching for geographic coordinates using the Google Maps URL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes you'd like to have a simple function that gives the geographic coordinates
    of a city by giving it just the name of that city. You may not be interested in
    installing any third-party libraries for this simple task.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this simple screen-scraping example, we use the Google Maps URL to query
    the latitude and longitude of a city. The URL used to query can be found after
    making a custom search on the Google Maps page. We can perform the following steps
    to extract some information from Google Maps.
  prefs: []
  type: TYPE_NORMAL
- en: Let us take the name of a city from the command line using the `argparse` module.
  prefs: []
  type: TYPE_NORMAL
- en: We can open the maps search URL using the `urlopen()` function of `urllib`.
    This will give an XML output if the URL is correct.
  prefs: []
  type: TYPE_NORMAL
- en: Now, process the XML output in order to get the geographic coordinates of that
    city.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 6.2 helps finding the geographic coordinates of a city using Google
    Maps, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this script, you should see something similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe takes a name of a city from the command line and passes that to
    the `find_lat_long()` function. This function queries the Google Maps service
    using the `urlopen()` function of `urllib` and gets the XML output. Then, the
    error string `'<error>'` is searched for. If that's not present, it means there
    are some good results.
  prefs: []
  type: TYPE_NORMAL
- en: If you print out the raw XML, it's a long stream of characters produced for
    the browser. In the browser, it would be interesting to display the layers in
    maps. But in our case, we just need the latitude and longitude.
  prefs: []
  type: TYPE_NORMAL
- en: From the raw XML, the latitude and longitude is extracted using the string method
    `find()`. This searches for the keyword "center". This list key possesses the
    geographic coordinates information. But it also contains the additional characters
    which are removed using the string method `replace()`.
  prefs: []
  type: TYPE_NORMAL
- en: You may try this recipe to find out the latitude/longitude of any known city
    of the world.
  prefs: []
  type: TYPE_NORMAL
- en: Searching for an article in Wikipedia
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Wikipedia is a great site to gather information about virtually anything, for
    example, people, places, technology, and what not. If you like to search for something
    on Wikipedia from your Python script, this recipe is for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Searching for an article in Wikipedia](img/3463OS_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to install the `pyyaml` third-party library from PyPI using `pip` or
    `easy_install` by entering `$ pip install pyyaml` or `$ easy_install pyyaml`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us search for the keyword `Islam` in Wikipedia and print each search result
    in one line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 6.3 explains how to search for an article in Wikipedia, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this recipe to query Wikipedia about Islam shows the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we collect the Wikipedia URL template for searching an article. We created
    a class called `Wikipedia`, which has two methods: `_get_content()` and `search_content()`.
    By default upon initialization, the class sets up its language attribute `lang`
    to `en` (English).'
  prefs: []
  type: TYPE_NORMAL
- en: The command-line query string is passed to the `search_content()` method. It
    then constructs the actual search URL by inserting variables such as language,
    query string, page offset, and number of results to return. The `search_content()`
    method can optionally take the parameters and the offset is determined by the
    `(page -1) * limit` expression.
  prefs: []
  type: TYPE_NORMAL
- en: The content of the search result is fetched via the `_get_content()` method
    which calls the `urlopen()` function of `urllib`. In the search URL, we set up
    the result format `yaml`, which is basically intended for plain text files. The
    `yaml` search result is then parsed with Python's `pyyaml` library.
  prefs: []
  type: TYPE_NORMAL
- en: The search result is processed by substituting the regular expressions found
    in each result item. For example, the `re.sub(r'(?m)<.*?>', '', snippet)` expression
    takes the snippet string and replaces a raw pattern `(?m)<.*?>)`. To learn more
    about regular expressions, visit the Python document page, available at [http://docs.python.org/2/howto/regex.html](http://docs.python.org/2/howto/regex.html).
  prefs: []
  type: TYPE_NORMAL
- en: In Wikipedia terminology, each article has a snippet or a short description.
    We create a list of dictionary items where each item contains the title and the
    snippet of each search result. The results are printed on the screen by looping
    through this list of dictionary items.
  prefs: []
  type: TYPE_NORMAL
- en: Searching for Google stock quote
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the stock quote of any company is of interest to you, this recipe can help
    you to find today's stock quote of that company.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We assume that you already know the symbol used by your favorite company to
    enlist itself on any stock exchange. If you don't know, get the symbol from the
    company website or just search in Google.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we use Google Finance ([http://finance.google.com/](http://finance.google.com/))
    to search for the stock quote of a given company. You can input the symbol via
    the command line, as shown in the next code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 6.4 describes how to search for Google stock quote, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this script, you will see an output similar to the following. Here,
    the stock quote for Google is searched by inputting the symbol `goog`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe uses the `urlopen()` function of `urllib` to get the stock data
    from the Google Finance website.
  prefs: []
  type: TYPE_NORMAL
- en: By using the regular expression library `re`, it locates the stock quote data
    in the first group of items. The `search()` function of `re` is powerful enough
    to search the content and filter the ID data of that particular company.
  prefs: []
  type: TYPE_NORMAL
- en: Using this recipe, we searched for the stock quote of Google, which was `868.86`
    on August 20, 2013.
  prefs: []
  type: TYPE_NORMAL
- en: Searching for a source code repository at GitHub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a Python programmer, you may already be familiar with GitHub ([http://www.github.com](http://www.github.com)),
    a source code-sharing website, as shown in the following screenshot. You can share
    your source code privately to a team or publicly to the world using GitHub. It
    has a nice API interface to query about any source code repository. This recipe
    may give you a starting point to create your own source code search engine.
  prefs: []
  type: TYPE_NORMAL
- en: '![Searching for a source code repository at GitHub](img/3463OS_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To run this recipe, you need to install the third-party Python library `requests`
    by entering `$ pip install requests` or `$ easy_install requests`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We would like to define a `search_repository()` function that will take the
    name of author (also known as coder), repository, and search key. In return, it
    will give us back the available result against the search key. From the GitHub
    API, the following are the available search keys: `issues_url`, `has_wiki`, `forks_url`,
    `mirror_url`, `subscription_url`, `notifications_url`, `collaborators_url`, `updated_at`,
    `private`, `pulls_url`, `issue_comment_url`, `labels_url`, `full_name`, `owner`,
    `statuses_url`, `id`, `keys_url`, `description`, `tags_url`, `network_count`,
    `downloads_url`, `assignees_url`, `contents_url`, `git_refs_url`, `open_issues_count`,
    `clone_url`, `watchers_count`, `git_tags_url`, `milestones_url`, `languages_url`,
    `size`, `homepage`, `fork`, `commits_url`, `issue_events_url`, `archive_url`,
    `comments_url`, `events_url`, `contributors_url`, `html_url`, `forks`, `compare_url`,
    `open_issues`, `git_url`, `svn_url`, `merges_url`, `has_issues`, `ssh_url`, `blobs_url`,
    `master_branch`, `git_commits_url`, `hooks_url`, `has_downloads`, `watchers`,
    `name`, `language`, `url`, `created_at`, `pushed_at`, `forks_count`, `default_branch`,
    `teams_url`, `trees_url`, `organization`, `branches_url`, `subscribers_url`, and
    `stargazers_url`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 6.5 gives the code to search for details of a source code repository
    at GitHub, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this script to search for the owner of the Python web framework
    Django, you can get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This script takes three command-line arguments: repository author (`--author`),
    repository name (`--repo`), and the item to search for (`--search_for`). The arguments
    are processed by the `argpase` module.'
  prefs: []
  type: TYPE_NORMAL
- en: Our `search_repository()` function appends the command-line arguments to a fixed
    search URL and receives the content by calling the `requests` module's `get()`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: The search results are, by default, returned in the JSON format. This content
    is then processed with the `json` module's `loads()` method. The search key is
    then looked for inside the result and the corresponding value of that key is returned
    back to the caller of the `search_repository()` function.
  prefs: []
  type: TYPE_NORMAL
- en: In the main user code, we check whether the search result is an instance of
    the Python dictionary. If yes, then the key/values are printed iteratively. Otherwise,
    the value is printed.
  prefs: []
  type: TYPE_NORMAL
- en: Reading news feed from BBC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are developing a social networking website with news and stories, you
    may be interested to present the news from various world news agencies, such as
    BBC and Reuters. Let us try to read news from BBC via a Python script.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe relies on Python''s third-party `feedparser` library. You can install
    this by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Or
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we collect the BBC's news feed URL from the BBC website. This URL can
    be used as a template to search news on various types, such as world, UK, health,
    business, and technology. So, we can take the type of news to display as user
    input. Then, we depend on the `read_news()` function, which will fetch the news
    from the BBC.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 6.6 explains how to read news feed from the BBC, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this script will show you the available news categories. If we choose
    technology as the category, you can get the latest news on technology, as shown
    in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, the `read_news()` function relies on Python's third-party module
    `feedparser`. The `feedparser` module's `parser()` method returns the feed data
    in a structured fashion.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, the `parser()` method parses the given feed URL. This URL is
    constructed from `BBC_FEED_URL` and user input.
  prefs: []
  type: TYPE_NORMAL
- en: After some valid feed data is obtained by calling `parse()`, the contents of
    the data is then printed, such as title, link, and description, of each feed entry.
  prefs: []
  type: TYPE_NORMAL
- en: Crawling links present in a web page
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At times you would like to find a specific keyword present in a web page. In
    a web browser, you can use the browser's in-page search facility to locate the
    terms. Some browsers can highlight it. In a complex situation, you would like
    to dig deep and follow every URL present in a web page and find that specific
    term. This recipe will automate that task for you.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us write a `search_links()` function that will take three arguments: the
    search URL, the depth of the recursive search, and the search key/term, since
    every URL may have links present in the content and that content may have more
    URLs to crawl. To limit the recursive search, we define a depth. Upon reaching
    that depth, no more recursive search will be done.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 6.7 gives the code for crawling links present in a web page, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this script to search [www.python.org](http://www.python.org) for
    `python`, you will see an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe can take three command-line inputs: search URL (`--url`), the query
    string (`--query`), and the depth of recursion (`--depth`). These inputs are processed
    by the `argparse` module.'
  prefs: []
  type: TYPE_NORMAL
- en: When the `search_links()` function is called with the previous arguments, this
    will recursively iterate on all the links found on that given web page. If it
    takes too long to finish, you would like to exit prematurely. For this reason,
    the `search_links()` function is placed inside a try-catch block which can catch
    the user's keyboard interrupt action, such as *Ctrl* + *C*.
  prefs: []
  type: TYPE_NORMAL
- en: The `search_links()` function keeps track of visited links via a list called
    `processed`. This is made global to give access to all the recursive function
    calls.
  prefs: []
  type: TYPE_NORMAL
- en: In a single instance of search, it is ensured that only HTTP URLs are processed
    in order to avoid the potential SSL certificate errors. The URL is split into
    a host and a path. The main crawling is initiated using the `HTTPConnection()`
    function of `httplib`. It gradually makes a `GET` request and a response is then
    processed using the regular expression module `re`. This collects all the links
    from the response. Each response is then examined for the search term. If the
    search term is found, it prints that incident.
  prefs: []
  type: TYPE_NORMAL
- en: The collected links are visited recursively in the same way. If any relative
    URL is found, that instance is converted into a full URL by prefixing `http://`
    to the host and the path. If the depth of search is greater than 0, the recursion
    is activated. It reduces the depth by 1 and runs the search function again. When
    the search depth becomes 0, the recursion ends.
  prefs: []
  type: TYPE_NORMAL
