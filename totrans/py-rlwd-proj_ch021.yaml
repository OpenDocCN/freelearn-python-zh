- en: Chapter 17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next Steps
  prefs: []
  type: TYPE_NORMAL
- en: The journey from raw data to useful information has only begun. There are often
    many more steps to getting insights that can be used to support enterprise decision-making.
    From here, the reader needs to take the initiative to extend these projects, or
    consider other projects. Some readers will want to demonstrate their grasp of
    Python while others will go more deeply into the area of exploratory data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Python is used for so many different things that it seems difficult to even
    suggest a direction for deeper understanding of the language, the libraries, and
    the various ways Python is used.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll touch on a few more topics related to exploratory data
    analysis. The projects in this book are only a tiny fraction of the kinds of problems
    that need to be solved on a daily basis.
  prefs: []
  type: TYPE_NORMAL
- en: Every analyst needs to balance the time between understanding the enterprise
    data being processed, searching for better ways to model the data, and effective
    ways to present the results. Each of these areas is a large domain of knowledge
    and skills.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start with a review of the architecture underlying the sequence of projects
    in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 17.1 Overall data wrangling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The applications and notebooks are designed around the following multi-stage
    architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Data acquisition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inspection of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleaning data; this includes validating, converting, standardizing, and saving
    intermediate results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarizing, and the start of modeling data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating deeper analysis and more sophisticated statistical models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The stages fit together as shown in [*Figure 17.1*](#17.1).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 17.1: Data Analysis Pipeline ](img/file79.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.1: Data Analysis Pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: The last step in this pipeline isn’t — of course — final. In many cases, the
    project evolves from exploration to monitoring and maintenance. There will be
    a long tail where the model continues to be confirmed. Some enterprise management
    oversight is an essential part of this ongoing confirmation.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, the long tail is interrupted by a change. This may be reflected
    by a model’s inaccuracy. There may be a failure to pass basic statistical tests.
    Uncovering the change and the reasons for change is why enterprise management
    oversight is so essential to data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: This long tail of analysis can last a long time. The responsibility may be passed
    from analyst to analyst. Stakeholders may come and go. An analyst often needs
    to spend precious time justifying an ongoing study that confirms the enterprise
    remains on course.
  prefs: []
  type: TYPE_NORMAL
- en: Other changes in enterprise processing or software will lead to outright failure
    in the analytical processing tools. The most notable changes are those to “upstream”
    applications. Sometimes these changes are new versions of software. Other times,
    the upstream changes are organizational in nature, and some of the foundational
    assumptions about the enterprise need to change. As the data sources change, the
    data acquisition part of this pipeline must also change. In some cases, the cleaning,
    validating, and standardizing must also change.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the rapid pace of change in the supporting tools — Python, JupyterLab,
    Matplotlib, etc. — it becomes essential to rebuild and retest these analytic applications
    periodically. The version numbers in `requirements.txt` files must be checked
    against Anaconda distributions, conda-forge, and the PyPI index. The tempo and
    nature of the changes make this maintenance task an essential part of any well-engineered
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of enterprise oversight and management involvement is sometimes dubbed
    ‘‘decision support.” We’ll look briefly at how data analysis and modeling is done
    as a service to decision-makers.
  prefs: []
  type: TYPE_NORMAL
- en: 17.2 The concept of “decision support”
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The core concept behind all data processing, including analytics and modeling,
    is to help some person make a decision. Ideally, a good decision will be based
    on sound data.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, decisions are made by software. Sometimes the decisions are simple
    rules that identify bad data, incomplete processes, or invalid actions. In other
    cases, the decisions are more nuanced, and we apply the term “artificial intelligence”
    to the software making the decision.
  prefs: []
  type: TYPE_NORMAL
- en: While many kinds of software applications make many automated decisions, a person
    is still — ultimately — responsible for those decisions being correct and consistent.
    This responsibility may be implemented as a person reviewing a periodic summary
    of decisions made.
  prefs: []
  type: TYPE_NORMAL
- en: This responsible stakeholder needs to understand the number and types of decisions
    being made by application software. They need to confirm the automated decisions
    reflect sound data as well as the stated policy, the governing principles of the
    enterprise, and any legal frameworks in which the enterprise operates.
  prefs: []
  type: TYPE_NORMAL
- en: This suggests a need for a meta-analysis and a higher level of decision support.
    The operational data is used to create a model that can make decisions. The results
    of the decisions become a dataset about the decision-making process; this is subject
    to analysis and modeling to confirm the proper behavior of the operational model.
  prefs: []
  type: TYPE_NORMAL
- en: In all cases, the ultimate consumer is the person who needs the data to decide
    if a process is working correctly or there are defects that need correction.
  prefs: []
  type: TYPE_NORMAL
- en: This idea of multiple levels of data processing leads to the idea of carefully
    tracking data sources to understand the meaning and any transformations applied
    to that data. We’ll look at metadata topics, next.
  prefs: []
  type: TYPE_NORMAL
- en: 17.3 Concept of metadata and provenance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The description of a dataset includes three important aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: The syntax or physical format and logical layout of the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The semantics, or meaning, of the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The provenance, or the origin and transformations applied to the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The physical format of a dataset is often summarized using the name of a well-known
    file format. For example, the data may be in CSV format. The order of columns
    in a CSV file may change, leading to a need to have headings or some metadata
    describing the logical layout of the columns within a CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: Much of this information can be enumerated in JSON schema definitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In some cases, the metadata might be yet another CSV file that has column numbers,
    preferred data types, and column names. We might have a secondary CSV file that
    looks like the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This metadata information describes the contents of a separate CSV file with
    the relevant data in it. This can be transformed into a JSON schema to provide
    a uniform metadata notation.
  prefs: []
  type: TYPE_NORMAL
- en: The provenance metadata has a more complicated set of relationships. The PROV
    model (see [https://www.w3.org/TR/prov-overview/](https://www.w3.org/TR/prov-overview/))
    describes a model that includes **Entity**, **Agent**, and **Activity**, which
    create or influence the creation of data. Within the PROV model, there are a number
    of relationships, including **Generation** and **Derivation**, that have a direct
    impact on the data being analyzed.
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways to serialize the information. The PROV-N standard provides
    a textual representation that’s relatively easy to read. The PROV-O standard defines
    an OWL ontology that can be used to describe the provenance of data. Ontology
    tools can query the graph of relationships to help an analyst better understand
    the data being analyzed.
  prefs: []
  type: TYPE_NORMAL
- en: The reader is encouraged to look at [https://pypi.org/project/prov/](https://pypi.org/project/prov/)
    for a Python implementation of the PROV standard for describing data provenance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll look at additional data modeling and machine learning
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 17.4 Next steps toward machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can draw a rough boundary between statistical modeling and machine learning.
    This is a hot topic of debate because — viewed from a suitable distance — all
    statistical modeling can be described as machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we’ve drawn a boundary to distinguish methods based on algorithms
    that are finite, definite, and effective. For example, the process of using the
    linear least squares technique to find a function that matches data is generally
    reproducible with an exact closed-form answer that doesn’t require tuning hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Even within our narrow domain of “statistical modeling,” we can encounter data
    sets for which linear least squares don’t behave well. One notable assumption
    of the least squares estimates, for example, is that the independent variables
    are all known exactly. If the *x* values are subject to observational error, a
    more sophisticated approach is required.
  prefs: []
  type: TYPE_NORMAL
- en: The boundary between “statistical modeling” and “machine learning” isn’t a crisp,
    simple distinction.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll note one characteristic feature of machine learning: tuning hyperparameters.
    The exploration of hyperparameters can become a complex side topic for building
    a useful model. This feature is important because of the jump in the computing
    cost between a statistical model and a machine learning model that requires hyperparameter
    tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are two points on a rough spectrum of computational costs:'
  prefs: []
  type: TYPE_NORMAL
- en: A statistical model may be created by a finite algorithm to reduce the data
    to a few parameters including the coefficients of a function that fits the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A machine learning model may involve a search through alternative hyperparameter
    values to locate a combination that produces a model passes some statistical tests
    for utility.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The search through hyperparameter values often involves doing substantial computation
    to create each variation of a model. Then doing additional substantial computations
    to measure the accuracy and general utility of the model. These two steps are
    iterated for various hyperparameter values, looking for the best model. This iterative
    search can make some machine learning approaches computationally intensive.
  prefs: []
  type: TYPE_NORMAL
- en: This overhead and hyperparameter search is not a universal feature of machine
    learning. For the purposes of this book, it’s where the author drew a line to
    limit the scope, complexity, and cost of the projects.
  prefs: []
  type: TYPE_NORMAL
- en: You are strongly encouraged to continue your projects by studying the various
    linear models available in scikit-learn. See [https://scikit-learn.org/stable/modules/linear_model.html](https://scikit-learn.org/stable/modules/linear_model.html).
  prefs: []
  type: TYPE_NORMAL
- en: The sequence of projects in this book is the first step toward creating a useful
    understanding from raw data.
  prefs: []
  type: TYPE_NORMAL
