["```py\nimport time\nimport concurrent.futures\n\ndef timer(name, steps, interval=0.1):\n    '''timer function that sleeps 'steps * interval' '''\n    for step in range(steps):\n        print(name, step)\n        time.sleep(interval)\n\nif __name__ == '__main__':\n    # Replace with concurrent.futures.ProcessPoolExecutor for\n    # multiple processes instead of threads\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        # Submit the function to the executor with some arguments\n        executor.submit(timer, steps=3, name='a')\n        # Sleep a tiny bit to keep the output order consistent\n        time.sleep(0.1)\n        executor.submit(timer, steps=3, name='b') \n```", "```py\n$ python3 T_00_concurrent_futures.py\na 0\nb 0\na 1\nb 1\na 2\nb 2 \n```", "```py\nimport time\nimport threading\n\ndef timer(name, steps, interval=0.1):\n    '''timer function that sleeps 'steps * interval' '''\n    for step in range(steps):\n        print(name, step)\n        time.sleep(interval)\n\n# Create the threads declaratively\na = threading.Thread(target=timer, kwargs=dict(name='a', steps=3))\nb = threading.Thread(target=timer, kwargs=dict(name='b', steps=3))\n\n# Start the threads\na.start()\n# Sleep a tiny bit to keep the output order consistent\ntime.sleep(0.1)\nb.start() \n```", "```py\nimport time\nimport threading\n\nclass Timer(threading.Thread):\n    def __init__(self, name, steps, interval=0.1):\n        self.steps = steps\n        self.interval = interval\n        # Small gotcha: threading.Thread has a built-in name\n        # parameter so be careful not to manually override it\n        super().__init__(name=name)\n\n    def run(self):\n        '''timer function that sleeps 'steps * interval' '''\n        for step in range(self.steps):\n            print(self.name, step)\n            time.sleep(self.interval)\na = Timer(name='a', steps=3)\nb = Timer(name='b', steps=3)\n\n# Start the threads\na.start()\n# Sleep a tiny bit to keep the output order consistent\ntime.sleep(0.1)\nb.start() \n```", "```py\nimport time\nimport multiprocessing\n\ndef timer(name, steps, interval=0.1):\n    '''timer function that sleeps 'steps * interval' '''\n    for step in range(steps):\n        print(name, step)\n        time.sleep(interval)\n\nif __name__ == '__main__':\n    # Create the processes declaratively\n    a = multiprocessing.Process(target=timer, kwargs=dict(name='a', steps=3))\n    b = multiprocessing.Process(target=timer, kwargs=dict(name='b', steps=3))\n\n    # Start the processes\n    a.start()\n    # Sleep a tiny bit to keep the output order consistent\n    time.sleep(0.1)\n    b.start() \n```", "```py\nimport time\nimport multiprocessing\n\nclass Timer(multiprocessing.Process):\n    def __init__(self, name, steps, interval=0.1):\n        self.steps = steps\n        self.interval = interval\n        # Similar to threading.Thread, multiprocessing.Process\n        # also supports the name parameter but you are not\n        # required to use it here.\n        super().__init__(name=name)\n\n    def run(self):\n        '''timer function that sleeps 'steps * interval' '''\n        for step in range(self.steps):\n            print(self.name, step)\n            time.sleep(self.interval)\n\nif __name__ == '__main__':\n    a = Timer(name='a', steps=3)\n    b = Timer(name='b', steps=3)\n\n    # Start the process\n    a.start()\n    # Sleep a tiny bit to keep the output order consistent\n    time.sleep(0.1)\n    b.start() \n```", "```py\nimport time\nimport threading\n\nclass Forever(threading.Thread):\n    def __init__(self):\n        self.stop = threading.Event()\n        super().__init__()\n\n    def run(self):\n        while not self.stop.is_set():\n            # Do whatever you need to do here\n            time.sleep(0.1)\n\nthread = Forever()\nthread.start()\n# Do whatever you need to do here\nthread.stop.set()\nthread.join() \n```", "```py\nimport time\nimport multiprocessing\n\nclass Forever(multiprocessing.Process):\n    def run(self):\n        while True:\n            # Do whatever you need to do here\n            time.sleep(0.1)\n\nif __name__ == '__main__':\n    process = Forever()\n    process.start()\n\n    # Kill our \"unkillable\" process\n    process.terminate()\n    # Wait for 10 seconds to properly exit      \n    process.join(10)\n\n    # If it still didn't exit, kill it\n    if process.exitcode is None:\n        process.kill() \n```", "```py\nimport timeit\nimport socket\nimport concurrent.futures\n\ndef getaddrinfo(*args):\n    # Call getaddrinfo but ignore the given parameter\n    socket.getaddrinfo('localhost', None)\n\ndef benchmark(threads, n=1000):\n    if threads > 1:\n        # Create the executor\n        with concurrent.futures.ThreadPoolExecutor(threads) \\\n                as executor:\n            executor.map(getaddrinfo, range(n))\n\n    else:\n        # Make sure to use 'list'. Otherwise the generator will\n        # not execute because it is lazy\n        list(map(getaddrinfo, range(n)))\n\nif __name__ == '__main__':\n    for threads in (1, 10, 50, 100):\n        print(f'Testing with {threads} threads and n={10} took: ',\n              end='')\n        print('{:.1f}'.format(timeit.timeit(\n            f'benchmark({threads})',\n            setup='from __main__ import benchmark',\n            number=10,\n        ))) \n```", "```py\n$ python3 T_07_thread_batch_processing.py\nTesting with 1 threads and n=10 took: 2.1\nTesting with 10 threads and n=10 took: 1.9\nTesting with 50 threads and n=10 took: 1.9\nTesting with 100 threads and n=10 took: 13.9 \n```", "```py\n$ python3 T_08_process_batch_processing.py\nTesting with 1 processes and n=10 took: 2.1\nTesting with 10 processes and n=10 took: 3.2\nTesting with 50 processes and n=10 took: 8.3\nTesting with 100 processes and n=10 took: 15.0 \n```", "```py\nimport timeit\nimport functools\nimport multiprocessing\nimport concurrent.futures\n\ndef triangle_number(n):\n    total = 0\n    for i in range(n + 1):\n        total += i\n\n    return total\n\ndef bench_mp(n, count, chunksize):\n    with multiprocessing.Pool() as pool:\n        # Generate a generator like [n, n, n, ..., n, n]\n        iterable = (n for _ in range(count))\n        list(pool.imap_unordered(triangle_number, iterable,\n                                 chunksize=chunksize))\n\ndef bench_ft(n, count, chunksize):\n    with concurrent.futures.ProcessPoolExecutor() as executor:\n        # Generate a generator like [n, n, n, ..., n, n]\n        iterable = (n for _ in range(count))\n        list(executor.map(triangle_number, iterable,\n                          chunksize=chunksize))\n\nif __name__ == '__main__':\n    timer = functools.partial(timeit.timeit, number=5)\n\n    n = 1000\n    chunksize = 50\n    for count in (100, 1000, 10000):\n        # Using <6 formatting for consistent alignment\n        args = ', '.join((\n            f'n={n:<6}',\n            f'count={count:<6}',\n            f'chunksize={chunksize:<6}',\n        ))\n        time_mp = timer(\n            f'bench_mp({args})',\n            setup='from __main__ import bench_mp',\n        )\n        time_ft = timer(\n            f'bench_ft({args})',\n            setup='from __main__ import bench_ft',\n        )\n\n        print(f'{args} mp: {time_mp:.2f}, ft: {time_ft:.2f}') \n```", "```py\n$ python3 T_09_multiprocessing_pool.py\nn=1000  , count=100   , chunksize=50     mp: 0.71, ft: 0.42\nn=1000  , count=1000  , chunksize=50     mp: 0.76, ft: 0.96\nn=1000  , count=10000 , chunksize=50     mp: 1.12, ft: 1.40 \n```", "```py\nimport multiprocessing\n\nsome_int = multiprocessing.Value('i', 123)\nwith some_int.get_lock():\n    some_int.value += 10\nprint(some_int.value)\n\nsome_double_array = multiprocessing.Array('d', [1, 2, 3])\nwith some_double_array.get_lock():\n    some_double_array[0] += 2.5\nprint(some_double_array[:]) \n```", "```py\nfrom multiprocessing import shared_memory\n\n# From process A we could write something\nname = 'share_a'\nshare_a = shared_memory.SharedMemory(name, create=True, size=4)\nshare_a.buf[0] = 10\n\n# From a different process, or the same one, we can access the data\nshare_a = shared_memory.SharedMemory(name)\nprint(share_a.buf[0])\n\n# Make sure to clean up after. And only once!\nshare_a.unlink() \n```", "```py\nfrom multiprocessing import shared_memory\n\nshared_list = shared_memory.ShareableList(['Hi', 1, False, None])\n# Changing type from str to bool here\nshared_list[0] = True\n# Don't forget to unlink()\nshared_list.shm.unlink() \n```", "```py\nimport multiprocessing\n\ndef triangle_number_local(n):\n    total = 0\n    for i in range(n + 1):\n        total += i\n\n    return total\n\ndef bench_local(n, count):\n    with multiprocessing.Pool() as pool:\n        results = pool.imap_unordered(\n            triangle_number_local,\n            (n for _ in range(count)),\n        )\n        print('Sum:', sum(results)) \n```", "```py\nimport multiprocessing\n\nclass Shared:\n    pass\n\ndef initializer(shared_value):\n    Shared.value = shared_value\n\ndef triangle_number_shared(n):\n    for i in range(n + 1):\n        with Shared.value.get_lock():\n            Shared.value.value += i\n\ndef bench_shared(n, count):\n    shared_value = multiprocessing.Value('i', 0)\n\n    # We need to explicitly share the shared_value. On Unix you\n    # can work around this by forking the process, on Windows it\n    # would not work otherwise\n    pool = multiprocessing.Pool(\n        initializer=initializer,\n        initargs=(shared_value,),\n    )\n\n    iterable = (n for _ in range(count))\n    list(pool.imap_unordered(triangle_number_shared, iterable))\n    print('Sum:', shared_value.value)\n\n    pool.close() \n```", "```py\nimport timeit\n\nif __name__ == '__main__':\n    n = 1000\n    count = 100\n    number = 5\n\n    for function in 'bench_local', 'bench_shared':\n        statement = f'{function}(n={n}, count={count})'\n        result = timeit.timeit(\n            statement, number=number,\n            setup=f'from __main__ import {function}',\n        )\n        print(f'{statement}: {result:.3f}') \n```", "```py\nbench_local(n=1000, count=100): 0.598\nbench_shared(n=1000, count=100): 4.157 \n```", "```py\ndef triangle_number_shared_efficient(n):\n    total = 0\n    for i in range(n + 1):\n        total += i\n\n    with Shared.value.get_lock():\n        Shared.value.value += total \n```", "```py\nimport multiprocessing\n\nmanager = multiprocessing.Manager()\nnamespace = manager.Namespace()\n\nnamespace.spam = 123\nnamespace.eggs = 456 \n```", "```py\ndef triangle_number_namespace(namespace, lock, n):\n    for i in range(n + 1):\n        with lock:\n            namespace.total += i\n\ndef bench_manager(n, count):\n    manager = multiprocessing.Manager()\n    namespace = manager.Namespace()\n    namespace.total = 0\n    lock = manager.Lock()\n    with multiprocessing.Pool() as pool:\n        list(pool.starmap(\n            triangle_number_namespace,\n            ((namespace, lock, n) for _ in range(count)),\n        ))\n        print('Sum:', namespace.total) \n```", "```py\ndef triangle_number_namespace_efficient(namespace, lock, n):\n    total = 0\n    for i in range(n + 1):\n        total += i\n\n    with lock:\n        namespace.total += i \n```", "```py\nbench_local(n=1000, count=100): 0.637\nbench_manager(n=1000, count=100): 1.476 \n```", "```py\nimport time\nimport concurrent.futures\n\ncounter = 10\n\ndef increment(name):\n    global counter\n    current_value = counter\n    print(f'{name} value before increment: {current_value}')\n    counter = current_value + 1\n    print(f'{name} value after increment: {counter}')\n\nprint(f'Before thread start: {counter}')\n\nwith concurrent.futures.ThreadPoolExecutor() as executor:\n    executor.map(increment, range(3))\nprint(f'After thread finish: {counter}') \n```", "```py\n$ python3 T_12_thread_safety.py\nBefore thread start: 10\n0 value before increment: 10\n0 value after increment: 11\n1 value before increment: 11\n1 value after increment: 12\n2 value before increment: 11\n2 value after increment: 12\n4 value before increment: 12\n4 value after increment: 13\n3 value before increment: 12\n3 value after increment: 13\nAfter thread finish: 13 \n```", "```py\nL.append(x)\nL1.extend(L2)\nx = L[i]\nx = L.pop()\nL1[i:j] = L2\nL.sort()\nx = y\nx.field = y\nD[x] = y\nD1.update(D2)\nD.keys() \n```", "```py\ni = i+1\nL.append(L[-1])\nL[i] = L[j]\nD[x] = D[x] + 1 \n```", "```py\n# This lock needs to be the same object for all threads\nlock = threading.Lock()\ni = 0\n\ndef increment():\n    global i\n    with lock():\n        i += 1 \n```", "```py\nimport time\nimport threading\n\na = threading.Lock()\nb = threading.Lock()\n\ndef thread_0():\n    print('thread 0 locking a')\n    with a:\n        time.sleep(0.1)\n        print('thread 0 locking b')\n        with b:\n            print('thread 0 everything locked')\n\ndef thread_1():\n    print('thread 1 locking b')\n    with b:\n        time.sleep(0.1)\n        print('thread 1 locking a')\n        with a:\n            print('thread 1 everything locked')\n\nthreading.Thread(target=thread_0).start()\nthreading.Thread(target=thread_1).start() \n```", "```py\nimport threading\nimport concurrent.futures\n\ncontext = threading.local()\n\ndef init_counter():\n    context.counter = 10\n\ndef increment(name):\n    current_value = context.counter\n    print(f'{name} value before increment: {current_value}')\n    context.counter = current_value + 1\n    print(f'{name} value after increment: {context.counter}')\n\ninit_counter()\nprint(f'Before thread start: {context.counter}')\n\nwith concurrent.futures.ThreadPoolExecutor(\n        initializer=init_counter) as executor:\n    executor.map(increment, range(5))\n\nprint(f'After thread finish: {context.counter}') \n```", "```py\n$ python3 T_15_thread_local.py\nBefore thread start: 10\n0 value before increment: 10\n0 value after increment: 11\n1 value before increment: 10\n2 value before increment: 11\n1 value after increment: 11\n3 value before increment: 10\n3 value after increment: 11\n2 value after increment: 12\n4 value before increment: 10\n4 value after increment: 11\nAfter thread finish: 10 \n```", "```py\nimport timeit\nimport multiprocessing\n\ndef busy_wait(n):\n    while n > 0:\n        n -= 1\n\ndef benchmark(n, processes, tasks):\n    with multiprocessing.Pool(processes=processes) as pool:\n        # Execute the busy_wait function 'tasks' times with\n        # parameter n\n        pool.map(busy_wait, [n for _ in range(tasks)])\n    # Create the executor\n\nif __name__ == '__main__':\n    n = 100000\n    tasks = 128\n    for exponent in range(6):\n        processes = int(2 ** exponent)\n        statement = f'benchmark({n}, {processes}, {tasks})'\n        result = timeit.timeit(\n            statement,\n            number=5,\n            setup='from __main__ import benchmark',\n        )\n        print(f'{statement}: {result:.3f}') \n```", "```py\n$ python3 T_16_hyper_threading.py\nbenchmark(100000, 1): 3.400\nbenchmark(100000, 2): 1.894\nbenchmark(100000, 4): 1.208\nbenchmark(100000, 8): 0.998\nbenchmark(100000, 16): 1.124\nbenchmark(100000, 32): 1.787 \n```", "```py\nhost = 'localhost'\nport = 12345\npassword = b'some secret password' \n```", "```py\ndef primes(n):\n    for i, prime in enumerate(prime_generator()):\n        if i == n:\n            return prime\n\ndef prime_generator():\n    n = 2\n    primes = set()\n    while True:\n        for p in primes:\n            if n % p == 0:\n                break\n        else:\n            primes.add(n)\n            yield n\n        n += 1 \n```", "```py\nimport multiprocessing\nfrom multiprocessing import managers\n\nimport constants\nimport functions\n\nqueue = multiprocessing.Queue()\nmanager = managers.BaseManager(address=('', constants.port),\n                               authkey=constants.password)\n\nmanager.register('queue', callable=lambda: queue)\nmanager.register('primes', callable=functions.primes)\n\nserver = manager.get_server()\nserver.serve_forever() \n```", "```py\nfrom multiprocessing import managers\n\nimport constants\n\nmanager = managers.BaseManager(\n    address=(constants.host, constants.port),\n    authkey=constants.password)\nmanager.register('queue')\nmanager.connect()\n\nqueue = manager.queue()\nfor i in range(1000):\n    queue.put(i) \n```", "```py\nfrom multiprocessing import managers\n\nimport functions\n\nmanager = managers.BaseManager(\n    address=(functions.host, functions.port),\n    authkey=functions.password)\nmanager.register('queue')\nmanager.register('primes')\nmanager.connect()\n\nqueue = manager.queue()\nwhile not queue.empty():\n    print(manager.primes(queue.get())) \n```", "```py\n$ python3 T_17_remote_multiprocessing/server.py \n```", "```py\n$ python3 T_17_remote_multiprocessing/submitter.py \n```", "```py\n$ python3 T_17_remote_multiprocessing/client.py \n```", "```py\n$ pip3 install -U \"dask[distributed]\" \n```", "```py\n$ pip3 install -U \"dask[complete]\" \n```", "```py\nimport sys\nimport datetime\n\nfrom dask import distributed\n\ndef busy_wait(n):\n    while n > 0:\n        n -= 1\n\ndef benchmark_dask(client):\n    start = datetime.datetime.now()\n\n    # Run up to 1 million\n    n = 1000000\n    tasks = int(sys.argv[1])  # Get number of tasks from argv\n\n    # Submit the tasks to Dask\n    futures = client.map(busy_wait, [n] * tasks, pure=False)\n    # Gather the results; this blocks until the results are ready\n    client.gather(futures)\n\n    duration = datetime.datetime.now() - start\n    per_second = int(tasks / duration.total_seconds())\n    print(f'{tasks} tasks at {per_second} per '\n          f'second, total time: {duration}')\n\nif __name__ == '__main__':\n    benchmark_dask(distributed.Client()) \n```", "```py\n$ python3 T_18_dask.py 128\n128 tasks at 71 per second, total time: 0:00:01.781836 \n```", "```py\nif __name__ == '__main__':\n    benchmark_dask(distributed.Client()) \n```", "```py\n$ python3 T_19_dask_single.py 128\n128 tasks at 20 per second, total time: 0:00:06.142977 \n```", "```py\n$ dask-scheduler\n[...]\ndistributed.scheduler - INFO - Scheduler at:  tcp://10.1.2.3:8786\ndistributed.scheduler - INFO - dashboard at:                :8787 \n```", "```py\n$ dask-worker --nprocs auto tcp://10.1.2.3:8786 \n```", "```py\nif __name__ == '__main__':\n    benchmark_dask(distributed.Client('localhost:8786')) \n```", "```py\n$ python3 T_20_dask_distributed.py 2048\n[...]\n2048 tasks at 405 per second, total time: 0:00:05.049570 \n```", "```py\n$ pip3 install -U \"ipython[all]\" ipyparallel \n```", "```py\n$ ipython profile create --parallel --profile=mastering_python\n[ProfileCreate] Generating default config file: '~/.ipython/profile_mastering_python/ipython_config.py'\n[ProfileCreate] Generating default config file: '~/.ipython/profile_mastering_python/ipython_kernel_config.py'\n[ProfileCreate] Generating default config file: '~/.ipython/profile_mastering_python/ipcontroller_config.py'\n[ProfileCreate] Generating default config file: '~/.ipython/profile_mastering_python/ipengine_config.py'\n[ProfileCreate] Generating default config file: '~/.ipython/profile_mastering_python/ipcluster_config.py' \n```", "```py\nc.InteractiveShellApp.extensions = [\n    'line_profiler',\n    'memory_profiler',\n] \n```", "```py\nc.HubFactory.client_ip = '*'\nc.RegistrationFactory.ip = '*' \n```", "```py\n$ ipcontroller --profile=mastering_python\n[IPControllerApp] Hub listening on tcp://*:58412 for registration.\n[IPControllerApp] Hub listening on tcp://127.0.0.1:58412 for registration.\n...\n [IPControllerApp] writing connection info to ~/.ipython/profile_mastering_python/security/ipcontroller-client.json\n[IPControllerApp] writing connection info to ~/.ipython/profile_mastering_python/security/ipcontroller-engine.json\n... \n```", "```py\n$ ipython profile create --parallel --profile=mastering_python \n```", "```py\n$ ipcluster engines --profile=mastering_python -n 4\n[IPClusterEngines] IPython cluster: started\n[IPClusterEngines] Starting engines with [daemon=False]\n[IPClusterEngines] Starting 4 Engines with LocalEngineSetLauncher \n```", "```py\nIn [1]: %timeit for _ in range(10): sum(range(10000000))\n1 loops, best of 3: 2.27 s per loop \n```", "```py\nIn [1]: import ipyparallel\n\nIn [2]: client = ipyparallel.Client(profile='mastering_python')\nIn [3]: view = client.load_balanced_view()\nIn [4]: %timeit view.map(lambda _: sum(range(10000000)), range(100)).wait()\n1 loop, best of 3: 909 ms per loop \n```", "```py\nIn [1]: import ipyparallel\n\nIn [2]: client = ipyparallel.Client(profile='mastering_python')\nIn [3]: view = client.load_balanced_view()\nIn [4]: @view.parallel()\n   ...: def loop():\n   ...:     return sum(range(10000000))\n   ...:\nIn [5]: loop.map(range(10))\nOut[5]: <AsyncMapResult: loop> \n```"]