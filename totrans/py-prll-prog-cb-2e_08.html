<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Heterogeneous Computing</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">This chapter will help us to explore the </span><strong><span class="koboSpan" id="kobo.3.1">Graphics Processing Unit</span></strong><span><span class="koboSpan" id="kobo.4.1"> </span></span><span><span class="koboSpan" id="kobo.5.1">(</span></span><strong><span class="koboSpan" id="kobo.6.1">GPU</span></strong><span><span class="koboSpan" id="kobo.7.1">)</span></span><span class="koboSpan" id="kobo.8.1"> programming techniques through the Python language. </span><span class="koboSpan" id="kobo.8.2">The continuous evolution of GPUs is revealing how these architectures can bring great benefits to performing complex calculations.</span></p>
<p><span class="koboSpan" id="kobo.9.1">GPUs certainly cannot replace CPUs. </span><span class="koboSpan" id="kobo.9.2">However, they are a well-structured and heterogeneous code that is able to exploit the strengths of both types of processors that can, in fact, bring considerable advantages.</span></p>
<p><span class="koboSpan" id="kobo.10.1">We will examine the main development environments for heterogeneous programming, namely, the </span><strong><span class="koboSpan" id="kobo.11.1">PyCUDA</span></strong><span><span class="koboSpan" id="kobo.12.1"> and </span></span><strong><span class="koboSpan" id="kobo.13.1">Numba</span></strong><span><span class="koboSpan" id="kobo.14.1"> environments for </span><strong><span class="koboSpan" id="kobo.15.1">Compute Unified Device Architecture</span></strong><span class="koboSpan" id="kobo.16.1"> (</span></span><strong><span class="koboSpan" id="kobo.17.1">CUDA</span></strong><span class="koboSpan" id="kobo.18.1">) and </span><strong><span class="koboSpan" id="kobo.19.1">PyOpenCL</span></strong><span class="koboSpan" id="kobo.20.1"> environments, which are for</span><em><span class="koboSpan" id="kobo.21.1"> </span></em><strong><span class="koboSpan" id="kobo.22.1">Open Computing Language</span></strong><span class="koboSpan" id="kobo.23.1"> (</span><strong><span class="koboSpan" id="kobo.24.1">OpenCL</span></strong><span class="koboSpan" id="kobo.25.1">) frameworks in their Python version.</span></p>
<p><span class="koboSpan" id="kobo.26.1">In this chapter, we will cover the following recipes:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.27.1">Understanding heterogeneous computing</span></li>
<li><span class="koboSpan" id="kobo.28.1">Understanding the GPU architecture</span></li>
<li><span class="koboSpan" id="kobo.29.1">Understanding GPU programming</span></li>
<li><span class="koboSpan" id="kobo.30.1">Dealing with PyCUDA</span></li>
<li><span class="koboSpan" id="kobo.31.1">Heterogeneous programming with PyCUDA</span></li>
<li><span class="koboSpan" id="kobo.32.1">Implementing memory management with PyCUDA</span></li>
<li><span class="koboSpan" id="kobo.33.1">Introducing PyOpenCL</span></li>
<li><span class="koboSpan" id="kobo.34.1">Building applications with PyOpenCL</span></li>
<li><span class="koboSpan" id="kobo.35.1">Element-wise expressions with PyOpenCL</span></li>
<li><span class="koboSpan" id="kobo.36.1">Evaluating PyOpenCL applications</span></li>
<li><span class="koboSpan" id="kobo.37.1">GPU programming with Numba</span></li>
</ul>
<p><span><span class="koboSpan" id="kobo.38.1">Let's start with understanding heterogeneous computing in detail.</span></span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Understanding heterogeneous computing</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Over the years, the search for better performance for increasingly complex calculations has led to the adoption of new techniques in the use of computers. </span><span class="koboSpan" id="kobo.2.2">One of these techniques is called </span><em><span class="koboSpan" id="kobo.3.1">heterogeneous computing</span></em><span class="koboSpan" id="kobo.4.1">, which aims to cooperate with different (or heterogeneous) processors in such a way as to have advantages (in particular) in terms of temporal computational efficiency.</span></p>
<p><span class="koboSpan" id="kobo.5.1">In this context, the processor on which the main program is run (generally the CPU) is called the</span><span><span class="koboSpan" id="kobo.6.1"> </span><em><span class="koboSpan" id="kobo.7.1">h</span></em></span><em><span class="koboSpan" id="kobo.8.1">ost</span></em><span class="koboSpan" id="kobo.9.1">, while the coprocessors (for example, the GPUs) are called</span><span><span class="koboSpan" id="kobo.10.1"> </span><em><span class="koboSpan" id="kobo.11.1">d</span></em></span><em><span class="koboSpan" id="kobo.12.1">evices</span></em><span class="koboSpan" id="kobo.13.1">. </span><span class="koboSpan" id="kobo.13.2">The latter are generally physically separated from the</span><span><span class="koboSpan" id="kobo.14.1"> h</span></span><span class="koboSpan" id="kobo.15.1">ost</span><span><span class="koboSpan" id="kobo.16.1"> </span></span><span class="koboSpan" id="kobo.17.1">and manage their own memory space, which is also separated from the</span><span><span class="koboSpan" id="kobo.18.1"> h</span></span><span class="koboSpan" id="kobo.19.1">ost's memory.</span></p>
<p><span class="koboSpan" id="kobo.20.1">In particular, following significant market demand, the</span><span><span class="koboSpan" id="kobo.21.1"> </span></span><span class="koboSpan" id="kobo.22.1">GPU has evolved into a highly parallel processor,</span><span><span class="koboSpan" id="kobo.23.1"> </span></span><span><span class="koboSpan" id="kobo.24.1">transforming the GPU from devices for</span></span><span><span class="koboSpan" id="kobo.25.1"> </span></span><span><span class="koboSpan" id="kobo.26.1">graphics rendering to devices for parallelizable</span></span><span class="koboSpan" id="kobo.27.1"> and computationally intensive general-purpose calculations.</span></p>
<p><span class="koboSpan" id="kobo.28.1">In fact, the use of GPU for tasks other than rendering graphics on the screen is called</span><span><span class="koboSpan" id="kobo.29.1"> h</span></span><span class="koboSpan" id="kobo.30.1">eterogeneous computing.</span></p>
<p><span class="koboSpan" id="kobo.31.1">Finally, the task of good GPU programming is to make the most of the great level of parallelism and mathematical capabilities offered by the graphics card, minimizing all the disadvantages presented by it, such as the delay of the physical connection between the host and device.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Understanding the GPU architecture</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">A GPU is a specialized CPU/core for vector processing of graphical data to render images from polygonal primitives. </span><span><span class="koboSpan" id="kobo.3.1">The task of a good GPU program is to make the most of the great level of parallelism and mathematical capabilities offered by the graphics card and minimize all the disadvantages presented by it, such as the delay in the physical connection between</span></span><span><span class="koboSpan" id="kobo.4.1"> the h</span></span><span class="koboSpan" id="kobo.5.1">ost</span><span><span class="koboSpan" id="kobo.6.1"> </span></span><span><span class="koboSpan" id="kobo.7.1">and</span></span><span><span class="koboSpan" id="kobo.8.1"> d</span></span><span class="koboSpan" id="kobo.9.1">evice</span><span><span class="koboSpan" id="kobo.10.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">GPUs are characterized by a highly parallel structure that allows you to manipulate large datasets in an efficient manner. </span><span class="koboSpan" id="kobo.11.2">This feature is combined with rapid improvements in hardware performance programs, bringing the attention of the scientific world to the possibility of using GPUs for purposes other than just rendering images.</span></p>
<p><span class="koboSpan" id="kobo.12.1">A GPU (refer to the following diagram) is composed of several processing units called </span><strong><span class="koboSpan" id="kobo.13.1">Streaming Multiprocessors </span></strong><span class="koboSpan" id="kobo.14.1">(</span><strong><span class="koboSpan" id="kobo.15.1">SMs</span></strong><span class="koboSpan" id="kobo.16.1">), which represent the first logic level of parallelism. </span><span class="koboSpan" id="kobo.16.2">In fact, each SM works simultaneously and independently from the others:</span></p>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.17.1"><img src="assets/12715105-d093-49e4-8e05-cdb976bc755c.png" style="width:41.25em;height:33.08em;"/></span></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="hps"><span class="koboSpan" id="kobo.18.1">GPU architecture</span></span></div>
<p><span class="koboSpan" id="kobo.19.1">Each SM is divided into a group of </span><strong><span class="koboSpan" id="kobo.20.1">Streaming Processors</span></strong><span class="koboSpan" id="kobo.21.1"> (</span><strong><span class="koboSpan" id="kobo.22.1">SPs</span></strong><span class="koboSpan" id="kobo.23.1">), which have a core that can run a thread sequentially. </span><span class="koboSpan" id="kobo.23.2">The SP represents the smallest unit of execution logic and the level of finer parallelism.</span></p>
<p><span class="koboSpan" id="kobo.24.1">In order to best program this type of architecture, we need to introduce GPU programming, which is described in the next section.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Understanding GPU programming</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">GPUs have become increasingly programmable. </span><span class="koboSpan" id="kobo.2.2">In fact, their set of instructions has been extended to allow the execution of a greater number of tasks.</span></p>
<p><span class="koboSpan" id="kobo.3.1">Today, on a GPU, it is possible to execute classic CPU programming instructions, such as cycles and conditions, memory access, and floating-point calculations. </span><span class="koboSpan" id="kobo.3.2">The two major discrete video card manufacturers—</span><strong><span class="koboSpan" id="kobo.4.1">NVIDIA</span></strong><span class="koboSpan" id="kobo.5.1"> and </span><strong><span class="koboSpan" id="kobo.6.1">AMD</span></strong><span class="koboSpan" id="kobo.7.1">—have developed their GPU architectures, providing developers with related development environments that allow programming in different programming languages, including Python.</span></p>
<p><span class="koboSpan" id="kobo.8.1">At present, developers have valuable tools for programming software that uses GPUs in</span><span><span class="koboSpan" id="kobo.9.1"> contexts that aren't </span></span><span class="koboSpan" id="kobo.10.1">purely graphics-related. </span><span class="koboSpan" id="kobo.10.2">Among the main development environments for heterogeneous computing, we have CUDA and OpenCL.</span></p>
<p><span class="koboSpan" id="kobo.11.1">Let's now have a look at them in detail.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">CUDA</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">CUDA is a proprietary hardware architecture of NVIDIA, which also gives its name to the related development environment. </span><span class="koboSpan" id="kobo.2.2">Currently, CUDA has a pool of hundreds of thousands of active developers, which demonstrates the growing interest that </span><span><span class="koboSpan" id="kobo.3.1">is developing around </span></span><span class="koboSpan" id="kobo.4.1">this technology in the parallel programming environment.</span></p>
<p><span class="koboSpan" id="kobo.5.1">CUDA offers extensions for the most commonly used programming languages, including Python. </span><span class="koboSpan" id="kobo.5.2">The most well known CUDA Python extensions are as follows:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.6.1">PyCUDA (</span><a href="https://mathema.tician.de/software/pycuda/"><span class="koboSpan" id="kobo.7.1">https://mathema.tician.de/software/PyCUDA/</span></a><span class="koboSpan" id="kobo.8.1">)</span></li>
<li><span class="koboSpan" id="kobo.9.1">Numba (</span><a href="http://numba.pydata.org"><span class="koboSpan" id="kobo.10.1">http://numba.pydata.org</span></a><span class="koboSpan" id="kobo.11.1">)</span></li>
</ul>
<p><span class="koboSpan" id="kobo.12.1">We'll use these extensions in the coming sections.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">OpenCL</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The second protagonist in parallel computing is OpenCL, </span><span><span class="koboSpan" id="kobo.3.1">which </span></span><span class="koboSpan" id="kobo.4.1">(unlike its NVIDIA counterpart) is open standard and can be used not only with GPUs of different manufacturers but also with microprocessors of different types.</span></p>
<p><span class="koboSpan" id="kobo.5.1">However, OpenCL is a more complete and versatile solution as it does not boast the maturity and simplicity of use that CUDA has.</span></p>
<p><span class="koboSpan" id="kobo.6.1">The OpenCL Python extension is PyOpenCL (</span><span class="MsoHyperlink"><a href="https://mathema.tician.de/software/pyopencl/"><span class="koboSpan" id="kobo.7.1">https://mathema.tician.de/software/pyopencl/</span></a><span class="koboSpan" id="kobo.8.1">).</span></span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.9.1">In the following sections, the CUDA and OpenCL programming models will be analyzed in their Python extension and </span><span><span class="koboSpan" id="kobo.10.1">will be </span></span><span class="koboSpan" id="kobo.11.1">accompanied by some interesting application examples.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Dealing with PyCUDA</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">PyCUDA is a binding library that provides access to CUDA's Python API by Andreas Klöckner. </span><span class="koboSpan" id="kobo.2.2">The main features include automatic cleanup, which is tied to an object's lifetime, thus preventing leaks, convenient abstraction over modules and buffers, full access to the driver, and built-in error handling. </span><span class="koboSpan" id="kobo.2.3">It is also very light. </span></p>
<p><span class="koboSpan" id="kobo.3.1">The project is open source under the MIT license, the documentation is very clear, and many different sources found online can provide help and support. </span><span class="koboSpan" id="kobo.3.2">The main purpose of PyCUDA is to let a developer invoke CUDA with minimal abstraction from Python, and it also supports CUDA metaprogramming and templatization.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Getting ready</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Please follow the instructions on the </span><span><span class="koboSpan" id="kobo.3.1">Andreas Klöckner</span></span><span class="koboSpan" id="kobo.4.1"> home page (</span><a href="https://mathema.tician.de/software/pycuda/"><span class="koboSpan" id="kobo.5.1">https://mathema.tician.de/software/pycuda/</span></a><span class="koboSpan" id="kobo.6.1">) to install PyCUDA.</span></p>
<p><span class="koboSpan" id="kobo.7.1">The next programming example has a dual function:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.8.1">The first is to verify that PyCUDA is properly installed.</span></li>
<li><span><span class="koboSpan" id="kobo.9.1">The</span></span><span class="koboSpan" id="kobo.10.1"> second is to read and to print the characteristics of the GPU cards.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How to do it...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Let's look at the steps, as follows:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.3.1">With the first instruction, we import the Python driver (that is, </span><kbd><span class="koboSpan" id="kobo.4.1">pycuda.driver</span></kbd><span class="koboSpan" id="kobo.5.1">) to the CUDA library installed on our PC:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.6.1">import pycuda.driver as drv</span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.7.1">Initialize CUDA. </span><span class="koboSpan" id="kobo.7.2">Note also that the following instruction must be called before any other instruction in the </span><kbd><span class="koboSpan" id="kobo.8.1">pycuda.driver</span></kbd><span><span class="koboSpan" id="kobo.9.1"> module:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.10.1">drv.init()</span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.11.1">Enumerate the number of GPU cards on the PC:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.12.1">print ("%d device(s) found." </span><span class="koboSpan" id="kobo.12.2">% drv.Device.count())</span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.13.1">For each of the GPU cards present, print the model name, the computing capability, and the total amount of memory on the device in kilobytes:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.14.1">for ordinal i n range(drv.Device.count()): 
       dev = drv.Device(ordinal) 
       print ("Device #%d: %s" % (ordinal, dev.name()) 
       print ("Compute Capability: %d.%d"% dev.compute_capability()) 
       print ("Total Memory: %s KB" % (dev.total_memory()//(1024))) </span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How it works...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The execution is pretty simple. </span><span class="koboSpan" id="kobo.2.2">In the first line of code, </span><kbd><span class="koboSpan" id="kobo.3.1">pycuda.driver</span></kbd><span class="koboSpan" id="kobo.4.1"> is imported and then initialized:</span></p>
<pre><span class="koboSpan" id="kobo.5.1">import pycuda.driver as drv  
drv.init() </span></pre>
<p><span class="koboSpan" id="kobo.6.1">The </span><kbd><span class="koboSpan" id="kobo.7.1">pycuda.driver</span></kbd> <span><span class="koboSpan" id="kobo.8.1">module </span></span><span class="koboSpan" id="kobo.9.1">exposes the driver level to the programming interface of CUDA, which is more flexible than the CUDA C runtime-level programming interface, and it has a few features that are not present in the runtime.</span></p>
<p><span class="koboSpan" id="kobo.10.1">Then, it cycles into the </span><kbd><span class="koboSpan" id="kobo.11.1">drv.Device.count()</span></kbd><span class="koboSpan" id="kobo.12.1"> function and, for each GPU card, the name of the card and its main characteristics (computing capability and total memory) are printed:</span></p>
<pre><span class="koboSpan" id="kobo.13.1">print ("Device #%d: %s" % (ordinal, dev.name()))  
print ("Compute Capability: %d.%d" % dev.compute_capability()) 
print ("Total Memory: %s KB" % (dev.total_memory()//(1024))) </span></pre>
<p><span><span class="koboSpan" id="kobo.14.1">Execute</span></span><span class="koboSpan" id="kobo.15.1"> the following code:</span></p>
<pre><strong><span class="koboSpan" id="kobo.16.1">C:\&gt;python dealingWithPycuda.py</span></strong></pre>
<p><span class="koboSpan" id="kobo.17.1">When you've done so, the installed GPU will be shown on the screen, as in the following example:</span></p>
<pre><strong><span class="koboSpan" id="kobo.18.1">1 device(s) found.</span></strong><br/><strong><span class="koboSpan" id="kobo.19.1">Device #0: GeForce GT 240</span></strong><br/><strong><span class="koboSpan" id="kobo.20.1">Compute Capability: 1.2</span></strong><br/><strong><span class="koboSpan" id="kobo.21.1">Total Memory: 1048576 KB</span></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">There's more...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The CUDA programming model (and consequently PyCUDA, which is a Python wrapper) is implemented through specific extensions to the standard library of the C language. </span><span class="koboSpan" id="kobo.2.2">These extensions have been created just like function calls in the standard C library, allowing a simple approach to a heterogeneous programming model that includes the host and device code. </span><span class="koboSpan" id="kobo.2.3">The management of the two logical parts is done by the </span><kbd><span class="koboSpan" id="kobo.3.1">nvcc</span></kbd><span class="koboSpan" id="kobo.4.1"> compiler.</span></p>
<p><span class="koboSpan" id="kobo.5.1">Here is a brief description of how this works:</span></p>
<ol>
<li><em><span class="koboSpan" id="kobo.6.1">Separate</span></em><span class="koboSpan" id="kobo.7.1"> device code from the host code.</span></li>
<li><em><span class="koboSpan" id="kobo.8.1">Invoke</span></em><span class="koboSpan" id="kobo.9.1"> a default compiler (for example, GCC) to compile the host code.</span></li>
<li><em><span class="koboSpan" id="kobo.10.1">Build</span></em><span class="koboSpan" id="kobo.11.1"> the device code in binary form (</span><kbd><span class="koboSpan" id="kobo.12.1">.cubin</span></kbd><span class="koboSpan" id="kobo.13.1"> objects) or in assembly form (</span><kbd><span class="koboSpan" id="kobo.14.1">PTX</span></kbd><span class="koboSpan" id="kobo.15.1"> objects):</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.16.1"><img src="assets/6c16c259-1075-4eb4-bf70-ad0b6ac78a12.png" style="width:39.50em;height:22.67em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="hps"><span class="koboSpan" id="kobo.17.1">PyCUDA execution model</span></span></div>
<p><span class="koboSpan" id="kobo.18.1">All the preceding steps are performed by PyCUDA during execution, with an increase in the application loading time compared to a CUDA application.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">See also</span></h1>
                </header>
            
            <article>
                
<ul>
<li><span class="koboSpan" id="kobo.2.1">The CUDA programming guide is available here: </span><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/"><span class="koboSpan" id="kobo.3.1">https://docs.nvidia.com/CUDA/CUDA-c-programming-guide/</span></a></li>
<li><span class="koboSpan" id="kobo.4.1">The PyCUDA documentation is available </span><span><span class="koboSpan" id="kobo.5.1">here</span></span><span class="koboSpan" id="kobo.6.1">: </span><a href="https://documen.tician.de/pycuda/"><span class="koboSpan" id="kobo.7.1">https://documen.tician.de/PyCUDA/</span></a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Heterogeneous programming with PyCUDA</span></h1>
                </header>
            
            <article>
                
<p><span class="hps"><span class="koboSpan" id="kobo.2.1">The CUDA programming model (and, hence, that of PyCUDA) is designed for the joint execution of a software application on a CPU and GPU, in order to perform the sequential parts of the application on the CPU and those that can be parallelized on the GPU.</span></span><span class="koboSpan" id="kobo.3.1"> Unfortunately, the computer is not smart enough to understand how to distribute the code</span><span><span class="koboSpan" id="kobo.4.1"> autonomously</span></span><span class="koboSpan" id="kobo.5.1">, so it is up to the developer to indicate which parts should be run by the CPU and by the GPU.</span></p>
<p><span class="koboSpan" id="kobo.6.1">In fact, a CUDA application is composed of serial components, which are executed by the system CPU or host, or by parallel components called kernels, which are executed by the GPU or by the device</span><span><span class="koboSpan" id="kobo.7.1"> instead</span></span><span class="koboSpan" id="kobo.8.1">. </span></p>
<p><span class="koboSpan" id="kobo.9.1">A kernel is defined as a </span><em><span class="koboSpan" id="kobo.10.1">grid</span></em><span class="koboSpan" id="kobo.11.1"> and can, in turn, be decomposed into blocks that are sequentially assigned to the various multiprocessors, thus implementing </span><em><span class="koboSpan" id="kobo.12.1">coarse-grained parallelism</span></em><span class="koboSpan" id="kobo.13.1">. </span><span class="koboSpan" id="kobo.13.2">Inside the blocks, there is the fundamental computational unit, the thread, with a very </span><em><span class="koboSpan" id="kobo.14.1">fine parallel granularity</span></em><span class="koboSpan" id="kobo.15.1">. A thread can belong to only one block and is identified by a unique index for the whole kernel. </span><span class="koboSpan" id="kobo.15.2">For convenience, there is the possibility of using two-dimensional indexes for blocks and three-dimensional indexes for threads. </span><span class="koboSpan" id="kobo.15.3">The kernels are executed sequentially between them. </span><span class="koboSpan" id="kobo.15.4">Blocks and threads, on the other hand, are executed in parallel. </span><span class="koboSpan" id="kobo.15.5">The number of threads running (in parallel) depends on their organization in blocks and on their requests in terms of resources, with respect to the resources available in the device.</span></p>
<div class="packt_infobox"><span><span class="koboSpan" id="kobo.16.1">To visualize the concepts expressed previously, please refer to (</span><em><span class="koboSpan" id="kobo.17.1">Figure 5</span></em><span class="koboSpan" id="kobo.18.1">) at </span><a href="https://sites.google.com/site/computationvisualization/programming/cuda/article1"><span class="koboSpan" id="kobo.19.1">https://sites.google.com/site/computationvisualization/programming/cuda/article1</span></a><span class="koboSpan" id="kobo.20.1">.</span><a href="https://sites.google.com/site/computationvisualization/programming/cuda/article1"/></span></div>
<p><span><span class="koboSpan" id="kobo.21.1">The blocks are designed to guarantee scalability. </span><span class="koboSpan" id="kobo.21.2">In fact, if you have an architecture with two multiprocessors and another with four, then, a GPU application can be performed on both architectures, obviously with different times and levels of parallelism.</span></span></p>
<p><span class="koboSpan" id="kobo.22.1">The execution of a heterogeneous program according to the PyCUDA programming model is thus structured as follows</span><span class="hps"><span class="koboSpan" id="kobo.23.1">:</span></span></p>
<ol>
<li><span class="hps"><em><span class="koboSpan" id="kobo.24.1">Allocate</span></em><span class="koboSpan" id="kobo.25.1"> memory </span></span><span class="hps"><span class="koboSpan" id="kobo.26.1">on the h</span></span><span class="koboSpan" id="kobo.27.1">ost.</span></li>
<li><em><span class="koboSpan" id="kobo.28.1">Transfer</span></em><span class="koboSpan" id="kobo.29.1"> data from the host</span><em><span class="koboSpan" id="kobo.30.1"> </span></em><span class="koboSpan" id="kobo.31.1">memory to the device memory.</span></li>
<li><em><span class="koboSpan" id="kobo.32.1">Run</span></em><span class="koboSpan" id="kobo.33.1"> the device through the invocation of the kernel functions.</span></li>
<li><em><span class="koboSpan" id="kobo.34.1">Transfer</span></em><span class="koboSpan" id="kobo.35.1"> the results from the device memory to the host memory.</span></li>
<li><em><span class="koboSpan" id="kobo.36.1">Release</span></em><span><span class="koboSpan" id="kobo.37.1"> the </span></span><span class="koboSpan" id="kobo.38.1">memory allocated on the device.</span></li>
</ol>
<p><span class="koboSpan" id="kobo.39.1">The following diagram shows the execution flow of a program according to the PyCUDA programming model:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.40.1"><img src="assets/d58c37c0-992d-4f12-b1e3-b152038611bc.png" style="width:34.58em;height:18.33em;"/></span></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="hps"><span class="koboSpan" id="kobo.41.1">PyCUDA programming model</span></span></div>
<p><span class="koboSpan" id="kobo.42.1">In the next example, we will go through a concrete example of the programming methodology to follow in order to build PyCUDA applications.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How to do it...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In order to show the PyCUDA programming model, we consider the task of having to double all the elements of a 5 × 5 matrix:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.3.1">We import the libraries needed for the task we want to perform:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.4.1">import PyCUDA.driver as CUDA 
import PyCUDA.autoinit 
from PyCUDA.compiler import SourceModule 
import numpy </span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.5.1">The </span><kbd><span class="koboSpan" id="kobo.6.1">numpy</span></kbd><span class="koboSpan" id="kobo.7.1"> library, which we imported, allows us to construct the input to our problem, that is, a 5 × 5 matrix whose values are chosen randomly:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.8.1">a = numpy.random.randn(5,5) 
a = a.astype(numpy.float32) </span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.9.1">The matrix, thus built, must be copied from the memory of the host to the memory of the device. </span><span class="koboSpan" id="kobo.9.2">For this, we allocate a memory space (</span><kbd><span class="koboSpan" id="kobo.10.1">a</span><em><span class="koboSpan" id="kobo.11.1">_</span></em><span class="koboSpan" id="kobo.12.1">gpu</span></kbd><span class="koboSpan" id="kobo.13.1">) </span><span><span class="koboSpan" id="kobo.14.1">on the device that is </span></span><span class="koboSpan" id="kobo.15.1">necessary to contain matrix </span><kbd><span class="koboSpan" id="kobo.16.1">a</span></kbd><span><span class="koboSpan" id="kobo.17.1">. </span></span><span class="koboSpan" id="kobo.18.1">For this purpose, we use the </span><kbd><span class="koboSpan" id="kobo.19.1">mem_alloc</span></kbd><span class="koboSpan" id="kobo.20.1"> function, which has</span><span><span class="koboSpan" id="kobo.21.1"> the allocated memory space</span></span><span class="koboSpan" id="kobo.22.1"> as its subject. </span><span class="koboSpan" id="kobo.22.2">In particular, the number of bytes of matrix </span><kbd><span class="koboSpan" id="kobo.23.1">a</span></kbd><span class="koboSpan" id="kobo.24.1">, as expressed by the </span><kbd><span class="koboSpan" id="kobo.25.1">a.nbytes</span></kbd><span><span class="koboSpan" id="kobo.26.1"> parameter, is as follows:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.27.1">a_gpu = cuda.mem_alloc(a.nbytes) </span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.28.1">After that, we can transfer the matrix from the host to the memory area, created specifically on the device by using the </span><kbd><span class="koboSpan" id="kobo.29.1">memcpy_htod</span></kbd><span class="koboSpan" id="kobo.30.1"> function:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.31.1">cuda.memcpy_htod(a_gpu, a) </span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.32.1">Inside the device, the </span><kbd><span><span class="koboSpan" id="kobo.33.1">doubleMatrix</span></span></kbd><span class="koboSpan" id="kobo.34.1"> kernel function will operate. </span><span class="koboSpan" id="kobo.34.2">Its purpose will be to multiply </span><span><span class="koboSpan" id="kobo.35.1">each element of the input matrix</span></span><span class="koboSpan" id="kobo.36.1"> by </span><kbd><span class="koboSpan" id="kobo.37.1">2</span></kbd><span class="koboSpan" id="kobo.38.1">. </span><span class="koboSpan" id="kobo.38.2">As you can see, the syntax of the </span><kbd><span class="koboSpan" id="kobo.39.1">doubleMatrix</span></kbd><span class="koboSpan" id="kobo.40.1"> function is C-like, while the </span><kbd><span class="koboSpan" id="kobo.41.1">SourceModule</span></kbd><span class="koboSpan" id="kobo.42.1"> statement is a real directive for the NVIDIA compiler (the </span><kbd><span class="koboSpan" id="kobo.43.1">nvcc</span></kbd><span class="koboSpan" id="kobo.44.1"> compiler), which creates a module that, in this case, </span><span><span class="koboSpan" id="kobo.45.1">consists </span></span><span class="koboSpan" id="kobo.46.1">of the </span><kbd><span class="koboSpan" id="kobo.47.1">doubleMatrix</span></kbd><span class="koboSpan" id="kobo.48.1"> function only:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.49.1">mod = SourceModule(""" 
  __global__ void doubles_matrix(float *a){ 
    int idx = threadIdx.x + threadIdx.y*4; 
    a[idx] *= 2;} 
  """)</span></pre>
<ol start="6">
<li><span class="koboSpan" id="kobo.50.1">With the </span><kbd><span class="koboSpan" id="kobo.51.1">func</span></kbd><span class="koboSpan" id="kobo.52.1"> </span><span><span class="koboSpan" id="kobo.53.1">parameter</span></span><span class="koboSpan" id="kobo.54.1">, </span><span><span class="koboSpan" id="kobo.55.1">we identify </span></span><span class="koboSpan" id="kobo.56.1">the </span><kbd><span class="koboSpan" id="kobo.57.1">doubleMatrix</span></kbd><span class="koboSpan" id="kobo.58.1"> function, which is contained in the </span><kbd><span class="koboSpan" id="kobo.59.1">mod</span></kbd><span class="koboSpan" id="kobo.60.1"> </span><span><span class="koboSpan" id="kobo.61.1">module:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.62.1">func = mod.get_function("doubles_matrix") </span></pre>
<ol start="7">
<li><span class="koboSpan" id="kobo.63.1">Finally, we run the kernel function. </span><span class="koboSpan" id="kobo.63.2">In order to successfully execute a kernel function on the device, the CUDA user must specify the input for the kernel and the size of the execution thread block. </span><span class="koboSpan" id="kobo.63.3">In the following case, the input is the </span><kbd><span class="koboSpan" id="kobo.64.1">a_gpu</span></kbd><span class="koboSpan" id="kobo.65.1"> matrix that was previously copied to the device, while the dimension of the thread block is </span><kbd><span class="koboSpan" id="kobo.66.1">(5,5,1)</span></kbd><span class="koboSpan" id="kobo.67.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.68.1">func(a_gpu, block=(5,5,1)) </span></pre>
<ol start="8">
<li><span class="koboSpan" id="kobo.69.1">Therefore, we allocate an area of memory of size equal to that of the input matrix </span><kbd><span class="koboSpan" id="kobo.70.1">a</span></kbd><span class="koboSpan" id="kobo.71.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.72.1">a_doubled = numpy.empty_like(a) </span></pre>
<ol start="9">
<li><span class="koboSpan" id="kobo.73.1">Then, we copy the contents of the memory area allocated to the device—that is, the </span><kbd><span class="koboSpan" id="kobo.74.1">a_gpu</span></kbd><span class="koboSpan" id="kobo.75.1"> matrix—to the previously defined memory area, </span><kbd><span class="koboSpan" id="kobo.76.1">a_doubled</span></kbd><span class="koboSpan" id="kobo.77.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.78.1">cuda.memcpy_dtoh(a_doubled, a_gpu) </span></pre>
<ol start="10">
<li><span><span class="koboSpan" id="kobo.79.1">Finally, w</span></span><span class="koboSpan" id="kobo.80.1">e print the contents of the input matrix </span><kbd><span class="koboSpan" id="kobo.81.1">a</span></kbd><span class="koboSpan" id="kobo.82.1"> and the output matrix in order to verify the quality of the implementation:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.83.1">print ("ORIGINAL MATRIX") 
print (a) 
print ("DOUBLED MATRIX AFTER PyCUDA EXECUTION") 
print (a_doubled) </span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How it works...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Let's start with looking at which libraries are imported for this example:</span></p>
<pre><span class="koboSpan" id="kobo.3.1">import PyCUDA.driver as CUDA 
import PyCUDA.autoinit 
from PyCUDA.compiler import SourceModule </span></pre>
<p><span class="koboSpan" id="kobo.4.1">In particular, the </span><kbd><span class="koboSpan" id="kobo.5.1">autoinit</span></kbd> <span><span class="koboSpan" id="kobo.6.1">import </span></span><span class="koboSpan" id="kobo.7.1">automatically identifies which GPU on our system is available for execution, while </span><kbd><span class="koboSpan" id="kobo.8.1">SourceModule</span></kbd><span class="koboSpan" id="kobo.9.1"> is the directive for the compiler of NVIDIA (</span><kbd><span class="koboSpan" id="kobo.10.1">nvcc</span></kbd><span class="koboSpan" id="kobo.11.1">) that allows us to identify the objects that must be compiled and uploaded to the device.</span></p>
<p><span class="koboSpan" id="kobo.12.1">Then, we build the 5 × 5 input matrix by using the </span><kbd><span class="koboSpan" id="kobo.13.1">numpy</span></kbd><span class="koboSpan" id="kobo.14.1"> library:</span></p>
<pre><span class="koboSpan" id="kobo.15.1">import numpy 
a = numpy.random.randn(5,5) </span></pre>
<p><span class="koboSpan" id="kobo.16.1">In this case, the elements in the matrix are converted to single-precision mode (since the graphics card on which this example is executed only supports single precision):</span></p>
<pre><span class="koboSpan" id="kobo.17.1">a = a.astype(numpy.float32) </span></pre>
<p><span class="koboSpan" id="kobo.18.1">Then, we copy the array from the host to the device, using the following two operations:</span></p>
<pre><span class="koboSpan" id="kobo.19.1">a_gpu = CUDA.mem_alloc(a.nbytes) 
CUDA.memcpy_htod(a_gpu, a) </span></pre>
<p><span class="koboSpan" id="kobo.20.1">Note that the device and host memory may never communicate during the execution of a kernel function. </span><span class="koboSpan" id="kobo.20.2">For this reason, in order to parallel execute the kernel function on the device, all input data relating to the kernel function must also be present in the memory of the device.</span></p>
<p><span class="koboSpan" id="kobo.21.1">It should also be noted that the </span><kbd><span class="koboSpan" id="kobo.22.1">a_gpu</span></kbd><span class="koboSpan" id="kobo.23.1"> matrix is linearized, that is, it is one-dimensional, and therefore we must manage it as such.</span></p>
<p><span class="koboSpan" id="kobo.24.1">Moreover, all these operations do not require kernel invocation. </span><span class="koboSpan" id="kobo.24.2">This means that they are made directly by the host.</span></p>
<p><span class="koboSpan" id="kobo.25.1">The </span><kbd><span class="koboSpan" id="kobo.26.1">SourceModule</span></kbd><span class="koboSpan" id="kobo.27.1"> entity allows the definition of the </span><kbd><span class="koboSpan" id="kobo.28.1">doubleMatrix</span></kbd><span><span class="koboSpan" id="kobo.29.1"> kernel function.</span></span><span class="koboSpan" id="kobo.30.1"> </span><kbd><span class="koboSpan" id="kobo.31.1">__global__</span></kbd><span class="koboSpan" id="kobo.32.1">, which is an</span><span><span class="koboSpan" id="kobo.33.1"> </span></span><kbd><span class="koboSpan" id="kobo.34.1">nvcc</span></kbd><span><span class="koboSpan" id="kobo.35.1"> directive, </span></span><span class="koboSpan" id="kobo.36.1">indicates that the </span><kbd><span class="koboSpan" id="kobo.37.1">doubleMatrix</span></kbd> <span><span class="koboSpan" id="kobo.38.1">function </span></span><span class="koboSpan" id="kobo.39.1">will be processed by the device:</span></p>
<pre><span class="koboSpan" id="kobo.40.1">mod = SourceModule(""" 
  __global__ void doubleMatrix(float *a) </span></pre>
<p><span class="koboSpan" id="kobo.41.1">Let's consider the kernel's body. </span><span><span class="koboSpan" id="kobo.42.1">The </span></span><kbd><span class="koboSpan" id="kobo.43.1">idx</span></kbd><span><span class="koboSpan" id="kobo.44.1"> parameter is the matrix index, which is identified by the </span></span><kbd><span class="koboSpan" id="kobo.45.1">threadIdx.x</span></kbd><span><span class="koboSpan" id="kobo.46.1"> and </span></span><kbd><span class="koboSpan" id="kobo.47.1">threadIdx.y</span></kbd><span><span class="koboSpan" id="kobo.48.1"> thread coordinates:</span></span></p>
<pre><span class="koboSpan" id="kobo.49.1">    int idx = threadIdx.x + threadIdx.y*4; 
    a[idx] *= 2; </span></pre>
<p><span class="koboSpan" id="kobo.50.1">Then, </span><kbd><span class="koboSpan" id="kobo.51.1">mod.get_function("doubleMatrix")</span></kbd><span class="koboSpan" id="kobo.52.1"> returns an identifier to the </span><kbd><span class="koboSpan" id="kobo.53.1">func</span></kbd><span class="koboSpan" id="kobo.54.1"> parameter:</span></p>
<pre><span class="koboSpan" id="kobo.55.1">func = mod.get_function("doubleMatrix ") </span></pre>
<p><span class="koboSpan" id="kobo.56.1">In order to execute the kernel, we need to configure the execution context. </span><span class="koboSpan" id="kobo.56.2">This means setting the three-dimensional structure of the threads that belong to the block grid by using the block parameter inside the </span><kbd><span class="koboSpan" id="kobo.57.1">func</span></kbd><span class="koboSpan" id="kobo.58.1"> call:</span></p>
<pre><span class="koboSpan" id="kobo.59.1">func(a_gpu, block = (5, 5, 1)) </span></pre>
<p><kbd><span class="koboSpan" id="kobo.60.1">block = (5, 5, 1)</span></kbd><span class="koboSpan" id="kobo.61.1"> tells us that we are calling a kernel function with the </span><kbd><span class="koboSpan" id="kobo.62.1">a_gpu</span></kbd><span class="koboSpan" id="kobo.63.1"> linearized input matrix and a single thread block of size </span><kbd><span class="koboSpan" id="kobo.64.1">5</span></kbd><span class="koboSpan" id="kobo.65.1"> (that is, </span><kbd><span class="koboSpan" id="kobo.66.1">5</span></kbd><span class="koboSpan" id="kobo.67.1"> threads) in the </span><em><span class="koboSpan" id="kobo.68.1">x</span></em><span class="koboSpan" id="kobo.69.1">-direction, </span><kbd><em><span class="koboSpan" id="kobo.70.1">5</span></em></kbd><span class="koboSpan" id="kobo.71.1"> threads in the </span><em><span class="koboSpan" id="kobo.72.1">y</span></em><span class="koboSpan" id="kobo.73.1">-direction, and 1 thread in the </span><em><span class="koboSpan" id="kobo.74.1">z</span></em><span class="koboSpan" id="kobo.75.1">-direction, which makes </span><em><span class="koboSpan" id="kobo.76.1">16</span></em><span class="koboSpan" id="kobo.77.1"> threads in total. </span><span class="koboSpan" id="kobo.77.2">Note that each thread executes the same kernel code (25 threads in total).</span></p>
<p><span class="koboSpan" id="kobo.78.1">After the computation in the GPU device, we use an array to store the results:</span></p>
<pre><span class="koboSpan" id="kobo.79.1">a_doubled = numpy.empty_like(a) 
CUDA.memcpy_dtoh(a_doubled, a_gpu) </span></pre>
<p><span class="koboSpan" id="kobo.80.1">To run the example, type</span><span><span class="koboSpan" id="kobo.81.1"> the following</span></span><span class="koboSpan" id="kobo.82.1"> on Command Prompt:</span></p>
<pre><strong><span class="koboSpan" id="kobo.83.1">C:\&gt;python heterogenousPycuda.py</span></strong></pre>
<p><span class="koboSpan" id="kobo.84.1">The output should be like this:</span></p>
<pre><strong><span class="koboSpan" id="kobo.85.1">ORIGINAL MATRIX</span></strong><br/><strong><span class="koboSpan" id="kobo.86.1">[[-0.59975582 1.93627465 0.65337795 0.13205571 -0.46468592]</span></strong><br/><strong><span class="koboSpan" id="kobo.87.1">[ 0.01441949 1.40946579 0.5343408 -0.46614054 -0.31727529]</span></strong><br/><strong><span class="koboSpan" id="kobo.88.1">[-0.06868593 1.21149373 -0.6035406 -1.29117763 0.47762445]</span></strong><br/><strong><span class="koboSpan" id="kobo.89.1">[ 0.36176383 -1.443097 1.21592784 -1.04906416 -1.18935871]</span></strong><br/><strong><span class="koboSpan" id="kobo.90.1">[-0.06960868 -1.44647694 -1.22041082 1.17092752 0.3686313 ]]</span><br/></strong><br/><strong><span class="koboSpan" id="kobo.91.1">DOUBLED MATRIX AFTER PyCUDA EXECUTION</span></strong><br/><strong><span class="koboSpan" id="kobo.92.1">[[-1.19951165 3.8725493 1.3067559 0.26411143 -0.92937183]</span></strong><br/><strong><span class="koboSpan" id="kobo.93.1">[ 0.02883899 2.81893158 1.0686816 -0.93228108 -0.63455057]</span></strong><br/><strong><span class="koboSpan" id="kobo.94.1">[-0.13737187 2.42298746 -1.2070812 -2.58235526 0.95524889]</span></strong><br/><strong><span class="koboSpan" id="kobo.95.1">[ 0.72352767 -2.886194 2.43185568 -2.09812832 -2.37871742]</span></strong><br/><strong><span class="koboSpan" id="kobo.96.1">[-0.13921736 -2.89295388 -2.44082164 2.34185504 0.73726263 ]]</span></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">There's more...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The key feature of CUDA that makes this programming model substantially different from other parallel models (normally used on CPUs) is that in order to be efficient, it requires thousands of threads to be active. </span><span class="koboSpan" id="kobo.2.2">This is made possible by the typical structure of GPUs, which use light threads and also allow the creation and modification of execution contexts in a very fast and efficient way.</span></p>
<p><span class="koboSpan" id="kobo.3.1">Note that the </span><span><span class="koboSpan" id="kobo.4.1">scheduling of </span></span><span class="koboSpan" id="kobo.5.1">threads is directly linked to the GPU architecture and its intrinsic parallelism. </span><span class="koboSpan" id="kobo.5.2">In fact, a block of threads is assigned to a single SM. </span><span class="koboSpan" id="kobo.5.3">Here, the threads are further divided into groups, called warps. </span><span class="koboSpan" id="kobo.5.4">The threads that belong to the same warp are managed by the </span><em><span class="koboSpan" id="kobo.6.1">warp scheduler</span></em><span class="koboSpan" id="kobo.7.1">. </span><span class="koboSpan" id="kobo.7.2">To take full advantage of the inherent parallelism of the SM, the threads of the same warp must execute the same instruction. </span><span class="koboSpan" id="kobo.7.3">If this condition does not occur, then we speak of </span><em><span class="koboSpan" id="kobo.8.1">threads divergence.</span></em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">See also</span></h1>
                </header>
            
            <article>
                
<ul>
<li><span class="koboSpan" id="kobo.2.1">The complete tutorial on using PyCUDA is available at the following site: </span><a href="https://documen.tician.de/pycuda/tutorial.html"><span class="koboSpan" id="kobo.3.1">https://documen.tician.de/pycuda/tutorial.html</span></a><span class="koboSpan" id="kobo.4.1">.</span></li>
<li><span class="koboSpan" id="kobo.5.1">To install PyCUDA on Windows 10, take a look </span><span><span class="koboSpan" id="kobo.6.1">at</span></span><span class="koboSpan" id="kobo.7.1"> the following link: </span><a href="https://github.com/kdkoadd/Win10-PyCUDA-Install"><span class="koboSpan" id="kobo.8.1">https://github.com/kdkoadd/Win10-PyCUDA-Install</span></a><span class="koboSpan" id="kobo.9.1">.</span><a href="https://github.com/kdkoadd/Win10-PyCUDA-Install"/></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Implementing memory management with PyCUDA</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">PyCUDA programs should respect the rules dictated by the structure and the internal organization of SM that impose constraints on thread performances. </span><span class="koboSpan" id="kobo.2.2">In fact, the knowledge and the correct use of various types of memory that the GPU makes available are fundamental in order to achieve maximum efficiency. </span><span class="koboSpan" id="kobo.2.3">In those GPU cards, enabled for CUDA use, there are four types of memory, which are as follows:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.3.1">Registers</span></strong><span class="koboSpan" id="kobo.4.1">: Each thread is assigned a memory register which only the assigned thread can access, even if the threads belong to the same block.</span></li>
<li><strong><span class="koboSpan" id="kobo.5.1">Shared memory</span></strong><span class="koboSpan" id="kobo.6.1">: Each block has its own shared memory between the threads that belong to it. </span><span class="koboSpan" id="kobo.6.2">Even this memory is extremely fast.</span></li>
<li><strong><span class="koboSpan" id="kobo.7.1">Constant memory</span></strong><span class="koboSpan" id="kobo.8.1">: All threads in a grid have constant access to the memory, but can </span><span><span class="koboSpan" id="kobo.9.1">only </span></span><span class="koboSpan" id="kobo.10.1">be accessed in reading. </span><span class="koboSpan" id="kobo.10.2">The data present in it persists for the entire duration of the application.</span></li>
<li class="CDPAlignLeft CDPAlign"><strong><span class="koboSpan" id="kobo.11.1">Global memory</span></strong><span class="koboSpan" id="kobo.12.1">: All the threads of the grid, and therefore all the kernels, have access to the global memory. </span><span class="koboSpan" id="kobo.12.2">Moreover, data persistence is exactly like a constant memory:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.13.1"><img src="assets/b43f49cb-fad9-4e64-8752-610540f62d30.png"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="hps"><span class="koboSpan" id="kobo.14.1">GPU memory model</span></span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Getting ready</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">For best performance, a PyCUDA program must, therefore, make the most of every type of memory. </span><span class="koboSpan" id="kobo.2.2">In particular, it must make the most of shared memory, minimizing access to memory on a global level.</span></p>
<p><span class="koboSpan" id="kobo.3.1">To do this, the problem domain is typically subdivided so that a single block of threads is able to execute its processing in a closed subset of data. In this way, the threads operating on the single block will all work together on the same shared memory area, optimizing access.</span></p>
<p><span class="koboSpan" id="kobo.4.1">The basic steps for each thread are as follows:</span></p>
<ol>
<li><span class="hps"><em><span class="koboSpan" id="kobo.5.1">Load</span></em><span class="koboSpan" id="kobo.6.1"> data from global memory to shared memory.</span></span></li>
<li><span class="hps"><em><span class="koboSpan" id="kobo.7.1">Synchronize</span></em><span class="koboSpan" id="kobo.8.1"> all threads of the block so that everyone can read safety positions and shared memory filled by other threads.</span></span></li>
<li><span class="hps"><em><span class="koboSpan" id="kobo.9.1">Process</span></em><span class="koboSpan" id="kobo.10.1"> the data of the shared memory. </span><span class="koboSpan" id="kobo.10.2">Making a new synchronization is necessary to ensure that the shared memory has been updated with the results.</span></span></li>
<li><span class="hps"><em><span class="koboSpan" id="kobo.11.1">Write</span></em><span class="koboSpan" id="kobo.12.1"> the results in global memory.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.13.1">To clarify this approach, </span><span><span class="koboSpan" id="kobo.14.1">in the following section, </span></span><span class="koboSpan" id="kobo.15.1">we will present an example based on the calculation of the product of two matrices.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How to do it...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The following code fragment shows the calculation of the product of two matrices, </span><em><span class="koboSpan" id="kobo.3.1">M×N</span></em><span class="koboSpan" id="kobo.4.1">, in the s</span><span><span class="koboSpan" id="kobo.5.1">tandard method, which is based on a</span></span><span class="koboSpan" id="kobo.6.1"> sequential approach. </span><span class="koboSpan" id="kobo.6.2">Each element of the output matrix, </span><kbd><span class="koboSpan" id="kobo.7.1">P</span></kbd><span class="koboSpan" id="kobo.8.1">, is obtained by taking a row element from matrix </span><kbd><span class="koboSpan" id="kobo.9.1">M</span></kbd><span class="koboSpan" id="kobo.10.1">, and a column element from matrix </span><kbd><span class="koboSpan" id="kobo.11.1">N</span></kbd><span class="koboSpan" id="kobo.12.1">:</span></p>
<pre><span class="koboSpan" id="kobo.13.1">void SequentialMatrixMultiplication(float*M,float *N,float *P, int width){ 
  for (int i=0; i&lt; width; ++i) 
      for(int j=0;j &lt; width; ++j) { 
          float sum = 0; 
          for (int k = 0 ; k &lt; width; ++k) { 
              float a = M[I * width + k]; 
              float b = N[k * width + j]; 
              sum += a * b; 
                     } 
         P[I * width + j] = sum; 
    } 
} 
P[I * width + j] = sum; </span></pre>
<p><span class="koboSpan" id="kobo.14.1">In this case, if each thread had been given the task of calculating each element of the matrix, then access to the memory would have dominated the execution time of the algorithm.</span></p>
<p><span class="koboSpan" id="kobo.15.1">What we can do is rely on a block of threads to calculate one output submatrix at a time. </span><span class="koboSpan" id="kobo.15.2">In this way, the threads that access the same memory block cooperate to optimize accesses, thereby minimizing the total calculation time:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.16.1">The first step is to load all the necessary modules to implement the algorithm:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.17.1">import numpy as np 
from pycuda import driver, compiler, gpuarray, tools </span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.18.1">Then, initialize the GPU device:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.19.1">import pycuda.autoinit </span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.20.1">We implement </span><kbd><span class="koboSpan" id="kobo.21.1">kernel_code_template</span></kbd><span class="koboSpan" id="kobo.22.1">, which implements the product of two matrices that are respectively indicated with </span><kbd><span class="koboSpan" id="kobo.23.1">a</span></kbd><span class="koboSpan" id="kobo.24.1"> and </span><kbd><span class="koboSpan" id="kobo.25.1">b</span></kbd><span class="koboSpan" id="kobo.26.1">, while the resulting matrix is indicated with the parameter </span><kbd><span class="koboSpan" id="kobo.27.1">c</span></kbd><span class="koboSpan" id="kobo.28.1">. </span><span class="koboSpan" id="kobo.28.2">Note that the </span><kbd><span class="koboSpan" id="kobo.29.1">MATRIX_SIZE</span></kbd><span class="koboSpan" id="kobo.30.1"> parameter will be defined in the next step:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.31.1">kernel_code_template = """ 
__global__ void MatrixMulKernel(float *a, float *b, float *c) 
{ 
    int tx = threadIdx.x; 
    int ty = threadIdx.y; 
    float Pvalue = 0; 
    for (int k = 0; k &lt; %(MATRIX_SIZE)s; ++k) { 
        float Aelement = a[ty * %(MATRIX_SIZE)s + k]; 
        float Belement = b[k * %(MATRIX_SIZE)s + tx]; 
        Pvalue += Aelement * Belement; 
    } 
    c[ty * %(MATRIX_SIZE)s + tx] = Pvalue; 
}""" </span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.32.1">The following parameter will be used to set the dimensions of the matrices. </span><span class="koboSpan" id="kobo.32.2">In this case, the size is 5 × 5:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.33.1">MATRIX_SIZE = 5</span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.34.1">We define the two input matrices, </span><kbd><span class="koboSpan" id="kobo.35.1">a_cpu</span></kbd><span class="koboSpan" id="kobo.36.1"> and </span><kbd><span class="koboSpan" id="kobo.37.1">b_cpu</span></kbd><span class="koboSpan" id="kobo.38.1">, that will contain random floating-point values:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.39.1">a_cpu = np.random.randn(MATRIX_SIZE, MATRIX_SIZE).astype(np.float32) 
b_cpu = np.random.randn(MATRIX_SIZE, MATRIX_SIZE).astype(np.float32)</span></pre>
<ol start="6">
<li><span class="koboSpan" id="kobo.40.1">Then, we calculate the product of the two matrices, </span><kbd><span class="koboSpan" id="kobo.41.1">a</span></kbd><span class="koboSpan" id="kobo.42.1"> and </span><kbd><span class="koboSpan" id="kobo.43.1">b</span></kbd><span class="koboSpan" id="kobo.44.1">, on the host device:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.45.1">c_cpu = np.dot(a_cpu, b_cpu) </span></pre>
<ol start="7">
<li><span class="koboSpan" id="kobo.46.1">We allocate memory areas on the device (GPU), equal in </span><span><span class="koboSpan" id="kobo.47.1">size </span></span><span class="koboSpan" id="kobo.48.1">to the input matrices:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.49.1">a_gpu = gpuarray.to_gpu(a_cpu)  
b_gpu = gpuarray.to_gpu(b_cpu) </span></pre>
<ol start="8">
<li><span class="koboSpan" id="kobo.50.1">We allocate a memory area on the GPU, equal in size to the output matrix resulting from the product of the two matrices. </span><span class="koboSpan" id="kobo.50.2">In this case, the resulting matrix, </span><kbd><span class="koboSpan" id="kobo.51.1">c_gpu</span></kbd><span class="koboSpan" id="kobo.52.1">, will have a size of 5 × 5:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.53.1">c_gpu = gpuarray.empty((MATRIX_SIZE, MATRIX_SIZE), np.float32) </span></pre>
<ol start="9">
<li><span class="koboSpan" id="kobo.54.1">The following </span><kbd><span class="koboSpan" id="kobo.55.1">kernel_code</span></kbd><span class="koboSpan" id="kobo.56.1"> redefines </span><kbd><span class="koboSpan" id="kobo.57.1">kernel_code_template</span></kbd><span class="koboSpan" id="kobo.58.1">, but with the </span><kbd><span class="koboSpan" id="kobo.59.1">matrix_size</span></kbd> <span><span class="koboSpan" id="kobo.60.1">parameter </span></span><span class="koboSpan" id="kobo.61.1">set:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.62.1">kernel_code = kernel_code_template % { 
    'MATRIX_SIZE': MATRIX_SIZE} </span></pre>
<ol start="10">
<li><span class="koboSpan" id="kobo.63.1">The </span><kbd><span class="koboSpan" id="kobo.64.1">SourceModule</span></kbd><span class="koboSpan" id="kobo.65.1"> directive tells </span><kbd><span class="koboSpan" id="kobo.66.1">nvcc</span></kbd><span><span class="koboSpan" id="kobo.67.1"> </span></span><span class="koboSpan" id="kobo.68.1">(</span><em><span class="koboSpan" id="kobo.69.1">NVIDIA CUDA Compiler)</span></em><span class="koboSpan" id="kobo.70.1"> that it will have to create a module—that is, a collection of functions—containing the</span><span><span class="koboSpan" id="kobo.71.1"> previously defined</span></span> <kbd><span class="koboSpan" id="kobo.72.1">kernel_code</span></kbd><span class="koboSpan" id="kobo.73.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.74.1">mod = compiler.SourceModule(kernel_code) </span></pre>
<ol start="11">
<li><span class="koboSpan" id="kobo.75.1">Finally, we take the </span><kbd><span class="koboSpan" id="kobo.76.1">MatrixMulKernel</span></kbd><span class="koboSpan" id="kobo.77.1"> function </span><span><span class="koboSpan" id="kobo.78.1">from the module, </span></span><kbd><span class="koboSpan" id="kobo.79.1">mod</span></kbd><span class="koboSpan" id="kobo.80.1">, to which we give the name </span><kbd><span class="koboSpan" id="kobo.81.1">matrixmul</span></kbd><span class="koboSpan" id="kobo.82.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.83.1">matrixmul = mod.get_function("MatrixMulKernel")</span></pre>
<ol start="12">
<li><span class="koboSpan" id="kobo.84.1">We execute the product between two matrices, </span><kbd><span class="koboSpan" id="kobo.85.1">a_gpu</span></kbd><span class="koboSpan" id="kobo.86.1"> and </span><kbd><span class="koboSpan" id="kobo.87.1">b_gpu</span></kbd><span class="koboSpan" id="kobo.88.1">, resulting in the </span><kbd><span class="koboSpan" id="kobo.89.1">c_gpu</span></kbd><span class="koboSpan" id="kobo.90.1"> matrix. </span><span class="koboSpan" id="kobo.90.2">The size of the thread block is defined as </span><kbd><span class="koboSpan" id="kobo.91.1">MATRIX_SIZE, MATRIX_SIZE, 1</span></kbd><span class="koboSpan" id="kobo.92.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.93.1">matrixmul( 
    a_gpu, b_gpu,  
    c_gpu,  
    block = (MATRIX_SIZE, MATRIX_SIZE, 1))</span></pre>
<ol start="13">
<li><span class="koboSpan" id="kobo.94.1">Print the input matrices:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.95.1">print ("-" * 80) 
print ("Matrix A (GPU):") 
print (a_gpu.get()) 
print ("-" * 80) 
print ("Matrix B (GPU):") 
print (b_gpu.get()) 
print ("-" * 80) 
print ("Matrix C (GPU):") 
print (c_gpu.get()) </span></pre>
<ol start="14">
<li><span class="koboSpan" id="kobo.96.1">To check the validity of the calculation performed on the GPU, we compare the results of the two implementations, which are the one performed on the host device (CPU) and </span><span><span class="koboSpan" id="kobo.97.1">the one performed </span></span><span class="koboSpan" id="kobo.98.1">on the device (GPU). </span><span class="koboSpan" id="kobo.98.2">To do this, we use the </span><kbd><span class="koboSpan" id="kobo.99.1">numpy allclose</span></kbd><span class="koboSpan" id="kobo.100.1"> directive, which verifies that two element-wise arrays are equal within a tolerance equal to </span><kbd><span class="koboSpan" id="kobo.101.1">1e-05</span></kbd><span class="koboSpan" id="kobo.102.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.103.1">np.allclose(c_cpu, c_gpu.get()) </span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How it works...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Let's consider the PyCUDA programming workflow. </span><span class="koboSpan" id="kobo.2.2">Let's prepare the input matrix, the output matrix, and where to store the results:</span></p>
<pre><span class="koboSpan" id="kobo.3.1">MATRIX_SIZE = 5 
a_cpu = np.random.randn(MATRIX_SIZE, MATRIX_SIZE).astype(np.float32) 
b_cpu = np.random.randn(MATRIX_SIZE, MATRIX_SIZE).astype(np.float32) 
c_cpu = np.dot(a_cpu, b_cpu) </span></pre>
<p><span class="koboSpan" id="kobo.4.1">Then, we transfer these matrices to the GPU device by using the </span><kbd><span class="koboSpan" id="kobo.5.1">gpuarray.to_gpu()</span></kbd> <span><span class="koboSpan" id="kobo.6.1">PyCUDA function:</span></span></p>
<pre><span class="koboSpan" id="kobo.7.1">a_gpu = gpuarray.to_gpu(a_cpu)  
b_gpu = gpuarray.to_gpu(b_cpu) 
c_gpu = gpuarray.empty((MATRIX_SIZE, MATRIX_SIZE), np.float32) </span></pre>
<p><span class="koboSpan" id="kobo.8.1">The core of the algorithm is the following kernel function. </span><span class="koboSpan" id="kobo.8.2">Let's remark that the </span><kbd><span class="koboSpan" id="kobo.9.1">__global__</span></kbd><span class="koboSpan" id="kobo.10.1"> keyword specifies that this function is a kernel function, which means that it will be executed by the device (GPU) following a call from the host code (CPU):</span></p>
<pre><span class="koboSpan" id="kobo.11.1">__global__ void MatrixMulKernel(float *a, float *b, float *c){</span><br/><span class="koboSpan" id="kobo.12.1">    int tx = threadIdx.x;</span><br/><span class="koboSpan" id="kobo.13.1">    int ty = threadIdx.y;</span><br/><span class="koboSpan" id="kobo.14.1">    float Pvalue = 0;</span><br/><span class="koboSpan" id="kobo.15.1">    for (int k = 0; k &lt; %(MATRIX_SIZE)s; ++k) {</span><br/><span class="koboSpan" id="kobo.16.1">        float Aelement = a[ty * %(MATRIX_SIZE)s + k];</span><br/><span class="koboSpan" id="kobo.17.1">        float Belement = b[k * %(MATRIX_SIZE)s + tx];</span><br/><span class="koboSpan" id="kobo.18.1">        Pvalue += Aelement * Belement;}</span><br/><span class="koboSpan" id="kobo.19.1">    c[ty * %(MATRIX_SIZE)s + tx] = Pvalue;</span><br/><span class="koboSpan" id="kobo.20.1">}</span></pre>
<p><kbd><span class="koboSpan" id="kobo.21.1">threadIdx.x</span></kbd><span class="koboSpan" id="kobo.22.1"> and </span><kbd><span class="koboSpan" id="kobo.23.1">threadIdy.y</span></kbd><span class="koboSpan" id="kobo.24.1"> are coordinates that allow the identification of the threads in the grid of two-dimensional blocks. </span><span class="koboSpan" id="kobo.24.2">Note that the threads within the grid block execute the same kernel code but on different pieces of data. </span><span class="koboSpan" id="kobo.24.3">If we compare the parallel version with the sequential one, then we immediately notice that the cycle indexes, </span><em><span class="koboSpan" id="kobo.25.1">i</span></em><span class="koboSpan" id="kobo.26.1"> and </span><em><span class="koboSpan" id="kobo.27.1">j</span></em><span class="koboSpan" id="kobo.28.1">, have been replaced by the </span><kbd><span class="koboSpan" id="kobo.29.1">threadIdx.x</span></kbd><span class="koboSpan" id="kobo.30.1"> and </span><kbd><span class="koboSpan" id="kobo.31.1">threadIdx.y</span></kbd><span class="koboSpan" id="kobo.32.1"> indexes.</span></p>
<p><span class="koboSpan" id="kobo.33.1">This means that in the parallel version, we will have only one iteration of the cycle. </span><span class="koboSpan" id="kobo.33.2">In fact, the </span><kbd><span class="koboSpan" id="kobo.34.1">MatrixMulKernel</span></kbd><span class="koboSpan" id="kobo.35.1"> </span><span><span class="koboSpan" id="kobo.36.1">kernel </span></span><span class="koboSpan" id="kobo.37.1">will be executed on a grid of dimensions of 5 × 5 parallel threads.</span></p>
<p><span class="koboSpan" id="kobo.38.1">This condition is expressed in the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.39.1"><img src="assets/65aa99d1-d699-4329-883c-543cb7ef15de.png" style="width:44.08em;height:51.33em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.40.1">Grid and block of thread organization for the example</span></div>
<p><span class="koboSpan" id="kobo.41.1">Then, we verify the product computation just by comparing the two resulting matrices:</span></p>
<pre><span class="koboSpan" id="kobo.42.1">np.allclose(c_cpu, c_gpu.get())</span></pre>
<p><span class="koboSpan" id="kobo.43.1">The output is as follows:</span></p>
<pre><strong><span class="koboSpan" id="kobo.44.1">C:\&gt;python memManagementPycuda.py</span></strong><br/><br/><strong><span class="koboSpan" id="kobo.45.1">---------------------------------------------------------------------</span></strong><br/><strong><span class="koboSpan" id="kobo.46.1">Matrix A (GPU):</span></strong><br/><strong><span class="koboSpan" id="kobo.47.1">[[ 0.90780383 -0.4782407 0.23222363 -0.63184392 1.05509627]</span></strong><br/><strong><span class="koboSpan" id="kobo.48.1"> [-1.27266967 -1.02834761 -0.15528528 -0.09468858 1.037099 ]</span></strong><br/><strong><span class="koboSpan" id="kobo.49.1"> [-0.18135822 -0.69884419 0.29881889 -1.15969539 1.21021318]</span></strong><br/><strong><span class="koboSpan" id="kobo.50.1"> [ 0.20939326 -0.27155793 -0.57454145 0.1466181 1.84723163]</span></strong><br/><strong><span class="koboSpan" id="kobo.51.1"> [ 1.33780348 -0.42343542 -0.50257754 -0.73388749 -1.883829 ]]</span></strong><br/><strong><span class="koboSpan" id="kobo.52.1">---------------------------------------------------------------------</span></strong><br/><strong><span class="koboSpan" id="kobo.53.1">Matrix B (GPU):</span></strong><br/><strong><span class="koboSpan" id="kobo.54.1">[[ 0.04523897 0.99969769 -1.04473436 1.28909719 1.10332143]</span></strong><br/><strong><span class="koboSpan" id="kobo.55.1"> [-0.08900332 -1.3893919 0.06948703 -0.25977209 -0.49602833]</span></strong><br/><strong><span class="koboSpan" id="kobo.56.1"> [-0.6463753 -1.4424541 -0.81715286 0.67685211 -0.94934392]</span></strong><br/><strong><span class="koboSpan" id="kobo.57.1"> [ 0.4485206 -0.77086055 -0.16582981 0.08478995 1.26223004]</span></strong><br/><strong><span class="koboSpan" id="kobo.58.1"> [-0.79841441 -0.16199949 -0.35969591 -0.46809086 0.20455229]]</span></strong><br/><strong><span class="koboSpan" id="kobo.59.1">---------------------------------------------------------------------</span></strong><br/><strong><span class="koboSpan" id="kobo.60.1">Matrix C (GPU):</span></strong><br/><strong><span class="koboSpan" id="kobo.61.1">[[-1.19226956 1.55315971 -1.44614291 0.90420711 0.43665022]</span></strong><br/><strong><span class="koboSpan" id="kobo.62.1"> [-0.73617989 0.28546685 1.02769876 -1.97204924 -0.65403283]</span></strong><br/><strong><span class="koboSpan" id="kobo.63.1"> [-1.62555301 1.05654192 -0.34626681 -0.51481217 -1.35338223]</span></strong><br/><strong><span class="koboSpan" id="kobo.64.1"> [-1.0040834 1.00310731 -0.4568972 -0.90064859 1.47408712]</span></strong><br/><strong><span class="koboSpan" id="kobo.65.1"> [ 1.59797418 3.52156591 -0.21708387 2.31396151 0.85150564]]</span></strong><br/><strong><span class="koboSpan" id="kobo.66.1">---------------------------------------------------------------------</span></strong><br/><br/><strong><span class="koboSpan" id="kobo.67.1">TRUE</span></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">There's more...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The data allocated in shared memory has limited visibility in the single-threaded block. </span><span class="koboSpan" id="kobo.2.2">It is easy to see that the </span><span><span class="koboSpan" id="kobo.3.1">PyCUDA </span></span><span class="koboSpan" id="kobo.4.1">programming model adapts to specific classes of applications.</span></p>
<p><span class="koboSpan" id="kobo.5.1">In particular, the features that these applications must present concern the presence of many mathematical operations, with a high degree of data parallelism (that is, the same sequence of operations being repeated on large amounts of data).</span></p>
<p><span class="koboSpan" id="kobo.6.1">The application fields that possess these characteristics all </span><span><span class="koboSpan" id="kobo.7.1">belong </span></span><span class="koboSpan" id="kobo.8.1">to the following sciences: cryptography, computational chemistry, and image and signal analysis.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">See also</span></h1>
                </header>
            
            <article>
                
<ul>
<li><span class="koboSpan" id="kobo.2.1">More examples of using Py</span><span><span class="koboSpan" id="kobo.3.1">CUDA</span></span> <span><span class="koboSpan" id="kobo.4.1">can be found</span></span><span class="koboSpan" id="kobo.5.1"> at </span><a href="https://github.com/zamorays/miniCursoPycuda"><span class="koboSpan" id="kobo.6.1">https://github.com/zamorays/miniCursoPycuda</span></a><span class="koboSpan" id="kobo.7.1">.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Introducing PyOpenCL</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">PyOpenCL is a sister project to PyCUDA. </span><span class="koboSpan" id="kobo.2.2">It is a binding library that provides full access to OpenCL's API from Python and is also by Andreas Klöckner. </span><span class="koboSpan" id="kobo.2.3">It features many of the same concepts as PyCUDA, including cleanup for out-of-scope objects, partial abstraction over data structures, and error handling, all with minimal overhead. </span><span class="koboSpan" id="kobo.2.4">The project is available under the MIT license; its documentation is very good and plenty of guides and tutorials can be found online.</span></p>
<p><span class="koboSpan" id="kobo.3.1">The main focus of PyOpenCL is to provide a lightweight connection between Python and OpenCL, but it also includes support for templates and metaprograms. </span><span class="koboSpan" id="kobo.3.2">The flow of a PyOpenCL program is almost exactly the same as a C or C++ program for OpenCL. </span><span class="koboSpan" id="kobo.3.3">The host program prepares the call of the device program, launches it, and then waits for the result.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Getting ready</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The main reference for the PyOpenCL installation is the </span><span><span class="koboSpan" id="kobo.3.1">Andreas Klöckner</span></span><span class="koboSpan" id="kobo.4.1"> home page: </span><a href="https://mathema.tician.de/software/pyopencl/"><span class="koboSpan" id="kobo.5.1">https://mathema.tician.de/software/pyopencl/</span></a><span class="koboSpan" id="kobo.6.1">.</span></p>
<p><span class="koboSpan" id="kobo.7.1">If you are using Anaconda, then it is advisable to perform the following steps:</span></p>
<ol>
<li><span><span class="koboSpan" id="kobo.8.1">Install the late</span></span><span><span class="koboSpan" id="kobo.9.1">s</span></span><span><span class="koboSpan" id="kobo.10.1">t Anaconda distribution with Python 3.7 from the following link: </span><a href="https://www.anaconda.com/distribution/#download-section"><span class="koboSpan" id="kobo.11.1">https://www.anaconda.com/distribution/#download-section</span></a><span class="koboSpan" id="kobo.12.1">. </span></span><span><span><span class="koboSpan" id="kobo.13.1">For this section, the Anaconda 2019.07 for Windows Installer has been installed.</span></span></span></li>
<li><span><span class="koboSpan" id="kobo.14.1">Get the </span></span><span><span class="koboSpan" id="kobo.15.1">PyOpenCL </span></span><span class="koboSpan" id="kobo.16.1">prebuilt binary</span><span><span class="koboSpan" id="kobo.17.1"> from Christoph Gohlke from this link: </span><a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/"><span class="koboSpan" id="kobo.18.1">https://www.lfd.uci.edu/~gohlke/pythonlibs/</span></a><span class="koboSpan" id="kobo.19.1">. </span><span class="koboSpan" id="kobo.19.2">Select the right combination of OS and CPython versions. </span><span class="koboSpan" id="kobo.19.3">Here, we use </span><kbd><span class="koboSpan" id="kobo.20.1">pyopencl-2019.1+cl12-cp37-cp37m-win_amd64.whl</span></kbd></span><span class="koboSpan" id="kobo.21.1">.</span></li>
</ol>
<ol start="3">
<li><span><span class="koboSpan" id="kobo.22.1">Use </span><kbd><span class="koboSpan" id="kobo.23.1">pip</span></kbd><span class="koboSpan" id="kobo.24.1"> to install the previous package. </span><span class="koboSpan" id="kobo.24.2">Simply type this in your Anaconda Prompt: </span></span></li>
</ol>
<pre style="padding-left: 60px"><span><strong><span class="koboSpan" id="kobo.25.1">(base) C:\&gt; pip install &lt;directory&gt;\pyopencl-2019.1+cl12-cp37-cp37m-win_amd64.whl</span></strong><br/></span></pre>
<p style="padding-left: 60px"><span><kbd><span class="koboSpan" id="kobo.26.1">&lt;directory&gt;</span></kbd><span class="koboSpan" id="kobo.27.1"> is the folder where the PyOpenCL package is located.</span></span></p>
<p><span class="koboSpan" id="kobo.28.1">Moreover, the following notation indicates that we are operating on the Anaconda Prompt:</span></p>
<pre><span><strong><span class="koboSpan" id="kobo.29.1">(base) C:\&gt;</span></strong></span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How to do it...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the following example, we will use a function of PyOpenCL that allows us to enumerate the features of the GPU on which it will operate.</span></p>
<p><span class="koboSpan" id="kobo.3.1">The code we implement is very simple and logical:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.4.1">In the first step, we import the </span><kbd><span class="koboSpan" id="kobo.5.1">pyopencl</span></kbd><span class="koboSpan" id="kobo.6.1"> </span><span><span class="koboSpan" id="kobo.7.1">library:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.8.1">import pyopencl as cl</span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.9.1">We build a function whose output will provide us with the characteristics of the GPU hardware in use:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.10.1">def print_device_info() :</span><br/><span class="koboSpan" id="kobo.11.1">    print('\n' + '=' * 60 + '\nOpenCL Platforms and Devices')</span><br/><span class="koboSpan" id="kobo.12.1">    for platform in cl.get_platforms():</span><br/><span class="koboSpan" id="kobo.13.1">        print('=' * 60)</span><br/><span class="koboSpan" id="kobo.14.1">        print('Platform - Name: ' + platform.name)</span><br/><span class="koboSpan" id="kobo.15.1">        print('Platform - Vendor: ' + platform.vendor)</span><br/><span class="koboSpan" id="kobo.16.1">        print('Platform - Version: ' + platform.version)</span><br/><span class="koboSpan" id="kobo.17.1">        print('Platform - Profile: ' + platform.profile)</span><br/><br/><span class="koboSpan" id="kobo.18.1">        for device in platform.get_devices():</span><br/><span class="koboSpan" id="kobo.19.1">            print(' ' + '-' * 56)</span><br/><span class="koboSpan" id="kobo.20.1">            print(' Device - Name: ' \</span><br/><span class="koboSpan" id="kobo.21.1">                  + device.name)</span><br/><span class="koboSpan" id="kobo.22.1">            print(' Device - Type: ' \</span><br/><span class="koboSpan" id="kobo.23.1">                  + cl.device_type.to_string(device.type))</span><br/><span class="koboSpan" id="kobo.24.1">            print(' Device - Max Clock Speed: {0} Mhz'\</span><br/><span class="koboSpan" id="kobo.25.1">                  .format(device.max_clock_frequency))</span><br/><span class="koboSpan" id="kobo.26.1">            print(' Device - Compute Units: {0}'\</span><br/><span class="koboSpan" id="kobo.27.1">                  .format(device.max_compute_units))</span><br/><span class="koboSpan" id="kobo.28.1">            print(' Device - Local Memory: {0:.0f} KB'\</span><br/><span class="koboSpan" id="kobo.29.1">                  .format(device.local_mem_size/1024.0))</span><br/><span class="koboSpan" id="kobo.30.1">            print(' Device - Constant Memory: {0:.0f} KB'\</span><br/><span class="koboSpan" id="kobo.31.1">                  .format(device.max_constant_buffer_size/1024.0))</span><br/><span class="koboSpan" id="kobo.32.1">            print(' Device - Global Memory: {0:.0f} GB'\</span><br/><span class="koboSpan" id="kobo.33.1">                  .format(device.global_mem_size/1073741824.0))</span><br/><span class="koboSpan" id="kobo.34.1">            print(' Device - Max Buffer/Image Size: {0:.0f} MB'\</span><br/><span class="koboSpan" id="kobo.35.1">                  .format(device.max_mem_alloc_size/1048576.0))</span><br/><span class="koboSpan" id="kobo.36.1">            print(' Device - Max Work Group Size: {0:.0f}'\</span><br/><span class="koboSpan" id="kobo.37.1">                  .format(device.max_work_group_size))</span><br/><span class="koboSpan" id="kobo.38.1">    print('\n')</span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.39.1">So, we implement the </span><kbd><span class="koboSpan" id="kobo.40.1">main</span></kbd><span class="koboSpan" id="kobo.41.1"> function, which calls the </span><span><span class="koboSpan" id="kobo.42.1">previously implemented</span></span><span class="koboSpan" id="kobo.43.1"> </span><kbd><span class="koboSpan" id="kobo.44.1">print_device_info</span></kbd><span class="koboSpan" id="kobo.45.1"> </span><span><span class="koboSpan" id="kobo.46.1">function</span></span><span class="koboSpan" id="kobo.47.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.48.1">if __name__ == "__main__":</span><br/><span class="koboSpan" id="kobo.49.1">    print_device_info()</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How it works...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The following command is used to import the </span><kbd><span class="koboSpan" id="kobo.3.1">pyopencl</span></kbd><span class="koboSpan" id="kobo.4.1"> library:</span></p>
<pre><span class="koboSpan" id="kobo.5.1">import pyopencl as cl</span></pre>
<p><span class="koboSpan" id="kobo.6.1">This allows us to use the </span><strong><kbd><span class="koboSpan" id="kobo.7.1">get_platforms</span></kbd><span class="koboSpan" id="kobo.8.1"> </span></strong><span class="koboSpan" id="kobo.9.1">method, which returns a list of platform instances, that is, a list of devices in the system:</span></p>
<pre><span class="koboSpan" id="kobo.10.1">for platform in cl.get_platforms():</span></pre>
<p><span class="koboSpan" id="kobo.11.1">Then, for each device found, the following main features are shown:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.12.1">Name and device type</span></li>
<li><span class="koboSpan" id="kobo.13.1">Max clock speed</span></li>
<li><span class="koboSpan" id="kobo.14.1">Compute units</span></li>
<li><span class="koboSpan" id="kobo.15.1">Local/constant/global memory</span></li>
</ul>
<p><span class="koboSpan" id="kobo.16.1">The output for this example is as follows:</span></p>
<pre><strong><span class="koboSpan" id="kobo.17.1">(base) C:\&gt;python deviceInfoPyopencl.py</span></strong><br/><br/><strong><span class="koboSpan" id="kobo.18.1">=============================================================</span><br/><span class="koboSpan" id="kobo.19.1">OpenCL Platforms and Devices</span><br/><span class="koboSpan" id="kobo.20.1">============================================================</span><br/><span class="koboSpan" id="kobo.21.1">Platform - Name: NVIDIA CUDA</span><br/><span class="koboSpan" id="kobo.22.1">Platform - Vendor: NVIDIA Corporation</span><br/><span class="koboSpan" id="kobo.23.1">Platform - Version: OpenCL 1.2 CUDA 10.1.152</span><br/><span class="koboSpan" id="kobo.24.1">Platform - Profile: FULL_PROFILE</span><br/><span class="koboSpan" id="kobo.25.1">    --------------------------------------------------------</span><br/><span class="koboSpan" id="kobo.26.1">    Device - Name: GeForce 840M</span><br/><span class="koboSpan" id="kobo.27.1">    Device - Type: GPU</span><br/><span class="koboSpan" id="kobo.28.1">    Device - Max Clock Speed: 1124 Mhz</span><br/><span class="koboSpan" id="kobo.29.1">    Device - Compute Units: 3</span><br/><span class="koboSpan" id="kobo.30.1">    Device - Local Memory: 48 KB</span><br/><span class="koboSpan" id="kobo.31.1">    Device - Constant Memory: 64 KB</span><br/><span class="koboSpan" id="kobo.32.1">    Device - Global Memory: 2 GB</span><br/><span class="koboSpan" id="kobo.33.1">    Device - Max Buffer/Image Size: 512 MB</span><br/><span class="koboSpan" id="kobo.34.1">    Device - Max Work Group Size: 1024</span><br/><span class="koboSpan" id="kobo.35.1">============================================================</span><br/><span class="koboSpan" id="kobo.36.1">Platform - Name: Intel(R) OpenCL</span><br/><span class="koboSpan" id="kobo.37.1">Platform - Vendor: Intel(R) Corporation</span><br/><span class="koboSpan" id="kobo.38.1">Platform - Version: OpenCL 2.0</span><br/><span class="koboSpan" id="kobo.39.1">Platform - Profile: FULL_PROFILE</span><br/><span class="koboSpan" id="kobo.40.1">    --------------------------------------------------------</span><br/><span class="koboSpan" id="kobo.41.1">    Device - Name: Intel(R) HD Graphics 5500</span><br/><span class="koboSpan" id="kobo.42.1">    Device - Type: GPU</span><br/><span class="koboSpan" id="kobo.43.1">    Device - Max Clock Speed: 950 Mhz</span><br/><span class="koboSpan" id="kobo.44.1">    Device - Compute Units: 24</span><br/><span class="koboSpan" id="kobo.45.1">    Device - Local Memory: 64 KB</span><br/><span class="koboSpan" id="kobo.46.1">    Device - Constant Memory: 64 KB</span><br/><span class="koboSpan" id="kobo.47.1">    Device - Global Memory: 3 GB</span><br/><span class="koboSpan" id="kobo.48.1">    Device - Max Buffer/Image Size: 808 MB</span><br/><span class="koboSpan" id="kobo.49.1">    Device - Max Work Group Size: 256</span><br/><span class="koboSpan" id="kobo.50.1">    --------------------------------------------------------</span><br/><span class="koboSpan" id="kobo.51.1">    Device - Name: Intel(R) Core(TM) i7-5500U CPU @ 2.40GHz</span><br/><span class="koboSpan" id="kobo.52.1">    Device - Type: CPU</span><br/><span class="koboSpan" id="kobo.53.1">    Device - Max Clock Speed: 2400 Mhz</span><br/><span class="koboSpan" id="kobo.54.1">    Device - Compute Units: 4</span><br/><span class="koboSpan" id="kobo.55.1">    Device - Local Memory: 32 KB</span><br/><span class="koboSpan" id="kobo.56.1">    Device - Constant Memory: 128 KB</span><br/><span class="koboSpan" id="kobo.57.1">    Device - Global Memory: 8 GB</span><br/><span class="koboSpan" id="kobo.58.1">    Device - Max Buffer/Image Size: 2026 MB</span><br/><span class="koboSpan" id="kobo.59.1">    Device - Max Work Group Size: 8192</span></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">There's more...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">OpenCL is currently managed by the Khronos Group, a non-profit consortium of companies that collaborate in defining the specifications of this (and many other) standards and compliance parameters for the creation of OpenCL-specific </span><span><span class="koboSpan" id="kobo.3.1">drivers</span></span><span><span class="koboSpan" id="kobo.4.1"> </span></span><span class="koboSpan" id="kobo.5.1">for each type of platform.</span></p>
<p><span class="koboSpan" id="kobo.6.1">These drivers also provide functions for compiling programs that are written in the kernel language: these are converted into programs in some form of intermediate language that is usually vendor-specific, and then executed on the reference architectures.</span></p>
<p class="mce-root"><span class="koboSpan" id="kobo.7.1">More info on OpenCL can be found at the following link: </span><a href="https://www.khronos.org/registry/OpenCL/"><span class="koboSpan" id="kobo.8.1">https://www.khronos.org/registry/OpenCL/</span></a><span class="koboSpan" id="kobo.9.1">.</span><a href="https://www.khronos.org/registry/OpenCL/"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">See also</span></h1>
                </header>
            
            <article>
                
<ul>
<li><span class="koboSpan" id="kobo.2.1">PyOpenCL documentation is available here: </span><a href="https://documen.tician.de/pyopencl/"><span class="koboSpan" id="kobo.3.1">https://documen.tician.de/pyopencl/</span></a><span class="koboSpan" id="kobo.4.1">.</span></li>
<li><span class="koboSpan" id="kobo.5.1">One of the best introductions to PyOpenCL, even if somewhat dated, can be found at the following link: </span><a href="http://www.drdobbs.com/open-source/easy-opencl-with-python/240162614"><span class="koboSpan" id="kobo.6.1">http://www.drdobbs.com/open-source/easy-opencl-with-python/240162614</span></a><span class="koboSpan" id="kobo.7.1">.</span><a href="http://www.drdobbs.com/open-source/easy-opencl-with-python/240162614"/></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Building applications with PyOpenCL</span></h1>
                </header>
            
            <article>
                
<p><span class="hps"><span class="koboSpan" id="kobo.2.1">The first step in the construction of a program for PyOpenCL is the coding of the host application. </span><span class="koboSpan" id="kobo.2.2">This is performed on the CPU and has the task of managing the possible execution of the kernel on the GPU card (that is, the device).</span></span></p>
<p><span class="koboSpan" id="kobo.3.1">A </span><em><span class="koboSpan" id="kobo.4.1">kernel</span></em><span class="koboSpan" id="kobo.5.1"> is a basic unit of executable code, similar to a C function. </span><span class="koboSpan" id="kobo.5.2">It can be data-parallel or task-parallel. </span><span class="koboSpan" id="kobo.5.3">However, the cornerstone of PyOpenCL is the exploitation of parallelism.</span></p>
<p><span class="koboSpan" id="kobo.6.1">A fundamental concept is a </span><em><span class="koboSpan" id="kobo.7.1">program</span></em><span class="koboSpan" id="kobo.8.1">, which is a collection of kernels and other functions, analogous to dynamic libraries. </span><span class="koboSpan" id="kobo.8.2">So, we can group instructions in a kernel and group different kernels into a program.</span></p>
<p><span class="koboSpan" id="kobo.9.1">Programs can be called from applications. </span><span class="koboSpan" id="kobo.9.2">We have the execution queues that indicate the order in which the kernels are executed. </span><span class="koboSpan" id="kobo.9.3">However, in some cases, these can be launched without following the original order.</span></p>
<p><span class="hps"><span class="koboSpan" id="kobo.10.1">We can finally list the fundamental elements for developing an application with PyOpenCL:</span></span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.11.1">Device</span></strong><span class="koboSpan" id="kobo.12.1">: This identifies the hardware in which the kernel code is to be executed. </span><span class="koboSpan" id="kobo.12.2">Note that the PyOpenCL application can be run on both CPU and GPU boards (as well as PyCUDA) but also on embedded devices such as</span><em><strong><span class="koboSpan" id="kobo.13.1"> </span></strong></em><strong><span class="koboSpan" id="kobo.14.1">Field-Programmable Gate Arrays</span></strong><em><strong><span class="koboSpan" id="kobo.15.1"> </span></strong></em><span class="koboSpan" id="kobo.16.1">(</span><strong><span class="koboSpan" id="kobo.17.1">FPGAs</span></strong><span class="koboSpan" id="kobo.18.1">).</span></li>
<li><strong><span class="koboSpan" id="kobo.19.1">Program</span></strong><span><span class="koboSpan" id="kobo.20.1">: This</span></span><span class="koboSpan" id="kobo.21.1"> is a group of kernels</span><span><span class="koboSpan" id="kobo.22.1"> that </span></span><span class="koboSpan" id="kobo.23.1">has the task of selecting which kernel must be run on the device.</span></li>
<li><strong><span class="koboSpan" id="kobo.24.1">Kernel</span></strong><span><span class="koboSpan" id="kobo.25.1">: This</span></span><span class="koboSpan" id="kobo.26.1"> is the code to execute on the device. </span><span class="koboSpan" id="kobo.26.2">A kernel is a C-like function, which means it can be compiled on any device that supports PyOpenCL drivers.</span></li>
<li><strong><span class="koboSpan" id="kobo.27.1">Command queue</span></strong><span><span class="koboSpan" id="kobo.28.1">: This</span></span><span class="koboSpan" id="kobo.29.1"> orders the execution of kernels on the device.</span></li>
<li><strong><span class="koboSpan" id="kobo.30.1">Context</span></strong><span class="koboSpan" id="kobo.31.1">: This is a group of devices that allows devices to receive kernels and transfer data.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.32.1">The following diagram shows how this data structure can work in a host application:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.33.1"><img src="assets/60b38941-28e9-4b2f-a920-d59c4f426b1e.png" style="width:38.58em;height:29.83em;"/></span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span class="hps"><span class="koboSpan" id="kobo.34.1">PyOpenCL programming model</span></span></div>
<p><span><span class="koboSpan" id="kobo.35.1">Again, we observe that a program can contain more functions to run on the device and that each kernel encapsulates only a single function from the program.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How to do it...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the following example, we show the basic steps to build an application with PyOpenCL: the task to be performed is the sum of two vectors. </span><span class="koboSpan" id="kobo.2.2">In order to have a readable output, we'll consider two vectors that each have 100 elements: each </span><em><span class="koboSpan" id="kobo.3.1">i-th</span></em><span class="koboSpan" id="kobo.4.1"> element of the resulting vector will be equal to the sum of the </span><em><span class="koboSpan" id="kobo.5.1">i-th</span></em><span class="koboSpan" id="kobo.6.1"> element of </span><strong><kbd><span class="koboSpan" id="kobo.7.1">vector_a</span></kbd></strong><span class="koboSpan" id="kobo.8.1">, plus the </span><em><span class="koboSpan" id="kobo.9.1">i-th</span></em><span class="koboSpan" id="kobo.10.1"> element of </span><strong><kbd><span class="koboSpan" id="kobo.11.1">vector_b</span></kbd></strong><span class="koboSpan" id="kobo.12.1">:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.13.1">Let's start by importing all the necessary libraries:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.14.1">import numpy as np 
import pyopencl as cl 
import numpy.linalg as la </span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.15.1">We define the size of the vectors to be added, as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.16.1">vector_dimension = 100 </span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.17.1">Here, the input vectors, </span><kbd><span class="koboSpan" id="kobo.18.1">vector_a</span></kbd><span class="koboSpan" id="kobo.19.1"> and </span><kbd><span class="koboSpan" id="kobo.20.1">vector_b</span></kbd><span class="koboSpan" id="kobo.21.1">, are defined:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.22.1">vector_a = np.random.randint(vector_dimension,size=vector_dimension) 
vector_b = np.random.randint(vector_dimension,size=vector_dimension) </span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.23.1">In sequence, we define </span><strong><kbd><span class="koboSpan" id="kobo.24.1">platform</span></kbd></strong><span class="koboSpan" id="kobo.25.1">, </span><strong><kbd><span class="koboSpan" id="kobo.26.1">device</span></kbd></strong><span class="koboSpan" id="kobo.27.1">, </span><strong><kbd><span class="koboSpan" id="kobo.28.1">context</span></kbd></strong><span class="koboSpan" id="kobo.29.1">, and </span><strong><kbd><span class="koboSpan" id="kobo.30.1">queue</span></kbd></strong><span class="koboSpan" id="kobo.31.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.32.1">platform = cl.get_platforms()[1] 
device = platform.get_devices()[0] 
context = cl.Context([device]) 
queue = cl.CommandQueue(context) </span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.33.1">Now, it's time to organize the memory areas that will contain the input vectors:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.34.1">mf = cl.mem_flags 
a_g = cl.Buffer(context, mf.READ_ONLY | mf.COPY_HOST_PTR,\ hostbuf=vector_a) 
b_g = cl.Buffer(context, mf.READ_ONLY | mf.COPY_HOST_PTR,\ hostbuf=vector_b) </span></pre>
<ol start="6">
<li><span class="koboSpan" id="kobo.35.1">Finally, we build the application kernel by using the </span><kbd><span class="koboSpan" id="kobo.36.1">Program</span></kbd><span class="koboSpan" id="kobo.37.1"> </span><span><span class="koboSpan" id="kobo.38.1">method:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.39.1">program = cl.Program(context, """ 
__kernel void vectorSum(__global const int *a_g, __global const int *b_g, __global int *res_g) { 
  int gid = get_global_id(0); 
  res_g[gid] = a_g[gid] + b_g[gid]; 
} 
""").build()</span></pre>
<ol start="7">
<li><span class="koboSpan" id="kobo.40.1">Then, we allocate the memory of the resulting matrix:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.41.1">res_g = cl.Buffer(context, mf.WRITE_ONLY, vector_a.nbytes) </span></pre>
<ol start="8">
<li><span class="koboSpan" id="kobo.42.1">Then, we call the kernel function:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.43.1">program.vectorSum(queue, vector_a.shape, None, a_g, b_g, res_g) </span></pre>
<ol start="9">
<li><span class="koboSpan" id="kobo.44.1">The memory space used to store the result is allocated in the host memory area </span><em><span class="koboSpan" id="kobo.45.1">(</span></em><kbd><span class="koboSpan" id="kobo.46.1">res_np</span></kbd><em><span class="koboSpan" id="kobo.47.1">)</span></em><span class="koboSpan" id="kobo.48.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.49.1">res_np = np.empty_like(vector_a) </span></pre>
<ol start="10">
<li><span class="koboSpan" id="kobo.50.1">Copy the result of the computation into the memory area created:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.51.1">cl._enqueue_copy(queue, res_np, res_g) </span></pre>
<ol start="11">
<li><span class="koboSpan" id="kobo.52.1">Finally, we print the results:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.53.1">print ("PyOPENCL SUM OF TWO VECTORS") 
print ("Platform Selected = %s" %platform.name ) 
print ("Device Selected = %s" %device.name) 
print ("VECTOR LENGTH = %s" %vector_dimension) 
print ("INPUT VECTOR A") 
print (vector_a) 
print ("INPUT VECTOR B") 
print (vector_b) 
print ("OUTPUT VECTOR RESULT A + B ") 
print (res_np) </span></pre>
<ol start="12">
<li><span class="koboSpan" id="kobo.54.1">Then, we perform a simple check in order to verify that the sum operation is correct:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.55.1">assert(la.norm(res_np - (vector_a + vector_b))) &lt; 1e-5 </span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How it works...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the following lines, after the relevant import, we define the input vectors</span><em><span class="koboSpan" id="kobo.3.1">:</span></em></p>
<pre><span class="koboSpan" id="kobo.4.1">vector_dimension = 100 
vector_a = np.random.randint(vector_dimension, size= vector_dimension) 
vector_b = np.random.randint(vector_dimension, size= vector_dimension) </span></pre>
<p><span class="koboSpan" id="kobo.5.1">Each vector contains 100 integer items, which are randomly selected through the </span><kbd><span class="koboSpan" id="kobo.6.1">numpy</span></kbd><span class="koboSpan" id="kobo.7.1"> function:</span></p>
<pre><span class="koboSpan" id="kobo.8.1">np.random.randint(max integer , size of the vector) </span></pre>
<p><span class="koboSpan" id="kobo.9.1">Then, we select the platform to achieve the computation by using the </span><kbd><span class="koboSpan" id="kobo.10.1">get_platform()</span></kbd><span class="koboSpan" id="kobo.11.1"> </span><span><span class="koboSpan" id="kobo.12.1">method:</span></span></p>
<pre><span class="koboSpan" id="kobo.13.1">platform = cl.get_platforms()[1] </span></pre>
<p><span class="koboSpan" id="kobo.14.1">Then, select the corresponding device. </span><span class="koboSpan" id="kobo.14.2">Here, </span><kbd><span class="koboSpan" id="kobo.15.1">platform.get_devices()[0]</span></kbd><span class="koboSpan" id="kobo.16.1"> corresponds to the Intel(R) HD Graphics 5500 graphics card:</span></p>
<pre><span class="koboSpan" id="kobo.17.1">device = platform.get_devices()[0]</span></pre>
<p><span class="koboSpan" id="kobo.18.1">In the following steps, the context and the queue are defined; PyOpenCL provides the method context (device selected) and queue (context selected):</span></p>
<pre><span class="koboSpan" id="kobo.19.1">context = cl.Context([device]) 
queue = cl.CommandQueue(context) </span></pre>
<p><span class="koboSpan" id="kobo.20.1">In order to perform the computation in the selected device, the input vector is copied to the device's memory:</span></p>
<pre><span class="koboSpan" id="kobo.21.1">mf = cl.mem_flags 
a_g = cl.Buffer(context, mf.READ_ONLY | mf.COPY_HOST_PTR,\</span><br/><span class="koboSpan" id="kobo.22.1">hostbuf=vector_a) 
b_g = cl.Buffer(context, mf.READ_ONLY | mf.COPY_HOST_PTR,\</span><br/><span class="koboSpan" id="kobo.23.1"> hostbuf=vector_b) </span></pre>
<p><span class="koboSpan" id="kobo.24.1">Then, we prepare the buffer for the resulting vector:</span></p>
<pre><span class="koboSpan" id="kobo.25.1">res_g = cl.Buffer(context, mf.WRITE_ONLY, vector_a.nbytes) </span></pre>
<p><span class="koboSpan" id="kobo.26.1">Here, the kernel code is defined:</span></p>
<pre><span class="koboSpan" id="kobo.27.1">program = cl.Program(context, """ 
__kernel void vectorSum(__global const int *a_g, __global const int *b_g, __global int *res_g) { 
  int gid = get_global_id(0); 
  res_g[gid] = a_g[gid] + b_g[gid];} 
""").build()</span></pre>
<p><kbd><span class="koboSpan" id="kobo.28.1">vectorSum</span></kbd><span class="koboSpan" id="kobo.29.1"> is the name of the kernel, and the parameter list defines the data types of the input arguments and output data type (both are integer vectors). </span><span class="koboSpan" id="kobo.29.2">Inside the kernel body, the sum of two vectors is defined in the following steps:</span></p>
<ol>
<li><em><span class="koboSpan" id="kobo.30.1">Initialize</span></em><span class="koboSpan" id="kobo.31.1"> the vector's index: </span><kbd><span class="koboSpan" id="kobo.32.1">int gid = get_global_id(0)</span></kbd><span class="koboSpan" id="kobo.33.1">.</span></li>
<li><em><span class="koboSpan" id="kobo.34.1">Sum</span></em><span class="koboSpan" id="kobo.35.1"> the vector's components: </span><kbd><span class="koboSpan" id="kobo.36.1">res_g[gid] = a_g[gid] + b_g[gid]</span></kbd><span class="koboSpan" id="kobo.37.1">.</span></li>
</ol>
<p><span class="koboSpan" id="kobo.38.1">In OpenCL (hence, in PyOpenCL), the buffers are attached to a context (</span><a href="https://documen.tician.de/pyopencl/runtime.html#pyopencl.Context"><span class="koboSpan" id="kobo.39.1">https://documen.tician.de/pyopencl/runtime.html#pyopencl.Context</span></a><span class="koboSpan" id="kobo.40.1">), which are moved to a device once the buffer is used on that device.</span></p>
<p><span class="koboSpan" id="kobo.41.1">Finally, we execute </span><kbd><span class="koboSpan" id="kobo.42.1">vectorSum</span></kbd><span class="koboSpan" id="kobo.43.1"> in the device:</span></p>
<pre><span class="koboSpan" id="kobo.44.1">program.vectorSum(queue, vector_a.shape, None, a_g, b_g, res_g)</span></pre>
<p><span class="koboSpan" id="kobo.45.1">To check the result, we use the </span><kbd><span class="koboSpan" id="kobo.46.1">assert</span></kbd><span class="koboSpan" id="kobo.47.1"> statement. </span><span class="koboSpan" id="kobo.47.2">This tests the result and triggers an error if the condition is false:</span></p>
<pre><span class="koboSpan" id="kobo.48.1">assert(la.norm(res_np - (vector_a + vector_b))) &lt; 1e-5</span></pre>
<p><span class="koboSpan" id="kobo.49.1">The output should be as follows:</span></p>
<pre><strong><span class="koboSpan" id="kobo.50.1">(base) C:\&gt;python vectorSumPyopencl.py 
</span><br/><span class="koboSpan" id="kobo.51.1">PyOPENCL SUM OF TWO VECTORS</span></strong><br/><strong><span class="koboSpan" id="kobo.52.1">Platform Selected = Intel(R) OpenCL</span></strong><br/><strong><span class="koboSpan" id="kobo.53.1">Device Selected = Intel(R) HD Graphics 5500</span></strong><br/><strong><span class="koboSpan" id="kobo.54.1">VECTOR LENGTH = 100</span></strong><br/><strong><span class="koboSpan" id="kobo.55.1">INPUT VECTOR A</span></strong><br/><strong><br/><span class="koboSpan" id="kobo.56.1">[45 46 0 97 96 98 83 7 51 21 72 70 59 65 79 92 98 24 56 6 70 64 59 0</span></strong><br/><strong><span class="koboSpan" id="kobo.57.1"> 96 78 15 21 4 89 14 66 53 20 34 64 48 20 8 53 82 66 19 53 11 17 39 11</span></strong><br/><strong><span class="koboSpan" id="kobo.58.1"> 89 97 51 53 7 4 92 82 90 78 31 18 72 52 44 17 98 3 36 69 25 87 86 68</span></strong><br/><strong><span class="koboSpan" id="kobo.59.1"> 85 16 58 4 57 64 97 11 81 36 37 21 51 22 17 6 66 12 80 50 77 94 6 70</span></strong><br/><strong><span class="koboSpan" id="kobo.60.1"> 21 86 80 69]</span></strong><br/><strong><br/><span class="koboSpan" id="kobo.61.1">INPUT VECTOR B</span></strong><br/><strong><span class="koboSpan" id="kobo.62.1">[25 8 76 57 86 96 58 89 26 31 28 92 67 47 72 64 13 93 96 91 91 36 1 75</span></strong><br/><strong><span class="koboSpan" id="kobo.63.1">  2 40 60 49 24 40 23 35 80 60 61 27 82 38 66 81 95 79 96 23 73 19 5 43</span></strong><br/><strong><span class="koboSpan" id="kobo.64.1">  2 47 17 88 46 76 64 82 31 73 43 17 35 28 48 89 8 61 23 17 56 7 84 36</span></strong><br/><strong><span class="koboSpan" id="kobo.65.1"> 95 60 34 9 4 5 74 59 6 89 84 98 25 50 38 2 3 43 64 96 47 79 12 82</span></strong><br/><strong><span class="koboSpan" id="kobo.66.1"> 72 0 78 5]</span></strong><br/><strong><br/><span class="koboSpan" id="kobo.67.1">OUTPUT VECTOR RESULT A + B</span></strong><br/><strong><span class="koboSpan" id="kobo.68.1">[70 54 76 154 182 194 141 96 77 52 100 162 126 112 151 156 111 117 </span></strong><strong><span class="koboSpan" id="kobo.69.1">152 </span><br/><span class="koboSpan" id="kobo.70.1"> 97 161 100 60 75 98 118 75 70 28 129 37 101 133 80 95 91 </span></strong><strong><span class="koboSpan" id="kobo.71.1">130 58 74 134 </span><br/><span class="koboSpan" id="kobo.72.1"> 177 145 115 76 84 36 44 54 91 144 68 141 53 80 </span></strong><strong><span class="koboSpan" id="kobo.73.1">156 164 121 151 74 35 </span><br/><span class="koboSpan" id="kobo.74.1"> 107 80 92 106 106 64 59 86 81 94 170 104 </span></strong><strong><span class="koboSpan" id="kobo.75.1">80 76 92 13 61 69 171 70 87 </span><br/><span class="koboSpan" id="kobo.76.1"> 125 121 119 76 72 55 8 69 55 </span></strong><strong><span class="koboSpan" id="kobo.77.1">144 146 124 173 18 152 93 86 158 74]</span></strong> </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">There's more...</span></h1>
                </header>
            
            <article>
                
<p><span class="hps"><span class="koboSpan" id="kobo.2.1">In this section, we have seen that the PyOpenCL execution model, like PyCUDA, involves a host processor that manages one or more heterogeneous devices. </span></span><span class="hps"><span class="koboSpan" id="kobo.3.1">In particular, each PyOpenCL command is sent to the devices from the host in the form of source code that is defined through the kernel function.</span></span></p>
<p><span class="hps"><span class="koboSpan" id="kobo.4.1">The source code is then loaded into a program object for the reference architecture, the program is compiled into the reference architecture, and the kernel object that is relative to the program is created.</span></span></p>
<p><span class="hps"><span class="koboSpan" id="kobo.5.1">A kernel object can be executed in a variable number of workgroups, creating an </span><em><span class="koboSpan" id="kobo.6.1">n</span></em><span class="koboSpan" id="kobo.7.1">-dimensional computation matrix that allows it to effectively subdivide the workload for a problem in </span><em><span class="koboSpan" id="kobo.8.1">n</span></em><span class="koboSpan" id="kobo.9.1">-dimensions (1, 2, or 3) in each workgroup. </span><span class="koboSpan" id="kobo.9.2">In turn, they are composed of a number of work items that work in parallel.</span></span></p>
<p><span class="hps"><span class="koboSpan" id="kobo.10.1">Balancing the workload for each workgroup based on the parallel computing capability of a device is one of the critical parameters for achieving good application performance.</span></span></p>
<p><span class="hps"><span class="koboSpan" id="kobo.11.1">A wrong balancing of the workload, together with the specific characteristics of each device (such as transfer latency, throughput, and bandwidth), can lead to a substantial loss of performance or compromise the portability of the code when executed without considering any system of dynamic acquisition of information in terms of device calculation capacities.</span></span></p>
<p><span class="hps"><span class="koboSpan" id="kobo.12.1">However, the accurate use of these technologies allows us to reach high levels of performance by combining the results of the calculation of different computational units.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">See also</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">More on PyOpenCL programming can be found at </span><span class="MsoHyperlink"><a href="https://pydanny-event-notes.readthedocs.io/en/latest/PyConPL2012/async_via_pyopencl.html"><span class="koboSpan" id="kobo.3.1">https://pydanny-event-notes.readthedocs.io/en/latest/PyConPL2012/async_via_pyopencl.html</span></a><span class="koboSpan" id="kobo.4.1">.</span><br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Element-wise expressions with PyOpenCL</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The element-wise functionality allows us to evaluate kernels on complex expressions (which are made of more operands) into a single computational pass.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Getting started</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The </span><kbd><span class="koboSpan" id="kobo.3.1">ElementwiseKernel (context, argument, operation, name, optional_parameters)</span></kbd><span class="koboSpan" id="kobo.4.1"> method</span><span><span class="koboSpan" id="kobo.5.1"> is</span></span><span class="koboSpan" id="kobo.6.1"> implemented in PyOpenCL to handle element-wise expressions.</span></p>
<p><span class="koboSpan" id="kobo.7.1">The main parameters are as follows:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.8.1">context</span></kbd><span class="koboSpan" id="kobo.9.1"> is the device or the group of devices to which the element-wise operation will be executed.</span></li>
<li><kbd><span class="koboSpan" id="kobo.10.1">argument</span></kbd><span class="koboSpan" id="kobo.11.1"> is a C-like argument list of all the parameters involved in the computation.</span></li>
<li><kbd><span class="koboSpan" id="kobo.12.1">operation</span></kbd><span class="koboSpan" id="kobo.13.1"> is a string that represents the operation to perform on the argument list.</span></li>
<li><kbd><span class="koboSpan" id="kobo.14.1">name</span></kbd><span class="koboSpan" id="kobo.15.1"> is the kernel's name that is associated with </span><kbd><span class="koboSpan" id="kobo.16.1">Elementwisekernel</span></kbd><span class="koboSpan" id="kobo.17.1">.</span></li>
<li><kbd><span class="koboSpan" id="kobo.18.1">optional_parameters</span></kbd> <span><span class="koboSpan" id="kobo.19.1">is</span></span><span class="koboSpan" id="kobo.20.1"> not important in this recipe.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How to do it...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Here, we consider the task of adding two integer vectors </span><span><span class="koboSpan" id="kobo.3.1">again:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.4.1">Start importing the relevant libraries:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.5.1">import pyopencl as cl</span><br/><span class="koboSpan" id="kobo.6.1">import pyopencl.array as cl_array</span><br/><span class="koboSpan" id="kobo.7.1">import numpy as np</span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.8.1">Define the context element (</span><kbd><span class="koboSpan" id="kobo.9.1">context</span></kbd><span class="koboSpan" id="kobo.10.1">) and the command queue (</span><kbd><span class="koboSpan" id="kobo.11.1">queue</span></kbd><span class="koboSpan" id="kobo.12.1">) </span><span><span class="koboSpan" id="kobo.13.1">:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.14.1">context = cl.create_some_context()</span><br/><span class="koboSpan" id="kobo.15.1">queue = cl.CommandQueue(context)</span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.16.1">Here, we set the vector dimension and the space allocation for the input and output vectors:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.17.1">vector_dim = 100 
vector_a=cl_array.to_device(queue,np.random.randint(100,\</span><br/><span class="koboSpan" id="kobo.18.1">size=vector_dim)) 
vector_b = cl_array.to_device(queue,np.random.randint(100,\ </span><br/><span class="koboSpan" id="kobo.19.1">size=vector_dim)) 
result_vector = cl_array.empty_like(vector_a) </span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.20.1">We set </span><kbd><span class="koboSpan" id="kobo.21.1">elementwiseSum</span></kbd><span class="koboSpan" id="kobo.22.1"> as the application of </span><kbd><span class="koboSpan" id="kobo.23.1">ElementwiseKernel</span></kbd><span class="koboSpan" id="kobo.24.1">, and then set it to a set of arguments that define the operations to be applied to the input vectors:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.25.1">elementwiseSum = cl.elementwise.ElementwiseKernel(context, "int *a,\</span><br/><span class="koboSpan" id="kobo.26.1">int *b, int *c", "c[i] = a[i] + b[i]", "sum")</span><br/><span class="koboSpan" id="kobo.27.1">elementwiseSum(vector_a, vector_b, result_vector)</span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.28.1">Finally, we print the result:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.29.1">print ("PyOpenCL ELEMENTWISE SUM OF TWO VECTORS")</span><br/><span class="koboSpan" id="kobo.30.1">print ("VECTOR LENGTH = %s" %vector_dimension)</span><br/><span class="koboSpan" id="kobo.31.1">print ("INPUT VECTOR A")</span><br/><span class="koboSpan" id="kobo.32.1">print (vector_a)</span><br/><span class="koboSpan" id="kobo.33.1">print ("INPUT VECTOR B")</span><br/><span class="koboSpan" id="kobo.34.1">print (vector_b)</span><br/><span class="koboSpan" id="kobo.35.1">print ("OUTPUT VECTOR RESULT A + B ")</span><br/><span class="koboSpan" id="kobo.36.1">print (result_vector)</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How it works...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the first lines of the script, we import all the requested modules.</span></p>
<p><span class="koboSpan" id="kobo.3.1">In order to initialize the context, we use the </span><kbd><span class="koboSpan" id="kobo.4.1">cl.create_some_context()</span></kbd><span class="koboSpan" id="kobo.5.1"> method. </span><span class="koboSpan" id="kobo.5.2">This asks the user which context must be used to perform the calculation:</span></p>
<pre><strong><span class="koboSpan" id="kobo.6.1">Choose platform:</span></strong><br/><strong><span class="koboSpan" id="kobo.7.1">[0] &lt;pyopencl.Platform 'NVIDIA CUDA' at 0x1c0a25aecf0&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.8.1">[1] &lt;pyopencl.Platform 'Intel(R) OpenCL' at 0x1c0a2608400&gt;</span></strong></pre>
<p><span class="koboSpan" id="kobo.9.1">Then, we need to instantiate the queue that will receive </span><kbd><span class="koboSpan" id="kobo.10.1">ElementwiseKernel</span></kbd><span class="koboSpan" id="kobo.11.1">:</span></p>
<pre><span class="koboSpan" id="kobo.12.1">queue = cl.CommandQueue(context)</span></pre>
<p><span class="koboSpan" id="kobo.13.1">Input and output vectors are instantiated. </span><span class="koboSpan" id="kobo.13.2">The input vectors, </span><kbd><span class="koboSpan" id="kobo.14.1">vector_a</span></kbd><span class="koboSpan" id="kobo.15.1"> and </span><kbd><span class="koboSpan" id="kobo.16.1">vector_b</span></kbd><span class="koboSpan" id="kobo.17.1">, are integer vectors of random values obtained using the </span><kbd><span class="koboSpan" id="kobo.18.1">random.randint</span></kbd><span class="koboSpan" id="kobo.19.1"> NumPy function. </span><span class="koboSpan" id="kobo.19.2">These vectors are then copied into the device by using the PyOpenCL statement:</span></p>
<pre><span class="koboSpan" id="kobo.20.1">cl.array_to_device(queue,array)</span></pre>
<p><span class="koboSpan" id="kobo.21.1">In </span><kbd><span class="koboSpan" id="kobo.22.1">ElementwiseKernel</span></kbd><span class="koboSpan" id="kobo.23.1">, an object is created:</span></p>
<pre><span class="koboSpan" id="kobo.24.1">elementwiseSum = cl.elementwise.ElementwiseKernel(context,\</span><br/><span class="koboSpan" id="kobo.25.1">               "int *a, int *b, int *c", "c[i] = a[i] + b[i]", "sum")</span></pre>
<div class="packt_tip"><span class="koboSpan" id="kobo.26.1">Note that a</span><span><span class="koboSpan" id="kobo.27.1">ll the arguments are in the form of a string formatted as a C argument list (they are all integers).</span><br/>
<br/></span> <span><span class="koboSpan" id="kobo.28.1">The operation is a C-like code snippet that carries out the operation, that is, the sum of the input vector elements.</span><br/></span><span class="koboSpan" id="kobo.29.1">The name of the function with which the kernel will be compiled is </span><kbd><span><span class="koboSpan" id="kobo.30.1">sum</span></span></kbd><span class="koboSpan" id="kobo.31.1">.</span></div>
<p><span class="koboSpan" id="kobo.32.1">Finally, we call the </span><kbd><span class="koboSpan" id="kobo.33.1">elementwiseSum</span></kbd><span class="koboSpan" id="kobo.34.1"> function with the arguments defined previously:</span></p>
<pre><span class="koboSpan" id="kobo.35.1">elementwiseSum(vector_a, vector_b, result_vector)</span></pre>
<p><span class="koboSpan" id="kobo.36.1">The example ends by printing the input vectors and the result obtained. </span><span class="koboSpan" id="kobo.36.2">The output looks like th</span><span><span class="koboSpan" id="kobo.37.1">is</span></span><span class="koboSpan" id="kobo.38.1">:</span></p>
<pre><strong><span class="koboSpan" id="kobo.39.1">(base) C:\&gt;python elementwisePyopencl.py</span></strong><br/><br/><strong><span class="koboSpan" id="kobo.40.1">Choose platform:</span></strong><br/><strong><span class="koboSpan" id="kobo.41.1">[0] &lt;pyopencl.Platform 'NVIDIA CUDA' at 0x1c0a25aecf0&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.42.1">[1] &lt;pyopencl.Platform 'Intel(R) OpenCL' at 0x1c0a2608400&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.43.1">Choice [0]:1</span><br/></strong><br/><strong><span class="koboSpan" id="kobo.44.1">Choose device(s):</span></strong><br/><strong><span class="koboSpan" id="kobo.45.1">[0] &lt;pyopencl.Device 'Intel(R) HD Graphics 5500' on 'Intel(R) OpenCL' at 0x1c0a1640db0&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.46.1">[1] &lt;pyopencl.Device 'Intel(R) Core(TM) i7-5500U CPU @ 2.40GHz' on 'Intel(R) OpenCL' at 0x1c0a15e53f0&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.47.1">Choice, comma-separated [0]:0</span><br/></strong><strong><span class="koboSpan" id="kobo.48.1">PyOpenCL ELEMENTWISE SUM OF TWO VECTORS</span></strong><br/><strong><span class="koboSpan" id="kobo.49.1">VECTOR LENGTH = 100</span></strong><br/><strong><span class="koboSpan" id="kobo.50.1">INPUT VECTOR A</span></strong><br/><strong><span class="koboSpan" id="kobo.51.1">[24 64 73 37 40 4 41 85 19 90 32 51 6 89 98 56 97 53 34 91 82 89 97 2</span></strong><br/><strong><span class="koboSpan" id="kobo.52.1"> 54 65 90 90 91 75 30 8 62 94 63 69 31 99 8 18 28 7 81 72 14 53 91 80</span></strong><br/><strong><span class="koboSpan" id="kobo.53.1"> 76 39 8 47 25 45 26 56 23 47 41 18 89 17 82 84 10 75 56 89 71 56 66 61</span></strong><br/><strong><span class="koboSpan" id="kobo.54.1"> 58 54 27 88 16 20 9 61 68 63 74 84 18 82 67 30 15 25 25 3 93 36 24 27</span></strong><br/><strong><span class="koboSpan" id="kobo.55.1"> 70 5 78 15]</span><br/></strong><br/><strong><span class="koboSpan" id="kobo.56.1">INPUT VECTOR B</span></strong><br/><strong><span class="koboSpan" id="kobo.57.1">[49 18 69 43 51 72 37 50 79 34 97 49 51 29 89 81 33 7 47 93 70 52 63 90</span></strong><br/><strong><span class="koboSpan" id="kobo.58.1"> 99 95 58 33 41 70 84 87 20 83 74 43 78 34 94 47 89 4 30 36 34 56 32 31</span></strong><br/><strong><span class="koboSpan" id="kobo.59.1"> 56 22 50 52 68 98 52 80 14 98 43 60 20 49 15 38 74 89 99 29 96 65 89 41</span></strong><br/><strong><span class="koboSpan" id="kobo.60.1"> 72 53 89 31 34 64 0 47 87 70 98 86 41 25 34 10 44 36 54 52 54 86 33 38</span></strong><br/><strong><span class="koboSpan" id="kobo.61.1"> 25 49 75 53]</span><br/></strong><br/><strong><span class="koboSpan" id="kobo.62.1">OUTPUT VECTOR RESULT A + B</span></strong><br/><strong><span class="koboSpan" id="kobo.63.1">[73 82 142 80 91 76 78 135 98 124 129 100 57 118 187 137 130 60 </span></strong><strong><span class="koboSpan" id="kobo.64.1">81 184 </span><br/><span class="koboSpan" id="kobo.65.1"> 152 141 160 92 153 160 148 123 132 145 114 95 82 177 137 112 </span></strong><strong><span class="koboSpan" id="kobo.66.1">109 133 </span><br/><span class="koboSpan" id="kobo.67.1"> 102 65 117 11 111 108 48 109 123 111 132 61 58 99 93 143 </span></strong><strong><span class="koboSpan" id="kobo.68.1">78 136 37 145 </span><br/><span class="koboSpan" id="kobo.69.1"> 84 78 109 66 97 122 84 164 155 118 167 121 155 102 </span></strong><strong><span class="koboSpan" id="kobo.70.1">130 107 116 119 50 </span><br/><span class="koboSpan" id="kobo.71.1"> 84 9 108 155 133 172 170 59 107 101 40 59 61 </span></strong><strong><span class="koboSpan" id="kobo.72.1">79 55 147 122 57 65 </span><br/><span class="koboSpan" id="kobo.73.1"> 95 54 153 68]</span></strong> </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">There's more...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">PyCUDA also has element-wise functionality:</span></p>
<pre><span class="koboSpan" id="kobo.3.1">ElementwiseKernel(arguments,operation,name,optional_parameters)</span></pre>
<p><span class="koboSpan" id="kobo.4.1">This feature has pretty much the same arguments as the function built for PyOpenCL, except for the context parameter. </span><span class="koboSpan" id="kobo.4.2">The same example this section, which is implemented through Py</span><span><span class="koboSpan" id="kobo.5.1">CUDA</span></span><span class="koboSpan" id="kobo.6.1">, has the following listing:</span></p>
<pre><span class="koboSpan" id="kobo.7.1">import pycuda.autoinit 
import numpy 
from pycuda.elementwise import ElementwiseKernel 
import numpy.linalg as la 
 
vector_dimension=100 
input_vector_a = np.random.randint(100,size= vector_dimension) 
input_vector_b = np.random.randint(100,size= vector_dimension) 
output_vector_c = gpuarray.empty_like(input_vector_a) 
 
elementwiseSum = ElementwiseKernel(" int *a, int * b, int *c",\ 
                             "c[i] = a[i] + b[i]"," elementwiseSum ") 
elementwiseSum(input_vector_a, input_vector_b,output_vector_c) 
 
print ("PyCUDA ELEMENTWISE SUM OF TWO VECTORS") 
print ("VECTOR LENGTH = %s" %vector_dimension) 
print ("INPUT VECTOR A") 
print (vector_a) 
print ("INPUT VECTOR B") 
print (vector_b) 
print ("OUTPUT VECTOR RESULT A + B ") 
print (result_vector) </span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">See also</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the following link, you'll find interesting examples of PyOpenCL</span><span><span class="koboSpan" id="kobo.3.1"> applications</span></span><span class="koboSpan" id="kobo.4.1">: </span><a href="https://github.com/romanarranz/PyOpenCL"><span class="koboSpan" id="kobo.5.1">https://github.com/romanarranz/PyOpenCL</span></a><span class="koboSpan" id="kobo.6.1">.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Evaluating PyOpenCL applications</span></h1>
                </header>
            
            <article>
                
<p><span class="hps"><span class="koboSpan" id="kobo.2.1">In this section, we are doing a comparative test of performance between CPU and GPU by using the PyOpenCL library.</span></span></p>
<p><span class="hps"><span class="koboSpan" id="kobo.3.1">In fact, before studying the performance of the algorithms to be implemented, it is also important to understand the computational advantages offered by the computing platform you have.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Getting started</span></h1>
                </header>
            
            <article>
                
<p><span class="hps"><span class="koboSpan" id="kobo.2.1">The specific characteristics of a computing system interfere with the computational time, and hence they represent an aspect of primary importance.</span></span></p>
<p><span class="hps"><span class="koboSpan" id="kobo.3.1">In the following example, we will perform a test in order to monitor performance on such a system:</span></span></p>
<ul>
<li><span class="hps"><span class="koboSpan" id="kobo.4.1">GPU: GeForce 840 M</span></span></li>
<li><span class="hps"><span class="hps"><span class="koboSpan" id="kobo.5.1">CPU: Intel Core i7 – 2.40 GHz </span></span></span></li>
<li><span class="koboSpan" id="kobo.6.1">RAM: 8 GB</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How to do it...</span></h1>
                </header>
            
            <article>
                
<p><span class="hps"><span class="koboSpan" id="kobo.2.1">In the following test, the calculation time of a mathematical operation, as the sum of two vectors with floating-point elements, will be evaluated and compared. </span><span class="koboSpan" id="kobo.2.2">To make the comparison, the same operation will be performed on two separate functions.</span></span></p>
<p><span class="hps"><span class="koboSpan" id="kobo.3.1">The first function is computed by the CPU</span><span><span class="koboSpan" id="kobo.4.1"> only,</span></span><span class="koboSpan" id="kobo.5.1"> while the second function is written by using the PyOpenCL library to use the GPU card. </span></span><span class="hps"><span class="koboSpan" id="kobo.6.1">The test is performed on vectors with a size of 10,000 elements</span></span><span class="koboSpan" id="kobo.7.1">.</span></p>
<p><span class="koboSpan" id="kobo.8.1">Here is the code:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.9.1">Import the relevant libraries. </span><span class="koboSpan" id="kobo.9.2">Note the import of the </span><kbd><span class="koboSpan" id="kobo.10.1">time</span></kbd><span class="koboSpan" id="kobo.11.1"> library to calculate the computation times, and the </span><kbd><span class="koboSpan" id="kobo.12.1">linalg</span></kbd><span class="koboSpan" id="kobo.13.1"> library, which is a tool of linear algebra tools of the </span><kbd><span class="koboSpan" id="kobo.14.1">numpy</span></kbd><span class="koboSpan" id="kobo.15.1"> library:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.16.1">from time import time 
import pyopencl as cl   
import numpy as np    
import deviceInfoPyopencl as device_info 
import numpy.linalg as la </span></pre>
<ol start="2">
<li><span class="hps"><span class="koboSpan" id="kobo.17.1">Then, we define the input vectors. </span><span class="koboSpan" id="kobo.17.2">They both contain </span><kbd><span class="koboSpan" id="kobo.18.1">10000</span></kbd><span class="koboSpan" id="kobo.19.1"> random elements of floating-point numbers:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.20.1">a = np.random.rand(10000).astype(np.float32) 
b = np.random.rand(10000).astype(np.float32) </span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.21.1">The following function computes the sum of the two vectors working on the CPU (host):</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.22.1">def test_cpu_vector_sum(a, b): 
    c_cpu = np.empty_like(a) 
    cpu_start_time = time() 
    for i in range(10000): 
            for j in range(10000): 
                    c_cpu[i] = a[i] + b[i] 
    cpu_end_time = time() 
    print("CPU Time: {0} s".format(cpu_end_time - cpu_start_time)) 
    return c_cpu </span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.23.1">The following function computes the sum of the two vectors working on the GPU (device):</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.24.1">def test_gpu_vector_sum(a, b): 
    platform = cl.get_platforms()[0] 
    device = platform.get_devices()[0] 
    context = cl.Context([device]) 
    queue = cl.CommandQueue(context,properties=\</span><br/><span class="koboSpan" id="kobo.25.1">                           cl.command_queue_properties.PROFILING_ENABLE)</span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.26.1">Within the </span><kbd><span class="koboSpan" id="kobo.27.1">test_gpu_vector_sum</span></kbd><span class="koboSpan" id="kobo.28.1"> </span><span><span class="koboSpan" id="kobo.29.1">function,</span></span><span class="koboSpan" id="kobo.30.1"> we prepare the memory buffers to contain the input vectors and the output vector:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.31.1">    a_buffer = cl.Buffer(context,cl.mem_flags.READ_ONLY \ 
                | cl.mem_flags.COPY_HOST_PTR, hostbuf=a) 
    b_buffer = cl.Buffer(context,cl.mem_flags.READ_ONLY \ 
                | cl.mem_flags.COPY_HOST_PTR, hostbuf=b) 
    c_buffer = cl.Buffer(context,cl.mem_flags.WRITE_ONLY, b.nbytes) </span></pre>
<ol start="6">
<li><span class="koboSpan" id="kobo.32.1">Still, within the </span><kbd><span class="koboSpan" id="kobo.33.1">test_gpu_vector_sum</span></kbd><span class="koboSpan" id="kobo.34.1"> </span><span><span class="koboSpan" id="kobo.35.1">function, </span></span><span class="koboSpan" id="kobo.36.1">we define the kernel that will computerize the sum of the two vectors on the device:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.37.1">    program = cl.Program(context, """ 
    __kernel void sum(__global const float *a,\ 
                      __global const float *b,\ 
                      __global float *c){ 
        int i = get_global_id(0); 
        int j; 
        for(j = 0; j &lt; 10000; j++){ 
            c[i] = a[i] + b[i];} 
    }""").build() </span></pre>
<ol start="7">
<li><span class="koboSpan" id="kobo.38.1">Then, we reset the </span><kbd><span class="koboSpan" id="kobo.39.1">gpu_start_time</span></kbd> <span><span class="koboSpan" id="kobo.40.1">variable </span></span><span class="koboSpan" id="kobo.41.1">before starting the calculation. </span><span class="koboSpan" id="kobo.41.2">After this, we calculate the sum of two vectors and then we evaluate the calculation time:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.42.1">    gpu_start_time = time() 
    event = program.sum(queue, a.shape, None,a_buffer, b_buffer,\ </span><br/><span class="koboSpan" id="kobo.43.1">            c_buffer) 
    event.wait() 
    elapsed = 1e-9*(event.profile.end - event.profile.start) 
    print("GPU Kernel evaluation Time: {0} s".format(elapsed)) 
    c_gpu = np.empty_like(a) 
    cl._enqueue_read_buffer(queue, c_buffer, c_gpu).wait() 
    gpu_end_time = time() 
    print("GPU Time: {0} s".format(gpu_end_time - gpu_start_time)) 
    return c_gpu </span></pre>
<ol start="8">
<li><span class="koboSpan" id="kobo.44.1">Finally, we perform the test, recalling the two functions defined previously:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.45.1">if __name__ == "__main__": 
    device_info.print_device_info() 
    cpu_result = test_cpu_vector_sum(a, b) 
    gpu_result = test_gpu_vector_sum(a, b) 
    assert (la.norm(cpu_result - gpu_result)) &lt; 1e-5 </span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How it works...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">As explained previously, the test consists of executing the calculation task, both on the CPU via the </span><kbd><span class="koboSpan" id="kobo.3.1">test_cpu_vector_sum</span></kbd><span class="koboSpan" id="kobo.4.1"> function, and then on the GPU via the </span><kbd><span class="koboSpan" id="kobo.5.1">test_gpu_vector_sum</span></kbd><span class="koboSpan" id="kobo.6.1"> function.</span></p>
<p><span class="koboSpan" id="kobo.7.1">Both functions report the execution time.</span></p>
<p><span class="koboSpan" id="kobo.8.1">Regarding the testing function on the CPU, </span><kbd><span class="koboSpan" id="kobo.9.1">test_cpu_vector_sum</span></kbd><span class="koboSpan" id="kobo.10.1">, it consists of a double calculation loop on </span><kbd><span class="koboSpan" id="kobo.11.1">10000</span></kbd><span class="koboSpan" id="kobo.12.1"> vector elements:</span></p>
<pre><span class="koboSpan" id="kobo.13.1">            cpu_start_time = time() 
               for i in range(10000): 
                         for j in range(10000): 
                             c_cpu[i] = a[i] + b[i] 
               cpu_end_time = time() </span></pre>
<p><span class="koboSpan" id="kobo.14.1">The total CPU time is the difference between the following:</span></p>
<pre><span class="koboSpan" id="kobo.15.1">    CPU Time = cpu_end_time - cpu_start_time </span></pre>
<p><span class="koboSpan" id="kobo.16.1">As for the </span><kbd><span class="koboSpan" id="kobo.17.1">test_gpu_vector_sum</span></kbd><span class="koboSpan" id="kobo.18.1"> function, you can see the following by looking at the execution kernel:</span></p>
<pre><span class="koboSpan" id="kobo.19.1">    __kernel void sum(__global const float *a, 
                      __global const float *b, 
                      __global float *c){ 
        int i=get_global_id(0); 
        int j; 
        for(j=0;j&lt; 10000;j++){ 
            c[i]=a[i]+b[i];} </span></pre>
<p><span class="koboSpan" id="kobo.20.1">The sum of the two vectors is performed through a single calculation loop.</span></p>
<p><span class="koboSpan" id="kobo.21.1">The result, as can be imagined, is a substantial reduction in the execution time for the </span><kbd><span class="koboSpan" id="kobo.22.1">test_gpu_vector_sum</span></kbd><span class="koboSpan" id="kobo.23.1"> </span><span><span class="koboSpan" id="kobo.24.1">function:</span></span></p>
<pre><strong><span class="koboSpan" id="kobo.25.1">(base) C:\&gt;python testApplicationPyopencl.py 
 
============================================================</span></strong><br/><strong><span class="koboSpan" id="kobo.26.1">OpenCL Platforms and Devices</span></strong><br/><strong><span class="koboSpan" id="kobo.27.1">============================================================</span></strong><br/><strong><span class="koboSpan" id="kobo.28.1">Platform - Name: NVIDIA CUDA</span></strong><br/><strong><span class="koboSpan" id="kobo.29.1">Platform - Vendor: NVIDIA Corporation</span></strong><br/><strong><span class="koboSpan" id="kobo.30.1">Platform - Version: OpenCL 1.2 CUDA 10.1.152</span></strong><br/><strong><span class="koboSpan" id="kobo.31.1">Platform - Profile: FULL_PROFILE</span></strong><br/><strong><span class="koboSpan" id="kobo.32.1">    --------------------------------------------------------</span></strong><br/><strong><span class="koboSpan" id="kobo.33.1">    Device - Name: GeForce 840M</span></strong><br/><strong><span class="koboSpan" id="kobo.34.1">    Device - Type: GPU</span></strong><br/><strong><span class="koboSpan" id="kobo.35.1">    Device - Max Clock Speed: 1124 Mhz</span></strong><br/><strong><span class="koboSpan" id="kobo.36.1">    Device - Compute Units: 3</span></strong><br/><strong><span class="koboSpan" id="kobo.37.1">    Device - Local Memory: 48 KB</span></strong><br/><strong><span class="koboSpan" id="kobo.38.1">    Device - Constant Memory: 64 KB</span></strong><br/><strong><span class="koboSpan" id="kobo.39.1">    Device - Global Memory: 2 GB</span></strong><br/><strong><span class="koboSpan" id="kobo.40.1">    Device - Max Buffer/Image Size: 512 MB</span></strong><br/><strong><span class="koboSpan" id="kobo.41.1">    Device - Max Work Group Size: 1024</span></strong><br/><strong><span class="koboSpan" id="kobo.42.1">============================================================</span></strong><br/><strong><span class="koboSpan" id="kobo.43.1">Platform - Name: Intel(R) OpenCL</span></strong><br/><strong><span class="koboSpan" id="kobo.44.1">Platform - Vendor: Intel(R) Corporation</span></strong><br/><strong><span class="koboSpan" id="kobo.45.1">Platform - Version: OpenCL 2.0</span></strong><br/><strong><span class="koboSpan" id="kobo.46.1">Platform - Profile: FULL_PROFILE</span></strong><br/><strong><span class="koboSpan" id="kobo.47.1">    --------------------------------------------------------</span></strong><br/><strong><span class="koboSpan" id="kobo.48.1">    Device - Name: Intel(R) HD Graphics 5500</span></strong><br/><strong><span class="koboSpan" id="kobo.49.1">    Device - Type: GPU</span></strong><br/><strong><span class="koboSpan" id="kobo.50.1">    Device - Max Clock Speed: 950 Mhz</span></strong><br/><strong><span class="koboSpan" id="kobo.51.1">    Device - Compute Units: 24</span></strong><br/><strong><span class="koboSpan" id="kobo.52.1">    Device - Local Memory: 64 KB</span></strong><br/><strong><span class="koboSpan" id="kobo.53.1">    Device - Constant Memory: 64 KB</span></strong><br/><strong><span class="koboSpan" id="kobo.54.1">    Device - Global Memory: 3 GB</span></strong><br/><strong><span class="koboSpan" id="kobo.55.1">    Device - Max Buffer/Image Size: 808 MB</span></strong><br/><strong><span class="koboSpan" id="kobo.56.1">    Device - Max Work Group Size: 256</span></strong><br/><strong><span class="koboSpan" id="kobo.57.1">    --------------------------------------------------------</span></strong><br/><strong><span class="koboSpan" id="kobo.58.1">    Device - Name: Intel(R) Core(TM) i7-5500U CPU @ 2.40GHz</span></strong><br/><strong><span class="koboSpan" id="kobo.59.1">    Device - Type: CPU</span></strong><br/><strong><span class="koboSpan" id="kobo.60.1">    Device - Max Clock Speed: 2400 Mhz</span></strong><br/><strong><span class="koboSpan" id="kobo.61.1">    Device - Compute Units: 4</span></strong><br/><strong><span class="koboSpan" id="kobo.62.1">    Device - Local Memory: 32 KB</span></strong><br/><strong><span class="koboSpan" id="kobo.63.1">    Device - Constant Memory: 128 KB</span></strong><br/><strong><span class="koboSpan" id="kobo.64.1">    Device - Global Memory: 8 GB</span></strong><br/><strong><span class="koboSpan" id="kobo.65.1">    Device - Max Buffer/Image Size: 2026 MB</span></strong><br/><strong><span class="koboSpan" id="kobo.66.1">    Device - Max Work Group Size: 8192</span></strong><br/><br/><br/><strong><span class="koboSpan" id="kobo.67.1">CPU Time: 39.505873918533325 s</span></strong><br/><strong><span class="koboSpan" id="kobo.68.1">GPU Kernel evaluation Time: 0.013606592 s</span></strong><br/><strong><span class="koboSpan" id="kobo.69.1">GPU Time: 0.019981861114501953 s</span></strong> </pre>
<p><span class="koboSpan" id="kobo.70.1">Even if the test is not computationally expansive, it provides useful indications of the potential of a GPU card.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">There's more...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">OpenCL is a standardized cross-platform API for developing applications that exploit parallel computing in heterogeneous systems. </span><span class="koboSpan" id="kobo.2.2">The similarities with CUDA are remarkable, including everything from the memory hierarchy to the direct correspondence between threads and work items.</span></p>
<p><span class="koboSpan" id="kobo.3.1">Even at the programming level, there are many similar aspects and extensions with the same functionality.</span></p>
<p><span class="koboSpan" id="kobo.4.1">However, OpenCL has a much more complex device management model due to its ability to support a wide variety of hardware. </span><span class="koboSpan" id="kobo.4.2">On the other hand, OpenCL is designed to have code portability between products from different manufacturers.</span></p>
<p><span class="koboSpan" id="kobo.5.1">CUDA, thanks to its greater maturity and dedicated hardware, offers simplified device management and higher-level APIs that make it preferable, but only if you are dealing with specific architectures (that is, NVIDIA graphic cards).</span></p>
<p><span class="koboSpan" id="kobo.6.1">The pros and cons of the CUDA and OpenCL libraries, as well as the PyCUDA and PyOpenCL</span><span><span class="koboSpan" id="kobo.7.1"> libraries</span></span><span class="koboSpan" id="kobo.8.1">, are explained in the following sections.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Pros of OpenCL and PyOpenCL</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The pros are as follows:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.3.1">They allow the use of heterogeneous systems with different types of microprocessors.</span></li>
<li><span class="koboSpan" id="kobo.4.1">The same code runs on different systems.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Cons of OpenCL and PyOpenCL</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The cons are as follows:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.3.1">Complex device management</span></li>
<li><span class="koboSpan" id="kobo.4.1">APIs not fully stable</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Pros of CUDA and PyCUDA</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The pros are as follows:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.3.1">APIs with very high abstraction levels</span></li>
<li><span class="koboSpan" id="kobo.4.1">Extensions for many programming languages</span></li>
<li><span class="koboSpan" id="kobo.5.1">Huge documentation and a very large community</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Cons of CUDA and PyCUDA</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The cons are as follows:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.3.1">Supports only the latest NVIDIA GPUs as devices</span></li>
<li><span class="koboSpan" id="kobo.4.1">Reduces heterogeneity to CPUs and GPUs</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">See also</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Andreas Klöckner has made a series of lectures on GPU programming with PyCuda and PyOpenCL available at </span><a href="https://www.bu.edu/pasi/courses/gpu-programming-with-pyopencl-and-pycuda/"><span class="koboSpan" id="kobo.3.1">https://www.bu.edu/pasi/courses/gpu-programming-with-pyopencl-and-pycuda/</span></a><span class="koboSpan" id="kobo.4.1"> and </span><a href="https://www.youtube.com/results?search_query=pyopenCL+and+pycuda"><span class="koboSpan" id="kobo.5.1">https://www.youtube.com/results?search_query=pyopenCL+and+pycuda</span></a><span class="koboSpan" id="kobo.6.1">.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">GPU programming with Numba</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Numba is a Python compiler that provides CUDA-based APIs. </span><span class="koboSpan" id="kobo.2.2">It has been designed primarily for numerical computing tasks, just like the NumPy library. </span><span class="koboSpan" id="kobo.2.3">In particular, the</span><span><span class="koboSpan" id="kobo.3.1"> </span></span><kbd><span class="koboSpan" id="kobo.4.1">numba</span></kbd><span><span class="koboSpan" id="kobo.5.1"> </span></span><span class="koboSpan" id="kobo.6.1">library manages and processes the array data types provided by NumPy.</span></p>
<p><span class="koboSpan" id="kobo.7.1">In fact, the exploitation of data parallelism, which is inherent in numerical computation involving arrays, is a natural choice for GPU accelerators.</span></p>
<p><span class="koboSpan" id="kobo.8.1">The Numba compiler works by specifying the signature types (or decorators) for Python functions and enabling the compilation at runtime (this type of compilation is also called</span><span><span class="koboSpan" id="kobo.9.1"> </span></span><em><span class="koboSpan" id="kobo.10.1">Just In Time</span></em><span class="koboSpan" id="kobo.11.1">).</span></p>
<p><span class="koboSpan" id="kobo.12.1">The most important decorators are as follows:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.13.1">jit</span></kbd><span class="koboSpan" id="kobo.14.1">: This allows the developer to write CUDA-like functions. </span><span class="koboSpan" id="kobo.14.2">When encountered, the compiler translates the code under the decorator into the pseudo-assembly PTX language, so that it can be executed by the GPU.</span></li>
<li><kbd><span class="koboSpan" id="kobo.15.1">autojit</span></kbd><span class="koboSpan" id="kobo.16.1">: </span><span><span class="koboSpan" id="kobo.17.1">This</span></span><span class="koboSpan" id="kobo.18.1"> annotates a function for a</span><span><span class="koboSpan" id="kobo.19.1"> </span></span><em><span class="koboSpan" id="kobo.20.1">deferred compilation</span></em><span><span class="koboSpan" id="kobo.21.1"> </span></span><span class="koboSpan" id="kobo.22.1">procedure, which means that the function with this signature is compiled exactly once.</span></li>
<li><kbd><span class="koboSpan" id="kobo.23.1">vectorize</span></kbd><span class="koboSpan" id="kobo.24.1">: </span><span><span class="koboSpan" id="kobo.25.1">This</span></span><span class="koboSpan" id="kobo.26.1"> creates a so-called</span><strong><span><span class="koboSpan" id="kobo.27.1"> </span></span><span class="koboSpan" id="kobo.28.1">NumPy Universal Function</span></strong><span class="koboSpan" id="kobo.29.1"> (</span><strong><span class="koboSpan" id="kobo.30.1">ufunc</span></strong><span class="koboSpan" id="kobo.31.1">) that takes a function and executes it in parallel with vector arguments.</span></li>
<li><kbd><span class="koboSpan" id="kobo.32.1">guvectorize</span></kbd><span class="koboSpan" id="kobo.33.1">: </span><span><span class="koboSpan" id="kobo.34.1">This </span></span><span class="koboSpan" id="kobo.35.1">builds a so-called </span><strong><span class="koboSpan" id="kobo.36.1">NumPy Generalized Universal Function</span></strong><span class="koboSpan" id="kobo.37.1"> (</span><span><strong><span class="koboSpan" id="kobo.38.1">gufunc</span></strong><span class="koboSpan" id="kobo.39.1">)</span></span><span class="koboSpan" id="kobo.40.1">. </span><span class="koboSpan" id="kobo.40.2">A</span><span><span class="koboSpan" id="kobo.41.1"> </span></span><kbd><span class="koboSpan" id="kobo.42.1">gufunc</span></kbd><span><span class="koboSpan" id="kobo.43.1"> </span></span><span class="koboSpan" id="kobo.44.1">object may operate on entire sub-arrays. </span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Getting ready</span></h1>
                </header>
            
            <article>
                
<p><span class="hps"><span class="koboSpan" id="kobo.2.1">Numba (release 0.45)</span><span><span class="koboSpan" id="kobo.3.1"> is compatible with Python 2.7 and 3.5 or later, as well as NumPy versions 1.7 to 1.16</span></span><span class="koboSpan" id="kobo.4.1">. </span></span></p>
<p><span class="hps"><span class="koboSpan" id="kobo.5.1">To install </span><kbd><span class="koboSpan" id="kobo.6.1">numba</span></kbd><span class="koboSpan" id="kobo.7.1">, it is recommended as per </span><kbd><span class="koboSpan" id="kobo.8.1">pyopencl</span></kbd><span class="koboSpan" id="kobo.9.1"> to use the Anaconda framework, so, from the Anaconda Prompt, just type the following:</span></span></p>
<pre><strong><span class="koboSpan" id="kobo.10.1">(base) C:\&gt; conda install numba</span></strong></pre>
<p><span class="koboSpan" id="kobo.11.1">In addition, to use the full potential of </span><kbd><span class="koboSpan" id="kobo.12.1">numba</span></kbd><span class="koboSpan" id="kobo.13.1">, the </span><kbd><span class="koboSpan" id="kobo.14.1">cudatoolkit</span></kbd><span class="koboSpan" id="kobo.15.1"> library must be installed:</span></p>
<pre><strong><span class="koboSpan" id="kobo.16.1">(base) C:\&gt; conda install cudatoolkit</span></strong></pre>
<p><span class="koboSpan" id="kobo.17.1">After that, it's possible to verify whether the CUDA library and GPU are properly detected.</span></p>
<p><span class="koboSpan" id="kobo.18.1">Open the Python interpreter from the Anaconda Prompt:</span></p>
<pre><strong><span class="koboSpan" id="kobo.19.1">(base) C:\&gt; python</span><br/><span class="koboSpan" id="kobo.20.1">Python 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. </span><span class="koboSpan" id="kobo.20.2">on win32</span><br/><span class="koboSpan" id="kobo.21.1">Type "help", "copyright", "credits" or "license" for more information.</span><br/><span class="koboSpan" id="kobo.22.1">&gt;&gt;</span></strong></pre>
<p class="mce-root"><span><span class="koboSpan" id="kobo.23.1">The first test entails checking whether the CUDA library (</span><kbd><span class="koboSpan" id="kobo.24.1">cudatoolkit</span></kbd><span class="koboSpan" id="kobo.25.1">) is properly installed:</span></span></p>
<pre class="mce-root"><strong><span class="koboSpan" id="kobo.26.1">&gt;&gt;&gt; import numba.cuda.api</span></strong><br/><strong><span class="koboSpan" id="kobo.27.1">&gt;&gt;&gt; import numba.cuda.cudadrv.libs</span></strong><br/><strong><span class="koboSpan" id="kobo.28.1">&gt;&gt;&gt; numba.cuda.cudadrv.libs.test()</span></strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"><span class="koboSpan" id="kobo.29.1">The following output shows the quality of the installation, where all the checks returned a positive result:</span></p>
<pre class="mce-root"><strong><span class="koboSpan" id="kobo.30.1">Finding cublas from Conda environment</span></strong><br/><strong><span class="koboSpan" id="kobo.31.1"> located at C:\Users\Giancarlo\Anaconda3\Library\bin\cublas64_10.dll</span></strong><br/><strong><span class="koboSpan" id="kobo.32.1"> trying to open library... </span><span class="koboSpan" id="kobo.32.2">ok</span></strong><br/><strong><span class="koboSpan" id="kobo.33.1">Finding cusparse from Conda environment</span></strong><br/><strong><span class="koboSpan" id="kobo.34.1"> located at C:\Users\Giancarlo\Anaconda3\Library\bin\cusparse64_10.dll</span></strong><br/><strong><span class="koboSpan" id="kobo.35.1"> trying to open library... </span><span class="koboSpan" id="kobo.35.2">ok</span></strong><br/><strong><span class="koboSpan" id="kobo.36.1">Finding cufft from Conda environment</span></strong><br/><strong><span class="koboSpan" id="kobo.37.1"> located at C:\Users\Giancarlo\Anaconda3\Library\bin\cufft64_10.dll</span></strong><br/><strong><span class="koboSpan" id="kobo.38.1"> trying to open library... </span><span class="koboSpan" id="kobo.38.2">ok</span></strong><br/><strong><span class="koboSpan" id="kobo.39.1">Finding curand from Conda environment</span></strong><br/><strong><span class="koboSpan" id="kobo.40.1"> located at C:\Users\Giancarlo\Anaconda3\Library\bin\curand64_10.dll</span></strong><br/><strong><span class="koboSpan" id="kobo.41.1"> trying to open library... </span><span class="koboSpan" id="kobo.41.2">ok</span></strong><br/><strong><span class="koboSpan" id="kobo.42.1">Finding nvvm from Conda environment</span></strong><br/><strong><span class="koboSpan" id="kobo.43.1"> located at C:\Users\Giancarlo\Anaconda3\Library\bin\nvvm64_33_0.dll</span></strong><br/><strong><span class="koboSpan" id="kobo.44.1"> trying to open library... </span><span class="koboSpan" id="kobo.44.2">ok</span></strong><br/><strong><span class="koboSpan" id="kobo.45.1">Finding libdevice from Conda environment</span></strong><br/><strong><span class="koboSpan" id="kobo.46.1"> searching for compute_20... </span><span class="koboSpan" id="kobo.46.2">ok</span></strong><br/><strong><span class="koboSpan" id="kobo.47.1"> searching for compute_30... </span><span class="koboSpan" id="kobo.47.2">ok</span></strong><br/><strong><span class="koboSpan" id="kobo.48.1"> searching for compute_35... </span><span class="koboSpan" id="kobo.48.2">ok</span></strong><br/><strong><span class="koboSpan" id="kobo.49.1"> searching for compute_50... </span><span class="koboSpan" id="kobo.49.2">ok</span></strong><br/><strong><span class="koboSpan" id="kobo.50.1">True</span></strong><br/><br/></pre>
<p><span class="koboSpan" id="kobo.51.1">In the second test, we verify the presence of a graphics card: </span></p>
<pre><strong><span class="koboSpan" id="kobo.52.1">&gt;&gt;&gt; numba.cuda.api.detect()</span></strong></pre>
<p><span class="koboSpan" id="kobo.53.1">The output shows the graphic card found and whether it is supported:</span></p>
<pre><span class="koboSpan" id="kobo.54.1">Found 1 CUDA devices</span><br/><span class="koboSpan" id="kobo.55.1">id 0 b'GeForce 840M' [SUPPORTED]</span><br/><span class="koboSpan" id="kobo.56.1">                      compute capability: 5.0</span><br/><span class="koboSpan" id="kobo.57.1">                           pci device id: 0</span><br/><span class="koboSpan" id="kobo.58.1">                              pci bus id: 8</span><br/><span class="koboSpan" id="kobo.59.1">Summary:</span><br/><span class="koboSpan" id="kobo.60.1">        1/1 devices are supported</span><br/><span class="koboSpan" id="kobo.61.1">True</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How to do it...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In this example, we provide a demonstration of the Numba compiler using the</span><span><span class="koboSpan" id="kobo.3.1"> </span></span><kbd><span class="koboSpan" id="kobo.4.1">@guvectorize</span></kbd><span class="koboSpan" id="kobo.5.1"> </span><span><span class="koboSpan" id="kobo.6.1">annotation.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">The task to execute is matrix multiplication:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.8.1">Import</span><span><span class="koboSpan" id="kobo.9.1"> </span></span><kbd><span class="koboSpan" id="kobo.10.1">guvectorize</span></kbd><span><span class="koboSpan" id="kobo.11.1"> </span></span><span class="koboSpan" id="kobo.12.1">from the</span><span><span class="koboSpan" id="kobo.13.1"> </span></span><kbd><span class="koboSpan" id="kobo.14.1">numba</span></kbd><span class="koboSpan" id="kobo.15.1"> library and the</span><span><span class="koboSpan" id="kobo.16.1"> </span></span><kbd><span class="koboSpan" id="kobo.17.1">numpy</span></kbd><span><span class="koboSpan" id="kobo.18.1"> </span></span><span class="koboSpan" id="kobo.19.1">module:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.20.1">from numba import guvectorize 
import numpy as np </span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.21.1">Using the</span><span><span class="koboSpan" id="kobo.22.1"> </span></span><kbd><span class="koboSpan" id="kobo.23.1">@guvectorize</span></kbd><span><span class="koboSpan" id="kobo.24.1"> </span></span><span class="koboSpan" id="kobo.25.1">decorator, w</span><span><span class="koboSpan" id="kobo.26.1">e define </span></span><span class="koboSpan" id="kobo.27.1">the</span><span><span class="koboSpan" id="kobo.28.1"> </span></span><kbd><span class="koboSpan" id="kobo.29.1">matmul</span></kbd><span><span class="koboSpan" id="kobo.30.1"> </span></span><span class="koboSpan" id="kobo.31.1">function, which will perform the matrix multiplication task:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.32.1">@guvectorize(['void(int64[:,:], int64[:,:], int64[:,:])'], 
             '(m,n),(n,p)-&gt;(m,p)') 
def matmul(A, B, C): 
    m, n = A.shape 
    n, p = B.shape 
    for i in range(m): 
        for j in range(p): 
            C[i, j] = 0 
            for k in range(n): 
                C[i, j] += A[i, k] * B[k, j] </span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.33.1">The input matrices are 10 × 10 in size, while the elements are integers:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.34.1">dim = 10 
A = np.random.randint(dim,size=(dim, dim)) 
B = np.random.randint(dim,size=(dim, dim)) </span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.35.1">Finally, we call the</span><span><span class="koboSpan" id="kobo.36.1"> </span></span><kbd><span class="koboSpan" id="kobo.37.1">matmul</span></kbd><span><span class="koboSpan" id="kobo.38.1"> </span></span><span class="koboSpan" id="kobo.39.1">function on the</span><span><span class="koboSpan" id="kobo.40.1"> previously defined</span></span><span class="koboSpan" id="kobo.41.1"> input matrices:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.42.1">C = matmul(A, B) </span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.43.1">We print the input matrices and the resulting matrix:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.44.1">print("INPUT MATRIX A") 
print(":\n%s" % A) 
print("INPUT MATRIX B") 
print(":\n%s" % B) 
print("RESULT MATRIX C = A*B") 
print(":\n%s" % C) </span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How it works...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The</span><span><span class="koboSpan" id="kobo.3.1"> </span></span><kbd><span class="koboSpan" id="kobo.4.1">@guvectorize</span></kbd><span><span class="koboSpan" id="kobo.5.1"> </span></span><span class="koboSpan" id="kobo.6.1">decorator works on array arguments, taking four arguments in order to specify the</span><span><span class="koboSpan" id="kobo.7.1"> </span></span><kbd><span class="koboSpan" id="kobo.8.1">gufunc</span></kbd><span><span class="koboSpan" id="kobo.9.1"> </span></span><span class="koboSpan" id="kobo.10.1">signature:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.11.1">The first three arguments specify the types of data to be managed and arrays of integers:</span><span><span class="koboSpan" id="kobo.12.1"> </span></span><kbd><span class="koboSpan" id="kobo.13.1">void(int64[:,:], int64[:,:], int64[:,:])</span></kbd><span class="koboSpan" id="kobo.14.1">.</span></li>
<li><span class="koboSpan" id="kobo.15.1">The last argument of</span><span><span class="koboSpan" id="kobo.16.1"> </span></span><kbd><span class="koboSpan" id="kobo.17.1">@guvectorize</span></kbd><span><span class="koboSpan" id="kobo.18.1"> </span></span><span class="koboSpan" id="kobo.19.1">specifies how to manipulate the matrix dimensions: </span><kbd><span class="koboSpan" id="kobo.20.1">(m,n),(n,p)-&gt;(m,p)</span></kbd><span class="koboSpan" id="kobo.21.1">.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.22.1">Then, the matrix multiplication operation </span><span><span class="koboSpan" id="kobo.23.1">is defined, where</span></span><span class="koboSpan" id="kobo.24.1"> </span><kbd><span class="koboSpan" id="kobo.25.1">A</span></kbd><span><span class="koboSpan" id="kobo.26.1"> </span></span><span class="koboSpan" id="kobo.27.1">and</span><span><span class="koboSpan" id="kobo.28.1"> </span></span><kbd><span class="koboSpan" id="kobo.29.1">B</span></kbd><span><span class="koboSpan" id="kobo.30.1"> </span></span><span class="koboSpan" id="kobo.31.1">are the input matrices and</span><span><span class="koboSpan" id="kobo.32.1"> </span></span><kbd><span class="koboSpan" id="kobo.33.1">C</span></kbd><span><span class="koboSpan" id="kobo.34.1"> </span></span><span class="koboSpan" id="kobo.35.1">is the output matrix:</span><span><span class="koboSpan" id="kobo.36.1"> </span></span><em><span class="koboSpan" id="kobo.37.1">A(m,n)* B(n,p) = C(m,p)</span></em><span><span class="koboSpan" id="kobo.38.1">, </span></span><span class="koboSpan" id="kobo.39.1">where</span><span><span class="koboSpan" id="kobo.40.1"> </span></span><em><span class="koboSpan" id="kobo.41.1">m</span></em><span class="koboSpan" id="kobo.42.1">,</span><span><span class="koboSpan" id="kobo.43.1"> </span></span><em><span class="koboSpan" id="kobo.44.1">n</span></em><span class="koboSpan" id="kobo.45.1">, and</span><span><span class="koboSpan" id="kobo.46.1"> </span></span><em><span class="koboSpan" id="kobo.47.1">p</span></em><span><span class="koboSpan" id="kobo.48.1"> </span></span><span class="koboSpan" id="kobo.49.1">are the matrix dimensions.</span></p>
<p><span class="koboSpan" id="kobo.50.1">The matrix product is performed through</span><span><span class="koboSpan" id="kobo.51.1"> </span></span><span class="koboSpan" id="kobo.52.1">three</span><span><span class="koboSpan" id="kobo.53.1"> </span></span><kbd><span class="koboSpan" id="kobo.54.1">for</span></kbd><span class="koboSpan" id="kobo.55.1"> loops along with the matrix indices:</span></p>
<pre><span class="koboSpan" id="kobo.56.1">      for i in range(m): 
            for j in range(p): 
                C[i, j] = 0 
                for k in range(n): 
                      C[i, j] += A[i, k] * B[k, j] </span></pre>
<p><span><span class="koboSpan" id="kobo.57.1">The </span></span><kbd><span class="koboSpan" id="kobo.58.1">randint</span></kbd><span><span class="koboSpan" id="kobo.59.1"> NumPy </span></span><span class="koboSpan" id="kobo.60.1">function is used here to build the input matrices of 10 × 10 dimensions:</span></p>
<pre><span class="koboSpan" id="kobo.61.1">dim = 10</span><br/><span class="koboSpan" id="kobo.62.1">A = np.random.randint(dim,size=(dim, dim))</span><br/><span class="koboSpan" id="kobo.63.1">B = np.random.randint(dim,size=(dim, dim))</span></pre>
<p><span class="koboSpan" id="kobo.64.1">Finally, the</span><span><span class="koboSpan" id="kobo.65.1"> </span></span><kbd><span class="koboSpan" id="kobo.66.1">matmul</span></kbd><span><span class="koboSpan" id="kobo.67.1"> </span></span><span class="koboSpan" id="kobo.68.1">function is called with these matrices as arguments, and the resultant</span><span><span class="koboSpan" id="kobo.69.1"> </span></span><kbd><span class="koboSpan" id="kobo.70.1">C</span></kbd><span><span class="koboSpan" id="kobo.71.1"> </span></span><span class="koboSpan" id="kobo.72.1">matrix is printed out:</span></p>
<pre><span class="koboSpan" id="kobo.73.1">C = matmul(A, B)</span><br/><span class="koboSpan" id="kobo.74.1">print("RESULT MATRIX C = A*B")</span><br/><span class="koboSpan" id="kobo.75.1">print(":\n%s" % C)</span></pre>
<p><span class="koboSpan" id="kobo.76.1">To execute this example, type the following:</span></p>
<pre><strong><span class="koboSpan" id="kobo.77.1">(base) C:\&gt;python matMulNumba.py</span></strong></pre>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.78.1">The result shows the two matrices given as input and the matrix resulting from their product:</span></p>
<pre><strong><span class="koboSpan" id="kobo.79.1">INPUT MATRIX A</span></strong><br/><strong><span class="koboSpan" id="kobo.80.1">:</span></strong><br/><strong><span class="koboSpan" id="kobo.81.1">[[8 7 1 3 1 0 4 9 2 2]</span></strong><br/><strong><span class="koboSpan" id="kobo.82.1"> [3 6 2 7 7 9 8 4 4 9]</span></strong><br/><strong><span class="koboSpan" id="kobo.83.1"> [8 9 9 9 1 1 1 1 8 0]</span></strong><br/><strong><span class="koboSpan" id="kobo.84.1"> [0 5 0 7 1 3 2 0 7 3]</span></strong><br/><strong><span class="koboSpan" id="kobo.85.1"> [4 2 6 4 1 2 9 1 0 5]</span></strong><br/><strong><span class="koboSpan" id="kobo.86.1"> [3 0 6 5 1 0 4 3 7 4]</span></strong><br/><strong><span class="koboSpan" id="kobo.87.1"> [0 9 7 2 1 4 3 3 7 3]</span></strong><br/><strong><span class="koboSpan" id="kobo.88.1"> [1 7 2 7 1 8 0 3 4 1]</span></strong><br/><strong><span class="koboSpan" id="kobo.89.1"> [5 1 5 0 7 7 2 3 0 9]</span></strong><br/><strong><span class="koboSpan" id="kobo.90.1"> [4 6 3 6 0 3 3 4 1 2]]</span></strong><br/><strong><span class="koboSpan" id="kobo.91.1">INPUT MATRIX B</span></strong><br/><strong><span class="koboSpan" id="kobo.92.1">:</span></strong><br/><strong><span class="koboSpan" id="kobo.93.1">[[2 1 4 6 6 4 9 9 5 2]</span></strong><br/><strong><span class="koboSpan" id="kobo.94.1"> [8 6 7 6 5 9 2 1 0 9]</span></strong><br/><strong><span class="koboSpan" id="kobo.95.1"> [4 1 2 4 8 2 9 5 1 4]</span></strong><br/><strong><span class="koboSpan" id="kobo.96.1"> [9 9 1 5 0 5 1 1 7 1]</span></strong><br/><strong><span class="koboSpan" id="kobo.97.1"> [8 7 8 3 9 1 4 3 1 5]</span></strong><br/><strong><span class="koboSpan" id="kobo.98.1"> [7 2 5 8 3 5 8 5 6 2]</span></strong><br/><strong><span class="koboSpan" id="kobo.99.1"> [5 3 1 4 3 7 2 9 9 5]</span></strong><br/><strong><span class="koboSpan" id="kobo.100.1"> [8 7 9 3 4 1 7 8 0 4]</span></strong><br/><strong><span class="koboSpan" id="kobo.101.1"> [3 0 4 2 3 8 8 8 6 2]</span></strong><br/><strong><span class="koboSpan" id="kobo.102.1"> [8 6 7 1 8 3 0 8 8 9]]</span></strong><br/><strong><span class="koboSpan" id="kobo.103.1">RESULT MATRIX C = A*B</span></strong><br/><strong><span class="koboSpan" id="kobo.104.1">:</span></strong><br/><strong><span class="koboSpan" id="kobo.105.1">[[225 172 201 161 170 172 189 230 127 169]</span></strong><br/><strong><span class="koboSpan" id="kobo.106.1"> [400 277 289 251 278 276 240 324 295 273]</span></strong><br/><strong><span class="koboSpan" id="kobo.107.1"> [257 171 177 217 208 254 265 224 176 174]</span></strong><br/><strong><span class="koboSpan" id="kobo.108.1"> [187 130 116 117 94 175 105 128 152 114]</span></strong><br/><strong><span class="koboSpan" id="kobo.109.1"> [199 133 117 143 168 156 143 214 188 157]</span></strong><br/><strong><span class="koboSpan" id="kobo.110.1"> [180 118 124 113 152 149 175 213 167 122]</span></strong><br/><strong><span class="koboSpan" id="kobo.111.1"> [238 142 186 165 188 215 202 200 139 192]</span></strong><br/><strong><span class="koboSpan" id="kobo.112.1"> [237 158 162 176 122 185 169 140 137 130]</span></strong><br/><strong><span class="koboSpan" id="kobo.113.1"> [249 160 220 159 249 125 201 241 169 191]</span></strong><br/><strong><span class="koboSpan" id="kobo.114.1"> [209 152 142 154 131 160 147 161 132 137]]</span></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">There's more...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Writing an algorithm for a reduction operation using PyCUDA can be quite complex. </span><span class="koboSpan" id="kobo.2.2">For this purpose, Numba provides the</span><span><span class="koboSpan" id="kobo.3.1"> </span></span><kbd><span class="koboSpan" id="kobo.4.1">@reduce</span></kbd><span><span class="koboSpan" id="kobo.5.1"> </span></span><span class="koboSpan" id="kobo.6.1">decorator for converting simple binary operations into </span><em><span class="koboSpan" id="kobo.7.1">reduction kernels</span></em><span class="koboSpan" id="kobo.8.1">.</span></p>
<p><span class="koboSpan" id="kobo.9.1">Reduction operations reduce a set of values to a single value. </span><span class="koboSpan" id="kobo.9.2">A typical example of a </span><span><span class="koboSpan" id="kobo.10.1">reduction</span></span><span class="koboSpan" id="kobo.11.1"> operation is to calculate the sum of all the elements of an array. </span><span class="koboSpan" id="kobo.11.2">As an example, consider the following array of elements: 1, 2, 3, 4, 5, 6, 7, 8.</span></p>
<p><span class="koboSpan" id="kobo.12.1">The sequential algorithm operates in the way shown in the diagram, that is, adding the elements of the array one after the other:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.13.1"><img src="assets/7e5ea317-7653-4c24-96f3-8ea106d866df.png" style="width:32.08em;height:27.33em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="hps"><span class="koboSpan" id="kobo.14.1">Sequential sum</span></span></div>
<p><span class="koboSpan" id="kobo.15.1">A parallel algorithm operates according to the following schema:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.16.1"><img src="assets/3704575d-6b42-4dc7-b4f9-01093cb44870.png" style="width:38.17em;height:14.50em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="hps"><span class="koboSpan" id="kobo.17.1">Parallel sum</span></span></div>
<p><span class="koboSpan" id="kobo.18.1">It is clear that the latter has the advantage of shorter execution time.</span></p>
<p><span class="koboSpan" id="kobo.19.1">By using Numba and the</span><span><span class="koboSpan" id="kobo.20.1"> </span></span><kbd><span class="koboSpan" id="kobo.21.1">@reduce</span></kbd><span><span class="koboSpan" id="kobo.22.1"> </span></span><span class="koboSpan" id="kobo.23.1">decorator, we can write an algorithm, </span><span><span class="koboSpan" id="kobo.24.1">in a few lines of code, </span></span><span class="koboSpan" id="kobo.25.1">for the parallel sum on an array of integers ranging from 1 to 10,000:</span></p>
<pre><span class="koboSpan" id="kobo.26.1">import numpy 
from numba import cuda 
 
@cuda.reduce 
def sum_reduce(a, b): 
    return a + b 
 
A = (numpy.arange(10000, dtype=numpy.int64)) + 1</span><br/><span class="koboSpan" id="kobo.27.1">print(A) 
got = sum_reduce(A)</span><br/><span class="koboSpan" id="kobo.28.1">print(got) </span></pre>
<p><span class="koboSpan" id="kobo.29.1">The previous example can be performed by typing the following command:</span></p>
<pre><strong><span class="koboSpan" id="kobo.30.1">(base) C:\&gt;python reduceNumba.py</span></strong></pre>
<p><span class="koboSpan" id="kobo.31.1">The</span><span><span class="koboSpan" id="kobo.32.1"> following</span></span><span class="koboSpan" id="kobo.33.1"> result </span><span><span class="koboSpan" id="kobo.34.1">is </span></span><span class="koboSpan" id="kobo.35.1">provided:</span></p>
<pre><span class="koboSpan" id="kobo.36.1">vector to reduce = [ 1 2 3 ... </span><span class="koboSpan" id="kobo.36.2">9998 9999 10000]</span><br/><span class="koboSpan" id="kobo.37.1">result = 50005000</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">See also</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the following repository, you can find many examples of Numba:</span><span><span class="koboSpan" id="kobo.3.1"> </span></span><a href="https://github.com/numba/numba-examples"><span class="koboSpan" id="kobo.4.1">https://github.com/numba/numba-examples</span></a><span class="koboSpan" id="kobo.5.1">.  An interesting introduction to Numba and CUDA programming can be found at </span><a href="https://nyu-cds.github.io/python-numba/"><span class="koboSpan" id="kobo.6.1">https://nyu-cds.github.io/python-numba/05-cuda/</span></a><span class="koboSpan" id="kobo.7.1">.</span><a href="https://nyu-cds.github.io/python-numba/"/></p>


            </article>

            
        </section>
    </body></html>