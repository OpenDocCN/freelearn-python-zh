<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<meta charset="utf-8"/>
<meta content="pandoc" name="generator"/>
<title>ch008.xhtml</title>

<!-- kobo-style -->
<style id="koboSpanStyle" type="text/css" xmlns="http://www.w3.org/1999/xhtml">.koboSpan { -webkit-text-combine: inherit; }</style>
</head>
<body epub:type="bodymatter">

<h1 data-number="8">Chapter 4<br/>
Data Acquisition Features: Web APIs and Scraping</h1>
<p>Data analysis often works with data from numerous sources, including databases, web services, and files prepared by other applications. In this chapter, you will be guided through two projects to add additional data sources to the baseline application from the previous chapter. These new sources include a web service query, and scraping data from a web page.</p>
<p>This chapter’s projects cover the following essential skills:</p>
<ul>
<li><p>Using the <strong>requests </strong>package for Web API integration. We’ll look at the Kaggle API, which requires signing up to create an API token.</p></li>
<li><p>Using the <strong>Beautiful Soup </strong>package to parse an HTML web page.</p></li>
<li><p>Adding features to an existing application and extending the test suite to cover these new alternative data sources.</p></li>
</ul>
<p>It’s important to recognize this application has a narrow focus on data acquisition. In later chapters, we’ll validate the data and convert it to a more useful form. This reflects a separation of the following distinct concerns:</p>
<ul>
<li><p>Downloading and extracting data from the source are the foci of this chapter and the next.</p></li>
<li><p>Inspection begins in <a href="ch010.xhtml#x1-1460006"><em>Chapter</em><em> 6</em></a>, <a href="ch010.xhtml#x1-1460006"><em>Project 2.1: Data Inspection Notebook</em></a>.</p></li>
<li><p>Validating and cleaning the data starts in <a href="ch013.xhtml#x1-2080009"><em>Chapter</em><em> 9</em></a>, <a href="ch013.xhtml#x1-2080009"><em>Project 3.1: Data</em> <em>Cleaning Base Application</em></a>.</p></li>
</ul>
<p>Each stage in the processing pipeline is allocated to separate projects. For more background, see <a href="ch006.xhtml#x1-470002"><em>Chapter</em><em> 2</em></a>, <a href="ch006.xhtml#x1-470002"><em>Overview of the Projects</em></a>.</p>
<p>We’ll start by looking at getting data using an API and a RESTful web service. This will focus on the Kaggle site, which means you will need to sign up with Kaggle to get your own, unique API key. The second project will scrape HTML content from a website that doesn’t offer a useful API. </p>

<h2 data-number="8.1">4.1  Project 1.2: Acquire data from a web service</h2>
<p>It’s common to need data provided by a Web API. One common design approach for web services is called RESTful; it’s based on a number of concepts related to using the HTTP protocol to transfer a representation of an object’s state.</p>
<p>For more information on RESTful services, see <em>Building RESTful Python Web Services</em> (<a class="url" href="https://www.packtpub.com/product/building-restful-python-web-services/9781786462251">https://www.packtpub.com/product/building-restful-python-web-services/9781786462251</a>).</p>
<p>A RESTful service generally involves using the HTTP protocol to respond to requests from client applications. The spectrum of request types includes a number of verbs like get, post, put, patch, and delete. In many cases, the service responds with a JSON document. It’s also possible to receive a file that’s a stream of NDJSON documents, or even a file that’s a ZIP archive of data.</p>
<p>We’ll start with a description of the application, and then move on to talk about the architectural approach. This will be followed with a detailed list of deliverables. </p>

<h3 data-number="8.1.1">4.1.1  Description</h3>
<p>Analysts and decision-makers need to acquire data for further analysis. In this case, the data is available from a RESTful web service. One of the most fun small data sets to work with is Anscombe’s Quartet – <a class="url" href="https://www.kaggle.com/datasets/carlmcbrideellis/data-anscombes-quartet">https://www.kaggle.com/datasets/carlmcbrideellis/data-anscombes-quartet</a></p>
<p>Parts of this application are an extension to the project in <a href="ch013.xhtml#x1-2080009"><em>Chapter</em><em> 9</em></a>, <a href="ch013.xhtml#x1-2080009"><em>Project</em> <em>3.1: Data Cleaning Base Application</em></a>. The essential behavior of this application will be similar to the previous project. This project will use a CLI application to grab data from a source.</p>
<p>The <strong>User Experience </strong>(<strong>UX</strong>) will also be a command-line application with options to fine-tune the data being gathered. Our expected command line should like something like the following:</p>
<div><div><pre class="console">% python src/acquire.py -o quartet -k ~/Downloads/kaggle.json \
  --zip carlmcbrideellis/data-anscombes-quartet</pre>
</div>
</div>
<p>The <code>-o</code><code> quartet</code> argument specifies a directory into which four results are written. These will have names like <code>quartet/series_1.json</code>.</p>
<p>The <code>-k</code><code> kaggle.json</code> argument is the name of a file with the username and Kaggle API token. This file is kept separate from the application software. In the example, the file was in the author’s <code>Downloads</code> folder.</p>
<p>The <code>--zip</code> argument provides the ”reference” — the owner and data set name — to open and extract. This information is found by examining the details of the Kaggle interface.</p>
<p>An additional feature is to get a filtered list of Kaggle data sets. This should be a separate <code>--search</code> operation that can be bundled into a single application program.</p>
<pre class="console">% python src/acquire.py --search -k ~/Downloads/kaggle.json</pre>
<p>This will apply some search criteria to emit a list of data sets that match the requirements. The lists tend to be quite large, so this needs to be used with care.</p>
<p>The credentials in the file are used to make the Kaggle API request. In the next sections, we’ll look at the Kaggle API in general. After that, we’ll look at the specific requests required to locate the reference to the target data set.</p>

<h4 class="likesubsubsectionHead" data-number="8.1.1.1">The Kaggle API</h4>
<p>See <a class="url" href="https://www.kaggle.com/docs/api">https://www.kaggle.com/docs/api</a> for information on the Kaggle API. This document describes some command-line code (in Python) that uses the API.</p>
<p>The technical details of the RESTful API requests are at <a class="url" href="https://github.com/Kaggle/kaggle-api/blob/master/KaggleSwagger.yaml">https://github.com/Kaggle/kaggle-api/blob/master/KaggleSwagger.yaml</a>. This document describes the requests and responses from the Kaggle API server.</p>
<p>To make use of the RESTful API or the command-line applications, you should register with Kaggle. First, sign up with <code>Kaggle.com</code>. Next, navigate to the public profile page. On this page, there’s an API section. This section has the buttons you will use to generate a unique API token for your registered username.</p>
<p>The third step is to click the <strong>Create New Token </strong>button to create the token file. This will download a small JSON file with your registered username and unique key. These credentials are required by the Kaggle REST API.</p>
<p>The ownership of this file can be changed to read-only by the owner. In Linux and macOS, this is done with the following command:</p>
<div><div><pre class="console">% chmod 400 ~/Downloads/kaggle.json</pre>
</div>
</div>
<div><div><p>Do not move the Kaggle credentials file named <code>kaggle.json</code> into a directory where your code is also located. It’s tempting, but it’s a terrible security mistake because the file could get saved to a code repository and become visible to anyone browsing your code. In some enterprises, posting keys in code repositories — even internal repositories — is a security lapse and a good reason for an employee to be terminated.</p>
<p>Because Git keeps a very complete history, it’s challenging to remove a commit that contains keys.</p>
<p><strong>Keep the credentials file separate from your code.</strong></p>
<p>It’s also a good idea to add <code>kaggle.json</code> to a <code>.gitignore</code> file to make extra sure that it won’t be uploaded as part of a commit and push.</p>
</div>
</div>


<h4 class="likesubsubsectionHead" data-number="8.1.1.2">About the source data</h4>
<p>This project will explore two separate kinds of source data. Both sources have the same base path of <a class="url" href="https://www.kaggle.com/api/v1/">https://www.kaggle.com/api/v1/</a>. Trying to query this base path won’t provide a useful response; it’s only the starting point for the paths that are built to locate specific resources.</p>
<ul>
<li><p>JSON documents with summaries of data sets or metadata about data sets. These come from appending <code>datasets/list</code> to the base path.</p></li>
<li><p>A ZIP archive that contains the data we’ll use as an example. This comes from appending <code>datasets/download/{ownerSlug}/{datasetSlug}</code> to the base path. The <code>ownerSlug</code> value is ”carlmcbrideellis”. The <code>datasetSlug</code> value is ”data-anscombes-quartet”. A given data set has a <code>ref</code> value as a reference string with the required ”ownerSlug/datasetSlug” format.</p></li>
</ul>
<p>The JSON documents require a function to extract a few relevant fields like <code>title</code>, <code>ref</code>, <code>url</code>, and <code>totalBytes</code>. This subset of the available metadata can make it easier to locate useful, interesting data sets. There are numerous other properties available for search, like <code>usabilityRating</code>; these attributes can distinguish good data sets from experiments or classroom work.</p>
<p>The suggested data set — the Anscombe Quartet — is available as a ZIP-compressed archive with a single item inside it. This means the application must handle ZIP archives and expand a file contained within the archive. Python offers the <code>zipfile</code> package to handle locating the CSV file within the archive. Once this file is found, the existing programming from the previous chapter (<a href="ch007.xhtml#x1-560003"><em>Chapter</em><em> 3</em></a>, <a href="ch007.xhtml#x1-560003"><em>Project 1.1: Data Acquisition Base Application</em></a>) can be used.</p>
<p>There are thousands of Kaggle data sets. We’ll suggest some alternatives to the Anscombe Quartet in the <a href="#x1-1080004"><em>Extras</em></a>.</p>
<p>This section has looked at the input, processing, and output of this application. In the next section, we’ll look at the overall architecture of the software. </p>



<h3 data-number="8.1.2">4.1.2  Approach</h3>
<p>We’ll take some guidance from the C4 model ( <a class="url" href="https://c4model.com">https://c4model.com</a>) when looking at our approach:</p>
<ul>
<li><p><strong>Context</strong>: For this project, a context diagram would show a user extracting data from a source. You may find it helpful to draw this diagram.</p></li>
<li><p><strong>Containers</strong>: One container is the user’s personal computer. The other container is the Kaggle website, which provides the data.</p></li>
<li><p><strong>Components</strong>: We’ll address the components below.</p></li>
<li><p><strong>Code</strong>: We’ll touch on this to provide some suggested directions.</p></li>
</ul>
<p>It’s important to consider this application as an extension to the project in <a href="ch007.xhtml#x1-560003"><em>Chapter</em><em> 3</em></a>, <a href="ch007.xhtml#x1-560003"><em>Project 1.1: Data Acquisition Base Application</em></a>. The base level of architectural design is provided in that chapter.</p>
<p>In this project, we’ll be adding a new <code>kaggle_client</code> module to download the data. The overall application in the <code>acquire</code> module will change to make use of this new module. The other modules should remain unchanged.</p>
<p>The legacy component diagram is shown in <a href="#4.1"><em>Figure 4.1</em></a>.</p>
<figure class="IMG---Figure">
<img alt="Figure 4.1: Legacy Components " src="img/file14.jpg"/>
<figcaption class="IMG---Caption">Figure 4.1: Legacy Components </figcaption>
</figure>
<p>A new architecture can handle both the examination of the JSON data set listing, as well as the download of a single ZIP file. This is shown in <a href="#4.2"><em>Figure 4.2</em></a>.</p>
<figure class="IMG---Figure">
<img alt="Figure 4.2: Revised Component Design " src="img/file15.jpg"/>
<figcaption class="IMG---Caption">Figure 4.2: Revised Component Design </figcaption>
</figure>
<p>The new module here is the <code>kaggle_client</code> module. This has a class, <code>RestAccess</code>, that provides methods to access Kaggle data. It can reach into the Kaggle data set collection and retrieve a desired ZIP file. Additional methods can be added to examine the list of data sets or get data set metadata.</p>
<p>The <code>RestAccess</code> class is initialized with the contents of the <code>kaggle.json</code> file. As part of initialization, it can create the required authentication object for use in all subsequent calls.</p>
<p>In the following sections, we’ll look at these features of the <code>RestAccess</code> class:</p>
<ul>
<li><p>Making API requests in general.</p></li>
<li><p>Getting the ZIP archive.</p></li>
<li><p>Getting the list of data sets.</p></li>
<li><p>Handling the rate-limiting response.</p></li>
</ul>
<p>We’ll start with the most important feature, making API requests in a general way.</p>

<h4 class="likesubsubsectionHead" data-number="8.1.2.1">Making API requests</h4>
<p>The component diagram shows the <code>requests</code> package as the preferred way to access RESTful APIs. This package should be added to the project’s <code>pyproject.toml</code> and installed as part of the project’s virtual environment.</p>
<p>It’s also sensible to make RESTful API requests with the <code>urllib</code> package. This is part of the standard library. It works nicely and requires no additional installation. The code can become rather complicated-looking, however, so it’s not as highly recommended as the <code>requests</code> package.</p>
<p>The essential benefit of using <code>requests</code> is creating an authentication object and providing it in each request. We often use code like the following example:</p>
<div><div><pre class="source-code">import json
from pathlib import Path
import requests.auth

keypath = Path.home() / "Downloads" / "kaggle.json"
with keypath.open() as keyfile:
    credentials = json.load(keyfile)
auth = requests.auth.HTTPBasicAuth(
    credentials[’username’], credentials[’key’]
)</pre>
</div>
</div>
<p>This can be part of the <code>__init__()</code> method of the <code>RestAccess</code> class.</p>
<p>The <code>auth</code> object created here can be used to make all subsequent requests. This will provide the necessary username and API token to validate the user. This means other methods can, for example, use <code>requests.get()</code> with a keyword parameter value of <code>auth=self.auth</code>. This will correctly build the needed <code>Authorization</code> headers in each request.</p>
<p>Once the class is initialized properly, we can look at the method for downloading a ZIP archive</p>


<h4 class="likesubsubsectionHead" data-number="8.1.2.2">Downloading a ZIP archive</h4>
<p>The <code>RestAccess</code> class needs a <code>get_zip()</code> method to download the ZIP file. The parameter is the URL for downloading the requested data set.</p>
<p>The best approach to building this URL for this data set is to combine three strings:</p>
<ul>
<li><p>The base address for the APIs, <code>https://www.kaggle.com/api/v1</code>.</p></li>
<li><p>The path for downloads, <code>/datasets/download/</code>.</p></li>
<li><p>The reference is a string with the form: <code>{ownerSlug}/{datasetSlug}</code>.</p></li>
</ul>
<p>This is an ideal place for a Python f-string to replace the reference in the URL pattern.</p>
<p>The output from the <code>get_zip()</code> method should be a <code>Path</code> object. In some cases, the ZIP archives are gigantic and can’t be processed entirely in memory. In these extreme cases, a more complicated, chunked download is required. For these smaller files used by this project, the download can be handled entirely in memory. Once the ZIP file has been written, the client of this <code>RestAccess</code> class can then open it and extract the useful member.</p>
<p>A separate client function or class will process the content of the ZIP archive file. The following is <strong>not </strong>part of the <code>RestAccess</code> class but is part of some client class or function that uses the <code>RestAccess</code> class.</p>
<p>Processing an element of an archive can be done with two nested <code>with</code> contexts. They would work like this:</p>
<ul>
<li><p>An outer <code>with</code> statement uses the <code>zipfile</code> module to open the archive, creating a <code>ZipFile</code> instance.</p></li>
<li><p>An inner <code>with</code> statement can open the specific member with the Anscombe quartet CSV file. Inside this context, the application can create a <code>csv.DictReader</code> and use the existing <code>Extract</code> class to read and process the data.</p></li>
</ul>
<p>What’s important here is we don’t need to unpack the ZIP archive and litter our storage with unzipped files. An application can open and process the elements using the <code>ZipFile.open()</code> method.</p>
<p>In addition to downloading the ZIP archive, we may also want to survey the available data sets. For this, a special iterator method is helpful. We’ll look at that next.</p>


<h4 class="likesubsubsectionHead" data-number="8.1.2.3">Getting the data set list</h4>
<p>The catalog of data sets is found by using the following path:</p>
<p><a class="url" href="https://www.kaggle.com/api/v1/datasets/list">https://www.kaggle.com/api/v1/datasets/list</a></p>
<p>The <code>RestAccess</code> class can have a <code>dataset_iter()</code> method to iterate through the collection of data sets. This is helpful for locating other data sets. It’s not required for finding the Anscombe’s Quartet, since the <code>ownerSlug</code> and <code>datasetSlug</code> reference information is already known.</p>
<p>This method can make a <code>GET</code> request via the <code>requests.get()</code> function to this URL. The response will be on the first page of available Kaggle data sets. The results are provided in pages, and each request needs to provide a page number as a parameter to get subsequent pages.</p>
<p>Each page of results will be a JSON document that contains a sequence of dictionary objects. It has the following kind of structure:</p>
<pre class="source-code">[
    {"id": some_number, "ref": "username/dataset", "title": ...},
    {"id": another_number, "ref": "username/dataset", "title": ...},
    etc.
]</pre>
<p>This kind of two-tiered structure — with pages and items within each page — is the ideal place to use a generator function to iterate through the pages. Within an outer cycle, an inner iteration can yield the individual data set rows from each page.</p>
<p>This nested iteration can look something like the following code snippet:</p>
<div><div><pre class="source-code">def dataset_iter(url: str, query: dict[str, str]) -&gt;
  Iterator[dict[str, str]]:
    page = 1
    while True:
        response = requests.get(url, params=quert | {"page": str(page)})
        if response.status_code == 200:
            details = response.json()
            if details:
                yield from iter(details)
                page += 1
            else:
                break
        elif response.status_code == 429:
            # Too Many Requests
            # Pause and try again processing goes here...
            pass
        else:
            # Unexpected response
            # Error processing goes here...
            break</pre>
</div>
</div>
<p>This shows the nested processing of the <code>while</code> statement ends when a response contains a page of results with zero entries in it. The processing to handle too many requests is omitted. Similarly, the logging of unexpected responses is also omitted.</p>
<p>A client function would use the <code>RestAccess</code> class to scan data sets and would look like the following example:</p>
<div><div><pre class="source-code">keypath = Path.home()/"Downloads"/"kaggle.json"
with keypath.open() as keyfile:
    credentials = json.load(keyfile)

reader = Access(credentials)
for row in reader.dataset_iter(list_url):
    print(row[’title’], row[’ref’], row[’url’], row[’totalBytes’])</pre>
</div>
</div>
<p>This will process all of the data set descriptions returned by the <code>RestReader</code> object, <code>reader</code>. The <code>dataset_iter()</code> method needs to accept a <code>query</code> parameter that can limit the scope of the search. We encourage you to read the OpenAPI specification to see what options are possible for the <code>query</code> parameter. These values will become part of the query string in the HTTP <code>GET</code> request.</p>
<p>Here’s the formal definition of the interface:</p>
<p><a class="url" href="https://github.com/Kaggle/kaggle-api/blob/master/KaggleSwagger.yaml">https://github.com/Kaggle/kaggle-api/blob/master/KaggleSwagger.yaml</a></p>
<p>Some of the query parameters include the following:</p>
<ul>
<li><p>The <code>filetype</code> query is helpful in locating data in JSON or CSV formats.</p></li>
<li><p>The <code>maxSize</code> query can constrain the data sets to a reasonable size. For initial exploration, 1MB is a good upper limit.</p></li>
</ul>
<p>Initial spike solutions — without regard to the rate limiting — will turn up at least 80 pages of possible data sets. Handling the rate-limiting response will produce more extensive results, at the cost of some time spent waiting. In the next section, we’ll expand this method to handle the error response.</p>


<h4 class="likesubsubsectionHead" data-number="8.1.2.4">Rate limiting</h4>
<p>As with many APIs, the Kaggle API imposes rate-limiting to avoid a <strong>Denial-of-Service </strong>(<strong>DoS</strong>) attack. For more information see <a class="url" href="https://cheatsheetseries.owasp.org/cheatsheets/Denial_of_Service_Cheat_Sheet.html">https://cheatsheetseries.owasp.org/cheatsheets/Denial_of_Service_Cheat_Sheet.html</a>.</p>
<p>Each user has a limited number of requests per second. While the limit is generous for most purposes, it will tend to prevent a simple scan of <strong>all </strong>data sets.</p>
<p>A status code of 429 in a Kaggle response tells the client application that too many requests were made. This ”too many requests” error response will have a header with the key <code>Retry-After</code>. This header’s value is the timeout interval (in seconds) before the next request can be made.</p>
<p>A reliable application will have a structure that handles the 429 vs. 200 responses gracefully. The example in the previous section has a simple <code>if</code> statement to check the condition <code>if</code><code> response.status_code</code><code> ==</code><code> 200</code>. This needs to be expanded to handle these three alternatives:</p>
<ul>
<li><p>A status code of 200 is a good response. If the page has any details, these can be processed; the value of <code>page</code> can be incremented. Otherwise, there’s no more data making it appropriate to break from the containing <code>while</code> statement.</p></li>
<li><p>A status code of 429 means too many requests were made. Get the value of the <code>Retry-After</code> and sleep for this period of time.</p></li>
<li><p>Any other status code indicates a problem and should be logged or raised as an exception.</p></li>
</ul>
<p>One possible algorithm for return rows and handling rate limiting delays is shown in <a href="#4.3"><em>Figure 4.3</em></a>.</p>
<figure class="IMG---Figure">
<img alt="Figure 4.3: Kaggle rate-limited paging " src="img/file16.jpg"/>
<figcaption class="IMG---Caption">Figure 4.3: Kaggle rate-limited paging </figcaption>
</figure>
<p>Handling rate limiting will make the application much easier to use. It will also produce more complete results. Using an effective search filter to reduce the number of rows to a sensible level will save a lot of waiting for the retry-after delays.</p>


<h4 class="likesubsubsectionHead" data-number="8.1.2.5">The main() function</h4>
<p>The current application design has these distinct features:</p>
<ol>
<li><div><p>Extract data from a local CSV file.</p>
</div></li>
<li><div><p>Download a ZIP archive and extract data from a CSV member of the archive.</p>
</div></li>
<li><div><p>(Optionally) Survey the data set list to find other interesting data sets to process.</p>
</div></li>
</ol>
<p>This suggests that our <code>main()</code> function should be a container for three distinct functions that implement each separate feature. The <code>main()</code> function can parse the command-line arguments, and then make a number of decisions:</p>
<ul>
<li><p><strong>Local Extract</strong>: If the <code>-o</code> option is present (without the <code>-k</code> option), then this is a local file extract. This was the solution from an earlier chapter.</p></li>
<li><p><strong>Download and Extract</strong>: If the <code>-k</code> and <code>-o</code> options are present, then this will be a download and extract. It will use the <code>RestAccess</code> object to get the ZIP archive. Once the archive is opened, the member processing is the solution from an earlier chapter.</p></li>
<li><p><strong>Survey</strong>: If the <code>-k</code> and <code>-s</code> (or <code>--search</code>) options are present, then this is a search for interesting data sets. You are encouraged to work out the argument design to provide the needed query parameters to the application.</p></li>
<li><p><strong>Otherwise</strong>: If none of the above patterns match the options, this is incoherent, and an exception should be raised.</p></li>
</ul>
<p>Each of these features requires a distinct function. A common alternative design is to use the <strong>Command </strong>pattern and create a class hierarchy with each of the features as a distinct subclass of some parent class.</p>
<p>One central idea is to keep the <code>main()</code> function small, and dispatch the detailed work to other functions or objects.</p>
<p>The other central idea is <strong>Don’t Repeat Yourself </strong>(<strong>DRY</strong>). This principle makes it imperative to <strong>never </strong>copy and paste code between the ”Download-and-Extract” feature and the ”Local-Extract” feature. The ”Download-and-Extract” processing must reuse the ”Local-Extract” processing either through subclass inheritance or calling one function from another.</p>
<p>Now that we have a technical approach, it’s time to look at the deliverables for this project. </p>



<h3 data-number="8.1.3">4.1.3  Deliverables</h3>
<p>This project has the following deliverables:</p>
<ul>
<li><p>Documentation in the <code>docs</code> folder.</p></li>
<li><p>Acceptance tests in the <code>tests/features</code> and <code>tests/steps</code> folders.</p></li>
<li><p>A miniature RESTful web service that provides test responses will be part of the acceptance test.</p></li>
<li><p>Unit tests for the application modules in the <code>tests</code> folder.</p></li>
<li><p>Mock objects for the <code>requests</code> module will be part of the unit tests.</p></li>
<li><p>Application to download and acquire data from a RESTful web service.</p></li>
</ul>
<p>Be sure to include additional packages like <code>requests</code> and <code>beautifulsoup4</code> in the <code>pyproject.toml</code> file. The <strong>pip-compile </strong>command can be used to create a <code>requirements.txt</code> usable by the <strong>tox </strong>tool for testing.</p>
<p>We’ll look at a few of these deliverables in a little more detail.</p>

<h4 class="likesubsubsectionHead" data-number="8.1.3.1">Unit tests for the RestAccess class</h4>
<p>For unit testing, we don’t want to involve the <code>requests</code> module. Instead, we need to make a mock interface for the <code>requests</code> module to confirm the application <code>RestAccess</code> module uses the <code>requests</code> classes properly.</p>
<p>There are two strategies for plugging in mock objects:</p>
<ul>
<li><p>Implement a dependency injection technique, where the target class is named at run time.</p></li>
<li><p>Use <em>Monkey Patching </em>to inject a mock class at test time.</p></li>
</ul>
<p>When working with external modules — modules where we don’t control the design — monkey patching is often easier than trying to work out a dependency injection technique. When we’re building the classes in a module, we often have a need to extend the definitions via subclasses. One of the reasons for creating unique, customized software is to implement change in the unique features of an application rapidly. The non-unique features (RESTful API requests, in this case) change very slowly and don’t benefit from flexibility.</p>
<p>We want to create two mock classes, one to replace the <code>requests.auth.HTTPBasicAuth</code> class, and one to replace the <code>requests.get()</code> function. The mock for the <code>HTTPBasicAuth</code> class doesn’t do anything; we want to examine the mock object to be sure it was called once with the proper parameters. The mock for the <code>requests.get()</code> function needs to create mock <code>Response</code> objects for various test scenarios.</p>
<p>We’ll need to use the <code>monkeypatch</code> fixture of the <code>pytest</code> module to replace the real objects with the mock objects for unit testing.</p>
<p>The idea is to create unit tests that have a structure similar to the following example:</p>
<div><div><pre class="source-code">from unittest.mock import Mock, sentinel, call

def test_rest_access(monkeypatch):
    mock_auth_class = Mock(
        name="Mocked HTTPBasicAuth class",
        return_value=sentinel.AUTH
    )
    monkeypatch.setattr(’requests.auth.HTTPBasicAuth’, mock_auth_class)
    mock_kaggle_json = {"username": sentinel.USERNAME, "key": sentinel.KEY}
    access = RestAccess(mock_kaggle_json)
    assert access.credentials == sentinel.AUTH
    assert mock_auth_class.mock_calls == [
        call(sentinel.USERNAME, sentinel.KEY)
    ]</pre>
</div>
</div>
<p>This test case creates a mock for the <code>HTTPBasicAuth</code> class. When the class is called to create an instance, it returns a <code>sentinel</code> object that can be verified by a test case.</p>
<p>The <code>monkeypatch</code> fixture replaces the <code>requests.auth.HTTPBasicAuth</code> class with the mock object. After this patch is applied, when the <code>RestAccess</code> class initialization attempts to create an instance of the <code>HTTPBasicAuth</code> class, it will invoke the mock, and will get a <code>sentinel</code> object instead.</p>
<p>The case confirms the <code>sentinel</code> object is used by the <code>RestAccess</code> instance. The test case also confirms the mock class was called exactly once with the expected values taken from the mocked value loaded from the <code>kaggle.json</code> file.</p>
<p>This test case relies on looking inside the <code>RestAccess</code> instance. This isn’t the best strategy for writing unit tests. A better approach is to provide a mock object for <code>requests.get()</code>. The test case should confirm the <code>requests.get()</code> is called with a keyword parameter, <code>auth</code>, with an argument value of the <code>sentinel.AUTH</code> object. The idea of this test strategy is to examine the external interfaces of the <code>RestAccess</code> class instead of looking at internal state changes.</p>


<h4 class="likesubsubsectionHead" data-number="8.1.3.2">Acceptance tests</h4>
<p>The acceptance tests need to rely on a <em>fixture </em>that mocks the Kaggle web service. The mock will be a process on your local computer, making it easy to stop and start the mock service to test the application. Using an address of <code>127.0.0.1:8080</code> instead of <code>www.kaggle.com</code> will direct RESTful API requests back to your own computer. The name <code>localhost:8080</code> can be used instead of the numeric address <code>127.0.0.1:8080</code>. (This address is called the <em>Loopback Address </em>because the requests loop back to the same host that created them, allowing testing to proceed without any external network traffic.)</p>
<p>Note that the URL scheme will change to <code>http:</code> from <code>https:</code>, also. We don’t want to implement the full Socket Security Layer (SSL) for acceptance testing. For our purposes, we can trust those components work.</p>
<p>This change to the URLs suggests the application should be designed in such a way to have the <code>https://www.kaggle.com</code> portion of each URL provided by a configuration parameter. Then acceptance tests can use <code>http://127.0.0.1:8080</code> without having to make any changes to the code.</p>
<p>The mock service must offer a few features of the Kaggle service. The local service needs to respond to <code>dataset/download</code> requests properly, providing a reply with the expected status code and the content with bytes that are a proper ZIP archive.</p>
<p>This mock service will run as a separate application. It will be started (and stopped) by <strong>behave </strong>for a scenario that needs the fixture.</p>
<p>We’ll start by looking at the way this service is described in a feature file. This will lead us to look at how to build the mock service. After that, we can look at how this is implemented using <strong>behave </strong>step definitions.</p>


<h4 class="likesubsubsectionHead" data-number="8.1.3.3">The feature file</h4>
<p>The downloading feature is clearly separate from the data acquisition feature. This suggests a new <code>.feature</code> file to provide scenarios to describe this feature.</p>
<p>Within this new feature file, we can have scenarios that specifically name the required fixture. A scenario might look like the following example:</p>
<div><div><pre class="source-code">@fixture.kaggle_server
Scenario: Request for carlmcbrideellis/data-anscombes-quartet
    extracts file from ZIP archive.
    A typical download command might be
    "python src/acquire.py -k kaggle.json -o quartet \
      --zip carlmcbrideellis/data-anscombes-quartet"

  Given proper keys are in "kaggle.json"
  When we run the kaggle download command
  Then log has INFO line with "header: [’mock’, ’data’]"
  And log has INFO line with "count: 1"</pre>
</div>
</div>
<p>The <code>@fixture.</code> tag follows the common tagging convention for associating specific fixtures with scenarios. There are many other purposes for tagging scenarios in addition to specifying the fixture to use.</p>
<p>In previous projects, a command to run the application was provided in the <code>When</code> step. For this scenario (and many others), the command text became too long to be usefully presented in the Gherkin text. This means the actual command needs to be provided by the function that implements this step.</p>
<p>This scenario’s <code>Then</code> steps look at the log created by the application to confirm the contents of the file.</p>
<div><div><p>A test scenario is part of the overall application’s requirements and design.</p>
<p>In the description provided in <a href="#x1-800001"><em>Description</em></a> there isn’t any mention of a log. This kind of gap is common. The test scenario provided additional definitions of the feature omitted from the plain test description.</p>
<p>Some people like to update the documentation to be complete and fully consistent. We encourage flexibility when working on enterprise applications where there are numerous stakeholders. It can be difficult to get everyone’s input into the initial presentation or document. Sometimes, requirements appear later in the process when more concrete issues, like expected operating scenarios, are discussed in detail.</p>
</div>
</div>
<p>The tag information will be used by the <strong>behave </strong>tool. We’ll look at how to write a <code>before_tag()</code> function to start (and stop) the special mock server for any scenario that needs it.</p>
<p>Before we look at the <strong>behave </strong>integration via a step definition, we’ll look at two approaches to testing the client application. The core concept is to create a mock-up of the few elements of the Kaggle API used by the data acquisition application. This mock-up must return responses used by test scenarios. There are two approaches:</p>
<ul>
<li><p>Create a web service application. For acceptance tests, this service must be started and stopped. The acquire application can be configured with a <code>http://localhost:8080</code> URL to connect to the test server instead of the Kaggle server. (There are a few common variations on the “localhost” address including <code>127.0.0.1</code> and <code>0.0.0.0</code>.)</p></li>
<li><p>The other approach is to provide a way to replace the <code>requests</code> module with a mocked version of the module. This mocked module returns responses appropriate to the test scenario. This can be done by manipulating the <code>sys.path</code> variable to include the directory containing the mocked version of <code>requests</code> in front of the <code>site-packages</code> directory, which has the real version. It can also be done by providing some application configuration settings that can be replaced with the mocked package.</p></li>
</ul>
<p>One approach to creating a complete service that will implement the fixture.</p>


<h4 class="likesubsubsectionHead" data-number="8.1.3.4">Injecting a mock for the requests package</h4>
<p>Replacing the <code>requests</code> package requires using dependency injection techniques in the acquire application. A static dependency arises from code like the following:</p>
<div><div><pre class="source-code">import requests
import requests.auth</pre>
</div>
</div>
<p>Later in the module, there may be code like <code>requests.get(...)</code> or <code>requests.auth.HTTPBasicAuth(...)</code>. The binding to the <code>requests</code> module is fixed by both the <code>import</code> statement and the references to <code>requests</code> and <code>requests.auth</code>.</p>
<p>The <code>importlib</code> module permits more dynamic binding of modules names, allowing some run-time flexibility. The following, for example, can be used to tailor imports.</p>
<div><div><pre class="source-code">if __name__ == "__main__":
    # Read from a configuration file
    requests_name = "requests"
    requests = importlib.import_module(requests_name)
    main(sys.argv[1:])</pre>
</div>
</div>
<p>The global variable, <code>requests</code>, has the imported module assigned to it. This module variable <strong>must </strong>be global; it’s an easy requirement to overlook when trying to configure the application for acceptance testing.</p>
<p>Note that the import of the <code>requests</code> module (or the mock version) is separated from the remaining <code>import</code> statements. This can be the source of some confusion for folks reading this code later, and suitable comments are important for clarifying the way this dependency injection works.</p>
<p>When we looked at unit testing in <a href="#x1-900003"><em>Unit tests for the RestAccess class</em></a>, we used the <strong>pytest </strong>fixture named <code>monkeypatch</code> to properly isolate modules for testing.</p>
<p>Monkey patching isn’t a great technique for acceptance testing because the code being tested is not <strong>exactly </strong>the code that will be used. While monkey patching and dependency injection are popular, there are always questions about testing patched software instead of the actual software. In some industries — particularly those where human lives might be at risk from computer-controlled machinery — the presence of a patch for testing may not be allowed. In the next section, we’ll look at building a mock service to create and test the acquire application without any patching or changes.</p>


<h4 class="likesubsubsectionHead" data-number="8.1.3.5">Creating a mock service</h4>
<p>A mock service can be built with any web services framework. There are two that are part of the standard library: the <code>http.server</code> package and the <code>wsgiref</code> package. Either of these can respond to HTTP requests, and can be used to create local services that can mock the Kaggle web service to permit testing our client.</p>
<p>Additionally, any of the well-known web service frameworks can be used to create the mock service. Using a tool like <strong>flask </strong>or <strong>bottle </strong>can make it slightly easier to build a suitable mock service.</p>
<p>To keep the server as simple as possible, we’ll use the <strong>bottle </strong>framework. This means adding <code>bottle==0.12.23</code> to the <code>pyproject.toml</code> file in the <code>[project.optional-dependencies]</code> section. This tool is only needed by developers.</p>
<p>The <strong>Bottle </strong>implementation of a RESTful API might look like this:</p>
<div><div><pre class="source-code">import csv
import io
import json
import zipfile
from bottle import route, run, request, HTTPResponse

@route(’/api/v1/datasets/list’)
def datasets_list(name):
    # Provide mock JSON documents and Rate throttling

@route(’/api/v1/datasets/download/&lt;ownerSlug&gt;/&lt;datasetSlug&gt;’)
def datasets_download(ownerSlug, datasetSlug):
    # Provide mock ZIP archive

if __name__ == "__main__":
    run(host=’127.0.0.1’, port=8080)</pre>
</div>
</div>
<p>While the Kaggle service has numerous paths and methods, this data acquisition project application doesn’t use all of them. The mock server only needs to provide routes for the paths the application will actually use.</p>
<p>The <code>datasets_list</code> function might include the following example response:</p>
<div><div><pre class="source-code">@route(’/api/v1/datasets/list’)
def datasets_list(name):
    page = request.query.page or ’1’
    if page == ’1’:
        mock_body = [
            # Provide attributes as needed by the application under test
            {’title’: ’example1’},
            {’title’: ’example2’},
        ]
        response = HTTPResponse(
            body=json.dumps(mock_body),
            status=200,
            headers={’Content-Type’: ’application/json’}
        )
    else:
        # For error-recovery scenarios, this response may change.
        response = HTTPResponse(
            body=json.dumps([]),
            status=200,
            headers={’Content-Type’: ’application/json’}
        )
    return response</pre>
</div>
</div>
<p>The <code>HTTPResponse</code> object contains the essential features of the responses as seen by the acquisition application’s download requests. Each response has content, a status code, and a header that is used to confirm the type of response.</p>
<p>For more comprehensive testing, it makes sense to add another kind of response with the status code of 429 and a header dictionary with <code>{’Retry-After’:</code><code> ’30’}</code>. For this case, the two values of <code>response</code> will be more dramatically distinct.</p>
<p>The download needs to provide a mocked ZIP archive. This can be done as shown in the following example:</p>
<div><div><pre class="source-code">@route(’/api/v1/datasets/download/&lt;ownerSlug&gt;/&lt;datasetSlug&gt;’)
def datasets_download(ownerSlug, datasetSlug):
    if ownerSlug == "carlmcbrideellis" and datasetSlug ==
      "data-anscombes-quartet":
        zip_content = io.BytesIO()
        with zipfile.ZipFile(zip_content, ’w’) as archive:
            target_path = zipfile.Path(archive, ’Anscombe_quartet_data.csv’)
            with target_path.open(’w’) as member_file:
                writer = csv.writer(member_file)
                writer.writerow([’mock’, ’data’])
                writer.writerow([’line’, ’two’])
        response = HTTPResponse(
            body=zip_content.getvalue(),
            status=200,
            headers={"Content-Type": "application/zip"}
        )
        return response
    # All other requests...
    response = HTTPResponse(
        status=404
    )
    return response</pre>
</div>
</div>
<p>This function will only respond to one specifically requested combination of <code>ownerSlug</code> and <code>datasetSlug</code>. Other combinations will get a 404 response, the status code for a resource that can’t be found.</p>
<p>The <code>io.BytesIO</code> object is an in-memory buffer that can be processed like a file. It is used by the <code>zipfile.ZipFile</code> class to create a ZIP archive. A single member is written to this archive. The member has a header row and a single row of data, making it easy to describe in a Gherkin scenario. The response is built from the bytes in this file, a status code of 200, and a header telling the client the content is a ZIP archive.</p>
<p>This service can be run on the desktop. You can use a browser to interact with this server and confirm it works well enough to test our application.</p>
<p>Now that we’ve seen the mock service that stands in for Kaggle.com, we can look at how to make the <strong>behave </strong>tool run this service when testing a specific scenario.</p>


<h4 class="likesubsubsectionHead" data-number="8.1.3.6">Behave fixture</h4>
<p>We’ve added a <code>fixture.kaggle_server</code> to the scenario. There are two steps to make this tag start the server process running for a given scenario. These steps are:</p>
<ol>
<li><div><p>Define a generator function. This will start a subprocess, yield something, and then kill the subprocess.</p>
</div></li>
<li><div><p>Define a <code>before_tag()</code> function to inject the generator function into the step processing.</p>
</div></li>
</ol>
<p>Here’s a generator function that will update the context, and start the mock Kaggle service.</p>
<div><div><pre class="source-code">from collections.abc import Iterator
from typing import Any
import subprocess
import time
import os
import sys
from behave import fixture, use_fixture
from behave.runner import Context

@fixture
def kaggle_server(context: Context) -&gt; Iterator[Any]:
    if "environment" not in context:
        context.environment = os.environ
    context.environment["ACQUIRE_BASE_URL"] = "http://127.0.0.1:8080"
    # Save server-side log for debugging
    server = subprocess.Popen(
        [sys.executable, "tests/mock_kaggle_bottle.py"],
    )
    time.sleep(0.5)  # 500 ms delay to allow the service to open a socket
    yield server
    server.kill()</pre>
</div>
</div>
<p>The portion of the function before the <code>yield</code> statement is used during the scenario setup. This will add value to the context that will be used to start the application under test. After the yielded value has been consumed by the <strong>Behave </strong>runner, the scenario executes. When the scenario is finished, one more value is requested from this generator; this request will execute the statements after the <code>yield</code> statement. There’s no subsequent <code>yield</code> statement; the <code>StopIteration</code> is the expected behavior of this function.</p>
<p>This <code>kaggle_server()</code> function must be used in a scenario when the <code>@fixture</code> tag is present. The following function will do this:</p>
<div><div><pre class="source-code">from behave import use_fixture
from behave.runner import Context

def before_tag(context: Context, tag: str) -&gt; None:
    if tag == "fixture.kaggle_server":
        # This will invoke the definition generator.
        # It consumes a value before and after the tagged scenario.
        use_fixture(kaggle_server, context)</pre>
</div>
</div>
<p>When the <code>@fixture.kaggle_server</code> tag is present, this function will inject the <code>kaggle_server()</code> generator function into the overall flow of processing by the runner. The runner will make appropriate requests of the <code>kaggle_server()</code> generator function to start and stop the service.</p>
<p>These two functions are placed into the <code>environment.py</code> module where the <strong>behave </strong>tool can find and use them.</p>
<p>Now that we have an acceptance test suite, we can turn to implement the required features of the <code>acquire</code> application.</p>


<h4 class="likesubsubsectionHead" data-number="8.1.3.7">Kaggle access module and refactored main application</h4>
<p>The goal, of course, is two-fold:</p>
<ul>
<li><p>Add a <code>kaggle_client.py</code> module. The unit tests will confirm this works.</p></li>
<li><p>Rewrite the <code>acquire.py</code> module from <a href="ch007.xhtml#x1-560003"><em>Chapter</em><em> 3</em></a>, <a href="ch007.xhtml#x1-560003"><em>Project 1.1: Data</em> <em>Acquisition Base Application</em></a> to add the download feature.</p></li>
</ul>
<p>The <a href="#x1-830002"><em>Approach</em></a> section provides some design guidance for building the application. Additionally, the previous chapter, <a href="ch007.xhtml#x1-560003"><em>Chapter</em><em> 3</em></a>, <a href="ch007.xhtml#x1-560003"><em>Project 1.1: Data Acquisition Base</em> <em>Application</em></a> provides the baseline application into which the new acquisition features should be added.</p>
<p>The acceptance tests will confirm the application works correctly.</p>
<p>Given this extended capability, you are encouraged to hunt for additional, interesting data sets. The new application can be revised and extended to acquire new, interesting data in other formats.</p>
<p>Now that we have data acquired from the web in a tidy, easy-to-use format, we can look at acquiring data that isn’t quite so tidy. In the next section, we’ll look at how to scrape data out of an HTML page. </p>




<h2 data-number="8.2">4.2  Project 1.3: Scrape data from a web page</h2>
<p>In some cases, we want data that’s provided by a website that doesn’t have a tidy API. The data is available via an HTML page. This means the data is surrounded by HTML <em>markup</em>, text that describes the semantics or structure of the data.</p>
<p>We’ll start with a description of the application, and then move on to talk about the architectural approach. This will be followed with a detailed list of deliverables. </p>

<h3 data-number="8.2.1">4.2.1  Description</h3>
<p>We’ll continue to describe projects designed to acquire some data for further analysis. In this case, we’ll look at data that is available from a website, but is embedded into the surrounding HTML markup. We’ll continue to focus on Anscombe’s Quartet data set because it’s small and diagnosing problems is relatively simple. A larger data set introduces additional problems with time and storage.</p>
<p>Parts of this application are an extension to the project in <a href="#x1-790001"><em>Project 1.2: Acquire</em> <em>data from a web service</em></a>. The essential behavior of this application will be similar to the previous project. This project will use a CLI application to grab data from a source.</p>
<p>The User Experience (UX) will also be a command-line application with options to fine-tune the data being gathered. Our expected command line should like something like the following:</p>
<div><div><pre class="console">% python src/acquire.py -o quartet --page "https://en.wikipedia.org/wiki/Anscombe’s_quartet" --caption "Anscombe’s quartet"</pre>
</div>
</div>
<p>The <code>-o</code><code> quartet</code> argument specifies a directory into which four results are written. These will have names like <code>quartet/series_1.json</code>.</p>
<p>The table is buried in the HTML of the URL given by the <code>--page</code> argument. Within this HTML, the target table has a unique <code>&lt;caption&gt;</code> tag: <code>&lt;caption&gt;Anscombe’s</code><code> quartet&lt;/caption&gt;</code>. </p>


<h3 data-number="8.2.2">4.2.2  About the source data</h3>
<p>This data embedded in HTML markup is generally marked up with the <code>&lt;table&gt;</code> tag. A table will often have the following markup:</p>
<div><div><pre class="source-code">&lt;table class="wikitable" style="text-align: center; margin-left:auto;
  margin-right:auto;" border="1"&gt;
    &lt;caption&gt;Anscombe’s quartet&lt;/caption&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;th colspan="2"&gt;I&lt;/th&gt;
        etc.
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;&lt;i&gt;x&lt;/i&gt;&lt;/td&gt;
        &lt;td&gt;&lt;i&gt;y&lt;/i&gt;&lt;/td&gt;
        etc.
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;10.0&lt;/td&gt;
        &lt;td&gt;8.04&lt;/td&gt;
        etc.
      &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;</pre>
</div>
</div>
<p>In this example, the overall <code>&lt;table&gt;</code> tag will have two child tags, a <code>&lt;caption&gt;</code> and a <code>&lt;tbody&gt;</code>.</p>
<p>The table’s body, within <code>&lt;tbody&gt;</code>, has a number of rows wrapped in <code>&lt;tr&gt;</code> tags. The first row has headings in <code>&lt;th&gt;</code> tags. The second row also has headings, but they use the <code>&lt;td&gt;</code> tags. The remaining rows have data, also in <code>&lt;td&gt;</code> tags.</p>
<p>This structure has a great deal of regularity, making it possible to use a parser like <strong>Beautiful Soup </strong>to locate the content.</p>
<p>The output will match the extraction processing done for the previous projects. See <a href="ch007.xhtml#x1-560003"><em>Chapter</em><em> 3</em></a>, <a href="ch007.xhtml#x1-560003"><em>Project 1.1: Data Acquisition Base Application</em></a>, for the essence of the data acquisition application.</p>
<p>This section has looked at the input and processing for this application. The output will match earlier projects. In the next section, we’ll look at the overall architecture of the software. </p>


<h3 data-number="8.2.3">4.2.3  Approach</h3>
<p>We’ll take some guidance from the C4 model ( <a class="url" href="https://c4model.com">https://c4model.com</a>) when looking at our approach:</p>
<ul>
<li><p><strong>Context</strong>: For this project, a context diagram would show a user extracting data from a source. You may find it helpful to draw this diagram.</p></li>
<li><p><strong>Containers</strong>: One container is the user’s personal computer. The other container is the Wikipedia website, which provides the data.</p></li>
<li><p><strong>Components</strong>: We’ll address the components below.</p></li>
<li><p><strong>Code</strong>: We’ll touch on this to provide some suggested directions.</p></li>
</ul>
<p>It’s important to consider this application as an extension to the project in <a href="ch007.xhtml#x1-560003"><em>Chapter</em><em> 3</em></a>, <a href="ch007.xhtml#x1-560003"><em>Project 1.1: Data Acquisition Base Application</em></a>. The base level of architectural design is provided in that chapter.</p>
<p>In this project, we’ll be adding a new <code>html_extract</code> module to capture and parse the data. The overall application in the <code>acquire</code> module will change to use the new features. The other modules should remain unchanged.</p>
<p>A new architecture that handles the download of HTML data and the extraction of a table from the source data is shown in <a href="#4.4"><em>Figure 4.4</em></a>.</p>
<figure class="IMG---Figure">
<img alt="Figure 4.4: Revised Component Design " src="img/file17.jpg"/>
<figcaption class="IMG---Caption">Figure 4.4: Revised Component Design </figcaption>
</figure>
<p>This diagram suggested classes for the new <code>html_extract</code> module. The <code>Download</code> class uses <code>urllib.request</code> to open the given URL and read the contents. It also uses the <code>bs4</code> module (<strong>Beautiful Soup</strong>) to parse the HTML, locate the table with the desired caption, and extract the body of the table.</p>
<p>The <code>PairBuilder</code> class hierarchy has four implementations, each appropriate for one of the four data series. Looking back at <a href="ch007.xhtml#x1-560003"><em>Chapter</em><em> 3</em></a>, <a href="ch007.xhtml#x1-560003"><em>Project 1.1: Data</em> <em>Acquisition Base Application</em></a>, there’s a profound difference between the table of data shown on the Wikipedia page, and the CSV source file shown in that earlier project. This difference in data organization requires slightly different pair-building functions.</p>

<h4 class="likesubsubsectionHead" data-number="8.2.3.1">Making an HTML request with urllib.request</h4>
<p>The process of reading a web page is directly supported by the <code>urllib.request</code> module. The <code>url_open()</code> function will perform a GET request for a given URL. The return value is a file-like object — with a <code>read()</code> method — that can be used to acquire the content.</p>
<p>This is considerably simpler than making a general RESTful API request where there are a variety of pieces of information to be uploaded and a variety of kinds of results that might be downloaded. When working with common GET requests, this standard library module handles the ordinary processing elegantly.</p>
<p>A suggested design for the first step in the operation is the following function:</p>
<pre class="source-code">from urllib.request import urlopen
from bs4 import BeautifulSoup, Tag

def get_page(url: str) -&gt; BeautifulSoup:
    return BeautifulSoup(
        urlopen(url), "html.parser"
    )</pre>
<p>The <code>urlopen()</code> function will open the URL as a file-like object, and provide that file to the <code>BeautifulSoup</code> class to parse the resulting HTML.</p>
<p>A <code>try:</code> statement to handle potential problems is not shown. There are innumerable potential issues when reaching out to a web service, and trying to parse the resulting content. You are encouraged to add some simple error reporting.</p>
<p>In the next section, we’ll look at extracting the relevant table from the parsed HTML.</p>


<h4 class="likesubsubsectionHead" data-number="8.2.3.2">HTML scraping and Beautiful Soup</h4>
<p>The Beautiful Soup data structure has a <code>find_all()</code> method to traverse the structure. This will look for tags with specific kinds of properties. This can examine the tag, the attributes, and even the text content of the tag.</p>
<p>See <a class="url" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all">https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all</a>.</p>
<p>In this case, we need to find a <code>&lt;table&gt;</code> tag with a <code>caption</code> tag embedded within it. That <code>caption</code> tag must have the desired text. This search leads to a bit more complex investigation of the structure. The following function can locate the desired table.</p>
<div><div><pre class="source-code">def find_table_caption(
        soup: BeautifulSoup,
        caption_text: str = "Anscombe’s quartet"
    ) -&gt; Tag:
    for table in soup.find_all(’table’):
        if table.caption:
            if table.caption.text.strip() == caption_text.strip():
                return table
    raise RuntimeError(f"&lt;table&gt; with caption {caption_text!r} not found")</pre>
</div>
</div>
<p>Some of the tables lack captions. This means the expression <code>table.caption.text</code> won’t work for string comparison because it may have a <code>None</code> value for <code>table.caption</code>. This leads to a nested cascade of <code>if</code> statements to be sure there’s a <code>&lt;caption&gt;</code> tag before checking the text value of the tag.</p>
<p>The <code>strip()</code> functions are used to remove leading and trailing whitespace from the text because blocks of text in HTML can be surrounded by whitespace that’s not displayed, making it surprising when it surfaces as part of the content. Stripping the leading and trailing whitespace makes it easier to match.</p>
<p>The rest of the processing is left for you to design. This processing involves finding all of the <code>&lt;tr&gt;</code> tags, representing rows of the table. Within each row (except the first) there will be a sequence of <code>&lt;td&gt;</code> tags representing the cell values within the row.</p>
<p>Once the text has been extracted, it’s very similar to the results from a <code>csv.reader</code>.</p>
<p>After considering the technical approach, it’s time to look at the deliverables for this project. </p>



<h3 data-number="8.2.4">4.2.4  Deliverables</h3>
<p>This project has the following deliverables:</p>
<ul>
<li><p>Documentation in the <code>docs</code> folder.</p></li>
<li><p>Acceptance tests in the <code>tests/features</code> and <code>tests/steps</code> folders.</p></li>
<li><p>Unit tests for the application modules in the <code>tests</code> folder.</p></li>
<li><p>Mock HTML pages for unit testing will be part of the unit tests.</p></li>
<li><p>Application to acquire data from an HTML page.</p></li>
</ul>
<p>We’ll look at a few of these deliverables in a little more detail.</p>

<h4 class="likesubsubsectionHead" data-number="8.2.4.1">Unit test for the html_extract module</h4>
<p>The <code>urlopen()</code> function supports the <code>http:</code> and <code>https:</code> schemes. It also supports the <code>file:</code> protocol. This allows a test case to use a URL of the form <code>file:///path/to/a/file.html</code> to read a local HTML file. This facilitates testing by avoiding the complications of accessing data over the internet.</p>
<p>For testing, it makes sense to prepare files with the expected HTML structure, as well as invalid structures. With some local files as examples, a developer can run test cases quickly.</p>
<div><div><p>Generally, it’s considered a best practice to mock the <code>BeautifulSoup</code> class. A fixture would respond to the various <code>find_all()</code> requests with mock tag objects.</p>
<p>When working with HTML, however, it seems better to provide mock HTML. The wide variety of HTML seen in the wild suggests that time spent with real HTML is immensely valuable for debugging.</p>
<p>Creating <code>BeautifulSoup</code> objects means the unit testing is more like integration testing. The benefits of being able to test a wide variety of odd and unusual HTML seems to be more valuable than the cost of breaking the ideal context for a unit test.</p>
</div>
</div>
<p>Having example HTML files plays well with the way <strong>pytest </strong>fixtures work. A fixture can create a file and return the path to the file in the form of a URL. After the test, the fixture can remove the file.</p>
<p>A fixture with a test HTML page might look like this:</p>
<div><div><pre class="source-code">from pytest import fixture
from textwrap import dedent

@fixture
def example_1(tmp_path):
    html_page = tmp_path / "works.html"
    html_page.write_text(
        dedent("""\
        &lt;!DOCTYPE html&gt;
        &lt;html&gt;
        etc.
        &lt;/html&gt;
        """
        )
    )
    yield f"file://{str(html_page)}"
    html_page.unlink()</pre>
</div>
</div>
<p>This fixture uses the <code>tmp_path</code> fixture to provide access to a temporary directory used only for this test. The file, <code>works.html</code>, is created, and filled with an HTML page. The test case should include multiple <code>&lt;table&gt;</code> tags, only one of which was the expected <code>&lt;caption&gt;</code> tag.</p>
<p>The <code>dedent()</code> function is a handy way to provide a long string that matches the prevailing Python indent. The function removes the indenting whitespace from each line; the resulting text object is not indented.</p>
<p>The return value from this fixture is a URL that can be used by the <code>urlopen()</code> function to open and read this file. After the test is completed, the final step (after the <code>yield</code> statement) will remove the file.</p>
<p>A test case might look something like the following:</p>
<div><div><pre class="source-code">def test_steps(example_1):
    soup = html_extract.get_page(example_1)
    table_tag = html_extract.find_table_caption(soup, "Anscombe’s quartet")
    rows = list(html_extract.table_row_data_iter(table_tag))
    assert rows == [
        [],
        [’Keep this’, ’Data’],
        [’And this’, ’Data’],
    ]</pre>
</div>
</div>
<p>The test case uses the <code>example_1</code> fixture to create a file and return a URL referring to the file. The URL is provided to a function being tested. The functions within the <code>html_extract</code> module are used to parse the HTML, locate the target table, and extract the individual rows.</p>
<p>The return value tells us the functions work properly together to locate and extract data. You are encouraged to work out the necessary HTML for good — and bad — examples.</p>


<h4 class="likesubsubsectionHead" data-number="8.2.4.2">Acceptance tests</h4>
<p>As noted above in <a href="#x1-1040004"><em>Unit test for the html</em><em>_extract module</em></a>, the acceptance test case HTML pages can be local files. A scenario can provide a local <code>file://</code> URL to the application and confirm the output includes properly parsed data.</p>
<p>The Gherkin language permits including large blocks of text as part of a scenario.</p>
<p>We can imagine writing the following kinds of scenarios in a feature file:</p>
<div><div><pre class="source-code">Scenario: Finds captioned table and extracts data
  Given an HTML page "example_1.html"
    """
      &lt;!DOCTYPE html&gt;
      &lt;html&gt;
        etc. with multiple tables.
      &lt;/html&gt;
    """
  When we run the html extract command
  Then log has INFO line with "header: [’Keep this’, ’Data’]"
  And log has INFO line with "count: 1"</pre>
</div>
</div>
<p>The HTML extract command is quite long. The content is available as the <code>context.text</code> parameter of the step definition function. Here’s what the step definition for this given step looks like:</p>
<pre class="source-code">from textwrap import dedent

@given(u'an HTML page "{filename}"')
def step_impl(context, filename):
    context.path = Path(filename)
    context.path.write_text(dedent(context.text))
    context.add_cleanup(context.path.unlink)</pre>
<p>The step definition puts the path into the context and then writes the HTML page to the given path. The <code>dedent()</code> function removes any leading spaces that may have been left in place by the <strong>behave </strong>tool. Since the path information is available in the context, it can be used by the <strong>When </strong>step. The <code>context.add_cleanup()</code> function will add a function that can be used to clean up the file when the scenario is finished. An alternative is to use the environment module’s <code>after_scenario()</code> function to clean up.</p>
<p>This scenario requires an actual path name for the supplied HTML page to be injected into the text. For this to work out well, the step definition needs to build a command from pieces. Here’s one approach:</p>
<div><div><pre class="source-code">@when(u’we run the html extract command’)
def step_impl(context):
    command = [
        ’python’, ’src/acquire.py’,
        ’-o’, ’quartet’,
        ’--page’, ’$URL’,
        ’--caption’, "Anscombe’s quartet"
    ]
    url = f"file://{str(context.path.absolute())}"
    command[command.index(’$URL’)] = url
    print(shlex.join(command))
    # etc. with subprocess.run() to execute the command</pre>
</div>
</div>
<p>In this example, the command is broken down into individual parameter strings. One of the strings must be replaced with the actual file name. This works out nicely because the <code>subprocess.run()</code> function works well with a parsed shell command. The <code>shlex.split()</code> function can be used to decompose a line, honoring the complex quoting rules of the shell, into individual parameter strings.</p>
<p>Now that we have an acceptance test suite, we may find the <code>acquire</code> application doesn’t pass all of the tests. It’s helpful to define done via an acceptance test and then develop the required HTML extract module and refactor the main application. We’ll look at these two components next.</p>


<h4 class="likesubsubsectionHead" data-number="8.2.4.3">HTML extract module and refactored main application</h4>
<p>The goal for this project is two-fold:</p>
<ul>
<li><p>Add an <code>html_extract.py</code> module. The unit tests will confirm this module works.</p></li>
<li><p>Rewrite the <code>acquire.py</code> module from <a href="ch007.xhtml#x1-560003"><em>Chapter</em><em> 3</em></a>, <a href="ch007.xhtml#x1-560003"><em>Project 1.1: Data</em> <em>Acquisition Base Application</em></a> to add the HTML download and extract the feature.</p></li>
</ul>
<p>The <a href="#x1-1000003"><em>Approach</em></a> section provides some design guidance for building the application. Additionally, the previous chapter, <a href="ch007.xhtml#x1-560003"><em>Chapter</em><em> 3</em></a>, <a href="ch007.xhtml#x1-560003"><em>Project 1.1: Data Acquisition Base</em> <em>Application</em></a>, provides the baseline application into which the new acquisition features should be added.</p>
<p>The acceptance tests will confirm the application works correctly to gather data from the Kaggle API.</p>
<p>Given this extended capability, you can hunt for data sets that are presented in web pages. Because of the consistency of Wikipedia, it is a good source of data. Many other sites provide relatively consistent HTML tables with interesting data.</p>
<p>In these two projects, we’ve extended our ability to acquire data from a wide variety of sources. </p>




<h2 data-number="8.3">4.3  Summary</h2>
<p>This chapter’s projects have shown examples of the following features of a data acquisition application:</p>
<ul>
<li><p>Web API integration via the <strong>requests </strong>package. We’ve used the Kaggle API as an example of a RESTful API that provides data for download and analysis.</p></li>
<li><p>Parsing an HTML web page using the <strong>Beautiful Soup </strong>package.</p></li>
<li><p>Adding features to an existing application and extending the test suite to cover these new alternative data sources.</p></li>
</ul>
<p>A challenging part of both of these projects is creating a suite of acceptance tests to describe the proper behavior. Pragmatically, a program without automated tests cannot be trusted. The tests are every bit as important as the code they’re exercising.</p>
<p>In some enterprises, the definition of done is breezy and informal. There may be a presentation or an internal memo or a whitepaper that describes the desired software. Formalizing these concepts into tangible test cases is often a significant effort. Achieving agreements can become a source of turmoil as stakeholders slowly refine their understanding of how the software will behave.</p>
<p>Creating mock web services is fraught with difficulty. Some API’s permit downloading an <code>openapi.json</code> file with the definition of the API complete with examples. Having concrete examples, provided by the host of the API, makes it much easier to create a mock service. A mock server can load the JSON specification, navigate to the example, and provide the official response.</p>
<p>Lacking an OpenAPI specification with examples, developers need to write spike solutions that download detailed responses. These responses can then be used to build mock objects. You are strongly encouraged to write side-bar applications to explore the Kaggle API to see how it works.</p>
<p>In the next chapter, we’ll continue this data extraction journey to include extracting data from SQL databases. Once we’ve acquired data, we’ll want to inspect it. <a href="ch010.xhtml#x1-1460006"><em>Chapter</em><em> 6</em></a>, <a href="ch010.xhtml#x1-1460006"><em>Project 2.1: Data Inspection Notebook</em></a>, will introduce an inspection step. </p>


<h2 data-number="8.4">4.4  Extras</h2>
<p>Here are some ideas for you to add to these projects. </p>

<h3 data-number="8.4.1">4.4.1  Locate more JSON-format data</h3>
<p>A search of Kaggle will turn up some other interesting data sets in JSON format.</p>
<ul>
<li><p><a class="url" href="https://www.kaggle.com/datasets/rtatman/iris-dataset-json-version">https://www.kaggle.com/datasets/rtatman/iris-dataset-json-version</a>: This data set is famous and available in a number of distinct formats.</p></li>
<li><p><a class="url" href="https://www.kaggle.com/datasets/conoor/stack-overflow-tags-usage">https://www.kaggle.com/datasets/conoor/stack-overflow-tags-usage</a></p></li>
<li><p><a class="url" href="https://www.kaggle.com/datasets/queyrusi/the-warship-dataset">https://www.kaggle.com/datasets/queyrusi/the-warship-dataset</a></p></li>
</ul>
<p>One of these is a JSON download. The other two are ZIP archives that contain JSON-format content.</p>
<p>This will require revising the application’s architecture to extract the JSON format data instead of CSV format data.</p>
<p>An interesting complication here is the distinction between CSV data and JSON data:</p>
<ul>
<li><p>CSV data is pure text, and later conversions are required to make useful Python objects.</p></li>
<li><p>Some JSON data is converted to Python objects by the parser. Some data (like datestamps) will be left as text.</p></li>
</ul>
<p>At acquisition time, this doesn’t have a significant impact. However, when we get to <a href="ch013.xhtml#x1-2080009"><em>Chapter</em><em> 9</em></a>, <a href="ch013.xhtml#x1-2080009"><em>Project 3.1: Data Cleaning Base Application</em></a>, we’ll have to account for data in a text-only form distinct from data with some conversions applied.</p>
<p>The Iris data set is quite famous. You can expand on the designs in this chapter to acquire Iris data from a variety of sources. The following steps could be followed:</p>
<ol>
<li><div><p>Start with the Kaggle data set in JSON format. Build the needed model, and extract modules to work with this format.</p>
</div></li>
<li><div><p>Locate other versions of this data set in other formats. Build the needed extract modules to work with these alternative formats.</p>
</div></li>
</ol>
<p>Once a core acquisition project is complete, you can leverage this other famous data set as an implementation choice for later projects. </p>


<h3 data-number="8.4.2">4.4.2  Other data sets to extract</h3>
<p>See the <strong>CO</strong><strong>2</strong> <strong>PPM — Trends in Atmospheric Carbon Dioxide </strong>data set, available at <a class="url" href="https://datahub.io/core/co2-ppm">https://datahub.io/core/co2-ppm</a>, for some data that is somewhat larger. This page has a link to an HTML table with the data.</p>
<p>See <a class="url" href="https://datahub.io/core/co2-ppm/r/0.html">https://datahub.io/core/co2-ppm/r/0.html</a> for a page with the complete data set as an HTML table. This data set is larger and more complicated than Anscombe’s Quartet. In <a href="ch010.xhtml#x1-1460006"><em>Chapter</em><em> 6</em></a>, <a href="ch010.xhtml#x1-1460006"><em>Project 2.1: Data</em> <em>Inspection Notebook</em></a>, we’ll address some of the special cases in this data set. </p>


<h3 data-number="8.4.3">4.4.3  Handling schema variations</h3>
<p>The two projects in this chapter each reflect a distinct schema for the source data.</p>
<p>One CSV format can be depicted via an <strong>Entity-Relationship Diagram</strong> (<strong>ERD</strong>), shown in <a href="#4.5"><em>Figure 4.5</em></a><em>.</em></p>
<figure class="IMG---Figure">
<img alt="Figure 4.5: Source entity-relationship diagram " src="img/file22.jpg"/>
<figcaption class="IMG---Caption">Figure 4.5: Source entity-relationship diagram </figcaption>
</figure>
<p>One column, <code>x_123</code>, is the x-value of three distinct series. Another column, <code>x_4</code>, is the x-value for one series.</p>
<p>A depiction of the HTML format as an ERD is shown in <a href="#4.6"><em>Figure 4.6</em></a><em>.</em></p>
<figure class="IMG---Figure">
<img alt="Figure 4.6: Notional entity-relationship diagram " src="img/file23.jpg"/>
<figcaption class="IMG---Caption">Figure 4.6: Notional entity-relationship diagram </figcaption>
</figure>
<p>The x-values are repeated as needed.</p>
<p>This difference requires several distinct approaches to extracting the source data.</p>
<p>In these projects, we’ve implemented this distinction as distinct subclasses of a <code>PairBuilder</code> superclass.</p>
<p>An alternative design is to create distinct functions with a common type signature:</p>
<div><div><pre class="source-code">    PairBuilder: TypeVar = Callable[[list[str]], XYPair]</pre>
</div>
</div>
<p>Making each conversion a function eliminates the overhead of the class definition.</p>
<p>This rewrite can be a large simplification. It will not change any acceptance tests. It will, however, require numerous changes to unit tests.</p>
<p>The functional design offers some simplification over class-based design. You are encouraged to perform the functional redesign of the suggestions in this book. </p>


<h3 data-number="8.4.4">4.4.4  CLI enhancements</h3>
<p>The CLI for these two projects is left wide open, permitting a great deal of design flexibility and alternatives. Because the CLI is part of externally visible behavior, it becomes necessary to write acceptance tests for the various CLI options and arguments.</p>
<p>As noted in <a href="ch007.xhtml#x1-670002"><em>Additional acceptance scenarios</em></a>, there are a number of acceptance test scenarios that are not on the ”happy path” where the application works. These additional scenarios serve to catalog a number of erroneous uses of the application.</p>
<p>This becomes more important as more features are added and the CLI becomes more complicated. You are encouraged to write acceptance tests for invalid CLI use. </p>


<h3 data-number="8.4.5">4.4.5  Logging</h3>
<p>Logging is an important part of data acquisition. There are a number of potential problems exposed by these two projects. A website might be unresponsive, or the API may have changed. The HTML may have been reformatted in some subtle way.</p>
<p>A <em>debug </em>or <em>verbose </em>mode should be available to expose the interactions with external services to be sure of the HTTP status codes and headers.</p>
<p>Additionally, count values should be displayed to summarize the bytes downloaded, the lines of text examined, and the number of <code>XYPair</code> objects created. The idea is to characterize the inputs, the various processing steps, and the outputs.</p>
<p>These counts are essential for confirming that data is processed and filtered correctly. They’re an important tool for making parts of the processing more observable. A user wants to confirm that all of the downloaded data is either part of the results or filtered and discarded for a good reason.</p>
<p>You are encouraged to include counts for input, processing, and output in the log. </p>



</body>
</html>
