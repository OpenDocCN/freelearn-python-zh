- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Programmable Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Initially, computer networks were something physical and static, with wires
    and hardware, but with advanced computation, virtualization, and connectivity,
    networks have become more flexible and configurable by software. In this chapter,
    we’re going to talk about how software has changed the picture for computer networks.
    We are going first to examine several different technologies used today to create
    networks via software, then we are going to examine the current standard technology,
    known as **software-defined networks** (**SDNs**).
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in the first chapter, computer networks can be quite complex and difficult
    to maintain. There are several different sets of equipment that range from routers,
    switches, and NATs to load balancers and more. In addition, within each piece
    of equipment, there are several different types of operation, such as *core* or
    *access* routers. Network equipment is typically configured individually with
    interfaces that are very different between each vendor. Although there are management
    systems that can help centralize the configuration in one single place, network
    equipment is configured at an individual level. This kind of operation means complexity
    in operation and slow innovation for new features.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the history of programmable networks and looking at those used in
    the present day
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Virtual network technologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SDNs and OpenFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding cloud computing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using OpenStack for networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the history of programmable networks and looking at those used in
    the present day
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Several years have passed since **programmable networks** were initially conceived
    by engineers, so let’s touch on a few historical milestones before we get into
    the current technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Active networking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Defense Advanced Research Projects Agency** (**DARPA**) began funding
    research in the mid-1990s to create a network that could easily be changed and
    customized by programming, called the **active networking** project. The main
    goal of the project was to create network technologies that, in contrast to then-current
    networks, were easy to innovate and evolve, allowing fast application and protocol
    development.
  prefs: []
  type: TYPE_NORMAL
- en: But it was not easy to create such a flexible network in the 1990s because programming
    languages, signaling and network protocols, and operating systems were not mature
    enough to accommodate such innovative ideas. For instance, operation systems were
    monolithic and adding features required recompilation and reboot. In addition,
    service APIs were non-existent and distributed programming languages were still
    in early development.
  prefs: []
  type: TYPE_NORMAL
- en: The active networking research programs explored radical alternatives to the
    services provided by the traditional internet stack via IP. Examples of this work
    can be found in projects such as **Global Environment for Network Innovations**
    (**GENIs**), which can be viewed at [https://www.geni.net/](https://www.geni.net/),
    **Nacional Science Foundation** (**NSF**), **Future Internet Design** (**FIND**)
    which can be viewed at [http://www.nets-find.net/](http://www.nets-find.net/),
    and **Future Internet Research and Experimentation Initiative** (**EU FIRE**).
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time, the active networking research community pursued two programming
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: The **capsule model**, where the code to execute at the nodes was carried in-band
    in data packets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **programmable router/switch model**, where the code to execute at the nodes
    was established by out-of-band mechanisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'Further reading can be found in *Active Networking – One View of the Past,
    Present, and Future*. Jonathan M. Smith and Scott M. Nettles – *IEEE TRANSACTIONS
    ON SYSTEMS - PART C: Application and Reviews, Vol 34 No 1, February 2004.*'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive into one of the first attempts at creating a node that was programmable,
    known as **NodeOS**.
  prefs: []
  type: TYPE_NORMAL
- en: NodeOS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the first goals of the active networking project was to create **NodeOS**.
    It was an operating system whose primary purpose was to support packet forwarding
    in an active network. NodeOS ran at the lowest level in an active node and multiplexed
    the node resources, such as memory and CPU, among the packet flows that traverse
    the node. NodeOS provided several important services to active network execution
    environments, including resource scheduling and accounting and fast packet input-output.
    The two important design milestones on NodeOS were the creation of **application
    programming interfaces** (**APIs**) and resource management.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Additional reading on NodeOS can be found at *An OS Interface for Active Routers
    April 2001* – *IEEE Journal on Selected Areas in Communications 19(3):473 – 487.*
  prefs: []
  type: TYPE_NORMAL
- en: Following this, we are now going to explore a few projects that were the early
    attempts from the community toward SDN.
  prefs: []
  type: TYPE_NORMAL
- en: Data and control plane separation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the major steps toward programmable networks and **SDNs** was the separation
    of the **control** and **data** planes. In [*Chapter 1*](B18165_01.xhtml#_idTextAnchor015),
    we discussed the difference between the control and data planes, and here we are
    going to discuss a bit of the history behind them. It’s worth remembering that
    the data plane is also known as the **forwarding plane**.
  prefs: []
  type: TYPE_NORMAL
- en: By the 1990s, such separation was already present on public telephone networks
    but was not yet implemented in computer networks or on the internet. As network
    complexity increased and internet services started to become the main revenue
    for several backbone providers, reliability, predictability, and performance were
    key points for network operators to seek better approaches for managing networks.
  prefs: []
  type: TYPE_NORMAL
- en: In the early 2000s, a community of researchers who either worked for or regularly
    interacted with network operators started to explore pragmatic approaches using
    either standard protocols or other imminent technologies that were just about
    to become deployable. At that time, routers and switches had tight integration
    between the control and forwarding planes. This coupling made various network
    management tasks difficult, such as debugging configuration problems and controlling
    routing behavior. To address these challenges, various efforts to separate the
    forwarding and control planes began to emerge. Let’s explore a few of the earlier
    efforts in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: IETF ForCES
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Forwarding and Control Element Separation** (**IETF ForCES**) working groups
    intended to create a framework, a list of requirements, a solution protocol, a
    logical function block library, and other associated documents in support of data
    and control element separation ([https://datatracker.ietf.org/wg/forces/about/](https://datatracker.ietf.org/wg/forces/about/)).'
  prefs: []
  type: TYPE_NORMAL
- en: The NetLink interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**NetLink** was perhaps the clearest separation of the control plane and data
    plane on the Linux kernel. In 2003, IETF published the *RFC3549* describing the
    separation of the **control plane components** (**CPCs**) and the **forwarding
    engine components** (**FECs**). *Figure 2.1* (from the original RFC) illustrates
    how Linux was using Netlink as the main separator between the control and data
    planes ([https://datatracker.ietf.org/doc/html/rfc3549](https://datatracker.ietf.org/doc/html/rfc3549)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Control and data plane separation, as shown in RFC3549](img/B18165_02_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Control and data plane separation, as shown in RFC3549
  prefs: []
  type: TYPE_NORMAL
- en: Netlink was first created on series 2.0 of the Linux kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Routing Control Platform
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Routing Control Platform** (**RCP**) is a pragmatic design to separate the
    control and data planes. The idea was to create a centralized control where all
    routing information was collected and then run an algorithm to select the best
    routing path for each of the routers of the network.'
  prefs: []
  type: TYPE_NORMAL
- en: RCP was implemented by collecting routing tables from external and internal
    **Border Gateway Protocol** (**BGP**) from the routers in the current network,
    using this information in a centralized manner to choose the best path to each
    of the routers. With this approach, it was possible to leverage the existing network
    devices and have control plane and data plane separation.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'More about RCP using BGP can be found in the paper *Design and implementation
    of a routing control platform* – Authors: Matthew Caesar, Donald Caldwell, Nick
    Feamster, Jennifer Rexford, Aman Shaikh, Jacobus van der Merwe – *NSDI’05: Proceedings
    of the 2nd conference on Symposium on Networked Systems Design & Implementation
    – Volume 2 May 2005 Pages 15–28.*'
  prefs: []
  type: TYPE_NORMAL
- en: SoftRouter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **SoftRouter** idea was presented at a conference in 2004 and patented in
    2005\. Again, the architecture had separation between control plane functions
    and data plane functions.
  prefs: []
  type: TYPE_NORMAL
- en: All control plane functions were implemented on general-purpose servers called
    **control elements** (**CEs**), which may be multiple hops away from the **forwarding
    elements** (**FEs**). There were two main types of network entities in the SoftRouter
    architecture, which were the FEs and CEs. Together, they constituted a **network
    element** (**NE**) router. The key difference from a traditional router was the
    absence of any control logic (such as OSPF or BGP) running locally. Instead, the
    control logic was hosted remotely.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: More details on SoftRouter can be found in the original 2004 paper *The SoftRouter
    Architecture* – T. V. Lakshman, T. Nandagopal, R. Ramjee, K. Sabnani, T. Woo –
    *Bell Laboratories, Lucent Technologies, ACM HOTNETS - January 2004.*
  prefs: []
  type: TYPE_NORMAL
- en: The path computation element architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In 2006, the IETF Network Working Group published an RFC describing an architecture
    of a centralized controlled entity to make route path decisions, which they called
    the **path computation element** (**PCE**) architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, PCE architecture was invented to solve a problem in **multiprotocol
    label switching** (**MPLS**) where the **label switch path** (**LSP**) calculations
    were becoming very slow and heavy for each router to calculate. It was designed
    to do the calculations on a server inside or outside the network instead.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: More details on PCE can be found in *RFC4655:* [https://datatracker.ietf.org/doc/html/rfc4655](https://datatracker.ietf.org/doc/html/rfc4655)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now look at the most important project of all, which was the work that
    went toward OpenFlow and SDNs.
  prefs: []
  type: TYPE_NORMAL
- en: Ethane
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Ethane** was one of the most important projects and culminated in the creation
    of OpenFlow and SDNs. Initially, it was just a project from a PhD student that
    defined a network as a group of data flows and network policies to control the
    traffic, which is another way to see the separation between the data plane and
    the control plane.'
  prefs: []
  type: TYPE_NORMAL
- en: The Ethane project had the idea of centralizing all network policies in one
    place. A new device joining the Ethane network should have all its communication
    turned off by default. The new device should get explicit permissions from the
    centralized server before connecting and its data flow should only be allowed
    on the permitted paths.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'More on the Ethane project can be found in the 2007 original paper *Ethane:
    taking control of the enterprise* – Authors: M. Casado, M. J. Freedman, J. Pettit,
    J. Luo, N. McKeown, Scott Shenker – *SIGCOMM ‘07: Proceedings of the 2007 conference,
    pages 1–12.*'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we explored a bit of the history behind programmable networks.
    We also explored a few of the main projects that led to the separation of the
    control and data planes, which was an important milestone toward SDNs. You should
    now be able to identify the significance of the separation and why it happened.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual network technologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Network virtualization** is when software acts like network hardware and
    it is accomplished by using logically simulated hardware platforms.'
  prefs: []
  type: TYPE_NORMAL
- en: Virtualization of networks is not a new concept, and we can find one of the
    first implementations in the mid-1970s with virtual circuits on X.25 networks.
    Later, other technologies also started using virtual concepts, such as Frame Relay
    and ATM, but they are now obsolete.
  prefs: []
  type: TYPE_NORMAL
- en: Loopback interfaces were based on electronics where loopbacks are used to create
    electric loops for the signal to return to its source for testing purposes. In
    1981, the IETF referred to the reserved address range `127.rrr.rrr.rrr` with `127.rrr.rrr.rrr`
    was officially called `127.0.0.1`)
  prefs: []
  type: TYPE_NORMAL
- en: Another early implementation of network virtualization was the **virtual** **LAN**
    or **VLAN**. By 1981, David Sincoskie was testing segmenting voice-over-Ethernet
    networks to facilitate fault tolerance, something similar to what VLAN does. However,
    it was only after 17 years that, in 1998, VLAN was finally published as a standard
    by IEEE by the name *802.1Q*. By the 2000s, switched networks dominated the landscape
    with switches, repeaters, and bridges, making VLANs commonplace. A LAN without
    a VLAN is virtually impossible today.
  prefs: []
  type: TYPE_NORMAL
- en: There are several other network virtualization technologies that are used today.
    Let’s explore the important ones in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual private networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the concept of creating an isolated secured network overlay that is
    implemented on network carriers, service providers, and over the internet.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, **virtual private network** (**VPN**) is a generic term that
    describes the use of public or private networks to create groups of users that
    are isolated from other network users, allowing them to communicate between themselves
    as if they were on a private network.
  prefs: []
  type: TYPE_NORMAL
- en: VPNs use end-to-end traffic encryption to enhance data separation, especially
    when using public networks, but this is not necessarily the case for all implementations.
    For instance, when using VPNs in MPLS networks, the traffic is not encrypted as
    it runs over private domains, and data separation exists only by packet encapsulation.
  prefs: []
  type: TYPE_NORMAL
- en: VPN is a generic name, but more specific names can be found, such as L3VPN,
    L2VPN, VPLS, Pseudo Wires, and VLLS, among others.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: More on VPN and all related families can be found at [https://datatracker.ietf.org/doc/html/rfc2764](https://datatracker.ietf.org/doc/html/rfc2764)
    and [https://datatracker.ietf.org/doc/html/rfc4026](https://datatracker.ietf.org/doc/html/rfc4026).
  prefs: []
  type: TYPE_NORMAL
- en: The VLAN was perhaps one of the most important virtualizations created in L2
    networks. Let’s now look at an interesting virtualization created for router gateways.
  prefs: []
  type: TYPE_NORMAL
- en: The Virtual Router Redundancy Protocol
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This protocol was initially created by Cisco in 1998 with the name **Hot Standby
    Router Protocol** (**HSRP**), defined in *RFC2281*. As the use of HSRP was very
    popular at the time, the IETF Network Working Group created the **Virtual Router
    Redundancy Protocol** (**VRRP**) (*RFC3768*).
  prefs: []
  type: TYPE_NORMAL
- en: The concept is simple, giving computers only one default gateway on their routing
    table by acquiring automatically using DHCP or by configuring manually. To use
    two routers redundantly, you might need to update all computers or use VRRP.
  prefs: []
  type: TYPE_NORMAL
- en: VRRP uses a virtual Ethernet address to associate with an IP address; this IP
    address is the default gateway to all computers on the network. *Figure 2.2* illustrates
    `10.0.0.1` using a virtual MAC address that is associated with both routers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – VRRP using a virtual Ethernet address as the default gateway](img/B18165_02_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – VRRP using a virtual Ethernet address as the default gateway
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: More details on VRRP and HSRP can be found at [https://datatracker.ietf.org/doc/html/rfc2281](https://datatracker.ietf.org/doc/html/rfc2281)
    and [https://datatracker.ietf.org/doc/html/rfc3768](https://datatracker.ietf.org/doc/html/rfc3768).
  prefs: []
  type: TYPE_NORMAL
- en: VLANs were created a long time ago, but its concept was used to extend to a
    more flexible usage, as we are going to see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The Virtual Extensible Local Area Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Perhaps the most important of all in virtualization today is the **Virtual Extensible
    Local Area Network** (**VXLAN**). This standard was published in 2014 and is heavily
    used for network virtualization to provide connectivity. With VXLANs, it’s possible
    to create a network with interfaces connected back-to-back to routers like they
    are physical entities, but in reality, they are virtual.
  prefs: []
  type: TYPE_NORMAL
- en: A VXLAN encapsulates data link layer Ethernet frames (layer 2) within the transport
    layer using UDP datagrams (layer 4). VXLAN endpoints, which terminate VXLAN tunnels
    and may be either virtual or physical switch ports, are known as **Virtual Tunnel
    Endpoints** (**VTEPs**).
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: More about VXLANs can be found at [https://datatracker.ietf.org/doc/html/rfc7348](https://datatracker.ietf.org/doc/html/rfc7348).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now explore an open source project that puts in place several virtual
    network technologies, including VLANs, VRRP, and VXLANs.
  prefs: []
  type: TYPE_NORMAL
- en: Open vSwitch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This open source project is perhaps the most important in network virtualization
    today. **Open vSwitch** (**OVS**) runs on any Linux-based virtualization platform
    (kernel 3.10 and newer) and is used to create connectivity in virtual and physical
    environments. The majority of the code is written in C, and it supports several
    protocols including VXLAN, IPSEC, and GRE, among others. OVS is an OpenStack component
    of SDNs and perhaps the most popular implementation of OpenFlow. A basic architecture
    of how OVS works can be found in *Figure 2.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Simplified OVS architecture](img/B18165_02_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Simplified OVS architecture
  prefs: []
  type: TYPE_NORMAL
- en: More details on OVS can be found at [https://github.com/openvswitch/ovs.git](https://github.com/openvswitch/ovs.git).
  prefs: []
  type: TYPE_NORMAL
- en: Linux Containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Linux Containers** (**LXC**) provides operating-system-level virtualization
    using CPU, network, memory, and I/O space isolation. Its first implementation
    was on Linux kernel 2.6.24 in January 2008, but the concept is old and can be
    found in a FreeBSD implementation called **jails** implemented in 1999 and published
    on FreeBSD 4.0 in March 2000 (details at: docs.freebsd.org/en/books/handbook/jails/).'
  prefs: []
  type: TYPE_NORMAL
- en: Today, more and more implementations of LXC can be found, but the concept of
    CPU, network, memory, and I/O space isolation is the same. The most popular LXC
    implementation today is **Docker**.
  prefs: []
  type: TYPE_NORMAL
- en: With LXC and Open vSwitch, it’s possible to create an entire virtual network
    topology with hundreds of routers. A powerful example is **Mininet** ([http://mininet.org/](http://mininet.org/)
    and [https://github.com/mininet/mininet](https://github.com/mininet/mininet)).
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: More on LXC and FreeBSD jail can be found at [https://en.wikipedia.org/wiki/LXC](https://en.wikipedia.org/wiki/LXC)
    and [https://en.wikipedia.org/wiki/FreeBSD_jail](https://en.wikipedia.org/wiki/FreeBSD_jail).
  prefs: []
  type: TYPE_NORMAL
- en: Containers for Linux can create most virtualizations, however they are limited
    by using the same operational system because containers share the same kernel.
    Virtual machines, as we’ll see next, can be used to virtualize a wide range of
    other operating systems.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual machines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LXC is powerful in isolating parts of the operating system; however, they aren’t
    able to run applications that require a different CPU or hardware. So, **virtual
    machines** (**VMs**) are there to add this extra virtualization by simulating
    physical hardware and CPU.
  prefs: []
  type: TYPE_NORMAL
- en: A VM can further isolate the operating system by creating a whole new layer
    of CPU, I/O, memory, and network. For instance, in network virtualization, it’s
    possible to run different operating systems with different CPUs, such as Juniper
    JunOS using Intel CPUs, and Cisco IOS using MIPS CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: The most popular open source implementation of VMs is **Xen** ([https://xenproject.org/](https://xenproject.org/)).
  prefs: []
  type: TYPE_NORMAL
- en: We do have much more to talk about regarding network virtualization, but that
    would be a topic for another book. At least for the time being, what we have examined
    in this section is sufficient to identify the main technologies used by programmable
    networks. At this point, you should be able to identify these technologies easily
    if you encounter them.
  prefs: []
  type: TYPE_NORMAL
- en: SDNs and OpenFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have investigated a few historical milestones of programmable networks and
    network virtualization that form the base of what we know today as SDNs. Next,
    let’s talk about the details behind SDNs.
  prefs: []
  type: TYPE_NORMAL
- en: In order for SDNs to be successful, they need to be flexible and programmable,
    making it simple to deploy and control traffic and manage their components. None
    of this could be done without separation between the control plane and the forwarding
    plane (the data plane).
  prefs: []
  type: TYPE_NORMAL
- en: SDN implementation is done by having an application that uses the decoupling
    of these two planes to construct the data flows of the network. This application
    can run in a network server or in a VM, which sends control packets to the network
    devices using an OpenFlow protocol when possible.
  prefs: []
  type: TYPE_NORMAL
- en: History of OpenFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenFlow is a standard protocol used in SDNs. Its origins can be traced back
    to 2006 with the project mentioned earlier in this chapter called Ethane. Eventually,
    the Ethane project led to what became known as OpenFlow, thanks to a joint research
    effort by teams at Stanford and Berkeley universities. The initial idea was to
    centrally manage policies using a flow-based network and a controller with a focus
    on network security; that is the reason for *Flow* being in the name *OpenFlow*.
  prefs: []
  type: TYPE_NORMAL
- en: After the initial work by Berkeley and Stanford, companies such as Nicira and
    Big Switch Networks started to raise significant amounts of venture capital funding
    to help push their products with ideas on a flow-based controlled network, but
    at that time no standards were yet published. A protocol was needed to move network
    control out of proprietary network switches and into control software that was
    open source and locally managed. This is the reason that the name *OpenFlow* has
    the word *Open* in it.
  prefs: []
  type: TYPE_NORMAL
- en: By 2011, the **Open Networking Foundation** (**ONF**) had been created with
    the aim of standardizing emerging technologies for networking and data center
    management. The founding members were Google, Facebook, and Microsoft, while Citrix,
    Cisco, Dell, HP, F5 Networks, IBM, NEC, Huawei, Juniper Networks, Oracle, and
    VMware joined later.
  prefs: []
  type: TYPE_NORMAL
- en: The ONF working group released the first version of the OpenFlow protocol in
    December 2009, and in February 2011 they made version 1.1 public. The most updated
    version is from March 2015 – version 1.5.1 ([https://opennetworking.org/wp-content/uploads/2014/10/openflow-switch-v1.5.1.pdf](https://opennetworking.org/wp-content/uploads/2014/10/openflow-switch-v1.5.1.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: SDN architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The simple architecture of an SDN is shown in *Figure 2.4*. The SDN controller
    has **northbound interfaces** (**NBIs**) toward business-level applications and
    **southbound interfaces** (**SBIs**) toward network devices.
  prefs: []
  type: TYPE_NORMAL
- en: To communicate with network devices, the SBI requires a control protocol. It
    is desirable for the control protocol to be OpenFlow; however, other protocols
    can be used if the device does not support it, such as Cisco OpFlex, SNMP, or
    even CLI via SSH (this will be covered in the next chapter).
  prefs: []
  type: TYPE_NORMAL
- en: The NBI is used to collect information from the business or for the business
    to collect information from the network (in *Figure 2.4*, this is represented
    by the application plane), for instance, allowing administrators to access the
    SDN controller to retrieve information about the network. Access to the controller
    is normally done via an API protocol.
  prefs: []
  type: TYPE_NORMAL
- en: 'Normally, the NBI is used for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting information from devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting the status of physical interfaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructing data flows between devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But the available methods on the NBI API will depend on the SDN application
    and what the vendor made available.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to emphasize that the NBI API for the SDN has no responsibility
    for managing the network devices, such as attributing configuration or doing software
    updates. The main responsibility of the SDN NBI API is to allow administrators
    and businesses to give directions to the SDN controller in order to make decisions
    on how traffic will flow through the network devices based on pre-defined criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at the simple architecture of an SDN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Basic SDN architecture](img/B18165_02_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Basic SDN architecture
  prefs: []
  type: TYPE_NORMAL
- en: Despite being used in SDN and being a very well-known term in the internet community,
    OpenFlow’s future might be not that bright. Let’s find out why.
  prefs: []
  type: TYPE_NORMAL
- en: OpenFlow and its future
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Looking at how the OpenFlow standard track is being updated and how vendors
    are implementing it, its future doesn’t look promising.
  prefs: []
  type: TYPE_NORMAL
- en: The first usable version of OpenFlow was published in 2011, known as version
    1.1\. Since then, updates have been incorporated until 2015 with version 1.5.1\.
    But more than six years have passed and no updates have been published yet.
  prefs: []
  type: TYPE_NORMAL
- en: Version 1.6 of OpenFlow has been available since 2016, but only for members
    of the ONF, which does not help the user’s confidence in OpenFlow’s future.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the lack of updates, Cisco (one of the major network vendors)
    has been working on its own version of OpenFlow called OpFlex since 2014 because
    it saw limitations in OpenFlow’s approach. Cisco also has made OpFlex open, allowing
    others to use without restriction and has started working on an RFC to publish
    OpFlex ([https://datatracker.ietf.org/doc/draft-smith-opflex/](https://datatracker.ietf.org/doc/draft-smith-opflex/)).
  prefs: []
  type: TYPE_NORMAL
- en: So, the SBIs described in *Figure 2.4* do not necessarily use OpenFlow. Today,
    SDN implementations vary and may use different types of SBIs that are associated
    to the methods available for device communication for creation of the traffic
    flow policies.
  prefs: []
  type: TYPE_NORMAL
- en: Other methods and protocols besides OpenFlow are being used with SDN communication,
    such as OpenStack, OpFlex, CLI vis SSH, SNMP, and NETCONF, among others.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve seen in this section, the SDN is a very well-delineated concept on
    how to work with programmable networks; however, because of the lack of OpenFlow
    adoption, SDNs have become more of a concept than a standard. From now on, you
    should have enough knowledge to decide whether your network automation should
    follow OpenFlow or not.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding cloud computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of **cloud computing** is to allow users to benefit from virtual technologies
    without having in-depth knowledge about them. The objective of cloud computing
    is to cut costs and help users focus on their core business instead of the physical
    infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud computing advocates for **Everything as a Service** (**EaaS**), including
    **Infrastructure as a Service** (**IaaS**), which is implemented by providing
    high-level APIs used to abstract various low-level details of underlying network
    infrastructure such as physical computing resources, locations, data partitioning,
    scaling, security, and backup.
  prefs: []
  type: TYPE_NORMAL
- en: Our focus here will be the networking services offered by cloud computing, which
    we usually refer to as cloud networking services.
  prefs: []
  type: TYPE_NORMAL
- en: Commercial cloud computing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Perhaps the most popular cloud computing service today is the 2002 Amazon-created
    subsidiary called **Amazon Web Services** (**AWS**). AWS uses its proprietary
    API to offer cloud services; one of them is created by using **AWS CloudFormation**
    to provide infrastructure as code.
  prefs: []
  type: TYPE_NORMAL
- en: In 2008, Google started offering cloud services; in 2010, Microsoft started
    offering Microsoft Azure; in 2011, IBM announced IBM SmartCloud; and in 2012,
    Oracle start offering Oracle Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are hundreds of other providers and a list of all can be found at the
    following link: [https://www.intricately.com/industry/cloud-hosting](https://www.intricately.com/industry/cloud-hosting).'
  prefs: []
  type: TYPE_NORMAL
- en: The OpenStack Foundation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **OpenStack Foundation** was the first initiative created by NASA and Rackspace
    to start an open source cloud service software. The foundation eventually changed
    its name to the **OpenInfra Foundation**, and today they have more than 500 members.
    Their work has been tremendous, and they created a great set of open source code
    for cloud computing. More details can be found at [https://openinfra.dev/about/](https://openinfra.dev/about/).
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Native Computing Foundation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It sounds a bit confusing, but the **CloudStack Foundation** and the **Cloud
    Native Computing Foundation** (**CNCF**) focus on different aspects of cloud services.
    The CNCF was basically created by Kubernetes as a Linux-container-based idea,
    and CloudStack is a bit older and based on VMs.
  prefs: []
  type: TYPE_NORMAL
- en: The CNCF is a Linux Foundation project that was founded in 2015 to help advance
    Linux container technologies and help to align the tech industry around its evolution.
    It was announced alongside Kubernetes 1.0, which was given to the Linux Foundation
    by Google as a seed technology.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve covered quite a bit about cloud computing, but the key takeaway is that
    even though it was originally intended to add programmability to computers, cloud
    computing is also growing in the network space. One of the most programmable networks
    in this space is OpenStack, which we are going to explore next.
  prefs: []
  type: TYPE_NORMAL
- en: Using OpenStack for networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In contrast to OpenFlow, OpenStack has been busy and promising. It started in
    2010 after a joint project between NASA and Rackspace. Rackspace wanted to rewrite
    the infrastructure code running its cloud servers and at the same time, Anso Labs
    (contracting for NASA) had published beta code for Nova, a Python-based cloud
    computing fabric controller.
  prefs: []
  type: TYPE_NORMAL
- en: By 2012, the OpenStack Foundation was established to promote OpenStack software
    to the cloud community. By 2018, more than 500 companies had joined the OpenStack
    Foundation. By the end of 2020, the foundation announced that would change its
    name starting in 2021 to the **Open Infrastructure Foundation**. The reason is
    that the foundation started to add other projects to OpenStack, and therefore
    the name would not reflect their goals.
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack tracks its versions with different names; the first version in 2010
    was called Austin, which included two components (Nova and Swift). By 2015, the
    new version of OpenStack had arrived, which was called Kilo and had 12 components.
    By October 2021, OpenStack Xena had been released, with 38 service components
    ([https://docs.openstack.org/xena/](https://docs.openstack.org/xena/)).
  prefs: []
  type: TYPE_NORMAL
- en: For us, what matters in OpenStack are the components that will allow us to automate
    the network infrastructure. Although not designed for physical devices, the API
    methods for networking might be extended to physical devices instead of being
    only used in cloud virtual environments.
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack Neutron
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of OpenStack is to create standard services that allow software engineers
    to integrate their applications with cloud computing services. The Xena version
    released in October 2021 had 38 services available.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important services for networking is called **Neutron** (or
    **OpenStack Networking**), which is an OpenStack project aimed at providing *network
    connectivity as a service* between interface devices. It implements the OpenStack
    networking API.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'Neutron API definitions can be found at the following link: [https://docs.openstack.org/api-ref/network/](https://docs.openstack.org/api-ref/network/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Neutron manages all networking configurations for the **virtual networking
    infrastructure** (**VNI**) and the access layer aspects of the **physical networking
    infrastructure** (**PNI**). It also enables projects to create advanced virtual
    network topologies, which may include services such as a firewall and a VPN. It
    provides networks, subnets, and routers as object abstractions. Each abstraction
    has functionality that mimics its physical counterpart: networks contain subnets,
    and routers route traffic between different subnets and networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For more details on Neutron, visit the following link: [https://docs.openstack.org/neutron](https://docs.openstack.org/neutron).'
  prefs: []
  type: TYPE_NORMAL
- en: The Neutron API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Neutron API** is a RESTful HTTP service that uses all aspects of the HTTP
    protocol, including methods, URIs, media types, response codes, and more. API
    clients can use existing features of the protocol, including caching, persistent
    connections, and content compression.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let’s look at the `HTTP GET` BGP peers’ method.
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain a list of BGP peers use a `HTTP` `GET` request to `/v2.0/bgp-peers`.
    The possible responses are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Normal response codes: `200`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Error response codes: `400, 401, 403`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fields that can be added to the API request:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **In** | **Type** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `fields` (optional) | Query | String | The fields that you want the server
    to return. If a `fields` query parameter is not specified, the networking API
    returns all attributes allowed by the policy settings. By using the `fields` parameter,
    the API returns only the requested set of attributes. A `fields` parameter can
    be specified multiple times. For example, if you specify `fields=id&fields=name`
    in the request URL, only the `id` and `name` attributes will be returned. |'
  prefs: []
  type: TYPE_TB
- en: Table 2.1 – API request fields
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameters that are returned in the API response are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **In** | **Type** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `bgp_peers` | Body | Array | A list of `bgp_peer` objects. Each `bgp_peer`
    object represents real BGP infrastructure, such as routers, route reflectors,
    and route servers. |'
  prefs: []
  type: TYPE_TB
- en: '| `remote_as` | Body | String | The remote autonomous system number of the
    BGP peer. |'
  prefs: []
  type: TYPE_TB
- en: '| `name` | Body | String | A more descriptive name of the BGP peer. |'
  prefs: []
  type: TYPE_TB
- en: '| `peer_ip` | Body | String | The IP address of the peer. |'
  prefs: []
  type: TYPE_TB
- en: '| `id` | Body | String | The ID of the BGP peer. |'
  prefs: []
  type: TYPE_TB
- en: '| `tenant_id` | Body | String | The ID of the tenant. |'
  prefs: []
  type: TYPE_TB
- en: '| `project_id` | Body | String | The ID of the project. |'
  prefs: []
  type: TYPE_TB
- en: Table 2.2 – API response fields
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of the API response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The API is well documented at the following link: [https://docs.openstack.org/api-ref/network/](https://docs.openstack.org/api-ref/network/).'
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen in this section, OpenStack is perhaps the cloud computing platform
    that is closest to network programming, as demonstrated by the CloudStack Neutron
    API. Additional features are probably going to be added as more network elements
    are migrated to the cloud. You should now be familiar with OpenStack terms and
    be able to explore them in depth if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we talked about how programmable networks have evolved up until
    the present day. We discussed the history of data plane and control plane separation.
    We’ve seen how network virtualization has improved over time. We also looked at
    some technologies and standards for SDNs and cloud networking, such as OpenFlow
    and OpenStack.
  prefs: []
  type: TYPE_NORMAL
- en: You now have the knowledge required to understand why some technologies are
    used today to automate and code networks.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’re going to dive deeper into the methods, protocols,
    and standards used to configure and communicate with network devices.
  prefs: []
  type: TYPE_NORMAL
