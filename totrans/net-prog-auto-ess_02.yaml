- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Programmable Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可编程网络
- en: Initially, computer networks were something physical and static, with wires
    and hardware, but with advanced computation, virtualization, and connectivity,
    networks have become more flexible and configurable by software. In this chapter,
    we’re going to talk about how software has changed the picture for computer networks.
    We are going first to examine several different technologies used today to create
    networks via software, then we are going to examine the current standard technology,
    known as **software-defined networks** (**SDNs**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，计算机网络是物理和静态的，由线和硬件组成，但随着高级计算、虚拟化和连接性的发展，网络变得更加灵活，可以通过软件进行配置。在本章中，我们将讨论软件是如何改变计算机网络图景的。我们将首先检查今天用于通过软件创建网络的几种不同技术，然后我们将检查当前的标准技术，称为**软件定义网络**（**SDNs**）。
- en: As we saw in the first chapter, computer networks can be quite complex and difficult
    to maintain. There are several different sets of equipment that range from routers,
    switches, and NATs to load balancers and more. In addition, within each piece
    of equipment, there are several different types of operation, such as *core* or
    *access* routers. Network equipment is typically configured individually with
    interfaces that are very different between each vendor. Although there are management
    systems that can help centralize the configuration in one single place, network
    equipment is configured at an individual level. This kind of operation means complexity
    in operation and slow innovation for new features.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第一章中看到的，计算机网络可以非常复杂且难以维护。从路由器、交换机和NAT到负载均衡器等，有多种不同的设备。此外，在每一件设备中，还有多种不同的操作类型，例如*核心*或*接入*路由器。网络设备通常由各个供应商配置，接口之间差异很大。尽管有可以帮助集中配置的管理系统，但网络设备通常在单个级别上进行配置。这种操作意味着操作复杂性和新功能创新的缓慢。
- en: 'In this chapter, we are going to cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Exploring the history of programmable networks and looking at those used in
    the present day
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索可编程网络的历史并查看当前使用的那些
- en: Virtual network technologies
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚拟网络技术
- en: SDNs and OpenFlow
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SDNs和OpenFlow
- en: Understanding cloud computing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解云计算
- en: Using OpenStack for networking
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用OpenStack进行网络配置
- en: Exploring the history of programmable networks and looking at those used in
    the present day
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索可编程网络的历史并查看当前使用的那些
- en: Several years have passed since **programmable networks** were initially conceived
    by engineers, so let’s touch on a few historical milestones before we get into
    the current technologies.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自从工程师最初构思**可编程网络**以来已经过去了几年，因此在我们深入了解当前技术之前，让我们简要回顾一些历史里程碑。
- en: Active networking
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主动网络
- en: The **Defense Advanced Research Projects Agency** (**DARPA**) began funding
    research in the mid-1990s to create a network that could easily be changed and
    customized by programming, called the **active networking** project. The main
    goal of the project was to create network technologies that, in contrast to then-current
    networks, were easy to innovate and evolve, allowing fast application and protocol
    development.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**国防高级研究计划局**（**DARPA**）从20世纪90年代中期开始资助研究，旨在创建一个可以通过编程轻松更改和定制的网络，称为**主动网络**项目。该项目的主要目标是创建网络技术，与当时现有的网络相比，这些技术易于创新和演进，允许快速的应用程序和协议开发。'
- en: But it was not easy to create such a flexible network in the 1990s because programming
    languages, signaling and network protocols, and operating systems were not mature
    enough to accommodate such innovative ideas. For instance, operation systems were
    monolithic and adding features required recompilation and reboot. In addition,
    service APIs were non-existent and distributed programming languages were still
    in early development.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但在20世纪90年代创建这样一个灵活的网络并不容易，因为编程语言、信令和网络协议以及操作系统还不够成熟，无法容纳这样的创新想法。例如，操作系统是单块的，添加功能需要重新编译和重启。此外，服务API不存在，分布式编程语言仍处于早期开发阶段。
- en: The active networking research programs explored radical alternatives to the
    services provided by the traditional internet stack via IP. Examples of this work
    can be found in projects such as **Global Environment for Network Innovations**
    (**GENIs**), which can be viewed at [https://www.geni.net/](https://www.geni.net/),
    **Nacional Science Foundation** (**NSF**), **Future Internet Design** (**FIND**)
    which can be viewed at [http://www.nets-find.net/](http://www.nets-find.net/),
    and **Future Internet Research and Experimentation Initiative** (**EU FIRE**).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 活跃的网络研究项目探索了通过IP提供的传统互联网堆栈服务的激进替代方案。这方面的例子可以在**全球网络创新环境**（**GENI**）项目中找到，该项目可以在[https://www.geni.net/](https://www.geni.net/)查看，**国家科学基金会**（**NSF**），**未来互联网设计**（**FIND**）可以在[http://www.nets-find.net/](http://www.nets-find.net/)查看，以及**未来互联网研究和实验倡议**（**EU
    FIRE**）。
- en: 'At the time, the active networking research community pursued two programming
    models:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当时，活跃的网络研究社区追求两种编程模型：
- en: The **capsule model**, where the code to execute at the nodes was carried in-band
    in data packets
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**胶囊模型**，其中节点上要执行的代码在数据包中带内传输'
- en: The **programmable router/switch model**, where the code to execute at the nodes
    was established by out-of-band mechanisms
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可编程路由器/交换机模型**，其中节点上要执行的代码由带外机制建立'
- en: Important note
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'Further reading can be found in *Active Networking – One View of the Past,
    Present, and Future*. Jonathan M. Smith and Scott M. Nettles – *IEEE TRANSACTIONS
    ON SYSTEMS - PART C: Application and Reviews, Vol 34 No 1, February 2004.*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在*《主动网络——过去、现在和未来的一个视角》*中可以找到更多阅读材料。作者：Jonathan M. Smith和Scott M. Nettles –
    *《IEEE系统——应用和评论部分》第34卷第1期，2004年2月*。
- en: Let’s dive into one of the first attempts at creating a node that was programmable,
    known as **NodeOS**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解第一个尝试创建可编程节点的尝试，这个节点被称为**NodeOS**。
- en: NodeOS
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NodeOS
- en: One of the first goals of the active networking project was to create **NodeOS**.
    It was an operating system whose primary purpose was to support packet forwarding
    in an active network. NodeOS ran at the lowest level in an active node and multiplexed
    the node resources, such as memory and CPU, among the packet flows that traverse
    the node. NodeOS provided several important services to active network execution
    environments, including resource scheduling and accounting and fast packet input-output.
    The two important design milestones on NodeOS were the creation of **application
    programming interfaces** (**APIs**) and resource management.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 活跃网络项目的第一个目标之一是创建**NodeOS**。这是一个主要目的是支持主动网络中数据包转发的操作系统。NodeOS在主动节点中运行在最低级别，并在穿越节点的数据包流之间复用节点资源，如内存和CPU。NodeOS为主动网络执行环境提供了几个重要的服务，包括资源调度和会计以及快速数据包输入/输出。NodeOS的两个重要设计里程碑是创建**应用程序编程接口**（**API**）和资源管理。
- en: Important note
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Additional reading on NodeOS can be found at *An OS Interface for Active Routers
    April 2001* – *IEEE Journal on Selected Areas in Communications 19(3):473 – 487.*
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在*《2001年4月主动路由器操作系统接口》* – *《IEEE选择通信领域杂志》第19卷第3期，第473-487页*中可以找到关于NodeOS的更多阅读材料。
- en: Following this, we are now going to explore a few projects that were the early
    attempts from the community toward SDN.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，我们现在将探索一些社区早期尝试SDN的项目。
- en: Data and control plane separation
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据平面和控制平面分离
- en: One of the major steps toward programmable networks and **SDNs** was the separation
    of the **control** and **data** planes. In [*Chapter 1*](B18165_01.xhtml#_idTextAnchor015),
    we discussed the difference between the control and data planes, and here we are
    going to discuss a bit of the history behind them. It’s worth remembering that
    the data plane is also known as the **forwarding plane**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 可编程网络和**SDN**的一个重要步骤是将**控制**和**数据**平面分离。在[*第一章*](B18165_01.xhtml#_idTextAnchor015)中，我们讨论了控制平面和数据平面的区别，在这里我们将讨论它们背后的历史。值得记住的是，数据平面也被称为**转发平面**。
- en: By the 1990s, such separation was already present on public telephone networks
    but was not yet implemented in computer networks or on the internet. As network
    complexity increased and internet services started to become the main revenue
    for several backbone providers, reliability, predictability, and performance were
    key points for network operators to seek better approaches for managing networks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 到了20世纪90年代，这种分离已经在公共电话网络上存在，但尚未在计算机网络或互联网上实施。随着网络复杂性的增加和互联网服务开始成为几个骨干提供商的主要收入来源，可靠性、可预测性和性能成为网络运营商寻求管理网络更好方法的关键点。
- en: In the early 2000s, a community of researchers who either worked for or regularly
    interacted with network operators started to explore pragmatic approaches using
    either standard protocols or other imminent technologies that were just about
    to become deployable. At that time, routers and switches had tight integration
    between the control and forwarding planes. This coupling made various network
    management tasks difficult, such as debugging configuration problems and controlling
    routing behavior. To address these challenges, various efforts to separate the
    forwarding and control planes began to emerge. Let’s explore a few of the earlier
    efforts in the following sections.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在2000年代初，一群研究人员开始探索使用标准协议或其他即将部署的技术来采取实用方法的社区，他们要么为网络运营商工作，要么定期与他们互动。当时，路由器和交换机在控制和转发平面之间有紧密的集成。这种耦合使得各种网络管理任务变得困难，例如调试配置问题和控制路由行为。为了应对这些挑战，开始出现各种将转发和控制平面分离的努力。以下几节将探讨一些早期的努力。
- en: IETF ForCES
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IETF ForCES
- en: '**Forwarding and Control Element Separation** (**IETF ForCES**) working groups
    intended to create a framework, a list of requirements, a solution protocol, a
    logical function block library, and other associated documents in support of data
    and control element separation ([https://datatracker.ietf.org/wg/forces/about/](https://datatracker.ietf.org/wg/forces/about/)).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**转发与控制元素分离**（**IETF ForCES**）工作组旨在创建一个框架，一个需求列表，一个解决方案协议，一个逻辑功能块库以及其他支持数据和控制元素分离的相关文档（[https://datatracker.ietf.org/wg/forces/about/](https://datatracker.ietf.org/wg/forces/about/))）。'
- en: The NetLink interface
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NetLink接口
- en: '**NetLink** was perhaps the clearest separation of the control plane and data
    plane on the Linux kernel. In 2003, IETF published the *RFC3549* describing the
    separation of the **control plane components** (**CPCs**) and the **forwarding
    engine components** (**FECs**). *Figure 2.1* (from the original RFC) illustrates
    how Linux was using Netlink as the main separator between the control and data
    planes ([https://datatracker.ietf.org/doc/html/rfc3549](https://datatracker.ietf.org/doc/html/rfc3549)).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**NetLink**可能是Linux内核中控制平面和数据平面最清晰的分离。2003年，IETF发布了*RFC3549*，描述了**控制平面组件**（**CPCs**）和**转发引擎组件**（**FECs**）的分离。*图2.1*（来自原始RFC）说明了Linux如何使用Netlink作为控制和数据平面之间的主要分隔符（[https://datatracker.ietf.org/doc/html/rfc3549](https://datatracker.ietf.org/doc/html/rfc3549)）。'
- en: '![Figure 2.1 – Control and data plane separation, as shown in RFC3549](img/B18165_02_001.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1 – 如RFC3549所示的控制平面和数据平面分离](img/B18165_02_001.jpg)'
- en: Figure 2.1 – Control and data plane separation, as shown in RFC3549
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 如RFC3549所示的控制平面和数据平面分离
- en: Netlink was first created on series 2.0 of the Linux kernel.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Netlink首次出现在Linux内核的2.0系列中。
- en: Routing Control Platform
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 路由控制平台
- en: '**Routing Control Platform** (**RCP**) is a pragmatic design to separate the
    control and data planes. The idea was to create a centralized control where all
    routing information was collected and then run an algorithm to select the best
    routing path for each of the routers of the network.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**路由控制平台**（**RCP**）是一种实用的设计，用于分离控制和数据平面。想法是在创建一个集中式控制，其中收集所有路由信息，然后运行算法为网络中的每个路由器选择最佳路由路径。'
- en: RCP was implemented by collecting routing tables from external and internal
    **Border Gateway Protocol** (**BGP**) from the routers in the current network,
    using this information in a centralized manner to choose the best path to each
    of the routers. With this approach, it was possible to leverage the existing network
    devices and have control plane and data plane separation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: RCP通过从当前网络中的路由器收集外部和内部的**边界网关协议**（**BGP**）路由表来实现，以集中方式使用这些信息为每个路由器选择最佳路径。这种方法使得可以利用现有的网络设备并实现控制平面和数据平面的分离。
- en: Important note
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'More about RCP using BGP can be found in the paper *Design and implementation
    of a routing control platform* – Authors: Matthew Caesar, Donald Caldwell, Nick
    Feamster, Jennifer Rexford, Aman Shaikh, Jacobus van der Merwe – *NSDI’05: Proceedings
    of the 2nd conference on Symposium on Networked Systems Design & Implementation
    – Volume 2 May 2005 Pages 15–28.*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在关于使用BGP的RCP的更多信息可以在论文《设计和实现一个路由控制平台》中找到——作者：马修·凯撒，唐纳德·卡尔多，尼克·费姆斯特，詹妮弗·雷克斯福德，阿曼·沙伊赫，雅各布斯·范德梅尔韦——*NSDI’05：第二届网络化系统设计与实现研讨会论文集——第二卷，2005年5月，第15-28页。*
- en: SoftRouter
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SoftRouter
- en: The **SoftRouter** idea was presented at a conference in 2004 and patented in
    2005\. Again, the architecture had separation between control plane functions
    and data plane functions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**SoftRouter**的想法在2004年的一次会议上提出，并在2005年获得专利。同样，该架构在控制平面功能与数据平面功能之间有分离。'
- en: All control plane functions were implemented on general-purpose servers called
    **control elements** (**CEs**), which may be multiple hops away from the **forwarding
    elements** (**FEs**). There were two main types of network entities in the SoftRouter
    architecture, which were the FEs and CEs. Together, they constituted a **network
    element** (**NE**) router. The key difference from a traditional router was the
    absence of any control logic (such as OSPF or BGP) running locally. Instead, the
    control logic was hosted remotely.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 所有控制平面功能都实现在通用服务器上，称为**控制元素**（**CEs**），这些服务器可能距离**转发元素**（**FEs**）有多跳之远。SoftRouter架构中有两种主要的网络实体，即FEs和CEs。它们共同构成了一个**网络元素**（**NE**）路由器。与传统路由器的主要区别是本地没有运行任何控制逻辑（如OSPF或BGP）。相反，控制逻辑是在远程托管。
- en: Important note
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: More details on SoftRouter can be found in the original 2004 paper *The SoftRouter
    Architecture* – T. V. Lakshman, T. Nandagopal, R. Ramjee, K. Sabnani, T. Woo –
    *Bell Laboratories, Lucent Technologies, ACM HOTNETS - January 2004.*
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于SoftRouter的细节可以在原始的2004年论文《The SoftRouter Architecture》中找到 – T. V. Lakshman,
    T. Nandagopal, R. Ramjee, K. Sabnani, T. Woo – *Bell Laboratories, Lucent Technologies,
    ACM HOTNETS - January 2004.*
- en: The path computation element architecture
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 路径计算元素架构
- en: In 2006, the IETF Network Working Group published an RFC describing an architecture
    of a centralized controlled entity to make route path decisions, which they called
    the **path computation element** (**PCE**) architecture.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 2006年，IETF网络工作组发布了一个RFC，描述了一个集中控制的实体架构，用于做出路由路径决策，他们将此称为**路径计算元素**（**PCE**）架构。
- en: Initially, PCE architecture was invented to solve a problem in **multiprotocol
    label switching** (**MPLS**) where the **label switch path** (**LSP**) calculations
    were becoming very slow and heavy for each router to calculate. It was designed
    to do the calculations on a server inside or outside the network instead.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，PCE架构的发明是为了解决**多协议标签交换**（**MPLS**）中存在的问题，即每个路由器计算**标签交换路径**（**LSP**）变得越来越慢和繁重。它被设计为在内部或外部的服务器上执行计算。
- en: Important note
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: More details on PCE can be found in *RFC4655:* [https://datatracker.ietf.org/doc/html/rfc4655](https://datatracker.ietf.org/doc/html/rfc4655)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于PCE的细节可以在RFC4655中找到：[https://datatracker.ietf.org/doc/html/rfc4655](https://datatracker.ietf.org/doc/html/rfc4655)
- en: Let’s now look at the most important project of all, which was the work that
    went toward OpenFlow and SDNs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在看看最重要的项目，即OpenFlow和SDN的工作。
- en: Ethane
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 乙烷
- en: '**Ethane** was one of the most important projects and culminated in the creation
    of OpenFlow and SDNs. Initially, it was just a project from a PhD student that
    defined a network as a group of data flows and network policies to control the
    traffic, which is another way to see the separation between the data plane and
    the control plane.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**乙烷**是最重要的项目之一，最终导致了OpenFlow和SDN的创建。最初，它只是一个博士生的项目，定义了一个网络为一组数据流和网络策略来控制流量，这也是另一种看待数据平面和控制平面分离的方式。'
- en: The Ethane project had the idea of centralizing all network policies in one
    place. A new device joining the Ethane network should have all its communication
    turned off by default. The new device should get explicit permissions from the
    centralized server before connecting and its data flow should only be allowed
    on the permitted paths.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Ethane项目有集中所有网络策略在一个地方的想法。加入Ethane网络的新设备默认应该关闭所有通信。新设备在连接之前应该从集中服务器获得明确的权限，并且其数据流只能在允许的路径上传输。
- en: Important note
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'More on the Ethane project can be found in the 2007 original paper *Ethane:
    taking control of the enterprise* – Authors: M. Casado, M. J. Freedman, J. Pettit,
    J. Luo, N. McKeown, Scott Shenker – *SIGCOMM ‘07: Proceedings of the 2007 conference,
    pages 1–12.*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '更多关于Ethane项目的细节可以在2007年的原始论文《Ethane: taking control of the enterprise》中找到 –
    作者：M. Casado, M. J. Freedman, J. Pettit, J. Luo, N. McKeown, Scott Shenker – *SIGCOMM
    ‘07: Proceedings of the 2007 conference, pages 1–12.*'
- en: In this section, we explored a bit of the history behind programmable networks.
    We also explored a few of the main projects that led to the separation of the
    control and data planes, which was an important milestone toward SDNs. You should
    now be able to identify the significance of the separation and why it happened.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了可编程网络背后的部分历史。我们还探讨了几个导致控制平面和数据平面分离的主要项目，这是SDN的重要里程碑。你现在应该能够识别分离的重要性以及为什么会发生这种情况。
- en: Virtual network technologies
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟网络技术
- en: '**Network virtualization** is when software acts like network hardware and
    it is accomplished by using logically simulated hardware platforms.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络虚拟化**是指软件像网络硬件一样工作，这是通过使用逻辑上模拟的硬件平台来实现的。'
- en: Virtualization of networks is not a new concept, and we can find one of the
    first implementations in the mid-1970s with virtual circuits on X.25 networks.
    Later, other technologies also started using virtual concepts, such as Frame Relay
    and ATM, but they are now obsolete.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 网络虚拟化不是一个新概念，我们可以在1970年代中期在X.25网络上的虚拟电路中找到第一个实现。后来，其他技术也开始使用虚拟概念，如帧中继和ATM，但它们现在已经过时。
- en: Loopback interfaces were based on electronics where loopbacks are used to create
    electric loops for the signal to return to its source for testing purposes. In
    1981, the IETF referred to the reserved address range `127.rrr.rrr.rrr` with `127.rrr.rrr.rrr`
    was officially called `127.0.0.1`)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 回环接口基于电子学，其中回环用于创建电回路，以便信号返回其源进行测试。1981年，IETF将保留地址范围`127.rrr.rrr.rrr`（其中`127.rrr.rrr.rrr`正式称为`127.0.0.1`）。
- en: Another early implementation of network virtualization was the **virtual** **LAN**
    or **VLAN**. By 1981, David Sincoskie was testing segmenting voice-over-Ethernet
    networks to facilitate fault tolerance, something similar to what VLAN does. However,
    it was only after 17 years that, in 1998, VLAN was finally published as a standard
    by IEEE by the name *802.1Q*. By the 2000s, switched networks dominated the landscape
    with switches, repeaters, and bridges, making VLANs commonplace. A LAN without
    a VLAN is virtually impossible today.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 网络虚拟化的另一个早期实现是**虚拟****局域网**（VLAN）。到1981年，David Sincoskie正在测试分割语音以太网网络以实现容错，这与VLAN的作用类似。然而，直到17年后的1998年，IEEE才将VLAN正式作为标准发布，命名为*802.1Q*。到2000年代，交换网络由交换机、中继器和桥接器主导，使得VLAN变得普遍。今天没有VLAN的局域网几乎是不可能的。
- en: There are several other network virtualization technologies that are used today.
    Let’s explore the important ones in the following sections.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 今天还有几种其他网络虚拟化技术被使用。接下来几节将探讨其中重要的技术。
- en: Virtual private networks
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 虚拟专用网络
- en: This is the concept of creating an isolated secured network overlay that is
    implemented on network carriers, service providers, and over the internet.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是创建一个在网络上、服务提供商和互联网上实现的隔离安全网络覆盖的概念。
- en: In other words, **virtual private network** (**VPN**) is a generic term that
    describes the use of public or private networks to create groups of users that
    are isolated from other network users, allowing them to communicate between themselves
    as if they were on a private network.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，**虚拟专用网络**（VPN）是一个通用术语，描述了使用公共或私人网络来创建与其他网络用户隔离的用户组，使他们能够像在私有网络中一样相互通信。
- en: VPNs use end-to-end traffic encryption to enhance data separation, especially
    when using public networks, but this is not necessarily the case for all implementations.
    For instance, when using VPNs in MPLS networks, the traffic is not encrypted as
    it runs over private domains, and data separation exists only by packet encapsulation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: VPN使用端到端流量加密来增强数据分离，尤其是在使用公共网络时，但这并不一定适用于所有实现。例如，在使用MPLS网络中的VPN时，由于流量在私有域中运行，因此不会进行加密，数据分离仅通过数据包封装存在。
- en: VPN is a generic name, but more specific names can be found, such as L3VPN,
    L2VPN, VPLS, Pseudo Wires, and VLLS, among others.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: VPN是一个通用名称，但还可以找到更具体的名称，例如L3VPN、L2VPN、VPLS、伪线（Pseudo Wires）和VLLS等。
- en: Important note
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: More on VPN and all related families can be found at [https://datatracker.ietf.org/doc/html/rfc2764](https://datatracker.ietf.org/doc/html/rfc2764)
    and [https://datatracker.ietf.org/doc/html/rfc4026](https://datatracker.ietf.org/doc/html/rfc4026).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于VPN及其相关家族的信息可以在[https://datatracker.ietf.org/doc/html/rfc2764](https://datatracker.ietf.org/doc/html/rfc2764)和[https://datatracker.ietf.org/doc/html/rfc4026](https://datatracker.ietf.org/doc/html/rfc4026)找到。
- en: The VLAN was perhaps one of the most important virtualizations created in L2
    networks. Let’s now look at an interesting virtualization created for router gateways.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: VLAN可能是L2网络中创建的最重要的一种虚拟化。现在让我们看看为路由器网关创建的一个有趣的虚拟化。
- en: The Virtual Router Redundancy Protocol
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 虚拟路由器冗余协议
- en: This protocol was initially created by Cisco in 1998 with the name **Hot Standby
    Router Protocol** (**HSRP**), defined in *RFC2281*. As the use of HSRP was very
    popular at the time, the IETF Network Working Group created the **Virtual Router
    Redundancy Protocol** (**VRRP**) (*RFC3768*).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 该协议最初由思科在1998年创建，命名为**热备用路由器协议**（**HSRP**），在*RFC2281*中定义。由于当时HSRP的使用非常流行，IETF网络工作组创建了**虚拟路由器冗余协议**（**VRRP**）（*RFC3768*）。
- en: The concept is simple, giving computers only one default gateway on their routing
    table by acquiring automatically using DHCP or by configuring manually. To use
    two routers redundantly, you might need to update all computers or use VRRP.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 概念很简单，通过自动使用DHCP或手动配置，在计算机的路由表中只给它们一个默认网关。为了冗余地使用两个路由器，你可能需要更新所有计算机或使用VRRP。
- en: VRRP uses a virtual Ethernet address to associate with an IP address; this IP
    address is the default gateway to all computers on the network. *Figure 2.2* illustrates
    `10.0.0.1` using a virtual MAC address that is associated with both routers.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: VRRP使用一个虚拟以太网地址来关联一个IP地址；这个IP地址是网络中所有计算机的默认网关。*图2.2*展示了使用与两个路由器都关联的虚拟MAC地址的`10.0.0.1`。
- en: '![Figure 2.2 – VRRP using a virtual Ethernet address as the default gateway](img/B18165_02_002.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图2.2 – 使用虚拟以太网地址作为默认网关的VRRP](img/B18165_02_002.jpg)'
- en: Figure 2.2 – VRRP using a virtual Ethernet address as the default gateway
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 – 使用虚拟以太网地址作为默认网关的VRRP
- en: Important note
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: More details on VRRP and HSRP can be found at [https://datatracker.ietf.org/doc/html/rfc2281](https://datatracker.ietf.org/doc/html/rfc2281)
    and [https://datatracker.ietf.org/doc/html/rfc3768](https://datatracker.ietf.org/doc/html/rfc3768).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 关于VRRP和HSRP的更多详细信息可以在[https://datatracker.ietf.org/doc/html/rfc2281](https://datatracker.ietf.org/doc/html/rfc2281)和[https://datatracker.ietf.org/doc/html/rfc3768](https://datatracker.ietf.org/doc/html/rfc3768)找到。
- en: VLANs were created a long time ago, but its concept was used to extend to a
    more flexible usage, as we are going to see in the next section.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: VLANs很久以前就被创建了，但其概念被用来扩展到更灵活的使用，正如我们将在下一节中看到的。
- en: The Virtual Extensible Local Area Network
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 虚拟可扩展局域网
- en: Perhaps the most important of all in virtualization today is the **Virtual Extensible
    Local Area Network** (**VXLAN**). This standard was published in 2014 and is heavily
    used for network virtualization to provide connectivity. With VXLANs, it’s possible
    to create a network with interfaces connected back-to-back to routers like they
    are physical entities, but in reality, they are virtual.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天的虚拟化中，也许最重要的是**虚拟可扩展局域网**（**VXLAN**）。这个标准于2014年发布，在网络虚拟化中得到了广泛的应用，以提供连接性。使用VXLAN，可以创建一个网络，其接口与路由器直接相连，就像它们是物理实体一样，但实际上它们是虚拟的。
- en: A VXLAN encapsulates data link layer Ethernet frames (layer 2) within the transport
    layer using UDP datagrams (layer 4). VXLAN endpoints, which terminate VXLAN tunnels
    and may be either virtual or physical switch ports, are known as **Virtual Tunnel
    Endpoints** (**VTEPs**).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: VXLAN封装了数据链路层以太网帧（第2层），在传输层使用UDP数据报（第4层）。终止VXLAN隧道且可能是虚拟或物理交换机端口的VXLAN端点被称为**虚拟隧道端点**（**VTEPs**）。
- en: Important note
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: More about VXLANs can be found at [https://datatracker.ietf.org/doc/html/rfc7348](https://datatracker.ietf.org/doc/html/rfc7348).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 关于VXLAN的更多信息可以在[https://datatracker.ietf.org/doc/html/rfc7348](https://datatracker.ietf.org/doc/html/rfc7348)找到。
- en: Let’s now explore an open source project that puts in place several virtual
    network technologies, including VLANs, VRRP, and VXLANs.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来探索一个开源项目，该项目实现了包括VLANs、VRRP和VXLANs在内的多种虚拟网络技术。
- en: Open vSwitch
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Open vSwitch
- en: This open source project is perhaps the most important in network virtualization
    today. **Open vSwitch** (**OVS**) runs on any Linux-based virtualization platform
    (kernel 3.10 and newer) and is used to create connectivity in virtual and physical
    environments. The majority of the code is written in C, and it supports several
    protocols including VXLAN, IPSEC, and GRE, among others. OVS is an OpenStack component
    of SDNs and perhaps the most popular implementation of OpenFlow. A basic architecture
    of how OVS works can be found in *Figure 2.3*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个开源项目可能是当今网络虚拟化中最重要的项目。**Open vSwitch**（**OVS**）运行在任何基于 Linux 的虚拟化平台（内核 3.10
    及更高版本）上，并用于在虚拟和物理环境中创建连接。大部分代码是用 C 编写的，它支持包括 VXLAN、IPSEC 和 GRE 在内的多个协议。OVS 是 SDN
    的 OpenStack 组件，也许是 OpenFlow 最受欢迎的实现。OVS 的工作基本架构可以在 *图 2.3* 中找到。
- en: '![Figure 2.3 – Simplified OVS architecture](img/B18165_02_003.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3 – 简化的 OVS 架构](img/B18165_02_003.jpg)'
- en: Figure 2.3 – Simplified OVS architecture
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 简化的 OVS 架构
- en: More details on OVS can be found at [https://github.com/openvswitch/ovs.git](https://github.com/openvswitch/ovs.git).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于 OVS 的详细信息可以在 [https://github.com/openvswitch/ovs.git](https://github.com/openvswitch/ovs.git)
    找到。
- en: Linux Containers
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Linux 容器
- en: '**Linux Containers** (**LXC**) provides operating-system-level virtualization
    using CPU, network, memory, and I/O space isolation. Its first implementation
    was on Linux kernel 2.6.24 in January 2008, but the concept is old and can be
    found in a FreeBSD implementation called **jails** implemented in 1999 and published
    on FreeBSD 4.0 in March 2000 (details at: docs.freebsd.org/en/books/handbook/jails/).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**Linux 容器**（**LXC**）通过使用 CPU、网络、内存和 I/O 空间隔离提供操作系统级别的虚拟化。它的首次实现是在 2008 年 1
    月的 Linux 内核 2.6.24 上，但这个概念很古老，可以在 1999 年实现的名为 **jails** 的 FreeBSD 实现中找到，该实现于 2000
    年 3 月在 FreeBSD 4.0 中发布（详情请见：docs.freebsd.org/en/books/handbook/jails/）。'
- en: Today, more and more implementations of LXC can be found, but the concept of
    CPU, network, memory, and I/O space isolation is the same. The most popular LXC
    implementation today is **Docker**.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，LXC 的实现越来越多，但 CPU、网络、内存和 I/O 空间隔离的概念是相同的。今天最受欢迎的 LXC 实现是 **Docker**。
- en: With LXC and Open vSwitch, it’s possible to create an entire virtual network
    topology with hundreds of routers. A powerful example is **Mininet** ([http://mininet.org/](http://mininet.org/)
    and [https://github.com/mininet/mininet](https://github.com/mininet/mininet)).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LXC 和 Open vSwitch，可以创建包含数百个路由器的整个虚拟网络拓扑。一个强大的例子是 **Mininet**（[http://mininet.org/](http://mininet.org/)
    和 [https://github.com/mininet/mininet](https://github.com/mininet/mininet)）。
- en: Important note
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: More on LXC and FreeBSD jail can be found at [https://en.wikipedia.org/wiki/LXC](https://en.wikipedia.org/wiki/LXC)
    and [https://en.wikipedia.org/wiki/FreeBSD_jail](https://en.wikipedia.org/wiki/FreeBSD_jail).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于 LXC 和 FreeBSD jail 的信息可以在 [https://en.wikipedia.org/wiki/LXC](https://en.wikipedia.org/wiki/LXC)
    和 [https://en.wikipedia.org/wiki/FreeBSD_jail](https://en.wikipedia.org/wiki/FreeBSD_jail)
    找到。
- en: Containers for Linux can create most virtualizations, however they are limited
    by using the same operational system because containers share the same kernel.
    Virtual machines, as we’ll see next, can be used to virtualize a wide range of
    other operating systems.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Linux 容器可以创建大多数虚拟化，然而由于它们使用相同的操作系统，因此受到限制，因为容器共享相同的内核。正如我们接下来将要看到的，虚拟机可以用来虚拟化广泛的其它操作系统。
- en: Virtual machines
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 虚拟机
- en: LXC is powerful in isolating parts of the operating system; however, they aren’t
    able to run applications that require a different CPU or hardware. So, **virtual
    machines** (**VMs**) are there to add this extra virtualization by simulating
    physical hardware and CPU.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: LXC 在隔离操作系统部分方面非常强大；然而，它们无法运行需要不同 CPU 或硬件的应用程序。因此，**虚拟机**（**VMs**）存在是为了通过模拟物理硬件和
    CPU 来添加额外的虚拟化。
- en: A VM can further isolate the operating system by creating a whole new layer
    of CPU, I/O, memory, and network. For instance, in network virtualization, it’s
    possible to run different operating systems with different CPUs, such as Juniper
    JunOS using Intel CPUs, and Cisco IOS using MIPS CPUs.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟机可以通过创建全新的 CPU、I/O、内存和网络层来进一步隔离操作系统。例如，在网络虚拟化中，可以运行使用不同 CPU 的不同操作系统，例如 Juniper
    JunOS 使用 Intel CPU，而 Cisco IOS 使用 MIPS CPU。
- en: The most popular open source implementation of VMs is **Xen** ([https://xenproject.org/](https://xenproject.org/)).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的开源虚拟机实现是 **Xen**（[https://xenproject.org/](https://xenproject.org/)）。
- en: We do have much more to talk about regarding network virtualization, but that
    would be a topic for another book. At least for the time being, what we have examined
    in this section is sufficient to identify the main technologies used by programmable
    networks. At this point, you should be able to identify these technologies easily
    if you encounter them.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 关于网络虚拟化，我们还有很多要讨论的，但这将是另一本书的主题。至少就目前而言，在本节中我们所探讨的已经足够用来识别可编程网络所使用的主要技术。此时，如果你遇到这些技术，你应该能够轻松地识别它们。
- en: SDNs and OpenFlow
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SDNs和OpenFlow
- en: We have investigated a few historical milestones of programmable networks and
    network virtualization that form the base of what we know today as SDNs. Next,
    let’s talk about the details behind SDNs.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经调查了一些可编程网络和网络虚拟化的历史里程碑，这些里程碑构成了我们今天所知道的SDN的基础。接下来，让我们谈谈SDN背后的细节。
- en: In order for SDNs to be successful, they need to be flexible and programmable,
    making it simple to deploy and control traffic and manage their components. None
    of this could be done without separation between the control plane and the forwarding
    plane (the data plane).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使SDN成功，它们需要具有灵活性和可编程性，使得部署和控制流量以及管理其组件变得简单。如果没有控制平面和转发平面（数据平面）之间的分离，这一切都无法实现。
- en: SDN implementation is done by having an application that uses the decoupling
    of these two planes to construct the data flows of the network. This application
    can run in a network server or in a VM, which sends control packets to the network
    devices using an OpenFlow protocol when possible.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: SDN的实现是通过一个应用程序来完成的，该应用程序使用这两个平面的解耦来构建网络的数据流。该应用程序可以在网络服务器或虚拟机中运行，当可能时，使用OpenFlow协议向网络设备发送控制数据包。
- en: History of OpenFlow
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenFlow的历史
- en: OpenFlow is a standard protocol used in SDNs. Its origins can be traced back
    to 2006 with the project mentioned earlier in this chapter called Ethane. Eventually,
    the Ethane project led to what became known as OpenFlow, thanks to a joint research
    effort by teams at Stanford and Berkeley universities. The initial idea was to
    centrally manage policies using a flow-based network and a controller with a focus
    on network security; that is the reason for *Flow* being in the name *OpenFlow*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: OpenFlow是SDN中使用的标准协议。其起源可以追溯到2006年，本章前面提到的Ethane项目。最终，Ethane项目在斯坦福大学和伯克利大学的研究团队共同努力下，导致了后来被称为OpenFlow的东西。最初的构想是使用基于流的网络和控制器集中管理策略，重点是网络安全；这就是为什么*Flow*在*OpenFlow*这个名字中的原因。
- en: After the initial work by Berkeley and Stanford, companies such as Nicira and
    Big Switch Networks started to raise significant amounts of venture capital funding
    to help push their products with ideas on a flow-based controlled network, but
    at that time no standards were yet published. A protocol was needed to move network
    control out of proprietary network switches and into control software that was
    open source and locally managed. This is the reason that the name *OpenFlow* has
    the word *Open* in it.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在伯克利和斯坦福的初步工作之后，尼克拉（Nicira）和Big Switch Networks等公司开始筹集大量的风险投资资金，以帮助推动基于流控制网络的产品理念，但当时还没有发布任何标准。需要一个协议将网络控制从专有网络交换机中移出，进入开源且本地管理的控制软件。这就是为什么*OpenFlow*这个名字中包含*Open*这个词的原因。
- en: By 2011, the **Open Networking Foundation** (**ONF**) had been created with
    the aim of standardizing emerging technologies for networking and data center
    management. The founding members were Google, Facebook, and Microsoft, while Citrix,
    Cisco, Dell, HP, F5 Networks, IBM, NEC, Huawei, Juniper Networks, Oracle, and
    VMware joined later.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 到2011年，**开放网络基金会**（**ONF**）已经成立，旨在标准化网络和数据中心管理的新兴技术。创始成员包括谷歌、Facebook和微软，而思杰、思科、戴尔、惠普、F5网络、IBM、NEC、华为、瞻博网络、甲骨文和VMware后来加入了。
- en: The ONF working group released the first version of the OpenFlow protocol in
    December 2009, and in February 2011 they made version 1.1 public. The most updated
    version is from March 2015 – version 1.5.1 ([https://opennetworking.org/wp-content/uploads/2014/10/openflow-switch-v1.5.1.pdf](https://opennetworking.org/wp-content/uploads/2014/10/openflow-switch-v1.5.1.pdf)).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ONF工作组于2009年12月发布了OpenFlow协议的第一个版本，并在2011年2月发布了1.1版本。最新版本是2015年3月的1.5.1版本（[https://opennetworking.org/wp-content/uploads/2014/10/openflow-switch-v1.5.1.pdf](https://opennetworking.org/wp-content/uploads/2014/10/openflow-switch-v1.5.1.pdf)）。
- en: SDN architecture
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SDN架构
- en: The simple architecture of an SDN is shown in *Figure 2.4*. The SDN controller
    has **northbound interfaces** (**NBIs**) toward business-level applications and
    **southbound interfaces** (**SBIs**) toward network devices.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: SDN的简单架构在*图2.4*中展示。SDN控制器面向业务级应用有**北向接口**（**NBIs**），面向网络设备有**南向接口**（**SBIs**）。
- en: To communicate with network devices, the SBI requires a control protocol. It
    is desirable for the control protocol to be OpenFlow; however, other protocols
    can be used if the device does not support it, such as Cisco OpFlex, SNMP, or
    even CLI via SSH (this will be covered in the next chapter).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与网络设备通信，SBI需要控制协议。理想情况下，控制协议应该是OpenFlow；然而，如果设备不支持它，可以使用其他协议，例如Cisco OpFlex、SNMP，甚至通过SSH的CLI（这将在下一章中介绍）。
- en: The NBI is used to collect information from the business or for the business
    to collect information from the network (in *Figure 2.4*, this is represented
    by the application plane), for instance, allowing administrators to access the
    SDN controller to retrieve information about the network. Access to the controller
    is normally done via an API protocol.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: NBI用于从业务或为业务从网络收集信息（在*图2.4*中，这表示为应用平面），例如，允许管理员访问SDN控制器以检索有关网络的信息。通常，通过API协议访问控制器。
- en: 'Normally, the NBI is used for the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，NBI用于以下方面：
- en: Getting information from devices
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从设备获取信息
- en: Getting the status of physical interfaces
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取物理接口的状态
- en: Configuring devices
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置设备
- en: Constructing data flows between devices
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在设备之间构建数据流
- en: But the available methods on the NBI API will depend on the SDN application
    and what the vendor made available.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，NBI API上可用的方法将取决于SDN应用以及厂商提供的功能。
- en: Important note
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: It’s important to emphasize that the NBI API for the SDN has no responsibility
    for managing the network devices, such as attributing configuration or doing software
    updates. The main responsibility of the SDN NBI API is to allow administrators
    and businesses to give directions to the SDN controller in order to make decisions
    on how traffic will flow through the network devices based on pre-defined criteria.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 需要强调的是，SDN的NBI API不负责管理网络设备，例如分配配置或进行软件更新。SDN NBI API的主要责任是允许管理员和业务向SDN控制器下达指令，以便根据预定义的标准对网络设备中的流量流向做出决策。
- en: 'Now, let’s look at the simple architecture of an SDN:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看SDN的简单架构：
- en: '![Figure 2.4 – Basic SDN architecture](img/B18165_02_004.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图2.4 – 基本SDN架构](img/B18165_02_004.jpg)'
- en: Figure 2.4 – Basic SDN architecture
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – 基本SDN架构
- en: Despite being used in SDN and being a very well-known term in the internet community,
    OpenFlow’s future might be not that bright. Let’s find out why.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管OpenFlow在SDN中被广泛应用，并在互联网社区中是一个广为人知的术语，但其未来可能并不那么光明。让我们来看看原因。
- en: OpenFlow and its future
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenFlow及其未来
- en: Looking at how the OpenFlow standard track is being updated and how vendors
    are implementing it, its future doesn’t look promising.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 观察OpenFlow标准的更新情况以及厂商的实施情况，其未来看起来并不乐观。
- en: The first usable version of OpenFlow was published in 2011, known as version
    1.1\. Since then, updates have been incorporated until 2015 with version 1.5.1\.
    But more than six years have passed and no updates have been published yet.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: OpenFlow的第一个可用版本于2011年发布，称为版本1.1。从那时起，更新一直持续到2015年的1.5.1版本。但是，已经过去了六年多，还没有发布任何更新。
- en: Version 1.6 of OpenFlow has been available since 2016, but only for members
    of the ONF, which does not help the user’s confidence in OpenFlow’s future.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: OpenFlow的1.6版本自2016年以来就已可用，但仅限于ONF的成员，这并不利于用户对OpenFlow未来的信心。
- en: In addition to the lack of updates, Cisco (one of the major network vendors)
    has been working on its own version of OpenFlow called OpFlex since 2014 because
    it saw limitations in OpenFlow’s approach. Cisco also has made OpFlex open, allowing
    others to use without restriction and has started working on an RFC to publish
    OpFlex ([https://datatracker.ietf.org/doc/draft-smith-opflex/](https://datatracker.ietf.org/doc/draft-smith-opflex/)).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 除了缺乏更新之外，思科（主要网络供应商之一）自2014年以来一直在开发自己的OpenFlow版本，称为OpFlex，因为它看到了OpenFlow方法的局限性。思科还将OpFlex开源，允许他人无限制地使用，并开始着手制定RFC以发布OpFlex（[https://datatracker.ietf.org/doc/draft-smith-opflex/](https://datatracker.ietf.org/doc/draft-smith-opflex/))。
- en: So, the SBIs described in *Figure 2.4* do not necessarily use OpenFlow. Today,
    SDN implementations vary and may use different types of SBIs that are associated
    to the methods available for device communication for creation of the traffic
    flow policies.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*图2.4*中描述的SBIs（服务化接口）不一定使用OpenFlow。今天，SDN的实施方式多种多样，可能使用与设备通信方法相关联的不同类型的SBIs，以创建流量策略。
- en: Other methods and protocols besides OpenFlow are being used with SDN communication,
    such as OpenStack, OpFlex, CLI vis SSH, SNMP, and NETCONF, among others.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 除了OpenFlow之外，还有其他方法和协议被用于SDN通信中，例如OpenStack、OpFlex、通过SSH的CLI、SNMP和NETCONF等。
- en: As we’ve seen in this section, the SDN is a very well-delineated concept on
    how to work with programmable networks; however, because of the lack of OpenFlow
    adoption, SDNs have become more of a concept than a standard. From now on, you
    should have enough knowledge to decide whether your network automation should
    follow OpenFlow or not.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本节中看到的，SDN（软件定义网络）是一个关于如何与可编程网络协同工作的非常明确的概念；然而，由于OpenFlow的采用不足，SDN更多地成为一个概念，而不是一个标准。从现在起，你应该有足够的知识来决定你的网络自动化是否应该遵循OpenFlow。
- en: Understanding cloud computing
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解云计算
- en: The goal of **cloud computing** is to allow users to benefit from virtual technologies
    without having in-depth knowledge about them. The objective of cloud computing
    is to cut costs and help users focus on their core business instead of the physical
    infrastructure.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算的目标是让用户无需深入了解虚拟技术就能从中受益。云计算的目的是降低成本，并帮助用户专注于核心业务，而不是物理基础设施。
- en: Cloud computing advocates for **Everything as a Service** (**EaaS**), including
    **Infrastructure as a Service** (**IaaS**), which is implemented by providing
    high-level APIs used to abstract various low-level details of underlying network
    infrastructure such as physical computing resources, locations, data partitioning,
    scaling, security, and backup.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算倡导**一切即服务**（**EaaS**），包括**基础设施即服务**（**IaaS**），通过提供高级API来实现，这些API用于抽象底层网络基础设施的各种低级细节，例如物理计算资源、位置、数据分区、扩展、安全性和备份。
- en: Our focus here will be the networking services offered by cloud computing, which
    we usually refer to as cloud networking services.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里的关注点将是云计算提供的网络服务，我们通常称之为云网络服务。
- en: Commercial cloud computing
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 商业云计算
- en: Perhaps the most popular cloud computing service today is the 2002 Amazon-created
    subsidiary called **Amazon Web Services** (**AWS**). AWS uses its proprietary
    API to offer cloud services; one of them is created by using **AWS CloudFormation**
    to provide infrastructure as code.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 可能目前最受欢迎的云计算服务是2002年由亚马逊创建的子公司**Amazon Web Services**（**AWS**）。AWS使用其专有API提供云服务；其中之一是通过使用**AWS
    CloudFormation**提供基础设施即代码的方式创建的。
- en: In 2008, Google started offering cloud services; in 2010, Microsoft started
    offering Microsoft Azure; in 2011, IBM announced IBM SmartCloud; and in 2012,
    Oracle start offering Oracle Cloud.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 2008年，谷歌开始提供云服务；2010年，微软开始提供微软Azure；2011年，IBM宣布推出IBM SmartCloud；2012年，Oracle开始提供Oracle
    Cloud。
- en: 'There are hundreds of other providers and a list of all can be found at the
    following link: [https://www.intricately.com/industry/cloud-hosting](https://www.intricately.com/industry/cloud-hosting).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 有数百个其他提供商，所有提供商的列表可以在以下链接中找到：[https://www.intricately.com/industry/cloud-hosting](https://www.intricately.com/industry/cloud-hosting)。
- en: The OpenStack Foundation
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenStack基金会
- en: The **OpenStack Foundation** was the first initiative created by NASA and Rackspace
    to start an open source cloud service software. The foundation eventually changed
    its name to the **OpenInfra Foundation**, and today they have more than 500 members.
    Their work has been tremendous, and they created a great set of open source code
    for cloud computing. More details can be found at [https://openinfra.dev/about/](https://openinfra.dev/about/).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**OpenStack基金会**是NASA和Rackspace共同发起的第一个项目，旨在启动开源云服务软件。该基金会最终将其名称更改为**OpenInfra基金会**，如今他们拥有超过500名成员。他们的工作非常出色，为云计算创建了一套优秀的开源代码。更多详情可以在[https://openinfra.dev/about/](https://openinfra.dev/about/)找到。'
- en: Cloud Native Computing Foundation
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Cloud Native Computing Foundation
- en: It sounds a bit confusing, but the **CloudStack Foundation** and the **Cloud
    Native Computing Foundation** (**CNCF**) focus on different aspects of cloud services.
    The CNCF was basically created by Kubernetes as a Linux-container-based idea,
    and CloudStack is a bit older and based on VMs.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来有点令人困惑，但**CloudStack基金会**和**云原生计算基金会（CNCF**）关注云计算的不同方面。CNCF基本上是由Kubernetes作为一个基于Linux容器的理念创建的，而CloudStack则稍微早一些，基于虚拟机。
- en: The CNCF is a Linux Foundation project that was founded in 2015 to help advance
    Linux container technologies and help to align the tech industry around its evolution.
    It was announced alongside Kubernetes 1.0, which was given to the Linux Foundation
    by Google as a seed technology.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: CNCF（云原生计算基金会）是一个成立于2015年的Linux基金会项目，旨在帮助推进Linux容器技术，并帮助技术行业围绕其发展进行协调。它与Kubernetes
    1.0一同宣布，Kubernetes是由谷歌赠予Linux基金会的种子技术。
- en: We’ve covered quite a bit about cloud computing, but the key takeaway is that
    even though it was originally intended to add programmability to computers, cloud
    computing is also growing in the network space. One of the most programmable networks
    in this space is OpenStack, which we are going to explore next.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了相当多的云计算内容，但关键要点是，尽管它最初是为了增加计算机的可编程性而设计的，但云计算在网络空间中也正在增长。这个空间中最可编程的网络之一是OpenStack，我们将在下一部分对其进行探讨。
- en: Using OpenStack for networking
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OpenStack进行网络
- en: In contrast to OpenFlow, OpenStack has been busy and promising. It started in
    2010 after a joint project between NASA and Rackspace. Rackspace wanted to rewrite
    the infrastructure code running its cloud servers and at the same time, Anso Labs
    (contracting for NASA) had published beta code for Nova, a Python-based cloud
    computing fabric controller.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 与OpenFlow相比，OpenStack一直活跃且有前景。它始于2010年，是NASA和Rackspace之间的一个联合项目。Rackspace希望重写其云服务器运行的基础设施代码，同时，Anso
    Labs（为NASA承包）发布了基于Python的云计算织物控制器Nova的beta代码。
- en: By 2012, the OpenStack Foundation was established to promote OpenStack software
    to the cloud community. By 2018, more than 500 companies had joined the OpenStack
    Foundation. By the end of 2020, the foundation announced that would change its
    name starting in 2021 to the **Open Infrastructure Foundation**. The reason is
    that the foundation started to add other projects to OpenStack, and therefore
    the name would not reflect their goals.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 到2012年，OpenStack基金会成立，旨在推广OpenStack软件到云计算社区。到2018年，已有超过500家公司加入了OpenStack基金会。到2020年底，基金会宣布从2021年开始将其名称更改为**开放基础设施基金会**。原因是基金会开始将其他项目添加到OpenStack中，因此名称将不再反映他们的目标。
- en: OpenStack tracks its versions with different names; the first version in 2010
    was called Austin, which included two components (Nova and Swift). By 2015, the
    new version of OpenStack had arrived, which was called Kilo and had 12 components.
    By October 2021, OpenStack Xena had been released, with 38 service components
    ([https://docs.openstack.org/xena/](https://docs.openstack.org/xena/)).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack使用不同的名称跟踪其版本；2010年的第一个版本被称为Austin，包括两个组件（Nova和Swift）。到2015年，新的OpenStack版本到来，被称为Kilo，有12个组件。到2021年10月，OpenStack
    Xena已经发布，有38个服务组件([https://docs.openstack.org/xena/](https://docs.openstack.org/xena/))。
- en: For us, what matters in OpenStack are the components that will allow us to automate
    the network infrastructure. Although not designed for physical devices, the API
    methods for networking might be extended to physical devices instead of being
    only used in cloud virtual environments.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们来说，OpenStack中重要的是那些将使我们能够自动化网络基础设施的组件。尽管不是为物理设备设计的，但网络API方法可能被扩展到物理设备，而不仅仅是用于云虚拟环境。
- en: OpenStack Neutron
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenStack Neutron
- en: The goal of OpenStack is to create standard services that allow software engineers
    to integrate their applications with cloud computing services. The Xena version
    released in October 2021 had 38 services available.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack的目标是创建标准服务，使软件工程师能够将他们的应用程序与云计算服务集成。2021年10月发布的Xena版本提供了38个服务。
- en: One of the most important services for networking is called **Neutron** (or
    **OpenStack Networking**), which is an OpenStack project aimed at providing *network
    connectivity as a service* between interface devices. It implements the OpenStack
    networking API.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 网络服务中最重要的服务之一被称为**Neutron**（或**OpenStack Networking**），这是一个旨在在接口设备之间提供*网络连接作为服务*的OpenStack项目。它实现了OpenStack网络API。
- en: Important note
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'Neutron API definitions can be found at the following link: [https://docs.openstack.org/api-ref/network/](https://docs.openstack.org/api-ref/network/).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Neutron API 定义可以在以下链接中找到：[https://docs.openstack.org/api-ref/network/](https://docs.openstack.org/api-ref/network/)。
- en: 'Neutron manages all networking configurations for the **virtual networking
    infrastructure** (**VNI**) and the access layer aspects of the **physical networking
    infrastructure** (**PNI**). It also enables projects to create advanced virtual
    network topologies, which may include services such as a firewall and a VPN. It
    provides networks, subnets, and routers as object abstractions. Each abstraction
    has functionality that mimics its physical counterpart: networks contain subnets,
    and routers route traffic between different subnets and networks.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元管理所有虚拟网络基础设施（**VNI**）和物理网络基础设施（**PNI**）的访问层配置。它还使项目能够创建高级虚拟网络拓扑，这可能包括防火墙和
    VPN 等服务。它提供网络、子网和路由器作为对象抽象。每个抽象都具有模拟其物理对应物的功能：网络包含子网，路由器在不同的子网和网络之间路由流量。
- en: 'For more details on Neutron, visit the following link: [https://docs.openstack.org/neutron](https://docs.openstack.org/neutron).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于 Neutron 的详细信息，请访问以下链接：[https://docs.openstack.org/neutron](https://docs.openstack.org/neutron)。
- en: The Neutron API
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经元 API
- en: The **Neutron API** is a RESTful HTTP service that uses all aspects of the HTTP
    protocol, including methods, URIs, media types, response codes, and more. API
    clients can use existing features of the protocol, including caching, persistent
    connections, and content compression.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**Neutron API** 是一个使用 HTTP 协议所有方面的 RESTful HTTP 服务，包括方法、URI、媒体类型、响应代码等。API
    客户端可以使用协议的现有功能，包括缓存、持久连接和内容压缩。'
- en: As an example, let’s look at the `HTTP GET` BGP peers’ method.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看看 `HTTP GET` BGP 对等体的方法。
- en: 'To obtain a list of BGP peers use a `HTTP` `GET` request to `/v2.0/bgp-peers`.
    The possible responses are as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取 BGP 对等体的列表，请使用 `HTTP` `GET` 请求到 `/v2.0/bgp-peers`。可能的响应如下：
- en: 'Normal response codes: `200`'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正常响应代码：`200`
- en: 'Error response codes: `400, 401, 403`'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误响应代码：`400, 401, 403`
- en: 'Fields that can be added to the API request:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 可添加到 API 请求的字段：
- en: '| **Name** | **In** | **Type** | **Description** |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| **名称** | **输入** | **类型** | **描述** |'
- en: '| `fields` (optional) | Query | String | The fields that you want the server
    to return. If a `fields` query parameter is not specified, the networking API
    returns all attributes allowed by the policy settings. By using the `fields` parameter,
    the API returns only the requested set of attributes. A `fields` parameter can
    be specified multiple times. For example, if you specify `fields=id&fields=name`
    in the request URL, only the `id` and `name` attributes will be returned. |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| `fields` (可选) | 查询 | 字符串 | 您希望服务器返回的字段。如果没有指定 `fields` 查询参数，网络 API 将返回策略设置允许的所有属性。通过使用
    `fields` 参数，API 仅返回请求的属性集。`fields` 参数可以指定多次。例如，如果您在请求 URL 中指定 `fields=id&fields=name`，则仅返回
    `id` 和 `name` 属性。|'
- en: Table 2.1 – API request fields
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.1 – API 请求字段
- en: 'The parameters that are returned in the API response are as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: API 响应中返回的参数如下：
- en: '| **Name** | **In** | **Type** | **Description** |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| **名称** | **输入** | **类型** | **描述** |'
- en: '| `bgp_peers` | Body | Array | A list of `bgp_peer` objects. Each `bgp_peer`
    object represents real BGP infrastructure, such as routers, route reflectors,
    and route servers. |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| `bgp_peers` | 主体 | 数组 | `bgp_peer` 对象的列表。每个 `bgp_peer` 对象代表实际的 BGP 基础设施，例如路由器、路由反射器和路由服务器。|'
- en: '| `remote_as` | Body | String | The remote autonomous system number of the
    BGP peer. |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| `remote_as` | 主体 | 字符串 | BGP 对等体的远程自治系统编号。|'
- en: '| `name` | Body | String | A more descriptive name of the BGP peer. |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| `name` | 主体 | 字符串 | BGP 对等体的更详细名称。|'
- en: '| `peer_ip` | Body | String | The IP address of the peer. |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| `peer_ip` | 主体 | 字符串 | 对等体的 IP 地址。|'
- en: '| `id` | Body | String | The ID of the BGP peer. |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| `id` | 主体 | 字符串 | BGP 对等体的 ID。|'
- en: '| `tenant_id` | Body | String | The ID of the tenant. |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| `tenant_id` | 主体 | 字符串 | 租户的 ID。|'
- en: '| `project_id` | Body | String | The ID of the project. |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| `project_id` | 主体 | 字符串 | 项目的 ID。|'
- en: Table 2.2 – API response fields
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.2 – API 响应字段
- en: 'The following is an example of the API response:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个 API 响应的示例：
- en: '[PRE0]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The API is well documented at the following link: [https://docs.openstack.org/api-ref/network/](https://docs.openstack.org/api-ref/network/).'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: API 在以下链接中有良好的文档：[https://docs.openstack.org/api-ref/network/](https://docs.openstack.org/api-ref/network/)。
- en: As we have seen in this section, OpenStack is perhaps the cloud computing platform
    that is closest to network programming, as demonstrated by the CloudStack Neutron
    API. Additional features are probably going to be added as more network elements
    are migrated to the cloud. You should now be familiar with OpenStack terms and
    be able to explore them in depth if necessary.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本节中看到的，OpenStack可能是与网络编程最接近的云计算平台，这由CloudStack Neutron API所证明。随着更多网络元素迁移到云端，可能还会添加更多功能。你现在应该熟悉OpenStack术语，并在必要时能够深入探索它们。
- en: Summary
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we talked about how programmable networks have evolved up until
    the present day. We discussed the history of data plane and control plane separation.
    We’ve seen how network virtualization has improved over time. We also looked at
    some technologies and standards for SDNs and cloud networking, such as OpenFlow
    and OpenStack.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了可编程网络是如何发展到今天的。我们讨论了数据平面和控制平面分离的历史。我们看到了网络虚拟化是如何随着时间的推移而改进的。我们还审视了一些SDN和云网络的技术和标准，例如OpenFlow和OpenStack。
- en: You now have the knowledge required to understand why some technologies are
    used today to automate and code networks.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在拥有了理解为什么今天使用某些技术来自动化和编写网络代码所需的知识。
- en: In the next chapter, we’re going to dive deeper into the methods, protocols,
    and standards used to configure and communicate with network devices.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将更深入地探讨用于配置和与网络设备通信的方法、协议和标准。
