<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-154"><a id="_idTextAnchor157"/>5</h1>
<h1 id="_idParaDest-155"><a id="_idTextAnchor158"/>Testing and Debugging FastAPI Applications</h1>
<p>In this chapter of our journey through mastering FastAPI, we pivot towards a crucial aspect of software development that ensures the reliability, robustness, and quality of your applications: testing and debugging. As we delve into this chapter, you’ll be equipped with the knowledge and tools necessary to create an effective testing environment, write and execute comprehensive tests, and debug your FastAPI applications with efficiency and precision.</p>
<p>Understanding how to properly test and debug is not just about finding errors; it’s about ensuring your application can withstand real-world use, manage high traffic without faltering, and provide a seamless user experience. By mastering these skills, you’ll be able to confidently enhance your applications, knowing that each line of code has been scrutinized and each potential bottleneck has been addressed.</p>
<p>We are going to create a proto application with a minimal setup to test the recipes.</p>
<p>By the end of this chapter, you will not only have a deep understanding of the testing frameworks and debugging strategies suitable for FastAPI but also practical experience in implementing these techniques to build more resilient applications. This knowledge is invaluable, as it directly impacts the quality of your software, its maintenance, and its scalability.</p>
<p>In this chapter we’re going to cover the following recipes:</p>
<ul>
<li>Setting up testing environments</li>
<li>Writing and running unit tests</li>
<li>Testing API endpoints</li>
<li>Handling logging messages</li>
<li>Debugging techniques</li>
<li>Performance testing for high traffic application</li>
</ul>
<h1 id="_idParaDest-156"><a id="_idTextAnchor159"/>Technical requirements</h1>
<p>To dive into the chapter and follow along with the recipes, ensure your setup includes the following essentials:</p>
<ul>
<li><strong class="bold">Python</strong>: Make sure to have a Python version 3.7 or higher installed on your computer.</li>
<li><code>fastapi</code> package in your working environment.</li>
<li><code>pytest</code> framework, which is a testing framework largely used to test Python code.</li>
</ul>
<p>The code used in the chapter is hosted on GitHub at the address: <a href="https://github.com/PacktPublishing/FastAPI-Cookbook/tree/main/Chapter05">https://github.com/PacktPublishing/FastAPI-Cookbook/tree/main/Chapter05</a>.</p>
<p>You can setup a virtual environment for the project within the project root folder is also recommended to manage dependencies efficiently and maintain project isolation. Within your virtual environment, you can install all the dependencies at once by using the <code>requirements.txt</code> provided on the GitHub repository in the project folder:</p>
<pre class="console">
$ pip install –r requirements.txt</pre> <p>A basic knowledge of HTTP protocol, although not required, can be beneficial.</p>
<h1 id="_idParaDest-157"><a id="_idTextAnchor160"/>Setting up testing environments</h1>
<p>This recipe will show you<a id="_idIndexMarker285"/> how to setup an efficient and effective testing environment tailored for FastAPI applications. By the end of the recipe, you will have a solid foundation for writing, running, and managing tests.</p>
<h2 id="_idParaDest-158"><a id="_idTextAnchor161"/>Getting ready</h2>
<p>Make sure you have an application running. If not you can start by creating a project folder <code>proto_app</code>.</p>
<p>If you haven’t installed the packages with the requirements.txt file provided on the GitHub repository, then install the testing libraries <code>pytest</code> and <code>httpx</code> in your environment with:</p>
<pre class="console">
$ pip install pytest pytest-asyncio httpx</pre> <p>In the project root folder create a new folder <code>proto_app</code> with a <code>main.py</code> module containing the<code> app</code> object instance:</p>
<pre class="source-code">
from fastapi import FastAPI
app = FastAPI()
@app.get("/home")
async def read_main():
    return {"message": "Hello World"}</pre> <p>With a minimal<a id="_idIndexMarker286"/> app setup, we can proceed by scaffolding our project to accommodate the tests.</p>
<h2 id="_idParaDest-159"><a id="_idTextAnchor162"/>How to do it...</h2>
<p>First, let’s start by structuring our project folder tree to accommodate tests.</p>
<ol>
<li>In the root directory let’s create a <code>pytest.ini</code> file and a <code>tests</code> folder containing the test module <code>test_main.py</code>. The project structure should look like this:<pre class="source-code">
<strong class="bold">protoapp/</strong>
<strong class="bold">|─ protoapp/</strong>
<strong class="bold">│  |─ main.py</strong>
<strong class="bold">|─ tests/</strong>
<strong class="bold">│</strong><strong class="bold">  |─ test_main.py</strong>
<code>pytest.ini</code> contains instructions for <code>pytest</code>. You can write in it:<pre class="source-code">
[pytest]
pythonpath = . protoapp</pre><p class="list-inset">This will add the project root and the folder <code>protoapp</code>, containing the code, to the <code>PYTHONPATH</code> when running <code>pytest</code>.</p></li> <li>Now, in the <code>test_main.py</code> module, let’s write a test for the <code>/home</code> endpoint we created <a id="_idIndexMarker287"/>earlier:<pre class="source-code">
import pytest
from httpx import ASGITransport, AsyncClient
from protoapp.main import app
@pytest.mark.asyncio
async def test_read_main():
    client = AsyncClient(
        transport=ASGITransport(app=app),
        base_url="http://test",
    )
    response = await client.get("/home")
    assert response.status_code == 200
    assert response.json() == {
        "message": "Hello World"
    }
<strong class="bold">$ pytest –-collect-only</strong></pre><p class="list-inset">You should get an output like:</pre><pre class="source-code"><strong class="bold">configfile: pytest.ini</strong>
<strong class="bold">plugins: anyio-4.2.0, asyncio-0.23.5, cov-4.1.0</strong>
<strong class="bold">asyncio: mode=Mode.STRICT</strong>
<strong class="bold">collected 1 item</strong>
<strong class="bold">&lt;Dir protoapp&gt;</strong>
<strong class="bold">  &lt;Dir tests&gt;</strong>
<strong class="bold">    </strong><strong class="bold">&lt;Module test_main.py&gt;</strong>
<code>pytest.ini</code></li><li>The <code>pytest</code> plugins used</li><li>The directory tests, the module <code>test_main.py and</code> the test <code>test_read_main</code> which is a coroutine</li></ul></li> <li>Now, from the command line terminal at the project root folder level, run the <code>pytest</code> command:<pre class="source-code">
<strong class="bold">$ pytest</strong></pre></li> </ol>
<p>You’ve just setup the environment to test our proto application.</p>
<h2 id="_idParaDest-160"><a id="_idTextAnchor163"/>See also</h2>
<p>The recipe has shown<a id="_idIndexMarker289"/> how to configure <code>pytest</code> within a <strong class="bold">FastAPI</strong> project with some of the good practices. Feel free to dig deeper into the <strong class="bold">Pytest</strong> official documentation at the links:</p>
<ul>
<li><em class="italic">Pytest </em><em class="italic">configuration</em>: <a href="https://docs.pytest.org/en/stable/reference/customize.xhtml">https://docs.pytest.org/en/stable/reference/customize.xhtml</a></li>
<li><em class="italic">Setup PYTHONPATH in </em><em class="italic">Pytest</em>: <a href="https://docs.pytest.org/en/7.1.x/explanation/pythonpath.xhtml">https://docs.pytest.org/en/7.1.x/explanation/pythonpath.xhtml</a></li>
<li><em class="italic">Pytest good </em><em class="italic">practices</em>: <a href="https://docs.pytest.org/en/7.1.x/explanation/goodpractices.xhtml">https://docs.pytest.org/en/7.1.x/explanation/goodpractices.xhtml</a></li>
</ul>
<h1 id="_idParaDest-161"><a id="_idTextAnchor164"/>Writing and running unit tests</h1>
<p>Once we setup <a id="_idIndexMarker290"/>our testing environment, we can focus on the process of writing and executing tests for FastAPI applications. Unit tests are essential for validating the behaviour of individual parts of your application in isolation, ensuring they perform as expected. In this recipe, you will learn to test the endpoints of your application.</p>
<h2 id="_idParaDest-162"><a id="_idTextAnchor165"/>Getting ready</h2>
<p>We will use <code>pytest</code> to test the FastAPI client in unit tests. Since the recipe will utilize common testing <em class="italic">fixtures</em>, used in most <strong class="bold">Python</strong> standard code, make sure to be familiar with the test fixtures before diving into the recipe. If this is not the case, you can always refer to the dedicated documentation page at the link: <a href="https://docs.pytest.org/en/7.1.x/reference/fixtures.xhtml">https://docs.pytest.org/en/7.1.x/reference/fixtures.xhtml</a>.</p>
<h2 id="_idParaDest-163"><a id="_idTextAnchor166"/>How to do it…</h2>
<p>We will start by creating a unit test for the same <code>GET /home</code> endpoint, but differently from the previous recipe. We will use the <code>TestClient</code> class provided by FastAPI.</p>
<p>Let’s create a fixture for that. Since it could be used by multiple tests let’s do it in a new <code>conftest.py</code> module under the <code>tests</code> folder. The <code>conftest.py</code> is a default file used by <code>pytest</code> to store common elements shared amongst test modules.</p>
<p>In the <code>conftest.py</code> let’s write:</p>
<pre class="source-code">
import pytest
from fastapi.testclient import TestClient
from protoapp.main import app
@pytest.fixture(scope="function")
def test_client(db_session_test):
    client = TestClient(app)
    yield client</pre> <p>We are now ready<a id="_idIndexMarker291"/> to leverage the <code>test_client</code> fixture to create a proper unit test for our endpoint.</p>
<p>We will write our test in the <code>test_main.py</code> module:</p>
<pre class="source-code">
def test_read_main_client(test_client):
    response = test_client.get("/home")
    assert response.status_code == 200
    assert response.json() == {"message": "Hello World"}</pre> <p>And that’s it. Compared to the previous test, this one is more compact and faster to write, thanks to the <code>TestClient</code> class provided by FastAPI package.</p>
<p>Now run <code>pytest</code>:</p>
<pre class="console">
$ pytest</pre> <p>You will see a message on the terminal showing that two tests have been collected and run successfully.</p>
<h2 id="_idParaDest-164"><a id="_idTextAnchor167"/>See also</h2>
<p>You can check more<a id="_idIndexMarker292"/> on the test client for FastAPI in the official <a id="_idIndexMarker293"/>documentation:</p>
<ul>
<li><em class="italic">FastAPI Test </em><em class="italic">Client</em>: <a href="https://fastapi.tiangolo.com/reference/testclient/">https://fastapi.tiangolo.com/reference/testclient/</a></li>
</ul>
<h1 id="_idParaDest-165"><a id="_idTextAnchor168"/>Testing API Endpoints</h1>
<p>Integration tests<a id="_idIndexMarker294"/> verify that different parts of your application work together as expected. They are crucial for ensuring that your system’s components interact correctly, especially when dealing with external services, databases, or other APIs.</p>
<p>In this recipe, we will test two endpoints that interact with an SQL database. One will add an item to the database, the other will read an item based on the ID.</p>
<h2 id="_idParaDest-166"><a id="_idTextAnchor169"/>Getting ready</h2>
<p>To apply the recipe you need your testing environment already setup for <code>pytest</code>. If this is not the case check the recipe <em class="italic">Setting up </em><em class="italic">testing environments</em> of the same chapter.</p>
<p>Also, the recipe will show you how to make integration tests with existing endpoints of the application. You can use it for your application or you can build the endpoints for our <code>protoapp</code> as follows.</p>
<p>If you are using the recipe to test your endpoint you can directly jump on the <em class="italic">How to it…</em> section and apply the rules to tour endpoints.</p>
<p>Otherwise, If you haven’t installed the packages from the <code>requirements.txt</code>, install <code>sqlalchemy</code> package in your environment:</p>
<pre class="console">
$ pip install "sqlalchemy&gt;=2.0.0"</pre> <p>Now let’s setup the database connection through the following steps.</p>
<ol>
<li>Under the <code>protoapp</code> folder, at the same level as the <code>main.py</code> module, let’s create a module <code>database.py</code> containing the setup of the database. Let’s start by creating the <code>Base</code> class:<pre class="source-code">
from sqlalchemy.orm import DeclarativeBase,
class Base(DeclarativeBase):
    pass</pre><p class="list-inset">We will use the <code>Base</code> class to define the <code>Item</code> mapping class.</p></li> <li>Then the database <code>Item</code> mapping class will be like:<pre class="source-code">
from sqlalchemy.orm import (
    Mapped,
    mapped_column,
)
class Item(Base):
    __tablename__ = "items"
    id: Mapped[int] = mapped_column(
        primary_key=True, index=True
    )
    name: Mapped[str] = mapped_column(index=True)
    color: Mapped[str]</pre></li> <li>Then, we<a id="_idIndexMarker295"/> define the database engine that will handle the session:<pre class="source-code">
DATABASE_URL = "sqlite:///./production.db"
engine = create_engine(DATABASE_URL)</pre><p class="list-inset">The engine object will be used to handle the session.</p></li> <li>Then, let’s bind the engine to the Base mapping class:<pre class="source-code">
Base.metadata.create_all(bind=engine)</pre><p class="list-inset">Now the engine can map the database table to our Python classes.</p></li> <li>Last in the <code>database.py</code> module let’s create a <code>SessionLocal</code> class that will generate the session as:<pre class="source-code">
SessionLocal = sessionmaker(
    autocommit=False, autoflush=False, bind=engine
)</pre><p class="list-inset">The <code>SessionLocal</code> is a class that will initialize the database session object.</p></li> <li>Finally, before creating the endpoints, let’s create a database session.<p class="list-inset">Since the app<a id="_idIndexMarker296"/> is relatively small, we can do it the same <code>main.py</code>:</p><pre class="source-code">
from protoapp.database import SessionLocal
def get_db_session():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()</pre><p class="list-inset">We will use the session to interact with the database.</p></li> </ol>
<p>Now that we have setup the database connection, in the <code>main.py</code> module, we can create the endpoints one to add an item to the database and one to read it. Let's do it as follows.</p>
<ol>
<li>Let's start by creating the request body for the endpoints as::<pre class="source-code">
from pydantic import BaseModel
class ItemSchema(BaseModel):
    name: str
    color: str</pre></li> <li>The endpoint used to add an item will then be:<pre class="source-code">
from fastapi import (
    Depends,
    Request,
    HTTPException,
    status
)
from sqlalchemy.orm import Session
@app.post(
"/item",
response_model=int,
status_code=status.HTTP_201_CREATED
)
def add_item(
    item: ItemSchema,
    db_session: Session = Depends(get_db_session),
):
    db_item = Item(name=item.name, color=item.color)
    db_session.add(db_item)
    db_session.commit()
    db_session.refresh(db_item)
    return db_item.id</pre><p class="list-inset">The endpoint will return the item ID affected when the item is stored in the database. </p></li> <li>Now that we have the endpoint to add the item, we can proceed by creating the endpoint<a id="_idIndexMarker297"/> to retrieve the item based on its ID:<pre class="source-code">
@app.get("/item/{item_id}", response_model=ItemSchema)
def get_item(
    item_id: int,
    db_session: Session = Depends(get_db_session),
):
    item_db = (
        db_session.query(Item)
        .filter(Item.id == item_id)
        .first()
    )
    if item_db is None:
        raise HTTPException(
            status_code=404, detail="Item not found"
        )
    return item_db</pre><p class="list-inset">If the ID does not correspond to any item in the database the endpoint will return a 404 status code.</p></li> </ol>
<p>We have just created the endpoints that will allow us to create an integration test.</p>
<h2 id="_idParaDest-167"><a id="_idTextAnchor170"/>How to do it…</h2>
<p>Once we have the endpoints, in the <code>tests</code> folder we should adapt our <code>test_client</code> fixture to use a different session than the one used in production.</p>
<p>We will break the process into two main actions:</p>
<ul>
<li>Adapt the <a id="_idIndexMarker298"/>test client to accommodate the testing database session</li>
<li>Create the test to simulate the interaction of the endpoints</li>
</ul>
<p>Let's do it by following these steps.</p>
<ol>
<li>First, In the <code>conftest.py</code> file we created earlier in the recipe <em class="italic">Writing and running unit tests</em>, let’s define a new engine that will use an in-memory SQLite database and bind it to the mapping <code>Base</code> class:<pre class="source-code">
from sqlalchemy.pool import StaticPool
from sqlalchemy import create_engine
engine = create_engine(
    "sqlite:///:memory:",
    connect_args={"check_same_thread": False},
    poolclass=StaticPool,
)
Base.metadata.create_all(bind=engine)  # Bind the engine</pre></li> <li>Let’s create a dedicated session maker for the testing session as:<pre class="source-code">
from sqlalchemy.orm import sessionmaker
TestingSessionLocal = sessionmaker(
    autocommit=False, autoflush=False, bind=engine
)</pre></li> <li>Similarly to the function <code>get_db_session</code> in the <code>main.py</code> module, we can create a fixture to retrieve the test session in the conftest.py module:<pre class="source-code">
@pytest.fixture
def test_db_session():
    db = TestingSessionLocal()
    try:
        yield db
    finally:
        db.close()</pre></li> <li>Then, we should <a id="_idIndexMarker299"/>modify the <code>test_client</code> to use this session instead of the production one. We can do it by overwriting the dependency that returns the session with the one we just created. FastAPI allows you to do it easily by calling the test client’s method <code>dependency_overrides</code> as:<pre class="source-code">
<strong class="bold">from protoapp.main import app, get_db_session</strong>
@pytest.fixture(scope="function")
def test_client(test_db_session):
    client = TestClient(app)
<strong class="bold">    app.dependency_overrides[get_db_session] = (</strong>
<strong class="bold">        lambda: test_db_session</strong>
<strong class="bold">    </strong><strong class="bold">)</strong>
    return client</pre><p class="list-inset">Each time the test client needs to call the session, the fixture will replace it with the test session that uses the in-memory database.</p></li> <li>Then, to verify the interaction of our application with the database, we create a test that:<ul><li>Create the item into the database through the <code>POST /</code><code>item</code> endpoint</li><li>Verify that the item is correctly created into the test database by using the test session</li><li>Retrieve the item through the <code>GET /</code><code>item</code> endpoint</li></ul><p class="list-inset">You can put<a id="_idIndexMarker300"/> the test into the <code>test_main.py</code> and here is how it would look like:</p><pre class="source-code">
def test_client_can_add_read_the_item_from_database(
    test_client, test_db_session
):
    response = test_client.get("/item/1")
    assert response.status_code == 404
    response = test_client.post(
        "/item", json={"name": "ball", "color": "red"}
    )
    assert response.status_code == 201
    # Verify the user was added to the database
    item_id = response.json()
    item = (
        test_db_session.query(Item)
        .filter(Item.id == item_id)
        .first()
    )
    assert item is not None
    response = test_client.get(f"item/{item_id}")
    assert response.status_code == 200
    assert response.json() == {
        "name": "ball",
        "color": "red",
    }</pre></li> </ol>
<p>You’ve just created an integration test for our proto application, feel free to enrich your application and <a id="_idIndexMarker301"/>create more tests accordingly.</p>
<h2 id="_idParaDest-168"><a id="_idTextAnchor171"/>See also</h2>
<p>We have setup an in-memory SQLite database for our tests. Since each session is bonded to a thread, the engine needs to be configured accordingly to not flush data.</p>
<p>The configuration strategy <a id="_idIndexMarker302"/>has been found on the following documentation page:</p>
<ul>
<li><em class="italic">SQLite In-Memory Database </em><em class="italic">Configuration</em>: <a href="https://docs.sqlalchemy.org/en/14/dialects/sqlite.xhtml#using-a-memory-database-in-multiple-threads">https://docs.sqlalchemy.org/en/14/dialects/sqlite.xhtml#using-a-memory-database-in-multiple-threads</a></li>
</ul>
<h1 id="_idParaDest-169"><a id="_idTextAnchor172"/>Running tests techniques</h1>
<p>By systematically <a id="_idIndexMarker303"/>covering all endpoints and scenarios, you ensure that your API behaves correctly under various conditions, providing confidence in your application’s functionality. Thoroughly testing API endpoints is essential for building reliable and robust applications.</p>
<p>The recipe will explain to you how to run tests individually or by group and how to check the test coverage of our code.</p>
<h2 id="_idParaDest-170"><a id="_idTextAnchor173"/>Getting ready</h2>
<p>To run the recipe, make sure you already have some tests in place, or you already followed all the previous recipes of the chapter. Also, make sure you have your PYTHONPATH for tests defined in your <code>pytest.ini</code>. Have a look at the recipe <em class="italic">Setting up </em><em class="italic">testing environments</em> on how to do it.</p>
<h2 id="_idParaDest-171"><a id="_idTextAnchor174"/>How to do it...</h2>
<p>We will start by looking at how to run tests by default grouping (individually or by module), and then we will cover a technique for customizing test grouping based on marks.</p>
<p>As you already know, all unit tests can be run from the terminal with the command:</p>
<pre class="console">
$ pytest</pre> <p>However, a test can be run individually according to the test call syntax:</p>
<pre class="console">
$ pytest &lt;test_module&gt;.py::&lt;test_name&gt;</pre> <p>For example, if we want to run the test function <code>test_read_main_client</code>, run:</p>
<pre class="console">
$ pytest tests/test_main.py::test_read_main</pre> <p>Sometimes test names become too complicated to remember or we have a specific need to run only a targeted set of tests. Here is where test marks come to the aid.</p>
<p>Let’s imagine we want to run only integration tests. In our app, the only integration test is represented by the function <code>tests_client_can_add_read_the_item_from_database</code>.</p>
<p>We can apply a mark by adding the specific decorator to the function:</p>
<pre class="source-code">
<strong class="bold">@pytest.mark.integration</strong>
def test_client_can_add_read_the_item_from_database(
    test_client, test_db_session
):
    # test content</pre> <p>Then, in the <code>pytest.ini</code> configuration add the <code>integration</code> marker in the dedicated sections to register the mark:</p>
<pre class="source-code">
[pytest]
pythonpath = protoapp .
<strong class="bold">markers =</strong>
<strong class="bold">    integration: marks tests as integration</strong></pre> <p>Now you can run the targeted tests by running:</p>
<pre class="console">
$ pytest –m integration -vv</pre> <p>In the output message, you will see that only the marked test has been selected and run. You can use markers to group your application’s tests based on logical criteria, for example by functional <a id="_idIndexMarker304"/>meaning one group for <strong class="bold">create, read, update and delete </strong>(<strong class="bold">CRUD</strong>) operations, one <a id="_idIndexMarker305"/>group for security operations, and so on.</p>
<h2 id="_idParaDest-172"><a id="_idTextAnchor175"/>Check test coverage</h2>
<p>To make sure that your <a id="_idIndexMarker306"/>endpoints are covered by testing as well as the text lines of your code, it can become useful to have an idea of the test coverage.</p>
<p>Test coverage is a metric used in software testing to measure the extent to which the source code of a program is executed when a particular test suite runs.</p>
<p>To use it with <code>pytest</code>, if you didn’t install the packages with the <code>requirements.txt</code>, you need to install <code>pytest-cov</code> package:</p>
<pre class="console">
$ pip install pytest-cov</pre> <p>The way it works is quite straightforward. You need to pass the source code root, in our case the <code>protoapp</code> directory, to the parameter <code>–cov</code> of <code>pytest</code> and tests root folder, in our case tests as follows:</p>
<pre class="console">
$ pytest –-cov protoapp tests</pre> <p>You will see a table in the output listing the coverage percentage for each module:</p>
<pre class="console">
Name                   Stmts   Miss  Cover
------------------------------------------
protoapp\database.py      16      0   100%
protoapp\main.py          37      4    89%
protoapp\schemas.py        8      8     0%
------------------------------------------
TOTAL                     61     12    80%</pre> <p>In addition, a file named <code>.coverage</code> has been created. This is a binary file containing data on the test coverage and that can be used with additional tools to generate reports out of it.</p>
<p>For example, if you run:</p>
<pre class="console">
$ coverage html</pre> <p>It will create a <a id="_idIndexMarker307"/>folder <code>htmlcov</code> with an <code>index.xhtml</code> page containing the coverage page and you can visualize it by opening it with a browser.</p>
<h2 id="_idParaDest-173"><a id="_idTextAnchor176"/>See also</h2>
<p>You can check more on various options to invoke unit tests with Pytest and how to evaluate test coverage<a id="_idIndexMarker308"/> at the official documentation links</p>
<ul>
<li><em class="italic">Invoke Unit test with </em><em class="italic">Pytest</em>: <a href="https://docs.pytest.org/en/7.1.x/how-to/usage.xhtml">https://docs.pytest.org/en/7.1.x/how-to/usage.xhtml</a></li>
<li><em class="italic">Pytest </em><em class="italic">Coverage</em>: <a href="https://pytest-cov.readthedocs.io/en/latest/">https://pytest-cov.readthedocs.io/en/latest/</a></li>
</ul>
<h1 id="_idParaDest-174"><a id="_idTextAnchor177"/>Handling logging messages</h1>
<p>Effectively managing logs in application development not only aids in identifying errors promptly but also <a id="_idIndexMarker309"/>provides valuable<a id="_idIndexMarker310"/> insights into user interactions, system performance, and potential security threats. It serves as a crucial tool for auditing, compliance, and optimizing resource utilization, ultimately enhancing the reliability and scalability of the software.</p>
<p>This recipe will show how to efficiently implement a logging system into our FastAPI application to monitor the calls to the API.</p>
<h2 id="_idParaDest-175"><a id="_idTextAnchor178"/>Getting ready</h2>
<p>We are going to use some basic features of the Python logging ecosystem.</p>
<p>Although the example is basic, you can refer to the official documentation to get familiar with related terms such as <strong class="bold">logger</strong>, <strong class="bold">handler</strong> , <strong class="bold">formatter</strong>, and <strong class="bold">log level</strong>. Follow this link:</p>
<p><a href="https://docs.python.org/3/howto/logging-cookbook.xhtml">https://docs.python.org/3/howto/logging-cookbook.xhtml</a>.</p>
<p>To implement logging into FastAPI, make sure you have a running application or use the <code>protoapp</code> we developed all along the chapter.</p>
<h2 id="_idParaDest-176"><a id="_idTextAnchor179"/>How to do it...</h2>
<p>We want to create a logger that prints the client’s calls information to the terminal and logs them into a file.</p>
<p>Let’s create the logger into a dedicated <code>logging.py</code> module under the folder <code>protoapp</code>, through the following steps.</p>
<ol>
<li>Let’s start by defining the logger with a level value to <code>INFO</code>:<pre class="source-code">
import logging
client_logger = logging.getLogger("client.logger")
logger.setLevel(logging.INFO)</pre><p class="list-inset">Since we want to stream the message to the console and store it in a file, we will need to define two separate handlers.</p></li> <li>Now let’s define the handler to print log messages to the console. We will use a <code>StreamHandler</code> object from the <code>logging</code> built-in package:<pre class="source-code">
console_handler = logging.StreamHandler()</pre><p class="list-inset">This will stream the message to the console.</p></li> <li>Let’s create <a id="_idIndexMarker311"/>a <a id="_idIndexMarker312"/>colorized formatter and add it to the handler we just created:<pre class="source-code">
from uvicorn.logging import ColourizedFormatter
console_formatter = ColourizedFormatter(
    "%(levelprefix)s CLIENT CALL - %(message)s",
    use_colors=True,
)
console_handler.setFormatter(console_formatter)</pre><p class="list-inset">The formatter will format log messages in the same of the default logger uvicorn logger used by FastAPI.</p></li> <li>Then let’s add the handler to the logger:<pre class="source-code">
client_logger.addHandler(console_handler)</pre><p class="list-inset">We have just set up the logger to print message to the console.</p></li> <li>Let’s repeat the previous <em class="italic">steps from 1 to 4</em> to create a handler that stores messages into a file and adds it to our <code>client_logger</code>:<pre class="source-code">
from logging.handlers import TimedRotatingFileHandler
file_handler = TimedRotatingFileHandler("app.log")
file_formatter = logging.Formatter(
    "time %(asctime)s, %(levelname)s: %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
file_handler.setFormatter(file_formatter)
client_logger.addHandler(file_handler)</pre><p class="list-inset">Now we have <a id="_idIndexMarker313"/>our <a id="_idIndexMarker314"/>logger setup. Each message will be streamed to the console and stored in a <code>app.log</code> file.</p></li> <li>Once we have built our <code>client_logger</code>, we need to use it in the code to get information about clients calls.<p class="list-inset">You can reach this by adding the logger and a dedicated middleware in the <code>main.py</code> module:</p><pre class="source-code">
from protoapp.logging import client_logger
# ... module content
@app.middleware("http")
async def log_requests(request: Request, call_next):
    client_logger.info(
        f"method: {request.method}, "
        f"call: {request.url.path}, "
        f"ip: {request.client.host}"
    )
    response = await call_next(request)
    return response</pre></li> <li>Now spin up the server:<pre class="source-code">
<strong class="bold">$ uvicorn protoapp.main:app</strong></pre></li> </ol>
<p>Try to call any of the endpoints we defined, you will see on the terminal the logs we just defined for the request and response. Also, you will find only the messages from our <code>logger_client</code> in a<a id="_idIndexMarker315"/> newly<a id="_idIndexMarker316"/> created <code>app.log</code> file automatically created by the application.</p>
<h2 id="_idParaDest-177"><a id="_idTextAnchor180"/>There’s more</h2>
<p>Defining a proper logging strategy would require a separate cookbook and it is out of the scope of the book. However, when building a logger into an application it is important to follow some guidelines:</p>
<ul>
<li><strong class="bold">Use standard Logging Levels Appropriately</strong>. A classical leveling system is made up of 4 levels: <strong class="bold">INFO</strong>, <strong class="bold">WARNING</strong>, <strong class="bold">ERROR</strong>, <strong class="bold">CRITICAL</strong>. You may need to have more or even less than four depending on the application. Anyway, place each message at the appropriate level.</li>
<li><strong class="bold">Consist Log Format</strong>. Maintain a consistent log format across your application. This includes consistent datetime formats, including the severity level, and describing the event clearly. A consistent format helps in parsing logs and automating log analysis.</li>
<li><strong class="bold">Include Contextual Information</strong>. Include relevant contextual information in your logs (e.g., user ID, transaction ID) to help trace and debug issues across your application’s workflow.</li>
<li><strong class="bold">Avoid Sensitive Information</strong>. Never log sensitive information such as passwords, API keys, or <strong class="bold">personal identifiable information</strong> (<strong class="bold">PII</strong>). If necessary, mask or hash these details.</li>
<li><strong class="bold">Make Efficient Logging</strong>. Be mindful of the performance impact of logging. Logging excessively can slow down your application and lead to log noise, making it hard to find useful information. Balance the need for information against the performance impact.</li>
</ul>
<p>And of course, this is not<a id="_idIndexMarker317"/> a<a id="_idIndexMarker318"/> comprehensive list.</p>
<h2 id="_idParaDest-178"><a id="_idTextAnchor181"/>See also</h2>
<p>Python distribution comes with a powerful built-in package for logging, feel to have a look at the <a id="_idIndexMarker319"/>official documentation:</p>
<ul>
<li><em class="italic">Python </em><em class="italic">logging</em>: <a href="https://docs.python.org/3/library/logging.xhtml">https://docs.python.org/3/library/logging.xhtml</a></li>
</ul>
<p>Furthermore, discover <a id="_idIndexMarker320"/>more on logging best practices and guidelines at the <strong class="bold">Sentry</strong> blog:</p>
<ul>
<li><em class="italic">Logging </em><em class="italic">Guidelines</em>: <a href="https://blog.sentry.io/logging-in-python-a-developers-guide/">https://blog.sentry.io/logging-in-python-a-developers-guide/</a></li>
</ul>
<p><strong class="bold">Sentry</strong> is a tool to monitor Python code.</p>
<h1 id="_idParaDest-179"><a id="_idTextAnchor182"/>Debugging techniques</h1>
<p>Mastering debugging <a id="_idIndexMarker321"/>application development is crucial for identifying and fixing issues efficiently. This recipe delves into the practical use of the debugger, leveraging tools and strategies to pinpoint problems in your FastAPI code.</p>
<h2 id="_idParaDest-180"><a id="_idTextAnchor183"/>Getting ready</h2>
<p>All you need to do to apply the recipe is to have a running application. We can keep on working with our <code>protoapp</code>.</p>
<h2 id="_idParaDest-181"><a id="_idTextAnchor184"/>How to do it...</h2>
<p>The Python distribution already comes with a default debugger called <code>pdb</code>. If you use an <strong class="bold">integrated development environment</strong> (<strong class="bold">IDE</strong>), it usually comes with an editor distribution <a id="_idIndexMarker322"/>debugger. Whatever you are using to debug your code, you must be familiar with the concept of breakpoints.</p>
<p>A <strong class="bold">breakpoint</strong> is a point within the code that pauses the execution and shows you the state of the code variables and calls. It can be attached with a condition that, if satisfied, activate it or skips otherwise.</p>
<p>Whether you are using the Python distribution debugger <code>pdb</code> or the one provided by your IDE, it can be useful to define a starting script to spin up the server.</p>
<p>Create on the <a id="_idIndexMarker323"/>project root folder a file called <code>run_server.py</code> containing the following code:</p>
<pre class="source-code">
import uvicorn
from protoapp.main import app
if __name__ == "__main__":
    uvicorn.run(app)</pre> <p>The script imports the <code>uvicorn</code> package and our application <code>app</code> and runs the application into the <code>uvicorn</code> server. It is equivalent to the launching command:</p>
<pre class="console">
$ uvicorn protoapp.main:app</pre> <p>Having a script gives us more flexibility to run the server and include it into a broader python routine if required.</p>
<p>To check that it is correctly setup run the script as you would run a normal python script:</p>
<pre class="console">
$ python run_server.py</pre> <p>With your favourite<a id="_idIndexMarker324"/> browser go to <code>localhost:8000/docs</code> and check that the documentation has been correctly generated.</p>
<h2 id="_idParaDest-182"><a id="_idTextAnchor185"/>Debugging with PDB</h2>
<p>The PDB debugger<a id="_idIndexMarker325"/> comes by default with any Python<a id="_idIndexMarker326"/> distribution. From Python versions higher than 3.7, you can define a breakpoint by simply adding the function call <code>breakpoint()</code> at the line of the code you want to pause, and then run the code as you would it normally.</p>
<p>If you then run the code, when it reaches the breakpoint line, the execution will automatically shift to debug mode, and you can run debugging commands from the terminal. You can find the list of the commands you can run by typing help:</p>
<pre class="console">
(Pdb) help</pre> <p>You can run commands to list variables, show the stack trace to check to recent frame, or define new breakpoints with conditions and more.</p>
<p>Here you can find the list of all the command available: <a href="https://docs.python.org/3/library/pdb.xhtml#debugger-commands">https://docs.python.org/3/library/pdb.xhtml#debugger-commands</a>.</p>
<p>You can also invoke <code>pdb</code> as a module. In this case <code>pdb</code> will automatically enter <strong class="bold">post-mortem</strong> debugging if the program exists abnormally:</p>
<pre class="console">
$ python –m pdb run_server.py</pre> <p>That means that <code>pdb</code> will restart the program automatically by preserving <code>pdb</code> module's execution state including breakpoints.</p>
<p>The same can be done when debugging tests by calling <code>pytest</code> as a module, for example:</p>
<pre class="console">
$ python –m pdb -m pytest tests</pre> <p>Another debugging strategy consists of leveraging the reload functionality of the <code>uvicorn</code> server. To do that, you need to modify the <code>run_server.py</code> file as:</p>
<pre class="source-code">
import uvicorn
if __name__ == "__main__":
    uvicorn.run("protoapp.main:app", reload=True)</pre> <p>Then, run the server without the <code>pdb</code> module:</p>
<pre class="console">
$ python run_server.py</pre> <p>In this way, you can always use the breakpoints at ease with the reloading server functionality.</p>
<p>At the time of <a id="_idIndexMarker327"/>writing, <code>unvicorn</code>.</p>
<h2 id="_idParaDest-183"><a id="_idTextAnchor186"/>Debugging with VS Code</h2>
<p>VS <a id="_idIndexMarker329"/>Code Python <a id="_idIndexMarker330"/>extension comes with its distribution debugger called <em class="italic">debugpy</em>. Configurations for the running environment can be managed in the <code>.vscode/launch.json</code> file. An example of the configuration file to debug our server is:</p>
<pre class="source-code">
{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python Debugger FastAPI server",
            "type": "debugpy",
            "request": "launch",
            "program": "run_server.py",
            "console": "integratedTerminal",
        },
}</pre> <p>The configuration specifies the type of debugger to use (<code>debugpy</code>), the program to run (our launching script <code>run_server.py</code>), and it can be found in the GUI options.</p>
<p>The <code>request</code> field specifies the mode to run the debugger, it can be <em class="italic">launch</em>, intended to run the program, or <em class="italic">attach</em>, intended to be attached to an already running instance, particularly useful to debug programs running on remote instances.</p>
<p>Debugging remote instance is out of the scope of the recipe, but you can find detailed instructions at on the official documentation: <a href="https://code.visualstudio.com/docs/python/debugging#_debugging-by-attaching-over-a-network-connection">https://code.visualstudio.com/docs/python/debugging#_debugging-by-attaching-over-a-network-connection</a></p>
<p>Debugging configuration can be setup to run unit tests as well by leveraging the <em class="italic">Test Explorer</em> extension. The <a id="_idIndexMarker331"/>extension will look for <a id="_idIndexMarker332"/>a configuration in the <code>launch.json</code> containing <code>"type": "python"</code> and <code>"purpose": ["debug-test"]</code> (or <code>"request": "test"</code>). An example of configuration to debug tests would be:</p>
<pre class="source-code">
{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Debug test",
            "type": "python",
            "request": "launch",
            "console": "integratedTerminal",
            "justMyCode": false,
            "stopOnEntry": true,
            "envFile": "${workspaceFolder}/.env.test",
            "purpose": ["debug-test"]
        }
    ]
}</pre> <p>You can find an <a id="_idIndexMarker333"/>extensive explication on<a id="_idIndexMarker334"/> the extension page from the VS Code marketplace at: <a href="https://marketplace.visualstudio.com/items?itemName=LittleFoxTeam.vscode-python-test-adapter">https://marketplace.visualstudio.com/items?itemName=LittleFoxTeam.vscode-python-test-adapter</a>.</p>
<h2 id="_idParaDest-184"><a id="_idTextAnchor187"/>Debugging with PyCharm</h2>
<p>PyCharm manages<a id="_idIndexMarker335"/> code execution through run/debug <a id="_idIndexMarker336"/>configurations, which are sets of named startup properties detailing execution parameters and environments. These configurations allow running scripts with different settings, such as using various Python interpreters, environment variables, and input sources.</p>
<p>Run/debug configurations are of two kinds:</p>
<ul>
<li>Temporary: Automatically generated for each run or debug session.</li>
<li>Permanent: Manually created from a template or by converting a temporary one, and saved within your project indefinitely until deleted.</li>
</ul>
<p>PyCharm by default uses an existing permanent configuration or creates a temporary one for each session. Temporary configurations are capped at five, with the oldest deleted for new ones. This limit can be adjusted in the settings (<strong class="bold">Settings</strong> | <strong class="bold">Advanced Settings</strong> | <strong class="bold">Run/Debug</strong> | <strong class="bold">Temporary configurations limit</strong>). Icons distinguish between permanent (opaque) and temporary (semi-transparent) configurations.</p>
<p>Each configuration can be stored in a single xml file that is automatically detected by the GUI.</p>
<p>An example of configuration for our FastAPI <code>protoapp</code> is the following:</p>
<pre class="source-code">
&lt;component name="ProjectRunConfigurationManager"&gt;
  &lt;configuration default="false" name="run_server"
    type="PythonConfigurationType" factoryName="Python"
    nameIsGenerated="true"&gt;
    &lt;module name="protoapp" /&gt;
    &lt;option name="INTERPRETER_OPTIONS" value="" /&gt;
    &lt;option name="PARENT_ENVS" value="true" /&gt;
    &lt;envs&gt;
      &lt;env name="PYTHONUNBUFFERED" value="1" /&gt;
    &lt;/envs&gt;
    &lt;option name="WORKING_DIRECTORY"
      value="$PROJECT_DIR$" /&gt;
    &lt;option name="IS_MODULE_SDK" value="true" /&gt;
    &lt;option name="ADD_CONTENT_ROOTS" value="true" /&gt;
    &lt;option name="ADD_SOURCE_ROOTS" value="true" /&gt;
    &lt;option name="SCRIPT_NAME"
      value="$PROJECT_DIR$/run_server.py" /&gt;
    &lt;option name="SHOW_COMMAND_LINE" value="false" /&gt;
    &lt;option name="MODULE_MODE" value="false" /&gt;
    &lt;option name="REDIRECT_INPUT" value="false" /&gt;
    &lt;option name="INPUT_FILE" value="" /&gt;
    &lt;method v="2" /&gt;
  &lt;/configuration&gt;
&lt;/component&gt;</pre> <p>You can find a<a id="_idIndexMarker337"/> detailed guide on how to setup it <a id="_idIndexMarker338"/>at the dedicated <a id="_idIndexMarker339"/>Pycharm documentation page at: <a href="https://www.jetbrains.com/help/pycharm/run-debug-configuration.xhtml">https://www.jetbrains.com/help/pycharm/run-debug-configuration.xhtml</a>.</p>
<h2 id="_idParaDest-185"><a id="_idTextAnchor188"/>See also</h2>
<p>Feel free to dig into each<a id="_idIndexMarker340"/> of the debugging solutions and concepts we just explained at the links:</p>
<ul>
<li><em class="italic">Python distribution </em><em class="italic">debugger</em>: <a href="https://docs.python.org/3/library/pdb.xhtml">https://docs.python.org/3/library/pdb.xhtml</a></li>
<li><em class="italic">Breakpoints</em>: <a href="https://docs.python.org/3/library/functions.xhtml#breakpoint">https://docs.python.org/3/library/functions.xhtml#breakpoint</a></li>
<li><em class="italic">Uvicorn </em><em class="italic">Settings</em>: <a href="https://www.uvicorn.org/settings/">https://www.uvicorn.org/settings/</a></li>
<li><em class="italic">Debugging with VS </em><em class="italic">Code</em>: <a href="https://code.visualstudio.com/docs/python/debugging">https://code.visualstudio.com/docs/python/debugging</a></li>
<li><em class="italic">Debugy </em><em class="italic">Debugger</em>: <a href="https://github.com/microsoft/debugpy/">https://github.com/microsoft/debugpy/</a></li>
<li><em class="italic">Debugging with </em><em class="italic">PyCharm</em>: <a href="https://www.jetbrains.com/help/pycharm/debugging-your-first-python-application.xhtml">https://www.jetbrains.com/help/pycharm/debugging-your-first-python-application.xhtml</a></li>
</ul>
<h1 id="_idParaDest-186"><a id="_idTextAnchor189"/>Performance testing for high traffic applications</h1>
<p>Performance testing<a id="_idIndexMarker341"/> is crucial for ensuring your application can handle real-world usage scenarios, especially under high load. By systematically implementing and running performance tests, analyzing results, and optimizing based on findings, you can significantly improve your application’s responsiveness, stability, and scalability.</p>
<p>The recipe will show the basics of how to benchmark your application with <strong class="bold">Locust</strong> framework.</p>
<h2 id="_idParaDest-187"><a id="_idTextAnchor190"/>Getting ready</h2>
<p>To run performance testing you need a working application, we will use our <code>protoapp</code>, and a testing framework. We will use <strong class="bold">Locust</strong> framework for the purpose, which a testing framework based on Python syntax.</p>
<p>You can find a detailed explication on the official documentation at: <a href="https://docs.locust.io/en/stable/">https://docs.locust.io/en/stable/</a>.</p>
<p>Before starting, make sure you installed it in your virtual environment by running:</p>
<pre class="console">
$ pip install locust</pre> <p>Now we are ready to setup our configuration file and run the locust instance.</p>
<h2 id="_idParaDest-188"><a id="_idTextAnchor191"/>How to do it...</h2>
<p>With the application running and the <code>locust</code> package installed, we will proceed by specifying our configuration to run the performance test.</p>
<p>Create a <code>locustfile.py</code> in your project root. This file will define the behavior of users interacting with your application under test.</p>
<p>A minimal<a id="_idIndexMarker342"/> example of <code>locustfile.py</code> can be:</p>
<pre class="source-code">
from locust import HttpUser, task
class ProtoappUser(HttpUser):
    host = "http://localhost:8000"
    @task
    def hello_world(self):
        self.client.get("/home")</pre> <p>The configuration defines a client class with the service address and the endpoint we want to test.</p>
<p>Start your FastAPI server with:</p>
<pre class="console">
$ uvicorn protoapp.main:app</pre> <p>Then in another terminal window run locust:</p>
<pre class="console">
$ locust</pre> <p>Open your browser and navigate to <code>http://localhost:8089</code> to access the web interface of the application.</p>
<p>The web interface is intuitively designed, making it straightforward to:</p>
<ul>
<li><strong class="bold">Set Concurrent Users</strong>: Specify the maximum number of users accessing the service simultaneously during peak usage.</li>
<li><strong class="bold">Configure Ramp-Up Rate</strong>: Determine the rate of new users added per second to simulate increasing traffic.</li>
</ul>
<p>After configuring these parameters, click the <code>/home</code> endpoint defined in the <code>locustfile.py</code>.</p>
<p>Alternatively, you can simulate traffic using the command line. Here’s how:</p>
<pre class="console">
$ locust --headless --users 10 --spawn-rate 1</pre> <p>This command runs Locust in a headless mode to simulate:</p>
<ul>
<li>10 users accessing your application concurrently.</li>
<li>A spawn rate of 1 user per second.</li>
</ul>
<p>You push your test <a id="_idIndexMarker343"/>experience further by including it in a <strong class="bold">Continuous Integration /Continuous Delivery</strong> (<strong class="bold">CI/CD</strong>) pipeline before deploying, or even into a larger testing routine.</p>
<p>Dig into the documentation to test every aspect of the traffic for your application.</p>
<p>You have all the tools to debug and fully test your application.</p>
<p>In the next <a id="_idIndexMarker344"/>chapter, we are going to build a comprehensive RESTful application interacting with an SQL database.</p>
<h2 id="_idParaDest-189"><a id="_idTextAnchor192"/>See also</h2>
<p>You can find more<a id="_idIndexMarker345"/> on Locust on the official documentation pages:</p>
<ul>
<li><em class="italic">Locust </em><em class="italic">QuickStart</em>: <a href="https://docs.locust.io/en/stable/quickstart.xhtml">https://docs.locust.io/en/stable/quickstart.xhtml</a></li>
<li><em class="italic">Writing a Locust </em><em class="italic">file</em>: <a href="https://docs.locust.io/en/stable/writing-a-locustfile.xhtml">https://docs.locust.io/en/stable/writing-a-locustfile.xhtml</a></li>
<li><em class="italic">Running Locust from the Command </em><em class="italic">Line</em>: <a href="https://docs.locust.io/en/stable/running-without-web-ui.xhtml">https://docs.locust.io/en/stable/running-without-web-ui.xhtml</a></li>
</ul>
</div>
</div></body></html>