- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Profiling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is quite common that written code doesn't behave perfectly after being tested
    with real data. Other than bugs, we can find the problem that the performance
    of the code is not adequate. Perhaps some requests are taking too much time, or
    perhaps the usage of memory is too high.
  prefs: []
  type: TYPE_NORMAL
- en: In those cases, it's difficult to know exactly what the key elements are, that
    are taking the most time or memory. While it's possible to try to follow the logic,
    normally once the code is released, the bottlenecks will be at points that are
    almost impossible to know beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: To get information on what exactly is going on and follow the code flow, we
    can use profilers to dynamically analyze the code and better understand how the
    code is executed, in particular, where most time is spent. This can lead to adjustments
    and improvements affecting the most significant elements of the code, driven by
    data, instead of vague speculation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Profiling basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of profilers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling code for time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partial profiling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory profiling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, we will take a look at the basic principles of profiling.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Profiling is a dynamic analysis that instruments code to understand how it runs.
    This information is extracted and compiled in a way that can be used to get a
    better knowledge of a particular behavior based on a real case, as the code is
    running as usual. This information can be used to improve the code.
  prefs: []
  type: TYPE_NORMAL
- en: Certain static analysis tools, as opposed to dynamic, can provide insight into
    aspects of the code. For example, they can be used to detect if certain code is
    dead code, meaning it's not called anywhere in the whole code. Or, they can detect
    some bugs, like the usage of variables that haven't been defined before, like
    when having a typo. But they don't work with the specifics of code that's actually
    being run. Profiling will bring specific data based on the use case instrumented
    and will return much more information on the flow of the code.
  prefs: []
  type: TYPE_NORMAL
- en: The normal application of profiling is to improve the performance of the code
    under analysis. By understanding how it executes in practice, it sheds light on
    the dynamics of the code modules and parts that could be causing problems. Then,
    actions can be taken in those specific areas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance can be understood in two ways: either *time performance* (how long
    code takes to execute) or *memory performance* (how much memory the code takes
    to execute). Both can be bottlenecks. Some code may take too long to execute or
    use a lot of memory, which may limit the hardware where it''s executed.'
  prefs: []
  type: TYPE_NORMAL
- en: We will focus more on time performance in this chapter, as it is typically a
    bigger problem, but we will also explain how to use a memory profiler.
  prefs: []
  type: TYPE_NORMAL
- en: A common case in software development is that you don't really know what your
    code is going to do until it gets executed. Clauses to cover corner cases that
    appear rare may execute much more than expected, and software works differently
    when there are big arrays, as some algorithms may not be adequate.
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that doing that analysis before having the system running is
    incredibly difficult, and at most times, futile, as the problematic pieces of
    code will very likely be completely unexpected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Programmers waste enormous amounts of time thinking about, or worrying about,
    the speed of noncritical parts of their programs, and these attempts at efficiency
    actually have a strong negative impact when debugging and maintenance are considered.
    We should forget about small efficiencies, say about 97% of the time: **premature
    optimization is the root of all evil**. Yet we should not pass up our opportunities
    in that critical 3%.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Donald Knuth – Structured Programing with GOTO Statements - 1974.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Profiling gives us the ideal tool to *not* prematurely optimize, but to optimize
    according to real, tangible data. The idea is that you cannot optimize what you
    cannot measure. The profiler measures so it can be acted upon.
  prefs: []
  type: TYPE_NORMAL
- en: The famous quote above is sometimes reduced to "premature optimization is the
    root of all evil," which is a bit reductionist and doesn't carry the nuance. Sometimes
    it's important to design elements with care and it's possible to plan in advance.
    As good as profiling (or other techniques) may be, they can only go so far. But
    it's important to understand, on most occasions, it's better to take the simple
    approach, as performance will be good enough, and it will be possible to improve
    it later in the few cases when it's not.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling can be achieved in different ways, each with its pros and cons.
  prefs: []
  type: TYPE_NORMAL
- en: Types of profilers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two main kinds of time profilers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deterministic profilers**, through a process of tracing. A deterministic
    profiler instruments the code and records each individual command. This makes
    deterministic profilers very detailed, as they can follow up the code on each
    step, but at the same time, the code is executed slower than without the instrumentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deterministic profilers are not great to execute continuously. Instead, they
    can be activated in specific situations, like while running specific tests offline,
    to find out problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statistical profiles**, through sampling. This kind of profiler, instead
    of instrumenting the code and detecting each operation, awakes at certain intervals
    and takes a sample of the current code execution stack. If this process is done
    for long enough, it captures the general execution of the program.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking a sample of the stack is similar to taking a picture. Imagine a train
    or subway hall where people are moving across to go from one platform to another.
    Sampling is analogous to taking pictures at periodic intervals, for example, once
    every 5 minutes. Sure, it's not possible to get exactly who comes from one platform
    and goes to another, but after a whole day, it will provide good enough information
    on how many people have been around and what platforms are the most popular.
  prefs: []
  type: TYPE_NORMAL
- en: While they don't give as detailed information as deterministic profiles, statistical
    profilers are much more lightweight and don't consume many resources. They can
    be enabled to constantly monitor live systems without interfering with their performance.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical profilers only make sense on systems that are under relative load,
    as in a system that is not stressed, they'll show that most time is spent waiting.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical profilers can be internal, if the sampling is done directly on the
    interpreter, or even external if it's a different program that is taking the samples.
    An external profiler has the advantage that, even if there's any problem with
    the sampling process, it won't interfere with the program being sampled.
  prefs: []
  type: TYPE_NORMAL
- en: Both profilers can be seen as complementary. Statistical profilers are good
    tools for understanding the most-visited parts of the code and where the system,
    aggregated, is spending time. They live in the live system, where the real case
    usages determine the behavior of the system.
  prefs: []
  type: TYPE_NORMAL
- en: The deterministic profilers are tools for analyzing specific use cases in the
    petri dish of the developer's laptop, where a specific task that is having some
    problem can be dissected and analyzed carefully, to be improved.
  prefs: []
  type: TYPE_NORMAL
- en: In some respects, statistical profilers are analogous to metrics and deterministic
    profilers to logs. One displays the aggregated elements and the other the specific
    elements. Deterministic profilers, contrary to logs, are not ideal tools for using
    in live systems without care, though.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, code will present *hotspots*, slow parts of it that get executed
    often. Finding the specific parts to focus attention on and then act on them is
    a great way to improve the overall speed.
  prefs: []
  type: TYPE_NORMAL
- en: These hotspots can be revealed by profiling, either by checking the *global*
    hotspots using a statistical profiler or the *specific* hotspots for a task with
    a deterministic profiler. The first will display the specific parts of the code
    that are most used in general, which allows us to understand the pieces that get
    hit more often and take the most time in aggregate. The deterministic profiler
    can show, for a specific task, how long it takes for each line of code, and determine
    what are the slow elements.
  prefs: []
  type: TYPE_NORMAL
- en: We won't look at statistical profilers as they require systems that are under
    load and they are difficult to create in a test that's fit for the scope of this
    book. You can check `py-spy` ([https://pypi.org/project/py-spy/](https://pypi.org/project/py-spy/))
    or `pyinstrument` ([https://pypi.org/project/pyinstrument/](https://pypi.org/project/pyinstrument/)).
  prefs: []
  type: TYPE_NORMAL
- en: Another kind of profiler is the memory profiler. A memory profiler records when
    memory is increased and decreased, tracking the usage of memory. Profiling memory
    is typically used to find out memory leaks, which are rare for a Python program,
    but they can happen.
  prefs: []
  type: TYPE_NORMAL
- en: Python has a garbage collector that releases memory automatically when an object
    is not referenced anymore. This happens without having to take any action, so
    compared with programs with manual memory assignment, like C/C++, the memory management
    is easier to handle. The garbage collection mechanism used for Python is called
    *reference counting*, and it frees memory immediately once a memory object is
    not used by anyone, as compared with other kinds of garbage collectors that wait.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of Python, memory leaks can be created by three main use cases,
    from more likely to least:'
  prefs: []
  type: TYPE_NORMAL
- en: Some objects are still referenced, even if they are not used anymore. This can
    typically happen if there are long-lived objects that keep small elements in big
    elements, like lists of dictionaries when they are added and not removed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An internal C extension is not managing the memory correctly. This may require
    further investigation with specific C profiling tools, which is out of scope for
    this book.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Complex reference cycles. A reference cycle is a group of objects that reference
    each other, e.g. object A references B and object B references A. While Python
    has algorithms to detect them and release the memory nonetheless, there''s the
    small possibility that the garbage collector is disabled or any other bug problem.
    You can see more information on the Python garbage collector here: [https://docs.python.org/3/library/gc.html](https://docs.python.org/3/library/gc.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most likely situation for extra usage of memory is an algorithm that uses
    a lot of memory, and detecting when the memory is allocated can be achieved with
    the help of a memory profiler.
  prefs: []
  type: TYPE_NORMAL
- en: Memory profiling is typically more complicated and takes more effort than time
    profiling.
  prefs: []
  type: TYPE_NORMAL
- en: Let's introduce some code and profile it.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling code for time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start by creating a short program that will calculate and display all
    prime numbers up to a particular number. Prime numbers are numbers that are only
    divisible by themselves and one.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by taking a naïve approach first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This code will take every number from 2 to the number under test (without including
    it), and check whether the number is divisible. If at any point it is divisible,
    the number is not a prime number.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate all the way from 1 to 5,000, to verify that we are not making any
    mistakes, we will include the first prime numbers lower than 100 and compare them.
    This is on GitHub, available as `primes_1.py` at [https://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/chapter_14_profiling/primes_1.py](https://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/chapter_14_profiling/primes_1.py).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The calculation of prime numbers is performed by creating a list of all numbers
    (from 1 to `NUM_PRIMES_UP_TO`) and verifying each of them. Only values that return
    `True` will be kept:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The next line `assert`s that the first prime numbers are the same as the ones
    defined in the `PRIMES` list, which is a hardcoded list of the first primes lower
    than 100.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The primes are finally printed. Let''s execute the program, timing its execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: From here, we will start analyzing the code to see what is going on internally
    and see if we can improve it.
  prefs: []
  type: TYPE_NORMAL
- en: Using the built-in cProfile module
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The easiest, faster way of profiling a module is to directly use the included
    `cProfile` module in Python. This module is part of the standard library and can
    be called as part of the external call, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Note this called the script normally, but also presented the profile analysis.
    The table shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ncalls`: Number of times each element has been called'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tottime`: Total time spent on each element, not including sub calls'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`percall`: Time per call on each element (not including sub calls)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cumtime`: Cumulative time – the total time spent on each element, including
    subcalls'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`percall`: Time per call on an element, including subcalls'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filename:lineno`: Each of the elements under analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, the time is clearly seen to be spent in the `check_if_prime` function,
    which is called 4,999 times, and it takes the practical totality of the time (744
    milliseconds compared with a total of 762).
  prefs: []
  type: TYPE_NORMAL
- en: While not easy to see here due to the fact that it's a small script, `cProfile`
    increases the time it takes to execute the code. There's an equivalent module
    called `profile` that's a direct replacement but implemented in pure Python, as
    opposed to a C extension. Please generally use `cProfile` as it's faster, but
    `profile` can be useful at certain moments, like when trying to extend the functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'While this text table can be enough for simple scripts like this one, the output
    can be presented as a file and then displayed with other tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to install the visualizer SnakeViz, installing it through `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, open the file with `snakeviz`, which will open a browser with the
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Graphical user interface, application'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17580_14_01.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14.1: Graphical representation of the profiling information. The full
    page is too big to fit here and has been cropped purposefully to show some of
    the info.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This graph is interactive, and we can click and hover on different elements
    to get more information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17580_14_02.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14.2: Checking the information about check_if_prime. The full page is
    too big to fit here and has been cropped purposefully to show some of the info.'
  prefs: []
  type: TYPE_NORMAL
- en: We can confirm here that the bulk of the time is spent on `check_if_prime`,
    but we don't get information about what's inside it.
  prefs: []
  type: TYPE_NORMAL
- en: This is because `cProfile` only has function granularity. You'll see how long
    each function call takes, but not a lower resolution. For this specifically simple
    function, this may not be enough.
  prefs: []
  type: TYPE_NORMAL
- en: Do not underestimate this tool. The code example presented is purposefully simple
    to avoid spending too much time explaining its use. Most of the time, localizing
    the function that's taking most of the time is good enough to visually inspect
    it and discover what's taking too long. Keep in mind that, in most practical situations,
    the time spent will be on external calls like DB accesses, remote requests, etc.
  prefs: []
  type: TYPE_NORMAL
- en: We will see how to use a profiler that has a higher resolution, analyzing each
    line of code.
  prefs: []
  type: TYPE_NORMAL
- en: Line profiler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To analyze the `check_if_prime` function, we need to first install the module
    `line_profiler`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: After it's installed, we will make a small change in the code, and save it as
    `primes_2.py`. We will add the decorator `@profile` for the `check_if_prime` function,
    to indicate to the line profiler to look into it.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that you should only profile sections of the code where you want
    to know more in this way. If all the code was profiled in this way, it would take
    a lot of time to analyze.
  prefs: []
  type: TYPE_NORMAL
- en: The code will be like this (the rest will be unaffected). You can check the
    whole file on GitHub at [https://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/chapter_14_profiling/primes_2.py](https://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/chapter_14_profiling/primes_2.py).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Execute the code now with `kernprof`, which will be installed after the installation
    of `line_profiler`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the execution took noticeably longer – 12 seconds compared with subsecond
    execution without the profiler enabled. Now we can take a look at the results
    with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can start analyzing the specifics of the algorithm used. The main problem
    seems to be that we are doing a lot of comparisons. Both lines 11 and 12 are being
    called too many times, though the time per hit is short. We need to find a way
    to reduce the number of times they're being called.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first one is easy. Once we find a `False` result, we don''t need to wait
    anymore; we can return directly, instead of continuing with the loop. The code
    will be like this (stored in `primes_3.py`, available at [https://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/chapter_14_profiling/primes_3.py](https://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/chapter_14_profiling/primes_3.py)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We see how time has gone down by a big factor (2 seconds compared with the 12
    seconds before, as measured by `time`) and we see the great reduction in time
    spent on comparisons (3,749,127 microseconds before, and then 473,788 microseconds),
    mainly due to the fact there are 10 times fewer comparisons, 1,563,868 compared
    with 12,487,503.
  prefs: []
  type: TYPE_NORMAL
- en: We can also improve and further reduce the number of comparisons by limiting
    the size of the loop.
  prefs: []
  type: TYPE_NORMAL
- en: Right now, the loop will try to divide the source number between all the numbers
    up to itself. For example, for 19, we try these numbers (as 19 is a prime number,
    it's not divisible by any except for itself).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Trying all these numbers is not necessary. At least, we can skip half of them,
    as no number will be divisible by a number higher than half itself. For example,
    19 divided by 10 or higher is less than 2.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, any factor of a number will be lower than its square root. This
    can be explained as follows: If a number is the factor of two or more numbers,
    the highest they may be is the square root of the whole number. So we check only
    the numbers up to the square root (rounded down):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: But we can reduce it even further. We only need to check the odd numbers after
    2, as any even number will be divisible by 2\. So, in this case, we even reduce
    it further.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To apply all of this, we need to tweak the code again and store it in `primes_4.py`,available
    on GitHub at [https://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/chapter_14_profiling/primes_4.py](https://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/chapter_14_profiling/primes_4.py):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The code always checks for divisibility by 2, unless the number is 2\. This
    is to keep returning 2 correctly as a prime.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we create a range of numbers that starts from 3 (we already tested 2)
    and continue until the square root of the number. We use the `math` module to
    perform the action and to floor the number to the nearest lower integer. The `range`
    function requires a `+1` of this number, as it doesn't include the defined number.
    Finally, the range step on 2 integers at time so that all the numbers are odd,
    since we started with 3.
  prefs: []
  type: TYPE_NORMAL
- en: For example, to test a number like 1,000, this is the equivalent code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note that 31 is returned as we added the `+1`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's profile the code again.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We see another big increase in performance. Let's see the line profile.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We've reduced the number of loop iterations drastically to 22,228, from 1.5
    million in `primes_3.py` and over 12 million in `primes_2.py`, when we started
    the line profiling. That's some serious improvement!
  prefs: []
  type: TYPE_NORMAL
- en: You can try to do the test to increase `NUM_PRIMES_UP_TO` in `primes_2.py` and
    `primes_4.py` and compare them. The change will be clearly perceptible.
  prefs: []
  type: TYPE_NORMAL
- en: The line approach should be used only for small sections. In general, we've
    seen how `cProfile` can be more useful, as it's easier to run and gives information.
  prefs: []
  type: TYPE_NORMAL
- en: Previous sections have assumed that we are able to run the whole script and
    then receive the results, but that may not be correct. Let's take a look at how
    to profile in sections of the program, for example, when a request is received.
  prefs: []
  type: TYPE_NORMAL
- en: Partial profiling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many scenarios, profilers will be useful in environments where the system
    is in operation and we cannot wait until the process finishes before obtaining
    profiling information. Typical scenarios are web requests.
  prefs: []
  type: TYPE_NORMAL
- en: If we want to analyze a particular web request, we may need to start a web server,
    produce a single request, and stop the process to obtain the result. This doesn't
    work as well as you may think due to some problems that we will see.
  prefs: []
  type: TYPE_NORMAL
- en: But first, let's create some code to explain this situation.
  prefs: []
  type: TYPE_NORMAL
- en: Example web server returning prime numbers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the final version of the function `check_if_prime` and create a
    web service that returns all the primes up to the number specified in the path
    of the request. The code will be the following, and it's fully available in the
    `server.py` file on GitHub at [https://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/chapter_14_profiling/server.py](https://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/chapter_14_profiling/server.py).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The code is better understood if you start from the end. The final block creates
    a web server using the base `HTTPServer` definition in the Python module `http.server`.
    Previously, we created the class `MyServer`, which defines what to do if there's
    a `GET` request in the `do_GET` method.
  prefs: []
  type: TYPE_NORMAL
- en: The `do_GET` method returns an HTML response with the result calculated by `get_result`.
    It adds all the required headers and formats the body in HTML.
  prefs: []
  type: TYPE_NORMAL
- en: The interesting bits of the process happen in the next functions.
  prefs: []
  type: TYPE_NORMAL
- en: '`get_result` is the root one. It first calls `extract_param` to get a number,
    up to which to calculate the threshold number for us to calculate primes up to.
    If correct, then that''s passed to `prime_numbers_up_to`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The function `extract_params` will extract a number from the URL path. It first
    removes any `/` character, and then tries to convert it into an integer and checks
    the integer is positive. For any errors, it returns `None`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The function `prime_numbers_up_to`, finally, calculates the prime numbers up
    to the number passed. This is similar to the code that we saw earlier in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Finally, `check_if_prime`, which we covered extensively earlier in the chapter,
    is the same as it was at `primes_4.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process can be started with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: And then tested by going to `http://localhost:8000/500` to try to get prime
    numbers up to 500.
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17580_14_03.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14.3: The interface displaying all primes up to 500'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have an understandable output. Let's move on to profiling
    the process we used to get it.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling the whole process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can profile the whole process by starting it under `cProfile` and then capturing
    its output with. We start it like this, make a single request to `http://localhost:8000/500`,
    and check the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We have stored the results in the file `server.prof`. This file can then be
    analyzed as before, using `snakeviz`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Which displays the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17580_14_04.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14.4: Diagram of the full profile. The full page is too big to fit here
    and has been cropped purposefully to show some of the info.'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the diagram shows that for the vast majority of the test duration,
    the code was waiting for a new request, and internally doing a poll action. This
    is part of the server code and not our code.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find the code that we care about, we can manually search in the long list
    below for `get_result`, which is the root of the interesting bits of our code.
    Be sure to select `Cutoff: None` to display all the functions.'
  prefs: []
  type: TYPE_NORMAL
- en: Once selected, the diagram will display from there onward. Be sure to scroll
    up to see the new diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated with low confidence](img/B17580_14_05.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14.5: The diagram showing from get_result. The full page is too big
    to fit here and has been cropped purposefully to show some of the info.'
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can see more of the general structure of the code execution. You can
    see that most of the time is spent on the multiple `check_if_prime` calls, which
    comprise the bulk of `prime_numbers_up_to` and the list comprehension included
    in it, and very little time is spent on `extract_params`.
  prefs: []
  type: TYPE_NORMAL
- en: 'But this approach has some problems:'
  prefs: []
  type: TYPE_NORMAL
- en: First of all, we need to go a full cycle between starting and stopping a process.
    This is cumbersome to do for requests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Everything that happens in the cycle is included. That adds noise to the analysis.
    Fortunately, we knew that the interesting part was in `get_result`, but that may
    not be evident. This case also uses a minimal structure but adding that in the
    case of a complex framework like Django can lead to a lot of .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we process two different requests, they will be added into the same file,
    again mixing the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These problems can be solved by applying the profiler to only the part that
    is of interest and producing a new file for each request.
  prefs: []
  type: TYPE_NORMAL
- en: Generating a profile file per request
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To be able to generate a different file with information per individual request,
    we need to create a decorator for easy access. This will profile and produce an
    independent file.
  prefs: []
  type: TYPE_NORMAL
- en: In the file `server_profile_by_request.py`, we get the same code as in `server.py`,
    but adding the following decorator.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The decorator defines a `wrapper` function that replaces the original function.
    We use the `wraps` decorator to keep the original name and docstring.
  prefs: []
  type: TYPE_NORMAL
- en: This is just a standard decorator process. A decorator function in Python is
    one that returns a function that then replaces the original one. As you can see,
    the original function `func` is still called inside the wrapper that replaces
    it, but it adds extra functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Inside, we start a profiler and run the function under it using the `runcall`
    function. This line is the core of it – using the profiler generated, we run the
    original function `func` with its parameters and store its returned value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: After that, we generate a new file that includes the current time and dump the
    stats in it with the `.dump_stats` call.
  prefs: []
  type: TYPE_NORMAL
- en: We also decorate the `get_result` function, so we start our profiling there.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The full code is available in the file `server_profile_by_request.py`, available
    on GitHub at [https://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/chapter_14_profiling/server_profile_by_request.py](https://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/chapter_14_profiling/server_profile_by_request.py).
  prefs: []
  type: TYPE_NORMAL
- en: Let's start the server now and make some calls through the browser, one to `http://localhost:8000/500`
    and another to `http://localhost:8000/800`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see how new files are created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'These files can be displayed using `snakeviz`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![Graphical user interface, chart'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17580_14_06.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14.6: The profile information of a single request. The full page is
    too big to fit here and has been cropped purposefully to show some of the info.'
  prefs: []
  type: TYPE_NORMAL
- en: Each file contains only the information from `get_result` onwards, which gets
    information only up to a point. Even more so, each file displays information only
    for a specific request, so it can be profiled individually, with a high level
    of detail.
  prefs: []
  type: TYPE_NORMAL
- en: The code can be adapted to adapt the filename more specifically to include details
    like call parameters, which can be useful. Another interesting possible adaptation
    is to create a random sample, so only 1 in X calls produces profiled code. This
    can help reduce the overhead of profiling and allow you to completely profile
    some requests.
  prefs: []
  type: TYPE_NORMAL
- en: This is different from a statistical profiler, as it will still completely profile
    some requests, instead of detecting what's going on at a particular time. This
    can help follow the flow of what happens for particular requests.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll see how to perform memory profiling.
  prefs: []
  type: TYPE_NORMAL
- en: Memory profiling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, applications use too much memory. The worst-case scenario is that
    they use more and more memory as time goes by, normally due to what's called a
    memory leak, maintaining memory that is no longer used, due to some mistake in
    the coding. Other problems can also include the fact that the usage of memory
    may be improved, as it's a limited resource.
  prefs: []
  type: TYPE_NORMAL
- en: To profile memory and analyze what the objects are that use the memory, we need
    first to create some example code. We will generate enough Leonardo numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Leonardo numbers are numbers that follow a sequence defined as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The first Leonardo number is one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second Leonardo number is also one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any other Leonardo number is the two previous Leonardo numbers plus one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leonardo numbers are similar to Fibonacci numbers. They are actually related
    to them. We use them instead of Fibonacci to show more variety. Numbers are fun!
  prefs: []
  type: TYPE_NORMAL
- en: We present the first 35 Leonardo numbers by creating a recursive function and
    store it in `leonardo_1.py`, available on GitHub at [https://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/chapter_14_profiling/leonardo_1.py](https://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/chapter_14_profiling/leonardo_1.py).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: You can run the code and see it takes progressively longer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: To speed up the process, we see that it's possible to use memorization techniques,
    which means to store the results and use them instead of calculating them all
    the time.
  prefs: []
  type: TYPE_NORMAL
- en: We change the code like this, creating the `leonardo_2.py` file (available on
    GitHub at [https://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/chapter_14_profiling/leonardo_2.py](https://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/chapter_14_profiling/leonardo_2.py)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This uses a global dictionary, `CACHE`, to store all Leonardo numbers, speeding
    up the process. Note that we increased the number of numbers to calculate from
    `35` to `35000`, a thousand times more. The process runs quite quickly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Let's take a look now at memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: Using memory_profiler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have our application storing information, let's use a profiler to
    show where the memory is stored.
  prefs: []
  type: TYPE_NORMAL
- en: We need to install the package `memory_profiler`. This package is similar to
    `line_profiler`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We can now add a `@profile` decorator in the `leonardo` function (stored in
    `leonardo_2p.py`, on GitHub at [https://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/chapter_14_profiling/leonardo_2p.py](https://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/chapter_14_profiling/leonardo_2p.py)),
    and run it using the `memory_profiler` module. You'll notice that it runs slower
    this time, but after the usual result, it displays a table.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: This table shows first the memory usage, and the increment or decrement, as
    well as how many times each line appears.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Line 9 gets executed only a few times. When it does, the amount of memory is
    around `38 MiB`, which will be the minimum memory used by the program.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The total memory used is almost `105 MiB`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The whole memory increase is localized in lines 12 and 13, when we create a
    new Leonardo number and when we store it in the `CACHE` dictionary. Note how we
    are never releasing memory here.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We don't really need to keep all the previous Leonardo numbers in memory at
    all times, and we can try a different approach to keep only a few.
  prefs: []
  type: TYPE_NORMAL
- en: Memory optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We create the file `leonardo_3.py` with the following code, available on GitHub
    at [https://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/chapter_14_profiling/leonardo_3.py](https://github.com/PacktPublishing/Python-Architecture-Patterns/blob/main/chapter_14_profiling/leonardo_3.py):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Note we keep the `@profile` decorator to run the memory profiler again. Most
    of the code is the same, but we added the following extra block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This code will keep the number of elements in the `CACHE` dictionary within
    a limit. When the limit is reached, it will remove the first element returned
    by `CACHE.keys()`, which will be the oldest.
  prefs: []
  type: TYPE_NORMAL
- en: Since Python 3.6, all Python dictionaries are ordered, so they'll return their
    keys in the order they have been input previously. We take advantage of that for
    this. Note we need to convert the result from `CACHE.keys()` (a `dict_keys` object)
    to a list to allow getting the first element.
  prefs: []
  type: TYPE_NORMAL
- en: The dictionary won't be able to grow. Let's now try to run it and see the results
    of the profiling.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we see how the memory remains stable at around the `38 MiB`, that
    we see is the minimum. In this case, note how there are no increments or decrements.
    Really what happens here is that increments and decrements are too small to be
    noticed. Because they cancel each other, the report is close to zero.
  prefs: []
  type: TYPE_NORMAL
- en: The `memory-profiler` module is also able to perform more actions, including
    showing the usage of memory based on time and plotting it, so you can see memory
    increasing or decreasing over time. Take a look at its full documentation at [https://pypi.org/project/memory-profiler/](https://pypi.org/project/memory-profiler/).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we described what profiling is and when it's useful to apply
    it. We described that profiling is a dynamic tool that allows you to understand
    how code runs. This information is useful in understanding the flow in a practice
    situation and being able to optimize the code with that information. Code can
    be optimized normally to execute faster, but other alternatives are open, like
    using fewer resources (normally memory), reducing external accesses, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'We described the main types of profilers: deterministic profilers, statistical
    profilers, and memory profilers. The first two are mostly oriented toward improving
    the performance of code and memory profilers analyze the memory used by the code
    in execution. Deterministic profilers instrument the code to detail the flow of
    the code as it''s executed. Statistical profilers sample the code at periodic
    times to provide a general view of the parts of the code that are executed more
    often.'
  prefs: []
  type: TYPE_NORMAL
- en: We then showed how to profile the code using deterministic profilers, presenting
    an example. We analyzed it first with the built-in module `cProfile`, which gives
    a function resolution. We saw how to use graphical tools to show the results.
    To dig deeper, we used the third-party module `line-profiler`, which goes through
    each of the code lines. Once the flow of the code is understood, it is optimized
    to greatly reduce its execution time.
  prefs: []
  type: TYPE_NORMAL
- en: The next step was to see how to profile a process intended to keep running,
    like a web server. We showed the problems with trying to profile the whole application
    in these cases and described how we can profile each individual request instead
    for clarity.
  prefs: []
  type: TYPE_NORMAL
- en: These techniques are also applicable to other situations like conditional profiling,
    profiling in only certain situations, like at certain times or one of each 100
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we also presented an example to profile memory and see how it's used
    by using the module `memory-profiler`.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn more details about how to find and fix problems
    in code, including in complex situations, through debugging techniques.
  prefs: []
  type: TYPE_NORMAL
