<html><head></head><body>
		<div><h1 id="_idParaDest-214" class="chapter-number"><a id="_idTextAnchor218"/><st c="0">8</st></h1>
			<h1 id="_idParaDest-215"><a id="_idTextAnchor219"/><st c="2">Building Workflows with Flask</st></h1>
			<p><st c="31">Workflows are sequences or groups of repetitive tasks, activities, or small processes that require a complete start-to-end execution to satisfy a particular business process. </st><st c="207">Each task is equivalent to routinary transactions such as sending emails, running scripts or terminal commands, data transformation and serialization, database transactions, and other highly computational operations. </st><st c="424">These tasks can be simple sequence, parallel, and </st><st c="474">complex types.</st></p>
			<p><st c="488">Several tools and platforms can provide best practices, rules, and technical specifications to build workflows for industry, enterprise, and scientific problems. </st><st c="651">However, most of these solutions require Java more than Python as their core language. </st><st c="738">Now, the main goal of this chapter is to prove that Python, particularly the Flask framework, can simulate workflows that utilize </st><strong class="bold"><st c="868">Business Process Modeling Notation</st></strong><st c="902"> (</st><strong class="bold"><st c="904">BPMN</st></strong><st c="908">) and also </st><strong class="bold"><st c="920">non-BPMN</st></strong><st c="928"> workflows using popular and modern platforms such as </st><em class="italic"><st c="982">Zeebe/Camunda</st></em><st c="995">, </st><em class="italic"><st c="997">Airflow 2.0</st></em><st c="1008">, and </st><em class="italic"><st c="1014">Temporal</st></em><st c="1022">. Moreover, the chapter will also showcase the use of </st><em class="italic"><st c="1076">Celery tasks</st></em><st c="1088"> in building custom workflows for </st><st c="1122">Flask applications.</st></p>
			<p><st c="1141">This chapter will cover the following topics that will discuss the different mechanisms and procedures in implementing workflow activities with the </st><st c="1290">Flask framework:</st></p>
			<ul>
				<li><st c="1306">Building workflows with </st><st c="1331">Celery tasks</st></li>
				<li><st c="1343">Creating BPMN and non-BPMN workflows </st><st c="1381">with SpiffWorkflow</st></li>
				<li><st c="1399">Building service tasks with the </st><st c="1432">Zeebe/Camunda platforms</st></li>
				<li><st c="1455">Using Airflow 2.x in orchestrating </st><st c="1491">API endpoints</st></li>
				<li><st c="1504">Implementing workflows </st><st c="1528">using Temporal.io</st></li>
			</ul>
			<h1 id="_idParaDest-216"><a id="_idTextAnchor220"/><st c="1545">Technical requirements</st></h1>
			<p><st c="1568">This chapter aims to implement </st><em class="italic"><st c="1600">Doctor’s Appointment Management Software</st></em><st c="1640"> that uses workflows to implement its business processes. </st><st c="1698">It has the following five different Flask projects, showcasing the varying workflow solutions to build the </st><st c="1805">Flask application:</st></p>
			<ul>
				<li><code><st c="1823">ch08-celery-redis</st></code><st c="1841">, which focuses on designing dynamic workflows with </st><st c="1893">Celery tasks.</st></li>
				<li><code><st c="1906">ch08-spiff-web</st></code><st c="1921">, which implements a web application for the appointment system using the </st><st c="1995">SpiffWorkflow library.</st></li>
				<li><code><st c="2017">ch08-temporal</st></code><st c="2031">, which uses the Temporal platform to build </st><st c="2075">distributed architecture.</st></li>
				<li><code><st c="2100">ch08-zeebe</st></code><st c="2111">, which utilizes the Zeebe/Camunda platform for </st><st c="2159">BPMN workflows.</st></li>
				<li><code><st c="2174">ch08-airflow</st></code><st c="2187">, which integrates with the Airflow 2.x workflow engine to manage </st><st c="2253">API services.</st></li>
			</ul>
			<p><st c="2266">Although with different workflow solutions, each of these projects targets the practical and optimal process performance for the </st><em class="italic"><st c="2396">user’s login transactions</st></em><st c="2421">, </st><em class="italic"><st c="2423">appointment processes</st></em><st c="2444">, </st><em class="italic"><st c="2446">doctor engagement</st></em><st c="2463">, </st><em class="italic"><st c="2465">billing processes</st></em><st c="2482">, and </st><em class="italic"><st c="2488">releasing transactions</st></em><st c="2510">. All database transactions are relational and use PostgreSQL as their database. </st><st c="2591">All these projects, on the other hand, are available </st><st c="2644">at </st><a href="https://github.com/PacktPublishing/Mastering-Flask-Web-Development/tree/main/ch08"><st c="2647">https://github.com/PacktPublishing/Mastering-Flask-Web-Development/tree/main/ch08</st></a><st c="2728">.</st></p>
			<h1 id="_idParaDest-217"><a id="_idTextAnchor221"/><st c="2729">Building workflows with Celery tasks</st></h1>
			<p><st c="2766">Utilizing </st><strong class="bold"><st c="2777">Celery</st></strong><st c="2783"> as a task queue manager and </st><strong class="bold"><st c="2812">Redis</st></strong><st c="2817"> as its broker was part of our </st><a href="B19383_05.xhtml#_idTextAnchor111"><em class="italic"><st c="2848">Chapter 5</st></em></a><st c="2857"> content. </st><st c="2867">The chapter explicitly discussed all setups and installations to build Flask-Celery-Redis integration. </st><st c="2970">It also</st><a id="_idIndexMarker673"/><st c="2977"> expounded on how Celery can run background processes asynchronously outside the context of Flask’s request-response transaction. </st><st c="3107">Additionally, this chapter will show us another feature of Celery that can solve business </st><st c="3197">process optimization.</st></p>
			<p><st c="3218">Celery has a mechanism to build dynamic workflows, types of workflows that run outside the bounds of some schema definitions and rules from start to end of workflow activities. </st><st c="3396">Its first requirement is</st><a id="_idIndexMarker674"/><st c="3420"> to wrap all tasks </st><st c="3439">in </st><em class="italic"><st c="3442">signatures</st></em><st c="3452">.</st></p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor222"/><st c="3453">Creating task signatures</st></h2>
			<p><st c="3478">In a typical scenario, calling Celery tasks requires invoking directly its </st><code><st c="3554">delay()</st></code><st c="3561"> method to run the underlying process the standard way or </st><code><st c="3619">apply_async()</st></code><st c="3632"> to run it asynchronously. </st><st c="3659">But to manage</st><a id="_idIndexMarker675"/><st c="3672"> Celery tasks to build custom dynamic workflows, individual tasks must invoke the </st><code><st c="3754">signature()</st></code><st c="3765"> or </st><code><st c="3769">s()</st></code><st c="3772"> method first. </st><st c="3787">This allows passing the Celery task invocation to workflow operations, linking a Celery task to another task as callbacks after its successful execution, and also helps manage its inputs, arguments, and execution options. </st><st c="4009">A signature is like a wrapper to a task ready to be passed as an argument to Celery’s </st><st c="4095">workflow operations.</st></p>
			<p><st c="4115">The following </st><code><st c="4130">add_login_task_wrapper()</st></code><st c="4154"> task, for instance, can be wrapped inside a signature just by calling its </st><code><st c="4229">signature()</st></code><st c="4240"> or </st><code><st c="4244">s()</st></code><st c="4247"> method:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="4255">@shared_task(ignore_result=False)</st></strong><st c="4289">
def add_login_task_wrapper(details):
    </st><strong class="bold"><st c="4327">async def add_login_task(details):</st></strong><st c="4361">
        try:
            async with db_session() as sess:
              async with sess.begin():
                repo = LoginRepository(sess)
                details_dict = loads(details)
                print(details_dict)
                login = Login(**details_dict)
                result = await repo.insert_login(login)
                if result:
                    return str(True)
                else:
                    return str(False)
        except Exception as e:
            print(e)
            return str(False)
    return </st><code><st c="4774">signature()</st></code><st c="4785"> method includes having a tuple argument, as in the </st><st c="4837">following snippet:</st></p>
			<pre class="source-code"><st c="4855">
add_login_task_wrapper.signature(</st><code><st c="4936">s()</st></code><st c="4939"> equivalent with a typical parameter list containing the arguments, as in the </st><st c="5017">following snippet:</st></p>
			<pre class="source-code"><st c="5035">
add_login_task_wrapper.s(</st><code><st c="5080">delay()</st></code><st c="5087">, </st><code><st c="5089">apply_async()</st></code><st c="5102">, or simply </st><code><st c="5114">()</st></code><st c="5116"> right after the </st><code><st c="5133">signature()</st></code><st c="5144"> call to run the task, if necessary. </st><st c="5181">Now, let us explore Celery’s built-in signatures called </st><em class="italic"><st c="5237">primitives</st></em><st c="5247">, used in building</st><a id="_idIndexMarker676"/><st c="5265"> simple and </st><st c="5277">complex workflows.</st></p>
			<h2 id="_idParaDest-219"><a id="_idTextAnchor223"/><st c="5295">Utilizing Celery primitives</st></h2>
			<p><st c="5323">Now, Celery provides the following core </st><a id="_idIndexMarker677"/><st c="5364">workflow operations called primitives, which are </st><a id="_idIndexMarker678"/><st c="5413">also signature objects themselves that take a list of task signatures to build dynamic </st><st c="5500">workflow transactions:</st></p>
			<ul>
				<li><code><st c="5522">chain()</st></code><st c="5530"> – A Celery function that takes a series of signatures that are linked together to form a chain of callbacks executed from left </st><st c="5658">to right.</st></li>
				<li><code><st c="5667">group()</st></code><st c="5675"> – A Celery operator that takes a list of signatures that will execute </st><st c="5746">in parallel.</st></li>
				<li><code><st c="5758">chord()</st></code><st c="5766"> – A Celery operator that takes a list of signatures that will execute in parallel but with a callback that will consolidate </st><st c="5891">their results.</st></li>
			</ul>
			<p><st c="5905">Let us first, in the next section, showcase Celery’s chained </st><st c="5967">workflow execution.</st></p>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor224"/><st c="5986">Implementing a sequential workflow</st></h2>
			<p><st c="6021">Celery primitives are the components of building dynamic Celery workflows. </st><st c="6097">The most commonly used primitive is the </st><em class="italic"><st c="6137">chain</st></em><st c="6142"> primitive, which can establish a pipeline of tasks with results passed from one task to another in a left-to-right manner. </st><st c="6266">Since it is dynamic, it can follow any specific </st><a id="_idIndexMarker679"/><st c="6314">sequence based on the software specification, but it prefers smaller and straightforward tasks to avoid unwanted performance degradation. </st><em class="italic"><st c="6452">Figure 8</st></em><em class="italic"><st c="6460">.1</st></em><st c="6462"> shows a workflow diagram that the </st><code><st c="6497">ch08-celery-redis</st></code><st c="6514"> project implemented for an efficient user </st><st c="6557">signup transaction:</st></p>
			<div><div><img src="img/B19383_08_001.jpg" alt="Figure 8.1 – Task signatures in a chain operation"/><st c="6576"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="6578">Figure 8.1 – Task signatures in a chain operation</st></p>
			<p><st c="6627">Similar to the </st><code><st c="6643">add_user_login_task_wrapper()</st></code><st c="6672"> task, </st><code><st c="6679">add_user_profile_task_wrapper()</st></code><st c="6710"> and </st><code><st c="6715">show_complete_profile_task_wrapper()</st></code><st c="6751"> are asynchronous Celery tasks that can emit their respective signature to establish a dynamic workflow. </st><st c="6856">The following endpoint function calls the signatures of these tasks in sequence using the </st><code><st c="6946">chain()</st></code><st c="6953"> primitive:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="6964">from modules.login.services.workflow_tasks import add_user_login_task_wrapper, add_user_profile_task_wrapper, show_complete_login_task_wrapper</st></strong>
<strong class="bold"><st c="7107">@login_bp.post('/login/user/add')</st></strong><st c="7141">
async def add_user_workflow():
    user_json = request.get_json()
    user_str = dumps(user_json)
    </st><strong class="bold"><st c="7232">task = chain(add_user_login_task_wrapper.s(user_str),</st></strong> <strong class="bold"><st c="7285">add_user_profile_task_wrapper.s(),</st></strong> <strong class="bold"><st c="7320">show_complete_login_task_wrapper.s())()</st></strong><strong class="bold"><st c="7360">result = task.get()</st></strong><st c="7380">
    records = loads(result)
    return jsonify(profile=records), 201</st></pre>			<p><st c="7441">The presence of </st><code><st c="7458">()</st></code><st c="7460"> at the end of the </st><code><st c="7479">chain()</st></code><st c="7486"> primitive means the execution of the chained sequence since </st><code><st c="7547">chain()</st></code><st c="7554"> is also a signature but a predefined one. </st><st c="7597">Now, the purpose of the </st><code><st c="7621">add_user_workflow()</st></code><st c="7640"> endpoint is to merge the </st><em class="italic"><st c="7666">INSERT</st></em><st c="7672"> transaction of the login credentials and the login profile details of the user instead of accessing two separate </st><a id="_idIndexMarker680"/><st c="7786">endpoints for the whole process. </st><st c="7819">Also, it’s there to render the login credentials to the user after a successful workflow execution. </st><st c="7919">So, all three tasks are in one execution frame with one JSON input of combined user profile and login details to the initial task, </st><code><st c="8050">add_user_login_task_wrapper()</st></code><st c="8079">. But what if tasks need arguments? </st><st c="8115">Does the </st><code><st c="8124">signature()</st></code><st c="8135"> method accept parameter(s) for its task? </st><st c="8177">Let’s take a look in the </st><st c="8202">next section.</st></p>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor225"/><st c="8215">Passing inputs to signatures</st></h2>
			<p><st c="8244">As mentioned earlier in this chapter, the</st><a id="_idIndexMarker681"/><st c="8286"> required arguments for the Celery tasks can be passed to the </st><code><st c="8348">s()</st></code><st c="8351"> or </st><code><st c="8355">signature()</st></code><st c="8366"> function. </st><st c="8377">In the given chained tasks, the </st><code><st c="8409">add_user_login_task_wrapper()</st></code><st c="8438"> is the only task among the three that needs input from the API, as depicted in its </st><st c="8522">code here:</st></p>
			<pre class="source-code"><st c="8532">
@shared_task(ignore_result=False)
</st><strong class="bold"><st c="8567">def add_user_login_task_wrapper(details):</st></strong><st c="8608">
    async def add_user_task(</st><strong class="bold"><st c="8633">details</st></strong><st c="8641">):
        try:
            async with db_session() as sess:
              async with sess.begin():
                repo = LoginRepository(sess)
                </st><strong class="bold"><st c="8737">details_dict = loads(details)</st></strong><st c="8766">
                … … … … … …
                login = Login(**user_dict)
                result = await repo.insert_login(login)
                if result:
                    profile_details = dumps(details_dict)
                    return profile_details
                else:
                    return ""
        except Exception as e:
            print(e)
            return ""
    return </st><code><st c="9014">details</st></code><st c="9021"> parameter is the complete JSON details passed from the endpoint function to the </st><code><st c="9102">s()</st></code><st c="9105"> method so that</st><a id="_idIndexMarker682"/><st c="9120"> the task will retrieve only the </st><em class="italic"><st c="9153">login credentials</st></em><st c="9170"> for the </st><em class="italic"><st c="9179">INSERT</st></em><st c="9185"> login transaction. </st><st c="9205">Now, the task will return the remaining details, the user profile information, as input to the next task in the sequence, </st><code><st c="9327">add_user_profile_task_wrapper()</st></code><st c="9358">. The following code shows the presence of a local parameter in the </st><code><st c="9426">add_user_profile_task_wrapper()</st></code><st c="9457"> task that will receive the result of the </st><st c="9499">previous task:</st></p>
			<pre class="source-code"><st c="9513">
@shared_task(ignore_result=False)
</st><strong class="bold"><st c="9548">def add_user_profile_task_wrapper(details):</st></strong><st c="9591">
    async def add_user_profile_task(</st><strong class="bold"><st c="9624">details</st></strong><st c="9632">):
        try:
            async with db_session() as sess:
              async with sess.begin():
                … … … … … …
                </st><strong class="bold"><st c="9711">role = profile_dict['role']</st></strong><st c="9738">
                result = False
                </st><strong class="bold"><st c="9754">if role == 0:</st></strong><strong class="bold"><st c="9767">repo = AdminRepository(sess)</st></strong><st c="9796">
                    admin = Administrator(**profile_dict)
                    result = await repo.insert_admin(admin)
                </st><strong class="bold"><st c="9875">elif role == 1:</st></strong><strong class="bold"><st c="9890">repo = DoctorRepository(sess)</st></strong><st c="9920">
                    doc = Doctor(**profile_dict)
                    result = await repo.insert_doctor(doc)
                </st><strong class="bold"><st c="9989">elif role == 2:</st></strong><strong class="bold"><st c="10004">repo = PatientRepository(sess)</st></strong><st c="10035">
                    patient = Patient(**profile_dict)
                    result = await repo.insert_patient(patient)
                … … … … … …
                … … … … … …
    return </st><code><st c="10335">add_user_profile_task_wrapper()</st></code><st c="10366">, the </st><code><st c="10372">details</st></code><st c="10379"> parameter pertains to the returned value of </st><code><st c="10424">add_user_login_task_wrapper()</st></code><st c="10453">. The first parameter will always receive the result of the preceding tasks. </st><st c="10530">Now, the </st><code><st c="10539">add_user_profile_task_wrapper()</st></code><st c="10570"> task will check the role to determine what table to insert the profile information in. </st><st c="10658">Then, it will return the </st><em class="italic"><st c="10683">username</st></em><st c="10691"> as input to the final task, </st><code><st c="10720">show_complete_login_task_wrapper()</st></code><st c="10754">, which will render the </st><st c="10778">user credentials.</st></p>
			<p><st c="10795">The dynamic workflow must have strict exception handling from the inside of the tasks and from the outside of the Celery workflow execution to establish a continuous and blockage-free passing of results</st><a id="_idIndexMarker684"/><st c="10998"> or input from the initial task to </st><st c="11033">the end.</st></p>
			<p><st c="11041">On the other hand, running independent Celery tasks requires a different Celery primitive operation called </st><code><st c="11149">group()</st></code><st c="11156">. Let us now scrutinize some parallel tasks from </st><st c="11205">our application.</st></p>
			<h2 id="_idParaDest-222"><a id="_idTextAnchor226"/><st c="11221">Running independent and parallel tasks</st></h2>
			<p><st c="11260">The </st><code><st c="11265">group()</st></code><st c="11272"> primitive can run tasks</st><a id="_idIndexMarker685"/><st c="11296"> concurrently and even return consolidated results from functional tasks. </st><st c="11370">Our sample grouped workflow, shown in </st><em class="italic"><st c="11408">Figure 8</st></em><em class="italic"><st c="11416">.2</st></em><st c="11418">, focuses only on void tasks that serialize a list of records to CSV files, so no consolidation of results </st><st c="11525">is needed:</st></p>
			<div><div><img src="img/B19383_08_002.jpg" alt="Figure 8.2 – Task signatures in grouped workflow"/><st c="11535"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="11670">Figure 8.2 – Task signatures in grouped workflow</st></p>
			<p><st c="11718">The </st><code><st c="11723">group()</st></code><st c="11730"> operation can accept varying Celery tasks with different arguments but prefers those that </st><em class="italic"><st c="11821">read from and write to files</st></em><st c="11849">, </st><em class="italic"><st c="11851">perform database transactions</st></em><st c="11880">, </st><em class="italic"><st c="11882">extract resources from API endpoints</st></em><st c="11918">, </st><em class="italic"><st c="11920">download files from external storages</st></em><st c="11957">, or </st><em class="italic"><st c="11962">perform any I/O operations</st></em><st c="11988">. Our </st><code><st c="11994">create_reports()</st></code><st c="12010"> endpoint function performs the grouped workflow presented in </st><em class="italic"><st c="12072">Figure 8</st></em><em class="italic"><st c="12080">.2</st></em><st c="12082">, which aims to back up the list of user administrators, patients, and doctors to their respective CSV files. </st><st c="12192">The following is the code of the </st><st c="12225">endpoint </st><a id="_idIndexMarker686"/><st c="12234">function:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="12243">from modules.admin.services.reports_tasks import generate_csv_admin_task_wrapper,</st></strong> <strong class="bold"><st c="12325">generate_csv_doctor_task_wrapper,</st></strong> <strong class="bold"><st c="12359">generate_csv_patient_task_wrapper</st></strong>
<strong class="bold"><st c="12393">@admin_bp.get('/admin/reports/create')</st></strong><st c="12432">
async def create_reports():
    admin_csv_filename = os.getcwd() + "/files/dams_admin.csv"
    patient_csv_filename = os.getcwd() + "/files/dams_patient.csv"
    doctor_csv_filename = os.getcwd() + "/files/dams_doc.csv"
    </st><strong class="bold"><st c="12641">workflow = group(</st></strong><strong class="bold"><st c="12658">generate_csv_admin_task_wrapper.s(admin_csv_filename),</st></strong> <strong class="bold"><st c="12713">generate_csv_doctor_task_wrapper.s(</st></strong><strong class="bold"><st c="12749">doctor_csv_filename),</st></strong> <strong class="bold"><st c="12771">generate_csv_patient_task_wrapper.s(</st></strong><strong class="bold"><st c="12808">patient_csv_filename))()</st></strong><strong class="bold"><st c="12833">workflow.get()</st></strong><st c="12848">
    return jsonify(message="done backup"), 201</st></pre>			<p><st c="12891">The </st><code><st c="12896">create_reports()</st></code><st c="12912"> endpoint passes different filenames to the three tasks. </st><st c="12969">The </st><code><st c="12973">generate_csv_admin_task_wrapper()</st></code><st c="13006">method will back up all administrator records to </st><code><st c="13056">dams_admin.csv</st></code><st c="13070">, </st><code><st c="13072">generate_csv_patient_task_wrapper()</st></code><st c="13107"> will dump all patient records to </st><code><st c="13141">dams_patient.csv</st></code><st c="13157">, and </st><code><st c="13163">generate_csv_doctor_task_wrapper()</st></code><st c="13197"> will save all doctor profiles to </st><code><st c="13231">dams_doctor.csv</st></code><st c="13246">. All three will concurrently execute after running the </st><code><st c="13302">group()</st></code><st c="13309"> operation.</st></p>
			<p><st c="13320">But if the concern is to manage all the results of these concurrently running tasks, the </st><code><st c="13410">chord()</st></code><st c="13417"> workflow operation, as shown in the next section, will be the best option for </st><st c="13496">this scenario.</st></p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor227"/><st c="13510">Using callbacks to manage task results</st></h2>
			<p><st c="13549">The </st><code><st c="13554">chord()</st></code><st c="13561"> primitive works like the </st><code><st c="13587">group()</st></code><st c="13594"> operation except for its callback task requirement, which will handle and</st><a id="_idIndexMarker687"/><st c="13668"> manage all results of the independent tasks. </st><st c="13714">The following API endpoint generates a report on a doctor’s appointments and </st><st c="13791">laboratory requests:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="13811">from modules.admin.services.doctor_stats_tasks import count_patients_doctor_task_wrapper, count_request_doctor_task_wrapper, create_doctor_stats_task_wrapper</st></strong>
<strong class="bold"><st c="13969">@admin_bp.get('/admin/doc/stats')</st></strong><st c="14003">
async def derive_doctor_stats():
    docid = request.args.get("docid")
    </st><strong class="bold"><st c="14071">workflow =</st></strong> <strong class="bold"><st c="14081">chord((count_patients_doctor_task_wrapper.s(docid), count_request_doctor_task_wrapper.s(docid)), create_doctor_stats_task_wrapper.s(docid))()</st></strong><strong class="bold"><st c="14223">result = workflow.get()</st></strong><st c="14247">
    return jsonify(message=result), 201</st></pre>			<p><st c="14283">The</st><code><st c="14287"> derive_doctor_stats()</st></code><st c="14309"> method aims to execute the workflow shown in </st><em class="italic"><st c="14355">Figure 8</st></em><em class="italic"><st c="14363">.3</st></em><st c="14365">, which uses the </st><code><st c="14382">chord()</st></code><st c="14389"> operation to run </st><code><st c="14407">count_patients_doctor_task_wrapper()</st></code><st c="14443"> to determine the number of patients of a particular doctor and </st><code><st c="14507">count_request_doctor_task_wrapper()</st></code><st c="14542"> to extract the total number of laboratory requests of the same doctor. </st><st c="14614">The results of the tasks are stored in a list according to the order of their executions before passing it to the callback task, </st><code><st c="14743">create_doctor_stats_task_wrapper()</st></code><st c="14777">, for processing. </st><st c="14795">Unlike in the </st><code><st c="14809">group()</st></code><st c="14816"> primitive, the results are managed by a callback task before returning the final result to the </st><st c="14912">API function:</st></p>
			<div><div><img src="img/B19383_08_003.jpg" alt="Figure 8.3 – Task signatures in chord() primitive"/><st c="14925"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="15032">Figure 8.3 – Task signatures in chord() primitive</st></p>
			<p><st c="15081">A sample output of the </st><code><st c="15105">create_doctor_stats_task_wrapper()</st></code><st c="15139"> task will be like this: “</st><em class="italic"><st c="15165">Doctor HSP-200 has 2 patients and 0 </st></em><em class="italic"><st c="15202">lab requests.</st></em><st c="15215">”</st></p>
			<p><st c="15217">There are lots of ways to build complex dynamic workflows using combinations of </st><code><st c="15297">chain()</st></code><st c="15304">, </st><code><st c="15306">group()</st></code><st c="15313">, and </st><code><st c="15319">chord()</st></code><st c="15326">, which will implement the workflows that the Flask applications need to optimize some business processes. </st><st c="15433">It is possible for a chained task to call the </st><code><st c="15479">group()</st></code><st c="15486"> primitive from the inside to spawn and run a group of independent tasks. </st><st c="15560">It is also feasible to use Celery’s </st><em class="italic"><st c="15596">subtasks </st></em><st c="15605">to implement conditional task executions. </st><st c="15647">There are also miscellaneous primitives such as </st><code><st c="15695">map()</st></code><st c="15700">, </st><code><st c="15702">starmap()</st></code><st c="15711">, and </st><code><st c="15717">chunks()</st></code><st c="15725"> that can manage</st><a id="_idIndexMarker688"/><st c="15741"> arguments of tasks in the workflow. </st><st c="15778">A Celery workflow is flexible and open to any implementation using its primitives and signatures since it targets dynamic workflows. </st><st c="15911">Celery workflows can read and execute workflows from XML files, such as BPMN workflows. </st><st c="15999">However, there is a workflow solution that can work on both dynamic and BPMN </st><st c="16076">workflows: SpiffWorkflow.</st></p>
			<h1 id="_idParaDest-224"><a id="_idTextAnchor228"/><st c="16101">Creating BPMN and non-BPMN workflows with SpiffWorkflow</st></h1>
			<p><strong class="bold"><st c="16157">SpiffWorkflow</st></strong><st c="16171"> is a flexible Python execution engine for workflow activities. </st><st c="16235">Its latest installment focuses more on BPMN models, but it always</st><a id="_idIndexMarker689"/><st c="16300"> has strong support classes to build and run non-BPMN</st><a id="_idIndexMarker690"/><st c="16353"> workflows translated into Python and JSON. </st><st c="16397">The library has a </st><em class="italic"><st c="16415">BPMN interpreter</st></em><st c="16431"> that can execute tasks indicated in</st><a id="_idIndexMarker691"/><st c="16467"> BPMN diagrams created by BPMN modeling tools and </st><em class="italic"><st c="16517">serializers</st></em><st c="16528"> to run </st><st c="16536">JSON-based workflows.</st></p>
			<p><st c="16557">To start SpiffWorkflow, we need to install some </st><st c="16606">required dependencies.</st></p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor229"/><st c="16628">Setting up the development environment</st></h2>
			<p><st c="16667">No broker or server is needed to run </st><a id="_idIndexMarker692"/><st c="16705">workflows with SpiffWorkflow. </st><st c="16735">However, installing the main plugin using the </st><code><st c="16781">pip</st></code><st c="16784"> command is </st><st c="16796">a requirement:</st></p>
			<pre class="console"><st c="16810">
pip install spiffworkflow</st></pre>			<p><st c="16836">Then, for serialization and parsing purposes, install the </st><code><st c="16895">lxml</st></code><st c="16899"> dependency:</st></p>
			<pre class="console"><st c="16911">
pip install lxml</st></pre>			<p><st c="16928">Since SpiffWorkflow uses the Celery client library for legacy support, install the </st><code><st c="17012">celery</st></code><st c="17018"> module:</st></p>
			<pre class="console"><st c="17026">
pip install celery</st></pre>			<p><st c="17045">Now, download and install a BPMN modeler tool that can provide BPMN diagrams supported by SpiffWorkflow. </st><st c="17151">This chapter uses the </st><em class="italic"><st c="17173">Camunda Modeler for Camunda 7 BPMN</st></em><st c="17207"> version to generate BPMN diagrams, which we can download from </st><a href="https://camunda.com/download/modeler/"><st c="17270">https://camunda.com/download/modeler/</st></a><st c="17307">. </st><em class="italic"><st c="17309">Figure 8</st></em><em class="italic"><st c="17317">.4</st></em><st c="17319"> provides a screenshot of the Camunda Modeler with a sample </st><st c="17379">BPMN diagram:</st></p>
			<div><div><img src="img/B19383_08_004.jpg" alt="Figure 8.4 – Camunda Modeler with BPMN model for Camunda 7"/><st c="17392"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="17627">Figure 8.4 – Camunda Modeler with BPMN model for Camunda 7</st></p>
			<p><st c="17685">The version of SpiffWorkflow used by this chapter can only parse and execute the BPMN model for the Camunda 7 platform. </st><st c="17806">Hopefully, its future releases can support Camunda 8 or higher versions of </st><st c="17881">BPMN diagrams.</st></p>
			<p><st c="17895">Let us now create our workflow using the BPMN </st><st c="17942">modeler tool.</st></p>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor230"/><st c="17955">Creating a BPMN diagram</st></h2>
			<p><st c="17979">BPMN is an open standard for business</st><a id="_idIndexMarker693"/><st c="18017"> process diagrams. </st><st c="18036">It is a graphical mechanism to visualize and simulate a systematic set of activities in one process flow that goals a successful result. </st><st c="18173">A BPMN diagram has a set of graphical elements, called </st><em class="italic"><st c="18228">flow objects</st></em><st c="18240">, composed of </st><em class="italic"><st c="18254">activities</st></em><st c="18264">, </st><em class="italic"><st c="18266">events</st></em><st c="18272">, </st><em class="italic"><st c="18274">sequence flows</st></em><st c="18288">, </st><st c="18290">and </st><em class="italic"><st c="18294">gateways</st></em><st c="18302">.</st></p>
			<p><st c="18303">An activity represents work that needs execution inside a workflow process. </st><st c="18380">A work can be simple and atomic, such as a </st><em class="italic"><st c="18423">task</st></em><st c="18427">, or complex, such as a </st><em class="italic"><st c="18451">sub-process</st></em><st c="18462">. When an activity is atomic and cannot break down further, despite the complexity of the process, then that is considered a task. </st><st c="18593">A task in BPMN is denoted as a </st><em class="italic"><st c="18624">rounded-corner rectangle shape</st></em><st c="18654"> component. </st><st c="18666">There are several types of tasks, but SpiffWorkflow only supports </st><st c="18732">the following:</st></p>
			<ul>
				<li><strong class="bold"><st c="18746">Manual task</st></strong><st c="18758"> – A non-automated task that a human can perform outside the context of </st><st c="18830">the workflow.</st></li>
				<li><strong class="bold"><st c="18843">Script task</st></strong><st c="18855"> – A task that runs a </st><st c="18877">modeler-defined script.</st></li>
				<li><strong class="bold"><st c="18900">User task</st></strong><st c="18910"> – A typical task that a human actor can carry out using some application-related operation, such as clicking </st><st c="19020">a button.</st></li>
			</ul>
			<p><st c="19029">The tasks presented in the BPMN diagram of </st><em class="italic"><st c="19073">Figure 8</st></em><em class="italic"><st c="19081">.4</st></em><st c="19083">, namely </st><strong class="bold"><st c="19092">Doctor’s Specialization Form</st></strong><st c="19120">, </st><strong class="bold"><st c="19122">List Specialized Doctors</st></strong><st c="19146">, </st><strong class="bold"><st c="19148">Doctor’s Availability Form</st></strong><st c="19174">, and </st><strong class="bold"><st c="19180">Patient Detail Form</st></strong><st c="19199">, are </st><em class="italic"><st c="19205">user tasks</st></em><st c="19215">. Usually, user tasks can represent actions such as web form handling, console-based transactions with user inputs, or transactions in applications involving editing and submitting form data. </st><st c="19407">On the other hand, the </st><strong class="bold"><st c="19430">Evaluate Form Data</st></strong><st c="19448"> and </st><strong class="bold"><st c="19453">Finalize Schedule</st></strong><st c="19470"> tasks are considered </st><em class="italic"><st c="19492">script tasks</st></em><st c="19504">.</st></p>
			<p><st c="19505">A </st><em class="italic"><st c="19508">sequence flow</st></em><st c="19521"> is a one-directional line connector between activities or tasks. </st><st c="19587">The BPMN standard allows adding descriptions or labels to sequence flows to determine which paths to take from one activity </st><st c="19711">to another.</st></p>
			<p><st c="19722">Now, the workflow will not work without </st><em class="italic"><st c="19763">start</st></em><st c="19768"> and </st><em class="italic"><st c="19773">stop events</st></em><st c="19784">. An </st><em class="italic"><st c="19789">event</st></em><st c="19794"> is an occurrence along the workflow required to execute due to some triggers to produce some result. </st><st c="19896">The start event, represented by a </st><em class="italic"><st c="19930">small and open circle with a thin-lined boundary</st></em><st c="19978">, triggers the start of the workflow. </st><st c="20016">The stop event, defined by a </st><em class="italic"><st c="20045">small, open circle with a single thick-lined boundary</st></em><st c="20098">, ends the workflow activities. </st><st c="20130">Other than these two, there are </st><em class="italic"><st c="20162">cancel</st></em><st c="20168">, </st><em class="italic"><st c="20170">signal</st></em><st c="20176">, </st><em class="italic"><st c="20178">error</st></em><st c="20183">, </st><em class="italic"><st c="20185">message</st></em><st c="20192">, </st><em class="italic"><st c="20194">timer</st></em><st c="20199">, and </st><em class="italic"><st c="20205">escalation</st></em><st c="20215"> events supported by SpiffWorkflow, and all these are represented </st><st c="20281">as circles.</st></p>
			<p><st c="20292">The </st><em class="italic"><st c="20297">diamond-shaped component</st></em><st c="20321"> in </st><em class="italic"><st c="20325">Figure 8</st></em><em class="italic"><st c="20333">.4</st></em><st c="20335"> is a </st><em class="italic"><st c="20341">gateway</st></em><st c="20348"> component. </st><st c="20360">It diverges or converges its incoming or outgoing process flows. </st><st c="20425">It can control multiple incoming and multiple outgoing process flows. </st><st c="20495">SpiffWorkflow supports the following types </st><st c="20538">of gateways:</st></p>
			<ul>
				<li><strong class="bold"><st c="20550">Exclusive gateway</st></strong><st c="20568"> – Caters to multiple incoming flows and will emit only one output flow based on </st><st c="20649">some evaluation.</st></li>
				<li><strong class="bold"><st c="20665">Parallel gateway</st></strong><st c="20682"> – Emits an independent </st><a id="_idIndexMarker694"/><st c="20706">process flow that will execute tasks without order but will wait for all the tasks </st><st c="20789">to finish.</st></li>
				<li><strong class="bold"><st c="20799">Event gateway</st></strong><st c="20813"> – Emits an outgoing flow based on some events from an </st><st c="20868">outside source.</st></li>
				<li><strong class="bold"><st c="20883">Inclusive gateway</st></strong><st c="20901"> – Caters to multiple incoming flows and can emit more than one output flow based on some </st><st c="20991">complex evaluation.</st></li>
			</ul>
			<p><st c="21010">The gateway in </st><em class="italic"><st c="21026">Figure 8</st></em><em class="italic"><st c="21034">.4</st></em><st c="21036"> is an example of an exclusive gateway because it will allow </st><strong class="bold"><st c="21097">the Finalize Schedule</st></strong><st c="21118"> task execution to proceed if, and only if, the form data is complete. </st><st c="21189">Otherwise, it will redirect the sequence flow to the </st><strong class="bold"><st c="21242">Doctor’s Specialization Form</st></strong><st c="21270"> web form task again for </st><st c="21295">data re-entry.</st></p>
			<p><st c="21309">Now, let us start the showcase on how SpiffWorkflow can interpret a BPMN diagram for </st><strong class="bold"><st c="21395">business process </st></strong><strong class="bold"><st c="21412">management</st></strong><st c="21422"> (</st><strong class="bold"><st c="21424">BPM</st></strong><st c="21427">).</st></p>
			<h2 id="_idParaDest-227"><a id="_idTextAnchor231"/><st c="21430">Implementing the BPMN workflow</st></h2>
			<p><st c="21461">SpiffWorkflow can translate mainly the </st><em class="italic"><st c="21501">user</st></em><st c="21505">, </st><em class="italic"><st c="21507">manual</st></em><st c="21513">, and </st><em class="italic"><st c="21519">script</st></em><st c="21525"> tasks of a BPMN diagram. </st><st c="21551">So, it can best </st><a id="_idIndexMarker695"/><st c="21567">handle business process optimization involving sophisticated web flows in a </st><st c="21643">web application.</st></p>
			<p><st c="21659">Since there is nothing to configure in the </st><code><st c="21703">create_app()</st></code><st c="21715"> factory or </st><code><st c="21727">main.py</st></code><st c="21734"> module for SpiffWorkflow, the next step after dependency module installations and the BPMN diagram design is the view function implementation for the BPMN diagram simulation. </st><st c="21910">The view functions must initiate and execute SpiffWorkflow tasks to run the entire </st><st c="21993">BPMN workflow.</st></p>
			<p><st c="22007">The first support class to call in the module script is </st><code><st c="22064">CamundaParser</st></code><st c="22077">, a support class found in the </st><code><st c="22108">SpiffWorkflow.camunda.parser.CamundaParser</st></code><st c="22150"> module of SpiffWorkflow. </st><st c="22176">The </st><code><st c="22180">CamundaParser</st></code><st c="22193"> class will parse the BPMN tags of the BPMN file based on the Camunda 7 standards. </st><st c="22276">The BPMN file is an XML document with tags corresponding to the </st><em class="italic"><st c="22340">flow objects</st></em><st c="22352"> of the workflow. </st><st c="22370">Now, the </st><code><st c="22379">CamundaParser</st></code><st c="22392"> class will need the name or ID of the BPMN definition to load the document and verify if the XML schema of the BPMN document is </st><a id="_idIndexMarker696"/><st c="22521">well formed and valid. </st><st c="22544">The following is the first portion of the </st><code><st c="22586">/view/appointment.py</st></code><st c="22606"> module of the </st><code><st c="22621">doctor</st></code><st c="22627"> Blueprint module that instantiates the </st><code><st c="22667">CamundaParser</st></code><st c="22680"> class that will load our </st><code><st c="22706">dams_appointment.bpmn</st></code><st c="22727"> file, the workflow design depicted in the BPMN workflow diagram of </st><em class="italic"><st c="22795">Figure 8</st></em><em class="italic"><st c="22803">.4</st></em><st c="22805">:</st></p>
			<pre class="source-code"><st c="22807">
from SpiffWorkflow.bpmn.workflow import BpmnWorkflow
from SpiffWorkflow.camunda.parser.CamundaParser import CamundaParser
from SpiffWorkflow.bpmn.specs.defaults import ScriptTask
from SpiffWorkflow.camunda.specs.user_task import UserTask
from SpiffWorkflow.task import Task, TaskState
from SpiffWorkflow.util.deep_merge import DeepMerge
</st><strong class="bold"><st c="23145">parser = CamundaParser()</st></strong>
<strong class="bold"><st c="23169">filepath = os.path.join("bpmn/dams_appointment.bpmn")</st></strong>
<strong class="bold"><st c="23223">parser.add_bpmn_file(filepath)</st></strong>
<code><st c="23304">add_bpmn_file()</st></code><st c="23319"> function of the API will load the BPMN file, while the </st><code><st c="23375">get_spec()</st></code><st c="23385"> function will parse the document starting with the process definition ID call. </st><st c="23465">Now, </st><em class="italic"><st c="23470">Figure 8</st></em><em class="italic"><st c="23478">.5</st></em><st c="23480"> shows a snapshot of the BPMN file with the process </st><st c="23532">definition ID:</st></p>
			<div><div><img src="img/B19383_08_005.jpg" alt="Figure 8.5 – A snapshot of a BPMN file containing the process definition ID"/><st c="23546"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="24570">Figure 8.5 – A snapshot of a BPMN file containing the process definition ID</st></p>
			<p><st c="24645">After activating SpiffWorkflow with its parser, the next step is to build web flows through the view functions. </st><st c="24758">The</st><a id="_idIndexMarker697"/><st c="24761"> view implementations will be a series of page redirections that will gather all the necessary form data values for the </st><em class="italic"><st c="24881">user tasks</st></em><st c="24891"> of the BPMN workflow. </st><st c="24914">The following </st><code><st c="24928">choose_specialization()</st></code><st c="24951"> view will be the first web form to start since it will simulate the </st><strong class="bold"><st c="25020">Doctor’s Specialization </st></strong><strong class="bold"><st c="25044">Form</st></strong><st c="25048"> task:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="25054">@doc_bp.route("/doctor/expertise",</st></strong><strong class="bold"><st c="25089">methods = ["GET", "POST"])</st></strong>
<strong class="bold"><st c="25116">async def choose_specialization():</st></strong><st c="25151">
    if request.method == "GET":
      return render_template("doc_specialization_form.html")
    session['specialization'] = request.form['specialization']
    </st><code><st c="25379">select_doctor()</st></code><st c="25394"> to list all doctors with the specialization indicated by </st><code><st c="25452">choose_specialization()</st></code><st c="25475">. The following snippet presents the code for the </st><code><st c="25525">select_doctor()</st></code><st c="25540"> view:</st></p>
			<pre class="source-code"><st c="25546">
@doc_bp.route("/doctor/select", methods = ["GET", "POST"])
async def select_doctor():
    if request.method == "GET":
        return render_template("doc_doctors_form.html")
    session['docid'] = request.form['docid']
    return redirect(url_for("doc_bp.reserve_schedule") )</st></pre>			<p><st c="25802">After the </st><code><st c="25813">select_doctor()</st></code><st c="25828"> view, the user will choose a date and time for the appointment through the </st><code><st c="25904">reserve_schedule()</st></code><st c="25922"> view. </st><st c="25929">The last view of the web flow is </st><code><st c="25962">provide_patient_details()</st></code><st c="25987">, which</st><a id="_idIndexMarker698"/><st c="25994"> will ask for the patient details needed for the diagnosis and payment. </st><st c="26066">The following code presents the implementation of the </st><code><st c="26120">reserve_schedule()</st></code><st c="26138"> view:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="26144">@doc_bp.route("/doctor/schedule",</st></strong><strong class="bold"><st c="26178">methods = ["GET", "POST"])</st></strong>
<strong class="bold"><st c="26205">async def reserve_schedule():</st></strong><st c="26235">
    if request.method == "GET":
      </st><strong class="bold"><st c="26264">return render_template("doc_schedule_form.html"), 201</st></strong><st c="26317">
    session['appt_date'] = request.form['appt_date']
    session['appt_time'] = request.form['appt_time']
    </st><code><st c="26505">provide_patient_details()</st></code><st c="26530">, will trigger the workflow execution besides its goal to extract the patient information required for the appointment scheduling and consolidate it with the other details from the previous views. </st><st c="26727">The following is the code for the </st><code><st c="26761">provide_patient_details()</st></code><st c="26786"> view:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="26792">from SpiffWorkflow.bpmn.workflow import BpmnWorkflow</st></strong>
<strong class="bold"><st c="26845">from SpiffWorkflow.camunda.parser.CamundaParser import CamundaParser</st></strong>
<strong class="bold"><st c="26914">from SpiffWorkflow.bpmn.specs.bpmn_task_spec import TaskSpec</st></strong>
<strong class="bold"><st c="26975">from SpiffWorkflow.camunda.specs.user_task import UserTask</st></strong>
<strong class="bold"><st c="27034">from SpiffWorkflow.task import Task, TaskState</st></strong>
<strong class="bold"><st c="27081">@doc_bp.route("/doctor/patient", methods = ["GET", "POST"])</st></strong>
<strong class="bold"><st c="27141">async def provide_patient_details():</st></strong><st c="27178">
    if request.method == "GET":
       </st><strong class="bold"><st c="27207">return render_template("doc_patient_form.html"), 201</st></strong><strong class="bold"><st c="27259">form_data = dict()</st></strong><st c="27278">
    form_data['specialization'] = </st><strong class="bold"><st c="27309">session['specialization']</st></strong><st c="27334">
    form_data['docid'] = </st><strong class="bold"><st c="27356">session['docid']</st></strong><st c="27372">
    form_data['appt_date'] = </st><strong class="bold"><st c="27398">session['appt_date']</st></strong><st c="27418">
    form_data['appt_time'] = </st><strong class="bold"><st c="27444">session['appt_time']</st></strong><st c="27464">
    form_data['ticketid'] = </st><strong class="bold"><st c="27489">request.form['ticketid']</st></strong><st c="27513">
    form_data['patientid'] = </st><strong class="bold"><st c="27539">request.form['patientid']</st></strong><st c="27564">
    form_data['priority_level'] = </st><strong class="bold"><st c="27595">request.form['priority_level']</st></strong><strong class="bold"><st c="27625">workflow = BpmnWorkflow(spec)</st></strong><strong class="bold"><st c="27655">workflow.do_engine_steps()</st></strong><st c="27682">
    ready_tasks: List[Task] = </st><strong class="bold"><st c="27709">workflow.get_tasks(TaskState.READY)</st></strong><st c="27744">
    while len(ready_tasks) &gt; 0:
        for task in ready_tasks:
            if isinstance(</st><strong class="bold"><st c="27812">task.task_spec</st></strong><st c="27827">, UserTask):
                upload_login_form_data(task, form_data)
            workflow.run_task_from_id(task_id=</st><strong class="bold"><st c="27914">task.id</st></strong><st c="27922">)
         else:
            task_details:TaskSpec = </st><strong class="bold"><st c="27955">task.task_spec</st></strong><st c="27969">
            print("Complete Task ", </st><strong class="bold"><st c="27994">task_details.name</st></strong><st c="28011">)
        </st><strong class="bold"><st c="28014">workflow.do_engine_steps()</st></strong><st c="28040">
        ready_tasks = </st><strong class="bold"><st c="28055">workflow.get_tasks(TaskState.READY)</st></strong><strong class="bold"><st c="28090">dashboard_page = workflow.data['finalize_sched']</st></strong><st c="28139">
    if dashboard_page:
      return render_template("doc_dashboard.html"), 201
    else:
      return redirect(url_for("doc_bp.choose_specialization"))</st></pre>			<p><em class="italic"><st c="28271">Session handling</st></em><st c="28288"> provides the </st><code><st c="28302">provide_patient_details()</st></code><st c="28327"> view with the ability to gather all appointment details from the previous web views. </st><st c="28413">As depicted in the given code, all session data, including</st><a id="_idIndexMarker699"/><st c="28471"> the patient details from its form, are placed in its </st><code><st c="28525">form_data</st></code><st c="28534"> dictionary. </st><st c="28547">Utilizing the session is a workaround because it is not feasible to fuse the workflow loops required by the SpiffWorkflow library and the web flows. </st><st c="28696">The last redirected page must initiate the workflow with the </st><code><st c="28757">BpmnWorkflow</st></code><st c="28769"> class. </st><st c="28777">But what is the difference between the </st><code><st c="28816">CamundaParser</st></code><st c="28829"> and </st><code><st c="28834">BpmnWorkflow</st></code><st c="28846"> API classes? </st><st c="28860">We answer this question in the </st><st c="28891">next section.</st></p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor232"/><st c="28904">Distinguishing between workflow specifications and instances</st></h2>
			<p><st c="28965">There are two categories of components in SpiffWorkflow: </st><em class="italic"><st c="29023">specification</st></em><st c="29036"> and </st><em class="italic"><st c="29041">instance</st></em><st c="29049"> objects. </st><code><st c="29059">CamundaParser</st></code><st c="29072">, through </st><a id="_idIndexMarker700"/><st c="29082">its </st><code><st c="29086">get_spec()</st></code><st c="29096"> method, returns a </st><code><st c="29115">WorkflowSpec</st></code><st c="29127"> instance object, a specification or model object that defines the BPMN workflow. </st><st c="29209">On the other hand, </st><code><st c="29228">BpmnWorkflow</st></code><st c="29240"> creates a </st><code><st c="29251">Workflow</st></code><st c="29259"> instance object, which tracks and returns actual workflow activities. </st><st c="29330">However, </st><code><st c="29339">BpmnWorkflow</st></code><st c="29351"> requires the workflow specification object as its constructor parameter </st><st c="29424">before instantiation.</st></p>
			<p><st c="29445">The workflow instance will provide all sequence flows from the start until the stop event and the tasks with their corresponding state. </st><st c="29582">All the states, such as </st><code><st c="29606">READY</st></code><st c="29611">, </st><code><st c="29613">CANCELLED</st></code><st c="29622">, </st><code><st c="29624">COMPLETED</st></code><st c="29633">, and </st><code><st c="29639">FUTURE</st></code><st c="29645">, are indicated in the </st><code><st c="29668">TaskState</st></code><st c="29677"> API coupled with hook methods found in the </st><code><st c="29721">Task</st></code><st c="29725"> instance object. </st><st c="29743">But how does SpiffWorkflow determine a BPMN task? </st><st c="29793">We will see that in the </st><st c="29817">next section.</st></p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor233"/><st c="29830">Identifying between task specifications and instances</st></h2>
			<p><st c="29884">As with the workflow, each SpiffWorkflow task has a specification object called </st><code><st c="29965">TaskSpec</st></code><st c="29973">, which provides details such as the </st><em class="italic"><st c="30010">name of the task definition</st></em><st c="30037"> and </st><em class="italic"><st c="30042">task type</st></em><st c="30051">, such as </st><code><st c="30061">UserTask</st></code><st c="30069"> or </st><code><st c="30073">ScriptTask</st></code><st c="30083">. On the other hand, the task instance object is named </st><code><st c="30138">Task</st></code><st c="30142">. The workflow instance</st><a id="_idIndexMarker701"/><st c="30165"> object provides </st><code><st c="30182">get_tasks()</st></code><st c="30193"> overrides that give all tasks based on a specific state or </st><code><st c="30253">TaskSpec</st></code><st c="30261"> instance. </st><st c="30272">Moreover, it has </st><code><st c="30289">get_task_from_id()</st></code><st c="30307"> to extract the </st><code><st c="30323">Task</st></code><st c="30327"> instance object based on </st><em class="italic"><st c="30353">task ID</st></em><st c="30360">, </st><code><st c="30362">get_task_spec_from_name()</st></code><st c="30387"> to retrieve the </st><code><st c="30404">TaskSpec</st></code><st c="30412"> name based on its indicated BPMN name, and </st><code><st c="30456">get_tasks_from_spec_name()</st></code><st c="30482"> to retrieve all tasks based on a </st><code><st c="30516">TaskSpec</st></code> <st c="30524">definition name.</st></p>
			<p><st c="30541">To traverse and track every </st><code><st c="30570">UserTask</st></code><st c="30578">, </st><code><st c="30580">ManualTask</st></code><st c="30590">, or </st><code><st c="30595">Gateway</st></code><st c="30602"> task and their trailing </st><code><st c="30627">ScriptTask</st></code><st c="30637"> task(s) based on the BPMN diagram starting from </st><code><st c="30686">StartEvent</st></code><st c="30696">, invoke the </st><code><st c="30709">do_engine_steps()</st></code><st c="30726"> of the workflow instance. </st><st c="30753">A loop must call the </st><code><st c="30774">do_engine_steps()</st></code><st c="30791"> method to track every activity in the workflow, including events and </st><code><st c="30861">ScriptTask</st></code><st c="30871"> tasks until it reaches </st><code><st c="30895">EndEvent</st></code><st c="30903">. Thus,  </st><code><st c="30911">provide_patient_details()</st></code><st c="30936"> has a </st><code><st c="30943">while</st></code><st c="30948"> loop in the </st><code><st c="30961">POST</st></code><st c="30965"> transaction to traverse the workflow and execute every </st><code><st c="31021">Task</st></code><st c="31025"> object with the </st><code><st c="31042">run_task_from_id()</st></code><st c="31060"> method of the </st><st c="31075">workflow instance.</st></p>
			<p><st c="31093">But running tasks, specifically </st><code><st c="31126">UserTask</st></code><st c="31134"> and </st><code><st c="31139">ScriptTask</st></code><st c="31149">, is not only concerned with the fulfillment of the workflow activity but also the passing of some </st><st c="31248">task data.</st></p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor234"/><st c="31258">Passing form data to UserTask</st></h2>
			<p><code><st c="31288">UserTask</st></code><st c="31297">’s form fields are among the several sources of BPMN workflow data. </st><st c="31366">The Camunda Modeler allows the BPMN </st><a id="_idIndexMarker702"/><st c="31402">designer to create form variables for each </st><code><st c="31445">UserTask</st></code><st c="31453"> task. </st><em class="italic"><st c="31460">Figure 8</st></em><em class="italic"><st c="31468">.6</st></em><st c="31470"> shows the three form fields, namely </st><code><st c="31507">patientid</st></code><st c="31516">, </st><code><st c="31518">ticketid</st></code><st c="31526">, and </st><code><st c="31532">priority_level</st></code><st c="31546">, of the </st><strong class="bold"><st c="31555">Patient Detail Form</st></strong><st c="31574"> task and the portion of the Camunda Modeler where to add </st><st c="31632">form variables:</st></p>
			<div><div><img src="img/B19383_08_006.jpg" alt="Figure 8.6 – Adding form fields to UserTask"/><st c="31647"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="31777">Figure 8.6 – Adding form fields to UserTask</st></p>
			<p><st c="31820">The presence of form fields in a custom-generated form requires data passing to these form variables through the view function. </st><st c="31949">Form fields without values will yield exceptions that can halt workflow </st><a id="_idIndexMarker703"/><st c="32021">executions, eventually ruining the Flask application. </st><st c="32075">The </st><code><st c="32079">while</st></code><st c="32084"> loop in the following code snippet of the </st><code><st c="32127">provide_patient_details()</st></code><st c="32152"> view calls an </st><code><st c="32167">upload_login_form_data()</st></code><st c="32191"> custom method that assigns values from the </st><code><st c="32235">form_data</st></code><st c="32244"> dictionary to each </st><code><st c="32264">UserTask</st></code> <st c="32272">form variable:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="32287">from SpiffWorkflow.util.deep_merge import DeepMerge</st></strong>
<strong class="bold"><st c="32339">def upload_login_form_data(task: UserTask, form_data):</st></strong><st c="32394">
    form = task.task_spec.form
    </st><strong class="bold"><st c="32422">data = {}</st></strong><st c="32431">
    if task.data is None:
        task.data = {}
    for field in form.fields:
        if field.id == "specialization":
            process_data = form_data["specialization"]
        elif field.id == "docid":
            process_data = form_data["docid"]
        elif field.id == "date_scheduled":
            process_data = form_data["appt_date"]
        … … … … … …
        </st><strong class="bold"><st c="32716">update_data(data, field.id,  process_data)</st></strong><strong class="bold"><st c="32757">DeepMerge.merge(task.data, data)</st></strong>
<strong class="bold"><st c="32790">@doc_bp.route("/doctor/patient", methods = ["GET", "POST"])</st></strong>
<strong class="bold"><st c="32850">async def provide_patient_details():</st></strong><st c="32887">
    … … … … … …
    while len(ready_tasks) &gt; 0:
        for task in ready_tasks:
            if isinstance(task.task_spec, UserTask):
                </st><strong class="bold"><st c="32993">upload_login_form_data(task, form_data)</st></strong><st c="33032">
            else:
                task_details:TaskSpec = task.task_spec
                print("Complete Task ", task_details.name)
            workflow.run_task_from_id(task_id=task.id)
        … … … … … …
        return redirect(url_for("doc_bp.choose_specialization"))</st></pre>			<p><st c="33232">The </st><code><st c="33237">upload_login_form_data()</st></code><st c="33261"> method determines each form field through its </st><em class="italic"><st c="33308">ID</st></em><st c="33310"> and extracts its appropriate </st><a id="_idIndexMarker704"/><st c="33340">value from the </st><code><st c="33355">form_data</st></code><st c="33364"> dictionary. </st><st c="33377">Then, the custom method, shown in the following snippet, assigns the value to the form field and uploads the field-value pair as </st><em class="italic"><st c="33506">workflow data</st></em><st c="33519"> using the </st><code><st c="33530">DeepMerge</st></code><st c="33539"> utility class </st><st c="33554">of SpiffWorkflow:</st></p>
			<pre class="source-code"><st c="33571">
def update_data(dct, name, value):
    path = name.split('.')
    current = dct
    for component in path[:-1]:
        if component not in current:
            current[component] = {}
        current = current[component]
    current[path[-1]] = value</st></pre>			<p><st c="33779">Technically, </st><code><st c="33793">update_data()</st></code><st c="33806"> creates a dictionary object containing the field name as the key and its corresponding </st><code><st c="33894">form_data</st></code><st c="33903"> value.</st></p>
			<p><st c="33910">But how about </st><code><st c="33925">ScriptTask</st></code><st c="33935">? Can it have</st><a id="_idIndexMarker705"/><st c="33948"> form variables, too? </st><st c="33970">Let’s explore that in the </st><st c="33996">next section.</st></p>
			<h2 id="_idParaDest-231"><a id="_idTextAnchor235"/><st c="34009">Adding input variables to ScriptTask</st></h2>
			<p><code><st c="34046">ScriptTask</st></code><st c="34057"> can also have input variables but not form fields. </st><st c="34109">These input variables also need values from the view function </st><a id="_idIndexMarker706"/><st c="34171">because these are essential parts of its expressions. </st><st c="34225">Sometimes, </st><code><st c="34236">ScriptTask</st></code><st c="34246"> does not need inputs from views because it can extract existing workflow data to build its conditional expression. </st><st c="34362">But for sure, it must emit output variable(s) that the succeeding </st><code><st c="34428">Gateway</st></code><st c="34435">, </st><code><st c="34437">ScriptTask</st></code><st c="34447">, or </st><code><st c="34452">UserTask</st></code><st c="34460"> task needs to pursue their execution. </st><em class="italic"><st c="34499">Figure 8</st></em><em class="italic"><st c="34507">.7</st></em><st c="34509"> shows the </st><code><st c="34553">proceed</st></code><st c="34560"> output variable and how it extracts and uses the profile information from the </st><st c="34639">workflow data:</st></p>
			<div><div><img src="img/B19383_08_007.jpg" alt="Figure 8.7 – Utilizing variables in ScriptTask"/><st c="34653"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="34909">Figure 8.7 – Utilizing variables in ScriptTask</st></p>
			<p><st c="34955">After running all the tasks and uploading all the values to the different variables in the workflow, the result of the</st><a id="_idIndexMarker707"/><st c="35074"> workflow must be variables that will decide the result of the view function; in our case, the </st><code><st c="35169">provide_patient_details()</st></code><st c="35194"> view. </st><st c="35201">Let us now retrieve these results to determine the type of responses our views </st><st c="35280">will render.</st></p>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor236"/><st c="35292">Managing the result of the workflow</st></h2>
			<p><st c="35328">The goal of our workflow through SpiffWorkflow is to determine the view page a route function will render. </st><st c="35436">Together with this is the execution of the required backend transactions, such as saving the </st><a id="_idIndexMarker708"/><st c="35529">scheduled appointment into the database, sending notifications to the doctors for the newly created appointment, and generating the necessary documents for the schedule. </st><st c="35699">The workflow’s generated data will determine the resulting processes of the view. </st><st c="35781">In our appointment workflow, when the generated </st><code><st c="35829">finalize_sched</st></code><st c="35843"> variable is </st><code><st c="35856">True</st></code><st c="35860">, the view will redirect the user to the doctor’s dashboard page. </st><st c="35926">Otherwise, the user will see the first page of the </st><st c="35977">data-gathering process.</st></p>
			<p><st c="36000">Let us now explore the capability of SpiffWorkflow to implement </st><st c="36065">non-BPMN workflows.</st></p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor237"/><st c="36084">Implementing a non-BPMN workflow</st></h2>
			<p><st c="36117">SpiffWorkflow can implement workflows in JSON or Python configurations. </st><st c="36190">In our </st><code><st c="36197">ch08-spiff-web</st></code><st c="36211"> project, we have the</st><a id="_idIndexMarker709"/><st c="36232"> following Python class that implements a prototype of a payment </st><st c="36297">process workflow:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="36314">from SpiffWorkflow.specs.WorkflowSpec import WorkflowSpec</st></strong>
<strong class="bold"><st c="36372">from SpiffWorkflow.specs.ExclusiveChoice import</st></strong> <strong class="bold"><st c="36420">ExclusiveChoice</st></strong>
<strong class="bold"><st c="36436">from SpiffWorkflow.specs.Simple import Simple</st></strong>
<strong class="bold"><st c="36482">from SpiffWorkflow.operators import Equal, Attrib</st></strong>
<strong class="bold"><st c="36532">class PaymentWorkflowSpec(WorkflowSpec):</st></strong><st c="36573">
    def __init__(self):
        super().__init__()
        </st><strong class="bold"><st c="36613">patient_pay = Simple(wf_spec=self, name='dams_patient_pay')</st></strong><st c="36672">
        patient_pay.ready_event.connect(  callback=tx_patient_pay)
        self.start.connect(taskspec=patient_pay)
        </st><strong class="bold"><st c="36772">payment_verify = ExclusiveChoice(wf_spec=self, name='payment_check')</st></strong><st c="36840">
        patient_pay.connect(taskspec=payment_verify)
        patient_release = Simple(wf_spec=self, name='dams_patient_release')
        cond = Equal(Attrib(name='amount'), Attrib(name='charge'))
        </st><strong class="bold"><st c="37013">payment_verify.connect_if(condition=cond,</st></strong> <strong class="bold"><st c="37054">task_spec=patient_release)</st></strong><st c="37081">
        patient_release.completed_event.connect( callback=tx_patient_release)
        patient_hold = Simple(wf_spec=self, name='dams_patient_onhold')
        payment_verify.connect(task_spec=patient_hold)
        </st><code><st c="37328">WorkflowSpec</st></code><st c="37341"> is responsible for the non-BPMN workflow implementation in Python format. </st><st c="37416">The constructor of the </st><code><st c="37439">WorkflowSpec</st></code><st c="37451"> sub-class creates generic, simple, and atomic tasks using the </st><code><st c="37514">Simple</st></code><st c="37520"> API of the </st><code><st c="37532">SpiffWorkflow.specs.Simple</st></code><st c="37558"> module. </st><st c="37567">The task can have more than one input and any number of output task variables. </st><st c="37646">There is also an </st><code><st c="37663">ExclusiveChoice</st></code><st c="37678"> sub-class that works like a gateway for </st><st c="37719">the workflow.</st></p>
			<p><st c="37732">Moreover, each task has a </st><code><st c="37759">connect()</st></code><st c="37768"> method to</st><a id="_idIndexMarker710"/><st c="37778"> establish sequence flows. </st><st c="37805">It also has event variables, such as </st><code><st c="37842">ready_event</st></code><st c="37853">, </st><code><st c="37855">cancelled_event</st></code><st c="37870">, </st><code><st c="37872">completed_event</st></code><st c="37887">, and </st><code><st c="37893">reached_event</st></code><st c="37906">, that run their respective callback method, such as our </st><code><st c="37963">tx_patient_pay()</st></code><st c="37979">, </st><code><st c="37981">tx_patient_release()</st></code><st c="38001">, and </st><code><st c="38007">tx_patient_onhold()</st></code><st c="38026"> methods. </st><st c="38036">Calling these event objects marks a transition from one task’s current state </st><st c="38113">to another.</st></p>
			<p><st c="38124">The </st><code><st c="38129">Attrib</st></code><st c="38135"> helper class recognizes a task variable and retrieves its data for comparison performed by internal API classes, such as </st><code><st c="38257">Equal</st></code><st c="38262">, </st><code><st c="38264">NotEqual</st></code><st c="38272">, and </st><code><st c="38278">LessThan</st></code><st c="38286">, of the </st><code><st c="38295">SpiffWorkflow.operators</st></code><st c="38318"> module.</st></p>
			<p><st c="38326">Let us now run our </st><code><st c="38346">PaymentWorkflowSpec</st></code><st c="38365"> workflow using a </st><st c="38383">view function.</st></p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor238"/><st c="38397">Running a non-BPMN workflow</st></h2>
			<p><st c="38425">Since this is not a Camunda-based workflow, running</st><a id="_idIndexMarker711"/><st c="38477"> the workflow does not need a parser. </st><st c="38515">Immediately wrap and instantiate the custom </st><code><st c="38559">WorkflowSpec</st></code><st c="38571"> sub-class inside the </st><code><st c="38593">Workflow</st></code><st c="38601"> class and call </st><code><st c="38617">get_tasks()</st></code><st c="38628"> inside the view function to prepare the non-BPMN workflow for the task traversal and executions. </st><st c="38726">But the following </st><code><st c="38744">start_payment_form()</st></code><st c="38764"> function opts for individual access of tasks using the workflow instance’s </st><code><st c="38840">get_tasks_from_spec_name()</st></code><st c="38866"> function instead of using a </st><code><st c="38895">while</st></code><st c="38900"> loop for </st><st c="38910">task traversal:</st></p>
			<pre class="source-code"><st c="38925">
@payment_bp.route("/payment/start", methods = ["GET", "POST"])
async def start_payment_form():
    if request.method == "GET":
        return render_template("payment_form.html"), 201
    … … … … … …
    </st><code><st c="39220">Task</st></code><st c="39224"> list will start </st><st c="39241">the workflow:</st></p>
			<pre class="source-code"><st c="39254">
    start_tasks: list[Task] = workflow_instance.get_tasks_from_spec_name( name='Start')
    for task in start_tasks:
        if task.state == TaskState.READY:
            workflow_instance.run_task_from_id( task_id=task.id)</st></pre>			<p><st c="39450">This </st><code><st c="39456">Task</st></code><st c="39460"> list will load all payment data to the workflow and execute the </st><code><st c="39525">tx_patient_pay()</st></code><st c="39541"> callback method</st><a id="_idIndexMarker712"/><st c="39557"> to process </st><st c="39569">payment transactions:</st></p>
			<pre class="source-code"><st c="39590">
    patient_pay_task: list[Task] = workflow_instance.get_tasks_from_spec_name( name='dams_patient_pay')
    for task in patient_pay_task:
        if task.state == TaskState.READY:
            task.set_data(ticketid=ticketid, patientid=patientid, charge=charge, amount=amount, discount=discount, status=status, date_released=date_released)
            workflow_instance.run_task_from_id(  task_id=task.id)</st></pre>			<p><st c="39954">This part of the workflow will execute the </st><code><st c="39998">ExclusiveChoice</st></code><st c="40013"> event to compare the payment amount paid by the </st><a id="_idIndexMarker713"/><st c="40062">patient against the patient’s </st><st c="40092">total charges:</st></p>
			<pre class="source-code"><st c="40106">
    payment_check_task: list[Task] = workflow_instance.get_tasks_from_spec_name( name='payment_check')
    for task in payment_check_task:
        if task.state == TaskState.READY:
            workflow_instance.run_task_from_id( task_id=task.id)</st></pre>			<p><st c="40324">If the patient fully paid the charges, the following tasks will execute the </st><code><st c="40401">tx_patient_release()</st></code><st c="40421"> callback method to clear and issue release notifications to </st><st c="40482">the patient:</st></p>
			<pre class="source-code"><st c="40494">
    for_releasing = False
    patient_release_task: list[Task] = workflow_instance.get_tasks_from_spec_name( name='dams_patient_release')
    for task in patient_release_task:
        if task.state == TaskState.READY:
            for_releasing = True
            workflow_instance.run_task_from_id( task_id=task.id)</st></pre>			<p><st c="40766">If the patient has partially paid the charges, the following tasks will execute the </st><code><st c="40851">tx_patient_onhold()</st></code> <st c="40870">callback method:</st></p>
			<pre class="source-code"><st c="40887">
    patient_onhold_task: list[Task] = workflow_instance.get_tasks_from_spec_name( name='dams_patient_onhold')
    for task in patient_onhold_task:
        if task.state == TaskState.READY:
            workflow_instance.run_task_from_id( task_id=task.id)
    if for_releasing == True:
        return redirect(url_for('payment_bp.release_patient'), code=307)
    else:
       return redirect(url_for('payment_bp.hold_patient'), code=307)</st></pre>			<p><st c="41272">The result of the workflow will decide on what page the view will redirect the user to, whether the </st><em class="italic"><st c="41373">releasing</st></em><st c="41382"> or </st><em class="italic"><st c="41386">on-hold</st></em><st c="41393"> page.</st></p>
			<p><st c="41399">Now, SpiffWorkflow will lessen the coding effort in building the workflow because it has defined API classes that support both BPMN and non-BPMN workflow implementation. </st><st c="41570">But what if the need is to trigger</st><a id="_idIndexMarker714"/><st c="41604"> workflows through API endpoints that SpiffWorkflow can </st><st c="41660">hardly handle?</st></p>
			<p><st c="41674">The next topic will focus on using a BPMN workflow engine that the Camunda platform uses in running tasks through </st><st c="41789">API endpoints.</st></p>
			<h1 id="_idParaDest-235"><a id="_idTextAnchor239"/><st c="41803">Building service tasks with the Zeebe/Camunda platforms</st></h1>
			<p><strong class="bold"><st c="41859">Camunda</st></strong><st c="41867"> is a popular lightweight workflow and decision automation engine with built-in powerful tools, such as the </st><em class="italic"><st c="41975">Camunda Modeler</st></em><st c="41990">, </st><em class="italic"><st c="41992">Cawemo</st></em><st c="41998">, and the </st><em class="italic"><st c="42008">Zeebe</st></em><st c="42013"> broker. </st><st c="42022">But this chapter is not about Camunda</st><a id="_idIndexMarker715"/><st c="42059"> but about using Camunda’s </st><em class="italic"><st c="42086">Zeebe server</st></em><st c="42098"> to deploy, run, and execute workflow tasks built by the </st><a id="_idIndexMarker716"/><st c="42155">Flask framework. </st><st c="42172">The goal is to create a Flask client application that will deploy and run BPMN workflows designed by the Camunda Modeler using the Zeebe </st><st c="42309">workflow engine.</st></p>
			<p><st c="42325">Let us start with the setup and configurations needed to integrate Flask with the </st><st c="42408">Zeebe server.</st></p>
			<h2 id="_idParaDest-236"><a id="_idTextAnchor240"/><st c="42421">Setting up the Zeebe server</st></h2>
			<p><st c="42449">The easiest way to</st><a id="_idIndexMarker717"/><st c="42468"> run the Zeebe server is to use Docker to run its </st><code><st c="42518">camunda/zeebe</st></code><st c="42531"> image. </st><st c="42539">So, read first the updated </st><em class="italic"><st c="42566">Docker Subscription Service Agreement</st></em><st c="42603"> before downloading and installing </st><a id="_idIndexMarker718"/><st c="42638">Docker Desktop, available </st><st c="42664">from </st><a href="https://docs.docker.com/desktop/install/windows-install/"><st c="42669">https://docs.docker.com/desktop/install/windows-install/</st></a><st c="42725">.</st></p>
			<p><st c="42726">After the installation, start the Docker engine, open a terminal, and run the following </st><st c="42815">Docker command:</st></p>
			<pre class="console"><st c="42830">
docker run --name zeebe --rm -p 26500-26502:26500-26502 -d --network=ch08-network camunda/zeebe:latest</st></pre>			<p><st c="42933">A </st><em class="italic"><st c="42936">Docker network</st></em><st c="42950">, as with our </st><code><st c="42964">ch08-network</st></code><st c="42976">, is needed to expose the ports to the development platform. </st><st c="43037">Zeebe’s port </st><code><st c="43050">26500</st></code><st c="43055"> is where the Flask client application will communicate to the server’s gateway API. </st><st c="43140">After using Zeebe, run the </st><code><st c="43167">docker stop</st></code><st c="43178"> command with </st><em class="italic"><st c="43192">Zeebe’s container ID</st></em><st c="43212"> to shut down </st><st c="43226">the broker.</st></p>
			<p><st c="43237">Now, the next step is to install the suitable Python Zeebe client for </st><st c="43308">the application.</st></p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor241"/><st c="43324">Installing the pyzeebe library</st></h2>
			<p><st c="43355">Lots of effective and popular Zeebe client libraries </st><a id="_idIndexMarker719"/><st c="43409">are Java-based. </st><st c="43425">However, </st><code><st c="43434">pyzeebe</st></code><st c="43441"> is one of the few Python external modules that are simple, easy to use, lightweight, and effective in establishing connectivity </st><a id="_idIndexMarker720"/><st c="43570">to the Zeebe server. </st><st c="43591">It is a </st><em class="italic"><st c="43599">gRPC</st></em><st c="43603">-based client library for Zeebe, typically designed to manage workflows that involve </st><st c="43689">RESTful services.</st></p>
			<p class="callout-heading"><st c="43706">Important note</st></p>
			<p class="callout"><st c="43721">gRPC is a flexible and high-performance RPC framework that can run in any environment and easily connect to any cluster, with support for access authentication, API health checking, load balancing, and open tracing. </st><st c="43938">All Zeebe client libraries use gRPC to communicate with </st><st c="43994">the server.</st></p>
			<p><st c="44005">Let us now install the </st><code><st c="44029">pyzeebe</st></code><st c="44036"> library using the </st><code><st c="44055">pip</st></code><st c="44058"> command:</st></p>
			<pre class="console"><st c="44067">
pip install pyzeebe</st></pre>			<p><st c="44087">After the installation and </st><a id="_idIndexMarker721"/><st c="44115">setup, it is time to create a BPMN workflow diagram using the </st><st c="44177">Camunda Modeler.</st></p>
			<h2 id="_idParaDest-238"><a id="_idTextAnchor242"/><st c="44193">Creating a BPMN diagram for pyzeebe</st></h2>
			<p><st c="44229">The </st><code><st c="44234">pyzeebe</st></code><st c="44241"> module can</st><a id="_idIndexMarker722"/><st c="44252"> load and parse BPMN files used by </st><em class="italic"><st c="44287">Camunda version 8.0</st></em><st c="44306">. Since it is a small library, it can only read and execute </st><code><st c="44366">ServiceTask</st></code><st c="44377"> tasks. </st><em class="italic"><st c="44385">Figure 8</st></em><em class="italic"><st c="44393">.8</st></em><st c="44395"> shows a BPMN diagram with two </st><code><st c="44426">ServiceTask</st></code><st c="44437"> tasks:  the </st><strong class="bold"><st c="44449">Get Diagnostics</st></strong><st c="44464"> task, which</st><a id="_idIndexMarker723"/><st c="44476"> retrieves all patients’ diagnoses, and the </st><strong class="bold"><st c="44520">Get Analysis</st></strong><st c="44532"> task, which returns the doctor’s resolutions or prescriptions to </st><st c="44598">the diagnoses:</st></p>
			<div><div><img src="img/B19383_08_008.jpg" alt="Figure 8.8 – A BPMN diagram with two ServiceTask tasks"/><st c="44612"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="44706">Figure 8.8 – A BPMN diagram with two ServiceTask tasks</st></p>
			<p><st c="44760">The next step is to load and run the final BPMN document using the </st><code><st c="44828">pyzeebe</st></code><st c="44835"> client library. </st><st c="44852">Running the workflow activities from the BPMN diagram is impossible without a </st><code><st c="44930">pyzeebe</st></code> <em class="italic"><st c="44937">worker</st></em><st c="44944"> and </st><em class="italic"><st c="44949">client</st></em><st c="44955">. But implementation of the worker must </st><st c="44995">come first.</st></p>
			<h2 id="_idParaDest-239"><a id="_idTextAnchor243"/><st c="45006">Creating a pyzeebe worker</st></h2>
			<p><st c="45032">A </st><code><st c="45035">pyzeebe</st></code><st c="45042"> worker or a </st><code><st c="45055">ZeebeWorker</st></code><st c="45066"> worker is a</st><a id="_idIndexMarker724"/><st c="45078"> typical Zeebe worker that handles all </st><code><st c="45117">ServiceTask</st></code><st c="45128"> tasks. </st><st c="45136">It runs asynchronously in the background using </st><code><st c="45183">asyncio</st></code><st c="45190">. </st><code><st c="45192">pyzeebe</st></code><st c="45199">, as an asynchronous library, prefers a </st><code><st c="45239">Flask[async]</st></code><st c="45251"> platform with </st><code><st c="45266">asyncio</st></code><st c="45273"> utilities. </st><st c="45285">But it requires </st><code><st c="45301">grpc.aio.Channel</st></code><st c="45317"> as a constructor </st><a id="_idIndexMarker725"/><st c="45335">parameter </st><st c="45345">before instantiation.</st></p>
			<p><st c="45366">The library provides three methods to create the needed channel, namely  </st><code><st c="45439">create_insecure_channel()</st></code><st c="45464">, </st><code><st c="45466">create_secure_channel()</st></code><st c="45489">, and </st><code><st c="45495">create_camunda_cloud_channel()</st></code><st c="45525">. All three instantiate the channel, but w </st><code><st c="45568">create_insecure_channel()</st></code><st c="45593"> disregards the TLS protocol, and </st><code><st c="45627">create_camunda_cloud_channel()</st></code><st c="45657"> considers the connection to the Camunda cloud. </st><st c="45705">Our </st><code><st c="45709">ch08-zeebe</st></code><st c="45719"> application uses the insecure one to instantiate the </st><code><st c="45773">ZeebeWorker</st></code><st c="45784"> worker and</st><a id="_idIndexMarker726"/><st c="45795"> eventually manage the </st><code><st c="45818">ServiceTask</st></code><st c="45829"> tasks indicated in our BPMN file. </st><st c="45864">The following </st><code><st c="45878">worker-tasks</st></code><st c="45890"> module script shows an independent Python application that contains the </st><code><st c="45963">ZeebeWorker</st></code><st c="45974"> instantiation and its tasks </st><st c="46003">or jobs:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="46011">from pyzeebe import ZeebeWorker, create_insecure_channel</st></strong><st c="46068">
import asyncio
from modules.models.config import db_session, init_db
from modules.doctors.repository.diagnosis import DiagnosisRepository
print('starting the Zeebe worker...')
print('initialize database connectivity...')
init_db()
channel = create_insecure_channel()
</st><code><st c="46479">ZeebeWorker</st></code><st c="46490"> worker with its constructor parameters. </st><st c="46531">The </st><code><st c="46535">initdb()</st></code><st c="46543"> call is included in the module because our tasks will need </st><st c="46603">CRUD</st><a id="_idIndexMarker727"/><st c="46607"> transactions:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="46621">@worker.task(task_type="select_diagnosis",</st></strong> <strong class="bold"><st c="46664">**Zeebe.TASK_DEFAULT_PARAMS)</st></strong>
<strong class="bold"><st c="46693">async def select_diagnosis(docid, patientid):</st></strong><st c="46739">
    async with db_session() as sess:
         async with sess.begin():
            try:
               repo = DiagnosisRepository(sess)
               records = await repo.   </st><st c="46858">select_diag_doc_patient(docid, patientid)
               diagnosis_rec = [rec.to_json() for rec in records]
               diagnosis_str = json.dumps(diagnosis_rec, default=json_date_serializer)
               return {"data": diagnosis_str}
            except Exception as e:
               print(e)
            return {"data": json.dumps([])}</st></pre>			<p><st c="47117">The </st><code><st c="47122">select_diagnosis()</st></code><st c="47140"> method is a </st><code><st c="47153">pyzeebe</st></code><st c="47160"> worker decorated with the </st><code><st c="47187">@worker.task()</st></code><st c="47201"> annotation. </st><st c="47214">The </st><code><st c="47218">task_type</st></code><st c="47227"> attribute of the </st><code><st c="47245">@worker.task()</st></code><st c="47259"> annotation indicates its </st><code><st c="47285">ServiceTask</st></code><st c="47296"> name in the</st><a id="_idIndexMarker728"/><st c="47308"> BPMN model. </st><st c="47321">The decorator can also include other attributes, such as </st><code><st c="47378">exception_handler</st></code><st c="47395"> and </st><code><st c="47400">timeout_ms</st></code><st c="47410">. Now, </st><code><st c="47417">select_diagnosis()</st></code><st c="47435"> looks for all patients’ diagnoses from the database with </st><code><st c="47493">docid </st></code><st c="47499">and </st><code><st c="47503">patientid</st></code><st c="47512"> parameters as filters to the search. </st><st c="47550">It returns a dictionary with a key named </st><code><st c="47591">data</st></code><st c="47595"> handling </st><st c="47605">the result:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="47616">@worker.task(task_type="retrieve_analysis", **Zeebe.TASK_DEFAULT_PARAMS)</st></strong>
<strong class="bold"><st c="47689">async def retrieve_analysis(records):</st></strong><st c="47727">
   try:
      records_diagnosis = json.loads(records)
      diagnosis_text = [dt['resolution'] for dt in records_diagnosis]
      return {"result": diagnosis_text}
   except Exception as e:
      print(e)
   return {"result": []}</st></pre>			<p><st c="47924">On the other hand, this </st><code><st c="47949">retrieve_analysis()</st></code><st c="47968"> task takes </st><code><st c="47980">records</st></code><st c="47987"> from </st><code><st c="47993">select_diagnosis()</st></code><st c="48011"> in string form but is serialized back to the list form with </st><code><st c="48072">json.loads()</st></code><st c="48084">. This task will extract</st><a id="_idIndexMarker729"/><st c="48108"> only all resolutions from the patients’ records</st><a id="_idIndexMarker730"/><st c="48156"> and return them to the caller. </st><st c="48188">The task returns a </st><st c="48207">dictionary also.</st></p>
			<p><st c="48223">The </st><em class="italic"><st c="48228">local parameter names</st></em><st c="48249"> and the </st><em class="italic"><st c="48258">dictionary keys</st></em><st c="48273"> returned by the worker’s tasks must be </st><em class="italic"><st c="48313">BPMN variable names</st></em><st c="48332"> because the client will also fetch these local parameters to assign values and dictionary keys for the output extraction for the preceding </st><code><st c="48472">ServiceTask</st></code><st c="48483"> task.</st></p>
			<p><st c="48489">Since our Flask client application uses its event loop, our worker must run on a separate event loop using </st><code><st c="48597">asyncio</st></code><st c="48604"> to avoid exceptions. </st><st c="48626">The following </st><code><st c="48640">worker_tasks.py</st></code><st c="48655"> snippet shows how to run the worker on an </st><code><st c="48698">asyncio</st></code><st c="48705"> environment:</st></p>
			<pre class="source-code"><st c="48718">
if __name__ == "__main__":
   </st><code><st c="48821">ZeebeWorker</st></code><st c="48832"> instance has a </st><code><st c="48848">work()</st></code><st c="48854"> coroutine that must be running asynchronously at the back using an independent event, disconnected from Flask operations. </st><st c="48977">Always run the module with the Python command, such as </st><code><st c="49032">python worker-tasks.py</st></code><st c="49054">.</st></p>
			<p><st c="49055">Let us now implement the </st><code><st c="49081">pyzeebe</st></code><st c="49088"> client.</st></p>
			<h2 id="_idParaDest-240"><a id="_idTextAnchor244"/><st c="49096">Implementing the pyzeebe client</st></h2>
			<p><st c="49128">The Flask application needs to</st><a id="_idIndexMarker731"/><st c="49159"> instantiate the </st><code><st c="49176">ZeebeClient</st></code><st c="49187"> class to connect to Zeebe. </st><st c="49215">As with the </st><code><st c="49227">ZeebeWorker</st></code><st c="49238">, it also requires the same </st><code><st c="49266">grpc.aio.Channel</st></code><st c="49282"> parameter as a constructor parameter before its instantiation. </st><st c="49346">Since </st><code><st c="49352">ZeebeClient</st></code><st c="49363"> behaves asynchronously like </st><code><st c="49392">ZeebeWorker</st></code><st c="49403">, all its operations must run asynchronously in the background as Celery tasks. </st><st c="49483">But, unlike the worker, </st><code><st c="49507">ZeebeClient</st></code><st c="49518"> appears in every Blueprint </st><a id="_idIndexMarker732"/><st c="49546">module as part of its Celery service</st><a id="_idIndexMarker733"/><st c="49582"> tasks. </st><st c="49590">The following is the </st><code><st c="49611">diagnosis_tasks</st></code><st c="49626"> module script of the </st><em class="italic"><st c="49648">doctor</st></em><st c="49654"> Blueprint module that instantiates </st><code><st c="49690">ZeebeClient</st></code><st c="49701"> with the </st><st c="49711">Celery tasks:</st></p>
			<pre class="source-code"><st c="49724">
from celery import shared_task
import asyncio
</st><strong class="bold"><st c="49771">from pyzeebe import ZeebeClient, create_insecure_channel</st></strong><st c="49827">
channel = create_insecure_channel(hostname="localhost", port=26500)
</st><code><st c="49959">ZeebeClient</st></code><st c="49970"> instance. </st><st c="49981">The port to connect the Zeebe client </st><st c="50018">is </st><code><st c="50021">26500</st></code><st c="50026">:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="50028">@shared_task(ignore_result=False)</st></strong>
<strong class="bold"><st c="50061">def deploy_zeebe_wf(bpmn_file):</st></strong><st c="50093">
    async def zeebe_wf(bpmn_file):
        try:
            </st><strong class="bold"><st c="50130">await client.deploy_process(bpmn_file)</st></strong><st c="50168">
            return True
        except Exception as e:
            print(e)
        return False
    </st><code><st c="50314">deploy_zeebe_wf()</st></code><st c="50331"> task is the first process to run before anything else. </st><st c="50387">The API endpoint calling this will load, parse, and deploy the BPMN file with the workflow to the Zeebe server using the asynchronous </st><code><st c="50521">deploy_process()</st></code><st c="50537"> method of </st><code><st c="50548">ZeebeClient</st></code><st c="50559">. The task will throw</st><a id="_idIndexMarker734"/><st c="50580"> an exception if the BPMN file has schema</st><a id="_idIndexMarker735"/><st c="50621"> problems, is not well formed, or </st><st c="50655">is invalid:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="50666">@shared_task(ignore_result=False)</st></strong>
<strong class="bold"><st c="50700">def run_zeebe_task(docid, patientid):</st></strong><st c="50738">
    async def zeebe_task(docid, patientid):
        try:
            process_instance_key, result = await </st><strong class="bold"><st c="50821">client.run_process_with_result(</st></strong><strong class="bold"><st c="50852">bpmn_process_id</st></strong><st c="50868">= "Process_Diagnostics", </st><strong class="bold"><st c="50894">variables</st></strong><st c="50903">={"</st><strong class="bold"><st c="50907">docid</st></strong><st c="50913">": docid, "</st><strong class="bold"><st c="50925">patientid</st></strong><st c="50935">":patientid}, variables_to_fetch =["</st><strong class="bold"><st c="50972">result</st></strong><st c="50979">"], timeout=10000)
            return result
        except Exception as e:
            print(e)
            return {}
    </st><code><st c="51147">ZeebeClient</st></code><st c="51159"> has two asynchronous methods that can execute process definitions in the BPMN file, and these are </st><code><st c="51258">run_process()</st></code><st c="51271"> and </st><code><st c="51276">run_process_with_result()</st></code><st c="51301">. Both methods pass values to the first task of the workflow, but only </st><code><st c="51372">run_process_with_result()</st></code><st c="51397"> returns an output value. </st><st c="51423">The given </st><code><st c="51433">run_zeebe_task()</st></code><st c="51449"> method will execute the first </st><code><st c="51480">ServiceTask</st></code><st c="51491"> task, the worker’s </st><code><st c="51511">select_diagnosis()</st></code><st c="51529"> task, pass values to its </st><code><st c="51555">docid</st></code><st c="51560"> and </st><code><st c="51565">patientid</st></code><st c="51574"> parameters, and retrieve the dictionary output of the last </st><code><st c="51634">ServiceTask</st></code><st c="51645"> task, </st><code><st c="51652">retrieve_analysis()</st></code><st c="51671">, indicated by the </st><code><st c="51690">result</st></code><st c="51696"> key. </st><st c="51702">A </st><code><st c="51704">ServiceTask</st></code><st c="51715"> task’s parameters are considered BPMN variables that the BPMN file or the </st><code><st c="51790">ZeebeClient</st></code><st c="51801"> operations can fetch at any time. </st><st c="51836">Likewise, the key of the dictionary returned by </st><code><st c="51884">ServiceTask</st></code><st c="51895"> becomes a BPMN variable, too. </st><st c="51926">So, the </st><code><st c="51934">variables</st></code><st c="51943"> parameter of the </st><code><st c="51961">run_process_with_result()</st></code><st c="51986"> method fetches the local parameters of the first worker’s task, and its </st><code><st c="52059">variables_to_fetch</st></code><st c="52077"> property retrieves the returned dictionary of any </st><code><st c="52128">ServiceTask</st></code><st c="52139"> task indicated </st><a id="_idIndexMarker736"/><st c="52155">by the </st><st c="52162">key name.</st></p>
			<p><st c="52171">To enable the </st><code><st c="52186">ZeebeClient</st></code><st c="52197"> operations, run Celery and the Redis broker. </st><st c="52243">Let us now implement API endpoints that will simulate the </st><st c="52301">diagnosis workflow.</st></p>
			<h2 id="_idParaDest-241"><a id="_idTextAnchor245"/><st c="52320">Building API endpoints</st></h2>
			<p><st c="52343">The following API endpoint passes the</st><a id="_idIndexMarker737"/><st c="52381"> filename of the BPMN file to the </st><code><st c="52415">pyzeebe</st></code><st c="52422"> client by calling the </st><code><st c="52445">deploy_zeebe_wf()</st></code> <st c="52462">Celery task:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="52475">@doc_bp.get("/diagnosis/bpmn/deploy")</st></strong>
<strong class="bold"><st c="52513">async def deploy_diagnosis_analysis_bpmn():</st></strong><st c="52557">
    try:
        filepath = os.path.join(Zeebe.BPMN_DUMP_PATH, "</st><strong class="bold"><st c="52610">dams_diagnosis.bpmn</st></strong><st c="52630">")
        </st><strong class="bold"><st c="52634">task = deploy_zeebe_wf.apply_async(args=[filepath])</st></strong><strong class="bold"><st c="52685">result = task.get()</st></strong><st c="52705">
        return jsonify(data=result), 201
    except Exception as e:
            print(e)
    return jsonify(data="error"), 500</st></pre>			<p><st c="52804">Afterward, the following </st><code><st c="52830">extract_analysis_text()</st></code><st c="52853"> endpoint can run the workflow by calling the </st><code><st c="52899">run_zeebe_task()</st></code> <st c="52915">Celery task:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="52928">@doc_bp.post("/diagnosis/analysis/text")</st></strong>
<strong class="bold"><st c="52969">async def extract_analysis_text():</st></strong><st c="53004">
        try:
            data = request.get_json()
            docid = data['docid']
            patientid = int(data['patientid'])
            </st><strong class="bold"><st c="53093">task = run_zeebe_task.apply_async(args=[docid,</st></strong> <strong class="bold"><st c="53139">patientid])</st></strong><strong class="bold"><st c="53151">result = task.get()</st></strong><st c="53171">
            return jsonify(result), 201
        except Exception as e:
            print(e)
        return jsonify(data="error"), 500</st></pre>			<p><st c="53265">The given endpoint will also pass the </st><code><st c="53304">docid</st></code><st c="53309"> and </st><code><st c="53314">patientid</st></code><st c="53323"> values to the </st><st c="53338">client task.</st></p>
			<p><st c="53350">The </st><code><st c="53355">pyzeebe</st></code><st c="53362"> library has many limitations, such as supporting </st><code><st c="53412">UserTask</st></code><st c="53420"> and web flows and implementing workflows that</st><a id="_idIndexMarker738"/><st c="53466"> call API endpoints for results. </st><st c="53499">Although connecting our Flask application to the enterprise Camunda platform can address these problems with </st><code><st c="53608">pyzeebe</st></code><st c="53615">, it is a practical and clever approach to use the Airflow 2.x </st><st c="53678">platform instead.</st></p>
			<h1 id="_idParaDest-242"><a id="_idTextAnchor246"/><st c="53695">Using Airflow 2.x in orchestrating API endpoints</st></h1>
			<p><strong class="bold"><st c="53744">Airflow 2.x</st></strong><st c="53756"> is an open source platform that provides workflow authorization, monitoring, scheduling, and maintenance with its easy-to-use UI dashboard. </st><st c="53897">It can manage </st><strong class="bold"><st c="53911">extract, transform, load</st></strong><st c="53935"> (</st><strong class="bold"><st c="53937">ETL</st></strong><st c="53940">) workflows </st><a id="_idIndexMarker739"/><st c="53953">and </st><st c="53957">data analytics.</st></p>
			<p><st c="53972">Airflow uses Flask Blueprints internally and allows</st><a id="_idIndexMarker740"/><st c="54024"> customization just by adding custom Blueprints in its Airflow directory. </st><st c="54098">However, the main goal of this </st><a id="_idIndexMarker741"/><st c="54129">chapter is to use Airflow as an API orchestration tool to run sets of workflow activities that consume API services </st><st c="54245">for resources.</st></p>
			<p><st c="54259">Let us begin with the installation of the Airflow </st><st c="54310">2.x platform.</st></p>
			<h2 id="_idParaDest-243"><a id="_idTextAnchor247"/><st c="54323">Installing and configuring Airflow 2.x</st></h2>
			<p><st c="54362">There is no direct Airflow 2.x installation for</st><a id="_idIndexMarker742"/><st c="54410"> the Windows platform yet. </st><st c="54437">But there is a Docker image that can run Airflow on Windows and operating systems with low </st><a id="_idIndexMarker743"/><st c="54528">memory resources. </st><st c="54546">Our approach was to install Airflow directly on WSL2 (Ubuntu) through Windows PowerShell and also use Ubuntu to implement our Flask application for </st><st c="54694">this topic.</st></p>
			<p><st c="54705">Now, follow the </st><st c="54722">next procedures:</st></p>
			<ol>
				<li><st c="54738">For Windows users, run the </st><code><st c="54766">wsl</st></code><st c="54769"> command on PowerShell and log in to its home account using the </st><em class="italic"><st c="54833">WSL credentials</st></em><st c="54848">.</st></li>
				<li><st c="54849">Then, run the </st><code><st c="54864">cd ~</st></code><st c="54868"> Linux command to ensure all installations happen in the </st><st c="54925">home directory.</st></li>
				<li><st c="54940">After installing Python 11.x and all its required Ubuntu libraries, create a virtual environment (for example, </st><code><st c="55052">ch08-airflow-env</st></code><st c="55068">) using the </st><code><st c="55081">python3 -m venv</st></code><st c="55096"> command for the </st><code><st c="55113">airflow</st></code> <st c="55120">module installation.</st></li>
				<li><st c="55141">Activate the virtual environment by running the </st><code><st c="55190">source &lt;</st></code><code><st c="55198">venv_folder&gt;/bin/activate</st></code><st c="55224"> command.</st></li>
				<li><st c="55233">Next, find a directory in the system that can be the Airflow core directory where all Airflow configurations and customizations happen. </st><st c="55370">In our case, it is the </st><code><st c="55393">/</st></code><code><st c="55394">mnt/c/Alibata/Development/Server/Airflow</st></code><st c="55434"> folder.</st></li>
				<li><st c="55442">Open the </st><code><st c="55452">bashrc</st></code><st c="55458"> configuration file and add the </st><code><st c="55490">AIRFLOW_HOME</st></code><st c="55502"> variable with the Airflow core directory path. </st><st c="55550">The following is a sample of registering </st><st c="55591">the variable:</st><pre class="source-code">
<code><st c="55684">airflow</st></code><st c="55691"> module using the </st><code><st c="55709">pip</st></code><st c="55712"> command:</st><pre class="source-code"><st c="55721">
pip install apache-airflow</st></pre></li>				<li><st c="55748">Initialize its metadata database and generate configuration files in the </st><code><st c="55822">AIRFLOW_HOME</st></code><st c="55834"> directory using the </st><code><st c="55855">airflow db </st></code><code><st c="55866">migrate</st></code><st c="55873"> command.</st></li>
				<li><st c="55882">Create an administrator </st><a id="_idIndexMarker744"/><st c="55907">account for its UI dashboard using the </st><a id="_idIndexMarker745"/><st c="55946">following command: </st><code><st c="55965">airflow users create --username &lt;user&gt; --password &lt;pass&gt; --firstname &lt;fname&gt; --lastname &lt;lname&gt; --role Admin --email &lt;xxxx@yyyy.com&gt;</st></code><st c="56097">. The role value should </st><st c="56121">be </st><code><st c="56124">Admin</st></code><st c="56129">.</st></li>
				<li><st c="56130">Verify if the user account is added to its database using the </st><code><st c="56193">airflow users </st></code><code><st c="56207">list</st></code><st c="56211"> command.</st></li>
				<li><st c="56220">At this point, log in to the </st><em class="italic"><st c="56250">root account</st></em><st c="56262"> and activate the virtual environment using </st><code><st c="56306">root</st></code><st c="56310">. Run the scheduler using the </st><code><st c="56340">airflow </st></code><code><st c="56348">scheduler</st></code><st c="56357"> command.</st></li>
				<li><st c="56366">With the root account, run the server using the </st><code><st c="56415">airflow webserver --port 8080</st></code><st c="56444"> command. </st><st c="56454">Port </st><code><st c="56459">8080</st></code><st c="56463"> is its </st><st c="56471">default port.</st></li>
				<li><st c="56484">Lastly, access the Airflow portal at </st><code><st c="56522">http://localhost:8080</st></code><st c="56543"> and use your </st><code><st c="56557">Admin</st></code><st c="56562"> account to log in to </st><st c="56584">the dashboard.</st></li>
			</ol>
			<p><em class="italic"><st c="56598">Figure 8</st></em><em class="italic"><st c="56607">.9</st></em><st c="56609"> shows the home dashboard of </st><st c="56638">Airflow 2.x:</st></p>
			<div><div><img src="img/B19383_08_009.jpg" alt="Figure 8.9 – The home page of the Airflow 2.x UI"/><st c="56650"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="57451">Figure 8.9 – The home page of the Airflow 2.x UI</st></p>
			<p><st c="57499">An Airflow architecture is composed of the </st><st c="57543">following components:</st></p>
			<ul>
				<li><strong class="bold"><st c="57564">Web server</st></strong><st c="57575"> – Runs the UI management dashboard and executes and </st><st c="57628">monitors tasks.</st></li>
				<li><strong class="bold"><st c="57643">Scheduler</st></strong><st c="57653"> – Checks the status of tasks, updates tasks’ state details in the metadata database, and queues the next </st><a id="_idIndexMarker746"/><st c="57759">task </st><st c="57764">for executions.</st></li>
				<li><strong class="bold"><st c="57779">Metadata database</st></strong><st c="57797"> – Stores the states of a task, </st><strong class="bold"><st c="57829">cross-communications</st></strong><st c="57849"> (</st><strong class="bold"><st c="57851">XComs</st></strong><st c="57856">) data, and </st><strong class="bold"><st c="57869">directed acyclic graph</st></strong><st c="57891"> (</st><strong class="bold"><st c="57893">DAG</st></strong><st c="57896">) variables; processes perform read and write</st><a id="_idIndexMarker747"/><st c="57942"> operations in </st><st c="57957">this database.</st></li>
				<li><strong class="bold"><st c="57971">Executor</st></strong><st c="57980"> – Executes tasks and updates the </st><st c="58014">metadata database.</st></li>
			</ul>
			<p><st c="58032">Next, let us create </st><st c="58053">workflow tasks.</st></p>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor248"/><st c="58068">Creating tasks</st></h2>
			<p><st c="58083">Airflow uses DAG files to implement tasks and their sequence flows. </st><st c="58152">A DAG is a high-level design of the workflow and exclusive</st><a id="_idIndexMarker748"/><st c="58210"> tasks based on their task definitions, schedules, relationships, and dependencies. </st><st c="58294">Airflow provides the API classes that implement a DAG in Python code. </st><st c="58364">But, before creating DAG files, open the </st><code><st c="58405">AIRFLOW_HOME</st></code><st c="58417"> directory and create a </st><code><st c="58441">dags</st></code><st c="58445"> sub-folder inside it. </st><em class="italic"><st c="58468">Figure 8</st></em><em class="italic"><st c="58476">.10</st></em><st c="58479"> shows our Airflow core directory with the created </st><code><st c="58530">dags</st></code><st c="58534"> folder:</st></p>
			<div><div><img src="img/B19383_08_010.jpg" alt="Figure 8.10 – Custom dags folder in AIRFLOW_HOME"/><st c="58542"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="58764">Figure 8.10 – Custom dags folder in AIRFLOW_HOME</st></p>
			<p><st c="58812">One of the files in our </st><code><st c="58837">$AIRFLOW_HOME/dag</st></code><st c="58854"> directory is </st><code><st c="58868">report_login_count_dag.py</st></code><st c="58893">, which builds a sequence flow composed of two orchestrated API executions, each with </st><a id="_idIndexMarker749"/><st c="58979">service tasks. </st><em class="italic"><st c="58994">Figure 8</st></em><em class="italic"><st c="59002">.11</st></em><st c="59005"> provides an overview of the </st><st c="59034">workflow design:</st></p>
			<div><div><img src="img/B19383_08_011.jpg" alt="Figure 8.11 – An overview of an Airflow DAG"/><st c="59050"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="59172">Figure 8.11 – An overview of an Airflow DAG</st></p>
			<p><code><st c="59215">DAG</st></code><st c="59219"> is an API class from the </st><code><st c="59245">airflow</st></code><st c="59252"> module that implements an entire workflow activity. </st><st c="59305">It is composed of different </st><em class="italic"><st c="59333">operators</st></em><st c="59342"> that represent tasks. </st><st c="59365">A DAG file can implement more than one DAG if needed. </st><st c="59419">The following code is the </st><code><st c="59445">DAG</st></code><st c="59448"> script in the </st><code><st c="59463">report_login_count_dag.py</st></code><st c="59488"> file that implements the workflow depicted in </st><em class="italic"><st c="59535">Figure 8</st></em><em class="italic"><st c="59543">.11</st></em><st c="59546">:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="59548">from airflow import DAG</st></strong>
<strong class="bold"><st c="59571">from airflow.operators.python import PythonOperator</st></strong>
<strong class="bold"><st c="59623">from airflow.providers.http.operators.http import</st></strong> <strong class="bold"><st c="59673">SimpleHttpOperator</st></strong><st c="59692">
from datetime import datetime
</st><strong class="bold"><st c="59723">with DAG(dag_id="report_login_count",</st></strong><st c="59760">
    description="Report the number of login accounts",
    </st><strong class="bold"><st c="59812">start_date=datetime(2023, 12, 27),</st></strong><strong class="bold"><st c="59846">schedule_interval="0 12 * * *",</st></strong><st c="59878">
    ) as </st><code><st c="59918">dag_id</st></code><st c="59924"> value. </st><st c="59932">Aside from </st><code><st c="59943">description</st></code><st c="59954">, DAG has parameters, such as </st><code><st c="59984">start_date</st></code><st c="59994"> and </st><code><st c="59999">schedule_interval</st></code><st c="60016">, that work like a Cron (time) scheduler for the workflow. </st><st c="60075">The </st><code><st c="60079">schedule_interval</st></code><st c="60096"> parameter can have the </st><code><st c="60120">@hourly</st></code><st c="60127">, </st><code><st c="60129">@daily</st></code><st c="60135">, </st><code><st c="60137">@weekly</st></code><st c="60144">, </st><code><st c="60146">@monthly</st></code><st c="60154">, or </st><code><st c="60159">@yearly</st></code><st c="60166"> Cron preset options run periodically or a Cron-based expression, such as </st><code><st c="60240">*/15 * * * *</st></code><st c="60252">, that schedules the workflow to run every </st><em class="italic"><st c="60295">15 minutes</st></em><st c="60305">. Setting</st><a id="_idIndexMarker750"/><st c="60314"> the parameter to </st><code><st c="60332">None</st></code><st c="60336"> will disable the periodic execution, requiring a trigger to run </st><st c="60401">the tasks:</st></p>
			<pre class="source-code"><st c="60411">
    task1 = </st><strong class="bold"><st c="60420">SimpleHttpOperator</st></strong><st c="60438">(
        </st><strong class="bold"><st c="60441">task_id="list_all_login",</st></strong><strong class="bold"><st c="60466">method="GET",</st></strong><strong class="bold"><st c="60480">http_conn_id="packt_dag",</st></strong><strong class="bold"><st c="60506">endpoint="/ch08/login/list/all",</st></strong><st c="60539">
        headers={"Content-Type": "application/json"},
        </st><strong class="bold"><st c="60586">response_check=lambda response:</st></strong> <strong class="bold"><st c="60617">handle_response(response),</st></strong><strong class="bold"><st c="60644">dag=dag</st></strong><st c="60652">
    )
    task2 = </st><strong class="bold"><st c="60663">PythonOperator</st></strong><st c="60677">(
        </st><strong class="bold"><st c="60680">task_id='count_login',</st></strong><strong class="bold"><st c="60702">python_callable=count_login,</st></strong><strong class="bold"><st c="60731">provide_context=True,</st></strong><strong class="bold"><st c="60753">do_xcom_push=True,</st></strong><strong class="bold"><st c="60772">dag=dag</st></strong><st c="60780">
    )</st></pre>			<p><st c="60782">An </st><em class="italic"><st c="60785">Airflow operator</st></em><st c="60801"> implements a task. </st><st c="60821">But, there are many types of operators to choose from depending on what kind </st><a id="_idIndexMarker751"/><st c="60898">of task the DAG requires. </st><st c="60924">Some widely used operators in training and workplaces are </st><st c="60982">the following:</st></p>
			<ul>
				<li><code><st c="60996">EmptyOperator</st></code><st c="61010"> – Initiates a </st><st c="61025">built-in execution.</st></li>
				<li><code><st c="61044">PythonOperator</st></code><st c="61059"> – Calls a Python function that implements a </st><st c="61104">business logic.</st></li>
				<li><code><st c="61119">BashOperator</st></code><st c="61132"> – Aims to run </st><code><st c="61147">bash</st></code><st c="61151"> commands.</st></li>
				<li><code><st c="61161">EmailOperator</st></code><st c="61175"> – Sends an email through </st><st c="61201">a protocol.</st></li>
				<li><code><st c="61212">SimpleHttpOperator</st></code><st c="61231"> – Sends an </st><st c="61243">HTTP request.</st></li>
			</ul>
			<p><st c="61256">Other operators may require installing the needed modules. </st><st c="61316">For example, the </st><code><st c="61333">PostgresOperator</st></code><st c="61349"> operator used for executing PostgreSQL commands requires installing the </st><code><st c="61422">apache-airflow[postgres]</st></code><st c="61446"> module through the </st><code><st c="61466">pip</st></code><st c="61469"> command.</st></p>
			<p><st c="61478">Each task must have a unique </st><code><st c="61508">task_id</st></code><st c="61515"> value for Airflow identification. </st><st c="61550">Our </st><code><st c="61554">Task1</st></code><st c="61559"> task is a </st><code><st c="61570">SimpleHTTPOperator</st></code><st c="61588"> operator that sends a </st><code><st c="61611">GET</st></code><st c="61614"> request to an HTTP </st><code><st c="61634">GET</st></code><st c="61637"> API endpoint expected to return a JSON resource. </st><st c="61687">It has an ID named </st><code><st c="61706">list_all_login</st></code><st c="61720"> and connects to Airflow’s HTTP connection object named </st><code><st c="61776">packt_dag</st></code><st c="61785">. All </st><code><st c="61791">SimpleHTTPOperator</st></code><st c="61809"> needs is a </st><code><st c="61821">Connection</st></code><st c="61831"> object, which stores the HTTP details of the external server resource that the operation will need to establish a connection. </st><st c="61958">Accessing the </st><code><st c="62075">Connection</st></code><st c="62085"> object. </st><em class="italic"><st c="62094">Figure 8</st></em><em class="italic"><st c="62102">.12</st></em><st c="62105"> shows the form that accepts HTTP details of the connection and creates </st><st c="62177">the object:</st></p>
			<div><div><img src="img/B19383_08_012.jpg" alt="Figure 8.12 – Creating an HTTP Connection object"/><st c="62188"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="62503">Figure 8.12 – Creating an HTTP Connection object</st></p>
			<p><st c="62551">Also, a </st><code><st c="62560">SimpleHTTPOperator</st></code><st c="62578"> operator provides a callback method indicated by its </st><code><st c="62632">response_check</st></code><st c="62646"> parameter. </st><st c="62658">The callback method accesses the response and other related data and can perform</st><a id="_idIndexMarker752"/><st c="62738"> evaluation and logging on the API response. </st><st c="62783">The following is the implementation of the callback method </st><st c="62842">of </st><code><st c="62845">Task1</st></code><st c="62850">:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="62852">def handle_response(response, **context):</st></strong><st c="62893">
    if response.status_code == 201:
        print("executed API successfully...")
        return True
    else:
        print("executed with errors...")
        return False</st></pre>			<p><st c="63027">On the other hand, </st><code><st c="63047">Task2</st></code><st c="63052"> is a </st><code><st c="63058">PythonOperator</st></code><st c="63072"> operator that runs a Python function, </st><code><st c="63111">count_login()</st></code><st c="63124">, for retrieving the JSON data from the API executed in </st><code><st c="63180">Task1</st></code><st c="63185"> and counting the number of records from the JSON resource. </st><st c="63245">Setting its </st><code><st c="63257">provide_context</st></code><st c="63272"> parameter to </st><code><st c="63286">True</st></code><st c="63290"> allows its </st><code><st c="63302">python_callable</st></code><st c="63317"> method to access the </st><code><st c="63339">taskInstance</st></code><st c="63351"> object that pulls the API resource from </st><code><st c="63392">Task1</st></code><st c="63397">. The </st><code><st c="63403">count_login()</st></code><st c="63416"> function can also set an </st><code><st c="63442">xcom</st></code><st c="63446"> variable, a form of workflow data, because the value of </st><code><st c="63503">Task2</st></code><st c="63508">’s </st><code><st c="63512">do_xcom_push</st></code><st c="63524"> parameter is </st><code><st c="63538">True</st></code><st c="63542">. The following snippet is the implementation </st><st c="63588">of </st><code><st c="63591">count_login()</st></code><st c="63604">:</st></p>
			<pre class="source-code"><st c="63606">
def count_login(</st><strong class="bold"><st c="63623">ti, **context</st></strong><st c="63637">):
    </st><strong class="bold"><st c="63641">data = ti.xcom_pull(task_ids=['list_all_login'])</st></strong><st c="63689">
    if not len(data):
        raise ValueError('Data is empty')
    </st><strong class="bold"><st c="63742">records_dict = json.loads(data[0])</st></strong><st c="63776">
    count = len(records_dict["records"])
    </st><strong class="bold"><st c="63814">ti.xcom_push(key="records", value=count)</st></strong><st c="63854">
    return count
    task3 = </st><strong class="bold"><st c="63876">SimpleHttpOperator</st></strong><st c="63894">(
        </st><strong class="bold"><st c="63897">task_id='report_count',</st></strong><strong class="bold"><st c="63920">method="GET",</st></strong><strong class="bold"><st c="63934">http_conn_id="packt_dag",</st></strong><strong class="bold"><st c="63960">endpoint="/ch08/login/report/count",</st></strong><st c="63997">
        data={"login_count": "{{ </st><strong class="bold"><st c="64023">task_instance.xcom_pull( task_ids=['list_all_login','count_login'], key='records')[0]</st></strong><st c="64108"> }}"},
        headers={"Content-Type": "application/json"},
        dag=dag
    )
    … … … … … …
</st><code><st c="64215">Task3</st></code><st c="64221"> is also a </st><code><st c="64232">SimpleHTTPOperator</st></code><st c="64250"> operator, but its </st><a id="_idIndexMarker753"/><st c="64269">goal is to call an HTTP </st><code><st c="64293">GET</st></code><st c="64296"> API and pass a request parameter, </st><code><st c="64331">login_count</st></code><st c="64342">, with a value derived from XCom data. </st><st c="64381">Operators can access Airflow built-in objects, such as </st><code><st c="64436">dag_run</st></code><st c="64443"> and </st><code><st c="64448">task_instance</st></code><st c="64461">, using the </st><code><st c="64473">{{ }}</st></code><st c="64478"> Jinja2 delimiter. </st><st c="64497">In </st><code><st c="64500">Task3</st></code><st c="64505">, </st><code><st c="64507">task_instance</st></code><st c="64520">, using its </st><code><st c="64532">xcom_pull()</st></code><st c="64543"> function, retrieves from the list of tasks the XCom variable records. </st><st c="64614">The result of </st><code><st c="64628">xcom_pull()</st></code><st c="64639"> is always a list with the value of the XCom variable at its </st><em class="italic"><st c="64700">0 index</st></em><st c="64707">.</st></p>
			<p><st c="64708">The last portion of the DAG file is where to place the sequence flow of the DAG’s task. </st><st c="64797">There are two ways to establish dependency from one task to another. </st><code><st c="64866">&gt;&gt;</st></code><st c="64868">, or the </st><em class="italic"><st c="64877">upstream dependency</st></em><st c="64896">, connects a flow from left to right, which means the execution of the task from the right depends on the success of the left task. </st><st c="65028">The other one, </st><code><st c="65043">&lt;&lt;</st></code><st c="65045"> or the </st><em class="italic"><st c="65053">downstream dependency</st></em><st c="65074">, follows the</st><a id="_idIndexMarker754"/><st c="65087"> reverse flow. </st><st c="65102">If two or more tasks depend on the same task, brackets enclose those dependent tasks, such as the </st><code><st c="65200">task1 &gt;&gt; [task2, task3]</st></code><st c="65223"> flow, where </st><code><st c="65236">task2</st></code><st c="65241"> and </st><code><st c="65246">task3</st></code><st c="65251"> are dependent tasks of </st><code><st c="65275">task1</st></code><st c="65280">. In the given DAG file, it is just a sequential flow from </st><code><st c="65339">task1</st></code> <st c="65344">to </st><code><st c="65348">task4</st></code><st c="65353">.</st></p>
			<p><st c="65354">What executes our tasks are called </st><em class="italic"><st c="65390">executors</st></em><st c="65399">. The </st><a id="_idIndexMarker755"/><st c="65405">default executor is </st><code><st c="65425">SequentialExecutor</st></code><st c="65443">, which runs the task flows one task at a time. </st><code><st c="65491">LocalExecutor</st></code><st c="65504"> runs the workflow sequentially, but the tasks may run in parallel mode. </st><st c="65577">There is </st><code><st c="65586">CeleryExecutor</st></code><st c="65600">, which runs workflows composed of Celery tasks, and </st><code><st c="65653">KubernetesExecutor</st></code><st c="65671">, which runs tasks on </st><st c="65693">a cluster.</st></p>
			<p><st c="65703">To deploy and re-deploy the DAG files, </st><em class="italic"><st c="65743">restart</st></em><st c="65750"> the scheduler and the web server. </st><st c="65785">Let us now implement an API endpoint function that will run the DAG deployed in the </st><st c="65869">Airflow server.</st></p>
			<h2 id="_idParaDest-245"><a id="_idTextAnchor249"/><st c="65884">Utilizing Airflow built-in REST endpoints</st></h2>
			<p><st c="65926">To trigger the DAG is to run the workflow. </st><st c="65970">Running </st><a id="_idIndexMarker756"/><st c="65978">a DAG requires using the Airflow UI’s DAG page, applying Airflow APIs for console-based triggers, or consuming Airflow’s built-in REST API with the Flask application or Postman. </st><st c="66156">This chapter implemented the </st><code><st c="66185">ch08-airflow</st></code><st c="66197"> project to provide the </st><code><st c="66221">report_login_count</st></code><st c="66239"> DAG with API endpoints for </st><code><st c="66267">Task1</st></code><st c="66272"> and </st><code><st c="66277">Task3</st></code><st c="66282"> executions and also to trigger the workflow using some Airflow REST endpoints. </st><st c="66362">The following is a custom endpoint function that triggers the </st><code><st c="66424">report_login_count</st></code><st c="66442"> DAG with a </st><code><st c="66454">dag_run_id</st></code><st c="66464"> value of a </st><st c="66476">UUID type:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="66486">@login_bp.get("/login/dag/report/login/count")</st></strong><st c="66533">
async def trigger_report_login_count():
    </st><strong class="bold"><st c="66574">token = "cGFja3RhZG1pbjpwYWNrdGFkbWlu"</st></strong><st c="66612">
    dag_id = "report_login_count"
    deployment_url = "localhost:8080"
    response = </st><strong class="bold"><st c="66688">requests.post(</st></strong><strong class="bold"><st c="66702">url=f"http://{deployment_url}</st></strong> <strong class="bold"><st c="66732">/api/v1/dags/{dag_id}/dagRuns",</st></strong><st c="66764">
        headers={
            </st><strong class="bold"><st c="66775">"Authorization": f"Basic {token}",</st></strong><st c="66809">
            "Content-Type": "application/json",
            "Accept": "*/*",
            "Connection": "keep-alive",
            "Accept-Encoding": "gzip, deflate, br"
        },
        </st><strong class="bold"><st c="66933">data = '{"dag_run_id": "d08a62c6-ed71-49fc-81a4-47991221aea5"}'</st></strong><st c="66996">
    )
    result = response.content.decode(encoding="utf-8")
    return jsonify(message=result), 201</st></pre>			<p><st c="67085">Airflow requires </st><em class="italic"><st c="67103">Basic authentication</st></em><st c="67123"> before consuming its REST endpoints. </st><st c="67161">Any REST access must include an </st><code><st c="67193">Authorization</st></code><st c="67206"> header with </st><a id="_idIndexMarker757"/><st c="67219">the generated token of a valid username and password. </st><st c="67273">Also, install a REST client module, such as </st><code><st c="67317">requests</st></code><st c="67325">, to consume the API libraries. </st><st c="67357">Running </st><code><st c="67365">/api/v1/dags/report_login_count/dagRuns</st></code><st c="67404"> with an HTTP </st><code><st c="67418">POST</st></code><st c="67422"> request will give us a JSON response </st><st c="67460">like this:</st></p>
			<pre class="source-code"><st c="67470">
{
    "conf": {},
    </st><strong class="bold"><st c="67485">"dag_id": "report_login_count",</st></strong><strong class="bold"><st c="67516">"dag_run_id": "01c04a4b-a3d9-4dc5-b0c3-e4e59e2db554",</st></strong><st c="67570">
    "data_interval_end": "2023-12-27T12:00:00+00:00",
    "data_interval_start": "2023-12-26T12:00:00+00:00",
    "end_date": null,
    "execution_date": "2023-12-27T13:55:44.910773+00:00",
    "external_trigger": true,
    "last_scheduling_decision": null,
    "logical_date": "2023-12-27T13:55:44.910773+00:00",
    "note": null,
    </st><strong class="bold"><st c="67871">"run_type": "manual",</st></strong><st c="67892">
    "start_date": null,
    "state": "queued"
}</st></pre>			<p><st c="67932">Then, running the following </st><a id="_idIndexMarker758"/><st c="67961">Airflow REST endpoint using the same </st><code><st c="67998">dag_run_id</st></code><st c="68008"> value will provide us with the result of </st><st c="68050">the workflow:</st></p>
			<pre class="source-code"><st c="68063">
@login_bp.get("/login/dag/xcom/values")
async def extract_xcom_count():
    try:
        token = "cGFja3RhZG1pbjpwYWNrdGFkbWlu"
        dag_id = "report_login_count"
        task_id = "return_report"
        </st><strong class="bold"><st c="68236">dag_run_id = "d08a62c6-ed71-49fc-81a4-47991221aea5"</st></strong><st c="68287">
        deployment_url = "localhost:8080"
        response = </st><strong class="bold"><st c="68333">requests.get(</st></strong><strong class="bold"><st c="68346">url=f"http://{deployment_url}</st></strong> <strong class="bold"><st c="68376">/api/v1/dags/{dag_id}/dagRuns</st></strong> <strong class="bold"><st c="68406">/{dag_run_id}/taskInstances/{task_id}</st></strong> <strong class="bold"><st c="68444">/xcomEntries/{'report_msg'}",</st></strong><st c="68474">
            headers={
                </st><strong class="bold"><st c="68485">"Authorization": f"Basic {token}",</st></strong><st c="68519">
                … … … … … …
            }
        )
        result = response.json()
        message = result['value']
        return jsonify(message=message)
    except Exception as e:
        print(e)
    return jsonify(message="")</st></pre>			<p><st c="68676">The given HTTP </st><code><st c="68692">GET</st></code><st c="68695"> request API</st><a id="_idIndexMarker759"/><st c="68707"> will provide us a JSON result </st><st c="68738">like so:</st></p>
			<pre class="source-code"><st c="68746">
{
    "message": "There are 20 users as of 2023-12-28 00:38:17.592287."
</st><st c="68815">}</st></pre>			<p><st c="68816">Airflow is a big platform that can offer us many solutions, especially in building pipelines of tasks for data transformation, batch processing, and data analytics. </st><st c="68981">Its strength is also in implementing API orchestration for microservices. </st><st c="69055">But for complex, long-running, and distributed workflow transactions, it is Temporal.io that can provide durable, reliable, and </st><st c="69183">scalable solutions.</st></p>
			<h1 id="_idParaDest-246"><a id="_idTextAnchor250"/><st c="69202">Implementing workflows using Temporal.io</st></h1>
			<p><st c="69243">The </st><strong class="bold"><st c="69248">Temporal.io</st></strong><st c="69259"> server manages loosely coupled workflows and activities, those not limited by the architecture of the Temporal.io platform. </st><st c="69384">Thus, all workflow components are coded from the ground up </st><a id="_idIndexMarker760"/><st c="69443">without hooks and callable methods appearing in the implementation. </st><st c="69511">The server expects the execution of activities rather than tasks. </st><st c="69577">In BPMN, an activity is more complex than a task. </st><st c="69627">The server is responsible for building a fault-tolerant workflow because it can recover failed activity execution by restarting its execution from </st><st c="69774">the start.</st></p>
			<p><st c="69784">So, let us begin this topic with the Temporal.io </st><st c="69834">server setup.</st></p>
			<h2 id="_idParaDest-247"><a id="_idTextAnchor251"/><st c="69847">Setting up the environment</st></h2>
			<p><st c="69874">The Temporal.io server has an installer for </st><em class="italic"><st c="69919">macOS</st></em><st c="69924">, </st><em class="italic"><st c="69926">Windows</st></em><st c="69933">, and </st><em class="italic"><st c="69939">Linux</st></em><st c="69944"> platforms. </st><st c="69956">For Windows users, download the ZIP file from the </st><a href="https://temporal.download/cli/archive/latest?platform=windows&amp;arch=amd64"><st c="70006">https://temporal.download/cli/archive/latest?platform=windows&amp;arch=amd64</st></a><st c="70078"> link. </st><st c="70085">Then, unzip the file to the local </st><a id="_idIndexMarker761"/><st c="70119">machine. </st><st c="70128">Start the server using the </st><code><st c="70155">temporal server </st></code><code><st c="70171">start-dev</st></code><st c="70180"> command.</st></p>
			<p><st c="70189">Now, to integrate our Flask application with the server, install the </st><code><st c="70259">temporalio</st></code><st c="70269"> module to the virtual environment using the </st><code><st c="70314">pip</st></code><st c="70317"> command. </st><st c="70327">Establish a server connection in the </st><code><st c="70364">main.py</st></code><st c="70371"> module of the application using the </st><code><st c="70408">Client</st></code><st c="70414"> class of the </st><code><st c="70428">temporalio</st></code><st c="70438"> module. </st><st c="70447">The following </st><code><st c="70461">main.py</st></code><st c="70468"> script shows how to instantiate a </st><code><st c="70503">Client</st></code><st c="70509"> instance:</st></p>
			<pre class="source-code"><st c="70519">
from temporalio.client import Client
from modules import create_app
import asyncio
app, celery_app= create_app("../config_dev.toml")
</st><strong class="bold"><st c="70653">async def connect_temporal(app):</st></strong><strong class="bold"><st c="70685">client = await Client.connect("localhost:7233")</st></strong><strong class="bold"><st c="70733">app.temporal_client = client</st></strong><st c="70762">
if __name__ == "__main__":
    </st><strong class="bold"><st c="70790">asyncio.run(connect_temporal(app))</st></strong><st c="70824">
    app.run(debug=True)</st></pre>			<p><st c="70844">The </st><code><st c="70849">connect_temporal()</st></code><st c="70867"> method instantiates the Client API class and creates a </st><code><st c="70923">temporal_client</st></code><st c="70938"> environment variable in the Flask platform for the API endpoints to run the workflow. </st><st c="71025">Since </st><code><st c="71031">main.py</st></code><st c="71038"> is the entry point module, an event loop will execute the method during the Flask </st><st c="71121">server startup.</st></p>
			<p><st c="71136">After setting up the Temporal.io server and its connection to the Flask application, let us discuss the distinct approach to</st><a id="_idIndexMarker762"/> <st c="71261">workflow implementation.</st></p>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor252"/><st c="71286">Implementing activities and a workflow</st></h2>
			<p><st c="71325">Temporal uses the code-first approach of implementing a workflow and its activities. </st><st c="71411">Activities in a Temporal platform must be </st><em class="italic"><st c="71453">idempotent</st></em><st c="71463">, meaning its parameters and results are non-changing through the </st><a id="_idIndexMarker763"/><st c="71529">course or history of its executions. </st><st c="71566">The following is an example of a complex but </st><st c="71611">idempotent activity:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="71631">from temporalio import activity</st></strong>
<strong class="bold"><st c="71663">@activity.defn</st></strong><st c="71678">
async def reserve_schedule(appointmentwf: AppointmentWf) -&gt; str:
    try:
        async with db_session() as sess:
              async with sess.begin():
                </st><strong class="bold"><st c="71807">repo = AppointmentRepository(sess)</st></strong><st c="71841">
                … … … … … … …
                </st><strong class="bold"><st c="71855">result = await repo.insert_appt(appt)</st></strong><st c="71892">
                if result == False:
                    … … … … … …
                    </st><strong class="bold"><st c="71925">return "failure"</st></strong><st c="71941">
                … … … … … …
                </st><strong class="bold"><st c="71953">return "success"</st></strong><st c="71969">
    except Exception as e:
        print(e)
        … … … … … …
    </st><code><st c="72084">@activity.defn</st></code><st c="72098"> annotation, with a workflow data class as a local parameter, and returns a non-varying and non-changing value. </st><st c="72210">The returned value can be a fixed string, number, or any string with a non-varying length. </st><st c="72301">Avoid returning collection or model objects with varying property values. </st><st c="72375">Our </st><code><st c="72379">reserve_schedule()</st></code><st c="72397"> activity accepts an </st><code><st c="72418">AppointmentWf</st></code><st c="72431"> object containing appointment details and saves the record of information into the database. </st><st c="72525">It returns only either </st><code><st c="72548">"successful"</st></code> <st c="72560">or </st><code><st c="72564">"failure"</st></code><st c="72573">.</st></p>
			<p><st c="72574">An activity is where access to external services, such as databases, emails, or APIs, is permitted by Temporal and </st><a id="_idIndexMarker764"/><st c="72690">not in the workflow implementation. </st><st c="72726">The following code is a </st><em class="italic"><st c="72750">Temporal workflow </st></em><st c="72768">that runs the </st><code><st c="72782">reserve_schedule()</st></code><st c="72800"> activity:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="72810">@workflow.defn(sandboxed=False)</st></strong><st c="72842">
class ReserveAppointmentWorkflow():
    def __init__(self) -&gt; None:
        </st><code><st c="73046">@workflow.defn</st></code><st c="73060"> decorator implements a workflow. </st><st c="73094">An example is our </st><code><st c="73112">ReserveAppointmentWorkflow</st></code><st c="73138"> class.</st></p>
			<p><st c="73145">It maintains the same execution state starting from the beginning, making it a deterministic workflow. </st><st c="73249">It also manages all its states through replays to determine some exceptions and provide recovery after a </st><st c="73354">non-deterministic state.</st></p>
			<p><st c="73378">Moreover, Temporal workflows are designed to run continuously without time limits but with proper scheduling to handle long-running and complex activities. </st><st c="73535">However, using threads for concurrency is not allowed in Temporal workflows. </st><st c="73612">It must have an instance method decorated by </st><code><st c="73657">@workflow.run</st></code><st c="73670"> to create a continuous loop for its activities. </st><st c="73719">The following </st><code><st c="73733">run()</st></code><st c="73738"> method accepts a request model with appointment details from the user and loops until the cancellation of the appointment, where </st><code><st c="73868">appointmentwf.status</st></code> <st c="73888">becomes </st><code><st c="73897">False</st></code><st c="73902">:</st></p>
			<pre class="source-code"><strong class="bold"><st c="73904">@workflow.run</st></strong><st c="73917">
    async def run(self, data: ReqAppointment) -&gt; None:
        duration = 12
        self.appointmentwf.ticketid = data.ticketid
        self.appointmentwf.patientid = data.patientid
        … … … … … …
        </st><strong class="bold"><st c="74085">while self.appointmentwf.status:</st></strong><st c="74117">
            self.appointmentwf.remarks = "Doctor reservation being processed...."
            </st><strong class="bold"><st c="74188">try:</st></strong><strong class="bold"><st c="74192">await workflow.execute_activity(</st></strong><strong class="bold"><st c="74225">reserve_schedule,</st></strong><strong class="bold"><st c="74243">self.appointmentwf,</st></strong><strong class="bold"><st c="74263">start_to_close_timeout=timedelta(</st></strong> <strong class="bold"><st c="74297">seconds=10),</st></strong><st c="74310">
                )
                </st><strong class="bold"><st c="74313">await asyncio.sleep(duration)</st></strong><strong class="bold"><st c="74342">except asyncio.CancelledError as err:</st></strong><st c="74380">
                self.appointmentwf.status = False
                self.appointmentwf.remarks = "Appointment with doctor done."
                </st><code><st c="74739">ReserveAppointmentWorkflow</st></code><st c="74765"> instance, when canceled, will throw a </st><code><st c="74804">CancelledError</st></code><st c="74818"> exception that will trigger the exception clause that sets </st><code><st c="74878">appointmentwf.status</st></code><st c="74898"> to </st><code><st c="74902">False</st></code><st c="74907"> and executes the </st><code><st c="74925">start_to_close()</st></code><st c="74941"> activity.</st></p>
			<p><st c="74951">Aside from the loop and the</st><a id="_idIndexMarker765"/><st c="74979"> constructor, a workflow implementation can emit </st><code><st c="75028">resultset</st></code><st c="75037"> instances or information about the workflow. </st><st c="75083">To carry this out, implement an instance method and decorate it with </st><code><st c="75152">@workflow.query</st></code><st c="75167">. The following method returns an </st><st c="75201">appointment record:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="75220">    @workflow.query</st></strong><st c="75236">
    def details(self) -&gt; AppointmentWf:
        return self.appointmentwf</st></pre>			<p><st c="75298">Unlike in Zeebe/Camunda, where the server executes and manages the workflow, the Temporal.io server does not run any workflow instance but the worker. </st><st c="75450">We learn more about workers in the </st><st c="75485">following section.</st></p>
			<h2 id="_idParaDest-249"><a id="_idTextAnchor253"/><st c="75503">Building a worker</st></h2>
			<p><st c="75521">A </st><code><st c="75579">Worker</st></code><st c="75585"> class from the </st><code><st c="75601">temporalio.worker</st></code><st c="75618"> module requires the </st><code><st c="75639">client</st></code><st c="75645"> connection, </st><code><st c="75658">task_queue</st></code><st c="75668">, </st><code><st c="75670">workflows</st></code><st c="75679">, and </st><code><st c="75685">activities</st></code><st c="75695"> as constructor </st><a id="_idIndexMarker767"/><st c="75711">parameters before its instantiation. </st><st c="75748">Our worker should be outside the </st><a id="_idIndexMarker768"/><st c="75781">context of Flask, so we added the </st><code><st c="75815">workflow_runner</st></code><st c="75830"> parameter to the parameters. </st><st c="75860">The following code is our implementation of the </st><st c="75908">Temporal worker:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="75924">from temporalio.client import Client</st></strong>
<strong class="bold"><st c="75961">from temporalio.worker import Worker,</st></strong> <strong class="bold"><st c="75999">UnsandboxedWorkflowRunner</st></strong><st c="76025">
import asyncio
from modules.admin.activities.workflow import reserve_schedule, close_schedule
from modules.models.workflow import appt_queue_id
from modules.workflows.transactions import ReserveAppointmentWorkflow
async def main():
    </st><strong class="bold"><st c="76258">client = await Client.connect("localhost:7233")</st></strong><st c="76305">
    worker = </st><strong class="bold"><st c="76315">Worker(</st></strong><strong class="bold"><st c="76322">client,</st></strong><strong class="bold"><st c="76330">task_queue=appt_queue_id,</st></strong><strong class="bold"><st c="76356">workflows=[ReserveAppointmentWorkflow],</st></strong><strong class="bold"><st c="76396">activities=[reserve_schedule, close_schedule],</st></strong><strong class="bold"><st c="76443">workflow_runner=UnsandboxedWorkflowRunner,</st></strong><st c="76486">
    )
    </st><strong class="bold"><st c="76489">await worker.run()</st></strong><st c="76507">
if __name__ == "__main__":
    print("Temporal worker started…")
    </st><code><st c="76679">Worker</st></code><st c="76685"> instance needs to know </st><a id="_idIndexMarker769"/><st c="76709">what workflows to queue and activities to run before the client application triggers their executions. </st><st c="76812">Now, passing the </st><code><st c="76829">UnsandboxedWorkflowRunner</st></code><st c="76854"> object to the </st><code><st c="76869">workflow_runner</st></code><st c="76884"> parameter indicates that our worker will be running as an independent Python application outside the context of our Flask platform or any sandbox environment, thus the setting of the </st><code><st c="77068">sandboxed</st></code><st c="77077"> parameter in the </st><code><st c="77095">@workflow.defn</st></code><st c="77109"> decorator of every workflow class to </st><code><st c="77147">False</st></code><st c="77152">. To run the worker, call and await the </st><code><st c="77192">run()</st></code><st c="77197"> method of the </st><code><st c="77212">Worker</st></code><st c="77218"> instance.</st></p>
			<p><st c="77228">Lastly, after implementing the</st><a id="_idIndexMarker770"/><st c="77259"> workflows, activities, and the worker, it is time to trigger the workflow </st><st c="77334">for execution.</st></p>
			<h2 id="_idParaDest-250"><a id="_idTextAnchor254"/><st c="77348">Running activities</st></h2>
			<p><st c="77367">The </st><code><st c="77372">ch08-temporal</st></code><st c="77385"> project is a</st><a id="_idIndexMarker771"/><st c="77398"> RESTful application in Flask, so to run a workflow, an API endpoint must import and use </st><code><st c="77487">app.temporal_client</st></code><st c="77506"> to connect to the server and to invoke the </st><code><st c="77550">start_workflow()</st></code><st c="77566"> method that will trigger the </st><st c="77596">workflow execution.</st></p>
			<p><st c="77615">The </st><code><st c="77620">start_workflow()</st></code><st c="77636"> method requires the workflow’s “</st><em class="italic"><st c="77669">run</st></em><st c="77673">” method, the single model object parameter, the unique workflow ID, and </st><code><st c="77747">task_queue</st></code><st c="77757">. The following API endpoint triggers the execution of our </st><code><st c="77816">ReserveAppointmentWorkflow</st></code><st c="77842"> class:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="77849">@admin_bp.route("/appointment/doctor", methods=["POST"])</st></strong><st c="77906">
async def request_appointment():
    </st><strong class="bold"><st c="77940">client = get_client()</st></strong><st c="77961">
    appt_json = request.get_json()
    appointment = ReqAppointment(**appt_json)
    </st><strong class="bold"><st c="78035">await client.start_workflow(</st></strong><strong class="bold"><st c="78063">ReserveAppointmentWorkflow.run,</st></strong><strong class="bold"><st c="78095">appointment,</st></strong><strong class="bold"><st c="78108">id=appointment.ticketid,</st></strong><strong class="bold"><st c="78133">task_queue=appt_queue_id,</st></strong><st c="78159">
    )
    message = jsonify({"message": "Appointment for doctor requested...."})
    response = make_response(message, 201)
    return response</st></pre>			<p><st c="78287">After a successful workflow trigger, another API can query the details or results of the workflow by extracting the workflow’s </st><code><st c="78415">WorkflowHandler</st></code><st c="78430"> class from the client using its </st><em class="italic"><st c="78463">workflow ID</st></em><st c="78474">. The following endpoint function shows how to retrieve the result of the </st><st c="78548">completed workflow:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="78567">@admin_bp.route("/appointment/details", methods=["GET"])</st></strong><st c="78624">
async def get_appointment_details():
    client = get_client()
    ticketid = request.args.get("ticketid")
    print(ticketid)
    handle = </st><strong class="bold"><st c="78749">client.get_workflow_handle_for( ReserveAppointmentWorkflow.run, ticketid)</st></strong><strong class="bold"><st c="78822">results = await handle.query(</st></strong> <strong class="bold"><st c="78852">ReserveAppointmentWorkflow.details)</st></strong><st c="78888">
    message = jsonify({
            "ticketid": results.ticketid,
            "patientid": results.patientid,
            "docid": results.docid,
            "date_scheduled": results.date_scheduled,
            "time_scheduled": results.time_scheduled,
        }
    )
    response = make_response(message, 200)
    return response</st></pre>			<p><st c="79137">To prove that Temporal workflows</st><a id="_idIndexMarker772"/><st c="79170"> can respond to cancellation events, the following API invokes the </st><code><st c="79237">cancel()</st></code><st c="79245"> method from the </st><code><st c="79262">WorkflowHandler</st></code><st c="79277"> class for its workflow to throw a </st><code><st c="79312">CancelledError</st></code><st c="79326"> exception, leading to the execution of the </st><code><st c="79370">close_schedule()</st></code><st c="79386"> activity:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="79396">@admin_bp.route("/appointment/close", methods=["DELETE"])</st></strong><st c="79454">
async def end_subscription():
    client = get_client()
    ticketid = request.args.get("ticketid")
    </st><strong class="bold"><st c="79547">handle = client.get_workflow_handle(ticketid,)</st></strong><strong class="bold"><st c="79593">await handle.cancel()</st></strong><st c="79615">
    message = jsonify({"message": "Requesting cancellation"})
    response = make_response(message, 202)
    return response</st></pre>			<p><st c="79728">Although there is still a lot to discuss about the architecture and the behavior of these big-time workflow</st><a id="_idIndexMarker773"/><st c="79836"> solutions, the main goal is to highlight the feasibility of integrating different workflow engines into the asynchronous Flask platform and take into consideration workarounds for integrations to work with </st><st c="80043">Flask applications.</st></p>
			<h1 id="_idParaDest-251"><a id="_idTextAnchor255"/><st c="80062">Summary</st></h1>
			<p><st c="80070">This chapter proved that </st><code><st c="80096">Flask[async]</st></code><st c="80108"> can work with different workflow engines, starting with Celery tasks. </st><code><st c="80179">Flask[async]</st></code><st c="80191">, combined with the workflows created by Celery’s signatures and primitives, works well in building chained, grouped, and </st><st c="80313">chorded processes.</st></p>
			<p><st c="80331">Then, </st><code><st c="80338">Flask[async]</st></code><st c="80350"> was proven to work with SpiffWorkflow for some BPMN serialization that focuses on </st><code><st c="80433">UserTask</st></code><st c="80441"> and </st><code><st c="80446">ScriptTask</st></code><st c="80456"> tasks. </st><st c="80464">Also, this chapter even considered solving BPMN enterprise problems using the Zeebe/Camunda platform that showcases </st><code><st c="80580">ServiceTask</st></code><st c="80591"> tasks.</st></p>
			<p><st c="80598">Moreover, </st><code><st c="80609">Flask[async]</st></code><st c="80621"> created an environment with Airflow 2.x to implement pipelines of tasks building an API orchestration. </st><st c="80725">In the last part, the chapter established the integration between </st><code><st c="80791">Flask[async]</st></code><st c="80803"> and Temporal.io and demonstrated the implementation of deterministic and </st><st c="80877">distributed workflows.</st></p>
			<p><st c="80899">This chapter provided a clear picture of the extensibility, usability, and scalability of the Flask framework in building scientific and big data applications and even BPMN-related and ETL-involved </st><st c="81098">business processes.</st></p>
			<p><st c="81117">The next chapter will discuss the different authentication and authorization mechanisms to secure </st><st c="81216">Flask applications.</st></p>
		</div>
	<div></body></html>