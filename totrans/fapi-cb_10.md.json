["```py\n$ pip install –r requirements.txt\n```", "```py\n$ pip install fastapi uvicorn grpcio grpcio-tools\n```", "```py\n    syntax = \"proto3\";\n    service GrpcServer{\n        rpc GetServerResponse(Message)\n        returns (MessageResponse) {}\n    }\n    ```", "```py\n    message Message{\n    string message = 1;\n    }\n    message MessageResponse{\n    string message = 1;\n    bool received = 2;\n    }\n    ```", "```py\n    $ python -m grpc_tools.protoc \\\n    --proto_path=. ./grpcserver.proto \\\n    --python_out=. \\\n    grpcserver_pb2_grpc.py and grpcserver_pb2.py. The grpcserver_pb2_grpc.py file contains the class to build the server a support function and a stub class that will be used by the client, while the grpcserver_pb2.py module contains the classes that define the messages. In our case, these are Message and MessageResponse.\n    ```", "```py\n    from grpcserver_pb2 import MessageResponse\n    from grpcserver_pb2_grpc import GrpcServerServicer\n    class Service(GrpcServerServicer):\n        async def GetServerResponse(\n            self, request, context\n        ):\n            message = request.message\n            logging.info(f\"Received message: {message}\")\n            result = (\n                \"Hello I am up and running, received: \"\n                f\"{message}\"\n            )\n            result = {\n                \"message\": result,\n                \"received\": True,\n            }\n            return MessageResponse(**result)\n    ```", "```py\n    import grpc\n    from grpcserver_pb2_grpc import (\n        add_GrpcServerServicer_to_server\n    )\n    async def serve():\n        server = grpc.aio.server()\n        add_GrpcServerServicer_to_server(\n            Service(), server\n        )\n        server.add_insecure_port(\"[::]:50051\")\n        logging.info(\"Starting server on port 50051\")\n        await server.start()\n        await server.wait_for_termination()\n    ```", "```py\n    import asyncio\n    import logging\n    if __name__ == \"__main__\":\n        logging.basicConfig(level=logging.INFO)\n        asyncio.run(serve())\n    ```", "```py\n$ python ./grpc_server.py\n```", "```py\nINFO:root:Starting server on port 50051\n```", "```py\n    from fastapi import FastAPI\n    app = FastAPI()\n    ```", "```py\n    from pydantic import BaseModel\n    class GRPCResponse(BaseModel):\n        message: str\n        received: bool\n    ```", "```py\n    grpc_channel = grpc.aio.insecure_channel(\n        \"localhost:50051\"\n    )\n    ```", "```py\n    @app.get(\"/grpc\")\n    async def call_grpc(message: str) -> GRPCResponse:\n        async with grpc_channel as channel:\n            grpc_stub = GrpcServerStub(channel)\n            response = await grpc_stub.GetServerResponse(\n                Message(message=message)\n            )\n            return response\n    ```", "```py\n$ uvicorn app.main:app\n```", "```py\n$ pip install fastapi uvicorn strawberry-graphql[fastapi]\n```", "```py\n    from pydantic import BaseModel\n    class User(BaseModel):\n        id: int\n        username: str\n        phone_number: str\n        country: str\n    ```", "```py\n    users_db: list[User] = [\n        User(\n            id=1,\n            username=\"user1\",\n            phone_number=\"1234567890\",\n            country=\"USA\",\n        ),\n    # other users\n    ]\n    ```", "```py\n    import strawberry\n    @strawberry.type\n    class User:\n        username: str\n        phone_number: str\n        country: str\n    ```", "```py\n    @strawberry.type\n    class Query:\n        @strawberry.field\n        def users(\n            self, country: str | None\n        ) -> list[User]:\n            return [\n                User(\n                    username=user.username,\n                    phone_number=user.phone_number,\n                    country=user.country,\n                )\n                for user in users_db\n                if user.country == country\n            ]\n    ```", "```py\n    from strawberry.fastapi import GraphQLRouter\n    schema = strawberry.Schema(Query)\n    graphql_app = GraphQLRouter(schema)\n    ```", "```py\n    from fastapi import FastAPI\n    from graphql_utils import graphql_app\n    app = FastAPI()\n    app.include_router(graphql_app, prefix=\"/graphql\")\n    ```", "```py\nhttp://localhost:8000/graphql. You will see an interactive page for the endpoint. The page is divided into two panels. The left contains the query editor and the right will show you the response.\nTry to make the following GraphQL query:\n\n```", "```py\n\n You will see the result on the right panel, which will look something like this:\n\n```", "```py\n\n You have learned how to create an interactive GraphQL endpoint. By combining RESTful endpoints with GraphQL, the potential for data querying and modification can be greatly expanded. Real-world scenarios may involve using REST endpoints to modify the database by adding, modifying, or removing records. GraphQL can then be used to query the database and extract valuable insights.\nSee also\nYou can consult the FastAPI official documentation on how to integrate GraphQL:\n\n*   *FastAPI GraphQL* *Documentation*: [https://fastapi.tiangolo.com/how-to/graphql/](https://fastapi.tiangolo.com/how-to/graphql/)\n\nAlso, in the Strawberry documentation, you can find a dedicated page on FastAPI integration:\n\n*   *Integrate FastAPI with* *Strawberry*: [https://strawberry.rocks/docs/integrations/fastapi](https://strawberry.rocks/docs/integrations/fastapi)\n\nUsing ML models with Joblib\nML models are powerful tools for data analysis, prediction, and decision-making in various applications. FastAPI provides a robust framework for building web services, making it an ideal choice for deploying ML models in production environments. In this recipe, we will see how to integrate an ML model with FastAPI using **Joblib**, a popular library for model serialization and deserialization in Python.\nWe will develop an AI-powered doctor application that can diagnose diseases by analyzing the symptoms provided.\nWarning\nNote that the diagnoses provided by the AI doctor should not be trusted in real-life situations, as it is not reliable.\nGetting ready\nPrior knowledge of ML is not mandatory but having some can be useful to help you follow the recipe.\nWe will apply the recipe to a new project, so create a folder named `ai_doctor` that we will use as the project root folder.\nTo ensure that you have all the necessary packages in your environment, you can install them using the `requirements.txt` file provided in the GitHub repository or from the command line:\n\n```", "```py\n\n We will download the model from the Hugging Face Hub, a centralized hub hosting pre-trained ML models that are ready to be used.\nWe will use the `human-disease-prediction` model, which is a relatively lightweight linear logistic regression model developed with the `scikit-learn` package. You can check it out at the following link: [https://huggingface.co/AWeirdDev/human-disease-prediction](https://huggingface.co/AWeirdDev/human-disease-prediction).\nTo download it, we will leverage the provided `huggingface_hub` Python package, so make sure you have it in your environment by running the following:\n\n```", "```py\n\n Once the installation is complete, we can proceed with building our AI doctor.\nHow to do it…\nLet’s follow these steps to create our AI doctor:\n\n1.  Let’s start by writing the code to accommodate the ML model. In the project root folder, let's create the `app` folder containing a module called `utils.py`. In the module, we will declare a `symptoms_list` list containing all the symptoms accepted by the model. You can download the file directly from the GitHub repository at [https://raw.githubusercontent.com/PacktPublishing/FastAPI-Cookbook/main/Chapter10/ai_doctor/app/utils.py](https://raw.githubusercontent.com/PacktPublishing/FastAPI-Cookbook/main/Chapter10/ai_doctor/app/utils.py).\n\n    You can find the complete list on the model’s documentation page at [https://huggingface.co/AWeirdDev/human-disease-prediction](https://huggingface.co/AWeirdDev/human-disease-prediction).\n\n2.  Still in the `app` folder, let’s create the `main.py` module that will contain the `FastAPI` server class object and the endpoint. To incorporate the model into our application, we will utilize the FastAPI lifespan feature.\n\n    We can define the lifespan context manager as follows:\n\n    ```", "```py\n\n    The `lifespan` context manager serves as middleware and carries out operations before and after server start and shutdown. It retrieves the model from the Hugging Face Hub and stores it in the `ml_model` dictionary, so it to be used across the endpoints without the need to reload it every time it is called.\n\n     3.  Once it has been defined, we need to pass it to the `FastAPI` object class as follows:\n\n    ```", "```py\n\n     4.  Now we need to create the endpoint that will take the symptoms as parameters and return the diagnosis.\n\n    The idea is to return each symptom as a path parameter. Since we have `132` possible symptoms, we will create the parameters object dynamically with Pydantic and restrict our model to the first ten symptoms. In the `main.py` file, let’s create the `Symptoms` class used to accept the parameters with the `pydantic.create_model` function as follows:\n\n    ```", "```py\n\n    We now have all that we need to create our `GET /``diagnosis` endpoint.\n\n     5.  Let’s create our endpoint as follows:\n\n    ```", "```py\n\nTo test it, as usual, spin up the server with `uvicorn` from the command line by running the following:\n\n```", "```py\n\n Open the interactive documentation from the browser at `http://localhost:8000/docs`. You will see the only `GET /diagnosis` endpoint and you will be able to select the symptoms. Try to select some of them and get your diagnosis from the AI doctor you have just created.\nYou have just created a FastAPI application that integrates an ML model. You can use the same model for different endpoints, but you can also integrate multiple models within the same application with the same strategy.\nSee also\nYou can check the guidelines on how to integrate an ML model into FastAPI on the official documentation page:\n\n*   *Lifespan* *Events*: [https://fastapi.tiangolo.com/advanced/events/?h=machine+learning#use-case](https://fastapi.tiangolo.com/advanced/events/?h=machine+learning#use-case)\n\nYou can have a look at the Hugging Face Hub platform documentation at the link:\n\n*   *Hugging Face Hub* *Documentation*: [https://huggingface.co/docs/hub/index](https://huggingface.co/docs/hub/index)\n\nTake a moment to explore the capabilities of the `scikit-learn` package by referring to the official documentation:\n\n*   *Scikit-learn* *Documentation*: [https://scikit-learn.org/stable/](https://scikit-learn.org/stable/)\n\nIntegrating FastAPI with Cohere\nCohere offers powerful language models and APIs that enable developers to build sophisticated AI-powered applications capable of understanding and generating human-like text.\nState-of-the-art language models, such as the **Generative Pre-trained Transformer** (**GPT**) series, have revolutionized how machines comprehend and generate natural language. These models, which are trained on vast amounts of text data, deeply understand human language patterns, semantics, and context.\nBy leveraging Cohere AI’s models, developers can empower their applications to engage in natural language conversations, answer queries, generate creative content, and perform a wide range of language-related tasks.\nIn this recipe, we will create an AI-powered chatbot using FastAPI and Cohere that suggests Italian cuisine recipes based on user queries.\nGetting ready\nBefore starting the recipe, you will need a Cohere account and an API key.\nYou can create your account at the page [https://dashboard.cohere.com/welcome/login](https://dashboard.cohere.com/welcome/login) by clicking the **Sign up** button at the top. At the time of writing, you can create an account by using your existing GitHub or Google account.\nOnce logged in, you will see a welcome page and a platform menu on the left with some options. Click on **API keys** to access the API menu.\nBy default, you will have a trial key that is free of charge, but it is rate limited and it cannot be used for commercial purposes. For the recipe, it will be largely sufficient.\nNow create the project root folder called `chef_ai` and store your API key in a file called `.env` under the project root folder as follows:\n\n```", "```py\n\n Warning\nIf you develop your project with a versioning system control such as Git, for example, make sure to not track any API keys. If you have done this already, even unintentionally, revoke the key from the Cohere API keys page and generate a new one.\nAside from the API key, make sure that you also have all the required packages in your environment. We will need `fastapi`, `uvicorn`, `cohere`, and `python-dotenv`. This last package will enable importing environment variables from the `.``env` file.\nYou can install all the packages with the `requirements.txt` file provided in the GitHub repository in the `chef_ai` project folder by running the following:\n\n```", "```py\n\n Alternatively you can install them one by one:\n\n```", "```py\n\n Once the installation is complete, we can dive into the recipe and create our “chef de cuisine” assistant.\nHow to do it…\nWe will create our chef cuisine assistant by using a message completion chat. Chat completion models take a list of messages as input and return a model-generated message as output. The first message to provide is the **system message**.\nA system message defines how a chatbot behaves in a conversation, such as adopting a specific tone or acting as a specialist such as a senior UX designer or software engineer. In our case, the system message will tell the chatbot to behave like a chef de cuisine.\nLet’s create an endpoint to call our chat through the following steps:\n\n1.  Let’s create a `handlers.py` module under the project root and import the Cohere API key from the `.``env` file:\n\n    ```", "```py\n\n     2.  Let’s write the system message as follows:\n\n    ```", "```py\n\n     3.  Define the Cohere asynchronous client as follows:\n\n    ```", "```py\n\n     4.  Before creating the function the generate the message, let’s import the required modules as:\n\n    ```", "```py\n\n     5.  Then, we can define the function to generate our message:\n\n    ```", "```py\n\n    The function will take in input the user query and the messages previously exchanged during the conversation. If the response is returned with no errors, the messages list is updated with the new interaction, otherwise an HTTPException error is raised.\n\n    We utilized `main.py` module, located under the project root folder, we can start defining the `messages` list in the application state at the startup with the lifespan context manager:\n\n    ```", "```py\n\n     6.  We then pass the `lifespan` context manager to the app object as:\n\n    ```", "```py\n\n     7.  Finally, we can create our endpoint as follows:\n\n    ```", "```py\n\n    We enforce a minimum length for the query message `(Body(min_length=1))` to prevent the model from returning an error response.\n\nYou have just created an endpoint that interacts with our chef de cuisine chatbot.\nTo test it, spin up the server with `uvicorn`:\n\n```", "```py\n\n Open the interactive documentation and start testing the endpoint. For example, you can prompt the model with a message such as the following:\n\n```", "```py\n\n Read the answer, then try asking the bot to replace some ingredients and continue the chat. Once you have completed your recipe, enjoy your meal!\nExercise\nWe have created a chatbot endpoint to interact with our assistant. However, for real-life applications, it can be useful to have an endpoint that returns all the messages exchanged.\nCreate a `GET /messages` endpoint that returns all the messages in a formatted way.\nAlso create an endpoint `POST /restart-conversation` that will flush all the messages and restart the conversation without any previous messages.\nSee also\nYou can have a look at the Cohere quickstart on building a chatbot on the official documentation page:\n\n*   *Building a* *Chatbot*: [https://docs.cohere.com/docs/building-a-chatbot](https://docs.cohere.com/docs/building-a-chatbot)\n\nIn production environment, depending on the project’s needs and budget, you might want to choose from the several models available. You can see an overview of the models provided by Cohere here:\n\n*   *Models* *Overview*: [https://docs.cohere.com/docs/models](https://docs.cohere.com/docs/models)\n\nIntegrating FastAPI with LangChain\nLangChain is a versatile interface for nearly any **Large Language Model** (**LLM**) that allows developers to create LLM applications and integrate them with external data sources and software workflows. It was launched in October 2022 and quickly became a top open source project on GitHub.\nWe will use LangChain and FastAPI to create an AI-powered assistant for an electronic goods store that provides recommendations and helps users.\nWe will set up a **Retrieval-Augmented Generation** (**RAG**) application, which involves empowering the model with personalized data to be trained. In this particular case, that would be a document of frequently asked questions.\nThis recipe will guide you through the process of integrating FastAPI with LangChain to create dynamic and interactive AI assistants that enhance the customer shopping experience.\nGetting ready\nBefore starting the recipe, you will need a Cohere API key. If you don’t have it, you can check the *Getting ready* section of the *Integrating FastAPI with* *Cohere* recipe.\nCreate a project directory called `ecotech_RAG` and place the API key within a `.env` file, labeled as `COHERE_API_KEY`.\nPrevious knowledge of LLM and RAG is not required but having it would help.\nAside from the `fastapi` and `uvicorn` packages, you will need to install `python-dotenv` and the packages related to LangChain. You can do this by using `requirements.txt` or by installing them with `pip` as follows:\n\n```", "```py\n\n Once the installation is complete, we can start building our AI shop assistant.\nHow to do it…\nWe are going to create an application with a single endpoint that interacts with an LLM from Cohere.\nThe idea behind LangChain is to provide a series of interconnected modules, forming a chain to establish a workflow linking the user query with the model output.\nWe will split the process of creating the endpoint to interact with the RAG AI assistant into the following steps:\n\n1.  Defining the prompts\n2.  Ingesting and vectorizing the documents\n3.  Building the model chain\n4.  Creating the endpoint\n\nLet’s start building our AI-powered assistant.\nDefining the prompts\nLike for the previous recipe, we will utilize a chat model that takes a list message as input. For this specific use case, however, we will supply the model with two messages: the system message and the user message. LangChain includes template objects for specific messages. Here are the steps to set up our prompts:\n\n1.  Under the root project, create a module called `prompting.py`. Let’s start the module by defining a template message that will be used as the system message:\n\n    ```", "```py\n\n    This is a common prompt for customer assistants that contains two variables: `question` and `context`. Those variables will be required to query the model.\n\n     2.  With that template, we can define the system message as follows:\n\n    ```", "```py\n\n     3.  The user message does not require specific context and can be defined as follows:\n\n    ```", "```py\n\n     4.  Then we can group both messages under the dedicated chat message `template` object as follows:\n\n    ```", "```py\n\nThis is all we need to set up the prompt object to query our model.\nIngesting and vectorizing the documents\nOur assistant will answer user questions by analyzing the documents we will provide to the model. Let’s create a `docs` folder under the project root that will contain the documents. First, download the `faq_ecotech.txt` file from the GitHub repository in the `ecotech_RAG/docs` project folder and save it in the local `docs` folder.\nYou can download it directly at [https://raw.githubusercontent.com/PacktPublishing/FastAPI-Cookbook/main/Chapter10/ecotech_RAG/docs/faq_ecotech.txt](https://raw.githubusercontent.com/PacktPublishing/FastAPI-Cookbook/main/Chapter10/ecotech_RAG/docs/faq_ecotech.txt).\nAlternatively, you can create your own FAQ file. Just ensure that each question and answer is separated by one empty line.\nThe information contained in the file will be used by our assistant to help the customers. However, to retrieve the information, we will need to split our documents into chunks and store them as vectors to optimize searching based on similarity.\nTo split the documents, we will use a character-based text splitter. To store chunks, we will use Chroma DB, an in-memory vector database.\nThen, let’s create a `documents.py` module that will contain the `load_documents` helper function that will upload the files into a variable as follows:\n\n```", "```py\n\n The `DirectoryLoader` class uploads the content of all the `.txt` files from the `docs` folder, then the `text_splitter` object reorganizes the documents into document chunks of `100` characters that will be then added to the `Chroma` database.\nBy utilizing the vectorized database alongside the user query, we can retrieve the relevant context to feed into our model, which will analyze the most significant portion.\nWe can write a function for this called `get_context` as follows:\n\n```", "```py\n\n The documents have to be stored and vectorized in numerical representations called embedding. This can be done with Chroma, an AI-native vector database.\nThen, through a similarity search operation (`db.similaratiry_search`) between the user query and the document chunks, we can retrieve the relevant content to pass as context to the model.\nWe have now retrieved the context to provide in the chat model system message template.\nBuilding the model chain\nOnce we have defined the mechanism to retrieve the context, we can build the chain model. Let’s build it through the following steps:\n\n1.  Let’s create a new module called `model.py`. Since we will use Cohere, we will upload the environment variables from the `.env` file with the `dotenv` package as follows:\n\n    ```", "```py\n\n     2.  Then we will define the model we are going to use:\n\n    ```", "```py\n\n    We will use the same module we used in the previous recipe, Command R+.\n\n     3.  Now we can gather the pieces we have created to leverage the power of LangChain by creating the chain pipeline to query the model as follows:\n\n    ```", "```py\n\nWe will use the chain object to create our endpoint to expose through the API.\nCreating the endpoint\nWe will make the `app` object instance with the endpoint in the `main.py` module under the project root folder. As always, let’s follow these steps to create it:\n\n1.  The operation of loading the documents can be quite CPU-intensive, especially in real-life applications. Therefore, we will define a lifespan context manager to execute this process only at server startup. The `lifespan` function will be structured as follows:\n\n    ```", "```py\n\n     2.  We can then pass it to the FastAPI object as follows:\n\n    ```", "```py\n\n     3.  Now, we can define a `POST /message` endpoint as follows:\n\n    ```", "```py\n\n     4.  The endpoint will accept a body string text as input and will return the response from the model as a string based on the documents provided in the `docs` folder at startup.\n\nTo test it, you can spin up the server from the following command:\n\n```", "```py\n\n Once the server has started, open the interactive documentation at `http://localhost:8000/docs` and you will see the `POST /message` endpoint we just created.\nTry first to send a message that is not related to the assistance, something like the following:\n\n```", "```py\n\n You will receive an answer like this:\n\n```", "```py\n\n Then try to ask, for example, the following:\n\n```", "```py\n\n You will get your assistance answer, which should be something like this:\n\n```", "```py\n\n You can double check that the answer is in line with what is written in the FAQ document in the `docs` folder.\nYou have just implemented a RAG AI-powered assistant with LangChain and FastAPI. You will now be able to implement your own AI assistant for your application.\nExercise\nWe have implemented the endpoint to interact with the chat model that will answer based on the document provided. However, real-life API applications will allow the addition of new documents interactively.\nCreate a new `POST /document` endpoint that will add a file in the `docs` folder and reload the documents in the code.\nHave a look at the *Working with file uploads and downloads* recipe in [*Chapter 2*](B21025_02.xhtml#_idTextAnchor052), *Working with Data*, to see how to upload files in FastAPI.\nSee also\nYou can have a look at the quickstart in the LangChain documentation:\n\n*   *LangChain* *Quickstart*: [https://python.langchain.com/v0.1/docs/get_started/quickstart/](https://python.langchain.com/v0.1/docs/get_started/quickstart/)\n\nWe have used Chroma, a vector database largely used for ML applications. Feel free to have a look at the documentation:\n\n*   *Chroma*: [https://docs.trychroma.com/](https://docs.trychroma.com/)\n\n```"]