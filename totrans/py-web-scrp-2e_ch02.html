<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Scraping the Data</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Scraping the Data</h1>
            </header>

            <article>
                
<p>In the previous chapter, we built a crawler which follows links to download the web pages we want. This is interesting but not useful-the crawler downloads a web page, and then discards the result. Now, we need to make this crawler achieve something by extracting data from each web page, which is known as <strong>scraping</strong>.</p>
<p>We will first cover browser tools&#160;to examine a web page, which you may already be familiar with if you have a web development background. Then, we will walk through three approaches to extract data from a web page using regular expressions, Beautiful Soup and lxml. Finally, the chapter will conclude with a comparison of these three scraping alternatives.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Analyzing a web page</li>
<li>Approaches to scrape a web page</li>
<li>Using the console</li>
<li>xpath selectors</li>
<li>Scraping results</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Analyzing a web page</h1>
            </header>

            <article>
                
<p>To understand how a web page is structured, we can try examining the source code. In most web browsers, the source code of a web page can be viewed by right-clicking on the page and selecting the <span class="packt_screen">View page source</span> option:</p>
<div class="CDPAlignCenter CDPAlign"><img height="441" width="351" class="aligncenter size-full wp-image-921 image-border" src="images/4364OS_02_01.jpg"/></div>
<p>For our example website, the data we are interested in is found on the country pages. Take a look at page source&#160;(via browser menu or right click browser menu). In the source for&#160;the example page for the United Kingdom (<a href="http://example.webscraping.com/view/United-Kingdom-239" target="_blank">http://example.webscraping.com/view/United-Kingdom-239</a>) you will find a table containing the country data (you can use search to find this in the page source code):</p>
<pre>&lt;table&gt; <br/>&lt;tr id="places_national_flag__row"&gt;&lt;td class="w2p_fl"&gt;&lt;label for="places_national_flag"      id="places_national_flag__label"&gt;National Flag:&lt;/label&gt;&lt;/td&gt;<br/>&lt;td class="w2p_fw"&gt;&lt;img src="/places/static/images/flags/gb.png" /&gt;&lt;/td&gt;&lt;td           class="w2p_fc"&gt;&lt;/td&gt;&lt;/tr&gt; <br/>... <br/>&lt;tr id="places_neighbours__row"&gt;&lt;td class="w2p_fl"&gt;&lt;label for="places_neighbours"      id="places_neighbours__label"&gt;Neighbours: &lt;/label&gt;&lt;/td&gt;&lt;td class="w2p_fw"&gt;&lt;div&gt;&lt;a href="/iso/IE"&gt;IE &lt;/a&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="w2p_fc"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
</pre>
<p>The lack of white space and formatting is not an issue for a web browser to interpret, but it is difficult for us to read. To help us interpret this table, we can&#160;use browser tools. To find your browser's developer tools, you can usually simply right click and select an option like <span class="packt_screen">Developer Tools</span>. Depending on the browser you use, you may have different developer tool options, but nearly every browser will have a tab titled <span class="packt_screen">Elements</span>&#160;or <span class="packt_screen">HTML</span>. In Chrome and Firefox, you can simply right click on an element on the page (what you are interested in scraping) and select <span class="packt_screen">Inspect Element</span>. For Internet Explorer, you need to open the <span class="packt_screen">Developer</span>&#160;toolbar by pressing <em>F12</em>. Then you can select items by clicking <em>Ctrl&#160;</em>+&#160;<em>B</em>. If you use a different browser without built-in developer tools, you may want to try&#160;the Firebug Lite extension, which is available for most web browsers at <a href="https://getfirebug.com/firebuglite" target="_blank"><span class="URLPACKT">https://getfirebug.com/firebuglite</span></a>.&#160;</p>
<p>When I right click on the table on the page and click <span class="packt_screen">Inspect Element</span> using Chrome, I see the following open panel with&#160;the surrounding HTML hierarchy of the selected element:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="images/chrome_inspect.png"/></div>
<p>In this&#160;screenshot, I can see that the <kbd>table</kbd> element sits inside a <kbd>form</kbd> element. I can also see that the attributes for the country are included in&#160;<kbd>tr</kbd>&#160; or table row elements with different CSS IDs (shown via the <kbd>id="places_national_flag__row"</kbd>). Depending on your browser, the coloring or layout might be different, but you should be able to click on the elements and navigate through the hierarchy to see the data on the page. If I expand the&#160;<kbd>tr</kbd> elements further by clicking on the arrows next to them, I notice the data for each of these rows is included is included within a <kbd>&lt;td&gt;</kbd> element of class <kbd>w2p_fw</kbd>, which is the child of a <kbd>&lt;tr&gt;</kbd> element, shown as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="172" width="496" class="image-border" src="images/expanded_elements.png"/></div>
<p>Now that we have investigated the page with our browser tools, we know the HTML hierarchy of the country data table, and have the necessary information to scrape that data from the page.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Three approaches to scrape a web page</h1>
            </header>

            <article>
                
<p>Now that we understand the structure of this web page we will investigate three different approaches to scraping its data, first with regular expressions, then with the popular <kbd>BeautifulSoup</kbd> module, and finally with the powerful <kbd>lxml</kbd> module.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Regular expressions</h1>
            </header>

            <article>
                
<p>If you are unfamiliar with regular expressions or need a reminder, there is a thorough overview available at <a href="https://docs.python.org/3/howto/regex.html" target="_blank">https://docs.python.org/3/howto/regex.html</a>. Even if you use regular expressions (or regex) with another programming language, I recommend stepping through it for a refresher on regex with Python.</p>
<div class="packt_tip"><span>Because each chapter might build or use parts of previous chapters, we recommend setting up your file</span> <span>structure</span> <span>similar to that in</span> <a href="https://github.com/kjam/wswp">the book repository</a><span>. All code can then be run from the&#160;</span><kbd>code</kbd> <span>directory in the repository so imports work properly. If you would like to set up a different</span> <span>structure</span><span>, note that you will need to change all imports from other chapters (such as the&#160;</span><kbd>from chp1.advanced_link_crawler&#160;</kbd> <span>in the following&#160;code).</span></div>
<p>To scrape the country area using regular expressions, we will first try matching the contents of the <kbd>&lt;td&gt;</kbd> element, as follows:</p>
<pre>&gt;&gt;&gt; import re <br/>&gt;&gt;&gt; from chp1.advanced_link_crawler import download <br/>&gt;&gt;&gt; url = 'http://example.webscraping.com/view/UnitedKingdom-239' <br/>&gt;&gt;&gt; html = download(url) <br/>&gt;&gt;&gt; re.findall(r'&lt;td class="w2p_fw"&gt;(.*?)&lt;/td&gt;', html) <br/>['&lt;img src="/places/static/images/flags/gb.png" /&gt;', <br/>  '244,820 square kilometres', <br/>  '62,348,447', <br/>  'GB', <br/>  'United Kingdom', <br/>  'London', <br/>  '&lt;a href="/continent/EU"&gt;EU&lt;/a&gt;', <br/>  '.uk', <br/>  'GBP', <br/>  'Pound', <br/>  '44', <br/>  '@# #@@|@## #@@|@@# #@@|@@## #@@|@#@ #@@|@@#@ #@@|GIR0AA', <br/>  '^(([A-Z]d{2}[A-Z]{2})|([A-Z]d{3}[A-Z]{2})|([A-Z]{2}d{2}     [A-Z]{2})|([A-Z]{2}d{3}[A-Z]{2})|([A-Z]d[A-Z]d[A-Z]{2})       |([A-Z]{2}d[A-Z]d[A-Z]{2})|(GIR0AA))$', <br/>  'en-GB,cy-GB,gd', <br/>  '&lt;div&gt;&lt;a href="/iso/IE"&gt;IE &lt;/a&gt;&lt;/div&gt;']
</pre>
<p>This result shows that the <kbd>&lt;td class="w2p_fw"&gt;</kbd> tag is used for multiple country attributes. If we simply wanted to scrape the country area, we can select the second matching element, as follows:</p>
<pre>&gt;&gt;&gt; re.findall('&lt;td class="w2p_fw"&gt;(.*?)&lt;/td&gt;', html)[1] <br/>'244,820 square kilometres'
</pre>
<p>This solution works but could easily fail if the web page is updated. Consider if this table is changed and the area is no longer in the second matching element. If we just need to scrape the data now, future changes can be ignored. However, if we want to re-scrape this data at some point, we want our solution to be as robust against layout changes as possible. To make this regular expression more specific, we can include the parent <kbd>&lt;tr&gt;</kbd> element, which has an ID, so it ought to be unique:</p>
<pre>&gt;&gt;&gt; re.findall('&lt;tr id="places_area__row"&gt;&lt;td class="w2p_fl"&gt;&lt;label for="places_area" id="places_area__label"&gt;Area: &lt;/label&gt;&lt;/td&gt;&lt;td class="w2p_fw"&gt;(.*?)&lt;/td&gt;', html) <br/>['244,820 square kilometres']
</pre>
<p>This iteration is better; however, there are many other ways the web page could be updated in a way that still breaks the regular expression. For example, double quotation marks might be changed to single, extra spaces could be added between the <kbd>&lt;td&gt;</kbd> tags, or the <kbd>area_label</kbd> could be changed. Here is an improved version to try and support these various possibilities:</p>
<pre>&gt;&gt;&gt; re.findall('''&lt;tr id="places_area__row"&gt;.*?&lt;tds*class=["']w2p_fw["']&gt;(.*?)&lt;/td&gt;''', html) ['244,820 square kilometres']
</pre>
<p>This regular expression is more future-proof but is difficult to construct, and quite&#160;unreadable. Also, there are still plenty of other minor layout changes that would break it, such as if a title attribute was added to the <kbd>&lt;td&gt;</kbd> tag or if the <kbd>tr</kbd> or&#160;<kbd>td</kbd> elements changed their&#160;CSS classes or IDs.</p>
<p>From this example, it is clear that regular expressions provide a quick way to scrape data but are too brittle and easily break when a web page is updated. Fortunately, there are better data extraction solutions such as the other scraping libraries we will cover throughout this chapter.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Beautiful Soup</h1>
            </header>

            <article>
                
<p><strong>Beautiful Soup</strong> is a popular library&#160;that parses a web page and provides a convenient interface to navigate content. If you do not already have this module, the latest version can be installed using this command:</p>
<pre><strong>    pip install beautifulsoup4</strong>
</pre>
<p>The first step with Beautiful Soup is to parse the downloaded HTML into a soup document. Many web pages do not contain perfectly valid HTML and Beautiful Soup needs to correct improper open and close tags. For example, consider this simple web page containing a list with missing attribute quotes and closing tags:</p>
<pre>        &lt;ul class=country&gt; <br/>            &lt;li&gt;Area <br/>            &lt;li&gt;Population <br/>        &lt;/ul&gt;
</pre>
<p>If the <kbd>Population</kbd> item is interpreted as a child of the <kbd>Area</kbd> item instead of the list, we could get unexpected results when scraping. Let us see how Beautiful Soup handles this:</p>
<pre>&gt;&gt;&gt; from bs4 import BeautifulSoup <br/>&gt;&gt;&gt; from pprint import pprint<br/>&gt;&gt;&gt; broken_html = '&lt;ul class=country&gt;&lt;li&gt;Area&lt;li&gt;Population&lt;/ul&gt;' <br/>&gt;&gt;&gt; # parse the HTML <br/>&gt;&gt;&gt; soup = BeautifulSoup(broken_html, 'html.parser') <br/>&gt;&gt;&gt; fixed_html = soup.prettify() <br/>&gt;&gt;&gt; pprint(fixed_html)<br/><br/>&lt;ul class="country"&gt;<br/> &lt;li&gt;<br/>  Area<br/>  &lt;li&gt;<br/>   Population<br/>  &lt;/li&gt;<br/> &lt;/li&gt;<br/>&lt;/ul&gt;
</pre>
<p>We can see that using the default <kbd>html.parser</kbd> did not result in properly parsed HTML. We can see from the previous snippet that it has used nested&#160;<kbd>li</kbd> elements, which might make it difficult to navigate. Luckily there are more options for parsers. We can install <strong>LXML</strong> (as described in the next section) or we can also use <strong>html5lib</strong>.&#160;To install&#160;<strong>html5lib</strong>, simply use pip:</p>
<pre><strong>pip install html5lib</strong>
</pre>
<p>Now, we can repeat this code, changing only the parser like so:</p>
<pre>&gt;&gt;&gt; soup = BeautifulSoup(broken_html, 'html5lib') <br/>&gt;&gt;&gt; fixed_html = soup.prettify() <br/>&gt;&gt;&gt; pprint(fixed_html)<br/>&lt;html&gt;<br/>   &lt;head&gt;<br/>   &lt;/head&gt;<br/>   &lt;body&gt;<br/>     &lt;ul class="country"&gt;<br/>       &lt;li&gt;<br/>         Area<br/>       &lt;/li&gt;<br/>       &lt;li&gt;<br/>         Population<br/>       &lt;/li&gt;<br/>     &lt;/ul&gt;<br/>   &lt;/body&gt;<br/>&lt;/html&gt;
</pre>
<p>&#160;Here, <kbd>BeautifulSoup</kbd>&#160;using <kbd>html5lib</kbd>&#160;was able to correctly interpret the missing attribute quotes and closing tags, as well as add the <kbd>&lt;html&gt;</kbd> and <kbd>&lt;body&gt;</kbd> tags to form a complete HTML document. You should see similar results if you used <kbd>lxml</kbd>.&#160;</p>
<p>Now, we can navigate to the elements we want using the <kbd>find()</kbd> and <kbd>find_all()</kbd> methods:</p>
<pre>&gt;&gt;&gt; ul = soup.find('ul', attrs={'class':'country'}) <br/>&gt;&gt;&gt; ul.find('li')  # returns just the first match <br/>&lt;li&gt;Area&lt;/li&gt; <br/>&gt;&gt;&gt; ul.find_all('li')  # returns all matches <br/>[&lt;li&gt;Area&lt;/li&gt;, &lt;li&gt;Population&lt;/li&gt;]
</pre>
<div class="packt_infobox">For a full list of available methods and parameters, the official Beautiful Soup documentation is available at <a href="http://www.crummy.com/software/BeautifulSoup/bs4/doc/" target="_blank">http://www.crummy.com/software/BeautifulSoup/bs4/doc/</a>.</div>
<p>Now, using these techniques, here is a full example to extract the country area from our example website:</p>
<pre>&gt;&gt;&gt; from bs4 import BeautifulSoup <br/>&gt;&gt;&gt; url = 'http://example.webscraping.com/places/view/United-Kingdom-239' <br/>&gt;&gt;&gt; html = download(url) <br/>&gt;&gt;&gt; soup = BeautifulSoup(html)   <br/>&gt;&gt;&gt; # locate the area row <br/>&gt;&gt;&gt; tr = soup.find(attrs={'id':'places_area__row'}) <br/>&gt;&gt;&gt; td = tr.find(attrs={'class':'w2p_fw'})  # locate the data element<br/>&gt;&gt;&gt; area = td.text  # extract the text from the data element<br/>&gt;&gt;&gt; print(area) <br/>244,820 square kilometres
</pre>
<p>This code is more verbose than regular expressions but easier to construct and understand. Also, we no longer need to worry about problems in minor layout changes, such as extra white space or tag attributes. We also know if the page contains broken HTML that Beautiful Soup can help clean the page and allow us to extract data from very broken website code.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Lxml</h1>
            </header>

            <article>
                
<p><strong>Lxml</strong> is a Python library&#160;built on top&#160;of the <kbd>libxml2</kbd> XML parsing library written in C, which helps make it faster than Beautiful Soup but also harder to install on some computers, specifically Windows. The latest installation instructions are available at <a href="http://lxml.de/installation.html" target="_blank"><span class="URLPACKT">http://lxml.de/installation.html</span></a>. If you run into difficulties installing the library on your own, you can also use Anaconda to do so: &#160;<a href="https://anaconda.org/anaconda/lxml" target="_blank">https://anaconda.org/anaconda/lxml</a>.</p>
<p>If you are unfamiliar with Anaconda, it is a package and environment manager primarily focused on open data science packages built by the folks at Continuum Analytics. You can download and install Anaconda by following their setup instructions here:&#160;<a href="https://www.continuum.io/downloads" target="_blank">https://www.continuum.io/downloads</a>. Note that using the Anaconda quick install will set your&#160;<kbd>PYTHON_PATH</kbd> to the Conda installation of Python.</p>
<p>As with Beautiful Soup, the first step when using <kbd>lxml</kbd>&#160;is parsing the potentially invalid HTML into a consistent format. Here is an example of parsing the same broken HTML:</p>
<pre>&gt;&gt;&gt; from lxml.html import fromstring, tostring<br/>&gt;&gt;&gt; broken_html = '&lt;ul class=country&gt;&lt;li&gt;Area&lt;li&gt;Population&lt;/ul&gt;' <br/>&gt;&gt;&gt; tree = fromstring(broken_html)  # parse the HTML  <br/>&gt;&gt;&gt; fixed_html = tostring(tree, pretty_print=True) <br/>&gt;&gt;&gt; print(fixed_html) <br/>&lt;ul class="country"&gt; <br/>    &lt;li&gt;Area&lt;/li&gt; <br/>    &lt;li&gt;Population&lt;/li&gt; <br/>&lt;/ul&gt;
</pre>
<p>As with <kbd>BeautifulSoup</kbd>, <kbd>lxml</kbd> was able to correctly parse the missing attribute quotes and closing tags, although it did not add the <kbd>&lt;html&gt;</kbd> and <kbd>&lt;body&gt;</kbd> tags. These are not requirements for standard XML and so are unnecessary for&#160;<kbd>lxml</kbd> to insert.</p>
<p>After parsing the input, <kbd>lxml</kbd> has a number of different options to select elements, such as XPath selectors and a <kbd>find()</kbd> method similar to Beautiful Soup. Instead, we will use CSS selectors here, because they are more compact and can be reused later in <a href="py-web-scrp-2e_ch05.html" target="_blank"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Dynamic Content</em> when parsing dynamic content. Some readers will already be familiar with them from their experience with jQuery selectors or use in front-end web application development. Later in this chapter we will compare performance of these selectors with XPath. To use CSS selectors, you might need to install the&#160;<kbd>cssselect</kbd> library like so:</p>
<pre><strong>pip install cssselect</strong>
</pre>
<p>Now we can&#160;use the <kbd>lxml</kbd> CSS selectors to extract the area data from the example page:</p>
<pre><strong>&gt;&gt;&gt; tree = fromstring(html) </strong><br/><strong>&gt;&gt;&gt; td = tree.cssselect('tr#places_area__row &gt; td.w2p_fw')[0] </strong><br/><strong>&gt;&gt;&gt; area = td.text_content() </strong><br/><strong>&gt;&gt;&gt; print(area) </strong><br/><strong>244,820 square kilometres</strong>
</pre>
<p>By using the&#160;<kbd>cssselect</kbd> method on our tree, we can utilize CSS syntax to select&#160;a table row element with the <kbd>places_area__row</kbd> ID, and then the child table data tag with the <kbd>w2p_fw</kbd> class. Since&#160;<kbd>cssselect</kbd> returns a list, we then index the first result and call the&#160;<kbd>text_content</kbd> method, which will iterate over all child elements and return concatenated text of each element. In this case, we only have one element, but this functionality is useful to know for more complex extraction examples.</p>
<p>You can see this code and the other code for this chapter in the book code repository: <a href="https://github.com/kjam/wswp/blob/master/code/chp2." target="_blank">https://github.com/kjam/wswp/blob/master/code/chp2.</a></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">CSS selectors and your Browser Console</h1>
            </header>

            <article>
                
<p>Like the notation we used to extract using <kbd>cssselect</kbd>,&#160;CSS selectors are patterns used for selecting HTML elements. Here are some examples of common selectors you should know:</p>
<pre>Select any tag: * <br/>Select by tag &lt;a&gt;: a <br/>Select by class of "link": .link  <br/>Select by tag &lt;a&gt; with class "link": a.link <br/>Select by tag &lt;a&gt; with ID "home": a#home <br/>Select by child &lt;span&gt; of tag &lt;a&gt;: a &gt; span <br/>Select by descendant &lt;span&gt; of tag &lt;a&gt;: a span <br/>Select by tag &lt;a&gt; with attribute title of "Home": a[title=Home]
</pre>
<p>The&#160;<kbd>cssselect</kbd> library&#160;implements most CSS3 selectors, and details on unsupported features (primarily browser interactions) are available at <a href="https://cssselect.readthedocs.io/en/latest/#supported-selectors" target="_blank">https://cssselect.readthedocs.io/en/latest/#supported-selectors</a>.</p>
<div class="packt_infobox">The CSS3 specification was produced by the W3C and is available for viewing at <a href="http://www.w3.org/TR/2011/REC-css3-selectors-20110929/" target="_blank">http://www.w3.org/TR/2011/REC-css3-selectors-20110929/</a>. There is also a useful and more accessible documentation from Mozilla on their developer's reference for CSS:&#160;<a href="https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors" target="_blank">https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors&#160;</a></div>
<p>Sometimes it is useful to test CSS selectors as we might not write them perfectly the first time. It is also a good idea to test them somewhere to debug any selection issues before writing many lines of Python code which may or may not work.</p>
<p>When a site uses JQuery, it's very easy to test CSS Selectors in the browser console. The console is a part of your browser developer tools and allows you to execute JavaScript (and, if supported, JQuery) on the current page.</p>
<div class="packt_infobox">To learn more about JQuery, there are several free online courses. The Code School course at&#160;<a href="http://try.jquery.com/" target="_blank">http://try.jquery.com/</a> has a variety of exercises if you are interested in diving a bit deeper.</div>
<p>The only syntax you need to know for using CSS selectors with JQuery is the simple object selection (i.e.&#160;<kbd>$('div.class_name');</kbd>). JQuery uses the&#160;<kbd>$</kbd> and parenthesis to select objects. Within the parenthesis you can write any CSS selector. Doing so in your browser console on a site that supports JQuery will allow you to look at the objects you have selected. Since we know the example website uses JQuery (either by inspecting the source code, or watching the&#160;Network tab&#160;and looking for JQuery to load, or using the&#160;<kbd>detectem</kbd> module), we can try selecting all&#160;<kbd>tr</kbd> elements using a CSS selector:</p>
<div class="CDPAlignCenter CDPAlign"><img height="250" width="296" class="image-border" src="images/console.png"/></div>
<p class="CDPAlignLeft CDPAlign">And simply by using the tag name, I can see every row for the country data. I can also try selecting elements using a longer CSS selector. Let's try selecting all&#160;<kbd>td</kbd> elements with class&#160;<kbd>w2p_fw</kbd>, since I know this is where the primary data on the page lies.</p>
<div class="CDPAlignCenter CDPAlign"><img height="357" width="315" class="image-border" src="images/td_console.png"/></div>
<p>You may also notice that when using your mouse to click on the returned elements, you can expand them and also highlight them in the above window (depending on what browser you are using). This is a tremendously useful way to test data. If the site you are scraping doesn't load JQuery or any other selector friendly libraries from your browser, you can perform the same lookups with the <kbd>document</kbd> object using simple JavaScript. The documentation for the <kbd>querySelector</kbd> method&#160;is available on <strong>Mozilla Developer Network</strong>:&#160;<a href="https://developer.mozilla.org/en-US/docs/Web/API/Document/querySelector" target="_blank">https://developer.mozilla.org/en-US/docs/Web/API/Document/querySelector</a>.&#160;</p>
<p>Even after using CSS selectors in your console and with <kbd>lxml</kbd>, it can be useful to learn XPath, which is what&#160;<kbd>lxml</kbd> converts all of your CSS selectors to before evaluating them.&#160;To keep learning how to use XPath, read on!</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">XPath Selectors</h1>
            </header>

            <article>
                
<p>There are times when using CSS selectors will not work. This is especially the case with very broken HTML or improperly formatted elements. Despite the best efforts of libraries like&#160;<kbd>BeautifulSoup</kbd> and&#160;<kbd>lxml</kbd> to properly parse and clean up the code; it will not always work - and in these cases, XPath can help you build very specific selectors based on hierarchical relationships of elements on the page.</p>
<p>XPath is a way of describing relationships as an hierarchy in XML documents. Because HTML is formed using XML elements, we can also use XPath to navigate and select elements from an HTML document.&#160;</p>
<div class="packt_infobox">To read more about XPath, check out the <strong>Mozilla developer documentation</strong>:&#160;<a href="https://developer.mozilla.org/en-US/docs/Web/XPath" target="_blank">https://developer.mozilla.org/en-US/docs/Web/XPath</a>.</div>
<p>XPath follows some basic syntax rules and has some similarities with CSS selectors. Take a look at the following chart for some quick references between the two.</p>
<table>
<tbody>
<tr>
<td><strong>Selector description</strong></td>
<td><strong>XPath Selector</strong></td>
<td><strong>CSS selector</strong></td>
</tr>
<tr>
<td>Select all links</td>
<td>'//a'</td>
<td>'a'</td>
</tr>
<tr>
<td>Select div with class "main"</td>
<td>'//div[@class="main"]'</td>
<td>'div.main'</td>
</tr>
<tr>
<td>Select ul with ID "list"</td>
<td>'//ul[@id="list"]'</td>
<td>'ul#list'</td>
</tr>
<tr>
<td>Select text from all paragraphs</td>
<td>'//p/text()'</td>
<td>'p'*</td>
</tr>
<tr>
<td>Select all divs which contain 'test' in the class</td>
<td>'//div[contains(@class, 'test')]'</td>
<td>'div [class*="test"]'</td>
</tr>
<tr>
<td>Select&#160;all divs with links or lists in them</td>
<td>'//div[a|ul] '</td>
<td>'div a, div ul'</td>
</tr>
<tr>
<td>Select a link with google.com in the href</td>
<td>'//a[contains(@href, "google.com")]</td>
<td>'a'*</td>
</tr>
</tbody>
</table>
<p>As you can see from the previous table, there are many similarities between the syntax. However, in the chart there are certain CSS selectors noted with a <kbd>*</kbd>. These indicate that it is not exactly possible to select these elements using CSS, and we have provided the best alternative. In these cases, if you were using&#160;<kbd>cssselect</kbd>&#160;you will need to do further manipulation or iteration within Python and/or <kbd>lxml</kbd>. Hopefully this comparison has shown an introduction to XPath and convinced you that it is more exacting and specific than simply using CSS.</p>
<p>Now that we have a basic introduction to the XPath syntax, let's see how we can use it for our example website:</p>
<pre><strong>&gt;&gt;&gt; tree = fromstring(html)</strong><br/><strong>&gt;&gt;&gt; area = tree.xpath('//tr[@id="places_area__row"]/td[@class="w2p_fw"]/text()')[0]</strong><br/><strong>&gt;&gt;&gt; print(area)</strong><br/><strong>244,820 square kilometres</strong>
</pre>
<p>Similar to CSS selectors, you can also test XPath selectors in your browser console. To do so, on a page with selectors simply use the <kbd>$x('pattern_here');</kbd> selector. Similarly, you can also use the&#160;<kbd>document</kbd> object from simple JavaScript and call the&#160;<kbd>evaluate</kbd> method.</p>
<div class="packt_infobox">The Mozilla developer network has a useful introduction to using XPath with JavaScript tutorial here: &#160;<a href="https://developer.mozilla.org/en-US/docs/Introduction_to_using_XPath_in_JavaScript" target="_blank">https://developer.mozilla.org/en-US/docs/Introduction_to_using_XPath_in_JavaScript</a></div>
<p>If we wanted to test looking for&#160;<kbd>td</kbd> elements with images in them to get the flag data from the country pages, we could test our XPath pattern in our browser first:</p>
<div class="CDPAlignCenter CDPAlign"><img height="431" width="427" class="image-border" src="images/xpath_console.png"/></div>
<p>Here we can see that we can use attributes to specify the data we want to extract (such as <kbd>@src</kbd>). By testing in the browser, we save debugging time by getting immediate and easy-to-read results.</p>
<p>We will be using both XPath and CSS selectors throughout this chapter and further chapters, so you can become more familiar with them and feel confident using them as you advance your web scraping capabilities.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">LXML and Family Trees</h1>
            </header>

            <article>
                
<p><kbd>lxml</kbd> also has the ability to traverse family trees within the HTML page. What is a family tree? When you used your browser's developer tools to investigate the elements on the page and you were able to expand or retract them, you were observing family relationships in the HTML. Every element on a web page can have parents, siblings and children. These relationships can help us more easily traverse the page.&#160;</p>
<p>For example, if I want to find all the elements at the same node depth level on the page, I would be looking for their siblings. Or maybe I want every element that is a child of a particular element on the page.&#160;<kbd>lxml</kbd> allows us to use many of these relationships with simple Python code.</p>
<p>As an example, let's investigate all children of the&#160;<kbd>table</kbd> element on the example page:</p>
<pre><strong>&gt;&gt;&gt; table = tree.xpath('//table')[0]</strong><br/><strong>&gt;&gt;&gt; table.getchildren()</strong><br/><strong>[&lt;Element tr at 0x7f525158ec78&gt;,</strong><br/><strong> &lt;Element tr at 0x7f52515ad638&gt;,</strong><br/><strong> &lt;Element tr at 0x7f52515ad5e8&gt;,</strong><br/><strong> &lt;Element tr at 0x7f52515ad688&gt;,</strong><br/><strong> &lt;Element tr at 0x7f52515ad728&gt;,</strong><br/><strong>...]</strong>
</pre>
<p>We can also see the table's siblings and parent elements:</p>
<pre><strong>&gt;&gt;&gt; prev_sibling = table.getprevious()</strong><br/><strong>&gt;&gt;&gt; print(prev_sibling)</strong><br/><strong>None</strong><br/><strong>&gt;&gt;&gt; next_sibling = table.getnext()</strong><br/><strong>&gt;&gt;&gt; print(next_sibling)</strong><br/><strong>&lt;Element div at 0x7f5252fe9138&gt;</strong><br/><strong>&gt;&gt;&gt; table.getparent()</strong><br/><strong>&lt;Element form at 0x7f52515ad3b8&gt;</strong>
</pre>
<p>If you need a more general way to access elements on the page, traversing the familial relationships combined with XPath expressions is a good way to ensure you don't miss any content. This can help you extract content from many different types of pages where you might be able to identify some important parts of the page simply by identifying content that appears near those elements on the page. This method will also work even when the elements do not have identifiable CSS selectors.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Comparing performance</h1>
            </header>

            <article>
                
<p>To help evaluate the trade-offs between the three scraping approaches described in the section, <em>Three approaches to scrape a web page</em>, it would be helpful to compare their relative efficiency. Typically, a scraper would extract multiple fields from a web page. So, for a more realistic comparison, we will implement extended versions of each scraper which extract all the available data from a country's web page. To get started, we need to return to our browser&#160;to check the format of the other country features, as shown here:</p>
<div class="CDPAlignCenter CDPAlign"><img height="343" width="538" class="aligncenter size-full wp-image-920 image-border" src="images/4364OS_02_04.jpg"/></div>
<p>By using our browser's inspect capabilities, we can see&#160;each table row has an ID starting with <kbd>places_</kbd> and ending with <kbd>__row</kbd>. The country data is contained within these rows in the same format as the area example. Here are implementations that use this information to extract all of the available country data:</p>
<pre>FIELDS = ('area', 'population', 'iso', 'country', 'capital', 'continent', 'tld', 'currency_code', 'currency_name', 'phone', 'postal_code_format', 'postal_code_regex', 'languages', 'neighbours') <br/><br/>import re <br/>def re_scraper(html): <br/>    results = {} <br/>    for field in FIELDS: <br/>        results[field] = re.search('&lt;tr id="places_%s__row"&gt;.*?&lt;td class="w2p_fw"&gt;(.*?)&lt;/td&gt;' % field, html).groups()[0] <br/>    return results <br/><br/>from bs4 import BeautifulSoup <br/>def bs_scraper(html): <br/>    soup = BeautifulSoup(html, 'html.parser') <br/>    results = {} <br/>    for field in FIELDS: <br/>        results[field] = soup.find('table').find('tr',id='places_%s__row' % field).find('td',                  class_='w2p_fw').text <br/>    return results <br/><br/>from lxml.html import fromstring<br/>def lxml_scraper(html): <br/>    tree = fromstring(html) <br/>    results = {} <br/>    for field in FIELDS: <br/>        results[field] = tree.cssselect('table &gt; tr#places_%s__row &gt; td.w2p_fw' % field)[0].text_content() <br/>    return results <br/><br/>def lxml_xpath_scraper(html):<br/>    tree = fromstring(html)<br/>    results = {}<br/>    for field in FIELDS:<br/>        results[field] = tree.xpath('//tr[@id="places_%s__row"]/td[@class="w2p_fw"]' % field)[0].text_content()<br/>    return results
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Scraping results</h1>
            </header>

            <article>
                
<p>Now that we have complete implementations for each scraper, we will test their relative performance with this snippet. The imports in the code expect your directory structure&#160;to be similar to the book's repository, so please adjust as necessary:</p>
<pre>import time<br/>import re<br/>from chp2.all_scrapers import re_scraper, bs_scraper, <br/>    lxml_scraper, lxml_xpath_scraper<br/>from chp1.advanced_link_crawler import download<br/><br/>NUM_ITERATIONS = 1000 # number of times to test each scraper<br/>html = download('http://example.webscraping.com/places/view/United-Kingdom-239')<br/><br/>scrapers = [<br/>   ('Regular expressions', re_scraper),<br/>   ('BeautifulSoup', bs_scraper),<br/>   ('Lxml', lxml_scraper),<br/>   ('Xpath', lxml_xpath_scraper)]<br/><br/>for name, scraper in scrapers:<br/>    # record start time of scrape<br/>    start = time.time()<br/>    for i in range(NUM_ITERATIONS):<br/>        if scraper == re_scraper:<br/>            re.purge()<br/>        result = scraper(html)<br/>        # check scraped result is as expected<br/>        assert result['area'] == '244,820 square kilometres'<br/>    # record end time of scrape and output the total<br/>    end = time.time()<br/>    print('%s: %.2f seconds' % (name, end - start))
</pre>
<p>This example will run each scraper 1000 times, check whether the scraped results are as expected, and then print the total time taken. The <kbd>download</kbd> function used here is the one defined in the preceding chapter. Note the highlighted line calling <kbd>re.purge()</kbd>; by default, the regular expression module will cache searches and this cache needs to be cleared to make a fair comparison with the other scraping approaches.</p>
<p>Here are the results from running this script on my computer:</p>
<pre>    <strong>$ python chp2/test_scrapers.py </strong><br/>    Regular expressions: 1.80 seconds<br/>    BeautifulSoup: 14.05 seconds<br/>    Lxml: 3.08 seconds<br/>    Xpath: 1.07 seconds
</pre>
<p>The results on your computer will quite likely be different because of the different hardware used. However, the relative difference between each approach should be similar. The results show Beautiful Soup is over six times slower than the other approaches when used to scrape our example web page. This result could be anticipated because <kbd>lxml</kbd> and the regular expression module were written in C, while <kbd>BeautifulSoup</kbd> is pure Python. An interesting fact is that <kbd>lxml</kbd> performed comparatively well with regular expressions, since <kbd>lxml</kbd> has the additional overhead of having to parse the input into its internal format before searching for elements. When scraping many features from a web page, this initial parsing overhead is reduced and <kbd>lxml</kbd> becomes even more competitive. As we can see with the XPath parser, <kbd>lxml</kbd> is able to directly compete with regular expressions.&#160;It really is an amazing module!</p>
<div class="packt_infobox">Although we strongly encourage you to use&#160;<kbd>lxml</kbd> for parsing, the biggest performance bottleneck for web scraping is usually the network. We will discuss approaches to parallelize workflows, allowing you to increase the speed of your crawlers by having multiple requests work in parallel.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Overview of Scraping</h1>
            </header>

            <article>
                
<p>The following table summarizes the advantages and disadvantages of each approach to scraping:</p>
<table class="table">
<tbody>
<tr>
<td><strong>Scraping approach</strong></td>
<td><strong>Performance</strong></td>
<td><strong>Ease of use</strong></td>
<td><strong>Ease to install</strong></td>
</tr>
<tr>
<td>Regular expressions</td>
<td>Fast</td>
<td>Hard</td>
<td>Easy (built-in module)</td>
</tr>
<tr>
<td>Beautiful Soup</td>
<td>Slow</td>
<td>Easy</td>
<td>Easy (pure Python)</td>
</tr>
<tr>
<td>Lxml</td>
<td>Fast</td>
<td>Easy</td>
<td>Moderately difficult</td>
</tr>
</tbody>
</table>
<p>If speed is not an issue to you and you prefer to only install libraries via pip, it would not be a problem to use a slower approach, such as Beautiful Soup. Or, if you just need to scrape a small amount of data and want to avoid additional dependencies, regular expressions might be an appropriate choice. However, in general, <kbd>lxml</kbd> is the best choice for scraping, because it is fast and robust, while regular expressions and Beautiful Soup are not as speedy or as easy to modify.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Adding a scrape callback to the link crawler</h1>
            </header>

            <article>
                
<p>Now that we know how to scrape the country data, we can integrate this into the link crawler built in <a href="py-web-scrp-2e_ch01.html" target="_blank">Chapter 1</a>, <em>Introduction to Web Scraping</em>. To allow reusing the same crawling code to scrape multiple websites, we will add a <kbd>callback</kbd> parameter to handle the scraping. A <kbd>callback</kbd> is a function that will be called after certain events (in this case, after a web page has been downloaded). This scrape <kbd>callback</kbd> will take a <kbd>url</kbd> and <kbd>html</kbd> as parameters and optionally return a list of further URLs to crawl. Here is the implementation, which is simple in Python:</p>
<pre>def link_crawler(..., scrape_callback=None): <br/>    ... <br/>    data = [] <br/>    if scrape_callback: <br/>        data.extend(scrape_callback(url, html) or []) <br/>        ...
</pre>
<p>The new code for the scraping <kbd>callback</kbd> function are highlighted in the preceding snippet, and the full source code for this version of the link crawler is available at <a href="https://github.com/kjam/wswp/blob/master/code/chp2/advanced_link_crawler.py" target="_blank"><span class="URLPACKT">https://github.com/kjam/wswp/blob/master/code/chp2/advanced_link_crawler.py</span></a>.</p>
<p>Now, this crawler can be used to scrape multiple websites by customizing the function passed to <kbd>scrape_callback</kbd>. Here is a modified version of the <kbd>lxml</kbd> example scraper that can be used for the <kbd>callback</kbd> function:</p>
<pre>def scrape_callback(url, html): <br/>    fields = ('area', 'population', 'iso', 'country', 'capital',<br/>              'continent', 'tld', 'currency_code', 'currency_name',<br/>              'phone', 'postal_code_format', 'postal_code_regex',<br/>              'languages', 'neighbours')<br/>    if re.search('/view/', url): <br/>        tree = fromstring(html) <br/>        all_rows = [<br/>            tree.xpath('//tr[@id="places_%s__row"]/td[@class="w2p_fw"]' % field)[0].text_content()<br/>            for field in fields] <br/>        print(url, all_rows)
</pre>
<p>This <kbd>callback</kbd> function will scrape the country data and print it out. We can test it by importing the two functions and calling them with our regular expression and URL:</p>
<pre>&gt;&gt;&gt; from chp2.advanced_link_crawler import link_crawler, scrape_callback<br/>&gt;&gt;&gt; link_crawler('http://example.webscraping.com', '/(index|view)/', scrape_callback=scrape_callback)
</pre>
<p>You should now see output showing the downloading of pages as well as some rows showing the URL and scraped data, like so:</p>
<pre>Downloading: http://example.webscraping.com/view/Botswana-30<br/>http://example.webscraping.com/view/Botswana-30 ['600,370 square kilometres', '2,029,307', 'BW', 'Botswana', 'Gaborone', 'AF', '.bw', 'BWP', 'Pula', '267', '', '', 'en-BW,tn-BW', 'ZW ZA NA ']
</pre>
<p>Usually, when scraping a website, we want to reuse the data rather than simply print it, so we will extend this example to save results to a CSV spreadsheet, as follows:</p>
<pre>import csv <br/>import re<br/>from lxml.html import fromstring<br/>class CsvCallback: <br/>    def __init__(self): <br/>        self.writer = csv.writer(open('../data/countries.csv', 'w')) <br/>        self.fields = ('area', 'population', 'iso', 'country', <br/>                       'capital', 'continent', 'tld', 'currency_code', 'currency_name', <br/>                       'phone', 'postal_code_format', 'postal_code_regex', <br/>                       'languages', 'neighbours') <br/>        self.writer.writerow(self.fields) <br/><br/>    def __call__(self, url, html): <br/>        if re.search('/view/', url): <br/>            tree = fromstring(html)<br/>            all_rows = [<br/>                tree.xpath(<br/>                  '//tr[@id="places_%s__row"]/td[@class="w2p_fw"]' % field)[0].text_content()<br/>                for field in self.fields] <br/>             self.writer.writerow(all_rows)
</pre>
<p>To build this <kbd>callback</kbd>, a class was used instead of a function so that the state of the <kbd>csv</kbd> writer could be maintained. This <kbd>csv</kbd> writer is instantiated in the constructor, and then written to multiple times in the <kbd>__call__</kbd> method. Note that <kbd>__call__</kbd> is a special method that is invoked when an object is "called" as a function, which is how the <kbd>cache_callback</kbd> is used in the link crawler. This means that <kbd>scrape_callback(url, html)</kbd> is equivalent to calling <kbd>scrape_callback.__call__(url, html)</kbd>. For further details on Python's special class methods, refer to <a href="https://docs.python.org/3/reference/datamodel.html#special-method-names"><span class="URLPACKT">https://docs.python.org/3/reference/datamodel.html#special-method-names</span></a>.</p>
<p>Here is how to pass this callback to the link crawler:</p>
<pre><strong>&gt;&gt;&gt; from chp2.advanced_link_crawler import link_crawler</strong><br/><strong>&gt;&gt;&gt; from chp2.csv_callback import CsvCallback</strong><br/><strong>&gt;&gt;&gt; link_crawler('http://example.webscraping.com/', '/(index|view)', max_depth=-1, scrape_callback=CsvCallback())</strong>
</pre>
<p>Note that the <kbd>CsvCallback</kbd> expects there to be a <kbd>data</kbd> directory on the same level as the parent folder from where you are running the code. This can also be modified, but we advise you to follow good coding practices and keep your code and data separate -- allowing you to keep your code under version control while having your <kbd>data</kbd> folder in the&#160;<kbd>.gitignore</kbd> file. Here's an example directory structure:</p>
<pre>wswp/<br/>|-- code/<br/>|    |-- chp1/<br/>|    |    + (code files from chp 1)<br/>|    +-- chp2/<br/>|         + (code files from chp 2)<br/>|-- data/<br/>|    + (generated data files)<br/>|-- README.md<br/>+-- .gitignore
</pre>
<p>&#160;Now, when the crawler is run with this <kbd>scrape_callback</kbd>, it will save results to a CSV file that can be viewed in an application such as Excel or LibreOffice. It might take a bit longer to run than the first time, as it is actively collecting information. When the scraper exits, you should be able to view your CSV with all the data:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="images/countries.png"/></div>
<p>Success! We have completed our first working scraper.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we walked through a variety of ways to scrape data from a web page. Regular expressions can be useful for a one-off scrape or to avoid the overhead of parsing the entire web page, and <kbd>BeautifulSoup</kbd> provides a high-level interface while avoiding any difficult dependencies. However, in general, <kbd>lxml</kbd> will be the best choice because of its speed and extensive functionality, so we will use it in future examples.</p>
<p>We also learned how to inspect HTML pages using browser tools and the console and define CSS selectors and XPath selectors to match and extract content from the downloaded pages.</p>
<p>In the next chapter we will introduce caching, which allows us to save web pages so they only need be downloaded the first time a crawler is run.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>
</body>
</html>