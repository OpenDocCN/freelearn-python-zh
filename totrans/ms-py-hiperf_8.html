<html><head></head><body><div class="chapter" title="Chapter&#xA0;8.&#xA0;Putting It All into Practice"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Putting It All into Practice</h1></div></div></div><p>Welcome to the last chapter of the book. If you've made it this far, you've gone over several optimization techniques, both specific to the Python programming language and generic ones applicable to other similar technologies.</p><p>You've also read about tools for profiling and visualizing those results. We also delved into one specific use case for Python, which is number crunching for scientific purposes. You learned about the tools that allow you to optimize the performance of your code.</p><p>In this final chapter, we'll go over one practical use case that covers all the technologies we covered in the earlier chapters (remember that some of the tools we've seen are alternatives, so using all of them is not really a good plan). We will write an initial version of the code, measure its performance, and then go through the optimization process to finally rewrite the code and measure the performance again.</p><div class="section" title="The problem to solve"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec39"/>The problem to solve</h1></div></div></div><p>Before we even <a id="id437" class="indexterm"/>start thinking about writing the initial version of our code, we need to understand the problem we're trying to solve.</p><p>Given the scope of the book, a full-blown application might be too big an undertaking, so we'll focus on a small task. It'll give us better control over what we want to do, and we won't run the risk of having too many things to optimize at the same time.</p><p>To keep things interesting, we'll split the problem into the following two parts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Part 1</strong></span>: This will take care of finding the data we want to process. It won't just be a dataset we download from some given URL. Instead, we'll scrape it from the Web.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Part 2</strong></span>: This will focus on processing the data obtained after solving the first part of the problem. In this step, we may perform the most CPU-intensive computations and calculate some statistics from the data gathered.</li></ul></div><p>In both cases, we'll create an initial version of the code that solves the problem without taking performance into account. Afterwards, we'll analyze each solution individually and try to improve <a id="id438" class="indexterm"/>them as much as we can.</p><div class="section" title="Getting data from the Web"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec69"/>Getting data from the Web</h2></div></div></div><p>The site we'll <a id="id439" class="indexterm"/>scrape is <span class="strong"><strong>Science Fiction &amp; Fantasy</strong></span> (<a class="ulink" href="http://scifi.stackexchange.com/">http://scifi.stackexchange.com/</a>). The site is dedicated to answering questions about sci-fi and fantasy topics. It is much like StackOverflow but meant for sci-fi and fantasy geeks.</p><p>To be more <a id="id440" class="indexterm"/>specific, we'll want to scrape the list of latest questions. For each question, we'll get the page with the question's text and all the available answers. After all the scraping and parsing is done, we'll save the relevant information in the JSON format for easier postprocessing.</p><p>Remember that we'll deal with HTML pages. However, we don't want that. We want to strip away all HTML code and save only the following items:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The question's title</li><li class="listitem" style="list-style-type: disc">The question's author</li><li class="listitem" style="list-style-type: disc">The question's body (the actual text of the question)</li><li class="listitem" style="list-style-type: disc">The body of the answers (if there are any)</li><li class="listitem" style="list-style-type: disc">The answer's author</li></ul></div><p>With this information, we'll be able to do some interesting postprocessing and get some relevant statistics (more on that in a minute).</p><p>Here is a quick example of how the output of this script should look:</p><div class="informalexample"><pre class="programlisting">{
  "questions": [
    {
      "title": "Ending of John Carpenter's The Thing",
      "body": "In the ending of John Carpenter's classic 1982 sci-fi horror film The Thing, is ...",
      "author": "JMFB",
      "answers": [
        {
          "body": "This is the million dollar question, ... Unfortunately, he is notoriously ... ",
           "author": "Richard",
        },
        {
          "body": "Not to point out what may seem obvious, but Childs isn't breathing. Note the total absence of ",
          "author": "user42"
          }
      ]
    },
    {
      "title": "Was it ever revealed what pedaling the bicycles in the second episode was doing?",
      "body": "I'm going to assume they were probably some sort of turbine...electricity...something, but I'd prefer to know for sure.",
       "author": "bartz",
      "answers": [
        {
          "body": "The Wikipedia synopsis states: most citizens make a living pedaling exercise bikes all day in order to generate power for their environment",
          "author": "Jack Nimble"
        }
      ]
    }
  ]
}</pre></div><p>This script will take care of saving all the information into one single JSON file, which will be predefined inside its code.</p><p>We'll try to keep the initial version of both scripts simple. This means using the least amount of modules. In this case, the main list of modules will be as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Beautiful Soup</strong></span> (<a class="ulink" href="http://www.crummy.com/software/BeautifulSoup/">http://www.crummy.com/software/BeautifulSoup/</a>): This is used to parse the HTML files, mainly because it provides a full parsing API, automatic encoding detection (which, if you've being in this business long enough, you've probably come to hate) and the ability to use selectors to traverse the parsed tree.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Requests</strong></span> (<a class="ulink" href="http://docs.python-requests.org/en/latest/">http://docs.python-requests.org/en/latest/</a>): This is used to make <a id="id441" class="indexterm"/>HTTP requests. Although Python already provides the required modules for this, this module simplifies the API and provides a more Pythonic way of handling this task.</li></ul></div><p>You can install both <a id="id442" class="indexterm"/>modules using the <code class="literal">pip</code> command-line tool:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ pip  install requests  beautifulsoup4</strong></span>
</pre></div><p>The following screenshot shows an example of the pages we'll be scraping and parsing in order to get the data:</p><div class="mediaobject"><img src="graphics/B02088_08_01.jpg" alt="Getting data from the Web"/></div></div><div class="section" title="Postprocessing the data"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec70"/>Postprocessing the data</h2></div></div></div><p>The second <a id="id443" class="indexterm"/>script will take care of reading the JSON-encoded file and getting some stats out of it. Since we want to make it interesting, we won't limit ourselves to just counting the number of questions per user (although we will get this stat as well). We'll also calculate the following elements:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Top ten users with most questions</li><li class="listitem" style="list-style-type: disc">Top ten users with most answers</li><li class="listitem" style="list-style-type: disc">Most common topics asked about</li><li class="listitem" style="list-style-type: disc">The shortest answer</li><li class="listitem" style="list-style-type: disc">Top ten most common phrases</li><li class="listitem" style="list-style-type: disc">Top ten most answered questions</li></ul></div><p>Since this <a id="id444" class="indexterm"/>book's main topic is performance and not <span class="strong"><strong>Natural Language Processing</strong></span> (<span class="strong"><strong>NLP</strong></span>), we will not delve into the details of the small amount of NLP that this script will have. Instead, we'll just limit ourselves to improving the performance based on what we've seen so far about Python.</p><p>The only non-built-in <a id="id445" class="indexterm"/>module we'll use in the first version of this script is <a id="id446" class="indexterm"/>
<span class="strong"><strong>NLTK</strong></span> (<a class="ulink" href="http://www.nltk.org">http://www.nltk.org</a>) to handle all the NLP functionalities.</p></div></div></div>
<div class="section" title="The initial code base"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec40"/>The initial code base</h1></div></div></div><p>Let's now list all <a id="id447" class="indexterm"/>of the code that we'll optimize in future, based on the earlier description.</p><p>The first of the following points is quite simple: a single file script that takes care of scraping and saving in JSON format like we discussed earlier. The flow is simple, and the order is as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">It will query the list of questions page by page.</li><li class="listitem">For each page, it will gather the question's links.</li><li class="listitem">Then, for each link, it will gather the information listed from the previous points.</li><li class="listitem">It will move on to the next page and start over again.</li><li class="listitem">It will finally save all of the data into a JSON file.</li></ol></div><p>The code is as follows:</p><div class="informalexample"><pre class="programlisting">from bs4 import BeautifulSoup
import requests
import json


SO_URL = "http://scifi.stackexchange.com"
QUESTION_LIST_URL = SO_URL + "/questions"
MAX_PAGE_COUNT = 20

global_results = []
initial_page = 1 #first page is page 1

def get_author_name(body):
  link_name = body.select(".user-details a")
  if len(link_name) == 0:
    text_name = body.select(".user-details")
    return text_name[0].text if len(text_name) &gt; 0 else 'N/A'
  else:
    return link_name[0].text

def get_question_answers(body):
  answers = body.select(".answer")
  a_data = []
  if len(answers) == 0:
    return a_data

  for a in answers:
    data = {
      'body': a.select(".post-text")[0].get_text(),
      'author': get_author_name(a)
    }
    a_data.append(data)
  return a_data

def get_question_data ( url ):
  print "Getting data from question page: %s " % (url)
  resp = requests.get(url)
  if resp.status_code != 200:
    print "Error while trying to scrape url: %s" % (url)
    return
  body_soup = BeautifulSoup(resp.text)
  #define the output dict that will be turned into a JSON structue
  q_data = {
    'title': body_soup.select('#question-header .question-hyperlink')[0].text,
    'body': body_soup.select('#question .post-text')[0].get_text(),
    'author': get_author_name(body_soup.select(".post-signature.owner")[0]),
    'answers': get_question_answers(body_soup)
  }
  return q_data


def get_questions_page ( page_num, partial_results ):
  print "====================================================="
  print " Getting list of questions for page %s" % (page_num)
  print "====================================================="

  url = QUESTION_LIST_URL + "?sort=newest&amp;page=" + str(page_num)
  resp = requests.get(url)
  if resp.status_code != 200:
    print "Error while trying to scrape url: %s" % (url)
    return
  body = resp.text
  main_soup = BeautifulSoup(body)

  #get the urls for each question
  questions = main_soup.select('.question-summary .question-hyperlink')
  urls = [ SO_URL + x['href'] for x in questions]
  for url in urls:
    q_data = get_question_data(url)
    partial_results.append(q_data)
  if page_num &lt; MAX_PAGE_COUNT:
    get_questions_page(page_num + 1, partial_results)


get_questions_page(initial_page, global_results)
with open('scrapping-results.json', 'w') as outfile:
  json.dump(global_results, outfile, indent=4)

print '----------------------------------------------------'
print 'Results saved'</pre></div><p>By looking at the preceding code, you'll notice that we kept our promise. Right now, we're only using the proposed external modules, plus the JSON module, which comes built-in with Python.</p><p>The second script, on the <a id="id448" class="indexterm"/>other hand, is split into two, mainly for organizational purposes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">analyzer.py</code>: This file contains the main code. It takes care of loading the JSON file into a <code class="literal">dict</code> structure and performs a series of calculations.</li><li class="listitem" style="list-style-type: disc"><code class="literal">visualizer.py</code>: This file simply contains a set of functions used to visualize the different results from the analyzer.</li></ul></div><p>Let's now take a look at the code in both these files. The first set of functions will be the utility functions used to <a id="id449" class="indexterm"/>sanitize the data, load it into memory, and so on:</p><div class="informalexample"><pre class="programlisting">#analyzer.py
import operator
import string
import nltk
from nltk.util import ngrams
import json
import re
import visualizer


SOURCE_FILE = './scrapping-results.json'


# Load the json file and return the resulting dict
def load_json_data(file):
  with open(file) as input_file:
    return json.load(input_file)

def analyze_data(d):
  return {
    'shortest_answer': get_shortest_answer(d),
    'most_active_users': get_most_active_users(d, 10),
    'most_active_topics': get_most_active_topics(d, 10),
    'most_helpful_user': get_most_helpful_user(d, 10),
    'most_answered_questions': get_most_answered_questions(d, 10),
    'most_common_phrases':  get_most_common_phrases(d, 10, 4),
  }


# Creates a single, lower cased string from the bodies of all questions
def flatten_questions_body(data):
  body = []
  for q in data:
    body.append(q['body'])
  return '. '.join(body)


# Creates a single, lower cased string from the titles of all questions
def flatten_questions_titles(data):
  body = []
  pattern = re.compile('(\[|\])')
  for q in data:
    lowered = string.lower(q['title'])
    filtered = re.sub(pattern, ' ', lowered)
    body.append(filtered)
  return '. '.join(body)</pre></div><p>The following set of functions are the ones that actually performs the <span class="emphasis"><em>counting</em></span> of data and gets the statistics we want by <a id="id450" class="indexterm"/>analyzing the JSON in different ways:</p><div class="informalexample"><pre class="programlisting"># Returns the top "limit" users with the most questions asked
def get_most_active_users(data, limit):
  names = {}
  for q in data:
    if q['author'] not in names:
      names[q['author']] = 1
    else:
      names[q['author']] += 1
  return sorted(names.items(), reverse=True, key=operator.itemgetter(1))[:limit]

def get_node_content(node):
  return ' '.join([x[0] for x in node])

# Tries to extract the most common topics from the question's titles
def get_most_active_topics(data, limit):
  body = flatten_questions_titles(data)
  sentences = nltk.sent_tokenize(body)
  sentences = [nltk.word_tokenize(sent) for sent in sentences]
  sentences = [nltk.pos_tag(sent) for sent in sentences]
  grammar = "NP: {&lt;JJ&gt;?&lt;NN.*&gt;}"
  cp = nltk.RegexpParser(grammar)
  results = {}
  for sent in sentences:
    parsed = cp.parse(sent)
    trees = parsed.subtrees(filter=lambda x: x.label() == 'NP')
    for t in trees:
      key = get_node_content(t)
      if key in results:
        results[key] += 1
      else:
        results[key] = 1
  return sorted(results.items(), reverse=True, key=operator.itemgetter(1))[:limit]

# Returns the user that has the most answers
def get_most_helpful_user(data, limit):
  helpful_users = {}
  for q in data:
    for a in q['answers']:
      if a['author'] not in helpful_users:
        helpful_users[a['author']] = 1
      else:
        helpful_users[a['author']] += 1

  return sorted(helpful_users.items(), reverse=True, key=operator.itemgetter(1))[:limit]

# returns the top "limit" questions with the most amount of answers
def get_most_answered_questions(d, limit):
  questions = {}

  for q in d:
    questions[q['title']] = len(q['answers'])
  return sorted(questions.items(), reverse=True, key=operator.itemgetter(1))[:limit]



# Finds a list of the most common phrases of 'length' length
def get_most_common_phrases(d, limit, length):
  body = flatten_questions_body(d)
  phrases = {}
  for sentence in nltk.sent_tokenize(body):
    words = nltk.word_tokenize(sentence)
    for phrase in ngrams(words, length):
      if all(word not in string.punctuation for word in phrase):
        key = ' '.join(phrase)
        if key in phrases:
          phrases[key] += 1
        else:
         phrases[key] = 1

  return sorted(phrases.items(), reverse=True, key=operator.itemgetter(1))[:limit]

# Finds the answer with the least amount of characters
def get_shortest_answer(d):
  
  shortest_answer = {
    'body': '',
    'length': -1
  }
  for q in d:
    for a in q['answers']:
      if len(a['body']) &lt; shortest_answer['length'] or shortest_answer['length'] == -1:
        shortest_answer = {
          'question': q['body'],
          'body': a['body'],
          'length': len(a['body'])
        }
  return shortest_answer</pre></div><p>The following code shows how to use the functions declared earlier and display their results. It all boils down <a id="id451" class="indexterm"/>to three steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">It loads the JSON into memory.</li><li class="listitem">It processes the data and saves the results into a dictionary.</li><li class="listitem">It goes over that dictionary to display the results.</li></ol></div><p>The preceding steps are performed in the following code:</p><div class="informalexample"><pre class="programlisting">data_dict = load_json_data(SOURCE_FILE)

results = analyze_data(data_dict)

print "=== ( Shortest Answer ) === "
visualizer.displayShortestAnswer(results['shortest_answer'])

print "=== ( Most Active Users ) === "
visualizer.displayMostActiveUsers(results['most_active_users'])

print "=== ( Most Active Topics ) === "
visualizer.displayMostActiveTopics(results['most_active_topics'])

print "=== ( Most Helpful Users ) === "
visualizer.displayMostHelpfulUser(results['most_helpful_user'])

print "=== ( Most Answered Questions ) === "
visualizer.displayMostAnsweredQuestions(results['most_answered_questions'])

print "=== ( Most Common Phrases ) === "
visualizer.displayMostCommonPhrases(results['most_common_phrases'])</pre></div><p>The code in the following <a id="id452" class="indexterm"/>file is merely used to format the output in a human-friendly way:</p><div class="informalexample"><pre class="programlisting">#visualizer.py
def displayShortestAnswer(data):
  print "A: %s" % (data['body'])
  print "Q: %s" % (data['question'])
  print "Length: %s characters" % (data['length'])

def displayMostActiveUsers(data):
  index = 1
  for u in data:
    print "%s - %s (%s)" % (index, u[0], u[1])
    index += 1

def displayMostActiveTopics(data):
  index = 1
  for u in data:
    print "%s - %s (%s)" % (index, u[0], u[1])
    index += 1

def displayMostHelpfulUser(data):
  index = 1
  for u in data:
    print "%s - %s (%s)" % (index, u[0], u[1])
    index += 1

def displayMostAnsweredQuestions(data):
  index = 1
  for u in data:
    print "%s - %s (%s)" % (index, u[0], u[1])
    index += 1

def displayMostCommonPhrases(data):
  index = 1
  for u in data:
    print "%s - %s (%s)" % (index, u[0], u[1])
    index += 1</pre></div><div class="section" title="Analyzing the code"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec71"/>Analyzing the code</h2></div></div></div><p>Analyzing the <a id="id453" class="indexterm"/>code will be done in two steps, just like we've being doing so far. For each project, we'll profile the code, get the numbers, consider our optimization alternatives, and then refactor and measure the code's performance again.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note30"/>Note</h3><p>As the process described earlier can lead to several iterations of profiling—refactoring—profiling again, we'll limit the steps to the final results. However, keep in mind that this process is long and takes time.</p></div></div><div class="section" title="Scraper"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec17"/>Scraper</h3></div></div></div><p>To start off the <a id="id454" class="indexterm"/>optimization process, let's first get some measurements so that we can compare our changes with them.</p><p>An easy-to-get number is the total time spent during the program's execution (in our example, and to keep things simple, we're limiting the total number of pages to query to 20).</p><p>Simply using the <code class="literal">time</code> command-line tool, we can get that number:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ time python scraper.py</strong></span>
</pre></div><p>The following screenshot shows that we have 7 minutes and 30 seconds to scrape and parse the 20 pages of questions, which translate into a 3 MB JSON file:</p><div class="mediaobject"><img src="graphics/B02088_08_02.jpg" alt="Scraper"/></div><p>The scraper script is essentially an IO-bound loop that pulls data from the Internet with a minimum amount of processing. So, the first and most logical optimization we can spot here is the lack of parallelization of the requests. Since our code is not really CPU-bound, we can safely use the multithreading module (refer to <a class="link" href="ch05.html" title="Chapter 5. Multithreading versus Multiprocessing">Chapter 5</a>, <span class="emphasis"><em>Multithreading versus Multiprocessing</em></span>) and get an interesting speed boost with minimum effort.</p><p>Just to clarify what we're going to be doing, the following diagram shows the current status of the scraper script:</p><div class="mediaobject"><img src="graphics/B02088_08_03.jpg" alt="Scraper"/></div><p>We're spending most <a id="id455" class="indexterm"/>of our running time on I/O operations, more specifically on the HTTP requests we're doing to get the list of questions and each question's page.</p><p>As we've seen earlier, I/O operations can be parallelized easily using the multithreading module. So, we will transform our script so it resembles as shown in the following diagram:</p><div class="mediaobject"><img src="graphics/B02088_08_04.jpg" alt="Scraper"/></div><p>Now, let's look at the actual optimized code. We'll first look at the <code class="literal">ThreadManager</code> class, which will take care of <a id="id456" class="indexterm"/>centralizing the configuration of the threads as well as the status of the entire parallel process:</p><div class="informalexample"><pre class="programlisting">from bs4 import BeautifulSoup
import requests
import json
import threading


SO_URL = "http://scifi.stackexchange.com"
QUESTION_LIST_URL = SO_URL + "/questions"
MAX_PAGE_COUNT = 20


class ThreadManager:
  instance = None
  final_results = []
  threads_done = 0
  totalConnections = 4 #Number of parallel threads working, will affect the total amount of pages per thread

  @staticmethod
  def notify_connection_end( partial_results ):
    print "==== Thread is done! ====="
    ThreadManager.threads_done += 1
    ThreadManager.final_results += partial_results
    if ThreadManager.threads_done == ThreadManager.totalConnections:
      print "==== Saving data to file! ===="
      with open('scrapping-results-optimized.json', 'w') as outfile:
        json.dump(ThreadManager.final_results, outfile, indent=4)</pre></div><p>The following functions take care of scraping the information from a page using <code class="literal">BeatifulSoup</code>, either by <a id="id457" class="indexterm"/>getting the lists of pages or getting the actual information for each question:</p><div class="informalexample"><pre class="programlisting">def get_author_name(body):
  link_name = body.select(".user-details a")
  if len(link_name) == 0:
    text_name = body.select(".user-details")
    return text_name[0].text if len(text_name) &gt; 0 else 'N/A'
  else:
    return link_name[0].text

def get_question_answers(body):
  answers = body.select(".answer")
  a_data = []
  if len(answers) == 0:
    return a_data

  for a in answers:
    data = {
      'body': a.select(".post-text")[0].get_text(),
      'author': get_author_name(a)
    }
    a_data.append(data)
  return a_data


def get_question_data ( url ):
  print "Getting data from question page: %s " % (url)
  resp = requests.get(url)
  if resp.status_code != 200:
    print "Error while trying to scrape url: %s" % (url)
    return
  body_soup = BeautifulSoup(resp.text)
  #define the output dict that will be turned into a JSON structue
  q_data = {
    'title': body_soup.select('#question-header .question-hyperlink')[0].text,
    'body': body_soup.select('#question .post-text')[0].get_text(),
    'author': get_author_name(body_soup.select(".post-signature.owner")[0]),
    'answers': get_question_answers(body_soup)
  }
  return q_data


def get_questions_page ( page_num, end_page, partial_results  ):
  print "====================================================="
  print " Getting list of questions for page %s" % (page_num)
  print "====================================================="

  url = QUESTION_LIST_URL + "?sort=newest&amp;page=" + str(page_num)
  resp = requests.get(url)
  if resp.status_code != 200:
    print "Error while trying to scrape url: %s" % (url)
  else:
    body = resp.text
    main_soup = BeautifulSoup(body)

    #get the urls for each question
    questions = main_soup.select('.question-summary .question-hyperlink')
    urls = [ SO_URL + x['href'] for x in questions]
    for url in urls:
      q_data = get_question_data(url)
     partial_results.append(q_data)
  if page_num + 1 &lt; end_page:
    get_questions_page(page_num + 1,  end_page, partial_results)
  else:
    ThreadManager.notify_connection_end(partial_results)
<span class="strong"><strong>pages_per_connection = MAX_PAGE_COUNT / ThreadManager.totalConnections</strong></span>
<span class="strong"><strong>for i in range(ThreadManager.totalConnections):</strong></span>
<span class="strong"><strong>  init_page = i * pages_per_connection</strong></span>
<span class="strong"><strong>  end_page = init_page + pages_per_connection</strong></span>
<span class="strong"><strong>  t = threading.Thread(target=get_questions_page,</strong></span>
<span class="strong"><strong>           args=(init_page, end_page, [],  ),</strong></span>
<span class="strong"><strong>           name='connection-%s' % (i))</strong></span>
  t.start()</pre></div><p>The highlighted code in the preceding snippet shows the main change done to the initial script. Instead of starting at page 1 and moving forward one by one, we're starting a preconfigured number of threads (using the <code class="literal">threading.Thread</code> class directly) that will call our <code class="literal">get_question_page</code> function in parallel. All we had to do was pass in that function as the target of each new thread.</p><p>After that, we also needed a way to centralize the configuration parameters and the temporary results from each thread. For that, we created the <code class="literal">ThreadManager</code> class.</p><p>With this change, we go from the 7 minutes mark all the way down to 2 minutes 13 seconds, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B02088_08_05.jpg" alt="Scraper"/></div><p>Tweaking the number of <a id="id458" class="indexterm"/>threads, for instance, might lead to even better numbers, but the main improvement is already there.</p></div></div><div class="section" title="Analyzer"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec72"/>Analyzer</h2></div></div></div><p>The code for the <a id="id459" class="indexterm"/>analyzer script is different compared to the scraper. Instead of having a heavy I/O-bound script, we have the opposite: a CPU-bound one. It does very little I/O, mainly to read the input file and output the results. So, we will focus on measuring in more detail.</p><p>Let's first get some basic measurements so that we know where we stand:</p><div class="mediaobject"><img src="graphics/B02088_08_06.jpg" alt="Analyzer"/></div><p>The preceding screenshot shows the output of the <code class="literal">time</code> command-line utility. So now that we have a base number to work with, we know we need to get the execution time lower than 3.5 seconds.</p><p>The first approach would be to use <code class="literal">cProfile</code> and start getting some numbers from the inside of our code. This should help us get a general overview of our program to start understanding where our pain points are. The output looks like the following screenshot:</p><div class="mediaobject"><img src="graphics/B02088_08_07.jpg" alt="Analyzer"/></div><p>There are two areas of interest in the preceding screenshot:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">On the left-hand side, we can see the functions and how much time they consume. Pay attention to how most of the list is composed of external functions, mainly from the <code class="literal">nltk</code> module (the first two are just consumers of the others below, so they don't really matter).</li><li class="listitem" style="list-style-type: disc">On the right-hand side, the <span class="strong"><strong>Callee Map</strong></span> looks way too complex to interpret it (quite apart from <a id="id460" class="indexterm"/>the fact that again, most of the functions listed there aren't from our code, but from the libraries we're using).</li></ul></div><p>With that being said, it looks like improving our code directly is not going to be a simple task. Instead, we might want to go on another route: since we're doing a lot of counting, we might benefit from typed code. So, let's try our hand at using Cython.</p><p>An initial analysis using the Cython command-line utility shows that most of our code can't directly be translated into C, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B02088_08_08.jpg" alt="Analyzer"/></div><p>The preceding screenshot shows a portion of the analysis of our code. We can clearly see the darker lines filling most of the screen, showing that most of our code can't be directly translated into C. Sadly, this is because we're dealing with a complex object in most of our functions, so there isn't much we can do about it.</p><p>Still, simply by compiling our code with Cython, we get much better results. So, let's take a look at how we need to modify the source so that we can compile it with Cython. The first file is basically the <a id="id461" class="indexterm"/>same as the original analyzer with the changes highlighted in the code and minus the actual function calls, as we're now turning it into an external library:</p><div class="informalexample"><pre class="programlisting">#analyzer_cython.pyx
import operator
import string
import nltk
from nltk.util import ngrams
import json
import re


SOURCE_FILE = './scrapping-results.json'

# Returns the top "limit" users with the most questions asked
def get_most_active_users(data, <span class="strong"><strong>int</strong></span> limit ):
  names = {}
  for q in data:
    if q['author'] not in names:
      names[q['author']] = 1
    else:
      names[q['author']] += 1
  return sorted(names.items(), reverse=True, key=operator.itemgetter(1))[:limit]

def get_node_content(node):
  return ' '.join([x[0] for x in node])

# Tries to extract the most common topics from the question's titles
def get_most_active_topics(data, <code class="literal">int</code> limit ):
  body = flatten_questions_titles(data)
  sentences = nltk.sent_tokenize(body)
  sentences = [nltk.word_tokenize(sent) for sent in sentences]
  sentences = [nltk.pos_tag(sent) for sent in sentences]
  grammar = "NP: {&lt;JJ&gt;?&lt;NN.*&gt;}"
  cp = nltk.RegexpParser(grammar)
  results = {}
  for sent in sentences:
    parsed = cp.parse(sent)
    trees = parsed.subtrees(filter=lambda x: x.label() == 'NP')
    for t in trees:
      key = get_node_content(t)
      if key in results:
        results[key] += 1
      else:
        results[key] = 1
  return sorted(results.items(), reverse=True, key=operator.itemgetter(1))[:limit]

# Returns the user that has the most answers
def get_most_helpful_user(data, <code class="literal">int</code> limit ):
  helpful_users = {}
  for q in data:
    for a in q['answers']:
      if a['author'] not in helpful_users:
        helpful_users[a['author']] = 1
      else:
        helpful_users[a['author']] += 1

  return sorted(helpful_users.items(), reverse=True, key=operator.itemgetter(1))[:limit]

# returns the top "limit" questions with the most amount of answers
def get_most_answered_questions(d, <code class="literal">int</code> limit ):
  questions = {}

  for q in d:
    questions[q['title']] = len(q['answers'])
  return sorted(questions.items(), reverse=True, key=operator.itemgetter(1))[:limit]


# Creates a single, lower cased string from the bodies of all questions
def flatten_questions_body(data):
  body = []
  for q in data:
    body.append(q['body'])
  return '. '.join(body)


# Creates a single, lower cased string from the titles of all questions
def flatten_questions_titles(data):
  body = []
  pattern = re.compile('(\[|\])')
  for q in data:
    lowered = string.lower(q['title'])
    filtered = re.sub(pattern, ' ', lowered)
    body.append(filtered)
  return '. '.join(body)

# Finds a list of the most common phrases of 'length' length
def get_most_common_phrases(d, <code class="literal">int</code> limit , <code class="literal">int</code> length ):
  body = flatten_questions_body(d)
  phrases = {}
  for sentence in nltk.sent_tokenize(body):
    words = nltk.word_tokenize(sentence)
    for phrase in ngrams(words, length):
      if all(word not in string.punctuation for word in phrase):
        key = ' '.join(phrase)
        if key in phrases:
          phrases[key] += 1
        else:
          phrases[key] = 1

  return sorted(phrases.items(), reverse=True, key=operator.itemgetter(1))[:limit]

# Finds the answer with the least amount of characters
def get_shortest_answer(d):
  <code class="literal">cdef int</code> shortest_length = 0;

  shortest_answer = {
    'body': '',
    'length': -1
  }
  for q in d:
    for a in q['answers']:
<span class="strong"><strong>      if len(a['body']) &lt; shortest_length or shortest_length == 0:</strong></span>
<span class="strong"><strong>        shortest_length = len(a['body'])</strong></span>
        shortest_answer = {
          'question': q['body'],
          'body': a['body'],
          'length': shortest_length
        }
  return shortest_answer

# Load the json file and return the resulting dict
def load_json_data(file):
  with open(file) as input_file:
    return json.load(input_file)

def analyze_data(d):
  return {
    'shortest_answer': get_shortest_answer(d),
    'most_active_users': get_most_active_users(d, 10),
    'most_active_topics': get_most_active_topics(d, 10),
    'most_helpful_user': get_most_helpful_user(d, 10),
    'most_answered_questions': get_most_answered_questions(d, 10),
    'most_common_phrases':  get_most_common_phrases(d, 10, 4),
  }</pre></div><p>The following file is the one that takes care of setting everything up for Cython to compile our code, we've seen this code before (refer to <a class="link" href="ch06.html" title="Chapter 6. Generic Optimization Options">Chapter 6</a>, <span class="emphasis"><em>Generic Optimization Options</em></span>):</p><div class="informalexample"><pre class="programlisting">#analyzer-setup.py
from distutils.core import setup
from Cython.Build import cythonize

setup(
  name = 'Analyzer app',
  ext_modules = cythonize("analyzer_cython.pyx"),
)</pre></div><p>The last file is the one that uses our new external library by importing the compiled module. The file calls on <a id="id462" class="indexterm"/>the <code class="literal">load_json_data</code> and <code class="literal">analyze_data</code> methods and, finally, uses the visualizer module to format the output:</p><div class="informalexample"><pre class="programlisting">#analyzer-use-cython.py
import analyzer_cython as analyzer
import visualizer



data_dict = analyzer.load_json_data(analyzer.SOURCE_FILE)

results = analyzer.analyze_data(data_dict)

print "=== ( Shortest Answer ) === "
visualizer.displayShortestAnswer(results['shortest_answer'])

print "=== ( Most Active Users ) === "
visualizer.displayMostActiveUsers(results['most_active_users'])

print "=== ( Most Active Topics ) === "
visualizer.displayMostActiveTopics(results['most_active_topics'])

print "=== ( Most Helpful Users ) === "
visualizer.displayMostHelpfulUser(results['most_helpful_user'])

print "=== ( Most Answered Questions ) === "
visualizer.displayMostAnsweredQuestions(results['most_answered_questions'])

print "=== ( Most Common Phrases ) === "
visualizer.displayMostCommonPhrases(results['most_common_phrases'])</pre></div><p>The preceding code can be compiled using the following line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ python analyzer-setup.py build_ext –inplace</strong></span>
</pre></div><p>Then, by running the <a id="id463" class="indexterm"/>
<code class="literal">analyzer-use-cython.py</code> script, we will get the following execution time:</p><div class="mediaobject"><img src="graphics/B02088_08_09.jpg" alt="Analyzer"/></div><p>The time went down from 3.5 to 1.3 seconds. This is quite an improvement from simply reorganizing of our code and compiling it using Cython, like we saw in <a class="link" href="ch06.html" title="Chapter 6. Generic Optimization Options">Chapter 6</a>, <span class="emphasis"><em>Generic Optimization Options</em></span>. This simple compilation can produce great results.</p><p>The code can be further broken down and rewritten to remove most of the need for complex structures, thus allowing us to declare the primitive types for all variables. We could even try to remove <code class="literal">nltk</code> and use some NLP library written in C, such as OpenNLP (<a class="ulink" href="http://opennlp.sourceforge.net/projects.html">http://opennlp.sourceforge.net/projects.html</a>).</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec41"/>Summary</h1></div></div></div><p>You've reached the end of the chapter and, with it, the end of this book. The examples provided in this last chapter are meant to show how a random piece of code can be analyzed and improved using the techniques shown in the previous chapters.</p><p>As not all techniques are compatible with each other, not all of them were applicable here. However, we were able to see how some of them work, more specifically, multithreading, profiling with <code class="literal">cProfile</code> and <code class="literal">kcachegrind</code>, and finally, compilation with Cython.</p><p>Thank you for reading and, hopefully, enjoying the book!</p></div></body></html>