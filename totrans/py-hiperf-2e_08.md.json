["```py\n    dsk = {\n      \"a\" : 2,\n      \"b\" : 2,\n    }\n\n```", "```py\n    dsk = {\n      \"a\" : 2,\n      \"b\" : 2,\n      \"result\": (lambda x, y: x + y, \"a\", \"b\")\n    }\n\n```", "```py\n    from operator import add\n    dsk = {\n      \"a\" : 2,\n      \"b\" : 2,\n      \"result\": (add, \"a\", \"b\")\n    }\n\n```", "```py\n    import dask\n\n    res = dask.get(dsk, \"result\")\n    print(res)\n    # Output:\n    # 4 \n\n```", "```py\n    import numpy as np\n    import dask.array as da\n\n    a = np.random.rand(30)\n\n    a_da = da.from_array(a, chunks=10)\n    # Result:\n    # dask.array<array-4..., shape=(30,), dtype=float64, chunksize=(10,)>\n\n```", "```py\n    dict(a_da.dask)\n    # Result\n    {('array-4c76', 0): (<function dask.array.core.getarray>, \n                         'array-original-4c76',\n                         (slice(0, 10, None),)),\n     ('array-4c76', 2): (<function dask.array.core.getarray>,\n                         'array-original-4c76',\n                         (slice(20, 30, None),)),\n     ('array-4c76', 1): (<function dask.array.core.getarray>, \n                         'array-original-4c76',\n                         (slice(10, 20, None),)),\n     'array-original-4c76': array([ ... ])\n    }\n\n```", "```py\n    N = 10000\n    chunksize = 1000 \n\n    x_data = np.random.uniform(-1, 1, N)\n    y_data = np.random.uniform(-1, 1, N)\n\n    x = da.from_array(x_data, chunks=chunksize)\n    y = da.from_array(y_data, chunks=chunksize)\n\n    hit_test = x ** 2 + y ** 2 < 1\n\n    hits = hit_test.sum()\n    pi = 4 * hits / N\n\n```", "```py\n    pi.compute() # Alternative: pi.compute(get=dask.get)\n    # Result:\n    # 3.1804000000000001\n\n```", "```py\n    import dask.bag as dab\n    dab.from_sequence(range(100), npartitions=4)\n    # Result:\n    # dask.bag<from_se..., npartitions=4>\n\n```", "```py\n    collection = dab.from_sequence([\"the cat sat on the mat\",\n                                    \"the dog sat on the mat\"], npartitions=2)\n\n binop = lambda total, x: total + x[\"count\"]\n    combine = lambda a, b: a + b\n    (collection\n     .map(str.split)\n     .concat()\n     .map(lambda x: {\"word\": x, \"count\": 1})\n     .foldby(lambda x: x[\"word\"], binop, 0, combine, 0)\n     .compute())\n    # Output:\n    # [('dog', 1), ('cat', 1), ('sat', 2), ('on', 2), ('mat', 2), ('the', 4)]\n\n```", "```py\n    collection = dab.from_sequence([\"the cat sat on the mat\",\n                                    \"the dog sat on the mat\"], npartitions=2)\n    words = collection.map(str.split).concat()\n    df = words.to_dataframe(['words'])\n    df.head()\n    # Result:\n    #   words\n    # 0   the\n    # 1   cat\n    # 2   sat\n    # 3    on\n    # 4   the\n\n```", "```py\n    df.words.value_counts().compute()\n    # Result:\n    # the    4\n    # sat    2\n    # on     2\n    # mat    2\n    # dog    1\n    # cat    1\n    # Name: words, dtype: int64\n\n```", "```py\n    from dask.distributed import Client\n\n    client = Client()\n    # Result:\n    # <Client: scheduler='tcp://127.0.0.1:46472' processes=4 cores=4>\n\n```", "```py\n    def square(x):\n       return x ** 2\n\n    fut = client.submit(square, 2)\n    # Result:\n    # <Future: status: pending, key: square-05236e00d545104559e0cd20f94cd8ab>\n\n    client.map(square)\n    futs = client.map(square, [0, 1, 2, 3, 4])\n    # Result:\n    # [<Future: status: pending, key: square-d043f00c1427622a694f518348870a2f>,\n    #  <Future: status: pending, key: square-9352eac1fb1f6659e8442ca4838b6f8d>,\n    #  <Future: status: finished, type: int, key: \n    #  square-05236e00d545104559e0cd20f94cd8ab>,\n    #  <Future: status: pending, key: \n    #  square-c89f4c21ae6004ce0fe5206f1a8d619d>,\n    #  <Future: status: pending, key: \n    #  square-a66f1c13e2a46762b092a4f2922e9db9>]\n\n```", "```py\n    client.gather(futs)\n    # Result:\n    # [0, 1, 4, 9, 16]\n\n```", "```py\n    pi.compute(get=client.get)\n\n```", "```py\n$ dask-scheduler\ndistributed.scheduler - INFO - -----------------------------------------------\ndistributed.scheduler - INFO - Scheduler at: tcp://192.168.0.102:8786\ndistributed.scheduler - INFO - bokeh at: 0.0.0.0:8788\ndistributed.scheduler - INFO - http at: 0.0.0.0:9786\ndistributed.bokeh.application - INFO - Web UI: http://127.0.0.1:8787/status/\ndistributed.scheduler - INFO - -----------------------------------------------\n\n```", "```py\n$ dask-worker 192.168.0.102:8786\ndistributed.nanny - INFO - Start Nanny at: 'tcp://192.168.0.102:45711'\ndistributed.worker - INFO - Start worker at: tcp://192.168.0.102:45928\ndistributed.worker - INFO - bokeh at: 192.168.0.102:8789\ndistributed.worker - INFO - http at: 192.168.0.102:46154\ndistributed.worker - INFO - nanny at: 192.168.0.102:45711\ndistributed.worker - INFO - Waiting to connect to: tcp://192.168.0.102:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO - Threads: 4\ndistributed.worker - INFO - Memory: 4.97 GB\ndistributed.worker - INFO - Local Directory: /tmp/nanny-jh1esoo7\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO - Registered to: tcp://192.168.0.102:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.nanny - INFO - Nanny 'tcp://192.168.0.102:45711' starts worker process 'tcp://192.168.0.102:45928'\n\n```", "```py\nclient = Client(address='192.168.0.102:8786')\n# Result: \n# <Client: scheduler='tcp://192.168.0.102:8786' processes=1 cores=4>\n\n```", "```py\n$ docker build -t pyspark \n\n```", "```py\n$ docker run -d -p 8888:8888 -p 4040:4040 pyspark\n22b9dbc2767c260e525dcbc562b84a399a7f338fe1c06418cbe6b351c998e239\n\n```", "```py\n    import pyspark\n    sc = pyspark.SparkContext('local[*]')\n\n    rdd = sc.parallelize(range(1000))\n    rdd.first()\n    # Result:\n    # 0\n\n```", "```py\n    rdd = sc.parallelize(range(1000))\n    # Result:\n    # PythonRDD[3] at RDD at PythonRDD.scala:48\n\n```", "```py\n    rdd = sc.parallelize(range(1000), 2)\n    rdd.getNumPartitions() # This function will return the number of partitions\n    # Result:\n    # 2\n\n```", "```py\n    square_rdd = rdd.map(lambda x: x**2)\n    # Result:\n    # PythonRDD[5] at RDD at PythonRDD.scala:48\n\n```", "```py\n    square_rdd.collect()\n    # Result:\n    # [0, 1, ... ]\n\n    square_rdd.take(10)\n    # Result:\n    # [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n```", "```py\n    import numpy as np\n\n    N = 10000\n    x = np.random.uniform(-1, 1, N)\n    y = np.random.uniform(-1, 1, N)\n\n    rdd_x = sc.parallelize(x)\n    rdd_y = sc.parallelize(y)\n\n    hit_test = rdd_x.zip(rdd_y).map(lambda xy: xy[0] ** 2 + xy[1] ** 2 < 1)\n    pi = 4 * hit_test.sum()/N\n\n```", "```py\n      import datetime\n\n      from uuid import uuid4\n      from random import randrange, choice\n\n      # We generate 20 users\n      n_users = 20 \n      users = [uuid4() for i in range(n_users)]\n\n      def random_time(start, end):\n          '''Return a random timestamp between start date and end \n          date'''\n          # We select a number of seconds\n          total_seconds = (end - start).total_seconds()\n          return start + \n          datetime.timedelta(seconds=randrange(total_seconds))\n\n      start = datetime.datetime(2017, 1, 1)\n      end = datetime.datetime(2017, 1, 7)\n\n      entries = []\n      N = 10000\n      for i in range(N):\n          entries.append({\n           'user': choice(users),\n           'timestamp': random_time(start, end)\n          })\n\n```", "```py\n    entries_rdd = sc.parallelize(entries)\n    entries_rdd.groupBy(lambda x: x['user']).first()\n    # Result:\n    # (UUID('0604aab5-c7ba-4d5b-b1e0-16091052fb11'),\n    #  <pyspark.resultiterable.ResultIterable at 0x7faced4cd0b8>)\n\n```", "```py\n    (entries_rdd\n     .groupBy(lambda x: x['user'])\n     .map(lambda kv: (kv[0], len(kv[1])))\n     .take(5))\n    # Result:\n    # [(UUID('0604aab5-c7ba-4d5b-b1e0-16091052fb11'), 536),\n    #  (UUID('d72c81c1-83f9-4b3c-a21a-788736c9b2ea'), 504),\n    #  (UUID('e2e125fa-8984-4a9a-9ca1-b0620b113cdb'), 498),\n    #  (UUID('b90acaf9-f279-430d-854f-5df74432dd52'), 561),\n    #  (UUID('00d7be53-22c3-43cf-ace7-974689e9d54b'), 466)]\n\n```", "```py\n    rdd = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"a\", 3), (\"b\", 4), (\"c\", 5)])\n    rdd.reduceByKey(lambda a, b: a + b).collect()\n    # Result:\n    # [('c', 5), ('b', 6), ('a', 4)]\n\n```", "```py\n    (entries_rdd\n     .map(lambda x: (x['user'], 1))\n     .reduceByKey(lambda a, b: a + b)\n     .take(3))\n    # Result:\n    # [(UUID('0604aab5-c7ba-4d5b-b1e0-16091052fb11'), 536),\n    #  (UUID('d72c81c1-83f9-4b3c-a21a-788736c9b2ea'), 504),\n    #  (UUID('e2e125fa-8984-4a9a-9ca1-b0620b113cdb'), 498)]\n\n```", "```py\n    (entries_rdd\n     .map(lambda x: (x['timestamp'].date(), 1))\n     .reduceByKey(lambda a, b: a + b)\n     .sortByKey()\n     .collect())\n    # Result:\n    # [(datetime.date(2017, 1, 1), 1685),\n    #  (datetime.date(2017, 1, 2), 1625),\n    #  (datetime.date(2017, 1, 3), 1663),\n    #  (datetime.date(2017, 1, 4), 1643),\n    #  (datetime.date(2017, 1, 5), 1731),\n    #  (datetime.date(2017, 1, 6), 1653)]\n\n```", "```py\n    from pyspark.sql import SparkSession\n    spark = SparkSession.builder.getOrCreate()\n\n```", "```py\n    # We will use the x_rdd and y_rdd defined previously.\n    rows = rdd_x.zip(rdd_y).map(lambda xy: Row(x=float(xy[0]), y=float(xy[1])))\n\n    rows.first() # Inspect the first element\n    # Result:\n    # Row(x=0.18432163061239137, y=0.632310101419016)\n\n```", "```py\n    df = spark.createDataFrame(rows)\n    df.show(5)\n    # Output:\n    # +-------------------+--------------------+\n    # |                  x|                   y|\n    # +-------------------+--------------------+\n    # |0.18432163061239137|   0.632310101419016|\n    # | 0.8159145525577987| -0.9578448778029829|\n    # |-0.6565050226033042|  0.4644773453129496|\n    # |-0.1566191476553318|-0.11542211978216432|\n    # | 0.7536730082381564| 0.26953055476074717|\n    # +-------------------+--------------------+\n    # only showing top 5 rows\n\n```", "```py\n    hits_df = df.selectExpr(\"pow(x, 2) + pow(y, 2) < 1 as hits\")\n    hits_df.show(5)\n    # Output:\n    # +-----+\n    # | hits|\n    # +-----+\n    # | true|\n    # |false|\n    # | true|\n    # | true|\n    # | true|\n    # +-----+\n    # only showing top 5 rows\n\n```", "```py\n    result = df.selectExpr('4 * sum(cast(pow(x, 2) + \n                           pow(y, 2) < 1 as int))/count(x) as pi')\n    result.first()\n    # Result:\n    # Row(pi=3.13976)\n\n```", "```py\n    from mpi4py import MPI\n\n    comm = MPI.COMM_WORLD\n    rank = comm.Get_rank()\n    print(\"This is process\", rank)\n\n```", "```py\n $ python mpi_example.py\n    This is process 0\n\n```", "```py\n $ mpiexec -n 4 python mpi_example.py\n    This is process 0\n    This is process 2\n    This is process 1\n    This is process 3\n\n```", "```py\n      from mpi4py import MPI\n\n      comm = MPI.COMM_WORLD\n      rank = comm.Get_rank()\n\n      import numpy as np\n\n      N = 10000\n\n      n_procs = comm.Get_size()\n\n      print(\"This is process\", rank)\n\n      # Create an array\n      x_part = np.random.uniform(-1, 1, int(N/n_procs))\n      y_part = np.random.uniform(-1, 1, int(N/n_procs))\n\n      hits_part = x_part**2 + y_part**2 < 1\n      hits_count = hits_part.sum()\n\n      print(\"partial counts\", hits_count)\n\n      total_counts = comm.reduce(hits_count, root=0)\n\n      if rank == 0:\n         print(\"Total hits:\", total_counts)\n         print(\"Final result:\", 4 * total_counts/N)\n\n```", "```py\n$ mpiexec -n 4 python mpi_pi.py\nThis is process 3\npartial counts 1966\nThis is process 1\npartial counts 1944\nThis is process 2\npartial counts 1998\nThis is process 0\npartial counts 1950\nTotal hits: 7858\nFinal result: 3.1432\n\n```"]