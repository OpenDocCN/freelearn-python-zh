- en: Chapter 4. Parallel Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With parallel processing you can increase the amount of calculations your program
    can do in a given time without needing a faster processor. The main idea is to
    divide a task into many sub-units and employ multiple processors to solve them
    independently.
  prefs: []
  type: TYPE_NORMAL
- en: CPUs containing several cores (2, 4, 6, 8, ...) have become a common trend in
    technology. Increasing the speed of a single processor is costly and problematic;
    while leveraging the parallel capabilities of cheaper multi-core processors is
    a feasible route to increase performance.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel processing lets you tackle large scale problems. Scientists and engineers
    commonly run parallel code on supercomputers—huge networks of standard processors—to
    simulate massive systems. Parallel techniques can also take advantage of graphics
    chips (a hardware optimized for parallelization).
  prefs: []
  type: TYPE_NORMAL
- en: Python can be used in all of these domains, allowing us to apply parallel processing
    to all sorts of problems with simplicity and elegance, opening the door to infinite
    possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Briefly introduce the fundamentals of parallel processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Illustrate how to parallelize simple problems with the multiprocessing Python
    library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn how to write programs with the **IPython parallel** framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further optimize our program using multithreading with Cython and OpenMP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to parallel programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to parallelize a program, we need to divide the problem into sub-units
    that can run independently (or almost independently) from each other.
  prefs: []
  type: TYPE_NORMAL
- en: A problem where the sub-units are totally independent from each other is called
    **embarrassingly parallel**. An element-wise operation on an array is a typical
    example—the operation needs only to know the element it is handling at the moment.
    Another example, is our particle simulator—since there are no interactions, each
    particle can evolve in time independently from the others. Embarrassingly parallel
    problems are very easy to implement and they perform optimally on parallel architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Other problems may be divided into sub-units but have to share some data to
    perform their calculations. In those cases, the implementation is less straightforward
    and can lead to performance issues because of the communication costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will illustrate the concept with an example. Imagine you have a particle
    simulator, but this time the particles attract other particles within a certain
    distance (as shown in the following figure). To parallelize this problem we divide
    the simulation box in regions and assign each region to a different processor.
    If we evolve the system for one step, some particles will interact with particles
    in a neighboring region. To perform the next iteration, the new particle positions
    of the neighboring region are required:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to parallel programming](img/8458OS_04_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Communication between processes is costly and can seriously hinder the performance
    of parallel programs. There exists two main ways to handle data communication
    in parallel programs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Shared memory**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed memory**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In shared memory, the sub-units have access to the same memory space. The advantage
    of this approach, is that you don't have to explicitly handle the communication
    as it is sufficient to write or read from the shared memory. However, problems
    arise when multiple processes try to access and change the same memory location
    at the same time. Care should be taken to avoid such conflict using synchronization
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In the distributed memory model each process is completely separated from the
    others and possesses its own memory space. In this case, communication is handled
    explicitly between the processes. The communication overhead is typically costlier
    compared to shared memory, as data can potentially travel through a network interface.
  prefs: []
  type: TYPE_NORMAL
- en: One common way to achieve parallelism with the shared memory model is **threads**.
    Threads are independent sub-tasks that originate from a process and share resources
    such as memory.
  prefs: []
  type: TYPE_NORMAL
- en: Python can spawn and handle threads, but they can't be used to increase performance
    due to the Python interpreter design—only one Python instruction is allowed to
    run at a time. This mechanism is called **Global Interpreter Lock** (**GIL**).
    What happens is that, each time a thread executes a Python statement, a lock is
    acquired which prevents other threads to run until it is released. The GIL avoids
    conflicts between threads, simplifying the implementation of the **CPython** interpreter.
    Despite this limitation, threads can still be used to provide concurrency in situations
    where the lock can be released, such as in time-consuming I/O operations or in
    C extensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The GIL can be completely avoided by using processes instead of threads. Processes
    don''t share the same memory area and are independent from each other—each process
    has its own interpreter. By using processes, we''ll have very few disadvantages:
    inter-process communication is less efficient than shared memory, but it is more
    flexible and explicit.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to parallel programming](img/8458OS_04_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The multiprocessing module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The standard `multiprocessing` module can be used to quickly parallelize simple
    tasks by spawning several processes. Its interface is easy-to-use and includes
    several utilities to handle task submission and synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: The Process and Pool classes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can create a process that runs independently by subclassing `multiprocessing.Process`.
    You can extend the `__init__` method to initialize resources and you can write
    the portion of the code destined to the subprocess by implementing a `Process.run`
    method. In the following code, we define a process that will wait for one second
    and print its assigned `id`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To spawn the process, we have to initialize our `Process` object and call the
    `Process.start` method. Notice that you don''t directly call `Process.run`: the
    call to `Process.start` will create a new process and, in turn, call the `Process.run`
    method. We can add the following lines at the end of the script to initialize
    and start the new process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The instructions after `Process.start` will be executed immediately without
    waiting for the process `p` to finish. To wait for the task completion you can
    use the method `Process.join`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can launch in the same way four different processes that will run in parallel.
    In a serial program, the total required time would be four seconds. Since we run
    it parallelly, each process will run at the same time, resulting in a 1-second
    wallclock time. In the following code, we create four processes and start them
    parallelly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the order of the execution of parallel processes is unpredictable,
    it ultimately depends on how the operating system schedules the process execution.
    You can verify this behavior by running the program multiple times—the order will
    be different at each run.
  prefs: []
  type: TYPE_NORMAL
- en: The `multiprocessing` module exposes a convenient interface that makes it easy
    to assign and distribute tasks to a set of processes, the `multiprocessing.Pool`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: The `multiprocessing.Pool` class spawns a set of processes—called **workers**—and
    lets submit tasks through the methods `apply`/`apply_async` and `map`/`map_async`.
  prefs: []
  type: TYPE_NORMAL
- en: The `Pool.map` method applies a function to each element of a list and returns
    the list of results. Its usage is equivalent to the built-in (serial) `map`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use a parallel map, you should first initialize a `multiprocessing.Pool`
    object. It takes the number of workers as its first argument; if not provided,
    that number will be equal to the number of cores in the system. You can initialize
    a `multiprocessing.Pool` object in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see `Pool.map` in action. If you have a function that computes the square
    of a number, you can map the function to the list by calling `Pool.map` and passing
    the function and the list of inputs as arguments, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Pool.map_async` method is just like `Pool.map` but returns an `AsyncResult`
    object instead of the actual result. When we call the normal `map`, the execution
    of the main program is stopped until all the workers are finished processing the
    result. With `map_async`, the `AsyncResult` object is returned immediately without
    blocking the main program and the calculations are done in the background. We
    can then retrieve the result by using the `AsyncResult.get` method at any time,
    as shown in the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`Pool.apply_async` assigns a task consisting of a single function to one of
    the workers. It takes the function and its arguments and returns an `AsyncResult`
    object. We can obtain an effect similar to `map` by using `apply_async`, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As an example, we will implement a canonical, embarassingly parallel program:
    the **Monte Carlo approximation of pi**.'
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo approximation of pi
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine we have a square with a side length of 2 units; its area will be 4 units.
    Now, we inscribe a circle with a radius 1 unit in this square, the area of the
    circle will be `pi * r^2`. By substituting the value of `r` in the previous equation
    we get that the numerical value for the area of the circle is `pi * (1)^2 = pi`.
    You can refer to the following figure for a graphical representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we shoot a lot of random points on this figure, some points will fall into
    the circle—we''ll call them **hits**—while the remaining points—**misses**—will
    be outside the circle. The idea of the Monte Carlo method is that the area of
    the circle will be proportional to the number of hits, while the area of the square
    will be proportional to the total number of shots. To get the value of `pi`, it
    is sufficient to divide the area of the circle (equal to `pi`) by the area of
    the square (equal to 4) and solve for `pi`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Monte Carlo approximation of pi](img/8458OS_04_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The strategy we will employ in our program will be:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate a lot of sample (*x*, *y*) numbers in the range (-1, 1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test if those numbers lie inside the circle by checking if `x**2 + y**2 == 1`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We first write a serial version and check if it works. Then, we can write the
    parallel version. The implementation of the serial program is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The accuracy of our approximation will improve as we increase the number of
    samples. You can notice that each loop iteration is independent from the other—this
    problem is embarassingly parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'To parallelize this code, we can write a function called `sample` that corresponds
    to a single hit-miss check. If the sample hits the circle, the function will return
    `1`; otherwise it will return `0`. By running `sample` multiple times and summing
    the results, we''ll get the total number of hits. We can run `sample` over multiple
    processors with `apply_async` and get the results in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can wrap the two versions in the functions `pi_serial` and `pi_apply_async`
    (you can find their implementation in the `pi.py` file) and benchmark the execution
    speed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the previous benchmark, our first parallel version literally cripples
    our code. The reason is that the time spent doing the actual calculation is small
    compared to the overhead required to send and distribute the tasks to the workers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve the issue, we have to make the overhead negligible compared to the
    calculation time. For example, we can ask each worker to handle more than one
    sample at a time, thus reducing the task communication overhead. We can write
    a function `sample_multiple` that processes more than one hit and modifies our
    parallel version by splitting our problem in 10, more intensive tasks as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can wrap this in a function called `pi_apply_async_chunked` and run it as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are much better; we more than doubled the speed of our program.
    You can also notice that the `user` metric is larger than `real`: the total CPU
    time is larger than the total time because more than one CPU worked at the same
    time. If you increase the number of samples, you will notice that the ratio of
    communication to calculation decreases, giving even better speedups.'
  prefs: []
  type: TYPE_NORMAL
- en: Everything is nice and simple when dealing with embarassingly parallel problems.
    But sometimes, you have to share data between processes.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronization and locks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Even if `multiprocessing` uses processes (with their own independent memory),
    it lets you define certain variables and arrays as shared memory. You can define
    a shared variable by using `multiprocessing.Value` passing its data type as a
    string (`i` integer, `d` double, `f` float, and so on). You can update the content
    of the variable through the `value` attribute, as shown in the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'When using shared memory, you should be aware of concurrent accesses. Imagine
    you have a shared integer variable and each process increments its value multiple
    times. You would define a process class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You can initialize the shared variable in the main program and pass it to `4`
    processes, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: If you run this program (`shared.py` in the code directory) you will notice
    that the final value of `counter` is not 4000, but it has random values (on my
    machine they are between 2000 and 2500). If we assume that the arithmetic is correct,
    we can conclude that there's a problem with the parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: What happens is that multiple processes are trying to access the same shared
    variable at the same time. The situation is best explained by looking at the following
    figure. In a serial execution, the first process reads (the number `0`), increments
    it, and writes the new value (`1`); the second process reads the new value (`1`),
    increments it, and writes it again (`2`). In the parallel execution, the two processes
    read the value (`0`), increment it, and write it (`1`) at the same time, leading
    to a wrong answer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Synchronization and locks](img/8458OS_04_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To solve this problem, we need to synchronize the access to this variable so
    that only one process at a time can access, increment, and write the value on
    the shared variable. This feature is provided by the `multiprocessing.Lock` class.
    A lock can be acquired and released through the `acquire` and `release` methods,
    or by using the lock as a context manager. When a process acquires a lock, other
    processes are prevented to acquire it until the lock is released.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define a global lock, and use it as a context manager to restrict the
    access to the counter, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Synchronization primitives such as locks are essential to solve many problems
    but you should avoid overusing them because they can decrease the performance
    of your program.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`multiprocessing` includes other communication and synchronization tools, you
    can refer to the official documentation for a complete reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://docs.python.org/3/library/multiprocessing.html](http://docs.python.org/3/library/multiprocessing.html)'
  prefs: []
  type: TYPE_NORMAL
- en: IPython parallel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: IPython's power is not limited to its advanced shell. Its `parallel` package
    includes a framework to setup and run calculations on single and multi-core machines,
    as well as on multiple nodes connected to a network. IPython is great because
    it gives an interactive twist to parallel computing and provides a common interface
    to different communication protocols.
  prefs: []
  type: TYPE_NORMAL
- en: To use `IPython.parallel`, you have to start a set of workers— **Engines**—that
    are managed by a **Controller** (an entity that mediates the communication between
    the client and the engines). The approach is totally different from multiprocessing;
    you start the worker processes separately, and they will wait indefinitely, listening
    for commands from the client.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start the controller and a set of engines (by default, one engine per processing
    unit) you can use the `ipcluster` shell command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'With `ipcluster` you can also set up multiple nodes to distribute your calculations
    over a network by writing a custom profile. You can refer to the official documentation
    for specific instructions at the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://ipython.org/ipython-doc/dev/parallel/parallel_process.html](http://ipython.org/ipython-doc/dev/parallel/parallel_process.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'After starting the controller and the engines, we can use an IPython shell
    to perform calculations in parallel. IPython provides two basic interfaces (or
    views): **direct** and **task-based**.'
  prefs: []
  type: TYPE_NORMAL
- en: Direct interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The direct interface lets you issue commands explicitly to each of the computing
    units. The interface is intuitive, flexible, and easy-to-use, especially when
    used in an interactive session.
  prefs: []
  type: TYPE_NORMAL
- en: 'After starting the engines, you have to start an IPython session in a separate
    shell to interact with them. By creating a client, you can establish a connection
    to the controller. In the following code, we import the `Client` class and create
    an instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The attribute `Client.ids` will give you a list of integers representing the
    available engines, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can issue commands to the engines by obtaining a `DirectView` instance.
    You can get a `DirectView` instance by either indexing the `Client` instance or
    by calling the `DirectView.direct_view` method. The following code shows different
    ways to obtain a `DirectView` instance from the previously created `Client`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'You can treat the engines like fresh IPython sessions. At the finest level,
    you can execute commands remotely by using the `DirectView.execute` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The command will be sent and executed individually by each engine. The return
    value will be an `AsyncResult` object and the actual return value can be retrieved
    using the `get` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following code, you can retrieve the data contained in a remote
    variable by using the `DirectView.pull` method and send the data to a remote variable
    with the `DirectView.push` method. The `DirectView` class also supports a convenient
    dictionary-like interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: It is possible to send and retrieve every object that can be serialized using
    the `pickle` module. On top of that, special handling is reserved for data structures
    such as **NumPy** arrays to increase the efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you issue a statement that causes an exception, you will receive a summary
    of the exceptions in each engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Engines should be treated as independent IPython sessions, and imports and
    custom-defined functions must be synchronized over the network. To import some
    libraries, both locally and in the engines, you can use the `DirectView.sync_imports`
    context manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: To submit calculations to the engines, `DirectView` provides some utilities
    for common use cases such as map and apply. The `DirectView.map` method works
    similarly to `Pool.map_async`, as shown in the following code snippet. You map
    a function to a sequence, returning an `AsyncResult` object
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'IPython provides a more convenient map implementation through the `DirectView.parallel`
    decorator. If you apply the decorator on a function, the function will now have
    a `map` method that can be applied to a sequence. In the following code, we apply
    the parallel decorator to the `square` function and map it over a series of numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To get the non-blocking version of `map`, you can either use the `DirectView.map_sync`
    method or pass the `block=True` option to the `DirectView.parallel` decorator.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `DirectView.apply` method behaves in a different way than `Pool.apply_async`.
    The function gets executed on *every* engine. For example, if we have selected
    four engines and we apply the `square` function, the function gets executed once
    per engine and it returns four results, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The `DirectiView.remote` decorator lets you create a function that will run
    directly on each engine. Its usage is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The `DirectView` also provides two other kinds of communication scheme: **scatter**
    and **gather**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scatter distributes a list of inputs to the engines. Imagine you have four
    inputs and four engines; you can distribute those inputs in a remote variable
    with `DirectView.scatter`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Scatter will try to distribute the inputs as equally as possible even when
    the number of inputs is not a multiple of the number of engines. The following
    code shows how a list of 11 computations gets processed in three batches of three
    items per batch and one batch of two items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `gather` function simply retrieves the scattered values and merges them
    back. In the following snippet, we merge back the scattered results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We can use the `scatter` and `gather` functions to parallelize one of our simulations.
    In our system, each particle is independent from the other, therefore, we can
    use `scatter` and `gather` to divide the particles equally between the available
    engines, evolve them, and get the particles back from the engines.
  prefs: []
  type: TYPE_NORMAL
- en: At first, we have to set up the engines. The `ParticleSimulator` class should
    be made available to all the engines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that the engines have started in a separate process and the `simul`
    module should be importable by them. You can achieve this in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: By launching `ipcluster` in the directory, where `simul.py` is located
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By adding that directory to `PYTHONPATH`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you're using the code examples, don't forget to compile the Cython extensions
    using `setup.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we create the particles and obtain a `DirectView` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can scatter the particles to a remote variable `particle_chunk`, perform
    the particle evolution using `DirectView.execute` and retrieve the particles.
    We do this using `scatter`, `execute`, and `gather`, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now wrap the parallel version and benchmark it against the serial one
    (refer to the file `simul_parallel.py`) in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The code is extremely simple and gives us a 2x speedup, scalable on any number
    of engines.
  prefs: []
  type: TYPE_NORMAL
- en: Task-based interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IPython has an interface that can handle computing tasks in a smart way. While
    this implies a less flexible interface from the user point of view, it can improve
    performance by balancing the load on the engines and by re-submitting failed jobs.
    In this section, we will introduce the `map` and `apply` functions in the task-based
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'The task interface is provided by the `LoadBalancedView` class, which can be
    obtained from a client using the `load_balanced_view` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: At this point we can run some tasks using `map` and `apply`. The `LoadBalancedView`
    class works similarly to `multiprocessing.Pool`, the tasks are submitted and handled
    by a scheduler; in the case of `LoadBalancedView`, the task assignment is based
    on how much load is present on an engine at a given time, ensuring that all the
    engines are working without downtimes.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s helpful to explain an important difference between `apply` in `DirectView`
    and `LoadBalancedView`. A call to `DirectView.apply` will run on *every* selected
    engine, while a call to `LoadBalancedView.apply` will schedule a *single* task
    to one of the engines. In the first case, the result will be a list, and in the
    latter, it will be a single value, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '`LoadBalancedView` is also able to handle failures and run tasks on engines
    when certain conditions are met. This feature is provided through a dependency
    system. We will not cover this aspect in this book, but interested readers can
    refer to the official documentation at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://ipython.org/ipython-doc/rel-1.1.0/parallel/parallel_task.html](http://ipython.org/ipython-doc/rel-1.1.0/parallel/parallel_task.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Parallel Cython with OpenMP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cython provides a convenient interface to perform shared-memory parallel processing
    through **OpenMP**. This lets you write extremely efficient parallel code directly
    in Cython without having to create a C wrapper.
  prefs: []
  type: TYPE_NORMAL
- en: OpenMP is a specification to write multithreaded programs, and includes series
    of C preprocessor directives to manage threads; these include communication patterns,
    load balancing, and synchronization features. Several C/C++ and Fortran compilers
    (including GCC) implement the OpenMP API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s introduce Cython parallel features with a small example. Cython provides
    a simple API based on OpenMP in the `cython.parallel` module. The simplest construct
    is `prange`: a construct that automatically distributes loop operations in multiple
    threads.'
  prefs: []
  type: TYPE_NORMAL
- en: First of all, we can write a serial version of a program that computes the square
    of each element of a NumPy array in the `hello_parallel.pyx` file. We get a buffer
    as input and we create an output array by populating it with the squares of the
    input array elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The serial version, `square_serial`, is shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can change the loop in a parallel version by substituting the range
    call with `prange`. There's a caveat, you need to make sure that the body of the
    loop is interpreter-free. As already explained, to make use of threads we need
    to release the GIL, since interpreter calls acquire and release the GIL, we should
    avoid them. Failure in doing so will result in compilation errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Cython, you can release the GIL by using `nogil`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can use the convenient option `nogil=True` of `prange` that
    will automatically wrap the loop in a `nogil` block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Attempts to call Python code in a `prange` block results in an error. This
    includes assignment operations, function calls, objects initialization, and so
    on. To include such operations in a `prange` block (you may want to do so for
    debugging purposes) you have to re-enable the GIL using the `with gil` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we need to recompile our extension. We need to change `setup.py`
    to enable OpenMP support. You have to specify the GCC option `-fopenmp` using
    the `Extension` class in `distutils` and pass it to the `cythonize` function.
    The following code shows the complete `setup.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Now that we know how to use `prange`, we can quickly parallelize the Cython
    version of our `ParticleSimulator`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we can take a look at the `c_evolve` function contained
    in the Cython module `cevolve.pyx` that we wrote in [Chapter 2](ch02.html "Chapter 2. Fast
    Array Operations with NumPy"), *Fast Array Operations with NumPy*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing we have to do is invert the order of the loops; we want the
    outermost loop to be the parallel one, where each iteration is independent from
    the other. Since the particles don''t interact with each other, we can change
    the order of iteration safely, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'At that point we can parallelize the loop using `prange`, we already removed
    the interpreter-related calls when we added static typing, so the `nogil` block
    can be applied safely, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now wrap the two different versions into separate functions and we can
    time them, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: With OpenMP, we are able to obtain a significant speedup compared to the serial
    Cython version by changing a single line of code.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parallel processing is an effective way to increase the speed of your programs
    or to handle large amounts of data. Embarassingly parallel problems are excellent
    candidates for parallelization and lead to a straightforward implementation and
    optimal scaling.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we illustrated the basics of parallel programming in Python.
    We learned how to use multiprocessing to easily parallelize programs with the
    tools already included in Python. Another more powerful tool for parallel processing
    is IPython parallel. This package allows you to interactively prototype parallel
    programs and manage a network of computing nodes effectively. Finally, we explored
    the easy-to-use multithreading capabilities of Cython and OpenMP.
  prefs: []
  type: TYPE_NORMAL
- en: During the course of this book, we learned the most effective techniques to
    design, benchmark, profile, and optimize Python applications. NumPy can be used
    to elegantly rewrite Python loops, and if it is not enough, you can use Cython
    to generate efficient C code. At the last stage, you can easily parallelize your
    program using the tools presented in this chapter.
  prefs: []
  type: TYPE_NORMAL
