<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Message Passing</span></h1>
                </header>
            
            <article>
                
<p class="mce-root"><span class="koboSpan" id="kobo.2.1">This chapter will briefly cover the </span><strong><span class="koboSpan" id="kobo.3.1">Message Passing Interface</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong><span class="koboSpan" id="kobo.5.1">MPI</span></strong><span class="koboSpan" id="kobo.6.1">), which is a specification for message exchange. </span><span class="koboSpan" id="kobo.6.2">The primary goal of </span><span><span class="koboSpan" id="kobo.7.1">the </span></span><span class="koboSpan" id="kobo.8.1">MPI is to es</span><span><span class="koboSpan" id="kobo.9.1">tablish an efficient, flexible</span></span><span><span><span class="koboSpan" id="kobo.10.1">, and portable standard for message exchange communication.</span></span></span></p>
<p><span><span class="koboSpan" id="kobo.11.1">Mainly, w</span></span><span class="koboSpan" id="kobo.12.1">e will show the functions of the library that include synchronous and asynchronous communication primitives, such as (send/receive) and (broadcast/all-to-all), the operations of combining the partial results of the calculation </span><span><span class="koboSpan" id="kobo.13.1">(</span></span><span class="koboSpan" id="kobo.14.1">gather/reduce</span><span><span class="koboSpan" id="kobo.15.1">),</span></span><span class="koboSpan" id="kobo.16.1"> and finally, the synchronization primitives between processes (barriers).</span></p>
<p class="packt_figure"><span class="koboSpan" id="kobo.17.1">Furthermore, the control functions of the communication network will be presented by defining the</span><span><span class="koboSpan" id="kobo.18.1"> </span></span><span class="koboSpan" id="kobo.19.1">topologies.</span></p>
<p><span class="koboSpan" id="kobo.20.1">In this chapter, we will cover the following recipes:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.21.1">Using the</span><span><span class="koboSpan" id="kobo.22.1"> </span></span><kbd><span class="koboSpan" id="kobo.23.1">mpi4py</span></kbd><span><span class="koboSpan" id="kobo.24.1"> </span></span><span class="koboSpan" id="kobo.25.1">Python module</span></li>
<li><span class="koboSpan" id="kobo.26.1">Implementing point-to-point</span><span><span class="koboSpan" id="kobo.27.1"> </span></span><span class="koboSpan" id="kobo.28.1">communication</span></li>
<li><span class="koboSpan" id="kobo.29.1">Avoiding</span><span><span class="koboSpan" id="kobo.30.1"> </span></span><span class="koboSpan" id="kobo.31.1">deadlock</span><span><span class="koboSpan" id="kobo.32.1"> </span></span><span class="koboSpan" id="kobo.33.1">problems</span></li>
<li><span class="koboSpan" id="kobo.34.1">Collective communication using a</span><span><span class="koboSpan" id="kobo.35.1"> </span></span><span class="koboSpan" id="kobo.36.1">broadcast</span></li>
<li><span class="koboSpan" id="kobo.37.1">Collective communication using the</span><span><span class="koboSpan" id="kobo.38.1"> </span></span><kbd><span class="koboSpan" id="kobo.39.1">scatter</span></kbd><span class="koboSpan" id="kobo.40.1"> function</span></li>
<li><span class="koboSpan" id="kobo.41.1">Collective communication using the</span><em><span><span class="koboSpan" id="kobo.42.1"> </span></span></em><kbd><span class="koboSpan" id="kobo.43.1">gather</span></kbd><span class="koboSpan" id="kobo.44.1"> function</span></li>
<li><span class="koboSpan" id="kobo.45.1">Collective communication using</span><span><span class="koboSpan" id="kobo.46.1"> </span></span><kbd><span class="koboSpan" id="kobo.47.1">Alltoall</span></kbd></li>
<li><span class="koboSpan" id="kobo.48.1">The reduction</span><span><span class="koboSpan" id="kobo.49.1"> </span></span><span class="koboSpan" id="kobo.50.1">operation</span></li>
<li><span class="koboSpan" id="kobo.51.1">Optimizing</span><span><span class="koboSpan" id="kobo.52.1"> </span></span><span class="koboSpan" id="kobo.53.1">communication</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Technical requirements</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">You will need the </span><kbd><span class="koboSpan" id="kobo.3.1">mpich</span></kbd><span class="koboSpan" id="kobo.4.1"> and </span><kbd><span class="koboSpan" id="kobo.5.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.6.1"> libraries for this chapter. </span></span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.7.1">The </span><kbd><span><span class="koboSpan" id="kobo.8.1">mpich</span></span></kbd><span class="koboSpan" id="kobo.9.1"> library is a portable implementation of MPI. </span><span class="koboSpan" id="kobo.9.2">It is free software and is available for various versions of Unix (including Linux and macOS) and Microsoft Windows. </span></p>
<p><span class="koboSpan" id="kobo.10.1">To install </span><kbd><span class="koboSpan" id="kobo.11.1">mpich</span></kbd><span class="koboSpan" id="kobo.12.1">, </span><span><span class="koboSpan" id="kobo.13.1">use the installer downloaded from the downloads page (</span></span><a href="http://www.mpich.org/static/downloads/1.4.1p1/"><span class="koboSpan" id="kobo.14.1">http://www.mpich.org/static/downloads/1.4.1p1/</span></a><span><span class="koboSpan" id="kobo.15.1">). </span><span class="koboSpan" id="kobo.15.2">Moreover, make sure to choose between the 32-bit or 64-bit versions to get the right one for your machine.</span></span></p>
<p><span><span class="koboSpan" id="kobo.16.1">The </span><kbd><span class="koboSpan" id="kobo.17.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.18.1"> Python module provides Python bindings for the MPI</span></span><span><span class="koboSpan" id="kobo.19.1"> (</span><a href="https://www.mpi-forum.org"><span class="koboSpan" id="kobo.20.1">https://www.mpi-forum.org</span></a></span><span><span class="koboSpan" id="kobo.21.1">) standard. </span><span class="koboSpan" id="kobo.21.2">It is implemented on top of the MPI-1/2/3 specification and exposes an API that is based on the standard MPI-2 C++ bindings.</span></span></p>
<p><span><span class="koboSpan" id="kobo.22.1">The installation procedure of </span></span><kbd><span class="koboSpan" id="kobo.23.1">mpi4py</span></kbd><span><span class="koboSpan" id="kobo.24.1"> on a Windows machine is as follows</span></span><span><span class="koboSpan" id="kobo.25.1">:</span></span></p>
<pre><strong><span class="koboSpan" id="kobo.26.1">C:&gt;pip install mpi4py</span></strong></pre>
<p><span class="koboSpan" id="kobo.27.1">Anaconda users must type the following:</span></p>
<pre><strong><span class="koboSpan" id="kobo.28.1">C:&gt;conda install mpi4py</span></strong></pre>
<p><span class="koboSpan" id="kobo.29.1">Note that for all the examples in this chapter, we used </span><kbd><span class="koboSpan" id="kobo.30.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.31.1"> installed by using the </span><kbd><span class="koboSpan" id="kobo.32.1">pip</span></kbd><span class="koboSpan" id="kobo.33.1"> installer</span></p>
<p><span class="koboSpan" id="kobo.34.1">This implies that the notation used to run the </span><kbd><span class="koboSpan" id="kobo.35.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.36.1"> examples is as follows:</span></p>
<pre><strong><span class="koboSpan" id="kobo.37.1">C:&gt;mpiexec -n x python mpi4py_script_name.py </span></strong></pre>
<p><span class="koboSpan" id="kobo.38.1">The </span><kbd><span class="koboSpan" id="kobo.39.1">mpiexec</span></kbd><span class="koboSpan" id="kobo.40.1"> </span><span><span class="koboSpan" id="kobo.41.1">command is the typical way to start parallel jobs:</span></span><span class="koboSpan" id="kobo.42.1"> </span><kbd><span class="koboSpan" id="kobo.43.1">x</span></kbd><span class="koboSpan" id="kobo.44.1"> is the total number of processes to use, while </span><kbd><span class="koboSpan" id="kobo.45.1">mpi4py_script_name.py</span></kbd><span class="koboSpan" id="kobo.46.1"> is the name of the script to be executed.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Understanding the MPI structure</span></h1>
                </header>
            
            <article>
                
<p class="packt_figure"><span><span class="koboSpan" id="kobo.2.1">The MPI standard defines the primitives for the management of virtual topologies, synchronization, and communication between processes.</span></span><span class="koboSpan" id="kobo.3.1"> </span><span><span class="koboSpan" id="kobo.4.1">There are several MPI implementations that differ in the version and features of the standard supported.</span></span></p>
<p class="packt_figure"><span><span><span class="koboSpan" id="kobo.5.1">We will introduce the MPI standard through the Python</span></span><span><span class="koboSpan" id="kobo.6.1"> </span></span><kbd><span class="koboSpan" id="kobo.7.1">mpi4py</span></kbd><span><span class="koboSpan" id="kobo.8.1"> library.</span></span></span></p>
<p><span class="koboSpan" id="kobo.9.1">Before the 1990s, writing parallel applications for different architectures was a more difficult job than what it is today. </span><span class="koboSpan" id="kobo.9.2">Many libraries facilitated the process, but there was not a standard way to do it. </span><span class="koboSpan" id="kobo.9.3">At that time, most parallel applications were destined for scientific research environments.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.10.1">The model that was most commonly adopted by the various libraries was the message-passing model, in which the communication between the processes takes place through the exchange of messages and without the use of shared resources. </span><span class="koboSpan" id="kobo.10.2">For example, the master process can assign a job to the slaves simply by sending a message that describes the work to be done. A second, very simple, example here is a parallel application that performs a merge sort. </span><span class="koboSpan" id="kobo.10.3">The data is sorted locally to the processes and the results are passed to other processes that will deal with the merge.</span></p>
<p><span class="koboSpan" id="kobo.11.1">Since the libraries largely used the same model, albeit with minor differences from each other, the authors of the various libraries met in 1992 to define a standard interface for the exchange of messages, and, from here, </span><span><span class="koboSpan" id="kobo.12.1">MPI</span></span><span class="koboSpan" id="kobo.13.1"> was born. </span><span class="koboSpan" id="kobo.13.2">This interface had to allow programmers to write portable parallel applications on most parallel architectures, using the same features and models they were already used to.</span></p>
<p><span class="koboSpan" id="kobo.14.1">Originally, MPI was designed for distributed memory architectures, which began to grow in popularity 20 years ago:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.15.1"><img src="assets/c5eade1c-0ee1-4194-a00c-d8686149c550.png" style="width:21.75em;height:12.08em;"/></span></p>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.16.1">The distributed memory architecture schema</span></div>
<p><span class="koboSpan" id="kobo.17.1">Over time, distributed memory systems began to be combined with each other, creating hybrid systems with distributed/shared memory:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.18.1"><img src="assets/0ac1b93c-4e24-4612-b1d7-1935c8f0a661.png" style="width:22.50em;height:12.42em;"/></span></p>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign"><span><span class="koboSpan" id="kobo.19.1">The </span></span><span class="koboSpan" id="kobo.20.1">hybrid system architecture schema</span></div>
<p><span class="koboSpan" id="kobo.21.1">Today, MPI runs on distributed memory, shared memory, and hybrid systems. </span><span class="koboSpan" id="kobo.21.2">However, the programming model remains that of distributed memory, although the true architecture on which the calculation is performed may be different.</span></p>
<p><span class="koboSpan" id="kobo.22.1">The strengths of MPI can be summarized as follows:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.23.1">Standardization</span></strong><span class="koboSpan" id="kobo.24.1">: It is supported by all </span><strong><span class="koboSpan" id="kobo.25.1">High-Performance</span></strong> <strong><span class="koboSpan" id="kobo.26.1">Computing</span></strong><span class="koboSpan" id="kobo.27.1"> (</span><strong><span class="koboSpan" id="kobo.28.1">HPC</span></strong><span class="koboSpan" id="kobo.29.1">) platforms.</span></li>
<li><strong><span class="koboSpan" id="kobo.30.1">Portability</span></strong><span class="koboSpan" id="kobo.31.1">: The changes applied to the source code are minimal, which is useful if you decide to use the application on a different platform that also supports the same standard.</span></li>
<li><strong><span class="koboSpan" id="kobo.32.1">Performance</span></strong><span class="koboSpan" id="kobo.33.1">: Manufacturers can create implementations optimized for a specific type of hardware and get better performance.</span></li>
<li><strong><span class="koboSpan" id="kobo.34.1">Functionality</span></strong><span class="koboSpan" id="kobo.35.1">: Over 440 routines are defined in MPI-3, but many parallel programs can be written using fewer than </span><span><span class="koboSpan" id="kobo.36.1">even </span></span><span class="koboSpan" id="kobo.37.1">10 routines.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.38.1">In the following sections, we will examine the main Python library for message passing: the </span><kbd><span class="koboSpan" id="kobo.39.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.40.1"> library.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Using the mpi4py Python module</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The Python programming language provides several MPI modules to write parallel programs. </span><span class="koboSpan" id="kobo.2.2">The most interesting of these is the </span><kbd><span class="koboSpan" id="kobo.3.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.4.1"> library. </span><span class="koboSpan" id="kobo.4.2">It is constructed on top of the MPI-1/2 specifications and provides an object-oriented interface, which closely follows the MPI-2 C++ bindings. </span><span class="koboSpan" id="kobo.4.3">A C MPI user could use this module without learning a new interface. </span><span class="koboSpan" id="kobo.4.4">Therefore, it is widely used as an almost-full package of an MPI library in Python.</span></p>
<p><span class="koboSpan" id="kobo.5.1">The main applications of the module, which will be described in this chapter, are as follows:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.6.1">Point-to-point communication</span></li>
<li><span class="koboSpan" id="kobo.7.1">Collective communication</span></li>
<li><span class="koboSpan" id="kobo.8.1">Topologies</span></li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How to do it...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Let's start our journey to the MPI library by examining the classic code of a program that prints the phrase </span><kbd><span class="koboSpan" id="kobo.3.1">Hello, world!</span></kbd><span class="koboSpan" id="kobo.4.1"> on each process that is instantiated:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.5.1">Import the </span><kbd><span class="koboSpan" id="kobo.6.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.7.1"> library:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.8.1">from mpi4py import MPI </span></pre>
<div class="packt_infobox"><span class="koboSpan" id="kobo.9.1">In MPI, the processes involved in the execution of a parallel program are identified by a sequence of non-negative integers called </span><strong><span class="koboSpan" id="kobo.10.1">ranks</span></strong><span class="koboSpan" id="kobo.11.1">.</span></div>
<ol start="2">
<li><span class="koboSpan" id="kobo.12.1">If we have a number (</span><em><span class="koboSpan" id="kobo.13.1">p</span></em><span class="koboSpan" id="kobo.14.1"> of processes) that runs a program, then the processes will have a </span><kbd><span class="koboSpan" id="kobo.15.1">rank</span></kbd><span class="koboSpan" id="kobo.16.1"> that goes from </span><em><span class="koboSpan" id="kobo.17.1">0</span></em><span class="koboSpan" id="kobo.18.1"> to </span><em><span class="koboSpan" id="kobo.19.1">p</span></em><span class="koboSpan" id="kobo.20.1">-1. </span><span class="koboSpan" id="kobo.20.2">In particular, in order to assess the rank of each process, we must use the </span><kbd><span class="koboSpan" id="kobo.21.1">COMM_WORLD</span></kbd><span class="koboSpan" id="kobo.22.1"> </span><span><span class="koboSpan" id="kobo.23.1">MPI function i</span></span><span class="koboSpan" id="kobo.24.1">n particular. </span><span class="koboSpan" id="kobo.24.2">This function </span><span><span class="koboSpan" id="kobo.25.1">is called a </span></span><strong><span class="koboSpan" id="kobo.26.1">communicator</span></strong><span><span class="koboSpan" id="kobo.27.1">, as it defines its own set of all processes that can</span></span><span><span class="koboSpan" id="kobo.28.1"> </span></span><span class="koboSpan" id="kobo.29.1">communicate together:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.30.1"> comm = MPI.COMM_WORLD </span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.31.1">Finally, the following </span><kbd><span class="koboSpan" id="kobo.32.1">Get_rank()</span></kbd> <span><span class="koboSpan" id="kobo.33.1">function </span></span><span class="koboSpan" id="kobo.34.1">returns </span><kbd><span class="koboSpan" id="kobo.35.1">rank</span></kbd><span class="koboSpan" id="kobo.36.1"> of the process calling it:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.37.1">rank = comm.Get_rank() </span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.38.1">Once evaluated, </span><kbd><span class="koboSpan" id="kobo.39.1">rank</span></kbd><span class="koboSpan" id="kobo.40.1"> is printed:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.41.1">print ("hello world from process ", rank)  </span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How it works...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">According to the MPI execution model, our application consists of</span><span><span class="koboSpan" id="kobo.3.1"> </span></span><em><span class="koboSpan" id="kobo.4.1">N</span></em><span><span class="koboSpan" id="kobo.5.1"> </span></span><span class="koboSpan" id="kobo.6.1">(5 in this example)</span><span><span class="koboSpan" id="kobo.7.1"> </span></span><span class="koboSpan" id="kobo.8.1">autonomous</span><span><span class="koboSpan" id="kobo.9.1"> </span></span><span class="koboSpan" id="kobo.10.1">processes, each with their own local memory able to communicate data through the exchange of messages.</span></p>
<p><span class="koboSpan" id="kobo.11.1">The</span><span><span class="koboSpan" id="kobo.12.1"> </span></span><span class="koboSpan" id="kobo.13.1">communicator</span><span><span class="koboSpan" id="kobo.14.1"> </span></span><span class="koboSpan" id="kobo.15.1">defines a group of processes that can communicate with each other. </span><span class="koboSpan" id="kobo.15.2">The</span><span><span class="koboSpan" id="kobo.16.1"> </span></span><kbd><span class="koboSpan" id="kobo.17.1">MPI_COMM_WORLD</span></kbd><span><span class="koboSpan" id="kobo.18.1"> work </span></span><span class="koboSpan" id="kobo.19.1">used here is the default communicator and includes all processes.</span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.20.1">The identification of a process is based on</span><span><span class="koboSpan" id="kobo.21.1"> </span></span><span class="koboSpan" id="kobo.22.1">ranks. </span><span class="koboSpan" id="kobo.22.2">Each process is assigned a rank for each communicator to which it belongs. </span><span class="koboSpan" id="kobo.22.3">The rank is an integer that is assigned, which starts from zero and identifies each individual process in the context of a specific communicator. </span><span class="koboSpan" id="kobo.22.4">The common practice is to define the process with a global rank of </span><em><span class="koboSpan" id="kobo.23.1">0</span></em><span class="koboSpan" id="kobo.24.1"> as the master process. </span><span class="koboSpan" id="kobo.24.2">Through the rank, the developer can specify what the sending process is and what the recipient processes are instead.</span></p>
<p><span class="koboSpan" id="kobo.25.1">It should be noted that, for illustration purposes only, the</span><span><span class="koboSpan" id="kobo.26.1"> </span></span><kbd><span class="koboSpan" id="kobo.27.1">stdout</span></kbd><span><span class="koboSpan" id="kobo.28.1"> </span></span><span class="koboSpan" id="kobo.29.1">output will not always be ordered, as multiple processes can apply at the same time by writing on the screen and the OS arbitrarily chooses the order. </span><span class="koboSpan" id="kobo.29.2">So, we are ready for a fundamental observation: every process involved in the execution of MPI runs the same compiled binary, so each process receives the same instructions to be executed.</span></p>
<p><span class="koboSpan" id="kobo.30.1">To execute the code, type the following command line:</span></p>
<pre><strong><span class="koboSpan" id="kobo.31.1">C:&gt;mpiexec -n 5 python helloworld_MPI.py </span></strong></pre>
<p class="mce-root"><span><span class="koboSpan" id="kobo.32.1">This is the result that we will get after executing this code (notice how the order of execution of the processes </span><em><span class="koboSpan" id="kobo.33.1">is not sequential</span></em><span class="koboSpan" id="kobo.34.1">):</span></span></p>
<pre><strong><span class="koboSpan" id="kobo.35.1">hello world from process  1 
hello world from process  0 
hello world from process  2 
hello world from process  3
hello world from process  4</span></strong></pre>
<div class="packt_infobox"><span class="koboSpan" id="kobo.36.1">It should be noted that the number of processes to be used is strictly dependent on the characteristics of the machine on which the program must run.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">There's more...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">MPI belongs to the </span><strong><span class="koboSpan" id="kobo.3.1">Single Program Multiple Data</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong><span class="koboSpan" id="kobo.5.1">SPMD</span></strong><span class="koboSpan" id="kobo.6.1">) programming technique.</span></p>
<p><span class="koboSpan" id="kobo.7.1">SPMD is a programming technique in which all processes execute the same program, each on different data. </span><span class="koboSpan" id="kobo.7.2">The distinction in executions between different processes occurs by differentiating the flow of the program, based on the local rank of the process.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span><span class="koboSpan" id="kobo.8.1">SPMD</span></span><span class="koboSpan" id="kobo.9.1"> is a programming technique in which a single program is executed by several processes at the same time, but each process can operate on different data. </span><span class="koboSpan" id="kobo.9.2">At the same time, the processes can execute both the same instruction and different instructions. </span><span class="koboSpan" id="kobo.9.3">Obviously, the program will contain appropriate instructions that allow the execution of only parts of the code and/or to operate on a subset of the data. </span><span class="koboSpan" id="kobo.9.4">This can be implemented using different programming models, and all executables start at the same time.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">See also</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The complete reference to the </span><kbd><span class="koboSpan" id="kobo.3.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.4.1"> library can be found at </span><a href="https://mpi4py.readthedocs.io/en/stable/"><span class="koboSpan" id="kobo.5.1">https://mpi4py.readthedocs.io/en/stable/</span></a><span class="koboSpan" id="kobo.6.1">.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Implementing point-to-point communication</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Point-to-point operations consist of the exchange of messages between two processes. </span><span class="koboSpan" id="kobo.2.2">In a perfect world, every sending operation would be perfectly synchronized with the respective reception operation. </span><span class="koboSpan" id="kobo.2.3">Obviously, this is not the case, and the MPI </span><span><span class="koboSpan" id="kobo.3.1">implementation </span></span><span class="koboSpan" id="kobo.4.1">must be able to preserve the data sent when the sender and recipient processes are not synchronized. </span><span class="koboSpan" id="kobo.4.2">Typically, this occurs using a buffer, which is transparent to the developer and entirely managed by the </span><kbd><span class="koboSpan" id="kobo.5.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.6.1"> library.</span></p>
<p><span class="koboSpan" id="kobo.7.1">The </span><kbd><span class="koboSpan" id="kobo.8.1">mpi4py</span></kbd><span><span class="koboSpan" id="kobo.9.1"> Python module </span></span><span class="koboSpan" id="kobo.10.1">enables point-to-point communication via two functions:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.11.1">Comm.Send(data, process_destination)</span></kbd><span class="koboSpan" id="kobo.12.1">:</span><span><span class="koboSpan" id="kobo.13.1"> </span></span><span class="koboSpan" id="kobo.14.1">This function sends data to the destination process identified by its rank in the communicator group.</span></li>
<li><kbd><span class="koboSpan" id="kobo.15.1">Comm.Recv(process_source)</span></kbd><span class="koboSpan" id="kobo.16.1">: This function receives data from the sourcing process, which is also identified by its rank in the communicator group.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.17.1">The </span><kbd><span class="koboSpan" id="kobo.18.1">Comm</span></kbd><span class="koboSpan" id="kobo.19.1"> parameter, which is short for </span><em><span class="koboSpan" id="kobo.20.1">communicator</span></em><span class="koboSpan" id="kobo.21.1">, defines the group of processes that may communicate through message passing using </span><kbd><span class="koboSpan" id="kobo.22.1">comm = MPI.COMM_WORLD</span></kbd><span class="koboSpan" id="kobo.23.1">.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How to do it...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the following example, we will utilize the </span><kbd><span class="koboSpan" id="kobo.3.1">comm.send</span></kbd><span class="koboSpan" id="kobo.4.1"> and </span><kbd><span class="koboSpan" id="kobo.5.1">comm.recv</span></kbd><span class="koboSpan" id="kobo.6.1"> directives to exchange messages between different processes:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.7.1">Import the relevant </span><kbd><span class="koboSpan" id="kobo.8.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.9.1"> library:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.10.1">from mpi4py import MPI</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<ol start="2">
<li><span class="koboSpan" id="kobo.11.1">Then, we define the communicator parameter, namely </span><kbd><span class="koboSpan" id="kobo.12.1">comm</span></kbd><span class="koboSpan" id="kobo.13.1">, through the </span><kbd><span class="koboSpan" id="kobo.14.1">MPI.COMM_WORLD</span></kbd><span class="koboSpan" id="kobo.15.1"> statement:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.16.1">comm=MPI.COMM_WORLD </span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.17.1">The </span><kbd><span class="koboSpan" id="kobo.18.1">rank</span></kbd><span class="koboSpan" id="kobo.19.1"> parameter is used to identify the process itself:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.20.1">rank = comm.rank </span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.21.1">It is useful to print out the </span><kbd><span class="koboSpan" id="kobo.22.1">rank</span></kbd><span class="koboSpan" id="kobo.23.1"> of a process:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.24.1">print("my rank is : " , rank) </span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.25.1">Then, we start considering the rank of the process. </span><span class="koboSpan" id="kobo.25.2">In this case, for the process of </span><kbd><span class="koboSpan" id="kobo.26.1">rank</span></kbd><span class="koboSpan" id="kobo.27.1"> equal to </span><kbd><span class="koboSpan" id="kobo.28.1">0</span></kbd><span class="koboSpan" id="kobo.29.1">, we set </span><kbd><span class="koboSpan" id="kobo.30.1">destination_process</span></kbd><span class="koboSpan" id="kobo.31.1"> and </span><kbd><span class="koboSpan" id="kobo.32.1">data</span></kbd><span class="koboSpan" id="kobo.33.1"> (in this case </span><kbd><span class="koboSpan" id="kobo.34.1">data = 10000000</span></kbd><span class="koboSpan" id="kobo.35.1">) to be sent:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.36.1">if rank==0: 
    data= 10000000 
    destination_process = 4 </span></pre>
<ol start="6">
<li><span class="koboSpan" id="kobo.37.1">Then, by using the </span><kbd><span class="koboSpan" id="kobo.38.1">comm.send</span></kbd><span class="koboSpan" id="kobo.39.1"> statement</span><span><span class="koboSpan" id="kobo.40.1">, </span></span><span class="koboSpan" id="kobo.41.1">the data that was previously set is sent to the destination process:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.42.1">    comm.send(data,dest=destination_process) 
    print ("sending data %s " %data + \  
           "to process %d" %destination_process) </span></pre>
<ol start="7">
<li><span class="koboSpan" id="kobo.43.1">For the process of </span><kbd><span class="koboSpan" id="kobo.44.1">rank</span></kbd><span class="koboSpan" id="kobo.45.1"> equal to </span><kbd><span class="koboSpan" id="kobo.46.1">1</span></kbd><span class="koboSpan" id="kobo.47.1">, the </span><kbd><span class="koboSpan" id="kobo.48.1">destination_process</span></kbd><span class="koboSpan" id="kobo.49.1"> value is </span><kbd><span class="koboSpan" id="kobo.50.1">8</span></kbd><span class="koboSpan" id="kobo.51.1">, while the data to be sent is the </span><kbd><span class="koboSpan" id="kobo.52.1">"hello"</span></kbd><span class="koboSpan" id="kobo.53.1"> </span><span><span class="koboSpan" id="kobo.54.1">string:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.55.1">if rank==1: 
    destination_process = 8 
    data= "hello" 
    comm.send(data,dest=destination_process) 
    print ("sending data %s :" %data + \  
           "to process %d" %destination_process) </span></pre>
<ol start="8">
<li><span class="koboSpan" id="kobo.56.1">The process of </span><kbd><span class="koboSpan" id="kobo.57.1">rank</span></kbd><span class="koboSpan" id="kobo.58.1"> equal to </span><kbd><span class="koboSpan" id="kobo.59.1">4</span></kbd><span class="koboSpan" id="kobo.60.1"> is a receiver process. </span><span class="koboSpan" id="kobo.60.2">Indeed, the source process (that is, the process of </span><kbd><span class="koboSpan" id="kobo.61.1">rank</span></kbd><span class="koboSpan" id="kobo.62.1"> equal to </span><kbd><span class="koboSpan" id="kobo.63.1">0</span></kbd><span class="koboSpan" id="kobo.64.1">) is set as a parameter in the </span><kbd><span class="koboSpan" id="kobo.65.1">comm.recv</span></kbd><span class="koboSpan" id="kobo.66.1"> statement:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.67.1">if rank==4: 
    data=comm.recv(source=0) </span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="9">
<li><span class="koboSpan" id="kobo.68.1">Now, using the following code, the data received from the process of </span><kbd><span class="koboSpan" id="kobo.69.1">0</span></kbd><span class="koboSpan" id="kobo.70.1"> must be displayed:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.71.1">    print ("data received is = %s" %data) </span></pre>
<ol start="10">
<li><span class="koboSpan" id="kobo.72.1">The last process to be set is number </span><kbd><span class="koboSpan" id="kobo.73.1">9</span></kbd><span class="koboSpan" id="kobo.74.1">. </span><span class="koboSpan" id="kobo.74.2">Here, we define the source process of </span><kbd><span class="koboSpan" id="kobo.75.1">rank</span></kbd><span class="koboSpan" id="kobo.76.1"> equal to </span><kbd><span class="koboSpan" id="kobo.77.1">1</span></kbd><span class="koboSpan" id="kobo.78.1"> as a parameter in the </span><kbd><span class="koboSpan" id="kobo.79.1">comm.recv</span></kbd><span class="koboSpan" id="kobo.80.1"> statement:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.81.1">if rank==8: 
    data1=comm.recv(source=1) </span></pre>
<ol start="11">
<li><span class="koboSpan" id="kobo.82.1">The </span><kbd><span class="koboSpan" id="kobo.83.1">data1</span></kbd><span class="koboSpan" id="kobo.84.1"> value is then printed:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.85.1"> print ("data1 received is = %s" %data1) </span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How it works...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">We ran the example with a total number of processes equal to </span><kbd><span class="koboSpan" id="kobo.3.1">9</span></kbd><span class="koboSpan" id="kobo.4.1">. </span><span class="koboSpan" id="kobo.4.2">So, in the</span><span><span class="koboSpan" id="kobo.5.1"> </span></span><kbd><span class="koboSpan" id="kobo.6.1">comm</span></kbd><span class="koboSpan" id="kobo.7.1"> </span><span><span class="koboSpan" id="kobo.8.1">communicator group, </span></span><span class="koboSpan" id="kobo.9.1">we have nine tasks that can communicate with each other:</span></p>
<pre><span class="koboSpan" id="kobo.10.1">comm=MPI.COMM_WORLD </span></pre>
<p><span class="koboSpan" id="kobo.11.1">Also, to identify a task or processes inside the group, we use their</span><span><span class="koboSpan" id="kobo.12.1"> </span></span><kbd><span class="koboSpan" id="kobo.13.1">rank</span></kbd><span><span class="koboSpan" id="kobo.14.1"> </span></span><span class="koboSpan" id="kobo.15.1">value:</span></p>
<pre><span class="koboSpan" id="kobo.16.1">rank = comm.rank </span></pre>
<p><span class="koboSpan" id="kobo.17.1">We have two sender processes and two receiver processes. The process of </span><kbd><span class="koboSpan" id="kobo.18.1">rank</span></kbd><span class="koboSpan" id="kobo.19.1"> equal to </span><kbd><span class="koboSpan" id="kobo.20.1">0</span></kbd><span class="koboSpan" id="kobo.21.1"> sends numerical data to the receiver process of </span><kbd><span class="koboSpan" id="kobo.22.1">rank</span></kbd><span class="koboSpan" id="kobo.23.1"> equal to </span><kbd><span class="koboSpan" id="kobo.24.1">4</span></kbd><span class="koboSpan" id="kobo.25.1">:</span></p>
<pre><span class="koboSpan" id="kobo.26.1">if rank==0: 
    data= 10000000 
    destination_process = 4 
    comm.send(data,dest=destination_process) </span></pre>
<p><span class="koboSpan" id="kobo.27.1">Similarly, we must specify the receiver process of</span><span><span class="koboSpan" id="kobo.28.1"> </span></span><kbd><span class="koboSpan" id="kobo.29.1">rank</span></kbd><span><span class="koboSpan" id="kobo.30.1"> </span></span><span class="koboSpan" id="kobo.31.1">equal to </span><kbd><span class="koboSpan" id="kobo.32.1">4</span></kbd><span class="koboSpan" id="kobo.33.1">. </span><span class="koboSpan" id="kobo.33.2">We also note that the</span><span><span class="koboSpan" id="kobo.34.1"> </span></span><kbd><span class="koboSpan" id="kobo.35.1">comm.recv</span></kbd><span><span class="koboSpan" id="kobo.36.1"> </span></span><span class="koboSpan" id="kobo.37.1">statement must contain, as an argument, the rank of the sender process:</span></p>
<pre><span class="koboSpan" id="kobo.38.1">if rank==4: 
    data=comm.recv(source=0) </span></pre>
<p><span class="koboSpan" id="kobo.39.1">For the other sender and receiver processes (the process of </span><kbd><span class="koboSpan" id="kobo.40.1">rank</span></kbd><span class="koboSpan" id="kobo.41.1"> equal to </span><kbd><span class="koboSpan" id="kobo.42.1">1</span></kbd><span class="koboSpan" id="kobo.43.1"> and the process of </span><kbd><span class="koboSpan" id="kobo.44.1">rank</span></kbd><span class="koboSpan" id="kobo.45.1"> equal to </span><kbd><span class="koboSpan" id="kobo.46.1">8</span></kbd><span class="koboSpan" id="kobo.47.1">, respectively), the situation is the same, the only difference being the type of data.</span></p>
<p><span class="koboSpan" id="kobo.48.1">In this case, for the sender process, we have a string that is to be sent:</span></p>
<pre><span class="koboSpan" id="kobo.49.1">if rank==1: 
    destination_process = 8 
    data= "hello" 
    comm.send(data,dest=destination_process) </span></pre>
<p><span class="koboSpan" id="kobo.50.1">For the receiver process of </span><kbd><span class="koboSpan" id="kobo.51.1">rank</span></kbd><span class="koboSpan" id="kobo.52.1"> equal to </span><kbd><span class="koboSpan" id="kobo.53.1">8</span></kbd><span class="koboSpan" id="kobo.54.1">, the rank of the sender process is pointed out:</span></p>
<pre><span class="koboSpan" id="kobo.55.1">if rank==8: 
    data1=comm.recv(source=1) </span></pre>
<p><span class="koboSpan" id="kobo.56.1">The following diagram summarizes the point-to-point communication protocol in</span><span><span class="koboSpan" id="kobo.57.1"> </span></span><kbd><span class="koboSpan" id="kobo.58.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.59.1">:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.60.1"><img src="assets/c92bb67f-1f34-4624-9dd7-9907f38c32e1.png" style="width:17.83em;height:19.00em;"/></span></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.61.1">The send/receive transmission protocol</span></div>
<div>
<p><span class="koboSpan" id="kobo.62.1">As you can see, it describes a two-step process, consisting of sending some</span><span><span class="koboSpan" id="kobo.63.1"> </span></span><strong><span class="koboSpan" id="kobo.64.1">DATA</span></strong><em><span><span class="koboSpan" id="kobo.65.1"> </span></span></em><span class="koboSpan" id="kobo.66.1">from one task (</span><em><span class="koboSpan" id="kobo.67.1">sender</span></em><span class="koboSpan" id="kobo.68.1">) and another task (</span><em><span class="koboSpan" id="kobo.69.1">receiver</span></em><span class="koboSpan" id="kobo.70.1">) </span><span><span class="koboSpan" id="kobo.71.1">receiving this data</span></span><span class="koboSpan" id="kobo.72.1">. </span><span class="koboSpan" id="kobo.72.2">The sending task must specify the data to be sent and its destination (the</span><span><span class="koboSpan" id="kobo.73.1"> </span></span><em><span class="koboSpan" id="kobo.74.1">receiver</span><span><span class="koboSpan" id="kobo.75.1"> </span></span></em><span class="koboSpan" id="kobo.76.1">process), while the receiving task has to specify the</span><span><span class="koboSpan" id="kobo.77.1"> </span></span><span class="koboSpan" id="kobo.78.1">source</span><span><span class="koboSpan" id="kobo.79.1"> </span></span><span class="koboSpan" id="kobo.80.1">of the message to be received.</span></p>
</div>
<p><span class="koboSpan" id="kobo.81.1">To run the script, we shall use </span><kbd><span class="koboSpan" id="kobo.82.1">9</span></kbd><span class="koboSpan" id="kobo.83.1"> processes:</span></p>
<pre><strong><span class="koboSpan" id="kobo.84.1">C:&gt;mpiexec -n 9 python pointToPointCommunication.py</span></strong> </pre>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.85.1">This is the output that you'll get after you run the script:</span></p>
<pre><strong><span class="koboSpan" id="kobo.86.1">my rank is : 7</span></strong><br/><strong><span class="koboSpan" id="kobo.87.1">my rank is : 5</span></strong><br/><strong><span class="koboSpan" id="kobo.88.1">my rank is : 2</span></strong><br/><strong><span class="koboSpan" id="kobo.89.1">my rank is : 6</span></strong><br/><strong><span class="koboSpan" id="kobo.90.1">my rank is : 3</span></strong><br/><strong><span class="koboSpan" id="kobo.91.1">my rank is : 1</span></strong><br/><strong><span class="koboSpan" id="kobo.92.1">sending data hello :to process 8</span></strong><br/><strong><span class="koboSpan" id="kobo.93.1">my rank is : 0</span></strong><br/><strong><span class="koboSpan" id="kobo.94.1">sending data 10000000 to process 4</span></strong><br/><strong><span class="koboSpan" id="kobo.95.1">my rank is : 4</span></strong><br/><strong><span class="koboSpan" id="kobo.96.1">data received is = 10000000</span></strong><br/><strong><span class="koboSpan" id="kobo.97.1">my rank is : 8</span></strong><br/><strong><span class="koboSpan" id="kobo.98.1">data1 received is = hello</span></strong> </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">There's more...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The </span><kbd><span class="koboSpan" id="kobo.3.1">comm.send()</span></kbd><span class="koboSpan" id="kobo.4.1"> and </span><kbd><span class="koboSpan" id="kobo.5.1">comm.recv()</span></kbd><span class="koboSpan" id="kobo.6.1"> </span><span><span class="koboSpan" id="kobo.7.1">functions </span></span><span class="koboSpan" id="kobo.8.1">are blocking functions, which means that they block the caller until the buffered data involved can be used</span><span><span class="koboSpan" id="kobo.9.1"> safely</span></span><span class="koboSpan" id="kobo.10.1">. </span><span class="koboSpan" id="kobo.10.2">Also, in MPI, there are two management methods of sending and receiving messages:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.11.1">Buffered mode</span></strong><span class="koboSpan" id="kobo.12.1">: The flow control returns to the program as soon as the data to be sent has been copied to a buffer. </span><span class="koboSpan" id="kobo.12.2">This does not mean that the message is sent or received.</span></li>
<li><strong><span class="koboSpan" id="kobo.13.1">Synchronous mode</span></strong><span class="koboSpan" id="kobo.14.1">: The function only gets terminated when the corresponding </span><kbd><span class="koboSpan" id="kobo.15.1">receive</span></kbd><span class="koboSpan" id="kobo.16.1"> function begins receiving the message.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">See also</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">An interesting tutorial on this topic can be found at </span><a href="https://github.com/antolonappan/MPI_tutorial"><span class="koboSpan" id="kobo.3.1">https://github.com/antolonappan/MPI_tutorial</span></a><span class="koboSpan" id="kobo.4.1">.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Avoiding deadlock problems</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">A common problem we face is deadlock. </span><span class="koboSpan" id="kobo.2.2">This is a situation where two (or more) processes block each other and wait for the other to perform a certain action that serves another and vice versa. The </span><kbd><span class="koboSpan" id="kobo.3.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.4.1"> module doesn't provide any specific functionality to resolve the deadlock problem, but there are some measures that the developer must follow in order to avoid the problem of deadlock.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How to do it...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Let's first analyze the following Python code, which will introduce a typical deadlock problem. </span><span class="koboSpan" id="kobo.2.2">We have two processes—</span><kbd><span class="koboSpan" id="kobo.3.1">rank</span></kbd><span class="koboSpan" id="kobo.4.1"> equal to </span><kbd><span class="koboSpan" id="kobo.5.1">1</span></kbd><span class="koboSpan" id="kobo.6.1"> and </span><kbd><span class="koboSpan" id="kobo.7.1">rank</span></kbd><span class="koboSpan" id="kobo.8.1"> equal to </span><kbd><span class="koboSpan" id="kobo.9.1">5</span></kbd><span class="koboSpan" id="kobo.10.1">—that communicate with each other and both have the data sender and data receiver functionalities:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.11.1">Import the </span><kbd><span class="koboSpan" id="kobo.12.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.13.1"> library:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.14.1">from mpi4py import MPI </span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.15.1">Define the communicator as </span><kbd><span class="koboSpan" id="kobo.16.1">comm</span></kbd><span class="koboSpan" id="kobo.17.1"> and the </span><kbd><span class="koboSpan" id="kobo.18.1">rank</span></kbd><span class="koboSpan" id="kobo.19.1"> parameter:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.20.1">comm=MPI.COMM_WORLD 
rank = comm.rank 
print("my rank is %i" % (rank)) </span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.21.1">The process of </span><kbd><span class="koboSpan" id="kobo.22.1">rank</span></kbd><span class="koboSpan" id="kobo.23.1"> equal to </span><kbd><span class="koboSpan" id="kobo.24.1">1</span></kbd><span class="koboSpan" id="kobo.25.1"> sends and receives data from the process </span><span><span class="koboSpan" id="kobo.26.1">of</span></span> <kbd><span class="koboSpan" id="kobo.27.1">rank</span></kbd><span class="koboSpan" id="kobo.28.1"> equal to </span><kbd><span class="koboSpan" id="kobo.29.1">5</span></kbd><span class="koboSpan" id="kobo.30.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.31.1">if rank==1: 
    data_send= "a" 
    destination_process = 5 
    source_process = 5 
    data_received=comm.recv(source=source_process) 
    comm.send(data_send,dest=destination_process) 
    print ("sending data %s " %data_send + \ 
           "to process %d" %destination_process) 
    print ("data received is = %s" %data_received) </span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.32.1">In the same way, here, we define the process of </span><kbd><span class="koboSpan" id="kobo.33.1">rank</span></kbd><span class="koboSpan" id="kobo.34.1"> equal to </span><kbd><span class="koboSpan" id="kobo.35.1">5</span></kbd><span class="koboSpan" id="kobo.36.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.37.1">if rank==5: 
    data_send= "b" </span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.38.1">The destination and sender processes are equal to </span><kbd><span class="koboSpan" id="kobo.39.1">1</span></kbd><span class="koboSpan" id="kobo.40.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.41.1">    destination_process = 1 
    source_process = 1  
    comm.send(data_send,dest=destination_process) 
    data_received=comm.recv(source=source_process) 
    print ("sending data %s :" %data_send + \ 
           "to process %d" %destination_process) 
    print ("data received is = %s" %data_received) </span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How it works...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">If we try to run this program (it makes sense to execute it with only two processes), then we note that none of the two processes can proceed:</span></p>
<pre><strong><span class="koboSpan" id="kobo.3.1">C:\&gt;mpiexec -n 9 python deadLockProblems.py</span></strong>
<br/><strong><span class="koboSpan" id="kobo.4.1">my rank is : 8</span><br/><span class="koboSpan" id="kobo.5.1">my rank is : 6</span><br/><span class="koboSpan" id="kobo.6.1">my rank is : 7</span><br/><span class="koboSpan" id="kobo.7.1">my rank is : 2</span><br/><span class="koboSpan" id="kobo.8.1">my rank is : 4</span><br/><span class="koboSpan" id="kobo.9.1">my rank is : 3</span><br/><span class="koboSpan" id="kobo.10.1">my rank is : 0</span><br/><span class="koboSpan" id="kobo.11.1">my rank is : 1</span><br/><span class="koboSpan" id="kobo.12.1">sending data a to process 5</span><br/><span class="koboSpan" id="kobo.13.1">data received is = b</span><br/><span class="koboSpan" id="kobo.14.1">my rank is : 5</span><br/><span class="koboSpan" id="kobo.15.1">sending data b :to process 1</span><br/><span class="koboSpan" id="kobo.16.1">data received is = a</span></strong></pre>
<p><span class="koboSpan" id="kobo.17.1">Both the processes prepare to receive a message from the other and get stuck there. </span><span class="koboSpan" id="kobo.17.2">This happens because of the </span><kbd><span class="koboSpan" id="kobo.18.1">comm.recv()</span></kbd> <span><span class="koboSpan" id="kobo.19.1">MPI function and</span></span><span class="koboSpan" id="kobo.20.1"> the </span><kbd><span class="koboSpan" id="kobo.21.1">comm.send()</span></kbd><span class="koboSpan" id="kobo.22.1"> MPI blocking them. </span><span class="koboSpan" id="kobo.22.2">This means that the calling process awaits their completion. </span><span class="koboSpan" id="kobo.22.3">As for the </span><kbd><span class="koboSpan" id="kobo.23.1">comm.send()</span></kbd><span class="koboSpan" id="kobo.24.1"> MPI, the completion occurs when the data has been sent and may be overwritten without modifying the message. </span></p>
<p><span class="koboSpan" id="kobo.25.1">The completion of the </span><kbd><span class="koboSpan" id="kobo.26.1">comm.recv()</span></kbd><span class="koboSpan" id="kobo.27.1"> MPI instead occurs when the data has been received and can be used. </span><span class="koboSpan" id="kobo.27.2">To solve this problem, the first idea is to invert the </span><kbd><span class="koboSpan" id="kobo.28.1">comm.recv()</span></kbd><span class="koboSpan" id="kobo.29.1"> MPI with the </span><kbd><span class="koboSpan" id="kobo.30.1">comm.send()</span></kbd><span class="koboSpan" id="kobo.31.1"> MPI, as follows:</span></p>
<pre><span class="koboSpan" id="kobo.32.1">if rank==1: 
    data_send= "a" 
    destination_process = 5 
    source_process = 5 
    comm.send(data_send,dest=destination_process) 
    data_received=comm.recv(source=source_process) </span><br/><br/><span class="koboSpan" id="kobo.33.1">    print ("sending data %s " %data_send + \</span><br/><span class="koboSpan" id="kobo.34.1">           "to process %d" %destination_process)</span><br/><span class="koboSpan" id="kobo.35.1">    print ("data received is = %s" %data_received)
     
if rank==5: 
    data_send= "b" 
    destination_process = 1 
    source_process = 1 
    data_received=comm.recv(source=source_process) 
    comm.send(data_send,dest=destination_process) </span><br/><br/><span class="koboSpan" id="kobo.36.1">    print ("sending data %s :" %data_send + \</span><br/><span class="koboSpan" id="kobo.37.1">           "to process %d" %destination_process)</span><br/><span class="koboSpan" id="kobo.38.1">    print ("data received is = %s" %data_received)</span></pre>
<p><span class="koboSpan" id="kobo.39.1">This solution, even if correct, does not guarantee that we will avoid deadlock. </span><span class="koboSpan" id="kobo.39.2">In fact, communication is performed through a buffer with the instruction of </span><kbd><span class="koboSpan" id="kobo.40.1">comm.send()</span></kbd><span class="koboSpan" id="kobo.41.1">.</span></p>
<p><span class="koboSpan" id="kobo.42.1">MPI copies the data to be sent. </span><span class="koboSpan" id="kobo.42.2">This mode works without problems, but only if the buffer is able to keep them all. </span><span class="koboSpan" id="kobo.42.3">If this does not happen, then there is a deadlock: the sender cannot finish sending the data because the buffer is busy, and the receiver cannot receive data because it is blocked by the </span><kbd><span class="koboSpan" id="kobo.43.1">comm.send()</span></kbd><span class="koboSpan" id="kobo.44.1"> </span><span><span class="koboSpan" id="kobo.45.1">MPI call,</span></span><span class="koboSpan" id="kobo.46.1"> which has not yet completed.</span></p>
<p><span class="koboSpan" id="kobo.47.1">At this point, the solution that allows us to avoid deadlocks is used to swap the sending and receiving functions so as to make them asymmetrical:</span></p>
<pre><span class="koboSpan" id="kobo.48.1">if rank==1: 
    data_send= "a" 
    destination_process = 5 
    source_process = 5 
    comm.send(data_send,dest=destination_process) 
    data_received=comm.recv(source=source_process) 
              
if rank==5: 
    data_send= "b" 
    destination_process = 1 
    source_process = 1 
    comm.send(data_send,dest=destination_process) 
    data_received=comm.recv(source=source_process) </span></pre>
<p><span class="koboSpan" id="kobo.49.1">Finally, we get the correct output:</span></p>
<pre><strong><span class="koboSpan" id="kobo.50.1">C:\&gt;mpiexec -n 9 python deadLockProblems.py 
 
my rank is : 4</span><br/><span class="koboSpan" id="kobo.51.1">my rank is : 0</span><br/><span class="koboSpan" id="kobo.52.1">my rank is : 3</span><br/><span class="koboSpan" id="kobo.53.1">my rank is : 8</span><br/><span class="koboSpan" id="kobo.54.1">my rank is : 6</span><br/><span class="koboSpan" id="kobo.55.1">my rank is : 7</span><br/><span class="koboSpan" id="kobo.56.1">my rank is : 2</span><br/><span class="koboSpan" id="kobo.57.1">my rank is : 1</span><br/><span class="koboSpan" id="kobo.58.1">sending data a to process 5</span><br/><span class="koboSpan" id="kobo.59.1">data received is = b</span><br/><span class="koboSpan" id="kobo.60.1">my rank is : 5</span><br/><span class="koboSpan" id="kobo.61.1">sending data b :to process 1</span><br/><span class="koboSpan" id="kobo.62.1">data received is = a</span><br/></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">There's more...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The solution proposed to the deadlock is not the only solution.</span></p>
<p><span class="koboSpan" id="kobo.3.1">There is, for example, a function that unifies the single call that sends a message to a given process and receives another message that comes from another process. </span><span class="koboSpan" id="kobo.3.2">This function is called </span><kbd><span class="koboSpan" id="kobo.4.1">Sendrecv</span></kbd><span class="koboSpan" id="kobo.5.1">:</span></p>
<pre><span class="koboSpan" id="kobo.6.1">Sendrecv(self, sendbuf, int dest=0, int sendtag=0, recvbuf=None, int source=0, int recvtag=0, Status status=None) </span></pre>
<p><span class="koboSpan" id="kobo.7.1">As you can see, the required parameters are the same as the </span><kbd><span class="koboSpan" id="kobo.8.1">comm.send()</span></kbd><span class="koboSpan" id="kobo.9.1">  and </span><kbd><span class="koboSpan" id="kobo.10.1">comm.recv()</span></kbd><span class="koboSpan" id="kobo.11.1"> MPI (in this case, also the function blocks). </span><span class="koboSpan" id="kobo.11.2">However, </span><kbd><span><span class="koboSpan" id="kobo.12.1">Sendrecv</span></span></kbd><span class="koboSpan" id="kobo.13.1"> offers the advantage of leaving the communication subsystem responsible for checking the dependencies between sending and receiving, thus avoiding the deadlock.</span></p>
<p><span class="hps"><span class="koboSpan" id="kobo.14.1">In this way, </span></span><span class="hps"><span class="koboSpan" id="kobo.15.1">the code of</span></span><span class="hps"><span class="koboSpan" id="kobo.16.1"> the previous example becomes the following</span></span><span class="koboSpan" id="kobo.17.1">:</span></p>
<pre><span class="koboSpan" id="kobo.18.1">if rank==1: 
    data_send= "a" 
    destination_process = 5 
    source_process = 5 
    data_received=comm.sendrecv(data_send,dest=\
                                destination_process,\ 
                                source =source_process) 
if rank==5: 
    data_send= "b" 
    destination_process = 1 
    source_process = 1 
    data_received=comm.sendrecv(data_send,dest=\ 
                                destination_process,\ 
                                source=source_process) </span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">See also</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">An interesting analysis of how parallel programming is difficult due to deadlock management can be found at </span><a href="https://codewithoutrules.com/2017/08/16/concurrency-python/"><span class="koboSpan" id="kobo.3.1">https://codewithoutrules.com/2017/08/16/concurrency-python/</span></a><span class="koboSpan" id="kobo.4.1">.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Collective communication using a broadcast</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">During the development of parallel code, we often find ourselves in a situation where we must share, between multiple processes, the value of a certain variable at runtime or certain operations on variables that each process provides (presumably with different values).</span></p>
<p><span class="koboSpan" id="kobo.3.1">To resolve these types of situations, communication trees are used (for example, process 0 sends data to the processes 1 and 2, which will, </span><span class="hps"><span class="koboSpan" id="kobo.4.1">respectively,</span></span><span class="koboSpan" id="kobo.5.1"> take care of sending them to processes 3, 4, 5, 6, and so on).</span></p>
<p><span class="koboSpan" id="kobo.6.1">Instead, MPI libraries provide functions that are ideal for the exchange of information or the use of multiple processes that are clearly optimized for the machine in which they are performed:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.7.1"><img src="assets/48fa28e4-27d9-4ee6-981e-3c72d22b1c27.png" style="width:19.75em;height:12.50em;"/></span></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.8.1">Broadcasting data from process 0 to processes 1, 2, 3, and 4</span></div>
<p><span class="koboSpan" id="kobo.9.1">A communication method that involves all the processes that belong to a communicator is called a collective communication. </span><span class="koboSpan" id="kobo.9.2">Consequently, collective communication generally involves more than two processes. </span><span class="koboSpan" id="kobo.9.3">However, instead of this, we will call the collective communication broadcast, wherein a single process sends the same data to any other process.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Getting ready</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The </span><kbd><span class="koboSpan" id="kobo.3.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.4.1"> broadcast functionalities are offered by the following method:</span></p>
<pre><span class="koboSpan" id="kobo.5.1">buf = comm.bcast(data_to_share, rank_of_root_process) </span></pre>
<p><span class="koboSpan" id="kobo.6.1">This function sends the information contained in the message process root to every other process that belongs to the </span><kbd><span class="koboSpan" id="kobo.7.1">comm</span></kbd><span class="koboSpan" id="kobo.8.1"> communicator.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How to do it...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Let's now see an example in which we've used the </span><kbd><span class="koboSpan" id="kobo.3.1">broadcast</span></kbd><span class="koboSpan" id="kobo.4.1"> function. </span><span class="koboSpan" id="kobo.4.2">We have a root process of </span><kbd><span class="koboSpan" id="kobo.5.1">rank</span></kbd><span class="koboSpan" id="kobo.6.1"> equal to </span><kbd><span class="koboSpan" id="kobo.7.1">0</span></kbd><span class="koboSpan" id="kobo.8.1"> that shares its own data, </span><kbd><span class="koboSpan" id="kobo.9.1">variable_to_share</span></kbd><span class="koboSpan" id="kobo.10.1">, with the other processes defined in the communicator group:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.11.1">Let's import the </span><kbd><span class="koboSpan" id="kobo.12.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.13.1"> library:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.14.1">from mpi4py import MPI </span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.15.1">Now, let's define the communicator and the </span><kbd><span class="koboSpan" id="kobo.16.1">rank</span></kbd><span class="koboSpan" id="kobo.17.1"> parameter:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.18.1">comm = MPI.COMM_WORLD 
rank = comm.Get_rank() </span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.19.1">As far as the process of </span><kbd><span class="koboSpan" id="kobo.20.1">rank</span></kbd> <span><span class="koboSpan" id="kobo.21.1">equal to </span><kbd><span class="koboSpan" id="kobo.22.1">0</span></kbd><span class="koboSpan" id="kobo.23.1"> is concerned</span></span><span class="koboSpan" id="kobo.24.1">, we define the variable to be shared among the other processes:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.25.1">if rank == 0: 
    variable_to_share = 100      
else: 
    variable_to_share = None </span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.26.1">Finally, we define a broadcast, having the </span><kbd><span class="koboSpan" id="kobo.27.1">rank</span></kbd><span class="koboSpan" id="kobo.28.1"> process equal to zero as its </span><kbd><span class="koboSpan" id="kobo.29.1">root</span></kbd><span class="koboSpan" id="kobo.30.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.31.1">variable_to_share = comm.bcast(variable_to_share, root=0) 
print("process = %d" %rank + " variable shared  = %d " \   
                               %variable_to_share) </span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How it works...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The root process of </span><kbd><span class="koboSpan" id="kobo.3.1">rank</span></kbd> <span><span class="koboSpan" id="kobo.4.1">equal to </span></span><kbd><span class="koboSpan" id="kobo.5.1">0</span></kbd><span class="koboSpan" id="kobo.6.1"> instantiates a variable,</span><span><span class="koboSpan" id="kobo.7.1"> </span></span><kbd><span class="koboSpan" id="kobo.8.1">variable_to_share</span></kbd><span class="koboSpan" id="kobo.9.1">, which is equal to</span><span><span class="koboSpan" id="kobo.10.1"> </span></span><kbd><span class="koboSpan" id="kobo.11.1">100</span></kbd><span class="koboSpan" id="kobo.12.1">. </span><span class="koboSpan" id="kobo.12.2">This variable will be shared with the other processes of the communication group:</span></p>
<pre><span class="koboSpan" id="kobo.13.1">if rank == 0: 
   variable_to_share = 100  </span></pre>
<p><span class="koboSpan" id="kobo.14.1">To perform this, we also introduce the broadcast communication statement:</span></p>
<pre><span class="koboSpan" id="kobo.15.1">variable_to_share = comm.bcast(variable_to_share, root=0) </span></pre>
<p><span class="koboSpan" id="kobo.16.1">Here, the parameters in the function are as follows:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.17.1">The data to be shared (</span><kbd><span class="koboSpan" id="kobo.18.1">variable_to_share</span></kbd><span class="koboSpan" id="kobo.19.1">).</span></li>
<li><span class="koboSpan" id="kobo.20.1">The root </span><span><span class="koboSpan" id="kobo.21.1">process, </span></span><span class="koboSpan" id="kobo.22.1">that is, the process of rank equal to 0 (</span><kbd><span class="koboSpan" id="kobo.23.1">root=0</span></kbd><span class="koboSpan" id="kobo.24.1">).</span></li>
</ul>
<p><span class="koboSpan" id="kobo.25.1">Running the code, we have a communication group of 10 processes, and</span><span><span class="koboSpan" id="kobo.26.1"> </span></span><kbd><span class="koboSpan" id="kobo.27.1">variable_to_share</span></kbd><span><span class="koboSpan" id="kobo.28.1"> </span></span><span class="koboSpan" id="kobo.29.1">is shared between the other processes in the group. </span><span class="koboSpan" id="kobo.29.2">Finally, the</span><span><span class="koboSpan" id="kobo.30.1"> </span></span><kbd><span class="koboSpan" id="kobo.31.1">print</span></kbd><span><span class="koboSpan" id="kobo.32.1"> </span></span><span class="koboSpan" id="kobo.33.1">statement visualizes the rank of the running process and the value of its variable:</span></p>
<pre><span class="koboSpan" id="kobo.34.1">print("process = %d" %rank + " variable shared  = %d " \   
                     %variable_to_share) </span></pre>
<p><span class="koboSpan" id="kobo.35.1">After setting </span><kbd><span class="koboSpan" id="kobo.36.1">10</span></kbd><span class="koboSpan" id="kobo.37.1"> processes, the output obtained is as follows:</span></p>
<pre><strong><span class="koboSpan" id="kobo.38.1">C:\&gt;mpiexec -n 10 python broadcast.py </span><br/><span class="koboSpan" id="kobo.39.1">process = 0 </span><br/><span class="koboSpan" id="kobo.40.1">variable shared = 100 </span><br/><span class="koboSpan" id="kobo.41.1">process = 8 </span><br/><span class="koboSpan" id="kobo.42.1">variable shared = 100 </span><br/><span class="koboSpan" id="kobo.43.1">process = 2 variable </span><br/><span class="koboSpan" id="kobo.44.1">shared = 100 </span><br/><span class="koboSpan" id="kobo.45.1">process = 3 </span><br/><span class="koboSpan" id="kobo.46.1">variable shared = 100 </span><br/><span class="koboSpan" id="kobo.47.1">process = 4 </span><br/><span class="koboSpan" id="kobo.48.1">variable shared = 100 </span><br/><span class="koboSpan" id="kobo.49.1">process = 5 </span><br/><span class="koboSpan" id="kobo.50.1">variable shared = 100 </span><br/><span class="koboSpan" id="kobo.51.1">process = 9 </span><br/><span class="koboSpan" id="kobo.52.1">variable shared = 100 </span><br/><span class="koboSpan" id="kobo.53.1">process = 6 </span><br/><span class="koboSpan" id="kobo.54.1">variable shared = 100 </span><br/><span class="koboSpan" id="kobo.55.1">process = 1 </span><br/><span class="koboSpan" id="kobo.56.1">variable shared = 100 </span><br/><span class="koboSpan" id="kobo.57.1">process = 7 </span><br/><span class="koboSpan" id="kobo.58.1">variable shared = 100</span></strong> </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">There's more...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Collective communication allows simultaneous data transmission between multiple processes in a group. </span><span class="koboSpan" id="kobo.2.2">The </span><kbd><span class="koboSpan" id="kobo.3.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.4.1"> library provides collective communications, but only in the blocking version (that is, it blocks the caller method until the buffered data involved can safely be used).</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.5.1">The most commonly used </span><span><span class="koboSpan" id="kobo.6.1">collective communication</span></span><span class="koboSpan" id="kobo.7.1"> operations are as follows:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.8.1">Barrier synchronization across the group's processes</span></li>
<li><span class="koboSpan" id="kobo.9.1">Communication functions:
</span><ul>
<li><span class="koboSpan" id="kobo.10.1">Broadcasting data from one process to all processes in the group</span></li>
<li><span class="koboSpan" id="kobo.11.1">Gathering data from all processes to one process</span></li>
<li><span class="koboSpan" id="kobo.12.1">Scattering data from one process to all processes</span></li>
</ul>
</li>
<li><span class="koboSpan" id="kobo.13.1">Reduction operations</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">See also</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Refer to this link (</span><a href="https://nyu-cds.github.io/python-mpi/"><span class="koboSpan" id="kobo.3.1">https://nyu-cds.github.io/python-mpi/</span></a><span class="koboSpan" id="kobo.4.1">) to find a complete introduction to Python and MPI.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Collective communication using the scatter function</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The scatter functionality is very similar to a scatter broadcast, but with one major difference: while </span><kbd><span class="koboSpan" id="kobo.3.1">comm.bcast</span></kbd><span class="koboSpan" id="kobo.4.1"> sends the same data to all listening processes, </span><kbd><span class="koboSpan" id="kobo.5.1">comm.scatter</span></kbd><span class="koboSpan" id="kobo.6.1"> can send chunks of data in an array to different processes.</span></p>
<p><span class="koboSpan" id="kobo.7.1">The following diagram illustrates the scatter functionality:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.8.1"><img src="assets/7a6e8b54-06df-43d5-ad30-aa1221ae3f85.png" style="width:27.33em;height:17.17em;"/></span></p>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.9.1">Scattering data from process 0 to processes 1, 2, 3, and 4</span></div>
<p><span class="koboSpan" id="kobo.10.1">The </span><strong><kbd><span class="koboSpan" id="kobo.11.1">comm.scatter</span></kbd></strong><span class="koboSpan" id="kobo.12.1"> function takes the elements of the array and distributes them to the processes according to their rank, for which the first element will be sent to process 0, the second element to process 1, and so on. </span><span class="koboSpan" id="kobo.12.2">The function implemented in </span><strong><kbd><span class="koboSpan" id="kobo.13.1">mpi4py</span></kbd></strong><span class="koboSpan" id="kobo.14.1"> is as follows:</span></p>
<pre><span class="koboSpan" id="kobo.15.1">recvbuf  = comm.scatter(sendbuf, rank_of_root_process) </span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How to do it...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the following example, we'll see how to distribute data to different processes using the </span><kbd><span class="koboSpan" id="kobo.3.1">scatter</span></kbd><span class="koboSpan" id="kobo.4.1"> functionality:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.5.1">Import the </span><kbd><span class="koboSpan" id="kobo.6.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.7.1"> library:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.8.1">from mpi4py import MPI </span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.9.1">Next, we define the </span><kbd><span class="koboSpan" id="kobo.10.1">comm</span></kbd><span class="koboSpan" id="kobo.11.1"> and </span><kbd><span class="koboSpan" id="kobo.12.1">rank</span></kbd><span class="koboSpan" id="kobo.13.1"> parameters in the usual way:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.14.1">comm = MPI.COMM_WORLD 
rank = comm.Get_rank() </span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.15.1">For the process of </span><kbd><span class="koboSpan" id="kobo.16.1">rank</span></kbd><span class="koboSpan" id="kobo.17.1"> equal to </span><kbd><span class="koboSpan" id="kobo.18.1">0</span></kbd><span class="koboSpan" id="kobo.19.1">, the following array will be scattered:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.20.1">if rank == 0: 
    array_to_share = [1, 2, 3, 4 ,5 ,6 ,7, 8 ,9 ,10]  </span><br/><span class="koboSpan" id="kobo.21.1">else: </span><br/><span class="koboSpan" id="kobo.22.1">    array_to_share = None </span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.23.1">Then, </span><kbd><span class="koboSpan" id="kobo.24.1">recvbuf</span></kbd><span class="koboSpan" id="kobo.25.1"> is set. </span><span class="koboSpan" id="kobo.25.2">The </span><kbd><span class="koboSpan" id="kobo.26.1">root</span></kbd><span class="koboSpan" id="kobo.27.1"> process is the process of </span><kbd><span class="koboSpan" id="kobo.28.1">rank</span></kbd> <span><span class="koboSpan" id="kobo.29.1">equal to </span></span><kbd><span class="koboSpan" id="kobo.30.1">0</span></kbd><span class="koboSpan" id="kobo.31.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.32.1">recvbuf = comm.scatter(array_to_share, root=0) 
print("process = %d" %rank + " recvbuf = %d " %recvbuf) </span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How it works...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The process of </span><kbd><span class="koboSpan" id="kobo.3.1">rank</span></kbd> <span><span class="koboSpan" id="kobo.4.1">equal to </span></span><kbd><span class="koboSpan" id="kobo.5.1">0</span></kbd><span class="koboSpan" id="kobo.6.1"> distributes the</span><span><span class="koboSpan" id="kobo.7.1"> </span></span><kbd><span class="koboSpan" id="kobo.8.1">array_to_share</span></kbd><span><span class="koboSpan" id="kobo.9.1"> </span></span><span class="koboSpan" id="kobo.10.1">data structure to other processes:</span></p>
<pre><span class="koboSpan" id="kobo.11.1">array_to_share = [1, 2, 3, 4 ,5 ,6 ,7, 8 ,9 ,10] </span></pre>
<p><span class="koboSpan" id="kobo.12.1">The</span><span><span class="koboSpan" id="kobo.13.1"> </span></span><kbd><span class="koboSpan" id="kobo.14.1">recvbuf</span></kbd><span><span class="koboSpan" id="kobo.15.1"> </span></span><span class="koboSpan" id="kobo.16.1">parameter indicates the value of the</span><span><span class="koboSpan" id="kobo.17.1"> </span></span><em><span class="koboSpan" id="kobo.18.1">i</span><sup><span class="koboSpan" id="kobo.19.1">th</span></sup></em><span class="koboSpan" id="kobo.20.1"> variable that will be sent to the process through the</span><span><span class="koboSpan" id="kobo.21.1"> </span></span><kbd><span class="koboSpan" id="kobo.22.1">comm.scatter</span></kbd><span><span class="koboSpan" id="kobo.23.1"> </span></span><span class="koboSpan" id="kobo.24.1">statement:</span></p>
<pre><span class="koboSpan" id="kobo.25.1">recvbuf = comm.scatter(array_to_share, root=0)</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p><span class="koboSpan" id="kobo.26.1">The output is as follows:</span></p>
<pre><strong><span class="koboSpan" id="kobo.27.1">C:\&gt;mpiexec -n 10 python scatter.py 
process = 0 variable shared  = 1 
process = 4 variable shared  = 5 
process = 6 variable shared  = 7 
process = 2 variable shared  = 3 
process = 5 variable shared  = 6 
process = 3 variable shared  = 4 
process = 7 variable shared  = 8 
process = 1 variable shared  = 2 
process = 8 variable shared  = 9 
process = 9 variable shared  = 10</span></strong> </pre>
<p><span><span class="koboSpan" id="kobo.28.1">We also remark that one of the restrictions to</span></span> <kbd><span class="koboSpan" id="kobo.29.1">comm.scatter</span></kbd><span><span class="koboSpan" id="kobo.30.1"> is that you can scatter as many elements as the processors you specify in the execution statement. </span><span class="koboSpan" id="kobo.30.2">In fact, if you attempt to scatter more elements than the processors specified (three, in this example), then you will get an error similar to the following:</span></span></p>
<pre><strong><span class="koboSpan" id="kobo.31.1">C:\&gt; mpiexec -n 3 python scatter.py 
Traceback (most recent call last): 
  File "scatter.py", line 13, in &lt;module&gt; 
    recvbuf = comm.scatter(array_to_share, root=0) 
  File "Comm.pyx", line 874, in mpi4py.MPI.Comm.scatter </span><br/><span class="koboSpan" id="kobo.32.1">  (c:\users\utente\appdata\local\temp\pip-build-h14iaj\mpi4py\</span><br/><span class="koboSpan" id="kobo.33.1">  src\mpi4py.MPI.c:73400) 
  File "pickled.pxi", line 658, in mpi4py.MPI.PyMPI_scatter </span><br/><span class="koboSpan" id="kobo.34.1">  (c:\users\utente\appdata\local\temp\pip-build-h14iaj\mpi4py\src\</span><br/><span class="koboSpan" id="kobo.35.1">  mpi4py.MPI.c:34035) 
  File "pickled.pxi", line 129, in mpi4py.MPI._p_Pickle.dumpv </span><br/><span class="koboSpan" id="kobo.36.1">  (c:\users\utente\appdata\local\temp\pip-build-h14iaj\mpi4py</span><br/><span class="koboSpan" id="kobo.37.1">  \src\mpi4py.MPI.c:28325) </span><br/><span class="koboSpan" id="kobo.38.1">  ValueError: expecting 3 items, got 10 
  mpiexec aborting job... 
 
</span><span class="koboSpan" id="kobo.38.2">job aborted: 
rank: node: exit code[: error message] 
0: Utente-PC: 123: mpiexec aborting job 
1: Utente-PC: 123 
2: Utente-PC: 123</span></strong> </pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">There's more...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The </span><kbd><span class="koboSpan" id="kobo.3.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.4.1"> library provides two other</span><span><span class="koboSpan" id="kobo.5.1"> </span></span><span class="koboSpan" id="kobo.6.1">functions that are used to scatter data:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.7.1">comm.scatter(sendbuf, recvbuf, root=0)</span></kbd><span class="koboSpan" id="kobo.8.1">: This function sends data from one process to all other processes in a communicator.</span></li>
<li><kbd><span class="koboSpan" id="kobo.9.1">comm.scatterv(sendbuf, recvbuf, root=0)</span></kbd><span class="koboSpan" id="kobo.10.1">: This function scatters data from one process to all other processes in a given group that provide a different amount of data and displacements at the sending side.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.11.1">The </span><kbd><span class="koboSpan" id="kobo.12.1">sendbuf</span></kbd><span class="koboSpan" id="kobo.13.1"> and </span><kbd><span class="koboSpan" id="kobo.14.1">recvbuf</span></kbd><span class="koboSpan" id="kobo.15.1"> arguments must be given in terms of a list (as in the </span><kbd><span class="koboSpan" id="kobo.16.1">comm.send</span></kbd><span class="koboSpan" id="kobo.17.1"> </span><span><span class="koboSpan" id="kobo.18.1">point-to-point function):</span></span></p>
<pre><span class="koboSpan" id="kobo.19.1">buf = [data, data_size, data_type] </span></pre>
<p><span class="koboSpan" id="kobo.20.1">Here, </span><kbd><span class="koboSpan" id="kobo.21.1">data</span></kbd><span class="koboSpan" id="kobo.22.1"> must be a buffer-like object of the </span><kbd><span class="koboSpan" id="kobo.23.1">data_size</span></kbd> <span><span class="koboSpan" id="kobo.24.1">size </span></span><span class="koboSpan" id="kobo.25.1">and of the </span><kbd><span class="koboSpan" id="kobo.26.1">data_type</span></kbd><span class="koboSpan" id="kobo.27.1"> </span><span><span class="koboSpan" id="kobo.28.1">type.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">See also</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">An interesting tutorial on MPI broadcasting is presented at </span><a href="https://pythonprogramming.net/mpi-broadcast-tutorial-mpi4py/"><span class="koboSpan" id="kobo.3.1">https://pythonprogramming.net/mpi-broadcast-tutorial-mpi4py/</span></a><span class="koboSpan" id="kobo.4.1">.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Collective communication using the gather function</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The </span><kbd><span class="koboSpan" id="kobo.3.1">gather</span></kbd><span class="koboSpan" id="kobo.4.1"> function performs the inverse of the </span><kbd><span class="koboSpan" id="kobo.5.1">scatter</span></kbd><span class="koboSpan" id="kobo.6.1"> function. </span><span class="koboSpan" id="kobo.6.2">In this case, all processes send data to a root process that collects the data received.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Getting ready</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The </span><kbd><span class="koboSpan" id="kobo.3.1">gather</span></kbd><span class="koboSpan" id="kobo.4.1"> function, which is implemented in </span><kbd><span class="koboSpan" id="kobo.5.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.6.1">, is as follows:</span></p>
<pre><span class="koboSpan" id="kobo.7.1">recvbuf  = comm.gather(sendbuf, rank_of_root_process) </span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.8.1">Here, </span><kbd><span class="koboSpan" id="kobo.9.1">sendbuf</span></kbd><span class="koboSpan" id="kobo.10.1"> is the data that is sent, and </span><kbd><span class="koboSpan" id="kobo.11.1">rank_of_root_process</span></kbd><span class="koboSpan" id="kobo.12.1"> represents the processing of the receiver of all the data:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.13.1"><img src="assets/3fc357c4-5541-4c15-94f4-2dd7bee64b9b.png" style="width:27.17em;height:15.58em;"/></span></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.14.1">Gathering data from processes 1, 2, 3, and 4</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How to do it...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the following example, we'll represent the condition shown in the preceding diagram, in which each process builds its own data, which is to be sent to the root processes that are identified with the </span><kbd><span class="koboSpan" id="kobo.3.1">rank</span></kbd><span class="koboSpan" id="kobo.4.1"> zero:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.5.1">Type the necessary import:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.6.1">from mpi4py import MPI </span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.7.1">Next, we define the following three parameters. </span><span class="koboSpan" id="kobo.7.2">The </span><kbd><span class="koboSpan" id="kobo.8.1">comm</span></kbd><span class="koboSpan" id="kobo.9.1"> parameter is the communicator, </span><kbd><span class="koboSpan" id="kobo.10.1">rank</span></kbd><span class="koboSpan" id="kobo.11.1"> provides the rank of the process, and </span><kbd><span class="koboSpan" id="kobo.12.1">size</span></kbd><span class="koboSpan" id="kobo.13.1"> is the total number of processes:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.14.1">comm = MPI.COMM_WORLD 
size = comm.Get_size() 
rank = comm.Get_rank() </span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.15.1">Here, we define the data to be gathered from the process of </span><kbd><span class="koboSpan" id="kobo.16.1">rank</span></kbd><span class="koboSpan" id="kobo.17.1"> zero:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.18.1">data = (rank+1)**2 </span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="4">
<li><span class="koboSpan" id="kobo.19.1">Finally, the gathering is provided through the </span><kbd><span class="koboSpan" id="kobo.20.1">comm.gather</span></kbd><span class="koboSpan" id="kobo.21.1"> function. </span><span class="koboSpan" id="kobo.21.2">Also, note that the root process (the process that will gather the data from the other ones) is the zero rank process:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.22.1">data = comm.gather(data, root=0) </span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.23.1">For the </span><kbd><span class="koboSpan" id="kobo.24.1">rank</span></kbd><span class="koboSpan" id="kobo.25.1"> equal to the </span><kbd><span class="koboSpan" id="kobo.26.1">0</span></kbd><span><span class="koboSpan" id="kobo.27.1"> process</span></span><span class="koboSpan" id="kobo.28.1">, the data gathered and the sending process are printed out:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.29.1">if rank == 0: 
    print ("rank = %s " %rank +\ 
          "...receiving data to other process") 
   for i in range(1,size): 
       value = data[i] 
       print(" process %s receiving %s from process %s"\ 
            %(rank , value , i)) </span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How it works...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The root process of </span><kbd><span class="koboSpan" id="kobo.3.1">0</span></kbd><span class="koboSpan" id="kobo.4.1"> receives data from the other four processes, as represented in the previous diagram.</span></p>
<p><span class="koboSpan" id="kobo.5.1">We set </span><em><span class="koboSpan" id="kobo.6.1">n (= 5)</span></em><span class="koboSpan" id="kobo.7.1"> processes sending their data:</span></p>
<pre><span class="koboSpan" id="kobo.8.1">    data = (rank+1)**2  </span></pre>
<p><span class="koboSpan" id="kobo.9.1">If the </span><kbd><span class="koboSpan" id="kobo.10.1">rank</span></kbd><span class="koboSpan" id="kobo.11.1"> of the process is </span><kbd><span class="koboSpan" id="kobo.12.1">0</span></kbd><span class="koboSpan" id="kobo.13.1">, then the data is collected in an array:</span></p>
<pre><span class="koboSpan" id="kobo.14.1">if rank == 0: 
    for i in range(1,size): 
        value = data[i] </span></pre>
<p><span class="koboSpan" id="kobo.15.1">The gathering of data is given, instead, by the following function:</span></p>
<pre><span class="koboSpan" id="kobo.16.1">data = comm.gather(data, root=0) </span></pre>
<p><span class="koboSpan" id="kobo.17.1">Finally, we run the code setting the group of processes equal to </span><kbd><span class="koboSpan" id="kobo.18.1">5</span></kbd><span class="koboSpan" id="kobo.19.1">:</span></p>
<pre><strong><span class="koboSpan" id="kobo.20.1">C:\&gt;mpiexec -n 5 python gather.py</span><br/><span class="koboSpan" id="kobo.21.1">rank = 0 ...receiving data to other process
process 0 receiving 4 from process 1
process 0 receiving 9 from process 2
process 0 receiving 16 from process 3
process 0 receiving 25 from process 4  </span></strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">There's more...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">To collect data, </span><kbd><span class="koboSpan" id="kobo.3.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.4.1"> provides the following functions:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.5.1">Gathering to one task</span><em><span class="koboSpan" id="kobo.6.1">:</span></em> <kbd><span class="koboSpan" id="kobo.7.1">comm.Gather</span></kbd><span class="koboSpan" id="kobo.8.1">, </span><kbd><span class="koboSpan" id="kobo.9.1">comm.Gatherv</span></kbd><span class="koboSpan" id="kobo.10.1">, and </span><kbd><span class="koboSpan" id="kobo.11.1">comm.gather</span></kbd></li>
<li><span class="koboSpan" id="kobo.12.1">Gathering to all tasks: </span><kbd><span class="koboSpan" id="kobo.13.1">comm.Allgather</span></kbd><span class="koboSpan" id="kobo.14.1">, </span><kbd><span class="koboSpan" id="kobo.15.1">comm.Allgatherv</span></kbd><span class="koboSpan" id="kobo.16.1">, and </span><kbd><span class="koboSpan" id="kobo.17.1">comm.allgather</span></kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">See also</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">More information on </span><kbd><span class="koboSpan" id="kobo.3.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.4.1"> can be found at </span><a href="http://www.ceci-hpc.be/assets/training/mpi4py.pdf"><span class="koboSpan" id="kobo.5.1">http://www.ceci-hpc.be/assets/training/mpi4py.pdf</span></a><span class="koboSpan" id="kobo.6.1">.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Collective communication using Alltoall</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The </span><kbd><span class="koboSpan" id="kobo.3.1">Alltoall</span></kbd><span class="koboSpan" id="kobo.4.1"> collective communication combines the </span><kbd><span class="koboSpan" id="kobo.5.1">scatter</span></kbd><span class="koboSpan" id="kobo.6.1"> and </span><kbd><span class="koboSpan" id="kobo.7.1">gather</span></kbd><span class="koboSpan" id="kobo.8.1"> functionalities.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How to do it...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the following example, we'll see an </span><kbd><span class="koboSpan" id="kobo.3.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.4.1"> implementation of </span><kbd><span class="koboSpan" id="kobo.5.1">comm.Alltoall</span></kbd><span class="koboSpan" id="kobo.6.1">. </span><span class="koboSpan" id="kobo.6.2">We</span><span><span class="koboSpan" id="kobo.7.1">'ll</span></span><span class="koboSpan" id="kobo.8.1"> consider a communicator a group of processes, where each process sends and receives an array of numerical data from the other processes defined in the group:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.9.1">For this example, the relevant </span><kbd><span class="koboSpan" id="kobo.10.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.11.1"> and </span><kbd><span class="koboSpan" id="kobo.12.1">numpy</span></kbd> <span><span class="koboSpan" id="kobo.13.1">libraries </span></span><span class="koboSpan" id="kobo.14.1">must be imported:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.15.1">from mpi4py import MPI 
import numpy </span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.16.1">As in the previous example, we need to set the same parameters, </span><kbd><span class="koboSpan" id="kobo.17.1">comm</span></kbd><span class="koboSpan" id="kobo.18.1">, </span><kbd><span class="koboSpan" id="kobo.19.1">size</span></kbd><span class="koboSpan" id="kobo.20.1">, and </span><kbd><span class="koboSpan" id="kobo.21.1">rank</span></kbd><span class="koboSpan" id="kobo.22.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.23.1">comm = MPI.COMM_WORLD 
size = comm.Get_size() 
rank = comm.Get_rank() </span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.24.1">Hence, we must define the data that each process will send (</span><kbd><span class="koboSpan" id="kobo.25.1">senddata</span></kbd><span class="koboSpan" id="kobo.26.1">) and, at the same time, receive (</span><kbd><span class="koboSpan" id="kobo.27.1">recvdata</span></kbd><span class="koboSpan" id="kobo.28.1">) from the other processes:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.29.1">senddata = (rank+1)*numpy.arange(size,dtype=int) 
recvdata = numpy.empty(size,dtype=int) </span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.30.1">Finally, the </span><kbd><span class="koboSpan" id="kobo.31.1">Alltoall</span></kbd><span class="koboSpan" id="kobo.32.1"> function is executed:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.33.1">comm.Alltoall(senddata,recvdata) </span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.34.1">The data that is sent and received for each process is displayed:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.35.1">print(" process %s sending %s receiving %s"\ 
      %(rank , senddata , recvdata)) </span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How it works...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The </span><kbd><span class="koboSpan" id="kobo.3.1">comm.alltoall</span></kbd><span class="koboSpan" id="kobo.4.1"> method takes the </span><em><span class="koboSpan" id="kobo.5.1">i</span><sup><span class="koboSpan" id="kobo.6.1">th</span></sup></em><span class="koboSpan" id="kobo.7.1"> object from </span><span><span class="koboSpan" id="kobo.8.1">the</span></span><span class="koboSpan" id="kobo.9.1"> </span><kbd><span class="koboSpan" id="kobo.10.1">sendbuf</span></kbd><span class="koboSpan" id="kobo.11.1"> </span><span><span class="koboSpan" id="kobo.12.1">argument</span></span><span class="koboSpan" id="kobo.13.1"> of task </span><kbd><span class="koboSpan" id="kobo.14.1">j</span></kbd><span class="koboSpan" id="kobo.15.1"> and copies it into the </span><em><span class="koboSpan" id="kobo.16.1">j</span><sup><span class="koboSpan" id="kobo.17.1">th</span></sup></em><span class="koboSpan" id="kobo.18.1"> object of the </span><kbd><span class="koboSpan" id="kobo.19.1">recvbuf</span></kbd><span class="koboSpan" id="kobo.20.1"> argument of task </span><kbd><span class="koboSpan" id="kobo.21.1">i</span></kbd><span class="koboSpan" id="kobo.22.1">.</span></p>
<p><span><span><span class="koboSpan" id="kobo.23.1">If we run</span></span></span><span class="koboSpan" id="kobo.24.1"> the code with a communicator group of </span><kbd><span class="koboSpan" id="kobo.25.1">5</span></kbd><span class="koboSpan" id="kobo.26.1"> processes, then our output is as follows:</span></p>
<pre><strong><span class="koboSpan" id="kobo.27.1">C:\&gt;mpiexec -n 5 python alltoall.py </span><br/><span class="koboSpan" id="kobo.28.1">process 0 sending [0 1 2 3 4] receiving [0 0 0 0 0] </span><br/><span class="koboSpan" id="kobo.29.1">process 1 sending [0 2 4 6 8] receiving [1 2 3 4 5] </span><br/><span class="koboSpan" id="kobo.30.1">process 2 sending [ 0 3 6 9 12] receiving [ 2 4 6 8 10] </span><br/><span class="koboSpan" id="kobo.31.1">process 3 sending [ 0 4 8 12 16] receiving [ 3 6 9 12 15] </span><br/><span class="koboSpan" id="kobo.32.1">process 4 sending [ 0 5 10 15 20] receiving [ 4 8 12 16 20]</span></strong> </pre>
<p><span class="koboSpan" id="kobo.33.1">We could also figure out what happened by using the following schema:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.34.1"><img src="assets/ae2c13f2-c674-4f5e-a05b-bf8982a010bd.png" style="width:37.58em;height:12.08em;"/></span></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.35.1">The Alltoall collective communication</span></div>
<p><span class="koboSpan" id="kobo.36.1">Our observations regarding the schema are as follows:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.37.1">The </span><em><span class="koboSpan" id="kobo.38.1">P0</span></em> <span><span class="koboSpan" id="kobo.39.1">process </span></span><span class="koboSpan" id="kobo.40.1">contains the [</span><strong><span class="koboSpan" id="kobo.41.1">0 1 2 3 4</span></strong><span class="koboSpan" id="kobo.42.1">] </span><span><span class="koboSpan" id="kobo.43.1"> data array</span></span><span class="koboSpan" id="kobo.44.1">, where it assigns 0 to itself, 1 to the </span><em><span class="koboSpan" id="kobo.45.1">P1 </span></em><span><span class="koboSpan" id="kobo.46.1">process,</span></span><span class="koboSpan" id="kobo.47.1"> 2 to the </span><em><span class="koboSpan" id="kobo.48.1">P2 </span></em><span><span class="koboSpan" id="kobo.49.1">process</span></span><span class="koboSpan" id="kobo.50.1">, 3 to the </span><em><span class="koboSpan" id="kobo.51.1">P3 </span></em><span><span class="koboSpan" id="kobo.52.1">process,</span></span><span class="koboSpan" id="kobo.53.1"> and 4 to the </span><em><span class="koboSpan" id="kobo.54.1">P4 </span></em><span><span class="koboSpan" id="kobo.55.1">process;</span></span></li>
<li><span class="koboSpan" id="kobo.56.1">The </span><em><span class="koboSpan" id="kobo.57.1">P1</span></em> <span><span class="koboSpan" id="kobo.58.1">process </span></span><span class="koboSpan" id="kobo.59.1">contains the [</span><strong><span class="koboSpan" id="kobo.60.1">0 2 4 6 8</span></strong><span class="koboSpan" id="kobo.61.1">] </span><span><span class="koboSpan" id="kobo.62.1">data array</span></span><span class="koboSpan" id="kobo.63.1">, where it assigns 0 to</span><span><span class="koboSpan" id="kobo.64.1"> the</span></span> <em><span class="koboSpan" id="kobo.65.1">P0 </span></em><span><span class="koboSpan" id="kobo.66.1">process</span></span><span class="koboSpan" id="kobo.67.1">, 2 to itself, 4 to the </span><em><span class="koboSpan" id="kobo.68.1">P2 </span></em><span><span class="koboSpan" id="kobo.69.1">process</span></span><span class="koboSpan" id="kobo.70.1">, 6 to the </span><em><span class="koboSpan" id="kobo.71.1">P3 </span></em><span><span class="koboSpan" id="kobo.72.1">process</span></span><span class="koboSpan" id="kobo.73.1">, and 8 to the </span><em><span class="koboSpan" id="kobo.74.1">P4 </span></em><span><span class="koboSpan" id="kobo.75.1">process;</span></span></li>
<li><span class="koboSpan" id="kobo.76.1">The </span><em><span class="koboSpan" id="kobo.77.1">P2</span></em> <span><span class="koboSpan" id="kobo.78.1">process </span></span><span class="koboSpan" id="kobo.79.1">contains the [</span><strong><span class="koboSpan" id="kobo.80.1">0 3 6 9 12</span></strong><span class="koboSpan" id="kobo.81.1">]</span><span><span class="koboSpan" id="kobo.82.1"> data array</span></span><span class="koboSpan" id="kobo.83.1">, where it assigns 0 to</span><span><span class="koboSpan" id="kobo.84.1"> the</span></span> <em><span class="koboSpan" id="kobo.85.1">P0 </span></em><span><span class="koboSpan" id="kobo.86.1">process</span></span><span class="koboSpan" id="kobo.87.1">, 3 to the </span><em><span class="koboSpan" id="kobo.88.1">P1 </span></em><span><span class="koboSpan" id="kobo.89.1">process</span></span><span class="koboSpan" id="kobo.90.1">, 6 to itself, 9 to the </span><em><span class="koboSpan" id="kobo.91.1">P3 </span></em><span><span class="koboSpan" id="kobo.92.1">process, </span></span><span class="koboSpan" id="kobo.93.1">and 12 to the </span><em><span class="koboSpan" id="kobo.94.1">P4 </span></em><span><span class="koboSpan" id="kobo.95.1">process;</span></span></li>
<li><span class="koboSpan" id="kobo.96.1">The </span><em><span class="koboSpan" id="kobo.97.1">P3</span></em> <span><span class="koboSpan" id="kobo.98.1">process </span></span><span class="koboSpan" id="kobo.99.1">contains the [</span><strong><span class="koboSpan" id="kobo.100.1">0 4 8 12 16</span></strong><span class="koboSpan" id="kobo.101.1">]</span><span><span class="koboSpan" id="kobo.102.1"> data array</span></span><span class="koboSpan" id="kobo.103.1">, where it assigns 0 to</span><span><span class="koboSpan" id="kobo.104.1"> the</span></span> <em><span class="koboSpan" id="kobo.105.1">P0 </span></em><span><span class="koboSpan" id="kobo.106.1">process</span></span><span class="koboSpan" id="kobo.107.1">, 4 to the </span><em><span class="koboSpan" id="kobo.108.1">P1 </span></em><span><span class="koboSpan" id="kobo.109.1">process</span></span><span class="koboSpan" id="kobo.110.1">, 8 to the </span><em><span class="koboSpan" id="kobo.111.1">P2</span></em><span class="koboSpan" id="kobo.112.1"> </span><span><span class="koboSpan" id="kobo.113.1">process</span></span><span class="koboSpan" id="kobo.114.1">, 12 to itself, and 16 to the </span><em><span class="koboSpan" id="kobo.115.1">P4 </span></em><span><span class="koboSpan" id="kobo.116.1">process;</span></span></li>
<li><span class="koboSpan" id="kobo.117.1">The </span><em><span class="koboSpan" id="kobo.118.1">P4</span></em> <span><span class="koboSpan" id="kobo.119.1">process </span></span><span class="koboSpan" id="kobo.120.1">contains the [</span><strong><span class="koboSpan" id="kobo.121.1">0 5 10 15 20</span></strong><span class="koboSpan" id="kobo.122.1">]</span><span><span class="koboSpan" id="kobo.123.1"> data array</span></span><span class="koboSpan" id="kobo.124.1">, where it assigns 0 to</span><span><span class="koboSpan" id="kobo.125.1"> the </span></span><em><span class="koboSpan" id="kobo.126.1">P0</span></em><span class="koboSpan" id="kobo.127.1"> </span><span><span class="koboSpan" id="kobo.128.1">process</span></span><span class="koboSpan" id="kobo.129.1">, 5 to the </span><em><span class="koboSpan" id="kobo.130.1">P1 </span></em><span><span class="koboSpan" id="kobo.131.1">process</span></span><span class="koboSpan" id="kobo.132.1">, 10 to the </span><em><span class="koboSpan" id="kobo.133.1">P2 </span></em><span><span class="koboSpan" id="kobo.134.1">process</span></span><span class="koboSpan" id="kobo.135.1">, 15 to the </span><em><span class="koboSpan" id="kobo.136.1">P3 </span></em><span><span class="koboSpan" id="kobo.137.1">process</span></span><span class="koboSpan" id="kobo.138.1">, and 20 to itself.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">There's more...</span></h1>
                </header>
            
            <article>
                
<p><kbd><span class="koboSpan" id="kobo.2.1">Alltoall</span></kbd><span class="koboSpan" id="kobo.3.1"> personalized communication is also known as a total exchange. </span><span class="koboSpan" id="kobo.3.2">This operation is used in a variety of parallel algorithms, such as the fast Fourier transform, matrix transpose, sample sort, and some parallel database join operations.</span></p>
<p><span class="koboSpan" id="kobo.4.1">In</span><span><span class="koboSpan" id="kobo.5.1"> </span></span><kbd><span class="koboSpan" id="kobo.6.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.7.1">, there are</span><span><span class="koboSpan" id="kobo.8.1"> </span></span><em><span class="koboSpan" id="kobo.9.1">three types</span></em><span><span class="koboSpan" id="kobo.10.1"> </span></span><span class="koboSpan" id="kobo.11.1">of</span><span><span class="koboSpan" id="kobo.12.1"> </span></span><kbd><span class="koboSpan" id="kobo.13.1">Alltoall</span></kbd><span><span class="koboSpan" id="kobo.14.1"> </span></span><span class="koboSpan" id="kobo.15.1">collective communication:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.16.1">comm.Alltoall(sendbuf, recvbuf)</span></kbd><span class="koboSpan" id="kobo.17.1">: The </span><kbd><span class="koboSpan" id="kobo.18.1">Alltoall</span></kbd><span class="koboSpan" id="kobo.19.1"> scatter/gather sends data from all-to-all processes in a group.</span></li>
<li><kbd><span class="koboSpan" id="kobo.20.1">comm.Alltoallv(sendbuf, recvbuf)</span></kbd><span class="koboSpan" id="kobo.21.1">: The </span><kbd><span class="koboSpan" id="kobo.22.1">Alltoall</span></kbd><span class="koboSpan" id="kobo.23.1"> scatter/gather vector sends data from </span><span><span class="koboSpan" id="kobo.24.1">all-to-all </span></span><span class="koboSpan" id="kobo.25.1">processes in a group, providing a different amount of data and displacements.</span></li>
<li><kbd><span class="koboSpan" id="kobo.26.1">comm.Alltoallw(sendbuf, recvbuf)</span></kbd><span class="koboSpan" id="kobo.27.1">: Generalized </span><kbd><span class="koboSpan" id="kobo.28.1">Alltoall</span></kbd><span class="koboSpan" id="kobo.29.1"> communication allows different counts, displacements, and datatypes for each partner.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">See also</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">An interesting analysis of MPI Python modules can be downloaded from </span><a href="https://www.duo.uio.no/bitstream/handle/10852/10848/WenjingLinThesis.pdf"><span class="koboSpan" id="kobo.3.1">https://www.duo.uio.no/bitstream/handle/10852/10848/WenjingLinThesis.pdf</span></a><span class="koboSpan" id="kobo.4.1">.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">The reduction operation</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Similar to </span><kbd><span class="koboSpan" id="kobo.3.1">comm.gather</span></kbd><span class="koboSpan" id="kobo.4.1">, </span><kbd><span class="koboSpan" id="kobo.5.1">comm.reduce</span></kbd><span class="koboSpan" id="kobo.6.1"> takes an array of input elements in each process and returns an array of output elements to the root process. </span><span class="koboSpan" id="kobo.6.2">The output elements contain the reduced result.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Getting ready</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In </span><kbd><span class="koboSpan" id="kobo.3.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.4.1">, we define the reduction operation through the following statement:</span></p>
<pre><span class="koboSpan" id="kobo.5.1">comm.Reduce(sendbuf, recvbuf, rank_of_root_process, op = type_of_reduction_operation) </span></pre>
<p><span class="koboSpan" id="kobo.6.1">We must note that the difference with the </span><kbd><span class="koboSpan" id="kobo.7.1">comm.gather</span></kbd><span class="koboSpan" id="kobo.8.1"> statement resides in the </span><kbd><span class="koboSpan" id="kobo.9.1">op</span></kbd><span class="koboSpan" id="kobo.10.1"> parameter, which is the operation that you wish to apply to your data, and the </span><kbd><span class="koboSpan" id="kobo.11.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.12.1"> module contains a set of reduction operations that can be used. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How to do it...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Now, we'll see how to implement the sum of an array of elements with the </span><kbd><span class="koboSpan" id="kobo.3.1">MPI.SUM</span></kbd><span class="koboSpan" id="kobo.4.1"> </span><span><span class="koboSpan" id="kobo.5.1">reduction operation</span></span><span class="koboSpan" id="kobo.6.1"> by using the reduction functionality. </span><span class="koboSpan" id="kobo.6.2">Each process will manipulate an array of size 10.</span></p>
<p><span class="koboSpan" id="kobo.7.1">For array manipulation, we use the functions provided by the </span><kbd><span class="koboSpan" id="kobo.8.1">numpy</span></kbd><span class="koboSpan" id="kobo.9.1"> Python module:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.10.1">Here, the relevant libraries, </span><kbd><span class="koboSpan" id="kobo.11.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.12.1"> and </span><kbd><span class="koboSpan" id="kobo.13.1">numpy</span></kbd><span class="koboSpan" id="kobo.14.1">, are imported:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.15.1">import numpy 
from mpi4py import MPI  </span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.16.1">Define the </span><kbd><span class="koboSpan" id="kobo.17.1">comm</span></kbd><span class="koboSpan" id="kobo.18.1">, </span><kbd><span class="koboSpan" id="kobo.19.1">size</span></kbd><span class="koboSpan" id="kobo.20.1">, and </span><kbd><span class="koboSpan" id="kobo.21.1">rank</span></kbd><span class="koboSpan" id="kobo.22.1"> parameters:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.23.1">comm = MPI.COMM_WORLD  
size = comm.size  
rank = comm.rank </span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.24.1">Then, the size of the array (</span><kbd><span class="koboSpan" id="kobo.25.1">array_size</span></kbd><span class="koboSpan" id="kobo.26.1">) is set:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.27.1">array_size = 10 </span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="4">
<li><span class="koboSpan" id="kobo.28.1">The data to be sent and received is defined:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.29.1">recvdata = numpy.zeros(array_size,dtype=numpy.int) 
senddata = (rank+1)*numpy.arange(array_size,dtype=numpy.int) </span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.30.1">The process sender and the sent data are printed out:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.31.1">print(" process %s sending %s " %(rank , senddata)) </span></pre>
<ol start="6">
<li><span class="koboSpan" id="kobo.32.1">Finally, the </span><kbd><span class="koboSpan" id="kobo.33.1">Reduce</span></kbd><span class="koboSpan" id="kobo.34.1"> operation is executed. </span><span class="koboSpan" id="kobo.34.2">Note that the </span><kbd><span class="koboSpan" id="kobo.35.1">root</span></kbd><span class="koboSpan" id="kobo.36.1"> </span><span><span class="koboSpan" id="kobo.37.1">process is </span></span><span class="koboSpan" id="kobo.38.1">set to </span><kbd><span class="koboSpan" id="kobo.39.1">0</span></kbd><span class="koboSpan" id="kobo.40.1"> and the </span><kbd><span class="koboSpan" id="kobo.41.1">op</span></kbd><span class="koboSpan" id="kobo.42.1"> parameter is set to </span><kbd><span class="koboSpan" id="kobo.43.1">MPI.SUM</span></kbd><span class="koboSpan" id="kobo.44.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.45.1">comm.Reduce(senddata,recvdata,root=0,op=MPI.SUM) </span></pre>
<ol start="7">
<li><span class="koboSpan" id="kobo.46.1">The output of the reduction operation is then shown, as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.47.1">print ('on task',rank,'after Reduce:    data = ',recvdata) </span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How it works...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">To perform the reduction sum, we use the</span><span><span class="koboSpan" id="kobo.3.1"> </span></span><kbd><span class="koboSpan" id="kobo.4.1">comm.Reduce</span></kbd><span><span class="koboSpan" id="kobo.5.1"> </span></span><span class="koboSpan" id="kobo.6.1">statement. </span><span class="koboSpan" id="kobo.6.2">Also, we identify with </span><kbd><span class="koboSpan" id="kobo.7.1">rank</span></kbd><span class="koboSpan" id="kobo.8.1"> zero, which is the </span><kbd><span class="koboSpan" id="kobo.9.1">root</span></kbd><span class="koboSpan" id="kobo.10.1"> process that will contain</span><span><span class="koboSpan" id="kobo.11.1"> </span></span><kbd><span class="koboSpan" id="kobo.12.1">recvbuf</span></kbd><span class="koboSpan" id="kobo.13.1">, which represents the final result of the computation:</span></p>
<pre><span class="koboSpan" id="kobo.14.1">comm.Reduce(senddata,recvdata,root=0,op=MPI.SUM) </span></pre>
<p><span class="koboSpan" id="kobo.15.1">It makes sense to run the code with a communicator group of </span><kbd><span class="koboSpan" id="kobo.16.1">10</span></kbd><span class="koboSpan" id="kobo.17.1"> processes, as this is the size of the manipulated array.</span></p>
<p><span class="koboSpan" id="kobo.18.1">The output appears as follows:</span></p>
<pre><strong><span class="koboSpan" id="kobo.19.1">C:\&gt;mpiexec -n 10 python reduction.py 
  process 1 sending [ 0 2 4 6 8 10 12 14 16 18]</span><br/><span class="koboSpan" id="kobo.20.1">on task 1 after Reduce: data = [0 0 0 0 0 0 0 0 0 0]</span><br/><span class="koboSpan" id="kobo.21.1"> process 5 sending [ 0 6 12 18 24 30 36 42 48 54]</span><br/><span class="koboSpan" id="kobo.22.1">on task 5 after Reduce: data = [0 0 0 0 0 0 0 0 0 0]</span><br/><span class="koboSpan" id="kobo.23.1"> process 7 sending [ 0 8 16 24 32 40 48 56 64 72]</span><br/><span class="koboSpan" id="kobo.24.1">on task 7 after Reduce: data = [0 0 0 0 0 0 0 0 0 0]</span><br/><span class="koboSpan" id="kobo.25.1"> process 3 sending [ 0 4 8 12 16 20 24 28 32 36]</span><br/><span class="koboSpan" id="kobo.26.1">on task 3 after Reduce: data = [0 0 0 0 0 0 0 0 0 0]</span><br/><span class="koboSpan" id="kobo.27.1"> process 9 sending [ 0 10 20 30 40 50 60 70 80 90]</span><br/><span class="koboSpan" id="kobo.28.1">on task 9 after Reduce: data = [0 0 0 0 0 0 0 0 0 0]</span><br/><span class="koboSpan" id="kobo.29.1"> process 6 sending [ 0 7 14 21 28 35 42 49 56 63]</span><br/><span class="koboSpan" id="kobo.30.1">on task 6 after Reduce: data = [0 0 0 0 0 0 0 0 0 0]</span><br/><span class="koboSpan" id="kobo.31.1"> process 2 sending [ 0 3 6 9 12 15 18 21 24 27]</span><br/><span class="koboSpan" id="kobo.32.1">on task 2 after Reduce: data = [0 0 0 0 0 0 0 0 0 0]</span><br/><span class="koboSpan" id="kobo.33.1"> process 8 sending [ 0 9 18 27 36 45 54 63 72 81]</span><br/><span class="koboSpan" id="kobo.34.1">on task 8 after Reduce: data = [0 0 0 0 0 0 0 0 0 0]</span><br/><span class="koboSpan" id="kobo.35.1"> process 4 sending [ 0 5 10 15 20 25 30 35 40 45]</span><br/><span class="koboSpan" id="kobo.36.1">on task 4 after Reduce: data = [0 0 0 0 0 0 0 0 0 0]</span><br/><span class="koboSpan" id="kobo.37.1"> process 0 sending [0 1 2 3 4 5 6 7 8 9]</span><br/><span class="koboSpan" id="kobo.38.1">on task 0 after Reduce: data = [ 0 55 110 165 220 275 330 385 440 495]</span></strong> </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">There's more...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Note that with the </span><kbd><span class="koboSpan" id="kobo.3.1">op=MPI.SUM</span></kbd><span class="koboSpan" id="kobo.4.1"> option, we apply the sum operation to all the elements of the column array. </span><span class="koboSpan" id="kobo.4.2">To better understand how the reduction operates, let's look at the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.5.1"><img src="assets/aa28bcc1-08cf-4699-b559-9f4f1e6e948c.png" style="width:41.75em;height:14.83em;"/></span></p>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.6.1">Reduction in collective communication</span></div>
<p><span class="koboSpan" id="kobo.7.1">The sending operation is as follows:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.8.1">The </span><strong><span class="koboSpan" id="kobo.9.1">P0</span></strong> <span><span class="koboSpan" id="kobo.10.1">process </span></span><span class="koboSpan" id="kobo.11.1">sends the [</span><strong><span class="koboSpan" id="kobo.12.1">0 1 2</span></strong><span class="koboSpan" id="kobo.13.1">] </span><span><span class="koboSpan" id="kobo.14.1">data array.</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">The </span><strong><span class="koboSpan" id="kobo.16.1">P1</span></strong> <span><span class="koboSpan" id="kobo.17.1">process </span></span><span class="koboSpan" id="kobo.18.1">sends the [</span><strong><span class="koboSpan" id="kobo.19.1">0 2 4</span></strong><span class="koboSpan" id="kobo.20.1">] </span><span><span class="koboSpan" id="kobo.21.1">data array.</span></span></li>
<li><span class="koboSpan" id="kobo.22.1">The </span><strong><span class="koboSpan" id="kobo.23.1">P2</span></strong> <span><span class="koboSpan" id="kobo.24.1">process </span></span><span class="koboSpan" id="kobo.25.1">sends the [</span><strong><span class="koboSpan" id="kobo.26.1">0 3 6</span></strong><span class="koboSpan" id="kobo.27.1">] </span><span><span class="koboSpan" id="kobo.28.1">data array.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.29.1">The reduction operation sums the </span><em><span class="koboSpan" id="kobo.30.1">i</span><sup><span class="koboSpan" id="kobo.31.1">th</span></sup></em><span class="koboSpan" id="kobo.32.1"> elements of each task and then puts the result in the </span><em><span class="koboSpan" id="kobo.33.1">i</span><sup><span class="koboSpan" id="kobo.34.1">th</span></sup></em><span class="koboSpan" id="kobo.35.1"> element of the array in the </span><strong><span class="koboSpan" id="kobo.36.1">P0</span></strong><span class="koboSpan" id="kobo.37.1"> root </span><span><span class="koboSpan" id="kobo.38.1">process</span></span><span class="koboSpan" id="kobo.39.1">. </span><span class="koboSpan" id="kobo.39.2">For the receiving operation, the </span><strong><span class="koboSpan" id="kobo.40.1">P0</span></strong> <span><span class="koboSpan" id="kobo.41.1">process </span></span><span class="koboSpan" id="kobo.42.1">receives the [</span><strong><span class="koboSpan" id="kobo.43.1">0 6 12</span></strong><span class="koboSpan" id="kobo.44.1">] </span><span><span class="koboSpan" id="kobo.45.1">data array.</span></span></p>
<p><span class="koboSpan" id="kobo.46.1">Some of the reduction</span><span><span class="koboSpan" id="kobo.47.1"> </span></span><span class="koboSpan" id="kobo.48.1">operations defined by MPI are as follows:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.49.1">MPI.MAX</span></kbd><span class="koboSpan" id="kobo.50.1">: This returns the maximum element.</span></li>
<li><kbd><span class="koboSpan" id="kobo.51.1">MPI.MIN</span></kbd><span class="koboSpan" id="kobo.52.1">: This returns the minimum element.</span></li>
<li><kbd><span class="koboSpan" id="kobo.53.1">MPI.SUM</span></kbd><span class="koboSpan" id="kobo.54.1">: This sums up the elements.</span></li>
<li><kbd><span class="koboSpan" id="kobo.55.1">MPI.PROD</span></kbd><span class="koboSpan" id="kobo.56.1">: This multiplies all elements.</span></li>
<li><kbd><span class="koboSpan" id="kobo.57.1">MPI.LAND</span></kbd><span class="koboSpan" id="kobo.58.1">: This performs the AND logical operation across the elements.</span></li>
<li><kbd><span class="koboSpan" id="kobo.59.1">MPI.MAXLOC</span></kbd><span class="koboSpan" id="kobo.60.1">: This returns the maximum value and the rank of the process that owns it.</span></li>
<li><kbd><span class="koboSpan" id="kobo.61.1">MPI.MINLOC</span></kbd><span class="koboSpan" id="kobo.62.1">: This returns the minimum value and the rank of the process that owns it.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">See also</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">At </span><a href="http://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/"><span class="koboSpan" id="kobo.3.1">http://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/</span></a><span class="koboSpan" id="kobo.4.1">, you can find a good tutorial on this topic and much more.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Optimizing communication</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">An interesting feature that is provided by MPI regards virtual topologies. </span><span class="koboSpan" id="kobo.2.2">As already noted, all the communication functions (point-to-point or collective) refer to a group of processes. </span><span class="koboSpan" id="kobo.2.3">We have always used the </span><kbd><span class="koboSpan" id="kobo.3.1">MPI_COMM_WORLD</span></kbd><span class="koboSpan" id="kobo.4.1"> group that includes all processes. </span><span class="koboSpan" id="kobo.4.2">It assigns a rank of </span><em><span class="koboSpan" id="kobo.5.1">0</span></em><span class="koboSpan" id="kobo.6.1"> to </span><em><span class="koboSpan" id="kobo.7.1">n-1</span></em><span class="koboSpan" id="kobo.8.1"> for each process that belongs to a communicator of the size </span><em><span class="koboSpan" id="kobo.9.1">n</span></em><span class="koboSpan" id="kobo.10.1">.</span></p>
<p><span><span class="koboSpan" id="kobo.11.1">However, MPI allows us to assign a virtual topology to a communicator. </span><span class="koboSpan" id="kobo.11.2">It defines an assignment of labels to the different processes: by building a virtual topology, each node will communicate only with its virtual neighbor, improving performance because it reduces execution times.</span></span></p>
<p><span class="koboSpan" id="kobo.12.1">For example, if the rank was randomly assigned, </span><span><span class="koboSpan" id="kobo.13.1">then </span></span><span class="koboSpan" id="kobo.14.1">a message could be forced to pass to many other nodes before it reaches the destination. </span><span class="koboSpan" id="kobo.14.2">Beyond the question of performance, a virtual topology makes sure that the code is clearer and more readable.</span></p>
<p><span class="koboSpan" id="kobo.15.1">MPI provides two building topologies. </span><span class="koboSpan" id="kobo.15.2">The first construct creates Cartesian topologies, while the latter creates any kind of topologies. </span><span class="koboSpan" id="kobo.15.3">Specifically, in the second case, we must supply the adjacency matrix of the graph that you want to build. </span><span class="koboSpan" id="kobo.15.4">We will </span><span><span class="koboSpan" id="kobo.16.1">only </span></span><span class="koboSpan" id="kobo.17.1">deal with Cartesian topologies, through which it is possible to build several structures that are widely used, such as mesh, ring, </span><span><span class="koboSpan" id="kobo.18.1">and</span></span><span class="koboSpan" id="kobo.19.1"> toroid.</span></p>
<p><span class="koboSpan" id="kobo.20.1">The </span><kbd><span class="koboSpan" id="kobo.21.1">mpi4py</span></kbd><span class="koboSpan" id="kobo.22.1"> function used to create a Cartesian topology is as follows:</span></p>
<pre><span class="koboSpan" id="kobo.23.1">comm.Create_cart((number_of_rows,number_of_columns))</span></pre>
<p><span class="koboSpan" id="kobo.24.1">Here, </span><kbd><span class="koboSpan" id="kobo.25.1">number_of_rows</span></kbd><span class="koboSpan" id="kobo.26.1"> and </span><kbd><span class="koboSpan" id="kobo.27.1">number_of_columns</span></kbd><span class="koboSpan" id="kobo.28.1"> specify the rows and columns of the grid that is to be made.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How to do it...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the following example, we see how to implement a Cartesian topology of the size </span><em><span class="koboSpan" id="kobo.3.1">M×N</span></em><span class="koboSpan" id="kobo.4.1">. </span><span class="koboSpan" id="kobo.4.2">Also, we define a set of coordinates to understand how all the processes are disposed of:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.5.1">Import all the relevant libraries:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.6.1">from mpi4py import MPI 
import numpy as np </span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.7.1">Define the following parameter in order to move along the topology:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.8.1">UP = 0 
DOWN = 1 
LEFT = 2 
RIGHT = 3 </span></pre>
<ol start="3">
<li><span><span class="koboSpan" id="kobo.9.1">For each process, </span></span><span class="koboSpan" id="kobo.10.1">the following array defines the neighbor processes:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.11.1">neighbour_processes = [0,0,0,0] </span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.12.1">In the </span><kbd><span class="koboSpan" id="kobo.13.1">main</span></kbd><span class="koboSpan" id="kobo.14.1"> program, the </span><kbd><span class="koboSpan" id="kobo.15.1">comm.rank</span></kbd><span class="koboSpan" id="kobo.16.1"> and </span><kbd><span class="koboSpan" id="kobo.17.1">size</span></kbd><span class="koboSpan" id="kobo.18.1"> parameters are then defined:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.19.1">if __name__ == "__main__": 
    comm = MPI.COMM_WORLD 
    rank = comm.rank 
    size = comm.size </span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.20.1">Now, let's build the topology:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.21.1">    grid_rows = int(np.floor(np.sqrt(comm.size))) 
    grid_column = comm.size // grid_rows </span></pre>
<ol start="6">
<li><span class="koboSpan" id="kobo.22.1">The following conditions ensure that the processes are always within the topology:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.23.1">    if grid_rows*grid_column &gt; size: 
        grid_column -= 1 
    if grid_rows*grid_column &gt; size: 
        grid_rows -= 1</span></pre>
<ol start="7">
<li><span class="koboSpan" id="kobo.24.1">The </span><kbd><span class="koboSpan" id="kobo.25.1">rank</span></kbd><span class="koboSpan" id="kobo.26.1"> equal to </span><kbd><span class="koboSpan" id="kobo.27.1">0</span></kbd><span class="koboSpan" id="kobo.28.1"> process starts the topology construction:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.29.1">    if (rank == 0) : 
        print("Building a %d x %d grid topology:"\ 
              % (grid_rows, grid_column) ) 
                
    cartesian_communicator = \ 
                           comm.Create_cart( \ 
                               (grid_rows, grid_column), \ 
                               periods=(False, False), \</span><br/><span class="koboSpan" id="kobo.30.1">                               reorder=True) 
    my_mpi_row, my_mpi_col = \ 
                cartesian_communicator.Get_coords\ 
                ( cartesian_communicator.rank )  
 
    neighbour_processes[UP], neighbour_processes[DOWN]\ 
                             = cartesian_communicator.Shift(0, 1) 
    neighbour_processes[LEFT],  \ 
                               neighbour_processes[RIGHT]  = \ 
                               cartesian_communicator.Shift(1, 1) 
    print ("Process = %s</span><br/><span class="koboSpan" id="kobo.31.1">    \row = %s\n \ 
    column = %s ----&gt; neighbour_processes[UP] = %s \ 
    neighbour_processes[DOWN] = %s \ 
    neighbour_processes[LEFT] =%s neighbour_processes[RIGHT]=%s" \ 
             %(rank, my_mpi_row, \ 
             my_mpi_col,neighbour_processes[UP], \ 
             neighbour_processes[DOWN], \ 
             neighbour_processes[LEFT] , \ 
             neighbour_processes[RIGHT])) </span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How it works...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">For each process, the output should read as follows: if </span><kbd><span class="koboSpan" id="kobo.3.1">neighbour_processes = -1</span></kbd><span class="koboSpan" id="kobo.4.1">, then it has no topological proximity, otherwise, </span><kbd><span class="koboSpan" id="kobo.5.1">neighbour_processes</span></kbd><span class="koboSpan" id="kobo.6.1"> shows the rank of the process closely.</span></p>
<p><span class="koboSpan" id="kobo.7.1">The resulting topology is a mesh of </span><em><span class="koboSpan" id="kobo.8.1">2</span></em><span class="koboSpan" id="kobo.9.1">×</span><em><span class="koboSpan" id="kobo.10.1">2</span></em><span class="koboSpan" id="kobo.11.1"> (refer to the previous diagram for a mesh representation), the size of which is equal to the number of processes in the input; that is, four:</span></p>
<pre><span class="koboSpan" id="kobo.12.1">grid_row = int(np.floor(np.sqrt(comm.size))) 
grid_column = comm.size // grid_row 
if grid_row*grid_column &gt; size: 
    grid_column -= 1 
if grid_row*grid_column &gt; size: 
    grid_rows -= 1</span></pre>
<p><span class="koboSpan" id="kobo.13.1">Then, the Cartesian topology is built using the </span><kbd><span class="koboSpan" id="kobo.14.1">comm.Create_cart</span></kbd><span class="koboSpan" id="kobo.15.1"> function (note also the parameter, </span><kbd><span class="koboSpan" id="kobo.16.1">periods = (False,False)</span></kbd><span class="koboSpan" id="kobo.17.1">):</span></p>
<pre><span class="koboSpan" id="kobo.18.1">cartesian_communicator = comm.Create_cart( \  
    (grid_row, grid_column), periods=(False, False), reorder=True) </span></pre>
<p><span class="koboSpan" id="kobo.19.1">To know the position of the process, we use the </span><kbd><span class="koboSpan" id="kobo.20.1">Get_coords()</span></kbd><span class="koboSpan" id="kobo.21.1"> method in the following form:</span></p>
<pre><span class="koboSpan" id="kobo.22.1">my_mpi_row, my_mpi_col =\ 
                cartesian_communicator.Get_coords(cartesian_communicator.rank ) </span></pre>
<p><span class="koboSpan" id="kobo.23.1">For the processes, in addition to getting their coordinates, we must calculate and find out which processes are topologically closer. </span><span class="koboSpan" id="kobo.23.2">For this purpose, we use the </span><kbd><span class="koboSpan" id="kobo.24.1">comm.Shift (rank_source,rank_dest)</span></kbd><span class="koboSpan" id="kobo.25.1"> function:</span></p>
<pre><br/><span class="koboSpan" id="kobo.26.1">neighbour_processes[UP], neighbour_processes[DOWN] =\            </span><br/><span class="koboSpan" id="kobo.27.1">                                  cartesian_communicator.Shift(0, 1) 
 
neighbour_processes[LEFT],  neighbour_processes[RIGHT] = \                                     </span><br/><span class="koboSpan" id="kobo.28.1">                                    cartesian_communicator.Shift(1, 1) </span></pre>
<p><span class="koboSpan" id="kobo.29.1">The </span><span><span class="koboSpan" id="kobo.30.1">topology </span></span><span class="koboSpan" id="kobo.31.1">obtained is as follows:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.32.1"><img src="assets/f716b5dc-9c4c-4e31-9fc3-b128b439f010.png" style="width:20.08em;height:13.83em;"/></span></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.33.1">The virtual mesh 2x2 topology</span></div>
<p><span class="koboSpan" id="kobo.34.1">As the diagram shows, the </span><em><span class="koboSpan" id="kobo.35.1">P0</span></em><span class="koboSpan" id="kobo.36.1"> process is chained to the </span><strong><span class="koboSpan" id="kobo.37.1">P1</span></strong> <kbd><span class="koboSpan" id="kobo.38.1">(RIGHT)</span></kbd><span class="koboSpan" id="kobo.39.1"> and </span><strong><span class="koboSpan" id="kobo.40.1">P2</span></strong> <kbd><span class="koboSpan" id="kobo.41.1">(DOWN)</span></kbd><span class="koboSpan" id="kobo.42.1"> processes. </span><span class="koboSpan" id="kobo.42.2">The </span><strong><span class="koboSpan" id="kobo.43.1">P1</span></strong><span class="koboSpan" id="kobo.44.1"> process is chained to the </span><strong><span class="koboSpan" id="kobo.45.1">P3</span></strong> <kbd><span class="koboSpan" id="kobo.46.1">(DOWN)</span></kbd><span class="koboSpan" id="kobo.47.1"> and </span><strong><span class="koboSpan" id="kobo.48.1">P0</span></strong> <kbd><span class="koboSpan" id="kobo.49.1">(LEFT)</span></kbd><span class="koboSpan" id="kobo.50.1"> processes, the </span><strong><span class="koboSpan" id="kobo.51.1">P3</span></strong><span class="koboSpan" id="kobo.52.1"> process is chained to the </span><strong><span class="koboSpan" id="kobo.53.1">P1</span></strong> <kbd><span class="koboSpan" id="kobo.54.1">(UP)</span></kbd><span class="koboSpan" id="kobo.55.1"> and </span><strong><span class="koboSpan" id="kobo.56.1">P2</span></strong> <kbd><span class="koboSpan" id="kobo.57.1">(LEFT)</span></kbd><span class="koboSpan" id="kobo.58.1"> processes, and the </span><strong><span class="koboSpan" id="kobo.59.1">P2</span></strong><span class="koboSpan" id="kobo.60.1"> process is chained to the </span><strong><span class="koboSpan" id="kobo.61.1">P3</span></strong> <kbd><span class="koboSpan" id="kobo.62.1">(RIGHT)</span></kbd><span class="koboSpan" id="kobo.63.1"> and </span><strong><span class="koboSpan" id="kobo.64.1">P0</span></strong> <kbd><span class="koboSpan" id="kobo.65.1">(UP)</span></kbd><span class="koboSpan" id="kobo.66.1"> processes.</span></p>
<p><span class="koboSpan" id="kobo.67.1">Finally,</span><span><span class="koboSpan" id="kobo.68.1"> by </span></span><span class="koboSpan" id="kobo.69.1">running the script, we obtain the following result:</span></p>
<pre><strong><span class="koboSpan" id="kobo.70.1">C:\&gt;mpiexec -n 4 python virtualTopology.py</span></strong><br/><strong><span class="koboSpan" id="kobo.71.1">Building a 2 x 2 grid topology:</span></strong><br/><strong><span class="koboSpan" id="kobo.72.1">Process = 0 row = 0 column = 0</span></strong><br/><strong><span class="koboSpan" id="kobo.73.1"> ----&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.74.1">neighbour_processes[UP] = -1</span></strong><br/><strong><span class="koboSpan" id="kobo.75.1">neighbour_processes[DOWN] = 2</span></strong><br/><strong><span class="koboSpan" id="kobo.76.1">neighbour_processes[LEFT] =-1</span></strong><br/><strong><span class="koboSpan" id="kobo.77.1">neighbour_processes[RIGHT]=1</span></strong><br/><br/><strong><span class="koboSpan" id="kobo.78.1">Process = 2 row = 1 column = 0</span></strong><br/><strong><span class="koboSpan" id="kobo.79.1"> ----&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.80.1">neighbour_processes[UP] = 0</span></strong><br/><strong><span class="koboSpan" id="kobo.81.1">neighbour_processes[DOWN] = -1</span></strong><br/><strong><span class="koboSpan" id="kobo.82.1">neighbour_processes[LEFT] =-1</span></strong><br/><strong><span class="koboSpan" id="kobo.83.1">neighbour_processes[RIGHT]=3</span></strong><br/><br/><strong><span class="koboSpan" id="kobo.84.1">Process = 1 row = 0 column = 1</span></strong><br/><strong><span class="koboSpan" id="kobo.85.1"> ----&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.86.1">neighbour_processes[UP] = -1</span></strong><br/><strong><span class="koboSpan" id="kobo.87.1">neighbour_processes[DOWN] = 3</span></strong><br/><strong><span class="koboSpan" id="kobo.88.1">neighbour_processes[LEFT] =0</span></strong><br/><strong><span class="koboSpan" id="kobo.89.1">neighbour_processes[RIGHT]=-1</span></strong><br/><br/><strong><span class="koboSpan" id="kobo.90.1">Process = 3 row = 1 column = 1</span></strong><br/><strong><span class="koboSpan" id="kobo.91.1"> ----&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.92.1">neighbour_processes[UP] = 1</span></strong><br/><strong><span class="koboSpan" id="kobo.93.1">neighbour_processes[DOWN] = -1</span></strong><br/><strong><span class="koboSpan" id="kobo.94.1">neighbour_processes[LEFT] =2</span></strong><br/><strong><span class="koboSpan" id="kobo.95.1">neighbour_processes[RIGHT]=-1</span></strong><br/><br/></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">There's more...</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">To obtain a toroidal topology of the size </span><em><span class="koboSpan" id="kobo.3.1">M</span></em><span class="koboSpan" id="kobo.4.1">×</span><em><span class="koboSpan" id="kobo.5.1">N</span></em><span class="koboSpan" id="kobo.6.1">, let's use </span><kbd><span class="koboSpan" id="kobo.7.1">comm.Create_cart</span></kbd><span class="koboSpan" id="kobo.8.1"> again, but, this time, let's set the </span><kbd><span class="koboSpan" id="kobo.9.1">periods</span></kbd><span class="koboSpan" id="kobo.10.1"> parameter to </span><kbd><span class="koboSpan" id="kobo.11.1">periods=(True,True)</span></kbd><span class="koboSpan" id="kobo.12.1">:</span></p>
<pre><span class="koboSpan" id="kobo.13.1">cartesian_communicator = comm.Create_cart( (grid_row, grid_column),\ 
                                 periods=(True, True), reorder=True) </span></pre>
<p><span class="koboSpan" id="kobo.14.1">The following output is obtained:</span></p>
<pre><strong><span class="koboSpan" id="kobo.15.1">C:\&gt;mpiexec -n 4 python virtualTopology.py</span><br/><span class="koboSpan" id="kobo.16.1">Process = 3 row = 1 column = 1</span><br/><span class="koboSpan" id="kobo.17.1">----&gt;</span><br/><span class="koboSpan" id="kobo.18.1">neighbour_processes[UP] = 1</span><br/><span class="koboSpan" id="kobo.19.1">neighbour_processes[DOWN] = 1</span><br/><span class="koboSpan" id="kobo.20.1">neighbour_processes[LEFT] =2</span><br/><span class="koboSpan" id="kobo.21.1">neighbour_processes[RIGHT]=2</span><br/><br/><span class="koboSpan" id="kobo.22.1">Process = 1 row = 0 column = 1</span><br/><span class="koboSpan" id="kobo.23.1">----&gt;</span><br/><span class="koboSpan" id="kobo.24.1">neighbour_processes[UP] = 3</span><br/><span class="koboSpan" id="kobo.25.1">neighbour_processes[DOWN] = 3</span><br/><span class="koboSpan" id="kobo.26.1">neighbour_processes[LEFT] =0</span><br/><span class="koboSpan" id="kobo.27.1">neighbour_processes[RIGHT]=0</span><br/><br/><span class="koboSpan" id="kobo.28.1">Building a 2 x 2 grid topology:</span><br/><span class="koboSpan" id="kobo.29.1">Process = 0 row = 0 column = 0</span><br/><span class="koboSpan" id="kobo.30.1">----&gt;</span><br/><span class="koboSpan" id="kobo.31.1">neighbour_processes[UP] = 2</span><br/><span class="koboSpan" id="kobo.32.1">neighbour_processes[DOWN] = 2</span><br/><span class="koboSpan" id="kobo.33.1">neighbour_processes[LEFT] =1</span><br/><span class="koboSpan" id="kobo.34.1">neighbour_processes[RIGHT]=1</span><br/><br/><span class="koboSpan" id="kobo.35.1">Process = 2 row = 1 column = 0</span><br/><span class="koboSpan" id="kobo.36.1">----&gt;</span><br/><span class="koboSpan" id="kobo.37.1">neighbour_processes[UP] = 0</span><br/><span class="koboSpan" id="kobo.38.1">neighbour_processes[DOWN] = 0</span><br/><span class="koboSpan" id="kobo.39.1">neighbour_processes[LEFT] =3</span><br/><span class="koboSpan" id="kobo.40.1">neighbour_processes[RIGHT]=3</span></strong> </pre>
<p><span class="koboSpan" id="kobo.41.1">The output covers the topology represented here:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.42.1"><img src="assets/0de01d9b-fe04-43f0-9700-68e7955534b8.png" style="width:27.08em;height:17.00em;"/></span></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.43.1">The virtual toroidal 2x2 topology</span></div>
<p><span class="koboSpan" id="kobo.44.1">The topology represented in the previous diagram indicates that the </span><strong><span class="koboSpan" id="kobo.45.1">P0</span></strong><span class="koboSpan" id="kobo.46.1"> process is chained to the </span><strong><span class="koboSpan" id="kobo.47.1">P1</span></strong><span class="koboSpan" id="kobo.48.1"> (</span><kbd><span class="koboSpan" id="kobo.49.1">RIGHT</span></kbd><span class="koboSpan" id="kobo.50.1"> and </span><kbd><span class="koboSpan" id="kobo.51.1">LEFT</span></kbd><span class="koboSpan" id="kobo.52.1">) and </span><strong><span class="koboSpan" id="kobo.53.1">P2</span></strong><span class="koboSpan" id="kobo.54.1"> (</span><kbd><span class="koboSpan" id="kobo.55.1">UP</span></kbd><span class="koboSpan" id="kobo.56.1"> and </span><kbd><span class="koboSpan" id="kobo.57.1">DOWN</span></kbd><span class="koboSpan" id="kobo.58.1">) processes, the </span><strong><span class="koboSpan" id="kobo.59.1">P1</span></strong><span class="koboSpan" id="kobo.60.1"> process is chained to the </span><strong><span class="koboSpan" id="kobo.61.1">P3</span></strong><span class="koboSpan" id="kobo.62.1"> (</span><kbd><span class="koboSpan" id="kobo.63.1">UP</span></kbd><span class="koboSpan" id="kobo.64.1"> and </span><kbd><span class="koboSpan" id="kobo.65.1">DOWN</span></kbd><span class="koboSpan" id="kobo.66.1">) and </span><strong><span class="koboSpan" id="kobo.67.1">P0</span></strong><span class="koboSpan" id="kobo.68.1"> (</span><kbd><span class="koboSpan" id="kobo.69.1">RIGHT</span></kbd><span class="koboSpan" id="kobo.70.1"> and </span><kbd><span class="koboSpan" id="kobo.71.1">LEFT</span></kbd><span class="koboSpan" id="kobo.72.1">) processes,</span><span><span class="koboSpan" id="kobo.73.1"> the</span></span><span class="koboSpan" id="kobo.74.1"> </span><strong><span class="koboSpan" id="kobo.75.1">P3</span></strong><span class="koboSpan" id="kobo.76.1"> process is chained to</span><span><span class="koboSpan" id="kobo.77.1"> the</span></span> <strong><span class="koboSpan" id="kobo.78.1">P1</span></strong><span class="koboSpan" id="kobo.79.1"> (</span><kbd><span class="koboSpan" id="kobo.80.1">UP</span></kbd><span class="koboSpan" id="kobo.81.1"> and </span><kbd><span class="koboSpan" id="kobo.82.1">DOWN</span></kbd><span class="koboSpan" id="kobo.83.1">) and </span><strong><span class="koboSpan" id="kobo.84.1">P2</span></strong><span class="koboSpan" id="kobo.85.1"> (</span><kbd><span class="koboSpan" id="kobo.86.1">RIGHT</span></kbd><span class="koboSpan" id="kobo.87.1"> and </span><kbd><span class="koboSpan" id="kobo.88.1">LEFT</span></kbd><span class="koboSpan" id="kobo.89.1">) processes, and</span><span><span class="koboSpan" id="kobo.90.1"> the</span></span> <strong><span class="koboSpan" id="kobo.91.1">P2</span></strong><span class="koboSpan" id="kobo.92.1"> process is chained to</span><span><span class="koboSpan" id="kobo.93.1"> the</span></span> <strong><span class="koboSpan" id="kobo.94.1">P3</span></strong><span class="koboSpan" id="kobo.95.1"> (</span><kbd><span class="koboSpan" id="kobo.96.1">LEFT</span></kbd><span class="koboSpan" id="kobo.97.1"> and </span><kbd><span class="koboSpan" id="kobo.98.1">RIGHT</span></kbd><span class="koboSpan" id="kobo.99.1">) and </span><strong><span class="koboSpan" id="kobo.100.1">P0</span></strong><span class="koboSpan" id="kobo.101.1"> (</span><kbd><span class="koboSpan" id="kobo.102.1">UP</span></kbd><span class="koboSpan" id="kobo.103.1"> and </span><kbd><span class="koboSpan" id="kobo.104.1">DOWN</span></kbd><span class="koboSpan" id="kobo.105.1">) processes.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">See also</span></h1>
                </header>
            
            <article>
                
<p class="packt_figure"><span class="koboSpan" id="kobo.2.1">More information on MPI can be found at </span><a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-topo.html"><span class="koboSpan" id="kobo.3.1">http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-topo.html</span></a><span class="koboSpan" id="kobo.4.1">.</span></p>


            </article>

            
        </section>
    </body></html>