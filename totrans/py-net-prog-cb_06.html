<html><head></head><body>
  <div><div><div><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Screen-scraping and Other Practical Applications</h1></div></div></div><p>In this chapter, we will cover the following topics:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Searching for business addresses using the Google Maps API</li><li class="listitem" style="list-style-type: disc">Searching for geographic coordinates using the Google Maps URL</li><li class="listitem" style="list-style-type: disc">Searching for an article in Wikipedia</li><li class="listitem" style="list-style-type: disc">Searching for Google stock quote</li><li class="listitem" style="list-style-type: disc">Searching for a source code repository at GitHub</li><li class="listitem" style="list-style-type: disc">Reading news feed from BBC</li><li class="listitem" style="list-style-type: disc">Crawling links present in a web page</li></ul></div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec60"/>Introduction</h1></div></div></div><p>This chapter shows some of the interesting Python scripts that you can write to extract useful information from the web, for example, searching for a business address, stock quote for a particular company or the latest news from a news agency website. These scripts demonstrate how Python can extract simple information in simpler ways without communicating with complex APIs.</p><p>Following these recipes, you should be able to write code for complex scenarios, for example, find the details about a business, including location, news, stock quote, and so on.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec61"/>Searching for business addresses using the Google Maps API</h1></div></div></div><p>You would like to search<a id="id377" class="indexterm"/> for the address<a id="id378" class="indexterm"/> of a well-known business in your area.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec123"/>Getting ready</h2></div></div></div><p>You can use the Python geocoding library <code class="literal">pygeocoder</code> to search for a local business. You need to install this library from <strong>PyPI</strong> with <code class="literal">pip</code> or <code class="literal">easy_install</code>, by entering <code class="literal">$ pip install pygeocoder</code> or <code class="literal">$ easy_install pygeocoder</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec124"/>How to do it...</h2></div></div></div><p>Let us find the address of Argos Ltd., a well-known UK retailer using a few lines of Python code.</p><p>Listing 6.1 gives a simple geocoding example to search for a business address, as follows:</p><div><pre class="programlisting">#!/usr/bin/env python

# Python Network Programming Cookbook -- Chapter - 6
# This program is optimized for Python 2.7.
# It may run on any other version with/without modifications.

from pygeocoder import Geocoder

def search_business(business_name):

  results = Geocoder.geocode(business_name)
  
  for result in results:
    print result

if __name__ == '__main__':
  business_name =  "Argos Ltd, London" 
  print "Searching %s" %business_name
  search_business(business_name)</pre></div><p>This recipe will print the address of Argos Ltd., as shown. The output may vary slightly based on the output from your installed geocoding library:</p><div><pre class="programlisting">
<strong>$ python 6_1_search_business_addr.py</strong>
<strong>Searching Argos Ltd, London </strong>

<strong>Argos Ltd, 110-114 King Street, London, Greater London W6 0QP, UK</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec125"/>How it works...</h2></div></div></div><p>This recipe relies on the Python third-party geocoder library.</p><p>This recipe defines a simple function <code class="literal">search_business()</code> <a id="id379" class="indexterm"/>that takes the business name as an input and passes that to the <code class="literal">geocode()</code> function. The <code class="literal">geocode()</code> function<a id="id380" class="indexterm"/> can return zero or more search<a id="id381" class="indexterm"/> results depending on your search term.</p><p>In this recipe, the <code class="literal">geocode()</code> function has got the business name Argos Ltd., London, as the search query. In return, it gives the address of Argos Ltd., which is 110-114 King Street, London, Greater London W6 0QP, UK.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec126"/>See also</h2></div></div></div><p>The <code class="literal">pygeocoder</code> library is <a id="id382" class="indexterm"/>powerful and has many interesting and useful features for geocoding. You may find more details on the developer's website at <a class="ulink" href="https://bitbucket.org/xster/pygeocoder/wiki/Home">https://bitbucket.org/xster/pygeocoder/wiki/Home</a>.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec62"/>Searching for geographic coordinates using the Google Maps URL</h1></div></div></div><p>Sometimes you'd like to have a <a id="id383" class="indexterm"/>simple function that <a id="id384" class="indexterm"/>gives the geographic coordinates of a city by giving it just the name of that city. You may not be interested in installing any third-party libraries for this simple task.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec127"/>How to do it...</h2></div></div></div><p>In this simple screen-scraping example, we use the Google Maps URL to query the latitude and longitude of a city. The URL used to query can be found after making a custom search on the Google Maps page. We can perform the following steps to extract some information from Google Maps.</p><p>Let us take the name of a city from the command line using the <code class="literal">argparse</code> module.</p><p>We can open the maps search URL using the <code class="literal">urlopen()</code> function<a id="id385" class="indexterm"/> of <code class="literal">urllib</code>. This will give an XML output if the URL is correct.</p><p>Now, process the XML output in order to get the geographic coordinates of that city.</p><p>Listing 6.2 helps finding the geographic coordinates of a city <a id="id386" class="indexterm"/>using <a id="id387" class="indexterm"/>Google Maps, as shown:</p><div><pre class="programlisting">#!/usr/bin/env python
# Python Network Programming Cookbook -- Chapter - 6
# This program is optimized for Python 2.7.
# It may run on any other version with/without modifications.
import argparse
import os
import urllib

ERROR_STRING = '&lt;error&gt;'

def find_lat_long(city):
  """ Find geographic coordinates """
  # Encode query string into Google maps URL
    url = 'http://maps.google.com/?q=' + urllib.quote(city) + 
'&amp;output=js'
    print 'Query: %s' % (url)

  # Get XML location from Google maps
    xml = urllib.urlopen(url).read()

    if ERROR_STRING in xml:
      print '\nGoogle cannot interpret the city.'
      return
    else:
    # Strip lat/long coordinates from XML
      lat,lng = 0.0,0.0
      center = xml[xml.find('{center')+10:xml.find('}',xml.find('{center'))]
      center = center.replace('lat:','').replace('lng:','')
      lat,lng = center.split(',')
      print "Latitude/Longitude: %s/%s\n" %(lat, lng)


    if __name__ == '__main__':
      parser = argparse.ArgumentParser(description='City Geocode 
Search')
      parser.add_argument('--city', action="store", dest="city", 
required=True)
      given_args = parser.parse_args() 

      print "Finding geographic coordinates of %s" 
%given_args.city
      find_lat_long(given_args.city)</pre></div><p>If you run this script, you should see something similar to the following:</p><div><pre class="programlisting">
<strong>$ python 6_2_geo_coding_by_google_maps.py --city=London </strong>
<strong>Finding geograhic coordinates of London </strong>
<strong>Query: http://maps.google.com/?q=London&amp;output=js </strong>
<strong>Latitude/Longitude: 51.511214000000002/-0.119824 </strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec128"/>How it works...</h2></div></div></div><p>This recipe takes a name of a city from the command line and passes that to the <code class="literal">find_lat_long()</code> function<a id="id388" class="indexterm"/>. This function queries the Google Maps service using the <code class="literal">urlopen()</code> function of <code class="literal">urllib</code> and gets the XML output. Then, the error string <code class="literal">'&lt;error&gt;'</code> is searched for. If that's not present, it means there are some good results.</p><p>If you print out the raw XML, it's a long stream of characters produced for the browser. In the browser, it would be<a id="id389" class="indexterm"/> interesting to display the<a id="id390" class="indexterm"/> layers in maps. But in our case, we just need the latitude and longitude.</p><p>From the raw XML, the latitude and longitude is extracted using the string method <code class="literal">find()</code>. This searches for the keyword "center". This list key possesses the geographic coordinates information. But it also contains the additional characters which are removed using the string method <code class="literal">replace()</code>.</p><p>You may try this recipe to find out the latitude/longitude of any known city of the world.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec63"/>Searching for an article in Wikipedia</h1></div></div></div><p>Wikipedia is a great site<a id="id391" class="indexterm"/> to gather information about virtually anything, <a id="id392" class="indexterm"/>for example, people, places, technology, and what not. If you like to search for something on Wikipedia from your Python script, this recipe is for you.</p><p>Here is an example:</p><div><img src="img/3463OS_06_01.jpg" alt="Searching for an article in Wikipedia"/></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec129"/>Getting ready</h2></div></div></div><p>You need to install the <code class="literal">pyyaml</code> third-party library from PyPI using <code class="literal">pip</code> or <code class="literal">easy_install</code> by entering <code class="literal">$ pip install pyyaml</code> or <code class="literal">$ easy_install pyyaml</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec130"/>How to do it...</h2></div></div></div><p>Let us search<a id="id393" class="indexterm"/> for the keyword <code class="literal">Islam</code> in Wikipedia and print each<a id="id394" class="indexterm"/> search result in one line.</p><p>Listing 6.3 explains how to search for an article in Wikipedia, as shown:</p><div><pre class="programlisting">#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Python Network Programming Cookbook -- Chapter - 6
# This program is optimized for Python 2.7.
# It may run on any other version with/without modifications

import argparse
import re
import yaml
import urllib
import urllib2

SEARCH_URL = 'http://%s.wikipedia.org/w/api.php?action=query&amp;list=search&amp;srsearch=%s&amp;sroffset=%d&amp;srlimit=%d&amp;format=yaml'

class Wikipedia:
    
  def __init__(self, lang='en'):
    self.lang = lang

  def _get_content(self, url):
    request = urllib2.Request(url)
    request.add_header('User-Agent', 'Mozilla/20.0')

    try:
      result = urllib2.urlopen(request)
      except urllib2.HTTPError, e:
        print "HTTP Error:%s" %(e.reason)
      except Exception, e:
        print "Error occurred: %s" %str(e)
      return result

  def search_content(self, query, page=1, limit=10):
    offset = (page - 1) * limit
    url = SEARCH_URL % (self.lang, urllib.quote_plus(query), 
offset, limit)
    content = self._get_content(url).read()

    parsed = yaml.load(content)
    search = parsed['query']['search']
    if not search:
    return

    results = []
    for article in search:
      snippet = article['snippet']
      snippet = re.sub(r'(?m)&lt;.*?&gt;', '', snippet)
      snippet = re.sub(r'\s+', ' ', snippet)
      snippet = snippet.replace(' . ', '. ')
      snippet = snippet.replace(' , ', ', ')
      snippet = snippet.strip()

    results.append({
      'title' : article['title'].strip(),
'snippet' : snippet
    })

    return results
 
if __name__ == '__main__':
  parser = argparse.ArgumentParser(description='Wikipedia search')
  parser.add_argument('--query', action="store", dest="query", 
required=True)
  given_args = parser.parse_args()
 
  wikipedia = Wikipedia()
  search_term = given_args.query
  print "Searching Wikipedia for %s" %search_term 
  results = wikipedia.search_content(search_term)
  print "Listing %s search results..." %len(results)
  for result in results:
    print "==%s== \n \t%s" %(result['title'], result['snippet'])
  print "---- End of search results ----"</pre></div><p>Running this recipe<a id="id395" class="indexterm"/> to query Wikipedia about Islam shows the<a id="id396" class="indexterm"/> following output:</p><div><pre class="programlisting">
<strong>$ python 6_3_search_article_in_wikipedia.py --query='Islam' </strong>
<strong>Searching Wikipedia for Islam </strong>
<strong>Listing 10 search results... </strong>
<strong>==Islam== </strong>
<strong>     Islam. (</strong>
<strong>ˈ</strong>
<strong> | </strong>
<strong>ɪ</strong>
<strong> | s | l | </strong>
<strong>ɑː</strong>
<strong> | m </strong>
<strong>الإسلام</strong>
<strong>, ar | ALA | al-</strong>
<strong>ʾ</strong>
<strong>Isl</strong>
<strong>ā</strong>
<strong>m  æl</strong>
<strong>ʔɪ</strong>
<strong>s</strong>
<strong>ˈ</strong>
<strong>læ</strong>
<strong>ː</strong>
<strong>m | IPA | ar-al_islam. ... </strong>

<strong>==Sunni Islam== </strong>
<strong>     Sunni Islam (</strong>
<strong>ˈ</strong>
<strong> | s | u</strong>
<strong>ː</strong>
<strong> | n | i or </strong>
<strong>ˈ</strong>
<strong> | s | </strong>
<strong>ʊ</strong>
<strong> | n | i |) is the </strong>
<strong>largest branch of Islam ; its adherents are referred to in Arabic as ... </strong>
<strong>==Muslim== </strong>
<strong>     A Muslim, also spelled Moslem is an adherent of Islam, a monotheistic Abrahamic religion based on the Qur'an —which Muslims consider the ... </strong>
<strong>==Sharia== </strong>
<strong>     is the moral code and religious law of Islam. Sharia deals with </strong>
<strong>many topics addressed by secular law, including crime, politics, and ... </strong>
<strong>==History of Islam== </strong>
<strong>     The history of Islam concerns the Islamic religion and its </strong>
<strong>adherents, known as Muslim s. " "Muslim" is an Arabic word meaning </strong>
<strong>"one who ... </strong>

<strong>==Caliphate== </strong>
<strong>     a successor to Islamic prophet Muhammad ) and all the Prophets </strong>
<strong>of Islam. The term caliphate is often applied to successions of </strong>
<strong>Muslim ... </strong>
<strong>==Islamic fundamentalism== </strong>
<strong>     Islamic ideology and is a group of religious ideologies seen as </strong>
<strong>advocating a return to the "fundamentals" of Islam : the Quran and </strong>
<strong>the Sunnah. ... </strong>
<strong>==Islamic architecture== </strong>
<strong>     Islamic architecture encompasses a wide range of both secular </strong>
<strong>and religious styles from the foundation of Islam to the present day. ... </strong>
<strong>---- End of search results ---- </strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec131"/>How it works...</h2></div></div></div><p>First, we collect the Wikipedia URL template for searching an article. We created a class called <code class="literal">Wikipedia</code>, which has two methods: <code class="literal">_get_content()</code> and <code class="literal">search_content()</code>. By default upon initialization, the class sets up its language attribute <code class="literal">lang</code> to <code class="literal">en</code> (English). </p><p>The command-line query string is passed to the <code class="literal">search_content()</code> method<a id="id397" class="indexterm"/>. It then constructs the actual search URL by inserting variables such as language, query string, page offset, and number<a id="id398" class="indexterm"/> of results to return. The <code class="literal">search_content()</code> method can <a id="id399" class="indexterm"/>optionally take the parameters and the offset is determined by the <code class="literal">(page -1)  * limit</code> expression.</p><p>The content of the search result is fetched via the <code class="literal">_get_content()</code> method<a id="id400" class="indexterm"/> which calls the <code class="literal">urlopen()</code> function of <code class="literal">urllib</code>. In the search URL, we set up the result format <code class="literal">yaml</code>, which is basically intended for plain text files. The <code class="literal">yaml</code> search result is then parsed with Python's <code class="literal">pyyaml</code> library.</p><p>The search result is processed by substituting the regular expressions found in each result item. For example, the <code class="literal">re.sub(r'(?m)&lt;.*?&gt;', '', snippet)</code> expression takes the snippet string and replaces a raw pattern <code class="literal">(?m)&lt;.*?&gt;)</code>. To learn more about regular expressions, visit the Python document page, available at <a class="ulink" href="http://docs.python.org/2/howto/regex.html">http://docs.python.org/2/howto/regex.html</a>.</p><p>In Wikipedia terminology, each article has a snippet or a short description. We create a list of dictionary items where each item contains the title and the snippet of each search result. The results are printed on the screen by looping through this list of dictionary items.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec64"/>Searching for Google stock quote</h1></div></div></div><p>If the stock quote of any <a id="id401" class="indexterm"/>company is of interest to you, this recipe can help you to find today's stock quote of that company.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec132"/>Getting ready</h2></div></div></div><p>We assume that you already<a id="id402" class="indexterm"/> know the symbol used by your favorite company to enlist itself on any stock exchange. If you don't know, get the symbol from the company website or just search in Google.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec133"/>How to do it...</h2></div></div></div><p>Here, we use <a id="id403" class="indexterm"/>Google Finance (<a class="ulink" href="http://finance.google.com/">http://finance.google.com/</a>) to search for the stock quote of a given company. You can input the symbol via the command line, as shown in the next code.</p><p>Listing 6.4 describes how to search for Google stock quote, as shown:</p><div><pre class="programlisting">#!/usr/bin/env python
# Python Network Programming Cookbook -- Chapter - 6
# This program is optimized for Python 2.7.
# It may run on any other version with/without modifications. 

import argparse
import urllib
import re
from datetime import datetime

SEARCH_URL = 'http://finance.google.com/finance?q='
 
def get_quote(symbol):
  content = urllib.urlopen(SEARCH_URL + symbol).read()
  m = re.search('id="ref_694653_l".*?&gt;(.*?)&lt;', content)
  if m:
    quote = m.group(1)
  else:
    quote = 'No quote available for: ' + symbol
  return quote

if __name__ == '__main__':
  parser = argparse.ArgumentParser(description='Stock quote 
search')
  parser.add_argument('--symbol', action="store", dest="symbol", 
required=True)
  given_args = parser.parse_args() 
  print "Searching stock quote for symbol '%s'" %given_args.symbol 
  print "Stock  quote for %s at %s: %s" %(given_args.symbol , 
datetime.today(),  get_quote(given_args.symbol))</pre></div><p>If you run this script, you will see an output similar to the following. Here, the stock quote for Google is searched by inputting the symbol <code class="literal">goog</code>, as shown:</p><div><pre class="programlisting">
<strong>$ python 6_4_google_stock_quote.py --symbol=goog </strong>
<strong>Searching stock quote for symbol 'goog' </strong>
<strong>Stock quote for goog at 2013-08-20 18:50:29.483380: 868.86 </strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec134"/>How it works...</h2></div></div></div><p>This recipe uses the <code class="literal">urlopen()</code> function of <code class="literal">urllib</code> to get the stock data from the Google Finance website.</p><p>By using <a id="id404" class="indexterm"/>the regular expression <a id="id405" class="indexterm"/>library <code class="literal">re</code>, it locates the stock quote data in the first group of items. The <code class="literal">search()</code> function of <code class="literal">re</code> is powerful enough to search the content and filter the ID data of that particular company.</p><p>Using this recipe, we searched for the stock quote of Google, which was <code class="literal">868.86</code> on August 20, 2013.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec65"/>Searching for a source code repository at GitHub</h1></div></div></div><p>As a Python programmer, you <a id="id406" class="indexterm"/>may already be familiar with GitHub<a id="id407" class="indexterm"/> (<a class="ulink" href="http://www.github.com">http://www.github.com</a>), a source code-sharing website, <a id="id408" class="indexterm"/>as shown in the following screenshot. You can share your source code privately to a team or publicly to the world using GitHub. It has a nice API interface to query about any source code repository. This recipe may give you a starting point to create your own source code search engine.</p><div><img src="img/3463OS_06_02.jpg" alt="Searching for a source code repository at GitHub"/></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec135"/>Getting ready</h2></div></div></div><p>To run this recipe, you need to install the third-party Python library <code class="literal">requests</code> by entering <code class="literal">$ pip install requests</code> or <code class="literal">$ easy_install requests</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec136"/>How to do it...</h2></div></div></div><p>We would like to define a <code class="literal">search_repository()</code> function<a id="id409" class="indexterm"/> that will take the name of author <a id="id410" class="indexterm"/>(also known as coder), <a id="id411" class="indexterm"/>repository, and search key. In return, it will give us back the available result against the search key. From the GitHub API, the following are the available search keys: <code class="literal">issues_url</code>, <code class="literal">has_wiki</code>, <code class="literal">forks_url</code>, <code class="literal">mirror_url</code>, <code class="literal">subscription_url</code>, <code class="literal">notifications_url</code>, <code class="literal">collaborators_url</code>, <code class="literal">updated_at</code>, <code class="literal">private</code>, <code class="literal">pulls_url</code>, <code class="literal">issue_comment_url</code>, <code class="literal">labels_url</code>, <code class="literal">full_name</code>, <code class="literal">owner</code>, <code class="literal">statuses_url</code>, <code class="literal">id</code>, <code class="literal">keys_url</code>, <code class="literal">description</code>, <code class="literal">tags_url</code>, <code class="literal">network_count</code>, <code class="literal">downloads_url</code>, <code class="literal">assignees_url</code>, <code class="literal">contents_url</code>, <code class="literal">git_refs_url</code>, <code class="literal">open_issues_count</code>, <code class="literal">clone_url</code>, <code class="literal">watchers_count</code>, <code class="literal">git_tags_url</code>, <code class="literal">milestones_url</code>, <code class="literal">languages_url</code>, <code class="literal">size</code>, <code class="literal">homepage</code>, <code class="literal">fork</code>, <code class="literal">commits_url</code>, <code class="literal">issue_events_url</code>, <code class="literal">archive_url</code>, <code class="literal">comments_url</code>, <code class="literal">events_url</code>, <code class="literal">contributors_url</code>, <code class="literal">html_url</code>, <code class="literal">forks</code>, <code class="literal">compare_url</code>, <code class="literal">open_issues</code>, <code class="literal">git_url</code>, <code class="literal">svn_url</code>, <code class="literal">merges_url</code>, <code class="literal">has_issues</code>, <code class="literal">ssh_url</code>, <code class="literal">blobs_url</code>, <code class="literal">master_branch</code>, <code class="literal">git_commits_url</code>, <code class="literal">hooks_url</code>, <code class="literal">has_downloads</code>, <code class="literal">watchers</code>, <code class="literal">name</code>, <code class="literal">language</code>, <code class="literal">url</code>, <code class="literal">created_at</code>, <code class="literal">pushed_at</code>, <code class="literal">forks_count</code>, <code class="literal">default_branch</code>, <code class="literal">teams_url</code>, <code class="literal">trees_url</code>, <code class="literal">organization</code>, <code class="literal">branches_url</code>, <code class="literal">subscribers_url</code>, and <code class="literal">stargazers_url</code>.</p><p>Listing 6.5 gives the code to <a id="id412" class="indexterm"/>search for details of a <a id="id413" class="indexterm"/>source code repository at GitHub, as shown:</p><div><pre class="programlisting">#!/usr/bin/env python
# Python Network Programming Cookbook -- Chapter - 6
# This program is optimized for Python 2.7.
# It may run on any other version with/without modifications.

SEARCH_URL_BASE = 'https://api.github.com/repos'

import argparse
import requests
import json

def search_repository(author, repo, search_for='homepage'):
  url = "%s/%s/%s" %(SEARCH_URL_BASE, author, repo)
  print "Searching Repo URL: %s" %url
  result = requests.get(url)
  if(result.ok):
    repo_info = json.loads(result.text or result.content)
    print "Github repository info for: %s" %repo
    result = "No result found!"
    keys = [] 
    for key,value in repo_info.iteritems():
      if  search_for in key:
          result = value
      return result

if __name__ == '__main__':
  parser = argparse.ArgumentParser(description='Github search')
  parser.add_argument('--author', action="store", dest="author", 
required=True)
  parser.add_argument('--repo', action="store", dest="repo", 
required=True)
  parser.add_argument('--search_for', action="store", 
dest="search_for", required=True)

  given_args = parser.parse_args() 
  result = search_repository(given_args.author, given_args.repo, 
given_args.search_for)
  if isinstance(result, dict):
    print "Got result for '%s'..." %(given_args.search_for)
    for key,value in result.iteritems():
    print "%s =&gt; %s" %(key,value)
  else:
    print "Got result for %s: %s" %(given_args.search_for, 
result)</pre></div><p>If you run this script to<a id="id414" class="indexterm"/> search for the owner of the<a id="id415" class="indexterm"/> Python web framework Django, you can get the following result:</p><div><pre class="programlisting">
<strong>$ python 6_5_search_code_github.py --author=django --repo=django --search_for=owner </strong>
<strong>Searching Repo URL: https://api.github.com/repos/django/django </strong>
<strong>Github repository info for: django </strong>
<strong>Got result for 'owner'... </strong>
<strong>following_url =&gt; https://api.github.com/users/django/following{/other_user} </strong>
<strong>events_url =&gt; https://api.github.com/users/django/events{/privacy} </strong>
<strong>organizations_url =&gt; https://api.github.com/users/django/orgs </strong>
<strong>url =&gt; https://api.github.com/users/django </strong>
<strong>gists_url =&gt; https://api.github.com/users/django/gists{/gist_id} </strong>
<strong>html_url =&gt; https://github.com/django </strong>
<strong>subscriptions_url =&gt; https://api.github.com/users/django/subscriptions </strong>
<strong>avatar_url =&gt; https://1.gravatar.com/avatar/fd542381031aa84dca86628ece84fc07?d=https%3A%2F%2Fidenticons.github.com%2Fe94df919e51ae96652259468415d4f77.png </strong>
<strong>repos_url =&gt; https://api.github.com/users/django/repos </strong>
<strong>received_events_url =&gt; https://api.github.com/users/django/received_events </strong>
<strong>gravatar_id =&gt; fd542381031aa84dca86628ece84fc07 </strong>
<strong>starred_url =&gt; https://api.github.com/users/django/starred{/owner}{/repo} </strong>
<strong>login =&gt; django </strong>
<strong>type =&gt; Organization </strong>
<strong>id =&gt; 27804 </strong>
<strong>followers_url =&gt; https://api.github.com/users/django/followers </strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec137"/>How it works...</h2></div></div></div><p>This script takes three command-line arguments: repository author (<code class="literal">--author</code>), repository name (<code class="literal">--repo</code>), and the item to search for (<code class="literal">--search_for</code>). The arguments are processed by the <code class="literal">argpase</code> module.</p><p>Our <code class="literal">search_repository()</code> function<a id="id416" class="indexterm"/> appends the command-line arguments to a fixed search URL and receives the content by calling the <code class="literal">requests</code> module's <code class="literal">get()</code> function.</p><p>The search results are, by default, returned in the JSON format. This content is then processed with the <code class="literal">json</code> module's <code class="literal">loads()</code> method. The search key is then looked for inside the result and the<a id="id417" class="indexterm"/> corresponding value of that key is returned back to the caller of the <code class="literal">search_repository()</code> function.</p><p>In the main user code, <a id="id418" class="indexterm"/>we check whether the search result is an instance of the Python dictionary. If yes, then the key/values are printed iteratively. Otherwise, the value is printed.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec66"/>Reading news feed from BBC</h1></div></div></div><p>If you are developing a <a id="id419" class="indexterm"/>social networking website with news and stories, you may <a id="id420" class="indexterm"/>be interested to present the news from various world news agencies, such as BBC and Reuters. Let us try to read news from BBC via a Python script.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec138"/>Getting ready</h2></div></div></div><p>This recipe relies on Python's third-party <code class="literal">feedparser</code> library. You can install this by running the following command:</p><div><pre class="programlisting">
<strong>$ pip install feedparser</strong>
</pre></div><p>Or</p><div><pre class="programlisting">
<strong>$ easy_install feedparser</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec139"/>How to do it...</h2></div></div></div><p>First, we collect the BBC's news feed URL from the BBC website. This URL can be used as a template to search news on various types, such as world, UK, health, business, and technology. So, we can take the type of news to display as user input. Then, we depend on the <code class="literal">read_news()</code> function<a id="id421" class="indexterm"/>, which will fetch the news from the BBC.</p><p>Listing 6.6 explains how to read news feed from the BBC, as shown in the following code:</p><div><pre class="programlisting">#!/usr/bin/env python
# Python Network Programming Cookbook -- Chapter - 6
# This program is optimized for Python 2.7. 
# It may run on any other version with/without modifications.


from datetime import datetime
import feedparser 
BBC_FEED_URL = 'http://feeds.bbci.co.uk/news/%s/rss.xml'

def read_news(feed_url):
  try:
    data = feedparser.parse(feed_url)
  except Exception, e:
    print "Got error: %s" %str(e)

  for entry in data.entries:
    print(entry.title)
    print(entry.link)
    print(entry.description)
    print("\n") 

if __name__ == '__main__':
  print "==== Reading technology news feed from bbc.co.uk 
(%s)====" %datetime.today()

  print "Enter the type of news feed: "
  print "Available options are: world, uk, health, sci-tech, 
business, technology"
  type = raw_input("News feed type:")
  read_news(BBC_FEED_URL %type)
  print "==== End of BBC news feed ====="</pre></div><p>Running this script will <a id="id422" class="indexterm"/>show you the available news categories. If we<a id="id423" class="indexterm"/> choose technology as the category, you can get the latest news on technology, as shown in the following command:</p><div><pre class="programlisting">
<strong>$ python 6_6_read_bbc_news_feed.py </strong>
<strong>==== Reading technology news feed from bbc.co.uk (2013-08-20 19:02:33.940014)==== </strong>
<strong>Enter the type of news feed:</strong>
<strong>Available options are: world, uk, health, sci-tech, business, technology </strong>
<strong>News feed type:technology </strong>
<strong>Xbox One courts indie developers </strong>
<strong>http://www.bbc.co.uk/news/technology-23765453#sa-ns_mchannel=rss&amp;ns_source=PublicRSS20-sa </strong>
<strong>Microsoft is to give away free Xbox One development kits to encourage independent developers to self-publish games for its forthcoming console. </strong>

<strong>Fast in-flight wi-fi by early 2014 </strong>
<strong>http://www.bbc.co.uk/news/technology-23768536#sa-ns_mchannel=rss&amp;ns_source=PublicRSS20-sa </strong>
<strong>Passengers on planes, trains and ships may soon be able to take advantage of high-speed wi-fi connections, says Ofcom. </strong>

<strong>Anonymous 'hacks council website' </strong>
<strong>http://www.bbc.co.uk/news/uk-england-surrey-23772635#sa-ns_mchannel=rss&amp;ns_source=PublicRSS20-sa </strong>
<strong>A Surrey council blames hackers Anonymous after references to a Guardian journalist's partner detained at Heathrow Airport appear on its website. </strong>

<strong>Amazon.com website goes offline </strong>
<strong>http://www.bbc.co.uk/news/technology-23762526#sa-ns_mchannel=rss&amp;ns_source=PublicRSS20-sa </strong>
<strong>Amazon's US website goes offline for about half an hour, the latest high-profile internet firm to face such a problem in recent days. </strong>

<strong>[TRUNCATED]</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec140"/>How it works...</h2></div></div></div><p>In this recipe, the <code class="literal">read_news()</code> function<a id="id424" class="indexterm"/> relies on Python's third-party module <code class="literal">feedparser</code>. The <code class="literal">feedparser</code> module's <code class="literal">parser()</code> method returns the feed data <a id="id425" class="indexterm"/>in a structured fashion.</p><p>In this recipe, the <code class="literal">parser()</code> method parses<a id="id426" class="indexterm"/> the given feed URL. This URL is constructed from <code class="literal">BBC_FEED_URL</code> and user input.</p><p>After some valid feed<a id="id427" class="indexterm"/> data is obtained by calling <code class="literal">parse()</code>, the contents of the data is then printed, such as title, link, and description, of each feed entry.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec67"/>Crawling links present in a web page</h1></div></div></div><p>At times you would like to find a specific keyword present in a web page. In a web browser, you can use the<a id="id428" class="indexterm"/> browser's in-page search facility to locate the terms. Some browsers can highlight it. In a complex situation, you would like to dig deep and follow every URL present in a web page and find that specific term. This recipe will automate that task for you.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec141"/>How to do it...</h2></div></div></div><p>Let us write a <code class="literal">search_links()</code> function<a id="id429" class="indexterm"/> that will take three arguments: the search URL, the depth of the recursive search, and the search key/term, since every URL may have links present in the content and that content may have more URLs to crawl. To limit the recursive search, we define a depth. Upon reaching that depth, no more recursive search will be done.</p><p>Listing 6.7 gives the code for crawling <a id="id430" class="indexterm"/>links present in a web page, as shown in the following code:</p><div><pre class="programlisting">#!/usr/bin/env python
# Python Network Programming Cookbook -- Chapter - 6
# This program is optimized for Python 2.7.
# It may run on any other version with/without modifications.

import argparse
import sys
import httplib
import re

processed = []

def search_links(url, depth, search):
  # Process http links that are not processed yet
  url_is_processed = (url in processed)
  if (url.startswith("http://") and (not url_is_processed)):
    processed.append(url)
    url = host = url.replace("http://", "", 1)
    path = "/"

    urlparts = url.split("/")
    if (len(urlparts) &gt; 1):
      host = urlparts[0]
      path = url.replace(host, "", 1)

     # Start crawling
     print "Crawling URL path:%s%s " %(host, path)
     conn = httplib.HTTPConnection(host)
     req = conn.request("GET", path)
     result = conn.getresponse()

    # find the links
    contents = result.read()
    all_links = re.findall('href="(.*?)"', contents)

    if (search in contents):
      print "Found " + search + " at " + url

      print " ==&gt; %s: processing %s links" %(str(depth), 
str(len(all_links)))
      for href in all_links:
      # Find relative urls
      if (href.startswith("/")):
        href = "http://" + host + href

        # Recurse links
        if (depth &gt; 0):
          search_links(href, depth-1, search)
    else:
      print "Skipping link: %s ..." %url


if __name__ == '__main__':
  parser = argparse.ArgumentParser(description='Webpage link 
crawler')
  parser.add_argument('--url', action="store", dest="url", 
required=True)
  parser.add_argument('--query', action="store", dest="query", 
required=True)
  parser.add_argument('--depth', action="store", dest="depth", 
default=2)

  given_args = parser.parse_args() 

  try:
    search_links(given_args.url,  
given_args.depth,given_args.query)
    except KeyboardInterrupt:
      print "Aborting search by user request."</pre></div><p>If you run this script <a id="id431" class="indexterm"/>to search <a class="ulink" href="http://www.python.org">www.python.org</a> for <code class="literal">python</code>, you will see an output similar to the following:</p><div><pre class="programlisting">
<strong>$ python 6_7_python_link_crawler.py --url='http://python.org' --query='python' </strong>
<strong>Crawling URL path:python.org/ </strong>
<strong>Found python at python.org </strong>
<strong> ==&gt; 2: processing 123 links </strong>
<strong>Crawling URL path:www.python.org/channews.rdf </strong>
<strong>Found python at www.python.org/channews.rdf </strong>
<strong> ==&gt; 1: processing 30 links </strong>
<strong>Crawling URL path:www.python.org/download/releases/3.4.0/ </strong>
<strong>Found python at www.python.org/download/releases/3.4.0/ </strong>
<strong> ==&gt; 0: processing 111 links </strong>
<strong>Skipping link: https://ep2013.europython.eu/blog/2013/05/15/epc20145-call-proposals ... </strong>
<strong>Crawling URL path:www.python.org/download/releases/3.2.5/ </strong>
<strong>Found python at www.python.org/download/releases/3.2.5/ </strong>
<strong> ==&gt; 0: processing 113 links </strong>
<strong>...</strong>
<strong>Skipping link: http://www.python.org/download/releases/3.2.4/ ... </strong>
<strong>Crawling URL path:wiki.python.org/moin/WikiAttack2013 </strong>
<strong>^CAborting search by user request. </strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec142"/>How it works...</h2></div></div></div><p>This recipe can take three command-line inputs: search URL (<code class="literal">--url</code>), the query string (<code class="literal">--query</code>), and the depth of recursion (<code class="literal">--depth</code>). These inputs are processed by the <code class="literal">argparse</code> module.</p><p>When the <code class="literal">search_links()</code> function<a id="id432" class="indexterm"/> is called with the previous arguments, this will recursively iterate on all the<a id="id433" class="indexterm"/> links found on that given web page. If it takes too long to finish, you would like to exit prematurely. For this reason, the <code class="literal">search_links()</code> function is placed inside a try-catch block which can catch the user's keyboard interrupt action, such as <em>Ctrl</em> + <em>C</em>.</p><p>The <code class="literal">search_links()</code> function keeps track of visited links via a list called <code class="literal">processed</code>. This is made global to give access to all the recursive function calls.</p><p>In a single instance of search, it is ensured that only HTTP URLs are processed in order to avoid the potential SSL certificate errors. The URL is split into a host and a path. The main crawling is initiated using the <code class="literal">HTTPConnection()</code> function<a id="id434" class="indexterm"/> of <code class="literal">httplib</code>. It gradually makes a <code class="literal">GET</code> request and a response is then processed using the regular expression module <code class="literal">re</code>. This collects all the links from the response. Each response is then examined for the search term. If the search term is found, it prints that incident.</p><p>The collected links are visited recursively in the same way. If any relative URL is found, that instance is converted into a full URL by prefixing <code class="literal">http://</code> to the host and the path. If the depth of search is greater than 0, the recursion is activated. It reduces the depth by 1 and runs the search function again. When the search depth becomes 0, the recursion ends.</p></div></div></div>
</body></html>