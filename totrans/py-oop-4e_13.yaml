- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Testing Object-Oriented Programs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Skilled Python programmers agree that testing is one of the most important
    aspects of software development. Even though this chapter is placed near the end
    of the book, it is not an afterthought; everything we have studied so far will
    help us when writing tests. In this chapter, we''ll look at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The importance of unit testing and test-driven development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standard library `unittest` module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pytest` tool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `mock` module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case study for this chapter, we'll focus – no surprise – on writing some
    tests for the case study examples.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start with some of the fundamental reasons why automated software testing
    is so important.
  prefs: []
  type: TYPE_NORMAL
- en: Why test?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many programmers already know how important it is to test their code. If you're
    among them, feel free to skim this section. You'll find the next section – where
    we actually see how to create tests in Python – much more scintillating.
  prefs: []
  type: TYPE_NORMAL
- en: If you're not convinced of the importance of testing, we remind you that without
    any tests, code will be broken, and no one has any way to know it. Read on!
  prefs: []
  type: TYPE_NORMAL
- en: Some people argue that testing is more important in Python code because of its
    dynamic nature; compiled languages such as Java and C++ are occasionally thought
    to be somehow *safer* because they enforce type checking at compile time. However,
    Python tests rarely check types. They check values. They make sure that the right
    attributes have been set at the right time or that the sequence has the right
    length, order, and values. These higher-level concepts need to be tested in any
    language. The real reason Python programmers test more than programmers of other
    languages is that it is so easy to test in Python!
  prefs: []
  type: TYPE_NORMAL
- en: But why test? Do we really need to test? What if we didn't test? To answer those
    questions, reflect on the last time you wrote any code. Did it run correctly the
    first time? Free of syntax errors? Free of logic problems? It's possible, in principle,
    to type in code that's perfect once in a while. As a practical matter, the number
    of obvious syntax errors that had to be corrected is an indicator that perhaps
    there are more subtle logic errors that also had to be corrected.
  prefs: []
  type: TYPE_NORMAL
- en: We don't need a formal, separate test to make sure our code works. Running the
    program, as we generally do, and fixing the errors is a crude form of testing.
    Python's interactive interpreter and near-zero compile times makes it easy to
    write a few lines of code and run the program to make sure those lines are doing
    what is expected. While acceptable at the beginning of a project, this turns into
    a liability that grows over time. Attempting to change a few lines of code can
    affect parts of the program that we haven't realized will be influenced by the
    changes, and without tests, we don't know what we broke. Attempts at redesigns
    or even small optimization rewrites can be plagued with problems. Furthermore,
    as a program grows, the number of paths that the interpreter can take through
    that code also grows, and it quickly becomes impossible or a crude manual test
    to exercise all of them.
  prefs: []
  type: TYPE_NORMAL
- en: To assure ourselves and others that our software works, we write automated tests.
    These are programs that automatically run certain inputs through other programs
    or parts of programs. We can run these test programs in seconds and cover far
    more potential input situations than one programmer would think to test every
    time they change something.
  prefs: []
  type: TYPE_NORMAL
- en: Software features that can't be demonstrated by automated tests simply don't
    exist.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- Extreme Programming Explained, Kent Beck'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'There are four main reasons to write tests:'
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that code is working the way the developer thinks it should
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To ensure that code continues working when we make changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To ensure that the developer understood the requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To ensure that the code we are writing has a maintainable interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we have automated tests, we can run them every time we change code, whether
    it is during initial development or maintenance releases. Testing can confirm
    that we didn't inadvertently break anything when adding or extending features.
  prefs: []
  type: TYPE_NORMAL
- en: The last two of the preceding points have interesting consequences. When we
    write tests, it helps us design the API, interface, or pattern that code takes.
    Thus, if we misunderstood the requirements, writing a test can help highlight
    the misunderstanding. From the other side, if we're not certain how we want to
    design a class, we can write a test that interacts with that class so we have
    an idea of the most natural way to confirm that the interface works. In fact,
    it is often beneficial to write the tests before we write the code we are testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some other interesting consequences of focusing on software testing.
    We''ll look at three of these consequences:'
  prefs: []
  type: TYPE_NORMAL
- en: Using tests to drive development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing different objectives for testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having a consistent pattern for test scenarios
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with using tests to drive the development effort.
  prefs: []
  type: TYPE_NORMAL
- en: Test-driven development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Write tests first* is the mantra of test-driven development. Test-driven development
    takes the *untested code is broken code* concept one step further and suggests
    that only unwritten code should be untested. We don''t write any code until after
    we have written the tests that will prove it works. The first time we run a test,
    it should fail, since the code hasn''t been written. Then, we write the code that
    ensures the test passes, and then write another test for the next segment of code.'
  prefs: []
  type: TYPE_NORMAL
- en: Test-driven development can be fun; it allows us to build little puzzles to
    solve. Then, we implement the code to solve those puzzles. After that, we make
    a more complicated puzzle, and we write code that solves the new puzzle without
    unsolving the previous one.
  prefs: []
  type: TYPE_NORMAL
- en: There are two goals of the test-driven methodology. The first is to ensure that
    tests really get written.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, writing tests first forces us to consider exactly how the code will
    be used. It tells us what methods objects need to have and how attributes will
    be accessed. It helps us break up the initial problem into smaller, testable problems,
    and then recombine the tested solutions into larger, also tested, solutions. Writing
    tests can thus become a part of the design process. Often, when we're writing
    a test for a new object, we discover anomalies in the design that force us to
    consider new aspects of the software.
  prefs: []
  type: TYPE_NORMAL
- en: Testing makes software better. Writing tests before we release the software
    makes it better before the final code is written.
  prefs: []
  type: TYPE_NORMAL
- en: All of the code examined in the book has been run through an automated test
    suite. It's the only way to be absolutely sure the examples are rock-solid, working
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Testing objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have a number of distinct objectives for running tests. These are often
    called types of testing, but the word "type" is heavily overused in the software
    industry. In this chapter, we''ll look at only two of these testing goals:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unit tests** confirm that software components work in isolation. We''ll focus
    on this first, since Fowler''s Test Pyramid seems to suggest unit testing creates
    the most value. If the various classes and functions each adhere to their interfaces
    and produce the expected results, then integrating them is also going to work
    nicely and have relatively few surprises. It''s common to use the **coverage**
    tool to be sure all the lines of code are exercised as part of the unit test suite.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration tests** – unsurprisingly – confirm software components work when
    integrated. Integration tests are sometimes called system tests, functional tests,
    and acceptance tests, among others. When an integration test fails, it often means
    an interface wasn''t defined properly, or a unit test didn''t include some edge
    case that''s exposed through the integration with other components. Integration
    testing seems to depend on having good unit testing, making it secondary in importance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We note that "unit" isn't formally defined by the Python language. This is an
    intentional choice. A unit of code is often a single function or a single class.
    It can be a single module, also. The definition gives us a little flexibility
    to identify isolated, individual units of code.
  prefs: []
  type: TYPE_NORMAL
- en: While there are many distinct objectives for tests, the techniques used tend
    to be similar. For additional material, see [https://www.softwaretestinghelp.com/types-of-software-testing/](https://www.softwaretestinghelp.com/types-of-software-testing/)
    for a list of over 40 different types of testing objectives; this is overwhelming,
    which is why we will only focus on unit tests and integration tests. All tests
    have a common pattern to them, and we'll look at a general pattern of testing
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Testing patterns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Writing code is often challenging. We need to figure out what the internal state
    of the object is, what state changes it undergoes, and determine the other objects
    it collaborates with. Throughout the book, we've provided a number of common patterns
    for designing classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tests, in a way, are simpler than class definitions, and all have essentially
    the same pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In some cases, the preconditions can be complex or perhaps the state changes
    or side effects are complex. They might be so complex that we have to break them
    into multiple steps. What''s important about this three-part pattern is how it
    disentangles the setup, execution, and expected results from each other. This
    model applies to a wide variety of tests. If we want to make sure the water''s
    hot enough to make another cup of tea, we''ll follow a similar set of steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '`GIVEN` a kettle of water on the stove'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AND` the burner is off'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WHEN` we flip open the lid on the kettle'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`THEN` we see steam escaping'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This pattern is quite handy for making sure we have a clear setup and an observable
    result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we need to write a function to compute an average of a list of numbers,
    excluding `None` values that might be in the sequence. We might start out like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We've roughed out a definition of the function, with a summary of how we think
    it should behave. The GIVEN step defines some data for our test case. The WHEN
    step defines precisely what we're going to be doing. Finally, the THEN step describes
    the expected results. The automated test tool can compare actual results against
    the stated expectation and report back if the test fails. We can then refine this
    into a separate test class or function using our preferred test framework. The
    ways unittest and pytest implement the concept differ slightly, but the core concept
    remains in both frameworks. Once that's done, the test should fail and we can
    start implementing the real code, given this test as a clear goal line we want
    to cross.
  prefs: []
  type: TYPE_NORMAL
- en: Some techniques that can help design test cases are **equivalence partitioning**
    and **boundary value analysis**. These help us decompose the domain of all possible
    inputs to a method or function into partitions. A common example is locating two
    partitions, "valid data" and "invalid data." Given the partitions, the values
    at the boundaries of the partitions become interesting values to use in test cases.
    See [https://www.softwaretestinghelp.com/what-is-boundary-value-analysis-and-equivalence-partitioning/](https://www.softwaretestinghelp.com/what-is-boundary-value-analysis-and-equivalence-partitioning/)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start by looking at the built-in testing framework, `unittest`. It has
    a disadvantage of being a bit wordy and complicated looking. It has the advantage
    of being built-in and usable immediately; no further installs are required.
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing with unittest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start our exploration with Python's built-in test library. This library
    provides a common object-oriented interface for *unit tests*. The Python library
    for this is called, unsurprisingly, `unittest`. It provides several tools for
    creating and running unit tests, the most important being the `TestCase` class.
    (The names follow a Java naming style, so many of the method names don't look
    very Pythonic.) The `TestCase` class provides a set of methods that allow us to
    compare values, set up tests, and clean up when they have finished.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we want to write a set of unit tests for a specific task, we create a
    subclass of `TestCase` and write individual methods to do the actual testing.
    These methods must all start with the name `test`. When this convention is followed,
    the tests automatically run as part of the test process. For simple examples,
    we can bundle the `GIVEN`, `WHEN`, and `THEN` concepts into the test method. Here''s
    a very simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This code subclasses the `TestCase` class and adds a method that calls the `TestCase.assertEqual()`
    method. The `GIVEN` step is a pair of values, 1 and 1.0\. The `WHEN` step is a
    kind of degenerate example because there's no new object created and no state
    change happening. The `THEN` step is the assertion that the two values will test
    as equal.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run the test case, this method will either succeed silently or it will
    raise an exception, depending on whether the two parameters are equal. If we run
    this code, the `main` function from `unittest` will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Did you know that floats and integers can be compared as equal?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add a failing test, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this code is more sinister, as integers and strings are not considered
    equal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The dot on the first line indicates that the first test (the one we wrote before)
    passed successfully; the letter `F` after it shows that the second test failed.
    Then, at the end, it gives us some informative summary telling us how and where
    the test failed, along with a count of the number of failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even the OS-level return code provides a useful summary. The return code is
    zero if all tests pass and non-zero if any tests fail. This helps when building
    continuous integration tools: if the `unittest` run fails, the proposed change
    shouldn''t be permitted.'
  prefs: []
  type: TYPE_NORMAL
- en: We can have as many test methods on one `TestCase` class as we like. As long
    as the method name begins with `test`, the test runner will execute each one as
    a separate, isolated test.
  prefs: []
  type: TYPE_NORMAL
- en: Each test should be completely independent of other tests.
  prefs: []
  type: TYPE_NORMAL
- en: Results or calculations from a test should have no impact on any other test.
  prefs: []
  type: TYPE_NORMAL
- en: In order to keep tests isolated from each other, we may have several tests with
    a common `GIVEN`, implemented by a common `setUp()` method. This suggests that
    we'll often have classes that are similar, and we'll need to use inheritance to
    design the tests so they can share features and still remain completely independent.
  prefs: []
  type: TYPE_NORMAL
- en: The key to writing good unit tests is keeping each test method as short as possible,
    testing a small unit of code with each test case. If our code does not seem to
    naturally break up into small, testable units, it's probably a sign that the code
    needs to be redesigned. The *Imitating objects using Mocks* section, later in
    this chapter, provides a way to isolate objects for testing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: The `unittest` module imposes a requirement to structure tests as a class definition.
    This is – in some ways – a bit of overhead. The `pytest` package has slightly
    more clever test discovery and a slightly more flexible way to construct tests
    as functions instead of methods of a class. We'll look at `pytest` next.
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing with pytest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can create unit tests using a library that provides a common framework for
    the test scenarios, along with a test runner to execute the tests and log results.
    Unit tests focus on testing the least amount of code possible in any one test.
    The standard library includes the `unittest` package. While widely used, this
    package tends to force us to create a fair amount of boilerplate code for each
    test case.
  prefs: []
  type: TYPE_NORMAL
- en: One of the more popular alternatives to the standard library `unittest` is `pytest`.
    This has the advantage of letting us write smaller, and more clear, test cases.
    The lack of overheads makes this a desirable alternative.
  prefs: []
  type: TYPE_NORMAL
- en: Since `pytest` is not part of the standard library, you'll need to download
    and install it yourself. You can get it from the `pytest` home page at [https://docs.pytest.org/en/stable/](https://docs.pytest.org/en/stable/).
    You can install it with any of the installers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a Terminal window, activate the virtual environment you''re working in.
    (If you''re using venv, for example, you might use `python -m venv c:\path\to\myenv`.)
    Then, use an OS command like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The Windows command should be the same as the command on macOS and Linux.
  prefs: []
  type: TYPE_NORMAL
- en: The `pytest` tool can use a substantially different test layout from the `unittest`
    module. It doesn't require test cases to be subclasses of `unittest.TestCase`.
    Instead, it takes advantage of the fact that Python functions are first-class
    objects and allows any properly named function to behave like a test. Rather than
    providing a bunch of custom methods for asserting equality, it uses the `assert` statement
    to verify results. This makes tests simpler, more readable, and, consequently,
    easier to maintain.
  prefs: []
  type: TYPE_NORMAL
- en: When we run `pytest`, it starts in the current folder and searches for any modules
    or sub packages with names beginning with the characters `test_`. (Including the
    `_` character.) If any functions in this module also start with `test` (no `_`
    required), they will be executed as individual tests. Furthermore, if there are
    any classes in the module whose name starts with `Test`, any methods on that class
    that start with `test_` will also be executed in the test environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'It also searches in a folder named – unsurprisingly – `tests`. Because of this,
    it''s common to have code broken up into two folders: the `src/` directory contains
    the working module, library, or application, while the `tests/` directory contains
    all the test cases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the following code, let''s port the simple `unittest` example we wrote
    earlier to `pytest`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For the same test, we've written two lines of more readable code, in comparison
    to the six lines required in our first `unittest` example.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we are not forbidden from writing class-based tests. Classes can be
    useful for grouping related tests together or for tests that need to access related
    attributes or methods on the class. The following example shows an extended class
    with a passing and a failing test; we''ll see that the error output is more comprehensive
    than that provided by the `unittest` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the class doesn''t have to extend any special objects to be discovered
    as a test case (although `pytest` will run standard `unittest TestCases` just
    fine). If we run `python -m pytest tests/<filename>`, the output looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The output starts with some useful information about the platform and interpreter.
    This can be useful for sharing or discussing bugs across disparate systems. The
    third line tells us the name of the file being tested (if there are multiple test
    modules picked up, they will all be displayed), followed by the familiar `.F`
    we saw in the `unittest` module; the `.` character indicates a passing test, while
    the letter `F` demonstrates a failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'After all tests have run, the error output for each of them is displayed. It
    presents a summary of local variables (there is only one in this example: the `self` parameter
    passed into the function), the source code where the error occurred, and a summary
    of the error message. In addition, if an exception other than an `AssertionError` is
    raised, `pytest` will present us with a complete traceback, including source code
    references.'
  prefs: []
  type: TYPE_NORMAL
- en: By default, `pytest` suppresses output from `print()` if the test is successful.
    This is useful for test debugging; when a test is failing, we can add `print()` statements
    to the test to check the values of specific variables and attributes as the test
    runs. If the test fails, these values are output to help with diagnosis. However,
    once the test is successful, the `print()` output is not displayed, and they are
    easily ignored. We don't have to clean up the test output by removing `print()`.
    If the tests ever fail again, due to future changes, the debugging output will
    be immediately available.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, this use of the `assert` statement exposes a potential problem
    to **mypy**. When we use the `assert` statement, **mypy** can examine the types,
    and will alert us to a potential problem with `assert 1 == "1"`. This code is
    unlikely to be right, and it will not only fail as a unit test, but will also
    fail a **mypy** inspection.
  prefs: []
  type: TYPE_NORMAL
- en: We've looked at how `pytest` supports the `WHEN` and `THEN` steps of a test
    using a function and the `assert` statement. Now, we need to look more closely
    at how to handle `GIVEN` steps. There are two ways to establish the `GIVEN` precondition
    for a test; we'll start with one that works for simple cases.
  prefs: []
  type: TYPE_NORMAL
- en: pytest's setup and teardown functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`pytest` supports setup and teardown capabilities, similar to the methods used
    in `unittest`, but it provides even more flexibility. We''ll discuss these general
    functions briefly; `pytest` provides us with a powerful fixtures capability, which
    we''ll discuss in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: If we are writing class-based tests, we can use two methods called `setup_method()`
    and `teardown_method()`. They are called before and after each test method in
    the class to perform setup and cleanup duties, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, `pytest` provides other setup and teardown functions to give us
    more control over when preparation and cleanup code is executed. The `setup_class()`
    and `teardown_class()` methods are expected to be class methods; they accept a
    single argument representing the class in question (there is no `self` argument
    because there's no instance; instead, the class is provided). These methods are
    run by `pytest` when the class is initiated rather than on each test run.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have the `setup_module()` and `teardown_module()` functions, which
    are run by `pytest` immediately before and after all tests (in functions or classes)
    in that module. These can be useful for *one-time* setup, such as creating a socket
    or database connection that will be used by all tests in the module. Be careful
    with this one, as it can accidentally introduce dependencies between tests if
    some object state isn't correctly cleaned up between tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'That short description doesn''t do a great job of explaining exactly when these
    methods are called, so let''s look at an example that illustrates exactly when
    it happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The sole purpose of the `BaseTest` class is to extract four methods that are
    otherwise identical to the two test classes, and use inheritance to reduce the
    amount of duplicate code. So, from the point of view of `pytest`, the two subclasses
    have not only two test methods each, but also two setup and two teardown methods
    (one at the class level, one at the method level).
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run these tests using `pytest` with the `print()` function output suppression
    disabled (by passing the `-s` or `--capture=no` flag), they show us when the various
    functions are called in relation to the tests themselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The setup and teardown methods for the module as a whole are executed at the
    beginning and end of the session. Then, the lone module-level test function is
    run. Next, the setup method for the first class is executed, followed by the two
    tests for that class. These tests are each individually wrapped in separate `setup_method()`
    and `teardown_method()` calls. After the tests have executed, the teardown method
    on the class is called. The same sequence happens for the second class, before
    the `teardown_module()` method is finally called, exactly once.
  prefs: []
  type: TYPE_NORMAL
- en: While these function names provide a lot of options for testing, we'll often
    have setup conditions that are shared across multiple test scenarios. These can
    be reused via a composition-based design; `pytest` calls these designs "fixtures."
    We'll look at fixtures next.
  prefs: []
  type: TYPE_NORMAL
- en: pytest fixtures for setup and teardown
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most common uses for the various setup functions is to ensure the
    GIVEN step of a test is prepared. This often involves creating objects and making
    sure certain class or module variables have known values before a test method
    is run.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to a set of special method names for a test class, `pytest` offers
    a completely different way of doing this, using what are known as **fixtures**.
    Fixtures are functions to build the `GIVEN` condition, prior to a test's `WHEN`
    step.
  prefs: []
  type: TYPE_NORMAL
- en: The `pytest` tool has a number of built-in fixtures, we can define fixtures
    in a configuration file and reuse them, and we can define unique fixtures as part
    of our tests. This allows us to separate configuration from the execution of tests,
    allowing fixtures to be used across multiple classes and modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a class that does a few computations that we need to test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This class extends the built-in `list` class to add three statistical summary
    methods, `mean()`, `median()`, and `mode()`. For each method, we need to have
    some set of data we can use; this configuration of a `StatsList` with known data
    is the fixture we'll be testing.
  prefs: []
  type: TYPE_NORMAL
- en: To use a fixture to create the `GIVEN` precondition, we add the fixture name
    as a parameter to our test function. When a test runs, the names of a test function's
    parameters will be located in the collection of fixtures, and those fixture-creating
    functions will be executed for us automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to test the `StatsList` class, we will want to repeatedly provide
    a list of valid integers. We can write our tests as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Each of the three test functions accepts a parameter named `valid_stats`; this
    parameter is created by `pytest` automatically calling the `valid_stats` function
    for us. The function was decorated with `@pytest.fixture` so it could be used
    in this special way by `pytest`.
  prefs: []
  type: TYPE_NORMAL
- en: And yes, the names must match. The **pytest** runtime looks for functions with
    the `@fixture` decorator that match the parameter name.
  prefs: []
  type: TYPE_NORMAL
- en: Fixtures can do a lot more than return simple objects. A `request` object can
    be passed into the fixture factory to provide extremely useful methods and attributes
    to modify the fixture's behavior. The `module`, `cls`, and `function` attributes
    of the `request` object allow us to see exactly which test is requesting the fixture.
    The `config` attribute of the `request` object allows us to check command-line
    arguments and a great deal of other configuration data.
  prefs: []
  type: TYPE_NORMAL
- en: If we implement the fixture as a generator, it can also run cleanup code after
    each test is run. This provides the equivalent of a teardown method on a per-fixture
    basis. We can use it to clean up files, close connections, empty lists, or reset
    queues. For unit tests, where items are isolated, a mock object is a better idea
    than performing a teardown on a stateful object. See the *Imitating objects using
    Mocks* section, later in this chapter, for a simpler approach that's ideal for
    unit testing.
  prefs: []
  type: TYPE_NORMAL
- en: For integration tests, we might want to test some code that creates, deletes,
    or updates files. We'll often use the `pytest` `tmp_path` fixture to write these
    to directories that can be deleted later, saving us from having to do a teardown
    in a test. While rarely needed for unit testing, a teardown is helpful for stopping
    subprocesses or removing database changes that are part of an integration test.
    We'll see this a little later in this section. First, let's look at a small example
    of a fixture with setup and teardown capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started on the concept of a fixture that does both setup and teardown,
    here''s a little bit of code that makes a backup copy of a file and writes a new
    file with a checksum of an existing file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: The source file exists; a new checksum is added to the directory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The source file and a checksum file both exist; in this case, the old checksum
    is copied to a backup location and a new checksum is written
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We won''t test both scenarios, but we will show how a fixture can create –
    and then delete – the files required for a test sequence. We''ll focus on the
    second scenario because it''s more complex. We''ll break the testing into two
    parts, starting with the fixture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `yield` statement is the secret for making this work. Our fixture is really
    a generator that produces one result and then waits for the next request of a
    value. The first result that''s created follows a number of steps: a working directory
    is created, a source file is created in the working directory, and then an old
    checksum file is created. The `yield` statement provides two paths to the test
    and waits for the next request. This work completes the `GIVEN` condition setup
    for the test.'
  prefs: []
  type: TYPE_NORMAL
- en: When the test function finishes, `pytest` will try to get one final item from
    this fixture. This lets the function unlink the files, removing them. There's
    no return value, which signals the end of the iteration. In addition to leveraging
    the generator protocol, the `working_directory` fixture relies on the `tmp_path`
    fixture of `pytest` to create a temporary working location for this test.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the test that uses this `working_directory` fixture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The test is marked with a `skipif` condition because this test won't work in
    Python 3.8; the `with_stem()` method of a `Path` isn't part of the older `pathlib`
    implementation. This assures us that the test is counted but noted as inappropriate
    for a specific Python release. We'll return to this in the *Skipping tests with
    pytest* section, later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The reference to the `working_directory` fixture forces `pytest` to execute
    the fixture function, providing the test scenario with two paths to be used as
    part of the GIVEN condition prior to testing. The WHEN step evaluates the `checksum_writer.checksum()`
    function with these two paths. The THEN steps are a sequence of `assert` statements
    to make sure the files are created with the expected values. After the test is
    run, `pytest` will use `next()` to get another item from the fixture; this action
    executes the code after the `yield`, resulting in a teardown after the test.
  prefs: []
  type: TYPE_NORMAL
- en: When testing components in isolation, we won't often need to use the teardown
    feature of a fixture. For integration tests, however, where a number of components
    are used in concert, it may be necessary to stop processes or remove files. In
    the next section, we'll look at a more sophisticated fixture. This kind of fixture
    can be used for more than a single test scenario.
  prefs: []
  type: TYPE_NORMAL
- en: More sophisticated fixtures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can pass a `scope` parameter to create a fixture that lasts longer than
    one test. This is useful when setting up an expensive operation that can be reused
    by multiple tests, as long as the resource reuse doesn''t break the atomic or
    unit nature of the test: one unit test should not rely on, and should not be impacted
    by, any other unit test.'
  prefs: []
  type: TYPE_NORMAL
- en: As an example, we'll define a server that's part of a client-server application.
    We want multiple web servers to send their log messages to a single, centralized
    log. In addition to isolated unit tests, we need to have an integration test.
    This test makes sure the web server and the log collector properly integrate with
    each other. The integration test will need to start and stop this log collection
    server.
  prefs: []
  type: TYPE_NORMAL
- en: There are at least three levels to the testing pyramid. Unit tests are the foundation,
    exercising each component in isolation. Integration tests are the middle of the
    pyramid, making sure the components integrate properly with each other. A system
    test or acceptance test is the top of the pyramid, making sure the entire suite
    of software does what it claims.
  prefs: []
  type: TYPE_NORMAL
- en: We'll look at a log collection server that accepts messages and writes them
    to a single, central file. These messages are defined by the `logging` module's
    `SocketHandler`. We can depict each message as a block of bytes with a header
    and a payload. In the following table, we've shown the structure using slices
    of the block of bytes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how a message is defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Slice start | Slice stop | Meaning | Python module and function for parsing
    |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 4 | payload_size | `struct.unpack(">L", bytes)` |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | payload_size+4 | payload | `pickle.loads(bytes)` |'
  prefs: []
  type: TYPE_TB
- en: The size of the header is shown as a four-byte slice, but the size shown here
    can be misleading. The header is formally and officially defined by a format string
    used by the `struct` module, `">L"`. The `struct` module has a function, `calcsize()`,
    to compute the actual length from the format string. Instead of using a literal
    4, which is derived from the size of the "`>L`" format, our code will derive the
    size, `size_bytes`, from the size format string, `size_format`. Using one proper
    source, `size_format`, for both pieces of information follows the design principle
    of Don't Repeat Yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example buffer with a message from the `logging` module embedded
    in it. The first line is the header with the payload size, a four-byte value.
    The next lines are the pickled data for a log message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To read these messages, we''ll need to collect the payload size bytes first.
    Then, we can consume the payload that follows. Here''s the socket server that
    reads the headers and the payloads and writes them to a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `socketserver.TCPServer` object will listen for connection requests from
    a client. When a client connects, it will create an instance of the `LogDataCatcher`
    class and evaluate the `handle()` method of that object to gather data from that
    client. The `handle()` method decodes the size and payload with a two-step dance.
    First, it reads a few bytes to find the size of the payload. It uses `struct.unpack()`
    to decode those bytes into a useful number, `payload_size`, and then reads the
    given number of bytes to get the payload. The `pickle.loads()` will load a Python
    object from the payload bytes. This is serialized into JSON notation using `json.dumps()`
    and written to the open file. Once a message has been handled, we can try to read
    the next few bytes to see if there's more data waiting. This server will absorb
    messages from the client until the connection is dropped, leading to an error
    in the read and an exit from the `while` statement.
  prefs: []
  type: TYPE_NORMAL
- en: This log collection server can absorb logging messages from an application anywhere
    in a network. This example implementation is single-threaded, meaning it only
    handles one client at a time. We can use additional mixins to create a multithreaded
    server that will accept messages from multiple sources. In this example, we want
    to focus on testing a single application that depends on this server.
  prefs: []
  type: TYPE_NORMAL
- en: 'For completeness, here''s the main script that starts the server running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We provide a host IP address, a port number, and the file to which we want all
    the messages written. As a practical matter, we might consider using the `argparse`
    module and the `os.environ` dictionary to provide these values to the application.
    For now, we've hardcoded them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the `remote_logging_app.py` application, which transmits log records
    to the log-catching server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This application creates two logging handlers. The `SocketHandler` instance
    will open a socket on the given server and port number, and start writing bytes.
    The bytes will include headers and payloads. The `StreamHandler` instance will
    write to the terminal window; this is the default log handler that we would get
    if we didn't create any special handlers. We configure our logger with both handlers
    so each log message goes both to our console and to the stream server collecting
    the messages. The actual work? A little bit of math to compute the factorial of
    a number. Each time we run this application, it should blast out 20 log messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test the integrated client and server, we need to start the server in a
    separate process. We don''t want to start and stop it many times (that takes a
    while), so we will start it once and use it in multiple tests. We''ll break this
    into two sections, starting with the two fixtures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `log_catcher` fixture will start the `log_catcher.py` server as a subprocess.
    This has a scope set to `"session"` in the `@fixture` decorator, which means it's
    done once for the whole testing session. The scope can be one of the strings `"function"`,
    `"class"`, `"module"`, `"package"`, or `"session"`, providing distinct places
    where the fixture is created and reused. The startup involves a tiny pause (250
    ms) to make sure the other process has started properly. When this fixture reaches
    the `yield` statement, this part of the `GIVEN` test setup is done.
  prefs: []
  type: TYPE_NORMAL
- en: The `logging_config` fixture will tweak the log configuration for the `remote_logging_app`
    module that's being tested. When we look at the `work()` function in the `remote_logging_app.py`
    module, we can see that it expects a module-level `logger` object. This test fixture
    creates a `SocketHandler` object, adds this to the `logger`, and then executes
    the `yield` statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once both of these fixtures have contributed to the `GIVEN` condition, we can
    define test cases that contain the `WHEN` steps. Here are two examples for two
    similar scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: These two scenarios both require the two fixtures. The `log_catcher` fixture,
    with a session scope, is prepared once and used for both tests. The `logging_config`
    fixture, however, has default scope, which means it's prepared for each test function.
  prefs: []
  type: TYPE_NORMAL
- en: The type hint of `None` follows the definition of the fixture as `Iterator[None]`.
    There's no value returned in the `yield` statement. For these tests, the setup
    operation is preparing the overall runtime environment by starting a process.
  prefs: []
  type: TYPE_NORMAL
- en: When a test function finishes, the `logging_config` fixture resumes after the
    `yield` statement. (This fixture is an iterator, and the `next()` function is
    used to try to get a second value from it.) This closes and removes the handler,
    cleanly breaking the network connection with the log catcher process.
  prefs: []
  type: TYPE_NORMAL
- en: When testing finishes overall, the `log_catcher` fixture can then terminate
    the child process. To help with debugging, we print any output. To be sure the
    test worked, we check the OS return code. Because the process was terminated (via
    `p.terminate()`), the return code should be the `signal.SIGTERM` value. Other
    return code values, particularly a return code of one, mean the log catcher crashed
    and the test failed.
  prefs: []
  type: TYPE_NORMAL
- en: We've omitted a detailed `THEN` check, but it would also be part of the `log_catcher`
    fixture. The existing `assert` statement makes sure the log catcher terminated
    with the expected return code. Once the catcher in the sky has finished absorbing
    log messages, this fixture should also read the log file to be sure it contains
    the expected entries for the two scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Fixtures can also be parameterized. We can use a decorator like `@pytest.fixture(params=[some,
    list, of, values])` to create multiple copies of a fixture, which will lead to
    multiple tests with each of the parameter values.
  prefs: []
  type: TYPE_NORMAL
- en: The sophistication of `pytest` fixtures makes them very handy for a wide variety
    of test setup and teardown requirements. Earlier in this section, we hinted at
    ways to mark tests as inappropriate for a particular version of Python. In the
    next section, we'll look at how we can mark tests to be skipped.
  prefs: []
  type: TYPE_NORMAL
- en: Skipping tests with pytest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is sometimes necessary to skip tests in `pytest`, for a similar variety
    of reasons: the code being tested hasn''t been written yet, the test only runs
    on certain interpreters or operating systems, or the test is time-consuming and
    should only be run under certain circumstances. In the previous section, one of
    our tests would not work in Python 3.8, and needed to be skipped.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to skip tests is by using the `pytest.skip()` function. It accepts
    a single argument: a string describing why it has been skipped. This function
    can be called anywhere. If we call it inside a test function, the test will be
    skipped. If we call it at the module level, all the tests in that module will
    be skipped. If we call it inside a fixture, all tests that reference the fixture
    will be skipped.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, in all these locations, it is often only desirable to skip tests
    if certain conditions have or have not been met. Since we can execute the `skip()` function
    at any place in Python code, we can execute it inside an `if` statement. We may
    write a test that looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This test will skip on most operating systems. It should run on the Pythonista
    port of Python for iOS. It shows how we can skip a scenario conditionally, and
    since the `if` statement can check any valid conditional, we have a lot of power
    over when tests are skipped. Often, we check `sys.version_info` to check the Python
    interpreter version, `sys.platform` to check the operating system, or `some_library.__version__`
    to check whether we have a recent enough version of a given module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since skipping an individual test method or function based on a condition is
    one of the most common uses of test skipping, `pytest` provides a convenient decorator
    that allows us to do this in one line. The decorator accepts a single string,
    which can contain any executable Python code that evaluates to a Boolean value.
    For example, the following test will only run on Python 3.9 or higher:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `pytest.mark.xfail` decorator marks a test as expected to fail. If the test
    is successful, it will be recorded as a failure (it failed to fail!). If it fails,
    it will be reported as expected behavior. In the case of `xfail`, the conditional
    argument is optional. If it is not supplied, the test will be marked as expected
    to fail under all conditions.
  prefs: []
  type: TYPE_NORMAL
- en: The `pytest` framework has a ton of other features besides those described here,
    and the developers are constantly adding innovative new ways to make your testing
    experience more enjoyable. They have thorough documentation on their website at [https://docs.pytest.org/](https://docs.pytest.org/).
  prefs: []
  type: TYPE_NORMAL
- en: The `pytest` tool can find and run tests defined using the standard `unittest` library,
    in addition to its own testing infrastructure. This means that if you want to
    migrate from `unittest` to `pytest`, you don't have to rewrite all your old tests.
  prefs: []
  type: TYPE_NORMAL
- en: We've looked at using a fixture to set up and tear down a complex environment
    for testing. This is helpful for some integration tests, but a better approach
    may be to imitate an expensive object or a risky operation. Additionally, any
    kind of teardown operation is inappropriate for unit tests. A unit test isolates
    each software component as a separate unit to be tested. This means we'll often
    replace all of the interface objects with imitations, called "mocks," to isolate
    the unit being tested. Next, we'll turn to creating mock objects to isolate units
    and imitate expensive resources.
  prefs: []
  type: TYPE_NORMAL
- en: Imitating objects using Mocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Isolated problems are easier to diagnose and solve. Figuring out why a gasoline
    car won''t start can be tricky because there are so many interrelated parts. If
    a test fails, uncovering all the interrelationships makes diagnosis of the problem
    difficult. We often want to isolate items by providing simplified imitations.
    It turns out there are two reasons to replace perfectly good code with imitation
    (or "mock") objects:'
  prefs: []
  type: TYPE_NORMAL
- en: The most common case is to isolate a unit under test. We want to create collaborating
    classes and functions so we can test one unknown component in an environment of
    known, trusted test fixtures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, we want to test code that requires an object that is either expensive
    or risky to use. Things like shared databases, filesystems, and cloud infrastructures
    can be very expensive to set up and tear down for testing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In some cases, this may lead to designing an API to have a testable interface.
    Designing for testability often means designing a more usable interface, too.
    In particular, we have to expose assumptions about collaborating classes so we
    can inject a mock object instead of an instance of an actual application class.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, imagine we have some code that keeps track of flight statuses
    in an external key-value store (such as `redis` or `memcache`), such that we can
    store the timestamp and the most recent status. The implementation will require
    the `redis` client; it''s not needed to write unit tests. The client can be installed
    with the `python -m pip install redis` command like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to run this with a real `redis` server, you''ll also need to download
    and install `redis`. This can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the Docker desktop to help manage this application. See [https://www.docker.com/products/docker-desktop](https://www.docker.com/products/docker-desktop).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `docker pull redis` command from a Terminal window to download a `redis`
    server image. This image can be used to build a running Docker container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can then start the server with `docker run -p 6379:6379 redis`. This will
    start a container running the `redis` image. Then you can use this for integration
    testing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An alternative that avoids **docker** involves a number of platform-specific
    steps. See [https://redislabs.com/ebook/appendix-a/](https://redislabs.com/ebook/appendix-a/)
    for a number of installation scenarios. The examples that follow will assume **docker**
    is being used; the minor changes that are required to switch to a native installation
    of `redis` are left as an exercise for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s some code that saves status in a `redis` cache server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `Status` class defines an enumeration of four string values. We've provided
    symbolic names like `Status.CANCELLED` so that we can have a finite, bounded domain
    of valid status values. The actual values stored in the database will be strings like
    `"CANCELLED"` that – for now – happen to match the symbols we'll be using in the
    application. In the future, the domain of values may expand or change, but we'd
    like to keep our application's symbolic names separate from the strings that appear
    in the database. It's common to use numeric codes with `Enum`, but they can be difficult
    to remember.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of things we ought to test for in the `change_status()` method.
    We check to be sure the `status` argument value really is a valid instance of
    the `Status` enumeration, but we could do more. We should check that it raises
    the appropriate error if the `flight` argument value isn't sensible. More importantly,
    we need a test to prove that the key and value have the correct formatting when
    the `set()` method is called on the `redis` object.
  prefs: []
  type: TYPE_NORMAL
- en: One thing we don't have to check in our unit tests, however, is that the `redis` object
    is storing the data properly. This is something that absolutely should be tested
    in integration or application testing, but at the unit test level, we can assume
    that the `py-redis` developers have tested their code and that this method does
    what we want it to. As a rule, unit tests should be self-contained; the unit under
    test should be isolated from outside resources, such as a running Redis instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of integrating with a Redis server, we only need to test that the `set()`
    method was called the appropriate number of times and with the appropriate arguments.
    We can use `Mock()` objects in our tests to replace the troublesome method with
    an object we can introspect. The following example illustrates the use of `Mock`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This test uses the `raises()` context manager to make sure the correct exception
    is raised when an inappropriate argument is passed in. In addition, it creates
    a `Mock` object for the `redis` instance that the `FlightStatusTracker` will use.
  prefs: []
  type: TYPE_NORMAL
- en: The mock object contains an attribute, `set`, which is a mock method that will
    always return `True`. The test, however, makes sure the `redis.set()` method is
    never called. If it is, it means there is a bug in our exception handling code.
  prefs: []
  type: TYPE_NORMAL
- en: Note the navigation into the mock object. We use `mock_redis.set` to examine
    the mocked `set()` method of a `Mock` object created by the `mock_redis` fixture.
    The `call_count` is an attribute that all `Mock` objects maintain.
  prefs: []
  type: TYPE_NORMAL
- en: While we can use code like `flt.redis = mock_redis` to replace a real object
    with a `Mock` object during a test, there is potential for problems. Simply replacing
    a value or even replacing a class method can only work for objects that are destroyed
    and created for each test function. If we need to patch items at the module level,
    the module isn't going to be reimported. A much more general solution is to use
    a patcher to inject a `Mock` object temporarily. In this example, we used the
    `monkeypatch` fixture of `pytest` to make a temporary change to the `FlightStatusTracker`
    object. A `monkeypatch` has its own automatic teardown at the end of a test, allowing
    us to use monkeypatched modules and classes without breaking other tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'This test case will be flagged by **mypy**. The **mypy** tool will object to
    using a string argument value for the status parameter of the `change_status()`
    function; this clearly must be an instance of the `Status` enumeration. A special
    comment can be added to silence the **mypy** argument type check, `# type: ignore
    [arg-type]`.'
  prefs: []
  type: TYPE_NORMAL
- en: Additional patching techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some cases, we only need to inject a special function or method for the duration
    of a single test. We may not really be creating a sophisticated `Mock` object
    that's used in multiple tests. We may only need a small `Mock` for a single test.
    In this case, we may not need to use all the features of the `monkeypatch` fixture,
    either. For example, if we want to test the timestamp formatting in the `Mock` method,
    we need to know exactly what `datetime.datetime.now()` is going to return. However,
    this value changes from run to run. We need some way to pin it to a specific datetime
    value so we can test it deterministically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Temporarily setting a library function to a specific value is one place where
    patching is essential. In addition to the `monkeypatch` fixture, the `unittest.mock`
    library provides a `patch` context manager. This context manager allows us to
    replace attributes on existing libraries with mock objects. When the context manager
    exits, the original attribute is automatically restored so as not to impact other
    test cases. Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We don't want our test results to depend on the computer's clock, so we built
    the `fake_now` object with a specific date and time we can expect to see in our
    test results. This kind of replacement is very common in unit tests.
  prefs: []
  type: TYPE_NORMAL
- en: The `patch()` context manager returns a `Mock` object that was used to replace
    some other object. In this case, the object being replaced is the entire `datetime`
    module inside the `flight_status_redis` module. When we assigned `mock_datetime.datetime`,
    we replaced the `datetime` class inside the mocked `datetime` module with our
    own `Mock` object; this new `Mock` defines one attribute, `now`. Because the `utcnow`
    attribute is a `Mock` that returns a value, it behaves like a method and returns
    a fixed, known value, `fake_now`. When the interpreter exits the `patch` context
    manager, the original `datetime` functionality is restored.
  prefs: []
  type: TYPE_NORMAL
- en: After calling our `change_status()` method with known values, we use the `assert_called_once_with()`
    method of the `Mock` object to ensure that the `now()` function was indeed called
    exactly once with the expected arguments (no arguments, in this case). We also
    use the `assert_called_once_with()` method on the `Mock` `redis.set` method to
    make sure it called with arguments that were formatted as we expected them to
    be. In addition to the "called once with," we can also check the exact list of
    mock calls that were made. This sequence is available in the `mock_calls` attribute
    of a `Mock` object.
  prefs: []
  type: TYPE_NORMAL
- en: Mocking dates so you can have deterministic test results is a common patching
    scenario. The technique applies to any stateful object, but is particularly important
    for external resources (like the clock) that exist outside our application.
  prefs: []
  type: TYPE_NORMAL
- en: For the special case of `datetime` and `time`, packages like `freezegun` can
    simplify the monkeypatching required so that a known, fixed date is available.
  prefs: []
  type: TYPE_NORMAL
- en: The patches we made in this example are intentionally sweeping. We replaced
    the entire `datetime` module with a `Mock` object. This will tend to expose unexpected
    uses of datetime features; if any method not specifically mocked (like the `now()`
    method was mocked) gets used, it will return `Mock` objects that are likely to
    crash code under test.
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous example also shows how testability needs to guide our API design.
    The `tracker` fixture has an interesting problem: it creates a `FlightStatusTracker`
    object, which constructs a Redis connection. After the Redis connection is built,
    we replace it. When we run tests for this code, however, we will discover that
    each test will create an unused Redis connection. Some tests may fail if there
    is no Redis server running. Because this test requires external resources, it''s
    not a proper unit test. There are two possible layers of failure: the code doesn''t
    work, or the unit tests don''t work because of some hidden external dependency.
    This can become a nightmare to sort out.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We could solve this problem by mocking the `redis.Redis` class. A `Mock` for
    this class can return a mock instance in a `setUp` method. A better idea, however,
    might be to rethink our implementation more fundamentally. Instead of constructing
    the `redis` instance inside `__init__`, we should allow the user to pass one in,
    as in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This allows us to pass a connection in when we are testing so that the `Redis`
    method never gets constructed. Additionally, it allows any client code that talks
    to `FlightStatusTracker` to pass in their own `redis` instance. There are a variety
    of reasons they might want to do this: they may have already constructed one for
    other parts of their code; they may have created an optimized implementation of
    the `redis` API; perhaps they have one that logs metrics to their internal monitoring
    systems. By writing a unit test, we''ve uncovered a use case that makes our API
    more flexible from the start, rather than waiting for clients to demand we support
    their exotic needs.'
  prefs: []
  type: TYPE_NORMAL
- en: This has been a brief introduction to the wonders of mocking code. Mock objects
    have been part of the standard `unittest` library since Python 3.3\. As you see
    from these examples, they can also be used with `pytest` and other test frameworks.
    Mock objects have other, more advanced features that you may need to take advantage
    of as your code becomes more complicated. For example, you can use the `spec`
    argument to invite a mock to imitate an existing class, so that it raises an error
    if code tries to access an attribute that does not exist on the imitated class.
    You can also construct mock methods that return different arguments each time
    they are called by passing a list as the `side_effect` argument. The `side_effect` parameter
    is quite versatile; you can also use it to execute arbitrary functions when the
    mock is called or to raise an exception.
  prefs: []
  type: TYPE_NORMAL
- en: The point of unit testing is to be sure that each "unit" works in isolation.
    Often, a unit is an individual class, and we'll need to mock the collaborators.
    In some cases, there's a composition of classes or a Façade for which a number
    of application classes can be tested together as a "unit." There's a clear boundary,
    however, when applying mocks inappropriately. If we need to look inside some external
    module or class (one we didn't write) to see how to mock its dependencies, we've
    taken a step too far.
  prefs: []
  type: TYPE_NORMAL
- en: Don't examine the implementation details of classes outside your application
    to see how to mock their collaborators; instead, mock the entire class you depend
    on.
  prefs: []
  type: TYPE_NORMAL
- en: This generally leads to providing a mock for an entire database or external
    API.
  prefs: []
  type: TYPE_NORMAL
- en: We can extend this idea of imitating objects one step further. There's a specialized
    fixture we use when we want to ensure data has been left untouched. We'll look
    at this next.
  prefs: []
  type: TYPE_NORMAL
- en: The sentinel object
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In many designs, we'll have a class with attribute values that can be provided
    as parameters to other objects, without really doing any processing on those objects.
    For example, we may provide a `Path` object to a class, and the class then provides
    this `Path` object to an OS function; the class we designed doesn't do anything
    more than save the object. From a unit testing perspective, the object is "opaque"
    to the class we're testing – the class we're writing doesn't look inside the object
    at state or methods.
  prefs: []
  type: TYPE_NORMAL
- en: The `unittest.mock` module provides a handy object, the `sentinel`, that can
    be used to create opaque objects that we can use in test cases to be sure that
    the application stored and forwarded the object untouched.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a class, `FileChecksum`, that saves an object computed by the `sha256()`
    function of the `hashlib` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can isolate this code from the other modules for unit testing purposes.
    We''ll create a `Mock` for the `hashlib` module, and we''ll use a `sentinel` for
    the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Our `mocked_hashlib` object provides a method, `sha256`, that returns the unique
    `sentinel.checksum` object. This is an object, created by the `sentinel` object,
    with very few methods or attributes. Any attribute name can be created as a unique
    object; we've chosen "checksum" here. The resulting object is designed for equality
    checks and nothing else. A `sentinel` in a test case is a way to be sure the `FileChecksum`
    class doesn't do anything wrong or unexpected with the objects it was given.
  prefs: []
  type: TYPE_NORMAL
- en: The test case creates a `FileChecksum` object. The test confirms that the file
    was the provided argument value, `source_file`. The test also confirms that the
    checksum matched the original `sentinel` object. This confirms that the `FileChecksum`
    instance stored the checksum results properly and presented the result as the
    value of the `checksum` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: If we change the implementation of the `FileChecksum` class to – for example
    – use properties instead of direct access to the attribute, the test will confirm
    the checksum was treated as an opaque object that came from the `hashlib.sha256()`
    function and was not processed in any other way.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve looked at two unit testing frameworks: the built-in `unittest` package
    and the external `pytest` package. They both provide ways for us to write clear,
    simple tests that can confirm that our application works. It''s important to have
    a clear objective defining the required amount of testing. Python has an easy-to-use
    coverage package that gives us one objective measure of test quality.'
  prefs: []
  type: TYPE_NORMAL
- en: How much testing is enough?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've already established that untested code is broken code. But how can we
    tell how well our code is tested? How do we know how much of our code is actually
    being tested and how much is broken? The first question is the more important
    one, but it's hard to answer. Even if we know we have tested every line of code
    in our application, we do not know that we have tested it properly. For example,
    if we write a `stats` test that only checks what happens when we provide a list
    of integers, it may still fail spectacularly if used on a list of floats, strings,
    or self-made objects. The onus of designing complete test suites still lies with
    the programmer.
  prefs: []
  type: TYPE_NORMAL
- en: The second question – how much of our code is actually being tested – is easy
    to verify. **Code coverage** is a count of the number of lines of code that are
    executed by a program. From the number of lines that are in the program as a whole,
    we know what percentage of the code was really tested or covered. If we additionally
    have an indicator that tells us which lines were not tested, we can more easily
    write new tests to ensure those lines are less likely to harbor problems.
  prefs: []
  type: TYPE_NORMAL
- en: The most popular tool for testing code coverage is called, memorably enough, `coverage.py`.
    It can be installed like most other third-party libraries, using the `python -m
    pip install coverage` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'We don''t have space to cover all the details of the coverage API, so we''ll
    just look at a few typical examples. If we have a Python script that runs all
    our unit tests for us (this could be using `unittest.main`,  `unittest` `discover`,
    or `pytest`), we can use the following command to perform coverage analysis for
    a specific unit test file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This command will create a file named `.coverage`, which holds the data from
    the run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Windows Powershell users can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use the `coverage report` command to get an analysis of the code
    coverage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting output should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This report lists the files that were executed (our unit test and the module
    it imported), the number of lines of code in each file, and the number of lines
    of code that were executed by the test. The two numbers are then combined to show
    the amount of code coverage. Not surprisingly, the entire test was executed, but
    only a fraction of the `stats` module was exercised.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we pass the `-m` option to the `report` command, it will add a column that
    identifies the lines that are missing from the test execution. The output looks
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The ranges of lines listed here identify the lines in the `stats` module that
    were not executed during the test run.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example code uses the same `stats` module we created earlier in this chapter.
    However, it deliberately uses a single test that fails to test a lot of code in
    the file. Here''s the test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This test doesn't test the median or mode functions, which correspond to the
    line numbers that the coverage output told us were missing.
  prefs: []
  type: TYPE_NORMAL
- en: The textual report provides sufficient information, but if we use the `coverage
    html` command, we can get an even more useful interactive HTML report, which we
    can view in a web browser. The interactive report has a number of useful filters
    we can enable. The web page even highlights which lines in the source code were
    and were not tested.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how it looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B17070_13_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.1: Interactive HTML coverage report'
  prefs: []
  type: TYPE_NORMAL
- en: We created the HTML report using the `coverage` module with `pytest`. To do
    this, we previously installed the `pytest` plugin for code coverage, using `python
    -m pip install pytest-cov`. The plugin adds several command-line options to `pytest`,
    the most useful being `--cover-report`, which can be set to `html`, `report`,
    or `annotate` (the latter actually modifies the original source code to highlight
    any lines that were not covered).
  prefs: []
  type: TYPE_NORMAL
- en: It can be helpful to include more than the `src` directory tree in coverage
    analysis. A large project may have a complex tests directory, including additional
    tools and supporting libraries. As the project evolves, there may be some test
    or support code that's obsolete, but hasn't been cleaned up yet.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, if we could somehow run a coverage report on this section of
    the chapter, we'd find that we have not covered most of what there is to know
    about code coverage! It is possible to use the coverage API to manage code coverage
    from within our own programs (or test suites), and `coverage.py` accepts numerous
    configuration options that we haven't touched on. We also haven't discussed the
    difference between statement coverage and branch coverage (the latter is much
    more useful and is the default in recent versions of `coverage.py`), or other
    styles of code coverage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bear in mind that while 100 percent code coverage is a goal that we should
    all strive for, 100 percent coverage is not enough! Just because a statement was
    tested does not mean that it was tested properly for all possible inputs. The
    boundary value analysis technique includes looking at five values to bracket the
    edge cases: a value below the minimum, the minimum, in the middle somewhere, the
    maximum, and a value above the maximum. For non-numeric types, there may not be
    a tidy range, but the advice can be adapted to other data structures. For lists
    and mappings, for example, this advice often suggests testing with empty lists
    or mapping with unexpected keys. The Hypothesis package ([https://pypi.org/project/hypothesis/](https://pypi.org/project/hypothesis/))
    can help with more sophisticated test cases.'
  prefs: []
  type: TYPE_NORMAL
- en: It's difficult to emphasize how important testing is. The test-driven development
    approach encourages us to describe our software via visible, testable objectives.
    We have to decompose complex problems into discrete, testable solutions. It's
    not uncommon to have more lines of test code than actual application code. A short
    but confusing algorithm is sometimes best explained through examples, and each
    example should be a test case.
  prefs: []
  type: TYPE_NORMAL
- en: Testing and development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the many ways these unit tests can help is when debugging application
    problems. When each unit seems to work in isolation, any remaining problems will
    often be the result of an improperly used interface between components. When searching
    for the root cause of a problem, a suite of passing tests acts as a set of signposts,
    directing the developer into the wilderness of untested features in the borderlands
    between components.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a problem is found, the cause is often one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Someone writing a new class failed to understand an interface with an existing
    class and used it incorrectly. This indicates a need for a new unit test to reflect
    the right way to use the interface. This new test should cause the new code to
    fail its expanded test suite. An integration test is also helpful, but not as
    important as the new unit test focused on interface details.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The interface was not spelled out in enough detail, and both parties using the
    interface need to reach an agreement on how the interface should be used. In this
    case, both sides of the interface will need additional unit tests to show what
    the interface should be. Both classes should fail these new unit tests; they can
    then be fixed. Additionally, an integration test can be used to confirm that the
    two classes agree.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The idea here is to use test cases to drive the development process. A "bug"
    or an "incident" needs to be translated into a test case that fails. Once we have
    a concrete expression of a problem in the form of a test case, we can create or
    revise software until all the tests pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'If bugs do occur, we''ll often follow a test-driven plan, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Write a test (or multiple tests) that duplicates or proves the bug in question
    is occurring. This test will, of course, fail. In more complex applications, it
    may be difficult to find the exact steps to recreate a bug in an isolated unit
    of code; finding this is valuable work, since it requires knowledge of the software,
    and captures the knowledge as a test scenario.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, write the code to make the tests stop failing. If the tests were comprehensive,
    the bug will be fixed, and we will know we didn't break something new while attempting
    to fix something.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Another benefit of test-driven development is the value of the test cases for
    further enhancement. Once the tests have been written, we can improve our code
    as much as we like and be confident that our changes didn''t break anything we
    have been testing for. Furthermore, we know exactly when our refactor is finished:
    when the tests all pass.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, our tests may not comprehensively test everything we need them to;
    maintenance or code refactoring can still cause undiagnosed bugs that don't show
    up in testing. Automated tests are not foolproof. As E. W. Dijkstra said, "Program
    testing can be used to show the presence of bugs, but never to show their absence!"
    We need to have good reasons why our algorithm is correct, as well as test cases
    to show that it doesn't have any problems.
  prefs: []
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll return to some material from an earlier chapter and apply some careful
    testing to be sure we''ve got a good, workable implementation. Back in *Chapter
    3*, *When Objects Are Alike*, we looked at the distance computations that are
    part of the *k*-nearest neighbors classifier. In that chapter, we looked at several
    computations that produced slightly different results:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Euclidean distance**: This is the direct line from one sample to another.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manhattan distance**: This follows streets-and-avenues around a grid (like
    the city of Manhattan), adding up the steps required along a series of straight-line
    paths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chebyshev distance**: This is the largest of the streets-and-avenues distances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sorensen distance**: This is a variation of the Manhattan distance that weights
    nearby steps more heavily than distant steps. It tends to magnify small distances,
    making more subtle discriminations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These algorithms all produce distinct results from the same inputs; they all
    involve complex-looking math, and they all need to be tested in isolation to ensure
    we have implemented them correctly. We'll start with unit tests of the distances.
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing the distance classes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need to create some test cases for each distance computation algorithm.
    When we look at the various equations, we can see that there are four pairs of
    relevant values from two samples: the sepal length and width, and the petal length
    and width. To be extremely thorough, we could create at least 16 distinct cases
    for each algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 0**: All four values are the same; the distance should be zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cases 1-4**: One of the four values is different between the two samples.
    For example, a test sample might have measurements of `("sepal_length": 5.1, "sepal_width":
    3.5, "petal_length": 1.4, "petal_width": 0.2`), where as a training sample might
    have measurements of (`"sepal_length": 5.2, "sepal_width": 3.5, "petal_length":
    1.4, "petal_width": 0.2`); only one of these values is distinct.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cases 5-10**: A pair of values is different.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cases 11-14**: Three values are different between the two samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Case 15**: All four values are different.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, the concepts of equivalence partitioning and boundary value analysis
    suggest that we also need to locate values where there is a profound state change.
    For example, invalid values will raise exceptions, something that should also
    be tested. This can create a number of sub-cases within each of the cases enumerated
    above.
  prefs: []
  type: TYPE_NORMAL
- en: We won't create all 16 cases for each of the four algorithms in this part of
    the case study. Instead, we'll take a close look at whether or not all 16 cases
    are really required. To get started, we'll limit ourselves to one case for each
    distance algorithm. This will be an example of case 15, where all four values
    of the two samples are different.
  prefs: []
  type: TYPE_NORMAL
- en: With mathematical results, we need to compute the expected answers outside the
    software we're building. We can, of course, try to compute the expected answers
    with pencil and paper or a spreadsheet.
  prefs: []
  type: TYPE_NORMAL
- en: One trick that can be helpful when working with more advanced math is to use
    the `sympy` package as a way to check the math more carefully.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the Euclidean distance between a known sample, *k*, and an unknown
    sample, *u*, has the following formal definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17070_13_001.png)'
  prefs: []
  type: TYPE_IMG
- en: This computes the distance among all four measurements. For example, the known
    sepal length is *k*[sl]. The other attributes have similar names.
  prefs: []
  type: TYPE_NORMAL
- en: 'While `sympy` can do a great many things, we want to use it for two specific
    purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: To confirm that our Python version of the formula really is correct
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To compute the expected results using specific variable substitutions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We do this by using `sympy` to perform the operations symbolically. Instead
    of plugging in specific floating-point values, we want to transform the Python
    expression into conventional mathematical notation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a test case that''s applied to the design, not the implementation.
    It confirms the code''s design is very likely to match the original intent. We''ve
    translated the nicely typeset names like *k*[sl] for "known sepal length" into
    a Pythonic (but not as easy to read) `k_sl`. Here''s our interaction with `sympy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We imported `sympy` and defined the batch of symbols that match the original
    formula. We need to define these objects so `sympy` will work with them as mathematical
    symbols, not ordinary Python objects. Then, we did our best to translate the Euclidean
    distance formula from math into Python. It seems right, but we'd like to be sure.
  prefs: []
  type: TYPE_NORMAL
- en: Note that when we asked for the value of `ED`, we didn't see the results of
    a Python computation. Because we've defined the variables as symbols, `sympy`
    builds a representation of the equation that we can work with.
  prefs: []
  type: TYPE_NORMAL
- en: When we used the `pretty()` function from `sympy`, it displayed an ASCII art
    version of our expression, which looks a lot like the original. We used the `use_unicode=False`
    option because that looked the best in this book. When printed with an appropriate
    font, the `use_unicode=True` version may be easier to read.
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula is something we can share with experts to be sure our test cases
    really do properly describe the behavior of this particular class. Because the
    formula looks right, we can evaluate it with concrete values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The `subs()` method substitutes values for the symbols in the formula. We then
    use the `evalf()` method to evaluate the result as a floating-point number. We
    can use this to create a unit test case for the class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we look at the test case, here''s an implementation of the Euclidean
    distance class. As an optimization, this uses `math.hypot()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'It seems like this implementation matches the math. The best way to check is
    to create an automated test. Recall that tests often have a `GIVEN`-`WHEN`-`THEN`
    outline. We can expand this to the following conceptual scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We can provide the values used in the symbolic computation for `U`, `K`, and
    the expected distance. We''ll start with a test fixture that supports the `GIVEN`
    step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve created a `TrainingKnownSample` and an `UnknownSample` object that we
    can use in subsequent tests. This fixture definition depends on a number of important
    type hints and definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We can provide the distance computation as a `WHEN` step, and a final `THEN`
    comparison in an `assert` statement. We need to use an `approx` object for comparison
    because we're working with floating-point values, and exact comparisons rarely
    work out well.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this application, the number of decimal places in the test case seems excessive.
    We''ve left all the digits so the values will fit with the defaults used by `approx`,
    which is a relative error of 1 x 10^(-6), or `1e-6` in Python notation. Here''s
    the rest of the test case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: This is pleasantly short and to the point. Given two samples, the distance result
    should match what we computed by hand, or computed using `sympy`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the distance classes needs a test case. Here are two other distance
    computations. The expected results come from validating the formula and providing
    concrete values, as we did previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: For the Chebyshev and Manhattan distances, we're adding the individual steps
    for each of the four attributes and computing the sum or finding the largest individual
    distance. We can work these out by hand and be confident that our expected answer
    is right.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Sorensen distance, however, is a little more complex and can benefit from
    a comparison with the symbolic results. Here''s the formal definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17070_13_002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here''s the symbolic definition that we can use to compare our implementation
    against the definition. The equation displayed looks a lot like the formal definition,
    giving us the confidence to use it to compute expected values. Here''s a definition
    extracted from the code that we''d like to check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The ASCII-art version of the formula looks a lot like the formal definition,
    giving us a lot of confidence that we can use `sympy` to compute expected answers.
    We''ll substitute specific example values to see what the expected results should
    be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we''re sure we have valid expected results, we can plug this expectation
    into a unit test case. Here''s how the test case looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: We've used `sympy` as a design aid to help us create unit test cases. It's not
    run as a regular part of the testing process. We only want to use it for the obscure
    cases where we aren't sure we can trust ourselves to compute an expected answer
    with paper and pencil.
  prefs: []
  type: TYPE_NORMAL
- en: As we noted at the beginning of this chapter's case study, there are 16 different
    combinations of values where the known and the unknown sample attributes are different.
    We've provided only one of the 16 combinations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `coverage` tool, we can see that all of the relevant code is tested
    with this one case. Do we really need the other 15 cases? There are two viewpoints:'
  prefs: []
  type: TYPE_NORMAL
- en: From a "black box" point of view, we don't know what's in the code, and we need
    to test all the combinations. This kind of black box testing relies on the assumption
    that the values could have some complex interdependency that can only be found
    through patient examination of all cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From a "white box" point of view, we can look at the various distance function
    implementations and see that all four attributes are treated uniformly. An examination
    of the code tells us a single case is sufficient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For Python applications, we suggest following white box testing unless there's
    a compelling reason to avoid looking at the code. We can use the coverage report
    to confirm that one case really has tested the relevant code.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of creating 16 different test cases for the various distance algorithms,
    we can focus our efforts on making sure the application is reliable and uses minimal
    computing resources. We can also focus on testing other parts of the application.
    We'll look at the `Hyperparameter` class next, because it depends on the `Distance`
    computation class hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing the Hyperparameter class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Hyperparameter` class relies on a distance computation. We have two strategies
    for testing a complex class like this:'
  prefs: []
  type: TYPE_NORMAL
- en: An integration test that uses the distance computations already tested
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A unit test that isolates the `Hyperparameter` class from any of the distance
    computations to be sure the class works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a general rule of thumb, every line of code needs to be exercised by at least
    one unit test. After that, integration tests can also be used to ensure that the
    interface definitions are honored by all of the modules, classes, and functions.
    The spirit of "test everything" is more important than "make the number come out
    right"; counting lines is one way to ensure that we've tested everything.
  prefs: []
  type: TYPE_NORMAL
- en: We'll look at testing the `classify()` method of the `Hyperparameter` class
    using `Mock` objects to isolate the `Hyperparameter` class from any of the distance
    computations. We'll also mock the `TrainingData` object to further isolate an
    instance of this class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the relevant code we''ll be testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The `algorithm` attribute of the `Hyperparameter` class is a reference to an
    instance of one of the distance computation objects. When we replace this, the
    `Mock` object must be callable and must return an appropriate sortable number.
  prefs: []
  type: TYPE_NORMAL
- en: The `data` attribute is a reference to a `TrainingData` object. The `Mock` to
    replace the `data` object must provide a `training` attribute that is a list of
    mocked samples. Since these values are provided to another mock without any intermediate
    processing, we can use a `sentinel` object to confirm that the training data was
    provided to the mocked distance function.
  prefs: []
  type: TYPE_NORMAL
- en: The idea can be summarized as watching the `classify()` method "go through the
    motions." We provide mocks and sentinels to confirm that requests are made and
    the results of those requests are captured.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the more complex test, we''ll need some mock sample data. This will rely
    on `sentinel` objects. The objects will be passed through to a mocked distance
    computation. Here''s the definition of some mocked sample objects we''ll use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'This fixture is a list of mocks for `KnownSamples`. We''ve provided a unique
    name for each sample to help with debugging. We''ve provided a `species` attribute,
    since that''s the attribute used by the `classify()` method. We didn''t provide
    any other attributes, because they aren''t used by the unit under test. We will
    use this `sample_data` fixture to create a `Hyperparameter` instance that will
    have a mock distance computation and this mock collection of data. Here''s the
    test fixture we''ll use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The `mocked_distance` object will provide a sequence of results that look like
    the results of distance computations. The distance computations are tested separately,
    and we've isolated the `classify()` method from the specific distance computations
    with this `Mock`. We've provided the list of mocked `KnownSample` instances via
    a `Mock` object that will behave like a weak reference; the training attribute
    of this mock object will be the given sample data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be sure the `Hyperparameter` instance makes the right requests, we evaluate
    the `classify()` method. Here''s the entire scenario, including these two final
    THEN steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '`GIVEN` a sample data fixture with five instances reflecting two species'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WHEN` we apply the *k*-NN algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`THEN` the result is the species with the closest three distances'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AND` the mock distance computation was invoked with all of the training data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s the final test, using the above fixtures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: This test case checks the distance algorithm to make sure the entire training
    set of data was used. It also confirms that the nearest neighbors were used to
    locate the resulting species for the unknown sample.
  prefs: []
  type: TYPE_NORMAL
- en: Since we tested the distance computations separately, we have a great deal of
    confidence in running an integration test that combines these various classes
    into a single, working application. For debugging purposes, it is very helpful
    to isolate each component into a separately tested unit.
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ve looked at a number of topics related to testing applications
    written in Python. These topics include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We described the importance of unit testing and test-driven development as a
    way to be sure our software does what is expected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We started by using the `unittest` module because it's part of the standard
    library and readily available. It seems a little wordy, but otherwise works well
    for confirming that our software works.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pytest` tool requires a separate installation, but it seems to produce
    tests that are slightly simpler than those written with the `unittest` module.
    More importantly, the sophistication of the fixture concept lets us create tests
    for a wide variety of scenarios.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `mock` module, part of the `unittest` package, lets us create mock objects
    to better isolate the unit of code being tested. By isolating each piece of code,
    we can narrow our focus on being sure it works and has the right interface. This
    makes it easier to combine components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code coverage is a helpful metric to ensure that our testing is adequate. Simply
    adhering to a numeric goal is no substitute for thinking, but it can help to confirm
    that efforts were made to be thorough and careful when creating test scenarios.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We''ve been looking at several kinds of tests with a variety of tools:'
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests with the `unittest` package or the `pytest` package, often using
    `Mock` objects to isolate the fixture or unit being tested.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration tests, also with `unittest` and `pytest`, where more complete integrated
    collections of components are tested.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Static analysis can use **mypy** to examine the data types to be sure they're
    used properly. This is a kind of test to ensure the software is acceptable. There
    are other kinds of static tests, and tools like `flake8`, `pylint`, and `pyflakes`
    can be used for these additional analyses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some research will turn up scores of additional types of tests. Each distinct
    type of test has a distinct objective or approach to confirming the software works.
    A performance test, for example, seeks to establish the software is fast enough
    and uses an acceptable number of resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can''t emphasize enough how important testing is. Without automated tests,
    software can''t be considered complete, or even usable. Starting from test cases
    lets us define the expected behavior in a way that''s specific, measurable, achievable,
    results-based, and trackable: SMART.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Practice test-driven development. That is your first exercise. It's easier to
    do this if you're starting a new project, but if you have existing code you need
    to work on, you can start by writing tests for each new feature you implement.
    This can become frustrating as you become more enamored with automated tests.
    The old, untested code will start to feel rigid and tightly coupled, and will
    become uncomfortable to maintain; you'll start feeling like changes you make are
    breaking the code and you have no way of knowing, for lack of tests. But if you
    start small, adding tests to the code base improves it over time. It's not unusual
    for there to be more test code than application code!
  prefs: []
  type: TYPE_NORMAL
- en: So, to get your feet wet with test-driven development, start a fresh project.
    Once you've started to appreciate the benefits (you will) and realize that the
    time spent writing tests is quickly regained in terms of more maintainable code,
    you'll want to start writing tests for existing code. This is when you should
    start doing it, not before. Writing tests for code that we *know *works is boring.
    It is hard to get interested in the project until we realize just how broken the
    code we thought was working really is.
  prefs: []
  type: TYPE_NORMAL
- en: Try writing the same set of tests using both the built-in `unittest` module
    and `pytest`. Which do you prefer? `unittest` is more similar to test frameworks
    in other languages, while `pytest` is arguably more Pythonic. Both allow us to
    write object-oriented tests and test object-oriented programs with ease.
  prefs: []
  type: TYPE_NORMAL
- en: We used `pytest` in our case study, but we didn't touch on any features that
    wouldn't have been easily testable using `unittest`. Try adapting the tests to
    use test skipping or fixtures. Try the various setup and teardown methods. Which
    feels more natural to you?
  prefs: []
  type: TYPE_NORMAL
- en: Try running a coverage report on the tests you've written. Did you miss testing
    any lines of code? Even if you have 100 percent coverage, have you tested all
    the possible inputs? If you're doing test-driven development, 100 percent coverage
    should follow quite naturally, as you will write a test before the code that satisfies
    that test. However, if you're writing tests for existing code, it is more likely
    that there will be edge conditions that go untested.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the case study code to 100 percent coverage can be tricky, since we've
    been skipping around and implementing some aspects of the case study in several
    different ways. It may be necessary to write several similar tests for alternative
    implementations of case study classes. It can help to make reusable fixtures so
    that we can provide consistent testing among the alternative implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'When creating test cases, it can help to think carefully about the values that
    are somehow different, such as the following, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: Empty lists when you expect full ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Negative numbers, zero, one, or infinity compared to positive integers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Floats that don't round to an exact decimal place
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strings when you expected numerals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unicode strings when you expected ASCII
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ubiquitous `None` value when you expected something meaningful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your tests cover such edge cases, your code will be in good shape.
  prefs: []
  type: TYPE_NORMAL
- en: 'The numeric methods for distance computations are something that might be better
    tested using the Hypothesis project. Check out the documentation here: [https://hypothesis.readthedocs.io/en/latest/](https://hypothesis.readthedocs.io/en/latest/).
    We can use Hypothesis to easily confirm that the order of operands in a distance
    computation doesn''t matter; that is, `distance(s1, s2) == distance(s2, s1)`,
    given any two samples. It''s often helpful to include Hypothesis testing to confirm
    that the essential *k*-nearest neighbors classifier algorithm works for randomly
    shuffled data; this will ensure there''s no bias for the first or last item in
    the training set.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have finally covered the most important topic in Python programming: automated
    testing. Test-driven development is considered a best practice. The standard library `unittest` module
    provides a great out-of-the-box solution for testing, while the `pytest` framework
    has some more Pythonic syntaxes. Mocks can be used to emulate complex classes
    in our tests. Code coverage gives us an estimate of how much of our code is being
    run by our tests, but it does not tell us that we have tested the right things.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we''ll jump into a completely different topic: concurrency.'
  prefs: []
  type: TYPE_NORMAL
