<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Legacy Modernization to Microservices-Centric Apps</h1>
                </header>
            
            <article>
                
<p>Legacy applications are typically mono<span>lithic, massiv</span>e, and inflexible, comprising millions of <span>lines of code. They are neither modular nor modern. It's very difficult to bring in any changes on particular portions of them. However, they have been contributing immensely in successfully running a majority of the business behemoths across the globe. Mainframe servers are the most powerful and high-</span>performance IT infrastructures hostin<span>g and running a variety of complex legacy applications. Though mission-critical applications are being run on mainframes, modern-day computing mandates for a kind of marriage between mainframe computing and web-scale computing. That is, we need easily-manageable and -maintainable applications. On the infrastructure side, we need open, highly compartmentalized, programmable, optimized, and organized IT infrastructures.</span></p>
<p>Thus, there is a push for legacy applications to embrace the newly-introduced innovations in software engineering. There are many noteworthy architectural styles, process optimization techniques, and tools to speed up the transformation process. Digital transformation forces us to deliver faster. Every organization’s priority is to have well-designed<br/>
applications, the ability to deploy to on-premise and cloud environments as well as deploy independently, update services, and deploy defect fixes and new features in hours or days, not months.</p>
<p>In this chapter, we'll discuss how the <strong>microservices architecture</strong> (<strong>MSA</strong>) is the way forward for modern applications that are highly nimble, versatile, and resilient.</p>
<p class="mce-root"/>
<p>The chapter objectives include the following: </p>
<ul>
<li>Describing the needs for legacy application modernization </li>
<li>Delineating why applications have to be modernized to be migrated and run in cloud environments</li>
<li>Depicting how the combination of microservices and containers is the preferred one for legacy modernization</li>
<li>Detailing the legacy modernization methodology </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>Readers should be comfortable with the following popular technologies and tools:</p>
<ul>
<li>Docker-enabled containerization platforms</li>
<li>The microservices architecture</li>
<li>API gateways and management suites</li>
<li>Microservices design principles and patterns</li>
<li>Microservices composition (orchestration and choreography) </li>
<li>Cloud operations</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A preview of containers and microservices</h1>
                </header>
            
            <article>
                
<p>With the surging popularity of the open source Docker containerization platform, the domain of containerization has accelerated in an unprecedented manner. Today, use of containerization has become widespread among IT professionals. Any kind of software can be easily containerized through automated tools. Thus, there are container-ready images in standardized format made available in public and private repositories. To modernize legacy applications, the use of containers as the most efficient wrapping mechanism is emerging and evolving.</p>
<p>The use of containers to modernize legacy applications brings forth a few advantages. The main point of containerization is to remove the infrastructure dependency from legacy applications. That is, containerized applications can run on any platform and infrastructure without any tweaking. Any of the infrastructure's complexities are instantly eliminated with this containerization paradigm. Thus, legacy applications become portable and can be integrated with other applications easily.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>Once containerized, any legacy application can interact with third-party applications. Enabling compatibility with web, cloud, and mobile platforms becomes easier and faster. With the aid of various platform solutions for delivering multi-container applications, legacy applications can be blended to become more relevant for their owners and end users. Migrating containerized legacy applications from a local cloud to remote clouds, or from one public cloud to another public cloud, becomes simple. The security, stability, and scalability of legacy applications can be strengthened by attaching additional capabilities from outside.</p>
<p>Thus, the noteworthy advancements being brought into the containerization phenomenon directly and indirectly impact legacy modernization. There are several crucial advantages of the containerization movement, especially for the strategic and tough goal of legacy modernization. Legacy software can be subdivided into many different domains, and each of those modules is being deployed in different containers. Now, to guarantee higher performance and throughput, different containers can be run on different infrastructures. An I/O-intensive application module can be made to run on physical/<strong>bare-metal</strong> (<strong>BM</strong>) servers. Some containerized application services can be deployed in public clouds to take advantage of their availability, scalability, and security capabilities. Thus, legacy applications that are modernized through the use of containers are typically empowered to be modern, agile, and adaptive.</p>
<p>Modernizing legacy applications is accomplished through the following methods:</p>
<ul>
<li><strong>Rearchitecting the application</strong>: A new appropriate and modern architecture is being produced and, accordingly, architectural decisions, components, connectivity, and capabilities are designed and implemented.</li>
<li><strong>Replatforming the application</strong>: The legacy application can be deployed in newer platforms in order to be categorized as a modern application.</li>
<li><strong>Refactoring the application</strong>: Applications are partitioned into many components, which target a specific business functionality. The identified business functionality is rewritten with microservices, which are then fused with one another to produce the desired application.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing the microservices architecture</h1>
                </header>
            
            <article>
                
<p>As mentioned several times in this book, MSA is being positioned as the next-generation application architectural pattern. Microservices are constructed, compiled, and deployed as a separate project. Containers are the most optimal unit of microservices development and deployment. Every microservice is blessed with its own data store. They aren't compiled into software solutions. Instead, they're implemented as standalone services to be integrated and orchestrated via standards-based and communication protocols.</p>
<p>Every service is being given an interface using the simple-to-use RESTful APIs. Microservices are carefully versioned not to adversely affect any other user microservices and applications. For example, for a <strong>business-to-consumer</strong> (<strong>B2C</strong>) e-commerce application, a microservice may be developed to provide a products catalog. Another microservice takes care of procurement, another is for payment, and so on. Thus, microservices are independently developed by distributed teams and then deployed.</p>
<p>Complicated and sophisticated applications are being constructed through the leverage of microservices. On the other side, massive applications are being dismantled into a collection of microservices.</p>
<p>Microservices are typically small and independent processes that communicate with one other using well-defined and language-agnostic APIs. They are usable, reusable, interoperable, dynamically-composable, and self-defined. Microservices expose their unique and business-centric capabilities through well-intended interfaces to the outside world. Thus software integration gets simplified and sped up in a formalized manner. There are cross-cutting concerns and horizontal (business-agnostic) functions that usually get replicated across the source code for any enterprise-grade software system. They can be centralized using microservices. Some functions have to be frequently updated and upgraded using advanced technologies. Thus, not only application development, but also the maintenance aspect are both worrisome factors for worldwide enterprises. These specialized functionalities can be built as microservices and then exposed so they can be easily found and bound in software applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why legacy modernization?</h1>
                </header>
            
            <article>
                
<p>Legacy software applications were written decades ago using outdated programming languages and are usually run on mainframe servers. They are massive in size, and hence, the goals of modification and maneuverability are very tough to implement. Third-party integration is another difficult assignment. To incorporate changes (induced by business, technology, and user) is time-consuming and error-prone. They're tightly coupled and usually run in a single process. All these properties make them unamendable and unapproachable for any kind of crucial advancements. They are not modern in the sense that they are not generally web and mobile-enabled. Precisely speaking, they aren't agile or adaptive. They're resisting technology upgrades, which makes them very expensive to maintain. Thus, moving from legacy applications to modern applications by leveraging a bevy of pioneering technologies, programming languages, and development frameworks and platforms has gained a lot of attention. With newer architectural patterns and styles emerging and evolving, software architects and developers are keen to embrace them to bring forth competitive applications that can easily fulfil the varying expectations of businesses and people.</p>
<p class="mce-root"/>
<p>The concept of DevOps is sweeping across IT organizations in order to eliminate any kind of friction between development teams and operation teams. Greenfield projects and cloud-native applications are being deployed and operated using this new concept. There are write-ups in this chapter about DevOps, which is being positioned as a must for the digital era. As there is constant chopping and changing being demanded in software solutions from various sources, the significance of DevOps is garnering more support.</p>
<p>For monolithic applications, functionalities are duplicated in the source code. Several units across the enterprise bring forth their functionalities, which have to be integrated with the core application. When a company buys another company, their source code has to be integrated. Such integrations ultimately lead to the duplication of the same functionality. If there's any modification to be incorporated in to a single functionality, the whole monolithic application has to be recompiled and redeployed. There are definitely some deficiencies as far as legacy applications are concerned. That's why modernization has started to gain attention.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Legacy-to-digital application modernization</h1>
                </header>
            
            <article>
                
<p>Application modernization is all about empowering currently-running applications to meet business goals and customer expectations, which are constantly evolving. There are a number of digital technologies and <strong>artificial intelligence</strong> (<strong>AI</strong>) algorithms emerging, which are evolving quickly. We have scores of edge/digitization technologies, such as sensors, actuators, implants, wearables, stickers, tags, codes, chips, controllers, smart dust, specks, beacons, and LED elements, for creating digitized entities out of physical, mechanical, and electrical systems. That is, our ground-level physical systems become digitized, joining mainstream computing.</p>
<p>A bevy of digital technologies (cloud, mobility, social, IoT, and analytics) are gathering a lot of attention because they're capable of bringing in a dazzling array of digital innovations, disruptions, and transformations. With the maturity of AI algorithms, the domain of digital intelligence is gaining prominence in making sense of digital data. Thus, the real digital transformation involves digital and digital-intelligence technologies. Also, there are outdated and old-fashioned applications and technologies aplenty. To attain a real digital transformation, these old, yet currently-running, applications have to be modified using modernization and migration tools. The hugely popular digital technologies that we've mentioned have to be leveraged to bring advancement to legacy applications.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Accomplishing modernization</h1>
                </header>
            
            <article>
                
<p>Many modernization and migration experts have spoken on how to accomplish modernization without bringing down existing business workloads and IT services. To perform modernization, whether to go for re-engineering, re-architecting, re-platforming, and so on, is the moot question to be answered. Other considerations include whether modernization helps applications maintain their original performance level and capabilities, such as scalability, availability, reliability, and security. The following diagram shows how a litany of application modernization and migration technologies come in handy for elevating legacy applications to newer applications that fulfill business needs, market demands, customer satisfaction, and higher productivity:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e9a041fd-2270-4e35-b5f8-e7ea232ce32a.png"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Source: https://www.winwire.com/wp-content/uploads/2018/07/Application-Modernization-Assess-Strategize-Modernize.png</div>
<p>Path-breaking technologies and breakthrough tools enable the smooth transition toward newer applications that are knowledge-filled, event-driven, service-oriented, cloud-hosted, process-aware, and people-centric.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Approaching legacy application modernization</h1>
                </header>
            
            <article>
                
<p>Having realized the need to modernize legacy software packages in order to be relevant to their stakeholders, partners, employees, and consumers, business organizations are asking their IT teams to come with viable techniques to speed up and simplify the aspects of modernizing legacy applications. There are a few strategically-sound application architectures, scores of methodologies, and best practices toward application modernization. Modernized applications are being readied to work on cloud environments, which are generally consolidated, centralized, shared, virtualized and containerized, and automated ones. In short, cloud environments are highly optimized and organized in order to be extremely and elegantly agile and adaptive. Applications have to be readied to run in clouds without any problem and to reap all the originally envisaged benefits of the cloud. The emergence of MSA can help to accelerate legacy modernization.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Microservices-centric legacy application modernization</h1>
                </header>
            
            <article>
                
<p>Microservices are fine-grained, self-defined, and autonomous services that produce and sustain next-generation, enterprise-grade applications. With MSA, every application is structured as a collection of microservices. In other words, when appropriately combining multiple microservices, we arrive at competent applications, which are cloud-hosted, service-oriented, event-driven, insight-filled, enterprise-grade, and people-centric. Software architects are very excited about the futuristic and artistic role of the MSA pattern in architecting production-ready software solutions. The design of complicated and sophisticated applications that leverage hundreds of different and distributed microservices as application modules is attracting a lot of attention. The MSA paradigm is being positioned as an agile application design methodology. This is being seen as a positive development in <strong>software engineering</strong> (<strong>SE</strong>), as there are many agile development techniques and frameworks. Also, the solidity of the DevOps culture in large-sized business enterprises is speeding up the rollout of applications into production environments. That is, issues and friction between development and deployment teams are being surmounted and this empowerment leads to speedier application deployment in various environments (development, testing, staging, and production). Thus, development and deployment are being augmented through a host of advancements in the IT field.</p>
<p class="mce-root"/>
<p>The flourishing MSA style significantly simplified designing and architecting applications. If our business workloads and IT services are made out of microservices, they become extremely flexible, scalable, customizable, and configurable. We've seen why worldwide enterprises are very serious in embracing the MSA pattern in order to be competitive in their day-to-day activities.</p>
<p>Architects and experts have come out with a few achievable mechanisms for smoothly transitioning from monolithic to microservices-based applications. The modernization steps are being accomplished through multiple phases. The first phase is illustrated in the following diagram (source: <a href="https://dzone.com/articles/choosing-a-legacy-software-modernization-strategy">https://dzone.com/articles/choosing-a-legacy-software-modernization-strategy</a>). This talks about identifying the distinct business functionalities of the legacy application and separating them out to implement them as microservices by giving the underlying service implements, which are typically polyglot, a RESTful API. In the <em>Service composition</em> section, we talked about phase two of the following diagram: </p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/eb4a2506-cd05-4dc1-a181-e76d1cc2ea35.png" style="width:55.92em;height:35.83em;"/></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Service extraction </h1>
                </header>
            
            <article>
                
<p>At the end of the first phase, we get a number of microservices with their own data-storage facilities. These microservices are modular in nature and interoperable, and they are intrinsically capable of interacting with one another through the APIs. The widely-recommended design approach is to design and develop microservices for different domains. The widely-used aspect is <strong>domain-driven design</strong> (<strong>DDD</strong>). For example, take an e-commerce application. The domains include the shopping cart, payment, shipping, notifications, credit verification, and analytics. Every domain is looked at as a business functionality and hence implemented as a microservice. There can be multiple methods within a microservice. There can be intra- as well as inter-microservice communication and collaboration. </p>
<p>The microservices within a domain are supposed to interact frequently. There are situations where microservices in a domain have to connect and correspond with microservices in other domains in order to fulfill different business processes and activities. Thus, an e-business and e-commerce application is bound to have several domains and hundreds of microservices. With containers emerging as the best fit for hosting and running microservices in a fault-tolerant manner, there can be thousands of containers to run microservices-centric applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Service composition </h1>
                </header>
            
            <article>
                
<p>The second phase (<a href="https://dzone.com/articles/choosing-a-legacy-software-modernization-strategy">https://dzone.com/articles/choosing-a-legacy-software-modernization-strategy</a>) is to go for service composition. In the other chapters, we discussed various composition techniques and tools; there are two approaches—orchestration and choreography.</p>
<p class="mce-root"/>
<p>The following diagram conveys how a centralized broker/hub/bus comes in handy when establishing connectivity and accomplishing data processing:</p>
<p class="CDPAlignCenter CDPAlign"><br/>
<img src="assets/2788cf20-7ff4-49bf-a318-bbce16c29b4e.png" style="width:42.75em;height:32.75em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Service migration</h1>
                </header>
            
            <article>
                
<p>The third phase is all about migrating microservices to cloud environments (private, public, and hybrid). There are several public cloud environments across the globe and they offer a number of benefits, such as affordability, scalability, availability, and security. Microservices can accommodate all kinds of changes induced by business needs, end users, expectations, and technology advancements. The scale and simplicity, along with frequent changes, make microservices the perfect tool for enterprise IT in fulfilling the various functional and non-functional requirements. Also, microservices enable the smooth transition from monolithic to microservices applications.</p>
<p class="mce-root"/>
<p>Not all applications are fit for modernization and migration to cloud environments. Applications have to be continuously subjected to a variety of investigations in order to double-check their value and validity. Application rationalization, optimization, and modernization are vital for applications to be continuously relevant for their administrators and users. With the arrival of highly-optimized and -organized IT infrastructures, applications need to be be able to work in newer environments while guaranteeing all the performance requirements. Some applications have to be meticulously refactored to be taken to the cloud. Experts prescribe a series of best practices for legacy modernization and migration. We'll look at one such approach in the following section; more details can be found at <a href="https://dzone.com/articles/choosing-a-legacy-software-modernization-strategy">https://dzone.com/articles/choosing-a-legacy-software-modernization-strategy</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Container-centric legacy application modernization</h1>
                </header>
            
            <article>
                
<p>Legacy applications can be directly containerized to be presented as modernized applications for cloud environments. The other prominent option is to subdivide the legacy application into multiple components, where each component caters to a business functionality. Now every business functionality can be converted into a microservice. Thus, there are microservices-centric and container-centric methods for effecting legacy modernization and migration to cloud environments. </p>
<p>Any application can be directly containerized, but this transition may not work in the long run. Hence, applications have to be segmented into smaller, more easily-manageable pieces. Then those segments have to be service-enabled and then containerized. There are best practices, enabling patterns, knowledge guides, success stories, case studies, optimized processes, integrated and insightful platforms, and proven procedures for smoothening this sort of legacy remediation. Best-in-class frameworks and automated tools are flourishing to tackle the complexities that are associated with legacy transformations. Here's a list expressed by one modernization expert.</p>
<p>The first step is to break the monolithic application into a group of distinguishable components. These components can be easily and elegantly service-enabled and containerized. Those containerized images are stocked in publicly-available image repositories. These components have to be extremely modular (loosely or lightly coupled and highly cohesive) in order to contribute to the goals of modernization and migration. These are the business functionalities, the typical middle-tier components in any three-tiered application.</p>
<p class="mce-root"/>
<p>Having created a collection of microservices that together make up the business logic of the application, the second step is to build data access as a service. That is, develop data services and expose them so that any application can use them to get the right data to complete business tasks. This setup decouples business and data logic so that there's no possibility for any kind of dependency-initiated issues. This data-logic layer is the final one, as per the specifications of three-tier applications. In the first step, we focused on creating application containers. In the second step, we talked about volume containers in order to store data to empower applications accordingly.</p>
<p>The last step is all about testing. For an enterprise-scale application, we can have several microservices and their instances. Also, containers as the service runtime are now manifold. Microservices and their many instances can be run in separate containers and hence there will be many containers in a typical IT environment; that is, for an application, there can be multiple interactive and collaborative microservices. Each microservice can be run in multiple containers in order to support redundancy. Widely-demanded high availability can be achieved through multiple containers for a single microservice. Due to the fickle nature of containers, architects recommend many containers for hosting one microservice. To ensure high availability, there can be many instances of microservices. This way, if one service or container goes down, its service instances deployed in other containers come to the rescue. However, the real difficulty lies in testing such an intertwined environment. Precisely speaking, monolithic applications need to be tuned to become distributed and complicated applications. Though modern applications are agile, affordable, and adaptive, the management and operational complexities of microservices-centric applications are bound to escalate. Further on, detecting errors and debugging them to make applications error-free is a tedious job indeed. There are a few automated testing tools emerging for testing microservices. Experts are unearthing various ways of testing distributed microservices. Also, the testing procedure is being illustrated for composite microservices. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Refactoring and rewriting</h1>
                </header>
            
            <article>
                
<p>We've been writing about how the powerful emergence of microservices architecture is instigating the need for legacy modernization in order to embrace modernity. As we all know, the key mitigation technique has been <em>divide and conquer</em>. This mantra has been shining not only to engineer fresh systems but also to disintegrate legacy systems. As legacy applications are hugely complicated, partitioning them into a number of smaller pieces is also being accomplished through segmentation approaches. Functionality-based segregation is one way of implementing disintegration. Once a legacy application is subdivided into a number of application modules, the real modernization begins; that is, there are many application services to be extricated out of archaic and massive applications.</p>
<p class="mce-root"/>
<p>As mentioned previously, the extracted application components are made into microservices in order to easily migrate them to cloud environments. There is a big gap between application components and microservices. Functionally, they are almost the same. But the structure of application components is hugely different from microservices. Therefore, it's paramount to devise a viable mechanism to close the gulf between application components and microservices. Refactoring or rewriting is the recommended approach to achieve modern legacy applications. Refactoring legacy software into a suite of microservices and enabling them to talk to one another on a per-need basis is being promoted as one of the surest ways to achieve modernization. With the faster maturity and stability of the microservices architectural style and containerization-enabled platforms, refactoring large-scale applications into fine-grained and well-designed microservices is gaining prominence.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Modernization technique terms</h1>
                </header>
            
            <article>
                
<p>Legacy modernization is being accomplished in several ways. Applications can be subdivided into a number of smaller service components and each service is being rewritten using a modern programming language while guaranteeing the legacy application's functionality. Even the original architecture of the legacy application can be changed using the latest architectural styles, such as SOA, EDA, or a combination of both. The legacy applications currently running on centralized and mainframe servers can be modernized to run on completely new platforms and infrastructures, such as cloud environments. Thus, the modernization strategy development and planning are done by taking several principles, goals, technologies, and tools into account. At different layers and levels, the required modernization gets done in a risk-free and strategically sound manner: </p>
<ul>
<li><strong>Refactoring</strong><span> typically refers to</span> reorganizing an application's source code in order to bring some clarity and changes. Suppose the code written isn't modular, or the code has a lot of duplicated code as it's been written by different teams over the years of its development and maintenance. The integration with newly-acquired company software may also bring in some complexities. There are several other reasons to modernize. So, factoring out the code in order to fulfill some new requirements (business, technology, and users) is being touted as code refactoring. </li>
<li><strong>Rearchitecting and redesigning</strong><span> both </span>mean the same thing. Legacy applications can be modernized by rearchitecting the application using newer technologies, such as middleware solutions, database systems, execution platforms, visualization toolkits, and software infrastructure solutions. Also, with newer patterns for designing application components, redesigning is gaining prominence. With these architectural and design changes, new applications can have higher performance, scalability, security, availability, resilience, and sustainability. With powerful message-oriented middleware platforms and newer databases, such as NoSQL and NewSQL, modernization is a continuous affair to satisfy evolving business needs and to ensure the delight of customers. </li>
<li><strong>Replatforming</strong> involves<span> taking the legacy application to be deployed and running it on newer platforms and infrastructures in order to accrue the platform's and infrastructure's benefits. </span></li>
<li><strong>Rewriting</strong> the application source code using modern programming languages is done in order to experience the distinct advantages of the new programming language. For example, <strong>Ballerina</strong> is a new programming language and is considered by some as the best-in-class language for coding microservices. This language simplifies service, data, and application integration. </li>
</ul>
<p>Here's a list of a few best practices while performing modernization:</p>
<ul>
<li>We must be sure that the refactoring technique works. After taking an easy-to-do application component out of the legacy application under transformation, then we must embark on refactoring or rewriting; that is, we can refactor the source code of the application component to convert it into a microservice. Otherwise, we can rewrite the code to craft a microservice without changing the functionality of the original application component. Then, the idea is to take the version that's very close to the final implementation.</li>
<li>There's no need to refactor or rewrite the other components of the legacy application. Use the stub mechanism to check whether the refurbished or rewritten component mimics the old behavior; that is, it's mandatory to establish a seamless and spontaneous connectivity for the existing application components (not yet refactored or rewritten) with the newly-formulated service to understand whether there's any deviation or deficiency. This way, little wastage of time is guaranteed for legacy modernization, which happens to be a time-consuming job.</li>
<li>Once a microservice is readied, it has to be taken to one or more software repositories (public and/or private) to enable access and usage by software developers and testers. This kind of centralized storage also contributes for source-code version control. There are <strong>continuous integration</strong> (<strong>CI</strong>) solutions to integrate, build, and test software services. With the overwhelming adoption of the enterprise DevOps concept, business applications get continuous and consistent integration, delivery, and deployment in order to take applications and services to their end users quickly, without any risk.</li>
<li>With the unprecedented popularity of the containerization movement, containerizing software applications has been getting more attention. Containerizing microservices brings a number of technical advantages to the table. There are write-ups and articles detailing how to quickly deliver and deploy microservices individually and collectively. With container clustering, orchestration, and management platform solutions emerging, the containerization paradigm is becoming penetrative, participative, and pervasive. There are a litany of automated tools to augment the various activities associated with cloud orchestration, realizing multi-cloud container applications, and infrastructure as code. The convergence of containers and microservices opens up a growing array of innovations, disruptions, and transformations. There are integrated platforms that enable the dream service era.</li>
<li>Application components have to be chosen for refactoring or rewriting using some priority factors. Not all components are suitable for modernization. There are a few important parameters required to do the prioritization properly. The key parameters to be considered include the ease of extraction, the roadmap, and possible risks. Some of the components may become obsolete. Thus, prioritization plays an important role in accomplishing the modernization job in an affordable, risk-free, and strategically-sound manner.</li>
<li>Data virtualization is the way forward for application modules to correspond realistically with the data sources of the legacy application. It's therefore recommended that instead of focusing on data-structure transformation, focus on creating and using appropriate proxies that can transform data from different databases into a standardized format. This standardization helps the newly-created microservices (whether through refactoring or rewriting) to connect and interact with the data without any major hitch.</li>
</ul>
<p>Microservices are small and accommodative. Fixing defects, security holes, and vulnerabilities is quite easy with microservices. Performance increments, root cause analysis, and unified threat management form composite applications that are business-critical, process-aware, and cloud-enabled. The faster deployment of microservices in production environments is another positive factor. Specific microservices in microservices-centric applications can be replaced with newer ones, whereas this sort of facility isn't available in monolithic applications. Adding features is therefore a smooth affair.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Legacy modernization through microservices</h1>
                </header>
            
            <article>
                
<p>There are many real-world reasons why the industry is very optimistic about MSA for modernizing legacy applications. There are several valid differences between monolithic and microservices-centric applications, as represented in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cc94cd4e-a4ef-4788-9d6f-7c9cdefb185b.png" style="width:42.25em;height:33.50em;"/></p>
<p>Monolithic applications follow a heavily-centralized architecture, whereas <strong>Microservices</strong> go for a distributed architecture. Due to the exponential increase in data size and the usage of commodity servers, the IT world is leaning toward distributed computing. Also, services are being developed and deployed by worldwide software developers to geographically-distributed servers. Thus, distributed computing can't be taken lightly anymore. MSA intrinsically supports the distributed computing characteristics and hence, is flourishing.</p>
<p>As previously explained, each microservice fulfills one or more business functionalities. That is, different business capabilities are implemented through separate microservices running on different processes. For an e-commerce application, we can have microservices for various modules, such as the shopping cart, customer analytics, the payment gateway, warehousing, shipping, replenishment, inventory management, and email notifications. Web-scale applications are embracing the MSA in order to be business-friendly.</p>
<p>For applications with frequently varying loads (where the number of users changes quickly and there can be sudden spikes in the number of messages with different data sizes), there is an insistence for mechanisms that innately support dynamic, horizontal, and real-time scalability. Through containerized microservices, we can achieve that kind of scalability, as containers are lightweight and can boot up in a matter of seconds.</p>
<p>Microservices eliminate the problems associated with the tightly-coupled application components of monolithic applications. Microservices can be integrated through various middleware products (synchronous and asynchronous communication). Due to the decoupling nature of microservices, microservices can be independently designed, developed, and deployed in production environments. That is, updating and upgrading microservices dooesn't affect other components of a microservices-centric application. There are a few novel testing and deployment methods emerging and evolving in order to bring the utmost dynamism to microservices environments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The distinctions of microservices</h1>
                </header>
            
            <article>
                
<p>Microservices are technology-agnostic, which means the technology lock-in problem gets resolved through the MSA style. Therefore, it's being touted as a solution that fully complies with the polyglot architecture pattern. And microservices can be coded using any programming language. There are several microservice development frameworks for speedy implementation and installation. MSA supports a number of the latest communication and data-transmission protocols. The service adapter ecosystem is continuously growing in order to integrate with disparate application services. The interfaces of microservices are very formalized and hence dependency issues don't rear their ugly heads.</p>
<p>Composite applications are quickly realized through static as well as dynamic service-composition methods. There are script languages for writing configuration and composition files. There are data representation, exchange, and persistence mechanisms. The closer association with the containerization movement has come at the right time for microservices to speed up software engineering tasks. On the other side, microservices emerge as the best at partitioning legacy and old-fashioned applications into multiple interactive and autonomous microservices. Thus, legacy modernization is being spruced up by business enterprises due to advancements in the MSA space. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The code samples</h1>
                </header>
            
            <article>
                
<p>In the GitHub repository, readers can find some examples for developing microservices using the Java language. Then, there are some examples of how to compose multiple microservices to produce process-aware and multi-container composite applications. Legacy refactoring is all about making changes in the code segments of a legacy function to present it as a microservice. The combination of such microservices results in microservices-centric modern applications. Microservices can be built from the ground up. Or, using automated tools, legacy application code snippets can be converted into microservices. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The major hurdles to overcome</h1>
                </header>
            
            <article>
                
<p>However, there are certain hurdles to overcome. This section lists them out. Though the MSA style is being proclaimed as the most optimized and organized approach for legacy modernization, there are a few tricky issues confronting business and IT teams:</p>
<ul>
<li>Microservices, as previously indicated, result in distributed systems and hence, developers have to explore how to extract the various business functions from the tightly-integrated monolithic applications, refactor or rewrite the chosen ones as microservices, and keep the newly-formed microservices linked to the remainder of the legacy application in order to verify whether everything works as wanted. This isn't an easy task and hence it's a definite obstacle on the modernization journey.</li>
<li>Previously, any monolithic application ran inside a single process. The participating application components talked to one another using the language feature. There are <strong>remote procedure calls</strong> (<strong>RPCs</strong>) and <strong>remote method invocations</strong> (<strong>RMIs</strong>); in-process communication is risk-free and fast. However, microservices' calls go over a network. RESTful interfaces are the dominant way of approaching distant microservices. Network latency and other network-related problems can damage the resilience of microservices. That is, inter-service communication can cause some network-associated issues. Microservices are being stuffed with APIs. APIs are the first point of contact for any microservice to interact with other microservices, which can be running in the same machine, rack, floor, or even in geographically-distributed cloud environments. API versions can differ, the data formats of messages getting exchanged between microservices APIs can create mismatch problems, network congestions can occur, and the varying loads on microservices may contribute for the slowdown and even failure of microservices.</li>
<li>Testing microservices brings its own problems. Monolithic applications can be easily tested because they have a combined, single code base. That isn't the case with microservices. In the case of microservices, every microservice has to be individually tested. Further on, microservices also have to be tested collectively. There are newer approaches that simplify testing microservices.</li>
</ul>
<p>Microservices acquire special consideration because of the widespread adoption of DevOps tools. The aim of DevOps is to speed up service deployment. That is, specific services can be chosen and deployed. Thus, to avail of all the benefits of the MSA, business enterprises have to embrace DevOps.</p>
<p>Without an ounce of doubt, microservices open up fresh possibilities and opportunities. Though there are a few options for legacy modernization, modernizing microservices-centric legacy applications is being pronounced as the best way forward. The microservices approach is the best way for enterprises to be digitally disrupted and transformed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Modernizing and migrating legacy applications – the role of cloud environments</h1>
                </header>
            
            <article>
                
<p>The technology space is continuously evolving, and is being strengthened with the adoption of newer technologies and tools. For example, the cloud paradigm has redefined the IT domain completely. Cloud technologies have made it possible to have highly optimized and organized IT to host any kind of business application. Hitherto unknown IT services are being formulated and delivered, and newer business models are emerging to cater to different sections and segments of the market, which is extremely knowledge-driven these days.</p>
<p>In the recent past, we've heard about containers and microservices more often. They are showing a lot of promise in bringing advancements in software engineering. The cloud journey is also progressing speedily. This progress means old applications need to be refurbished using the latest technologies, so they can be hosted and managed in cloud environments. That is, massive applications are partitioned into a pool of microservices, then they are containerized, and stocked in container-image directories. Container orchestration platforms then provision container resources, fetch container images, and deploy them.</p>
<p>The extensive use of DevOps concepts has accelerated IT operations. A kind of DevOps pipeline (the end-to-end workflow) gets created to sequence the actions to be taken to take the code to production environments. When the source code of any freshly-baked or changed microservice is committed into code repositories, typically a signal is sent to the chosen CI tool (such as Jenkins). CI tools are for building, testing, and integrating software applications. Then, continuous delivery and deployment tools take the application to production environments. In this chapter, we've talked extensively about how legacy applications are being modernized into a spectrum of microservices and then migrated to cloud environments.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The need for cloud environments</h1>
                </header>
            
            <article>
                
<p>The cloud idea has brought a paradigm shift into the IT space. Businesses are consciously embracing this technology in order to be relevant to their customers and consumers. Cloud environments are software-defined and hence cloud infrastructures (server machines, storage appliances, and networking components) are programmable, network-accessible, remotely monitorable and manageable, and customizable. All kinds of infrastructures can be virtualized and containerizable. Thus, clouds are consolidated, centralized, and shareable. Increasingly, cloud environments are being integrated, allowing us to have hybrid and multi-cloud environments. Further on, with the steady growth of the device ecosystem, there's a rush toward setting up and sustaining fog or edge clouds in order to accomplish real-time data capture, processing, knowledge discovery, and dissemination and actuation. That is, our everyday environments (homes, hotels, hospitals, and so on) are being stuffed with a variety of heterogeneous devices. These devices are mainly resource-constrained and some are resource-intensive. These devices are deeply connected and embedded systems. There are solutions and approaches to dynamically create device clouds. Thus, edge clouds are expanding the cloud landscape. There are a litany of communication and data-transmission protocols for enabling <strong>device-to-device</strong> (<strong>D2D</strong>) integration.</p>
<p>There are IoT gateway solutions and brokers for mediating between devices at the ground level and software applications getting hosted in faraway cloud environments; that is, device-to-cloud.</p>
<p><strong>Device-to-cloud</strong> (<strong>D2C</strong>) integration is gaining prominence these days with the increased availability and adoption of device middleware solutions. In short, there are new cloud environments emerging and evolving in order to cater to varying demands. That is, there are public, private, hybrid, and edge clouds. Multi-cloud environments are also being considered by enterprises to escape the vendor lock-in problem.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>With enabling technologies being unearthed and supported, federated cloud environments may soon emerge to meet some special requirements. Clouds are highly optimized and organized IT infrastructures. Resource utilization has gone up significantly with cloudification. Appropriate cloud locations are chosen to guarantee higher performance. Clouds are intrinsically meeting various non-functional requirements, such as scalability, availability, security, and flexibility. Web-scale and customer-facing applications are already deployed in public cloud environments. Through extra security measures, such as <strong>virtual private networks</strong> (<strong>VPNs</strong>), firewall and load-balancing appliances, and intrusion-detection and prevention systems, public cloud environments are secured and made safe in order to boost the confidence of users and enterprises alike. Workloads are subjected to a variety of investigations and deployed in the most appropriate physical machines/BM servers, <strong>virtual machines</strong> (<strong>VMs</strong>), and containers. There are pioneering scheduling solutions and algorithms for tasks and resources; that is, scheduling resources for all kinds of incoming jobs is fully automated. Then, there are energy-efficiency methods being applied in order to ensure power conservation and reduce heat dissipation. There are cost efficiencies being accrued out of the cloudification movement.</p>
<p>Thus, IT infrastructures are set to become agile, adaptive, and affordable in their offerings. These are the reasons why the cloud paradigm is becoming an important ingredient in the IT journey. Businesses are being automated and augmented through IT disruptions and transformations. People-empowerment through scores of innovations in the IT space is the next realistic target. Thus, we have clouds being formed out of commodity servers, appliances, and high-end server machines. There are converged and hyper-converged infrastructures to serve as cloud environments. In the recent past, it was forecast that the future belongs to networked and embedded devices that form ad hoc and purpose-specific clouds quickly to achieve specific data-processing requirements. Thus, applications are being modernized as pools of services, and these services are being taken to cloud environments (public, private, hybrid, and edge/fog). Due to the surging popularity of the cloud as the one-stop IT solution for all kinds of business needs, legacy modernization and migration is seeing heightened attention. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A case study for legacy modernization and migration</h1>
                </header>
            
            <article>
                
<p><span><span>Having understood the strategic significance of integrated and insightful applications, enterprises are strategizing and planning modernization and migration plans. As reported earlier, most of the enterprise, cloud, web, and embedded applications are being built as containerized and microservices-centric applications; legacy applications should be modified with a variety of automation processes and products.</span></span></p>
<p>Blu Age Velocity (<a href="https://www.bluage.com/products/blu-age-velocity">https://www.bluage.com/products/blu-age-velocity</a>) is famous for automated modernization. This offering automates and accelerates modernizing legacy applications. It can do both reverse and forward engineering. That is, it can translate legacy applications into microservices-centric applications. Readers can find many case studies and references for automated application modernization at <a href="https://www.bluage.com/references">https://www.bluage.com/references</a>.</p>
<p>We all know that there are two key data-processing methods. Batch or bulk processing is all about batching or accumulating data to initiate the processing at scheduled hours. However, with the availability of real-time data-processing technologies and platforms, real-time data processing is picking up. Also, data starts to lose its value as time passes, and hence it should be collected, cleansed, and crunched immediately in order to extricate timely insights. In the mainframe era, batch processing was the main method for data processing due to inherent IT resource constraints. Legacy applications are predominantly single-threaded and hence parallel execution isn't possible at the language level. With multi-threaded languages and applications, parallel execution gains immense prominence. With multi-core and multi-processor computers becoming affordable, parallel processing at the infrastructure level is being achieved. With the emergence of virtual machines and containers, having multiple instances of these server resources leads us to fast-track the application's execution. Newer programming languages intrinsically support multi-threading and hence concurrent processing is prominent these days. With cloud infrastructures increasingly being compartmentalized, the goal of doing tasks in parallel has gained momentum. Now, with the surge of edge, local, and remote cloud environments, these restrictions are slowly fading away and real-time analytics is booming. That is, legacy applications that previously did batch processing are modernized to do real-time processing using cloud-based platforms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The combination of microservices and serverless computing speeds up legacy modernization</h1>
                </header>
            
            <article>
                
<p>As mentioned previously, microservices can be orchestrated to craft bigger and better applications. For legacy modernization, business capabilities are subdivided into discrete microservices. On the reverse side, these easily-manageable, independently-deployable, and autonomous services can be composed into process-aware, business-critical, and composite services. Microservices are operating in their own environments and interact with other services in a loosely-coupled manner. Thus, microservices are isolated and hence if there's a problem with one microservice, it doesn't affect other microservices. Because of this independence, microservices can be replaced and substituted by advanced services, replicated across, updated, and upgraded without impacting others.</p>
<p class="mce-root"/>
<p>Serverless computing is a recent phenomenon unleashed by various public <strong>cloud service providers</strong> (<strong>CSPs</strong>), such as AWS, Azure, IBM, and Google. The server infrastructure is being readied and taken care of by CSPs for smoothly-running functions. That's why the buzzword <strong>function as a service</strong> (<strong>FaaS</strong>) is being widely reported. The idea is as follows. We started with bare-metal servers. Then virtual machines and containers came into the picture as the optimized resource and runtime for applications. Now, we are heading toward functions. Many developers started to create modular systems using code-level components (functions). These code-level components are attached to current applications on a per-need basis in order to enable applications to be relevant to their constituents. A library is a collection of functions. Then the empowered applications are compiled and deployed to be subscribed to and used for a small fee. This arrangement can guarantee high performance. But the reusability of functions is not up to the level of microservices.</p>
<p>Primarily, the configuration management challenges of functions cause a lot of trouble. That is, different application projects may mandate different versions of the function. If a library of functions gets updated, all the applications that depend on the library have to go through a series of updates, recompiles, and redeployments. On the other hand, as repeatedly written in this book, microservices are self-contained and the goal of reusability is easily accomplished.</p>
<p>Serverless computing is emerging as an ideal underpinning for hosting and running microservices. Thus, it's clear that the MSA and serverless computing will provide an extensible and scalable environment. With the automation level rising continuously in cloud environments, service developers and assemblers won't need to bother setting up infrastructural components in order to run, verify, and showcase their services. Through containers, functions are deployed as an accessible entity. Containers can be quickly created to run functions and microservices. Containers are famous for real-time horizontal scalability. When the number of users goes up, new instances get provisioned immediately in order to tackle the extra load.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Microservices enabled with RESTful APIs are the hot commodity these days. Microservices have emerged as the optimal unit of application development and deployment, not only for building and running enterprise-grade and production-ready applications, but also for modernizing currently-running applications. That is, legacy applications are being dismantled as a collection of microservices. Because of their unique features, microservices are becoming established as the most appropriate unit for migrating applications to cloud environments; that is, microservices contribute immensely to crafting and running cloud-enabled applications. Fresh applications are being directly developed in cloud environments, called cloud-native applications. This chapter discussed legacy modernization, why it's becoming essential, and how the MSA pattern assists in creating modern applications from outdated applications.</p>
<p>With the unprecedented adoption of the microservices architecture as the most beneficial architectural style for designing and developing next-generation business-critical and IoT applications, the RESTful paradigm has earned a new lease of life. That is, due to the simplicity and sustainability of RESTful APIs, microservices are increasingly stuffed with RESTful APIs. This book covered the practical and theoretical information aspects of how RESTful services and APIs contribute to futuristic and flexible web/cloud, mobile, and IoT applications. </p>


            </article>

            
        </section>
    </body></html>