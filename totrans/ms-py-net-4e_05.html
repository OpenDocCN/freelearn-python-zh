<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer076">
    <h1 class="chapterNumber">5</h1>
    <h1 class="chapterTitle" id="_idParaDest-134">Docker Containers for Network Engineers</h1>
    <p class="normal">Computer hardware virtualization has revolutionized and changed the way we approach infrastructure. Gone are the days when we must dedicate hardware to a single host and operating system. We now have the option to share precious hardware such as CPU, memory, and disk space with multiple virtual machines, each with its own operating system and applications. Because software executed on these virtual machines is separated from the underlying hardware resources, we are free to allocate a different combination of hardware resources to virtual machines based on their specific needs. Nowadays, it is hard to imagine a world without virtual machines. </p>
    <p class="normal">As much as virtual machines are great for application building, they do take a while to build, spin up, and, ultimately, tear down. The reason is that the virtualization technology associated with virtual machines completely simulates the actual hardware for which the hardware is indistinguishable from the guest virtual machines. </p>
    <p class="normal">The question might now be: is there a way to speed up the life cycle of applications with even more virtualization? The answer is yes, with the help of containers.</p>
    <p class="normal">Containers and virtual machines are similar in that they both allow sharing of computing resources amongst different isolated applications. The difference is that virtual machines are abstracted at the Hypervisor level, whereas containers are abstracted within the operating system by a container engine. Containers<a id="_idIndexMarker380"/> are often referred to as OS-level virtualization. </p>
    <figure class="mediaobject"><img alt="Graphical user interface  Description automatically generated" src="../Images/B18403_05_01.png"/></figure>
    <p class="packt_figref">Figure 5.1: Virtual Machine and Container Comparison (source: https://www.atlassian.com/microservices/cloud-computing/containers-vs-vms)</p>
    <p class="normal">In a full virtual machine, we can install different operating systems, such as Windows and Linux. Because container virtualization is being handled by the operating system, each container will have the same operating system. However, the application and its associated resources will be isolated and run independently of each other. The container engine will separate the configuration, software bundle, and libraries from each container. </p>
    <p class="normal">Container <a id="_idIndexMarker381"/>virtualization is not new; <strong class="keyWord">Linux containers</strong> (<strong class="keyWord">LXC</strong>), Solaris containers, Docker, and Podman are examples of such implementation. In this chapter, we will look at the most popular container technology today, Docker. We will discuss the following topics related to Docker containers: </p>
    <ul>
      <li class="bulletList">Docker overview</li>
      <li class="bulletList">Building Python applications with Docker </li>
      <li class="bulletList">Container networking</li>
      <li class="bulletList">Containers in the network engineering field</li>
      <li class="bulletList">Docker and Kubernetes</li>
    </ul>
    <p class="normal">We will be using containers for some of the technologies we will learn in this book; this is a good place to start getting familiar with containers. </p>
    <p class="normal">Let’s start by looking at a high-level overview of Docker. </p>
    <h1 class="heading-1" id="_idParaDest-135">Docker Overview</h1>
    <p class="normal">Docker is a set of <a id="_idIndexMarker382"/>products and tools that supports the delivery of containers. It was started by the company dotCloud in 2008 (renamed to Docker, Inc. in 2013). The set of tools includes the container technology of Docker, the container engine called Docker Engine, the cloud-based <a id="_idIndexMarker383"/>repository of containers called Docker Hub, and <a id="_idIndexMarker384"/>the desktop graphical user interface software called <a id="_idIndexMarker385"/>Docker Desktop. </p>
    <p class="normal">Docker has<a id="_idIndexMarker386"/> two versions, <strong class="keyWord">Docker Community Edition</strong> (<strong class="keyWord">Docker-CE</strong>) and <strong class="keyWord">Docker Enterprise Edition</strong> (<strong class="keyWord">Docker-EE</strong>). Docker-CE is a<a id="_idIndexMarker387"/> free and open-source platform based on the Apache 2.0 license, while Docker-EE is a premium version geared toward enterprises. When the term “Docker” is mentioned in this book, we are referring to the Community Edition. </p>
    <p class="normal">There are three main components in a Docker container environment: </p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Building and <a id="_idIndexMarker388"/>Development: These include the tools used to build a container, including the CLI commands, the images, and the repositories where we get the various base images. In Docker, we use a Dockerfile to specify most of the building steps for a container. </li>
      <li class="numberedList">Docker Engine: This is the daemon<a id="_idIndexMarker389"/> running in the background. We can use the Docker command to manage the daemon. </li>
      <li class="numberedList">Container Orchestration: During<a id="_idIndexMarker390"/> development, we will typically use <code class="inlineCode">Docker-compose</code> from Docker to manage a multi-container environment. In production, a common tool is a<a id="_idIndexMarker391"/> Google-originated tool called Kubernetes (<a href="https://kubernetes.io/"><span class="url">https://kubernetes.io/</span></a>).</li>
    </ol>
    <p class="normal">In the next section, we will discuss the advantages of Docker. </p>
    <h2 class="heading-2" id="_idParaDest-136">Advantages of Docker</h2>
    <p class="normal">There are many <a id="_idIndexMarker392"/>advantages of Docker. We will summarize some of them here: </p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Docker containers are fast to deploy and destroy. </li>
      <li class="numberedList">Containers reset gracefully. The containers are transient and ephemeral, leaving no residual artifacts when restarted. This leaves a clean state whenever a new container is spawned. </li>
      <li class="numberedList">It is self-contained and deterministic. Containers are often delivered with configuration files with instructions on how the container can be rebuilt. We can be sure each container image is built in the same way. </li>
      <li class="numberedList">It allows seamless integration between application development and DevOps. Because of the advantages stated above, many companies have deployed. Docker images directly in the production environment. The container can be reproduced exactly as the developer intended and tested into production. </li>
    </ol>
    <p class="normal">Now that we have a general <a id="_idIndexMarker393"/>understanding of Docker, it is time to build our first Python applications in a Docker container. </p>
    <h1 class="heading-1" id="_idParaDest-137">Building Python applications in Docker</h1>
    <p class="normal">A Docker container is a very<a id="_idIndexMarker394"/> popular way to build Python applications. </p>
    <h2 class="heading-2" id="_idParaDest-138">Installing Docker</h2>
    <p class="normal">Of course, we will need to<a id="_idIndexMarker395"/> install Docker to start using it. We will follow DigitalOcean’s excellent installation guide for Ubuntu 22.04 (<a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-22-04"><span class="url">https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-22-04</span></a>). If you are using other versions of the Linux distribution, you can simply use the drop-down menu from the documentation to pick a different version. For installation on Mac or Windows, my<a id="_idIndexMarker396"/> recommendation would be to install Docker Desktop (<a href="https://docs.docker.com/desktop/"><span class="url">https://docs.docker.com/desktop/</span></a>). It will include the Docker Engine, CLI client, and GUI application. </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>sudo apt-get update
<span class="hljs-con-meta">$ </span>sudo apt-get -y upgrade
<span class="hljs-con-meta">$ </span>sudo apt install apt-transport-https ca-certificates curl software-properties-common
<span class="hljs-con-meta">$ </span>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">echo</span> <span class="hljs-con-string">"deb [arch=</span><span class="hljs-con-subst">$(dpkg --print-architecture)</span><span class="hljs-con-string"> signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu </span><span class="hljs-con-subst">$(lsb_release -cs)</span><span class="hljs-con-string"> stable"</span> | sudo <span class="hljs-con-built_in">tee</span> /etc/apt/sources.list.d/docker.list &gt; /dev/null
<span class="hljs-con-meta">$ </span>sudo apt update
<span class="hljs-con-meta">$ </span>apt-cache policy docker-ce
<span class="hljs-con-meta">$ </span>sudo apt install docker-ce
</code></pre>
    <div class="packt_tip">
      <p class="normal">There are some<a id="_idIndexMarker397"/> optional but useful post-installation steps for Linux at <a href="https://docs.docker.com/engine/install/linux-postinstall/"><span class="url">https://docs.docker.com/engine/install/linux-postinstall/</span></a>.</p>
    </div>
    <p class="normal">We can check the status of our<a id="_idIndexMarker398"/> Docker installation: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>sudo systemctl status docker
● docker.service - Docker Application Container Engine
     Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)
     Active: active (running) since Sun 2022-09-11 15:02:27 PDT; 5s ago
TriggeredBy: ● docker.socket
       Docs: https://docs.docker.com
   
</code></pre>
    <p class="normal">In the next section, we will see how we can build a Python application in Docker containers. </p>
    <h2 class="heading-2" id="_idParaDest-139">Useful Docker commands</h2>
    <p class="normal">We will need to use some commands to build, run, and test our containers. </p>
    <div class="note">
      <p class="normal">For more <a id="_idIndexMarker399"/>Docker CLI references, check out the documentation: <a href="https://docs.docker.com/engine/reference/run/"><span class="url">https://docs.docker.com/engine/reference/run/</span></a>.</p>
    </div>
    <p class="normal">Here are some of the <a id="_idIndexMarker400"/>commands we will be using in this chapter: </p>
    <ul>
      <li class="bulletList"><code class="inlineCode">docker run</code>: <code class="inlineCode">docker run</code> is used to specify the image to derive the container from (by default, it is Docker Hub), network settings, name, and other settings. </li>
      <li class="bulletList"><code class="inlineCode">docker container ls</code>: lists the containers; by default, it only lists currently running containers. </li>
      <li class="bulletList"><code class="inlineCode">docker exec</code>: runs a command on a running container. </li>
      <li class="bulletList"><code class="inlineCode">docker network</code>: used when we need to manage Docker networks, such as to create, list, and remove Docker networks. </li>
      <li class="bulletList"><code class="inlineCode">docker image</code>: manages Docker images. </li>
    </ul>
    <p class="normal">There are many more <a id="_idIndexMarker401"/>CLI commands, but these are enough to get us started. For a complete reference, check out the link provided in the information box. </p>
    <h2 class="heading-2" id="_idParaDest-140">Building hello world </h2>
    <p class="normal">The first step is to <a id="_idIndexMarker402"/>make sure we have reachability to Docker Hub to retrieve an image. To do so, Docker provides a very simple <code class="inlineCode">hello-world</code> app: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker run hello-world
Hello from Docker!
This message shows that your installation appears to be working correctly.
To generate this message, Docker took the following steps:
 1. The Docker client contacted the Docker daemon.
 2. The Docker daemon pulled the "hello-world" image from the Docker Hub.
    (amd64)
 3. The Docker daemon created a new container from that image which runs the
    executable that produces the output you are currently reading.
 4. The Docker daemon streamed that output to the Docker client, which sent it
    to your terminal.
&lt;skip&gt;
</code></pre>
    <p class="normal">We can see the various steps the Docker client needed to do to display the message. We can display the Docker processes that ran: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker ps -a
CONTAINER ID   IMAGE                  COMMAND            CREATED              STATUS                          PORTS     NAMES
3cb4f91b6388   hello-world            "/hello"           About a minute ago   Exited (0) About a minute ago             fervent_torvalds
</code></pre>
    <p class="normal">We can see the <code class="inlineCode">hello-world</code> image information: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker images hello-world
REPOSITORY    TAG       IMAGE ID       CREATED         SIZE
hello-world   latest    feb5d9fea6a5   11 months ago   13.3kB
</code></pre>
    <p class="normal">Now we can build <a id="_idIndexMarker403"/>our first Python application. </p>
    <h2 class="heading-2" id="_idParaDest-141">Building our application</h2>
    <p class="normal">Let’s start by thinking<a id="_idIndexMarker404"/> about what we will build. Since we built a few Ansible playbooks in the last chapter, how about we containerize the <code class="inlineCode">ios_config_backup.yml</code> playbook so we can share this with other team members? </p>
    <p class="normal">We will create a new folder to keep all the files together. If you recall, for us to build a Docker image, there is a <a id="_idIndexMarker405"/>special file called a Dockerfile. We will also create such a file in the directory:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">mkdir</span> ansible_container &amp;&amp; <span class="hljs-con-built_in">cd</span> ansible_container
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">touch</span> Dockerfile
</code></pre>
    <p class="normal">We will also copy the <em class="italic">host_vars</em> folder, <em class="italic">ansible.cfg</em>, <em class="italic">hosts</em>, and <em class="italic">ios_config_backup.yml</em> files into this folder. We should also make sure the playbook runs as expected before we build the Docker container from it. </p>
    <p class="normal">Docker builds itself in a layered fashion, starting with a base image. In the Dockerfile, we will specify the following lines: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Getting base image</span>
<span class="hljs-keyword">FROM</span> ubuntu:<span class="hljs-number">22.04</span>
<span class="hljs-comment"># No need for interactive prompt</span>
<span class="hljs-keyword">ENV</span> DEBIAN_FRONTEND=noninteractive
</code></pre>
    <p class="normal">The lines starting with a “#” mark are comments, just like in Python. The <code class="inlineCode">FROM</code> keyword specifies the base image we will retrieve from the default Docker Hub. All the official Ubuntu images can be found on the site, <a href="https://hub.docker.com/_/ubuntu"><span class="url">https://hub.docker.com/_/ubuntu</span></a>. In the <code class="inlineCode">ENV</code> statement, we specified no need for interactive prompts. </p>
    <div class="packt_tip">
      <p class="normal">The <a id="_idIndexMarker406"/>Dockerfile reference can be viewed at <a href="https://docs.docker.com/engine/reference/builder/"><span class="url">https://docs.docker.com/engine/reference/builder/</span></a>.</p>
    </div>
    <p class="normal">Let us build this image: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker build --tag ansible-docker:v0.1 .
</code></pre>
    <p class="normal">The <code class="inlineCode">build</code> command builds from<a id="_idIndexMarker407"/> the Dockerfile in the local directory while tagging the final image to be <code class="inlineCode">ansible-docker</code> with version 0.1. Once completed, we can view the image: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker images
REPOSITORY              TAG             IMAGE ID       CREATED         SIZE
ansible-docker          v0.1            e99f103e2d36   3 seconds ago   864MB
</code></pre>
    <div class="note">
      <p class="normal">If we need to remove an image before the rebuild, we can delete the image with “<code class="inlineCode">docker rmi &lt;image id&gt;</code>.”</p>
    </div>
    <p class="normal">We can start the container based on the image: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker run -it --name ansible-host1 ansible-docker:v0.1
root@96108c94e1d2:/# lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 22.04.1 LTS
Release:        22.04
Codename:       jammy
root@96108c94e1d2:/#
</code></pre>
    <p class="normal">It will drop us into the bash shell prompt, and the container will stop itself once we exit. In order for it to run in a detached mode, we will need to start it with a <code class="inlineCode">"-d"</code> flag. Let’s go ahead and delete the container and recreate it with the flag: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker ps -a
CONTAINER ID   IMAGE                 COMMAND   CREATED         STATUS                      PORTS     NAMES
&lt;container id&gt;   ansible-docker:v0.1   "bash"    2 minutes ago   Exited (0) 52 seconds ago             ansible-host1
<span class="hljs-con-meta">$ </span>docker <span class="hljs-con-built_in">rm</span> &lt;container <span class="hljs-con-built_in">id</span>&gt;
<span class="hljs-con-meta">$ </span>docker run -it -d --name ansible-host1 ansible-docker:v0.1
</code></pre>
    <div class="packt_tip">
      <p class="normal">Remember to substitute your container ID. A nice shortcut to delete all containers in one setting is <code class="inlineCode">docker rm -f $(docker ps -a -q)</code>.</p>
    </div>
    <p class="normal">The container now runs in <a id="_idIndexMarker408"/>detached mode, and we can execute an interactive prompt on the container: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker ps
CONTAINER ID   IMAGE                 COMMAND   CREATED              STATUS          PORTS     NAMES
d3b6a6ec90e5   ansible-docker:v0.1   "bash"    About a minute ago   Up 58 seconds             ansible-host1
<span class="hljs-con-meta">$ </span>docker <span class="hljs-con-built_in">exec</span> -it ansible-host1 bash
root@d3b6a6ec90e5:/# ls
</code></pre>
    <p class="normal">We can go ahead and stop the container, then delete it: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker stop ansible-host1
<span class="hljs-con-meta">$ </span>docker <span class="hljs-con-built_in">rm</span> ansible-host1
</code></pre>
    <p class="normal">We will introduce a few more Dockerfile commands: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Getting base image</span>
<span class="hljs-keyword">FROM</span> ubuntu:<span class="hljs-number">22.04</span>
<span class="hljs-comment"># No need for interactive prompt</span>
<span class="hljs-keyword">ENV</span> DEBIAN_FRONTEND=noninteractive
<span class="hljs-comment"># Run any command, i.e. install packages</span>
<span class="hljs-keyword">RUN</span> apt update &amp;&amp; apt install -y python3.10 python3-pip ansible vim
<span class="hljs-keyword">RUN</span> pip install ansible-pylibssh
<span class="hljs-comment"># specify a working directory</span>
<span class="hljs-keyword">WORKDIR</span> /app
<span class="hljs-keyword">COPY</span> . /app
</code></pre>
    <p class="normal">The <code class="inlineCode">RUN</code> command executes the shell commands as if we were typing them in the shell. We can specify the working directory as <code class="inlineCode">/app</code> on the container, then copy everything in the current working directory (<code class="inlineCode">host_vars</code>, hosts, playbook, etc.) to the <code class="inlineCode">/app</code> directory on the remote container. </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker images
&lt;find the image id&gt;
<span class="hljs-con-meta">$ </span>docker rmi &lt;image <span class="hljs-con-built_in">id</span>&gt;
<span class="hljs-con-meta">$ </span>docker build --tag ansible-docker:v0.1 .
</code></pre>
    <div class="packt_tip">
      <p class="normal">We will keep the same tag, but if we would like to make it a new release, we can always tag it as <code class="inlineCode">v0.2</code>.</p>
    </div>
    <p class="normal">We will launch the <a id="_idIndexMarker409"/>container again and execute the <code class="inlineCode">ansible-playbook</code>: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker run -it -d --name ansible-host1 ansible-docker:v0.1
docker exec -it ansible-host1 bash
root@5ef5e9c85065:/app# pwd
/app
root@5ef5e9c85065:/app# ls
ansible.cfg  dockerfile  host_vars  hosts  ios_config_backup.yml
root@5ef5e9c85065:/app# ansible-playbook -i hosts ios_config_backup.yml 
PLAY [Back Up IOS Device Configurations] ********************************************************************
TASK [backup] ***********************************************************************************************
changed: [iosv-2]
changed: [iosv-1]
PLAY RECAP **************************************************************************************************
iosv-1                     : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
iosv-2                     : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
root@5ef5e9c85065:/app# ls backup/
iosv-1_config.2022-09-12@23:01:07  iosv-2_config.2022-09-12@23:01:07
</code></pre>
    <p class="normal">Once the container is launched, we can start and stop via the hostname: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker stop ansible-host1
<span class="hljs-con-meta">$ </span>docker start ansible-host1
</code></pre>
    <p class="normal">Congratulations on working through the complete container workflow! This might not seem much now, but it is a big step. The steps might seem a bit foreign now, but don’t worry, they will become<a id="_idIndexMarker410"/> more familiar as we get more practice under our belt. </p>
    <h2 class="heading-2" id="_idParaDest-142">Sharing Docker images</h2>
    <p class="normal">The last step will be to share<a id="_idIndexMarker411"/> the container images. One way to do it would be to tar zip the directory and share the file. Another way is to push the image to a repository accessible to whoever needs access. Docker Hub is one of the most popular repositories, but many others exist. They generally offer several different subscription price tiers. </p>
    <figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" src="../Images/B18403_05_02.png"/></figure>
    <p class="packt_figref">Figure 5.2: Docker Hub Pricing (source: <a href="https://www.docker.com/pricing/"><span class="url">https://www.docker.com/pricing/</span></a>)</p>
    <p class="normal">Besides sharing the container image, having an accessible repository is crucial in a DevOps <strong class="keyWord">CI/CD</strong> (<strong class="keyWord">Continuous Integration/Continuous Delivery</strong>) process. For example, we might be checking in the code with an automated build and test process. Once all the validation test passes, we can automatically push the image to the repository and deploy it to production. We will create a private repository on Docker Hub: </p>
    <figure class="mediaobject"><img alt="Graphical user interface, text, application  Description automatically generated" src="../Images/B18403_05_03.png"/></figure>
    <p class="packt_figref">Figure 5.3: Docker Hub Repository</p>
    <p class="normal">Then we will log in via the<a id="_idIndexMarker412"/> Docker CLI: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker login
</code></pre>
    <p class="normal">Then we can tag the existing image following the remote repository, then push toward it. Notice in the output<a id="_idIndexMarker413"/> below that the destination tag name matches the repository name on Docker Hub. This allows flexibility in local naming while conforming to the remote team naming conventions. </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker tag ansible-docker:v0.1 ericchou1/mastering-python-networking-example:ch05-ansible-dockerv-v0.1
<span class="hljs-con-meta">$ </span>docker push ericchou1/mastering-python-networking-example:ch05-ansible-dockerv-v0.1
</code></pre>
    <p class="normal">Once the image finishes uploading, we can access the image and we can use it directly or use it as a base image in another Dockerfile.</p>
    <figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" src="../Images/B18403_05_04.png"/></figure>
    <p class="packt_figref">Figure 5.4: New Uploaded Image</p>
    <p class="normal">In the next section, we <a id="_idIndexMarker414"/>will see how to coordinate multi-container setup locally during development. </p>
    <h2 class="heading-2" id="_idParaDest-143">Container orchestration with Docker-compose</h2>
    <p class="normal">Modern applications often have interdependencies with each other. For example, for a web application, we usually <a id="_idIndexMarker415"/>have a “stack” of applications. The popular LAMP stack is an acronym denoting Linux, Apache, MySQL, and PHP/Python to specify the components required to deliver a web application. In the world of Docker, we can use <a id="_idIndexMarker416"/>docker-compose (<a href="https://docs.docker.com/compose/"><span class="url">https://docs.docker.com/compose/</span></a>) to specify how multiple containers should be built and run simultaneously. </p>
    <p class="normal">If you have installed Docker Desktop for Mac or Windows, docker-compose is already included. In the Linux environment, docker-compose needs to be installed separately. We will follow DigitalOcean’s guide for docker-compose (<a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-compose-on-ubuntu-22-04"><span class="url">https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-compose-on-ubuntu-22-04</span></a>): </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">mkdir</span> -p ~/.docker/cli-plugins/
<span class="hljs-con-meta">$ </span>curl -SL https://github.com/docker/compose/releases/download/v2.3.3/docker-compose-linux-x86_64 -o ~/.docker/cli-plugins/docker-compose
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">chmod</span> +x ~/.docker/cli-plugins/docker-compose
<span class="hljs-con-meta">$ </span>docker compose version
Docker Compose version v2.3.3
</code></pre>
    <p class="normal">Docker-compose uses a YAML file named <code class="inlineCode">docker-compose.yml</code> to construct the environment. There are<a id="_idIndexMarker417"/> lots of knobs to specify different service dependencies, persistent volumes, and opening public ports. Let’s put together a simple example: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">version:</span> <span class="hljs-string">'3.9'</span>
<span class="hljs-attr">services:</span>
  <span class="hljs-attr">ansible:</span> 
    <span class="hljs-attr">build:</span> 
      <span class="hljs-attr">dockerfile:</span> <span class="hljs-string">dockerfile</span>
  <span class="hljs-attr">db:</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">postgres:14.1-alpine</span>
    <span class="hljs-attr">environment:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">POSTGRES_USER=postgres</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">POSTGRES_PASSWORD=postgres</span>
    <span class="hljs-attr">ports:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">'5432:5432'</span>
    <span class="hljs-attr">volumes:</span> 
      <span class="hljs-bullet">-</span> <span class="hljs-string">db:/var/lib/postgresql/data</span>
<span class="hljs-attr">volumes:</span>
  <span class="hljs-attr">db:</span>
    <span class="hljs-attr">driver:</span> <span class="hljs-string">local</span>
</code></pre>
    <p class="normal">Here is what the file specifies: </p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">The file specifies two services, <code class="inlineCode">ansible</code> and <code class="inlineCode">db</code>. Each of the services is similar to the <code class="inlineCode">docker run</code> commands. </li>
      <li class="numberedList">The <code class="inlineCode">ansible</code> service builds with the current Dockerfile in the current working directory named <code class="inlineCode">dockerfile</code>.</li>
      <li class="numberedList">We map the host port 5434 to the container port 5434. </li>
      <li class="numberedList">We specify two environmental variables for the Postgres database. </li>
      <li class="numberedList">We use a volume named <code class="inlineCode">db</code> so that the database information written is persisted in the volume.</li>
    </ol>
    <div class="note">
      <p class="normal">For more information on Docker-compose, please visit <a href="https://docs.docker.com/compose/"><span class="url">https://docs.docker.com/compose/</span></a>.</p>
    </div>
    <p class="normal">We can run the <a id="_idIndexMarker418"/>combined service with the <em class="italic">docker-compose</em> command: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker compose up
…
 Container ansible_container-db-1       Created                                                       0.0s
  Container ansible_container-ansible-1  Created                                                       0.0s
ansible_container-db-1       | 
ansible_container-db-1       | PostgreSQL Database directory appears to contain a database; Skipping initialization
ansible_container-db-1       | 
ansible_container-db-1       | 2022-09-13 00:18:45.195 UTC [1] LOG:  starting PostgreSQL 14.1 on x86_64-pc-linux-musl, compiled by gcc (Alpine 10.3.1_git20211027) 10.3.1 20211027, 64-bit
ansible_container-db-1       | 2022-09-13 00:18:45.196 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
ansible_container-db-1       | 2022-09-13 00:18:45.196 UTC [1] LOG:  listening on IPv6 address "::", port 5432
ansible_container-db-1       | 2022-09-13 00:18:45.198 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
ansible_container-db-1       | 2022-09-13 00:18:45.201 UTC [21] LOG:  database system was shut down at 2022-09-13 00:18:36 UTC
ansible_container-db-1       | 2022-09-13 00:18:45.204 UTC [1] LOG:  database system is ready to accept connections
…
</code></pre>
    <p class="normal">The services are launched concurrently. We can then tear down both services: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker compose down
[+] Running 3/3
  Container ansible_container-db-1       Removed                                                       0.2s
  Container ansible_container-ansible-1  Removed                                                       0.0s
  Network ansible_container_default      Removed                                                       0.1s
</code></pre>
    <p class="normal">We have only built simple applications thus far in the book. This might make more sense when we learn about building a Web API later in the book. For now, it is good to consider how we can launch multiple containers via docker-compose. </p>
    <p class="normal">As network engineers, it<a id="_idIndexMarker419"/> would be interesting to know how networking is done in a Docker environment. That is the subject of the next section.</p>
    <h1 class="heading-1" id="_idParaDest-144">Container networking</h1>
    <p class="normal">Container networking is not an easy topic to cover because of its scope and the number of technologies it touches. The <a id="_idIndexMarker420"/>space spans from Linux networking, how the particular type of Linux (Ubuntu, Red Hat, etc.) implements networking, to Docker’s implementation of networking. Adding to the complexity is the fact that Docker is a fast-moving project, and many third-party plugins are available.</p>
    <p class="normal">In this section, we will stick to the basics of the networking options offered by Docker by default. We will then briefly explain the options of overlay, Macvlan, and network plugins. </p>
    <p class="normal">When we launch a container, it can reach the internet by default. Let’s do a quick test by launching an Ubuntu container and attaching to it: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker run -it ubuntu:22.04
&lt;container launches and attached&gt;
root@dcaa61a548be:/# apt update &amp;&amp; apt install -y net-tools iputils-ping
root@dcaa61a548be:/# ifconfig
eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet 172.17.0.2  netmask 255.255.0.0  broadcast 172.17.255.255
&lt;skip&gt;
root@dcaa61a548be:/# ping -c 1 www.cisco.com
PING e2867.dsca.akamaiedge.net (104.71.231.76) 56(84) bytes of data.
64 bytes from a104-71-231-76.deploy.static.akamaitechnologies.com (104.71.231.76): icmp_seq=1 ttl=53 time=11.1 ms
--- e2867.dsca.akamaiedge.net ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 11.147/11.147/11.147/0.000 ms
</code></pre>
    <p class="normal">We can see the host has a private IP different from our host’s IP. It can also reach the Ubuntu repository to install software as well as ping the outside network. How does it do that? By default, Docker created three types of networks: <code class="inlineCode">bridge</code>, <code class="inlineCode">host</code>, and <code class="inlineCode">none</code>. Let’s launch a second Terminal <a id="_idIndexMarker421"/>window while keeping the host running in the first Terminal window: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker network <span class="hljs-con-built_in">ls</span>
NETWORK ID     NAME      DRIVER    SCOPE
78e7ab7ea276   bridge    bridge    local
93c142329fc9   host      host      local
da9fe0ed2308   none      null      local
</code></pre>
    <p class="normal">The <em class="italic">none</em> network option is straightforward. It disables all networking and makes the container sit on a network island by itself. This leaves us with the <code class="inlineCode">bridge</code> and <code class="inlineCode">host</code> options. By default, Docker puts the host in the bridge<a id="_idIndexMarker422"/> network, <code class="inlineCode">docker0</code>, with a <strong class="keyWord">virtual Ethernet</strong> (<strong class="keyWord">veth</strong>) interface (<a href="https://man7.org/linux/man-pages/man4/veth.4.html"><span class="url">https://man7.org/linux/man-pages/man4/veth.4.html</span></a>) to allow it to communicate to the internet: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>ip <span class="hljs-con-built_in">link</span> show
3: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default 
    link/ether 02:42:86:7f:f2:40 brd ff:ff:ff:ff:ff:ff
21: veth3fda84e@if20: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default 
    link/ether 9a:f8:83:ae:cb:ea brd ff:ff:ff:ff:ff:ff link-netnsid 0
</code></pre>
    <p class="normal">If we launch another container, we will see an additional veth interface created and put into the same bridge group. By default, they can communicate with each other. </p>
    <h2 class="heading-2" id="_idParaDest-145">Container host network</h2>
    <p class="normal">We can also share the host <a id="_idIndexMarker423"/>network with the container. Let’s start an Ubuntu container in the host network. We will also install Python 3.10 and other software packages: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker run -it --network host ubuntu:22.04
root@network-dev-4:/# apt update &amp;&amp; apt install -y net-tools iputils-ping python3.10 vim
root@network-dev-4:/# ifconfig ens160
ens160: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet 192.168.2.126  netmask 255.255.255.0  broadcast 192.168.2.255
</code></pre>
    <p class="normal">If we check now, we can see the container now shares the same IP as the host network. We can create a simple HTML <a id="_idIndexMarker424"/>page and start the Python3 built-in web server on the container: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attribute">root</span>@network-dev-<span class="hljs-number">4</span>:/# cat index.html 
<span class="hljs-section">&lt;html&gt;</span>
<span class="hljs-section">&lt;head&gt;&lt;/head&gt;</span>
<span class="hljs-section">&lt;body&gt;&lt;h1&gt;</span><span class="hljs-attribute">Hello</span> Networkers!&lt;/h1&gt;&lt;/body&gt;
<span class="hljs-section">&lt;/html&gt;</span> 
<span class="hljs-attribute">root</span>@network-dev-<span class="hljs-number">4</span>:/# python3.<span class="hljs-number">10</span> -m http.server
<span class="hljs-attribute">Serving</span> HTTP <span class="hljs-literal">on</span> <span class="hljs-number">0.0.0.0</span> port <span class="hljs-number">8000</span> (http://<span class="hljs-number">0.0.0.0:8000</span>/) ...
</code></pre>
    <p class="normal">If we open up the IP address with port 8000 in a browser, we can see the page we created! </p>
    <figure class="mediaobject"><img alt="Graphical user interface, text, application  Description automatically generated" src="../Images/B18403_05_05.png"/></figure>
    <p class="packt_figref">Figure 5.5: Index Page of Container Host</p>
    <div class="packt_tip">
      <p class="normal">If you have a firewall on your host (such as iptables or ufw) turned on, make sure to open up port 8000 so you can see the page.</p>
    </div>
    <p class="normal">The host network option is useful when we need to expose containers for public service. </p>
    <h2 class="heading-2" id="_idParaDest-146">Custom bridge network</h2>
    <p class="normal">We can also create custom <a id="_idIndexMarker425"/>bridge networks and group containers together. We will create the network first: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker network create network1
</code></pre>
    <p class="normal">We can now assign the containers to the custom bridge network: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker run -it --network network1 ubuntu:22.04
root@41a977cd9c5b:/# apt update &amp;&amp; apt install -y net-tools iputils-ping
root@41a977cd9c5b:/# ifconfig
eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet 172.18.0.2  netmask 255.255.0.0  broadcast 172.18.255.255
&lt;skip&gt;
root@41a977cd9c5b:/# ping -c 1 www.cisco.com
PING e2867.dsca.akamaiedge.net (23.206.3.148) 56(84) bytes of data.
64 bytes from a23-206-3-148.deploy.static.akamaitechnologies.com (23.206.3.148): icmp_seq=1 ttl=53 time=13.2 ms
</code></pre>
    <p class="normal">The host is now in its custom <a id="_idIndexMarker426"/>bridge network. It has network access to the public internet and other containers in the same bridge network. If we want to expose a particular port to a container in the custom bridge network, we can use the <code class="inlineCode">–publish</code> option to map the port to the <code class="inlineCode">local host</code>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker run -it --network network1 --publish 8000:8000 ubuntu:22.04
</code></pre>
    <p class="normal">We can remove the network via the <code class="inlineCode">docker network rm</code>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>docker network <span class="hljs-con-built_in">ls</span>
NETWORK ID     NAME       DRIVER    SCOPE
30aa5d7887bc   network1   bridge    local
<span class="hljs-con-meta">$ </span>docker network <span class="hljs-con-built_in">rm</span> network1
</code></pre>
    <p class="normal">The custom network option is great for developing multi-container projects that need isolation from each other. Up to this point, we have been looking at networking options in a single host. In the next section, we will see the options for inter-host communication between containers. </p>
    <h2 class="heading-2" id="_idParaDest-147">Other container network options</h2>
    <p class="normal">If we look<a id="_idIndexMarker427"/> closely at the <code class="inlineCode">docker</code> <code class="inlineCode">network</code> <code class="inlineCode">ls</code> output, we can see the columns of <code class="inlineCode">driver</code> and <code class="inlineCode">scope</code>. Docker’s network subsystem is pluggable, using drivers. The core networking functions were provided by the default drivers of <code class="inlineCode">bridge</code>, <code class="inlineCode">host</code>, and <code class="inlineCode">none</code>.</p>
    <p class="normal">Other notable drivers are listed below: </p>
    <ul>
      <li class="bulletList">Overlay: The overlay network creates a distributed network among multiple Docker daemon hosts. </li>
      <li class="bulletList">Macvlan: The macvlan network option is meant for applications needing to be directly connected to the physical network. </li>
      <li class="bulletList">Third-party network plugins: We can install third-party network plugins (<a href="https://hub.docker.com/search?q=&amp;type=plugin)"><span class="url">https://hub.docker.com/search?q=&amp;type=plugin)</span></a> for additional features. For example, the vSphere-storage plugin (<a href="https://hub.docker.com/r/vmware/vsphere-storage-for-docker"><span class="url">https://hub.docker.com/r/vmware/vsphere-storage-for-docker</span></a>) enables customers to address persistent storage requirements for containers in a vSphere environment. </li>
    </ul>
    <p class="normal">An overlay network driver is probably the option we will need to use beyond the development stage. It is meant to handle the routing of the packets to and from the Docker daemon host and the correct destination container. For example, an overlay ingress network would handle<a id="_idIndexMarker428"/> the incoming traffic and load balance to the correct container. Due to its complexity, this is typically handled by the orchestration tool of choice, such as Swarm or Kubernetes. If we use a public cloud provider, such as Google Kubernetes Engine, they might even handle this overlay network for us. </p>
    <h1 class="heading-1" id="_idParaDest-148">Containers in the network engineering field</h1>
    <p class="normal">Container technologies are transforming how infrastructure is built in modern days. We now have an additional layer of abstraction we can use to overcome limitations on physical space, power, cooling, and other factors. This is especially true of the need to move toward more<a id="_idIndexMarker429"/> environmentally-friendly data centers. </p>
    <p class="normal">There are many <a id="_idIndexMarker430"/>new challenges and opportunities associated with the new container-based world: </p>
    <ul>
      <li class="bulletList">Networking in the container world. As we saw in the last section, there are lots of options that exist when it comes to networking in containers. </li>
      <li class="bulletList">DevOps. One of the challenges when trying to implement DevOps practices in network engineering is the lack of options for flexible, virtualized network devices. Containers can potentially solve that problem if we can virtualize our network along with the hosts. </li>
      <li class="bulletList">Lab and Testing. If we can virtualize our network via container images, this makes lab and testing much easier. </li>
    </ul>
    <p class="normal">We will discuss DevOps in <em class="chapterRef">Chapter 12</em>, <em class="italic">Continuous Integration with</em><em class="chapterRef"> </em><em class="italic">GitLab</em>; in the next section, we will look at a new way to test and run containerized network operating systems. </p>
    <h2 class="heading-2" id="_idParaDest-149">Containerlab</h2>
    <p class="normal">Containerlab (<a href="https://containerlab.dev/"><span class="url">https://containerlab.dev/</span></a>) is a way to run containerized network operating systems. It is a project <a id="_idIndexMarker431"/>started by the team at Nokia led by Roman Dodin (<a href="https://twitter.com/ntdvps"><span class="url">https://twitter.com/ntdvps</span></a>). The <a id="_idIndexMarker432"/>team is also responsible for<a id="_idIndexMarker433"/> developing <strong class="keyWord">SR Linux</strong> (<strong class="keyWord">Service Router Linux</strong>), an open <strong class="keyWord">network operating system</strong> (<strong class="keyWord">NOS</strong>). Although born out of Nokia, Containerlab has multi-vendor<a id="_idIndexMarker434"/> support with Arista cEOS, Azure SONiC, Juniper cRPD, and many others. Let’s do a quick example to illustrate the workflow of Containerlab. To install, we can follow the installation steps (<a href="https://containerlab.dev/install/"><span class="url">https://containerlab.dev/install/</span></a>) for Debian-based systems. To isolate the installation, we can create a new directory:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">mkdir</span> container_lab &amp;&amp; <span class="hljs-con-built_in">cd</span> container_lab
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">echo</span> <span class="hljs-con-string">"deb [trusted=yes] https://apt.fury.io/netdevops/ /"</span> | sudo <span class="hljs-con-built_in">tee</span> -a /etc/apt/sources.list.d/netdevops.list
<span class="hljs-con-meta">$ </span>sudo apt update &amp;&amp; sudo apt install containerlab
</code></pre>
    <p class="normal">We will define a <code class="inlineCode">clab</code> file to define the topology, image, and starting configurations. There are several example labs under <code class="inlineCode">/etc/containerlab/lab-examples/</code>. We will use the two-node lab example (<a href="https://github.com/srl-labs/containerlab/blob/main/lab-examples/srl02/srl2.cfg"><span class="url">https://github.com/srl-labs/containerlab/blob/main/lab-examples/srl02/srl2.cfg</span></a>) with two SR Linux devices connected over an Ethernet interface. Since SR Linux container images can be downloaded over a public repository, this will save us the step of needing to download the container image separately. We will call this lab topology <code class="inlineCode">srl02.clab.yml</code>: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># topology documentation: http://containerlab.dev/lab-examples/two-srls/</span>
<span class="hljs-comment"># https://github.com/srl-labs/containerlab/blob/main/lab-examples/srl02/srl02.clab.yml</span>
<span class="hljs-attr">name:</span> <span class="hljs-string">srl02</span>
<span class="hljs-attr">topology:</span>
  <span class="hljs-attr">nodes:</span>
    <span class="hljs-attr">srl1:</span>
      <span class="hljs-attr">kind:</span> <span class="hljs-string">srl</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">ghcr.io/nokia/srlinux</span>
      <span class="hljs-attr">startup-config:</span> <span class="hljs-string">srl1.cfg</span>
    <span class="hljs-attr">srl2:</span>
      <span class="hljs-attr">kind:</span> <span class="hljs-string">srl</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">ghcr.io/nokia/srlinux</span>
      <span class="hljs-attr">startup-config:</span> <span class="hljs-string">srl2.cfg</span>
  <span class="hljs-attr">links:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">endpoints:</span> [<span class="hljs-string">"srl1:e1-1"</span>, <span class="hljs-string">"srl2:e1-1"</span>]
</code></pre>
    <p class="normal">As indicated in the file, the topology consists of nodes and links. The nodes are the NOS systems, while the<a id="_idIndexMarker435"/> links define how they are connected. The two device configuration files are vendor-specific, in this case, SR Linux configurations: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cat</span> srl1.cfg 
set / interface ethernet-1/1
set / interface ethernet-1/1 subinterface 0
set / interface ethernet-1/1 subinterface 0 ipv4
set / interface ethernet-1/1 subinterface 0 ipv4 address 192.168.0.0/31
set / interface ethernet-1/1 subinterface 0 ipv6
set / interface ethernet-1/1 subinterface 0 ipv6 address 2002::192.168.0.0/127
set / network-instance default
set / network-instance default interface ethernet-1/1.0
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cat</span> srl2.cfg 
set / interface ethernet-1/1
set / interface ethernet-1/1 subinterface 0
set / interface ethernet-1/1 subinterface 0 ipv4
set / interface ethernet-1/1 subinterface 0 ipv4 address 192.168.0.1/31
set / interface ethernet-1/1 subinterface 0 ipv6
set / interface ethernet-1/1 subinterface 0 ipv6 address 2002::192.168.0.1/127
</code></pre>
    <p class="normal">We can now launch the lab with <code class="inlineCode">containerlab</code> <code class="inlineCode">deploy</code>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>sudo containerlab deploy --topo srl02.clab.yml 
[sudo] password for echou: 
INFO[0000] Containerlab v0.31.1 started                 
INFO[0000] Parsing &amp; checking topology file: srl02.clab.yml
…
</code></pre>
    <div class="packt_tip">
      <p class="normal">Technically, we do not need the <code class="inlineCode">—topo</code> option to specify a topology. Containerlab will look for an <code class="inlineCode">*.clab.y*ml</code> topology file by default. However, I find it a good practice to specify a topology file in case we have several topology files in the same directory.</p>
    </div>
    <p class="normal">If successful, we will <a id="_idIndexMarker436"/>see the device information. The device names are in the format of <code class="inlineCode">clab-{ lab name }-{ device name }</code>: </p>
    <pre class="programlisting con"><code class="hljs-con">+---+-----------------+--------------+-----------------------+------+---------+----------------+----------------------+
| # |      Name       | Container ID |         Image         | Kind |  State  |  IPv4 Address  |     IPv6 Address     |
+---+-----------------+--------------+-----------------------+------+---------+----------------+----------------------+
| 1 | clab-srl02-srl1 | 7cae81c710d8 | ghcr.io/nokia/srlinux | srl  | running | 172.20.20.2/24 | 2001:172:20:20::2/64 |
| 2 | clab-srl02-srl2 | c75f274284ef | ghcr.io/nokia/srlinux | srl  | running | 172.20.20.3/24 | 2001:172:20:20::3/64 |
+---+-----------------+--------------+-----------------------+------+---------+----------------+----------------------+
</code></pre>
    <p class="normal">We can access the device via <code class="inlineCode">ssh</code> to the device; the default username and passwords are both <code class="inlineCode">admin</code>: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>ssh admin@172.20.20.3
admin@172.20.20.3's password: 
Using configuration file(s): []
Welcome to the srlinux CLI.
Type 'help' (and press &lt;ENTER&gt;) if you need any help using this.
--{ running }--[  ]--
A:srl1# show version 
-------------------------------------------------------------------------------------------------------------------------------
Hostname             : srl1
Chassis Type         : 7220 IXR-D2
Part Number          : Sim Part No.
Serial Number        : Sim Serial No.
System HW MAC Address: 1A:85:00:FF:00:00
Software Version     : v22.6.3
Build Number         : 302-g51cb1254dd
Architecture         : x86_64
Last Booted          : 2022-09-12T03:12:15.195Z
Total Memory         : 1975738 kB
Free Memory          : 219406 kB
-------------------------------------------------------------------------------------------------------------------------------
--{ running }--[  ]--
A:srl1#
A:srl1# quit
</code></pre>
    <p class="normal">A directory is created <a id="_idIndexMarker437"/>with the associated files for the lab: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">ls</span> clab-srl02/*
clab-srl02/ansible-inventory.yml  clab-srl02/topology-data.json
clab-srl02/ca:
root  srl1  srl2
clab-srl02/srl1:
config  topology.yml
clab-srl02/srl2:
config  topology.yml
</code></pre>
    <p class="normal">We can also see there is an additional bridge network created with the two veth interfaces connected to the bridge network: </p>
    <pre class="programlisting con"><code class="hljs-con">(venv) $ ip link show
11: br-4807fa9091c5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default 
    link/ether 02:42:72:7a:9d:af brd ff:ff:ff:ff:ff:ff
13: veth3392afa@if12: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master br-4807fa9091c5 state UP mode DEFAULT group default 
    link/ether be:f0:1a:f2:12:23 brd ff:ff:ff:ff:ff:ff link-netnsid 1
15: veth7417e97@if14: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master br-4807fa9091c5 state UP mode DEFAULT group default 
    link/ether 92:53:d3:ac:20:93 brd ff:ff:ff:ff:ff:ff link-netnsid 0
</code></pre>
    <p class="normal">We can tear down the lab with the <code class="inlineCode">containerlab destroy</code> command: </p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>sudo containerlab destroy --topo srl02.clab.yml 
[sudo] password for echou: 
INFO[0000] Parsing &amp; checking topology file: srl02.clab.yml 
INFO[0000] Destroying lab: srl02                        
INFO[0001] Removed container: clab-srl02-srl2           
INFO[0001] Removed container: clab-srl02-srl1           
INFO[0001] Removing containerlab host entries from /etc/hosts file
</code></pre>
    <p class="normal">I don’t know about you, but Containerlab is the easiest way to launch a networking lab that I have seen. With more vendor support, it might one day become the only lab and testing software we need for network testing. </p>
    <p class="normal">In the next section, we <a id="_idIndexMarker438"/>will briefly discuss the relationship between Docker and Kubernetes with a very brief overview of Kubernetes. </p>
    <h1 class="heading-1" id="_idParaDest-150">Docker and Kubernetes</h1>
    <p class="normal">As we have seen, Docker images and <a id="_idIndexMarker439"/>orchestration can be done with the tools provided by the Docker community. However, it is almost impossible to think about Docker containers without Kubernetes. This is<a id="_idIndexMarker440"/> because when it comes to container orchestration, Kubernetes is becoming the de facto standard in doing so. There is not enough space to cover Kubernetes in this chapter, but because of its strong ties to container orchestration, we should at least know the basics about Kubernetes. </p>
    <p class="normal">Kubernetes (<a href="https://kubernetes.io/"><span class="url">https://kubernetes.io/</span></a>) was<a id="_idIndexMarker441"/> originally developed by Google, but the project is now managed by the Cloud Native Computing Foundation. It is an open-source container orchestration system that automatically deploys, scales, and manages containers. The project was well-received by the community right from the beginning since it had a proven track record of scale with Google’s internal usage. </p>
    <p class="normal">Kubernetes uses a master as the controlling unit that manages worker nodes to deploy containers. Each worker node can have one or more pods, which are the smallest units of units in Kubernetes. The pods are where the containers will be deployed. When the containers are deployed, they are generally grouped into different types of sets spread across the pods. </p>
    <p class="normal">Most public cloud providers (AWS, Azure, Google, and DigitalOcean) offer managed Kubernetes clusters that users can try. The Kubernetes documentation (<a href="https://kubernetes.io/docs/home/"><span class="url">https://kubernetes.io/docs/home/</span></a>) also offers many tutorials for step-by-step guides to learn more about the technology. </p>
    <h1 class="heading-1" id="_idParaDest-151">Summary</h1>
    <p class="normal">In this chapter, we learned about container virtualization. Containers are similar to virtual machines in their ability to isolate computing resources but different in the sense that they are lightweight and fast to deploy. </p>
    <p class="normal">We saw how to use Docker containers to build Python applications and docker-compose to build multi-container applications on a single host. </p>
    <p class="normal">Later in the chapter, we learned how networks are constructed with Docker containers by using the default bridge, custom bridges, and host options. Containers can also help with network operating system testing using the Containerlab project. </p>
    <p class="normal">In the next chapter, we will look at how we can use Python in network security.</p>
    <h1 class="heading-1">Join our book community</h1>
    <p class="normal">To join our community for this book – where you can share feedback, ask questions to the author, and learn about new releases – follow the QR code below:</p>
    <p class="normal"><a href="https://packt.link/networkautomationcommunity"><span class="url">https://packt.link/networkautomationcommunity</span></a></p>
    <p class="normal"><img alt="" src="../Images/QR_Code2903617220506617062.png"/></p>
  </div>
</body></html>