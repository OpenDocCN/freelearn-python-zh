<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;Optimization Techniques"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Optimization Techniques</h1></div></div></div><p>In this chapter, we will learn how to optimize our Python code to get better responsive programs. But, before we dive into this, I would like to stress that do not optimize until it is necessary. A better-readable program has a better life and maintainability than a tersely-optimized program. First, we will take a look at simple optimization tricks to keep a program optimized. We should have knowledge about them so that we can apply easy optimizations from the start. Then, we will look at profiling to find bottlenecks in the current program and apply optimizations where we need them. As a last resort, we can compile in the C language and provide functionality as an extension to Python. Here is the gist of topics that we will cover:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Writing optimized code</li><li class="listitem" style="list-style-type: disc">Profiling to find bottlenecks</li><li class="listitem" style="list-style-type: disc">Using fast libraries</li><li class="listitem" style="list-style-type: disc">Using C speeds</li></ul></div><div class="section" title="Writing optimized code"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec47"/>Writing optimized code</h1></div></div></div><p>
<span class="strong"><strong>Key 1: Easy optimizations for code.</strong></span>
</p><p>We should pay <a id="id193" class="indexterm"/>close attention to not use loops inside loops, giving us quadratic behavior. We can use built-ins, such as map, ZIP, and reduce, instead of using loops if possible. For example, in the following code, the one with map is faster because the looping is implicit and done at C level. By plotting their run times respectively on graph as <code class="literal">test 1</code> and <code class="literal">test 2</code>, we see that it is nearly constant for PyPy but reduces a lot for CPython, as follows:</p><div class="informalexample"><pre class="programlisting">def sqrt_1(ll):
    """simple for loop"""
    res = []
    for i in ll:
        res.append(math.sqrt(i))
    return res

def sqrt_2(ll):
    "builtin map"
    return list(map(math.sqrt,ll))
The test 1 is for sqrt_1(list(range(1000))) and test 22 sqrt_2(list(range(1000))).</pre></div><p>The following image<a id="id194" class="indexterm"/> is a graphical representation of the preceding code:</p><div class="mediaobject"><img src="graphics/B04885_07_01.jpg" alt="Writing optimized code"/></div><p>Generators should be used, when the result that is consumed is averagely smaller than the total result consumed. In other words, the result that is generated in the end may not be used. They also serve to conserve memory because no temporary result is stored but generated on demand. In the following example, <code class="literal">sqrt_5</code> creates a generator, while <code class="literal">sqrt_6</code> creates a list. The <code class="literal">use_combo</code> instance breaks out of the loop of iteration after a given number of iterations. Test 1 runs <code class="literal">use_combo(sqrt_5,range(10),5)</code> and all results are consumed from iterator, whereas test 2 is for the <code class="literal">use_combo(sqrt_6,range(10),5)</code> generator. Test 1 should take more time than test 2 as it creates results for all ranges of inputs. Tests 3, and 4 are run with a range<a id="id195" class="indexterm"/> of <code class="literal">25</code>, and tests 5, and 6 are run with a range of <code class="literal">100</code>. As it can be seen, the time consumption variation increases with no of elements in the list:</p><div class="informalexample"><pre class="programlisting">def sqrt_5(ll):
    "simple for loop, yield"
    for i in ll:
        yield i,math.sqrt(i)

def sqrt_6(ll):
    "simple for loop"
    res = []
    for i in ll:
        res.append((i,math.sqrt(i)))
    return res

def use_combo(combofunc,ll,no):
    for i,j in combofunc(ll):
        if i&gt;no:
            return j</pre></div><p>The following image is the graphical representation of the preceding code:</p><div class="mediaobject"><img src="graphics/B04885_07_02.jpg" alt="Writing optimized code"/></div><p>When we are inside <a id="id196" class="indexterm"/>a loop and reference an outside namespace variable, it is first searched in local, then nonlocal, followed by global, and then built-in scopes. If the number of repetitions are more, then such overheads add up. We can reduce namespace lookup by making such global/built-in objects available in the local namespace. For example, in the following code snippet, <code class="literal">sqrt_7(test2)</code> will be faster <code class="literal">than sqrt_1(test1)</code> because of the same reasons:</p><div class="informalexample"><pre class="programlisting">def sqrt_1(ll):
    """simple for loop"""
    res = []
    for i in ll:
        res.append(math.sqrt(i))
    return res

def sqrt_7(ll):
    "simple for loop,local"
    sqrt = math.sqrt
    res = []
    for i in ll:
        res.append(sqrt(i))
    return res</pre></div><p>The following image<a id="id197" class="indexterm"/> is the graphical representation of the preceding code:</p><div class="mediaobject"><img src="graphics/B04885_07_03.jpg" alt="Writing optimized code"/></div><p>The cost of subclassing is not much and subclassing doesn't make method calls slower even if common sense says that it will take lot of time to look a method up on the inheritance hierarchy. Let's take the following example:</p><div class="informalexample"><pre class="programlisting">class Super1(object):
    def get_sqrt(self,no):
        return math.sqrt(no)

class Super2(Super1):
    pass

class Super3(Super2):
    pass

class Super4(Super3):
    pass

class Super5(Super4):
    pass

class Super6(Super5):
    pass

class Super7(Super6):
    pass

class Actual(Super7):
    """method resolution via hierarchy"""
    pass

class Actual2(object):
    """method resolution single step"""
    def get_sqrt(self,no):
        return math.sqrt(no)

def use_sqrt_class(aclass,ll):
    cls_instance = aclass()
    res = []
    for i in ll: 
        res.append(cls_instance.get_sqrt(i))
    return res</pre></div><p>Here, if we call <code class="literal">get_sqrt</code> on the <code class="literal">Actual(case1)</code> class, we need to search it seven levels deep in its base classes, whereas for the <code class="literal">Actual2(case2)</code> class it is present on the class itself. The following graph is our plot for both scenarios:</p><div class="mediaobject"><img src="graphics/B04885_07_04.jpg" alt="Writing optimized code"/></div><p>Also, if we are <a id="id198" class="indexterm"/>using too many checks in the program logic for return codes or error conditions, we should see how many such checks are really needed. We can write the program logic without using any checks and then get the errors in the exception handling logic. This makes the code easy to understand. As in the following example, the <code class="literal">getf_1</code> function uses checks to filter out error conditions, but too many checks are making code hard to understand. The other <code class="literal">get_f2</code> function is the same application logic or algorithm with exception handling. For test 1 <code class="literal">(get_f1)</code> and test 2 <code class="literal">(get_f2)</code>, no file is present, so all exceptions are raised. In this scenario, the exception handling logic, that is test 2, takes more time. For test 3 <code class="literal">(get_f1)</code> and test 4 <code class="literal">(get_f2)</code>, the file and key are present; hence, no error is raised. In this case, test 4 takes less time, as follows:</p><div class="informalexample"><pre class="programlisting">def getf_1(ll):
    "simple for loop,checks"
    res = []
    for fname in ll:
        curr = []
        if os.path.exists(fname):
            f = open(fname,"r")
            try:
                fdict = json.load(f)
            except (TypeError, ValueError):
                curr = [fname,None,"Unable to read Json"]
            finally:
                f.close()
            if 'name' in fdict:
                curr = [fname,fdict["name"],'']
            else:
                curr = [fname,None,"Key not found in file"]
        else:
            curr = [fname,None,"file not found"]
        res.append(curr)
    return res

def getf_2(ll):
    "simple for loop, try-except"
    res = []
    for fname in ll:
        try:
            f = open(fname,"r")
            res.append([fname,json.load(f)['name'],''])
        except IOError:
            res.append([fname,None,"File Not Found Error"])
        except TypeError:
            res.append([fname,None,'Unable to read Json'])
        except KeyError:
            res.append([fname,None,'Key not found in file'])
        except Exception as e:
            res.append([fname,None,str(e)])
        finally:
            if 'f' in locals():
                f.close()
    return res</pre></div><p>The following image is the graphical representation of the preceding code:</p><div class="mediaobject"><img src="graphics/B04885_07_05.jpg" alt="Writing optimized code"/></div><p>Function calling<a id="id199" class="indexterm"/> has overheads and if the performance bottlenecks can be removed by reducing function calls, we should do so. Typically, functions call in loops. In the following example, when we wrote logic inline, it took less time. Also, for PyPy such effects are less in general as most called functions in loops are generally called with the same type of arguments; hence, they get compiled. Any further call to these functions is like calling a C language function:</p><div class="informalexample"><pre class="programlisting">def please_sqrt(no):
    return math.sqrt(no)

def get_sqrt(no):
    return please_sqrt(no)

def use_sqrt1(ll,no):
    for i in ll:
        res = get_sqrt(i)
        if res &gt;= no:
            return i

def use_sqrt2(ll,no):
    for i in ll:
        res = math.sqrt(i)
        if res &gt;= no:
            return i</pre></div><p>The following image <a id="id200" class="indexterm"/>is the graphical representation of the preceding code:</p><div class="mediaobject"><img src="graphics/B04885_07_06.jpg" alt="Writing optimized code"/></div></div></div>
<div class="section" title="Profiling to find bottlenecks"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec48"/>Profiling to find bottlenecks</h1></div></div></div><p>
<span class="strong"><strong>Key 2: Identifying application performance bottlenecks.</strong></span>
</p><p>We should not rely<a id="id201" class="indexterm"/> on our intuition on how to optimize application. There are two major ways for logic slowdown; one is CPU time taken, and the second is the wait for results from some other entity. By profiling, we can find out such cases in which we can tweak logic, and language syntax to get better performance on the same hardware. The following code is a <code class="literal">showtime</code> decorator that I use to calculate the time taken to call a function. It is simple and effective to get rapid answers:</p><div class="informalexample"><pre class="programlisting">from datetime import datetime,timedelta
from functools import wraps
import time

def showtime(func):

    @wraps(func)
    def wrap(*args,**kwargs):
        st = time.time() #time clock can be used too
        result = func(*args,**kwargs)
        et = time.time()
        print("%s:%s"%(func.__name__,et-st))
        return result
    return wrap

@showtime
def func_c():
    for i in range(1000):
        for j in range(1000):
            for k in range(100):
                pass

if __name__ == '__main__':
    func_c()</pre></div><p>This will give us the following output:</p><div class="informalexample"><pre class="programlisting">(py35) [ ch7 ] $ python code_1_8.py 
func_c:1.3181400299072266</pre></div><p>When profiling a single large function that does a lot of stuff, we may need to know on what particular line we are spending the most time. This query can be answered using the <code class="literal">line_profiler</code> module. You can get it with <code class="literal">pip install line_profiler</code>. It shows the time that is spent per line. To get results, we should decorate the function with a <a id="id202" class="indexterm"/>special profile decorator that will be used by <code class="literal">line_profiler</code>:</p><div class="informalexample"><pre class="programlisting">from datetime import datetime,timedelta
from functools import wraps
import time
import line_profiler

l = []
def func_a():
    global l
    for i in range(10000):
        l.append(i)

def func_b():
    m = list(range(100000))

def func_c():
    func_a()
    func_b()
    k = list(range(100000))

if __name__ == '__main__':
    profiler = line_profiler.LineProfiler()
    profiler.add_function(func_c)
    profiler.run('func_c()')
    profiler.print_stats()</pre></div><p>This will give us the following output:</p><div class="informalexample"><pre class="programlisting">Timer unit: 1e-06 s

Total time: 0.007759 s
File: code_1_9.py
Function: func_c at line 15

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    15                                           def func_c():
    16         1         2976   2976.0     38.4      func_a()
    17         1         2824   2824.0     36.4      func_b()
    18         1         1959   1959.0     25.2      k = list(range(100000))</pre></div><p>Another way of profiling is using the <code class="literal">kernprof</code> program that is supplied with the <code class="literal">line_profiler</code> module. We have to decorate the function to be a profiler by the <code class="literal">@profile</code> decorator<a id="id203" class="indexterm"/> and run the program, as shown in the following code snippet:</p><div class="informalexample"><pre class="programlisting">from datetime import datetime,timedelta
from functools import wraps
import time

l = []
def func_a():
    global l
    for i in range(10000):
        l.append(i)

def func_b():
    m = list(range(100000))

@profile
def func_c():
    func_a()
    func_b()
    k = list(range(100000))</pre></div><p>The output for this will be as follows:</p><div class="informalexample"><pre class="programlisting">(py35) [ ch7 ] $ kernprof -l -v code_1_10.py
Wrote profile results to code_1_10.py.lprof
Timer unit: 1e-06 s

Total time: 0 s
File: code_1_10.py
Function: func_c at line 14

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    14                                           @profile
    15                                           def func_c():
    16                                               func_a()
    17                                               func_b()
    18                                               k = list(range(100000))</pre></div><p>Memory profilers are a very good tool to estimate memory consumption in a program. To profile <a id="id204" class="indexterm"/>a function, simply decorate it with profile and run the program like this:</p><div class="informalexample"><pre class="programlisting">from memory_profiler import profile

@profile(precision=4)
def sample():
    l1 = [ i for i in range(10000)]
    l2 = [ i for i in range(1000)]
    l3 = [ i for i in range(100000)]
    return 0

if __name__ == '__main__':
    sample()</pre></div><p>To get details on the command line, use the following code:</p><div class="informalexample"><pre class="programlisting">(py36)[ ch7 ] $ python  ex7_1.py 
Filename: ex7_1.py

Line #    Mem usage    Increment   Line Contents
================================================
     8     12.6 MiB      0.0 MiB   @profile
     9                             def sample():
    10     13.1 MiB      0.5 MiB       l1 = [ i for i in range(10000)]
    11     13.1 MiB      0.0 MiB       l2 = [ i for i in range(1000)]
    12     17.0 MiB      3.9 MiB       l3 = [ i for i in range(100000)]
    13     17.0 MiB      0.0 MiB       return 0


Filename: ex7_1.py

Line #    Mem usage    Increment   Line Contents
================================================
    10     13.1 MiB      0.0 MiB       l1 = [ i for i in range(10000)]


Filename: ex7_1.py

Line #    Mem usage    Increment   Line Contents
================================================
    12     17.0 MiB      0.0 MiB       l3 = [ i for i in range(100000)]


Filename: ex7_1.py

Line #    Mem usage    Increment   Line Contents
================================================
    11     13.1 MiB      0.0 MiB       l2 = [ i for i in range(1000)]</pre></div><p>We can also <a id="id205" class="indexterm"/>use it to debug long-running programs. The following code is for a simple socket server. It adds lists to the global lists variable, which never gets deleted. Saving contents in <code class="literal">simple_serv.py</code> is as follows:</p><div class="informalexample"><pre class="programlisting">from SocketServer import BaseRequestHandler,TCPServer

lists = []

class Handler(BaseRequestHandler):
    def handle(self):
        data = self.request.recv(1024).strip()
        lists.append(list(range(100000)))
        self.request.sendall("server got "+data)

if __name__ == '__main__':
    HOST,PORT = "localhost",9999
    server = TCPServer((HOST,PORT),Handler)
    server.serve_forever()</pre></div><p>Now, run the program via profiler as follows:</p><div class="informalexample"><pre class="programlisting">mprof run simple_serv.py</pre></div><p>Put some bogus hits to the server. I used the <code class="literal">netcat</code> utility:</p><div class="informalexample"><pre class="programlisting">[ ch7 ] $ nc localhost 9999 &lt;&lt;END
hello
END</pre></div><p>Kill the server after some time and plot the memory consumed over time with the following code:</p><div class="informalexample"><pre class="programlisting">[ ch7 ] $ mprof plot</pre></div><p>We get a good graph showing us memory consumption over time:</p><div class="mediaobject"><img src="graphics/B04885_07_07.jpg" alt="Profiling to find bottlenecks"/></div><p>Other than getting<a id="id206" class="indexterm"/> program memory consumption, we may be interested in objects carrying spaces. Objgraph (<a class="ulink" href="https://pypi.python.org/pypi/objgraph">https://pypi.python.org/pypi/objgraph</a>) is able to graph object links<a id="id207" class="indexterm"/> for your programs. Guppy (<a class="ulink" href="https://pypi.python.org/pypi/guppy/">https://pypi.python.org/pypi/guppy/</a>) is another <a id="id208" class="indexterm"/>package that has heapy, which is a heap analysis tool. It is very helpful to see the number of objects on heap for a running program. As of this writing, it was only available for Python 2. For analysis of a long-running process, Dowser (<a class="ulink" href="https://pypi.python.org/pypi/dowser">https://pypi.python.org/pypi/dowser</a>) is also a good choice. We can use <a id="id209" class="indexterm"/>Dowser to see the memory consumption to run Celery or a WSGI server. Django-Dowser is good and provides the same functionality as an app, but as the name suggests, it only works with Django.</p></div>
<div class="section" title="Using fast libraries"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec49"/>Using fast libraries</h1></div></div></div><p>
<span class="strong"><strong>Key 3: Use easy drop-in faster libraries.</strong></span>
</p><p>There are libraries <a id="id210" class="indexterm"/>out there that can help a lot in optimizing code, rather than writing some optimized routines yourself. For example, if we have a list that needs to be fast at FIFO, we may use the <code class="literal">blist</code> package. We can use C versions of libraries, such as <code class="literal">cStringIO</code> (faster StringIO), <code class="literal">ujson</code> (faster JSON handling), <code class="literal">numpy</code> (math, and vectors), and <code class="literal">lxml</code> (XML handling). Most of the libraries that are listed here are just a Python wrapper over C libraries. You only need to search once for your problem domain. Other than this, we can make a C, or C++ library interface with Python very easily, which is also our next topic.</p></div>
<div class="section" title="Using C speeds"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec50"/>Using C speeds</h1></div></div></div><p>
<span class="strong"><strong>Key 4: Running at C speeds.</strong></span>
</p><div class="section" title="SWIG"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec24"/>SWIG</h2></div></div></div><p>SWIG is <a id="id211" class="indexterm"/>an interface compiler that connects programs written in C, and C++ with scripting languages. We can use SWIG to call C, C++ compiled in Python. Let's say that we have a factorial computing library in C, with source code in the <code class="literal">fact.c</code> file and the corresponding <code class="literal">fact.h</code> header file:</p><p>The source code in <code class="literal">fact.c</code> is as follows:</p><div class="informalexample"><pre class="programlisting">#include "fact.h"
long int fact(long int n) {
    if (n &lt; 0){
        return 0;
    }
    if (n == 0) {
        return 1;
    }
    else {
        return n * fact(n-1);
    }
}</pre></div><p>The source code in <code class="literal">fact.h</code> is as follows:</p><div class="informalexample"><pre class="programlisting">long int fact(long int n);</pre></div><p>Now, we need to write an interface file for SWIG, which tells it what it needs to be exposed to Python:</p><div class="informalexample"><pre class="programlisting">/* File: fact.i */
%module fact
%{
#define SWIG_FILE_WITH_INIT
#include "fact.h"
%}
long int fact(long int n);</pre></div><p>Here, module indicates the module name for the Python library, and <code class="literal">SWIG_FILE_WITH_INIT</code> indicates that the resulting C code should be built with a Python extension. The <a id="id212" class="indexterm"/>content in <code class="literal">{% %}</code> is used in the C wrap code that is generated. We have three files, <code class="literal">fact.c</code>, <code class="literal">fact.h</code>, and <code class="literal">fact.i</code>, in directory. We run SWIG to generate <code class="literal">wrapper_code</code> as follows:</p><div class="informalexample"><pre class="programlisting">swig3.0 -python -O -py3 fact.i</pre></div><p>The <code class="literal">-O</code> option is used for optimizations and <code class="literal">-py3</code> is for Python 3 specific features.</p><p>This generates <code class="literal">fact.py</code> and <code class="literal">fact_wrap.c</code>. The <code class="literal">fact.py</code> is a Python module and <code class="literal">fact_wrap.c</code> is the glue code between C and Python:</p><div class="informalexample"><pre class="programlisting">gcc -fpic -c fact_wrap.c fact.c -I/home/arun/.virtualenvs/py3/include/python3.4m</pre></div><p>Here, I have to include my <code class="literal">python.h</code> path to compile it. This will generate <code class="literal">fact.o</code> and <code class="literal">fact_wrap.o</code>. Now, the last part is to create a dynamic linked library, as follows:</p><div class="informalexample"><pre class="programlisting">gcc -shared fact.o fact_wrap.o  -o _fact.so</pre></div><p>The <code class="literal">_fact.so</code> file is used by the <code class="literal">fact.py</code> to run C functions. Now, we can use the fact module in our Python programs:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from fact import fact
&gt;&gt;&gt; fact(10)
3628800
&gt;&gt;&gt; fact(5)
120
&gt;&gt;&gt; fact(20)
2432902008176640000</pre></div></div><div class="section" title="CFFI"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec25"/>CFFI</h2></div></div></div><p>The <span class="strong"><strong>C Foreign Function Interface</strong></span> (<span class="strong"><strong>CFFI</strong></span>) for Python is one tool that looks the best to me because <a id="id213" class="indexterm"/>of the easy setup and interface. It works on an ABI and API level.</p><p>Using our factorial C programs here as well, we first create a shared library for the code:</p><div class="informalexample"><pre class="programlisting">gcc -shared fact.o  -o _fact.so
gcc -fpic -c fact.c -o fact.o</pre></div><p>Now, we have a <code class="literal">_fact.so</code> shared library object in our current directory. To load this in the Python environment, we can perform this action which is very straightforward. We should have header files for the library so that we can use declarations. Install the CFFI package from distribution or pip that is needed for this, as follows:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from cffi import FFI
&gt;&gt;&gt; ffi = FFI()
&gt;&gt;&gt; ffi.cdef("""
... long int fact(long int num);
... """)
&gt;&gt;&gt; C = ffi.dlopen("./_fact.so")
&gt;&gt;&gt; C.fact(20)
2432902008176640000</pre></div><p>We can reduce<a id="id214" class="indexterm"/> import times for the module if we do not call <code class="literal">cdef</code> in the import modules. We can write another <code class="literal">setup_fact_ffi.py</code> module that gives us a <code class="literal">fact_ffi.py</code> module with the compiled information. Hence, the load times decrease a lot:</p><div class="informalexample"><pre class="programlisting">from cffi import FFI

ffi = FFI()
ffi.set_source("fact_ffi", None)
ffi.cdef("""
    long int fact(long int n);
""")

if __name__ == "__main__":
ffi.compile()

python setup_fact_ffi.py </pre></div><p>We now can use this module to get <code class="literal">ffi</code> and load our shared library as follows:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from fact_ffi import ffi
&gt;&gt;&gt; ll = ffi.dlopen("./_fact.so")
&gt;&gt;&gt; ll.fact(20)
2432902008176640000
&gt;&gt;&gt; </pre></div><p>Until this point, as we were using a precompiled shared library, we didn't need a compiler. Let's suppose that there is this small C function that you need in Python, and you do not want to write another .c file for it, then this is how it can be done. You can also extend it to shared libraries as well.</p><p>First, we define a <code class="literal">build_ffi.py</code> file, which will compile and create a module for us:</p><div class="informalexample"><pre class="programlisting">__author__ = 'arun'

from cffi import FFI
ffi = FFI()

ffi.set_source("_fact_cffi",
    """
    long int fact(long int n) {
    if (n &lt; 0){
        return 0;
    }
    if (n == 0) {
        return 1;
    }
    else {
        return n * fact(n-1);
    }
}
""",
               libraries=[]
    )

ffi.cdef("""
long int fact(long int n);
""")

if __name__ == '__main__':
    ffi.compile()</pre></div><p>When we run <a id="id215" class="indexterm"/>Python <code class="literal">fact_build.py</code>, this will create a <code class="literal">_fact_cffi.cpython-34m.so</code> module. To use it, we have to import it and use the <code class="literal">lib</code> variable to access the module:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from  _fact_cffi import ffi,lib
&gt;&gt;&gt; lib.fact(20)</pre></div></div><div class="section" title="Cython"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec26"/>Cython</h2></div></div></div><p>Cython is like a <a id="id216" class="indexterm"/>superset of Python in which we can optionally give static declarations. The source code is compiled to C/C++ extension modules.</p><p>We write our old factorial program in <code class="literal">fact_cpy.pyx</code> as follows:</p><div class="informalexample"><pre class="programlisting">cpdef double fact(int num):
    cdef double res = 1
    if num &lt; 0:
        return -1
    elif num == 0:
        return 1 
    else:
        for i in range(1,num + 1):
            res = res*i
return res</pre></div><p>Here <code class="literal">cpdef</code> is the<a id="id217" class="indexterm"/> function declaration for CPython that creates a Python function and conversion logic for arguments, and a C function that actually executes. <code class="literal">cpdef</code> is defining the data type for the res variable, which helps in speedup.</p><p>We have to create a <code class="literal">setup.py</code> file to compile this code into an extension module (we can directly use it by using <code class="literal">pyximport</code> but we will leave that for now). The contents for the <code class="literal">setup.py</code> file will be as follows:</p><div class="informalexample"><pre class="programlisting">from distutils.core import setup
from Cython.Build import cythonize

setup(
  name = 'factorial library',
    ext_modules = cythonize("fact_cpy.pyx"),
    )</pre></div><p>Now to build the module, all we have to do is type in the following command, and we get a <code class="literal">fact_cpy.cpython-34m.so</code> file in the current directory:</p><div class="informalexample"><pre class="programlisting">python setup.py build_ext --inplace</pre></div><p>Using this in Python is as follows:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from fact_cpy import fact
&gt;&gt;&gt; fact(20)
2432902008176640000</pre></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec51"/>Summary</h1></div></div></div><p>In this chapter, we saw various techniques that are used to optimize and profile code. I will, again, point out that we should always focus on first writing the correct program, then writing test cases for it, and then optimizing it. We should write code with optimizations that we know at that time or without optimization the first time, and we should hunt for them only if we need them from a business perspective. Compiling a C module can give a good speedup for CPU-intensive tasks. Also, we can give up GIL in C modules, which can also help us in increasing performance. But, all of this was on single system. Now, in the next chapter, we will see how we can improve performance when the tricks that were discussed in this chapter are not sufficient for a real-life scenario.</p></div></body></html>