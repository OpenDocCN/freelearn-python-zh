<html><head></head><body>
<div><div><div><h1 id="_idParaDest-230"><em class="italic"><a id="_idTextAnchor215"/>Chapter 12</em>: Deadlocks</h1>
			<p><strong class="bold">Deadlocks</strong>, one of the most common concurrency problems, will be the first problem that we will analyze in this book. In this chapter, we will discuss the theoretical causes of deadlocks in concurrent programming. We will cover a classical synchronization problem in concurrency, called the <strong class="bold">dining philosophers problem</strong>, as a real-life example of a deadlock. We will also illustrate an actual implementation of a deadlock in Python and discuss several methods to address this problem. This chapter will also cover the concept of livelocks, which are relevant to deadlocks and are also a common problem in concurrent programming.</p>
			<p>The following topics will be covered in this chapter:</p>
			<ul>
				<li>The concept of deadlocks</li>
				<li>Approaches to deadlock situations</li>
				<li>The concept of livelocks</li>
			</ul>
			<p>By the end of this chapter, we will have gained a deep understanding of the problem, its place in concurrent programming, and the practical approaches to solving it.</p>
			<h1 id="_idParaDest-231"><a id="_idTextAnchor216"/>Technical requirements</h1>
			<p>The code files for this chapter can be accessed through this link: <a href="https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter12">https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter12</a>.</p>
			<h1 id="_idParaDest-232"><a id="_idTextAnchor217"/>The concept of deadlocks</h1>
			<p>In concurrent programming, a deadlock refers to a specific situation in which no progress can be made, and the program becomes locked in its current state. In most cases, this <a id="_idIndexMarker945"/>phenomenon is caused by a lack of, or mishandled, coordination between different lock objects (for thread synchronization purposes). In this section, we will discuss a thought experiment, commonly known as the dining philosophers problem, to illustrate <a id="_idIndexMarker946"/>the concept of a deadlock and its causes; from there, you will learn how to simulate the problem in a Python concurrent program.</p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor218"/>The dining philosophers problem</h2>
			<p>The dining philosophers problem was first introduced by Edgar Dijkstra, a leading pioneer in <a id="_idIndexMarker947"/>concurrent programming, in 1965. This problem was first demonstrated using different technical terms (resource contention in <a id="_idIndexMarker948"/>computer systems) and was later rephrased by Tony Hoare, a British computer scientist and the inventor of the quicksort sorting algorithm. The problem statement is as follows.</p>
			<p>Five philosophers sit around a table, and each has a bowl of food in front of them. Placed between these five bowls of food are five forks, so each philosopher has one fork to their left and one fork to their right. This setup is shown in the following diagram:</p>
			<div><div><img src="img/Figure_12.1_B17499.jpg" alt="Figure 12.1 – An illustration of the dining philosophers problem " width="868" height="929"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.1 – An illustration of the dining philosophers problem</p>
			<p>Each silent philosopher is to alternate between thinking and eating. Each is required to have both of the forks around them to be able to pick up the food from their bowl, and no fork can be shared between two or more different philosophers. When a philosopher finishes eating a specific amount of food, they are to place both of the forks back in their respective, original locations. At this point, the philosophers around that philosopher will be able to use those forks.</p>
			<p>Since the philosophers are silent and cannot communicate with each other, they have no way to let <a id="_idIndexMarker949"/>each other know they need the forks to eat. In other words, the only way for a philosopher to eat is to have both of the forks already <a id="_idIndexMarker950"/>available to them. For this problem, a set of instructions must be designed for the philosophers to efficiently switch between eating and thinking so that each philosopher is provided with enough food.</p>
			<p>Now, a potential approach to this problem would be the following set of instructions:</p>
			<ol>
				<li>A philosopher must think until the fork on their left becomes available. When that happens, the philosopher is to pick it up.</li>
				<li>A philosopher must think until the fork on their right becomes available. When that happens, the philosopher is to pick it up.</li>
				<li>If a philosopher is holding two forks, they will eat a specific amount of food from the bowl in front of them, and then the following will apply:<ul><li>Afterward, the philosopher has to put the right fork down in its original place.</li><li>Afterward, the philosopher has to put the left fork down in its original place.</li></ul></li>
				<li>The process repeats from the first step.</li>
			</ol>
			<p>It is clear to see how this set of instructions can lead to a situation where no progress can be made; namely, if, in the beginning, all of the philosophers start to execute their instructions at the same time. Since all of the forks are on the table at the beginning and are therefore available to be picked up by nearby philosophers, each philosopher will be able to execute the first instruction (picking up the fork on their left).</p>
			<p>Now, after this step, each philosopher will be holding a fork with their left hand, and no forks will be left on the table. Since no philosopher has both forks in their hands, they cannot eat their food. Furthermore, the set of instructions that they were given specifies that only after a philosopher has eaten a specific amount of food can they put their forks down on the table. This means that so long as a philosopher has not eaten, they will not release the fork that they are holding.</p>
			<p>So, as each philosopher is holding only one fork with their left hand, this means they cannot eat or put down the fork they are holding. The only time a philosopher gets to eat their food is when their neighboring philosopher puts their fork down, which is only possible if they can eat their food; this creates a never-ending circle of conditions that can never be satisfied. This situation is, in essence, the nature of a deadlock, in which all of the elements of a system are stuck in place, and no progress can be made.</p>
			<p>It is not difficult to imagine real-world situations that involve shared resources and are modeled by this dining philosophers problem. For example, the original problems that inspired Dijkstra to construct this formulation involved working with external devices such as tape drives.</p>
			<p>Another example is in terms of banks: to execute transactions between two bank accounts, you <a id="_idIndexMarker951"/>must ensure that both accounts are locked <a id="_idIndexMarker952"/>from other transactions for the correct amount of money to be transferred. Here, the analogy does not exactly hold – a philosopher corresponds to a transaction that locks accounts (the forks) – but the same technical difficulties could arise. Other examples include making online reservations and allowing a database to be modified by multiple clients at the same time.</p>
			<p>With that said, we will be exclusively focusing on the formal dining philosophers problem as it provides a clean, abstract setting that could easily be analyzed and taken apart. With that in mind, let's consider the formal concept of a deadlock and the relevant theories around it.</p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor219"/>A deadlock in a concurrent system</h2>
			<p>Given a concurrent program with multiple threads or processes, the execution flow enters a deadlock if a <a id="_idIndexMarker953"/>process (or thread) is waiting on a resource that is being held and utilized by another process, which is, in turn, waiting for another resource that is held by a different process. In other words, processes cannot proceed with their execution instructions while waiting for resources that can only be released after the execution is completed; therefore, these processes are unable to change their execution states.</p>
			<p>A deadlock is also defined by the conditions that a concurrent program needs to have at the same time <a id="_idIndexMarker954"/>for a deadlock to occur. These conditions were first proposed by the computer <a id="_idIndexMarker955"/>scientist Edward G. Coffman, Jr., and are therefore known as the <strong class="bold">Coffman conditions</strong>. These conditions are as follows:</p>
			<ul>
				<li>At least one resource <a id="_idIndexMarker956"/>has to be in a non-shareable state. This means that that resource is being <a id="_idIndexMarker957"/>held by an individual process (or thread) and cannot be accessed by others; the resource can only be accessed and held by a single process (or thread) at any given time. This condition is also known as <strong class="bold">mutual exclusion</strong>.</li>
				<li>One process (or thread) exists that is simultaneously accessing a resource and waiting for another held by other processes (or threads). In other words, this process (or thread) needs <a id="_idIndexMarker958"/>access to two resources to execute its instructions, one of which <a id="_idIndexMarker959"/>it is already holding, the other of which it is waiting for from other processes (or threads). This condition is called <strong class="bold">hold and wait</strong>.</li>
				<li>Resources can only be released by a process (or a thread) holding them if there are specific <a id="_idIndexMarker960"/>instructions for the <a id="_idIndexMarker961"/>process (or thread) to do so. This is to say that unless the process (or thread) voluntarily and actively releases the resource, that resource remains in a non-shareable state. This is the <strong class="bold">no preemption condition</strong>.</li>
				<li>The final condition is called <strong class="bold">circular wait</strong>. As its name suggests, this condition specifies that <a id="_idIndexMarker962"/>a set of processes (or threads) exist so <a id="_idIndexMarker963"/>that the first process (or thread) in the set is waiting for a resource to be released by the second process (or thread), which, in turn, needs to be waiting for the third process (or thread); finally, the last process (or thread) in the set is waiting for the first one.</li>
			</ul>
			<p>Let's quickly take a look at a basic example of a deadlock. Consider a concurrent program in which there are two different processes (process <strong class="bold">A</strong> and process <strong class="bold">B</strong>) and two different resources (resource <strong class="bold">R1</strong> and resource <strong class="bold">R2</strong>), as follows:</p>
			<div><div><img src="img/Figure_12.2_B17499.jpg" alt="Figure 12.2 – Sample deadlock diagram " width="345" height="276"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.2 – Sample deadlock diagram</p>
			<p>Neither of the resources can be shared across separate processes, and each process needs to access <a id="_idIndexMarker964"/>both resources to execute its instructions. Take process <strong class="bold">A</strong>, for example. It is already holding resource <strong class="bold">R1</strong>, but it also needs <strong class="bold">R2</strong> to proceed with its execution. However, <strong class="bold">R2</strong> cannot be acquired by process <strong class="bold">A</strong>, as it is being held by process <strong class="bold">B</strong>. So, process <strong class="bold">A</strong> cannot proceed. The same goes for process <strong class="bold">B</strong>, which is holding <strong class="bold">R2</strong> and needs <strong class="bold">R1</strong> to proceed. <strong class="bold">R1</strong> is, in turn, held by process <strong class="bold">A</strong>.</p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor220"/>Python simulation</h2>
			<p>In this section, we <a id="_idIndexMarker965"/>will implement the preceding situation in an actual Python program. Specifically, we <a id="_idIndexMarker966"/>will have two locks (we will call them <strong class="bold">lock A</strong> and <strong class="bold">lock B</strong>) and two separate threads interacting with the locks (<strong class="bold">thread A</strong> and <strong class="bold">thread B</strong>). In our program, we will set up a situation in which thread A has acquired lock A and is waiting to acquire lock B, which has already been acquired by thread B, which is, in turn, waiting for lock A to be released.</p>
			<p>If you have already downloaded the code for this book from the relevant GitHub page, then go ahead and navigate to the <code>Chapter12</code> folder. Let's consider the <code>Chapter12/example1.py</code> file, as follows:</p>
			<pre>import threading
import time
def thread_a():
    print('Thread A is starting...')
    print('Thread A waiting to acquire lock A.')
    lock_a.acquire()
    print('Thread A has acquired lock A, performing some \
      calculation...')
    time.sleep(2)
    print('Thread A waiting to acquire lock B.')
    lock_b.acquire()
    print('Thread A has acquired lock B, performing some \
      calculation...')
    time.sleep(2)
    print('Thread A releasing both locks.')
    lock_a.release()
    lock_b.release()
def thread_b():
    print('Thread B is starting...')
    print('Thread B waiting to acquire lock B.')
    lock_b.acquire()
    print('Thread B has acquired lock B, performing some \
      calculation...')
    time.sleep(5)
    print('Thread B waiting to acquire lock A.')
    lock_a.acquire()
    print('Thread B has acquired lock A, performing some \
      calculation...')
    time.sleep(5)
    print('Thread B releasing both locks.')
    lock_b.release()
    lock_a.release()
lock_a = threading.Lock()
lock_b = threading.Lock()
thread1 = threading.Thread(target=thread_a)
thread2 = threading.Thread(target=thread_b)
thread1.start()
thread2.start()
thread1.join()
thread2.join()
print('Finished.')</pre>
			<p>In this <a id="_idIndexMarker967"/>script, the <code>thread_a()</code> and <code>thread_b()</code> functions specify thread <a id="_idIndexMarker968"/>A and thread B, respectively. In our main program, we also have two <code>threading.Lock</code> objects: lock A and lock B. The general structure of the thread instructions is as follows:</p>
			<ol>
				<li value="1">Start the thread.</li>
				<li>Try to acquire the lock with the same name as the thread (thread A will try to acquire lock A, while thread B will try to acquire lock B).</li>
				<li>Perform some calculations.</li>
				<li>Try to acquire the other lock (thread A will try to acquire lock B, while thread B will try to acquire lock A).</li>
				<li>Perform some other calculations.</li>
				<li>Release both locks.</li>
				<li>End the thread.<p class="callout-heading">Note</p><p class="callout">Note that we are using the <code>time.sleep()</code> function to simulate the action of some calculations being processed.</p></li>
			</ol>
			<p>First of all, we are <a id="_idIndexMarker969"/>starting both threads A and B almost simultaneously, within <a id="_idIndexMarker970"/>the main program. With the structure of the thread instruction set in mind, we can see that at this point, both threads will be initiated; thread A will try to acquire lock A and will succeed in doing so since lock A is still available at this point. The same goes for thread B and lock B. The two threads will then go on to perform some calculations on their own.</p>
			<p>Let's consider the current state of our program: lock A has been acquired by thread A, and lock B has been acquired by thread B. After their respective calculation processes are complete, thread A will then try to acquire lock B, and thread B will try to acquire lock A. We can easily see that this is the beginning of our deadlock situation: since lock B is already being held by thread B and cannot be acquired by thread A, thread B, for the same reason, cannot acquire lock A.</p>
			<p>Both of the threads will now wait infinitely to acquire their respective second lock. However, the only way a lock can be released is for a thread to continue its execution instructions and release all of the locks it has at the end. So, our program will be stuck in its execution at this point, and no further progress will be made.</p>
			<p>The following diagram further illustrates the process of how the deadlock unfolds, in sequence:</p>
			<div><div><img src="img/Figure_12.3_B17499.jpg" alt="Figure 12.3 – Deadlock sequence diagram " width="1181" height="631"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.3 – Deadlock sequence diagram</p>
			<p>Now, let's look <a id="_idIndexMarker971"/>at the deadlock that we have created in action. By running <a id="_idIndexMarker972"/>the script, you should obtain the following output:</p>
			<pre>&gt; python example1.py
Thread A is starting...
Thread A waiting to acquire lock A.
Thread B is starting...
Thread A has acquired lock A, performing some calculation...
Thread B waiting to acquire lock B.
Thread B has acquired lock B, performing some calculation...
Thread A waiting to acquire lock B.
Thread B waiting to acquire lock A.</pre>
			<p>As we discussed previously, since each thread is trying to acquire a lock that is currently held by the <a id="_idIndexMarker973"/>other thread, the only way for a lock to be released is for <a id="_idIndexMarker974"/>a thread to continue its execution. This is a deadlock, and your program will hang infinitely, never reaching the final <code>print</code> statement in the last line of the program. </p>
			<p>This behavior is undesirable in every way. In the next section, we will be discussing potential approaches to prevent deadlocks from occurring.</p>
			<h1 id="_idParaDest-236"><a id="_idTextAnchor221"/>Approaches to deadlock situations</h1>
			<p>Intuitively, each of the following approaches looks to eliminate one of the four Coffman conditions from <a id="_idIndexMarker975"/>our program to prevent deadlocks. Our first solution is to implement ranking among the competing resources.</p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor222"/>Implementing ranking among resources</h2>
			<p>From both the dining philosophers problem and our Python example, we can see that the last condition of <a id="_idIndexMarker976"/>the four Coffman conditions, circular wait, is at the heart of the deadlock problem. It specifies that the different processes (or threads) in our concurrent<a id="_idIndexMarker977"/> program wait for resources held by other processes (or threads) circularly. By taking a closer look, we can see that the root cause for this condition is the order (or lack thereof) in which the processes (or threads) access the resources.</p>
			<p>In the dining philosophers problem, each philosopher is instructed to pick up the fork on their left first, while in our Python example, the threads always try to acquire the locks with the same name before performing any calculations. As you have seen, when the philosophers want to start eating at the same time, they will pick up their respective left forks and will be stuck in an infinite waiting loop. Similarly, when the two threads start their execution at the same time, they will acquire their locks and, again, wait for the other locks infinitely.</p>
			<p>The conclusion that we can infer from this is that if, instead of accessing the resources arbitrarily, the processes (or threads) were to access them in a predetermined, static order, the circular nature of the way that they acquire and wait for the resources will be eliminated. So, for our two-lock Python example, instead of having thread A try to acquire lock A and thread B try to acquire lock B in their respective execution instructions, we <a id="_idIndexMarker978"/>will want both threads to try and acquire the locks<a id="_idIndexMarker979"/> in the same order. For example, both threads will now try to acquire lock A first, perform some calculations, try to acquire lock B, perform further calculations, and, finally, release both threads.</p>
			<p>This change is implemented in the <code>Chapter12/example2.py</code> file, as follows:</p>
			<pre>import threading
import time
def thread_a():
    print('Thread A is starting...')
    print('Thread A waiting to acquire lock A.')
    lock_a.acquire()
    print('Thread A has acquired lock A, performing some \
      calculation...')
    time.sleep(2)
    print('Thread A waiting to acquire lock B.')
    lock_b.acquire()
    print('Thread A has acquired lock B, performing some \
      calculation...')
    time.sleep(2)
    print('Thread A releasing both locks.')
    lock_a.release()
    lock_b.release()
def thread_b():
    print('Thread B is starting...')
    print('Thread B waiting to acquire lock A.')
    lock_a.acquire()
    print('Thread B has acquired lock A, performing some \
      calculation...')
    time.sleep(5)
    print('Thread B waiting to acquire lock B.')
    lock_b.acquire()
    print('Thread B has acquired lock B, performing some \
      calculation...')
    time.sleep(5)
    print('Thread B releasing both locks.')
    lock_b.release()
    lock_a.release()
lock_a = threading.Lock()
lock_b = threading.Lock()
thread1 = threading.Thread(target=thread_a)
thread2 = threading.Thread(target=thread_b)
thread1.start()
thread2.start()
thread1.join()
thread2.join()
print('Finished.')</pre>
			<p>This version of <a id="_idIndexMarker980"/>the script is now able to finish its execution and should<a id="_idIndexMarker981"/> produce the following output:</p>
			<pre>&gt; python3 example2.py
Thread A is starting...
Thread A waiting to acquire lock A.
Thread A has acquired lock A, performing some calculation...
Thread B is starting...
Thread B waiting to acquire lock A.
Thread A waiting to acquire lock B.
Thread A has acquired lock B, performing some calculation...
Thread A releasing both locks.
Thread B has acquired lock A, performing some calculation...
Thread B waiting to acquire lock B.
Thread B has acquired lock B, performing some calculation...
Thread B releasing both locks.
Finished.</pre>
			<p>This approach efficiently eliminates the deadlock problem in our two-lock example, but how well <a id="_idIndexMarker982"/>does it hold up for the dining philosophers problem? To answer<a id="_idIndexMarker983"/> this question, let's try to simulate the problem and the solution in Python by ourselves. The <code>Chapter12/example3.py</code> file contains the implementation of the dining philosophers problem in Python, as follows:</p>
			<pre>import threading
# The philosopher thread
def philosopher(left, right):
    while True:
        with left:
             with right:
                 print(f'Philosopher at \
                  {threading.currentThread()} is eating.')
# The chopsticks
N_FORKS = 5
forks = [threading.Lock() for n in range(N_FORKS)]
# Create all of the philosophers
phils = [ \
  threading.Thread(target=philosopher,args=(forks[n], forks \
    [(n + 1) % N_FORKS])) for n in range(N_FORKS)]
# Run all of the philosophers
for p in phils:
    p.start()</pre>
			<p>Here, we have the <code>philosopher()</code> function as the underlying logic for our separate threads. It takes in two <code>Threading.Lock</code> objects and simulates the previously discussed eating procedure, with two context managers. In our main program, we create a list of five lock objects, named <code>forks</code>, and a list of five threads, named <code>phils</code>, with the specification that the first thread will take in the first and second locks, the second <a id="_idIndexMarker984"/>thread will take in the second and third locks, and so on; and the fifth thread will take in the fifth and first locks (in order). Finally, we start<a id="_idIndexMarker985"/> all five threads simultaneously.</p>
			<p>If we run the script, we will see that deadlock occurs almost immediately. The following is my output, up until the program hangs infinitely:</p>
			<pre>&gt; python3 example3.py
Philosopher at &lt;Thread(Thread-1, started 123145445048320)&gt; is 
eating.
Philosopher at &lt;Thread(Thread-1, started 123145445048320)&gt; is 
eating.
Philosopher at &lt;Thread(Thread-1, started 123145445048320)&gt; is 
eating.
Philosopher at &lt;Thread(Thread-1, started 123145445048320)&gt; is 
eating.
Philosopher at &lt;Thread(Thread-1, started 123145445048320)&gt; is 
eating.
Philosopher at &lt;Thread(Thread-1, started 123145445048320)&gt; is 
eating.
Philosopher at &lt;Thread(Thread-3, started 123145455558656)&gt; is 
eating.
Philosopher at &lt;Thread(Thread-1, started 123145445048320)&gt; is 
eating.
Philosopher at &lt;Thread(Thread-3, started 123145455558656)&gt; is 
eating.
Philosopher at &lt;Thread(Thread-3, started 123145455558656)&gt; is 
eating.
Philosopher at &lt;Thread(Thread-3, started 123145455558656)&gt; is 
eating.
Philosopher at &lt;Thread(Thread-3, started 123145455558656)&gt; is 
eating.
Philosopher at &lt;Thread(Thread-5, started 123145466068992)&gt; is 
eating.
Philosopher at &lt;Thread(Thread-3, started 123145455558656)&gt; is 
eating.
Philosopher at &lt;Thread(Thread-3, started 123145455558656)&gt; is 
eating.</pre>
			<p>The question that naturally follows is: how can we implement an order in which the locks are acquired in the <code>philosopher()</code> function? We will be using the built-in <code>id()</code> function <a id="_idIndexMarker986"/>in Python here, which returns the unique, constant<a id="_idIndexMarker987"/> identity of the parameter, as the keys to sort the lock objects. We will also implement a custom context manager to factor out this sorting logic in a separate class. Navigate to <code>Chapter12/example4.py</code> for this specific implementation, which looks as follows:</p>
			<pre>class acquire(object):
    def __init__(self, *locks):
        self.locks = sorted(locks, key=lambda x: id(x))
    def __enter__(self):
        for lock in self.locks:
            lock.acquire()
    def __exit__(self, ty, val, tb):
        for lock in reversed(self.locks):
            lock.release()
        return False
# The philosopher thread
def philosopher(left, right):
    while True:
        with acquire(left,right):
             print(f'Philosopher at \
               {threading.currentThread()} is eating.')</pre>
			<p>With the main program remaining the same, this script will produce an output showing that this solution of ranking can effectively address the dining philosophers problem.</p>
			<p>However, there is a problem with this approach when it is applied to some particular cases. Keeping the high-level idea of concurrency in mind, we know that one of our main goals when applying concurrency to our programs is to improve the speed. Let's go back to our two-lock example and examine the execution time of our program with resource <a id="_idIndexMarker988"/>ranking implemented. Take a look at the <code>Chapter12/example5.py</code> file; it is simply the two-lock program with ranked (or ordered) locking<a id="_idIndexMarker989"/> implemented, combined with a timer that has been added to keep track of how much time it takes for the two threads to finish executing.</p>
			<p>After running the script, your output should look similar to the following:</p>
			<pre>&gt; python3 example5.py
Thread A is starting...
Thread A waiting to acquire lock A.
Thread B is starting...
Thread A has acquired lock A, performing some calculation...
Thread B waiting to acquire lock A.
Thread A waiting to acquire lock B.
Thread A has acquired lock B, performing some calculation...
Thread A releasing both locks.
Thread B has acquired lock A, performing some calculation...
Thread B waiting to acquire lock B.
Thread B has acquired lock B, performing some calculation...
Thread B releasing both locks.
Took 14.01 seconds.
Finished.</pre>
			<p>Here, you can see that the combined execution of both threads took around 14 seconds. However, if we take a closer look at the specific instructions in the two threads, we will see that aside from interacting with the locks, thread A would take around 4 seconds to do its <a id="_idIndexMarker990"/>calculations (simulated by two <code>time.sleep(2)</code> commands), while thread B<a id="_idIndexMarker991"/> would take around 10 seconds (two <code>time.sleep(5)</code> commands).</p>
			<p>Does this mean that our program is taking as long as it would if we were to execute the two threads sequentially? We will test this theory with our <code>Chapter12/example6.py</code> file, in which we specify that each thread should execute its instructions one at a time with the same <code>thread_a()</code> and <code>thread_b()</code> functions:</p>
			<pre>lock_a = threading.Lock()
lock_b = threading.Lock()
thread1 = threading.Thread(target=thread_a)
thread2 = threading.Thread(target=thread_b)
start = timer()
thread1.start()
thread1.join()
thread2.start()
thread2.join()
print('Took %.2f seconds.' % (timer() - start))
print('Finished.')</pre>
			<p>If you run <a id="_idIndexMarker992"/>this script, you will see that this sequential version of our two-lock<a id="_idIndexMarker993"/> program will take the same amount of time as its concurrent counterpart:</p>
			<pre>&gt; python3 example6.py
Thread A is starting...
Thread A waiting to acquire lock A.
Thread A has acquired lock A, performing some calculation...
Thread A waiting to acquire lock B.
Thread A has acquired lock B, performing some calculation...
Thread A releasing both locks.
Thread B is starting...
Thread B waiting to acquire lock A.
Thread B has acquired lock A, performing some calculation...
Thread B waiting to acquire lock B.
Thread B has acquired lock B, performing some calculation...
Thread B releasing both locks.
Took 14.01 seconds.
Finished.</pre>
			<p>This interesting phenomenon is a direct result of the heavy requirements that we have placed on the locks in the program. In other words, since each thread has to acquire both locks to complete its execution, each lock cannot be acquired by more than one thread at any given time. The locks must be acquired in a specific order, and the execution of individual threads cannot happen simultaneously. If we were to go back and examine the output produced by the <code>Chapter12/example5.py</code> file, it would be apparent that thread B could not start its calculations after thread A released both locks at the end of its execution.</p>
			<p>It is quite intuitive, then, to conclude that if you placed enough locks on the resources of your concurrent program, it would become entirely sequential in its execution, and, combined with the overhead of concurrent programming functionalities, it would have an even <a id="_idIndexMarker994"/>worse speed than the purely sequential version<a id="_idIndexMarker995"/> of the program. However, we did not see this sequentiality that's created by locks in the dining philosophers problem (simulated in Python). This is because, in the two-thread problem, two locks were enough to sequentialize the program execution, while five were not enough to do the same for the dining philosophers problem.</p>
			<p>We will explore another instance of this phenomenon in <a href="B17499_13_Final_SS_ePub.xhtml#_idTextAnchor228"><em class="italic">Chapter 13</em></a>, <em class="italic">Starvation</em>.</p>
			<h2 id="_idParaDest-238"><a id="_idTextAnchor223"/>Ignoring locks and sharing resources</h2>
			<p><strong class="bold">Locks</strong> are undoubtedly an important tool in synchronization tasks, and in concurrent programming in <a id="_idIndexMarker996"/>general. However, if using locks leads to an undesirable situation, such <a id="_idIndexMarker997"/>as a deadlock, then it is natural for us to explore the option<a id="_idIndexMarker998"/> of simply not using locks in our concurrent<a id="_idIndexMarker999"/> programs. By ignoring locks, our program's resources effectively become shareable among different processes/threads in a <a id="_idIndexMarker1000"/>concurrent program, thus eliminating the first of the four Coffman conditions: <strong class="bold">mutual exclusion</strong>.</p>
			<p>This approach to the problem of a deadlock is straightforward to implement; let's try this with the two preceding examples. In the two-lock example, we simply remove the code that specifies any interaction<a id="_idIndexMarker1001"/> with the lock objects both inside the thread functions<a id="_idIndexMarker1002"/> and in the main program. In other words, we are not utilizing a locking mechanism anymore. The <code>Chapter12/example7.py</code> file contains the implementation of this approach, as follows:</p>
			<pre>import threading
import time
from timeit import default_timer as timer
def thread_a():
    print('Thread A is starting...')
    print('Thread A is performing some calculation...')
    time.sleep(2)
    print('Thread A is performing some calculation...')
    time.sleep(2)
def thread_b():
    print('Thread B is starting...')
    print('Thread B is performing some calculation...')
    time.sleep(5)
    print('Thread B is performing some calculation...')
    time.sleep(5)
thread1 = threading.Thread(target=thread_a)
thread2 = threading.Thread(target=thread_b)
start = timer()
thread1.start()
thread2.start()
thread1.join()
thread2.join()
print('Took %.2f seconds.' % (timer() - start))
print('Finished.')</pre>
			<p>If you run the script, your<a id="_idIndexMarker1003"/> output should look similar<a id="_idIndexMarker1004"/> to the following:</p>
			<pre>&gt; python3 example7.py
Thread A is starting...
Thread A is performing some calculation...
Thread B is starting...
Thread B is performing some calculation...
Thread A is performing some calculation...
Thread B is performing some calculation...
Took 10.00 seconds.
Finished.</pre>
			<p>It is clear that since we are not using locks to restrict access to any calculation processes, the executions <a id="_idIndexMarker1005"/>of the two threads have become entirely independent of <a id="_idIndexMarker1006"/>one another, so the threads were run completely in parallel. For this reason, we also obtained a better speed: since the threads ran in parallel, the total time that the whole program took was the same as the time that the longer of the two threads took (in other words, thread <code>B</code>, with <code>10</code> seconds).</p>
			<p>What about the dining philosophers problem? It seems that we can also conclude that without locks (the forks), the problem can be solved easily. Since the resources (food) are unique to each philosopher (in other words, no philosopher should eat another philosopher's food), it should be the case that each philosopher can proceed with their execution without worrying about the others. By ignoring the locks, each can be executed in parallel, similar to what we saw in our two-lock example.</p>
			<p>Doing this, however, means that we are completely misunderstanding the problem. We know that locks are utilized so that processes and threads can access the shared resources in a program in a systematic, coordinated way, to avoid mishandling the data. Therefore, removing any locking mechanisms in a concurrent program means that the likelihood of the shared resources, which are now free from access limitations, being manipulated in an uncoordinated way (and therefore, becoming corrupted) increases significantly.</p>
			<p>So, by ignoring locks, it is relatively likely that we will need to completely redesign and restructure our concurrent program. If the shared resources still need to be accessed and manipulated in an organized way, other synchronization methods will need to be implemented. The logic of our processes and threads might need to be altered to interact with this new synchronization method appropriately, the execution time might be negatively affected by this change in the structure of the program, and other potential synchronization problems might also arise.</p>
			<h3 id="_idParaDest-239">An additional note about locks</h3>
			<p>While the approach of dismissing locking mechanisms in our program to eliminate deadlocks might <a id="_idIndexMarker1007"/>raise some questions and concerns, it does effectively reveal a new point for us about lock objects in Python: it is possible for an element of a concurrent program to completely bypass the locks when accessing a given resource. In other words, lock objects only prevent different processes/threads from accessing and manipulating a shared resource if those processes or threads acquire the lock objects.</p>
			<p>Locks, then, do not lock anything. They are simply flags that help indicate whether a resource should be accessed at a given time; if a poorly instructed, or even malicious, process/thread attempts to access that resource without checking that the lock object exists, it will most likely be able to do that without difficulty. In other words, locks are not connected to the resources that they are supposed to lock, and they most certainly do not block processes/threads from accessing those resources.</p>
			<p>So, simply using locks<a id="_idIndexMarker1008"/> is inefficient for designing and implementing a secure, dynamic, concurrent data structure. To achieve that, we would need to either add more concrete links between the locks and their corresponding resources or utilize a different synchronization tool altogether (for example, atomic message queues).</p>
			<h3 id="_idParaDest-240">Concluding note on deadlock solutions</h3>
			<p>In this chapter, you have seen two of the most common approaches to the deadlock problem. Each addresses <a id="_idIndexMarker1009"/>one of the four Coffman conditions, and while both (somewhat) successfully prevent deadlocks from occurring in our examples, each raises different, additional problems and concerns. So, it is important to truly understand the nature of your concurrent programs to know which of the two is applicable, if either of them is.</p>
			<p>It is also possible that some programs, through deadlocks, are revealed to us as unsuitable to be made concurrent; some programs are better left sequential and will be made worse with forced concurrency. As we have discussed, while concurrency provides significant improvements in many areas of our applications, some are inherently inappropriate for concurrent programming. In deadlock situations, developers should be ready to consider different approaches to designing a concurrent program and should not be reluctant to implement another method when one concurrent approach does not work.</p>
			<h1 id="_idParaDest-241"><a id="_idTextAnchor224"/>The concept of livelocks</h1>
			<p>The concept of a livelock is <a id="_idIndexMarker1010"/>connected to a deadlock; some even consider it an alternate version of a deadlock. In a livelock situation, the processes (or threads) in the concurrent program can switch their states; in fact, they switch states constantly. Yet, they simply switch back and forth infinitely, and no progress is made. We will now consider an actual scenario of a livelock.</p>
			<p>Suppose that a pair of spouses are eating dinner together at a table. They only have one fork to share, so only one of them can eat at any given point. Additionally, the spouses are polite to each <a id="_idIndexMarker1011"/>other, so even if one spouse is hungry and wants to eat their food, they will leave the fork on the table if their partner is also hungry. This specification is at the heart of creating a livelock for this problem: when both spouses are hungry, each will wait for the other to eat first, creating an infinite loop in which each spouse switches between wanting to eat and waiting for the other spouse to eat.</p>
			<p>Let's simulate this problem in Python. Navigate to <code>Chapter12/example8.py</code> and take a look at the <code>Spouse</code> class:</p>
			<pre>class Spouse(threading.Thread):
    def __init__(self, name, partner):
        threading.Thread.__init__(self)
        self.name = name
        self.partner = partner
        self.hungry = True
    def run(self):
        while self.hungry:
            print('%s is hungry and wants to eat.' % self.name)
            if self.partner.hungry:
                print('%s is waiting for their partner to eat \
                  first...' % self.name)
            else:
                with fork:
                    print('%s has stared eating.' % self.name)
                    time.sleep(5)
                    print('%s is now full.' % self.name)
                    self.hungry = False</pre>
			<p>This class inherits from the <code>threading.Thread</code> class and implements the logic that we discussed previously. It takes in a name for the <code>Spouse</code> instance and another <code>Spouse</code> object as <a id="_idIndexMarker1012"/>its partner; when initialized, a <code>Spouse</code> object is also always hungry (the <code>hungry</code> attribute is always set to <code>True</code>). The <code>run()</code> function in the class specifies the logic when the thread is started: so long as the <code>Spouse</code> object's <code>hungry</code> attribute is set to <code>True</code>, the object will attempt to use the fork, which is a lock object, to eat. However, it always checks whether its partner also has its <code>hungry</code> attribute set to <code>True</code>, in which case it will not proceed to acquire the lock, and will instead wait for its partner to do it.</p>
			<p>In our main program, we create the fork as a lock object first; then, we create two <code>Spouse</code> thread objects, which are each other's <code>partner</code> attributes. Finally, we start both threads and run the program until both threads finish executing:</p>
			<pre>fork = threading.Lock()
partner1 = Spouse('Wife', None)
partner2 = Spouse('Husband', partner1)
partner1.partner = partner2
partner1.start()
partner2.start()
partner1.join()
partner2.join()
print('Finished.')</pre>
			<p>If you run the script, you will see that, as we discussed, each thread will go into an infinite loop, switching between wanting to eat and waiting for its partner to eat; the program will run forever until Python is interrupted. The following code shows the first few lines of output that I obtained:</p>
			<pre>&gt; python3 example8.py
Wife is hungry and wants to eat.
Wife is waiting for their partner to eat first...
Husband is hungry and wants to eat.
Wife is hungry and wants to eat.
Husband is waiting for their partner to eat first...
Wife is waiting for their partner to eat first...
Husband is hungry and wants to eat.
Wife is hungry and wants to eat.
Husband is waiting for their partner to eat first...
Wife is waiting for their partner to eat first...
Husband is hungry and wants to eat.
Wife is hungry and wants to eat.
Husband is waiting for their partner to eat first...
...</pre>
			<p>And with that, we can <a id="_idIndexMarker1013"/>conclude our discussion on deadlocks and livelocks.</p>
			<h1 id="_idParaDest-242"><a id="_idTextAnchor225"/>Summary</h1>
			<p>In this chapter, we covered the causes of deadlocks in concurrent applications and implemented approaches to prevent them from occurring. Our examples have shown that concurrency cannot always be achieved straightforwardly and that some situations may require special handling. These discussions have prepared us for deadlocks in the real world and pointed us toward potential solutions.</p>
			<p>In the next chapter, we will discuss another common problem in concurrent programming: starvation.</p>
			<h1 id="_idParaDest-243"><a id="_idTextAnchor226"/>Questions</h1>
			<ol>
				<li value="1">What can lead to a deadlock situation, and why is it undesirable?</li>
				<li>How is the dining philosophers problem related to the problem of a deadlock?</li>
				<li>What are the four Coffman conditions?</li>
				<li>How can resource ranking solve the problem of a deadlock? What other problems can occur when this is implemented?</li>
				<li>How can ignoring locks solve the problem of a deadlock? What other problems can occur when this is implemented?</li>
				<li>How is a livelock related to a deadlock?</li>
			</ol>
			<h1 id="_idParaDest-244"><a id="_idTextAnchor227"/>Further reading</h1>
			<ul>
				<li><em class="italic">Parallel Programming with Python</em>, by Jan. Palach, Packt Publishing Ltd, 2014</li>
				<li><em class="italic">Python Parallel Programming Cookbook</em>, by Giancarlo Zaccone, Packt Publishing Ltd, 2015</li>
				<li><em class="italic">Python Thread Deadlock Avoidance</em> (<code>dabeaz.blogspot.com/2009/11/python-thread-deadlock-avoidance_20</code>)</li>
			</ul>
		</div>
	</div>
</div>
</body></html>