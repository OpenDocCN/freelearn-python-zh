["```py\n# section 1 \nUser-agent: BadCrawler \nDisallow: / \n\n# section 2 \nUser-agent: * \nCrawl-delay: 5 \nDisallow: /trap \n\n# section 3 \nSitemap: http://example.webscraping.com/sitemap.xml \n\n```", "```py\n<?xml version=\"1.0\" encoding=\"UTF-8\"?> \n<urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"> \n  <url><loc>http://example.webscraping.com/view/Afghanistan-1</loc></url> \n  <url><loc>http://example.webscraping.com/view/Aland-Islands-2</loc></url> \n  <url><loc>http://example.webscraping.com/view/Albania-3</loc></url> \n  ... \n</urlset> \n\n```", "```py\ndocker pull scrapinghub/splash\npip install detectem\n\n```", "```py\n       $ det http://example.webscraping.com\n       [('jquery', '1.11.0')]\n\n```", "```py\n$ docker pull wappalyzer/cli\n\n```", "```py\n$ docker run wappalyzer/cli http://example.webscraping.com\n\n```", "```py\n{'applications': \n[{'categories': ['Javascript Frameworks'],\n     'confidence': '100',\n     'icon': 'Modernizr.png',\n     'name': 'Modernizr',\n     'version': ''},\n {'categories': ['Web Servers'],\n     'confidence': '100',\n     'icon': 'Nginx.svg',\n     'name': 'Nginx',\n     'version': ''},\n {'categories': ['Web Frameworks'],\n     'confidence': '100',\n     'icon': 'Twitter Bootstrap.png',\n     'name': 'Twitter Bootstrap',\n     'version': ''},\n {'categories': ['Web Frameworks'],\n     'confidence': '100',\n     'icon': 'Web2py.png',\n     'name': 'Web2py',\n     'version': ''},\n {'categories': ['Javascript Frameworks'],\n     'confidence': '100',\n     'icon': 'jQuery.svg',\n     'name': 'jQuery',\n     'version': ''},\n {'categories': ['Javascript Frameworks'],\n     'confidence': '100',\n     'icon': 'jQuery UI.svg',\n     'name': 'jQuery UI',\n     'version': '1.10.3'},\n {'categories': ['Programming Languages'],\n     'confidence': '100',\n     'icon': 'Python.png',\n     'name': 'Python',\n     'version': ''}],\n 'originalUrl': 'http://example.webscraping.com',\n 'url': 'http://example.webscraping.com'}\n\n```", "```py\n   pip install python-whois\n\n```", "```py\n   >>> import whois\n >>> print(whois.whois('appspot.com'))\n {\n ...\n \"name_servers\": [\n \"NS1.GOOGLE.COM\", \n \"NS2.GOOGLE.COM\", \n \"NS3.GOOGLE.COM\", \n \"NS4.GOOGLE.COM\", \n \"ns4.google.com\", \n \"ns2.google.com\", \n \"ns1.google.com\", \n \"ns3.google.com\"\n ], \n \"org\": \"Google Inc.\", \n \"emails\": [\n \"abusecomplaints@markmonitor.com\", \n \"dns-admin@google.com\"\n ]\n }\n\n```", "```py\nimport urllib.request\ndef download(url): \n    return urllib.request.urlopen(url).read() \n\n```", "```py\nimport urllib.request\nfrom urllib.error import URLError, HTTPError, ContentTooShortError\n\ndef download(url):\n    print('Downloading:', url)\n    try:\n        html = urllib.request.urlopen(url).read()\n    except (URLError, HTTPError, ContentTooShortError) as e:\n        print('Download error:', e.reason)\n        html = None\n    return html\n\n```", "```py\ndef download(url, num_retries=2): \n    print('Downloading:', url)\n    try: \n        html = urllib.request.urlopen(url).read()\n    except (URLError, HTTPError, ContentTooShortError) as e: \n        print('Download error:', e.reason)\n        html = None \n        if num_retries > 0: \n                 if hasattr(e, 'code') and 500 <= e.code < 600: \n                # recursively retry 5xx HTTP errors \n                return download(url, num_retries - 1) \n    return html\n\n```", "```py\n >>> download('http://httpstat.us/500')\nDownloading: http://httpstat.us/500\nDownload error: Internal Server Error\nDownloading: http://httpstat.us/500\nDownload error: Internal Server Error\nDownloading: http://httpstat.us/500\nDownload error: Internal Server Error\n\n```", "```py\ndef download(url, user_agent='wswp', num_retries=2): \n    print('Downloading:', url) \n    request = urllib.request.Request(url) \n    request.add_header('User-agent', user_agent)\n    try: \n        html = urllib.request.urlopen(request).read() \n    except (URLError, HTTPError, ContentTooShortError) as e:\n        print('Download error:', e.reason)\n        html = None \n        if num_retries > 0: \n            if hasattr(e, 'code') and 500 <= e.code < 600: \n                # recursively retry 5xx HTTP errors \n                return download(url, num_retries - 1) \n    return html\n\n```", "```py\nimport re\n\ndef download(url, user_agent='wswp', num_retries=2, charset='utf-8'): \n    print('Downloading:', url) \n    request = urllib.request.Request(url) \n    request.add_header('User-agent', user_agent)\n    try: \n        resp = urllib.request.urlopen(request)\n        cs = resp.headers.get_content_charset()\n        if not cs:\n            cs = charset\n        html = resp.read().decode(cs)\n    except (URLError, HTTPError, ContentTooShortError) as e:\n        print('Download error:', e.reason)\n        html = None \n        if num_retries > 0: \n            if hasattr(e, 'code') and 500 <= e.code < 600: \n            # recursively retry 5xx HTTP errors \n            return download(url, num_retries - 1) \n    return html\n\ndef crawl_sitemap(url): \n    # download the sitemap file \n    sitemap = download(url) \n    # extract the sitemap links \n    links = re.findall('<loc>(.*?)</loc>', sitemap) \n    # download each link \n    for link in links: \n        html = download(link) \n        # scrape html here \n        # ... \n\n```", "```py\n    >>> crawl_sitemap('http://example.webscraping.com/sitemap.xml')\nDownloading: http://example.webscraping.com/sitemap.xml\nDownloading: http://example.webscraping.com/view/Afghanistan-1\nDownloading: http://example.webscraping.com/view/Aland-Islands-2\nDownloading: http://example.webscraping.com/view/Albania-3\n...\n\n```", "```py\nimport itertools \n\ndef crawl_site(url):\n    for page in itertools.count(1): \n        pg_url = '{}{}'.format(url, page) \n        html = download(pg_url) \n        if html is None: \n            break \n        # success - can scrape the result\n\n```", "```py\n>>> crawl_site('http://example.webscraping.com/view/-')\nDownloading: http://example.webscraping.com/view/-1\nDownloading: http://example.webscraping.com/view/-2\nDownloading: http://example.webscraping.com/view/-3\nDownloading: http://example.webscraping.com/view/-4\n[...]\n\n```", "```py\ndef crawl_site(url, max_errors=5):\n    for page in itertools.count(1): \n        pg_url = '{}{}'.format(url, page) \n        html = download(pg_url) \n        if html is None: \n            num_errors += 1\n            if num_errors == max_errors:\n                # max errors reached, exit loop\n                break\n        else:\n            num_errors = 0\n            # success - can scrape the result\n\n```", "```py\nimport re \n\ndef link_crawler(start_url, link_regex): \n    \"\"\" Crawl from the given start URL following links matched by link_regex \n    \"\"\" \n    crawl_queue = [start_url] \n    while crawl_queue: \n        url = crawl_queue.pop() \n        html = download(url) \n        if html is not None:\n            continue\n        # filter for links matching our regular expression \n        for link in get_links(html): \n            if re.match(link_regex, link): \n                crawl_queue.append(link) \n\ndef get_links(html): \n    \"\"\" Return a list of links from html \n    \"\"\" \n    # a regular expression to extract all links from the webpage \n    webpage_regex = re.compile(\"\"\"<a[^>]+href=[\"'](.*?)[\"']\"\"\", re.IGNORECASE) \n    # list of all links from the webpage \n    return webpage_regex.findall(html) \n\n```", "```py\n>>> link_crawler('http://example.webscraping.com', '/(index|view)/') \nDownloading: http://example.webscraping.com \nDownloading: /index/1 \nTraceback (most recent call last): \n  ... \nValueError: unknown url type: /index/1 \n\n```", "```py\nfrom urllib.parse import urljoin\n\ndef link_crawler(start_url, link_regex): \n    \"\"\" Crawl from the given start URL following links matched by link_regex \n    \"\"\" \n    crawl_queue = [start_url] \n    while crawl_queue: \n        url = crawl_queue.pop() \n        html = download(url) \n        if not html:\n            continue\n        for link in get_links(html): \n            if re.match(link_regex, link): \n                abs_link = urljoin(start_url, link) \n                crawl_queue.append(abs_link) \n\n```", "```py\ndef link_crawler(start_url, link_regex): \n    crawl_queue = [start_url] \n    # keep track which URL's have seen before \n    seen = set(crawl_queue) \n    while crawl_queue: \n        url = crawl_queue.pop() \n        html = download(url)\n        if not html:\n            continue \n        for link in get_links(html): \n            # check if link matches expected regex \n            if re.match(link_regex, link): \n                abs_link = urljoin(start_url, link) \n                # check if have already seen this link \n                if abs_link not in seen: \n                    seen.add(abs_link) \n                    crawl_queue.append(abs_link) \n\n```", "```py\n    >>> from urllib import robotparser\n>>> rp = robotparser.RobotFileParser()\n>>> rp.set_url('http://example.webscraping.com/robots.txt')\n>>> rp.read()\n>>> url = 'http://example.webscraping.com'\n>>> user_agent = 'BadCrawler'\n>>> rp.can_fetch(user_agent, url)\nFalse\n>>> user_agent = 'GoodCrawler'\n>>> rp.can_fetch(user_agent, url)\nTrue\n\n```", "```py\ndef get_robots_parser(robots_url):\n    \" Return the robots parser object using the robots_url \"\n    rp = robotparser.RobotFileParser()\n    rp.set_url(robots_url)\n    rp.read()\n    return rp\n\n```", "```py\ndef link_crawler(start_url, link_regex, robots_url=None, user_agent='wswp'):\n    ...\n    if not robots_url:\n        robots_url = '{}/robots.txt'.format(start_url)\n    rp = get_robots_parser(robots_url)\n\n```", "```py\n... \nwhile crawl_queue: \n    url = crawl_queue.pop() \n    # check url passes robots.txt restrictions \n    if rp.can_fetch(user_agent, url):\n         html = download(url, user_agent=user_agent) \n         ... \n    else: \n        print('Blocked by robots.txt:', url) \n\n```", "```py\n>>> link_crawler('http://example.webscraping.com', '/(index|view)/', user_agent='BadCrawler')\nBlocked by robots.txt: http://example.webscraping.com \n\n```", "```py\nproxy = 'http://myproxy.net:1234' # example string \nproxy_support = urllib.request.ProxyHandler({'http': proxy})\nopener = urllib.request.build_opener(proxy_support)\nurllib.request.install_opener(opener) \n# now requests via urllib.request will be handled via proxy\n\n```", "```py\ndef download(url, user_agent='wswp', num_retries=2, charset='utf-8', proxy=None): \n    print('Downloading:', url) \n    request = urllib.request.Request(url) \n    request.add_header('User-agent', user_agent)\n    try: \n        if proxy:\n            proxy_support = urllib.request.ProxyHandler({'http': proxy})\n            opener = urllib.request.build_opener(proxy_support)\n            urllib.request.install_opener(opener)\n        resp = urllib.request.urlopen(request)\n        cs = resp.headers.get_content_charset()\n        if not cs:\n            cs = charset\n        html = resp.read().decode(cs)\n    except (URLError, HTTPError, ContentTooShortError) as e:\n        print('Download error:', e.reason)\n        html = None \n        if num_retries > 0: \n            if hasattr(e, 'code') and 500 <= e.code < 600: \n            # recursively retry 5xx HTTP errors \n            return download(url, num_retries - 1) \n    return html\n\n```", "```py\nfrom urllib.parse import urlparse\nimport time\n\nclass Throttle: \n    \"\"\"Add a delay between downloads to the same domain \n    \"\"\" \n    def __init__(self, delay): \n        # amount of delay between downloads for each domain \n        self.delay = delay \n        # timestamp of when a domain was last accessed \n        self.domains = {} \n\n    def wait(self, url): \n        domain = urlparse(url).netloc \n        last_accessed = self.domains.get(domain) \n\n        if self.delay > 0 and last_accessed is not None: \n            sleep_secs = self.delay - (time.time() - last_accessed) \n            if sleep_secs > 0: \n                # domain has been accessed recently \n                # so need to sleep \n                time.sleep(sleep_secs) \n        # update the last accessed time \n        self.domains[domain] = time.time() \n\n```", "```py\nthrottle = Throttle(delay) \n... \nthrottle.wait(url) \nhtml = download(url, user_agent=user_agent, num_retries=num_retries, \n                proxy=proxy, charset=charset) \n\n```", "```py\ndef link_crawler(..., max_depth=4): \n    seen = {} \n    ... \n    if rp.can_fetch(user_agent, url): \n        depth = seen.get(url, 0)\n        if depth == max_depth:\n            print('Skipping %s due to depth' % url)\n            continue\n        ...\n        for link in get_links(html):\n            if re.match(link_regex, link):\n                abs_link = urljoin(start_url, link)\n                if abs_link not in seen: \n                    seen[abs_link] = depth + 1 \n                    crawl_queue.append(abs_link) \n\n```", "```py\n    >>> start_url = 'http://example.webscraping.com/index'\n>>> link_regex = '/(index|view)'\n>>> link_crawler(start_url, link_regex, user_agent='BadCrawler')\nBlocked by robots.txt: http://example.webscraping.com/\n\n```", "```py\n    >>> link_crawler(start_url, link_regex, max_depth=1)\nDownloading: http://example.webscraping.com//index\nDownloading: http://example.webscraping.com/index/1\nDownloading: http://example.webscraping.com/view/Antigua-and-Barbuda-10\nDownloading: http://example.webscraping.com/view/Antarctica-9\nDownloading: http://example.webscraping.com/view/Anguilla-8\nDownloading: http://example.webscraping.com/view/Angola-7\nDownloading: http://example.webscraping.com/view/Andorra-6\nDownloading: http://example.webscraping.com/view/American-Samoa-5\nDownloading: http://example.webscraping.com/view/Algeria-4\nDownloading: http://example.webscraping.com/view/Albania-3\nDownloading: http://example.webscraping.com/view/Aland-Islands-2\nDownloading: http://example.webscraping.com/view/Afghanistan-1\n\n```", "```py\npip install requests\n\n```", "```py\ndef download(url, user_agent='wswp', num_retries=2, proxies=None):\n    print('Downloading:', url)\n    headers = {'User-Agent': user_agent}\n    try:\n        resp = requests.get(url, headers=headers, proxies=proxies)\n        html = resp.text\n        if resp.status_code >= 400:\n            print('Download error:', resp.text)\n            html = None\n            if num_retries and 500 <= resp.status_code < 600:\n                # recursively retry 5xx HTTP errors\n                return download(url, num_retries - 1)\n    except requests.exceptions.RequestException as e:\n        print('Download error:', e.reason)\n        html = None\n\n```"]