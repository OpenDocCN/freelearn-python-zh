- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clean Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final chapter, we focus on how everything fits together in the design
    of a whole system. This is more of a theoretical chapter. Given the nature of
    the topic, it would be too complex to delve down into the more low-level details.
    Besides, the point is precisely to escape from those details, assume that all
    the principles explored in previous chapters are assimilated, and focus on the
    design of a system at scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main goals of this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Designing software systems that can be maintained in the long run
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working effectively on a software project by maintaining quality attributes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Studying how all concepts applied to code relate to systems in general
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter explores how clean code evolves into a clean architecture, and
    conversely how clean code is also the cornerstone of good architecture. A software
    solution is effective if it has quality. The architecture needs to enable this
    by achieving quality attributes (performance, testability, maintainability, and
    so on). But then the code needs to also enable this on every component.
  prefs: []
  type: TYPE_NORMAL
- en: The first section starts by exploring the relationship between the code and
    the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: From clean code to clean architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section is a discussion of how concepts that were emphasized in previous
    chapters reappear in a slightly different shape when we consider aspects of large
    systems. There is an interesting resemblance to how concepts that apply to more
    detailed design, as well as code, also apply to large systems and architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'The concepts explored in previous chapters were related to single applications,
    generally, a project, which might be a single repository (or a few), of a source
    control version system (Git). This is not to say that those design ideas are only
    applicable to code, or that they are of no use when thinking of an architecture,
    for two reasons: the code is the foundation of the architecture, and, if it''s
    not written carefully, the system will fail regardless of how well thought-out
    the architecture is.'
  prefs: []
  type: TYPE_NORMAL
- en: Second, some principles that were covered in previous chapters do not apply
    only to code but are design ideas instead. The clearest example comes from design
    patterns. They are high-level abstractions. With them, we can get a quick picture
    of how a component in our architecture might appear, without going into the details
    of the code.
  prefs: []
  type: TYPE_NORMAL
- en: But large enterprise systems typically consist of many of these applications,
    and now it's time to start thinking in terms of a larger design, in the form of
    a distributed system.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we discuss the main topics that have been discussed
    throughout the book, but now from the perspective of a whole system.
  prefs: []
  type: TYPE_NORMAL
- en: Software architecture is good if it's effective. The most common aspects to
    look at in a good architecture are the so-called quality attributes (traits like
    scalability, security, performance, and endurance are the most common ones). This
    makes sense; after all, you want your system to handle an increase of load without
    collapsing, and to be able to work continuously for indefinite periods of time
    without requiring maintenance, and also to be extensible to support new requirements.
  prefs: []
  type: TYPE_NORMAL
- en: But the operational aspects of an architecture also make it clean. Traits like
    operability, continuous integration, and how easy it is to release changes also
    influence the overall quality of the system.
  prefs: []
  type: TYPE_NORMAL
- en: Separation of concerns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inside an application, there are multiple components. Their code is divided
    into other subcomponents, such as modules or packages, and the modules into classes
    or functions, and the classes into methods. Throughout the book, the emphasis
    has been on keeping these components as small as possible, particularly in the
    case of functions—functions should do one thing and be small.
  prefs: []
  type: TYPE_NORMAL
- en: Several reasons were presented to justify this rationale. Small functions are
    easier to understand, follow, and debug. They are also easier to test. The smaller
    the pieces in our code, the easier it will be to write unit tests for it.
  prefs: []
  type: TYPE_NORMAL
- en: For the components of each application, we wanted different traits, mainly high
    cohesion and low coupling. By dividing components into smaller units, each one
    with a single and well-defined responsibility, we achieve a better structure where
    changes are easier to manage. In the face of new requirements, there will be a
    single correct place to make the changes, and the rest of the code should probably
    be unaffected.
  prefs: []
  type: TYPE_NORMAL
- en: When we talk about code, we say *component* to refer to one of these cohesive
    units (it might be a class, for example). When speaking in terms of architecture,
    a component means anything in the system that can be treated as a working unit.
    The term component itself is quite vague, so there is no universally accepted
    definition in software architecture of what this means more concretely. The concept
    of a working unit is something that can vary from project to project. A component
    should be able to be released or deployed with its own cycles, independently from
    the rest of the system.
  prefs: []
  type: TYPE_NORMAL
- en: For Python projects, a component could be a package, but a service can also
    be a component. Notice how two different concepts, with different levels of granularity,
    can be considered under the same category. To give an example, the event systems
    we used in previous chapters could be considered a component. They are a working
    unit with a clearly defined purpose (to enrich events identified from logs). They
    can be deployed independently from the rest (whether as a Python package, or,
    if we expose their functionality, as a service; more on that later), and they're
    a part of the entire system, but not the whole application itself.
  prefs: []
  type: TYPE_NORMAL
- en: In the examples in previous chapters, we saw idiomatic code, and we also highlighted
    the importance of good design for our code, with objects that have single, well-defined
    responsibilities being isolated, orthogonal, and easier to maintain. This very
    same criteria, which applies to a detailed design (functions, classes, methods),
    also applies to the components of software architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind good design principles when looking at the big picture.
  prefs: []
  type: TYPE_NORMAL
- en: It's probably undesirable for a large system to be just one component. A monolithic
    application will act as the single source of truth, responsible for everything
    in the system, and that will carry a lot of undesired consequences (harder to
    isolate and identify changes, to test effectively, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: In the same way, our code will be harder to maintain if we are not careful and
    place everything in one place, the application will suffer from similar problems
    if its components aren't treated with the same level of attention.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of creating cohesive components in a system can have more than one
    implementation, depending on the level of abstraction we require.
  prefs: []
  type: TYPE_NORMAL
- en: One option would be to identify common logic that is likely to be reused multiple
    times and place it in a Python package (we will discuss the details later in the
    chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Another alternative would be to break the application down into multiple smaller
    services, in a microservice architecture. The idea is to have components with
    a single and well-defined responsibility and achieve the same functionality as
    a monolithic application by making those services cooperate and exchange information.
  prefs: []
  type: TYPE_NORMAL
- en: Monolithic applications and microservices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most important idea from the previous section is the concept of separating
    concerns: different responsibilities should be distributed across diverse components.
    Just as in our code (a more detailed level of design) it wouldn''t be good to
    have a giant object that knows everything, in our architecture, there shouldn''t
    be a single component owning everything.'
  prefs: []
  type: TYPE_NORMAL
- en: There is, however, an important distinction. Different components don't necessarily
    mean different services. It's possible to divide the application into smaller
    Python packages (we'll look at packaging later in the chapter) and create a single
    service composed of many dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Separating responsibilities into different services is a good idea that has
    some benefits, but it also comes at a cost.
  prefs: []
  type: TYPE_NORMAL
- en: In the case that there's code that needs to be reused across several other services,
    a typical response is to encapsulate that into a microservice to be called by
    many other services in the company. This isn't the only way to reuse code. Consider
    the possibility of packaging that logic as a library to be imported by other components.
    Of course, this is only viable as long as all other components are written in
    the same language; otherwise, yes, the microservices pattern is the only option
    left.
  prefs: []
  type: TYPE_NORMAL
- en: 'Microservices architecture has the advantage of total decoupling: different
    services can be written in different languages or frameworks, and even be deployed
    independently. They can also be tested in isolation. This comes at a cost. They
    also need a strong contract for clients to know how to interact with this service,
    and they''re also subject to **service-level agreements** (**SLAs**) and **service-level
    objectives** (**SLOs**), respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'They also incur increased latency: having to call external services to get
    data (whether via HTTP or gRPC) takes a toll on the overall performance.'
  prefs: []
  type: TYPE_NORMAL
- en: An application composed of fewer services is more rigid and can't be deployed
    independently. It could even be more fragile as it might become a single point
    of failure. On the other hand, it could be more efficient (since we're avoiding
    expensive I/O calls), and we could still achieve a good separation of components
    by using Python packages.
  prefs: []
  type: TYPE_NORMAL
- en: The food for thought of this section is to ponder the right architectural style
    between creating a new service or using Python packages.
  prefs: []
  type: TYPE_NORMAL
- en: Abstractions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is where encapsulation appears again. When it comes to our systems (as
    we do in relation to code), we want to speak in terms of the domain problem and
    leave the implementation details as hidden as possible.
  prefs: []
  type: TYPE_NORMAL
- en: In the same way that code has to be expressive (almost to the point of being
    self-documenting) and have the right abstractions that reveal the solution to
    the essential problem (minimizing accidental complexity), the architecture should
    tell us what the system is about. Details such as the solution used to persist
    data on disk, the web framework of choice, the libraries used to connect to external
    agents, and interaction between systems are not relevant. What is relevant is
    what the system does. A concept such as a screaming architecture (SCREAM) reflects
    this idea.
  prefs: []
  type: TYPE_NORMAL
- en: The **Dependency Inversion Principle** (**DIP**), explained in *Chapter 4*,
    *The SOLID Principles*, is of great help in this regard; we don't want to depend
    upon concrete implementations but rather abstractions. In the code, we place abstractions
    (or interfaces) on the boundaries, the dependencies, those parts of the application
    that we don't control and might change in the future. We do this because we want
    to invert the dependencies and let them have to adapt to our code (by having to
    comply with an interface), not the other way round.
  prefs: []
  type: TYPE_NORMAL
- en: Creating abstractions and inverting dependencies are good practices, but they're
    not enough. We want our entire application to be independent and isolated from
    things that are out of our control. And this goes even further than just abstracting
    with objects—we need layers of abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: This is a subtle yet important difference with respect to the detailed design.
    In the DIP, it is recommended to create an interface that could be implemented
    with the `abc` module from the standard library, for instance. Because Python
    works with duck typing, while using an abstract class might be helpful, it's not
    mandatory, as we can easily achieve the same effect with regular objects as long
    as they comply with the required interface.
  prefs: []
  type: TYPE_NORMAL
- en: The dynamic typing nature of Python allows us to have these alternatives. When
    thinking in terms of architecture, there is no such thing. As it will become clearer
    with the following example, we need to abstract dependencies entirely, and there
    is no feature of Python that can do that for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some might argue: "Well, the **Object-Relational Mapper** (**ORM**) is a good
    abstraction for a database, isn''t it?" No. The ORM itself is a dependency and,
    as such, is out of our control. It would be even better to create an intermediate
    layer, an adapter, between the API of the ORM and our application.'
  prefs: []
  type: TYPE_NORMAL
- en: This means that we don't abstract the database just with an ORM; we use the
    abstraction layer we create on top of it to define objects of our own that belong
    to our domain. If that abstraction just happens to use an ORM underneath, that's
    a coincidence; the domain layer (where our business logic lies) shouldn't be concerned
    with it.
  prefs: []
  type: TYPE_NORMAL
- en: Having abstractions of our own gives us more flexibility and control over the
    application. We might even later decide that we don't want an ORM at all (let's
    say because we want more control over the database engine we're using), and if
    we coupled our application with a specific ORM (or any library in general), it'll
    be harder to change that in the future. The idea is to insulate the core of our
    application from external dependencies we don't have control over.
  prefs: []
  type: TYPE_NORMAL
- en: The application then imports this component, and uses the entities provided
    by this layer, but not the other way round. The abstraction layer should not know
    about the logic of our application; it's even truer that the database should know
    nothing about the application itself. If that were the case, the database would
    be coupled to our application. The goal is to invert the dependency—this layer
    provides an API, and every storage component that wants to connect has to conform
    to this API. This is the concept of a *hexagonal architecture* (HEX).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we analyze concrete tools that will help us create components
    to use in our architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Software components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have a large system now, and we need to scale it. It also has to be maintainable.
    At this point, the concerns aren't only technical but also organizational. This
    means it's not just about managing software repositories; each repository will
    most likely belong to an application, and it will be maintained by a team who
    owns that part of the system.
  prefs: []
  type: TYPE_NORMAL
- en: This demands that we keep in mind how a large system is divided into different
    components. This can have many phases, from a very simple approach about, say,
    creating Python packages, to more complex scenarios in a microservice architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The situation could be even more complex when different languages are involved,
    but in this chapter, we will assume they are all Python projects.
  prefs: []
  type: TYPE_NORMAL
- en: These components need to interact, as do the teams. The only way this can work
    at scale is if all the parts agree on an interface, a contract.
  prefs: []
  type: TYPE_NORMAL
- en: Packages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Python package is a convenient way to distribute software and reuse code in
    a more general way. Packages that have been built can be published to an artifact
    repository (such as an internal PyPi server for the company), from where they
    will be downloaded by the rest of the applications that require them.
  prefs: []
  type: TYPE_NORMAL
- en: The motivation behind this approach has many elements to it—it's about reusing
    code at large, and also achieving conceptual integrity.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we discuss the basics of packaging a Python project that can be published
    in a repository. The default repository might be PyPi (the *Python Package Index*,
    at [https://pypi.org/](https://pypi.org/)), but also could be internal; or custom
    setups will work with the same basics.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to simulate that we have created a small library, and we will use
    that as an example to review the main points to take into consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from all the open-source libraries available, sometimes we might need
    some extra functionality—perhaps our application uses a particular idiom repeatedly
    or relies on a function or mechanism quite heavily and the team has devised a
    better function for these particular needs. In order to work more effectively,
    we can place this abstraction into a library, and encourage all team members to
    use the idioms as provided by it, because doing so will help avoid mistakes and
    reduce bugs.
  prefs: []
  type: TYPE_NORMAL
- en: That's typically the case when you own a service and a client library for that
    service. You don't want clients calling your API directly, so instead, you provide
    them with a client library. The code for this library will be wrapped into a Python
    package and distributed through the internal package management systems.
  prefs: []
  type: TYPE_NORMAL
- en: Potentially, there are infinite examples that could suit this scenario. Maybe
    the application needs to extract a lot of `.tar.gz` files (in a particular format)
    and has faced security problems in the past with malicious files that ended up
    with path traversal attacks.
  prefs: []
  type: TYPE_NORMAL
- en: As a mitigation measure, the functionality for abstracting custom file formats
    securely was put in a library that wraps the default one and adds some extra checks.
    This sounds like a good idea.
  prefs: []
  type: TYPE_NORMAL
- en: Or maybe there is a configuration file that has to be written, or parsed in
    a particular format, and this requires many steps to be followed in order; again,
    creating a helper function to wrap this, and using it in all the projects that
    need it, constitutes a good investment, not only because it saves a lot of code
    repetition, but also because it makes it harder to make mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: The gain is not only complying with the DRY principle (avoiding code duplication,
    encouraging reuse) but also that the abstracted functionality represents a single
    point of reference of how things should be done, hence contributing to the attainment
    of conceptual integrity.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, the minimum layout for a library would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The important part is the `setup.py` file, which contains the definition for
    the package. In this file, all the important definitions of the project (its requirements,
    dependencies, name, description, and so on) are specified.
  prefs: []
  type: TYPE_NORMAL
- en: The `apptool` directory under `src` is the name of the library we're working
    on. This is a typical Python project, so we place here all the files we need.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of the `setup.py` file could be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This minimal example contains the key elements of the project. The `name` argument
    in the `setup` function is used to give the name that the package will have in
    the repository (under this name, we run the command to install it; in this case,
    it's `pip install apptool`). It's not strictly required that it matches the name
    of the project directory (`src/apptool`), but it's highly recommended, so it's
    easier for users.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, since both names match, it's easier to see the relationship between
    `pip install apptool` and then, in our code, `from apptool import myutil`. But
    the latter corresponds to the name under the `src/` directory and the former to
    the one specified in the `setup.py` file.
  prefs: []
  type: TYPE_NORMAL
- en: The version is important to keep different releases going on, and then the packages
    are specified. By using the `find_packages()` function, we automatically discover
    everything that's a package, in this case under the `src/` directory. Searching
    under this directory helps to avoid mixing up files beyond the scope of the project
    and, for instance, accidentally releasing tests or a broken structure of the project.
  prefs: []
  type: TYPE_NORMAL
- en: 'A package is built by running the following commands, assuming its run inside
    a virtual environment with the dependencies installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This will place the artifacts in the `dist/` directory, from where they can
    later be published either to PyPi or to the internal package repository of the
    company.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key points in packaging a Python project are:'
  prefs: []
  type: TYPE_NORMAL
- en: Test and verify that the installation is platform-independent and that it doesn't
    rely on any local setup (this can be achieved by placing the source files under
    an `src/` directory). This means that the package that is constructed should not
    depend upon files that are on your local machine and won't be available when shipped
    (nor in a custom directory structure).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure that unit tests aren't shipped as part of the package being built.
    This is meant for production. In the Docker image that will run in production,
    you don't need extra files (for example, the fixtures) that aren't strictly needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Separate dependencies—what the project strictly needs to run is not the same
    as what developers require.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's a good idea to create entry points for the commands that are going to be
    required the most.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `setup.py` file supports multiple other parameters and configurations and
    can be affected in a much more complicated manner. If our package requires several
    operating system libraries to be installed, it's a good idea to write some logic
    in the `setup.py` file to compile and build the extensions that are required.
    This way, if something is amiss, it will fail early on in the installation process,
    and if the package provides a helpful error message, the user will be able to
    fix the dependencies more quickly and continue.
  prefs: []
  type: TYPE_NORMAL
- en: Installing such dependencies represents another difficult step in making the
    application ubiquitous and easy to run by any developer regardless of their platform
    of choice. The best way to surmount this obstacle is to abstract the platform
    by creating a Docker image, as we will discuss in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Managing dependencies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before describing how we''ll leverage Docker containers to deliver our application,
    it''s important to take a look at a **Software Configuration Management** (**SCM**)
    issue, namely: how do we list the dependencies for our applications, so that they''re
    repeatable?'
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that issues in software might not only come from our code. External
    dependencies also impact the final delivery. At all times, you'd want to know
    the full list of packages and their versions that were delivered. This is called
    a baseline.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is that if at any time a dependency introduced an issue with our software,
    you''d want to be able to pinpoint it quickly. More importantly, you''d also want
    your builds to be repeatable: given everything else is unchanged, a new build
    should produce the exact same artifacts as the last one.'
  prefs: []
  type: TYPE_NORMAL
- en: The software is delivered to production by following a development pipeline.
    This starts in a first environment, then the tests run on it (integration, acceptance,
    and so on), and then through continuous integration and continuous deployment,
    it moves through the different stages of the pipeline (for example, if you have
    a beta-testing environment, or pre-production before it ultimately reaches production).
  prefs: []
  type: TYPE_NORMAL
- en: Docker is great at ensuring the exact same image is moved along the pipeline,
    but there's no guarantee that if you run the same version of the code (the same
    `git commit`, let's say) again through the pipeline, you'll get the same results.
    That work is on us, and it's what we're exploring in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say the `setup.py` file of our web package looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, there''s only one dependency (declared in the `install_requires`
    parameter), and it''s controlling a version interval. This is usually a good practice:
    we want to at least work with a specific version of a package, but we are also
    interested in not going beyond the next major version (as major versions can carry
    backward-incompatible changes).'
  prefs: []
  type: TYPE_NORMAL
- en: We set the versions like this because we're interested in getting updates for
    our dependencies (there are tools like `Dependabot`—[https://dependabot.com/](https://dependabot.com/)—that
    automatically detect when there are new releases for our dependencies and can
    open a new `pull` request), but we still want to know the exact version that was
    installed at any given time.
  prefs: []
  type: TYPE_NORMAL
- en: Not only that, but we also want to track the full tree of dependencies, meaning
    transitive dependencies should also be listed.
  prefs: []
  type: TYPE_NORMAL
- en: One way of doing that is by using pip-tools ([https://github.com/jazzband/pip-tools](https://github.com/jazzband/pip-tools))
    and compiling the `requirements.txt` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is to use this tool to generate the requirements file from the `setup.py`
    file, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This will generate a `requirements.txt` file that we are going to use in our
    `Dockerfile` to install the dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Always install the dependencies in your `Dockerfile` from the `requirements.txt`
    file, in order to have builds that are deterministic from the point of view of
    version control.
  prefs: []
  type: TYPE_NORMAL
- en: The file that lists the requirements should be placed under version control,
    and whenever we want to upgrade a dependency, we run the command again with the
    `–U` flag and track the new version of the requirements file.
  prefs: []
  type: TYPE_NORMAL
- en: Having all dependencies listed is not only good for repeatability, but it also
    adds clarity. If you are using many dependencies, it can happen that there are
    some conflicts with versions, and this will be easier to spot if we know which
    package imports which library (and on what version). But once again, this is only
    part of the problem. There are more considerations we need to take into account
    when dealing with dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Other considerations when managing dependencies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By default, when installing dependencies, `pip` will use the public repository
    from the internet ([https://pypi.org/](https://pypi.org/)). It's also possible
    to install from other indexes, or even version control systems.
  prefs: []
  type: TYPE_NORMAL
- en: This has some problems and limitations. For starters, you will depend on the
    availability of those services. There's also the caveat that you won't be able
    to publish your internal packages (which contain your company's intellectual property)
    on a public repository. And finally, there's the problem that we don't really
    know for sure how reliable or trustworthy some of the authors are in terms of
    keeping the versions of the artifacts accurate and secure (for example, some authors
    might want to republish a different version of the code with the same version
    number, something that's obviously wrong and not allowed, but all systems have
    flaws). I don't recall a particular issue like this in Python, but I do remember
    a few years ago this happened in the JavaScript community when someone deleted
    a package from the NPM registry (REGISTER01), and by unpublishing this library,
    lots of other builds broke. Even if PyPi doesn't allow this, we don't want to
    be at the mercy of someone else's good (or bad) faith.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution is simple: your company must have an internal server for dependencies,
    and all builds must target this internal repository. Regardless of how this is
    implemented (on-premises, on the cloud, by using an open-source tool, or by outsourcing
    to a provider) the idea is that new, needed dependencies have to be added to this
    repository, and this is also where the internal packages are published as well.'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure this internal repository gets updated and configure all repositories
    to receive upgrades when new versions of your dependencies are made available.
    Keep in mind that this is also another form of technical debt. There are several
    reasons for this. As we've discussed in previous chapters, technical debt is not
    just about poorly written code. When new technology is made available, you're
    missing out on those features, which means you could probably be making better
    use of the technology available. More importantly, packages might have security
    vulnerabilities that are discovered over time, so you'd want to upgrade to make
    sure your software is patched.
  prefs: []
  type: TYPE_NORMAL
- en: Having outdated versions of dependencies is another form of technical debt.
    Make the habit of using the latest versions available of your dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Don''t let too much time pass before upgrading dependencies because the more
    you wait, the harder it will be to catch up. After all, that''s the whole point
    of continuous integration: you''d want to integrate changes (including new dependencies)
    continuously, in an incremental way, provided you have automated tests that run
    as part of the build and act as a safety net for regressions.'
  prefs: []
  type: TYPE_NORMAL
- en: Configure a tool that automatically sends pull requests for new versions of
    your dependencies, and also configure automatic security checks on them.
  prefs: []
  type: TYPE_NORMAL
- en: This workflow should require minimal work. The idea is that you configure the
    `setup.py` file of your project with a range of versions and have the requirements
    file. When there are new versions available, the tool you've configured for your
    repository will rebuild the requirements file, which will list all packages and
    their new versions (which will show up in the difference of the `pull` request
    the tool opens). If the build is green, and there's nothing suspicious in the
    difference the `pull` request shows, you can go ahead and `merge`, trusting the
    continuous integration would have caught the issues. If, on the other hand, the
    build fails, that will require your intervention to adjust.
  prefs: []
  type: TYPE_NORMAL
- en: Artifact versions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There's a trade-off between stability and cutting-edge software. Having the
    latest versions is usually positive, because it means we get the latest features
    and bug fixes just by upgrading. That's when the new version doesn't bring incompatible
    changes (the downside). For that reason, software is managed in versions with
    a clear meaning.
  prefs: []
  type: TYPE_NORMAL
- en: When we establish a range of desired versions, we want to get upgrades, but
    at the same time not be too aggressive and break the application.
  prefs: []
  type: TYPE_NORMAL
- en: If we only upgrade the dependencies and write the new version of the requirements
    file, we should be publishing a new version of our artifact (after all, we're
    delivering something new, hence different). This can be a minor or micro version,
    but the important part is that we have to abide by the same rules we expect from
    third-party libraries when we're publishing our own custom artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: A good reference for this in Python is PEP-440 ([https://www.python.org/dev/peps/pep-0440/](https://www.python.org/dev/peps/pep-0440/)),
    which describes how to set the version numbers in the `setup.py` file for our
    libraries.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we take a look at a different technology that will also
    help us create components to deliver our code.
  prefs: []
  type: TYPE_NORMAL
- en: Docker containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter is dedicated to architecture, so the term container refers to something
    completely different from a Python container (an object with a `__contains__`
    method), explored in *Chapter 2*, *Pythonic Code*. A container is a process that
    runs in the operating system under a group with certain restrictions and isolation
    considerations. Concretely, we refer to `Docker` containers, which allow managing
    applications (services or processes) as independent components.
  prefs: []
  type: TYPE_NORMAL
- en: Containers represent another way of delivering software. Creating Python packages
    that take into account the considerations in the previous section is more suitable
    for libraries, or frameworks, where the goal is to reuse code and take advantage
    of using a single place where specific logic is gathered.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of containers, the objective will not be creating libraries but
    applications (most of the time). However, an application or platform does not
    necessarily mean an entire service. The idea of building containers is to create
    small components that represent a service with a small and clear purpose.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will mention Docker when we talk about containers, and we
    will explore the basics of how to create Docker images and containers for Python
    projects. Keep in mind that this is not the only technology for launching applications
    into containers, and also that it's completely independent of Python.
  prefs: []
  type: TYPE_NORMAL
- en: A Docker container needs an image to run on, and this image is created from
    other base images. But the images we create can themselves serve as base images
    for other containers. We will want to do that in cases where there is a common
    base in our application that can be shared across many containers. A potential
    use would be creating a base image that installs a package (or many) in the way
    we described in the previous section, and also all of its dependencies, including
    those at the operating system level. As discussed in *Chapter 9*, *Common Design
    Patterns*, a package we create can depend not only on other Python libraries,
    but also on a particular platform (a specific operating system), and particular
    libraries preinstalled in that operating system, without which the package will
    simply not install and will fail.
  prefs: []
  type: TYPE_NORMAL
- en: Containers are a great portability tool for this. They can help us ensure that
    our application will have a canonical way of running, and they will also ease
    the development process a lot (reproducing scenarios across environments, replicating
    tests, on-boarding new team members, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: Docker helps avoid platform-dependent issues. The idea is that we package our
    Python application as a Docker container image, and this will be useful for developing
    and testing locally, as well as for launching our software in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, in the past, Python was hard to deploy because of its nature. Since
    it''s an interpreted language, the code you write will be run by the Python virtual
    machine on the host on production. So, you need to make sure the target platform
    will have the version of the interpreter you''re expecting it to have. Moreover,
    the packaging of the dependencies was also hard: this was done by packaging everything
    into a virtual environment and running it. Things got harder if you had platform-dependent
    specifics, and some of your dependencies used C extensions. And here I''m not
    even talking about Windows or Linux; sometimes, even different versions of Linux
    (Debian-based versus Red Hat-based) had different versions of the C libraries
    needed for the code to run, so the only true way to test your application and
    make sure it''d run properly was to use a virtual machine, and compile everything
    against the right architecture. In modern applications, most of those pains should
    go away. Now you''ll have a `Dockerfile` in your root directory, with the instructions
    to build that application. And your application is delivered in production also
    by running it in Docker.'
  prefs: []
  type: TYPE_NORMAL
- en: Just as packages are the way we reuse code and unify criteria, containers represent
    the way we create the different services of the application. They meet the criteria
    behind the principle of **Separation of Concerns** (**SoC**) of the architecture.
    Each service is another kind of component that will encapsulate a set of functionalities
    independently of the rest of the application. These containers ought to be designed
    in such a way that they favor maintainability—if the responsibilities are clearly
    divided, a change in a service should not impact any other part of the application
    whatsoever.
  prefs: []
  type: TYPE_NORMAL
- en: We cover the basics of how to create a Docker container from a Python project
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Use case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As an example of how we might organize the components of our application, and
    how the previous concepts might work in practice, we present the following simple
    example.
  prefs: []
  type: TYPE_NORMAL
- en: The use case is that there is an application for delivering food, and this application
    has a specific service for tracking the status of each delivery at its different
    stages. We are going to focus only on this particular service, regardless of how
    the rest of the application might appear. The service has to be really simple—a
    REST API that, when asked about the status of a particular order, will return
    a JSON response with a descriptive message.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to assume that the information about each particular order is stored
    in a database, but this detail should not matter at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our service has two main concerns for now: getting the information about a
    particular order (from wherever this might be stored), and presenting this information
    in a useful way to the clients (in this case, delivering the results in JSON format,
    exposed as a web service).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As the application has to be maintainable and extensible, we want to keep these
    two concerns as hidden as possible and focus on the main logic. Therefore, these
    two details are abstracted and encapsulated into Python packages that the main
    application with the core logic will use, as shown in *Figure 10.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16567_10_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: A service application (named "Web service") that makes use of
    two Python packages, one of which connects to a database.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we briefly demonstrate how the code might appear,
    in terms of the packages mainly, and how to create services from these, in order
    to finally see what conclusions we can infer.
  prefs: []
  type: TYPE_NORMAL
- en: The code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The idea of creating Python packages in this example is to illustrate how abstracted
    and isolated components can be made, in order to work effectively. In reality,
    there is no actual need for these to be Python packages; we could just create
    the right abstractions as part of the "delivery service" project, and, while the
    correct isolation is preserved, it will work without any issues.
  prefs: []
  type: TYPE_NORMAL
- en: Creating packages makes more sense when there is logic that is going to be repeated
    and is expected to be used across many other applications (that will import from
    those packages) because we want to favor code reuse. In this particular case,
    there are no such requirements, so it might be beyond the scope of the design,
    but such distinction still makes clearer the idea of a "pluggable architecture"
    or component, something that is really a wrapper abstracting technical details
    we don't really want to deal with, much less depend upon.
  prefs: []
  type: TYPE_NORMAL
- en: The `storage` package is in charge of retrieving the data that is required and
    presenting this to the next layer (the delivery service) in a convenient format,
    something that is suitable for the business rules. The main application should
    now know where this data came from, what its format is, and so on. This is the
    entire reason why we have such an abstraction in between, so the application doesn't
    use a row or an ORM entity directly, but rather something workable.
  prefs: []
  type: TYPE_NORMAL
- en: Domain models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following definitions apply to classes for business rules. Notice that they
    are meant to be pure business objects, not bound to anything in particular. They
    aren't models of an ORM, or objects of an external framework, and so on. The application
    should work with these objects (or objects with the same criteria).
  prefs: []
  type: TYPE_NORMAL
- en: 'In each case, the docstring documents the purpose of each class, according
    to the business rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: From this code, we can already get an idea of what the application will look
    like—we want to have a `DeliveryOrder` object, which will have its own status
    (as an internal collaborator), and once we have that, we will call its `message()`
    method to return this information to the user.
  prefs: []
  type: TYPE_NORMAL
- en: Calling from the application
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is how these objects are going to be used in the application. Notice how
    this depends on the previous packages (`web` and `storage`), but not the other
    way round:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the previous section, the `domain` objects were shown and here the code for
    the application is displayed. Aren't we missing something? Sure, but is it something
    we really need to know now? Not necessarily.
  prefs: []
  type: TYPE_NORMAL
- en: The code inside the `storage` and `web` packages was deliberately left out (although
    the reader is more than encouraged to look at it—the repository for the book contains
    the full example). Also, and this was done on purpose, the names of such packages
    were chosen so as not to reveal any technical details—`storage` and `web`.
  prefs: []
  type: TYPE_NORMAL
- en: Look again at the code in the previous listing. Can you tell which frameworks
    are being used? Does it say whether the data comes from a text file, a database
    (if so, of what type? SQL? NoSQL?), or another service (the web, for instance)?
    Assume that it comes from a relational database. Is there any clue as to how this
    information is retrieved (manual SQL queries? Through an ORM?)?
  prefs: []
  type: TYPE_NORMAL
- en: What about the web? Can we guess what frameworks are used?
  prefs: []
  type: TYPE_NORMAL
- en: The fact that we cannot answer any of these questions is probably a good sign.
    Those are details, and details ought to be encapsulated. We can't answer these
    questions unless we take a look at what's inside those packages.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is another way of answering the previous questions, and it comes in the
    form of a question itself: why do we need to know that? Looking at the code, we
    can see that there is a `DeliveryOrder`, created with an identifier of a delivery,
    and that it has a `get()` method, which returns an object representing the status
    of the delivery. If all of this information is correct, that''s all we should
    care about. What difference does it make how it is done?'
  prefs: []
  type: TYPE_NORMAL
- en: The abstractions we created make our code declarative. In declarative programming,
    we declare the problem we want to solve, not how we want to solve it. It's the
    opposite of imperative, in which we have to make all the steps required explicit
    in order to get something (for instance, connect to the database, run this query,
    parse the result, load it into this object, and so on). In this case, we are declaring
    that we just want to know the status of the delivery given by some identifier.
  prefs: []
  type: TYPE_NORMAL
- en: These packages are in charge of dealing with the details and presenting what
    the application needs in a convenient format, namely objects of the kind presented
    in the previous section. We just have to know that the `storage` package contains
    an object that, given an ID for a delivery and a storage client (this dependency
    is being injected into this example for simplicity, but other alternatives are
    also possible), it will retrieve `DeliveryOrder`, which we can then ask to compose
    the message.
  prefs: []
  type: TYPE_NORMAL
- en: This architecture provides convenience and makes it easier to adapt to changes,
    as it protects the kernel of the business logic from the external factors that
    can change.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine we want to change how the information is retrieved. How hard would
    that be? The application relies on an API, like the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: So, it would just be about changing how the `get()` method works, adapting it
    to the new implementation detail. All we need is for this new object to return
    `DeliveryOrder` on its `get()` method and that would be all. We can change the
    query, the ORM, the database, and so on, and, in all cases, the code in the application
    does not need to change!
  prefs: []
  type: TYPE_NORMAL
- en: Adapters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Still, without looking at the code in the packages, we can conclude that they
    work as interfaces for the technical details of the application.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, since we are seeing the application from a high-level perspective,
    without needing to look at the code, we can imagine that inside those packages
    there must be an implementation of the adapter design pattern (introduced in *Chapter
    9*, *Common Design Patterns*). One or more of these objects is adapting an external
    implementation to the API defined by the application. This way, dependencies that
    want to work with the application must conform to the API, and an adapter will
    have to be made.
  prefs: []
  type: TYPE_NORMAL
- en: There is one clue pertaining to this adapter in the code for the application
    though. Notice how the view is constructed. It inherits from a class named `View`
    that comes from our `web` package. We can deduce that this `View` is, in turn,
    a class derived from one of the web frameworks that might be being used, creating
    an adapter by inheritance. The important thing to note is that once this is done,
    the only object that matters is our `View` class, because, in a way, we are creating
    our own framework, which is based on adapting an existing one (but again, changing
    the framework will mean just changing the adapters, not the entire application).
  prefs: []
  type: TYPE_NORMAL
- en: Starting from the next section, we'll take a look at what the services look
    like internally.
  prefs: []
  type: TYPE_NORMAL
- en: The services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To create the service, we are going to launch the Python application inside
    a Docker container. Starting from a base image, the container will have to install
    the dependencies for the application to run, which also has dependencies at the
    operating system level.
  prefs: []
  type: TYPE_NORMAL
- en: This is actually a choice because it depends on how the dependencies are used.
    If a package we use requires other libraries on the operating system to compile
    at installation time, we can avoid this simply by building a wheel for our platform
    of the library and installing this directly. If the libraries are needed at runtime,
    then there is no choice but to make them part of the image of the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will discuss one of the many ways of preparing a Python application
    to be run inside a Docker container. This is one of the numerous alternatives
    for packaging a Python project into a container. First, we take a look at what
    the structure of the directories looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `libs` directory can be ignored since it's just the place where the dependencies
    are placed (it's displayed here to keep them in mind when they are referenced
    in the `setup.py` file, but they could be placed in a different repository and
    installed remotely via `pip`).
  prefs: []
  type: TYPE_NORMAL
- en: We have `Makefile` with some helper commands, then the `setup.py` file, and
    the application itself inside the `statusweb` directory. A common difference between
    packaging applications and libraries is that while the latter specify their dependencies
    in the `setup.py` file, the former have a `requirements.txt` file from where dependencies
    are installed via `pip install -r requirements.txt`. Normally, we would do this
    in the `Dockerfile`, but in order to keep things simpler, in this particular example,
    we will assume that taking the dependencies from the `setup.py` file is enough.
    This is because besides this consideration, there are a lot more considerations
    to be taken into account when dealing with dependencies, such as freezing the
    version of the packages, tracking indirect dependencies, using extra tools such
    as `pipenv`, and more topics that are beyond the scope of the chapter. In addition,
    it is also customary to make the `setup.py` file read from `requirements.txt`
    for consistency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have the content of the `setup.py` file, which states some details of
    the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The first thing we notice is that the application declares its dependencies,
    which are the packages we created and placed under `libs/`, namely `web` and `storage`,
    abstracting and adapting to some external components. These packages, in turn,
    will have dependencies, so we will have to make sure the container installs all
    the required libraries when the image is being created so that they can install
    successfully, and then this package afterward.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second thing we notice is the definition of the `entry_points` keyword
    argument passed to the `setup` function. This is not strictly mandatory, but it''s
    a good idea to create an entry point. When the package is installed in a virtual
    environment, it shares this directory along with all its dependencies. A virtual
    environment is a structure of directories with the dependencies of a given project.
    It has many subdirectories, but the most important ones are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<virtual-env-root>/lib/<python-version>/site-packages`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<virtual-env-root>/bin`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first one contains all the libraries installed in that virtual environment.
    If we were to create a virtual environment with this project, that directory would
    contain the `web` and `storage` packages, along with all its dependencies, plus
    some extra basic ones and the current project itself.
  prefs: []
  type: TYPE_NORMAL
- en: The second, `/bin/`, contains the binary files and commands available when that
    virtual environment is active. By default, it would just be the version of Python,
    `pip`, and some other basic commands. When we create a console entry point, a
    binary with that declared name is placed there, and, as a result, we have that
    command available to run when the environment is active. When this command is
    called, it will run the function that is specified with all the context of the
    virtual environment. That means it is a binary we can call directly without having
    to worry about whether the virtual environment is active, or whether the dependencies
    are installed in the path that is currently running.
  prefs: []
  type: TYPE_NORMAL
- en: 'The definition is the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The left-hand side of the equals sign declares the name of the entry point.
    In this case, we will have a command named `status-service` available. The right-hand
    side declares how that command should be run. It requires the package where the
    function is defined, followed by the function name after :`.` In this case, it
    will run the `main` function declared in `statusweb/service.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is followed by a definition of the `Dockerfile`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The image is built based on a lightweight Linux image with Python installed,
    and then the operating system dependencies are installed so that our libraries
    can be deployed. Following the previous consideration, this `Dockerfile` simply
    copies the libraries, but this might as well be installed from a `requirements.txt`
    file accordingly. After all the `pip install` commands are ready, it copies the
    application in the working directory, and the entry point from Docker (the `CMD`
    command, not to be confused with the Python one) calls the entry point of the
    package where we placed the function that launches the process. For local development,
    we'd still use the `Dockerfile`, in conjunction with a `docker-compose.yml` file
    with the definitions of all the services (including dependencies such as databases),
    base images, and how they are linked and interconnected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the container running, we can launch it and run a small test
    on it to get an idea of how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Let's analyze the architectural traits for the code we've seen so far, starting
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many conclusions to be drawn from the previous implementation. While
    it might seem like a good approach, there are cons that come with the benefits;
    after all, no architecture or implementation is perfect. This means that a solution
    such as this one cannot be good for all cases, so it will pretty much depend on
    the circumstances of the project, the team, the organization, and more.
  prefs: []
  type: TYPE_NORMAL
- en: While it's true that the main idea of the solution is to abstract details as
    much as possible, as we shall see, some parts cannot be fully abstracted away,
    and also the contracts between the layers imply an abstraction leak.
  prefs: []
  type: TYPE_NORMAL
- en: After all, technology always creeps in. For example, if we were to change our
    implementation from a REST service to serve our data through GraphQL, we would
    have to adapt how the application server is configured and built, but still, we
    should be able to have a structure very similar to the preceding one. Even if
    we want to make a more radical change so as to transform our service into a gRPC
    server, we would of course be forced to adapt some glue code, but we should still
    be able to use our packages as much as possible. The changes needed should be
    kept to a minimum.
  prefs: []
  type: TYPE_NORMAL
- en: The dependency flow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Notice that dependencies flow in only one direction, as they move closer to
    the kernel, where the business rules lie. This can be traced by looking at the
    `import` statements. The application imports everything it needs from storage,
    for example, and in no part is this inverted.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking this rule would create coupling. The way the code is arranged now means
    that there is a weak dependency between the application and storage. The API is
    such that we need an object with a `get()` method, and any storage that wants
    to connect to the application needs to implement this object according to this
    specification. The dependencies are therefore inverted—it's up to every storage
    to implement this interface, in order to create an object according to what the
    application is expecting.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Not everything can be abstracted away. In some cases, it's simply not possible,
    and in others, it might not be convenient. Let's start with the convenience aspect.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, there is an adapter of the web framework of choice to a clean
    API to be presented to the application. In a more complex scenario, such a change
    might not be possible. Even with this abstraction, parts of the library are still
    visible to the application. It's not entirely a problem to be completely isolated
    from the web framework because, sooner or later, we will need some of its features
    or technical details.
  prefs: []
  type: TYPE_NORMAL
- en: The important takeaway here is not the adapter, but the idea of hiding technical
    details as much as possible. That means that the best thing that is displayed
    on the listing for the code of the application is not the fact that there is an
    adapter between our version of the web framework and the actual one, but instead,
    the fact that the latter is not mentioned by name in any part of the visible code.
    The service has made clear that `web` is just a dependency (a detail being imported)
    and revealed the intention behind what it was supposed to do. The goal is to reveal
    the intention (as in the code) and to defer details as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: As to what things cannot be isolated, those are the elements that are closest
    to the code. In this case, the web application is using the objects operating
    within them in an asynchronous fashion. That is a hard constraint we cannot circumvent.
    It's true that whatever is inside the `storage` package can be changed, refactored,
    and modified, but whatever these modifications might be, it still needs to preserve
    the interface, and that includes the asynchronous interface.
  prefs: []
  type: TYPE_NORMAL
- en: Testability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Again, much like with the code, the architecture can benefit from separating
    pieces into smaller components. The fact that dependencies are now isolated and
    controlled by separate components leaves us with a cleaner design for the main
    application, and now it's easier to ignore the boundaries to focus on testing
    the core of the application.
  prefs: []
  type: TYPE_NORMAL
- en: We could create a patch for the dependencies and write unit tests that are simpler
    (they won't need a database), or launch an entire web service, for instance. Working
    with pure `domain` objects means it will be easier to understand the code and
    the unit tests. Even the adapters will not need that much testing because their
    logic should be very simple.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep in mind the software testing pyramid mentioned in *Chapter 8*, *Unit Testing
    and Refactoring*. We want to have a large number of unit tests, followed by fewer
    component tests, and finally even fewer integration tests. Separating our architecture
    into different components goes a long way for component testing: we can mock up
    our dependencies and test some components in isolation.'
  prefs: []
  type: TYPE_NORMAL
- en: This is both cheaper and faster, but it doesn't mean that we shouldn't have
    integration tests at all. To make sure our final application works as expected,
    we need integration tests that will exercise all components of our architecture
    (be that microservices or packages), working together.
  prefs: []
  type: TYPE_NORMAL
- en: Intention revealing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Intention revealing is a critical concept for our code—every name has to be
    wisely chosen, clearly communicating what it's supposed to do. Every function
    should tell a story. We should keep functions short, concerns separated, and dependencies
    isolated and assign the right meaning to abstractions in every part of the code.
  prefs: []
  type: TYPE_NORMAL
- en: Good architecture should reveal the intent of the system it entails. It should
    not mention the tools it's built with; those are details, and as we discussed
    at length, details should be hidden and encapsulated.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The principles of good software design apply to all levels. In the same way
    that we want to write readable code, and for that we need to keep in mind the
    intention-revealing aspects of the code, the architecture also has to express
    the intent of the problem it is trying to solve.
  prefs: []
  type: TYPE_NORMAL
- en: All these ideas are interconnected. The same intention revealing that ensures
    our architecture is defined in terms of the domain problem also leads us to abstract
    details as much as possible, create layers of abstraction, invert dependencies,
    and separate concerns.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to reusing code, Python packages are a great and flexible option.
    Criteria such as cohesion and the single responsibility principle are the most
    important considerations when deciding to create a package. In line with having
    components with cohesion and few responsibilities, the concept of microservices
    comes into play, and for that, we have seen how a service can be deployed in a Docker
    container starting from a packaged Python application.
  prefs: []
  type: TYPE_NORMAL
- en: As with everything in software engineering, there are limitations and there
    are exceptions. It will not always be possible to abstract things as much as we
    would like to or to completely isolate dependencies. Sometimes, it will just not
    be possible (or practical) to comply with the principles explained here in the
    book. But that is probably the best piece of advice the reader should take from
    the book—they are just principles, not laws. If it's not possible, or practical,
    to abstract from a framework, it should not be a problem. Remember what has been
    quoted from the *Zen of Python* itself throughout the book—*practicality beats
    purity*.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is a list of information you can refer to:'
  prefs: []
  type: TYPE_NORMAL
- en: '*SCREAM*: *Screaming Architecture* ([https://8thlight.com/blog/uncle-bob/2011/09/30/Screaming-Architecture.html](https://8thlight.com/blog/uncle-bob/2011/09/30/Screaming-Architecture.html))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*CLEAN-01*: *The Clean Architecture* ([https://8thlight.com/blog/uncle-bob/2012/08/13/the-clean-architecture.html](https://8thlight.com/blog/uncle-bob/2012/08/13/the-clean-architecture.html))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*HEX*: *Hexagonal Architecture* ([https://staging.cockburn.us/hexagonal-architecture/](https://staging.cockburn.us/hexagonal-architecture/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*PEP-508*: Dependency specification for Python software packages ([https://www.python.org/dev/peps/pep-0508/](https://www.python.org/dev/peps/pep-0508/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Packaging and distributing projects in Python: [https://python-packaging-user-guide.readthedocs.io/guides/distributing-packages-using-setuptools/#distributing-packages](https://python-packaging-user-guide.readthedocs.io/guides/distributing-packages-using-setuptools/#di)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*PEP-440*: [https://www.python.org/dev/peps/pep-0440/](https://www.python.org/dev/peps/pep-0440/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*REGISTER01*: [https://www.theregister.com/2016/03/23/npm_left_pad_chaos/](https://www.theregister.com/2016/03/23/npm_left_pad_chaos/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Python packaging user guide: [https://packaging.python.org/](https://packaging.python.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AWS builder''s library: *Going faster with continuous delivery* ([https://aws.amazon.com/builders-library/going-faster-with-continuous-delivery/](https://aws.amazon.com/builders-library/going-faster-with-continuous-delivery/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summing it all up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The content of this book is a reference, a possible way of implementing a software
    solution by following the mentioned criteria. These criteria are explained through
    examples, and the rationale for every decision is presented. The reader might
    very well disagree with the approaches taken in the examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, I encourage you to disagree: the more viewpoints there are, the richer
    the debate. But regardless of opinions, it''s important to make clear that what
    is presented here is by no means a strong directive, something that must be followed
    imperatively. Quite the opposite; it''s a way of presenting a solution and a set
    of ideas that you might find helpful.'
  prefs: []
  type: TYPE_NORMAL
- en: As introduced at the beginning, the goal of this book was not to give you recipes
    or formulas that you can apply directly, but rather to develop your critical thinking.
    Idioms and syntax features come and go; they change over time. But ideas and core
    software concepts remain. With these tools and the examples provided, you should
    have a better understanding of what clean code means.
  prefs: []
  type: TYPE_NORMAL
- en: I sincerely hope this book has helped you become a better developer than you
    were before you started it, and I wish you much success in your projects.
  prefs: []
  type: TYPE_NORMAL
- en: '**Share your experience**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thank you for taking the time to read this book. If you enjoyed this book,
    help others to find it. Leave a review at: [https://www.amazon.com/dp/1800560214](https://www.amazon.com/dp/1800560214)'
  prefs: []
  type: TYPE_NORMAL
