- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency is the art of making a computer do (or appear to do) multiple things
    at once. Historically, this meant inviting the processor to switch between different
    tasks many times per second. In modern systems, it can also literally mean doing
    two or more things simultaneously on separate processor cores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Concurrency is not inherently an object-oriented topic, but Python''s concurrent
    systems provide object-oriented interfaces, as we''ve covered throughout the book.
    This chapter will introduce you to the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Futures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AsyncIO
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dining philosophers benchmark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The case study for this chapter will address ways we can speed up model testing
    and hyperparameter tuning. We can't make the computation go away, but we can leverage
    a modern, multi-core computer to get it done in less time.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent processes can become complicated. The basic concepts are fairly simple,
    but the bugs that can occur are notoriously difficult to track down when the sequence
    of state changes is unpredictable. However, for many projects, concurrency is
    the only way to get the performance we need. Imagine if a web server couldn't
    respond to a user's request until another user's request had been completed! We'll
    see how to implement concurrency in Python, and some common pitfalls to avoid.
  prefs: []
  type: TYPE_NORMAL
- en: The Python language explicitly executes statements in order. To consider concurrent
    execution of statements, we'll need to take a step away from Python.
  prefs: []
  type: TYPE_NORMAL
- en: Background on concurrent processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Conceptually, it can help to think of concurrent processing by imagining a group
    of people who can't see each other and are trying to collaborate on a task. Perhaps
    their vision is impaired or blocked by screens, or their workspace has awkward
    doorways they can't quite see through. These people can, however, pass tokens,
    notes, and work-in-process to each other.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a small delicatessen in an old seaside resort city (on the Atlantic
    coast of the US) with an awkward countertop layout. The two sandwich chefs can't
    see or hear each other. While the owner can afford to pay two fine chefs, the
    owner can't afford more than one serving tray. Due to the awkward complications
    of the ancient building, the chefs can't really see the tray, either. They're
    forced to reach down below their counter to be sure the serving tray is in place.
    Then, assured the tray is there, they carefully place their work of art – complete
    with pickles and a few potato chips – onto the tray. (They can't see the tray,
    but they're spectacular chefs who can place a sandwich, pickles, and chips flawlessly.)
  prefs: []
  type: TYPE_NORMAL
- en: The owner, however, can see the chefs. Indeed, passers-by can watch the chefs
    work. It's a great show. The owner typically deals the order tickets out to each
    chef in strict alternation. And ordinarily, the one and only serving tray can
    be placed so the sandwich arrives, and is presented at the table with a flourish.
    The chefs, as we said, have to wait to feel the tray before their next creation
    warms someone's palate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then one day, one of the chefs (we''ll call him Michael, but his friends call
    him Mo) is nearly done with an order, but has to run to the cooler for more of
    those dill pickles everyone loves. This delays Mo''s prep time, and the owner
    sees that the other chef, Constantine, looks like he''ll finish just a fraction
    of a second before Mo. Even though Mo has returned with the pickles, and is ready
    with the sandwich, the owner does something embarrassing. The rule is clear: check
    first, then place the sandwich. Everyone in the shop knows this. When the owner
    moves the tray from the opening below Mo''s station to the opening below Constantine''s,
    then Mo placed their creation – what would have been a delightful Reuben sandwich
    with extra sauerkraut – into the empty space where a tray should have been, where
    it splashes onto the delicatessen floor, embarrassing everyone.'
  prefs: []
  type: TYPE_NORMAL
- en: How could the foolproof method of checking for the tray, then depositing the
    sandwich have failed to work? It had survived the test of many busy lunch hours,
    and yet, a small disruption in the regular sequence of events, and a mess ensues.
    The separation in time between testing for the tray and depositing the sandwich
    is an opportunity for the owner to make a state change.
  prefs: []
  type: TYPE_NORMAL
- en: There's a race between owner and chefs. Preventing unexpected state changes
    is the essential design problem for concurrent programming.
  prefs: []
  type: TYPE_NORMAL
- en: One solution could be to use a semaphore – a flag – to prevent unexpected changes
    to the tray. This is a kind of shared lock. Each chef is forced to seize the flag
    before plating; and once they have the flag, they can be confident the owner won't
    move the tray until they return the flag to the little flag-stand between the
    chef stations.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent work requires some method for synchronizing access to shared resources.
    One essential power of large, modern computers is managing concurrency through
    operating system features, collectively called the kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Older and smaller computers, with a single core in a single CPU, had to interleave
    everything. The clever coordination made things appear to be working at the same
    time. Newer multi-core computers (and large multi-processor computers) can actually
    perform operations concurrently, making the scheduling of work a bit more involved.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have several ways to achieve concurrent processing:'
  prefs: []
  type: TYPE_NORMAL
- en: The operating system lets us run more than one program at a time. The Python
    `subprocess` module gives us ready access to these capabilities. The `multiprocessing`
    module provides a number of convenient ways to work. This is relatively easy to
    start, but each program is carefully sequestered from all other programs. How
    can they share data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some clever software libraries allow a single program to have multiple concurrent
    threads of operation. The Python `threading` module gives us access to multi-threading.
    This is more complex to get started, and each thread has complete access to the
    data in all other threads. How can we coordinate updates to shared data structures?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, `concurrent.futures` and `asyncio` provide easier-to-use wrappers
    around the underlying libraries. We'll start this chapter by looking at Python's
    use of the `threading` library to allow many things to happen concurrently in
    a single OS process. This is simple, but has some challenges when working with
    shared data structures.
  prefs: []
  type: TYPE_NORMAL
- en: Threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A thread is a sequence of Python byte-code instructions that may be interrupted
    and resumed. The idea is to create separate, concurrent threads to allow computation
    to proceed while the program is waiting for I/O to happen.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a server can start processing a new network request while it waits
    for data from a previous request to arrive. Or an interactive program might render
    an animation or perform a calculation while waiting for the user to press a key.
    Bear in mind that while a person can type more than 500 characters per minute,
    a computer can perform billions of instructions per second. Thus, a ton of processing
    can happen between individual key presses, even when typing quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s theoretically possible to manage all this switching between activities
    within your program, but it would be virtually impossible to get right. Instead,
    we can rely on Python and the operating system to take care of the tricky switching
    part, while we create objects that appear to be running independently but simultaneously.
    These objects are called **threads**. Let''s take a look at a basic example. We''ll
    start with the essential definition of the thread''s processing, as shown in the
    following class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'A thread in our running application must extend the `Thread` class and implement
    the `run` method. Any code executed by the `run` method is a separate thread of
    processing, scheduled independently. Our thread is relying on a global variable,
    `THE_ORDERS`, which is a shared object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we''ve defined the orders as a simple, fixed list of values.
    In a larger application, we might be reading these from a socket or a queue object.
    Here''s the top-level program that starts things running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This will create two threads. The new threads don't start running until we call
    the `start()` method on the object. When the two threads have started, they both
    pop a value from the list of orders and then commence to perform a large computation
    and – eventually – report their status.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that the sandwiches aren't completed in the exact order that they were
    presented in the `THE_ORDERS` list. Each chef works at their own (randomized)
    pace. Changing the seed will change the times, and may adjust the order slightly.
  prefs: []
  type: TYPE_NORMAL
- en: What's important about this example is the threads are sharing data structures,
    and the concurrency is an illusion created by clever scheduling of the threads
    to interleave work from the two chef threads.
  prefs: []
  type: TYPE_NORMAL
- en: The only update to a shared data structure in this small example is to pop from
    a list. If we were to create our own class and implement more complex state changes,
    we could uncover a number of interesting and confusing issues with using threads.
  prefs: []
  type: TYPE_NORMAL
- en: The many problems with threads
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Threads can be useful if appropriate care is taken to manage shared memory,
    but modern Python programmers tend to avoid them for several reasons. As we'll
    see, there are other ways to code concurrent programming that are receiving more
    attention from the Python community. Let's discuss some of the pitfalls before
    moving on to alternatives to multithreaded applications.
  prefs: []
  type: TYPE_NORMAL
- en: Shared memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The main problem with threads is also their primary advantage. Threads have
    access to all the process memory and thus all the variables. A disregard for the
    shared state can too easily cause inconsistencies.
  prefs: []
  type: TYPE_NORMAL
- en: Have you ever encountered a room where a single light has two switches and two
    different people turn them on at the same time? Each person (thread) expects their
    action to turn the lamp (a variable) on, but the resulting value (the lamp) is
    off, which is inconsistent with those expectations. Now imagine if those two threads
    were transferring funds between bank accounts or managing the cruise control for
    a vehicle.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this problem in threaded programming is to *synchronize* access
    to any code that reads or (especially) writes a shared variable. Python's `threading`
    library offers the `Lock` class, which can be used via the `with` statement to
    create a context where a single thread has access to update shared objects.
  prefs: []
  type: TYPE_NORMAL
- en: The synchronization solution works in general, but it is way too easy to forget
    to apply it to shared data in a specific application. Worse, bugs due to inappropriate
    use of synchronization are really hard to track down because the order in which
    threads perform operations is inconsistent. We can't easily reproduce the error.
    Usually, it is safest to force communication between threads to happen using a
    lightweight data structure that already uses locks appropriately. Python offers
    the `queue.Queue` class to do this; a number of threads can write to a queue,
    where a single thread consumes the results. This gives us a tidy, reusable, proven
    technique for having multiple threads sharing a data structure. The `multiprocessing.Queue`
    class is nearly identical; we will discuss this in the *Multiprocessing* section
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In some cases, these disadvantages might be outweighed by the one advantage
    of allowing shared memory: it''s fast. If multiple threads need access to a huge
    data structure, shared memory can provide that access quickly. However, this advantage
    is usually nullified by the fact that, in Python, it is impossible for two threads
    running on different CPU cores to be performing calculations at exactly the same
    time. This brings us to our second problem with threads.'
  prefs: []
  type: TYPE_NORMAL
- en: The global interpreter lock
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to efficiently manage memory, garbage collection, and calls to machine
    code in native libraries, Python has a **global interpreter lock**, or **GIL**.
    It's impossible to turn off, and it means that thread scheduling is constrained
    by the GIL preventing any two threads from doing computations at the exact same
    time; the work is interleaved artificially. When a thread makes an OS request
    – for example, to access the disk or network – the GIL is released as soon as
    the thread starts waiting for the OS request to complete.
  prefs: []
  type: TYPE_NORMAL
- en: The GIL is disparaged, mostly by people who don't understand what it is or the
    benefits it brings to Python. While it can interfere with multithreaded compute-intensive
    programming, the impact for other kinds of workloads is often minimal. When confronted
    with a compute-intensive algorithm, it may help to switch to using the `dask`
    package to manage the processing. See [https://dask.org](https://dask.org) for
    more information on this alternative. The book *Scalable Data Analysis in Python
    with Dask* can be informative, also.
  prefs: []
  type: TYPE_NORMAL
- en: While the GIL can be a problem in the reference implementation of Python that
    most people use, it can be selectively disabled in IronPython. See *The IronPython
    Cookbook* for details on how to release the GIL for compute-intensive processing
    in IronPython.
  prefs: []
  type: TYPE_NORMAL
- en: Thread overhead
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One additional limitation of threads, as compared to other asynchronous approaches
    we will be discussing later, is the cost of maintaining each thread. Each thread
    takes up a certain amount of memory (both in the Python process and the operating
    system kernel) to record the state of that thread. Switching between the threads
    also uses a (small) amount of CPU time. This work happens seamlessly without any
    extra coding (we just have to call `start()` and the rest is taken care of), but
    the work still has to happen somewhere.
  prefs: []
  type: TYPE_NORMAL
- en: These costs can be amortized over a larger workload by reusing threads to perform
    multiple jobs. Python provides a `ThreadPool` feature to handle this. It behaves
    identically to `ProcessPool`, which we will discuss shortly, so let's defer that
    discussion until later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll look at the principal alternative to multi-threading.
    The `multiprocessing` module lets us work with OS-level subprocesses.
  prefs: []
  type: TYPE_NORMAL
- en: Multiprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Threads exist within a single OS process; that's why they can share access to
    common objects. We can do concurrent computing at the process level, also. Unlike
    threads, separate processes cannot directly access variables set up by other processes.
    This independence is helpful because each process has its own GIL and its own
    private pool of resources. On a modern multi-core processor, a process may have
    its own core, permitting concurrent work with other cores.
  prefs: []
  type: TYPE_NORMAL
- en: The `multiprocessing` API was originally designed to mimic the `threading` API.
    However, the `multiprocessing` interface has evolved, and in recent versions of
    Python, it supports more features more robustly. The `multiprocessing` library
    is designed for when CPU-intensive jobs need to happen in parallel and multiple
    cores are available. Multiprocessing is not as useful when the processes spend
    a majority of their time waiting on I/O (for example, network, disk, database,
    or keyboard), but it is the way to go for parallel computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `multiprocessing` module spins up new operating system processes to do
    the work. This means there is an entirely separate copy of the Python interpreter
    running for each process. Let''s try to parallelize a compute-heavy operation
    using similar constructs to those provided by the `threading` API, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This example just ties up the CPU computing the sum of 100 million odd numbers.
    You may not consider this to be useful work, but it can warm up your laptop on
    a chilly day!
  prefs: []
  type: TYPE_NORMAL
- en: The API should be familiar; we implement a subclass of `Process` (instead of `Thread`)
    and implement a `run` method. This method prints out the OS **process ID** (**PID**),
    a unique number assigned to each process on the machine, before doing some intense
    (if misguided) work.
  prefs: []
  type: TYPE_NORMAL
- en: Pay special attention to the `if __name__ == "__main__":` guard around the module-level
    code that prevents it from running if the module is being imported, rather than
    run as a program. This is good practice in general, but when using the `multiprocessing`
    module, it is essential. Behind the scenes, the `multiprocessing` module may have
    to reimport our application module inside each of the new processes in order to
    create the class and execute the `run()` method. If we allowed the entire module
    to execute at that point, it would start creating new processes recursively until
    the operating system ran out of resources, crashing your computer.
  prefs: []
  type: TYPE_NORMAL
- en: 'This demo constructs one process for each processor core on our machine, then
    starts and joins each of those processes. On a 2020-era MacBook Pro with a 2 GHz
    Quad-Core Intel Core i5, the output looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The first eight lines are the process ID that was printed inside each `MuchCPU` instance.
    The last line shows that the 100 million summations can run in about 20 seconds.
    During those 20 seconds, all eight cores were running at 100 percent, and the
    fans were buzzing away trying to dissipate the heat.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we subclass `threading.Thread` instead of `multiprocessing.Process` in `MuchCPU`,
    the output looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This time, the threads are running inside the same OS process and take over
    three times as long to run. The display showed that no core was particularly busy,
    suggesting the work was being shunted around among the various cores. The general
    slowdown is the cost of the GIL interleaving compute-intensive work.
  prefs: []
  type: TYPE_NORMAL
- en: We might expect the single process version to be at least eight times as long
    as the eight-process version. The lack of a simple multiplier suggests there are
    a number of factors involved in how the low-level instructions are processed by
    Python, the OS schedulers, and even the hardware itself. This suggests that predictions
    are difficult, and it's best to plan on running multiple performance tests with
    multiple software architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Starting and stopping individual `Process` instances involves a lot of overhead.
    The most common use case is to have a pool of workers and assign tasks to them.
    We'll look at this next.
  prefs: []
  type: TYPE_NORMAL
- en: Multiprocessing pools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because each process is kept meticulously separate by the operating system,
    interprocess communication becomes an important consideration. We need to pass
    data between these separate processes. One really common example is having one
    process write a file that another process can read. When the two processes are
    reading and writing a file, and running concurrently, we have to be sure the reader
    is waiting for the writer to produce data. The operating system *pipe* structure
    can accomplish this. Within the shell, we can write `ps -ef | grep python` and
    pass output from the `ps` command to the `grep` command. The two commands run
    concurrently. For Windows PowerShell users, there are similar kinds of pipeline
    processing, using different command names. (See [https://docs.microsoft.com/en-us/powershell/scripting/learn/ps101/04-pipelines?view=powershell-7.1](https://docs.microsoft.com/en-us/powershell/scripting/learn/ps101/04-pipelines?view=powershell-7.1)
    for examples.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The `multiprocessing` package provides some additional ways to implement interprocess
    communication. Pools can seamlessly hide the way data is moved between processes.
    Using a pool looks much like a function call: you pass data into a function, it
    is executed in another process or processes, and when the work is done, a value
    is returned. It is important to understand how much work is being done to support
    this: objects in one process are pickled and passed into an operating system process
    pipe. Then, another process retrieves data from the pipe and unpickles it. The
    requested work is done in the subprocess and a result is produced. The result
    is pickled and passed back through the pipe. Eventually, the original process
    unpickles and returns it. Collectively, we call this pickling, transferring, and
    unpickling *serializing* the data. For more information, see *Chapter 9*, *Strings,
    Serialization, and File Paths*.'
  prefs: []
  type: TYPE_NORMAL
- en: The serialization to communicate between processes takes time and memory. We
    want to get as much useful computation done with the smallest serialization cost.
    The ideal mix depends on the size and complexity of the objects being exchanged,
    meaning that different data structure designs will have different performance
    levels.
  prefs: []
  type: TYPE_NORMAL
- en: Performance predictions are difficult to make. It's essential to profile the
    application to ensure the concurrency design is effective.
  prefs: []
  type: TYPE_NORMAL
- en: Armed with this knowledge, the code to make all this machinery work is surprisingly
    simple. Let's look at the problem of calculating all the prime factors of a list
    of random numbers. This is a common part of a variety of cryptography algorithms
    (not to mention attacks on those algorithms!).
  prefs: []
  type: TYPE_NORMAL
- en: 'It requires months, possibly years of processing power to factor the 232-digit
    numbers used by some encryption algorithms. The following implementation, while
    readable, is not at all efficient; it would take years to factor even a 100-digit
    number. That''s okay because we want to see it using lots of CPU time factoring
    9-digit numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Let's focus on the parallel processing aspects, as the brute force recursive
    algorithm for calculating factors is pretty clear. We create the `to_factor` list
    of 40,960 individual numbers. Then we construct a multiprocessing `pool` instance.
  prefs: []
  type: TYPE_NORMAL
- en: By default, this pool creates a separate process for each of the CPU cores in
    the machine it is running on.
  prefs: []
  type: TYPE_NORMAL
- en: The `map()` method of the pool accepts a function and an iterable. The pool
    pickles each of the values in the iterable and passes it to an available worker
    process in the pool, which executes the function on it. When that process is finished
    doing its work, it pickles the resulting list of factors and passes it back to
    the pool. Then, if the pool has more work available, the worker takes on the next
    job.
  prefs: []
  type: TYPE_NORMAL
- en: Once all the workers in the pool are finished processing (which could take some
    time), the `results` list is passed back to the original process, which has been
    waiting patiently for all this work to complete. The results of `map()` will be
    in the same order as the requests. This makes it sensible to use `zip()` to match
    up the original value with the computed prime factors.
  prefs: []
  type: TYPE_NORMAL
- en: It is often more useful to use the similar `map_async()` method, which returns
    immediately even though the processes are still working. In that case, the `results`
    variable would not be a list of values, but a contract (or a deal or an obligation)
    to return a list of values in the future when the client calls `results.get()`.
    This future object also has methods such as `ready()` and `wait()`, which allow
    us to check whether all the results are in yet. This is suitable for processing
    where the completion time is highly variable.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, if we don't know all the values we want to get results for in
    advance, we can use the `apply_async()` method to queue up a single job. If the
    pool has a process that isn't already working, it will start immediately; otherwise,
    it will hold onto the task until there is a free worker process available.
  prefs: []
  type: TYPE_NORMAL
- en: Pools can also be `closed`; they refuse to take any further tasks, but continue
    to process everything currently in the queue. They can also be `terminated`, which
    goes one step further and refuses to start any jobs still in the queue, although
    any jobs currently running are still permitted to complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a number of constraints on how many workers make sense, including
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Only `cpu_count()` processes can be computing simultaneously; any number can
    be waiting. If the workload is CPU-intensive, a larger pool of workers won't compute
    any faster. If the workload involves a lot of input/output, however, a large pool
    might improve the rate at which work is completed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For very large data structures, the number of workers in the pool may need to
    be reduced to make sure memory is used effectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communication between processes is expensive; easily serialized data is the
    best policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating new processes takes a non-zero amount of time; a pool of a fixed size
    helps minimize the impact of this cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The multiprocessing pool gives us a tremendous amount of computing power with
    relatively little work on our part. We need to define a function that can perform
    the parallelized computation, and we need to map arguments to that function using
    an instance of the `multiprocessing.Pool` class.
  prefs: []
  type: TYPE_NORMAL
- en: In many applications, we need to do more than a mapping from a parameter value
    to a complex result. For these applications, the simple `poll.map()` may not be
    enough. For more complicated data flows, we can make use of explicit queues of
    pending work and computed results. We'll look at creating a network of queues
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Queues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we need more control over communication between processes, the `queue`.`Queue`
    data structure is useful. There are several variants offering ways to send messages
    from one process to one or more other processes. Any picklable object can be sent
    into a `Queue`, but remember that pickling can be a costly operation, so keep
    such objects small. To illustrate queues, let's build a little search engine for
    text content that stores all relevant entries in memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'This particular search engine scans all files in the current directory in parallel.
    A process is constructed for each core on the CPU. Each of these is instructed
    to load some of the files into memory. Let''s look at the function that does the
    loading and searching:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Remember, the `search()` function is run in a separate process (in fact, it
    is run in `cpu_count()` separate processes) from the main process that created
    the queues. Each of these processes is started with a list of `pathlib.Path` objects,
    and two `multiprocessing.Queue` objects; one for incoming queries and one to send
    outgoing results. These queues automatically pickle the data in the queue and
    pass it into the subprocess over a pipe. These two queues are set up in the main
    process and passed through the pipes into the search function inside the child
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: The type hints reflect the way **mypy** wants details about the structure of
    data in each queue. When `TYPE_CHECKING` is `True`, it means **mypy** is running,
    and needs enough details to be sure the objects in the application match the descriptions
    of the objects in each of the queues. When `TYPE_CHECKING` is `False`, this is
    the ordinary runtime for the application, and the structural details of the queued
    messages can't be provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `search()` function does two separate things:'
  prefs: []
  type: TYPE_NORMAL
- en: When it starts, it opens and reads all the supplied files in the list of `Path`
    objects. Each line of text in those files is accumulated into the `lines` list.
    This preparation is relatively expensive, but it's done exactly once.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `while` statement is the main event processing loop for search. It uses
    `query_q.get()` to get a request from its queue. It searches lines. It uses `results_q.put()`
    to put a response into the results queue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `while` statement has the characteristic design pattern for queue-based
    processing. The process will get a value from a queue of some work to perform,
    perform the work, and then put the result into another queue. We can decompose
    very large and complex problems into processing steps and queues so that the work
    is done concurrently, producing more results in less time. This technique also
    lets us tailor the processing steps and the number of workers to make best use
    of a processor.
  prefs: []
  type: TYPE_NORMAL
- en: The main part of the application builds this pool of workers and their queues.
    We'll follow the **Façade** design pattern (refer back to *Chapter 12*, *Advanced
    Design Patterns* for more information). The idea here is to define a class, `DirectorySearch`,
    to wrap the queues and the pool of worker processes into a single object.
  prefs: []
  type: TYPE_NORMAL
- en: This object can set up the queues and the workers, and an application can then
    interact with them by posting a query and consuming the replies.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `setup_search()` method prepares the worker subprocesses. The `[i::cpus]`
    slice operation lets us break this list into a number of equally-sized parts.
    If the number of CPUs is 8, the step size will be 8, and we'll use 8 different
    offset values from 0 to 7\. We also construct a list of `Queue` objects to send
    data into each worker process. Finally, we construct a **single** results queue.
    This is passed into all of the worker subprocesses. Each of them can put data
    into the queue and it will be aggregated in the main process.
  prefs: []
  type: TYPE_NORMAL
- en: Once the queues are created and the workers started, the `search()` method provides
    the target to all the workers at one time. They can then all commence examining
    their separate collections of data to emit answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we''re searching a fairly large number of directories, we use a generator
    function, `all_source()`, to locate all the `*.py` `Path` objects under the given
    `base` directory. Here''s the function to find all the source files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `all_source()` function uses the `os.walk()` function to examine a directory
    tree, rejecting file directories that are filled with files we don't want to look
    at. This function uses the `fnmatch` module to match a file name against the kind
    of wild-card patterns the Linux shell uses. We can use a pattern parameter of
    `'*.py'`, for example, to find all files with names ending in `.py`. This seeds
    the `setup_search()` method of the `DirectorySearch` class.
  prefs: []
  type: TYPE_NORMAL
- en: The `teardown_search()` method of the `DirectorySearch` class puts a special
    termination value into each queue. Remember, each worker is a separate process,
    executing the `while` statement inside the `search()` function and reading from
    a queue of requests. When it reads a `None` object, it will break out of the `while`
    statement and exit the function. We can then use the `join()` to collect all the
    child processes, cleaning up politely. (If we don't do the `join()`, some Linux
    distros can leave "zombie processes" – children not properly rejoined with their
    parent because the parent crashed; these consume system resources and often require
    a reboot.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s look at the code that makes a search actually happen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This code creates a `DirectorySearch` object, `ds`, and provides all of the
    source paths starting from the parent of the current working directory, via `base
    = Path.cwd().parent`. Once the workers are prepared, the `ds` object performs
    searches for a few common strings, `"import"`, `"class"`, and `"def"`. Note that
    we''ve commented out the `print(line)` statement that shows the useful results.
    For now, we''re interested in performance. The initial file reads take a fraction
    of a second to get started. Once all the files are read, however, the time to
    do the search is dramatic. On a MacBook Pro with 134 files of source code, the
    output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The search for `"import"` took about 111 milliseconds (0.111 seconds.) Why was
    this so slow compared with the other two searches? It's because the `search()`
    function was still reading the files when the first request was put in the queue.
    The first request's performance reflects the one-time startup cost of loading
    the file content into memory. The next two requests run in about 1 millisecond
    each. That's amazing! Almost 1,000 searches per second on a laptop with only a
    few lines of Python code.
  prefs: []
  type: TYPE_NORMAL
- en: This example of queues to feed data among workers is a single-host version of
    what could become a distributed system. Imagine the searches were being sent out
    to multiple host computers and then recombined. Now imagine you had access to
    the fleet of computers in Google's data centers and you might understand why they
    can return search results so quickly!
  prefs: []
  type: TYPE_NORMAL
- en: We won't discuss it here, but the `multiprocessing` module includes a manager
    class that can take a lot of the boilerplate out of the preceding code. There
    is even a version of `multiprocessing.Manager` that can manage subprocesses on
    remote systems to construct a rudimentary distributed application. Check the Python
    `multiprocessing` documentation if you are interested in pursuing this further.
  prefs: []
  type: TYPE_NORMAL
- en: The problems with multiprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As with threads, multiprocessing also has problems, some of which we have already
    discussed. Sharing data between processes is costly. As we have discussed, all
    communication between processes, whether by queues, OS pipes, or even shared memory,
    requires serializing the objects. Excessive serialization can dominate processing
    time. Shared memory objects can help by limiting the serialization to the initial
    setup of the shared memory. Multiprocessing works best when relatively small objects
    are passed between processes and a tremendous amount of work needs to be done
    on each one.
  prefs: []
  type: TYPE_NORMAL
- en: Using shared memory can avoid the cost of repeated serialization and deserialization.
    There are numerous limitations on the kinds of Python objects that can be shared.
    Shared memory can help performance, but can also lead to somewhat more complex-looking
    Python objects.
  prefs: []
  type: TYPE_NORMAL
- en: The other major problem with multiprocessing is that, like threads, it can be
    hard to tell which process a variable or method is being accessed in. In multiprocessing,
    the worker processes inherit a great deal of data from the parent process. This
    isn't shared, it's a one-time copy. A child can be given a copy of a mapping or
    a list and mutate the object. The parent won't see the effects of the child's
    mutation.
  prefs: []
  type: TYPE_NORMAL
- en: A big advantage of multiprocessing is the absolute independence of processes.
    We don't need to carefully manage locks, because the data is not shared. Additionally,
    the internal operating system limits on numbers of open files are allocated at
    the process level; we can have a large number of resource-intensive processes.
  prefs: []
  type: TYPE_NORMAL
- en: When designing concurrent applications, the focus is on maximizing the use of
    the CPU to do as much work in as short a time as possible. With so many choices,
    we always need to examine the problem to figure out which of the many available
    solutions is the best one for that problem.
  prefs: []
  type: TYPE_NORMAL
- en: The notion of concurrent processing is too broad for there to be one right way
    to do it. Each distinct problem has a best solution. It's important to write code
    in a way that permits adjustment, tuning, and optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve looked at the two principal tools for concurrency in Python: threads
    and processes. Threads exist within a single OS process, sharing memory and other
    resources. Processes are independent of each other, which makes interprocess communication
    a necessary overhead. Both of these approaches are amenable to the concept of
    a pool of concurrent workers waiting to work and providing results at some unpredictable
    time in the future. This abstraction of results becoming available in the future
    is what shapes the `concurrent.futures` module. We''ll look at this next.'
  prefs: []
  type: TYPE_NORMAL
- en: Futures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start looking at a more asynchronous way of implementing concurrency.
    The concept of a "future" or a "promise" is a handy abstraction for describing
    concurrent work. A **future** is an object that wraps a function call. That function
    call is run in the *background*, in a thread or a separate process. The `future`
    object has methods to check whether the computation has completed and to get the
    results. We can think of it as a computation where the results will arrive in
    the future, and we can do something else while waiting for them.
  prefs: []
  type: TYPE_NORMAL
- en: See [https://hub.packtpub.com/asynchronous-programming-futures-and-promises/](https://hub.packtpub.com/asynchronous-programming-futures-and-promises/)
    for some additional background.
  prefs: []
  type: TYPE_NORMAL
- en: In Python, the `concurrent.futures` module wraps either `multiprocessing` or
    `threading` depending on what kind of concurrency we need. A future doesn't completely
    solve the problem of accidentally altering shared state, but using futures allows
    us to structure our code such that it can be easier to track down the cause of
    the problem when we do so.
  prefs: []
  type: TYPE_NORMAL
- en: Futures can help manage boundaries between the different threads or processes.
    Similar to the multiprocessing pool, they are useful for **call and answer** type
    interactions, in which processing can happen in another thread (or process) and
    then at some point in the future (they are aptly named, after all), you can ask
    it for the result. It's a wrapper around multiprocessing pools and thread pools,
    but it provides a cleaner API and encourages nicer code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see another, more sophisticated file search and analyze example. In
    the last section, we implemented a version of the Linux `grep` command. This time,
    we''ll create a simple version of the `find` command that bundles in a clever
    analysis of Python source code. We''ll start with the analytical part since it''s
    central to the work we need done concurrently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We've defined a few things here. We started with a named tuple, `ImportResult`,
    which binds a `Path` object and a set of strings together. It has a property,
    `focus`, that looks for the specific string, `"typing"`, in the set of strings.
    We'll see why this string is so important in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ImportVisitor` class is built using the `ast` module in the standard library.
    An **Abstract Syntax Tree** (**AST**) is the parsed source code, usually from
    a formal programming language. Python code, after all, is just a bunch of characters;
    the AST for Python code groups the text into meaningful statements and expressions,
    variable names, and operators, all of the syntactic components of the language.
    A visitor has a method to examine the parsed code. We provided overrides for two
    methods of the `NodeVisitor` class so we will visit only the two kinds of import
    statements: `import x`, and `from x import y`. The details of how each `node`
    data structure works are a bit beyond this example, but the `ast` module documentation
    in the Standard Library describes the unique structure of each Python language
    construct.'
  prefs: []
  type: TYPE_NORMAL
- en: The `find_imports()` function reads some source, parses the Python code, visits
    the `import` statements, and then returns an `ImportResult` with the original
    `Path` and the set of names found by the visitor. This is – in many ways – a lot
    better than a simple pattern match for `"import"`. For example, using an `ast.NodeVisitor`
    will skip over comments and ignore the text inside character string literals,
    two jobs that are hard with regular expressions.
  prefs: []
  type: TYPE_NORMAL
- en: There isn't anything particularly special about the `find_imports()` function,
    but note how it does not access any global variables. All interaction with the
    external environment is passed into the function or returned from it. This is
    not a technical requirement, but it is the best way to keep your brain inside
    your skull when programming with futures.
  prefs: []
  type: TYPE_NORMAL
- en: We want to process hundreds of files in dozens of directories, though. The best
    approach is to have lots and lots of these running all at the same time, clogging
    the cores of our CPU with lots and lots of computing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We're leveraging the same `all_source()` function shown in the *Queues* section
    earlier in this chapter; this needs a base directory to start searching in, and
    a pattern, like `"*.py"`, to find all the files with the `.py` extension. We've
    created a `ThreadPoolExecutor`, assigned to the `pool` variable, with two dozen
    worker threads, all waiting for something to do. We create a list of `Future`
    objects in the `analyzers` object. This list is created by a list comprehension
    applying the `pool.submit()` method to our search function, `find_imports()`,
    and a `Path` from the output of `all_source()`.
  prefs: []
  type: TYPE_NORMAL
- en: The threads in the pool will immediately start working on the submitted list
    of tasks. As each thread finishes work, it saves the results in the `Future` object
    and picks up some more work to do.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, in the foreground, the application uses a generator expression to
    evaluate the `result()` method of each `Future` object. Note that the futures
    are visited using the `futures.as_completed()` generator. The function starts
    providing complete `Future` objects as they become available. This means the results
    may not be in the order they were originally submitted. There are other ways to
    visit the futures; we can, for example, wait until all are complete and then visit
    them in the order they were submitted, in case that's important.
  prefs: []
  type: TYPE_NORMAL
- en: We extract the result from each `Future`. From the type hints, we can see that
    this will be an `ImportResult` object with a `Path` and a set of strings; these
    are the names of the imported modules. We can sort the results, so the files show
    up in some sensible order.
  prefs: []
  type: TYPE_NORMAL
- en: On a MacBook Pro, this takes about 1.689 milliseconds (0.001689 seconds) to
    process each file. The 24 individual threads easily fit in a single process without
    stressing the operating system. Increasing the number of threads doesn't materially
    affect the elapsed runtime, suggesting any remaining bottleneck is not concurrent
    computation, but the initial scan of the directory tree and the creation of the
    thread pool.
  prefs: []
  type: TYPE_NORMAL
- en: And the `focus` feature of the `ImportResult` class? Why is the `typing` module
    special? We needed to review each chapter's type hints when a new release of **mypy**
    came out during the development of this book. It was helpful to separate the modules
    into those that required careful checking and those that didn't need to be revised.
  prefs: []
  type: TYPE_NORMAL
- en: And that's all that is required to develop a futures-based I/O-bound application.
    Under the hood, it's using the same thread or process APIs we've already discussed,
    but it provides a more understandable interface and makes it easier to see the
    boundaries between concurrently running functions (just don't try to access global
    variables from inside the future!).
  prefs: []
  type: TYPE_NORMAL
- en: Accessing outside variables without proper synchronization can result in a problem
    called a **race** **condition**. For example, imagine two concurrent writes trying
    to increment an integer counter. They start at the same time and both read the
    current value of the shared variable as 5\. One thread is first in the race; it
    increments the value and writes 6\. The other thread comes in second; it increments
    what the variable was and also writes 6\. But if two processes are trying to increment
    a variable, the expected result would be that it gets incremented by 2, so the
    result should be 7.
  prefs: []
  type: TYPE_NORMAL
- en: Modern wisdom is that the easiest way to avoid doing this is to keep as much
    state as possible private and share them through known-safe constructs, such as
    queues or futures.
  prefs: []
  type: TYPE_NORMAL
- en: For many applications, the `concurrent.futures` module is the place to start
    with designing the Python code. The lower-level `threading` and `multiprocessing`
    modules offer some additional constructs for very complex cases.
  prefs: []
  type: TYPE_NORMAL
- en: Using `run_in_executor()` allows an application to leverage the `concurrent.futures`
    module's `ProcessPoolExecutor` or `ThreadPoolExecutor` classes to farm work out
    to multiple processes or multiple threads. This provides a lot of flexibility
    within a tidy, ergonomic API.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, we don't really need concurrent processes. In some cases, we
    simply need to be able to toggle back and forth between waiting for data and computing
    when data becomes available. The `async` features of Python, including the `asyncio`
    module, can interleave processing within a single thread. We'll look at this variation
    on the theme of concurrency next.
  prefs: []
  type: TYPE_NORMAL
- en: AsyncIO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AsyncIO is the current state of the art in Python concurrent programming. It
    combines the concept of futures and an event loop with coroutines. The result
    is about as elegant and easy to understand as it is possible to get when writing
    responsive applications that don't seem to waste time waiting for input.
  prefs: []
  type: TYPE_NORMAL
- en: For the purposes of working with Python's `async` features, a *coroutine* is
    a function that is waiting for an event, and also can provide events to other
    coroutines. In Python, we implement coroutines using `async def`. A function with
    `async` must work in the context of an **event loop** which switches control among
    the coroutines waiting for events. We'll see a few Python constructs using `await`
    expressions to show where the event loop can switch to another `async` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s crucial to recognize that `async` operations are interleaved, and not
    – generally – parallel. At most one coroutine is in control and processing, and
    all the others are waiting for an event. The idea of interleaving is described
    as **cooperative multitasking**: an application can be processing data while also
    waiting for the next request message to arrive. As data becomes available, the
    event loop can transfer control to one of the waiting coroutines.'
  prefs: []
  type: TYPE_NORMAL
- en: AsyncIO has a bias toward network I/O. Most networking applications, especially
    on the server side, spend a lot of time waiting for data to come in from the network.
    AsyncIO can be more efficient than handling each client in a separate thread;
    then some threads can be working while others are waiting. The problem is the
    threads use up memory and other resources. AsyncIO uses coroutines to interleave
    processing cycles when the data becomes available.
  prefs: []
  type: TYPE_NORMAL
- en: Thread scheduling depends on OS requests the thread makes (and to an extent,
    the GIL's interleaving of threads). Process scheduling depends on the overall
    scheduler for the operating system. Both thread and process scheduling are **preemptive**
    – the thread (or process) can be interrupted to allow a different, higher-priority
    thread or process to control the CPU. This means thread scheduling is unpredictable,
    and locks are important if multiple threads are going to update a shared resource.
    At the OS level, shared locks are required if two processes want to update a shared
    OS resource like a file. Unlike threads and processes, AsyncIO coroutines are
    **non-preemptive**; they explicitly hand control to each other at specific points
    in the processing, removing the need for explicit locking of shared resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `asyncio` library provides a built-in *event loop*: this is the loop that
    handles interleaving control among the running coroutines. However, the event
    loop comes with a cost. When we run code in an `async` task on the event loop,
    that code *must* return immediately, blocking neither on I/O nor on long-running
    calculations. This is a minor thing when writing our own code, but it means that
    any standard library or third-party functions that block on I/O must be wrapped
    with an `async def` function that can handle the waiting politely.'
  prefs: []
  type: TYPE_NORMAL
- en: When working with `asyncio`, we'll write our application as a set of coroutines
    that use `async` and `await` syntax to interleave control via the event loop.
    The top-level "main" program's job, then, is simplified to running the event loop
    so the coroutines can then hand control back and forth, interleaving waiting and
    working.
  prefs: []
  type: TYPE_NORMAL
- en: AsyncIO in action
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A canonical example of a blocking function is the `time.sleep()` call. We can''t
    call the `time` module''s `sleep()` directly, because it would seize control,
    stalling the event loop. We''ll use the version of `sleep()` in the `asyncio`
    module. Used in an `await` expression, the event loop can interleave another coroutine
    while waiting for the `sleep()` to finish. Let''s use the asynchronous version
    of this call to illustrate the basics of an AsyncIO event loop, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This example covers several features of AsyncIO programming. The overall processing
    is started by the `asyncio.run()` function. This starts the event loop, executing
    the `sleepers()` coroutine. Within the `sleepers()` coroutine, we create a handful
    of individual tasks; these are instances of the `random_sleep()` coroutine with a
    given argument value. The `random_sleep()` uses `asyncio.sleep()` to simulate a
    long-running request.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because this is built using `async def` functions and an `await` expression
    around `asyncio.sleep()`, execution of the `random_sleep()` functions and the
    overall `sleepers()` function is interleaved. While the `random_sleep()` requests
    are started in order of their `counter` parameter value, they finish in a completely
    different order. Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We can see the `random_sleep()` function with a `counter` value of `4` had the
    shortest sleep time, and was given control first when it finished the `await asyncio.sleep()`
    expression. The order of waking is strictly based on the random sleep interval,
    and the event loop's ability to hand control from coroutine to coroutine.
  prefs: []
  type: TYPE_NORMAL
- en: As asynchronous programmers, we don't need to know too much about what happens
    inside that `run()` function, but be aware that a lot is going on to track which
    of the coroutines is waiting and which should have control at the current moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'A task, in this context, is an object that `asyncio` knows how to schedule
    in the event loop. This includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines defined with the `async def` statement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`asyncio.Future` objects. These are almost identical to the `concurrent.futures`
    you saw in the previous section, but for use with `asyncio`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any awaitable object, that is, one with an `__await__()` function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this example, all the tasks are coroutines; we'll see some of the others
    in later examples.
  prefs: []
  type: TYPE_NORMAL
- en: Look a little more closely at that `sleepers()` coroutine. It first constructs
    instances of the `random_sleep()` coroutine. These are each wrapped in an `asyncio.create_task()`
    call, which adds these as futures to the loop's task queue so they can execute
    and start immediately when control is returned to the loop.
  prefs: []
  type: TYPE_NORMAL
- en: Control is returned to the event loop whenever we call `await`. In this case,
    we call `await asyncio.gather()` to yield control to other coroutines until all
    the tasks are finished.
  prefs: []
  type: TYPE_NORMAL
- en: Each of the `random_sleep()` coroutines prints a starting message, then sends
    control back to the event loop for a specific amount of time using its own `await`
    calls. When the sleep has completed, the event loop passes control back to the
    relevant `random_sleep()` task, which prints its awakening message before returning.
  prefs: []
  type: TYPE_NORMAL
- en: The `async` keyword acts as documentation notifying the Python interpreter (and
    coder) that the coroutine contains the `await` calls. It also does some work to
    prepare the coroutine to run on the event loop. It behaves much like a decorator;
    in fact, back in Python 3.4, it used to be implemented as an `@asyncio.coroutine` decorator.
  prefs: []
  type: TYPE_NORMAL
- en: Reading an AsyncIO future
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An AsyncIO coroutine executes each line of code in order until it encounters
    an `await` expression, at which point it returns control to the event loop. The
    event loop then executes any other tasks that are ready to run, including the
    one that the original coroutine was waiting on. Whenever that child task completes,
    the event loop sends the result back into the coroutine so that it can pick up
    execution until it encounters another `await` expression or returns.
  prefs: []
  type: TYPE_NORMAL
- en: This allows us to write code that executes synchronously until we explicitly
    need to wait for something. As a result, there is no non-deterministic behavior
    of threads, so we don't need to worry nearly so much about shared state.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s a good idea to limit shared state: a *share nothing* philosophy can prevent
    a ton of difficult bugs stemming from sometimes hard-to-imagine timelines of interleaved
    operations.'
  prefs: []
  type: TYPE_NORMAL
- en: Think of the OS schedulers as intentionally and wickedly evil; they will maliciously
    (somehow) find the worst possible sequence of operations among processes, threads,
    or coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: The real value of AsyncIO is the way it allows us to collect logical sections
    of code together inside a single coroutine, even if we are waiting for other work
    elsewhere. As a specific instance, even though the `await asyncio.sleep` call
    in the `random_sleep()` coroutine is allowing a ton of stuff to happen inside
    the event loop, the coroutine itself looks like it's doing everything in order.
    This ability to read related pieces of asynchronous code without worrying about
    the machinery that waits for tasks to complete is the primary benefit of the AsyncIO
    module.
  prefs: []
  type: TYPE_NORMAL
- en: AsyncIO for networking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AsyncIO was specifically designed for use with network sockets, so let's implement
    a server using the `asyncio` module. Looking back at *Chapter 13*, *Testing Object-Oriented
    Programs*, we created a fairly complex server to catch log entries being sent
    from one process to another process using sockets. At the time, we used it as
    an example of a complex resource we didn't want to set up and tear down for each
    test.
  prefs: []
  type: TYPE_NORMAL
- en: We'll rewrite that example, creating an `asyncio`-based server that can handle
    requests from a (large) number of clients. It can do this by having lots of coroutines,
    all waiting for log records to arrive. When a record arrives, one coroutine can
    save the record, doing some computation, while the remaining coroutines wait.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Chapter 13*, we were interested in writing a test for the integration of
    a log catcher process with separate log-writing client application processes.
    Here''s an illustration of the relationships involved:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17070_14_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.1: The Log Catcher in the Sky'
  prefs: []
  type: TYPE_NORMAL
- en: The log catcher process creates a socket server to wait for connections from
    all client applications. Each of the client applications uses `logging.SocketHandler`
    to direct log messages to the waiting server. The server collects the messages
    and writes them to a single, central log file.
  prefs: []
  type: TYPE_NORMAL
- en: This test was based on an example back in *Chapter 12*, which suffered from
    a weak implementation. To keep things simple in that chapter, the log server only
    worked with one application client at a time. We want to revisit the idea of a
    server that collects log messages. This improved implementation will handle a
    very large number of concurrent clients because it uses AsyncIO techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'The central part of this design is a coroutine that reads log entries from
    a socket. This involves waiting for the bytes that comprise a header, then decoding
    the header to compute the size of the payload. The coroutine can read the right
    number of bytes for the log message payload, and then use a separate coroutine
    to process the payload. Here''s the `log_catcher()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This `log_catcher()` function implements the protocol used by the `logging`
    module's `SocketHandler` class. Each log entry is a block of bytes we can decompose
    into a header and a payload. We need to read the first few bytes, saved in `size_header`,
    to get the size of the message which follows. Once we have the size, we can wait
    for the payload bytes to arrive. Since the two reads are both `await` expressions,
    other coroutines can work while this function is waiting for the header and payload bytes to
    arrive.
  prefs: []
  type: TYPE_NORMAL
- en: The `log_catcher()` function is invoked by a server that provides the coroutine
    with a `StreamReader` and `StreamWriter`. These two objects wrap the socket pair
    that is created by the TCP/IP protocol. The stream reader (and the writer) are
    properly async-aware objects, and we can use `await` when waiting to read bytes
    from the client.
  prefs: []
  type: TYPE_NORMAL
- en: This `log_catcher()` function waits for socket data, then provides data to another
    coroutine, `log_writer()`, for conversion and writing. The `log_catcher()` function's
    job is to do a lot of waiting, and then shuttle the data from reader to writer;
    it also does an internal computation to count messages from a client. Incrementing
    a counter is not much, but it is work that can be done while waiting for data
    to arrive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a function, `serialize()`, and a coroutine, `log_writer()`, to convert
    log entries to JSON notation and write them to a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `serialize()` function needs to have an open file, `TARGET`, to which the
    log messages are written. The file open (and close) needs to be taken care of
    elsewhere in the application; we'll look at these operations below. The `serialize()`
    function is used by the `log_writer()` coroutine. Because `log_writer()` is an
    `async` coroutine, other coroutines will be waiting to read and decode input messages
    while this coroutine is writing them.
  prefs: []
  type: TYPE_NORMAL
- en: The `serialize()` function actually does a fair amount of computation. It also
    harbors a profound problem. The file write operation can be blocked, that is,
    stuck waiting for the operating system to finish the work. Writing to a disk means
    handing the work to a disk device and waiting until the device responds that the
    write operation is complete. While a microsecond to write a 1,000-character line
    of data may seem fast, it's forever to a CPU. This means all file operations will
    block their thread waiting for the operation to complete. To work politely with
    the other coroutines in the main thread, we assign this blocking work to a separate
    thread. This is why the `log_writer()` coroutine uses the `asyncio.to_thread()`
    to allocate this work to a separate thread.
  prefs: []
  type: TYPE_NORMAL
- en: Because the `log_writer()` coroutine uses `await` on this separate thread, it
    returns control to the event loop while the thread waits for the write to complete.
    This polite `await` allows other coroutines to work while the `log_writer()` coroutine
    is waiting for `serialize()` to complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve passed two kinds of work to a separate thread:'
  prefs: []
  type: TYPE_NORMAL
- en: A compute-intensive operation. These are the `pickle.loads()` and `json.dumps()`
    operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A blocking OS operation. This is `TARGET.write()`. These blocking operations
    include most operating system requests, including file operations. They do not
    include the various network streams that are already part of the `asyncio` module.
    As we saw in the `log_catcher()` function above, the streams are already polite
    users of the event loop.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This technique of passing work to a thread is how we can make sure the event
    loop is spending as much time waiting as possible. If all the coroutines are waiting
    for an event, then whatever happens next will be responded to as quickly as possible.
    This principle of many waiters is the secret to a responsive service.
  prefs: []
  type: TYPE_NORMAL
- en: The `LINE_COUNT` global variable can raise some eyebrows. Recall from previous
    sections, we raised dire warnings about the consequences of multiple threads updating
    a shared variable concurrently. With `asyncio`, we don't have preemption among
    threads. Because each coroutine uses explicit `await` requests to give control
    to other coroutines via the event loop, we can update this variable in the `log_writer()`
    coroutine knowing the state change will effectively be atomic – an indivisible
    update – among all the coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make this example complete, here are the imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the top-level dispatcher that starts this service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `main()` function contains an elegant way to automatically create new `asyncio.Task`
    objects for each network connection. The `asyncio.start_server()` function listens
    at the given host address and port number for incoming socket connections. For
    each connection, it creates a new `Task` instance using the `log_catcher()` coroutine;
    this is added to the event loop's collection of coroutines. Once the server is started,
    the `main()` function lets it provide services forever using the server's `serve_forever()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: The `add_signal_handler()` method of a loop deserves some explanation. For non-Windows
    operating systems, a process is terminated via a signal from the operating system.
    The signals have small numeric identifiers and symbolic names. For example, the
    terminate signal has a numeric code of 15, and a name of `signal.SIGTERM`. When
    a parent process terminates a child process, this signal is sent. If we do nothing
    special, this signal will simply stop the Python interpreter. When we use the
    Ctrl + C sequence on the keyboard, this becomes a `SIGINT` signal, which leads
    Python to raise a `KeyboardInterrupt` exception.
  prefs: []
  type: TYPE_NORMAL
- en: The `add_signal_handler()` method of the loop lets us examine incoming signals
    and handle them as part of our AsyncIO processing loop. We don't want to simply
    stop with an unhandled exception. We want to finish the various coroutines, and
    allow any write threads executing the `serialize()` function to complete normally.
    To make this happen, we connect the signal to the `server.close()` method. This
    ends the `serve_forever()` process cleanly, letting all the coroutines finish.
  prefs: []
  type: TYPE_NORMAL
- en: For Windows, we have to work outside the AsyncIO processing loop. This additional
    code is required to connect the low-level signals to a function that will close
    down the server cleanly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We've defined three standard signals, `SIGINT`, `SIGTERM`, and `SIGABRT`, as
    well as a Windows-specific signal, `SIGBREAK`. These will all close the server,
    ending the handling of requests and closing down the processing loop when all
    of the pending coroutines have completed.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw in the previous AsyncIO example, the main program is also a succinct
    way to start the event loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This will open a file, setting the global `TARGET` variable used by the `serialize()`
    function. It uses the `main()` function to create the server that waits for connections.
    When the `serve_forever()` task is canceled with a `CancelledError` or `KeyboardInterrupt`
    exception, we can put a final summary line onto the log file. This line confirms
    that things completed normally, allowing us to verify that no lines were lost.
  prefs: []
  type: TYPE_NORMAL
- en: For Windows, we need to use the `run_until_complete()` method, instead of the
    more comprehensive `run()` method. We also need to put one more coroutine, `asyncio.sleep()`,
    into the event loop to wait for the final processing from any other coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: Pragmatically, we might want to use the `argparse` module to parse command-line
    arguments. We might want to use a more sophisticated file-handling mechanism in
    `log_writer()` so we can limit the size of log files.
  prefs: []
  type: TYPE_NORMAL
- en: Design considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's look at some of the features of this design. First, the `log_writer()`
    coroutine passes bytes into and out of the external thread running the `serialize()`
    function. This is better than decoding the JSON in a coroutine in the main thread
    because the (relatively expensive) decoding can happen without stopping the main
    thread's event loop.
  prefs: []
  type: TYPE_NORMAL
- en: This call to `serialize()` is, in effect, a future. In the *Futures* section,
    earlier in this chapter, we saw there are a few lines of boilerplate for using
    `concurrent.futures`. However, when we use futures with AsyncIO, there are almost
    none at all! When we use `await asyncio.to_thread()`, the `log_writer()` coroutine
    wraps the function call in a future and submits it to the internal thread pool
    executor. Our code can then return to the event loop until the future completes,
    allowing the main thread to process other connections, tasks, or futures. It is
    particularly important to put blocking I/O requests into separate threads. When
    the future is done, the `log_writer()` coroutine can finish waiting and can do
    any follow-up processing.
  prefs: []
  type: TYPE_NORMAL
- en: The `main()` coroutine used `start_server()`; the server listens for connection
    requests. It will provide client-specific AsyncIO read and write streams to each
    task created to handle a distinct connection; the task will wrap the `log_catcher()`
    coroutine. With the AsyncIO streams, reading from a stream is a potentially blocking
    call so we can call it with `await`. This means politely returning to the event
    loop until bytes start arriving.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can help to consider how the workload grows inside this server. Initially,
    the `main()` function is the only coroutine. It creates the `server`, and now
    both `main()` and the `server` are in the event loop''s collection of waiting
    coroutines. When a connection is made, the server creates a new task, and the
    event loop now contains `main()`, the `server`, and an instance of the `log_catcher()`
    coroutine. Most of the time, all of these coroutines are waiting for something
    to do: either a new connection for the server, or a message for the `log_catcher()`.
    When a message arrives, it''s decoded and handed to `log_writer()`, and yet another
    coroutine is available. No matter what happens next, the application is ready
    to respond. The number of waiting coroutines is limited by available memory, so
    a lot of individual coroutines can be patiently waiting for work to do.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll take a quick look at a log-writing application that uses this log
    catcher. The application doesn't do anything useful, but it can tie up a lot of
    cores for a long period of time. This will show us how responsive AsyncIO applications
    can be.
  prefs: []
  type: TYPE_NORMAL
- en: A log writing demonstration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To demonstrate how this log catching works, this client application writes a
    bunch of messages and does an immense amount of computing. To see how responsive
    the log catcher is, we can start a bunch of copies of this application to stress-test
    the log catcher.
  prefs: []
  type: TYPE_NORMAL
- en: This client doesn't leverage `asyncio`; it's a contrived example of compute-intensive
    work with a few I/O requests wrapped around it. Using coroutines to perform the
    I/O requests concurrently with the computation is – by design – unhelpful in this
    example.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve written an application that applies a variation on the bogosort algorithm
    to some random data. Here''s some information on this sorting algorithm: [https://rosettacode.org/wiki/Sorting_algorithms/Bogosort](https://rosettacode.org/wiki/Sorting_algorithms/Bogosort).
    This isn''t a practical algorithm, but it''s simple: it enumerates all possible
    orderings, searching for one that is the desired, ascending order. Here are the
    imports and an abstract superclass, `Sorter`, for sorting algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll define a concrete implementation of the abstract `Sorter` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `is_ordered()` method of the `BogoSort` class checks to see if the list
    of objects has been sorted properly. The `sort()` method generates all permutations
    of the data, searching for a permutation that satisfies the constraint defined
    by `is_sorted()`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that a set of *n* values has *n!* permutations, so this is a spectacularly
    inefficient sort algorithm. There are over six billion permutations of 13 values;
    on most computers, this algorithm can take years to sort 13 items into order.
  prefs: []
  type: TYPE_NORMAL
- en: 'A `main()` function handles the sorting and writes a few log messages. It does
    a lot of computation, tying up CPU resources doing nothing particularly useful.
    Here''s a `main` program we can use to make log requests while our inefficient
    sort is grinding up processing time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The top-level script starts by creating a `SocketHandler` instance; this writes
    log messages to the log catcher service shown above. A `StreamHandler` instance
    writes message to console. Both of these are provided as handlers for all the
    defined loggers. Once the logging is configured, the `main()` function is invoked
    with a random workload.
  prefs: []
  type: TYPE_NORMAL
- en: On an 8-core MacBook Pro, this was run with 128 workers, all inefficiently sorting
    random numbers. The internal OS `time` command describes the workload as using
    700% of a core; that is, seven of the eight cores were completely occupied. And
    yet, there's still plenty of time left over to handle the log messages, edit this
    document, and play music in the background. Using a faster sort algorithm, we
    started 256 workers and generated 5,632 log messages in about 4.4 seconds. This
    is 1,280 transactions per second and we were still only using 628% of the available
    800%. Your performance may vary. For network-intensive workloads, AsyncIO seems
    to do a marvelous job of allocating precious CPU time to the coroutine with work
    to be done, and minimizing the time threads are blocked waiting for something
    to do.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to observe that AsyncIO is heavily biased toward network resources
    including sockets, queues, and OS pipes. The file system is not a first-class
    part of the `asyncio` module, and therefore requires us to use the associated
    thread pool to handle processing that will be blocked until it's finished by the
    operating system.
  prefs: []
  type: TYPE_NORMAL
- en: We'll take a diversion to look at AsyncIO to write a client-side application.
    In this case, we won't be creating a server, but instead leveraging the event
    loop to make sure a client can process data very quickly.
  prefs: []
  type: TYPE_NORMAL
- en: AsyncIO clients
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because it is capable of handling many thousands of simultaneous connections,
    AsyncIO is very common for implementing servers. However, it is a generic networking
    library and provides full support for client processes as well. This is pretty
    important, since many microservices act as clients to other servers.
  prefs: []
  type: TYPE_NORMAL
- en: Clients can be much simpler than servers, as they don't have to be set up to
    wait for incoming connections. We can leverage the `await` `asyncio.gather()`
    function to parcel out a lot of work, and wait to process the results when they've
    completed. This can work well with `asyncio.to_thread()` which assigns blocking
    requests to separate threads, permitting the main thread to interleave work among
    the coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: We can also create individual tasks that can be interleaved by the event loop.
    This allows the coroutines that implement the tasks to cooperatively schedule
    reading data along with computing the data that was read.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we'll use the `httpx` library to provide an AsyncIO-friendly
    HTTP request. This additional package needs to be installed with `conda install
    https` (if you're using *conda* as a virtual environment manager) or `python -m
    pip install httpx`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an application to make requests to the US weather service, implemented
    using `asyncio`. We''ll focus on forecast zones useful for sailors in the Chesapeake
    Bay area. We''ll start with some definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Given the `Zone` named tuple, we can analyze the directory of marine forecast
    products, and create a list of `Zone` instances that starts like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Depending on where you're going to be sailing, you may want additional or different
    zones.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need a `MarineWX` class to describe the work to be done. This is an example
    of a **Command** pattern, where each instance is another thing we wish to do.
    This class has a `run()` method to gather data from a weather service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the `run()` method downloads the text document from the weather
    service via an instance of the `httpx` module's `AsyncClient` class. A separate
    property, `advisory()`, parses the text, looking for a pattern that marks a marine
    weather advisory. The sections of the weather service document really are marked
    by three periods, a block of text, and three periods. The Marine Forecast system
    is designed to provide an easy-to-process format with a tiny document size.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, this isn''t unique or remarkable. We''ve defined a repository of zone
    information, and a class that gathers data for a zone. Here''s the important part:
    a `main()` function that uses the AsyncIO tasks to gather as much data as quickly
    as possible.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The `main()` function, when run in the `asyncio` event loop, will launch a number
    of tasks, each of which is executing the `MarineWX.run()` method for a different
    zone. The `gather()` function waits until all of them have finished to return
    the list of futures.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we don't really want the future result from the created threads;
    we want the state changes that have been made to all of the `MarineWX` instances.
    These will be a collection of `Zone` objects and the forecast details. This client
    runs pretty quickly – we got all thirteen forecasts in about 300 milliseconds.
  prefs: []
  type: TYPE_NORMAL
- en: The `httpx` project supports the decomposition of fetching the raw data and
    processing the data into separate coroutines. This permits the waiting for data
    to be interleaved with processing.
  prefs: []
  type: TYPE_NORMAL
- en: We've hit most of the high points of AsyncIO in this section, and the chapter
    has covered many other concurrency primitives. Concurrency is a hard problem to
    solve, and no one solution fits all use cases. The most important part of designing
    a concurrent system is deciding which of the available tools is the correct one
    to use for the problem. We have seen the advantages and disadvantages of several
    concurrent systems, and now have some insight into which are the better choices
    for different types of requirements.
  prefs: []
  type: TYPE_NORMAL
- en: The next topic touches on the question of how "expressive" a concurrency framework
    or package can be. We'll see how `asyncio` solves a classic computer science problem
    with a short, clean-looking application program.
  prefs: []
  type: TYPE_NORMAL
- en: The dining philosophers benchmark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The faculty of the College of Philosophy in an old seaside resort city (on the
    Atlantic coast of the US) has a long-standing tradition of dining together every
    Sunday night. The food is catered from Mo's Deli, but is always – always – a heaping
    bowl of spaghetti. No one can remember why, but Mo's a great chef, and each week's
    spaghetti is a unique experience.
  prefs: []
  type: TYPE_NORMAL
- en: The philosophy department is small, having five tenured faculty members. They're
    also impoverished and can only afford five forks. Because the dining philosophers
    each require two forks to enjoy their pasta, they sit around a circular table,
    so each philosopher has access to two nearby forks.
  prefs: []
  type: TYPE_NORMAL
- en: 'This requirement for two forks to eat leads to an interesting resource contention
    problem, shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17070_14_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.2: The dining philosophers'
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, a philosopher, say Philosopher 4, the department chairperson, and an
    Ontologist, will acquire the two closest forks, Fork 4 and Fork 0, required to
    eat. Once they've eaten, they release the forks so they can spend some time on philosophy.
  prefs: []
  type: TYPE_NORMAL
- en: There's a problem waiting to be solved. If each philosopher is right-handed,
    they will reach out, grab the fork on their right, and – unable to grab another
    fork – are stopped. The system is **deadlocked** because no philosopher can acquire
    the resources to eat.
  prefs: []
  type: TYPE_NORMAL
- en: 'One possible solution could break the deadlock by using a timeout: if a philosopher
    can''t acquire a second fork in a few seconds, they set their first fork down,
    wait a few seconds, and try again. If they all proceed at the same tempo, this
    results in a cycle of each philosopher getting one fork, waiting a few seconds,
    setting their forks down, and trying again. Funny, but unsatisfying.'
  prefs: []
  type: TYPE_NORMAL
- en: A better solution is to permit only four philosophers at a time to sit at the
    table. This ensures that at least one philosopher can acquire two forks and eat.
    While that philosopher is philosophizing, the forks are now available to their
    two neighbors. Additionally, the first to finish philosophizing can leave the
    table, allowing the fifth to be seated and join the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: 'How does this look in code? Here''s the philosopher, defined as a coroutine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Each philosopher needs to know a few things:'
  prefs: []
  type: TYPE_NORMAL
- en: Their own unique identifier. This directs them to the two adjacent forks they're
    permitted to use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `Semaphore` – the footman – who seats them at the table. It's the footman's
    job to have an upper bound on how many can be seated, thereby avoiding deadlock.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A global collection of forks, represented by a sequence of `Lock` instances,
    that will be shared by the philosophers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The philosopher''s mealtime is described by acquiring and using resources.
    This is implemented with the `async with` statements. The sequence of events looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: A philosopher acquires a seat at the table from the footman, a `Semaphore`.
    We can think of the footman as holding a silver tray with four "you may eat" tokens.
    A philosopher must have a token before they can sit. Leaving the table, a philosopher
    drops their token on the tray. The fifth philosopher is eagerly waiting for the
    token drop from the first philosopher who finishes eating.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A philosopher acquires the fork with their ID number and the next higher-numbered
    fork. The modulo operator assures that the counting of "next" wraps around to
    zero; `(4+1) % 5` is 0.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With a seat at the table and with two forks, the philosopher may enjoy their
    pasta. Mo often uses kalamata olives and pickled artichoke hearts; it's delightful.
    Once a month there might be some anchovies or feta cheese.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After eating, a philosopher releases the two fork resources. They're not done
    with dinner, however. Once they've set the forks down, they then spend time philosophizing
    about life, the universe, and everything.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, they relinquish their seat at the table, returning their "you may eat"
    token to the footman, in case another philosopher is waiting for it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Looking at the `philosopher()` function, we can see that the forks are a global
    resource, but the semaphore is a parameter. There's no compelling technical reason
    to distinguish between the global collection of `Lock` objects to represent the
    forks and the `Semaphore` as a parameter. We showed both to illustrate the two
    common choices for providing data to coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the imports for this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The overall dining room is organized like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The `main()` coroutine creates the collection of forks; these are modeled as
    `Lock` objects that a philosopher can acquire. The footman is a `BoundedSemaphore`
    object with a limit one fewer than the size of the faculty; this avoids a deadlock.
    For each serving, the department is represented by a collection of `philosopher()`
    coroutines. The `asyncio.gather()` waits for all of the department's coroutines
    to complete their work – eating and philosophizing.
  prefs: []
  type: TYPE_NORMAL
- en: The beauty of this benchmark problem is to show how well the processing can
    be stated in the given programming language and library. With the `asyncio` package,
    the code is extremely elegant, and seems to be a succinct and expressive representation
    of a solution to the problem.
  prefs: []
  type: TYPE_NORMAL
- en: The `concurrent.futures` library can make use of an explicit `ThreadPool`. It
    can approach this level of clarity but involves a little bit more technical overhead.
  prefs: []
  type: TYPE_NORMAL
- en: The `threading` and `multiprocessing` libraries can also be used directly to
    provide a similar implementation. Using either of these involves even more technical
    overhead than the `concurrent.futures` library. If the eating or philosophizing
    involved real computational work – not simply sleeping – we would see that a `multiprocessing`
    version would finish the soonest because the computation can be spread among several
    cores. If the eating or philosophizing was mostly waiting for I/O to complete,
    it would be more like the implementation shown here, and using `asyncio` or using
    `concurrent.futures` with a thread pool would work out nicely.
  prefs: []
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the problems that often plagues data scientists working on machine learning
    applications is the amount of time it takes to "train" a model. In our specific
    example of the *k*-nearest neighbors implementation, training means performing
    the hyperparameter tuning to find an optimal value of *k* and the right distance
    algorithm. In the previous chapters of our case study, we've tacitly assumed there
    will be an optimal set of hyperparameters. In this chapter, we'll look at one
    way to locate the optimal parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In more complex and less well-defined problems, the time spent training the
    model can be quite long. If the volume of data is immense, then very expensive
    compute and storage resources are required to build and train the model.
  prefs: []
  type: TYPE_NORMAL
- en: As an example of a more complex model, look at the MNIST dataset. See [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)
    for the source data for this dataset and some kinds of analysis that have been
    performed. This problem requires considerably more time to locate optimal hyperparameters
    than our small Iris classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: In our case study, hyperparameter tuning is an example of a compute-intensive
    application. There's very little I/O; if we use shared memory, there's no I/O.
    This means that a process pool to allow parallel computation is essential. We
    can wrap the process pool in AsyncIO coroutines, but the extra `async` and `await`
    syntax seems unhelpful for this kind of compute-intensive example. Instead, we'll
    use the `concurrent.futures` module to build our hyperparameter tuning function.
    The design pattern for `concurrent.futures` is to make use of a processing pool
    to farm out the various testing computations to a number of workers, and gather
    the results to determine which combination is optimal. A process pool means each
    worker can occupy a separate core, maximizing compute time. We'll want to run
    as many tests of `Hyperparameter` instances at the same time as possible.
  prefs: []
  type: TYPE_NORMAL
- en: In previous chapters, we looked at several ways to define the training data
    and the hyperparameter tuning values. In this case study, we'll use some model
    classes from *Chapter 7*, *Python Data Structures*. From this chapter, we'll be
    using the `TrainingKnownSample` and the `TestingKnownSample` class definitions.
    We'll need to keep these in a `TrainingData` instance. And, most importantly,
    we'll need `Hyperparameter` instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can summarize the model like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17070_14_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.3: The Hyperparameter model'
  prefs: []
  type: TYPE_NORMAL
- en: We want to emphasize the `KnownTestingSample` and `KnownTrainingSample` classes.
    We are looking at testing, and won't be doing anything with `UnknownSample` instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our tuning strategy can be described as **grid search**. We can imagine a grid
    with the alternative values for *k* across the top and the different distance
    algorithms down the side. We''ll fill in each cell of the grid with a result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This lets us compare a range of *k* values and distance algorithms to see which
    combination is best. We don't really want to print the results, though. We want
    to save them in a list, sort them to find the highest-quality result, and use
    that as the preferred `Hyperparameter` configuration for classifying unknown samples.
  prefs: []
  type: TYPE_NORMAL
- en: '(Spoiler alert: for this Iris dataset, they''re all pretty good.)'
  prefs: []
  type: TYPE_NORMAL
- en: Each test run is completely independent. We can, therefore, do them all concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: 'To show what we''ll be running concurrently, here''s the test method of the `Hyperparameter` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We'll use each test sample, performing the classification algorithm. If the
    known result matches the species assigned by the `classify()` algorithm, we'll
    count this as a pass. If the classification algorithm doesn't match the known
    result, we'll count this as a failure. The percentage of correct matches is one
    way to gauge the quality of a classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an overall testing function, `load_and_tune()`. This function loads
    the raw data into memory from the `bezdekiris.data` file, which can be found in
    the code repository for this book. The function includes the use of a `ProcessPoolExecutor`
    to run a number of workers concurrently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We've used the `workers.submit()` to provide a function, the `test()` method
    of a `Hyperparameter` instance, `h`, to the pool of workers. The result is a `Future[Hyperparameter]` that
    will (eventually) have a `Hyperparameter` as a result. Each submitted future,
    managed by the `ProcessPoolExecutor`, will evaluate this function, saving the
    resulting `Hyperparameter` object as the future's result.
  prefs: []
  type: TYPE_NORMAL
- en: Is this use of the `ProcessPoolExecutor` optimal? Because we have such a small
    pool of data, it seems to work well. The overhead of serializing the training
    data for each submission is minimal. For a larger set of training and testing
    samples, we will run into performance problems serializing all the data. Since
    the samples are string and float objects, we can change the data structure to
    use shared memory. This is a radical restructuring that needs to exploit the Flyweight
    design pattern from *Chapter 12*, *Advanced Design Patterns*.
  prefs: []
  type: TYPE_NORMAL
- en: We used the `Future[Hyperparameter]` type hint to remind the **mypy** tool that
    we expect the `test()` method to return a `Hyperparameter` result. It's important
    to make sure the expected result type matches the result type from the function
    actually provided to `submit()`.
  prefs: []
  type: TYPE_NORMAL
- en: When we examine the `Future[Hyperparameter]` object, the `result` function will
    provide the `Hyperparameter` that was processed in the worker thread. We can collect
    these to locate an optimal hyperparameter set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly, they''re all quite good, varying between 97% and 100% accuracy.
    Here''s a short snippet of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Why is the quality so consistently high? There are a number of reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The source data was carefully curated and prepared by the authors of the original
    study.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are only four features for each sample. The classification isn't complex
    and there aren't a lot of opportunities for near-miss classifications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of the four features, two are very strongly correlated with the resulting species.
    The other two have weaker correlations between a feature and the species.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the reasons for choosing this example is because the data allows us to
    enjoy a success without the complications of struggling with a poorly-designed
    problem, data that's difficult to work with, or a high level of noise that drowns
    out the import signal hidden in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the `iris.names` file, section 8, we see the following summary statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: These statistics suggest that using only two of the features would be better
    than using all four features. Indeed, ignoring the sepal width might provide even
    better results.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on to more sophisticated problems will introduce new challenges. The
    essential Python programming shouldn't be part of the problem anymore. It should
    help to craft workable solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ve looked closely at a variety of topics related to concurrent processing
    in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: Threads have an advantage of simplicity for many cases. This has to be balanced
    against the GIL interfering with compute-intensive multi-threading.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiprocessing has an advantage of making full use of all cores of a processor.
    This has to be balanced against interprocess communication costs. If shared memory
    is used, there is the complication of encoding and accessing the shared objects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `concurrent.futures` module defines an abstraction – the future – that can
    minimize the differences in application programming used for accessing threads
    or processes. This makes it easy to switch and see which approach is fastest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `async`/`await` features of the Python language are supported by the AsyncIO
    package. Because these are coroutines, there isn't true parallel processing; control
    switches among the coroutines allow a single thread to interleave between waiting
    for I/O and computing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dining philosophers benchmark can be helpful for comparing different kinds
    of concurrency language features and libraries. It's a relatively simple problem
    with some interesting complexities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perhaps the most important observation is the lack of a trivial one-size-fits-all
    solution to concurrent processing. It's essential to create – and measure – a
    variety of solutions to determine a design that makes best use of the computing
    hardware.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've covered several different concurrency paradigms in this chapter and still
    don't have a clear idea of when each one is useful. In the case study, we hinted
    that it's generally best to develop a few different strategies before committing
    to one that is measurably better than the others. The final choice must be based
    on measurements of the performance of multi-threaded and multi-processing solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency is a huge topic. As your first exercise, we encourage you to search
    the web to discover what are considered to be the latest Python concurrency best
    practices. It can help to investigate material that isn't Python-specific to understand
    the operating system primitives like semaphores, locks, and queues.
  prefs: []
  type: TYPE_NORMAL
- en: If you have used threads in a recent application, take a look at the code and
    see how you can make it more readable and less bug-prone by using futures. Compare
    thread and multiprocessing futures to see whether you can gain anything by using
    multiple CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Try implementing an AsyncIO service for some basic HTTP requests. If you can
    get it to the point that a web browser can render a simple GET request, you'll
    have a good understanding of AsyncIO network transports and protocols.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you understand the race conditions that happen in threads when you
    access shared data. Try to come up with a program that uses multiple threads to
    set shared values in such a way that the data deliberately becomes corrupt or
    invalid.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 8*, *The Intersection of Object-Oriented and Functional Programming*,
    we looked at an example that used `subprocess.run()` to execute a number of `python
    -m doctest` commands on files within a directory. Review that example and rewrite
    the code to run each subprocess in parallel using a `futures.ProcessPoolExecutor`.
  prefs: []
  type: TYPE_NORMAL
- en: Looking back at *Chapter 12*, *Advanced Design Patterns*, there's an example
    that runs an external command to create the figures for each chapter. This relies
    on an external application, `java`, which tends to consume a lot of CPU resources
    when it runs. Does concurrency help with this example? Running multiple, concurrent
    Java programs seems to be a terrible burden. Is this a case where the default
    value for the size of a process pool is too large?
  prefs: []
  type: TYPE_NORMAL
- en: When looking at the case study, an important alternative is to use shared memory
    to allow multiple concurrent processes sharing a common set of raw data. Using
    shared memory means either sharing bytes or sharing a list of simple objects.
    Sharing bytes works well for packages like NumPy, but doesn't work well for our
    Python class definitions. This suggests that we can create a `SharedList` object
    that contains all of the sample values. We'll need to apply the Flyweight design
    pattern to present attributes with useful names extracted from the list in shared
    memory. An individual `FlyweightSample`, then, will extract four measurements
    and a species assignment. Once the data is prepared, what are the performance
    differences among concurrent processes and threads within a process? What changes
    are required to the `TrainingData` class to avoid loading testing and training
    samples until they're needed?
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter ends our exploration of object-oriented programming with a topic
    that isn't very object-oriented. Concurrency is a difficult problem, and we've
    only scratched the surface. While the underlying OS abstractions of processes
    and threads do not provide an API that is remotely object-oriented, Python offers
    some really good object-oriented abstractions around them. The threading and multiprocessing
    packages both provide an object-oriented interface to the underlying mechanics.
    Futures are able to encapsulate a lot of the messy details into a single object.
    AsyncIO uses coroutine objects to make our code read as though it runs synchronously,
    while hiding ugly and complicated implementation details behind a very simple
    loop abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading *Python Object-Oriented Programming*, *Fourth Edition*.
    We hope you've enjoyed the ride and are eager to start implementing object-oriented
    software in all your future projects!
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image18112.png)'
  prefs: []
  type: TYPE_IMG
- en: '[packt.com](http://packt.com)'
  prefs: []
  type: TYPE_NORMAL
- en: Subscribe to our online digital library for full access to over 7,000 books
    and videos, as well as industry leading tools to help you plan your personal development
    and advance your career. For more information, please visit our website.
  prefs: []
  type: TYPE_NORMAL
- en: Why subscribe?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spend less time learning and more time coding with practical eBooks and Videos
    from over 4,000 industry professionals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn better with Skill Plans built especially for you
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get a free eBook or video every month
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully searchable for easy access to vital information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy and paste, print, and bookmark content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Did you know that Packt offers eBook versions of every book published, with
    PDF and ePub files available? You can upgrade to the eBook version at [www.Packt.com](http://www.Packt.com)
    and as a print book customer, you are entitled to a discount on the eBook copy.
    Get in touch with us at [customercare@packtpub.com](http://customercare@packtpub.com)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: At [www.Packt.com](http://www.Packt.com), you can also read a collection of
    free technical articles, sign up for a range of free newsletters, and receive
    exclusive discounts and offers on Packt books and eBooks.
  prefs: []
  type: TYPE_NORMAL
