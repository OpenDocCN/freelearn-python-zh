<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Predicting Sentiments in Words</h1>
                </header>
            
            <article>
                
<p>This chapter presents the following recipes:</p>
<ul>
<li>Building a Naive Bayes classifier</li>
<li>Logistic regression classifier</li>
<li>Splitting the dataset for training and testing</li>
<li>Evaluating the accuracy using cross-validation</li>
<li>Analyzing the sentiment of a sentence</li>
<li>Identifying patterns in text using topic modeling</li>
<li>Application of sentiment analyses</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a Naive Bayes classifier</h1>
                </header>
            
            <article>
                
<p>A Naive Bayes classifier employs Bayes' theorem to construct a supervised model.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Import the following packages:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.naive_bayes import GaussianNB<br/>import numpy as np<br/>import matplotlib.pyplot as plt</pre>
<ol start="2">
<li>Use the following data file, which includes comma-separated arithmetical data:</li>
</ol>
<pre style="padding-left: 60px">in_file = 'data_multivar.txt'<br/>a = []<br/>b = []<br/>with open(in_file, 'r') as f:<br/>  for line in f.readlines():<br/>    data = [float(x) for x in line.split(',')]<br/>    a.append(data[:-1])<br/>    b.append(data[-1])<br/>a = np.array(a)<br/>b = np.array(b)</pre>
<ol start="3">
<li>Construct a Naive Bayes classifier:</li>
</ol>
<pre style="padding-left: 60px">classification_gaussiannb = GaussianNB()<br/>classification_gaussiannb.fit(a, b)<br/>b_pred = classification_gaussiannb.predict(a)</pre>
<ol start="4">
<li>Calculate the accuracy of Naive Bayes:</li>
</ol>
<pre style="padding-left: 60px">correctness = 100.0 * (b == b_pred).sum() / a.shape[0]<br/>print "correctness of the classification =", round(correctness, 2), "%"</pre>
<ol start="5">
<li>Plot the classifier result:</li>
</ol>
<pre style="padding-left: 60px">def plot_classification(classification_gaussiannb, a , b):<br/>  a_min, a_max = min(a[:, 0]) - 1.0, max(a[:, 0]) + 1.0<br/>  b_min, b_max = min(a[:, 1]) - 1.0, max(a[:, 1]) + 1.0<br/>  step_size = 0.01<br/>  a_values, b_values = np.meshgrid(np.arange(a_min, a_max,   step_size), np.arange(b_min, b_max, step_size))<br/>  mesh_output1 = classification_gaussiannb.predict(np.c_[a_values.ravel(), b_values.ravel()])<br/>  mesh_output2 = mesh_output1.reshape(a_values.shape)<br/>  plt.figure()<br/>  plt.pcolormesh(a_values, b_values, mesh_output2, cmap=plt.cm.gray)<br/>  plt.scatter(a[:, 0], a[:, 1], c=b , s=80, edgecolors='black', linewidth=1,cmap=plt.cm.Paired)</pre>
<ol start="6">
<li>Specify the boundaries of the figure:</li>
</ol>
<pre style="padding-left: 60px">plt.xlim(a_values.min(), a_values.max())<br/>plt.ylim(b_values.min(), b_values.max())<br/><em># specify the ticks on the X and Y axes<br/></em>plt.xticks((np.arange(int(min(a[:, 0])-1), int(max(a[:, 0])+1), 1.0)))<br/>plt.yticks((np.arange(int(min(a[:, 1])-1), int(max(a[:, 1])+1), 1.0)))<br/>plt.show()<br/>plot_classification(classification_gaussiannb, a, b)</pre>
<p><span>The accuracy obtained after executing a Naive Bayes classifier is shown in the following screenshot:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-662 image-border" src="Images/b8202659-17dd-4313-b6a6-afe44e72f943.png" style="width:61.25em;height:4.33em;" width="735" height="52"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>Please refer to the following articles:</p>
<ul>
<li>To get to know how the classifier works with an example refer to the following link:</li>
</ul>
<p style="padding-left: 60px"><a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">https://en.wikipedia.org/wiki/Naive_Bayes_classifier</a></p>
<ul>
<li>To learn more about text classification with the proposed classifier, refer to the following link:</li>
</ul>
<p style="padding-left: 60px"><a href="http://sebastianraschka.com/Articles/2014_naive_bayes_1.html" target="_blank">http://sebastianraschka.com/Articles/2014_naive_bayes_1.html</a></p>
<ul>
<li>To learn more about the Naive Bayes Classification Algorithm, refer to the following link:</li>
</ul>
<p style="padding-left: 60px"><a href="http://software.ucv.ro/~cmihaescu/ro/teaching/AIR/docs/Lab4-NaiveBayes.pdf">http://software.ucv.ro/~cmihaescu/ro/teaching/AIR/docs/Lab4-NaiveBayes.pdf</a></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Logistic regression classifier</h1>
                </header>
            
            <article>
                
<p>This approach can be chosen where the output can take only two values, 0 or 1, pass/fail, win/lose, alive/dead, or healthy/sick, and so on. In cases where the dependent variable has more than two outcome categories, it may be analyzed using multinomial logistic regression.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>After installing the essential packages, let's construct some training labels:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>from sklearn import linear_model<br/>import matplotlib.pyplot as plt<br/>a = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])<br/>b = np.array([1, 1, 1, 2, 2, 2])</pre>
<ol start="2">
<li>Initiate the classifier:</li>
</ol>
<pre style="padding-left: 60px">classification = linear_model.LogisticRegression(solver='liblinear', C=100)<br/>classification.fit(a, b)</pre>
<ol start="3">
<li>Sketch datapoints and margins:</li>
</ol>
<pre style="padding-left: 60px">def plot_classification(classification, a , b):<br/>  a_min, a_max = min(a[:, 0]) - 1.0, max(a[:, 0]) + 1.0<br/>  b_min, b_max = min(a[:, 1]) - 1.0, max(a[:, 1]) + 1.0 step_size = 0.01<br/>  a_values, b_values = np.meshgrid(np.arange(a_min, a_max, step_size), np.arange(b_min, b_max, step_size))<br/>  mesh_output1 = classification.predict(np.c_[a_values.ravel(), b_values.ravel()])<br/>  mesh_output2 = mesh_output1.reshape(a_values.shape)<br/>  plt.figure()<br/>  plt.pcolormesh(a_values, b_values, mesh_output2, cmap=plt.cm.gray)<br/>  plt.scatter(a[:, 0], a[:, 1], c=b , s=80, edgecolors='black',linewidth=1,cmap=plt.cm.Paired)<br/><strong><br/>  # specify the boundaries of the figure<br/></strong>  plt.xlim(a_values.min(), a_values.max())<br/>  plt.ylim(b_values.min(), b_values.max())<br/><strong><br/>  # specify the ticks on the X and Y axes<br/></strong>  plt.xticks((np.arange(int(min(a[:, 0])-1), int(max(a[:, 0])+1), 1.0)))<br/>  plt.yticks((np.arange(int(min(a[:, 1])-1), int(max(a[:, 1])+1), 1.0)))<br/>  plt.show()<br/>  plot_classification(classification, a, b)</pre>
<p>The command to execute logistic regression is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-663 image-border" src="Images/3fb38bc6-ec53-4e9a-b132-b2a9359e8284.png" style="width:43.25em;height:2.25em;" width="603" height="31"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Splitting the dataset for training and testing</h1>
                </header>
            
            <article>
                
<p>Splitting helps to partition the dataset into training and testing sequences.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Add the following code fragment into the same Python file:</li>
</ol>
<pre style="padding-left: 60px">from sklearn import cross_validation<br/>from sklearn.naive_bayes import GaussianNB<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>in_file = 'data_multivar.txt'<br/>a = []<br/>b = []<br/>with open(in_file, 'r') as f:<br/>  for line in f.readlines():<br/>    data = [float(x) for x in line.split(',')]<br/>    a.append(data[:-1])<br/>    b.append(data[-1])<br/>a = np.array(a)<br/>b = np.array(b)</pre>
<ol start="2">
<li>Allocate 75% of data for training and 25% of data for testing:</li>
</ol>
<pre style="padding-left: 60px">a_training, a_testing, b_training, b_testing = cross_validation.train_test_split(a, b, test_size=0.25, random_state=5)<br/>classification_gaussiannb_new = GaussianNB()<br/>classification_gaussiannb_new.fit(a_training, b_training)</pre>
<ol start="3">
<li>Evaluate the classifier performance on test data:</li>
</ol>
<pre style="padding-left: 60px">b_test_pred = classification_gaussiannb_new.predict(a_testing)</pre>
<ol start="4">
<li>Compute the accuracy of the classifier system:</li>
</ol>
<pre style="padding-left: 60px">correctness = 100.0 * (b_testing == b_test_pred).sum() / a_testing.shape[0]<br/>print "correctness of the classification =", round(correctness, 2), "%"</pre>
<ol start="5">
<li>Plot the datapoints and the boundaries for test data:</li>
</ol>
<pre style="padding-left: 60px">def plot_classification(classification_gaussiannb_new, a_testing , b_testing):<br/>  a_min, a_max = min(a_testing[:, 0]) - 1.0, max(a_testing[:, 0]) + 1.0<br/>  b_min, b_max = min(a_testing[:, 1]) - 1.0, max(a_testing[:, 1]) + 1.0<br/>  step_size = 0.01<br/>  a_values, b_values = np.meshgrid(np.arange(a_min, a_max, step_size), np.arange(b_min, b_max, step_size))<br/>  mesh_output = classification_gaussiannb_new.predict(np.c_[a_values.ravel(), b_values.ravel()])<br/>  mesh_output = mesh_output.reshape(a_values.shape)<br/>  plt.figure()<br/>  plt.pcolormesh(a_values, b_values, mesh_output, cmap=plt.cm.gray)<br/>  plt.scatter(a_testing[:, 0], a_testing[:, 1], c=b_testing , s=80, edgecolors='black', linewidth=1,cmap=plt.cm.Paired)<br/><strong>  # specify the boundaries of the figure<br/></strong>  plt.xlim(a_values.min(), a_values.max())<br/>  plt.ylim(b_values.min(), b_values.max())<br/>  # specify the ticks on the X and Y axes<br/>  plt.xticks((np.arange(int(min(a_testing[:, 0])-1), int(max(a_testing[:, 0])+1), 1.0)))<br/>  plt.yticks((np.arange(int(min(a_testing[:, 1])-1), int(max(a_testing[:, 1])+1), 1.0)))<br/>  plt.show()<br/>plot_classification(classification_gaussiannb_new, a_testing, b_testing)</pre>
<p><span>The accuracy obtained while splitting the dataset is shown in the following screenshot:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-664 image-border" src="Images/b51cfabd-087e-4e04-b172-47370b19c295.png" style="width:60.58em;height:13.25em;" width="727" height="159"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Evaluating the accuracy using cross-validation</h1>
                </header>
            
            <article>
                
<p>Cross-validation is essential in machine learning. Initially, we split the datasets into a train set and a test set. Next, in order to construct a robust classifier, we repeat this procedure, but we need to avoid overfitting the model. Overfitting indicates that we get excellent prediction results for the train set, but very poor results for the test set. Overfitting causes poor generalization of the model.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Import the packages:</li>
</ol>
<pre style="padding-left: 60px">from sklearn import cross_validation<br/>from sklearn.naive_bayes import GaussianNB<br/>import numpy as np<br/>in_file = 'cross_validation_multivar.txt'<br/>a = []<br/>b = []<br/>with open(in_file, 'r') as f:<br/>  for line in f.readlines():<br/>    data = [float(x) for x in line.split(',')]<br/>    a.append(data[:-1])<br/>    b.append(data[-1])<br/>a = np.array(a)<br/>b = np.array(b)<br/>classification_gaussiannb = GaussianNB()</pre>
<ol start="2">
<li>Compute the accuracy of the classifier:</li>
</ol>
<pre style="padding-left: 60px">num_of_validations = 5<br/>accuracy = cross_validation.cross_val_score(classification_gaussiannb, a, b, scoring='accuracy', cv=num_of_validations)<br/>print "Accuracy: " + str(round(100* accuracy.mean(), 2)) + "%"<br/>f1 = cross_validation.cross_val_score(classification_gaussiannb, a, b, scoring='f1_weighted', cv=num_of_validations)<br/>print "f1: " + str(round(100*f1.mean(), 2)) + "%"<br/>precision = cross_validation.cross_val_score(classification_gaussiannb,a, b, scoring='precision_weighted', cv=num_of_validations)<br/>print "Precision: " + str(round(100*precision.mean(), 2)) + "%"<br/>recall = cross_validation.cross_val_score(classification_gaussiannb, a, b, scoring='recall_weighted', cv=num_of_validations)<br/>print "Recall: " + str(round(100*recall.mean(), 2)) + "%"</pre>
<ol start="3">
<li>The result obtained after executing cross-validation is shown as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="Images/f4e29007-8020-462b-9e3c-9e20de11f795.png" width="1285" height="176"/></div>
<p>In order to know how it works on a given sentence dataset, refer to the following:</p>
<ul>
<li>Introduction to logistic regression:</li>
</ul>
<p style="padding-left: 60px"><a href="https://machinelearningmastery.com/logistic-regression-for-machine-learning/" target="_blank">https://machinelearningmastery.com/logistic-regression-for-machine-learning/</a></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Analyzing the sentiment of a sentence</h1>
                </header>
            
            <article>
                
<p>Sentiment analysis refers to procedures of finding whether a specified part of text is positive, negative, or neutral. This technique is frequently considered to find out how people think about a particular situation. It evaluates the sentiments of consumers in different forms, such as advertising campaigns, social media, and e-commerce customers.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Create a new file and import the chosen packages:</li>
</ol>
<pre style="padding-left: 60px">import nltk.classify.util<br/>from nltk.classify import NaiveBayesClassifier<br/>from nltk.corpus import movie_reviews</pre>
<ol start="2">
<li>Describe a function to extract features:</li>
</ol>
<pre style="padding-left: 60px">def collect_features(word_list):<br/>  word = []<br/>  return dict ([(word, True) for word in word_list])</pre>
<ol start="3">
<li>Adopt movie reviews in NLTK as training data:</li>
</ol>
<pre style="padding-left: 60px">if __name__=='__main__':<br/>  plus_filenum = movie_reviews.fileids('pos')<br/>  minus_filenum = movie_reviews.fileids('neg')</pre>
<ol start="4">
<li>Divide the data into positive and negative reviews:</li>
</ol>
<pre style="padding-left: 60px">  feature_pluspts = [(collect_features(movie_reviews.words(fileids=[f])),<br/>'Positive') for f in plus_filenum]<br/>  feature_minuspts = [(collect_features(movie_reviews.words(fileids=[f])),<br/>'Negative') for f in minus_filenum]</pre>
<ol start="5">
<li>Segregate the data into training and testing datasets:</li>
</ol>
<pre style="padding-left: 60px">  threshold_fact = 0.8<br/>  threshold_pluspts = int(threshold_fact * len(feature_pluspts))<br/>  threshold_minuspts = int(threshold_fact * len(feature_minuspts))</pre>
<ol start="6">
<li>Extract the features:</li>
</ol>
<pre style="padding-left: 60px">  feature_training = feature_pluspts[:threshold_pluspts] + feature_minuspts[:threshold_minuspts]<br/>  feature_testing = feature_pluspts[threshold_pluspts:] + feature_minuspts[threshold_minuspts:]<br/>  print "nNumber of training datapoints:", len(feature_training)<br/>  print "Number of test datapoints:", len(feature_testing)</pre>
<ol start="7">
<li>Consider the Naive Bayes classifier and train it with an assigned objective:</li>
</ol>
<pre style="padding-left: 60px">  # Train a Naive Bayes classifiers<br/>  classifiers = NaiveBayesClassifier.train(feature_training)<br/>  print "nAccuracy of the classifiers:",nltk.classify.util.accuracy(classifiers,feature_testing)<br/>  print "nTop 10 most informative words:"<br/>  for item in classifiers.most_informative_features()[:10]:print item[0]<br/><strong>  # Sample input reviews<br/></strong>  in_reviews = [<br/>  "The Movie was amazing",<br/>  "the movie was dull. I would never recommend it to anyone.",<br/>  "The cinematography is pretty great in the movie",<br/>  "The direction was horrible and the story was all over the place"<br/>  ]<br/>  print "nPredictions:"<br/>  for review in in_reviews:<br/>    print "nReview:", review<br/>  probdist = classifiers.prob_classify(collect_features(review.split()))<br/>  predict_sentiment = probdist.max()<br/>  print "Predicted sentiment:", predict_sentiment<br/>  print "Probability:", round(probdist.prob(predict_sentiment), 2)</pre>
<ol start="8">
<li>The result obtained for sentiment analysis is shown as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-665 image-border" src="Images/fcd24c1b-a62b-4009-9d67-f11bffbc3b22.png" style="width:27.25em;height:22.42em;" width="646" height="531"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Identifying patterns in text using topic modeling</h1>
                </header>
            
            <article>
                
<p>The theme modeling refers to the procedure of recognizing hidden patterns in manuscript information. The objective is to expose some hidden thematic configuration in a collection of documents.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Import the following packages:</li>
</ol>
<pre style="padding-left: 60px">from nltk.tokenize import RegexpTokenizer<br/>from nltk.stem.snowball import SnowballStemmer<br/>from gensim import models, corpora<br/>from nltk.corpus import stopwords</pre>
<ol start="2">
<li>Load the input data:</li>
</ol>
<pre style="padding-left: 60px">def load_words(in_file):<br/>  element = []<br/>  with open(in_file, 'r') as f:<br/>    for line in f.readlines():<br/>      element.append(line[:-1])<br/>  return element</pre>
<ol start="3">
<li>Class to pre-process text:</li>
</ol>
<pre style="padding-left: 60px">classPreprocedure(object):<br/>  def __init__(self):<br/><strong>    # Create a regular expression tokenizer<br/></strong>    self.tokenizer = RegexpTokenizer(r'w+')</pre>
<ol start="4">
<li>Obtain a list of stop words to terminate the program execution:</li>
</ol>
<pre style="padding-left: 60px">    self.english_stop_words= stopwords.words('english')</pre>
<ol start="5">
<li>Create a Snowball stemmer:</li>
</ol>
<pre style="padding-left: 60px">    self.snowball_stemmer = SnowballStemmer('english')  </pre>
<ol start="6">
<li>Define a function to perform tokenizing, stop word removal, and stemming:</li>
</ol>
<pre style="padding-left: 60px">  def procedure(self, in_data):<br/><strong># Tokenize the string<br/></strong>    token = self.tokenizer.tokenize(in_data.lower())</pre>
<ol start="7">
<li>Eliminate stop words from the text:</li>
</ol>
<pre style="padding-left: 60px">    tokenized_stopwords = [x for x in token if not x in self.english_stop_words]</pre>
<ol start="8">
<li>Implement stemming on the tokens:</li>
</ol>
<pre style="padding-left: 60px">    token_stemming = [self.snowball_stemmer.stem(x) for x in tokenized_stopwords]</pre>
<ol start="9">
<li>Return the processed tokens:</li>
</ol>
<pre style="padding-left: 60px">    return token_stemming</pre>
<ol start="10">
<li>Load the input data from the <kbd>main</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">if __name__=='__main__':<br/><strong>  # File containing input data<br/></strong>  in_file = 'data_topic_modeling.txt'<br/><strong>  # Load words<br/></strong>  element = load_words(in_file)</pre>
<ol start="11">
<li>Create an object:</li>
</ol>
<pre style="padding-left: 60px">  preprocedure = Preprocedure()</pre>
<ol start="12">
<li>Process the file and extract the tokens:</li>
</ol>
<pre style="padding-left: 60px">  processed_tokens = [preprocedure.procedure(x) for x in element]</pre>
<ol start="13">
<li>Create a dictionary based on the tokenized documents:</li>
</ol>
<pre style="padding-left: 60px">  dict_tokens = corpora.Dictionary(processed_tokens)<br/>  corpus = [dict_tokens.doc2bow(text) for text in processed_tokens]</pre>
<ol start="14">
<li>Develop an LDA model, define required parameters, and initialize the LDA objective:</li>
</ol>
<pre style="padding-left: 60px">  num_of_topics = 2<br/>  num_of_words = 4<br/>  ldamodel = models.ldamodel.LdaModel(corpus,num_topics=num_of_topics, id2word=dict_tokens, passes=25)<br/>  print "Most contributing words to the topics:"<br/>  for item in ldamodel.print_topics(num_topics=num_of_topics, num_words=num_of_words):<br/>    print "nTopic", item[0], "==&gt;", item[1]</pre>
<ol start="15">
<li>The result obtained when <kbd>topic_modelling.py</kbd> is executed is shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-666 image-border" src="Images/453ec4ce-69e2-4397-b09f-db674eaeb13b.png" style="width:43.50em;height:7.42em;" width="709" height="121"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Applications of sentiment analysis</h1>
                </header>
            
            <article>
                
<p>Sentiment analysis is used in social media such as Facebook and Twitter, to find the sentiments (positive/negative) of the general public over an issue. They are also used to establish the sentiments of people regarding advertisements and how people feel about your product, brand, or service.</p>


            </article>

            
        </section>
    </div>



  </body></html>