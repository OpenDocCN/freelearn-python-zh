- en: Scraping the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we built a crawler which follows links to download
    the web pages we want. This is interesting but not useful-the crawler downloads
    a web page, and then discards the result. Now, we need to make this crawler achieve
    something by extracting data from each web page, which is known as **scraping**.
  prefs: []
  type: TYPE_NORMAL
- en: We will first cover browser tools to examine a web page, which you may already
    be familiar with if you have a web development background. Then, we will walk
    through three approaches to extract data from a web page using regular expressions,
    Beautiful Soup and lxml. Finally, the chapter will conclude with a comparison
    of these three scraping alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing a web page
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approaches to scrape a web page
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the console
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: xpath selectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scraping results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing a web page
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand how a web page is structured, we can try examining the source
    code. In most web browsers, the source code of a web page can be viewed by right-clicking
    on the page and selecting the View page source option:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4364OS_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For our example website, the data we are interested in is found on the country
    pages. Take a look at page source (via browser menu or right click browser menu).
    In the source for the example page for the United Kingdom ([http://example.webscraping.com/view/United-Kingdom-239](http://example.webscraping.com/view/United-Kingdom-239))
    you will find a table containing the country data (you can use search to find
    this in the page source code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The lack of white space and formatting is not an issue for a web browser to
    interpret, but it is difficult for us to read. To help us interpret this table,
    we can use browser tools. To find your browser's developer tools, you can usually
    simply right click and select an option like Developer Tools. Depending on the
    browser you use, you may have different developer tool options, but nearly every
    browser will have a tab titled Elements or HTML. In Chrome and Firefox, you can
    simply right click on an element on the page (what you are interested in scraping)
    and select Inspect Element. For Internet Explorer, you need to open the Developer toolbar
    by pressing *F12*. Then you can select items by clicking *Ctrl *+ *B*. If you
    use a different browser without built-in developer tools, you may want to try the
    Firebug Lite extension, which is available for most web browsers at [https://getfirebug.com/firebuglite](https://getfirebug.com/firebuglite).
  prefs: []
  type: TYPE_NORMAL
- en: 'When I right click on the table on the page and click Inspect Element using
    Chrome, I see the following open panel with the surrounding HTML hierarchy of
    the selected element:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/chrome_inspect.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this screenshot, I can see that the `table` element sits inside a `form`
    element. I can also see that the attributes for the country are included in `tr` 
    or table row elements with different CSS IDs (shown via the `id="places_national_flag__row"`).
    Depending on your browser, the coloring or layout might be different, but you
    should be able to click on the elements and navigate through the hierarchy to
    see the data on the page. If I expand the `tr` elements further by clicking on
    the arrows next to them, I notice the data for each of these rows is included
    is included within a `<td>` element of class `w2p_fw`, which is the child of a
    `<tr>` element, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/expanded_elements.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have investigated the page with our browser tools, we know the HTML
    hierarchy of the country data table, and have the necessary information to scrape
    that data from the page.
  prefs: []
  type: TYPE_NORMAL
- en: Three approaches to scrape a web page
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we understand the structure of this web page we will investigate three
    different approaches to scraping its data, first with regular expressions, then
    with the popular `BeautifulSoup` module, and finally with the powerful `lxml`
    module.
  prefs: []
  type: TYPE_NORMAL
- en: Regular expressions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are unfamiliar with regular expressions or need a reminder, there is
    a thorough overview available at [https://docs.python.org/3/howto/regex.html](https://docs.python.org/3/howto/regex.html).
    Even if you use regular expressions (or regex) with another programming language,
    I recommend stepping through it for a refresher on regex with Python.
  prefs: []
  type: TYPE_NORMAL
- en: Because each chapter might build or use parts of previous chapters, we recommend
    setting up your file structure similar to that in [the book repository](https://github.com/kjam/wswp).
    All code can then be run from the `code` directory in the repository so imports
    work properly. If you would like to set up a different structure, note that you
    will need to change all imports from other chapters (such as the `from chp1.advanced_link_crawler `
    in the following code).
  prefs: []
  type: TYPE_NORMAL
- en: 'To scrape the country area using regular expressions, we will first try matching
    the contents of the `<td>` element, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This result shows that the `<td class="w2p_fw">` tag is used for multiple country
    attributes. If we simply wanted to scrape the country area, we can select the
    second matching element, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This solution works but could easily fail if the web page is updated. Consider
    if this table is changed and the area is no longer in the second matching element.
    If we just need to scrape the data now, future changes can be ignored. However,
    if we want to re-scrape this data at some point, we want our solution to be as
    robust against layout changes as possible. To make this regular expression more
    specific, we can include the parent `<tr>` element, which has an ID, so it ought
    to be unique:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This iteration is better; however, there are many other ways the web page could
    be updated in a way that still breaks the regular expression. For example, double
    quotation marks might be changed to single, extra spaces could be added between
    the `<td>` tags, or the `area_label` could be changed. Here is an improved version
    to try and support these various possibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This regular expression is more future-proof but is difficult to construct,
    and quite unreadable. Also, there are still plenty of other minor layout changes
    that would break it, such as if a title attribute was added to the `<td>` tag
    or if the `tr` or `td` elements changed their CSS classes or IDs.
  prefs: []
  type: TYPE_NORMAL
- en: From this example, it is clear that regular expressions provide a quick way
    to scrape data but are too brittle and easily break when a web page is updated.
    Fortunately, there are better data extraction solutions such as the other scraping
    libraries we will cover throughout this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Beautiful Soup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Beautiful Soup** is a popular library that parses a web page and provides
    a convenient interface to navigate content. If you do not already have this module,
    the latest version can be installed using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step with Beautiful Soup is to parse the downloaded HTML into a soup
    document. Many web pages do not contain perfectly valid HTML and Beautiful Soup
    needs to correct improper open and close tags. For example, consider this simple
    web page containing a list with missing attribute quotes and closing tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If the `Population` item is interpreted as a child of the `Area` item instead
    of the list, we could get unexpected results when scraping. Let us see how Beautiful
    Soup handles this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that using the default `html.parser` did not result in properly
    parsed HTML. We can see from the previous snippet that it has used nested `li`
    elements, which might make it difficult to navigate. Luckily there are more options
    for parsers. We can install **LXML** (as described in the next section) or we
    can also use **html5lib**. To install **html5lib**, simply use pip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can repeat this code, changing only the parser like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here, `BeautifulSoup` using `html5lib` was able to correctly interpret the missing
    attribute quotes and closing tags, as well as add the `<html>` and `<body>` tags
    to form a complete HTML document. You should see similar results if you used `lxml`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can navigate to the elements we want using the `find()` and `find_all()`
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: For a full list of available methods and parameters, the official Beautiful
    Soup documentation is available at [http://www.crummy.com/software/BeautifulSoup/bs4/doc/](http://www.crummy.com/software/BeautifulSoup/bs4/doc/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, using these techniques, here is a full example to extract the country
    area from our example website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This code is more verbose than regular expressions but easier to construct and
    understand. Also, we no longer need to worry about problems in minor layout changes,
    such as extra white space or tag attributes. We also know if the page contains
    broken HTML that Beautiful Soup can help clean the page and allow us to extract
    data from very broken website code.
  prefs: []
  type: TYPE_NORMAL
- en: Lxml
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Lxml** is a Python library built on top of the `libxml2` XML parsing library
    written in C, which helps make it faster than Beautiful Soup but also harder to
    install on some computers, specifically Windows. The latest installation instructions
    are available at [http://lxml.de/installation.html](http://lxml.de/installation.html).
    If you run into difficulties installing the library on your own, you can also
    use Anaconda to do so:  [https://anaconda.org/anaconda/lxml](https://anaconda.org/anaconda/lxml).'
  prefs: []
  type: TYPE_NORMAL
- en: If you are unfamiliar with Anaconda, it is a package and environment manager
    primarily focused on open data science packages built by the folks at Continuum
    Analytics. You can download and install Anaconda by following their setup instructions
    here: [https://www.continuum.io/downloads](https://www.continuum.io/downloads).
    Note that using the Anaconda quick install will set your `PYTHON_PATH` to the
    Conda installation of Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with Beautiful Soup, the first step when using `lxml` is parsing the potentially
    invalid HTML into a consistent format. Here is an example of parsing the same
    broken HTML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As with `BeautifulSoup`, `lxml` was able to correctly parse the missing attribute
    quotes and closing tags, although it did not add the `<html>` and `<body>` tags.
    These are not requirements for standard XML and so are unnecessary for `lxml`
    to insert.
  prefs: []
  type: TYPE_NORMAL
- en: 'After parsing the input, `lxml` has a number of different options to select
    elements, such as XPath selectors and a `find()` method similar to Beautiful Soup.
    Instead, we will use CSS selectors here, because they are more compact and can
    be reused later in [Chapter 5](py-web-scrp-2e_ch05.html), *Dynamic Content* when
    parsing dynamic content. Some readers will already be familiar with them from
    their experience with jQuery selectors or use in front-end web application development.
    Later in this chapter we will compare performance of these selectors with XPath.
    To use CSS selectors, you might need to install the `cssselect` library like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can use the `lxml` CSS selectors to extract the area data from the example
    page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: By using the `cssselect` method on our tree, we can utilize CSS syntax to select a
    table row element with the `places_area__row` ID, and then the child table data
    tag with the `w2p_fw` class. Since `cssselect` returns a list, we then index the
    first result and call the `text_content` method, which will iterate over all child
    elements and return concatenated text of each element. In this case, we only have
    one element, but this functionality is useful to know for more complex extraction
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see this code and the other code for this chapter in the book code
    repository: [https://github.com/kjam/wswp/blob/master/code/chp2.](https://github.com/kjam/wswp/blob/master/code/chp2.)'
  prefs: []
  type: TYPE_NORMAL
- en: CSS selectors and your Browser Console
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like the notation we used to extract using `cssselect`, CSS selectors are patterns
    used for selecting HTML elements. Here are some examples of common selectors you
    should know:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `cssselect` library implements most CSS3 selectors, and details on unsupported
    features (primarily browser interactions) are available at [https://cssselect.readthedocs.io/en/latest/#supported-selectors](https://cssselect.readthedocs.io/en/latest/#supported-selectors).
  prefs: []
  type: TYPE_NORMAL
- en: The CSS3 specification was produced by the W3C and is available for viewing
    at [http://www.w3.org/TR/2011/REC-css3-selectors-20110929/](http://www.w3.org/TR/2011/REC-css3-selectors-20110929/).
    There is also a useful and more accessible documentation from Mozilla on their
    developer's reference for CSS: [https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors ](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors)
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes it is useful to test CSS selectors as we might not write them perfectly
    the first time. It is also a good idea to test them somewhere to debug any selection
    issues before writing many lines of Python code which may or may not work.
  prefs: []
  type: TYPE_NORMAL
- en: When a site uses JQuery, it's very easy to test CSS Selectors in the browser
    console. The console is a part of your browser developer tools and allows you
    to execute JavaScript (and, if supported, JQuery) on the current page.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about JQuery, there are several free online courses. The Code
    School course at [http://try.jquery.com/](http://try.jquery.com/) has a variety
    of exercises if you are interested in diving a bit deeper.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only syntax you need to know for using CSS selectors with JQuery is the
    simple object selection (i.e. `$(''div.class_name'');`). JQuery uses the `$` and
    parenthesis to select objects. Within the parenthesis you can write any CSS selector.
    Doing so in your browser console on a site that supports JQuery will allow you
    to look at the objects you have selected. Since we know the example website uses
    JQuery (either by inspecting the source code, or watching the Network tab and
    looking for JQuery to load, or using the `detectem` module), we can try selecting
    all `tr` elements using a CSS selector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/console.png)'
  prefs: []
  type: TYPE_IMG
- en: And simply by using the tag name, I can see every row for the country data.
    I can also try selecting elements using a longer CSS selector. Let's try selecting
    all `td` elements with class `w2p_fw`, since I know this is where the primary
    data on the page lies.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/td_console.png)'
  prefs: []
  type: TYPE_IMG
- en: You may also notice that when using your mouse to click on the returned elements,
    you can expand them and also highlight them in the above window (depending on
    what browser you are using). This is a tremendously useful way to test data. If
    the site you are scraping doesn't load JQuery or any other selector friendly libraries
    from your browser, you can perform the same lookups with the `document` object
    using simple JavaScript. The documentation for the `querySelector` method is available
    on **Mozilla Developer Network**: [https://developer.mozilla.org/en-US/docs/Web/API/Document/querySelector](https://developer.mozilla.org/en-US/docs/Web/API/Document/querySelector).
  prefs: []
  type: TYPE_NORMAL
- en: Even after using CSS selectors in your console and with `lxml`, it can be useful
    to learn XPath, which is what `lxml` converts all of your CSS selectors to before
    evaluating them. To keep learning how to use XPath, read on!
  prefs: []
  type: TYPE_NORMAL
- en: XPath Selectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are times when using CSS selectors will not work. This is especially the
    case with very broken HTML or improperly formatted elements. Despite the best
    efforts of libraries like `BeautifulSoup` and `lxml` to properly parse and clean
    up the code; it will not always work - and in these cases, XPath can help you
    build very specific selectors based on hierarchical relationships of elements
    on the page.
  prefs: []
  type: TYPE_NORMAL
- en: XPath is a way of describing relationships as an hierarchy in XML documents.
    Because HTML is formed using XML elements, we can also use XPath to navigate and
    select elements from an HTML document.
  prefs: []
  type: TYPE_NORMAL
- en: To read more about XPath, check out the **Mozilla developer documentation**: [https://developer.mozilla.org/en-US/docs/Web/XPath](https://developer.mozilla.org/en-US/docs/Web/XPath).
  prefs: []
  type: TYPE_NORMAL
- en: XPath follows some basic syntax rules and has some similarities with CSS selectors.
    Take a look at the following chart for some quick references between the two.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Selector description** | **XPath Selector** | **CSS selector** |'
  prefs: []
  type: TYPE_TB
- en: '| Select all links | ''//a'' | ''a'' |'
  prefs: []
  type: TYPE_TB
- en: '| Select div with class "main" | ''//div[@class="main"]'' | ''div.main'' |'
  prefs: []
  type: TYPE_TB
- en: '| Select ul with ID "list" | ''//ul[@id="list"]'' | ''ul#list'' |'
  prefs: []
  type: TYPE_TB
- en: '| Select text from all paragraphs | ''//p/text()'' | ''p''* |'
  prefs: []
  type: TYPE_TB
- en: '| Select all divs which contain ''test'' in the class | ''//div[contains(@class,
    ''test'')]'' | ''div [class*="test"]'' |'
  prefs: []
  type: TYPE_TB
- en: '| Select all divs with links or lists in them | ''//div[a&#124;ul] '' | ''div
    a, div ul'' |'
  prefs: []
  type: TYPE_TB
- en: '| Select a link with google.com in the href | ''//a[contains(@href, "google.com")]
    | ''a''* |'
  prefs: []
  type: TYPE_TB
- en: As you can see from the previous table, there are many similarities between
    the syntax. However, in the chart there are certain CSS selectors noted with a
    `*`. These indicate that it is not exactly possible to select these elements using
    CSS, and we have provided the best alternative. In these cases, if you were using `cssselect` you
    will need to do further manipulation or iteration within Python and/or `lxml`.
    Hopefully this comparison has shown an introduction to XPath and convinced you
    that it is more exacting and specific than simply using CSS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a basic introduction to the XPath syntax, let''s see how we
    can use it for our example website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Similar to CSS selectors, you can also test XPath selectors in your browser
    console. To do so, on a page with selectors simply use the `$x('pattern_here');`
    selector. Similarly, you can also use the `document` object from simple JavaScript
    and call the `evaluate` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Mozilla developer network has a useful introduction to using XPath with
    JavaScript tutorial here:  [https://developer.mozilla.org/en-US/docs/Introduction_to_using_XPath_in_JavaScript](https://developer.mozilla.org/en-US/docs/Introduction_to_using_XPath_in_JavaScript)'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we wanted to test looking for `td` elements with images in them to get the
    flag data from the country pages, we could test our XPath pattern in our browser
    first:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/xpath_console.png)'
  prefs: []
  type: TYPE_IMG
- en: Here we can see that we can use attributes to specify the data we want to extract
    (such as `@src`). By testing in the browser, we save debugging time by getting
    immediate and easy-to-read results.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using both XPath and CSS selectors throughout this chapter and further
    chapters, so you can become more familiar with them and feel confident using them
    as you advance your web scraping capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: LXML and Family Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`lxml` also has the ability to traverse family trees within the HTML page.
    What is a family tree? When you used your browser''s developer tools to investigate
    the elements on the page and you were able to expand or retract them, you were
    observing family relationships in the HTML. Every element on a web page can have
    parents, siblings and children. These relationships can help us more easily traverse
    the page.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, if I want to find all the elements at the same node depth level
    on the page, I would be looking for their siblings. Or maybe I want every element
    that is a child of a particular element on the page. `lxml` allows us to use many
    of these relationships with simple Python code.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let''s investigate all children of the `table` element on the
    example page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also see the table''s siblings and parent elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: If you need a more general way to access elements on the page, traversing the
    familial relationships combined with XPath expressions is a good way to ensure
    you don't miss any content. This can help you extract content from many different
    types of pages where you might be able to identify some important parts of the
    page simply by identifying content that appears near those elements on the page.
    This method will also work even when the elements do not have identifiable CSS
    selectors.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To help evaluate the trade-offs between the three scraping approaches described
    in the section, *Three approaches to scrape a web page*, it would be helpful to
    compare their relative efficiency. Typically, a scraper would extract multiple
    fields from a web page. So, for a more realistic comparison, we will implement
    extended versions of each scraper which extract all the available data from a
    country''s web page. To get started, we need to return to our browser to check
    the format of the other country features, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4364OS_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'By using our browser''s inspect capabilities, we can see each table row has
    an ID starting with `places_` and ending with `__row`. The country data is contained
    within these rows in the same format as the area example. Here are implementations
    that use this information to extract all of the available country data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Scraping results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have complete implementations for each scraper, we will test their
    relative performance with this snippet. The imports in the code expect your directory
    structure to be similar to the book''s repository, so please adjust as necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This example will run each scraper 1000 times, check whether the scraped results
    are as expected, and then print the total time taken. The `download` function
    used here is the one defined in the preceding chapter. Note the highlighted line
    calling `re.purge()`; by default, the regular expression module will cache searches
    and this cache needs to be cleared to make a fair comparison with the other scraping
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the results from running this script on my computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The results on your computer will quite likely be different because of the different
    hardware used. However, the relative difference between each approach should be
    similar. The results show Beautiful Soup is over six times slower than the other
    approaches when used to scrape our example web page. This result could be anticipated
    because `lxml` and the regular expression module were written in C, while `BeautifulSoup`
    is pure Python. An interesting fact is that `lxml` performed comparatively well
    with regular expressions, since `lxml` has the additional overhead of having to
    parse the input into its internal format before searching for elements. When scraping
    many features from a web page, this initial parsing overhead is reduced and `lxml`
    becomes even more competitive. As we can see with the XPath parser, `lxml` is
    able to directly compete with regular expressions. It really is an amazing module!
  prefs: []
  type: TYPE_NORMAL
- en: Although we strongly encourage you to use `lxml` for parsing, the biggest performance
    bottleneck for web scraping is usually the network. We will discuss approaches
    to parallelize workflows, allowing you to increase the speed of your crawlers
    by having multiple requests work in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of Scraping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following table summarizes the advantages and disadvantages of each approach
    to scraping:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Scraping approach** | **Performance** | **Ease of use** | **Ease to install**
    |'
  prefs: []
  type: TYPE_TB
- en: '| Regular expressions | Fast | Hard | Easy (built-in module) |'
  prefs: []
  type: TYPE_TB
- en: '| Beautiful Soup | Slow | Easy | Easy (pure Python) |'
  prefs: []
  type: TYPE_TB
- en: '| Lxml | Fast | Easy | Moderately difficult |'
  prefs: []
  type: TYPE_TB
- en: If speed is not an issue to you and you prefer to only install libraries via
    pip, it would not be a problem to use a slower approach, such as Beautiful Soup.
    Or, if you just need to scrape a small amount of data and want to avoid additional
    dependencies, regular expressions might be an appropriate choice. However, in
    general, `lxml` is the best choice for scraping, because it is fast and robust,
    while regular expressions and Beautiful Soup are not as speedy or as easy to modify.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a scrape callback to the link crawler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we know how to scrape the country data, we can integrate this into
    the link crawler built in [Chapter 1](py-web-scrp-2e_ch01.html), *Introduction
    to Web Scraping*. To allow reusing the same crawling code to scrape multiple websites,
    we will add a `callback` parameter to handle the scraping. A `callback` is a function
    that will be called after certain events (in this case, after a web page has been
    downloaded). This scrape `callback` will take a `url` and `html` as parameters
    and optionally return a list of further URLs to crawl. Here is the implementation,
    which is simple in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The new code for the scraping `callback` function are highlighted in the preceding
    snippet, and the full source code for this version of the link crawler is available
    at [https://github.com/kjam/wswp/blob/master/code/chp2/advanced_link_crawler.py](https://github.com/kjam/wswp/blob/master/code/chp2/advanced_link_crawler.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, this crawler can be used to scrape multiple websites by customizing the
    function passed to `scrape_callback`. Here is a modified version of the `lxml`
    example scraper that can be used for the `callback` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This `callback` function will scrape the country data and print it out. We
    can test it by importing the two functions and calling them with our regular expression
    and URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You should now see output showing the downloading of pages as well as some
    rows showing the URL and scraped data, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Usually, when scraping a website, we want to reuse the data rather than simply
    print it, so we will extend this example to save results to a CSV spreadsheet,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: To build this `callback`, a class was used instead of a function so that the
    state of the `csv` writer could be maintained. This `csv` writer is instantiated
    in the constructor, and then written to multiple times in the `__call__` method.
    Note that `__call__` is a special method that is invoked when an object is "called"
    as a function, which is how the `cache_callback` is used in the link crawler.
    This means that `scrape_callback(url, html)` is equivalent to calling `scrape_callback.__call__(url,
    html)`. For further details on Python's special class methods, refer to [https://docs.python.org/3/reference/datamodel.html#special-method-names](https://docs.python.org/3/reference/datamodel.html#special-method-names).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how to pass this callback to the link crawler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the `CsvCallback` expects there to be a `data` directory on the same
    level as the parent folder from where you are running the code. This can also
    be modified, but we advise you to follow good coding practices and keep your code
    and data separate -- allowing you to keep your code under version control while
    having your `data` folder in the `.gitignore` file. Here''s an example directory
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when the crawler is run with this `scrape_callback`, it will save results
    to a CSV file that can be viewed in an application such as Excel or LibreOffice.
    It might take a bit longer to run than the first time, as it is actively collecting
    information. When the scraper exits, you should be able to view your CSV with
    all the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/countries.png)'
  prefs: []
  type: TYPE_IMG
- en: Success! We have completed our first working scraper.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we walked through a variety of ways to scrape data from a web
    page. Regular expressions can be useful for a one-off scrape or to avoid the overhead
    of parsing the entire web page, and `BeautifulSoup` provides a high-level interface
    while avoiding any difficult dependencies. However, in general, `lxml` will be
    the best choice because of its speed and extensive functionality, so we will use
    it in future examples.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned how to inspect HTML pages using browser tools and the console
    and define CSS selectors and XPath selectors to match and extract content from
    the downloaded pages.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we will introduce caching, which allows us to save web pages
    so they only need be downloaded the first time a crawler is run.
  prefs: []
  type: TYPE_NORMAL
