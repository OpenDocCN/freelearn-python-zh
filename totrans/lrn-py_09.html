<html><head></head><body><div class="chapter" title="Chapter&#xA0;9.&#xA0;Data Science"><div class="titlepage"><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Data Science</h1></div></div></div><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td valign="top"> </td><td valign="top"><p><span class="emphasis"><em>"If we have data, let's look at data. If all we have are opinions, let's go with mine."</em></span></p></td><td valign="top"> </td></tr><tr><td valign="top"> </td><td colspan="2" align="right" valign="top" style="text-align: center">--<span class="attribution"><span class="emphasis"><em>Jim Barksdale, former Netscape CEO</em></span></span></td></tr></table></div><p>
<span class="strong"><strong>Data science</strong></span> is <a id="id613" class="indexterm"/>a very broad term, and can assume several different meanings according to context, understanding, tools, and so on. There are countless books about this subject, which is not suitable for the faint-hearted.</p><p>In order to do proper data science, you need to know mathematics and statistics at the very least. Then, you may want to dig into other subjects such as pattern recognition and machine learning and, of course, there is a plethora of languages and tools you can choose from.</p><p>Unless I transform into <span class="emphasis"><em>The Amazing Fabrizio</em></span> in the next few minutes, I won't be able to talk about everything; I won't even get close to it. Therefore, in order to render this chapter meaningful, we're going to work on a cool project together.</p><p>About 3 years ago, I was working for a top-tier social media company in London. I stayed there for 2 years, and I was privileged to work with several people whose brilliance I can only start to describe. We were the first in the world to have access to the Twitter Ads API, and we were partners with Facebook as well. That means a lot of data.</p><p>Our analysts were dealing with a huge number of campaigns and they were struggling with the amount of work they had to do, so the development team I was a part of tried to help by introducing them to Python and to the tools Python gives you to deal with data. It was a very interesting journey that led me to mentor several people in the company and eventually to Manila where, for 2 weeks, I gave intensive training in Python and data science to our analysts there.</p><p>The project we're going to do together in this chapter is a lightweight version of the final example I presented to my Manila students. I have rewritten it to a size that will fit this chapter, and made a few adjustments here and there for teaching purposes, but all the main concepts are there, so it should be fun and instructional for you to code along.</p><p>On our journey, we're going to meet a few of the tools you can find in the Python ecosystem when it comes to dealing with data, so let's start by talking about Roman gods.</p><div class="section" title="IPython and Jupyter notebook"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec72"/>IPython and Jupyter notebook</h1></div></div></div><p>In 2001, Fernando<a id="id614" class="indexterm"/> Perez was a graduate student in physics at CU Boulder, and <a id="id615" class="indexterm"/>was trying to improve the Python shell so that he could have some niceties like those he was used to when he was working with tools such as Mathematica and Maple. The result of that effort took the name <span class="strong"><strong>IPython</strong></span>.</p><p>In a nutshell, that small script began as an enhanced version of the Python shell and, through the effort of other coders and eventually proper funding from several different companies, it became the wonderful and successful project it is today. Some 10 years after its birth, a notebook environment was created, powered by technologies like WebSockets, the Tornado web server, jQuery, CodeMirror, and MathJax. The ZeroMQ library was also used to handle the messages between the notebook interface and the Python core that lies behind it.</p><p>The IPython notebook has become so popular and widely used that eventually, all sorts of goodies have been added to it. It can handle widgets, parallel computing, all sorts of media formats, and much, much more. Moreover, at some point, it became possible to code in languages other than Python from within the notebook.</p><p>This has led to a huge project that only recently has been split into two: IPython has been stripped down to focus more on the kernel part and the shell, while the notebook has become a brand new project called <span class="strong"><strong>Jupyter</strong></span>. Jupyter allows interactive scientific computations to be done in more than 40 languages.</p><p>This chapter's project will all be coded and run in a Jupyter notebook, so let me explain in a few words what a notebook is.</p><p>A notebook environment is a web page that exposes a simple menu and the cells in which you can run Python code. Even though the cells are separate entities that you can run individually, they all share the same Python kernel. This means that all the names that you define in a cell (the variables, functions, and so on) will be available in any other cell.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note54"/>Note</h3><p>Simply put, a Python kernel is a process in which Python is running. The notebook web page is therefore an interface exposed to the user for driving this kernel. The web page communicates to it using a very fast messaging system.</p></div></div><p>Apart from all the graphical advantages, the beauty to have such an environment consists in the ability of running a Python script in chunks, and this can be a tremendous advantage. Take a script that is connecting to a database to fetch data and then manipulate that data. If you do it in the conventional way, with a Python script, you have to fetch the data every time you want to experiment with it. Within a notebook environment, you can fetch the <a id="id616" class="indexterm"/>data in a cell and then manipulate and experiment with it in other <a id="id617" class="indexterm"/>cells, so fetching it every time is not necessary.</p><p>The notebook environment is also extremely helpful for data science because it allows for step-by-step introspection. You do one chunk of work and then verify it. You then do another chunk and verify again, and so on.</p><p>It's also invaluable for prototyping because the results are there, right in front of your eyes, immediately available.</p><p>If you want<a id="id618" class="indexterm"/> to know more about these tools, please check out <a class="ulink" href="http://ipython.org/">http://ipython.org/</a> and <a class="ulink" href="http://jupyter.org/">http://jupyter.org/</a>.</p><p>I have created <a id="id619" class="indexterm"/>a very simple example notebook with a <code class="literal">fibonacci</code> function that gives you the list of all Fibonacci numbers smaller than a given <code class="literal">N</code>. In my browser, it looks like this:</p><div class="mediaobject"><img src="graphics/4715_09_01.jpg" alt="IPython and Jupyter notebook"/></div><p>Every cell has an <span class="strong"><strong>In []</strong></span> label. If there's nothing between the braces, it means that cell has never been executed. If there is a number, it means that the cell has been executed, and the number represents the order in which the cell was executed. Finally, a <span class="strong"><strong>*</strong></span> means that the cell is currently being executed.</p><p>You can see in the picture that in the first cell I have defined the <code class="literal">fibonacci</code> function, and I have<a id="id620" class="indexterm"/> executed it. This has the effect of placing the <code class="literal">fibonacci</code> name <a id="id621" class="indexterm"/>in the global frame associated with the notebook, therefore the <code class="literal">fibonacci</code> function is now available to the other cells as well. In fact, in the second cell, I can run <code class="literal">fibonacci(100)</code> and see the results in <span class="strong"><strong>Out [2]</strong></span>. In the third cell, I have shown you one of the several magic functions you can find in a notebook in the second cell. <span class="strong"><strong>%timeit</strong></span> runs the code several times and provides you with a nice benchmark for it. All the measurements for the list comprehensions and generators I did in <a class="link" href="ch05.html" title="Chapter 5. Saving Time and Memory">Chapter 5</a>, <span class="emphasis"><em>Saving Time and Memory</em></span> were carried out with this nice feature.</p><p>You can execute a cell as many times as you want, and change the order in which you run them. Cells are very malleable, you can also put in markdown text or render them as headers.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note55"/>Note</h3><p>
<span class="strong"><strong>Markdown</strong></span> is <a id="id622" class="indexterm"/>a lightweight markup language with plain text formatting syntax designed so that it can be converted to HTML and many other formats.</p></div></div><p>Also, whatever you place in the last row of a cell will be automatically printed for you. This is very handy because you're not forced to write <code class="literal">print(...)</code> explicitly.</p><p>Feel free to explore the notebook environment; once you're friends with it, it's a long-lasting relationship, I promise.</p><p>In order to run the notebook, you have to install a handful of libraries, each of which collaborates with the others to make the whole thing work. Alternatively, you can just install Jupyter and it will take care of everything for you. For this chapter, there are a few other dependencies that we need to install, so please run the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ pip install jupyter pandas matplotlib fake-factory delorean xlwt</strong></span>
</pre></div><p>Don't worry, I'll introduce you to each of these gradually. Now, when you're done installing these libraries (it may take a few minutes), you can start the notebook:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ jupyter notebook</strong></span>
</pre></div><p>This will open a page in your browser at this address: <code class="literal">http://localhost:8888/</code>.</p><p>Go to that page and create a new notebook using the menu. When you have it and you're comfortable with it, we're ready to go.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip18"/>Tip</h3><p>If you experience any issues setting up the notebook environment, please don't get discouraged. If you get an error, it's usually just a matter of searching a little bit on the web and you'll end up on a page where someone else has had the same issue, and they have explained how to fix it. Try your best to have the notebook environment up and running before continuing with the chapter.</p></div></div><p>Our project <a id="id623" class="indexterm"/>will take place in a notebook, therefore I will tag each code snippet <a id="id624" class="indexterm"/>with the cell number it belongs to, so that you can easily reproduce the code and follow along.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip19"/>Tip</h3><p>If you familiarize yourself with the keyboard shortcuts (look in the notebook's help section), you will be able to move between cells and handle their content without having to reach for the mouse. This will make you more proficient and way faster when you work in a notebook.</p></div></div></div></div>
<div class="section" title="Dealing with data"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec73"/>Dealing with data</h1></div></div></div><p>Typically, when<a id="id625" class="indexterm"/> you deal with data, this is the path you go through: you fetch it, you clean and manipulate it, then you inspect it and present results as values, spreadsheets, graphs, and so on. I want you to be in charge of all three steps of the process without having any external dependency on a data provider, so we're going to do the following:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We're going to create the data, simulating the fact that it comes in a format which is not perfect or ready to be worked on.</li><li class="listitem">We're going to clean it and feed it to the main tool we'll use in the project: <span class="strong"><strong>DataFrame</strong></span> of <code class="literal">pandas</code>.</li><li class="listitem">We're going to manipulate the data in the DataFrame.</li><li class="listitem">We're going to save the DataFrame to a file in different formats.</li><li class="listitem">Finally, we're going to inspect the data and get some results out of it.</li></ol></div><div class="section" title="Setting up the notebook"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec109"/>Setting up the notebook</h2></div></div></div><p>First <a id="id626" class="indexterm"/>things first, we need to set up the notebook. This means imports and a bit of configuration.</p><p>
<code class="literal">#1</code>
</p><div class="informalexample"><pre class="programlisting">import json
import calendar
import random
from datetime import date, timedelta

import faker
import numpy as np
from pandas import DataFrame
from delorean import parse
import pandas as pd

# make the graphs nicer
pd.set_option('display.mpl_style', 'default')</pre></div><p>Cell <code class="literal">#1</code> takes<a id="id627" class="indexterm"/> care of the imports. There are quite a few new things here: the <code class="literal">calendar</code>, <code class="literal">random</code> and <code class="literal">datetime</code> modules are part of the standard library. Their names are self-explanatory, so let's look at <code class="literal">faker</code>. The <code class="literal">fake-factory</code> library gives you this module, which you can use to prepare fake data. It's very useful in tests, when you prepare your fixtures, to get all sorts of things such as names, e-mail addresses, phone numbers, credit card details, and much more. It is all fake, of course.</p><p>
<code class="literal">numpy</code> is the NumPy library, the fundamental package for scientific computing with Python. I'll spend a few words on it later on in the chapter.</p><p>
<code class="literal">pandas</code> is the very core upon which the whole project is based. It stands for <span class="strong"><strong>Python Data Analysis Library</strong></span>. Among many others, it provides the <span class="strong"><strong>DataFrame</strong></span>, a matrix-like data structure with advanced processing capabilities. It's customary to import the <code class="literal">DataFrame</code> separately and then do <code class="literal">import pandas as pd</code>.</p><p>
<code class="literal">delorean</code> is a nice third-party library that speeds up dealing with dates dramatically. Technically, we could do it with the standard library, but I see no reason not to expand a bit the range of the example and show you something different.</p><p>Finally, we have an instruction on the last line that will make our graphs at the end a little bit nicer, which doesn't hurt.</p></div><div class="section" title="Preparing the data"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec110"/>Preparing the data</h2></div></div></div><p>We want to <a id="id628" class="indexterm"/>achieve the following data structure: we're going to have a list of user objects. Each user object will be linked to a number of campaign objects.</p><p>In Python, everything is an object, so I'm using this term in a generic way. The user object may be a string, a dict, or something else.</p><p>A <span class="strong"><strong>campaign</strong></span> in <a id="id629" class="indexterm"/>the social media world is a promotional campaign that a media agency runs on social media networks on behalf of a client.</p><p>Remember that we're going to prepare this data so that it's not in perfect shape (but it won't be so bad either...).</p><p>
<code class="literal">#2</code>
</p><div class="informalexample"><pre class="programlisting">fake = faker.Faker()</pre></div><p>Firstly, we<a id="id630" class="indexterm"/> instantiate the <code class="literal">Faker</code> that we'll use to create the data.</p><p>
<code class="literal">#3</code>
</p><div class="informalexample"><pre class="programlisting">usernames = set()
usernames_no = 1000
# populate the set with 1000 unique usernames
while len(usernames) &lt; usernames_no:
    usernames.add(fake.user_name())</pre></div><p>Then we need usernames. I want 1,000 unique usernames, so I loop over the length of the <code class="literal">usernames</code> set until it has 1,000 elements. A set doesn't allow duplicated elements, therefore uniqueness is guaranteed.</p><p>
<code class="literal">#4</code>
</p><div class="informalexample"><pre class="programlisting">def get_random_name_and_gender():
    skew = .6  # 60% of users will be female
    male = random.random() &gt; skew
    if male:
        return fake.name_male(), 'M'
    else:
        return fake.name_female(), 'F'

def get_users(usernames):
    users = []
    for username in usernames:
        name, gender = get_random_name_and_gender()
        user = {
            'username': username,
            'name': name,
            'gender': gender,
            'email': fake.email(),
            'age': fake.random_int(min=18, max=90),
            'address': fake.address(),
        }
        users.append(json.dumps(user))
    return users

users = get_users(usernames)
users[:3]</pre></div><p>Here, we create a list of users. Each <code class="literal">username</code> has now been augmented to a full-blown <code class="literal">user</code> dict, with other details such as name, gender, e-mail, and so on. Each <code class="literal">user</code> dict is then dumped to JSON and added to the list. This data structure is not optimal, of course, but we're simulating a scenario where users come to us like that.</p><p>Note the skewed use of <code class="literal">random.random()</code> to make 60% of users female. The rest of the logic should be very easy for you to understand.</p><p>Note also<a id="id631" class="indexterm"/> the last line. Each cell automatically prints what's on the last line; therefore, the output of this is a list with the first three users:</p><p>
<code class="literal">Out #4</code>
</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>['{"gender": "F", "age": 48, "email": "jovani.dickinson@gmail.com", "address": "2006 Sawayn Trail Apt. 207\\nHyattview, MO 27278", "username": "darcy00", "name": "Virgia Hilpert"}',</strong></span>
<span class="strong"><strong> '{"gender": "F", "age": 58, "email": "veum.javen@hotmail.com", "address": "5176 Andres Plains Apt. 040\\nLakinside, GA 92446", "username": "renner.virgie", "name": "Miss Clarabelle Kertzmann MD"}',</strong></span>
<span class="strong"><strong> '{"gender": "M", "age": 33, "email": "turner.felton@rippin.com", "address": "1218 Jacobson Fort\\nNorth Doctor, OK 04469", "username": "hettinger.alphonsus", "name": "Ludwig Prosacco"}']</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note56"/>Note</h3><p>I hope you're following along with your own notebook. If you do, please note that all data is generated using random functions and values; therefore, you will see different results. They will change every time you execute the notebook.</p></div></div><p>
<code class="literal">#5</code>
</p><div class="informalexample"><pre class="programlisting"># campaign name format:
# <span class="strong"><strong>InternalType_StartDate_EndDate_TargetAge_TargetGender_Currency</strong></span>
def get_type():
    # just some gibberish internal codes
    types = ['AKX', 'BYU', 'GRZ', 'KTR']
    return random.choice(types)

def get_start_end_dates():
    duration = random.randint(1, 2 * 365)
    offset = random.randint(-365, 365)
    start = date.today() - timedelta(days=offset)
    end = start + timedelta(days=duration)
    
    def _format_date(date_):
        return date_.strftime("%Y%m%d")
    
    return _format_date(start), _format_date(end)

def get_age():
    age = random.randint(20, 45)
    age -= age % 5
    diff = random.randint(5, 25)
    diff -= diff % 5
    return '{}-{}'.format(age, age + diff)

def get_gender():
    return random.choice(('M', 'F', 'B'))

def get_currency():
    return random.choice(('GBP', 'EUR', 'USD'))

def get_campaign_name():
    separator = '_'
    type_ = get_type()
    start_end = separator.join(get_start_end_dates())
    age = get_age()
    gender = get_gender()
    currency = get_currency()
    return separator.join(
        (type_, start_end, age, gender, currency))</pre></div><p>In <code class="literal">#5</code>, we <a id="id632" class="indexterm"/>define the logic to generate a campaign name. Analysts use spreadsheets all the time and they come up with all sorts of coding techniques to compress as much information as possible into the campaign names. The format I chose is a simple example of that technique: there is a code that tells the campaign type, then start and end dates, then the target age and gender, and finally the currency. All values are separated by an underscore.</p><p>In the <code class="literal">get_type</code> function, I use <code class="literal">random.choice()</code> to get one value randomly out of a collection. Probably more interesting is <code class="literal">get_start_end_dates</code>. First, I get the duration for the campaign, which goes from 1 day to 2 years (randomly), then I get a random offset in time which I subtract from today's date in order to get the start date. Given that the offset is a random number between -365 and 365, would anything be different if I added it to today's date instead of subtracting it?</p><p>When I have both the start and end dates, I return a stringified version of them, joined by an underscore.</p><p>Then, we have a bit of modular trickery going on with the age calculation. I hope you remember the modulo operator (<code class="literal">%</code>) from <a class="link" href="ch02.html" title="Chapter 2. Built-in Data Types">Chapter 2</a>, <span class="emphasis"><em>Built-in Data Types</em></span>.</p><p>What happens here is that I want a date range that has multiples of 5 as extremes. So, there are many ways to do it, but what I do is to get a random number between 20 and 45 for the left extreme, and remove the remainder of the division by 5. So, if, for example, I get 28, I will remove <span class="emphasis"><em>28 % 5 = 3</em></span> to it, getting 25. I could have just used <code class="literal">random.randrange()</code>, but it's hard to resist modular division.</p><p>The rest of the functions are just some other applications of <code class="literal">random.choice()</code> and the last one, <code class="literal">get_campaign_name</code>, is nothing more than a collector for all these puzzle pieces that returns the final campaign name.</p><p>
<code class="literal">#6</code>
</p><div class="informalexample"><pre class="programlisting">def get_campaign_data():
    name = get_campaign_name()
    budget = random.randint(10**3, 10**6)
    spent = random.randint(10**2, budget)    
    clicks = int(random.triangular(10**2, 10**5, 0.2 * 10**5))    
    impressions = int(random.gauss(0.5 * 10**6, 2))
    return {
        'cmp_name': name,
        'cmp_bgt': budget,
        'cmp_spent': spent,
        'cmp_clicks': clicks,
        'cmp_impr': impressions
    }</pre></div><p>In <code class="literal">#6</code>, we<a id="id633" class="indexterm"/> write a function that creates a complete campaign object. I used a few different functions from the <code class="literal">random</code> module. <code class="literal">random.randint()</code> gives you an integer between two extremes. The problem with it is that it follows a uniform probability distribution, which means that any number in the interval has the same probability of coming up.</p><p>Therefore, when dealing with a lot of data, if you distribute your fixtures using a uniform distribution, the results you will get will all look similar. For this reason, I chose to use <code class="literal">triangular</code> and <code class="literal">gauss</code>, for <code class="literal">clicks</code> and <code class="literal">impressions</code>. They use different probability distributions so that we'll have something more interesting to see in the end.</p><p>Just to make sure we're on the same page with the terminology: <code class="literal">clicks</code> represents the number of clicks on a campaign advertisement, <code class="literal">budget</code> is the total amount of money allocated for the campaign, <code class="literal">spent</code> is how much of that money has already been spent, and <code class="literal">impressions</code> is the number of times the campaign has been fetched, as a resource, from its source, regardless of the amount of clicks that were performed on the campaign. Normally, the amount of impressions is greater than the amount of clicks.</p><p>Now that we have the data, it's time to put it all together:</p><p>
<code class="literal">#7</code>
</p><div class="informalexample"><pre class="programlisting">def get_data(users):
    data = []
    for user in users:
        campaigns = [get_campaign_data()
                     for _ in range(random.randint(2, 8))]
        data.append({'user': user, 'campaigns': campaigns})
    return data</pre></div><p>As you<a id="id634" class="indexterm"/> can see, each item in <code class="literal">data</code> is a dict with a user and a list of campaigns that are associated with that user.</p></div><div class="section" title="Cleaning the data"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec111"/>Cleaning the data</h2></div></div></div><p>Let's<a id="id635" class="indexterm"/> start cleaning the data:</p><p>
<code class="literal">#8</code>
</p><div class="informalexample"><pre class="programlisting">rough_data = get_data(users)
rough_data[:2]  # let's take a peek</pre></div><p>We simulate fetching the data from a source and then inspect it. The notebook is the perfect tool to inspect your steps. You can vary the granularity to your needs. The first item in <code class="literal">rough_data</code> looks like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[{'campaigns': [{'cmp_bgt': 130532,</strong></span>
<span class="strong"><strong>    'cmp_clicks': 25576,</strong></span>
<span class="strong"><strong>    'cmp_impr': 500001,</strong></span>
<span class="strong"><strong>    'cmp_name': 'AKX_20150826_20170305_35-50_B_EUR',</strong></span>
<span class="strong"><strong>    'cmp_spent': 57574},</strong></span>
<span class="strong"><strong>   ... omit ...</strong></span>
<span class="strong"><strong>   {'cmp_bgt': 884396,</strong></span>
<span class="strong"><strong>    'cmp_clicks': 10955,</strong></span>
<span class="strong"><strong>    'cmp_impr': 499999,</strong></span>
<span class="strong"><strong>    'cmp_name': 'KTR_20151227_20151231_45-55_B_GBP',</strong></span>
<span class="strong"><strong>    'cmp_spent': 318887}],</strong></span>
<span class="strong"><strong>  'user': '{"age": 44, "username": "jacob43",</strong></span>
<span class="strong"><strong>            "name": "Holland Strosin",</strong></span>
<span class="strong"><strong>            "email": "humberto.leuschke@brakus.com",</strong></span>
<span class="strong"><strong>            "address": "1038 Runolfsdottir Parks\\nElmapo...",</strong></span>
<span class="strong"><strong>            "gender": "M"}'}]</strong></span>
</pre></div><p>So, we now start working with it.</p><p>
<code class="literal">#9</code>
</p><div class="informalexample"><pre class="programlisting">data = []
for datum in rough_data:
    for campaign in datum['campaigns']:
        campaign.update({'user': datum['user']})
        data.append(campaign)
data[:2]  # let's take another peek</pre></div><p>The first thing we need to do in order to be able to feed a DataFrame with this data is to denormalize it. This means transforming the data into a list whose items are campaign dicts, augmented with their relative user dict. Users will be duplicated in each campaign they belong to. The first item in <code class="literal">data</code> looks like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[{'cmp_bgt': 130532,</strong></span>
<span class="strong"><strong>  'cmp_clicks': 25576,</strong></span>
<span class="strong"><strong>  'cmp_impr': 500001,</strong></span>
<span class="strong"><strong>  'cmp_name': 'AKX_20150826_20170305_35-50_B_EUR',</strong></span>
<span class="strong"><strong>  'cmp_spent': 57574,</strong></span>
<span class="strong"><strong>  'user': '{"age": 44, "username": "jacob43",</strong></span>
<span class="strong"><strong>            "name": "Holland Strosin",</strong></span>
<span class="strong"><strong>            "email": "humberto.leuschke@brakus.com",</strong></span>
<span class="strong"><strong>            "address": "1038 Runolfsdottir Parks\\nElmaport...",</strong></span>
<span class="strong"><strong>            "gender": "M"}'}]</strong></span>
</pre></div><p>You can <a id="id636" class="indexterm"/>see that the user object has been brought into the campaign dict which was repeated for each campaign.</p></div><div class="section" title="Creating the DataFrame"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec112"/>Creating the DataFrame</h2></div></div></div><p>Now it's <a id="id637" class="indexterm"/>time to create the <code class="literal">DataFrame</code>:</p><p>
<code class="literal">#10</code>
</p><div class="informalexample"><pre class="programlisting">df = DataFrame(data)
df.head()</pre></div><p>Finally, we will create the <code class="literal">DataFrame</code> and inspect the first five rows using the <code class="literal">head</code> method. You should see something like this:</p><div class="mediaobject"><img src="graphics/4715_09_02.jpg" alt="Creating the DataFrame"/></div><p>Jupyter renders the output of the <code class="literal">df.head()</code> call as HTML automatically. In order to have a text-based output, simply wrap <code class="literal">df.head()</code> in a <code class="literal">print</code> call.</p><p>The <code class="literal">DataFrame</code> structure is very powerful. It allows us to do a great deal of manipulation on its contents. You can filter by rows, columns, aggregate on data, and many other operations. You can operate with rows or columns without suffering the time penalty you would have to pay if you were working on data with pure Python. This happens because, under the covers, <code class="literal">pandas</code> is harnessing the power of the NumPy library, which itself draws its incredible speed from the low-level implementation of its core. NumPy stands for <span class="strong"><strong>Numeric Python</strong></span>, and it<a id="id638" class="indexterm"/> is one of the most widely used libraries in the data science environment.</p><p>Using a<a id="id639" class="indexterm"/> DataFrame allows us to couple the power of NumPy with spreadsheet-like capabilities so that we'll be able to work on our data in a fashion that is similar to what an analyst could do. Only, we do it with code.</p><p>But let's go back to our project. Let's see two ways to quickly get a bird's eye view of the data:</p><p>
<code class="literal">#11</code>
</p><div class="informalexample"><pre class="programlisting">df.count()</pre></div><p>
<code class="literal">count</code> yields a count of all the non-empty cells in each column. This is good to help you understand how sparse your data can be. In our case, we have no missing values, so the output is:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cmp_bgt       4974</strong></span>
<span class="strong"><strong>cmp_clicks    4974</strong></span>
<span class="strong"><strong>cmp_impr      4974</strong></span>
<span class="strong"><strong>cmp_name      4974</strong></span>
<span class="strong"><strong>cmp_spent     4974</strong></span>
<span class="strong"><strong>user          4974</strong></span>
<span class="strong"><strong>dtype: int64</strong></span>
</pre></div><p>Nice! We have 4,974 rows, and the data type is integers (<code class="literal">dtype: int64</code> means long integers because they take 64 bits each). Given that we have 1,000 users and the amount of campaigns per user is a random number between 2 and 8, we're exactly in line with what I was expecting.</p><p>
<code class="literal">#12</code>
</p><div class="informalexample"><pre class="programlisting">df.describe()</pre></div><p>
<code class="literal">describe</code> is a nice and quick way to introspect a bit further:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>             cmp_bgt    cmp_clicks       cmp_impr      cmp_spent</strong></span>
<span class="strong"><strong>count    4974.000000   4974.000000    4974.000000    4974.000000</strong></span>
<span class="strong"><strong>mean   503272.706876  40225.764978  499999.495979  251150.604343</strong></span>
<span class="strong"><strong>std    289393.747465  21910.631950       2.035355  220347.594377</strong></span>
<span class="strong"><strong>min      1250.000000    609.000000  499992.000000     142.000000</strong></span>
<span class="strong"><strong>25%    253647.500000  22720.750000  499998.000000   67526.750000</strong></span>
<span class="strong"><strong>50%    508341.000000  36561.500000  500000.000000  187833.000000</strong></span>
<span class="strong"><strong>75%    757078.250000  55962.750000  500001.000000  385803.750000</strong></span>
<span class="strong"><strong>max    999631.000000  98767.000000  500006.000000  982716.000000</strong></span>
</pre></div><p>As you can see, it gives us several measures such as <code class="literal">count</code>, <code class="literal">mean</code>, <code class="literal">std</code> (standard deviation), <code class="literal">min</code>, <code class="literal">max</code>, and shows how data is distributed in the various quadrants. Thanks to this method, we could already have a rough idea of how our data is structured.</p><p>Let's see which are the three campaigns with the highest and lowest budgets:</p><p>
<code class="literal">#13</code>
</p><div class="informalexample"><pre class="programlisting">df.sort_index(by=['cmp_bgt'], ascending=False).head(3)</pre></div><p>This <a id="id640" class="indexterm"/>gives the following output (truncated):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>      cmp_bgt  cmp_clicks  cmp_impr                  cmp_name</strong></span>
<span class="strong"><strong>4655   999631       15343    499997  AKX_20160814_20180226_40</strong></span>
<span class="strong"><strong>3708   999606       45367    499997  KTR_20150523_20150527_35</strong></span>
<span class="strong"><strong>1995   999445       12580    499998  AKX_20141102_20151009_30</strong></span>
</pre></div><p>And (<code class="literal">#14</code>) a call to <code class="literal">.tail(3)</code>, shows us the ones with the lowest budget.</p><div class="section" title="Unpacking the campaign name"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec28"/>Unpacking the campaign name</h3></div></div></div><p>Now it's time<a id="id641" class="indexterm"/> to increase the complexity up a bit. First of all, we want to get rid of that horrible campaign name (<code class="literal">cmp_name</code>). We need to explode it into parts and put each part in one dedicated column. In order to do this, we'll use the <code class="literal">apply</code> method of the <span class="strong"><strong>Series</strong></span> object.</p><p>The <code class="literal">pandas.core.series.Series</code> class is basically a powerful wrapper around an array (think of it as a list with augmented capabilities). We can extrapolate a <code class="literal">Series</code> object from a <code class="literal">DataFrame</code> by accessing it in the same way we do with a key in a dict, and we can call <code class="literal">apply</code> on that <code class="literal">Series</code> object, which will run a function feeding each item in the <code class="literal">Series</code> to it. We compose the result into a new <code class="literal">DataFrame</code>, and then join that <code class="literal">DataFrame</code> with our beloved <code class="literal">df</code>.</p><p>
<code class="literal">#15</code>
</p><div class="informalexample"><pre class="programlisting">def unpack_campaign_name(name):
    # very optimistic method, assumes data in campaign name
    # is always in good state
    type_, start, end, age, gender, currency = name.split('_')
    start = parse(start).date
    end = parse(end).date
    return type_, start, end, age, gender, currency

campaign_data = df['cmp_name'].apply(unpack_campaign_name)
campaign_cols = [
    'Type', 'Start', 'End', 'Age', 'Gender', 'Currency']
campaign_df = DataFrame(
    campaign_data.tolist(), columns=campaign_cols, index=df.index)
campaign_df.head(3)</pre></div><p>Within <code class="literal">unpack_campaign_name</code>, we split the campaign <code class="literal">name</code> in parts. We use <code class="literal">delorean.parse()</code> to get a proper date object out of those strings (<code class="literal">delorean</code> makes it really easy to do it, doesn't it?), and then we return the objects. A quick peek at the last line reveals:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  Type       Start         End    Age Gender Currency</strong></span>
<span class="strong"><strong>0  KTR  2016-06-16  2017-01-24  20-30      M      EUR</strong></span>
<span class="strong"><strong>1  BYU  2014-10-25  2015-07-31  35-50      B      USD</strong></span>
<span class="strong"><strong>2  BYU  2015-10-26  2016-03-17  35-50      M      EUR</strong></span>
</pre></div><p>Nice! One<a id="id642" class="indexterm"/> important thing: even if the dates appear as strings, they are just the representation of the real <code class="literal">date</code> objects that are hosted in the <code class="literal">DataFrame</code>.</p><p>Another very important thing: when joining two DataFrame instances, it's imperative that they have the same index, otherwise <code class="literal">pandas</code> won't be able to know which rows go with which. Therefore, when we create <code class="literal">campaign_df</code>, we set its index to the one from <code class="literal">df</code>. This enables us to join them. When creating this DataFrame, we also pass the columns names.</p><p>
<code class="literal">#16</code>
</p><div class="informalexample"><pre class="programlisting">df = df.join(campaign_df)</pre></div><p>And after the join, we take a peek, hoping to see matching data (output truncated):</p><p>
<code class="literal">#17</code>
</p><div class="informalexample"><pre class="programlisting">df[['cmp_name'] + campaign_cols].head(3)</pre></div><p>Gives:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>                            cmp_name Type       Start         End</strong></span>
<span class="strong"><strong>0  KTR_20160616_20170124_20-30_M_EUR  KTR  2016-06-16  2017-01-24</strong></span>
<span class="strong"><strong>1  BYU_20141025_20150731_35-50_B_USD  BYU  2014-10-25  2015-07-31</strong></span>
<span class="strong"><strong>2  BYU_20151026_20160317_35-50_M_EUR  BYU  2015-10-26  2016-03-17</strong></span>
</pre></div><p>As you can see, the join was successful; the campaign name and the separate columns expose the same data. Did you see what we did there? We're accessing the <code class="literal">DataFrame</code> using the square brackets syntax, and we pass a list of column names. This will produce a brand new <code class="literal">DataFrame</code>, with those columns (in the same order), on which we then call <code class="literal">head()</code>.</p></div><div class="section" title="Unpacking the user data"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec29"/>Unpacking the user data</h3></div></div></div><p>We <a id="id643" class="indexterm"/>now do the exact same thing for each piece of <code class="literal">user</code> JSON data. We call <code class="literal">apply</code> on the <code class="literal">user</code> Series, running the <code class="literal">unpack_user_json</code> function, which takes a JSON <code class="literal">user</code> object and transforms it into a list of its fields, which we can then inject into a brand new DataFrame <code class="literal">user_df</code>. After that, we'll join <code class="literal">user_df</code> back with <code class="literal">df</code>, like we did with <code class="literal">campaign_df</code>.</p><p>
<code class="literal">#18</code>
</p><div class="informalexample"><pre class="programlisting">def unpack_user_json(user):
    # very optimistic as well, expects user objects
    # to have all attributes
    user = json.loads(user.strip())
    return [
        user['username'],
        user['email'],
        user['name'],
        user['gender'],
        user['age'],
        user['address'],
    ]

user_data = df['user'].apply(unpack_user_json)
user_cols = [
    'username', 'email', 'name', 'gender', 'age', 'address']
user_df = DataFrame(
    user_data.tolist(), columns=user_cols, index=df.index)</pre></div><p>Very <a id="id644" class="indexterm"/>similar to the previous operation, isn't it? We should also note here that, when creating <code class="literal">user_df</code>, we need to instruct <code class="literal">DataFrame</code> about the column names and, very important, the index. Let's join (<code class="literal">#19</code>) and take a quick peek (<code class="literal">#20</code>):</p><div class="informalexample"><pre class="programlisting">df = df.join(user_df)
df[['user'] + user_cols].head(2)</pre></div><p>The output shows us that everything went well. We're good, but we're not done yet.</p><p>If you call <code class="literal">df.columns</code> in a cell, you'll see that we still have ugly names for our columns. Let's change that:</p><p>
<code class="literal">#21</code>
</p><div class="informalexample"><pre class="programlisting">better_columns = [
    'Budget', 'Clicks', 'Impressions',
    '<span class="strong"><strong>cmp_name</strong></span>', 'Spent', '<span class="strong"><strong>user</strong></span>',
    'Type', 'Start', 'End',
    'Target Age', 'Target Gender', 'Currency',
    'Username', 'Email', 'Name',
    'Gender', 'Age', 'Address',
]
df.columns = better_columns</pre></div><p>Good! Now, with the exception of <code class="literal">'cmp_name'</code> and <code class="literal">'user'</code>, we only have nice names.</p><p>Completing the <code class="literal">datasetNext</code> step will be to add some extra columns. For each campaign, we have the amount of clicks and impressions, and we have the spent. This allows us to introduce three measurement ratios: <span class="strong"><strong>CTR</strong></span>, <span class="strong"><strong>CPC</strong></span>, and <span class="strong"><strong>CPI</strong></span>. They stand for <span class="strong"><strong>Click Through Rate</strong></span>, <span class="strong"><strong>Cost Per Click</strong></span>, and <span class="strong"><strong>Cost Per Impression</strong></span>, respectively.</p><p>The last two<a id="id645" class="indexterm"/> are easy to understand, but CTR is not. Suffice <a id="id646" class="indexterm"/>it to say that it is the ratio between clicks<a id="id647" class="indexterm"/> and impressions. It gives you a measure of how many<a id="id648" class="indexterm"/> clicks were performed on a campaign advertisement per impression: the higher this number, the more successful the advertisement is in attracting users to click on it.</p><p>
<code class="literal">#22</code>
</p><div class="informalexample"><pre class="programlisting">def calculate_extra_columns(df):
    # Click Through Rate
    df['CTR'] = df['Clicks'] / df['Impressions']
    # Cost Per Click
    df['CPC'] = df['Spent'] / df['Clicks']
    # Cost Per Impression
    df['CPI'] = df['Spent'] / df['Impressions']
calculate_extra_columns(df)</pre></div><p>I wrote this as a function, but I could have just written the code in the cell. It's not important. What I want you to notice here is that we're adding those three columns with one line of code each, but the <code class="literal">DataFrame</code> applies the operation automatically (the division, in this case) to each pair of cells from the appropriate columns. So, even if they are masked as three divisions, these are actually <span class="emphasis"><em>4974 * 3</em></span> divisions, because they are performed for each row. Pandas does a lot of work for us, and also does a very good job in hiding the complexity of it.</p><p>The function, <code class="literal">calculate_extra_columns,</code> takes a <code class="literal">DataFrame</code>, and works directly on it. This <a id="id649" class="indexterm"/>mode of operation is called <span class="strong"><strong>in-place</strong></span>. Do you remember how <code class="literal">list.sort()</code> was sorting the list? It is the same deal.</p><p>We can take a look at the results by filtering on the relevant columns and calling <code class="literal">head</code>.</p><p>
<code class="literal">#23</code>
</p><div class="informalexample"><pre class="programlisting">df[['Spent', 'Clicks', 'Impressions',
    'CTR', 'CPC', 'CPI']].head(3)</pre></div><p>This shows us that the calculations were performed correctly on each row:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    Spent  Clicks  Impressions       CTR       CPC       CPI</strong></span>
<span class="strong"><strong>0   57574   25576       500001  0.051152  2.251095  0.115148</strong></span>
<span class="strong"><strong>1  226319   61247       499999  0.122494  3.695185  0.452639</strong></span>
<span class="strong"><strong>2    4354   15582       500004  0.031164  0.279425  0.008708</strong></span>
</pre></div><p>Now, I want to verify the accuracy of the results manually for the first row:</p><p>
<code class="literal">#24</code>
</p><div class="informalexample"><pre class="programlisting">clicks = df['Clicks'][0]
impressions = df['Impressions'][0]
spent = df['Spent'][0]
CTR = df['CTR'][0]
CPC = df['CPC'][0]
CPI = df['CPI'][0]
print('CTR:', CTR, clicks / impressions)
print('CPC:', CPC, spent / clicks)
print('CPI:', CPI, spent / impressions)</pre></div><p>It yields the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>CTR: 0.0511518976962 0.0511518976962</strong></span>
<span class="strong"><strong>CPC: 2.25109477635 2.25109477635</strong></span>
<span class="strong"><strong>CPI: 0.115147769704 0.115147769704</strong></span>
</pre></div><p>This is <a id="id650" class="indexterm"/>exactly what we saw in the previous output. Of course, I wouldn't normally need to do this, but I wanted to show you how can you perform calculations this way. You can access a Series (a column) by passing its name to the <code class="literal">DataFrame</code>, in square brackets, and then you access each row by its position, exactly as you would with a regular list or tuple.</p><p>We're almost done with our <code class="literal">DataFrame</code>. All we are missing now is a column that tells us the duration of the campaign and a column that tells us which day of the week corresponds to the start date of each campaign. This allows me to expand on how to play with date objects.</p><p>
<code class="literal">#25</code>
</p><div class="informalexample"><pre class="programlisting">def get_day_of_the_week(day):
    number_to_day = dict(enumerate(calendar.day_name, 1))
    return number_to_day[day.isoweekday()]

def get_duration(row):
    return (row['End'] - row['Start']).days

df['Day of Week'] = <span class="strong"><strong>df['Start'].apply</strong></span>(get_day_of_the_week)
df['Duration'] = <span class="strong"><strong>df.apply</strong></span>(get_duration, <span class="strong"><strong>axis=1</strong></span>)</pre></div><p>We used two different techniques here but first, the code.</p><p>
<code class="literal">get_day_of_the_week</code> takes a date object. If you cannot understand what it does, please take a few moments to try and understand for yourself before reading the explanation. Use the inside-out technique like we've done a few times before.</p><p>So, as I'm sure you know by now, if you put <code class="literal">calendar.day_name</code> in a <code class="literal">list</code> call, you get <code class="literal">['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']</code>. This means that, if we enumerate <code class="literal">calendar.day_name</code> starting from 1, we get pairs such as <code class="literal">(1, 'Monday')</code>, <code class="literal">(2, 'Tuesday')</code>, and so on. If we feed these pairs to a dict, we get a mapping between the days of the week as numbers (1, 2, 3, ...) and their names. When the mapping is created, in order to get the name of a day, we just need to know its number. To get it, we call <code class="literal">date.isoweekday()</code>, which tells us which day of the week that date is (as a number). You feed that into the mapping and, boom! You have the name of the day.</p><p>
<code class="literal">get_duration</code> is<a id="id651" class="indexterm"/> interesting as well. First, notice it takes an entire row, not just a single value. What happens in its body is that we perform a subtraction between a campaign end and start dates. When you subtract date objects the result is a <code class="literal">timedelta</code> object, which represents a given amount of time. We take the value of its <code class="literal">.days</code> property. It is as simple as that.</p><p>Now, we can introduce the fun part, the application of those two functions.</p><p>The first application is performed on a <code class="literal">Series</code> object, like we did before for <code class="literal">'user'</code> and <code class="literal">'cmp_name'</code>, there is nothing new here.</p><p>The second one is applied to the whole DataFrame and, in order to instruct Pandas to perform that operation on the rows, we pass <code class="literal">axis=1</code>.</p><p>We can verify the results very easily, as shown here:</p><p>
<code class="literal">#26</code>
</p><div class="informalexample"><pre class="programlisting">df[['Start', 'End', 'Duration', 'Day of Week']].head(3)</pre></div><p>Yields:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>        Start         End  Duration Day of Week</strong></span>
<span class="strong"><strong>0  2015-08-26  2017-03-05       557   Wednesday</strong></span>
<span class="strong"><strong>1  2014-10-15  2014-12-19        65   Wednesday</strong></span>
<span class="strong"><strong>2  2015-02-22  2016-01-14       326      Sunday</strong></span>
</pre></div><p>So, we now know that between the 26th of August 2015 and the 5th of March 2017 there are 557 days, and that the 26th of August 2015 was a Wednesday.</p><p>If you're wondering what the purpose of this is, I'll provide an example. Imagine that you have a campaign that is tied to a sports event that usually takes place on a Sunday. You may want to inspect your data according to the days so that you can correlate them to the various measurements you have. We're not going to do it in this project, but it was useful to see, if only for the different way of calling <code class="literal">apply()</code> on a DataFrame.</p></div><div class="section" title="Cleaning everything up"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec30"/>Cleaning everything up</h3></div></div></div><p>Now that <a id="id652" class="indexterm"/>we have everything we want, it's time to do the final cleaning: remember we still have the <code class="literal">'cmp_name'</code> and <code class="literal">'user'</code> columns. Those are useless now, so they have to go. Also, I want to reorder the columns in the DataFrame so that it is more relevant to the data it now contains. In order to do this, we just need to filter <code class="literal">df</code> on the column list we want. We'll get back a brand new DataFrame that we can reassign to <code class="literal">df</code> itself.</p><p>
<code class="literal">#27</code>
</p><div class="informalexample"><pre class="programlisting">final_columns = [
    'Type', 'Start', 'End', 'Duration', 'Day of Week', 'Budget',
    'Currency', 'Clicks', 'Impressions', 'Spent', 'CTR', 'CPC',
    'CPI', 'Target Age', 'Target Gender', 'Username', 'Email',
    'Name', 'Gender', 'Age'
]
df = df[final_columns]</pre></div><p>I have<a id="id653" class="indexterm"/> grouped the campaign information at the beginning, then the measurements, and finally the user data at the end. Now our DataFrame is clean and ready for us to inspect.</p><p>Before we start going crazy with graphs, what about taking a snapshot of our DataFrame so that we can easily reconstruct it from a file, rather than having to redo all the steps we did to get here. Some analysts may want to have it in spreadsheet form, to do a different kind of analysis than the one we want to do, so let's see how to save a DataFrame to a file. It's easier done than said.</p></div></div><div class="section" title="Saving the DataFrame to a file"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec113"/>Saving the DataFrame to a file</h2></div></div></div><p>We can <a id="id654" class="indexterm"/>save a DataFrame in many different ways. You can type <code class="literal">df.to_</code> and then press <span class="emphasis"><em>Tab</em></span> to make auto-completion pop up, to see all the possible options.</p><p>We're <a id="id655" class="indexterm"/>going to save our DataFrame in three different formats, just for fun: <span class="strong"><strong>comma-separated values</strong></span> (<span class="strong"><strong>CSV</strong></span>), JSON, and Excel spreadsheet.</p><p>
<code class="literal">#28</code>
</p><div class="informalexample"><pre class="programlisting">df.to_csv('df.csv')</pre></div><p>
<code class="literal">#29</code>
</p><div class="informalexample"><pre class="programlisting">df.to_json('df.json')</pre></div><p>
<code class="literal">#30</code>
</p><div class="informalexample"><pre class="programlisting">df.to_excel('df.xls')</pre></div><p>The CSV file looks like this (output truncated):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Type,Start,End,Duration,Day of Week,Budget,Currency,Clicks,Impres</strong></span>
<span class="strong"><strong>0,GRZ,2015-03-15,2015-11-10,240,Sunday,622551,GBP,35018,500002,787</strong></span>
<span class="strong"><strong>1,AKX,2016-06-19,2016-09-19,92,Sunday,148219,EUR,45185,499997,6588</strong></span>
<span class="strong"><strong>2,BYU,2014-09-25,2016-07-03,647,Thursday,537760,GBP,55771,500001,3</strong></span>
</pre></div><p>And the JSON one like this (again, output truncated):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>{</strong></span>
<span class="strong"><strong>  "Type": {</strong></span>
<span class="strong"><strong>    "0": "GRZ",</strong></span>
<span class="strong"><strong>    "1": "AKX",</strong></span>
<span class="strong"><strong>    "2": "BYU",</strong></span>
</pre></div><p>So, it's extremely easy to save a DataFrame in many different formats, and the good news is that the opposite is also true: it's very easy to load a spreadsheet into a DataFrame. The programmers <a id="id656" class="indexterm"/>behind Pandas went a long way to ease our tasks, something to be grateful for.</p></div><div class="section" title="Visualizing the results"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec114"/>Visualizing the results</h2></div></div></div><p>Finally, the juicy bits. In this section, we're going to visualize some results. From a data science <a id="id657" class="indexterm"/>perspective, I'm not very interested in going deep into analysis, especially because the data is completely random, but nonetheless, this code will get you started with graphs and other features.</p><p>Something I learned in my life—and this may come as a surprise to you—is that <span class="emphasis"><em>looks also counts</em></span> so it's very important that when you present your results, you do your best to <span class="emphasis"><em>make them pretty</em></span>.</p><p>I won't try to prove to you how truthful that last statement is, but I really do believe in it. If you recall the last line of cell <code class="literal">#1</code>:</p><div class="informalexample"><pre class="programlisting"># make the graphs nicer
pd.set_option('display.mpl_style', 'default')</pre></div><p>Its purpose is to make the graphs we will look at in this section a little bit prettier.</p><p>Okay, so, first of all we have to instruct the notebook that we want to use <code class="literal">matplotlib</code> <code class="literal">inline</code>. This means that when we ask Pandas to plot something, we will have the result rendered in the cell output frame. In order to do this, we just need one simple instruction:</p><p>
<code class="literal">#31</code>
</p><div class="informalexample"><pre class="programlisting">%matplotlib inline</pre></div><p>You can also instruct the notebook to do this when you start it from the console by passing a parameter, but I wanted to show you this way too, since it can be annoying to have to restart the notebook just because you want to plot something. In this way, you can do it on the fly and then keep working.</p><p>Next, we're going to set some parameters on <code class="literal">pylab</code>. This is for plotting purposes and it will remove a warning that a font hasn't been found. I suggest that you do not execute this line and keep going. If you get a warning that a font is missing, come back to this cell and run it.</p><p>
<code class="literal">#32</code>
</p><div class="informalexample"><pre class="programlisting">import pylab
pylab.rcParams.update({'font.family' : 'serif'})</pre></div><p>This basically tells Pylab to use the first available serif font. It is simple but effective, and you can experiment with other fonts too.</p><p>Now that the DataFrame is complete, let's run <code class="literal">df.describe()</code> (<code class="literal">#33</code>) again. The results should look something like this:</p><div class="mediaobject"><img src="graphics/4715_09_03.jpg" alt="Visualizing the results"/></div><p>This kind <a id="id658" class="indexterm"/>of quick result is perfect to satisfy those managers who have 20 seconds to dedicate to you and just want rough numbers.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note57"/>Note</h3><p>Once again, please keep in mind that our campaigns have different currencies, so these numbers are actually meaningless. The point here is to demonstrate the DataFrame capabilities, not to get to a correct or detailed analysis of real data.</p></div></div><p>Alternatively, a graph is usually much better than a table with numbers because it's much easier to read it and it gives you immediate feedback. So, let's graph out the four pieces of information we have on each campaign: budget, spent, clicks, and impressions.</p><p>
<code class="literal">#34</code>
</p><div class="informalexample"><pre class="programlisting">df[['Budget', 'Spent', 'Clicks', 'Impressions']].hist(
    bins=16, figsize=(16, 6));</pre></div><p>We extrapolate those four columns (this will give us another DataFrame made with only those columns) and call the histogram <code class="literal">hist()</code> method on it. We give some measurements on the bins and figure sizes, but basically everything is done automatically.</p><p>One important thing: since this instruction is the only one in this cell (which also means, it's the last one), the notebook will print its result before drawing the graph. To suppress this behavior and have only the graph drawn with no printing, just add a semicolon at the end (you thought I was reminiscing about Java, didn't you?). Here are the graphs:</p><div class="mediaobject"><img src="graphics/4715_09_04.jpg" alt="Visualizing the results"/></div><p>They are <a id="id659" class="indexterm"/>beautiful, aren't they? Did you notice the serif font? How about the meaning of those figures? If you go back to <code class="literal">#6</code> and take a look at the way we generate the data, you will see that all these graphs make perfect sense.</p><p>
<span class="strong"><strong>Budget</strong></span> is simply a random integer in an interval, therefore we were expecting a <span class="emphasis"><em>uniform distribution</em></span>, and there we have it; it's practically a constant line.</p><p>
<span class="strong"><strong>Spent</strong></span> is a <span class="emphasis"><em>uniform distribution</em></span> as well, but the high end of its interval is the budget, which is moving, this means we should expect something like a quadratic hyperbole that decreases to the right. And there it is as well.</p><p>
<span class="strong"><strong>Clicks</strong></span> was generated with a <span class="emphasis"><em>triangular distribution</em></span> with mean roughly 20% of the interval size, and you can see that the peak is right there, at about 20% to the left.</p><p>Finally, <span class="strong"><strong>Impressions</strong></span> was a <span class="emphasis"><em>Gaussian distribution</em></span>, which is the one that assumes the famous bell shape. The mean was exactly in the middle and we had standard deviation of 2. You can see that the graph matches those parameters.</p><p>Good! Let's plot out the measures we calculated:</p><p>
<code class="literal">#35</code>
</p><div class="informalexample"><pre class="programlisting">df[['CTR', 'CPC', 'CPI']].hist(
    bins=20, figsize=(16, 6));</pre></div><div class="mediaobject"><img src="graphics/4715_09_05.jpg" alt="Visualizing the results"/></div><p>We can<a id="id660" class="indexterm"/> see that the cost per click is highly skewed to the left, meaning that most of the CPC values are very low. The cost per impression has a similar shape, but less extreme.</p><p>Now, all this is nice, but if you wanted to analyze only a particular segment of the data, how would you do it? We can apply a mask to a DataFrame, so that we get another one with only the rows that satisfy the mask condition. It's like applying a global row-wise <code class="literal">if</code> clause.</p><p>
<code class="literal">#36</code>
</p><div class="informalexample"><pre class="programlisting">mask = (df.Spent &gt; 0.75 * df.Budget)
df[mask][['Budget', 'Spent', 'Clicks', 'Impressions']].hist(
    bins=15, figsize=(16, 6), color='g');</pre></div><p>In this case, I prepared a mask to filter out all the rows for which the spent is less than or equal to 75% of the budget. In other words, we'll include only those campaigns for which we have spent at least three quarters of the budget. Notice that in the mask I am showing you an alternative way of asking for a DataFrame column, by using direct property access (<code class="literal">object.property_name</code>), instead of dict-like access (<code class="literal">object['property_name']</code>). If <code class="literal">property_name</code> is a valid Python name, you can use both ways interchangeably (JavaScript works like this as well).</p><p>The mask is applied in the same way that we access a dict with a key. When you apply a mask to a DataFrame, you get back another one and we select only the relevant columns on this, and call <code class="literal">hist()</code> again. This time, just for fun, we want the results to be painted green:</p><div class="mediaobject"><img src="graphics/4715_09_06.jpg" alt="Visualizing the results"/></div><p>Note that<a id="id661" class="indexterm"/> the shapes of the graphs haven't changed much, apart from the spent, which is quite different. The reason for this is that we've asked only for the rows where spent is at least 75% of the budget. This means that we're including only the rows where spent is close to the budget. The budget numbers come from a uniform distribution. Therefore, it is quite obvious that the spent is now assuming that kind of shape. If you make the boundary even tighter, and ask for 85% or more, you'll see spent become more and more like budget.</p><p>Let's now ask for something different. How about the measure of spent, click, and impressions grouped by day of the week?</p><p>
<code class="literal">#37</code>
</p><div class="informalexample"><pre class="programlisting">df_weekday = df.groupby(['Day of Week']).sum()
df_weekday[['Impressions', 'Spent', 'Clicks']].plot(
    figsize=(16, 6), subplots=True);</pre></div><p>The first line creates a new <code class="literal">DataFrame</code>, <code class="literal">df_weekday</code>, by asking for a grouping by <code class="literal">'Day of Week'</code> on <code class="literal">df</code>. The function used to aggregate the data is addition.</p><p>The second line gets a slice of <code class="literal">df_weekday</code> using a list of column names, something we're accustomed to by now. On the result we call <code class="literal">plot()</code>, which is a bit different to <code class="literal">hist()</code>. The option <code class="literal">subplots=True</code> makes <code class="literal">plot</code> draw three independent graphs:</p><div class="mediaobject"><img src="graphics/4715_09_07.jpg" alt="Visualizing the results"/></div><p>Interestingly<a id="id662" class="indexterm"/> enough, we can see that most of the action happens on Thursdays. If this were meaningful data, this would potentially be important information to give to our clients, and this is the reason I'm showing you this example.</p><p>Note that the days are sorted alphabetically, which scrambles them up a bit. Can you think of a quick solution that would fix the issue? I'll leave it to you as an exercise to come up with something.</p><p>Let's finish this presentation section with a couple more things. First, a simple aggregation. We want to aggregate on <code class="literal">'Target Gender'</code> and <code class="literal">'Target Age'</code>, and show <code class="literal">'Impressions'</code> and <code class="literal">'Spent'</code>. For both, we want to see the mean and the standard deviation.</p><p>
<code class="literal">#38</code>
</p><div class="informalexample"><pre class="programlisting">agg_config = {
    'Impressions': {
        'Mean Impr': 'mean',
        'Std Impr': 'std',
    },
    'Spent': ['mean', 'std'],
}

df.groupby(['Target Gender', 'Target Age']).agg(agg_config)</pre></div><p>It's very easy to do it. We will prepare a dictionary that we'll use as a configuration. I'm showing you two options to do it. We use a nicer format for <code class="literal">'Impressions'</code>, where we pass a nested dict with description/function as key/value pairs. On the other hand, for <code class="literal">'Spent'</code>, we just use a simpler list with just the function names.</p><p>Then, we perform a grouping on the <code class="literal">'Target Gender'</code> and <code class="literal">'Target Age'</code> columns, and we pass our configuration dict to the <code class="literal">agg()</code> method. The result is truncated and rearranged <a id="id663" class="indexterm"/>a little bit to make it fit, and shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>                       Impressions              Spent</strong></span>
<span class="strong"><strong>                    Mean Impr  Std Impr    mean            std</strong></span>
<span class="strong"><strong>Target Target                                             </strong></span>
<span class="strong"><strong>Gender Age                                            </strong></span>
<span class="strong"><strong>B      20-25           500000  2.189102  239882  209442.168488</strong></span>
<span class="strong"><strong>       20-30           500000  2.245317  271285  236854.155720</strong></span>
<span class="strong"><strong>       20-35           500000  1.886396  243725  174268.898935</strong></span>
<span class="strong"><strong>       20-40           499999  2.100786  247740  211540.133771</strong></span>
<span class="strong"><strong>       20-45           500000  1.772811  148712  118603.932051</strong></span>
<span class="strong"><strong>...                    ...       ...     ...            ...</strong></span>
<span class="strong"><strong>M      20-25           500000  2.022023  212520  215857.323228</strong></span>
<span class="strong"><strong>       20-30           500000  2.111882  292577  231663.713956</strong></span>
<span class="strong"><strong>       20-35           499999  1.965177  255651  222790.960907</strong></span>
<span class="strong"><strong>       20-40           499999  1.932473  282515  250023.393334</strong></span>
<span class="strong"><strong>       20-45           499999  1.905746  271077  219901.462405</strong></span>
</pre></div><p>This is the textual representation, of course, but you can also have the HTML one. You can see that <code class="literal">Spent</code> has the <code class="literal">mean</code> and <code class="literal">std</code> columns whose labels are simply the function names, while <code class="literal">Impressions</code> features the nice titles we added to the configuration dict.</p><p>Let's do <a id="id664" class="indexterm"/>one more thing before we wrap this chapter up. I want to show you something called a <span class="strong"><strong>pivot table</strong></span>. It's kind of a buzzword in the data environment, so an example such as this one, albeit very simple, is a must.</p><p>
<code class="literal">#39</code>
</p><div class="informalexample"><pre class="programlisting">pivot = df.pivot_table(
    values=['Impressions', 'Clicks', 'Spent'],
    index=['Target Age'],
    columns=['Target Gender'],
    aggfunc=np.sum
)
pivot</pre></div><p>We create a pivot table that shows us the correlation between the target age and impressions, clicks, and spent. These last three will be subdivided according to the target gender. The aggregation function used to calculate the results is the <code class="literal">numpy.sum</code> function (<code class="literal">numpy.mean</code> would be the default, had I not specified anything).</p><p>After creating the pivot table, we simply print it with the last line in the cell, and here's a crop of the result:</p><div class="mediaobject"><img src="graphics/4715_09_08.jpg" alt="Visualizing the results"/></div><p>It's pretty <a id="id665" class="indexterm"/>clear and provides very useful information when the data is meaningful.</p><p>That's it! I'll leave you to discover more about the wonderful world of IPython, Jupyter, and data science. I strongly encourage you to get comfortable with the notebook environment. It's much better than a console, it's extremely practical and fun to use, and you can even do slides and documents with it.</p></div></div>
<div class="section" title="Where do we go from here?"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec74"/>Where do we go from here?</h1></div></div></div><p>Data science is indeed a fascinating subject. As I said in the introduction, those who want to delve into its meanders need to be well trained in mathematics and statistics. Working with data that has been interpolated incorrectly renders any result about it useless. The same goes for data that has been extrapolated incorrectly or sampled with the wrong frequency. To give you an example, imagine a population of individuals that are aligned in a queue. If, for some reason, the gender of that population alternated between male and female, the queue would be something like this: F-M-F-M-F-M-F-M-F...</p><p>If you sampled it taking only the even elements, you would draw the conclusion that the population was made up only of males, while sampling the odd ones would tell you exactly the opposite.</p><p>Of course, this was just a silly example, I know, but believe me it's very easy to make mistakes in this field, especially when dealing with big data where sampling is mandatory and therefore, the quality of the introspection you make depends, first and foremost, on the quality of the sampling itself.</p><p>When it comes to data science and Python, these are the main tools you want to look at:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>NumPy</strong></span> (<a class="ulink" href="http://www.numpy.org/">http://www.numpy.org/</a>): This is the fundamental package for scientific<a id="id666" class="indexterm"/> computing with Python. It contains a powerful N-dimensional array object, sophisticated (broadcasting) functions, tools for integrating C/C++ and Fortran code, useful linear algebra, Fourier transform, random number capabilities, and much more.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Scikit-Learn</strong></span> (<a class="ulink" href="http://scikit-learn.org/stable/">http://scikit-learn.org/stable/</a>): This is probably the most <a id="id667" class="indexterm"/>popular machine learning library in Python. It has simple and efficient tools for data mining and data analysis, accessible to everybody, and reusable in various contexts. It's built on NumPy, SciPy, and Matplotlib.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Pandas</strong></span> (<a class="ulink" href="http://pandas.pydata.org/">http://pandas.pydata.org/</a>): This is an open source, BSD-licensed library<a id="id668" class="indexterm"/> providing high-performance, easy-to-use data structures, and data analysis tools. We've used it throughout this whole chapter.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>IPython</strong></span> (<a class="ulink" href="http://ipython.org/">http://ipython.org/</a>) / Jupyter (<a class="ulink" href="http://jupyter.org/">http://jupyter.org/</a>): These <a id="id669" class="indexterm"/>provide a rich architecture for interactive computing.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Matplotlib</strong></span> (<a class="ulink" href="http://matplotlib.org/">http://matplotlib.org/</a>): This is a Python 2D plotting library that <a id="id670" class="indexterm"/>produces publication-quality figures in a variety of hard copy formats and interactive environments across platforms. Matplotlib can be used in Python scripts, the Python and IPython shell and notebook, web application servers, and six graphical user interface toolkits.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Numba</strong></span> (<a class="ulink" href="http://numba.pydata.org/">http://numba.pydata.org/</a>): This gives you the power to speed up your <a id="id671" class="indexterm"/>applications with high performance functions written directly in Python. With a few annotations, array-oriented and math-heavy Python code can be just-in-time compiled to native machine instructions, similar in performance to C, C++, and Fortran, without having to switch languages or Python interpreters.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Bokeh</strong></span> (<a class="ulink" href="http://bokeh.pydata.org/en/latest/">http://bokeh.pydata.org/en/latest/</a>): It's a Python-interactive <a id="id672" class="indexterm"/>visualization library that targets modern web browsers for presentation. Its goal is to provide elegant, concise construction of novel graphics in the style of D3.js, but also deliver this capability with high-performance interactivity over very large or streaming datasets.</li></ul></div><p>Other than these single libraries, you can also find ecosystems such as <span class="strong"><strong>SciPy</strong></span> (<a class="ulink" href="http://scipy.org/">http://scipy.org/</a>) and <span class="strong"><strong>Anaconda</strong></span> (<a class="ulink" href="https://www.continuum.io/">https://www.continuum.io/</a>), which bundle several different <a id="id673" class="indexterm"/>packages in order to give you something that just works <a id="id674" class="indexterm"/>in an "out-of-the-box" fashion.</p><p>Installing all these tools and their several dependencies is hard on some systems, so I suggest that you try out ecosystems as well and see if you are comfortable with them. It may be worth it.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec75"/>Summary</h1></div></div></div><p>In this chapter, we talked about data science. Rather than attempting to explain anything about this extremely wide subject, we delved into a project. We familiarized ourselves with the Jupyter notebook, and with different libraries such as Pandas, Matplotlib, NumPy.</p><p>Of course, having to compress all this information into one single chapter means I could only touch briefly on the subjects I presented. I hope the project we've gone through together has been comprehensive enough to give you a good idea about what could potentially be the workflow you might follow when working in this field.</p><p>The next chapter is dedicated to web development. So, make sure you have a browser ready and let's go!</p></div></body></html>