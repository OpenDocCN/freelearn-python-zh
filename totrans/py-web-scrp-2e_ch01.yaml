- en: Introduction to Web Scraping
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络爬虫简介
- en: Welcome to the wide world of web scraping! Web scraping is used by many fields
    to collect data not easily available in other formats. You could be a journalist,
    working on a new story, or a data scientist extracting a new dataset. Web scraping
    is a useful tool even for just a casual programmer, if you need to check your
    latest homework assignments on your university page and have them emailed to you.
    Whatever your motivation, we hope you are ready to learn!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到网络爬虫的广阔世界！网络爬虫被许多领域用于收集其他格式难以获取的数据。你可能是一名记者，正在撰写新故事，或者是一名数据科学家，正在提取新的数据集。即使只是对编程感兴趣的程序员，如果需要检查你在大学页面上最新的作业并将其通过电子邮件发送给你，网络爬虫也是一个有用的工具。无论你的动机是什么，我们都希望你已经准备好学习！
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introducing the field of web scraping
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍网络爬虫领域
- en: Explaining the legal challenges
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释法律挑战
- en: Explaining Python 3 setup
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释Python 3设置
- en: Performing background research on our target website
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对我们的目标网站进行背景研究
- en: Progressively building our own advanced web crawler
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逐步构建我们自己的高级网络爬虫
- en: Using non-standard libraries to help scrape the Web
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用非标准库帮助抓取网络
- en: When is web scraping useful?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 何时使用网络爬虫？
- en: Suppose I have a shop selling shoes and want to keep track of my competitor's
    prices. I could go to my competitor's website each day and compare each shoe's
    price with my own; however this will take a lot of time and will not scale well
    if I sell thousands of shoes or need to check price changes frequently. Or maybe
    I just want to buy a shoe when it's on sale. I could come back and check the shoe
    website each day until I get lucky, but the shoe I want might not be on sale for
    months. These repetitive manual processes could instead be replaced with an automated
    solution using the web scraping techniques covered in this book.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我有一个卖鞋的商店，并想跟踪竞争对手的价格。我每天可以访问竞争对手的网站，并将每双鞋的价格与我自己的进行比较；然而，这将花费很多时间，如果我要卖成千上万双鞋或需要频繁检查价格变化，则这种方法不会很好地扩展。或者，也许我只是想在鞋子打折时购买。我每天回来检查鞋子网站，直到我幸运地找到，但我想买的鞋子可能几个月都不会打折。这些重复的手动过程可以用本书中介绍的网络爬虫技术自动解决。
- en: In an ideal world, web scraping wouldn't be necessary and each website would
    provide an API to share data in a structured format. Indeed, some websites do
    provide APIs, but they typically restrict the data that is available and how frequently
    it can be accessed. Additionally, a website developer might change, remove, or
    restrict the backend API. In short, we cannot rely on APIs to access the online
    data we may want. Therefore we need to learn about web scraping techniques.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个理想的世界里，网络爬虫是不必要的，每个网站都会提供一个API来以结构化格式共享数据。确实，一些网站提供了API，但它们通常限制可用的数据以及可以访问的频率。此外，网站开发者可能会更改、删除或限制后端API。简而言之，我们不能依赖API来访问我们可能想要的在线数据。因此，我们需要了解网络爬虫技术。
- en: Is web scraping legal?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络爬虫是否合法？
- en: Web scraping, and what is legally permissible when web scraping, are still being
    established despite numerous rulings over the past two decades. If the scraped
    data is being used for personal and private use, and within fair use of copyright
    laws, there is usually no problem. However, if the data is going to be republished,
    if the scraping is aggressive enough to take down the site, or if the content
    is copyrighted and the scraper violates the terms of service, then there are several
    legal precedents to note.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管过去二十年中已有众多裁决，但网络爬虫及其在法律上允许的范围仍在建立中。如果抓取的数据仅用于个人和私人用途，并且在不侵犯版权法的前提下，通常不会有问题。然而，如果数据将要被重新发布，如果抓取行为足够激进以至于可能使网站崩溃，或者如果内容受版权保护且抓取者违反了服务条款，那么有几个法律先例需要注意。
- en: In *Feist Publications, Inc. v. Rural Telephone Service Co.*, the United States
    Supreme Court decided scraping and republishing facts, such as telephone listings,
    are allowed. A similar case in Australia, *Telstra Corporation Limited v. Phone
    Directories Company Pty Ltd*, demonstrated that only data with an identifiable
    author can be copyrighted. Another scraped content case in the United States,
    evaluating the reuse of Associated Press stories for an aggregated news product,
    was ruled a violation of copyright in *Associated Press v. Meltwater.*  A European
    Union case in Denmark, *ofir.dk vs home.dk*, concluded that regular crawling and
    deep linking is permissible.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *Feist Publications, Inc. v. Rural Telephone Service Co.* 一案中，美国最高法院裁决，抓取和重新发布事实，如电话簿，是被允许的。在澳大利亚的一个类似案例，*Telstra
    Corporation Limited v. Phone Directories Company Pty Ltd* 中，表明只有可识别作者的资料才能获得版权。美国另一个抓取内容的案例，评估为聚合新闻产品重新使用美联社的故事，被判定为违反了
    *Associated Press v. Meltwater* 的版权。在丹麦的一个欧盟案例，*ofir.dk vs home.dk* 中，得出结论认为，常规抓取和深度链接是允许的。
- en: There have also been several cases in which companies have charged the plaintiff
    with aggressive scraping and attempted to stop the scraping via a legal order.
    The most recent case, *QVC v. Resultly*, ruled that, unless the scraping resulted
    in private property damage, it could not be considered intentional harm, despite
    the crawler activity leading to some site stability issues.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些案例中，公司指控原告进行积极的抓取，并试图通过法律命令来阻止抓取。最近的案例，*QVC v. Resultly*，裁决除非抓取导致私人财产损害，否则不能被视为故意伤害，尽管爬虫活动导致了一些网站稳定性问题。
- en: These cases suggest that, when the scraped data constitutes public facts (such
    as business locations and telephone listings), it can be republished following
    fair use rules. However, if the data is original (such as opinions and reviews
    or private user data), it most likely cannot be republished for copyright reasons.
    In any case, when you are scraping data from a website, remember you are their
    guest and need to behave politely; otherwise, they may ban your IP address or
    proceed with legal action. This means you should make download requests at a reasonable
    rate and define a user agent to identify your crawler. You should also take measures
    to review the Terms of Service of the site and ensure the data you are taking
    is not considered private or copyrighted.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些案例表明，当抓取的数据构成公共事实（如商业地点和电话簿）时，可以按照合理使用规则重新发布。然而，如果数据是原创的（如意见和评论或私人用户数据），则很可能因版权原因不能重新发布。在任何情况下，当你从网站抓取数据时，请记住你是他们的客人，需要礼貌行事；否则，他们可能会禁止你的IP地址或采取法律行动。这意味着你应该以合理的速率进行下载请求，并定义一个用户代理来识别你的爬虫。你还应该采取措施审查网站的条款和条件，确保你获取的数据不被视为私人或受版权保护。
- en: If you have doubts or questions, it may be worthwhile to consult a media lawyer
    regarding the precedents in your area of residence.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有疑问或问题，咨询居住地区的媒体律师了解先例可能是有益的。
- en: 'You can read more about these legal cases at the following sites:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下网站了解更多关于这些法律案例的信息：
- en: '**Feist Publications Inc. v. Rural Telephone Service Co.**'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Feist Publications Inc. v. Rural Telephone Service Co.**'
- en: ([http://caselaw.lp.findlaw.com/scripts/getcase.pl?court=US&vol=499&invol=340](http://caselaw.lp.findlaw.com/scripts/getcase.pl?court=US&vol=499&invol=340))
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ([http://caselaw.lp.findlaw.com/scripts/getcase.pl?court=US&vol=499&invol=340](http://caselaw.lp.findlaw.com/scripts/getcase.pl?court=US&vol=499&invol=340))
- en: '**Telstra Corporation Limited v. Phone Directories Company Pvt Ltd**'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Telstra Corporation Limited v. Phone Directories Company Pvt Ltd**'
- en: ([http://www.austlii.edu.au/au/cases/cth/FCA/2010/44.html](http://www.austlii.edu.au/au/cases/cth/FCA/2010/44.html))
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ([http://www.austlii.edu.au/au/cases/cth/FCA/2010/44.html](http://www.austlii.edu.au/au/cases/cth/FCA/2010/44.html))
- en: '**Associated Press v.Meltwater**'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Associated Press v. Meltwater**'
- en: ([http://www.nysd.uscourts.gov/cases/show.php?db=special&id=279](http://www.nysd.uscourts.gov/cases/show.php?db=special&id=279))
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ([http://www.nysd.uscourts.gov/cases/show.php?db=special&id=279](http://www.nysd.uscourts.gov/cases/show.php?db=special&id=279))
- en: '**ofir.dk vs home.dk**'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ofir.dk vs home.dk**'
- en: ([http://www.bvhd.dk/uploads/tx_mocarticles/S_-_og_Handelsrettens_afg_relse_i_Ofir-sagen.pdf](http://www.bvhd.dk/uploads/tx_mocarticles/S_-_og_Handelsrettens_afg_relse_i_Ofir-sagen.pdf))
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ([http://www.bvhd.dk/uploads/tx_mocarticles/S_-_og_Handelsrettens_afg_relse_i_Ofir-sagen.pdf](http://www.bvhd.dk/uploads/tx_mocarticles/S_-_og_Handelsrettens_afg_relse_i_Ofir-sagen.pdf))
- en: '**QVC v. Resultly**'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**QVC v. Resultly**'
- en: ([https://www.paed.uscourts.gov/documents/opinions/16D0129P.pdf](https://www.paed.uscourts.gov/documents/opinions/16D0129P.pdf))
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ([https://www.paed.uscourts.gov/documents/opinions/16D0129P.pdf](https://www.paed.uscourts.gov/documents/opinions/16D0129P.pdf))
- en: Python 3
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python 3
- en: Throughout this second edition of *Web Scraping with Python*, we will use Python
    3\. The Python Software Foundation has announced Python 2 will be phased out of
    development and support in 2020; for this reason, we and many other Pythonistas
    aim to move development to the support of Python 3, which at the time of this
    publication is at version 3.6\. This book is complaint with Python 3.4+.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本《使用Python进行网络爬取》的第二版中，我们将使用Python 3。Python软件基金会已经宣布Python 2将在2020年停止开发和支持；因此，我们和许多其他Python开发者都旨在将开发转移到对Python
    3的支持上，在本书出版时，Python 3的版本是3.6。本书符合Python 3.4+的要求。
- en: If you are familiar with using [Python Virtual Environments](https://docs.python.org/3/library/venv.html) or
    [Anaconda](https://www.continuum.io/downloads), you likely already know how to
    set up Python 3 in a new environment. If you'd like to install Python 3 globally,
    we recommend searching for your operating system-specific documentation. For my
    part, I simply use **Virtual Environment Wrapper** ([https://virtualenvwrapper.readthedocs.io/en/latest/](https://virtualenvwrapper.readthedocs.io/en/latest/))
    to easily maintain many different environments for different projects and versions
    of Python. Using either Conda environments or virtual environments is highly recommended,
    so that you can easily change dependencies based on your project needs without
    affecting other work you are doing. For beginners, I recommend using Conda as
    it requires less setup. The Conda *introductory documentation* ([https://conda.io/docs/intro.html](https://conda.io/docs/intro.html))
    is a good place to start!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您熟悉使用[Python虚拟环境](https://docs.python.org/3/library/venv.html)或[Anaconda](https://www.continuum.io/downloads)，您可能已经知道如何在新的环境中设置Python
    3。如果您想全局安装Python 3，我们建议搜索您操作系统的特定文档。就我个人而言，我简单地使用**虚拟环境包装器**([https://virtualenvwrapper.readthedocs.io/en/latest/](https://virtualenvwrapper.readthedocs.io/en/latest/))来轻松维护针对不同项目和Python版本的不同环境。使用Conda环境或虚拟环境都强烈推荐，这样您可以根据项目需求轻松更改依赖项，而不会影响您正在进行的其他工作。对于初学者，我建议使用Conda，因为它需要的设置较少。Conda的**入门文档**([https://conda.io/docs/intro.html](https://conda.io/docs/intro.html))是一个良好的起点！
- en: From this point forward, all code and commands will assume you have Python 3
    properly installed and are working with a Python 3.4+ environment. If you see
    Import or Syntax errors, please check that you are in the proper environment and
    look for pesky Python 2.7 file paths in your Traceback.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，所有代码和命令都将假设您已正确安装Python 3，并且正在使用Python 3.4+环境。如果您看到导入或语法错误，请检查您是否处于正确的环境，并在Traceback中查找烦人的Python
    2.7文件路径。
- en: Background research
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景研究
- en: Before diving into crawling a website, we should develop an understanding about
    the scale and structure of our target website. The website itself can help us
    via the `robots.txt` and `Sitemap` files, and there are also external tools available
    to provide further details such as Google Search and `WHOIS`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入爬取网站之前，我们应该了解我们目标网站的规模和结构。网站本身可以通过`robots.txt`和`Sitemap`文件来帮助我们，同时也有外部工具可以提供更多详细信息，例如谷歌搜索和`WHOIS`。
- en: Checking robots.txt
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查robots.txt
- en: 'Most websites define a `robots.txt` file to let crawlers know of any restrictions
    when crawling their website. These restrictions are just a suggestion but good
    web citizens will follow them. The `robots.txt` file is a valuable resource to
    check before crawling to minimize the chance of being blocked, and to discover
    clues about the website''s structure. More information about the `robots.txt`
    protocol is available at [http://www.robotstxt.org](http://www.robotstxt.org).
    The following code is the content of our example `robots.txt`, which is available
    at [http://example.webscraping.com/robots.txt](http://example.webscraping.com/robots.txt):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数网站都会定义一个`robots.txt`文件，以便爬虫在爬取网站时知道任何限制。这些限制只是建议，但好的网络公民会遵守它们。在爬取之前检查`robots.txt`文件是一个宝贵的资源，可以最小化被阻止的机会，并发现有关网站结构的线索。有关`robots.txt`协议的更多信息，请访问[http://www.robotstxt.org](http://www.robotstxt.org)。以下代码是我们示例`robots.txt`的内容，可在[http://example.webscraping.com/robots.txt](http://example.webscraping.com/robots.txt)找到：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In section 1, the `robots.txt` file asks a crawler with user agent `BadCrawler`
    not to crawl their website, but this is unlikely to help because a malicious crawler
    would not respect `robots.txt` anyway. A later example in this chapter will show
    you how to make your crawler follow `robots.txt` automatically.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1节中，`robots.txt`文件要求一个名为`BadCrawler`的用户代理不要爬取他们的网站，但这不太可能有所帮助，因为恶意爬虫无论如何都不会尊重`robots.txt`。本章后面的一个示例将向您展示如何使您的爬虫自动遵循`robots.txt`。
- en: Section 2 specifies a crawl delay of 5 seconds between download requests for
    all user-agents, which should be respected to avoid overloading their server(s).
    There is also a `/trap` link to try to block malicious crawlers who follow disallowed
    links. If you visit this link, the server will block your IP for one minute! A
    real website would block your IP for much longer, perhaps permanently, but then
    we could not continue with this example.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 第2节指定了所有用户代理之间下载请求的延迟时间为5秒，应予以尊重，以避免过载他们的服务器。还有一个`/trap`链接，试图阻止遵循不允许链接的恶意爬虫。如果您访问此链接，服务器将阻止您的IP地址一分钟！一个真正的网站会永久性地阻止您的IP地址，但那样我们就无法继续这个例子了。
- en: Section 3 defines a `Sitemap` file, which will be examined in the next section.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第3节定义了一个`sitemap`文件，将在下一节中进行检查。
- en: Examining the Sitemap
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查Sitemap
- en: '`Sitemap` files are provided bywebsites to help crawlers locate their updated
    content without needing to crawl every web page. For further details, the sitemap
    standard is defined at [http://www.sitemaps.org/protocol.html](http://www.sitemaps.org/protocol.html).
    Many web publishing platforms have the ability to generate a sitemap automatically.
    Here is the content of the  `Sitemap` file located in the listed `robots.txt`
    file:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sitemap`文件由网站提供，以帮助爬虫定位其更新的内容，而无需爬取每个网页。有关更多详细信息，请参阅[http://www.sitemaps.org/protocol.html](http://www.sitemaps.org/protocol.html)中定义的`sitemap`标准。许多网络发布平台都有自动生成`sitemap`文件的能力。以下是位于列出的`robots.txt`文件中的`sitemap`文件的内容：'
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This sitemap provides links to all the web pages, which will be used in the
    next section to build our first crawler. `Sitemap` files provide an efficient
    way to crawl a website, but need to be treated carefully because they can be missing,
    out-of-date, or incomplete.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此`sitemap`提供了指向所有网页的链接，将在下一节中用于构建我们的第一个爬虫。`Sitemap`文件提供了高效爬取网站的方法，但需要小心处理，因为它们可能缺失、过时或不完整。
- en: Estimating the size of a website
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估算网站规模
- en: The size of the target website will affect how we crawl it. If the website is
    just a few hundred URLs, such as our example website, efficiency is not important.
    However, if the website has over a million web pages, downloading each sequentially
    would take months. This problem is addressed later in [Chapter 4](py-web-scrp-2e_ch04.html)
    , *Concurrent Downloading*, on distributed downloading.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 目标网站的规模将影响我们如何爬取它。如果网站只有几百个URL，例如我们的示例网站，效率并不重要。然而，如果网站有超过一百万个网页，按顺序下载将需要数月。这个问题将在[第4章](py-web-scrp-2e_ch04.html)中解决，即*并发下载*。
- en: A quick way to estimate the size of a website is to check the results of Google's
    crawler, which has quite likely already crawled the website we are interested
    in. We can access this information through a Google search with the `site` keyword
    to filter the results to our domain. An interface to this and other advanced search
    parameters are available at [http://www.google.com/advanced_search](http://www.google.com/advanced_search).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 估算网站规模的一个快速方法是检查谷歌爬虫的结果，它很可能已经爬取了我们感兴趣的网站。我们可以通过使用`site`关键词进行谷歌搜索来访问这些信息，以过滤结果到我们的域名。此界面和其他高级搜索参数的界面可在[http://www.google.com/advanced_search](http://www.google.com/advanced_search)找到。
- en: 'Here are the site search results for our example website when searching Google
    for `site:example.webscraping.com`:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是当在谷歌搜索`site:example.webscraping.com`时，我们示例网站的网站搜索结果：
- en: '![](img/image_01_001.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_01_001.jpg)'
- en: As we can see, Google currently estimates more than 200 web pages (this result
    may vary), which is around the website size. For larger websites, Google's estimates
    may be less accurate.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，目前谷歌估计有超过200个网页（此结果可能有所不同），这大约是网站的规模。对于更大的网站，谷歌的估计可能不太准确。
- en: 'We can filter these results to certain parts of the website by adding a URL
    path to the domain. Here are the results for `site:example.webscraping.com/view`,
    which restricts the site search to the country web pages:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过向域名添加URL路径来过滤这些结果到网站的特定部分。以下是`site:example.webscraping.com/view`的结果，它将网站搜索限制在国家网页：
- en: '![](img/image_01_002.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_01_002.jpg)'
- en: Again, your results may vary in size; however, this additional filter is useful
    because ideally you only want to crawl the part of a website containing useful
    data rather than every page.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，您得到的结果大小可能会有所不同；然而，这个额外的过滤器是有用的，因为理想情况下，您只想爬取包含有用数据的网站部分，而不是每个页面。
- en: Identifying the technology used by a website
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别网站所使用的技术
- en: The type of technology used to build a websitewill affect how we crawl it. A
    useful tool to check the kind of technologies a website is built with is the module
    `detectem`, which requires Python 3.5+ and Docker. If you don't already have Docker
    installed, follow the instructions for your operating system at [https://www.docker.com/products/overview](https://www.docker.com/products/overview).
    Once Docker is installed, you can run the following commands.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 用于构建网站的技术类型将影响我们如何爬取它。一个检查网站所使用技术的有用工具是模块 `detectem`，它需要 Python 3.5+ 和 Docker。如果您还没有安装
    Docker，请按照您操作系统的说明在[https://www.docker.com/products/overview](https://www.docker.com/products/overview)进行操作。一旦
    Docker 安装完成，您就可以运行以下命令。
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This will pull the latest Docker image from ScrapingHub and install the package
    via `pip`. It is recommended to use a Python virtual environment ([https://docs.python.org/3/library/venv.html](https://docs.python.org/3/library/venv.html))
    or a Conda environment ([https://conda.io/docs/using/envs.html)](https://conda.io/docs/using/envs.html))
    and to check the project's ReadMe page ([https://github.com/spectresearch/detectem](https://github.com/spectresearch/detectem))
    for any updates or changes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从 ScrapingHub 拉取最新的 Docker 镜像并通过 `pip` 安装软件包。建议使用 Python 虚拟环境 ([https://docs.python.org/3/library/venv.html](https://docs.python.org/3/library/venv.html))
    或 Conda 环境 ([https://conda.io/docs/using/envs.html](https://conda.io/docs/using/envs.html))，并检查项目的
    ReadMe 页面 ([https://github.com/spectresearch/detectem](https://github.com/spectresearch/detectem))
    以获取任何更新或更改。
- en: Why use environments?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用环境？
- en: Imagine if your project was developed with an earlier version of a library such
    as `detectem`, and then, in a later version, `detectem` introduced some backwards-incompatible
    changes that break your project. However, different projects you are working on
    would like to use the newer version. If your project uses the system-installed
    `detectem`, it is eventually going to break when libraries are updated to support
    other projects.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您的项目使用的是 `detectem` 等库的早期版本，然后在后续版本中，`detectem` 引入了一些与旧版本不兼容的更改，这破坏了您的项目。然而，您正在工作的不同项目都希望使用较新版本。如果您的项目使用的是系统安装的
    `detectem`，那么当库更新以支持其他项目时，它最终会崩溃。
- en: Ian Bicking's `virtualenv` provides a clever hack to this problem by copying
    the system Python executable and its dependencies into a local directory to create
    an isolated Python environment. This allows a project to install specific versions
    of Python libraries locally and independently of the wider system. You can even
    utilize different versions of Python in different virtual environments. Further
    details are available in the documentation at [https://virtualenv.pypa.io](https://virtualenv.pypa.io).
    Conda environments offer similar functionality using the Anaconda Python path.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Ian Bicking 的 `virtualenv` 通过将系统 Python 可执行文件及其依赖项复制到本地目录来创建一个隔离的 Python 环境，巧妙地解决了这个问题。这允许项目在本地独立于更广泛的系统安装特定版本的
    Python 库。您甚至可以在不同的虚拟环境中使用不同的 Python 版本。更多详细信息请参阅文档[https://virtualenv.pypa.io](https://virtualenv.pypa.io)。Conda
    环境使用 Anaconda Python 路径提供类似的功能。
- en: 'The `detectem` module uses a series of requests and responses to detect technologies
    used by the website, based on a series of extensible modules. It uses Splash ([https://github.com/scrapinghub/splash](https://github.com/scrapinghub/splash)),
    a scriptable browser developed by ScrapingHub ([https://scrapinghub.com/](https://scrapinghub.com/)).
    To run the module, simply use the `det` command:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`detectem` 模块使用一系列请求和响应来检测网站使用的技术，基于一系列可扩展的模块。它使用 Splash ([https://github.com/scrapinghub/splash](https://github.com/scrapinghub/splash))，这是一个由
    ScrapingHub ([https://scrapinghub.com/](https://scrapinghub.com/)) 开发的可脚本化浏览器。要运行该模块，只需使用
    `det` 命令：'
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can see the example website uses a common JavaScript library, so its content
    is likely embedded in the HTML and should be relatively straightforward to scrape.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到示例网站使用了一个常见的 JavaScript 库，因此其内容很可能嵌入在 HTML 中，应该相对容易抓取。
- en: 'Detectem is still fairly young and aims to eventually have Python parity to
    Wappalyzer ([https://github.com/AliasIO/Wappalyzer](https://github.com/AliasIO/Wappalyzer)),
    a Node.js-based project supporting parsing of many different backends as well
    as ad networks, JavaScript libraries, and server setups. You can also run Wappalyzer
    via Docker. To first download the Docker image, run:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Detectem 仍然相对较新，并旨在最终实现与 Wappalyzer ([https://github.com/AliasIO/Wappalyzer](https://github.com/AliasIO/Wappalyzer))
    相似的 Python 兼容性，Wappalyzer 是一个基于 Node.js 的项目，支持解析许多不同的后端以及广告网络、JavaScript 库和服务器设置。您也可以通过
    Docker 运行 Wappalyzer。首先下载 Docker 镜像，运行：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, you can run the script from the Docker instance:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以从 Docker 实例运行脚本：
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is a bit hard to read, but if we copy and paste it into a JSON linter,
    we can see the many different libraries and technologies detected:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 输出有点难以阅读，但如果我们将其复制粘贴到JSON检查器中，我们可以看到检测到的许多不同库和技术：
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, we can see that Python and the `web2py` frameworks were detected with
    very high confidence. We can also see that the frontend CSS framework Twitter
    Bootstrap is used. Wappalyzer also detected Modernizer.js and the use of Nginx
    as the backend server. Because the site is only using JQuery and Modernizer, it
    is unlikely the entire page is loaded by JavaScript. If the website was instead
    built with AngularJS or React, then its content would likely be loaded dynamically.
    Or, if the website used ASP.NET, it would be necessary to use sessions and form
    submissions to crawl web pages. Working with these more difficult cases will be
    covered later in [Chapter 5](py-web-scrp-2e_ch05.html), *Dynamic Content* and
    [Chapter 6](py-web-scrp-2e_ch06.html), *Interacting with Forms*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到Python和`web2py`框架被以非常高的置信度检测到。我们还可以看到，前端CSS框架Twitter Bootstrap被使用。Wappalyzer还检测到Modernizer.js和作为后端服务器的Nginx的使用。因为该网站只使用了JQuery和Modernizer，所以整个页面不太可能完全由JavaScript加载。如果网站是用AngularJS或React构建的，那么其内容很可能是动态加载的。或者，如果网站使用了ASP.NET，那么在爬取网页时可能需要使用会话和表单提交。关于这些更复杂的情况将在[第5章](py-web-scrp-2e_ch05.html)，*动态内容*和[第6章](py-web-scrp-2e_ch06.html)，*与表单交互*中稍后讨论。
- en: Finding the owner of a website
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查找网站所有者
- en: 'For some websites it may matter to us who the owner is. For example, if the
    owner is known to block web crawlers then it would be wise to be more conservative
    in our download rate. To find who owns a website we can use the `WHOIS` protocol
    to see who is the registered owner of the domain name. A Python wrapper to this
    protocol, documented at [https://pypi.python.org/pypi/python-whois](https://pypi.python.org/pypi/python-whois),
    can be installed via `pip`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些网站，网站所有者可能对我们很重要。例如，如果已知所有者会阻止网络爬虫，那么在下载速率上更加保守是明智的。为了找到网站的所有者，我们可以使用`WHOIS`协议来查看域名的注册所有者。这个协议的Python封装，在[https://pypi.python.org/pypi/python-whois](https://pypi.python.org/pypi/python-whois)有文档说明，可以通过`pip`安装：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here is the most informative part of the `WHOIS` response when querying the
    appspot.com domain with this module:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此模块查询appspot.com域名时，`WHOIS`响应的最有信息部分如下：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can see here that this domain is owned by Google, which is correct; this
    domain is for the Google App Engine service. Google often blocks web crawlers
    despite being fundamentally a web crawling business themselves. We would need
    to be careful when crawling this domain because Google often blocks IPs that quickly
    scrape their services; and you, or someone you live or work with, might need to
    use Google services. I have experienced being asked to enter captchas to use Google
    services for short periods, even after running only simple search crawlers on
    Google domains.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这个域名属于谷歌，这是正确的；这个域名是用于谷歌应用引擎服务的。尽管谷歌本身是一个网络爬虫业务，但谷歌经常阻止网络爬虫。我们在爬取这个域名时需要小心，因为谷歌经常阻止快速抓取其服务的IP；而且你，或者你生活或工作在一起的人，可能需要使用谷歌服务。我经历过在谷歌域名上仅运行简单的搜索爬虫后，还被要求输入验证码才能短暂使用谷歌服务。
- en: Crawling your first website
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 爬取你的第一个网站
- en: 'In order to scrape a website, we first need to download its web pages containing
    the data of interest, a process known as **crawling**. There are a number of approaches
    that can be used to crawl a website, and the appropriate choice will depend on
    the structure of the target website. This chapter will explore how to download
    web pages safely, and then introduce the following three common approaches to
    crawling a website:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了抓取一个网站，我们首先需要下载包含感兴趣数据的网页，这个过程称为**爬取**。有几种方法可以用来爬取一个网站，合适的选择将取决于目标网站的结构。本章将探讨如何安全地下载网页，然后介绍以下三种常见的爬取网站方法：
- en: Crawling a sitemap
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爬取网站地图
- en: Iterating each page using database IDs
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据库ID迭代每一页
- en: Following web page links
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟随网页链接
- en: We have so far used the terms scraping and crawling interchangeably, but let's
    take a moment to define the similarities and differences in these two approaches.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经将抓取和爬取这两个术语互换使用，但让我们花点时间来定义这两种方法之间的相似之处和不同之处。
- en: Scraping versus crawling
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抓取与爬取的区别
- en: Depending on the information you are after and the site content and structure,
    you may need to either build a web scraper or a website crawler. What is the difference?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你想要的信息以及网站的内容和结构，你可能需要构建一个网络抓取器或网站爬虫。它们有什么区别？
- en: A web scraper is usually built to target a particular website or sites and to
    garner specific information on those sites. A web scraper is built to access these
    specific pages and will need to be modified if the site changes or if the information
    location on the site is changed. For example, you might want to build a web scraper
    to check the daily specials at your favorite local restaurant, and to do so you
    would scrape the part of their site where they regularly update that information.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 网络抓取器通常构建来针对特定的网站或网站，并收集这些网站上的特定信息。网络抓取器构建来访问这些特定页面，如果网站发生变化或网站上的信息位置发生变化，则需要对其进行修改。例如，你可能想构建一个网络抓取器来检查你最喜欢的当地餐厅的每日特价，为此你需要抓取他们网站上定期更新该信息的部分。
- en: In contrast, a web crawler is usually built in a generic way; targeting either
    websites from a series of top-level domains or for the entire web. Crawlers can
    be built to gather more specific information, but are usually used to *crawl*
    the web, picking up small and generic bits of information from many different
    sites or pages and following links to other pages.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，网络爬虫通常以通用方式构建；针对一系列顶级域名网站或整个网络。爬虫可以构建来收集更具体的信息，但通常用于*爬取*网络，从许多不同的网站或页面中获取小而通用的信息，并跟随链接到其他页面。
- en: In addition to crawlers and scrapers, we will also cover web spiders in [Chapter
    8](py-web-scrp-2e_ch08.html), *Scrapy.* Spiders can be used for crawling a specific
    set of sites or for broader crawls across many sites or even the Internet.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 除了爬虫和抓取器，我们还将涵盖第8章中的网络蜘蛛，*Scrapy*。蜘蛛可以用于爬取一组特定的网站或进行跨许多网站甚至整个互联网的更广泛爬取。
- en: Generally, we will use specific terms to reflect our use cases; as you develop
    your web scraping, you may notice distinctions in technologies, libraries, and
    packages you may want to use. In these cases, your knowledge of the differences
    in these terms will help you select an appropriate package or technology based
    on the terminology used (such as, is it only for scraping? Is it also for spiders?).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们将使用特定的术语来反映我们的用例；随着你开发网络抓取，你可能会注意到在你想使用的科技、库和包中存在的区别。在这些情况下，你对这些术语差异的了解将帮助你根据使用的术语选择合适的包或技术（例如，它是否仅用于抓取？它是否也用于蜘蛛？）。
- en: Downloading a web page
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载网页
- en: 'To scrape web pages, we first need to download them. Here is a simple Python
    script that uses Python''s `urllib` module to download a URL:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要抓取网页，我们首先需要下载它们。以下是一个简单的Python脚本，它使用Python的`urllib`模块下载URL：
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'When a URL is passed, this function will download the web page and return the
    HTML. The problem with this snippet is that, when downloading the web page, we
    might encounter errors that are beyond our control; for example, the requested
    page may no longer exist. In these cases, `urllib` will raise an exception and
    exit the script. To be safer, here is a more robust version to catch these exceptions:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当传递一个URL时，此函数将下载网页并返回HTML。这个片段的问题在于，在下载网页时，我们可能会遇到超出我们控制范围的错误；例如，请求的页面可能已不存在。在这些情况下，`urllib`将引发异常并退出脚本。为了更安全，这里有一个更健壮的版本来捕获这些异常：
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now, when a download or URL error is encountered, the exception is caught and
    the function returns `None`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当遇到下载或URL错误时，异常会被捕获，函数返回`None`。
- en: Throughout this book, we will assume you are creating files with code that is
    presented without prompts (like the code above). When you see code that begins
    with a Python prompt `>>>` or and IPython prompt `In [1]:`, you will need to either
    enter that into the main file you have been using, or save the file and import
    those functions and classes into your Python interpreter. If you run into any
    issues, please take a look at the code in the book repository at [https://github.com/kjam/wswp](https://github.com/kjam/wswp).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们将假设你正在创建没有提示（如上面的代码）的代码文件。当你看到以Python提示`>>>`或IPython提示`In [1]:`开始的代码时，你需要将其输入你一直在使用的主文件中，或者保存文件并将这些函数和类导入Python解释器中。如果你遇到任何问题，请查看[https://github.com/kjam/wswp](https://github.com/kjam/wswp)代码库中的代码。
- en: Retrying downloads
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重试下载
- en: Often, the errors encountered when downloading are temporary; an example is
    when the web server is overloaded and returns a `503 Service Unavailable` error.
    For these errors, we can retry the download after a short time because the server
    problem may now be resolved. However, we do not want to retry downloading for
    all errors. If the server returns `404 Not Found`, then the web page does not
    currently exist and the same request is unlikely to produce a different result.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，下载过程中遇到的错误是临时的；例如，当网络服务器过载并返回`503 Service Unavailable`错误时。对于这些错误，我们可以在短时间后重试下载，因为服务器问题可能现在已经解决。然而，我们不希望对所有错误都进行重试。如果服务器返回`404
    Not Found`，则表示网页当前不存在，相同的请求不太可能产生不同的结果。
- en: 'The full list of possible HTTP errors is defined by the *Internet Engineering
    Task Force*, and is available for viewing at [https://tools.ietf.org/html/rfc7231#section-6](https://tools.ietf.org/html/rfc7231#section-6).
    In this document, we can see that `4xx` errors occur when there is something wrong
    with our request and `5xx` errors occur when there is something wrong with the
    server. So, we will ensure our `download` function only retries the `5xx` errors.
    Here is the updated version to support this:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 可能的完整HTTP错误列表由**互联网工程任务组**定义，可在[https://tools.ietf.org/html/rfc7231#section-6](https://tools.ietf.org/html/rfc7231#section-6)查看。在这份文档中，我们可以看到`4xx`错误发生在我们的请求有误时，而`5xx`错误发生在服务器有误时。因此，我们将确保我们的`download`函数只重试`5xx`错误。以下是支持此功能的更新版本：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, when a `download` error is encountered with a `5xx` code, the `download` error
    is retried by recursively calling itself. The function now also takes an additional
    argument for the number of times the download can be retried, which is set to
    two times by default. We limit the number of times we attempt to download a web
    page because the server error may not recover. To test this functionality we can
    try downloading [http://httpstat.us/500](http://httpstat.us/500), which returns
    the 500 error code:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当遇到带有`5xx`代码的`download`错误时，通过递归调用自身来重试`download`错误。该函数现在还接受一个额外的参数，用于指定可以重试下载的次数，默认设置为两次。我们限制尝试下载网页的次数，因为服务器错误可能无法恢复。为了测试这个功能，我们可以尝试下载[http://httpstat.us/500](http://httpstat.us/500)，它将返回500错误代码：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As expected, the `download` function now tries downloading the web page, and
    then, on receiving the 500 error, it retries the download twice before giving
    up.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，`download`函数现在尝试下载网页，然后在收到500错误后，在放弃之前重试下载两次。
- en: Setting a user agent
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置用户代理
- en: By default, `urllib` will download content with the `Python-urllib/3.x` user
    agent, where `3.x` is the environment's current version of `Python`. It would
    be preferable to use an identifiable user agent in case problems occur with our
    web crawler. Also, some websites block this default user agent, perhaps after
    they have experienced a poorly made Python web crawler overloading their server.
    For example,  [http://www.meetup.com/](http://www.meetup.com/) currently returns
    a `403 Forbidden `when requesting the page with `urllib`'s default user agent.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`urllib`将以`Python-urllib/3.x`用户代理下载内容，其中`3.x`是当前环境的`Python`版本。如果我们的网络爬虫出现问题，使用可识别的用户代理会更好。此外，一些网站可能会阻止这个默认用户代理，可能是在他们经历过一个制作不佳的Python网络爬虫导致服务器过载之后。例如，[http://www.meetup.com/](http://www.meetup.com/)在用`urllib`的默认用户代理请求页面时，目前返回`403
    Forbidden`。
- en: 'To download sites reliably, we will need to have control over setting the user
    agent. Here is an updated version of our `download` function with the default
    user agent set to `''wswp''` (which stands for**Web Scraping with Python**):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可靠地下载网站，我们需要控制设置用户代理。以下是更新后的`download`函数，默认用户代理设置为`'wswp'`（代表**使用Python进行网络爬取**）：
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If you now try meetup.com, you will see valid HTML. Our download function can
    now be reused in later code to catch errors, retry the site when possible, and
    set the user agent.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在尝试访问meetup.com，你会看到有效的HTML。我们的下载函数现在可以在后续代码中重用，以捕获错误，在可能的情况下重试网站，并设置用户代理。
- en: Sitemap crawler
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网站地图爬虫
- en: For our first simple crawler, we will use the sitemap discovered in the example
    website's `robots.txt` to download all the web pages. To parse the sitemap, we
    will use a simple regular expression to extract URLs within the `<loc>` tags.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的第一个简单爬虫，我们将使用在示例网站的`robots.txt`中发现的网站地图来下载所有网页。为了解析网站地图，我们将使用一个简单的正则表达式来提取`<loc>`标签内的URL。
- en: 'We will need to update our code to handle encoding conversions as our current
    `download` function simply returns bytes. Note that a more robust parsing approach
    called **CSS selectors** will be introduced in the next chapter. Here is our first
    example crawler:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要更新我们的代码以处理编码转换，因为我们的当前 `download` 函数仅返回字节。注意，在下一章中将介绍一种更健壮的解析方法，称为 **CSS
    选择器**。以下是我们的第一个示例爬虫：
- en: '[PRE14]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we can run the sitemap crawler to download all countries from the example
    website:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以运行站点地图爬虫来下载示例网站上的所有国家：
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As shown in our `download` method above, we had to update the character encoding
    to utilize regular expressions with the website response. The Python `read` method
    on the response will return bytes, and the `re` module expects a string. Our code
    depends on the website maintainer to include the proper character encoding in
    the response headers. If the character encoding header is not returned, we default
    to UTF-8 and hope for the best. Of course, this decoding will throw an error if
    either the header encoding returned is incorrect or if the encoding is not set
    and also not UTF-8\. There are some more complex ways to guess encoding (see: [https://pypi.python.org/pypi/chardet](https://pypi.python.org/pypi/chardet)),
    which are fairly easy to implement.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，在我们的 `download` 方法中，我们必须更新字符编码以使用网站响应的正则表达式。Python 的 `read` 方法在响应上返回字节，而
    `re` 模块期望一个字符串。我们的代码依赖于网站维护者将正确的字符编码包含在响应头中。如果未返回字符编码头，我们默认为 UTF-8 并寄希望于最好的结果。当然，如果返回的头部编码不正确，或者未设置编码且不是
    UTF-8，这种解码将引发错误。还有一些更复杂的方法可以猜测编码（见：[https://pypi.python.org/pypi/chardet](https://pypi.python.org/pypi/chardet)），这些方法相对容易实现。
- en: For now, the Sitemap crawler works as expected. But as discussed earlier, `Sitemap`
    files often cannot be relied on to provide links to every web page. In the next
    section, another simple crawler will be introduced that does not depend on the
    `Sitemap` file.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Sitemap 爬虫按预期工作。但如前所述，`Sitemap` 文件往往不能保证提供指向每个网页的链接。在下一节中，将介绍另一个简单的爬虫，它不依赖于
    `Sitemap` 文件。
- en: If you don't want to continue the crawl at any time you can hit *Ctrl + C* or
    c*md* + *C* to exit the Python interpreter or program execution.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在任何时候不想继续爬取，可以按 *Ctrl + C* 或 *cmd* + *C* 退出 Python 解释器或程序执行。
- en: ID iteration crawler
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ID 迭代爬虫
- en: 'In this section, we will take advantage of a weakness in the website structure
    to easily access all the content. Here are the URLs of some sample countries:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将利用网站结构中的弱点，轻松访问所有内容。以下是某些样本国家的 URL：
- en: '[http://example.webscraping.com/view/Afghanistan-1](http://example.webscraping.com/view/Afghanistan-1)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://example.webscraping.com/view/Afghanistan-1](http://example.webscraping.com/view/Afghanistan-1)'
- en: '[http://example.webscraping.com/view/Australia-2](http://example.webscraping.com/view/Australia-2)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://example.webscraping.com/view/Australia-2](http://example.webscraping.com/view/Australia-2)'
- en: '[http://example.webscraping.com/view/Brazil-3](http://example.webscraping.com/view/Brazil-3)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://example.webscraping.com/view/Brazil-3](http://example.webscraping.com/view/Brazil-3)'
- en: 'We can see that the URLs only differ in the final section of the URL path,
    with the country name (known as a slug) and ID. It is a common practice to include
    a slug in the URL to help with search engine optimization. Quite often, the web
    server will ignore the slug and only use the ID to match relevant records in the
    database. Let''s check whether this works with our example website by removing
    the slug and checking the page [http://example.webscraping.com/view/1](http://example.webscraping.com/view/1):'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，URLs 只在 URL 路径的最后部分有所不同，即国家名称（称为 slug）和 ID。在 URL 中包含 slug 是一种常见的做法，有助于搜索引擎优化。通常，Web
    服务器会忽略 slug，而只使用 ID 来匹配数据库中的相关记录。让我们通过删除 slug 并检查页面 [http://example.webscraping.com/view/1](http://example.webscraping.com/view/1)
    来检查这在我们示例网站上是否有效：
- en: '![](img/chp1_using_id_to_load_page.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/chp1_using_id_to_load_page.png)'
- en: 'The web page still loads! This is useful to know because now we can ignore
    the slug and simply utilize database IDs to download all the countries. Here is
    an example code snippet that takes advantage of this trick:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 网页仍然可以加载！这一点很有用，因为现在我们可以忽略 slug，而只需利用数据库 ID 下载所有国家。以下是一个利用这个技巧的示例代码片段：
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we can use the function by passing in the base URL:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过传递基本 URL 来使用该功能：
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here, we iterate the ID until we encounter a download error, which we assume
    means our scraper has reached the last country. A weakness in this implementation
    is that some records may have been deleted, leaving gaps in the database IDs.
    Then, when one of these gaps is reached, the crawler will immediately exit. Here
    is an improved version of the code that allows a number of consecutive download
    errors before exiting:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们迭代ID，直到遇到下载错误，我们假设这意味着我们的爬虫已经到达了最后一个国家。这种实现的一个弱点是，一些记录可能已被删除，导致数据库ID中留下空隙。然后，当达到这些空隙之一时，爬虫将立即退出。以下是代码的改进版本，它允许在退出之前发生多次连续的下载错误：
- en: '[PRE18]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The crawler in the preceding code now needs to encounter five consecutive `download`
    errors to stop iteration, which decreases the risk of stopping iteration prematurely
    when some records have been deleted or hidden.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码中的爬虫现在需要遇到五个连续的`download`错误才能停止迭代，这降低了在记录被删除或隐藏时提前停止迭代的风险。
- en: Iterating the IDs is a convenient approach to crawling a website, but is similar
    to the sitemap approach in that it will not always be available. For example,
    some websites will check whether the slug is found in the URL and if not return
    a `404 Not Found` error. Also, other websites use large nonsequential or nonnumeric
    IDs, so iterating is not practical. For example, Amazon uses ISBNs, as the ID
    for the available books, that have at least ten digits. Using an ID iteration
    for ISBNs would require testing billions of possible combinations, which is certainly
    not the most efficient approach to scraping the website content.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代ID是爬取网站的一种方便方法，但与站点地图方法类似，它并不总是可用。例如，一些网站会检查slug是否出现在URL中，如果没有，则返回`404 Not
    Found`错误。此外，其他网站使用大型的非顺序或非数字ID，因此迭代并不实用。例如，亚马逊使用至少有十个数字的ISBN作为可用书籍的ID。对于ISBN的ID迭代需要进行数十亿种可能的组合测试，这当然不是抓取网站内容的最有效方法。
- en: As you've been following along, you might have noticed some download errors
    with the message `TOO MANY REQUESTS` . Don't worry about them at the moment; we
    will cover more about handling these types of error in the *Advanced Features* section
    of this chapter.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在您跟随的过程中，您可能已经注意到一些带有消息`TOO MANY REQUESTS`的下载错误。目前不必担心这些错误；我们将在本章的*高级功能*部分中介绍更多关于处理这些类型错误的内容。
- en: Link crawlers
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 链接爬虫
- en: So far, we have implemented two simple crawlers that take advantage of the structure
    of our sample website to download all published countries. These techniques should
    be used when available, because they minimize the number of web pages to download.
    However, for other websites, we need to make our crawler act more like a typical
    user and follow links to reach the interesting content.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经实现了两个简单的爬虫，利用我们样本网站的架构下载所有已发布的国家。当这些技术可用时，应该使用它们，因为它们最小化了需要下载的网页数量。然而，对于其他网站，我们需要让我们的爬虫更像一个典型用户，并跟随链接以到达有趣的内容。
- en: 'We could simply download the entire website by following every link. However,
    this would likely download many web pages we don''t need. For example, to scrape
    user account details from an online forum, only account pages need to be downloaded
    and not discussion threads. The link crawler we use in this chapter will use regular
    expressions to determine which web pages it should download. Here is an initial
    version of the code:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过跟随每个链接简单地下载整个网站。然而，这可能会下载许多我们不需要的网页。例如，为了从在线论坛抓取用户账户详情，只需要下载账户页面，而不需要讨论线程。本章中我们使用的链接爬虫将使用正则表达式来确定它应该下载哪些网页。以下是代码的初始版本：
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: To run this code, simply call the `link_crawler` function with the URL of the
    website you want to crawl and a regular expression to match links you want to
    follow. For the example website, we want to crawl the index with the list of countries
    and the countries themselves.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此代码，只需调用`link_crawler`函数，并传入您想要爬取的网站的URL以及匹配您想要跟随的链接的正则表达式。对于示例网站，我们想要爬取包含国家列表和国家本身的索引。
- en: 'We know from looking at the site that the index links follow this format:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 从查看网站可以看出，索引链接遵循以下格式：
- en: '[http://example.webscraping.com/index/1](http://example.webscraping.com/index/1)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://example.webscraping.com/index/1](http://example.webscraping.com/index/1)'
- en: '[http://example.webscraping.com/index/2](http://example.webscraping.com/index/2)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://example.webscraping.com/index/2](http://example.webscraping.com/index/2)'
- en: 'The country web pages follow this format:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 国家网页遵循以下格式：
- en: '[http://example.webscraping.com/view/Afghanistan-1](http://example.webscraping.com/view/Afghanistan-1)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://example.webscraping.com/view/Afghanistan-1](http://example.webscraping.com/view/Afghanistan-1)'
- en: '[http://example.webscraping.com/view/Aland-Islands-2](http://example.webscraping.com/view/Aland-Islands-2)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://example.webscraping.com/view/Aland-Islands-2](http://example.webscraping.com/view/Aland-Islands-2)'
- en: 'So a simple regular expression to match both types of web page is `/(index|view)/`.
    What happens when the crawler is run with these inputs? You receive the following
    `download` error:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个简单的正则表达式来匹配这两种类型的网页是 `/(index|view)/`。当爬虫使用这些输入运行时会发生什么？你会收到以下 `download`
    错误：
- en: '[PRE20]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Regular expressions are great tools for extracting information from strings,
    and I recommend every programmer [learn how to read and write a few of them](https://regexone.com/).
    That said, they tend to be quite brittle and easily break. We'll cover more advanced
    ways to extract links and identify their pages as we advance through the book.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式是提取字符串信息的好工具，我建议每个程序员都[学习如何阅读和编写几个](https://regexone.com/)。尽管如此，它们往往相当脆弱，容易出错。随着我们继续阅读本书，我们将介绍更多高级的提取链接和识别它们页面的方法。
- en: 'The problem with downloading `/index/1` is that it only includes the path of
    the web page and leaves out the protocol and server, which is known as a **relative
    link**. Relative links work when browsing because the web browser knows which
    web page you are currently viewing and takes the steps necessary to resolve the
    link. However, `urllib` doesn''t have this context. To help `urllib` locate the
    web page, we need to convert this link into an **absolute link**, which includes
    all the details to locate the web page. As might be expected, Python includes
    a module in `urllib` to do just this, called `parse`. Here is an improved version
    of `link_crawler` that uses the `urljoin` method to create the absolute links:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 下载 `/index/1` 的问题在于它只包括网页的路径，而遗漏了协议和服务器，这被称为**相对链接**。相对链接在浏览时工作，因为网络浏览器知道你当前正在查看哪个网页，并采取必要的步骤来解析链接。然而，`urllib`
    并没有这个上下文。为了帮助 `urllib` 定位网页，我们需要将这个链接转换成**绝对链接**，它包含所有定位网页的详细信息。正如预期的那样，Python
    在 `urllib` 中包含了一个模块来完成这项工作，称为 `parse`。以下是一个使用 `urljoin` 方法创建绝对链接的改进版 `link_crawler`：
- en: '[PRE21]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'When this example is run, you can see it downloads the matching web pages;
    however, it keeps downloading the same locations over and over. The reason for
    this behavior is that these locations have links to each other. For example, Australia
    links to Antarctica and Antarctica links back to Australia, so the crawler will
    continue to queue the URLs and never reach the end of the queue. To prevent re-crawling
    the same links, we need to keep track of what''s already been crawled. The following
    updated version of `link_crawler` stores the URLs seen before, to avoid downloading
    duplicates:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个例子运行时，你可以看到它下载了匹配的网页；然而，它不断地下载相同的位置。这种行为的原因是这些位置之间有链接。例如，澳大利亚链接到南极洲，南极洲又链接回澳大利亚，因此爬虫将继续排队
    URL 并永远不会到达队列的末尾。为了防止重新爬取相同的链接，我们需要跟踪已经爬取过的内容。以下 `link_crawler` 的更新版本存储了之前看到的
    URL，以避免下载重复的内容：
- en: '[PRE22]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: When this script is run, it will crawl the locations and then stop as expected.
    We finally have a working link crawler!
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个脚本运行时，它将爬取位置并按预期停止。我们最终得到了一个工作的链接爬虫！
- en: Advanced features
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级功能
- en: Now, let's add some features to make our link crawler more useful for crawling
    other websites.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们添加一些功能，使我们的链接爬虫在爬取其他网站时更有用。
- en: Parsing robots.txt
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解析 robots.txt
- en: 'First, we need to interpret `robots.txt` to avoid downloading blocked URLs.
    Python `urllib` comes with the `robotparser` module, which makes this straightforward,
    as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要解释 `robots.txt` 以避免下载被阻止的 URL。Python 的 `urllib` 包含一个 `robotparser` 模块，这使得这个过程变得简单，如下所示：
- en: '[PRE23]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The `robotparser` module loads a `robots.txt` file and then provides a `can_fetch()`function,
    which tells you whether a particular user agent is allowed to access a web page
    or not. Here, when the user agent is set to `'BadCrawler'`, the `robotparser`
    module says that this web page can not be fetched, as we saw in the definition
    in the example site's `robots.txt`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`robotparser` 模块加载一个 `robots.txt` 文件，然后提供一个 `can_fetch()` 函数，它告诉你是否允许特定的用户代理访问网页。在这里，当用户代理设置为
    `''BadCrawler''` 时，`robotparser` 模块表示这个网页不能被获取，正如我们在示例网站的 `robots.txt` 中的定义所看到的。'
- en: 'To integrate `robotparser` into the link crawler, we first want to create a
    new function to return the  `robotparser` object:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 `robotparser` 集成到链接爬虫中，我们首先想要创建一个新的函数来返回 `robotparser` 对象：
- en: '[PRE24]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We need to reliably set the `robots_url`; we can do so by passing an extra
    keyword argument to our function. We can also set a default value catch in case
    the user does not pass the variable. Assuming the crawl will start at the root
    of the site, we can simply add `robots.txt` to the end of the URL. We also need
    to define the `user_agent`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要可靠地设置`robots_url`；我们可以通过向我们的函数传递一个额外的关键字参数来实现。我们还可以为用户未传递变量设置一个默认值。假设爬虫将从网站的根目录开始，我们只需将`robots.txt`添加到URL的末尾。我们还需要定义`user_agent`：
- en: '[PRE25]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, we add the parser check in the `crawl` loop:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在`crawl`循环中添加解析器检查：
- en: '[PRE26]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We can test our advanced link crawler and its use of `robotparser` by using
    the bad user agent string.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用不良的用户代理字符串来测试我们的高级链接爬虫及其对`robotparser`的使用。
- en: '[PRE27]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Supporting proxies
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持代理
- en: 'Sometimes it''s necessary to access a website through a proxy. For example,
    Hulu is blocked in many countries outside the United States as are some videos
    on YouTube. Supporting proxies with `urllib` is not as easy as it could be. We
    will cover `requests` for a more user-friendly Python HTTP module that can also
    handle proxies later in this chapter. Here''s how to support a proxy with `urllib`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，通过代理访问网站是必要的。例如，Hulu在美国以外的许多国家被封锁，YouTube上的一些视频也是如此。使用`urllib`支持代理并不像它可能的那样简单。在本章的后面，我们将介绍`requests`，这是一个更用户友好的Python
    HTTP模块，也可以处理代理。以下是使用`urllib`支持代理的方法：
- en: '[PRE28]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here is an updated version of the `download` function to integrate this:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是更新后的`download`函数，以集成这些功能：
- en: '[PRE29]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The current `urllib` module does not support `https` proxies by default (Python
    3.5). This may change with future versions of Python, so check the latest documentation.
    Alternatively, you can use the documentation's recommended recipe ([https://code.activestate.com/recipes/456195/](https://code.activestate.com/recipes/456195/))
    or keep reading to learn how to use the `requests` library.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的`urllib`模块默认不支持`https`代理（Python 3.5）。这可能会随着Python未来版本的更新而改变，所以请检查最新的文档。或者，您可以使用文档中推荐的配方（[https://code.activestate.com/recipes/456195/](https://code.activestate.com/recipes/456195/))，或者继续阅读以了解如何使用`requests`库。
- en: Throttling downloads
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节流下载
- en: 'If we crawl a website too quickly, we risk being blocked or overloading the
    server(s). To minimize these risks, we can throttle our crawl by waiting for a
    set delay between downloads. Here is a class to implement this:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们爬取网站的速度太快，我们可能会被封锁或过载服务器。为了最小化这些风险，我们可以在下载之间等待一个设定的延迟来给爬虫节流。以下是一个实现此功能的类：
- en: '[PRE30]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This `Throttle` class keeps track of when each domain was last accessed and
    will sleep if the time since the last access is shorter than the specified delay.
    We can add throttling to the crawler by calling `throttle` before every download:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`Throttle`类会跟踪每个域名最后一次访问的时间，如果自上次访问以来时间短于指定的延迟，则会休眠。我们可以在每次下载之前调用`throttle`来给爬虫添加节流：'
- en: '[PRE31]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Avoiding spider traps
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 避免蜘蛛陷阱
- en: Currently, our crawler will follow any link it hasn't seen before. However,
    some websites dynamically generate their content and can have an infinite number
    of web pages. For example, if the website has an online calendar with links provided
    for the next month and year, then the next month will also have links to the next
    month, and so on for however long the widget is set (this can be a LONG time).
    The site may offer the same functionality with simple pagination navigation, essentially
    paginating over empty search result pages until the maximum pagination is reached.
    This situation is known as a **spider trap**.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们的爬虫会跟随它之前未见过的任何链接。然而，一些网站会动态生成其内容，并且可以拥有无限数量的网页。例如，如果一个网站有一个包含下个月和年份链接的在线日历，那么下个月也将有链接指向下下个月，依此类推，直到小部件设置的时间长度（这可能是一个很长的时间）。该网站可能通过简单的分页导航提供相同的功能，本质上是在空搜索结果页面上进行分页，直到达到最大分页数。这种情况被称为**蜘蛛陷阱**。
- en: 'A simple way to avoid getting stuck in a spider trap is to track how many links
    have been followed to reach the current web page, which we will refer to as `depth`.
    Then, when a maximum depth is reached, the crawler does not add links from that
    web page to the queue. To implement maximum depth, we will change the `seen` variable,
    which currently tracks visited web pages, into a dictionary to also record the
    depth the links were found at:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 避免陷入蜘蛛陷阱的一个简单方法是通过跟踪到达当前网页所跟随的链接数量，我们将此称为`depth`。然后，当达到最大深度时，爬虫不会将该网页的链接添加到队列中。为了实现最大深度，我们将当前跟踪已访问网页的`seen`变量更改为字典，以记录链接被找到的深度：
- en: '[PRE32]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now, with this feature, we can be confident the crawl will complete eventually.
    To disable this feature, `max_depth` can be set to a negative number so the current
    depth will never be equal to it.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有了这个功能，我们可以有信心爬取最终会完成。要禁用此功能，可以将`max_depth`设置为负数，这样当前深度永远不会等于它。
- en: Final version
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终版本
- en: The full source code for this advanced link crawler can be downloaded at [https://github.com/kjam/wswp/blob/master/code/chp1/advanced_link_crawler.py](https://github.com/kjam/wswp/blob/master/code/chp1/advanced_link_crawler.py).
    Each of the sections in this chapter has matching code in the repository at[ ](https://github.com/kjam/wswp)https://github.com/kjam/wswp.
    To easily follow along, feel free to fork the repository and use it to compare
    and test your own code.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这个高级链接爬虫的完整源代码可以在[https://github.com/kjam/wswp/blob/master/code/chp1/advanced_link_crawler.py](https://github.com/kjam/wswp/blob/master/code/chp1/advanced_link_crawler.py)下载。本章的每个部分在[https://github.com/kjam/wswp](https://github.com/kjam/wswp)的存储库中都有对应的代码。为了方便跟踪，您可以随意复制存储库并使用它来比较和测试您自己的代码。
- en: 'To test the link crawler, let''s try setting the user agent to `BadCrawler`,
    which, as we saw earlier in this chapter, was blocked by `robots.txt`. As expected,
    the crawl is blocked and finishes immediately:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试链接爬虫，让我们尝试将用户代理设置为`BadCrawler`，正如我们在本章前面看到的，它被`robots.txt`阻止了。不出所料，爬取被阻止并立即结束：
- en: '[PRE33]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, let''s try using the default user agent and setting the maximum depth
    to `1` so that only the links from the home page are downloaded:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使用默认用户代理并将最大深度设置为`1`，这样只下载主页的链接：
- en: '[PRE34]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As expected, the crawl stopped after downloading the first page of countries.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，爬取在下载了国家的第一页后停止。
- en: Using the requests library
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用requests库
- en: Although we have built a fairly advanced parser using only `urllib`, the majority
    of scrapers written in Python today utilize the `requests` library to manage complex
    HTTP requests. What started as a small library to help wrap `urllib` features
    in something "human-readable" is now a very large project with hundreds of contributors.
    Some of the features available include built-in handling of encoding, important
    updates to SSL and security, as well as easy handling of POST requests, JSON,
    cookies, and proxies.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们只使用`urllib`构建了一个相当高级的解析器，但如今大多数用Python编写的爬虫都利用`requests`库来管理复杂的HTTP请求。最初这个小型库是为了帮助将`urllib`的功能包装成“可读性更强”的东西，现在已经成为一个拥有数百位贡献者的庞大项目。其中一些可用功能包括内置的编码处理、对SSL和安全的重大更新，以及轻松处理POST请求、JSON、cookies和代理。
- en: Throughout most of this book, we will utilize the requests library for its simplicity
    and ease of use, and because it has become the de facto standard for most web
    scraping.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的大部分内容中，我们将利用requests库的简单性和易用性，因为它已经成为大多数网络爬取的事实标准。
- en: To install `requests`, simply use `pip:`
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装`requests`，只需使用`pip`：
- en: '[PRE35]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: For an in-depth overview of all features, you should read the documentation
    at [http://python-requests.org](http://python-requests.org) or browse the source
    code at [https://github.com/kennethreitz/requests](https://github.com/kennethreitz/requests).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解所有功能，您应该阅读[http://python-requests.org](http://python-requests.org)上的文档或浏览[https://github.com/kennethreitz/requests](https://github.com/kennethreitz/requests)上的源代码。
- en: 'To compare differences using the two libraries, I''ve also built the advanced
    link crawler so that it can use requests. You can see the code at [https://github.com/kjam/wswp/blob/master/code/chp1/advanced_link_crawler_using_requests.py](https://github.com/kjam/wswp/blob/master/code/chp1/advanced_link_crawler_using_requests.py).
    The main `download` function shows the key differences. The `requests` version
    is as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 要比较这两个库之间的差异，我还构建了一个高级链接爬虫，使其可以使用`requests`。您可以在[https://github.com/kjam/wswp/blob/master/code/chp1/advanced_link_crawler_using_requests.py](https://github.com/kjam/wswp/blob/master/code/chp1/advanced_link_crawler_using_requests.py)中查看代码。主要的`download`函数展示了关键差异。`requests`版本如下：
- en: '[PRE36]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'One notable difference is the ease of use of having `status_code` as an available
    attribute for each request. Additionally, we no longer need to test for character
    encoding, as the `text` attribute on our `Response` object does so automatically.
    In the rare case of an non-resolvable URL or timeout, they are all handled by `RequestException`
    so it makes for an easy catch statement. Proxy handling is also taken care of by
    simply passing a dictionary of proxies (that is `{''http'': ''http://myproxy.net:1234'',
    ''https'': ''https://myproxy.net:1234''}`).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '一个显著的区别是，每个请求都有一个可用的`status_code`属性，这使得使用起来更加方便。此外，我们不再需要测试字符编码，因为我们的`Response`对象的`text`属性会自动完成这项工作。在极少数无法解析的URL或超时的情况下，所有这些情况都由`RequestException`处理，这使得捕获语句变得简单。代理处理也可以通过简单地传递一个包含代理的字典（即`{''http'':
    ''http://myproxy.net:1234'', ''https'': ''https://myproxy.net:1234''}`）来完成。'
- en: We will continue to compare and use both libraries, so that you are familiar
    with them depending on your needs and use case. I strongly recommend using `requests`
    whenever you are handling more complex websites, or need to handle important humanizing
    methods such as using cookies or sessions. We will talk more about these methods
    in [Chapter 6](py-web-scrp-2e_ch06.html), *Interacting with Forms*.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续比较并使用这两个库，以便您可以根据自己的需求和用例熟悉它们。我强烈建议在处理更复杂的网站或需要处理重要的人性化方法（例如使用cookies或会话）时使用`requests`库。我们将在[第6章](py-web-scrp-2e_ch06.html)“与表单交互”中更多地讨论这些方法。
- en: Summary
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter introduced web scraping and developed a sophisticated crawler that
    will be reused in the following chapters. We covered the usage of external tools
    and modules to get an understanding of a website, user agents, sitemaps, crawl
    delays, and various advanced crawling techniques.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了网络爬虫技术，并开发了一个复杂的爬虫，该爬虫将在后续章节中重复使用。我们涵盖了使用外部工具和模块来了解网站、用户代理、网站地图、爬取延迟以及各种高级爬取技术。
- en: In the next chapter, we will explore how to scrape data from crawled web pages.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何从爬取的网页中抓取数据。
