- en: Introduction to Web Scraping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to the wide world of web scraping! Web scraping is used by many fields
    to collect data not easily available in other formats. You could be a journalist,
    working on a new story, or a data scientist extracting a new dataset. Web scraping
    is a useful tool even for just a casual programmer, if you need to check your
    latest homework assignments on your university page and have them emailed to you.
    Whatever your motivation, we hope you are ready to learn!
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the field of web scraping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explaining the legal challenges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explaining Python 3 setup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing background research on our target website
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Progressively building our own advanced web crawler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using non-standard libraries to help scrape the Web
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When is web scraping useful?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose I have a shop selling shoes and want to keep track of my competitor's
    prices. I could go to my competitor's website each day and compare each shoe's
    price with my own; however this will take a lot of time and will not scale well
    if I sell thousands of shoes or need to check price changes frequently. Or maybe
    I just want to buy a shoe when it's on sale. I could come back and check the shoe
    website each day until I get lucky, but the shoe I want might not be on sale for
    months. These repetitive manual processes could instead be replaced with an automated
    solution using the web scraping techniques covered in this book.
  prefs: []
  type: TYPE_NORMAL
- en: In an ideal world, web scraping wouldn't be necessary and each website would
    provide an API to share data in a structured format. Indeed, some websites do
    provide APIs, but they typically restrict the data that is available and how frequently
    it can be accessed. Additionally, a website developer might change, remove, or
    restrict the backend API. In short, we cannot rely on APIs to access the online
    data we may want. Therefore we need to learn about web scraping techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Is web scraping legal?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Web scraping, and what is legally permissible when web scraping, are still being
    established despite numerous rulings over the past two decades. If the scraped
    data is being used for personal and private use, and within fair use of copyright
    laws, there is usually no problem. However, if the data is going to be republished,
    if the scraping is aggressive enough to take down the site, or if the content
    is copyrighted and the scraper violates the terms of service, then there are several
    legal precedents to note.
  prefs: []
  type: TYPE_NORMAL
- en: In *Feist Publications, Inc. v. Rural Telephone Service Co.*, the United States
    Supreme Court decided scraping and republishing facts, such as telephone listings,
    are allowed. A similar case in Australia, *Telstra Corporation Limited v. Phone
    Directories Company Pty Ltd*, demonstrated that only data with an identifiable
    author can be copyrighted. Another scraped content case in the United States,
    evaluating the reuse of Associated Press stories for an aggregated news product,
    was ruled a violation of copyright in *Associated Press v. Meltwater.*  A European
    Union case in Denmark, *ofir.dk vs home.dk*, concluded that regular crawling and
    deep linking is permissible.
  prefs: []
  type: TYPE_NORMAL
- en: There have also been several cases in which companies have charged the plaintiff
    with aggressive scraping and attempted to stop the scraping via a legal order.
    The most recent case, *QVC v. Resultly*, ruled that, unless the scraping resulted
    in private property damage, it could not be considered intentional harm, despite
    the crawler activity leading to some site stability issues.
  prefs: []
  type: TYPE_NORMAL
- en: These cases suggest that, when the scraped data constitutes public facts (such
    as business locations and telephone listings), it can be republished following
    fair use rules. However, if the data is original (such as opinions and reviews
    or private user data), it most likely cannot be republished for copyright reasons.
    In any case, when you are scraping data from a website, remember you are their
    guest and need to behave politely; otherwise, they may ban your IP address or
    proceed with legal action. This means you should make download requests at a reasonable
    rate and define a user agent to identify your crawler. You should also take measures
    to review the Terms of Service of the site and ensure the data you are taking
    is not considered private or copyrighted.
  prefs: []
  type: TYPE_NORMAL
- en: If you have doubts or questions, it may be worthwhile to consult a media lawyer
    regarding the precedents in your area of residence.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read more about these legal cases at the following sites:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feist Publications Inc. v. Rural Telephone Service Co.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ([http://caselaw.lp.findlaw.com/scripts/getcase.pl?court=US&vol=499&invol=340](http://caselaw.lp.findlaw.com/scripts/getcase.pl?court=US&vol=499&invol=340))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Telstra Corporation Limited v. Phone Directories Company Pvt Ltd**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ([http://www.austlii.edu.au/au/cases/cth/FCA/2010/44.html](http://www.austlii.edu.au/au/cases/cth/FCA/2010/44.html))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Associated Press v.Meltwater**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ([http://www.nysd.uscourts.gov/cases/show.php?db=special&id=279](http://www.nysd.uscourts.gov/cases/show.php?db=special&id=279))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**ofir.dk vs home.dk**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ([http://www.bvhd.dk/uploads/tx_mocarticles/S_-_og_Handelsrettens_afg_relse_i_Ofir-sagen.pdf](http://www.bvhd.dk/uploads/tx_mocarticles/S_-_og_Handelsrettens_afg_relse_i_Ofir-sagen.pdf))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**QVC v. Resultly**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ([https://www.paed.uscourts.gov/documents/opinions/16D0129P.pdf](https://www.paed.uscourts.gov/documents/opinions/16D0129P.pdf))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Python 3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this second edition of *Web Scraping with Python*, we will use Python
    3\. The Python Software Foundation has announced Python 2 will be phased out of
    development and support in 2020; for this reason, we and many other Pythonistas
    aim to move development to the support of Python 3, which at the time of this
    publication is at version 3.6\. This book is complaint with Python 3.4+.
  prefs: []
  type: TYPE_NORMAL
- en: If you are familiar with using [Python Virtual Environments](https://docs.python.org/3/library/venv.html) or
    [Anaconda](https://www.continuum.io/downloads), you likely already know how to
    set up Python 3 in a new environment. If you'd like to install Python 3 globally,
    we recommend searching for your operating system-specific documentation. For my
    part, I simply use **Virtual Environment Wrapper** ([https://virtualenvwrapper.readthedocs.io/en/latest/](https://virtualenvwrapper.readthedocs.io/en/latest/))
    to easily maintain many different environments for different projects and versions
    of Python. Using either Conda environments or virtual environments is highly recommended,
    so that you can easily change dependencies based on your project needs without
    affecting other work you are doing. For beginners, I recommend using Conda as
    it requires less setup. The Conda *introductory documentation* ([https://conda.io/docs/intro.html](https://conda.io/docs/intro.html))
    is a good place to start!
  prefs: []
  type: TYPE_NORMAL
- en: From this point forward, all code and commands will assume you have Python 3
    properly installed and are working with a Python 3.4+ environment. If you see
    Import or Syntax errors, please check that you are in the proper environment and
    look for pesky Python 2.7 file paths in your Traceback.
  prefs: []
  type: TYPE_NORMAL
- en: Background research
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before diving into crawling a website, we should develop an understanding about
    the scale and structure of our target website. The website itself can help us
    via the `robots.txt` and `Sitemap` files, and there are also external tools available
    to provide further details such as Google Search and `WHOIS`.
  prefs: []
  type: TYPE_NORMAL
- en: Checking robots.txt
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most websites define a `robots.txt` file to let crawlers know of any restrictions
    when crawling their website. These restrictions are just a suggestion but good
    web citizens will follow them. The `robots.txt` file is a valuable resource to
    check before crawling to minimize the chance of being blocked, and to discover
    clues about the website''s structure. More information about the `robots.txt`
    protocol is available at [http://www.robotstxt.org](http://www.robotstxt.org).
    The following code is the content of our example `robots.txt`, which is available
    at [http://example.webscraping.com/robots.txt](http://example.webscraping.com/robots.txt):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In section 1, the `robots.txt` file asks a crawler with user agent `BadCrawler`
    not to crawl their website, but this is unlikely to help because a malicious crawler
    would not respect `robots.txt` anyway. A later example in this chapter will show
    you how to make your crawler follow `robots.txt` automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Section 2 specifies a crawl delay of 5 seconds between download requests for
    all user-agents, which should be respected to avoid overloading their server(s).
    There is also a `/trap` link to try to block malicious crawlers who follow disallowed
    links. If you visit this link, the server will block your IP for one minute! A
    real website would block your IP for much longer, perhaps permanently, but then
    we could not continue with this example.
  prefs: []
  type: TYPE_NORMAL
- en: Section 3 defines a `Sitemap` file, which will be examined in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Examining the Sitemap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Sitemap` files are provided bywebsites to help crawlers locate their updated
    content without needing to crawl every web page. For further details, the sitemap
    standard is defined at [http://www.sitemaps.org/protocol.html](http://www.sitemaps.org/protocol.html).
    Many web publishing platforms have the ability to generate a sitemap automatically.
    Here is the content of the  `Sitemap` file located in the listed `robots.txt`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This sitemap provides links to all the web pages, which will be used in the
    next section to build our first crawler. `Sitemap` files provide an efficient
    way to crawl a website, but need to be treated carefully because they can be missing,
    out-of-date, or incomplete.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating the size of a website
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The size of the target website will affect how we crawl it. If the website is
    just a few hundred URLs, such as our example website, efficiency is not important.
    However, if the website has over a million web pages, downloading each sequentially
    would take months. This problem is addressed later in [Chapter 4](py-web-scrp-2e_ch04.html)
    , *Concurrent Downloading*, on distributed downloading.
  prefs: []
  type: TYPE_NORMAL
- en: A quick way to estimate the size of a website is to check the results of Google's
    crawler, which has quite likely already crawled the website we are interested
    in. We can access this information through a Google search with the `site` keyword
    to filter the results to our domain. An interface to this and other advanced search
    parameters are available at [http://www.google.com/advanced_search](http://www.google.com/advanced_search).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the site search results for our example website when searching Google
    for `site:example.webscraping.com`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_01_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, Google currently estimates more than 200 web pages (this result
    may vary), which is around the website size. For larger websites, Google's estimates
    may be less accurate.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can filter these results to certain parts of the website by adding a URL
    path to the domain. Here are the results for `site:example.webscraping.com/view`,
    which restricts the site search to the country web pages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_01_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Again, your results may vary in size; however, this additional filter is useful
    because ideally you only want to crawl the part of a website containing useful
    data rather than every page.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the technology used by a website
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The type of technology used to build a websitewill affect how we crawl it. A
    useful tool to check the kind of technologies a website is built with is the module
    `detectem`, which requires Python 3.5+ and Docker. If you don't already have Docker
    installed, follow the instructions for your operating system at [https://www.docker.com/products/overview](https://www.docker.com/products/overview).
    Once Docker is installed, you can run the following commands.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This will pull the latest Docker image from ScrapingHub and install the package
    via `pip`. It is recommended to use a Python virtual environment ([https://docs.python.org/3/library/venv.html](https://docs.python.org/3/library/venv.html))
    or a Conda environment ([https://conda.io/docs/using/envs.html)](https://conda.io/docs/using/envs.html))
    and to check the project's ReadMe page ([https://github.com/spectresearch/detectem](https://github.com/spectresearch/detectem))
    for any updates or changes.
  prefs: []
  type: TYPE_NORMAL
- en: Why use environments?
  prefs: []
  type: TYPE_NORMAL
- en: Imagine if your project was developed with an earlier version of a library such
    as `detectem`, and then, in a later version, `detectem` introduced some backwards-incompatible
    changes that break your project. However, different projects you are working on
    would like to use the newer version. If your project uses the system-installed
    `detectem`, it is eventually going to break when libraries are updated to support
    other projects.
  prefs: []
  type: TYPE_NORMAL
- en: Ian Bicking's `virtualenv` provides a clever hack to this problem by copying
    the system Python executable and its dependencies into a local directory to create
    an isolated Python environment. This allows a project to install specific versions
    of Python libraries locally and independently of the wider system. You can even
    utilize different versions of Python in different virtual environments. Further
    details are available in the documentation at [https://virtualenv.pypa.io](https://virtualenv.pypa.io).
    Conda environments offer similar functionality using the Anaconda Python path.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `detectem` module uses a series of requests and responses to detect technologies
    used by the website, based on a series of extensible modules. It uses Splash ([https://github.com/scrapinghub/splash](https://github.com/scrapinghub/splash)),
    a scriptable browser developed by ScrapingHub ([https://scrapinghub.com/](https://scrapinghub.com/)).
    To run the module, simply use the `det` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We can see the example website uses a common JavaScript library, so its content
    is likely embedded in the HTML and should be relatively straightforward to scrape.
  prefs: []
  type: TYPE_NORMAL
- en: 'Detectem is still fairly young and aims to eventually have Python parity to
    Wappalyzer ([https://github.com/AliasIO/Wappalyzer](https://github.com/AliasIO/Wappalyzer)),
    a Node.js-based project supporting parsing of many different backends as well
    as ad networks, JavaScript libraries, and server setups. You can also run Wappalyzer
    via Docker. To first download the Docker image, run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you can run the script from the Docker instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is a bit hard to read, but if we copy and paste it into a JSON linter,
    we can see the many different libraries and technologies detected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that Python and the `web2py` frameworks were detected with
    very high confidence. We can also see that the frontend CSS framework Twitter
    Bootstrap is used. Wappalyzer also detected Modernizer.js and the use of Nginx
    as the backend server. Because the site is only using JQuery and Modernizer, it
    is unlikely the entire page is loaded by JavaScript. If the website was instead
    built with AngularJS or React, then its content would likely be loaded dynamically.
    Or, if the website used ASP.NET, it would be necessary to use sessions and form
    submissions to crawl web pages. Working with these more difficult cases will be
    covered later in [Chapter 5](py-web-scrp-2e_ch05.html), *Dynamic Content* and
    [Chapter 6](py-web-scrp-2e_ch06.html), *Interacting with Forms*.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the owner of a website
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For some websites it may matter to us who the owner is. For example, if the
    owner is known to block web crawlers then it would be wise to be more conservative
    in our download rate. To find who owns a website we can use the `WHOIS` protocol
    to see who is the registered owner of the domain name. A Python wrapper to this
    protocol, documented at [https://pypi.python.org/pypi/python-whois](https://pypi.python.org/pypi/python-whois),
    can be installed via `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the most informative part of the `WHOIS` response when querying the
    appspot.com domain with this module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We can see here that this domain is owned by Google, which is correct; this
    domain is for the Google App Engine service. Google often blocks web crawlers
    despite being fundamentally a web crawling business themselves. We would need
    to be careful when crawling this domain because Google often blocks IPs that quickly
    scrape their services; and you, or someone you live or work with, might need to
    use Google services. I have experienced being asked to enter captchas to use Google
    services for short periods, even after running only simple search crawlers on
    Google domains.
  prefs: []
  type: TYPE_NORMAL
- en: Crawling your first website
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to scrape a website, we first need to download its web pages containing
    the data of interest, a process known as **crawling**. There are a number of approaches
    that can be used to crawl a website, and the appropriate choice will depend on
    the structure of the target website. This chapter will explore how to download
    web pages safely, and then introduce the following three common approaches to
    crawling a website:'
  prefs: []
  type: TYPE_NORMAL
- en: Crawling a sitemap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterating each page using database IDs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following web page links
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have so far used the terms scraping and crawling interchangeably, but let's
    take a moment to define the similarities and differences in these two approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping versus crawling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on the information you are after and the site content and structure,
    you may need to either build a web scraper or a website crawler. What is the difference?
  prefs: []
  type: TYPE_NORMAL
- en: A web scraper is usually built to target a particular website or sites and to
    garner specific information on those sites. A web scraper is built to access these
    specific pages and will need to be modified if the site changes or if the information
    location on the site is changed. For example, you might want to build a web scraper
    to check the daily specials at your favorite local restaurant, and to do so you
    would scrape the part of their site where they regularly update that information.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, a web crawler is usually built in a generic way; targeting either
    websites from a series of top-level domains or for the entire web. Crawlers can
    be built to gather more specific information, but are usually used to *crawl*
    the web, picking up small and generic bits of information from many different
    sites or pages and following links to other pages.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to crawlers and scrapers, we will also cover web spiders in [Chapter
    8](py-web-scrp-2e_ch08.html), *Scrapy.* Spiders can be used for crawling a specific
    set of sites or for broader crawls across many sites or even the Internet.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, we will use specific terms to reflect our use cases; as you develop
    your web scraping, you may notice distinctions in technologies, libraries, and
    packages you may want to use. In these cases, your knowledge of the differences
    in these terms will help you select an appropriate package or technology based
    on the terminology used (such as, is it only for scraping? Is it also for spiders?).
  prefs: []
  type: TYPE_NORMAL
- en: Downloading a web page
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To scrape web pages, we first need to download them. Here is a simple Python
    script that uses Python''s `urllib` module to download a URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When a URL is passed, this function will download the web page and return the
    HTML. The problem with this snippet is that, when downloading the web page, we
    might encounter errors that are beyond our control; for example, the requested
    page may no longer exist. In these cases, `urllib` will raise an exception and
    exit the script. To be safer, here is a more robust version to catch these exceptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now, when a download or URL error is encountered, the exception is caught and
    the function returns `None`.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we will assume you are creating files with code that is
    presented without prompts (like the code above). When you see code that begins
    with a Python prompt `>>>` or and IPython prompt `In [1]:`, you will need to either
    enter that into the main file you have been using, or save the file and import
    those functions and classes into your Python interpreter. If you run into any
    issues, please take a look at the code in the book repository at [https://github.com/kjam/wswp](https://github.com/kjam/wswp).
  prefs: []
  type: TYPE_NORMAL
- en: Retrying downloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, the errors encountered when downloading are temporary; an example is
    when the web server is overloaded and returns a `503 Service Unavailable` error.
    For these errors, we can retry the download after a short time because the server
    problem may now be resolved. However, we do not want to retry downloading for
    all errors. If the server returns `404 Not Found`, then the web page does not
    currently exist and the same request is unlikely to produce a different result.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full list of possible HTTP errors is defined by the *Internet Engineering
    Task Force*, and is available for viewing at [https://tools.ietf.org/html/rfc7231#section-6](https://tools.ietf.org/html/rfc7231#section-6).
    In this document, we can see that `4xx` errors occur when there is something wrong
    with our request and `5xx` errors occur when there is something wrong with the
    server. So, we will ensure our `download` function only retries the `5xx` errors.
    Here is the updated version to support this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when a `download` error is encountered with a `5xx` code, the `download` error
    is retried by recursively calling itself. The function now also takes an additional
    argument for the number of times the download can be retried, which is set to
    two times by default. We limit the number of times we attempt to download a web
    page because the server error may not recover. To test this functionality we can
    try downloading [http://httpstat.us/500](http://httpstat.us/500), which returns
    the 500 error code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the `download` function now tries downloading the web page, and
    then, on receiving the 500 error, it retries the download twice before giving
    up.
  prefs: []
  type: TYPE_NORMAL
- en: Setting a user agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, `urllib` will download content with the `Python-urllib/3.x` user
    agent, where `3.x` is the environment's current version of `Python`. It would
    be preferable to use an identifiable user agent in case problems occur with our
    web crawler. Also, some websites block this default user agent, perhaps after
    they have experienced a poorly made Python web crawler overloading their server.
    For example,  [http://www.meetup.com/](http://www.meetup.com/) currently returns
    a `403 Forbidden `when requesting the page with `urllib`'s default user agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'To download sites reliably, we will need to have control over setting the user
    agent. Here is an updated version of our `download` function with the default
    user agent set to `''wswp''` (which stands for**Web Scraping with Python**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: If you now try meetup.com, you will see valid HTML. Our download function can
    now be reused in later code to catch errors, retry the site when possible, and
    set the user agent.
  prefs: []
  type: TYPE_NORMAL
- en: Sitemap crawler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our first simple crawler, we will use the sitemap discovered in the example
    website's `robots.txt` to download all the web pages. To parse the sitemap, we
    will use a simple regular expression to extract URLs within the `<loc>` tags.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will need to update our code to handle encoding conversions as our current
    `download` function simply returns bytes. Note that a more robust parsing approach
    called **CSS selectors** will be introduced in the next chapter. Here is our first
    example crawler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can run the sitemap crawler to download all countries from the example
    website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As shown in our `download` method above, we had to update the character encoding
    to utilize regular expressions with the website response. The Python `read` method
    on the response will return bytes, and the `re` module expects a string. Our code
    depends on the website maintainer to include the proper character encoding in
    the response headers. If the character encoding header is not returned, we default
    to UTF-8 and hope for the best. Of course, this decoding will throw an error if
    either the header encoding returned is incorrect or if the encoding is not set
    and also not UTF-8\. There are some more complex ways to guess encoding (see: [https://pypi.python.org/pypi/chardet](https://pypi.python.org/pypi/chardet)),
    which are fairly easy to implement.
  prefs: []
  type: TYPE_NORMAL
- en: For now, the Sitemap crawler works as expected. But as discussed earlier, `Sitemap`
    files often cannot be relied on to provide links to every web page. In the next
    section, another simple crawler will be introduced that does not depend on the
    `Sitemap` file.
  prefs: []
  type: TYPE_NORMAL
- en: If you don't want to continue the crawl at any time you can hit *Ctrl + C* or
    c*md* + *C* to exit the Python interpreter or program execution.
  prefs: []
  type: TYPE_NORMAL
- en: ID iteration crawler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will take advantage of a weakness in the website structure
    to easily access all the content. Here are the URLs of some sample countries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://example.webscraping.com/view/Afghanistan-1](http://example.webscraping.com/view/Afghanistan-1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://example.webscraping.com/view/Australia-2](http://example.webscraping.com/view/Australia-2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://example.webscraping.com/view/Brazil-3](http://example.webscraping.com/view/Brazil-3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can see that the URLs only differ in the final section of the URL path,
    with the country name (known as a slug) and ID. It is a common practice to include
    a slug in the URL to help with search engine optimization. Quite often, the web
    server will ignore the slug and only use the ID to match relevant records in the
    database. Let''s check whether this works with our example website by removing
    the slug and checking the page [http://example.webscraping.com/view/1](http://example.webscraping.com/view/1):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/chp1_using_id_to_load_page.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The web page still loads! This is useful to know because now we can ignore
    the slug and simply utilize database IDs to download all the countries. Here is
    an example code snippet that takes advantage of this trick:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can use the function by passing in the base URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we iterate the ID until we encounter a download error, which we assume
    means our scraper has reached the last country. A weakness in this implementation
    is that some records may have been deleted, leaving gaps in the database IDs.
    Then, when one of these gaps is reached, the crawler will immediately exit. Here
    is an improved version of the code that allows a number of consecutive download
    errors before exiting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The crawler in the preceding code now needs to encounter five consecutive `download`
    errors to stop iteration, which decreases the risk of stopping iteration prematurely
    when some records have been deleted or hidden.
  prefs: []
  type: TYPE_NORMAL
- en: Iterating the IDs is a convenient approach to crawling a website, but is similar
    to the sitemap approach in that it will not always be available. For example,
    some websites will check whether the slug is found in the URL and if not return
    a `404 Not Found` error. Also, other websites use large nonsequential or nonnumeric
    IDs, so iterating is not practical. For example, Amazon uses ISBNs, as the ID
    for the available books, that have at least ten digits. Using an ID iteration
    for ISBNs would require testing billions of possible combinations, which is certainly
    not the most efficient approach to scraping the website content.
  prefs: []
  type: TYPE_NORMAL
- en: As you've been following along, you might have noticed some download errors
    with the message `TOO MANY REQUESTS` . Don't worry about them at the moment; we
    will cover more about handling these types of error in the *Advanced Features* section
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Link crawlers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have implemented two simple crawlers that take advantage of the structure
    of our sample website to download all published countries. These techniques should
    be used when available, because they minimize the number of web pages to download.
    However, for other websites, we need to make our crawler act more like a typical
    user and follow links to reach the interesting content.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could simply download the entire website by following every link. However,
    this would likely download many web pages we don''t need. For example, to scrape
    user account details from an online forum, only account pages need to be downloaded
    and not discussion threads. The link crawler we use in this chapter will use regular
    expressions to determine which web pages it should download. Here is an initial
    version of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: To run this code, simply call the `link_crawler` function with the URL of the
    website you want to crawl and a regular expression to match links you want to
    follow. For the example website, we want to crawl the index with the list of countries
    and the countries themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'We know from looking at the site that the index links follow this format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://example.webscraping.com/index/1](http://example.webscraping.com/index/1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://example.webscraping.com/index/2](http://example.webscraping.com/index/2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The country web pages follow this format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://example.webscraping.com/view/Afghanistan-1](http://example.webscraping.com/view/Afghanistan-1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://example.webscraping.com/view/Aland-Islands-2](http://example.webscraping.com/view/Aland-Islands-2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So a simple regular expression to match both types of web page is `/(index|view)/`.
    What happens when the crawler is run with these inputs? You receive the following
    `download` error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Regular expressions are great tools for extracting information from strings,
    and I recommend every programmer [learn how to read and write a few of them](https://regexone.com/).
    That said, they tend to be quite brittle and easily break. We'll cover more advanced
    ways to extract links and identify their pages as we advance through the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem with downloading `/index/1` is that it only includes the path of
    the web page and leaves out the protocol and server, which is known as a **relative
    link**. Relative links work when browsing because the web browser knows which
    web page you are currently viewing and takes the steps necessary to resolve the
    link. However, `urllib` doesn''t have this context. To help `urllib` locate the
    web page, we need to convert this link into an **absolute link**, which includes
    all the details to locate the web page. As might be expected, Python includes
    a module in `urllib` to do just this, called `parse`. Here is an improved version
    of `link_crawler` that uses the `urljoin` method to create the absolute links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'When this example is run, you can see it downloads the matching web pages;
    however, it keeps downloading the same locations over and over. The reason for
    this behavior is that these locations have links to each other. For example, Australia
    links to Antarctica and Antarctica links back to Australia, so the crawler will
    continue to queue the URLs and never reach the end of the queue. To prevent re-crawling
    the same links, we need to keep track of what''s already been crawled. The following
    updated version of `link_crawler` stores the URLs seen before, to avoid downloading
    duplicates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: When this script is run, it will crawl the locations and then stop as expected.
    We finally have a working link crawler!
  prefs: []
  type: TYPE_NORMAL
- en: Advanced features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's add some features to make our link crawler more useful for crawling
    other websites.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing robots.txt
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we need to interpret `robots.txt` to avoid downloading blocked URLs.
    Python `urllib` comes with the `robotparser` module, which makes this straightforward,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The `robotparser` module loads a `robots.txt` file and then provides a `can_fetch()`function,
    which tells you whether a particular user agent is allowed to access a web page
    or not. Here, when the user agent is set to `'BadCrawler'`, the `robotparser`
    module says that this web page can not be fetched, as we saw in the definition
    in the example site's `robots.txt`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To integrate `robotparser` into the link crawler, we first want to create a
    new function to return the  `robotparser` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to reliably set the `robots_url`; we can do so by passing an extra
    keyword argument to our function. We can also set a default value catch in case
    the user does not pass the variable. Assuming the crawl will start at the root
    of the site, we can simply add `robots.txt` to the end of the URL. We also need
    to define the `user_agent`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we add the parser check in the `crawl` loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We can test our advanced link crawler and its use of `robotparser` by using
    the bad user agent string.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Supporting proxies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes it''s necessary to access a website through a proxy. For example,
    Hulu is blocked in many countries outside the United States as are some videos
    on YouTube. Supporting proxies with `urllib` is not as easy as it could be. We
    will cover `requests` for a more user-friendly Python HTTP module that can also
    handle proxies later in this chapter. Here''s how to support a proxy with `urllib`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an updated version of the `download` function to integrate this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The current `urllib` module does not support `https` proxies by default (Python
    3.5). This may change with future versions of Python, so check the latest documentation.
    Alternatively, you can use the documentation's recommended recipe ([https://code.activestate.com/recipes/456195/](https://code.activestate.com/recipes/456195/))
    or keep reading to learn how to use the `requests` library.
  prefs: []
  type: TYPE_NORMAL
- en: Throttling downloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we crawl a website too quickly, we risk being blocked or overloading the
    server(s). To minimize these risks, we can throttle our crawl by waiting for a
    set delay between downloads. Here is a class to implement this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This `Throttle` class keeps track of when each domain was last accessed and
    will sleep if the time since the last access is shorter than the specified delay.
    We can add throttling to the crawler by calling `throttle` before every download:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Avoiding spider traps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Currently, our crawler will follow any link it hasn't seen before. However,
    some websites dynamically generate their content and can have an infinite number
    of web pages. For example, if the website has an online calendar with links provided
    for the next month and year, then the next month will also have links to the next
    month, and so on for however long the widget is set (this can be a LONG time).
    The site may offer the same functionality with simple pagination navigation, essentially
    paginating over empty search result pages until the maximum pagination is reached.
    This situation is known as a **spider trap**.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple way to avoid getting stuck in a spider trap is to track how many links
    have been followed to reach the current web page, which we will refer to as `depth`.
    Then, when a maximum depth is reached, the crawler does not add links from that
    web page to the queue. To implement maximum depth, we will change the `seen` variable,
    which currently tracks visited web pages, into a dictionary to also record the
    depth the links were found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Now, with this feature, we can be confident the crawl will complete eventually.
    To disable this feature, `max_depth` can be set to a negative number so the current
    depth will never be equal to it.
  prefs: []
  type: TYPE_NORMAL
- en: Final version
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The full source code for this advanced link crawler can be downloaded at [https://github.com/kjam/wswp/blob/master/code/chp1/advanced_link_crawler.py](https://github.com/kjam/wswp/blob/master/code/chp1/advanced_link_crawler.py).
    Each of the sections in this chapter has matching code in the repository at[ ](https://github.com/kjam/wswp)https://github.com/kjam/wswp.
    To easily follow along, feel free to fork the repository and use it to compare
    and test your own code.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test the link crawler, let''s try setting the user agent to `BadCrawler`,
    which, as we saw earlier in this chapter, was blocked by `robots.txt`. As expected,
    the crawl is blocked and finishes immediately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s try using the default user agent and setting the maximum depth
    to `1` so that only the links from the home page are downloaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the crawl stopped after downloading the first page of countries.
  prefs: []
  type: TYPE_NORMAL
- en: Using the requests library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we have built a fairly advanced parser using only `urllib`, the majority
    of scrapers written in Python today utilize the `requests` library to manage complex
    HTTP requests. What started as a small library to help wrap `urllib` features
    in something "human-readable" is now a very large project with hundreds of contributors.
    Some of the features available include built-in handling of encoding, important
    updates to SSL and security, as well as easy handling of POST requests, JSON,
    cookies, and proxies.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout most of this book, we will utilize the requests library for its simplicity
    and ease of use, and because it has become the de facto standard for most web
    scraping.
  prefs: []
  type: TYPE_NORMAL
- en: To install `requests`, simply use `pip:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: For an in-depth overview of all features, you should read the documentation
    at [http://python-requests.org](http://python-requests.org) or browse the source
    code at [https://github.com/kennethreitz/requests](https://github.com/kennethreitz/requests).
  prefs: []
  type: TYPE_NORMAL
- en: 'To compare differences using the two libraries, I''ve also built the advanced
    link crawler so that it can use requests. You can see the code at [https://github.com/kjam/wswp/blob/master/code/chp1/advanced_link_crawler_using_requests.py](https://github.com/kjam/wswp/blob/master/code/chp1/advanced_link_crawler_using_requests.py).
    The main `download` function shows the key differences. The `requests` version
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'One notable difference is the ease of use of having `status_code` as an available
    attribute for each request. Additionally, we no longer need to test for character
    encoding, as the `text` attribute on our `Response` object does so automatically.
    In the rare case of an non-resolvable URL or timeout, they are all handled by `RequestException`
    so it makes for an easy catch statement. Proxy handling is also taken care of by
    simply passing a dictionary of proxies (that is `{''http'': ''http://myproxy.net:1234'',
    ''https'': ''https://myproxy.net:1234''}`).'
  prefs: []
  type: TYPE_NORMAL
- en: We will continue to compare and use both libraries, so that you are familiar
    with them depending on your needs and use case. I strongly recommend using `requests`
    whenever you are handling more complex websites, or need to handle important humanizing
    methods such as using cookies or sessions. We will talk more about these methods
    in [Chapter 6](py-web-scrp-2e_ch06.html), *Interacting with Forms*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced web scraping and developed a sophisticated crawler that
    will be reused in the following chapters. We covered the usage of external tools
    and modules to get an understanding of a website, user agents, sitemaps, crawl
    delays, and various advanced crawling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore how to scrape data from crawled web pages.
  prefs: []
  type: TYPE_NORMAL
