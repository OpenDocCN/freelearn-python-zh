- en: Achieve a Goal through Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the background on reinforcement learning that we provided in the previous
    chapter, we will go one step forward with GoPiGo3, making it not only perform
    perception tasks, but also trigger chained actions in sequence to achieve a pre-defined
    goal. That it is to say, it will have to decide what action to execute at every
    step of the simulation to achieve the goal. At the end of the execution of every
    action, it will be provided with a reward, which will show how good the decision
    was by the amount of reward given. After some training, this reinforcement will naturally
    drive its next decisions, improving the performance of the task.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let's say that we set a target location and instruct the robot
    that it has to carry an object there. The way in which GoPiGo3 will be told that
    it is performing well is by giving it rewards. This way of providing feedback
    encourages it to pursue the goal. In specific terms, the robot has to select from
    a set of possible actions (move forward, backward, left, or right), and select
    the most effective action in each step, since the optimum action will depend on
    the robot's physical location in the environment.
  prefs: []
  type: TYPE_NORMAL
- en: The field of machine learning that deals with this kind of problem is known
    as **reinforcement learning**, and it is a very active topic of research. It has
    surpassed human performance in some scenarios, as in the recent case of **Alpha
    Go**, [https://deepmind.com/blog/article/alphago-zero-starting-scratch](https://deepmind.com/blog/article/alphago-zero-starting-scratch).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the environment with TensorFlow, Keras, and Anaconda
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing the ROS machine learning packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting the training task parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training GoPiGo3 to reach a target location while avoiding obstacles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will find in the practice case how GoPiGo3 learns by trying different actions,
    being encouraged to select the most effective action for every location. You may
    have guessed that this is a very costly computational task, and you will get an
    idea of the challenge robotics engineers are facing nowadays to make robots smarter.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will make use of the code located in the `Chapter12_Reinforcement_Learning` folder: [https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter12_Reinforcement_Learning](https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter12_Reinforcement_Learning).
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy the files of this chapter to the ROS workspace, putting them inside the
    `src` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As usual, you need to rebuild the workspace in the laptop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Once you have the code for the chapter, we dedicate the next section to describing
    and installing the software stack for the practical project we will develop.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the environment with TensorFlow, Keras, and Anaconda
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Together with **Anaconda**, which you were instructed to install in the previous
    chapter, you will now install the machine learning tools **TensorFlow** and **Keras**.
    You will need them to make the neural networks that are required to solve the
    reinforcement learning tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TensorFlow** is the low-level layer of your machine learning environment.
    It deals with the mathematical operations involved in the creation of neural networks.
    Since they are mathematically resolved as matrix operations, you need a framework
    that is effective at solving this algebra, and TensorFlow is one of the most efficient
    frameworks for that. The name of the library comes from the mathematical concept
    of a *tensor*, which can be understood as a matrix with more than two dimensions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keras** is the high-level layer of your machine learning environment. This
    library lets you easily define the structure of the neural network in a declarative
    way: you just have to define the structure of the nodes and edges, and TensorFlow
    (the low-level layer) will take care of all the mathematical operations to create
    the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, we will make use of the isolation feature that Anaconda provides. Remember
    that in the previous chapter you created a **Conda** environment called **gym**,
    inside which you installed **OpenAI Gym** together with TensorFlow and Keras.
    Now you will be instructed to work in a different Conda environment, where you
    will install only the modules you will need for this chapter. This way, you keep
    the code for each chapter isolated, as they apply to different projects. In fact,
    you will install specific versions of TensorFlow and Keras that may be different
    than the latest versions that were used in the previous chapter. This is a common
    way to proceed in Python projects.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have clarified what each component provides, let's install each of them.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow backend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, create a dedicated `conda` environment called `tensorflow`. It consists
    of a virtual space that lets users isolate the set of Python packages you will
    use for a specific project. This has the advantage of making it straightforward
    to replicate the environment in another machine with minimal effort:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The second line produces the activation and binds the next installations to
    this `tensorflow` environment.
  prefs: []
  type: TYPE_NORMAL
- en: Conda enviroment are isolated buckets that contain the python modules you need
    for a specific **project**. For example, for **tensorflow** environment, every
    Python module you install with either `conda install` or `pip install` will be
    placed at `~/anaconda2/envs/tensorflow/bin`. The **activation** means that whenever
    a Python scripts needs to import some module, it will look for it at such **project**
    path.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you can proceed to install TensorFlow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, you should also install `matplotlib` and `pyqtgraph` in order
    to draw graphs of the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then check for the versions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: These last two commands have been added to give you practical examples of common `conda` commands.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Keras is a high-level neural network API, written in Python and capable of
    running on top of TensorFlow. You can easily install a specific version with this
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We have specified version 2.1.5to make sure that you run the code in exactly
    the same environment we tested it in. The latest version at the time of writing
    is 2.3.1.
  prefs: []
  type: TYPE_NORMAL
- en: ROS dependency packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To use **ROS** and **Anaconda** together, you must additionally install the
    ROS dependency packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You can check the version of any of them by using `pip show`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will describe the machine learning package of the code
    for this chapter, which you should have already cloned to your ROS workspace,
    as per the *Technical requirements* section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the ROS Machine Learning packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this chapter implements the classical reinforcement learning methodology
    of training a neural network. This neural network is mathematically similar to
    the one we introduced in [Chapter 10](3bf944de-e0f8-4e78-a38b-47796c91185b.xhtml), *Applying
    Machine Learning in Robotics*, stacking layers of (hidden) nodes to establish
    a relationship between the states (the input layer) and the actions (the output
    layer).
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm we will use for reinforcement learning is called **Deep Q-Network**
    (**DQN**) and was introduced in [Chapter 11](c5f1bcd8-306d-4ef8-b0ad-982c8bbb2b73.xhtml),
    *Machine Learning with OpenAI Gym* in the *Running an environment *section. In
    the next section, *Setting the training task parameters*, you will be given the
    operational description of states, actions, and rewards that characterize the
    reinforcement learning problem that we are going to solve with ROS.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will present the training scenarios, and then we will explain how the
    files inside the ROS packages are chained to launch a training task.
  prefs: []
  type: TYPE_NORMAL
- en: Training scenarios
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section is devoted to explaining how the reinforcement learning package – the
    content of the code sample provided with the book repository – is organized.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s take into account that we are going to train the robot for two
    scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scenario 1: **Travel to a target location**. This scene is shown in the following
    image and consists of a square limited by four walls. There are no obstacles in
    the environment. The target location can be any point within the limits of the
    walls:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/709dcd93-2212-43d4-a12b-7853f71f5567.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Scenario 2: **Travel to a target location avoiding obstacles**. This scene
    consists of the same square plus four static cylindrical obstacles. The target
    location can be any point within the limits of the walls, except for the locations
    of the obstacles:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/0dd52063-189c-403b-8275-99abfa6fca1c.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have presented the training scenarios, let's describe the ROS package
    that we will use for reinforcement learning for GoPiGo3.
  prefs: []
  type: TYPE_NORMAL
- en: ROS package structure for running a reinforcement learning task
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The stack of code for this chapter is composed of the following packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**`gopigo3_model`** is the ROS compilation that allows us to visualize GoPiGo3
    in RViz using the launch file `gopigo3_rviz.launch`. The robot configuration is
    the familiar 3D solid model that we have used in the previous chapters. More precisely,
    it corresponds to the GoPiGo3 complete version, which includes the distance sensor,
    the Pi camera, and the laser distance sensor, that is, the model that was built
    in [Chapter 8](25ac032c-5bfe-47ff-aa5a-f178dbff7c57.xhtml),  *Virtual SLAM and
    Navigation Using Gazebo*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`gopigo3_gazebo`** is built on top of the previous package, allowing us to
    simulate GoPiGo3 in Gazebo using the `gopigo3_world.launch` file. The URDF model
    that this file loads is the same as the one in the visualization in the `gopigo3_model`
    package.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`gopigo3_dqn`** is the specific package for carrying out the reinforcement
    learning task with GoPiGo3\. As it is ROS, it is modular, and the decoupling we
    provide by separating model, simulation, and reinforcement learning makes it straightforward
    to use this same package to train other robots.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this ROS package, we use the **DQN** algorithm to train the robot. Remember
    that DQN was introduced in the previous chapter, in the *Running an environment *section.
    In brief, what DQN will do is establish a relation between the *states of the
    robot* and the *actions to execute* using a neural network, where the *states*
    are the **input layer** and the *actions* are the **output layer**.
  prefs: []
  type: TYPE_NORMAL
- en: The mathematics behind reinforcement learning theory is complex, and it is not
    absolutely necessary to learn how to use this technique to train simple robots
    as GoPiGo3\. So, let's focus on the configuration of the training tasks, abstracting
    the specific implementation in Python of the example that supports the practical
    elements of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the training task parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, we are briefly going to introduce the three essential concepts
    in reinforcement learning: **states**, **actions**, and **rewards**. In this section,
    we will give you minimal information so that you can understand the practical
    exercise in this chapter. In this case, we are applying the strategy of *focus
    on the practice to really understand the theory*.'
  prefs: []
  type: TYPE_NORMAL
- en: This method of *focus on the practice to really understand the theor*y is especially
    required for complex topics that are better understood if you follow an empirical
    approach with easy-to-run examples. This preliminary *practical success* should
    provide you with enough motivation to get deeper into the topic, a task that in
    any case will be hard both in the algorithms and in the mathematics behind them.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s proceed to define these core concepts involved in the learning task
    of the robot:'
  prefs: []
  type: TYPE_NORMAL
- en: A **state** is an *observation of the environment*. Thanks to the data streaming
    from the LDS, the state is characterized by the range and the angle to the goal
    location. For example, if you get LDS measurements with a one-degree resolution,
    each state will be a set of 360 points, each value corresponding to every angle
    in the full circumference of 360º. As the robot moves, the state changes, and
    this is reflected in the new set of 360 range values provided by the LDS. Remember
    that each range value corresponds to the distance to the nearest obstacle in that
    specific direction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **action** is what the robot can do using its motors to move in the environment,
    that is, translate and/or rotate to approach the target. By executing an action,
    the robot moves and changes its **state** – defined by the new set of range values
    coming from the LDS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **reward** is the *prize* you give to the robot every time it executes an
    action. The prize you give for each action in every possible state is called the
    **reward policy**, and it is an essential part of the success of a reinforcement
    learning task. Hence, it has to be defined by the user (the *trainer*). In plain
    words, you will give a greater reward to an action the robot performs that is
    more effective in achieving the goal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the practical case, we are going to run the reward policy as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If the obstacle is in the forward half space of the robot (180° angle covering
    left to right in the forward motion direction), it obtains a positive angle-based
    reward ranging from 0 to 5\. The maximum value (5) corresponds to the case in
    which the robot is oriented in the target direction. We specify this half-space
    by the relative angle being between -90° and +90° (in this angle the reward is
    the minimum, that is 0).  The reference direction is the line crossing the target
    location and the robot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If it is in the back half space of the robot (180° angle covering left to right
    opposite to forward half space), the obtained reward is negative*,* ranging from
    0 to -5 (linear dependence with respect to the angle: 0 at 90° and -90°, and -5
    at -180°).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the current distance from the goal is above a preset threshold, the agent
    obtains adistance-based reward >2. If it is below this threshold, the reward is
    lower than 2, approaching the minimum value of 1 as the robot gets closer. Then,
    the **approaching reward** is the dot product of the angle and distance-based
    rewards*=* **[a]** *** **[b]**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the robot achieves the goal, a **success reward** of 200 is given.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the robot crashes into an obstacle, a **penalty** of 150 is given, that is,
    a negative reward of -150.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reward is cumulative, and we could add any of these terms to the reward
    at every step of the simulation:'
  prefs: []
  type: TYPE_NORMAL
- en: Approaching reward
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Success reward
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obstacle penalty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **step** in the simulation is *what happens to the robot between two consecutive
    states*. And what happens is, that the robot executes an action, and – as a consequence
    – it moves, changing its state and obtaining a reward based on the policy defined
    in this section.
  prefs: []
  type: TYPE_NORMAL
- en: After this task configuration is understood, you are ready to run the training
    of GoPiGo3 in the two scenarios that have been defined.
  prefs: []
  type: TYPE_NORMAL
- en: Training GoPiGo3 to reach a target location while avoiding obstacles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prior to running training in the scenario, we should note the adjustment of
    a parameter that dramatically affects the computational cost. This is the horizontal
    sampling of the LDS, since the **state** of the robot is characterized by the
    set of range values in a given step of the simulation. In previous chapters, when
    we performed navigation in Gazebo, we used a sampling rate of 720 for LDS. This
    means that we have circumferential range measurements at 1º resolution.
  prefs: []
  type: TYPE_NORMAL
- en: For this example of reinforcement learning, we are reducing the sampling to
    24, which means a range resolution of 15º. The positive aspect of this decision
    is that you reduce the **state** vector from 360 items to 24, which is a factor
    of 15\. You may have guessed that this will make the simulation more computationally
    efficient. In contrast, you will find that the drawback is that GoPiGo3 loses
    its perception capability, since it will only be able to detect objects whose
    angle coverage with respect to the point of view of the robot is larger than 15º.
    At a distance of 1 meter, this is equivalent to a minimum obstacle width of 27
    cm.
  prefs: []
  type: TYPE_NORMAL
- en: On a positive note, as the robot gets closer to an obstacle, its discrimination
    capability improves. For example, at a distance of 10 cm, an arc of 15º means
    it can detect obstacles with a minimum width of 5.4 cm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The horizontal sampling is set in the URDF model, in the part of the file that
    describes the LDS, located at `./gopigo3_model/urdf/gopigo3.gazebo`. The number
    to specify for obtaining rays with 15º spacing is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7961a60-89f9-457c-93c9-1ac292cc2cb1.png)'
  prefs: []
  type: TYPE_IMG
- en: Since LDS covers from 0º to 360º, to get 24 equally spaced rays, you have add
    one more sample, making it 25, since 0º and 360º are actually the same angle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the LDS part of the URDF file has to be modified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: By setting the `<visualize>` tag to `true`, the ray tracing is shown in Gazebo.
  prefs: []
  type: TYPE_NORMAL
- en: 'Is this sampling enough to ensure that the robot can be effectively trained?
    Let''s answer the question comparing the number of rays for every case. This first
    figure shows the 0.5° actual resolution of the physical LDS. Rays are so close
    to each other that you cannot almost see the resolution. It provides very faithful
    sensing of the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e6ea91e-bbd6-41fc-8e7e-d2e1cff32850.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This second image shows the case of 24 samples and 15º resolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f61be4c-4697-4f54-b419-8a8fab1d0d75.png)'
  prefs: []
  type: TYPE_IMG
- en: In this picture, the ray tracing shows that, even with so few rays, the obstacles
    are detected, since only one ray is needed to identify an obstacle. This fact
    helps to mitigate the loss of perception resolution. However, bear in mind that
    the robot will not be able to know the obstacle's width, only that it will be
    less than 30º. Why? Because you need three rays to detect an obstacle of finite
    width, the central ray detecting it and the extreme ones not interfering with
    it. Hence, this upper limit for the obstacle width is equal to twice the angular
    distance between adjacent rays, that is, *2 x 15º = 30º*. In some cases, this
    may be too imprecise, but, for the simple scenario we are using in this example,
    it should be precise enough.
  prefs: []
  type: TYPE_NORMAL
- en: How to run the simulations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before beginning the training process for each scenario, let''s recap what
    we learned in the previous chapter about visualization (RViz) and simulation (Gazebo)
    to establish a link with the learning process, which will make use of these tools
    and related scripts:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To launch the visualization in RViz, you have to simply execute the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To launch the simulation in Gazebo, you may proceed in a similar manner using
    this single command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, to run the reinforcement learning task you first have to launch Gazebo
    – as explained in step 2 – but with the selected training environment, instead
    of the general `gopigo3_world.launch`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'These two commands run the task in **scenario 1** described earlier. To perform
    the training for **scenario 2**, you only need to execute the corresponding launch
    files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The first line loads the scenario 2 environment, and the second launches the
    training task for it.
  prefs: []
  type: TYPE_NORMAL
- en: The following two sub-sections show ROS in action by executing the commands
    in step 3.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 1 – travel to a target location
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Follow the next procedure to make sure that the training process happens as
    expected:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, launch the virtual robot model in Gazebo:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you can start the training process. But first, you have to be in the
    `tensorflow` virtual environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, start the training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You might receive an error like this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Don''t worry, you can solve it by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Hence, if you receive the error above, solve it as suggested and launch the
    training script again. Then subscribe to the relevant topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`get_action` is a `Float32MultiArray` message type whose data definition is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see each component:'
  prefs: []
  type: TYPE_NORMAL
- en: GoPiGo3 always has a linear velocity of 0.15 m/s. The `action` item changes
    the angular velocity from -1.5 to 1.5 rad/s in steps of 0.75, to cover the integer
    range from 0 to 4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The obtained `reward` in each step is as was described in the *Setting the training
    task parameters* section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`score` is the cumulative reward that the robot obtains in each episode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The corresponding ROS graph can be seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2304b8fd-3115-40c7-8100-ce730b6c0962.png)'
  prefs: []
  type: TYPE_IMG
- en: The key node in this graph is `gopigo3_dqn_stage_1`, which takes the robot state
    from the Gazebo simulation and performs the training task by issuing `cmd_vel`
    messages (remember that the velocity commands that drive the robot are published in
    this topic) and getting the reward for every new state GoPiGo3 achieves.
  prefs: []
  type: TYPE_NORMAL
- en: 'The episode record can be followed in the console log of terminal **T2**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dbd99d98-dc54-42d8-aeba-dcaa644b9dc1.png)'
  prefs: []
  type: TYPE_IMG
- en: The red square is the target location and the blue lines are the LDS rays as
    described at the beginning of the section.
  prefs: []
  type: TYPE_NORMAL
- en: This first scenario aims to get you familiar with the training process in ROS.
    Let's move on to scenario 2, in which we will give quantitative information about
    how the training process improves through the episodes.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 2 – travel to a target location avoiding the obstacles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The procedure to start the training is similar to scenario 1:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We only need to change the name of the files and use the relevant ones:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to see graphically how the GoPiGo3 learning process is performing,
    execute the `result_graph.launch` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows all the content you should see on the screen
    of your laptop:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c69b591e-269a-44e6-8acc-eddd92fdc9a1.png)'
  prefs: []
  type: TYPE_IMG
- en: The red graph shows the **Total rewa****rd** obtained in each episode. Remember
    that an episode is defined as the sequence of steps that comes to an end when
    a criterion is met. In this problem, an episode finishes if the goal (the red
    square) is reached or if there is a collision with an obstacle. These graphs show
    the evolution in the first 300 episodes.
  prefs: []
  type: TYPE_NORMAL
- en: The green graph represents the average Q value of the trained model. Remember
    that this is the action-value function *Q(s,a)* that tells you how good it is
    to execute an action *a* in a given state *s*. This concept was explained in the
    previous chapter in the basic example of the self-driving cab in the *Q-learning
    explained* section.
  prefs: []
  type: TYPE_NORMAL
- en: You can see how, on average, GoPiGo3 is performing better as it accumulates
    experience. But how is it operationally using that experience? The answer comes
    from the reinforcement learning algorithm that has been applied, that is, by associating
    effective actions to every state given the rewards the robot has been receiving
    in that state during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we should note that in an execution environment like this, the ROS
    graph alternates between two states. One is when the robot executes an action
    by publishing a `cmd_vel` message that moves the robot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f812a21e-7455-4e92-b73b-80c61c91bf5a.png)'
  prefs: []
  type: TYPE_IMG
- en: For clarity, in this graph we are excluding the nodes of the launch file issued
    in terminal `T3`**.**
  prefs: []
  type: TYPE_NORMAL
- en: 'The other ROS graph corresponds to the instants in which the agent computes
    the next state using the set of 24 values coming from the LDS in the `/scan` topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb27cf2e-180a-47e0-8940-abb25374aced.png)'
  prefs: []
  type: TYPE_IMG
- en: At this point, you have covered the end-to-end training process of GoPiGo3\.
    The next challenge is to test the trained model.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the trained model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Taking into account that we are using the **Deep Q-Network** (**DQN**)algorithm,
    what we have to save from the training process is the structure of the neural
    network and the weights of the edges. This is what the `gopigo3_dqn_stage_2` node
    performs every 10 episodes. Hence, you can find the saved models inside the `./gopigo3_dqn/save_model` folder, and
    the weights of the network are stored in the `h5` file type. Every file contains
    in its name the scenario (`stage_1` or `stage_2`) and the episode number. Follow
    these steps to evaluate the trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: Select the file with the highest episode number, that is, `stage_2_1020.h5`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Every `h5` file contains the weights of the DQN network as it appears at the
    end of the episode referenced by the filename. For example, `stage_2_1020.h5`
    refers to the network of scenario 2 at the end of episode 1020.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use these weights, you basically have to use the same Python script
    of the training model (`./nodes/gopigo3_dqn_stage_2`), but initialize with different
    values the parameters marked in bold letters in the snippet below that reproduces
    the first lines of the `class ReinforceAgent()` definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, what each parameter provides is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`self.load_model = True` tells the script to load the weights of a pretrained
    model.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.load_episode = 1020`  sets the number of the episode from which you want
    to load the DQN network weights, being the corresponding file `stage_2_1020.h5`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then rename the Python script as `gopigo3_dqn_stage_2-test` and generate the
    new launch file `gopigo3_dqn_stage_2-test.launch`, which will call the created
    test script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'To launch the test process, follow the same steps as for running the training
    scenario, but using the test version of the launch file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that for `T2` you have to activate the TensorFlow environment with
    the `$ conda activate tensorflow` command. When it starts, you will see in `T2`,
    a message telling you that a model from episode 1380 will be used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'If you plot the graphs for the first episodes (as per the command in terminal
    `T3`), you can confirm that the values are pretty good, that is, above 2,000 for
    the *Total reward*  and above 100 for the *Average max Q-value*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ce631c69-8dce-4f2b-a122-9c9a83514da0.png)'
  prefs: []
  type: TYPE_IMG
- en: Although you are testing the model, every ten episodes the network's weights
    are saved to an `h5` file referencing the current episode number.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has been a quick and practical introduction to how you can apply
    reinforcement learning so that a robot can perform useful tasks such as transporting
    materials to a target location. You should be aware that this kind of machine
    learning technique is at the very beginning of its maturity, and there are as
    yet few practical solutions working in the real world. The reason is that the
    process of training is very expensive in terms of time and cost, since you have
    to perform thousands of episodes to get a well-trained model, and later replay
    the process with the physical robot to address behavioral differences between the
    real world and the simulated environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Be aware that the training process in Gazebo is not a substitute for training
    in the real world: a simulation necessarily implies a simplification of the reality,
    and every difference between the training environment (Gazebo) and the physical
    world introduces new states that can be missing in the training set, and hence
    the trained neural network will not be able to perform well in such situations.
    The solution? More training to cover more states, something that also means higher
    cost.'
  prefs: []
  type: TYPE_NORMAL
- en: In the last part of the chapter, we have also covered the testing of a model
    using the same scenario in which the robot was trained. A more formal testing
    approach requires that you check how the trained model generalizes to different
    conditions in the scenario, such as having more obstacles or moving their positions. This
    is a complex topic, since reinforcement learning algorithms currently struggle
    to achieve generalization. Why? Because, when you introduce changes in the scenario,
    you are generating new states in the models. Since they are new to the robot,
    it will not know the most effective action to execute. Hence, new training is
    required to explore these new states.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning is currently a very active field of research, and we
    should expect great advances in the years to come. What we should see is reinforcement
    learning being applied to real robots at a reasonable cost (that is, training
    a robot at a pace that doesn't require thousands of episodes) and providing techniques
    for the generalization of the models to environments other than those that were
    used for training.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter closes the introduction to the application of machine learning
    in robotics. In this book, we have only scratched the surface of its potential,
    and, if you have followed the explanations you should have checked by yourself
    that this is a complex field where at some point you will have to master statistics,
    data analytics, and neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, it is a field where the focus is on experimentation. Instead
    of trying to model the reality with analytical formulas or computer-aided simulation,
    you observe the real world, get data from sensors, and try to infer patterns of
    behavior from them. Hence, the ability to successfully apply machine learning
    to robotics relies on being capable of streaming data continuously so that the
    robot can make smart decisions in real time. And the first step is to produce
    well-trained models. For this reason, a robot will be able to develop smart behavior
    in the medium and long term, as it accumulates experience that can be made available
    in structured trained models.
  prefs: []
  type: TYPE_NORMAL
- en: This is a challenging goal, both for the data scientist and the software engineer.
    They should work together to create mature robot frameworks that benefit from
    machine learning as much as common web applications and digital businesses are
    today.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, many thanks for reading the book. At this point, you are challenged
    to explore advanced ROS topics, and we hope you can also become an active contributor
    to the ROS open source community.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the essential concepts of reinforcement learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) Robot actions and penalties
  prefs: []
  type: TYPE_NORMAL
- en: B) Neural networks and deep learning
  prefs: []
  type: TYPE_NORMAL
- en: C) States, actions, and rewards
  prefs: []
  type: TYPE_NORMAL
- en: Why do you need to use neural networks in reinforcement learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) Because the robot needs to use deep learning to recognize objects and obstacles.
  prefs: []
  type: TYPE_NORMAL
- en: B) Because the robot has to learn to associate states with the most effective
    actions.
  prefs: []
  type: TYPE_NORMAL
- en: C) We do not need neural networks in reinforcement learning; we apply different
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: How do you encourage the robot to achieve the goal of the task?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) By giving it rewards when it performs *good* actions.
  prefs: []
  type: TYPE_NORMAL
- en: B) By giving it penalties when it performs *bad* actions.
  prefs: []
  type: TYPE_NORMAL
- en: C) By giving it rewards when it performs *good* actions, and penalties in the
    case of *bad* actions.
  prefs: []
  type: TYPE_NORMAL
- en: Can you apply the reinforcement learning ROS package from this chapter to other
    robots?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) Yes, because we have separated the robot model, the scenario and the training
    algorithm into different packages.
  prefs: []
  type: TYPE_NORMAL
- en: B) No, because you have to rewrite the ROS package for every scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'C) No: it is specific for training GoPiGo3.'
  prefs: []
  type: TYPE_NORMAL
- en: Do you need to use the full data feed coming from a real LDS to train a robot?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A) Yes: if you want to obtain accurate results; you have to use all the data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'B) No: you have to decide the ray tracing density as a function of the typical
    size of the obstacles in the scenario.'
  prefs: []
  type: TYPE_NORMAL
- en: 'C) No: it depends on how much accuracy you require.'
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To delve deeper into the concepts explained in this chapter you can follow
    the following references:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Practical Reinforcement Learning* from **Coursera**: [https://www.coursera.org/learn/practical-rl](https://www.coursera.org/learn/practical-rl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Welcome to Deep Reinforcement Learning Part 1: DQN [https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-dqn-c3cab4d41b6b](https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-dqn-c3cab4d41b6b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables
    and Neural Networks [https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simple Reinforcement Learning with Tensorflow Part 4: Deep Q-Networks and Beyond [https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
