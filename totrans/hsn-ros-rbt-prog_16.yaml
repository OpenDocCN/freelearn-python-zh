- en: Achieve a Goal through Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过强化学习实现目标
- en: After the background on reinforcement learning that we provided in the previous
    chapter, we will go one step forward with GoPiGo3, making it not only perform
    perception tasks, but also trigger chained actions in sequence to achieve a pre-defined
    goal. That it is to say, it will have to decide what action to execute at every
    step of the simulation to achieve the goal. At the end of the execution of every
    action, it will be provided with a reward, which will show how good the decision
    was by the amount of reward given. After some training, this reinforcement will naturally
    drive its next decisions, improving the performance of the task.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章提供的强化学习背景之后，我们将进一步使用GoPiGo3，使其不仅执行感知任务，还能按顺序触发连锁动作以实现预定义的目标。也就是说，它将不得不在模拟的每一步决定执行什么动作以实现目标。在执行每个动作的末尾，它将获得一个奖励，这个奖励将通过给予的奖励量来显示决策有多好。经过一些训练，这种强化将自然会引导其后续决策，从而提高任务的表现。
- en: For example, let's say that we set a target location and instruct the robot
    that it has to carry an object there. The way in which GoPiGo3 will be told that
    it is performing well is by giving it rewards. This way of providing feedback
    encourages it to pursue the goal. In specific terms, the robot has to select from
    a set of possible actions (move forward, backward, left, or right), and select
    the most effective action in each step, since the optimum action will depend on
    the robot's physical location in the environment.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们设定了一个目标位置，并指示机器人必须携带一个物体到那里。GoPiGo3将被告知表现良好的方式是通过给予它奖励。这种方式提供反馈鼓励它追求目标。具体来说，机器人必须从一组可能的行为（向前移动、向后移动、向左或向右移动）中选择，并在每一步选择最有效的行为，因为最佳行为将取决于机器人在环境中的物理位置。
- en: The field of machine learning that deals with this kind of problem is known
    as **reinforcement learning**, and it is a very active topic of research. It has
    surpassed human performance in some scenarios, as in the recent case of **Alpha
    Go**, [https://deepmind.com/blog/article/alphago-zero-starting-scratch](https://deepmind.com/blog/article/alphago-zero-starting-scratch).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这类问题的机器学习领域被称为**强化学习**，这是一个非常活跃的研究课题。在某些场景中，它已经超越了人类的表现，例如最近的**Alpha Go**案例，[https://deepmind.com/blog/article/alphago-zero-starting-scratch](https://deepmind.com/blog/article/alphago-zero-starting-scratch)。
- en: 'The following topics will be covered in the chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Preparing the environment with TensorFlow, Keras, and Anaconda
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow、Keras和Anaconda准备环境
- en: Installing the ROS machine learning packages
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装ROS机器学习包
- en: Setting the training task parameters
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置训练任务参数
- en: Training GoPiGo3 to reach a target location while avoiding obstacles
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练GoPiGo3到达目标位置同时避开障碍物
- en: You will find in the practice case how GoPiGo3 learns by trying different actions,
    being encouraged to select the most effective action for every location. You may
    have guessed that this is a very costly computational task, and you will get an
    idea of the challenge robotics engineers are facing nowadays to make robots smarter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践案例中，您将看到GoPiGo3如何通过尝试不同的动作来学习，被鼓励为每个位置选择最有效的动作。您可能已经猜到，这是一个非常昂贵的计算任务，您将了解机器人工程师目前面临的挑战，即如何使机器人变得更聪明。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will make use of the code located in the `Chapter12_Reinforcement_Learning` folder: [https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter12_Reinforcement_Learning](https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter12_Reinforcement_Learning).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用位于`Chapter12_Reinforcement_Learning`文件夹中的代码：[https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter12_Reinforcement_Learning](https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter12_Reinforcement_Learning)。
- en: 'Copy the files of this chapter to the ROS workspace, putting them inside the
    `src` folder:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 将本章的文件复制到ROS工作空间中，将它们放在`src`文件夹内：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As usual, you need to rebuild the workspace in the laptop:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，您需要在笔记本电脑上重新构建工作空间：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Once you have the code for the chapter, we dedicate the next section to describing
    and installing the software stack for the practical project we will develop.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您有了本章的代码，我们将下一节专门用于描述和安装我们将要开发的实际项目的软件栈。
- en: Preparing the environment with TensorFlow, Keras, and Anaconda
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow、Keras和Anaconda准备环境
- en: 'Together with **Anaconda**, which you were instructed to install in the previous
    chapter, you will now install the machine learning tools **TensorFlow** and **Keras**.
    You will need them to make the neural networks that are required to solve the
    reinforcement learning tasks:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与你在上一章中被指导安装的**Anaconda**一起，你现在将安装机器学习工具**TensorFlow**和**Keras**。你需要它们来构建解决强化学习任务所需的神经网络：
- en: '**TensorFlow** is the low-level layer of your machine learning environment.
    It deals with the mathematical operations involved in the creation of neural networks.
    Since they are mathematically resolved as matrix operations, you need a framework
    that is effective at solving this algebra, and TensorFlow is one of the most efficient
    frameworks for that. The name of the library comes from the mathematical concept
    of a *tensor*, which can be understood as a matrix with more than two dimensions.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow**是机器学习环境中的低级层。它处理神经网络创建中涉及的数学运算。由于它们在数学上被解析为矩阵运算，你需要一个有效的框架来解决这个代数问题，而TensorFlow是解决这个问题的最有效框架之一。这个库的名字来源于数学概念**张量**，它可以被理解为具有超过两个维度的矩阵。'
- en: '**Keras** is the high-level layer of your machine learning environment. This
    library lets you easily define the structure of the neural network in a declarative
    way: you just have to define the structure of the nodes and edges, and TensorFlow
    (the low-level layer) will take care of all the mathematical operations to create
    the network.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Keras**是机器学习环境的高级层。这个库让你能够以声明性方式轻松定义神经网络的架构：你只需定义节点和边的结构，TensorFlow（低级层）将负责所有数学运算以创建网络。'
- en: Here, we will make use of the isolation feature that Anaconda provides. Remember
    that in the previous chapter you created a **Conda** environment called **gym**,
    inside which you installed **OpenAI Gym** together with TensorFlow and Keras.
    Now you will be instructed to work in a different Conda environment, where you
    will install only the modules you will need for this chapter. This way, you keep
    the code for each chapter isolated, as they apply to different projects. In fact,
    you will install specific versions of TensorFlow and Keras that may be different
    than the latest versions that were used in the previous chapter. This is a common
    way to proceed in Python projects.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将利用Anaconda提供的隔离功能。记住，在上一章中，你创建了一个名为**gym**的**Conda**环境，在其中你安装了**OpenAI
    Gym**、TensorFlow和Keras。现在，你将被指导在一个不同的Conda环境中工作，在这个环境中，你将只安装本章所需的模块。这样，你可以将每个章节的代码隔离，因为它们适用于不同的项目。实际上，你将安装TensorFlow和Keras的特定版本，这些版本可能与上一章中使用的最新版本不同。这是Python项目中常见的一种做法。
- en: Once we have clarified what each component provides, let's install each of them.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们明确了每个组件提供的内容，让我们安装每个组件。
- en: TensorFlow backend
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow后端
- en: 'First, create a dedicated `conda` environment called `tensorflow`. It consists
    of a virtual space that lets users isolate the set of Python packages you will
    use for a specific project. This has the advantage of making it straightforward
    to replicate the environment in another machine with minimal effort:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建一个名为`tensorflow`的专用`conda`环境。它由一个虚拟空间组成，允许用户隔离用于特定项目的Python包集合。这有一个优点，就是可以轻松地将环境复制到另一台机器上，几乎不需要任何努力：
- en: 'Let''s run the following commands:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们运行以下命令：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The second line produces the activation and binds the next installations to
    this `tensorflow` environment.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第二行产生激活并绑定后续安装到这个`tensorflow`环境。
- en: Conda enviroment are isolated buckets that contain the python modules you need
    for a specific **project**. For example, for **tensorflow** environment, every
    Python module you install with either `conda install` or `pip install` will be
    placed at `~/anaconda2/envs/tensorflow/bin`. The **activation** means that whenever
    a Python scripts needs to import some module, it will look for it at such **project**
    path.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Conda环境是包含特定**项目**所需的Python模块的隔离桶。例如，对于**tensorflow**环境，你使用`conda install`或`pip
    install`安装的每个Python模块都将放置在`~/anaconda2/envs/tensorflow/bin`。**激活**意味着每当Python脚本需要导入某个模块时，它将在这个**项目**路径中查找它。
- en: 'Now you can proceed to install TensorFlow:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你可以继续安装TensorFlow：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Additionally, you should also install `matplotlib` and `pyqtgraph` in order
    to draw graphs of the results:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，你还应该安装`matplotlib`和`pyqtgraph`，以便绘制结果的图表：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then check for the versions:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后检查版本：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: These last two commands have been added to give you practical examples of common `conda` commands.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这最后两个命令已被添加，为您提供常见的`conda`命令的实际示例。
- en: Deep learning with Keras
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Keras进行深度学习
- en: 'Keras is a high-level neural network API, written in Python and capable of
    running on top of TensorFlow. You can easily install a specific version with this
    command:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Keras是一个高级神经网络API，用Python编写，能够在TensorFlow之上运行。您可以使用以下命令轻松安装特定版本：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We have specified version 2.1.5to make sure that you run the code in exactly
    the same environment we tested it in. The latest version at the time of writing
    is 2.3.1.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定了版本2.1.5，以确保您在与我们测试代码相同的精确环境中运行代码。写作时的最新版本是2.3.1。
- en: ROS dependency packages
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ROS依赖包
- en: 'To use **ROS** and **Anaconda** together, you must additionally install the
    ROS dependency packages:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要一起使用**ROS**和**Anaconda**，您还必须安装ROS依赖包：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can check the version of any of them by using `pip show`:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`pip show`命令检查它们的任何版本：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the next section, we will describe the machine learning package of the code
    for this chapter, which you should have already cloned to your ROS workspace,
    as per the *Technical requirements* section.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将描述本章代码的机器学习包，您应该已经按照*技术要求*部分将其克隆到您的ROS工作空间中。
- en: Understanding the ROS Machine Learning packages
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解ROS机器学习包
- en: The code for this chapter implements the classical reinforcement learning methodology
    of training a neural network. This neural network is mathematically similar to
    the one we introduced in [Chapter 10](3bf944de-e0f8-4e78-a38b-47796c91185b.xhtml), *Applying
    Machine Learning in Robotics*, stacking layers of (hidden) nodes to establish
    a relationship between the states (the input layer) and the actions (the output
    layer).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码实现了经典的强化学习方法，即训练一个神经网络。这个神经网络在数学上与我们第10章中介绍的类似，即*在机器人中应用机器学习*，通过堆叠(隐藏)节点层来建立状态（输入层）和动作（输出层）之间的关系。
- en: The algorithm we will use for reinforcement learning is called **Deep Q-Network**
    (**DQN**) and was introduced in [Chapter 11](c5f1bcd8-306d-4ef8-b0ad-982c8bbb2b73.xhtml),
    *Machine Learning with OpenAI Gym* in the *Running an environment *section. In
    the next section, *Setting the training task parameters*, you will be given the
    operational description of states, actions, and rewards that characterize the
    reinforcement learning problem that we are going to solve with ROS.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于强化学习的算法称为**深度Q网络**（**DQN**），在第11章*使用OpenAI Gym进行机器学习*的*运行环境*部分中介绍。在下一节*设置训练任务参数*中，您将获得描述状态、动作和奖励的操作描述，这些是我们在ROS中将要解决的强化学习问题的特征。
- en: Next, we will present the training scenarios, and then we will explain how the
    files inside the ROS packages are chained to launch a training task.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍训练场景，然后我们将解释如何将ROS包内部的文件链接起来以启动一个训练任务。
- en: Training scenarios
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练场景
- en: This section is devoted to explaining how the reinforcement learning package – the
    content of the code sample provided with the book repository – is organized.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本节致力于解释强化学习包——书中代码示例库中的内容——是如何组织的。
- en: 'First, let''s take into account that we are going to train the robot for two
    scenarios:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们考虑我们将训练机器人为两种场景：
- en: 'Scenario 1: **Travel to a target location**. This scene is shown in the following
    image and consists of a square limited by four walls. There are no obstacles in
    the environment. The target location can be any point within the limits of the
    walls:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 场景1：**到达目标位置**。这个场景在以下图片中显示，由四个墙壁限定的正方形组成。环境中没有障碍物。目标位置可以是墙壁限制内的任何点：
- en: '![](img/709dcd93-2212-43d4-a12b-7853f71f5567.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/709dcd93-2212-43d4-a12b-7853f71f5567.png)'
- en: 'Scenario 2: **Travel to a target location avoiding obstacles**. This scene
    consists of the same square plus four static cylindrical obstacles. The target
    location can be any point within the limits of the walls, except for the locations
    of the obstacles:'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 场景2：**避开障碍物到达目标位置**。这个场景由相同的正方形加上四个静态圆柱形障碍物组成。目标位置可以是墙壁限制内的任何点，除了障碍物的位置：
- en: '![](img/0dd52063-189c-403b-8275-99abfa6fca1c.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0dd52063-189c-403b-8275-99abfa6fca1c.png)'
- en: Now that we have presented the training scenarios, let's describe the ROS package
    that we will use for reinforcement learning for GoPiGo3.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了训练场景，接下来让我们描述我们将用于GoPiGo3强化学习的ROS包。
- en: ROS package structure for running a reinforcement learning task
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行强化学习任务的ROS包结构
- en: 'The stack of code for this chapter is composed of the following packages:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码堆栈由以下包组成：
- en: '**`gopigo3_model`** is the ROS compilation that allows us to visualize GoPiGo3
    in RViz using the launch file `gopigo3_rviz.launch`. The robot configuration is
    the familiar 3D solid model that we have used in the previous chapters. More precisely,
    it corresponds to the GoPiGo3 complete version, which includes the distance sensor,
    the Pi camera, and the laser distance sensor, that is, the model that was built
    in [Chapter 8](25ac032c-5bfe-47ff-aa5a-f178dbff7c57.xhtml),  *Virtual SLAM and
    Navigation Using Gazebo*.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`gopigo3_model`**是允许我们使用`gopigo3_rviz.launch`启动文件在RViz中可视化GoPiGo3的ROS编译。机器人配置是我们熟悉的3D实体模型，我们在前面的章节中使用过。更确切地说，它对应于GoPiGo3完整版本，包括距离传感器、Pi相机和激光距离传感器，即[第8章](25ac032c-5bfe-47ff-aa5a-f178dbff7c57.xhtml)中构建的模型，*使用Gazebo进行虚拟SLAM和导航*。'
- en: '**`gopigo3_gazebo`** is built on top of the previous package, allowing us to
    simulate GoPiGo3 in Gazebo using the `gopigo3_world.launch` file. The URDF model
    that this file loads is the same as the one in the visualization in the `gopigo3_model`
    package.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`gopigo3_gazebo`**建立在之前的包之上，使我们能够使用`gopigo3_world.launch`文件在Gazebo中模拟GoPiGo3。该文件加载的URDF模型与`gopigo3_model`包中的可视化相同。'
- en: '**`gopigo3_dqn`** is the specific package for carrying out the reinforcement
    learning task with GoPiGo3\. As it is ROS, it is modular, and the decoupling we
    provide by separating model, simulation, and reinforcement learning makes it straightforward
    to use this same package to train other robots.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`gopigo3_dqn`**是执行GoPiGo3强化学习任务的具体包。由于它是ROS，它是模块化的，我们通过分离模型、模拟和强化学习来提供的解耦使得使用这个相同的包来训练其他机器人变得简单。'
- en: In this ROS package, we use the **DQN** algorithm to train the robot. Remember
    that DQN was introduced in the previous chapter, in the *Running an environment *section.
    In brief, what DQN will do is establish a relation between the *states of the
    robot* and the *actions to execute* using a neural network, where the *states*
    are the **input layer** and the *actions* are the **output layer**.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个ROS包中，我们使用**DQN**算法来训练机器人。记住，DQN是在上一章的*运行环境*部分介绍的。简而言之，DQN将使用神经网络建立*机器人状态*和*执行动作*之间的关系，其中*状态*是**输入层**，而*动作*是**输出层**。
- en: The mathematics behind reinforcement learning theory is complex, and it is not
    absolutely necessary to learn how to use this technique to train simple robots
    as GoPiGo3\. So, let's focus on the configuration of the training tasks, abstracting
    the specific implementation in Python of the example that supports the practical
    elements of this chapter.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习理论背后的数学很复杂，对于训练简单的GoPiGo3机器人来说，学习如何使用这项技术并不是绝对必要的。所以，让我们专注于训练任务的配置，抽象出本章实践元素所支持的示例的具体Python实现。
- en: Setting the training task parameters
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置训练任务参数
- en: 'At this point, we are briefly going to introduce the three essential concepts
    in reinforcement learning: **states**, **actions**, and **rewards**. In this section,
    we will give you minimal information so that you can understand the practical
    exercise in this chapter. In this case, we are applying the strategy of *focus
    on the practice to really understand the theory*.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们将简要介绍强化学习中的三个基本概念：**状态**、**动作**和**奖励**。在本节中，我们将提供最少的信息，以便你能够理解本章中的实际练习。在这种情况下，我们正在应用*注重实践以真正理解理论*的策略。
- en: This method of *focus on the practice to really understand the theor*y is especially
    required for complex topics that are better understood if you follow an empirical
    approach with easy-to-run examples. This preliminary *practical success* should
    provide you with enough motivation to get deeper into the topic, a task that in
    any case will be hard both in the algorithms and in the mathematics behind them.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这种*注重实践以真正理解理论*的方法对于复杂主题尤为重要，如果你遵循一个带有易于运行的示例的经验方法，这些主题将更容易理解。这种初步的*实践成功*应该为你提供足够的动力深入研究这个主题，无论如何，这个任务在算法及其背后的数学上都会很困难。
- en: 'So, let''s proceed to define these core concepts involved in the learning task
    of the robot:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们继续定义机器人学习任务中涉及的核心概念：
- en: A **state** is an *observation of the environment*. Thanks to the data streaming
    from the LDS, the state is characterized by the range and the angle to the goal
    location. For example, if you get LDS measurements with a one-degree resolution,
    each state will be a set of 360 points, each value corresponding to every angle
    in the full circumference of 360º. As the robot moves, the state changes, and
    this is reflected in the new set of 360 range values provided by the LDS. Remember
    that each range value corresponds to the distance to the nearest obstacle in that
    specific direction.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态**是环境的**观察**。得益于来自LDS的数据流，状态由到达目标位置的范围和角度来表征。例如，如果你使用一度的分辨率获取LDS测量值，每个状态将是一组360个点，每个值对应于360度全圆周上的每个角度。随着机器人的移动，状态会发生变化，这在新提供的360个范围值集中得到了反映。记住，每个范围值对应于该特定方向最近障碍物的距离。'
- en: The **action** is what the robot can do using its motors to move in the environment,
    that is, translate and/or rotate to approach the target. By executing an action,
    the robot moves and changes its **state** – defined by the new set of range values
    coming from the LDS.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作**是机器人可以通过其电机在环境中执行的操作，即平移和/或旋转以接近目标。通过执行动作，机器人移动并改变其**状态**——由来自LDS的新范围值集定义。'
- en: The **reward** is the *prize* you give to the robot every time it executes an
    action. The prize you give for each action in every possible state is called the
    **reward policy**, and it is an essential part of the success of a reinforcement
    learning task. Hence, it has to be defined by the user (the *trainer*). In plain
    words, you will give a greater reward to an action the robot performs that is
    more effective in achieving the goal.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励**是你每次机器人执行动作时给予它的**奖品**。你为每个可能状态中的每个动作提供的奖品称为**奖励策略**，它是强化学习任务成功的关键部分。因此，它必须由用户（即**训练师**）定义。简单来说，你会给机器人执行的动作赋予更大的奖励，这些动作更有助于实现目标。'
- en: 'For the practical case, we are going to run the reward policy as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实际案例，我们将按照以下方式运行奖励策略：
- en: If the obstacle is in the forward half space of the robot (180° angle covering
    left to right in the forward motion direction), it obtains a positive angle-based
    reward ranging from 0 to 5\. The maximum value (5) corresponds to the case in
    which the robot is oriented in the target direction. We specify this half-space
    by the relative angle being between -90° and +90° (in this angle the reward is
    the minimum, that is 0).  The reference direction is the line crossing the target
    location and the robot.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果障碍物位于机器人前半空间（覆盖前进运动方向的左到右的180°角度），它将获得一个从0到5的基于角度的正奖励。最大值（5）对应于机器人面向目标方向的情况。我们通过相对角度在-90°到+90°之间（在这个角度中，奖励是最小的，即0）来指定这个半空间。参考方向是穿过目标位置和机器人的直线。
- en: 'If it is in the back half space of the robot (180° angle covering left to right
    opposite to forward half space), the obtained reward is negative*,* ranging from
    0 to -5 (linear dependence with respect to the angle: 0 at 90° and -90°, and -5
    at -180°).'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果它位于机器人的后半空间（与前进半空间相反的左到右的180°角度），获得的奖励是**负的**，范围从0到-5（与角度的线性相关：90°和-90°时为0，-180°时为-5）。
- en: If the current distance from the goal is above a preset threshold, the agent
    obtains adistance-based reward >2. If it is below this threshold, the reward is
    lower than 2, approaching the minimum value of 1 as the robot gets closer. Then,
    the **approaching reward** is the dot product of the angle and distance-based
    rewards*=* **[a]** *** **[b]**.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果当前距离目标超过预设的阈值，代理将获得一个基于距离的奖励>2。如果它低于这个阈值，奖励将低于2，随着机器人靠近目标，奖励接近最小值1。然后，**接近奖励**是角度和基于距离的奖励的点积*=*
    **[a]** *** **[b]**。
- en: If the robot achieves the goal, a **success reward** of 200 is given.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果机器人达到目标，将获得200的**成功奖励**。
- en: If the robot crashes into an obstacle, a **penalty** of 150 is given, that is,
    a negative reward of -150.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果机器人撞到障碍物，将给予150的**惩罚**，即-150的负奖励。
- en: 'The reward is cumulative, and we could add any of these terms to the reward
    at every step of the simulation:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励是累积的，我们可以在模拟的每一步添加这些术语中的任何一个：
- en: Approaching reward
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接近奖励
- en: Success reward
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成功奖励
- en: Obstacle penalty
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 障碍物惩罚
- en: A **step** in the simulation is *what happens to the robot between two consecutive
    states*. And what happens is, that the robot executes an action, and – as a consequence
    – it moves, changing its state and obtaining a reward based on the policy defined
    in this section.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟中的**步骤**是“在两个连续状态之间机器人发生了什么”。发生的事情是，机器人执行一个动作，然后——作为结果——它移动，改变其状态，并基于本节中定义的策略获得奖励。
- en: After this task configuration is understood, you are ready to run the training
    of GoPiGo3 in the two scenarios that have been defined.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了此任务配置之后，你就可以运行在已定义的两个场景中 GoPiGo3 的训练了。
- en: Training GoPiGo3 to reach a target location while avoiding obstacles
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 GoPiGo3 在避开障碍物的同时到达目标位置
- en: Prior to running training in the scenario, we should note the adjustment of
    a parameter that dramatically affects the computational cost. This is the horizontal
    sampling of the LDS, since the **state** of the robot is characterized by the
    set of range values in a given step of the simulation. In previous chapters, when
    we performed navigation in Gazebo, we used a sampling rate of 720 for LDS. This
    means that we have circumferential range measurements at 1º resolution.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在场景中运行训练之前，我们应该注意调整一个参数，这个参数会显著影响计算成本。这是 LDS 的水平采样，因为机器人的**状态**由模拟给定步骤中的范围值集合来表征。在之前的章节中，当我们使用
    Gazebo 进行导航时，我们使用了 LDS 的采样率为 720。这意味着我们有 1° 分辨率的周向范围测量。
- en: For this example of reinforcement learning, we are reducing the sampling to
    24, which means a range resolution of 15º. The positive aspect of this decision
    is that you reduce the **state** vector from 360 items to 24, which is a factor
    of 15\. You may have guessed that this will make the simulation more computationally
    efficient. In contrast, you will find that the drawback is that GoPiGo3 loses
    its perception capability, since it will only be able to detect objects whose
    angle coverage with respect to the point of view of the robot is larger than 15º.
    At a distance of 1 meter, this is equivalent to a minimum obstacle width of 27
    cm.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个强化学习的例子，我们将采样减少到 24，这意味着范围分辨率为 15°。这个决定的积极方面是，你将 **状态** 向量从 360 项减少到 24，这是一个
    15 倍的因子。你可能已经猜到了，这将使模拟更加计算高效。相反，你会发现缺点是 GoPiGo3 失去了它的感知能力，因为它只能检测到与机器人视角的角度覆盖大于
    15° 的物体。在 1 米的距离上，这相当于最小障碍物宽度为 27 厘米。
- en: On a positive note, as the robot gets closer to an obstacle, its discrimination
    capability improves. For example, at a distance of 10 cm, an arc of 15º means
    it can detect obstacles with a minimum width of 5.4 cm.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 说到积极的一面，随着机器人接近障碍物，其辨别能力提高。例如，在 10 厘米的距离上，15 度的弧线意味着它可以检测到最小宽度为 5.4 厘米的障碍物。
- en: 'The horizontal sampling is set in the URDF model, in the part of the file that
    describes the LDS, located at `./gopigo3_model/urdf/gopigo3.gazebo`. The number
    to specify for obtaining rays with 15º spacing is as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 水平采样设置在 URDF 模型中，在描述 LDS 的文件部分，位于 `./gopigo3_model/urdf/gopigo3.gazebo`。为了获得
    15° 间隔的光线，需要指定的数字如下：
- en: '![](img/f7961a60-89f9-457c-93c9-1ac292cc2cb1.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f7961a60-89f9-457c-93c9-1ac292cc2cb1.png)'
- en: Since LDS covers from 0º to 360º, to get 24 equally spaced rays, you have add
    one more sample, making it 25, since 0º and 360º are actually the same angle.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 LDS 覆盖从 0° 到 360°，为了得到 24 个等间隔的光线，你需要添加一个额外的样本，使其变为 25，因为 0° 和 360° 实际上是同一个角度。
- en: 'Then, the LDS part of the URDF file has to be modified as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，必须按照以下方式修改 URDF 文件的 LDS 部分：
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: By setting the `<visualize>` tag to `true`, the ray tracing is shown in Gazebo.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将 `<visualize>` 标签设置为 `true`，在 Gazebo 中显示光线追踪。
- en: 'Is this sampling enough to ensure that the robot can be effectively trained?
    Let''s answer the question comparing the number of rays for every case. This first
    figure shows the 0.5° actual resolution of the physical LDS. Rays are so close
    to each other that you cannot almost see the resolution. It provides very faithful
    sensing of the environment:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的采样是否足够确保机器人能够得到有效的训练？让我们通过比较每种情况下的光线数量来回答这个问题。这张第一张图显示了物理 LDS 的 0.5° 实际分辨率。光线非常接近，几乎看不到分辨率。它提供了对环境的非常忠实的感觉：
- en: '![](img/7e6ea91e-bbd6-41fc-8e7e-d2e1cff32850.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7e6ea91e-bbd6-41fc-8e7e-d2e1cff32850.png)'
- en: 'This second image shows the case of 24 samples and 15º resolution:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这第二张图显示了 24 个样本和 15° 分辨率的案例：
- en: '![](img/3f61be4c-4697-4f54-b419-8a8fab1d0d75.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3f61be4c-4697-4f54-b419-8a8fab1d0d75.png)'
- en: In this picture, the ray tracing shows that, even with so few rays, the obstacles
    are detected, since only one ray is needed to identify an obstacle. This fact
    helps to mitigate the loss of perception resolution. However, bear in mind that
    the robot will not be able to know the obstacle's width, only that it will be
    less than 30º. Why? Because you need three rays to detect an obstacle of finite
    width, the central ray detecting it and the extreme ones not interfering with
    it. Hence, this upper limit for the obstacle width is equal to twice the angular
    distance between adjacent rays, that is, *2 x 15º = 30º*. In some cases, this
    may be too imprecise, but, for the simple scenario we are using in this example,
    it should be precise enough.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图片中，光线追踪显示，即使只有这么少的射线，也能检测到障碍物，因为只需要一束射线就能识别障碍物。这一事实有助于减轻感知分辨率的损失。然而，请注意，机器人将无法知道障碍物的宽度，只知道它将小于
    30º。为什么？因为需要三束射线来检测有限宽度的障碍物，中心射线检测它，而两端的射线不会干扰它。因此，障碍物宽度的上限等于相邻射线之间的角距离的两倍，即 *2
    x 15º = 30º*。在某些情况下，这可能会不够精确，但，对于我们在本例中使用的简单场景，应该足够精确。
- en: How to run the simulations
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何运行模拟
- en: 'Before beginning the training process for each scenario, let''s recap what
    we learned in the previous chapter about visualization (RViz) and simulation (Gazebo)
    to establish a link with the learning process, which will make use of these tools
    and related scripts:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始每个场景的训练过程之前，让我们回顾一下在前一章中学到的关于可视化（RViz）和模拟（Gazebo）的知识，以便与学习过程建立联系，我们将使用这些工具和相关脚本：
- en: 'To launch the visualization in RViz, you have to simply execute the following
    command:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在 RViz 中启动可视化，你只需简单地执行以下命令：
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To launch the simulation in Gazebo, you may proceed in a similar manner using
    this single command:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Gazebo 中启动模拟，你可以使用以下单个命令以类似的方式继续操作：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, to run the reinforcement learning task you first have to launch Gazebo
    – as explained in step 2 – but with the selected training environment, instead
    of the general `gopigo3_world.launch`:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，要运行强化学习任务，你首先必须启动 Gazebo – 如步骤 2 中解释的那样 – 但使用选定的训练环境，而不是通用的 `gopigo3_world.launch`：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'These two commands run the task in **scenario 1** described earlier. To perform
    the training for **scenario 2**, you only need to execute the corresponding launch
    files:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个命令运行了之前描述的 **场景 1** 中的任务。要为 **场景 2** 进行训练，你只需要执行相应的启动文件：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The first line loads the scenario 2 environment, and the second launches the
    training task for it.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行加载了场景 2 环境，第二行启动了针对该环境的训练任务。
- en: The following two sub-sections show ROS in action by executing the commands
    in step 3.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两个子部分通过执行步骤 3 中的命令展示了 ROS 的实际应用。
- en: Scenario 1 – travel to a target location
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 场景 1 – 前往目标位置
- en: 'Follow the next procedure to make sure that the training process happens as
    expected:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤进行，以确保训练过程按预期进行：
- en: 'First, launch the virtual robot model in Gazebo:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，在 Gazebo 中启动虚拟机器人模型：
- en: '[PRE14]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, you can start the training process. But first, you have to be in the
    `tensorflow` virtual environment:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以开始训练过程。但首先，你必须处于 `tensorflow` 虚拟环境中：
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, start the training:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，开始训练：
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You might receive an error like this one:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会收到如下错误：
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Don''t worry, you can solve it by executing the following command:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 不要担心，你可以通过执行以下命令来解决它：
- en: '[PRE18]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Hence, if you receive the error above, solve it as suggested and launch the
    training script again. Then subscribe to the relevant topics:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你收到上述错误，按照建议解决它，然后再次启动训练脚本。然后订阅相关主题：
- en: '[PRE19]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`get_action` is a `Float32MultiArray` message type whose data definition is
    as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_action` 是一个 `Float32MultiArray` 消息类型，其数据定义如下：'
- en: '[PRE20]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s see each component:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看每个组件：
- en: GoPiGo3 always has a linear velocity of 0.15 m/s. The `action` item changes
    the angular velocity from -1.5 to 1.5 rad/s in steps of 0.75, to cover the integer
    range from 0 to 4.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GoPiGo3 总是保持 0.15 m/s 的线性速度。`动作` 项以 0.75 为步长将角速度从 -1.5 到 1.5 rad/s 进行调整，以覆盖从
    0 到 4 的整数范围。
- en: The obtained `reward` in each step is as was described in the *Setting the training
    task parameters* section.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一步获得的 `奖励` 如在 *设置训练任务参数* 部分所述。
- en: '`score` is the cumulative reward that the robot obtains in each episode.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`得分` 是机器人在每个回合中获得的累积奖励。'
- en: 'The corresponding ROS graph can be seen in the following figure:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的 ROS 图可以在以下图中看到：
- en: '![](img/2304b8fd-3115-40c7-8100-ce730b6c0962.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2304b8fd-3115-40c7-8100-ce730b6c0962.png)'
- en: The key node in this graph is `gopigo3_dqn_stage_1`, which takes the robot state
    from the Gazebo simulation and performs the training task by issuing `cmd_vel`
    messages (remember that the velocity commands that drive the robot are published in
    this topic) and getting the reward for every new state GoPiGo3 achieves.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图中的关键节点是 `gopigo3_dqn_stage_1`，它从 Gazebo 模拟中获取机器人状态，并通过发布 `cmd_vel` 消息（记住，驱动机器人的速度命令是在这个主题中发布的）执行训练任务，并为
    GoPiGo3 每达到的新状态获取奖励（记住，在训练过程中，机器人在该状态下收到的奖励与每个新状态相关联）。
- en: 'The episode record can be followed in the console log of terminal **T2**:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在终端 **T2** 的控制台日志中跟踪事件记录：
- en: '![](img/dbd99d98-dc54-42d8-aeba-dcaa644b9dc1.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dbd99d98-dc54-42d8-aeba-dcaa644b9dc1.png)'
- en: The red square is the target location and the blue lines are the LDS rays as
    described at the beginning of the section.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 红色方块是目标位置，蓝色线条是如本节开头所述的 LDS 射线。
- en: This first scenario aims to get you familiar with the training process in ROS.
    Let's move on to scenario 2, in which we will give quantitative information about
    how the training process improves through the episodes.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个场景的目的是让您熟悉 ROS 中的训练过程。让我们继续到场景 2，我们将给出关于训练过程如何通过事件进行改进的定量信息。
- en: Scenario 2 – travel to a target location avoiding the obstacles
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 场景 2 – 避开障碍物到达目标位置
- en: 'The procedure to start the training is similar to scenario 1:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 开始训练的程序与场景 1 类似：
- en: 'We only need to change the name of the files and use the relevant ones:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只需要更改文件名并使用相关的文件：
- en: '[PRE21]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If you want to see graphically how the GoPiGo3 learning process is performing,
    execute the `result_graph.launch` file:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您想图形化地查看 GoPiGo3 的学习过程，请执行 `result_graph.launch` 文件：
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The following screenshot shows all the content you should see on the screen
    of your laptop:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了您应该在笔记本电脑屏幕上看到的所有内容：
- en: '![](img/c69b591e-269a-44e6-8acc-eddd92fdc9a1.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c69b591e-269a-44e6-8acc-eddd92fdc9a1.png)'
- en: The red graph shows the **Total rewa****rd** obtained in each episode. Remember
    that an episode is defined as the sequence of steps that comes to an end when
    a criterion is met. In this problem, an episode finishes if the goal (the red
    square) is reached or if there is a collision with an obstacle. These graphs show
    the evolution in the first 300 episodes.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 红色图显示了每个事件中获得的**总奖励**。请记住，一个事件定义为当满足某个标准时结束的步骤序列。在这个问题中，如果达到目标（红色方块）或与障碍物发生碰撞，则事件结束。这些图显示了前
    300 个事件中的演变。
- en: The green graph represents the average Q value of the trained model. Remember
    that this is the action-value function *Q(s,a)* that tells you how good it is
    to execute an action *a* in a given state *s*. This concept was explained in the
    previous chapter in the basic example of the self-driving cab in the *Q-learning
    explained* section.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 绿色图表示训练模型的平均 Q 值。请记住，这是动作值函数 *Q(s,a)*，它告诉您在给定状态 *s* 下执行动作 *a* 是多么好。这个概念在上一章的“Q-learning
    解释”部分的基本自动驾驶出租车示例中已经解释过了。
- en: You can see how, on average, GoPiGo3 is performing better as it accumulates
    experience. But how is it operationally using that experience? The answer comes
    from the reinforcement learning algorithm that has been applied, that is, by associating
    effective actions to every state given the rewards the robot has been receiving
    in that state during the training process.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，随着经验的积累，GoPiGo3 的平均表现如何变得更好。但是它是如何操作性地使用这些经验的呢？答案来自于已经应用的强化学习算法，即通过将有效的动作与训练过程中机器人在该状态下收到的奖励相关联。
- en: 'Finally, we should note that in an execution environment like this, the ROS
    graph alternates between two states. One is when the robot executes an action
    by publishing a `cmd_vel` message that moves the robot:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们应该注意，在这种执行环境中，ROS 图在两种状态之间交替。一种是当机器人通过发布 `cmd_vel` 消息执行动作时：
- en: '![](img/f812a21e-7455-4e92-b73b-80c61c91bf5a.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f812a21e-7455-4e92-b73b-80c61c91bf5a.png)'
- en: For clarity, in this graph we are excluding the nodes of the launch file issued
    in terminal `T3`**.**
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰起见，在这个图中，我们排除了在终端 `T3` 生成的启动文件中的节点**。**
- en: 'The other ROS graph corresponds to the instants in which the agent computes
    the next state using the set of 24 values coming from the LDS in the `/scan` topic:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个 ROS 图表示的是代理使用来自 `/scan` 主题的 LDS 的 24 个值计算下一个状态的瞬间：
- en: '![](img/bb27cf2e-180a-47e0-8940-abb25374aced.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bb27cf2e-180a-47e0-8940-abb25374aced.png)'
- en: At this point, you have covered the end-to-end training process of GoPiGo3\.
    The next challenge is to test the trained model.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经完成了 GoPiGo3 的端到端训练过程。下一个挑战是测试训练好的模型。
- en: Testing the trained model
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试训练好的模型
- en: 'Taking into account that we are using the **Deep Q-Network** (**DQN**)algorithm,
    what we have to save from the training process is the structure of the neural
    network and the weights of the edges. This is what the `gopigo3_dqn_stage_2` node
    performs every 10 episodes. Hence, you can find the saved models inside the `./gopigo3_dqn/save_model` folder, and
    the weights of the network are stored in the `h5` file type. Every file contains
    in its name the scenario (`stage_1` or `stage_2`) and the episode number. Follow
    these steps to evaluate the trained model:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们正在使用**深度Q网络**（**DQN**）算法，我们必须从训练过程中保存的是神经网络的架构和边的权重。这正是`gopigo3_dqn_stage_2`节点每10个回合执行的操作。因此，你可以在`./gopigo3_dqn/save_model`文件夹中找到保存的模型，网络的权重存储在`h5`文件类型中。每个文件的名字中都包含场景（`stage_1`或`stage_2`）和回合编号。按照以下步骤评估训练好的模型：
- en: Select the file with the highest episode number, that is, `stage_2_1020.h5`.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择具有最高回合编号的文件，即`stage_2_1020.h5`。
- en: Every `h5` file contains the weights of the DQN network as it appears at the
    end of the episode referenced by the filename. For example, `stage_2_1020.h5`
    refers to the network of scenario 2 at the end of episode 1020.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`h5`文件都包含DQN网络在所提及的回合结束时出现的权重。例如，`stage_2_1020.h5`指的是在第1020回合结束时场景2的网络。
- en: 'In order to use these weights, you basically have to use the same Python script
    of the training model (`./nodes/gopigo3_dqn_stage_2`), but initialize with different
    values the parameters marked in bold letters in the snippet below that reproduces
    the first lines of the `class ReinforceAgent()` definition:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用这些权重，你基本上必须使用与训练模型相同的Python脚本（`./nodes/gopigo3_dqn_stage_2`），但初始化以下代码片段中用粗体字母标记的参数为不同的值，该代码片段重现了`class
    ReinforceAgent()`定义的前几行：
- en: '[PRE23]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then, what each parameter provides is as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，每个参数提供的内容如下：
- en: '`self.load_model = True` tells the script to load the weights of a pretrained
    model.'
  id: totrans-162
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.load_model = True`告诉脚本加载预训练模型的权重。'
- en: '`self.load_episode = 1020`  sets the number of the episode from which you want
    to load the DQN network weights, being the corresponding file `stage_2_1020.h5`.'
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.load_episode = 1020`设置你想要加载DQN网络权重的回合编号，对应的文件是`stage_2_1020.h5`。'
- en: 'Then rename the Python script as `gopigo3_dqn_stage_2-test` and generate the
    new launch file `gopigo3_dqn_stage_2-test.launch`, which will call the created
    test script:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将Python脚本重命名为`gopigo3_dqn_stage_2-test`，并生成新的启动文件`gopigo3_dqn_stage_2-test.launch`，它将调用创建的测试脚本：
- en: '[PRE24]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To launch the test process, follow the same steps as for running the training
    scenario, but using the test version of the launch file:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要启动测试过程，遵循与运行训练场景相同的步骤，但使用测试版本的启动文件：
- en: '[PRE25]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Remember that for `T2` you have to activate the TensorFlow environment with
    the `$ conda activate tensorflow` command. When it starts, you will see in `T2`,
    a message telling you that a model from episode 1380 will be used:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记住，对于`T2`，你必须使用`$ conda activate tensorflow`命令激活TensorFlow环境。当它启动时，你将在`T2`中看到一个消息，告诉你将使用第1380集的模型：
- en: '[PRE26]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'If you plot the graphs for the first episodes (as per the command in terminal
    `T3`), you can confirm that the values are pretty good, that is, above 2,000 for
    the *Total reward*  and above 100 for the *Average max Q-value*:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你绘制了前几个回合的图表（如终端`T3`中的命令所示），你可以确认值相当不错，即*总奖励*超过2,000，*平均最大Q值*超过100：
- en: '![](img/ce631c69-8dce-4f2b-a122-9c9a83514da0.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ce631c69-8dce-4f2b-a122-9c9a83514da0.png)'
- en: Although you are testing the model, every ten episodes the network's weights
    are saved to an `h5` file referencing the current episode number.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你正在测试模型，但每十个回合，网络的权重都会保存到一个以当前回合编号命名的`h5`文件中。
- en: Summary
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter has been a quick and practical introduction to how you can apply
    reinforcement learning so that a robot can perform useful tasks such as transporting
    materials to a target location. You should be aware that this kind of machine
    learning technique is at the very beginning of its maturity, and there are as
    yet few practical solutions working in the real world. The reason is that the
    process of training is very expensive in terms of time and cost, since you have
    to perform thousands of episodes to get a well-trained model, and later replay
    the process with the physical robot to address behavioral differences between the
    real world and the simulated environment.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 本章简要而实用地介绍了如何应用强化学习，以便机器人能够执行有用的任务，如将材料运输到目标位置。你应该意识到，这种机器学习技术还处于其成熟度的初期，目前在实际世界中还没有多少可行的解决方案。原因是训练过程在时间和成本上都非常昂贵，因为你必须执行数千个场景才能得到一个训练良好的模型，然后使用物理机器人重新播放该过程以解决现实世界与模拟环境之间的行为差异。
- en: 'Be aware that the training process in Gazebo is not a substitute for training
    in the real world: a simulation necessarily implies a simplification of the reality,
    and every difference between the training environment (Gazebo) and the physical
    world introduces new states that can be missing in the training set, and hence
    the trained neural network will not be able to perform well in such situations.
    The solution? More training to cover more states, something that also means higher
    cost.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Gazebo中的训练过程不能替代现实世界的训练：模拟必然意味着对现实的简化，并且训练环境（Gazebo）与物理世界之间的每一个差异都会引入训练集中可能缺失的新状态，因此训练好的神经网络在那种情况下将无法表现良好。解决方案？更多的训练来覆盖更多状态，这也意味着更高的成本。
- en: In the last part of the chapter, we have also covered the testing of a model
    using the same scenario in which the robot was trained. A more formal testing
    approach requires that you check how the trained model generalizes to different
    conditions in the scenario, such as having more obstacles or moving their positions. This
    is a complex topic, since reinforcement learning algorithms currently struggle
    to achieve generalization. Why? Because, when you introduce changes in the scenario,
    you are generating new states in the models. Since they are new to the robot,
    it will not know the most effective action to execute. Hence, new training is
    required to explore these new states.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后部分，我们同样介绍了使用与机器人训练相同场景来测试模型的方法。更正式的测试方法要求你检查训练好的模型在场景中不同条件下（例如，有更多障碍物或移动它们的位置）的泛化能力。这是一个复杂的话题，因为当前的强化学习算法在实现泛化方面仍然存在困难。为什么？因为，当你改变场景时，你会在模型中生成新的状态。由于这些状态对机器人来说是新的，它将不知道执行最有效动作的方法。因此，需要新的训练来探索这些新状态。
- en: Reinforcement learning is currently a very active field of research, and we
    should expect great advances in the years to come. What we should see is reinforcement
    learning being applied to real robots at a reasonable cost (that is, training
    a robot at a pace that doesn't require thousands of episodes) and providing techniques
    for the generalization of the models to environments other than those that were
    used for training.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习目前是一个非常活跃的研究领域，我们应该期待在未来的几年里取得重大进展。我们应该看到的是，强化学习以合理的成本（即以不需要数千个场景的节奏训练机器人）应用于实际机器人，并提供将模型泛化到训练场景之外的环境的技术。
- en: This chapter closes the introduction to the application of machine learning
    in robotics. In this book, we have only scratched the surface of its potential,
    and, if you have followed the explanations you should have checked by yourself
    that this is a complex field where at some point you will have to master statistics,
    data analytics, and neural networks.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了机器学习在机器人应用中的入门。在这本书中，我们只是触及了其潜力的表面，如果你已经跟随了这些解释，你应该自己检查过，这是一个复杂的领域，在某个时刻你将不得不掌握统计学、数据分析以及神经网络。
- en: At the same time, it is a field where the focus is on experimentation. Instead
    of trying to model the reality with analytical formulas or computer-aided simulation,
    you observe the real world, get data from sensors, and try to infer patterns of
    behavior from them. Hence, the ability to successfully apply machine learning
    to robotics relies on being capable of streaming data continuously so that the
    robot can make smart decisions in real time. And the first step is to produce
    well-trained models. For this reason, a robot will be able to develop smart behavior
    in the medium and long term, as it accumulates experience that can be made available
    in structured trained models.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，这也是一个以实验为重点的领域。与其试图用分析公式或计算机辅助模拟来模拟现实，不如观察现实世界，从传感器获取数据，并尝试从中推断行为模式。因此，成功将机器学习应用于机器人的能力取决于能够连续流式传输数据，以便机器人在实时中做出明智的决策。第一步是产生经过良好训练的模型。因此，机器人能够在中长期能够发展智能行为，因为它积累了可以在结构化训练模型中利用的经验。
- en: This is a challenging goal, both for the data scientist and the software engineer.
    They should work together to create mature robot frameworks that benefit from
    machine learning as much as common web applications and digital businesses are
    today.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个具有挑战性的目标，对于数据科学家和软件工程师来说都是如此。他们应该共同努力，创建成熟的机器人框架，使其从机器学习中获得的好处与今天常见的网络应用和数字业务一样多。
- en: Finally, many thanks for reading the book. At this point, you are challenged
    to explore advanced ROS topics, and we hope you can also become an active contributor
    to the ROS open source community.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，非常感谢阅读这本书。在这个时候，你被挑战去探索高级ROS主题，我们希望你也可以成为ROS开源社区的积极贡献者。
- en: Questions
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What are the essential concepts of reinforcement learning?
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 强化学习的核心概念是什么？
- en: A) Robot actions and penalties
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: A) 机器人动作和惩罚
- en: B) Neural networks and deep learning
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: B) 神经网络和深度学习
- en: C) States, actions, and rewards
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: C) 状态、动作和奖励
- en: Why do you need to use neural networks in reinforcement learning?
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在强化学习中需要使用神经网络？
- en: A) Because the robot needs to use deep learning to recognize objects and obstacles.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: A) 因为机器人需要使用深度学习来识别物体和障碍物。
- en: B) Because the robot has to learn to associate states with the most effective
    actions.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: B) 因为机器人必须学会将状态与最有效的动作相关联。
- en: C) We do not need neural networks in reinforcement learning; we apply different
    algorithms.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: C) 在强化学习中，我们不需要神经网络；我们应用不同的算法。
- en: How do you encourage the robot to achieve the goal of the task?
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何鼓励机器人实现任务的既定目标？
- en: A) By giving it rewards when it performs *good* actions.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: A) 通过在它执行**良好**行为时给予奖励。
- en: B) By giving it penalties when it performs *bad* actions.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: B) 通过在它执行**不良**行为时给予惩罚。
- en: C) By giving it rewards when it performs *good* actions, and penalties in the
    case of *bad* actions.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: C) 通过在它执行**良好**行为时给予奖励，在执行**不良**行为时给予惩罚。
- en: Can you apply the reinforcement learning ROS package from this chapter to other
    robots?
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以将本章中提到的强化学习ROS包应用于其他机器人吗？
- en: A) Yes, because we have separated the robot model, the scenario and the training
    algorithm into different packages.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: A) 是的，因为我们已经将机器人模型、场景和训练算法分成了不同的包。
- en: B) No, because you have to rewrite the ROS package for every scenario.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: B) 不，因为对于每个场景，你必须重写ROS包。
- en: 'C) No: it is specific for training GoPiGo3.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: C) 不，它是特定于GoPiGo3训练的。
- en: Do you need to use the full data feed coming from a real LDS to train a robot?
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你是否需要使用来自真实LDS的完整数据流来训练机器人？
- en: 'A) Yes: if you want to obtain accurate results; you have to use all the data.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: A) 是的：如果你想获得准确的结果；你必须使用所有数据。
- en: 'B) No: you have to decide the ray tracing density as a function of the typical
    size of the obstacles in the scenario.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: B) 不，你必须将光线追踪密度作为场景中障碍物典型大小的函数来决定。
- en: 'C) No: it depends on how much accuracy you require.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: C) 不一定：这取决于你需要的精度有多少。
- en: Further reading
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'To delve deeper into the concepts explained in this chapter you can follow
    the following references:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解本章中解释的概念，你可以参考以下参考资料：
- en: '*Practical Reinforcement Learning* from **Coursera**: [https://www.coursera.org/learn/practical-rl](https://www.coursera.org/learn/practical-rl)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Coursera上的《实用强化学习》**：[https://www.coursera.org/learn/practical-rl](https://www.coursera.org/learn/practical-rl)'
- en: 'Welcome to Deep Reinforcement Learning Part 1: DQN [https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-dqn-c3cab4d41b6b](https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-dqn-c3cab4d41b6b)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欢迎来到深度强化学习第1部分：DQN [https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-dqn-c3cab4d41b6b](https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-dqn-c3cab4d41b6b)
- en: 'Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables
    and Neural Networks [https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Tensorflow的简单强化学习第0部分：使用表格和神经网络的Q学习 [https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)
- en: 'Simple Reinforcement Learning with Tensorflow Part 4: Deep Q-Networks and Beyond [https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Tensorflow的简单强化学习第4部分：深度Q网络及其之后 [https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df)
