["```py\n>>> import requests\n>>> r = requests.get(\"http://en.wikipedia.org/wiki/List_of_algorithms\")\n>>> r\n<Response [200]>\n>>> r.text\nu'<!DOCTYPE html>\\n<html lang=\"en\" dir=\"ltr\" class=\"client-nojs\">\\n<head>\\n<meta charset=\"UTF-8\" />\\n<title>List of algorithms - Wikipedia, the free encyclopedia</title>\\n...\n\n```", "```py\n$ pip install beautifulsoup4\n\n```", "```py\n>>> from bs4 import BeautifulSoup\n>>> soup = BeautifulSoup(<HTML_DOCUMENT_STRING>)\n\n```", "```py\n>>> from bs4 import BeautifulSoup\n>>> soup = BeautifulSoup(\"<h1 id='message'>Hello, Requests!</h1>\")\n\n```", "```py\n    >>> tag = soup.h1\n    >>> type(tag)\n    <class 'bs4.element.Tag'>\n\n    ```", "```py\n    >>> tag.name\n    'h1'\n\n    ```", "```py\n    >>> tag['id']\n    'message'\n\n    ```", "```py\n>>> from bs4 import BeautifulSoup\n>>> soup = BeautifulSoup(\"<h1 id='message'>Hello, Requests!</h1>\") >>> type(soup)\n<class 'bs4.BeautifulSoup'>\n\n```", "```py\n>>> tag.string\nu'Hello, Requests!'\n\n```", "```py\n>>> soup = BeautifulSoup(\"<p><!-- This is comment --></p>\")\n>>> comment = soup.p.string\n>>> type(comment)\n<class 'bs4.element.Comment'>\n\n```", "```py\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\" />\n    <title>\n      Chapter 6 - Web Scrapping with Python Requests and BeatuifulSoup\n    </title>\n  </head>\n  <body>\n    <div class=\"surveys\">\n      <div class=\"survey\" id=\"1\">\n        <p class=\"question\">\n          <a href=\"/surveys/1\">Are you from India?</a>\n        </p>\n        <ul class=\"responses\">\n          <li class=\"response\">Yes - <span class=\"score\">21</span>\n          </li>\n          <li class=\"response\">No - <span class=\"score\">19</span>\n          </li>\n        </ul>\n      </div>\n      <div class=\"survey\" id=\"2\">\n        <p class=\"question\">\n          <a href=\"/surveys/2\">Have you ever seen the rain?</a>\n        </p>\n        <ul class=\"responses\">\n          <li class=\"response\">Yes - <span class=\"score\">40</span>\n          </li>\n          <li class=\"response\">No - <span class=\"score\">0</span>\n          </li>\n        </ul>\n      </div>\n      <div class=\"survey\" id=\"3\">\n        <p class=\"question\">\n          <a href=\"/surveys/1\">Do you like grapes?</a>\n        </p>\n        <ul class=\"responses\">\n          <li class=\"response\">Yes - <span class=\"score\">34</span>\n          </li>\n          <li class=\"response\">No - <span class=\"score\">6</span>\n          </li>\n        </ul>\n      </div>\n    </div>\n  </body>\n</html>\n```", "```py\n>>> from bs4 import BeautifulSoup\n>>> soup = BeautifulSoup(open(\"scraping_example.html\"))\n\n```", "```py\n>>> soup.html\n<html lang=\"en\">\n...\n...\n</html>\n```", "```py\n>>> soup.html.head.title\n<title>Chapter 6 - Web Scraping with Python Requests and BeatuifulSoup</title>\n\n```", "```py\n>>> soup.html.head.meta\n<meta charset=\"utf-8\"/>\n\n```", "```py\n>>> for child in soup.html.children:\n...     print child.name\n...\nhead\nbody\n\n```", "```py\n>>> soup.head.find_next_sibling()\n<body>\n <div class=\"surveys\">\n .\n .\n .\n </div>\n</body>\n\n```", "```py\n>>> soup.body.find_previous_sibling\n<head><meta charset=\"utf-8\"/><title>... </title></head>\n\n```", "```py\n>>> soup.div.parent.name\n'body'\n\n>>> for parent in soup.div.parents:\n...     print parent.name\n...\nbody\nhtml\n[document]\n\n```", "```py\n>>> soup.head.find_previous().name\n'html'\n>>> soup.head.find_next().name\n'meta'\n\n```", "```py\n    >>> soup.title.string\n    u'Chapter 6 - Web Scrapping with Python Requests and BeatuifulSoup'\n\n    ```", "```py\n    >>> soup.title.string = 'Web Scrapping with Python Requests and BeatuifulSoup by Balu and Rakhi'\n\n    ```", "```py\n    >>> soup.title.string\n    u'Web Scrapping with Python Requests and BeatuifulSoup by Balu and Rakhi'\n\n    ```", "```py\n>>> START_PAGE, END_PAGE = 1, 10\n>>> URL = \"http://www.majortests.com/gre/wordlist_0%d\"\n>>> def generate_urls(url, start_page, end_page):\n...     urls = []\n...     for page in range(start_page, end_page):\n...         urls.append(url % page)\n...     return urls\n...\n>>> generate_urls(URL, START_PAGE, END_PAGE)\n['http://www.majortests.com/gre/wordlist_01', 'http://www.majortests.com/gre/wordlist_02', 'http://www.majortests.com/gre/wordlist_03', 'http://www.majortests.com/gre/wordlist_04', 'http://www.majortests.com/gre/wordlist_05', 'http://www.majortests.com/gre/wordlist_06', 'http://www.majortests.com/gre/wordlist_07', 'http://www.majortests.com/gre/wordlist_08', 'http://www.majortests.com/gre/wordlist_09']\n\n```", "```py\n>>> import requests\n>>> def get_resource(url):\n...     return requests.get(url)\n...\n>>> get_resource(\"http://www.majortests.com/gre/wordlist_01\")\n<Response [200]>\n\n```", "```py\n<div class=\"grid_9 alpha\">\n  <h3>Group 1</h3>\n  <a name=\"1\"></a>\n  <table class=\"wordlist\">\n    <tbody>\n      <tr>\n        <th>Abhor</th>\n        <td>hate</td>\n      </tr>\n      <tr>\n        <th>Bigot</th>\n        <td>narrow-minded, prejudiced person</td>\n      </tr>\n      ...\n      ...\n    </tbody>\n  </table>\n</div>\n```", "```py\ndef make_soup(html_string):\n    return BeautifulSoup(html_string)\n```", "```py\ndef get_words_from_soup(soup):\n    words = {}\n\n    for count, wordlist_table in enumerate(\n    soup.find_all(class_='wordlist')):\n\n        title = \"Group %d\" % (count + 1)\n\n        new_words = {}\n        for word_entry in wordlist_table.find_all('tr'):\n            new_words[word_entry.th.text] = word_entry.td.text\n\n        words[title] = new_words\n\n    return words\n```", "```py\ndef save_as_json(data, output_file):\n    \"\"\" Writes the given data into the specified output file\"\"\"\n    with open(output_file, 'w') as outfile:\n        json.dump(data, outfile)\n```", "```py\nimport json\nimport time\n\nimport requests\n\nfrom bs4 import BeautifulSoup\n\nSTART_PAGE, END_PAGE, OUTPUT_FILE = 1, 10, 'words.json'\n\n# Identify the URL\nURL = \"http://www.majortests.com/gre/wordlist_0%d\"\n\ndef generate_urls(url, start_page, end_page):\n    \"\"\"\n    This method takes a 'url' and returns a generated list of url strings\n\n        params: a 'url', 'start_page' number and 'end_page' number\n        return value: a list of generated url strings\n    \"\"\"\n    urls = []\n    for page in range(start_page, end_page):\n        urls.append(url % page)\n    return urls\n\ndef get_resource(url):\n    \"\"\"\n    This method takes a 'url' and returns a 'requests.Response' object\n\n        params: a 'url'\n        return value: a 'requests.Response' object\n    \"\"\"\n    return requests.get(url)\n\ndef make_soup(html_string):\n    \"\"\"\n    This method takes a 'html string' and returns a 'BeautifulSoup' object\n\n        params: html page contents as a string\n        return value: a 'BeautifulSoup' object\n    \"\"\"\n    return BeautifulSoup(html_string)\n\ndef get_words_from_soup(soup):\n\n    \"\"\"\n    This method extracts word groups from a given 'BeautifulSoup' object\n\n        params: a BeautifulSoup object to extract data\n        return value: a dictionary of extracted word groups\n    \"\"\"\n\n    words = {}\n    count = 0\n\n    for wordlist_table in soup.find_all(class_='wordlist'):\n\n        count += 1\n        title = \"Group %d\" % count\n\n        new_words = {}\n        for word_entry in wordlist_table.find_all('tr'):\n            new_words[word_entry.th.text] = word_entry.td.text\n\n        words[title] = new_words\n        print \" - - Extracted words from %s\" % title\n\n    return words\n\ndef save_as_json(data, output_file):\n    \"\"\" Writes the given data into the specified output file\"\"\"\n            json.dump(data, open(output_file, 'w'))\n\ndef scrapper_bot(urls):\n    \"\"\"\n    Scrapper bot:\n        params: takes a list of urls\n\n        return value: a dictionary of word lists containing\n                      different word groups\n    \"\"\"\n\n    gre_words = {}\n    for url in urls:\n\n        print \"Scrapping %s\" % url.split('/')[-1]\n\n        # step 1\n\n        # get a 'url'\n\n        # step 2\n        html = requets.get(url)\n\n        # step 3\n        # identify the desired pieces of data in the url using Browser tools\n\n        #step 4\n        soup = make_soup(html.text)\n\n        # step 5\n        words = get_words_from_soup(soup)\n\n        gre_words[url.split('/')[-1]] = words\n\n        print \"sleeping for 5 seconds now\"\n        time.sleep(5)\n\n    return gre_words\n\nif __name__ == '__main__':\n\n    urls = generate_urls(URL, START_PAGE, END_PAGE+1)\n\n    gre_words = scrapper_bot(urls)\n\n    save_as_json(gre_words, OUTPUT_FILE)\n```", "```py\n{\"wordlist_04\":\n    {\"Group 10\":\n        {\"Devoured\": \"greedily eaten/consumed\",\n         \"Magnate\": \"powerful businessman\",\n         \"Cavalcade\": \"procession of vehicles\",\n         \"Extradite\": \"deport from one country back to the home...\n    .\n    .\n    .\n}\n```"]