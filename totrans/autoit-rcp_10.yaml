- en: Chapter 10. Data Analysis and Visualizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have so much of data and its just lying around? Ever wondered how you could
    easily analyze data and generate insights? Curious about the data analysis process?
    Well, you are at the right place!
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Reading, selecting, and interpreting data with visualizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating insights with data filtering and aggregation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating social media analysis for businesses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*In God we trust. All others must bring data                              
                                       -W. Edwards Demming, Statistician*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Today, businesses heavily rely on data to get insights into what customers need,
    what channels they will use to buy, and so on. This way, businesses can take informed
    decisions about launching a new product or coming up with new offers. But how
    do businesses achieve this? What does decision making actually involve?
  prefs: []
  type: TYPE_NORMAL
- en: Data-based decision making refers to the process of data inspection, scrubbing
    or cleaning, data transformation, and generating models on top of data for the
    purpose of generating insights, discovering useful information, and drawing conclusions.
    For instance, an e-commerce company would use this process to analyze consumer
    buying patterns and suggest appropriate time slots for coming up with promotional
    offers for a select group of products. In fact, businesses analyze static or real-time
    data for multiple purposes, such as generating trends, building forecast models,
    or simply to extract structured information from raw data. Data analysis has multiple
    facets and approaches and can be briefly categorized under business intelligence,
    predictive analytics, and text mining.
  prefs: []
  type: TYPE_NORMAL
- en: '**Business intelligence** (**BI**) is capable of handling large amounts of
    structured and, sometimes, unstructured data to allow for the easy interpretation
    of these large volumes of data. Identifying new opportunities based on insights
    into data can provide businesses with a competitive advantage and stability.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Predictive analytics** encompasses the application of various statistical
    models, such as machine learning, to analyze historical data and current trends,
    in order to come up with predictions for future or unknown events. It involves
    generating models and capturing relationships among data features for risk assessment
    and decision making.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text analytics** is the process of deriving quality information from structured
    or unstructured text data. Text analytics involves linguistic, statistical, and
    contextual techniques to extract and classify information for businesses.'
  prefs: []
  type: TYPE_NORMAL
- en: However, data-based decisions are not easy and cannot be taken in a jiffy. Data-based
    decision making is a step-by-step process involving multiple operations. Let's
    understand the complete process in detail in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Steps to data-based decision making
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At a high level, the process can be categorized into the following phases.
    Of course, you can customize the process to suit your objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Define hypothesis and data requirements**: Before you start the process,
    you should be clear with your business goals--they should be **Specific, Measurable,
    Acceptable, Relevant, and Timely** (**SMART**). You don''t want to start collecting
    data without being clear about the problem you''re solving. As far as possible,
    come up with clear problem statements, such as "What has been the trend for mobile
    sales in the consumer space for the last three quarters?" Or something futuristic
    such as "Will I be able to sell electronic goods this winter with a profit margin
    of 150%?" Can you come up with a statement like this for your company?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data source**: You should also be clear about the source of your data. Are
    you relying on your own company database to perform the data analysis? Are you
    also relying on any third-party market research or trends to base your analysis
    on? If you are using third-party data, how do you plan to extract the data from
    the source (possibly through an API) and put it in your data store?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data collection**: Now that you''re clear about what you want to generate
    insights into, the next step is to collect data in the required format. For instance,
    if you want data on the trend of mobile sales, you should collect data for the
    factors that influence mobile sales, such as new product introductions (product),
    offers (price), payment options, and date/time of purchase, among other relevant
    factors. Also, you should have an agreeable or a standard way of storing data;
    for instance, I may store the mobile sales per unit in USD and not in EUR, or
    I may store the sales in days and not in hours. Identifying a representative sample
    is really useful in such cases. Representative samples accurately reflect the
    entire population and definitely help in analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data transformation**: Now you know where to collect the data from and in
    what format, it''s time to decide where you want to load the data. It could be
    a plain old CSV or an SQL database; you need to know beforehand so that you can
    organize the data in the best way and get ready for analysis. This step can be
    referred to as transformation, as it involves extracting data from the source
    data system to a destination data system. In large scales, data is stored in a
    data warehouse system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data cleansing**: Once the data is processed and organized, it''s time to
    look at the data sanity. Transformed data may be incompatible, contain duplicates,
    or may at least contain measurement, sampling, and data entry errors. Data cleansing
    involves the removal of inaccurate data, adding default values for missing data,
    removing outliers, and resolving any other data inconsistency issues. You really
    have to be careful while dumping outliers; you should decide on the ways you want
    to remove them--is it a simple deletion of records or imputing them with the mean/mode
    of the other observations? You''re the best decision maker in this case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data analysis**: Once we have the data cleansed and ready for use, it''s
    time for deeper analysis. You can analyze the data for business intelligence,
    or generate predictive models, using statistical techniques such as Logistic Regression.
    You could also perform text analysis on top of it to generate insights and arrive
    at decisions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data visualization**: Once the analysis is done, it can be reported in many
    formats so that the analysis can be effectively communicated to the audience.
    Data visualization uses information display, such as tables and charts, to help
    communicate key messages contained in the data. Visualizations also help users
    to interpret their assumptions and generate meaningful information from the analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data interpretation and feedback**: This phase helps you answer three main
    questions. Does the analysis answer the questions you began with? Does it help
    you validate your assumptions, that is, accept or reject your hypothesis? Do you
    need more data to improve your models or conclusions? It''s not complete until
    your conclusions don''t flow back into the system. The feedback loop makes sure
    that the predictive models are enriched and trained well for future use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OK, that''s a good start! I think you must have got a fair idea of the complete
    process: data collection to generating insights. You will realize that a few of
    these steps, such as defining objectives, data collection, and transforming data,
    are custom to the market context and the problem being solved.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus on a few generic aspects, such as collecting
    real-time data, reading data, performing data analysis, and data visualization.
    We will take a look at the popular Python modules that will help us read the data
    efficiently and analyze the data to generate insights. You will also learn about
    the Python modules that help interpret data and generate visualizations (charts).
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this chapter, we will also look at a typical business process
    that can be automated with the knowledge we built with the recipes covered in
    the chapter. This chapter will help you start your journey as a data scientist,
    but doesn't cover extensive topics, such as statistical techniques or predictive
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'During the course of this chapter, we will use the following Python modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas` ([http://pandas.pydata.org/pandas-docs/version/0.15.2/tutorials.html](http://pandas.pydata.org/pandas-docs/version/0.15.2/tutorials.html))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy` ([http://www.numpy.org/](http://www.numpy.org/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib` ([http://matplotlib.org/](http://matplotlib.org/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seaborn` ([https://pypi.python.org/pypi/seaborn/](https://pypi.python.org/pypi/seaborn/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading, selecting, and interpreting data with visualizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will have the help of a known dataset. We will use TechCrunch''s
    Continental USA CSV file that contains the listing of 1,460 company funding rounds.
    This is how it looks. It contains data points, such as the company name, number
    of employees, funding date, amounts raised, and type of funding (series A or angel
    funding):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reading, selecting, and interpreting data with visualizations](img/image_10_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s install the modules that we will use to read and select the data
    from this CSV file. Before doing that, we will set up a virtual environment and
    activate it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'OK, cool! Now, let''s install `pandas`. We will use `pandas` to read our CSV
    file and select the data to perform analysis. We install `pandas` with our favorite
    utility, `python-pip`. The following are the installation logs for `pandas` on
    my Mac OSX:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: Installing the `pandas` module also installs the `numpy` module for me. In fact,
    I had installed these modules on my machine earlier as well; hence, a lot of these
    modules get picked up from cache. On your machine, the installation logs may differ.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, let''s install `matplotlib` and `seaborn`; libraries that will be used
    by us for visualizations. The following are the installation logs on my machine,
    first for `matplotlib`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As you can see these modules are installed on my machine, so the installation
    logs may differ when you install these modules for the first time on your machine. Here
    are the logs for seaborn:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let''s just download the CSV file from [https://support.spatialkey.com/spatialkey-sample-csv-data/](http://samplecsvs.s3.amazonaws.com/TechCrunchcontinentalUSA.csv).
    The direct download link for the TechCrunch file is [http://samplecsvs.s3.amazonaws.com/TechCrunchcontinentalUSA.csv](http://samplecsvs.s3.amazonaws.com/TechCrunchcontinentalUSA.csv).
    You can download this file with the `wget` command as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s go ahead and write our first piece of Python code to read the CSV
    file. We read the CSV file and print the first five rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the preceding code example, we read the first five records of the CSV file:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_002.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'The `pandas` module reads the file''s contents and converts them to a data
    frame of rows and columns. Now, if you look at the output of the preceding code,
    you will notice that an index column gets added to the file contents. With `pandas`,
    it''s easy to parse the date, tell if the dates in our CSV file have the date
    first or month first (UK or US format), and make the date column as the index
    column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you run the preceding code snippet, you should be able to see the index
    column, **fundedDate**, as shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_003.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Neat! Now, we''re able to read the data, but how about selecting some data
    so that we can perform some analysis on top of it. Let''s select the column that
    depicts the amount of funding raised by the companies (the **raisedAmt** column):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that in the following screenshot, we have printed the top five records
    of the companies that have raised funding:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_004.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'OK, cool! So we can select the column of our choice and get the data we wanted
    to analyze. Let''s see if we can generate some nice visualizations for it. The
    following recipe generates a line chart for the funding rounds reported for all
    the years (*x* axis), based on the amount raised (*y* axis):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the following screenshot, see how the rate of funding (or the rate of reporting)
    increased, and with that, the amounts raised also saw a steady increase!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_005.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Fantastic! I know you have already started to like what we''re doing here.
    Let''s move forward and see whether we can select multiple columns from the CSV
    file. In the following example, we get the data for 50 rows, with the column names
    being **company**, **category**, and **fundedDate**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding code snippet is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_006.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'OK, great! Now let''s select one of these columns and perform some analysis
    on top of it. In the following code example, we select the **category** column
    that gives us the categories of all the reported funding rounds. We then process
    the selected column to get the most common category of the company that got funded:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding code snippet is:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Data and numbers give a lot of information, but the impact can actually only
    be seen through visualizations. Let''s see whether we can plot the above data
    as a horizontal bar chart. The following recipe does the job for us:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'On the *y* axis, we have the category of the company that got funded, and the
    *x* axis is the total count of companies in a given category. Also, we save the
    plotted chart in a PDF file named `categoriesFunded.pdf`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_007.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Whoa! So many web companies got funded? Awesome! I too should start a web company--it
    increases the chances of getting funded.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section we dealt with the two major aspects of data analysis. First,
    we covered how to read the dataset from a CSV file and select the appropriate
    data (rows or columns) from our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the first code snippet, we used the help of the `pandas` module to read the
    CSV file. In `pandas`, we have the `read_csv(csv_file)` method that takes the
    CSV file path as an argument. The `pandas` module reads the file and stores the
    file contents as data frames. DataFrame is a two-dimensional, labeled-in-memory
    data structure with columns of potentially different types. It mimics the structure
    of a spreadsheet or an SQL table or a dictionary of series objects. It has a nice
    set of methods and attributes to select, index, and filter data. In our first
    recipe, we read the CSV file and generated a DataFrame object `df`. Using `df`
    we selected the first five rows of our CSV file with `df[:5]`. See how easy it
    becomes to select the rows of a CSV file with `pandas`.
  prefs: []
  type: TYPE_NORMAL
- en: We can do a few more things with the `read_csv()` method. By default, `pandas`
    adds another index column to our dataset, but we can specify which column from
    the CSV file should be used for indexing our data. We achieved this by passing
    the `index_col` parameter to the `read_csv()` method. We also converted the string
    dates present in the **fundedDate** column of the CSV file to a datetime format
    with the `parse_dates` parameter and said that the date is in a format where the
    day is the first part of the date with the `dayfirst` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: After getting the DataFrame and using **fundedDate** as index, we used `df['raisedAmt'][:5]`
    to select the **raisedAmt** column and print the first five rows. We then used
    the `seaborn` library to set the style of our plot with `sns.set_style("darkgrid")`
    and generated the bar chart with the `plot()` method. The `seaborn` library is
    used for generating nice visualizations and is implemented on `matplotlib`.
  prefs: []
  type: TYPE_NORMAL
- en: Using the `matplotlib` library, we created an object, `plt`, which was then
    used to label our chart with the `ylabel()` and `xlabel()` methods. The `plt`
    object was also used to finally store the resulting chart in the PDF format with
    the `savefig()` method.
  prefs: []
  type: TYPE_NORMAL
- en: In the second example, we selected multiple columns with `fundings[['company',
    'category' and 'fundedDate']]`. We selected three columns from a CSV file in one
    line of code. We then plotted a horizontal bar chart with the `plot()` method
    and specified the type of chart with `kind=barh`. Finally, we made use of the
    `matplotlib` library to label the *x* axis with the `xlabel()` method and saved
    the chart with the `savefig()` method. As you can see, we didn't have to use the
    `seaborn` library to plot the chart; we could simply do it with `matplotlib`.
  prefs: []
  type: TYPE_NORMAL
- en: Generating insights using data filtering and aggregation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reading CSV files and selecting multiple columns is easy with `pandas`. In this
    section, we will take a look at how to slice and dice data, essentially filtering data
    with `pandas`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will use the same set of libraries (the following ones)
    that we used in the previous recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas` for filtering and data analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib` and `seaborn` for plotting charts and saving the data in a PDF
    file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start by importing the required libraries and reading the CSV file with
    the `read_csv()` method. The following code carries out the operations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s filter on the data frame and use multiple columns to filter data.
    Say we filter funding records on the funding category, state, and the selected
    city in the state. This can be achieved with the following piece of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding piece of code returns all the funding records in the State of
    California (CA) for the cities Palo Alto, San Francisco, San Mateo, Los Angeles,
    and Redwood City for the web companies. The following is a partial screenshot
    of the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_008.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'OK cool, now let''s see if we can get the count of funding for the companies
    in the web category by city names. The following code will get us the details
    we need:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding piece of code is the number of funding rounds received
    by the companies in the **web** category in the selected cities:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_009.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Wow! The preceding analysis was quite useful; you got to know that the web companies
    in San Francisco city have received the funding 195 times (from the data we have).
    Looks like if you are a web company in San Francisco, all your funding worries
    are over. Well, that sounds logical and simple.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'But wait, isn''t this information incomplete? How about gathering data for
    companies in all categories including **web** and then representing the data for
    the companies in the **web** category as a percentage of all the categories? This
    way, we will know whether you should have your company in ''San Francisco'' or
    in any other city. OK then, let''s get the count of funding rounds for companies
    in all categories (including **web**), for all the selected cities in CA. The
    following code will get us the information we need:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output of the preceding code snippet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_010.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Nice! Now, let''s get the data for the companies in the **web** category as
    a percentage of companies in all the categories for the selected cities of CA.
    We can get this data by simply dividing the data for the **web** categories by
    all the categories and then multiplying by 100 to represent it as a percentage.
    The following code snippet will help us in this case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s plot this data as a horizontal bar chart with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following screenshot helps us compare the funding rounds for web companies
    with respect to the funding rounds for companies in all other categories and for
    the cities in California:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_011.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: After the analysis what did you figure? Do you still want to set up your company
    in San Francisco? If you are a web company in Los Angeles, even though the funding
    rounds are limited, there is a higher chance (0.925) of your company getting funded
    than when you're in San Francisco (0.855), at least from the data points we have.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at another example. Say we want to analyze our data
    to see what months have historically received more funding than others. Additionally,
    can we also relate this with the rounds of funding [such as series A or angel
    funding]? Interesting thought! But how do we get there? The `pandas` module has
    support for grouping the data and data aggregation, which will come to our rescue
    for this analysis. Let''s solve this problem step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s read the CSV file and select two columns: **raisedAmt** and **rounds**.
    Let''s also add another column, `month`, as the index column to this data frame.
    The following code gets the data frame ready for further analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we need to get the data for the funds raised based on the month. The following
    code does exactly what we need:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, if we plot the data for this analysis, we will see how the funding fluctuates
    on a monthly basis for all the years of data we have:![How to do it...](img/image_10_012.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cool, looks like it's better to ask for funding in January. Maybe, the investors
    are in a good mood after the Christmas and New Year vacation; what do you think?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, if you want to analyze and build a correlation between the month of year,
    amount raised, and the round of funding, we can get the data with the following
    piece of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding code snippet is a data frame arranged as shown
    in the following screenshot. The data is grouped by **month** and **round** columns
    of funding, and **raisedAmt** is aggregated accordingly:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_013.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the first problem, we load the data from the CSV file as a data frame with
    the `read_csv()` method. Then, we filter the data based on multiple factors, where
    the state is `CA` and the company category is `web` and the cities are `Palo Alto`,
    `San Francisco`, `San Mateo`, `Los Angeles`, and `Redwood City`. Filtering is
    a column-based operation and is pretty straight forward; it gets you the relevant
    data frame after applying the criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we calculated the count of funding rounds, grouped by cities for the **web**
    companies with the `value_counts()` method. We did the same exercise for the funding
    rounds for companies in all the categories, including **web**.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we simply divided the data and got the data for the **web** companies
    as a percentage of the data for all the categories. The `pandas` module handled
    this operation for us seamlessly. It used both the data points for the same city
    for analysis without us even worrying about it.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we plotted the horizontal bar chart with the `plot()` method, depicted
    the percentages for each city individually, and got the insight we were looking
    for.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second example, we first got the data frame by selecting multiple columns:
    **raisedAmt** and **round**. We also added a new column to the **month** DataFrame
    and treated it as an index column.'
  prefs: []
  type: TYPE_NORMAL
- en: Then, we grouped the data based on **month** with the help of the **groupby()**
    method. We then summed up the funding amounts to get the amount of funds raised
    based on **month**. To get the total funds, we used the **aggregation()** method
    and added the data points to get the required information.
  prefs: []
  type: TYPE_NORMAL
- en: Also, to build the correlation between the funds raised with respect to **month**
    and **round**, we grouped the data frame by **month** and **round** and again
    applied the aggregation on **raisedAmt**.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding two recipes, we learned about data analysis and visualization,
    and we extensively used the `pandas` Python module in our recipes. The `pandas` module
    is a very comprehensive library and has capabilities such as working with time
    series, advanced indexing techniques, merging and concatenating objects, and working
    with different datasets (JSON and Excel), amongst others. I highly encourage you
    to go through the `pandas` API Reference to learn more about this awesome library.
  prefs: []
  type: TYPE_NORMAL
- en: In the next recipe, let's see whether we can apply the knowledge we have gained
    so far in this chapter by helping Judy automate her task.
  prefs: []
  type: TYPE_NORMAL
- en: Automating social media analysis for businesses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Judy is a columnist at a leading magazine in London. As a writer, she is always
    interested in recent topics. She collects data and analyzes it to come up with
    insights that would be interesting to her readers.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, Judy is interested in the battle between Apple's iPhone and Samsung's
    Note and wants to publish an article in her magazine. She plans to collect the
    data by talking to people on the streets and reading blog posts, but she knows
    she will get an awful lot of information from social media. She is aware of the
    fact that people take to Twitter these days to express their pleasure or disappointment
    about using a product and also refer products to their friends on social media.
    However, she is worried about the fact that she has to go through this huge volume
    of social media data for her article.
  prefs: []
  type: TYPE_NORMAL
- en: You are a data scientist and Judy's colleague. Can you help Judy with her needs?
    It is an opportunity for you to show off your data skills!
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin by analyzing Judy's problems. To begin with, Judy needs to collect
    data from an ever-growing social media platforms such as Twitter. Secondly, she
    needs to analyze this data to generate interesting insights. So, we should be
    able to build a system that caters to both her problems. Also, you may want to
    build a system that only solves her current needs, but she should be able to use
    it for any projects in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s install all the modules that we''ll need to work on this problem. We
    already have `pandas`, `matplotlib`, and `seaborn` installed. For this problem,
    we will also install `tweepy`, a module to work with Twitter data. Let''s install
    tweepy with our very own `python-pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OK nice, we're now armed with all the modules. So, let's get started by collecting
    data from Twitter. Twitter has this amazing set, Streaming APIs, which helps developers
    collect tweets in real time. We will use this library for our data collection
    needs too.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code uses the Twitter Streaming APIs to collect data and store
    it in a text file with each tweet stored in the JSON format. We look for tweets
    that have two keywords, `iPhone 7` and `Note 5`. For this chapter, I ran the code
    for around 5 minutes, but for Judy we may have to run it for hours, or even days,
    to collect the maximum data to generate accurate insights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Okay, now that we have the data from Twitter flowing in, let's write a code
    to analyze this data and see whether we can find any interesting stuff that can
    be shared with Judy for her article.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Apple iPhone and Samsung Note are such popular products on a global level that
    people talk about these products from all around the world. It''d be really interesting
    to find the different languages used by consumers to talk about these products
    on Twitter. The following code does exactly what we wanted to do with Twitter
    data. It goes through the stored tweets, figures out the languages of all the
    tweets, and groups them to plot the top four languages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If we run the preceding code snippet, it plots the bar chart for the top languages
    people have used to tweet about iPhone 7 and Note 5.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_014.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Awesome! I think Judy will love this analysis. Even though the top language
    is English (**en**), as expected, it is quite interesting to see that the other
    three languages are Italian (**it**), Spanish (**es**), and Portuguese (**pt**).
    This will be a cool feed for her article.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The results that you will get from this exercise will depend on when you run
    the data collection program. For instance, if you run it for a short duration
    between 2 and 8 a.m. GMT time, you will see more tweets in Chinese or Japanese
    as it is daytime in these countries. By the way, wouldn't it be interesting to
    analyze what are the best times of the day when people tweet? You may find some
    correlation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s go further and do some more cool stuff with this data. How about performing
    text-based analysis on the tweets to get consumer sentiments about these products?
    By sentiment, I mean, is a tweet cursing these products, appreciating a product''s
    feature, or just a passing comment? But wait; is it possible to get this kind
    of data? Yes, absolutely. The following code uses Python''s NTLK-based APIs to
    perform sentiment analysis on tweets (text) to determine its polarity: positive,
    negative or neutral sentiment. It then groups this data to represent it with a
    bar chart and saves it in a PDF file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you run the preceding piece of code, you will get a bar chart with the sentiment
    data for all the stored tweets. Just by looking at the graph, you will know that
    the consumers have generally spoken positively about both products:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_015.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: There are a few complaints about them, hence the sentiments with negative polarity,
    but then there are neutral comments that could just be product referrals or comments
    on the products. Neat!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: While this is nice, don't you think Judy might be interested to know how many
    of these negative tweets are about the iPhone or Note? Well, I will leave that
    to you; I'm sure you will figure that out for Judy.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the first problem, we first collected the data required for analysis. Twitter's
    Streaming APIs set helps us collect this information in real time. To use Streaming
    APIs, you need to register for a developer app with Twitter and collect your consumer
    key, consumer secret, auth token, and auth secret. It is a straightforward process
    and can be easily looked up on [https://dev.twitter.com](https://dev.twitter.com).
    So, in the data collection example (the first example in this recipe), we instantiated
    the `OAuthHandler` class to get Twitter's authorization object, `auth`, and then
    used it to set the authorization token and secret with the `set_access_token()`
    method. The `Stream` class of Twitter's Streaming APIs is bound to the `listener`
    class and returns the `twitterStream` object. The `listener` class inherits the
    `StreamListener` class, which will monitor incoming tweets and take actions on
    the arriving tweets in the `on_data()` method. The filtering of tweets is done
    with the `twitterStream.filter()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we know that the incoming tweets are available in the `on_data()` method;
    we hooked on to it to store the tweets in the `twitter_data.txt` file. For this,
    we opened the file in the write (`w`) mode and used the `write()` method to write
    the tweet to the file in the JSON format. With this, we finished the first recipe
    and collected the data required by Judy. Now, it's time to perform the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: For the first insight on getting the languages, we started by opening the `twitter_data.txt`
    file in the read(`r`) mode. We read through all the tweets (the JSON format) and
    appended them to the `tweets` array. Using `pandas`, we created an empty DataFrame
    object, `tweet_df`, with `pd.DataFrame()`. With Python's `map()` method, we operated
    on the `tweets` array and added a new column, `lang`, to our empty DataFrame.
    The `value_counts()` method was then used to get the count of all the languages
    of the tweets under analysis and was stored in the variable, `tweets_by_lang`.
  prefs: []
  type: TYPE_NORMAL
- en: The other part of the code is as usual, where we created a `plt` object from
    matplotlib and used the `plot()` method to generate a bar chart using the `seaborn`
    library. We set the axis labels with the `set_xlabel()` and `set_ylabel()` methods
    and used the colors `green`, `blue`, `red`, and `black` to manifest the different
    languages. Finally, we saved the plot in a PDF file with the `savefig()` method.
  prefs: []
  type: TYPE_NORMAL
- en: For the second insight involving sentiment analysis, we started by reading through
    all the tweets from `twitter_data.txt` and storing them to the `tweets` array.
    We then created an empty data frame, probabilities, processed all the tweets for
    sentiment analysis, and stored the analysis in the `prob` array. We then added
    a column, `text`, to our empty data frame using the `map()` method on the `prob`
    array.
  prefs: []
  type: TYPE_NORMAL
- en: Our data frame, `probabilities['text']`, now contains sentiments for all the
    tweets we analyzed. Following the regular set of operations, we got the set of
    values for `positive`, `negative`, and `neutral` sentiments for the analyzed tweets
    and plotted them as a bar chart.
  prefs: []
  type: TYPE_NORMAL
- en: If you look at all the examples in this recipe, we have divided the task of
    data collection and analysis as separate programs. If our visualizations are massive,
    we can even separate them out. This makes sure that Judy can use the data collection
    Python program to gather information on another set of keywords for her articles
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: She can also run the analysis on the data sets she has by making small changes
    to the analysis and visualization parameters from our program. So, for all her
    articles in the future, we have managed to automate the data collection, analysis,
    and visualization process for her.
  prefs: []
  type: TYPE_NORMAL
- en: I'm already seeing a smile on Judy's face. I hope you enjoyed this chapter.
    I'm confident that the knowledge you gained in this chapter will get you started
    into the world of data and visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have just scratched the surface of text-based sentiment analysis in the preceding
    recipe. Sentiment analysis involves text classifications, tokenization, semantic
    reasoning, and much more interesting stuff. I highly encourage you to go through
    a book on NLTK to learn more about working with text in Python at [http://www.nltk.org/book/](http://www.nltk.org/book/).
  prefs: []
  type: TYPE_NORMAL
