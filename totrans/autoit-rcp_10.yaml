- en: Chapter 10. Data Analysis and Visualizations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章 数据分析和可视化
- en: You have so much of data and its just lying around? Ever wondered how you could
    easily analyze data and generate insights? Curious about the data analysis process?
    Well, you are at the right place!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 你有如此多的数据，它们只是随意地躺在那里？你是否想过如何轻松地分析数据并生成见解？对数据分析过程好奇吗？那么，你就在正确的位置！
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Reading, selecting, and interpreting data with visualizations
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用可视化阅读、选择和解释数据
- en: Generating insights with data filtering and aggregation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据过滤和聚合生成见解
- en: Automating social media analysis for businesses
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为企业自动化社交媒体分析
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: '*In God we trust. All others must bring data                              
                                       -W. Edwards Demming, Statistician*'
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我们信仰上帝。其他人必须带来数据  - W. Edwards Demming，统计学家*'
- en: Today, businesses heavily rely on data to get insights into what customers need,
    what channels they will use to buy, and so on. This way, businesses can take informed
    decisions about launching a new product or coming up with new offers. But how
    do businesses achieve this? What does decision making actually involve?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，企业严重依赖数据来了解客户的需求，他们将通过哪些渠道购买，等等。这样，企业可以就推出新产品或提出新优惠做出明智的决策。但企业是如何实现这一点的？决策实际上涉及什么？
- en: Data-based decision making refers to the process of data inspection, scrubbing
    or cleaning, data transformation, and generating models on top of data for the
    purpose of generating insights, discovering useful information, and drawing conclusions.
    For instance, an e-commerce company would use this process to analyze consumer
    buying patterns and suggest appropriate time slots for coming up with promotional
    offers for a select group of products. In fact, businesses analyze static or real-time
    data for multiple purposes, such as generating trends, building forecast models,
    or simply to extract structured information from raw data. Data analysis has multiple
    facets and approaches and can be briefly categorized under business intelligence,
    predictive analytics, and text mining.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 基于数据的决策制定是指数据检查、清洗或清理、数据转换以及在数据之上生成模型的过程，目的是生成见解、发现有用信息并得出结论。例如，一家电子商务公司会使用这个过程来分析消费者购买模式，并为选定的一组产品提出促销优惠的适当时间。实际上，企业分析静态或实时数据用于多个目的，例如生成趋势、建立预测模型或简单地从原始数据中提取结构化信息。数据分析具有多个方面和方法，可以简要地归类为商业智能、预测分析和文本挖掘。
- en: '**Business intelligence** (**BI**) is capable of handling large amounts of
    structured and, sometimes, unstructured data to allow for the easy interpretation
    of these large volumes of data. Identifying new opportunities based on insights
    into data can provide businesses with a competitive advantage and stability.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**商业智能**（**BI**）能够处理大量结构化和有时非结构化的数据，以便轻松解释这些大量数据。基于对数据的洞察力识别新机会可以为业务提供竞争优势和稳定性。'
- en: '**Predictive analytics** encompasses the application of various statistical
    models, such as machine learning, to analyze historical data and current trends,
    in order to come up with predictions for future or unknown events. It involves
    generating models and capturing relationships among data features for risk assessment
    and decision making.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测分析**包括应用各种统计模型，如机器学习，来分析历史数据和当前趋势，以便对未来或未知事件做出预测。它涉及生成模型并捕捉数据特征之间的关系，以进行风险评估和决策制定。'
- en: '**Text analytics** is the process of deriving quality information from structured
    or unstructured text data. Text analytics involves linguistic, statistical, and
    contextual techniques to extract and classify information for businesses.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本分析**是从结构化或非结构化文本数据中提取质量信息的过程。文本分析涉及语言、统计和上下文技术，以提取和分类信息，为商业提供支持。'
- en: However, data-based decisions are not easy and cannot be taken in a jiffy. Data-based
    decision making is a step-by-step process involving multiple operations. Let's
    understand the complete process in detail in the next section.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，基于数据的决策并不容易，也不能一蹴而就。基于数据的决策制定是一个涉及多个操作的逐步过程。让我们在下一节中详细了解整个过程。
- en: Steps to data-based decision making
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于数据的决策制定步骤
- en: 'At a high level, the process can be categorized into the following phases.
    Of course, you can customize the process to suit your objectives:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，这个过程可以分为以下阶段。当然，你可以根据你的目标定制这个过程：
- en: '**Define hypothesis and data requirements**: Before you start the process,
    you should be clear with your business goals--they should be **Specific, Measurable,
    Acceptable, Relevant, and Timely** (**SMART**). You don''t want to start collecting
    data without being clear about the problem you''re solving. As far as possible,
    come up with clear problem statements, such as "What has been the trend for mobile
    sales in the consumer space for the last three quarters?" Or something futuristic
    such as "Will I be able to sell electronic goods this winter with a profit margin
    of 150%?" Can you come up with a statement like this for your company?'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义假设和数据需求**：在开始这个过程之前，你应该对自己的业务目标有清晰的认识——它们应该是**具体、可衡量、可接受、相关和及时**的（**SMART**）。你不希望在不清楚要解决的问题的情况下开始收集数据。尽可能明确地提出问题陈述，例如：“过去三个季度消费者领域的移动销售趋势是什么？”或者某种未来性的问题，例如：“这个冬天我能否以150%的利润率销售电子产品？”你能为你公司提出这样的陈述吗？'
- en: '**Data source**: You should also be clear about the source of your data. Are
    you relying on your own company database to perform the data analysis? Are you
    also relying on any third-party market research or trends to base your analysis
    on? If you are using third-party data, how do you plan to extract the data from
    the source (possibly through an API) and put it in your data store?'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据来源**：你也应该清楚你的数据来源。你是依赖你自己的公司数据库来进行数据分析吗？你也在依赖任何第三方市场研究或趋势作为分析的基础吗？如果你使用第三方数据，你计划如何从源（可能通过API）提取数据并将其放入你的数据存储中？'
- en: '**Data collection**: Now that you''re clear about what you want to generate
    insights into, the next step is to collect data in the required format. For instance,
    if you want data on the trend of mobile sales, you should collect data for the
    factors that influence mobile sales, such as new product introductions (product),
    offers (price), payment options, and date/time of purchase, among other relevant
    factors. Also, you should have an agreeable or a standard way of storing data;
    for instance, I may store the mobile sales per unit in USD and not in EUR, or
    I may store the sales in days and not in hours. Identifying a representative sample
    is really useful in such cases. Representative samples accurately reflect the
    entire population and definitely help in analysis.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据收集**：现在你已经清楚了你想要从中获得洞察力的内容，下一步就是以所需格式收集数据。例如，如果你想了解移动销售的趋势，你应该收集影响移动销售的因素的数据，如新产品推出（产品）、优惠（价格）、支付选项和购买日期/时间，以及其他相关因素。此外，你应该有一个可接受或标准的存储数据的方式；例如，我可能以美元为单位存储每单位的移动销售，而不是欧元，或者我可能以天为单位存储销售，而不是小时。在这种情况下，确定一个代表性的样本非常有用。代表性的样本能够准确反映整个群体，并肯定有助于分析。'
- en: '**Data transformation**: Now you know where to collect the data from and in
    what format, it''s time to decide where you want to load the data. It could be
    a plain old CSV or an SQL database; you need to know beforehand so that you can
    organize the data in the best way and get ready for analysis. This step can be
    referred to as transformation, as it involves extracting data from the source
    data system to a destination data system. In large scales, data is stored in a
    data warehouse system.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据转换**：现在你知道了从哪里以及以什么格式收集数据，是时候决定你希望将数据加载到何处了。它可能是一个普通的CSV文件或一个SQL数据库；你事先需要知道这一点，以便以最佳方式组织数据并准备好分析。这一步可以被称为转换，因为它涉及到从源数据系统提取数据到目标数据系统。在大规模情况下，数据存储在数据仓库系统中。'
- en: '**Data cleansing**: Once the data is processed and organized, it''s time to
    look at the data sanity. Transformed data may be incompatible, contain duplicates,
    or may at least contain measurement, sampling, and data entry errors. Data cleansing
    involves the removal of inaccurate data, adding default values for missing data,
    removing outliers, and resolving any other data inconsistency issues. You really
    have to be careful while dumping outliers; you should decide on the ways you want
    to remove them--is it a simple deletion of records or imputing them with the mean/mode
    of the other observations? You''re the best decision maker in this case.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据清洗**：一旦数据被处理和组织，就需要检查数据的合理性。转换后的数据可能不兼容，包含重复项，或者至少包含测量、采样和数据输入错误。数据清洗涉及删除不准确的数据，为缺失数据添加默认值，移除异常值，并解决任何其他数据不一致问题。在移除异常值时，你必须非常小心；你应该决定你想要如何移除它们——是简单地删除记录，还是用其他观察值的平均值/众数来填充？在这种情况下，你是最棒的决策者。'
- en: '**Data analysis**: Once we have the data cleansed and ready for use, it''s
    time for deeper analysis. You can analyze the data for business intelligence,
    or generate predictive models, using statistical techniques such as Logistic Regression.
    You could also perform text analysis on top of it to generate insights and arrive
    at decisions.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分析**：一旦数据被清洗并准备好使用，就到了进行深入分析的时候了。你可以使用诸如逻辑回归等统计技术来分析数据以生成商业智能，或者生成预测模型。你还可以在上面进行文本分析，以生成洞察并做出决策。'
- en: '**Data visualization**: Once the analysis is done, it can be reported in many
    formats so that the analysis can be effectively communicated to the audience.
    Data visualization uses information display, such as tables and charts, to help
    communicate key messages contained in the data. Visualizations also help users
    to interpret their assumptions and generate meaningful information from the analysis.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据可视化**：一旦完成分析，就可以以多种格式报告，以便有效地将分析传达给受众。数据可视化使用信息展示，如表格和图表，来帮助传达数据中包含的关键信息。可视化还有助于用户解释他们的假设，并从分析中生成有意义的见解。'
- en: '**Data interpretation and feedback**: This phase helps you answer three main
    questions. Does the analysis answer the questions you began with? Does it help
    you validate your assumptions, that is, accept or reject your hypothesis? Do you
    need more data to improve your models or conclusions? It''s not complete until
    your conclusions don''t flow back into the system. The feedback loop makes sure
    that the predictive models are enriched and trained well for future use.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据解释和反馈**：这一阶段帮助你回答三个主要问题。分析是否回答了你最初提出的问题？它是否帮助你验证你的假设，即接受或拒绝你的假设？你是否需要更多数据来改进你的模型或结论？只有当你的结论能够反馈回系统中，这个过程才算完整。反馈循环确保预测模型在未来的使用中得到丰富和良好的训练。'
- en: 'OK, that''s a good start! I think you must have got a fair idea of the complete
    process: data collection to generating insights. You will realize that a few of
    these steps, such as defining objectives, data collection, and transforming data,
    are custom to the market context and the problem being solved.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这是一个不错的开始！我想你一定对整个过程有了相当的了解：从数据收集到生成洞察。你会意识到其中的一些步骤，如定义目标、数据收集和数据转换，是特定于市场环境和要解决的问题的。
- en: In this chapter, we will focus on a few generic aspects, such as collecting
    real-time data, reading data, performing data analysis, and data visualization.
    We will take a look at the popular Python modules that will help us read the data
    efficiently and analyze the data to generate insights. You will also learn about
    the Python modules that help interpret data and generate visualizations (charts).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将关注一些通用方面，例如收集实时数据、读取数据、执行数据分析以及数据可视化。我们将探讨一些流行的Python模块，这些模块将帮助我们高效地读取数据并分析数据以生成洞察。你还将了解帮助解释数据和生成可视化（图表）的Python模块。
- en: At the end of this chapter, we will also look at a typical business process
    that can be automated with the knowledge we built with the recipes covered in
    the chapter. This chapter will help you start your journey as a data scientist,
    but doesn't cover extensive topics, such as statistical techniques or predictive
    modeling.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我们还将探讨一个典型的业务流程，该流程可以使用本章中涵盖的食谱所构建的知识进行自动化。本章将帮助你开始作为数据科学家的旅程，但并不涵盖广泛的主题，如统计技术或预测建模。
- en: 'During the course of this chapter, we will use the following Python modules:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的进行过程中，我们将使用以下Python模块：
- en: '`pandas` ([http://pandas.pydata.org/pandas-docs/version/0.15.2/tutorials.html](http://pandas.pydata.org/pandas-docs/version/0.15.2/tutorials.html))'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas` ([http://pandas.pydata.org/pandas-docs/version/0.15.2/tutorials.html](http://pandas.pydata.org/pandas-docs/version/0.15.2/tutorials.html))'
- en: '`numpy` ([http://www.numpy.org/](http://www.numpy.org/))'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy` ([http://www.numpy.org/](http://www.numpy.org/))'
- en: '`matplotlib` ([http://matplotlib.org/](http://matplotlib.org/))'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib` ([http://matplotlib.org/](http://matplotlib.org/))'
- en: '`seaborn` ([https://pypi.python.org/pypi/seaborn/](https://pypi.python.org/pypi/seaborn/))'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seaborn` ([https://pypi.python.org/pypi/seaborn/](https://pypi.python.org/pypi/seaborn/))'
- en: Reading, selecting, and interpreting data with visualizations
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用可视化读取、选择和解释数据
- en: 'In this recipe, we will have the help of a known dataset. We will use TechCrunch''s
    Continental USA CSV file that contains the listing of 1,460 company funding rounds.
    This is how it looks. It contains data points, such as the company name, number
    of employees, funding date, amounts raised, and type of funding (series A or angel
    funding):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用一个已知的数据集。我们将使用 TechCrunch 的美国大陆 CSV 文件，其中包含 1,460 家公司融资轮次的列表。它看起来是这样的。它包含数据点，例如公司名称、员工人数、融资日期、筹集的资金金额和融资类型（A
    轮或天使投资）：
- en: '![Reading, selecting, and interpreting data with visualizations](img/image_10_001.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![使用可视化读取、选择和解释数据](img/image_10_001.jpg)'
- en: 'Now, let''s install the modules that we will use to read and select the data
    from this CSV file. Before doing that, we will set up a virtual environment and
    activate it:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们安装我们将用于从该 CSV 文件读取和选择数据的模块。在这样做之前，我们将设置一个虚拟环境并激活它：
- en: '[PRE0]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'OK, cool! Now, let''s install `pandas`. We will use `pandas` to read our CSV
    file and select the data to perform analysis. We install `pandas` with our favorite
    utility, `python-pip`. The following are the installation logs for `pandas` on
    my Mac OSX:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 好的，太棒了！现在，让我们来安装 `pandas`。我们将使用 `pandas` 来读取我们的 CSV 文件并选择数据进行分析。我们使用我们最喜欢的工具
    `python-pip` 来安装 `pandas`。以下是我 Mac OSX 上安装 `pandas` 的日志：
- en: '[PRE1]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Installing the `pandas` module also installs the `numpy` module for me. In fact,
    I had installed these modules on my machine earlier as well; hence, a lot of these
    modules get picked up from cache. On your machine, the installation logs may differ.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 安装 `pandas` 模块也自动安装了 `numpy` 模块。实际上，我之前已经在我的机器上安装了这些模块；因此，许多这些模块都是从缓存中获取的。在您的机器上，安装日志可能会有所不同。
- en: 'Next, let''s install `matplotlib` and `seaborn`; libraries that will be used
    by us for visualizations. The following are the installation logs on my machine,
    first for `matplotlib`:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们安装 `matplotlib` 和 `seaborn`；这些库将用于我们的可视化。以下是我机器上的安装日志，首先是 `matplotlib`：
- en: '[PRE2]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As you can see these modules are installed on my machine, so the installation
    logs may differ when you install these modules for the first time on your machine. Here
    are the logs for seaborn:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如您所见，这些模块已安装在我的机器上，因此您在第一次在自己的机器上安装这些模块时，安装日志可能会有所不同。以下是 seaborn 的日志：
- en: '[PRE3]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: How to do it...
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'First, let''s just download the CSV file from [https://support.spatialkey.com/spatialkey-sample-csv-data/](http://samplecsvs.s3.amazonaws.com/TechCrunchcontinentalUSA.csv).
    The direct download link for the TechCrunch file is [http://samplecsvs.s3.amazonaws.com/TechCrunchcontinentalUSA.csv](http://samplecsvs.s3.amazonaws.com/TechCrunchcontinentalUSA.csv).
    You can download this file with the `wget` command as follows:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们从 [https://support.spatialkey.com/spatialkey-sample-csv-data/](http://samplecsvs.s3.amazonaws.com/TechCrunchcontinentalUSA.csv)
    下载 CSV 文件。TechCrunch 文件的直接下载链接是 [http://samplecsvs.s3.amazonaws.com/TechCrunchcontinentalUSA.csv](http://samplecsvs.s3.amazonaws.com/TechCrunchcontinentalUSA.csv)。您可以使用以下
    `wget` 命令下载此文件：
- en: '[PRE4]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, let''s go ahead and write our first piece of Python code to read the CSV
    file. We read the CSV file and print the first five rows:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们开始编写我们的第一段 Python 代码来读取 CSV 文件。我们读取 CSV 文件并打印前五行：
- en: '[PRE5]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In the preceding code example, we read the first five records of the CSV file:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们读取了 CSV 文件的前五条记录：
- en: '![How to do it...](img/image_10_002.jpg)'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做到这一点...](img/image_10_002.jpg)'
- en: 'The `pandas` module reads the file''s contents and converts them to a data
    frame of rows and columns. Now, if you look at the output of the preceding code,
    you will notice that an index column gets added to the file contents. With `pandas`,
    it''s easy to parse the date, tell if the dates in our CSV file have the date
    first or month first (UK or US format), and make the date column as the index
    column:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`pandas` 模块读取文件内容并将其转换为行和列的数据框。现在，如果您查看前面代码的输出，您会注意到文件内容中添加了一个索引列。使用 `pandas`，很容易解析日期，判断我们
    CSV 文件中的日期是先月后日（英国或美国格式），并将日期列设置为索引列：'
- en: '[PRE6]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If you run the preceding code snippet, you should be able to see the index
    column, **fundedDate**, as shown in the following screenshot:'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你运行前面的代码片段，你应该能够看到索引列 **fundedDate**，如下面的屏幕截图所示：
- en: '![How to do it...](img/image_10_003.jpg)'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/image_10_003.jpg)'
- en: 'Neat! Now, we''re able to read the data, but how about selecting some data
    so that we can perform some analysis on top of it. Let''s select the column that
    depicts the amount of funding raised by the companies (the **raisedAmt** column):'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 真不错！现在，我们能够读取数据了，但如何选择一些数据以便我们可以在其上进行一些分析呢。让我们选择描述公司筹集资金金额的列（**raisedAmt** 列）：
- en: '[PRE7]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note that in the following screenshot, we have printed the top five records
    of the companies that have raised funding:'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，在下面的屏幕截图中，我们打印了获得资助的公司的前五条记录：
- en: '![How to do it...](img/image_10_004.jpg)'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/image_10_004.jpg)'
- en: 'OK, cool! So we can select the column of our choice and get the data we wanted
    to analyze. Let''s see if we can generate some nice visualizations for it. The
    following recipe generates a line chart for the funding rounds reported for all
    the years (*x* axis), based on the amount raised (*y* axis):'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 好的，酷！所以我们可以选择我们想要的列并获取我们想要分析的数据。让我们看看我们是否可以为它生成一些漂亮的可视化。下面的方法基于筹集的金额（*y* 轴）为所有年份报告的融资轮次生成一个折线图（*x*
    轴）：
- en: '[PRE8]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the following screenshot, see how the rate of funding (or the rate of reporting)
    increased, and with that, the amounts raised also saw a steady increase!
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在下面的屏幕截图中，看看资助率（或报告率）是如何增加的，随之，筹集的资金也稳步增加！
- en: '![How to do it...](img/image_10_005.jpg)'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/image_10_005.jpg)'
- en: 'Fantastic! I know you have already started to like what we''re doing here.
    Let''s move forward and see whether we can select multiple columns from the CSV
    file. In the following example, we get the data for 50 rows, with the column names
    being **company**, **category**, and **fundedDate**:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 太棒了！我知道你已经开始喜欢我们在这里做的事情了。让我们继续前进，看看我们是否可以从 CSV 文件中选择多个列。在下面的示例中，我们获取了 50 行数据，列名为
    **company**、**category** 和 **fundedDate**：
- en: '[PRE9]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output of the preceding code snippet is as follows:'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前一个代码片段的输出如下：
- en: '![How to do it...](img/image_10_006.jpg)'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/image_10_006.jpg)'
- en: 'OK, great! Now let''s select one of these columns and perform some analysis
    on top of it. In the following code example, we select the **category** column
    that gives us the categories of all the reported funding rounds. We then process
    the selected column to get the most common category of the company that got funded:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 好的，太好了！现在让我们选择这些列中的一个，并在此基础上进行一些分析。在下面的代码示例中，我们选择了 **category** 列，它给出了所有报告的融资轮次的类别。然后我们处理所选列以获取获得资助的公司最常见的类别：
- en: '[PRE10]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output of the preceding code snippet is:'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前一个代码片段的输出是：
- en: '[PRE11]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Data and numbers give a lot of information, but the impact can actually only
    be seen through visualizations. Let''s see whether we can plot the above data
    as a horizontal bar chart. The following recipe does the job for us:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据和数字提供了很多信息，但实际的影响实际上只能通过可视化来看到。让我们看看我们是否可以将上述数据绘制为水平条形图。下面的方法为我们完成了这项工作：
- en: '[PRE12]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'On the *y* axis, we have the category of the company that got funded, and the
    *x* axis is the total count of companies in a given category. Also, we save the
    plotted chart in a PDF file named `categoriesFunded.pdf`:'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 *y* 轴上，我们有获得资助的公司的类别，而 *x* 轴是给定类别中公司的总数。此外，我们将绘制的图表保存为名为 `categoriesFunded.pdf`
    的 PDF 文件：
- en: '![How to do it...](img/image_10_007.jpg)'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/image_10_007.jpg)'
- en: Whoa! So many web companies got funded? Awesome! I too should start a web company--it
    increases the chances of getting funded.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！这么多网络公司获得了资助？太棒了！我也应该开始创办一家网络公司——这增加了获得资助的机会。
- en: How it works...
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this section we dealt with the two major aspects of data analysis. First,
    we covered how to read the dataset from a CSV file and select the appropriate
    data (rows or columns) from our dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们处理了数据分析的两个主要方面。首先，我们介绍了如何从 CSV 文件中读取数据集并从我们的数据集中选择适当的数据（行或列）。
- en: In the first code snippet, we used the help of the `pandas` module to read the
    CSV file. In `pandas`, we have the `read_csv(csv_file)` method that takes the
    CSV file path as an argument. The `pandas` module reads the file and stores the
    file contents as data frames. DataFrame is a two-dimensional, labeled-in-memory
    data structure with columns of potentially different types. It mimics the structure
    of a spreadsheet or an SQL table or a dictionary of series objects. It has a nice
    set of methods and attributes to select, index, and filter data. In our first
    recipe, we read the CSV file and generated a DataFrame object `df`. Using `df`
    we selected the first five rows of our CSV file with `df[:5]`. See how easy it
    becomes to select the rows of a CSV file with `pandas`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个代码片段中，我们使用 `pandas` 模块的帮助来读取 CSV 文件。在 `pandas` 中，我们有 `read_csv(csv_file)`
    方法，它接受 CSV 文件路径作为参数。`pandas` 模块读取文件并将文件内容存储为数据框。数据框是一个二维、内存中标记的数据结构，具有可能不同类型的列。它模仿了电子表格、SQL
    表或系列对象字典的结构。它有一套很好的方法和属性来选择、索引和过滤数据。在我们的第一个食谱中，我们读取了 CSV 文件并生成了一个 DataFrame 对象
    `df`。使用 `df` 我们选择了 CSV 文件的头五行，使用 `df[:5]`。看看使用 `pandas` 选择 CSV 文件行是多么简单。
- en: We can do a few more things with the `read_csv()` method. By default, `pandas`
    adds another index column to our dataset, but we can specify which column from
    the CSV file should be used for indexing our data. We achieved this by passing
    the `index_col` parameter to the `read_csv()` method. We also converted the string
    dates present in the **fundedDate** column of the CSV file to a datetime format
    with the `parse_dates` parameter and said that the date is in a format where the
    day is the first part of the date with the `dayfirst` parameter.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `read_csv()` 方法做更多的事情。默认情况下，`pandas` 会为我们的数据集添加另一个索引列，但我们可以指定 CSV 文件中的哪一列应该用于索引我们的数据。我们通过将
    `index_col` 参数传递给 `read_csv()` 方法实现了这一点。我们还使用 `parse_dates` 参数将 CSV 文件中 **fundedDate**
    列的字符串日期转换为 datetime 格式，并通过 `dayfirst` 参数指定日期格式，其中日期的第一部分是日期。
- en: After getting the DataFrame and using **fundedDate** as index, we used `df['raisedAmt'][:5]`
    to select the **raisedAmt** column and print the first five rows. We then used
    the `seaborn` library to set the style of our plot with `sns.set_style("darkgrid")`
    and generated the bar chart with the `plot()` method. The `seaborn` library is
    used for generating nice visualizations and is implemented on `matplotlib`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在获取 DataFrame 并使用 **fundedDate** 作为索引后，我们使用 `df['raisedAmt'][:5]` 来选择 **raisedAmt**
    列并打印前五行。然后我们使用 `seaborn` 库通过 `sns.set_style("darkgrid")` 设置了我们的绘图风格，并使用 `plot()`
    方法生成了条形图。`seaborn` 库用于生成漂亮的可视化效果，并基于 `matplotlib` 实现。
- en: Using the `matplotlib` library, we created an object, `plt`, which was then
    used to label our chart with the `ylabel()` and `xlabel()` methods. The `plt`
    object was also used to finally store the resulting chart in the PDF format with
    the `savefig()` method.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `matplotlib` 库，我们创建了一个对象 `plt`，然后使用 `ylabel()` 和 `xlabel()` 方法来标记我们的图表。`plt`
    对象还用于最终使用 `savefig()` 方法将生成的图表保存为 PDF 格式。
- en: In the second example, we selected multiple columns with `fundings[['company',
    'category' and 'fundedDate']]`. We selected three columns from a CSV file in one
    line of code. We then plotted a horizontal bar chart with the `plot()` method
    and specified the type of chart with `kind=barh`. Finally, we made use of the
    `matplotlib` library to label the *x* axis with the `xlabel()` method and saved
    the chart with the `savefig()` method. As you can see, we didn't have to use the
    `seaborn` library to plot the chart; we could simply do it with `matplotlib`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个示例中，我们使用 `fundings[['company', 'category' and 'fundedDate']]` 选择了多个列。我们一行代码中从
    CSV 文件中选择了三个列。然后我们使用 `plot()` 方法绘制了一个水平条形图，并使用 `kind=barh` 指定了图表类型。最后，我们使用 `xlabel()`
    方法通过 `matplotlib` 库标记了 *x* 轴，并使用 `savefig()` 方法保存了图表。正如你所看到的，我们不需要使用 `seaborn`
    库来绘制图表；我们可以简单地使用 `matplotlib` 来完成。
- en: Generating insights using data filtering and aggregation
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用数据过滤和聚合生成洞察
- en: Reading CSV files and selecting multiple columns is easy with `pandas`. In this
    section, we will take a look at how to slice and dice data, essentially filtering data
    with `pandas`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `pandas` 读取 CSV 文件并选择多个列非常简单。在本节中，我们将探讨如何切片和切块数据，本质上就是使用 `pandas` 过滤数据。
- en: Getting ready
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In this section, we will use the same set of libraries (the following ones)
    that we used in the previous recipe:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用与之前食谱中相同的库集合（以下列出的库）：
- en: '`pandas` for filtering and data analysis'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas` 用于过滤和分析数据'
- en: '`matplotlib` and `seaborn` for plotting charts and saving the data in a PDF
    file'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`matplotlib`和`seaborn`来绘制图表并将数据保存到PDF文件中
- en: How to do it...
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s start by importing the required libraries and reading the CSV file with
    the `read_csv()` method. The following code carries out the operations:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从导入所需的库和读取CSV文件使用`read_csv()`方法开始。以下代码执行了这些操作：
- en: '[PRE13]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, let''s filter on the data frame and use multiple columns to filter data.
    Say we filter funding records on the funding category, state, and the selected
    city in the state. This can be achieved with the following piece of code:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们在数据框上应用筛选，并使用多个列来筛选数据。比如说，我们根据融资类别、州和州内选定的城市来筛选融资记录。这可以通过以下代码片段实现：
- en: '[PRE14]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The preceding piece of code returns all the funding records in the State of
    California (CA) for the cities Palo Alto, San Francisco, San Mateo, Los Angeles,
    and Redwood City for the web companies. The following is a partial screenshot
    of the output:'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码返回了加州（CA）州帕洛阿尔托、旧金山、圣马特奥、洛杉矶和雷德伍德城的所有网络公司的所有融资记录。以下是一个输出截图的部分：
- en: '![How to do it...](img/image_10_008.jpg)'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/image_10_008.jpg)'
- en: 'OK cool, now let''s see if we can get the count of funding for the companies
    in the web category by city names. The following code will get us the details
    we need:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 好极了，现在让我们看看我们是否可以通过城市名称来获取网络类别公司的融资次数。以下代码将为我们提供所需详细信息：
- en: '[PRE15]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output of the preceding piece of code is the number of funding rounds received
    by the companies in the **web** category in the selected cities:'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码的输出是选定城市中**网络**类别公司收到的融资轮次数量：
- en: '![How to do it...](img/image_10_009.jpg)'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/image_10_009.jpg)'
- en: Wow! The preceding analysis was quite useful; you got to know that the web companies
    in San Francisco city have received the funding 195 times (from the data we have).
    Looks like if you are a web company in San Francisco, all your funding worries
    are over. Well, that sounds logical and simple.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 哇！前面的分析非常有用；你了解到旧金山的网络公司已经收到了195次（根据我们的数据）的融资。看起来如果你是旧金山的网络公司，你所有的融资担忧都已经结束了。嗯，这听起来合乎逻辑且简单。
- en: 'But wait, isn''t this information incomplete? How about gathering data for
    companies in all categories including **web** and then representing the data for
    the companies in the **web** category as a percentage of all the categories? This
    way, we will know whether you should have your company in ''San Francisco'' or
    in any other city. OK then, let''s get the count of funding rounds for companies
    in all categories (including **web**), for all the selected cities in CA. The
    following code will get us the information we need:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 但是等等，这个信息不是不完整吗？我们为什么不收集所有类别（包括**网络**）公司的数据，然后以所有类别的百分比来表示**网络**类别中的公司数据呢？这样我们就可以知道你是否应该将你的公司在“旧金山”或其他任何城市。好的，那么让我们统计一下CA州所有类别（包括**网络**）中所有选定城市的融资轮次。以下代码将为我们提供所需的信息：
- en: '[PRE16]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here is the output of the preceding code snippet:'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是上述代码片段的输出：
- en: '![How to do it...](img/image_10_010.jpg)'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/image_10_010.jpg)'
- en: 'Nice! Now, let''s get the data for the companies in the **web** category as
    a percentage of companies in all the categories for the selected cities of CA.
    We can get this data by simply dividing the data for the **web** categories by
    all the categories and then multiplying by 100 to represent it as a percentage.
    The following code snippet will help us in this case:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 太好了！现在，让我们获取CA州选定城市中**网络**类别公司占所有类别公司的百分比数据。我们可以通过简单地将**网络**类别的数据除以所有类别并乘以100来表示为百分比。以下代码片段将帮助我们完成这项工作：
- en: '[PRE17]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, let''s plot this data as a horizontal bar chart with the following code:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们用以下代码将此数据绘制成水平条形图：
- en: '[PRE18]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following screenshot helps us compare the funding rounds for web companies
    with respect to the funding rounds for companies in all other categories and for
    the cities in California:'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下截图帮助我们比较了网络公司与所有其他类别公司和加州各城市融资轮次的情况：
- en: '![How to do it...](img/image_10_011.jpg)'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/image_10_011.jpg)'
- en: After the analysis what did you figure? Do you still want to set up your company
    in San Francisco? If you are a web company in Los Angeles, even though the funding
    rounds are limited, there is a higher chance (0.925) of your company getting funded
    than when you're in San Francisco (0.855), at least from the data points we have.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分析之后，你有什么发现？你仍然想在旧金山设立公司吗？如果你是洛杉矶的互联网公司，尽管资金轮次有限，但你获得资金的机会（0.925）比在旧金山（0.855）要高，至少从我们的数据点来看。
- en: 'Now, let''s take a look at another example. Say we want to analyze our data
    to see what months have historically received more funding than others. Additionally,
    can we also relate this with the rounds of funding [such as series A or angel
    funding]? Interesting thought! But how do we get there? The `pandas` module has
    support for grouping the data and data aggregation, which will come to our rescue
    for this analysis. Let''s solve this problem step by step:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看另一个例子。假设我们想分析我们的数据，看看哪些月份历史上比其他月份筹集的资金更多。此外，我们还能否将此与资金轮次（如A轮或天使投资）联系起来？这是一个有趣的思考！但我们如何做到这一点？`pandas`模块支持对数据进行分组和聚合，这将帮助我们进行这项分析。让我们一步一步解决这个问题：
- en: 'First, let''s read the CSV file and select two columns: **raisedAmt** and **rounds**.
    Let''s also add another column, `month`, as the index column to this data frame.
    The following code gets the data frame ready for further analysis:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们读取CSV文件并选择两列：**筹集金额**和**轮次**。我们还将添加另一个列，`month`作为数据框的索引列。以下代码为后续分析准备好了数据框：
- en: '[PRE19]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now we need to get the data for the funds raised based on the month. The following
    code does exactly what we need:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要根据月份获取筹集资金的数据。以下代码正好做了我们需要的：
- en: '[PRE20]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now, if we plot the data for this analysis, we will see how the funding fluctuates
    on a monthly basis for all the years of data we have:![How to do it...](img/image_10_012.jpg)
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，如果我们绘制用于此分析的数据，我们将看到我们拥有的所有年份数据中，资金在每月是如何波动的：![如何做...](img/image_10_012.jpg)
- en: Cool, looks like it's better to ask for funding in January. Maybe, the investors
    are in a good mood after the Christmas and New Year vacation; what do you think?
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 很酷，看起来一月份申请资金会更好。也许，投资者在圣诞节和新年假期之后心情不错；你怎么看？
- en: 'Now, if you want to analyze and build a correlation between the month of year,
    amount raised, and the round of funding, we can get the data with the following
    piece of code:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，如果你想分析和建立年度月份、筹集金额和资金轮次之间的相关性，我们可以用以下代码片段获取数据：
- en: '[PRE21]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output of the preceding code snippet is a data frame arranged as shown
    in the following screenshot. The data is grouped by **month** and **round** columns
    of funding, and **raisedAmt** is aggregated accordingly:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前一段代码片段的输出是一个按照以下截图所示的数据框。数据按照资金**月份**和**轮次**列进行分组，并且**筹集金额**相应地汇总：
- en: '![How to do it...](img/image_10_013.jpg)'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/image_10_013.jpg)'
- en: How it works...
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: For the first problem, we load the data from the CSV file as a data frame with
    the `read_csv()` method. Then, we filter the data based on multiple factors, where
    the state is `CA` and the company category is `web` and the cities are `Palo Alto`,
    `San Francisco`, `San Mateo`, `Los Angeles`, and `Redwood City`. Filtering is
    a column-based operation and is pretty straight forward; it gets you the relevant
    data frame after applying the criteria.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个问题，我们使用`read_csv()`方法将数据从CSV文件加载为数据框。然后，我们根据多个因素过滤数据，其中州是`CA`，公司类别是`web`，城市是`Palo
    Alto`、`San Francisco`、`San Mateo`、`Los Angeles`和`Redwood City`。过滤是一个基于列的操作，相当直接；它应用了标准后，会得到相关的数据框。
- en: Then, we calculated the count of funding rounds, grouped by cities for the **web**
    companies with the `value_counts()` method. We did the same exercise for the funding
    rounds for companies in all the categories, including **web**.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`value_counts()`方法按城市计算了**网络**公司资金轮次的数量。我们也对所有类别公司的资金轮次进行了同样的练习，包括**网络**。
- en: Finally, we simply divided the data and got the data for the **web** companies
    as a percentage of the data for all the categories. The `pandas` module handled
    this operation for us seamlessly. It used both the data points for the same city
    for analysis without us even worrying about it.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们简单地划分了数据，并得到了作为所有类别数据百分比的**网络**公司数据。`pandas`模块为我们无缝地处理了这个操作。它使用了相同城市的分析数据点，甚至我们都不用担心这一点。
- en: Finally, we plotted the horizontal bar chart with the `plot()` method, depicted
    the percentages for each city individually, and got the insight we were looking
    for.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second example, we first got the data frame by selecting multiple columns:
    **raisedAmt** and **round**. We also added a new column to the **month** DataFrame
    and treated it as an index column.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Then, we grouped the data based on **month** with the help of the **groupby()**
    method. We then summed up the funding amounts to get the amount of funds raised
    based on **month**. To get the total funds, we used the **aggregation()** method
    and added the data points to get the required information.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Also, to build the correlation between the funds raised with respect to **month**
    and **round**, we grouped the data frame by **month** and **round** and again
    applied the aggregation on **raisedAmt**.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding two recipes, we learned about data analysis and visualization,
    and we extensively used the `pandas` Python module in our recipes. The `pandas` module
    is a very comprehensive library and has capabilities such as working with time
    series, advanced indexing techniques, merging and concatenating objects, and working
    with different datasets (JSON and Excel), amongst others. I highly encourage you
    to go through the `pandas` API Reference to learn more about this awesome library.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: In the next recipe, let's see whether we can apply the knowledge we have gained
    so far in this chapter by helping Judy automate her task.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Automating social media analysis for businesses
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Judy is a columnist at a leading magazine in London. As a writer, she is always
    interested in recent topics. She collects data and analyzes it to come up with
    insights that would be interesting to her readers.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Currently, Judy is interested in the battle between Apple's iPhone and Samsung's
    Note and wants to publish an article in her magazine. She plans to collect the
    data by talking to people on the streets and reading blog posts, but she knows
    she will get an awful lot of information from social media. She is aware of the
    fact that people take to Twitter these days to express their pleasure or disappointment
    about using a product and also refer products to their friends on social media.
    However, she is worried about the fact that she has to go through this huge volume
    of social media data for her article.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: You are a data scientist and Judy's colleague. Can you help Judy with her needs?
    It is an opportunity for you to show off your data skills!
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin by analyzing Judy's problems. To begin with, Judy needs to collect
    data from an ever-growing social media platforms such as Twitter. Secondly, she
    needs to analyze this data to generate interesting insights. So, we should be
    able to build a system that caters to both her problems. Also, you may want to
    build a system that only solves her current needs, but she should be able to use
    it for any projects in the future.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s install all the modules that we''ll need to work on this problem. We
    already have `pandas`, `matplotlib`, and `seaborn` installed. For this problem,
    we will also install `tweepy`, a module to work with Twitter data. Let''s install
    tweepy with our very own `python-pip`:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: How to do it...
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OK nice, we're now armed with all the modules. So, let's get started by collecting
    data from Twitter. Twitter has this amazing set, Streaming APIs, which helps developers
    collect tweets in real time. We will use this library for our data collection
    needs too.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code uses the Twitter Streaming APIs to collect data and store
    it in a text file with each tweet stored in the JSON format. We look for tweets
    that have two keywords, `iPhone 7` and `Note 5`. For this chapter, I ran the code
    for around 5 minutes, but for Judy we may have to run it for hours, or even days,
    to collect the maximum data to generate accurate insights:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Okay, now that we have the data from Twitter flowing in, let's write a code
    to analyze this data and see whether we can find any interesting stuff that can
    be shared with Judy for her article.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Apple iPhone and Samsung Note are such popular products on a global level that
    people talk about these products from all around the world. It''d be really interesting
    to find the different languages used by consumers to talk about these products
    on Twitter. The following code does exactly what we wanted to do with Twitter
    data. It goes through the stored tweets, figures out the languages of all the
    tweets, and groups them to plot the top four languages:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: If we run the preceding code snippet, it plots the bar chart for the top languages
    people have used to tweet about iPhone 7 and Note 5.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_014.jpg)'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Awesome! I think Judy will love this analysis. Even though the top language
    is English (**en**), as expected, it is quite interesting to see that the other
    three languages are Italian (**it**), Spanish (**es**), and Portuguese (**pt**).
    This will be a cool feed for her article.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The results that you will get from this exercise will depend on when you run
    the data collection program. For instance, if you run it for a short duration
    between 2 and 8 a.m. GMT time, you will see more tweets in Chinese or Japanese
    as it is daytime in these countries. By the way, wouldn't it be interesting to
    analyze what are the best times of the day when people tweet? You may find some
    correlation.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s go further and do some more cool stuff with this data. How about performing
    text-based analysis on the tweets to get consumer sentiments about these products?
    By sentiment, I mean, is a tweet cursing these products, appreciating a product''s
    feature, or just a passing comment? But wait; is it possible to get this kind
    of data? Yes, absolutely. The following code uses Python''s NTLK-based APIs to
    perform sentiment analysis on tweets (text) to determine its polarity: positive,
    negative or neutral sentiment. It then groups this data to represent it with a
    bar chart and saves it in a PDF file:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'If you run the preceding piece of code, you will get a bar chart with the sentiment
    data for all the stored tweets. Just by looking at the graph, you will know that
    the consumers have generally spoken positively about both products:'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_015.jpg)'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: There are a few complaints about them, hence the sentiments with negative polarity,
    but then there are neutral comments that could just be product referrals or comments
    on the products. Neat!
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: While this is nice, don't you think Judy might be interested to know how many
    of these negative tweets are about the iPhone or Note? Well, I will leave that
    to you; I'm sure you will figure that out for Judy.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the first problem, we first collected the data required for analysis. Twitter's
    Streaming APIs set helps us collect this information in real time. To use Streaming
    APIs, you need to register for a developer app with Twitter and collect your consumer
    key, consumer secret, auth token, and auth secret. It is a straightforward process
    and can be easily looked up on [https://dev.twitter.com](https://dev.twitter.com).
    So, in the data collection example (the first example in this recipe), we instantiated
    the `OAuthHandler` class to get Twitter's authorization object, `auth`, and then
    used it to set the authorization token and secret with the `set_access_token()`
    method. The `Stream` class of Twitter's Streaming APIs is bound to the `listener`
    class and returns the `twitterStream` object. The `listener` class inherits the
    `StreamListener` class, which will monitor incoming tweets and take actions on
    the arriving tweets in the `on_data()` method. The filtering of tweets is done
    with the `twitterStream.filter()` method.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Now, we know that the incoming tweets are available in the `on_data()` method;
    we hooked on to it to store the tweets in the `twitter_data.txt` file. For this,
    we opened the file in the write (`w`) mode and used the `write()` method to write
    the tweet to the file in the JSON format. With this, we finished the first recipe
    and collected the data required by Judy. Now, it's time to perform the analysis.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: For the first insight on getting the languages, we started by opening the `twitter_data.txt`
    file in the read(`r`) mode. We read through all the tweets (the JSON format) and
    appended them to the `tweets` array. Using `pandas`, we created an empty DataFrame
    object, `tweet_df`, with `pd.DataFrame()`. With Python's `map()` method, we operated
    on the `tweets` array and added a new column, `lang`, to our empty DataFrame.
    The `value_counts()` method was then used to get the count of all the languages
    of the tweets under analysis and was stored in the variable, `tweets_by_lang`.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: The other part of the code is as usual, where we created a `plt` object from
    matplotlib and used the `plot()` method to generate a bar chart using the `seaborn`
    library. We set the axis labels with the `set_xlabel()` and `set_ylabel()` methods
    and used the colors `green`, `blue`, `red`, and `black` to manifest the different
    languages. Finally, we saved the plot in a PDF file with the `savefig()` method.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: For the second insight involving sentiment analysis, we started by reading through
    all the tweets from `twitter_data.txt` and storing them to the `tweets` array.
    We then created an empty data frame, probabilities, processed all the tweets for
    sentiment analysis, and stored the analysis in the `prob` array. We then added
    a column, `text`, to our empty data frame using the `map()` method on the `prob`
    array.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Our data frame, `probabilities['text']`, now contains sentiments for all the
    tweets we analyzed. Following the regular set of operations, we got the set of
    values for `positive`, `negative`, and `neutral` sentiments for the analyzed tweets
    and plotted them as a bar chart.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: If you look at all the examples in this recipe, we have divided the task of
    data collection and analysis as separate programs. If our visualizations are massive,
    we can even separate them out. This makes sure that Judy can use the data collection
    Python program to gather information on another set of keywords for her articles
    in the future.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: She can also run the analysis on the data sets she has by making small changes
    to the analysis and visualization parameters from our program. So, for all her
    articles in the future, we have managed to automate the data collection, analysis,
    and visualization process for her.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: I'm already seeing a smile on Judy's face. I hope you enjoyed this chapter.
    I'm confident that the knowledge you gained in this chapter will get you started
    into the world of data and visualizations.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have just scratched the surface of text-based sentiment analysis in the preceding
    recipe. Sentiment analysis involves text classifications, tokenization, semantic
    reasoning, and much more interesting stuff. I highly encourage you to go through
    a book on NLTK to learn more about working with text in Python at [http://www.nltk.org/book/](http://www.nltk.org/book/).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
