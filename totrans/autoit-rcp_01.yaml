- en: Chapter 1. Working with the Web
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 1 章：使用网络
- en: Can you image a life without the Internet? For almost everything, right from
    exchanging information to ordering food, we rely heavily on the Internet today.
    Let's go through the interesting world of the World Wide Web and cover numerous
    ways with which we can interact with it using Python modules.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 你能想象一个没有互联网的生活吗？对于几乎所有的事情，从交换信息到订购食物，我们今天都严重依赖互联网。让我们深入了解万维网的世界，并探讨我们如何使用 Python
    模块与之交互的多种方式。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下食谱：
- en: Making HTTP requests
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发送 HTTP 请求
- en: A brief look at web scraping
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简要了解网络爬虫
- en: Parsing and extracting web content
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析和提取网页内容
- en: Downloading content from the Web
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从网络下载内容
- en: Working with third-party REST APIs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用第三方 REST API 进行工作
- en: Asynchronous HTTP server in Python
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 中的异步 HTTP 服务器
- en: Web automation with selenium bindings
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 selenium 绑定进行网络自动化
- en: Automating lead generation with web scraping
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用网络爬虫自动化生成潜在客户
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Internet has made life so easy that sometimes you just don't realize the power
    of it. Checking out your friend's status, calling your parents, responding to
    an important business e-mail, or playing a game--we rely on the **World Wide Web**
    (**WWW**) today for almost everything.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网使生活变得如此简单，有时您甚至没有意识到它的力量。查看朋友的动态，给父母打电话，回复重要的商务电子邮件，或玩游戏——我们今天几乎在所有事情上都依赖于**万维网**（**WWW**）。
- en: Thankfully, Python has a rich set of modules that help us perform various tasks
    on the Web. Phew! Not only could you make simple HTTP requests retrieve data from
    websites or download pages and images, you could also parse the page content to
    gather information and analyze it to generate meaningful insights with Python.
    And wait; did I mention that you could spawn a browser in an automated fashion
    to perform a daily mundane task?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Python 拥有一套丰富的模块，帮助我们执行网络上的各种任务。哇！您不仅可以发送简单的 HTTP 请求从网站检索数据或下载页面和图片，还可以解析页面内容以收集信息，并使用
    Python 进行分析以生成有意义的见解。等等；我提到过您可以通过自动化方式启动浏览器来执行日常例行任务吗？
- en: 'The recipes in this chapter will primarily focus on the Python modules that
    can be treated as the tool of choice while performing the preceding operations
    on the Web. Specifically, we will focus on the following Python modules in this
    chapter:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的食谱将主要关注在执行上述网络操作时可以作为首选工具的 Python 模块。具体来说，在本章中，我们将重点关注以下 Python 模块：
- en: '`requests` ([http://docs.python-requests.org/en/master/](http://docs.python-requests.org/en/master/))'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`requests` ([http://docs.python-requests.org/en/master/](http://docs.python-requests.org/en/master/))'
- en: '`urllib2` ([https://docs.python.org/2/library/urllib2.html](https://docs.python.org/2/library/urllib2.html))'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`urllib2` ([https://docs.python.org/2/library/urllib2.html](https://docs.python.org/2/library/urllib2.html))'
- en: '`lxml` ([https://pypi.python.org/pypi/lxml](https://pypi.python.org/pypi/lxml))'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lxml` ([https://pypi.python.org/pypi/lxml](https://pypi.python.org/pypi/lxml))'
- en: '`BeautifulSoup4` ([https://pypi.python.org/pypi/beautifulsoup4](https://pypi.python.org/pypi/beautifulsoup4))'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BeautifulSoup4` ([https://pypi.python.org/pypi/beautifulsoup4](https://pypi.python.org/pypi/beautifulsoup4))'
- en: '`selenium` ([http://selenium-python.readthedocs.org/](http://selenium-python.readthedocs.org/))'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`selenium` ([http://selenium-python.readthedocs.org/](http://selenium-python.readthedocs.org/))'
- en: Note
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: While the recipes in this chapter will give you an overview of how to interact
    with the Web using Python modules, I encourage you to try out and develop code
    for multiple use cases, which will benefit you as an individual and your project
    on an organizational scale.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本章的食谱将为您概述如何使用 Python 模块与网络交互，但我鼓励您尝试并开发适用于多种用例的代码，这将使您个人以及您的项目在组织规模上受益。
- en: Making HTTP requests
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发送 HTTP 请求
- en: Throughout the following recipes in this chapter, we will use Python v2.7 and
    the `requests` (v2.9.1) module of Python. This recipe will show you how to make
    HTTP requests to web pages on the Internet.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章接下来的食谱中，我们将使用 Python v2.7 和 Python 的 `requests` (v2.9.1) 模块。本食谱将向您展示如何向互联网上的网页发送
    HTTP 请求。
- en: But before going there, let's understand the **Hypertext Transfer Protocol **(**HTTP**)
    in brief. HTTP is a stateless application protocol for data communication on the
    WWW. A typical HTTP Session involves a sequence of request or response transactions.
    The client initiates a **TCP** connection to the Server on a dedicated IP and
    Port; when the Server receives the request, it responds with the response code
    and text. HTTP defines request methods (HTTP verbs like `GET`, `POST`), which indicate
    the desired action to be taken on the given Web URL.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 但在深入之前，让我们简要了解**超文本传输协议**（**HTTP**）。HTTP是一种无状态的应用协议，用于在WWW上进行数据通信。典型的HTTP会话涉及一系列请求或响应事务。客户端在专用的IP和端口上向服务器发起**TCP**连接；当服务器收到请求时，它会以响应代码和文本进行响应。HTTP定义了请求方法（如`GET`、`POST`等HTTP动词），这些方法指示对给定Web
    URL要采取的操作。
- en: In this recipe, we'll learn how to make HTTP `GET`/`POST` requests using Python's
    `requests` module. We'll also learn how to POST `json` data and handle HTTP exceptions.
    Cool, let's jump in.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将学习如何使用Python的`requests`模块进行HTTP `GET`/`POST`请求。我们还将学习如何POST `json`数据并处理HTTP异常。太酷了，让我们开始吧。
- en: Getting ready
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To step through this recipe, you will need to install Python v2.7\. Once installed,
    you will need to install Python `pip`. **PIP** stands for **Pip Installs Packages**
    and is a program that can be used to download and install the required Python
    packages on your computer. Lastly, we'll need the `requests` module to make HTTP
    requests.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要逐步完成这个菜谱，你需要安装Python v2.7。安装后，你需要安装Python `pip`。**PIP**代表**Pip Installs Packages**，是一个可以用于在计算机上下载和安装所需Python包的程序。最后，我们需要`requests`模块来发送HTTP请求。
- en: We will start by installing the `requests` module (I'll leave the Python and
    `pip` installation for you to perform on your machine, based on your operating
    system). No other prerequisites are required. So, hurry up and let's get going!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先安装`requests`模块（我将把Python和`pip`的安装留给你根据你的操作系统在你的机器上执行）。不需要其他先决条件。所以，快点，让我们开始吧！
- en: How to do it...
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'On your Linux/Mac computer, go to Terminal and run the following command:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的Linux/Mac计算机上，转到终端并运行以下命令：
- en: '[PRE0]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You only need to use `sudo` if you don't have permissions to Python site packages,
    else `sudo` is not required.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 只有在没有权限访问Python站点包时，你才需要使用`sudo`，否则不需要`sudo`。
- en: 'The following code helps you make a HTTP `GET` request with Python''s `requests`
    module:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码帮助你使用Python的`requests`模块进行HTTP `GET`请求：
- en: '[PRE1]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You will observe the following output:![How to do it...](img/B05370_01_01.jpg)
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将看到以下输出：![如何操作...](img/B05370_01_01.jpg)
- en: 'Creating a HTTP `GET` request with data payload is also trivial with requests.
    The following code helps you in achieving this. This is how you can also check
    the URL request that will be sent:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用requests创建带有数据负载的HTTP `GET`请求也很简单。以下代码帮助你实现这一点。这就是你如何检查将要发送的URL请求：
- en: '[PRE2]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![How to do it...](img/image_01_002.jpg)'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/image_01_002.jpg)'
- en: 'Let''s now make a HTTP `POST` call using the `requests` module. This is similar
    to filling up and posting a login or signup form on a website:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们使用`requests`模块进行HTTP `POST`调用。这类似于在网站上填写并提交登录或注册表单：
- en: '[PRE3]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![How to do it...](img/image_01_003.jpg)'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/image_01_003.jpg)'
- en: 'Handling errors and exceptions is also very convenient with requests. The following code
    snippet shows an example of error handling. If you run this code without an Internet
    connection on your machine, it will result in an exception. The exception handler
    catches the exception and states that it failed to establish a new connection,
    as expected:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用requests处理错误和异常也非常方便。以下代码片段展示了错误处理的示例。如果你在没有网络连接的情况下运行此代码，它将引发异常。异常处理程序捕获异常并指出它未能建立新的连接，正如预期的那样：
- en: '[PRE4]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: How it works...
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'In the this recipe, we looked at how to make different types of HTTP requests
    with Python''s `requests` module. Let''s look at how this code works:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们探讨了如何使用Python的`requests`模块进行不同类型的HTTP请求。让我们看看这段代码是如何工作的：
- en: In the first example, we made a `GET` request to [http://ip.jsontest.com](http://ip.jsontest.com) 
    and got the response code and response text. It returns the current IP address
    of our computer on the Internet.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一个示例中，我们向[http://ip.jsontest.com](http://ip.jsontest.com) 发起了`GET`请求并获得了响应代码和响应文本。它返回我们计算机在互联网上的当前IP地址。
- en: In the second example, we made a HTTP `GET` request with the payload data. Look
    how the request URL contains `?q=chetan`, and it searches all the repositories
    by the name, Chetan, on GitHub.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we made a `POST` request with the payload data being `{'key1', 'value1'}`.
    This is like submitting an online form, as we observed in the *How to do it* section.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `requests` module has a `Response` object, `r`, which includes various
    methods. These methods help in extracting response, status code and other information
    required while working with the Web:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r.status_code` - Returns the response code'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r.json()` - Converts the response to `.json` format'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r.text` - Returns the response data for the query'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r.content` - Includes the HTML and XML tags in the response content'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r.url` - Defines the Web URL of the request made'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We also looked at the exception handling with the `requests` module, wherein,
    if there was no Internet, an exception occurred and the `requests` module could
    easily catch this exception. This was achieved with the `requests.exceptions`
    class of the `requests` module.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cool, that was neat! Making HTTP requests on the Web is just the beginning.
    There's still more in terms of what we can do with the Web, such as working with
    page contents. So, let's see what's next.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: A brief look at web scraping
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we learn how to perform **web scraping**, let's understand what scraping
    means. In the Web world, scraping is a way to sift through the pages of a website
    with the intention of extracting the required information in the said format with
    the help of a computer program. For example, if I want to get the title and date
    of all the articles published on a blog, I could write a program to scrape through
    the blog, get the required data, and store it in a database or a flat file, based
    on the requirement.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping is often confused with web crawling. The **web crawler** is a bot
    that systematically browses the Web with the purpose of web indexing and is used
    by search engines to index web pages so that users can search the Web more effectively.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: But scraping is not easy. The data, which is interesting to us, is available
    on a blog or website in a particular format, say XML tags or embedded in HTML
    tags. So, it is important for us to know the format before we begin extracting
    the data we need. Also, the web scraper should know the format in which the extracted
    data needs to be stored in order to act on it later. It is also important to understand
    that the scraping code will fail should the HTML or XML format change, even though
    the browser display may be the same.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Legality of web scraping
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Web scraping has always been under the scanner in legal terms. Can you do web
    scraping? How legal or ethical is it? Can we use the data obtained from scraping
    for profit?
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: This subject has been under a lot of discussion, but at a high level, you may
    get into issues with web scraping if you scrape the Web for copyright information,
    violate the Computer Fraud and Abuse Act, or violate a website's terms of service.
    For instance, if you're scraping the Web to get public data, you should still
    be fine. However, it is very contextual and you need to be careful about what
    you're scraping and how you are using the data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few pointers on the Web on data scraping:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Web_scraping#Legal_issues](https://en.wikipedia.org/wiki/Web_scraping#Legal_issues)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.quora.com/What-is-the-legality-of-web-scraping](https://www.quora.com/What-is-the-legality-of-web-scraping)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We take an example of pricing data from the [https://github.com/](https://github.com/)
    website to demonstrate web scraping with Python. This is a really trivial example
    but gets us up to speed with scraping. Let's get started and scrape some interesting
    data with this Python recipe.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Open the Google Chrome browser on your computer and open the [https://github.com/pricing/](https://github.com/pricing/) web
    page. On this page, you will notice multiple pricing plans namely, **Personal**,
    **Organization**, and **Enterprise**.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, on your browser, right-click on the pricing of the **Personal** plan and
    click on the **Inspect** element, as shown in the following screenshot:![How to
    do it...](img/image_01_004.jpg)
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you click on **Inspect**, the Chrome browser's console log opens up, which
    will help you understand the HTML structure of GitHub's pricing page, as follows:![How
    to do it...](img/image_01_005.jpg)
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you look at the highlighted HTML `span` - `<span class="default-currency">$7</span>`,
    you'll know that this web page uses the `default-currency` class to list down
    the pricing of plans. We'll now use this property to extract the prices of multiple
    GitHub plans.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'But before doing that, let''s install the Python module, `lxml`, which will
    be needed to extract content from the preceding HTML document. Install the `lxml`
    and `requests` modules:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, open your favorite editor and type this code snippet:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If you look at the preceding code, we used the `default-currency` class and
    `pricing-card-name display-heading-3` to get the pricing and pricing plan. If
    you run the code snippet, the output of the program will be as follows:![How to
    do it...](img/image_01_006.jpg)
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: With web scrapping you will see issues when the HTML tags for the web content
    has changed. For instance, if a CSS class name gets changed or an anchor is replaced
    with a button, the scraping code may not fetch the data you need. So, make sure
    you change your Python code accordingly.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed earlier, we need to find out an appropriate way of extracting
    information. So, in this example, we first got the HTML tree for the [https://github.com/pricing/](https://github.com/pricing/)
    page. We got the tree with the help of the `fromstring()` method that converts
    the contents of the page (string format) to the HTML format.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Then, using the `lxml` module and the `tree_xpath()` method, we looked for the `default-currency` class
    and `pricing-card-name display-heading-3` to get the pricing and pricing plans.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: See how we used the complete XPath, `h3[@class='class-name']`, to locate the
    pricing plans and the  `//span[@class="default-currency"]` XPath to select the
    actual pricing data. Once the elements were selected, we printed the text data
    that was returned to us as a Python list.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: That's it; we scraped the GitHub page for the required data. Nice and simple.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You learnt what web scrapers are, and how they go ahead and extract interesting
    information from the Web. You also understood how they are different from web
    crawlers. But then, there's always something more!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping involves extraction, which cannot happen until we parse the HTML
    content from the web page to get the data interesting to us. In the next recipe,
    we'll learn about parsing HTML and XML content in detail.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Parsing and extracting web content
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well, now we're confident about making HTTP requests to multiple URLs. We also
    looked at a simple example of web scraping.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: But WWW is made up of pages with multiple data formats. If we want to scrape
    the Web and make sense of the data, we should also know how to parse different
    formats in which data is available on the Web.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll discuss how to s.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data on the Web is mostly in the HTML or XML format. To understand how to parse
    web content, we'll take an example of an HTML file. We'll learn how to select
    certain HTML elements and extract the desired data. For this recipe, you need
    to install the `BeautifulSoup` module of Python. The `BeautifulSoup` module is
    one of the most comprehensive Python modules that will do a good job of parsing
    HTML content. So, let's get started.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start by installing `BeautifulSoup` on our Python instance. The following command
    will help us install the module. We install the latest version, which is `beautifulsoup4`:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, let''s take a look at the following HTML file, which will help us learn
    how to parse the HTML content:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s name this file as `python.html`. Our HTML file is hand-crafted so that
    we can learn the multiple ways of parsing it to get the required data from it.
    `Python.html` has typical HTML tags given as follows:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`<head>` - It is the container of all head elements like `<title>`.'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<body>` - It defines the body of the HTML document.'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<p>` - This element defines a paragraph in HTML.'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<span>` - It is used to group inline elements in a document.'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<strong>` - It is used to apply a bold style to the text present under this
    tag.'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<a>` - It represents a hyperlink or anchor and contains `<href>` that points
    to the hyperlink.'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<class>` - It is an attribute that points to a class in a style sheet.'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<div id>` - It is a container that encapsulates other page elements and divides
    the content into sections. Every section can be identified by attribute `id`.'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If we open this HTML in a browser, this is how it'll look:![How to do it...](img/image_01_007.jpg)
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's now write some Python code to parse this HTML file. We start by creating
    a `BeautifulSoup` object.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: We always need to define the parser. In this case we used `lxml` as the parser.
    The parser helps us read files in a designated format so that querying data becomes
    easy.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output of the preceding code is seen in the following screenshot:'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_01_008.jpg)'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: OK, that's neat, but how do we retrieve data? Before we try to retrieve data,
    we need to select the HTML elements that contain the data we need.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can select or find HTML elements in different ways. We could select elements
    with ID, CSS, or tags. The following code uses `python.html` to demonstrate this
    concept:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output of the preceding code can be viewed in the following screenshot:'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_01_009.jpg)'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Now let''s move on and get the actual content from the HTML file. The following are
    a few ways in which we can extract the data of interest:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output of the preceding code snippet is as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_01_010.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
- en: Whoopie! See how we got all the text we wanted from the HTML elements.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, you learnt the skill of finding or selecting different HTML
    elements based on ID, CSS, or tags.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: In the second code example of this recipe, we used `find_all(**'**a**'**)` to
    get all the anchor elements from the HTML file. When we used the `find_all()`
    method, we got multiple instances of the match as an array. The `select()` method
    helps you reach the element directly.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: We also used `find(**'**div**'**, <divId>)` or `select(<divId>)` to select HTML
    elements by `div Id`. Note how we selected the `inventor` element with `div` ID `#inventor`
    in two ways using the `find()` and `select()` methods. Actually, the select method
    can also be used as `select(<class**-**name>)` to select HTML elements with a
    CSS class name. We used this method to select element `wow` in our example.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: In the third code example, we searched for all the anchor elements in the HTML page
    and looked at the first index with `soup.find_all(**'**a**'**)[0]`. Note that
    since we have only one anchor tag, we used the index 0 to select that element,
    but if we had multiple anchor tags, it could be accessed with index 1\. Methods
    like `getText()` and attributes like `text` (as seen in the preceding examples)
    help in extracting the actual content from the elements.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cool, so we understood how to parse a web page (or an HTML page) with Python.
    You also learnt how to select or find HTML elements by ID, CSS, or tags. We also
    looked at examples of how to extract the required content from HTML. What if we
    want to download the contents of a page or file from the Web? Let's see if we
    can achieve that in our next recipe.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Downloading content from the Web
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, in the earlier recipe, we saw how to make HTTP requests, and you also learnt
    how to parse a web response. It's time to move ahead and download content from
    the Web. You know that the WWW is not just about HTML pages. It contains other
    resources, such as text files, documents, and images, among many other formats.
    Here, in this recipe, you'll learn ways to download images in Python with an example.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To download images, we will need two Python modules, namely `BeautifulSoup`
    and `urllib2`. We could use the `requests` module instead of `urrlib2`, but this
    will help you learn about `urllib2` as an alternative that can be used for HTTP
    requests, so you can boast about it.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before starting this recipe, we need to answer two questions. What kind of
    images would we like to download? From which location on the Web do I download
    the images? In this recipe, we download *Avatar *movie images from Google ([https://google.com](https://google.com))
    images search. We download the top five images that match the search criteria.
    For doing this, let''s import the Python modules and define the variables we''ll
    need:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'OK then, let''s now create a `BeautifulSoup` object with URL parameters and
    appropriate headers. See the use of `User-Agent` while making HTTP calls with
    Python''s `urllib` module. The `requests` module uses its own `User-Agent` while
    making `HTTP` calls:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Google images are hosted as static content under the domain name `http://www.gstatic.com/`.
    So, using the `BeautifulSoup` object, we now try to find all the images whose
    source URL contains `http://www.gstatic.com/`. The following code does exactly
    the same thing:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output of the preceding code snippet can be seen in the following screenshot.
    Note how we get the image source URL on the Web for the top five images:'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_01_011.jpg)'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Now that we have the source URL of all the images, let''s download them. The
    following Python code uses the `urlopen()` method to `read()` the image and downloads
    it onto the local file system:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: When the images get downloaded, we can see them on our editor. The following
    snapshot shows the top five images we downloaded and `Project_3.jpg` looks as
    follows:![How to do it...](img/B05370_01_35.jpg)
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, in this recipe, we looked at downloading content from the Web. First, we
    defined the parameters for download. Parameters are like configurations that define
    the location where the downloadable resource is available and what kind of content
    is to be downloaded. In our example, we defined that we have to download *Avatar*
    movie images and, that too, from **Google**.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Then we created the `BeautifulSoup` object, which will make the URL request
    using the `urllib2` module. Actually, `urllib2.Request()` prepares the request
    with the configuration, such as headers and the URL itself, and `urllib2.urlopen()`
    actually makes the request. We wrapped the HTML response of the `urlopen()` method
    and created a `BeautifulSoup` object so that we could parse the HTML response.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Next, we used the soup object to search for the top five images present in the
    HTML response. We searched for images based on the `img` tag with the `find_all()`
    method. As we know, `find_all()` returns a list of image URLs where the picture
    is available on **Google**.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we iterated through all the URLs and again used the `urlopen()` method
    on URLs to `read()` the images. `Read()` returns the image in a raw format as
    binary data. We then used this raw image to write to a file on our local file
    system. We also added a logic to name the image (they actually auto-increment)
    so that they're uniquely identified in the local file system.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: That's nice! Exactly what we wanted to achieve! Now let's up the ante a bit
    and see what else we can explore in the next recipe.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Working with third-party REST APIs
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've covered ground on scraping, crawling, and parsing, it's time
    for another interesting work that we can do with Python, which is working with
    third-party APIs. I'd assume many of us are aware and might have a basic understanding
    of **REST API**. So, let's get started!
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To demonstrate the understanding, we take the case of GitHub gists. Gists in
    GitHub are the best way to share your work, a small code snippet that helps your
    colleague or a small app with multiple files that gives an understanding of a
    concept. GitHub allows the creation, listing, deleting, and updating of gists,
    and it presents a classical case of working with GitHub REST APIs.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: So, in this section, we use our very own `requests` module to make HTTP requests
    to GitHub REST API to create, update, list, or delete gists.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The following steps will show you how to work with GitHub REST APIs using Python.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To work with GitHub REST APIs, we need to create a **Personal access token**.
    For doing that, log in to [https://github.com/](https://github.com/) and browse
    to [https://github.com/settings/tokens](https://github.com/settings/tokens)  and
    click on **Generate new  token**:![How to do it...](img/image_01_024.jpg)
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You'll now be taken to the **New personal access token** page. Enter a description
    at the top of the page and check the **gists** option among the scopes given out.
    Note that scope represents the access for your token. For instance, if you just
    select **gists**, you can use GitHub APIs to work on the **gists** resource but
    not on other resources such as **repo** or users. For this recipe, the **gists**
    scope is just what we need:![How to do it...](img/image_01_025.jpg)
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you click on **Generate token**, you'd be presented with a screen containing
    your personal access token. Keep this token confidential with you.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With the access token available, let''s start working with APIs and create
    a new gist. With create, we add a new resource, and for doing this, we make an
    HTTP `POST` request on GitHub APIs, such as in the following code:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: If I now go to my `gists` page on GitHub, I should see the newly created gist.
    And voila, it's available!![How to do it...](img/image_01_026.jpg)
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hey, we were successful in creating the gist with the GitHub APIs. That''s
    cool, but can we now view this `gist`? In the preceding example, we also printed
    the URL of the newly created gist. It will be in the format,`https://gist.github.com/<username>/<gist_id>`.
    We now use this **gist_id** to get the details of the gist, which means we make
    a HTTP `GET` request on the **gist_id**:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We created a new gist with the HTTP `POST` request and got the details of the
    gist with the HTTP `GET` request in the previous steps. Now, let's update this
    gist with the HTTP `PATCH` request.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: Many third-party libraries choose to use the `PUT` request to update a resource,
    but HTTP `PATCH` can also be used for this operation, as chosen by GitHub.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following code demonstrates updating the gist:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now, if I look at my GitHub login and browse to this gist, the contents of the
    gist have been updated. Awesome! Don't forget to see the **Revisions** in the
    screenshot--see it got updated to revision **2**:![How to do it...](img/image_01_027.jpg)
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now comes the most destructive API operation--yes deleting the gist. GitHub
    provides an API for removing the gist by making use of the HTTP **`` `DELETE`
    ``** operation on its `/gists/<gist_id>` resource. The following code helps us
    delete the `gist`:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Let's quickly find out if the gist is now available on the GitHub website? We
    can do that by browsing the gist URL on any web browser. And what does the browser
    say? It says **404** resource not found, so we have successfully deleted the gist!
    Refer to the following screenshot:![How to do it...](img/image_01_028.jpg)
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, let''s list all the gists in your account. For this we make an HTTP
    `GET` API call on the `/users/<username>/gists` resource:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output of the preceding code for my account is as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_01_029.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: How it works...
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python's `requests` module helps in making HTTP `GET`/`POST`/`PUT`/`PATCH` and
    `DELETE` API calls on GitHub's resources. These operations, also known as HTTP
    verbs in the REST terminology, are responsible for taking certain actions on the
    URL resources.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in the examples, the HTTP `GET` request helps in listing the gists,
    `POST` creates a new gist, `PATCH` updates a gist, and `DELETE` completely removes
    the gist. Thus, in this recipe, you learnt how to work with third-party REST APIs--an
    essential part of WWW today--using Python.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many third-party applications that are written as REST APIs. You may
    want to try them out the same way we did for GitHub. For example, both Twitter
    and Facebook have great APIs and the documents are also easy to understand and
    use. Of course, they do have Python bindings.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous HTTP server in Python
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you realize, many web applications that we interact with, are by default
    synchronous. A client connection gets established for every request made by the
    client and a callable method gets invoked on the server side. The server performs
    the business operation and writes the response body to the client socket. Once
    the response is exhausted, the client connection gets closed. All these operations
    happen in sequence one after the other--hence, synchronous.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: But the Web today, as we see it, cannot rely on synchronous modes of operations
    only. Consider the case of a website that queries data from the Web and retrieves
    the information for you. (For instance, your website allows for integration with
    **Facebook** and every time a user visits a certain page of your website, you
    pull data from his **Facebook** account.) Now, if we develop this web application
    in a synchronous manner, for every request made by the client, the server would
    make an I/O call to either the database or over the network to retrieve information
    and then present it back to the client. If these I/O requests take a longer time
    to respond, the server gets blocked waiting for the response. Typically web servers
    maintain a thread pool that handles multiple requests from the client. If a server
    waits long enough to serve requests, the thread pool may get exhausted soon and
    the server will get stalled.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Solution? In comes the asynchronous ways of doing things!
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we will use Tornado, an asynchronous framework developed in
    Python. It has support for both Python 2 and Python 3 and was originally developed
    at FriendFeed ([http://blog.friendfeed.com/](http://blog.friendfeed.com/)). Tornado
    uses a non-blocking network I/O and solves the problem of scaling to tens of thousands
    of live connections (`C10K problem`). I like this framework and enjoy developing
    code with it. I hope you''d too! Before we get into the *How to do it* section,
    let''s first install tornado by executing the following command:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: How to do it...
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''re now ready to develop our own HTTP server that works on an asynchronous
    philosophy. The following code represents an asynchronous server developed in
    the `tornado` web framework:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Run the server as:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The server is now running on port 8888 and ready to receive requests.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, launch any browser of your choice and browse to `http://localhost:8888/`.
    On the server, you'll see the following output:![How to do it...](img/image_01_034.jpg)
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our asynchronous web server is now up and running and accepting requests on
    port 8888\. But what is asynchronous about this? In fact, tornado works on the
    philosophy of a single-threaded event loop. This event loop keeps polling for
    events and passes it on to the corresponding event handlers.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, when the app is run, it starts by running the `ioloop`.
    The `ioloop` is a single-threaded event loop and is responsible for receiving
    requests from the clients. We have defined the `get()` method, which is decorated
    with `@tornado.web.asynchronous`, which makes it asynchronous. When a user makes
    a HTTP GET request on `http://localhost:8888/`, the `get()` method is triggered
    that internally makes an I/O call to [http://ip.jsontest.com](http://ip.jsontest.com).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Now, a typical synchronous web server would wait for the response of this I/O
    call and block the request thread. But tornado being an asynchronous framework,
    it triggers a task, adds it to a queue, makes the I/O call, and returns the thread
    of execution back to the event loop.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: The event loop now keeps monitoring the task queue and polls for a response
    from the I/O call. When the event is available, it executes the event handler,
    `async*_*callback()`, to print the content and its response and then stops the
    event loop.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Event-driven web servers such as tornado make use of kernel-level libraries
    to monitor for events. These libraries are `kqueue`, `epoll`, and so on. If you''re
    really interested, you should do more reading on this. Here are a few resources:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[https://linux.die.net/man/4/epoll](https://linux.die.net/man/4/epoll)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.freebsd.org/cgi/man.cgi?query=kqueue&sektion=2](https://www.freebsd.org/cgi/man.cgi?query=kqueue&sektion=2)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web automation with selenium bindings
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In all the recipes so far, we had a dedicated URL to make HTTP requests, be
    it calling a REST API or downloading content from the Web. But then, there are
    services that don't have a defined API resource or need to log in to the Web to
    perform operations. In such cases, you don't have much control over the requests,
    as it is the same URL that serves multiple different content, based on the user
    session or cookie. Then what do we do?
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Well, how about controlling the browser itself to achieve tasks in such scenarios?
    Controlling the browser itself? Interesting, isn't it?
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we'll use Python's selenium module. Selenium ([http://www.seleniumhq.org/](http://www.seleniumhq.org/))
    is a portable software framework for web applications and automates browser actions.
    You could automate mundane tasks with selenium. Selenium spawns a browser and
    helps you perform tasks as though a human is doing them. Selenium supports some
    of the most popularly used browsers like Firefox, Chrome, Safari, and Internet
    Explorer, among others. Let's take an example of logging in to Facebook with Python's
    selenium in this recipe.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start by installing selenium bindings for Python. Installing selenium can
    be done with the following command:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let''s start by first creating a browser object. We use the Firefox browser
    for spawning the browser instance:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The following screenshot shows how a selenium web driver object got created.
    It also has a unique session ID:![How to do it...](img/image_01_035.jpg)
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we ask the browser to browse to the Facebook home page. The following code
    helps us achieve this:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Once you run the preceding code, you will see a Firefox browser opened, and
    it connects to the Facebook login page, as in the following screenshot:![How to
    do it...](img/image_01_036.jpg)
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the next step, we locate the e-mail and password elements and enter the
    appropriate data:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_01_037.jpg)'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Once we have selected the **Email** and **Password** text inputs, we now fill
    them with the correct **Email** and **Password**. The following code will enable
    entering **Email** and **Password**:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![How to do it...](img/image_01_038.jpg)'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Now that we have entered **Email** and **Password**, the last thing to do is
    submit the form and click on the **Log In** button. We do this by finding the
    element by ID and clicking on the element:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: If you have entered the correct e-mail ID and password, you'd have logged in
    to Facebook!
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we used the selenium WebDriver Python APIs. **WebDrive**r is
    the latest inclusion in selenium APIs and drives browsers natively like a user. It
    can drive locally or on a remote machine using the selenium server. In this example,
    we ran it on the local machine. Basically, the selenium server runs on a local
    machine on a default port 4444 and selenium WebDriver APIs interact with the selenium
    server to take actions on the browser.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we first created a WebDriver instance using the Firefox browser.
    We then used the WebDriver API to browse to the Facebook homepage. We then parsed
    the HTML page and located the **Email** and **Password** input elements. How did
    we find the elements? Yes, similar to what we did in the web scraping example.
    As we have the developer console in Chrome, we can install the firebug plugin
    in Firefox. Using this plugin, we can get the HTML elements for **Email** and
    **Password**. See the following screenshot:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/image_01_039.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: Once we figured the HTML element names, we programmatically created an HTML element
    object using WebDriver's  `find_element_by_name()` method. WebDriver API has a
    method `send_keys()` that can work on element objects and enter the required text
    (in this case `email` and `password`). The last operation is to submit the form,
    and we performed it by finding the **Log In** object and clicking on it.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We looked at a very basic example with the selenium WebDriver Python bindings.
    Now it's up to your imagination what you can achieve with selenium, automating mundane
    tasks.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Automating lead generation with web scraping
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Ryan* is a marketing manager at *Dely In*c. Dely is a food delivery start-up
    and is trying to establish itself in the city of London. Dely is good at logistics
    and wants to aggregate restaurants on their platform, so when consumers order
    food from these restaurants, Dely will be responsible for the actual delivery.
    Dely is hoping that with every delivery they do, they will get a percentage cut
    from the restaurants. In return, restaurants have to think about their kitchen
    and not the logistical aspects. If you carefully think, virtually, every restaurant,
    big or small, is their probable lead. Dely wants to reach out to these restaurants
    and hopes to add them to their platform and fulfill their delivery needs.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Ryan is responsible for getting in touch with restaurants and wants to run a
    marketing campaign on all the target restaurants. But before he can do this, he
    needs to create a database of all the restaurants in London. He needs details,
    such as the name of the restaurant, the street address, and the contact number
    so that he can reach these restaurants. Ryan knows all his leads are listed on
    Yelp, but doesn't know where to start. Also, if he starts looking at all restaurants
    manually, it will take him a huge amount of time. With the knowledge you gained
    in this chapter, can you help Ryan with lead generation?
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Legality of web scraping
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We covered the legal aspects of web scraping in the initial parts of the chapter.
    I would like to warn you again on this. The example covered in this chapter, again,
    is for you to understand how to perform web scraping. Also, here we're scraping
    Yelp for public data, which is commonly available, as in this case, it is available
    on the restaurant's website itself.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, if you look at Ryan's problem, he needs an automated way of collecting
    the database of all the restaurants listed in London. Yes, you got it right. Web
    scraping can help Ryan build this database. Can it be that easy? Let's see in
    this recipe.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: For this recipe, we don't need any extra modules. We'll use the `BeautifulSoup `and
    `urllib` Python modules that we used in the previous recipes of this chapter.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We start by going to the Yelp website ([https://yelp.com/](https://yelp.com/))
    and searching for all the restaurants in the city of London. When you do that,
    you'll get a list of all the restaurants in London. Observe the URL that displays
    the search criteria. It is [https://www.yelp.com/search?find_desc=Restaurants&find_loc=London](https://www.yelp.com/search?find_desc=Restaurants&find_loc=London).
    See the following screenshot for reference:![How to do it...](img/image_01_040.jpg)
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, if you click on any of the restaurants' link that shows up in the search
    results, we should get the details that Ryan needs. See the following screenshot,
    where we get the details of *Ffiona's Restaurant*. Note how every restaurant has
    a dedicated URL; in this case, it is [https://www.yelp.com/biz/ffionas-restaurant-london?osq=Restaurants](https://www.yelp.com/biz/ffionas-restaurant-london?osq=Restaurants).
    Also note that on this page, we have the name of the restaurant, the street address,
    and even the contact number. All the details that Ryan needs for his campaign;
    that's cool!![How to do it...](img/image_01_041.jpg)
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OK nice, so we now know how to get the list of restaurants and also fetch the
    relevant details for a restaurant. But how do we achieve this in an automated
    way? As we saw in the web scraping example, we need to look for the HTML elements
    on the web pages from where we can collect this data.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's start with the search page. Open the search page ([https://www.yelp.com/search?find_desc=Restaurants&find_loc=London](https://www.yelp.com/search?find_desc=Restaurants&find_loc=London))
    on your Chrome browser. Now, right-click on the first restaurant's URL and click
    on **Inspect** to get the HTML elements. If you notice, in the following screenshot,
    all the restaurants that are listed on the search page have a common CSS class
    name, `biz-name`, which indicates the name of the restaurant. It also contains
    the `href` tag, which points to the dedicated URL of the restaurant. In our screenshot,
    we get the name, **Ffiona's Restaurant**, and the `href` points to the restaurant's
    URL, [https://yelp.com/biz/ffionas-restaurant-london?osq=Resturants](https://yelp.com/biz/ffionas-restaurant-london?osq=Resturants).![How
    to do it...](img/image_01_042.jpg)
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's look at the dedicated page of the restaurant to see how we collect
    the street address and the contact number of the restaurant with the HTML elements.
    We perform the same operation, right-click, and **Inspect **to get the HTML elements
    of street address and contact number. See the following screenshot for reference.
    Note that for the street address, we have a separate CSS class, `street-address`,
    and the contact number is available under a span with the class name, **biz-phone**.![How
    to do it...](img/image_01_043.jpg)
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Awesome! So, we now have all the HTML elements that can be used to scrape the
    data in an automated way. Let''s now look at the implementation. The following Python
    code performs these operations in an automated way:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: OK, great! Now, if we run the preceding Python code, we get the details of the
    top 10 restaurants in *London,* along with their names, street addresses and contact
    numbers. Refer to the following screenshot:![How to do it...](img/image_01_044.jpg)
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the preceding screenshot, we get the records of 10 restaurants in London
    provided by Yelp. **Title** is the name of the restaurant and **Street Address**
    and **Phone Number** are self-explanatory. Awesome! We did it for Ryan.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding code snippet, we built the search criteria. We searched on
    [https://yelp.com](https://yelp.com) and looked for restaurants in *London*. With
    these details, we got the search URL on Yelp.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: We then created a `urllib` object and used the `urlopen()` method on this search
    URL to `read()` the list of all the restaurants provided by Yelp matching the
    search criteria. The list of all the restaurants is stored as an HTML page, which is
    stored in the variable, `s_html`.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Using the `BeautifulSoup` module, we created a soup instance on the HTML content
    so that we could start extracting the required data using the CSS elements.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Initially, we browsed the top 10 results of the search on Yelp and got the URLs
    of the restaurants. We stored these URLs in the URL Python list. To get the URL,
    we selected the CSS class name `biz-name` using the code `soup_s.select(.biz-name)[:10]`.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: We also defined a method, `scrape()`, which takes the restaurant URL as a parameter.
    In this method, we read the details of the restaurant, such as name, street address,
    and contact number, using the CSS class names `biz-page-title`, `street-address`,
    and `biz-phone`, respectively. To get the exact data, we selected the HTML elements
    using `title=soup.select`(`.biz-page-title`) and got the data with `title[0].getText().strip()`.
    Note that the `select()` method returns the found element as an array, so we need
    to look for index `0` to get the actual text.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: We iterated through all the restaurant URLs in a `while` loop and scraped the
    URL using the `scrape()` method to get the details for each restaurant. It prints
    the name, street address, and contact number for each restaurant on your console,
    as we saw in the preceding screenshot.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: To improve on the performance of our screaping program, we performed data extraction
    for every restaurant in an independent thread. We created a new thread with `t
    = Thread(target=scrape,args=(url[i],))` and got the results from each of them
    with the `t.join()` call.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: That’s it, folks! Ryan is extremely happy with this effort. In this example,
    we helped Ryan and automated a critical business task for him. Throughout this
    book we'll look at various use cases where Python can be leveraged to automate
    business processes and make them efficient. Interested in more? Well, see you
    in the next chapter.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
