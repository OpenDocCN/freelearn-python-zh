<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Design Techniques and Strategies</h1>
            </header>

            <article>
                
<p>In this chapter, we will take a step back and look into the broader topics in computer algorithm design. As your experience with programming grows, certain patterns begin to become apparent to you. And just like with any other skilled trade, you cannot do without some techniques and principles to achieve the means. In the world of algorithms, there are a plethora of these techniques and design principles. A working knowledge and mastery of these techniques is required to tackle harder problems in the field.</p>
<p>We will look at the ways in which algorithms are generally classified. Other design techniques will be treated alongside implementation of some of the algorithms.</p>
<p>The aim of this chapter is not to make you a pro at algorithm design and strategy but to unveil the large expanse of algorithms in a few pages.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Classification of algorithms</h1>
            </header>

            <article>
                
<p>There exist a number of classification schemes that are based on the goal that an algorithm has to achieve. In the previous chapters, we implemented a number of algorithms. One question that may arise is, do these algorithms share the same form? If yes, what are the similarities and characteristics being used as the basis? If no, can the algorithms be grouped into classes?</p>
<p>These are the questions we will examine as we tackle the major modes of classifying algorithms.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Classification by implementation</h1>
            </header>

            <article>
                
<p>When translating a series of steps or processes into a working algorithm, there are a number of forms that it may take. The heart of the algorithm may employ some assets, described further in this section.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Recursion</h1>
            </header>

            <article>
                
<p>Recursive algorithms are the ones that make calls to themselves until a certain condition is satisfied. Some problems are more easily expressed by implementing their solution through recursion. One classic example is the Towers of Hanoi. There are also different types of recursive algorithms, some of which include single and multiple recursion, indirect recursion, anonymous recursion, and generative recursion. An iterative algorithm, on the other hand, uses a series of steps or a repetitive construct to formulate a solution. This repetitive construct could be a simple <kbd>while</kbd> loop or any other kind of loop. Iterative solutions also come to mind more easily than their recursive implementations.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Logical</h1>
            </header>

            <article>
                
<p>One implementation of an algorithm is expressing it as a controlled logical deduction. This logic component is comprised of the axioms that will be used in the computation. The control component determines the manner in which deduction is applied to the axioms. This is expressed in the form a<em>lgorithm = logic + control</em>. This forms the basis of the logic programming paradigm.</p>
<p>The logic component determines the meaning of the algorithm. The control component only affects its efficiency. Without modifying the logic, the efficiency can be improved by improving the control component.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Serial or parallel</h1>
            </header>

            <article>
                
<p>The RAM model of most computers allows for the assumption that computing is done one instruction at a time.</p>
<p>Serial algorithms, also known as <strong>sequential algorithms</strong>, are algorithms that are executed sequentially. Execution commences from start to finish without any other execution procedure.</p>
<p>To be able to process several instructions at once, a different model or computing technique is required. Parallel algorithms perform more than one operation at a time. In the PRAM model, there are serial processors that share a global memory. The processors can also perform various arithmetic and logical operations in parallel. This enables the execution of several instructions at one time.</p>
<p>The parallel/distributed algorithms divide a problem into subproblems among its processors to collect the results. Some sorting algorithms can be efficiently parallelized, while iterative algorithms are generally parallelizable.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Deterministic versus nondeterministic algorithms</h1>
            </header>

            <article>
                
<p>Deterministic algorithms will produce the same output without fail every time the algorithm is run with the same input. There are some sets of problems that are so complex in the design of their solutions that expressing their solution in a deterministic way can be a challenge. Nondeterministic algorithms can change the order of execution or some internal subprocess that leads to a change in the final result any time the algorithm is run. As such, with every run of a nondeterministic algorithm, the output of the algorithm is different. For instance, an algorithm that makes use of a probabilistic value will yield different outputs on successive execution depending on the value of the random number generated.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Classification by complexity</h1>
            </header>

            <article>
                
<p>To determine the complexity of an algorithm is to try to estimate how much space (memory) and time is used overall during the computation or program execution.</p>
<div class="packt_tip"><a href="a98d7333-0a20-49b1-8bf1-3e007ddb9793.xhtml">Chapter 3</a>, <em>Principles of Algorithm Design</em>, presents more comprehensive coverage of the subject matter on complexity. We will summarize what we learned there here.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Complexity curves</h1>
            </header>

            <article>
                
<p>Now consider a problem of magnitude <em>n</em>. To determine the time complexity of an algorithm, we denote it with <strong>T</strong>(n). The value may fall under <strong>O</strong>(<em>1</em>), <strong>O</strong>(<em>log n</em>), <strong>O</strong>(<em>n</em>), <strong>O</strong>(<em>n log(n)</em>), <strong>O</strong>(<em>n<sup>2</sup></em>), <strong>O</strong>(<em>n<sup>3</sup></em>), or <strong>O</strong>(<em>2<sup>n</sup></em>). Depending on the steps an algorithm performs, the time complexity may or may not be affected. The notation <strong>O</strong>(<em>n</em>) captures the growth rate of an algorithm.</p>
<p>Let's now examine a practical scenario. By which means do we arrive at the conclusion that the bubble sort algorithm is slower than the quick sort algorithm? Or in general, how do we measure the efficiency of one algorithm against the other? Well, we can compare the Big O of any number of algorithms to determine their efficiency. It is this approach that gives us a time measure or the growth rate, which charts the behavior of the algorithm as <em>n</em> gets bigger.</p>
<p>Here is a graph of the different runtimes that an algorithm's performance may fall under:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border aligncenter" height="267" src="assets/image_12_001.jpg" width="445"/></div>
<p>In ascending order, the list of runtimes from better to worse is given as <strong>O</strong>(1), <strong>O</strong>(log n), <strong>O</strong>(<em>n</em>), <strong>O</strong>(<em>n log n</em>), <strong>O</strong>(<em>n<sup>2</sup></em>), <strong>O</strong>(<em>n<sup>3</sup></em>), and <strong>O</strong>(<em>2<sup>n</sup></em>).</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Classification by design</h1>
            </header>

            <article>
                
<p>In this section, we will present the categories of algorithms based on the design of the various algorithms used in solving problems.</p>
<p>A given problem may have a number of solutions. When the algorithms of these solutions are analyzed, it becomes evident that some implement a certain technique or pattern. It is these techniques that we will discuss here, and in a later section, in greater detail.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Divide and conquer</h1>
            </header>

            <article>
                
<p>This approach to problem-solving is just as its name suggests. To solve (conquer) certain problems, this algorithm divides the problem into subproblems identical to the original problem that can easily be solved. Solutions to the subproblems are combined in such a way that the final solution is the solution of the origin problem.</p>
<p>The way in which the problems are broken down into smaller chunks is mostly by recursion. We will examine this technique in detail in the upcoming sections. Some algorithms that use this technique include merge sort, quick sort, and binary search.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Dynamic programming</h1>
            </header>

            <article>
                
<p>This technique is similar to divide and conquer, in that a problem is broken down into smaller problems. In divide and conquer, each subproblem has to be solved before its results can be used to solve bigger problems. By contrast, dynamic programming does not compute the solution to an already encountered subproblem. Rather, it uses a remembering technique to avoid the recomputation.</p>
<p>Dynamic programming problems have two characteristics: <strong>optimal substructure</strong> and <strong>overlapping subproblem</strong>. We will talk more on this in the next section.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Greedy algorithms</h1>
            </header>

            <article>
                
<p>For a certain category of problems, determining the best solution is really difficult. To make up for the lack of optimal solution, we resort to an approach where we select out of a bunch of options or choices the closest solution that is the most promising in obtaining a solution.</p>
<p>Greedy algorithms are much easier to conceive because the guiding rule is for one to always select the solution that yields the most benefit and continue doing that, hoping to reach a perfect solution.</p>
<p>This technique aims to find a global optimal final solution by making a series of local optimal choices. The local optimal choice seems to lead to the solution. In real life, most of those local optimal choices made are suboptimal. As such, most greedy algorithms have a poor asymptotic time complexity.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Technical implementation</h1>
            </header>

            <article>
                
<p>Let's dig into the implementation of some of the theoretical programming techniques that we discussed previously in this chapter. We will start with dynamic programming.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Dynamic programming</h1>
            </header>

            <article>
                
<p>As we have already described, in this approach, we divide a problem into smaller subproblems. In finding the solutions to the subprograms, care is taken not to recompute any of the previously encountered subproblems.</p>
<p>This sounds a bit like recursion, but things are a little broader here. A problem may lend itself to being solved by using dynamic programming but will not necessarily take the form of making recursive calls.</p>
<p>A property of a problem that will make it an ideal candidate for being solved with dynamic programming is that it should have an overlapping set of subproblems.</p>
<p>Once we realize that the form of subproblems has repeated itself during computation, we need not compute it again. Instead, we return the result of a pre-computed value of that subproblem previously encountered.</p>
<p>To avoid a situation where we never have to re-evaluate a subproblem, we need an efficient way in which we can store the results of each subproblem. The following two techniques are readily available.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Memoization</h1>
            </header>

            <article>
                
<p>This technique starts from the initial problem set and divides it into small subproblems. After the solution to a subprogram has been determined, we store the result to that particular subproblem. In the future, when this subproblem is encountered, we only return its pre-computed result.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Tabulation</h1>
            </header>

            <article>
                
<p>In tabulation, we settle on an approach where we fill a table of solutions to subproblems and then combine them to solve bigger problems.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">The Fibonacci series</h1>
            </header>

            <article>
                
<p>We will use the Fibonacci series to illustrate both memoization and tabulation techniques of generating the series.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">The Memoization technique</h1>
            </header>

            <article>
                
<p>Let's generate the Fibonacci series to the fifth term:</p>
<pre>
    1 1 2 3 5
</pre>
<p>A recursive style of a program to generate the sequence is as follows:</p>
<pre>
    def fib(n): <br/>        if n &lt;= 2: <br/>            return 1 <br/>        else: <br/>            return fib(n-1) + fib(n-2) 
</pre>
<p>The code is very simple but a little tricky to read because of the recursive calls being made that end up solving the problem.</p>
<p>When the base case is met, the <kbd>fib()</kbd> function returns 1. If <kbd>n</kbd> is equal to or less than 2, the base case is met.</p>
<p>If the base case is not met, we will call the <kbd>fib()</kbd> function again and this time supply the first call with <kbd>n-1</kbd> and the second with <kbd>n-2</kbd>:</p>
<pre>
    return fib(n-1) + fib(n-2) 
</pre>
<p>The layout of the strategy to solve the i<sup>th</sup> term in the Fibonacci sequence is as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border aligncenter" height="177" src="assets/image_12_002.jpg" width="249"/></div>
<p>A careful observation of the preceding tree shows some interesting patterns. The call to <kbd>f(1)</kbd> happens twice. The call to <kbd>f(1)</kbd> happens thrice. Also, the call to <kbd>f(3)</kbd> happens twice.</p>
<p>The return values of the function calls to all the times that <kbd>fib(2)</kbd> was called never changes. The same goes for <kbd>fib(1)</kbd> and <kbd>fib(3)</kbd>. The computational time is wasted since the same result is returned for the function calls with the same parameters.</p>
<p>These repeated calls to a function with the same parameters and output suggest that there is an overlap. Certain computations are reoccurring down in the smaller subproblems.</p>
<p>A better approach would be to store the results of the computation of <kbd>fib(1)</kbd> the first time it is encountered. This also applies to <kbd>fib(2)</kbd> and <kbd>fib(3)</kbd>. Later, anytime we encounter a call to <kbd>fib(1)</kbd>, <kbd>fib(2)</kbd>, or <kbd>fib(3)</kbd>, we simply return their respective results.</p>
<p>The diagram of our <kbd>fib</kbd> calls will now look like this:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border aligncenter" height="208" src="assets/image_12_003.jpg" width="254"/></div>
<p>We have now completely eliminated the need to compute <kbd>fib(3)</kbd>, <kbd>fib(2)</kbd>, and <kbd>fib(1)</kbd>. This typifies the memoization technique wherein breaking a problem into its subproblems, there is no recomputation of overlapping calls to functions. The overlapping function calls in our Fibonacci example are <kbd>fib(1)</kbd>, <kbd>fib(2)</kbd>, and <kbd>fib(3)</kbd>:</p>
<pre>
    def dyna_fib(n, lookup): <br/>        if n &lt;= 2: <br/>            lookup[n] = 1 <br/><br/>        if lookup[n] is None: <br/>            lookup[n] = dyna_fib(n-1, lookup) + dyna_fib(n-2, lookup) <br/><br/>        return lookup[n] 
</pre>
<p>To create a list of 1,000 elements, we do the following and pass it to the lookup parameter of the <kbd>dyna_fib</kbd> function:</p>
<pre>
    map_set = [None]*(10000) 
</pre>
<p>This list will store the value of the computation of the various calls to the <kbd>dyna_fib()</kbd> function:</p>
<pre>
    if n &lt;= 2: <br/>        lookup[n] = 1 
</pre>
<p>Any call to the <kbd>dyna_fib()</kbd> with <kbd>n</kbd> being less than or equal to 2 will return 1. When <kbd>dyna_fib(1)</kbd> is evaluated, we store the value at index 1 of <kbd>map_set</kbd>:</p>
<p>Write the condition for <kbd>lookup[n]</kbd>, as the following:</p>
<pre>
if lookup[n] is None:<br/>    lookup[n] = dyna_fib(n-1, lookup) + dyna_fib(n-2, lookup)
</pre>
<p>We pass lookup so that it can be referenced when evaluating the subproblems. The calls to <kbd>dyna_fib(n-1, lookup)</kbd> and <kbd>dyna_fib(n-2, lookup)</kbd> are stored in <kbd>lookup[n]</kbd>. When we run our updated implementation of the function to find the i<sup>th</sup> term of the Fibonacci series, we realize that there is considerable improvement. This implementation runs much faster than our initial implementation. Supply the value 20 to both implementations and witness the difference in the execution speed. The algorithm sacrificed space complexity for time because of the use of memory in storing the result to the function calls.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">The tabulation technique</h1>
            </header>

            <article>
                
<p>There is a second technique in dynamic programming, which involves the use of a table of results or matrix in some cases to store results of computations for later use.</p>
<p>This approach solves the bigger problem by first working out a route to the final solution. In the case of the <kbd>fib()</kbd> function, we will develop a table with the values of <kbd>fib(1)</kbd> and <kbd>fib(2)</kbd> predetermined. Based on these two values, we will work our way up to <kbd>fib(n)</kbd>:</p>
<pre>
    def fib(n): <br/><br/>        results = [1, 1] <br/><br/>        for i in range(2, n): <br/>            results.append(results[i-1] + results[i-2]) <br/><br/>        return results[-1] 
</pre>
<p>The <kbd>results</kbd> variable is at index 0, and 1 the values, 1 and 1. This represents the return values of <kbd>fib(1)</kbd> and <kbd>fib(2)</kbd>. To calculate the values of the <kbd>fib()</kbd> function for higher than 2, we simply call the <kbd>for</kbd> loop appends the sum of the <kbd>results[i-1] + results[i-2]</kbd> to the list of results.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Divide and conquer</h1>
            </header>

            <article>
                
<p>This programming approach to problem-solving emphasizes the need to break down a problem into smaller problems of the same type or form of the original problem. These subproblems are solved and combined to solve the original problem. The following three steps are associated with this kind of programming.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Divide</h1>
            </header>

            <article>
                
<p>To divide means to break down an entity or problem. Here, we devise the means to break down the original problem into subproblems. We can achieve this through iterative or recursive calls.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Conquer</h1>
            </header>

            <article>
                
<p>It is impossible to continue to break the problems into subproblems indefinitely. At some point, the smallest indivisible problem will return a solution. Once this happens, we can reverse our thought process and say that if we know the solution to the smallest problem possible, we can obtain the final solution to the original problem.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Merge</h1>
            </header>

            <article>
                
<p>To obtain the final solution, we need to combine the smaller solutions to the smaller problems in order to solve the bigger problem.</p>
<p>There are other variants to the divide and conquer algorithm, such as merge and combine, and conquer and solve.</p>
<p>Algorithms that make use of the divide and conquer principle include merge sorting, quick sort, and Strassen's matrix multiplication. We will go through an implementation of the merge sort as we started earlier in <a href="a98d7333-0a20-49b1-8bf1-3e007ddb9793.xhtml" target="_blank">Chapter 3</a>, <em>Principles of Algorithm Design</em>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Merge sort</h1>
            </header>

            <article>
                
<p>The merge sort algorithm is based on the divide and conquer rule. Given a list of unsorted elements, we split the list into approximately two halves. We continue to divide the two halves recursively. After a while, the sublists created as a result of the recursive call will contain only one element. At that point, we begin to merge the solutions in the conquer or merge step:</p>
<pre>
    def merge_sort(unsorted_list): <br/>        if len(unsorted_list) == 1: <br/>            return unsorted_list <br/><br/>        mid_point = int((len(unsorted_list))/2) <br/><br/>        first_half = unsorted_list[:mid_point] <br/>        second_half = unsorted_list[mid_point:] <br/><br/>        half_a = merge_sort(first_half) <br/>        half_b = merge_sort(second_half) <br/><br/>        return merge(half_a, half_b) 
</pre>
<p>Our implementation starts by accepting the list of unsorted elements into the <kbd>merge_sort</kbd> function. The <kbd>if</kbd> statement is used to establish the base case, where if there is only one element in the <kbd>unsorted_list</kbd>, we simply return that list again. If there is more than one element in the list, we find the approximate middle using <kbd>mid_point = int((len(unsorted_list))/2)</kbd>.</p>
<p>Using this <kbd>mid_point</kbd>, we divide the list into two sublists, namely <kbd>first_half</kbd> and <kbd>second_half</kbd>:</p>
<pre>
    first_half = unsorted_list[:mid_point] <br/>    second_half = unsorted_list[mid_point:] 
</pre>
<p>A recursive call is made by passing the two sublists to the <kbd>merge_sort</kbd> function again:</p>
<pre>
    half_a = merge_sort(first_half)  <br/>    half_b = merge_sort(second_half) 
</pre>
<p>Enter the merge step. When <kbd>half_a</kbd> and <kbd>half_b</kbd> have been passed their values, we call the merge function that will merge or combine the two solutions stored in <kbd>half_a</kbd> and <kbd>half_b</kbd>, which are lists:</p>
<pre>
    def merge(first_sublist, second_sublist): <br/>        i = j = 0 <br/>        merged_list = [] <br/><br/>        while i &lt; len(first_sublist) and j &lt; len(second_sublist): <br/>            if first_sublist[i] &lt; second_sublist[j]: <br/>                merged_list.append(first_sublist[i]) <br/>                i += 1 <br/>            else: <br/>                merged_list.append(second_sublist[j]) <br/>                j += 1 <br/><br/>        while i &lt; len(first_sublist): <br/>            merged_list.append(first_sublist[i]) <br/>            i += 1 <br/><br/>        while j &lt; len(second_sublist): <br/>            merged_list.append(second_sublist[j]) <br/>            j += 1 <br/><br/>        return merged_list 
</pre>
<p>The merge function takes the two lists we want to merge together, <kbd>first_sublist</kbd> and <kbd>second_sublist</kbd>. The <kbd>i</kbd> and <kbd>j</kbd> variables are initialized to <kbd>0</kbd> and are used as pointers to tell us where in the two lists we are with respect to the merging process. The final <kbd>merged_list</kbd> will contain the merged list:</p>
<pre>
    while i &lt; len(first_sublist) and j &lt; len(second_sublist): <br/>        if first_sublist[i] &lt; second_sublist[j]: <br/>            merged_list.append(first_sublist[i]) <br/>            i += 1 <br/>        else: <br/>            merged_list.append(second_sublist[j]) <br/>            j += 1 
</pre>
<p>The <kbd>while</kbd> loop starts comparing the elements in <kbd>first_sublist</kbd> and <kbd>second_sublist</kbd>. The <kbd>if</kbd> statement selects the smaller of the two, <kbd>first_sublist[i]</kbd> or <kbd>second_sublist[j]</kbd>, and appends it to <kbd>merged_list</kbd>. The <kbd>i</kbd> or <kbd>j</kbd> index is incremented to reflect the point we are at with the merging step. The <kbd>while</kbd> loop only when either sublist is empty.</p>
<p>There may be elements left behind in either <kbd>first_sublist</kbd> or <kbd>second_sublist</kbd>. The last two <kbd>while</kbd> loops make sure that those elements are added to the <kbd>merged_list</kbd> before it is returned.</p>
<p>The last call to <kbd>merge(half_a, half_b)</kbd> will return the sorted list.</p>
<p>Let's give the algorithm a dry run by playing the last step of merging the two sublists <kbd>[4, 6, 8]</kbd> and <kbd>[5, 7, 11, 40]</kbd>:</p>
<table class="table">
<tbody>
<tr>
<td>
<p><strong>Step</strong></p>
</td>
<td>
<p><kbd>first_sublist</kbd></p>
</td>
<td>
<p><kbd>second_sublist</kbd></p>
</td>
<td>
<p><kbd>merged_list</kbd></p>
</td>
</tr>
<tr>
<td>
<p>Step 0</p>
</td>
<td>
<p>[<strong>4</strong> 6 8]</p>
</td>
<td>
<p>[<strong>5</strong> 7 11 40]</p>
</td>
<td>
<p>[]</p>
</td>
</tr>
<tr>
<td>
<p>Step 1</p>
</td>
<td>
<p>[ <strong>6</strong> 8]</p>
</td>
<td>
<p>[<strong>5</strong> 7 11 40]</p>
</td>
<td>
<p>[4]</p>
</td>
</tr>
<tr>
<td>
<p>Step 2</p>
</td>
<td>
<p>[ <strong>6</strong> 8]</p>
</td>
<td>
<p>[ <strong>7</strong> 11 40]</p>
</td>
<td>
<p>[4 5]</p>
</td>
</tr>
<tr>
<td>
<p>Step 3</p>
</td>
<td>
<p>[ <strong>8</strong>]</p>
</td>
<td>
<p>[ <strong>7</strong> 11 40]</p>
</td>
<td>
<p>[4 5 6]</p>
</td>
</tr>
<tr>
<td>
<p>Step 4</p>
</td>
<td>
<p>[ <strong>8</strong>]</p>
</td>
<td>
<p>[ <strong>11</strong> 40]</p>
</td>
<td>
<p>[4 5 6 7]</p>
</td>
</tr>
<tr>
<td>
<p>Step 5</p>
</td>
<td>
<p>[ ]</p>
</td>
<td>
<p>[ <strong>11</strong> 40]</p>
</td>
<td>
<p>[4 5 6 7 8]</p>
</td>
</tr>
</tbody>
</table>
<p>Note that the text in bold represents the current item referenced in the loops <kbd>first_sublist</kbd> (which uses the index <kbd>i</kbd>) and <kbd>second_sublist</kbd> (which uses the index <kbd>j</kbd>).</p>
<p>At this point in the execution, the third <kbd>while</kbd> loop in the merge function kicks in to move 11 and 40 into the <kbd>merged_list</kbd>. The returned <kbd>merged_list</kbd> will contain the fully sorted list.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Greedy algorithms</h1>
            </header>

            <article>
                
<p>As we said earlier, greedy algorithms make decisions that yield the largest benefit in the interim. It is the hope of this technique that by making these high yielding benefit choices, the total path will lead to an overall good solution or end.</p>
<p>Examples of greedy algorithms include <strong>Prim's algorithm</strong> for finding the minimum spanning trees, the <strong>Knapsack problem</strong>, and the <strong>Travelling Salesman problem</strong>, just to mention a few.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Coin-counting problem</h1>
            </header>

            <article>
                
<p>Let's examine a very simple use of this greedy technique. In some arbitrary country, we have the denominations 1 GHC, 5 GHC, and 8 GHC. Given an amount such as 12 GHC, we may want to find the least possible number of denominations needed to provide change. Using the greedy approach, we pick the largest value from our denomination to divide 12 GHC. We use 8 because it yields the best possible means by which we can reduce the amount 12 GHC into lower denominations.</p>
<p>The remainder, 4 GHC, cannot be divided by 5, so we try the 1 GHC denomination and realize that we can multiply it by 4 to obtain 4 GHC. At the end of the day, the least possible number of denominations to create 12 GHC is to get a one 8 GHC and four 1 GHC notes.</p>
<p>So far, our greedy algorithm seems to be doing pretty well. A function that returns the respective denominations is as follows:</p>
<pre>
    def basic_small_change(denom, total_amount): <br/>        sorted_denominations = sorted(denom, reverse=True) <br/><br/>        number_of_denoms = [] <br/><br/>        for i in sorted_denominations: <br/>            div = total_amount / i <br/>            total_amount = total_amount % i <br/>            if div &gt; 0: <br/>                number_of_denoms.append((i, div)) <br/><br/>        return number_of_denoms 
</pre>
<p>This greedy algorithm always starts by using the largest denomination possible. <kbd>denom</kbd> is a list of denominations. <kbd>sorted(denom, reverse=True)</kbd> will sort the list in reverse so that we can obtain the largest denomination at index 0. Now, starting from index 0 of the sorted list of denominations, <kbd>sorted_denominations</kbd>, we iterate and apply the greedy technique:</p>
<pre>
    for i in sorted_denominations: <br/>        div = total_amount / i <br/>        total_amount = total_amount % i <br/>        if div &gt; 0: <br/>            number_of_denoms.append((i, div)) 
</pre>
<p>The loop will run through the list of denominations. Each time the loop runs, it obtains the quotient, <kbd>div</kbd>, by dividing the <kbd>total_amount</kbd> by the current denomination, <kbd>i</kbd>. <kbd>total_amount</kbd> is updated to store the remainder for further processing. If the quotient is greater than 0, we store it in <kbd>number_of_denoms</kbd>.</p>
<p>Unfortunately, there are instances where our algorithm fails. For instance, when passed 14 GHS, our algorithm returns one 8 GHC and four 1 GHS. This output is, however, not the optimal solution. The right solution will be to use two 5 GHC and two 1 GHC denominations.</p>
<p>A better greedy algorithm is presented here. This time, the function returns a list of tuples that allow us to investigate the better results:</p>
<pre>
    def optimal_small_change(denom, total_amount): <br/><br/>        sorted_denominations = sorted(denom, reverse=True) <br/><br/>        series = [] <br/>        for j in range(len(sorted_denominations)): <br/>            term = sorted_denominations[j:] <br/><br/>            number_of_denoms = [] <br/>            local_total = total_amount <br/>            for i in term: <br/>                div = local_total / i <br/>                local_total = local_total % i <br/>                if div &gt; 0: <br/>                    number_of_denoms.append((i, div)) <br/><br/>            series.append(number_of_denoms) <br/><br/>        return series 
</pre>
<p>The outer <kbd>for</kbd> loop enables us to limit the denominations from which we find our solution:</p>
<pre>
    for j in range(len(sorted_denominations)): <br/>        term = sorted_denominations[j:] <br/>        ...     
</pre>
<p>Assuming that we have the list [5, 4, 3] in <kbd>sorted_denominations</kbd>, slicing it with <kbd>[j:]</kbd> helps us obtain the sublists [5, 4, 3], [4, 3], and [3], from which we try to get the right combination to create the change.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Dijkstra's shortest path algorithm</h1>
            </header>

            <article>
                
<p>We introduce and study Dijkstra's algorithm. This algorithm is an example of a greedy algorithm. It finds the shortest distance from a source to all other nodes or vertices in a graph. By the end of this section, you will come to understand why it is classified as a greedy algorithm.</p>
<p>Consider the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border aligncenter" height="164" src="assets/CH_12_01.png" width="240"/></div>
<p>By inspection, the first answer to the question of finding the shortest path between node <strong>A</strong> and node <strong>D</strong> that comes to mind is the edge with value or distance 9. From the diagram, it would seem that the straight path from node <strong>A</strong> to <strong>D</strong> would also yield the shortest route between the two nodes. But the assumption that the edge connecting the two nodes is the shortest route does not always hold true.</p>
<p>This shortsighted approach of selecting the first option when solving a problem is what gives the algorithm its name and class. Having found the supposed shortest route or distance, the algorithm continues to refine and improve its results.</p>
<p>Other paths from node <strong>A</strong> to node <strong>D</strong> prove to be shorter than our initial pick. For instance, travelling from node <strong>A</strong> to node <strong>B</strong> to node <strong>C</strong> will incur a total distance of 10. But the route through node <strong>A</strong> to <strong>E</strong>, <strong>F</strong>, and <strong>D</strong> is even shorter.</p>
<p>We will implement the shortest path algorithm with a single source. Our result should help us determine the shortest path from the origin, which in this case is <strong>A</strong>, to any other node in the graph.</p>
<p>The shortest path from node <strong>A</strong> to node <strong>C</strong> is 7 through node <strong>B</strong>. Likewise, the shortest path to <strong>F</strong> is through node <strong>E</strong> with a total distance of 5.</p>
<p>In order to come up with an algorithm to help us find the shortest path in a graph, let's solve the problem by hand. Thereafter, we will present the working solution in Python.</p>
<p>In the chapter on graphs, we saw how we could represent a graph with an adjacency list. We will use it with a slight modification to enable us capture the distance on every edge. A table will be used to also keep track of the shortest distance from the source in the graph to any other node. A Python dictionary will be used to implement this table. Here is one such table:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>Node</strong></p>
</td>
<td>
<p><strong>Shortest distance from source</strong></p>
</td>
<td>
<p><strong>Previous node</strong></p>
</td>
</tr>
<tr>
<td>
<p>A</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>None</p>
</td>
</tr>
<tr>
<td>
<p>B</p>
</td>
<td>
<p>∞</p>
</td>
<td>
<p>None</p>
</td>
</tr>
<tr>
<td>
<p>C</p>
</td>
<td>
<p>∞</p>
</td>
<td>
<p>None</p>
</td>
</tr>
<tr>
<td>
<p>D</p>
</td>
<td>
<p>∞</p>
</td>
<td>
<p>None</p>
</td>
</tr>
<tr>
<td>
<p>E</p>
</td>
<td>
<p>∞</p>
</td>
<td>
<p>None</p>
</td>
</tr>
<tr>
<td>
<p>F</p>
</td>
<td>
<p>∞</p>
</td>
<td>
<p>None</p>
</td>
</tr>
</tbody>
</table>
<p>The adjacency list for the diagram and table is as follows:</p>
<pre>
    graph = dict() <br/>    graph['A'] = {'B': 5, 'D': 9, 'E': 2} <br/>    graph['B'] = {'A': 5, 'C': 2} <br/>    graph['C'] = {'B': 2, 'D': 3} <br/>    graph['D'] = {'A': 9, 'F': 2, 'C': 3} <br/>    graph['E'] = {'A': 2, 'F': 3} <br/>    graph['F'] = {'E': 3, 'D': 2} 
</pre>
<p>The nested dictionary holds the distance and adjacent nodes.</p>
<p>This table forms the basis for our effort as we try to solve the problem at hand. When the algorithm starts, we have no idea what the shortest distance from the source (<strong>A</strong>) to any of the nodes is. To play it safe, we set the values in that column to infinity with the exception of node <strong>A</strong>. From the starting node, the distance covered from node <strong>A</strong> to node <strong>A</strong> is 0. So we can safely use this value as the shortest distance from node <strong>A</strong> to itself. No prior nodes have been visited when the algorithm begins. We therefore mark the previous node column of node as <kbd>None</kbd>.</p>
<p>In step 1 of the algorithm, we start by examining the adjacent nodes of node <strong>A</strong>. To find the shortest distance from node <strong>A</strong> to node <strong>B</strong>, we need to find the distance from the start node to the previous node of node B, which happens to be node <strong>A</strong>, and add it to the distance from node <strong>A</strong> to node <strong>B</strong>. We do this for other adjacent nodes of <strong>A</strong>, which are <strong>B</strong>, <strong>E</strong>, and <strong>D</strong>.</p>
<p>Using the adjacent node <strong>B</strong> as an example, the distance from the start node to the previous node is 0. The distance from the previous node to the current node (<strong>B</strong>) is 5. This sum is compared with the data in the shortest distance column of node B. Since 5 is less than infinity(<strong>∞</strong>), we replace <strong>∞</strong> with the smallest of the two, which is 5.</p>
<p>Any time the shortest distance of a node is replaced by a lesser value, we need to update the previous node column too. At the end of the first step, our table looks as follows:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>Node</strong></p>
</td>
<td>
<p><strong>Shortest distance from source</strong></p>
</td>
<td>
<p><strong>Previous node</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>A*</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>None</p>
</td>
</tr>
<tr>
<td>
<p>B</p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>A</p>
</td>
</tr>
<tr>
<td>
<p>C</p>
</td>
<td>
<p>∞</p>
</td>
<td>
<p>None</p>
</td>
</tr>
<tr>
<td>
<p>D</p>
</td>
<td>
<p>9</p>
</td>
<td>
<p>A</p>
</td>
</tr>
<tr>
<td>
<p>E</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>A</p>
</td>
</tr>
<tr>
<td>
<p>F</p>
</td>
<td>
<p>∞</p>
</td>
<td>
<p>None</p>
</td>
</tr>
</tbody>
</table>
<p>At this point, node <strong>A</strong> is considered visited. As such, we add node <strong>A</strong> to the list of visited nodes. In the table, we show that node <strong>A</strong> has been visited by making the text bold and appending an asterisk sign to it.</p>
<p>In the second step, we find the node with the shortest distance using our table as a guide. Node <strong>E</strong> with its value 2 has the shortest distance. This is what we can infer from the table about node <strong>E</strong>. To get to node <strong>E</strong>, we must visit node <strong>A</strong> and cover a distance of 2. From node A, we cover a distance of 0 to get to the starting node, which is node <strong>A</strong> itself.</p>
<p>The adjacent nodes of node <strong>E</strong> are <strong>A</strong> and <strong>F</strong>. But node <strong>A</strong> has already been visited, so we only consider node <strong>F</strong>. To find the shortest route or distance to node <strong>F</strong>, we must find the distance from the starting node to node <strong>E</strong> and add it to the distance between node <strong>E</strong> and <strong>F</strong>. We can find the distance from the starting node to node <strong>E</strong> by looking at the shortest distance column of node <strong>E</strong>, which has the value 2. The distance from node <strong>E</strong> to <strong>F</strong> can be obtained from the adjacency list we developed in Python earlier in this section. This distance is 3. These two sum up to 5, which is less than infinity. Remember we are on examining the adjacent node <strong>F</strong>. Since there are more adjacent nodes of node <strong>E</strong>, we mark node <strong>E</strong> as visited. Our updated table will have the following values:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>Node</strong></p>
</td>
<td>
<p><strong>Shortest distance from source</strong></p>
</td>
<td>
<p><strong>Previous node</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>A*</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>None</p>
</td>
</tr>
<tr>
<td>
<p>B</p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>A</p>
</td>
</tr>
<tr>
<td>
<p>C</p>
</td>
<td>
<p>∞</p>
</td>
<td>
<p>None</p>
</td>
</tr>
<tr>
<td>
<p>D</p>
</td>
<td>
<p>9</p>
</td>
<td>
<p>A</p>
</td>
</tr>
<tr>
<td>
<p><strong>E*</strong></p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>A</p>
</td>
</tr>
<tr>
<td>
<p>F</p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>E</p>
</td>
</tr>
</tbody>
</table>
<p>At this point, we initiate another step. The smallest value in the shortest distance column is 5. We choose <strong>B</strong> instead of <strong>F</strong> purely on an alphabetical basis. The adjacent nodes of <strong>B</strong> are <strong>A</strong> and <strong>C</strong>, but node <strong>A</strong> has already been visited. Using the rule we established earlier, the shortest distance from <strong>A</strong> to <strong>C</strong> is 7. We arrive at this number because the distance from the starting node to node <strong>B</strong> is 5, while the distance from node <strong>B</strong> to <strong>C</strong> is 2. Since the sum, 7, is less than infinity, we update the shortest distance to 7 and update the previous node column with node <strong>B</strong>. Now <strong>B</strong> is also marked as visited. The new state of the table is as follows:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>Node</strong></p>
</td>
<td>
<p><strong>Shortest distance from source</strong></p>
</td>
<td>
<p><strong>Previous node</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>A*</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>None</p>
</td>
</tr>
<tr>
<td>
<p><strong>B*</strong></p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>A</p>
</td>
</tr>
<tr>
<td>
<p>C</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>B</p>
</td>
</tr>
<tr>
<td>
<p>D</p>
</td>
<td>
<p>9</p>
</td>
<td>
<p>A</p>
</td>
</tr>
<tr>
<td>
<p><strong>E*</strong></p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>A</p>
</td>
</tr>
<tr>
<td>
<p>F</p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>E</p>
</td>
</tr>
</tbody>
</table>
<p>The node with the shortest distance yet unvisited is node <strong>F</strong>. The adjacent nodes of <strong>F</strong> are nodes <strong>D</strong> and <strong>E</strong>. But node <strong>E</strong> has already been visited. As such, we focus on finding the shortest distance from the starting node to node <strong>D</strong>. We calculate this distance by adding the distance from node <strong>A</strong> to <strong>F</strong> to the distance from node <strong>F</strong> to <strong>D</strong>. This sums up to 7, which is less than 9. Thus, we update the 9 with 7 and replace <strong>A</strong> with <strong>F</strong> in node <strong>D</strong>'s previous node column. Node <strong>F</strong> is now marked as visited. Here is the updated table up to this point:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>Node</strong></p>
</td>
<td>
<p><strong>Shortest distance from source</strong></p>
</td>
<td>
<p><strong>Previous node</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>A*</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>None</p>
</td>
</tr>
<tr>
<td>
<p><strong>B*</strong></p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>A</p>
</td>
</tr>
<tr>
<td>
<p>C</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>B</p>
</td>
</tr>
<tr>
<td>
<p>D</p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>F</p>
</td>
</tr>
<tr>
<td>
<p><strong>E*</strong></p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>A</p>
</td>
</tr>
<tr>
<td>
<p><strong>F*</strong></p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>E</p>
</td>
</tr>
</tbody>
</table>
<p>Now, the two unvisited nodes are <strong>C</strong> and <strong>D</strong>. In alphabetical order, we choose to examine <strong>C</strong> because both nodes have the same shortest distance from the starting node <strong>A</strong>.</p>
<p>However, all the adjacent nodes of <strong>C</strong> have been visited. Thus, we have nothing to do but mark node C as visited. The table remains unchanged at this point.</p>
<p>Lastly, we take node <strong>D</strong> and find out that all its adjacent nodes have been visited too. We only mark it as visited. The table remains unchanged:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>Node</strong></p>
</td>
<td>
<p><strong>Shortest distance from source</strong></p>
</td>
<td>
<p><strong>Previous node</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>A*</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>None</p>
</td>
</tr>
<tr>
<td>
<p><strong>B*</strong></p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>A</p>
</td>
</tr>
<tr>
<td>
<p><strong>C*</strong></p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>B</p>
</td>
</tr>
<tr>
<td>
<p><strong>D*</strong></p>
</td>
<td>
<p>7</p>
</td>
<td>
<p>F</p>
</td>
</tr>
<tr>
<td>
<p><strong>E*</strong></p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>A</p>
</td>
</tr>
<tr>
<td>
<p><strong>F*</strong></p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>E</p>
</td>
</tr>
</tbody>
</table>
<p>Let's verify this table with our graph. From the graph, we know that the shortest distance from <strong>A</strong> to <strong>F</strong> is 5. We will need to go through <strong>E</strong> to get to node <strong>F</strong>. According to the table, the shortest distance from the source column for node <strong>F</strong> is the value 5. This is true. It is also tells us that to get to node <strong>F</strong>, we need to visit node <strong>E</strong>, and from <strong>E</strong>, node <strong>A</strong>, which is our starting node. This is actually the shortest path.</p>
<p>We begin the program for finding the shortest distance by representing the table that enables us to track the changes in our graph. For the given diagram we used, here is a dictionary representation of the table:</p>
<pre>
    table = dict() <br/>    table = { <br/>        'A': [0, None], <br/>        'B': [float("inf"), None], <br/>        'C': [float("inf"), None], <br/>        'D': [float("inf"), None], <br/>        'E': [float("inf"), None], <br/>        'F': [float("inf"), None], <br/>    } 
</pre>
<p>The initial state of the table uses <kbd>float("inf")</kbd> to represent infinity. Each key in the dictionary maps to a list. At the first index of the list is stored the shortest distance from the source <kbd>A</kbd>. At the second index is the stored the previous node:</p>
<pre>
    DISTANCE = 0 <br/>    PREVIOUS_NODE = 1 <br/>    INFINITY = float('inf') 
</pre>
<p>To avoid the use of magic numbers, we use the preceding constants. The shortest path column's index is referenced by <kbd>DISTANCE</kbd>. The previous node column's index is referenced by <kbd>PREVIOUS_NODE</kbd>.</p>
<p>Now all is set for the main function. It will take the graph, represented by the adjacency list, the table, and the starting node as parameters:</p>
<pre>
    def find_shortest_path(graph, table, origin): <br/>        visited_nodes = [] <br/>        current_node = origin <br/>        starting_node = origin 
</pre>
<p>We keep the list of visited nodes in the list, <kbd>visited_nodes</kbd>. The <kbd>current_node</kbd> and <kbd>starting_node</kbd> variables will both point to the node in the graph we choose to make our starting node. The <kbd>origin</kbd> value is the reference point for all other nodes with respect to finding the shortest path.</p>
<p>The heavy lifting of the whole process is accomplished by the use of a <kbd>while</kbd> loop:</p>
<pre>
    while True: <br/>        adjacent_nodes = graph[current_node] <br/>        if set(adjacent_nodes).issubset(set(visited_nodes)): <br/>            # Nothing here to do. All adjacent nodes have been visited. <br/>            pass <br/>        else: <br/>            unvisited_nodes = <br/>                set(adjacent_nodes).difference(set(visited_nodes)) <br/><br/>            for vertex in unvisited_nodes: <br/><br/>                distance_from_starting_node = <br/>                    get_shortest_distance(table, vertex) <br/>                if distance_from_starting_node == INFINITY and <br/>                   current_node == starting_node: <br/>                    total_distance = get_distance(graph, vertex, <br/>                                                  current_node) <br/>                else: <br/>                    total_distance = get_shortest_distance (table, <br/>                    current_node) + get_distance(graph, current_node, <br/>                                                 vertex) <br/><br/>                if total_distance &lt; distance_from_starting_node: <br/>                    set_shortest_distance(table, vertex, <br/>                                          total_distance) <br/>                    set_previous_node(table, vertex, current_node) <br/><br/>        visited_nodes.append(current_node) <br/><br/>        if len(visited_nodes) == len(table.keys()): <br/>            break <br/><br/>        current_node = get_next_node(table,visited_nodes) 
</pre>
<p>Let's break down what the <kbd>while</kbd> loop is doing. In the body of the <kbd>while</kbd> loop, we obtain the current node in the graph we want to investigate with the line <kbd>adjacent_nodes = graph[current_node]</kbd>. <kbd>current_node</kbd> should have been set prior. The <kbd>if</kbd> statement is used to find out whether all the adjacent nodes of <kbd>current_node</kbd> have been visited. When the <kbd>while</kbd> loop is executed the fir<em>s</em>t time, <kbd>current_node</kbd> will contain A and <kbd>adjacent_nodes</kbd> will contain nodes B, D, and E. <kbd>visited_nodes</kbd> will be empty too. If all nodes have been visited, we only move on to the statements further down the program. Else, we begin a whole other step.</p>
<p>The statement <kbd>set(adjacent_nodes).difference(set(visited_nodes))</kbd> returns the nodes that have not been visited. The loop iterates over this list of unvisited nodes:</p>
<pre>
    distance_from_starting_node = get_shortest_distance(table, vertex) 
</pre>
<p>The helper method <kbd>get_shortest_distance(table, vertex)</kbd> will return the value stored in the shortest distance column of our table using one of the unvisited nodes referenced by <kbd>vertex</kbd>:</p>
<pre>
    if distance_from_starting_node == INFINITY and current_node == <br/>       starting_node: <br/>    total_distance = get_distance(graph, vertex, current_node) 
</pre>
<p>When we are examining the adjacent nodes of the starting node, <kbd>distance_from_starting_node == INFINITY and current_node == starting_node</kbd> will evaluate to True, in which case we only have to get the distance between the starting node and vertex by referencing the graph:</p>
<pre>
    total_distance = get_distance(graph, vertex, current_node) 
</pre>
<p>The <kbd>get_distance</kbd> method is another helper method we use to obtain the value (distance) of the edge between <kbd>vertex</kbd> and <kbd>current_node</kbd>.</p>
<p>If the condition fails, then we assign <kbd>total_distance</kbd> the sum of the distance from the starting node to <kbd>current_node</kbd> and the distance between <kbd>current_node</kbd> and <kbd>vertex</kbd>.</p>
<p>Once we have our total distance, we need to check whether this <kbd>total_distance</kbd> is less than the existing data in the shortest distance column in our table. If it is less, then we use the two helper methods to update that row:</p>
<pre>
    if total_distance &lt; distance_from_starting_node: <br/>        set_shortest_distance(table, vertex, total_distance) <br/>    set_previous_node(table, vertex, current_node) 
</pre>
<p>At this point, we add the <kbd>current_node</kbd> to the list of visited nodes:</p>
<pre>
    visited_nodes.append(current_node) 
</pre>
<p>If all nodes have been visited, then we must exit the <kbd>while</kbd> loop. To check whether all the nodes have been visited, we compare the length of the <kbd>visited_nodes</kbd> list to the number of keys in our table. If they have become equal, we simply exit the <kbd>while</kbd> loop.</p>
<p>The helper method, <kbd>get_next_node</kbd>, is used to fetch the next node to visit. It is this method that helps us find the minimum value in the shortest distance column from the starting nodes using our table.</p>
<p>The whole method ends by returning the updated table. To print the table, we use the following statements:</p>
<pre>
    shortest_distance_table = find_shortest_path(graph, table, 'A') <br/>    for k in sorted(shortest_distance_table): <br/>        print("{} - {}".format(k,shortest_distance_table[k])) <strong><br/></strong>
</pre>
<p>Output for the preceding statement:</p>
<pre>
<strong><br/></strong><strong>&gt;&gt;&gt; <br/></strong><strong>A - [0, None]<br/></strong><strong>B - [5, 'A']<br/></strong><strong>C - [7, 'B']<br/></strong><strong>D - [7, 'F']<br/></strong><strong>E - [2, 'A']<br/></strong><strong>F - [5, 'E']</strong>
</pre>
<p>For the sake of completeness, let's find out what the helper methods are doing:</p>
<pre>
    def get_shortest_distance(table, vertex): <br/>        shortest_distance = table[vertex][DISTANCE] <br/>        return shortest_distance 
</pre>
<p>The <kbd>get_shortest_distance</kbd> function returns the value stored in the zero<sup>th</sup> index of our table. At that index, we always store the shortest distance from the starting node up to <kbd>vertex</kbd>. The <kbd>set_shortest_distance</kbd> function only sets this value by the following:</p>
<pre>
    def set_shortest_distance(table, vertex, new_distance): <br/>        table[vertex][DISTANCE] = new_distance 
</pre>
<p>When we update the shortest distance of a node, we update its previous node using the following method:</p>
<pre>
    def set_previous_node(table, vertex, previous_node): <br/>        table[vertex][PREVIOUS_NODE] = previous_node 
</pre>
<p>Remember that the constant, <kbd>PREVIOUS_NODE</kbd>, equals 1. In the table, we store the value of the <kbd>previous_node</kbd> at <kbd>table[vertex][PREVIOUS_NODE]</kbd>.</p>
<p>To find the distance between any two nodes, we use the <kbd>get_distance</kbd> function:</p>
<pre>
    def get_distance(graph, first_vertex, second_vertex): <br/>        return graph[first_vertex][second_vertex] 
</pre>
<p>The last helper method is the <kbd>get_next_node</kbd> function:</p>
<pre>
    def get_next_node(table, visited_nodes): <br/>        unvisited_nodes = <br/>            list(set(table.keys()).difference(set(visited_nodes))) <br/>        assumed_min = table[unvisited_nodes[0]][DISTANCE] <br/>        min_vertex = unvisited_nodes[0] <br/>        for node in unvisited_nodes: <br/>            if table[node][DISTANCE] &lt; assumed_min: <br/>                assumed_min = table[node][DISTANCE] <br/>                min_vertex = node <br/><br/>        return min_vertex 
</pre>
<p>The <kbd>get_next_node</kbd> function resembles a function to find the smallest item in a list.</p>
<p>The function starts off by finding the unvisited nodes in our table by using <kbd>visited_nodes</kbd> to obtain the difference between the two sets of lists. The very first item in the list of <kbd>unvisited_nodes</kbd> is assumed to be the smallest in the shortest distance column of <kbd>table</kbd>. If a lesser value is found while the <kbd>for</kbd> loop runs, the <kbd>min_vertex</kbd> will be updated. The function then returns <kbd>min_vertex</kbd> as the unvisited vertex or node with the smallest shortest distance from the source.</p>
<p>The worst-case running time of Dijkstra's algorithm is <strong>O</strong>(<em>|E| + |V| log |V|</em>), where <em>|V|</em> is the number of vertices and <em>|E|</em> is the number of edges.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Complexity classes</h1>
            </header>

            <article>
                
<p>The problems that computer algorithms try to solve fall within a range of difficulty by which their solutions are arrived at. In this section, we will discuss the complexity classes N, NP, NP-complete, and NP-hard problems.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">P versus NP</h1>
            </header>

            <article>
                
<p>The advent of computers has sped up the rate at which certain tasks are performed. In general, computers are good at perfecting the art of calculation and all problems that can be reduced to a set of mathematical computations. However, this assertion is not entirely true. There are some nature or classes of problems that just take an enormous amount of time for the computer to make a sound guess, let alone find the right solution.</p>
<p>In computer science, the class of problems that computers can solve within polynomial time using a step-wise process of logical steps is called P-type problems, where P stands for polynomial. These are relatively easy to solve.</p>
<p>Then there is another class of problems that is considered very hard to solve. The word "hard problem" is used to refer to the way in which problems increase in difficulty when trying to find a solution. However, the good thing is that despite the fact that these problems have a high growth rate of difficulty, it is possible to determine whether a proposed solution solves the problem in polynomial time. These are the NP-Type problems. NP here stands for nondeterministic polynomial time.</p>
<p>Now the million dollar question is, does N = NP?</p>
<div class="packt_infobox">The proof for <em>N = NP</em> is one of the Millennium Prize Problems from the Clay Mathematics Institute that attract a $1,000,000 prize for a correct solution. These problems number 7 in number.</div>
<p>The Travelling Salesman problem is an example of an NP-Type problem. The problem statement says: given that there are <em>n</em> number of cities in some country, find the shortest route between all the cities, thus making the trip a cost-effective one. When the number of cities is small, this problem can be solved in a reasonable amount of time. However, when the number of cities is above any two-digit number, the time taken by the computer is remarkably long.</p>
<p>A lot of computer systems and cybersecurity is based on the RSA encryption algorithm. The strength of the algorithm and its security is due to the fact that it is based on the integer factoring problem, which is an NP-Type problem.</p>
<p>Finding the prime factors of a prime number composed of many digits is very difficult. When two large prime numbers are multiplied, a large non-prime number is obtained with only two large prime factors. Factorization of this number is where many cryptographic algorithms borrow their strength:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="369" src="assets/p_vs_np.jpg" width="427"/></div>
<p class="CDPAlignLeft CDPAlign">All P-type problems are subsets of NP problems. This means that any problem that can be solved in polynomial time can also be verified in polynomial time.</p>
<p>But the question, is P = NP? investigates whether problems that can be verified in polynomial time can also be solved in polynomial time. In particular, if they are equal, it would mean that problems that are solved by trying a number of possible solutions can be solved without the need to actually try all the possible solutions, invariably creating some sort of shortcut proof.</p>
<p>The proof, when finally discovered, will definitely have serious consequences in the fields of cryptography, game theory, mathematics, and many other fields.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">NP-Hard</h1>
            </header>

            <article>
                
<p>A problem is NP-Hard if all other problems in NP can be polynomial time reducible or mapped to it.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">NP-Complete</h1>
            </header>

            <article>
                
<p>A problem is considered an NP-complete problem if it is first of all an NP hard and is also found in the <kbd>NP</kbd> class.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this last chapter, we looked at the theories that support the computer science field. Without the use of too much mathematical rigor, we saw some of the main categories into which algorithms are classified. Other design techniques in the field, such as the divide and conquer, dynamic programming, and greedy algorithms, were also discussed, along with sample implementations.</p>
<p>Lastly, one of the outstanding problems yet to be solved in the field of mathematics was tackled. We saw how the proof for P = NP? will definitely be a game-changer in a number of fields, if such a proof is ever discovered.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>