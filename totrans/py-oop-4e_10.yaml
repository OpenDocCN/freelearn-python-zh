- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Iterator Pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ve discussed how many of Python''s built-ins and idioms seem, at first
    blush, to fly in the face of object-oriented principles, but are actually providing
    access to real objects under the hood. In this chapter, we''ll discuss how the `for` loop,
    which seems so structured, is actually a lightweight wrapper around a set of object-oriented
    principles. We''ll also see a variety of extensions to this syntax that automatically
    create even more types of object. We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What design patterns are
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The iterator protocol – one of the most powerful design patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List, set, and dictionary comprehensions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generator functions, and how they build on other patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The case study for this chapter will revisit the algorithms for partitioning
    sample data into testing and training subsets to see how the iterator design pattern
    applies to this part of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start with an overview of what design patterns are and why they're so
    important.
  prefs: []
  type: TYPE_NORMAL
- en: Design patterns in brief
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When engineers and architects decide to build a bridge, or a tower, or a building,
    they follow certain principles to ensure structural integrity. There are various
    possible designs for bridges (suspension and cantilever, for example), but if
    the engineer doesn't use one of the standard designs, and doesn't have a brilliant
    new design, it is likely the bridge they design will collapse.
  prefs: []
  type: TYPE_NORMAL
- en: Design patterns are an attempt to bring this same formal definition for correctly
    designed structures to software engineering. There are many different design patterns
    to solve different general problems. Design patterns are applied to solve a common
    problem faced by developers in some specific situation. The design pattern is
    a suggestion as to the ideal solution for that problem, in terms of object-oriented
    design. What's central to a pattern is that it is reused often in unique contexts.
    One clever solution is a good idea. Two similar solutions might be a coincidence.
    Three or more reuses of an idea and it starts to look like a repeating pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing design patterns and choosing to use them in our software does not, however,
    guarantee that we are creating a *correct* solution. In 1907, the Québec Bridge (to
    this day, the longest cantilever bridge in the world, just short of a kilometer
    long) collapsed before construction was completed, because the engineers who designed
    it grossly underestimated the weight of the steel used to construct it. Similarly,
    in software development, we may incorrectly choose or apply a design pattern,
    and create software that *collapses* under normal operating situations or when stressed
    beyond its original design limits.
  prefs: []
  type: TYPE_NORMAL
- en: Any one design pattern proposes a set of objects interacting in a specific way
    to solve a general problem. The job of the programmer is to recognize when they
    are facing a specific version of such a problem, then to choose and adapt the
    general pattern to their precise needs.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll look deeply at the iterator design pattern. This pattern
    is so powerful and pervasive that the Python developers have provided multiple
    syntaxes to access the object-oriented principles underlying the pattern. We will
    be covering other design patterns in the next two chapters. Some of them have
    language support and some don't, but none of them are so intrinsically a part
    of the Python coder's daily life as the iterator pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Iterators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In typical design pattern parlance, an **iterator** is an object with a `next()` method
    and a `done()` method; the latter returns `True` if there are no items left in
    the sequence. In a programming language without built-in support for iterators,
    the iterator would be used like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In Python, iteration is available across many language features, so the method
    gets a special name, `__next__`. This method can be accessed using the `next(iterator)` built-in.
    Rather than a `done()` method, Python's iterator protocol raises the `StopIteration`
    exception to notify the client that the iterator has completed. Finally, we have
    the much more readable `for item in iterator:` syntax to actually access items
    in an iterator instead of messing around with a `while` statement. Let's look
    at each these in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: The iterator protocol
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `Iterator` abstract base class, in the `collections.abc` module, defines
    the *iterator* protocol in Python. This definition is also referenced by the `typing`
    module to provide suitable type hints. At the foundation, any `Collection` class
    definition must be `Iterable`. To be `Iterable` means implementing an `__iter__()`
    method; this method creates an `Iterator` object.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17070_10_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: The abstractions for Iterable'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, an `Iterator` class must define a `__next__()` method that the `for` statement
    (and other features that support iteration) can call to get a new element from
    the sequence. In addition, every `Iterator` class must also fulfill the `Iterable` interface.
    This means an `Iterator` will also provide an `__iter__()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'This might sound a bit confusing, so have a look at the following example.
    Note that this is a very verbose way to solve this problem. It explains iteration
    and the two protocols in question, but we''ll be looking at several more readable
    ways to get this effect later in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This example defines a `CapitalIterable` class whose job is to loop over each
    of the words in a string and output them with the first letter capitalized. We
    formalized this by using the `Iterable[str]` type hint as a superclass to make
    it clear what our intention was. Most of the work of this iterable class is delegated
    to the `CapitalIterator` implementation. One way to interact with this iterator
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This example first constructs an iterable, assigning it to a variable with the
    boringly obvious name of `iterable`. It then retrieves a `CapitalIterator` instance
    from the `iterable` object. The distinction may need explanation; the iterable
    is an object with elements that can be iterated over. Normally, these elements
    can be looped over multiple times, maybe even at the same time or in overlapping
    code. The iterator, on the other hand, represents a specific location in that
    iterable; some of the items have been consumed and some have not. Two different
    iterators might be at different places in the list of words, but any one iterator
    can mark only one place.
  prefs: []
  type: TYPE_NORMAL
- en: Each time `next()` is called on the iterator, it returns another token from
    the iterable, in order, and updates its internal state to point to the next item.
    Eventually, the iterator will be exhausted (won't have any more elements to return),
    in which case a `StopIteration` exception is raised, and we break out of the `while`
    statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Python has a simpler syntax for constructing an iterator from an iterable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the `for` statement, in spite of not looking remotely object-oriented,
    is actually a shortcut to some fundamentally object-oriented design principles.
    Keep this in mind as we discuss comprehensions, as they, too, appear to be the
    polar opposite of an object-oriented tool. Yet, they use the same iteration protocol
    as `for` statements and are another kind of shortcut.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of iterable classes in Python is large. We''re not surprised when
    strings, tuples, and lists are iterable. A set, clearly, must be iterable, even
    if the order of elements may be difficult to predict. A mapping will iterate over
    the keys by default; other iterators are available. A file iterates over the available
    lines. A regular expression has a method, `finditer()`, that is an iterator over
    each instance of a matching substring that it can find. The `Path.glob()` method
    will iterate over matching items in a directory. The `range()` object is also
    an iterator. You get the idea: anything even vaguely collection-like will support
    some kind of iterator.'
  prefs: []
  type: TYPE_NORMAL
- en: Comprehensions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Comprehensions are simple, but powerful, syntaxes that allow us to transform
    or filter an iterable object in as little as one line of code. The resultant object
    can be a perfectly normal list, set, or dictionary, or it can be a *generator
    expression* that can be efficiently consumed while keeping just one element in
    memory at a time.
  prefs: []
  type: TYPE_NORMAL
- en: List comprehensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: List comprehensions are one of the most powerful tools in Python, so people
    tend to think of them as advanced. They're not. Indeed, we've taken the liberty
    of littering previous examples with comprehensions, assuming you would understand
    them. While it's true that advanced programmers use comprehensions a lot, it's
    not because they're advanced. It's because a comprehension is so fundamental to
    Python, it can handle many of the most common operations in application software.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at one of those common operations; namely, converting a
    list of items into a list of related items. Specifically, let''s assume we just
    read a list of strings from a file, and now we want to convert it to a list of
    integers. We know every item in the list is an integer, and we want to do some
    activity (say, calculate an average) on those numbers. Here''s one simple way
    to approach it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This works fine and it''s only three lines of code. If you aren''t used to
    comprehensions, you may not even think it looks ugly! Now, look at the same code
    using a list comprehension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We're down to one line and, importantly for performance, we've dropped an `append`
    method call for each item in the list. Overall, it's pretty easy to tell what's
    going on, even if you're not used to comprehension syntax.
  prefs: []
  type: TYPE_NORMAL
- en: The square brackets indicate, as always, that we're creating a list. Inside
    this list is a `for` clause that iterates over each item in the input sequence.
    The only thing that may be confusing is what's happening between the list's opening
    brace and the start of the `for` statement. Whatever expression is provided here
    is applied to *each* of the items in the input list. The item in question is referenced
    by the `num` variable from the `for` clause. So, this expression applies the `int` function
    to each element and stores the resulting integer in the new list.
  prefs: []
  type: TYPE_NORMAL
- en: Terminology-wise, we call this a **mapping**. We are applying the result expression,
    `int(num)` in this example, to map values from the source iterable to create a
    resulting iterable list.
  prefs: []
  type: TYPE_NORMAL
- en: That's all there is to a basic list comprehension. Comprehensions are highly
    optimized, making them far faster than `for` statements when processing a large
    number of items. When used wisely, they're also more readable. These are two compelling
    reasons to use them widely.
  prefs: []
  type: TYPE_NORMAL
- en: 'Converting one list of items into a related list isn''t the only thing we can
    do with a list comprehension. We can also choose to exclude certain values by
    adding an `if` statement inside the comprehension. We call this a **filter**.
    Have a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The essential difference between this example and the previous one is the `if
    len(num) < 3` clause. This extra code excludes any strings with more than two
    characters. The `if` clause is applied to each element **before** the final `int()` function,
    so it's testing the length of a string. Since our input strings are all integers
    at heart, it excludes any number over 99.
  prefs: []
  type: TYPE_NORMAL
- en: A list comprehension can be used to map input values to output values, applying
    a filter along the way to include or exclude any values that meet a specific condition.
    A great many algorithms involve mapping and filtering operations.
  prefs: []
  type: TYPE_NORMAL
- en: Any iterable can be the input to a list comprehension. In other words, anything
    we can wrap in a `for` statement can also be used as the source for a comprehension.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, text files are iterable; each call to `__next__()` on the file''s
    iterator will return one line of the file. We can examine the lines of a text
    file by naming the open file in the `for` clause of a list comprehension. We can
    then use the `if` clause to extract interesting lines of text. This example finds
    a subset of lines in a test file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we've added some whitespace to make the comprehension more
    readable (list comprehensions don't *have* to fit on one physical line even though
    they're one logical line). This example creates a list of lines that have the
    "`>>>`" prompt in them. The presence of "`>>>`" suggests there might be a doctest
    example in this file. The list of lines has `rstrip()` applied to remove trailing
    whitespace, like the `\n` that ends each line of text returned by the iterator.
    The resulting list object, `examples`, suggests some of the test cases that can
    be found within the code. (This isn't as clever as doctest's own parser.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s extend this example to capture the line numbers for each example with
    a "`>>>`" prompt in it. This is a common requirement, and the built-in `enumerate()`
    function helps us pair a number with each item provided by the iterator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `enumerate()` function consumes an iterable, providing an iterable sequence
    of two-tuples of a number and the original item. If the line passes our "`>>>`"
    test, we'll create a two-tuple of the number and the cleaned-up text. We've done
    some sophisticated processing in – effectively – one line of code. Essentially,
    though, it's a filter and a mapping. First it extracts tuples from the source,
    then it filters the lines that match the given `if` clause, then it evaluates
    the `(number, line.rstrip())` expression to create resulting tuples, and finally,
    collects it all into a list object. The ubiquity of this iterate-filter-map-collect
    pattern drives the idea behind a list comprehension.
  prefs: []
  type: TYPE_NORMAL
- en: Set and dictionary comprehensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Comprehensions aren't restricted to lists. We can use a similar syntax with
    braces to create sets and dictionaries as well. Let's start with sets. One way
    to create a set is to wrap a list comprehension in the `set()` constructor, which
    converts it to a set. But why waste memory on an intermediate list that gets discarded,
    when we can create a set directly?
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example that uses a named tuple to model author/title/genre triples,
    and then retrieves a set of all the authors that write in a specific genre:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve defined a small library of instances of the `Book` class. We can create
    a set from each of these objects by using a set comprehension. It looks a lot
    like a list comprehension, but uses `{}` instead of `[]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The highlighted set comprehension sure is short in comparison to the demo-data
    setup! If we were to use a list comprehension, of course, Terry Pratchett would
    have been listed twice. As it is, the nature of sets removes the duplicates, and
    we end up with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note that sets don't have a defined ordering, so your output may differ from
    this example. For testing purposes, we'll sometimes set the `PYTHONHASHSEED` environment
    variable to impose an order. This introduces a tiny security vulnerability, so
    it's only suitable for testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Still using braces, we can introduce a colon to make `key:value` pairs required
    to create a dictionary comprehension. For example, it may be useful to quickly
    look up the author or genre in a dictionary if we know the title. We can use a
    dictionary comprehension to map titles to `books` objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have a dictionary, and can look up books by title using the normal syntax,
    `fantasy_titles['Nightwatch']`. We've created a high-performance index from a
    lower-performance sequence.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, comprehensions are not advanced Python, nor are they features that
    subvert object-oriented programming. They are a more concise syntax for creating
    a list, set, or dictionary from an existing iterable source of data.
  prefs: []
  type: TYPE_NORMAL
- en: Generator expressions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes we want to process a new sequence without pulling a new list, set,
    or dictionary into system memory. If we're iterating over items one at a time,
    and don't actually care about having a complete container (such as a list or dictionary)
    created, a container is a waste of memory. When processing one item at a time,
    we only need the current object available in memory at any one moment. But when
    we create a container, all the objects have to be stored in that container before
    we start processing them.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider a program that processes log files. A very simple log
    might contain information in this format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Log files for popular web servers, databases, or email servers can contain many
    gigabytes of data (one of the authors once had to clean nearly two terabytes of
    logs off a misbehaving system). If we want to process each line in the log, we
    can't use a list comprehension; it would create a list containing every line in
    the file. This probably wouldn't fit in RAM and could bring the computer to its
    knees, depending on the operating system.
  prefs: []
  type: TYPE_NORMAL
- en: If we used a `for` statement on the log file, we could process one line at a
    time before reading the next one into memory. Wouldn't be nice if we could use
    comprehension syntax to get the same effect?
  prefs: []
  type: TYPE_NORMAL
- en: This is where generator expressions come in. They use the same syntax as comprehensions,
    but they don't create a final container object. We call them **lazy**; they reluctantly
    produce values on demand. To create a generator expression, wrap the comprehension
    in `()` instead of `[]` or `{}`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code parses a log file in the previously presented format and
    outputs a new log file that contains only the `WARNING` lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We've opened the `sample.log` file, a file perhaps too large to fit in memory.
    A generator expression will filter out the warnings (in this case, it uses the `if` syntax
    and leaves the line unmodified). This is lazy, and doesn't really do anything
    until we consume its output. We can open another file as a subset. The final `for`
    statement consumes each individual line from the `warning_lines` generator. At
    no time is the full log file read into memory; the processing happens one line
    at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run it on our sample file, the resulting `warnings.log` file looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Of course, with a short input file, we could have safely used a list comprehension,
    doing all the processing in memory. When the file is millions of lines long, the
    generator expression will have a huge impact on both memory and speed.
  prefs: []
  type: TYPE_NORMAL
- en: The core of a comprehension is the generator expression. Wrapping a generator
    in `[]` creates a list. Wrapping a generator in `{}` creates a set. Using `{}`
    and `:` to separate keys and values creates a dictionary. Wrapping a generator
    in `()` is still a generator expression, not a tuple.
  prefs: []
  type: TYPE_NORMAL
- en: Generator expressions are frequently most useful inside function calls. For
    example, we can call `sum`, `min`, or `max` on a generator expression instead
    of a list, since these functions process one object at a time. We're only interested
    in the aggregate result, not any intermediate container.
  prefs: []
  type: TYPE_NORMAL
- en: In general, of the four options, a generator expression should be used whenever
    possible. If we don't actually need a list, set, or dictionary, but simply need
    to filter or apply a mapping to items in a sequence, a generator expression will
    be most efficient. If we need to know the length of a list, or sort the result,
    remove duplicates, or create a dictionary, we'll have to use the comprehension
    syntax and create a resulting collection.
  prefs: []
  type: TYPE_NORMAL
- en: Generator functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generator functions embody the essential features of a generator expression,
    which is the generalization of a comprehension. The generator function syntax
    looks even less object-oriented than anything we've seen, but we'll discover that
    once again, it is a syntax shortcut to create a kind of iterator object. It helps
    us build processing following the standard iterator-filter-mapping pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take the log file example a little further. If we want to decompose the
    log into columns, we'll have to do a more significant transformation as part of
    the mapping step. This will involve a regular expression to find the timestamp,
    the severity word, and the message as a whole. We'll look at a number of solutions
    to this problem to show how generators and generator functions can be applied
    to create the objects we want.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a version, avoiding generator expressions entirely:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve defined a regular expression to match three groups:'
  prefs: []
  type: TYPE_NORMAL
- en: The complex date string, `(\w\w\w \d\d, \d\d\d\d \d\d:\d\d:\d\d),` which is
    a generalization of strings like "`Apr 05, 2021 20:04:41`".
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The severity level, `(\w+)`, which matches a run of letters, digits, or underscores.
    This will match words like INFO and DEBUG.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An optional message, `(.*)`, which will collect all characters to the end of
    the line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This pattern is assigned to the `pattern` variable. As an alternative, we could
    also use `split(' ')` to break the line into space-separated words; the first
    four words are the date, the next word is the severity, and all the remaining
    words are the message. This isn't as flexible as defining a regular expression.
  prefs: []
  type: TYPE_NORMAL
- en: The decomposition of the line into groups involves two steps. First, we apply
    `pattern.match()` to the line of text to create a `Match` object. Then we interrogate
    the `Match` object for the sequence of groups that matched. We have a `cast(Match[str],
    pattern.match(line))` to tell **mypy** that every line will create a `Match` object.
    The type hint for `re.match()` is `Optional[Match]` because it returns a `None`
    when there's no `Match`. We're using `cast()` to make the claim that every line
    will match, and if it doesn't match, we want this function to raise an exception.
  prefs: []
  type: TYPE_NORMAL
- en: This deeply nested function seems maintainable, but so many levels of indent
    in so few lines is kind of ugly. More alarmingly, if there is some irregularity
    in the file, and we want to handle the case where the `pattern.match(line)` returns
    `None`, we'd have to include another `if` statement, leading to even deeper levels
    of nesting. Deeply nested conditional processing leads to statements where the
    conditions under which they are executed can be obscure.
  prefs: []
  type: TYPE_NORMAL
- en: The reader has to mentally integrate all of the preceding `if` statements to
    work out the condition. This can be a problem with this kind of solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s consider a truly object-oriented solution, without any shortcuts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve defined a formal `WarningReformat` iterator that emits the three-tuple
    of the date, warning, and message. We''ve used a type hint of `tuple[str, ...]`
    because it matches the output from the `self.pattern.match(line).groups()` expression:
    it''s a sequence of strings, with no constraint on how many will be present. The
    iterator is initialized with a `TextIO` object, something file-like that has a
    `readline()` method.'
  prefs: []
  type: TYPE_NORMAL
- en: This `__next__()` method reads lines from the file, discarding any lines that
    are not `WARNING` lines. When we encounter a `WARNING` line, we parse it and return
    the three-tuple of strings.
  prefs: []
  type: TYPE_NORMAL
- en: The `extract_and_parse_2()` function uses an instance of the `WarningReformat`
    class in a `for` statement; this will evaluate the `__next__()` method repeatedly
    to process the subsequent `WARNING` line. When we run out of lines, the `WarningReformat`
    class raises a `StopIteration` exception to tell the function statement we're
    finished iterating. It's pretty ugly compared to the other examples, but it's
    also powerful; now that we have a class in our hands, we can do whatever we want
    with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that background behind us, we finally get to see true generators in action.
    This next example does *exactly* the same thing as the previous one: it creates
    an object with a `__next__()` method that raises `StopIteration` when it''s out
    of inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `yield` statement in the `warning_filters()` function is the key to generators.
    When Python sees `yield` in a function, it takes that function and wraps it up
    in an object that follows the `Iterator` protocol, not unlike the class defined
    in our previous example. Think of the `yield` statement as similar to the `return` statement;
    it returns a line. Unlike `return`, however, the function is only suspended. When
    it is called again (via `next()`), it will start where it left off – on the line
    after the `yield` statement – instead of at the beginning of the function. In
    this example, there is no line *a**fter* the `yield` statement, so it jumps to
    the next iteration of the `for` statement. Since the `yield` statement is inside
    an `if` statement, it only yields lines that contain `WARNING`.
  prefs: []
  type: TYPE_NORMAL
- en: 'While it looks like this is just a function looping over the lines, it is actually
    creating a special type of object, a generator object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: All the function does is create and return a generator object. In this example,
    an empty list was provided, and a generator was built. The generator object has
    `__iter__()` and `__next__()` methods on it, just like the one we created from
    a class definition in the previous example. (Using the `dir()` built-in function
    on it will reveal what else is part of a generator.) Whenever the `__next__()`
    method is called, the generator runs the function until it finds a `yield` statement.
    It then suspends execution, retaining the current state, and returning the value
    from `yield`. The next time the `__next__()` method is called, it restores the
    state and picks up execution where it left off.
  prefs: []
  type: TYPE_NORMAL
- en: 'This generator function is nearly identical to this generator expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see how these various patterns align. The generator expression has all
    the elements of the statements, slightly compressed, and in a different order:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17070_10_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Generator functions compared with generator expressions'
  prefs: []
  type: TYPE_NORMAL
- en: A comprehension, then, is a generator wrapped in `[]` or `{}` to create a concrete
    object. In some cases, it can make sense to use `list()`, `set()`, or `dict()`
    as a wrapper around a generator. This is helpful when we're considering replacing
    the generic collection with a customized collection of our own. Changing `list()`
    into `MySpecialContainer()` seems to make the change more apparent.
  prefs: []
  type: TYPE_NORMAL
- en: The generator expression has the advantage of being short and appearing right
    where it's needed. The generator function has a name and parameters, meaning it
    can be reused. More importantly, a generator function can have multiple statements
    and more complex processing logic in the cases where statements are needed. One
    common reason for switching from a generator expression to a function is to add
    exception handling.
  prefs: []
  type: TYPE_NORMAL
- en: Yield items from another iterable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often, when we build a generator function, we end up in a situation where we
    want to yield data from another iterable object, possibly a list comprehension
    or generator expression we constructed inside the generator, or perhaps some external
    items that were passed into the function. We'll look at how to do this with the
    `yield from` statement.
  prefs: []
  type: TYPE_NORMAL
- en: Let's adapt the generator example a bit so that instead of accepting an input
    file, it accepts the name of a directory. The idea is to keep our existing warnings
    filter generator in place, but tweak the structure of the functions that use it.
    We'll operate on iterators as both input and result; this way the same function
    could be used regardless of whether the log lines came from a file, memory, the
    web, or another iterator.
  prefs: []
  type: TYPE_NORMAL
- en: 'This version of the code illustrates a new `file_extract()` generator. This
    does some basic setup before yielding information from the `warnings_filter()`
    generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Our top-level function `extract_and_parse_d()` has a slight change to use the
    `file_extract()` function instead of opening a file and applying the `warnings_filter()`
    to one file. The `file_extract()` generator will yield all of the `WARNING` lines
    from *all* of the files provided in the argument value.
  prefs: []
  type: TYPE_NORMAL
- en: The `yield from` syntax is a useful shortcut when writing chained generators.
  prefs: []
  type: TYPE_NORMAL
- en: 'What''s central in this example is the laziness of each of the generators involved.
    Consider what happens when the `extract_and_parse_d()` function, the client, makes a
    demand:'
  prefs: []
  type: TYPE_NORMAL
- en: The client evaluates `file_extract(log_files)`. Since this is in a `for` statement,
    there's an `__iter__()` method evaluation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `file_extract()` generator gets an iterator from the `path_iter` iterable,
    and uses this to get the next `Path` instance. The `Path` object is used to create
    a file object that's provided to the `warnings_filter()` generator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `warnings_filter()` generator uses the file's iterator over lines to read
    until it finds a `WARNING` line, which it parses, yielding exactly one tuple.
    The fewest number of lines were read to find this line.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `file_extract()` generator is yielding from the `warnings_filter()` generator,
    so the single tuple is provided to the ultimate client, the `extract_and_parse_d()`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `extract_and_parse_d()` function writes the single tuple to the open CSV
    file, and then demands another tuple. This request goes to `file_extract()`, which
    pushes the demand down to `warnings_filter()`, which pushes the demand to an open
    file to provide lines until a `WARNING` line is found.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each generator is lazy and provides one response, doing the least amount of
    work it can get away with to produce the result. This means that a directory with
    a huge number of giant log files is processed by having one open log file, and
    one current line being parsed and processed. It won't fill memory no matter how
    large the files are.
  prefs: []
  type: TYPE_NORMAL
- en: We've seen how generator functions can provide data to other generator functions.
    We can do this with ordinary generator expressions, also. We'll make some small
    changes to the `warnings_filter()` function to show how we can create a stack
    of generator expressions.
  prefs: []
  type: TYPE_NORMAL
- en: Generator stacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The generator function (and the generator expression) for `warnings_filter`
    makes an unpleasant assumption. The use of `cast()` makes a claim to **mypy**
    that''s – perhaps – a bad claim to make. Here''s the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The use of `cast()` is a way of claiming the `pattern.match()` will always yield
    a `Match[str]` object. This isn't a great assumption to make. Someone may change
    the format of the log file to include a multiple-line message, and our `WARNING`
    filter would crash every time we encountered a multi-line message.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a message that would cause problems followed by a message that''s easy
    to process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The first line has the word `WARN` in a multi-line message that will break our
    assumption about lines that contain the word `WARN`. We need to handle this with
    a little more care.
  prefs: []
  type: TYPE_NORMAL
- en: We can rewrite this generator expression to create a generator function, and
    add an assignment statement (to save the `Match` object) and an `if` statement
    to further decompose the filtering. We can use the walrus operator `:=` to save
    the `Match` object, also.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could reframe the generator expression as the following generator function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'As we noted above, this complex filtering tends toward deeply nested `if` statements,
    which can create logic that''s difficult to summarize. In this case, the two conditions
    aren''t terribly complex. An alternative is to change this into a series of map
    and filter stages, each of which does a separate, small transformation on the
    input. We can decompose the matching and filtering into the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Map the source line to an `Optional[Match[str]]` object using the `pattern.match()`
    method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filter to reject any `None` objects, passing only good `Match` objects and applying the
    `groups()` method to create a `List[str]`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filter the strings to reject the non-`WARN` lines, and pass the `WARN` lines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each of these stages is a generator expression following the standard pattern.
    We can expand the `warnings_filter` expression into a stack of three expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: These expressions are, of course, utterly lazy. The final `warnings_filter`
    uses the iterable, `group_iter`. This iterable gets matches from another generator,
    `possible_match_iter`, which gets source text lines from the `source` object,
    an iterable source of lines. Since each of these generators is getting items from
    another lazy iterator, there's only one line of data being processed through the
    `if` clause and the final expression clause at each stage of this process.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we can exploit the surrounding `()` to break each expression into
    multiple lines. This can help show the map or filter operation that's embodied
    in each expression.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can inject additional processing as long as it fits this essential mapping-and-filtering
    design pattern. Before moving on, we''re going to switch to a slightly more friendly
    regular expression for locating lines in our log file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This regular expression is broken into three adjacent strings; Python will automatically
    concatenate string literals. The expression uses three named groups. The date-time
    stamp, for example, is group number one, a hard-to-remember bit of trivia. The
    `?P<dt>` inside the `()` means the `groupdict()` method of a `Match` object will
    have the key `dt` in the resulting dictionary. As we introduce more processing
    steps, we'll need to be much more clear about the intermediate results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an image of the regular expression that can sometimes be helpful:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17070_10_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Log line regular expression diagram'
  prefs: []
  type: TYPE_NORMAL
- en: Let's expand this example to transform the date-time stamp to another format.
    This involves injecting a transformation from the input format to the desired
    output format. We can do this in one big gulp, or we can do it in a series of
    small sips.
  prefs: []
  type: TYPE_NORMAL
- en: 'This sequence of steps makes it easier to add or change one individual step
    without breaking the entire pipeline of processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We've created two additional stages. One parses the input time to create a Python
    `datetime` object; the second stage formats the `datetime` object as an ISO. Breaking
    the transformation down into small steps lets us treat each mapping operation
    and each filtering operating as discrete, separate steps. We can add, change,
    and delete with a little more flexibility when we create these smaller, easier-to-understand
    steps. The idea is to isolate each transformation into a separate object, described
    by a generator expression.
  prefs: []
  type: TYPE_NORMAL
- en: The result of the `dt_iter` expression is an iterable over anonymous tuples.
    This is a place where a `NamedTuple` can add clarity. See *Chapter 7*, *Python
    Data Structures*, for more information on `NamedTuple`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have an additional way to look at these transformational steps, using the
    built-in `map()` and `filter()` functions. These functions provide features similar
    to generator expressions, using another, slightly different syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The lambda objects are anonymous functions. Each lambda is a callable object
    with parameters and a single expression that is evaluated and returned. There's
    no name and no statements in the body of a lambda. Each stage in this pipeline
    is a discrete mapping or filtering operation. While we can combine mapping and
    filtering into a single `map(lambda ..., filter(lambda ..., source))`, this can
    be too confusing to be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: The `possible_match_iter` applies the `pattern.match()` to each line. The `good_match_iter`
    uses the special `filter(None, source)` that passes non-`None` objects, and rejects
    `None` objects. The `group_iter` uses a lambda to evaluate `m.groups()` for each
    object, `m`, in `good_match_iter`. The `warnings_iter` will filter the `group_iter`
    results, keeping only the `WARN` lines, and rejecting all others. The `dt_iter`
    and the final `warnings_filter` expressions perform a conversion from the source
    datetime format to a generic `datetime` object, followed by formatting the `datetime`
    object in a different string format.
  prefs: []
  type: TYPE_NORMAL
- en: We've seen a number of ways of approaching a complex map-filter problem. We
    can write nested `for` and `if` statements. We can create explicit `Iterator`
    subclass definitions. We can create iterator-based objects using function definitions
    that include the `yield` statement. This provides us the formal interface of the
    `Iterator` class without the lengthy boilerplate required to define `__iter__()`
    and `__next__()` methods. Additionally, we can use generator expressions and even
    comprehensions to apply the iterator design pattern in a number of common contexts.
  prefs: []
  type: TYPE_NORMAL
- en: The iterator pattern is a foundational aspect of Python programming. Every time
    we work with a collection, we'll be iterating through the items, and we'll be
    using an iterator. Because iteration is so central, there are a variety of ways
    of tackling the problem. We can use `for` statements, generator functions, generator
    expressions, and we can build our own iterator classes.
  prefs: []
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python makes extensive use of iterators and iterable collections. This underlying
    aspect appears in many places. Each `for` statement makes implicit use of this.
    When we use functional programming techniques, such as generator expressions,
    and the `map()`, `filter()`, and `reduce()` functions, we're exploiting iterators.
  prefs: []
  type: TYPE_NORMAL
- en: Python has an `itertools` module full of additional iterator-based design patterns.
    This is worthy of study because it provides many examples of common operations
    that are readily available using built-in constructs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply these concepts in a number of places in our case study:'
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning all the original samples into testing and training subsets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing a particular *k* and distance hyperparameter set by classifying all
    the test cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *k*-nearest neighbors (*k*-NN) algorithm itself and how it locates the *k* nearest
    neighbors from all the training samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The common aspect of these three processing examples is the "for all" aspect
    of each one. We'll take a little side-trip into the math behind comprehensions
    and generator functions. The math isn't terribly complex, but the following section
    can be thought of as deep background. After this digression, we'll dive into partitioning
    data into training and testing subsets using the iterator concepts.
  prefs: []
  type: TYPE_NORMAL
- en: The Set Builder background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Formally, we can summarize operations like partitioning, testing, and even locating
    nearest neighbors with a logic expression. Some developers like the formality
    of it because it can help describe the processing without forcing a specific Python
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the essential rule for partitioning, for example. This involves a "for
    all" condition that describes the elements of a set of samples, *S*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17070_10_001.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, for all *s* in the universe of available samples, *S*, the value
    of *s* is either in the training set, *R*, or the testing set, *E*. This summarizes
    the result of a successful partition of the data. It doesn't describe an algorithm,
    directly, but having this rule can help us be sure we haven't missed something
    important.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also summarize a performance metric for testing. The recall metric has
    a "for all" implied by the *∑* construct:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17070_10_002.png)'
  prefs: []
  type: TYPE_IMG
- en: The quality score, *q*, is the sum for all *e* in the testing set, *E*, of 1
    where the `knn()` classifier applied to *e* matches the species for *e*, `s(e)`,
    otherwise 0\. This can map nicely to a Python generator expression.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *k*-NN algorithm involves a bit more complexity in its definition. We can
    think of it as a partitioning problem. We need to start with a collection of ordered
    pairs. Each pair is the distance from an unknown, *u*, to a training sample, *r*,
    summarized as ![](img/B17070_10_003.png). As we saw in *Chapter 3*, there are
    a number of ways to compute this distance. This has to be done for all training
    samples, *r,* in the universe of training samples, *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17070_10_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then we need to partition these distances into two subsets, *N*, and *F* (near
    and far) such that all distances in *N* are less than or equal to all distances
    in *F*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17070_10_005.png)'
  prefs: []
  type: TYPE_IMG
- en: We also need to be sure the number of elements in the near set, *N,* is equal
    to the desired number of neighbors, *k*.
  prefs: []
  type: TYPE_NORMAL
- en: This final formalism exposes an interesting nuance to the computation. What
    if there are more than *k* neighbors with the same distance metric? Should *all* of
    the equidistant training samples be included in voting? Or should we arbitrarily
    slice exactly *k* of the equidistant samples? If we "arbitrarily" slice, what's
    the exact rule that gets used for choosing among the equidistant training samples?
    Does the selection rule even matter? These could be significant issues, and they're
    outside the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: The example later in the chapter uses the `sorted()` function, which tends to
    preserve the original order. Can this lead to a bias to our classifier when confronted
    with equidistant choices? This, too, may be a significant issue, and it's also
    outside the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Given a little bit of set theory, we can tackle the ideas of partitioning the
    data, and computing the *k* nearest neighbors, making use of the common iterator
    features. We'll start with the partitioning algorithm's implementation in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple partitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our goal is to separate testing and training data. There's a tiny bump in the
    road, however, called **deduplication**. The statistical measures of overall quality
    rely on the training and testing sets being independent; this means we need to
    avoid duplicate samples being split between testing and training sets. Before
    we can create testing and training partitions, we need to find any duplicates.
  prefs: []
  type: TYPE_NORMAL
- en: We can't – easily – compare each sample with all of the other samples. For a
    large set of samples, this may take a very long time. A pool of ten thousand samples
    would lead to 100 million checks for duplication. This isn't practical. Instead,
    we can partition our data into subgroups where the values for all the measured
    features are *likely* to be equal. Then, from those subgroups, we can choose testing
    and training samples. This lets us avoid comparing every sample with all of the
    other samples to look for duplicates.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use Python''s internal hash values, we can create buckets containing
    samples that may have equal values. In Python, if items are equal, they must have
    the same integer hash value. The inverse is not true: items may coincidentally
    have the same hash value, but may not actually be equal.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, we can say this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17070_10_006.png)'
  prefs: []
  type: TYPE_IMG
- en: That is, if two objects in Python, *a* and *b*, are equal, they must also have
    the same hash value ![](img/B17070_10_007.png). The reverse is not true because
    equality is more than a simple hash value check; it's possible that ![](img/B17070_10_008.png);
    the hash values may be the same, but the underlying objects aren't actually equal.
    We call this a "hash collision" of two unequal values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing this thought, the following is a matter of definition for modulo:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17070_10_009.png)'
  prefs: []
  type: TYPE_IMG
- en: If two values are equal, they are also equal to any modulo of those values.
    When we want to know if `a == b`, we can ask if `a % 2 == b % 2`; if both numbers
    are odd or both numbers are even, then there's a chance `a` and `b` could be equal.
    If one number is even and the other is odd, there's no way they can be equal.
  prefs: []
  type: TYPE_NORMAL
- en: For complex objects, we can use `hash(a) % m == hash(b) % m`. If the two hash
    values, modulo *m*, are the same, then the hash values could be the same, and
    the two objects, `a` and `b`, could also be equal. We know it's possible for several
    objects to have the same hash value, and even more objects to have the same hash
    value modulo *m*.
  prefs: []
  type: TYPE_NORMAL
- en: While this doesn't tell us if two items are equal, this technique limits the
    domain of objects required for exact equality comparison to very small pools of
    a few items instead of the entire set of all samples. We can avoid duplicates
    if we avoid splitting up one of these subgroups.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a view of seven samples broken into three subgroups based on their
    hash codes modulo 3\. Most of the subgroups have items that are potentially equal,
    but actually aren''t equal. One of the groups has an actual duplicate sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17070_10_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: Partitioning sample data to locate duplicates'
  prefs: []
  type: TYPE_NORMAL
- en: To find the duplicate sample, we don't need to compare each sample against the
    other six. We can look within each subgroup and compare a few samples to see if
    they happen to be duplicates.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind this deduplication approach is to separate the entire suite
    of samples into sixty buckets where the samples have equal hash values, modulo
    sixty. Samples in the same bucket *could* be equal, and as a simple expedient,
    we can treat them as equal. Samples in separate buckets have different hash values
    and cannot possibly be equal.
  prefs: []
  type: TYPE_NORMAL
- en: We can avoid having duplicate samples in both testing and training by using
    an entire bucket's set of samples together. That way, the duplicates are either
    all testing or all training, but never split.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a partition function that first creates 60 separate buckets for samples.
    Then, some fraction of the buckets are allocated for testing, the rest for training.
    Specifically, 12, 15, or 20 buckets out of 60 are about 20%, 25%, or 33% of the
    population. Here''s an implementation that deduplicates as it partitions into
    testing and training subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'There are three steps in this partitioning:'
  prefs: []
  type: TYPE_NORMAL
- en: We create sixty separate lists of samples that – because the hashes are equal
    – may have duplicates. We keep these batches together to avoid splitting duplicates
    so they're in both testing and training subsets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We build two lists of iterators. Each list has an iterator over a subset of
    the buckets. The `training_rule()` function is used to make sure we get 12/60,
    15/60, or 20/60 buckets in testing, and the rest in training. Since each of these
    iterators is lazy, these lists of iterators can be used to accumulate samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we use the `itertools.chain` to consume values from a sequence of generators.
    A chain of iterators will consume the items from each of the various individual
    bucket-level iterators to create the two final partitions of samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that the type hint for `ModuloDict` defines a subtype of the generic `DefaultDict`.
    It provides a key of `int` and the value will be a `list[KnownSample]` instances.
    We've provided this named type to avoid repeating the long definition of the dictionaries
    we'll be working with.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `itertools.chain()` is a pretty clever kind of iterator. It consumes data
    from other iterators. Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We created two `range()` objects, `p1`, and `p2`. A chain object will be an
    iterator, and we used the `list()` function to consume all the values.
  prefs: []
  type: TYPE_NORMAL
- en: The steps above can create a large mapping as an intermediate data structure.
    It also creates sixty generators, but these don't require very much memory. The
    final two lists contain references to the same `Sample` objects as the partitioning
    dictionary. The good news is the mapping is temporary and only exists during this
    function.
  prefs: []
  type: TYPE_NORMAL
- en: This function also depends on a `training_rule()` function. This function has
    a type hint of `Callable[[int], bool]`. Given the index value for a partition
    (a value from 0 to 59, inclusive), we can assign it to a testing or training partition.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use different implementations to get to 80%, 75%, or 66% testing data.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The above lambda object will perform a 75% training and 25% testing split.
  prefs: []
  type: TYPE_NORMAL
- en: Once we've partitioned the data, we can use iterators for classifying samples
    as well as testing the quality of our classification process.
  prefs: []
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The testing process can also be defined as a higher-order function, a function
    that accepts a function as a parameter value. We can summarize the testing effort
    as a map-reduce problem. Given a `Hyperparameter` with a *k* value and a distance
    algorithm, we need to use an iterator for the following two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: A function classifies all test samples, mapping each test sample to 1 if the
    classification was correct or 0 for an incorrect classification. This is the map
    part of map-reduce.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function computes a summary with the count of correct values from the long
    sequence of actual classified samples. This is the reduce part of map-reduce.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python provides high-level functions for these map and reduce operations. This
    allows us to focus on the details of the mapping and ignore the boilerplate part
    of iterating over the data items.
  prefs: []
  type: TYPE_NORMAL
- en: Looking forward to the next section, we'll want to refactor the `Hyperparameter`
    class to split the classifier algorithm into a separate, standalone function.
    We'll make the classifier function a **Strategy** that we provide when we create
    an instance of the `Hyperparameter` class. Doing this means we can more easily
    experiment with some alternatives. We'll look at three different ways to approach
    refactoring a class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s one definition that relies on an external classifier function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The `test()` method uses two mapping operations and a reduce operation. First,
    we define a generator that will map each testing sample to a `ClassifiedKnownSample`
    object. This object has the original sample and the results of the classification.
  prefs: []
  type: TYPE_NORMAL
- en: Second, we define a generator that will map each `ClassifiedKnownSample` object
    to a 1 (for a test that matched the expected species) or a 0 (for a test that
    failed). This generator depends on the first generator to provide values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The actual work is the summation: this consumes values from the second generator.
    The second generator consumes objects from the first generator. This technique
    can minimize the volume of data in memory at any one time. It also decomposes
    a complex algorithm into two separate steps, allowing us to make changes as necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: There's an optimization available here, also. The value of `t.classification` in
    the second generator is `self.classify(t.sample.sample)`. It's possible to reduce
    this to a single generator and eliminate creating intermediate `ClassifiedKnownSample`
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how the test operation looks. We can build a `Hyperparameter` instance
    using a function for distance, `manhattan()`, and a classifier function, `k_nn_1()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We'll look at the implementations of various classifiers in the next two sections.
    We'll start with the base definition, `k_nn_1()`, and then look at one based on
    the `bisect` module next.
  prefs: []
  type: TYPE_NORMAL
- en: The essential k-NN algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can summarize the *k*-NN algorithm as having the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a list of all (distance, training sample) pairs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort these in ascending order.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pick to the first *k*, which will be the *k* nearest neighbors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chose the mode (the highest frequency) label for the *k* nearest neighbors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The implementation would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: While clear, this does accumulate a large number of distance values in the `distances`
    list object, when only *k* are actually needed. The `sorted()` function consumes
    the source generator and creates a (potentially large) list of intermediate values.
  prefs: []
  type: TYPE_NORMAL
- en: One of the high-cost parts of this specific *k*-NN algorithm is sorting the
    entire set of training data after the distances have been computed. We summarize
    the complexity with the description as an *O(n log n)* operation. A way to avoid
    cost is to avoid sorting the entire set of distance computations.
  prefs: []
  type: TYPE_NORMAL
- en: '*Steps 1* to *3* can be optimized to keep only the *k* smallest distance values.
    We can do this by using the `bisect` module to locate the position in a sorted
    list where a new value can be inserted. If we only keep values that are smaller
    than the *k* values in the list, we can avoid a lengthy sort.'
  prefs: []
  type: TYPE_NORMAL
- en: k-NN using the bisect module
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here''s an alternative implementation of *k*-NN that tries to avoid sorting
    all of the distance computations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each training sample:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the distance from this training sample to the unknown sample.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If it's greater than the last of the *k* nearest neighbors seen so far, discard
    the new distance.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Otherwise, find a spot among the *k* values; insert the new item; truncate the
    list to length *k*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the frequencies of result values among the *k* nearest neighbors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the mode (the highest frequency) among the *k* nearest neighbors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If we seed the list of *k* nearest neighbors with floating point infinity values,
    ![](img/B17070_10_011.png) to mathematicians, `float("inf")` in Python, then the
    first few computed distances, *d*, will be kept because ![](img/B17070_10_012.png).
    After the first *k* distances have been computed, the remaining distances must
    be smaller than one of the *k* neighbor''s distances to be relevant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of sorting all distances into a big list, we''re inserting (and removing)
    one distance from a much smaller list. After the first *k* distances are computed,
    this algorithm involves two kinds of state change: a new item is inserted into
    the *k* nearest neighbors, and the most distant of the *k+1* neighbors is removed.
    While this doesn''t change the overall complexity in a dramatic way, these are
    relatively inexpensive operations when performed on a very small list of only
    *k* items.'
  prefs: []
  type: TYPE_NORMAL
- en: k-NN using the heapq module
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have yet another trick up our sleeve. We can use the `heapq` module to maintain
    a sorted list of items. This lets us implement the sorting operation as each item
    is placed into the overall list. This doesn't reduce the general complexity of
    the processing, but it replaces two inexpensive insert and pop operations with *potentially* less
    expensive insert operations.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to start with an empty list and insert items into the list, ensuring
    that (a) the items are kept in order, and (b) the item at the head of the list
    always has the least distance. The heap queue algorithm can maintain an upper
    bound on the size of the queue. Keeping only *k* items should also reduce the
    volume of data required in memory.
  prefs: []
  type: TYPE_NORMAL
- en: We can then pop *k* items from the heap to retrieve the nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This is elegantly simple. It's not, however, remarkably fast. It turns out that
    the cost of computing the distances outweighs the cost savings from using a more
    advanced heap queue to reduce the number of items being sorted.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can compare these distinct *k*-NN algorithms by providing a consistent set
    of training and test data. We''ll use a function like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: We've created a consistent `Hyperparameter` instance. Each instance has a common
    value of *k* and a common distance function; they have a distinct classifier algorithm.
    We can execute the `test()` method and display the time required.
  prefs: []
  type: TYPE_NORMAL
- en: 'A `main()` function can use this to examine the various classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We've applied each of the classifiers to a consistent set of data. We haven't
    shown the `a_lot_of_data()` function. This creates two lists of `TrainingKnownSample`
    and `TestingKnownSample` instances. We've left this as an exercise for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the performance results comparing these alterative *k*-NN algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '| algorithm | test quality | time |'
  prefs: []
  type: TYPE_TB
- en: '| k_nn_1 | q= 241/ 1000 | 6.553s |'
  prefs: []
  type: TYPE_TB
- en: '| k_nn_b | q= 241/ 1000 | 3.992s |'
  prefs: []
  type: TYPE_TB
- en: '| k_nn_q | q= 241/ 1000 | 5.294s |'
  prefs: []
  type: TYPE_TB
- en: The test quality is the number of correct test cases. The number is low because
    the data is completely random, and a correct classification rate of about 25%
    is what's expected if our random data uses four different species names.
  prefs: []
  type: TYPE_NORMAL
- en: The original algorithm, `k_nn_1`, is the slowest, something we suspected. This
    provides the necessary evidence that optimization of this may be necessary. The `bisect` based
    processing, row `k_nn_b` in the table, suggests that working with a small list
    outweighs the costs of performing the bisect operation many times. The `heapq` processing
    time, row `k_nn_h`, was better than the original algorithm, but only by about
    20%.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to do both a theoretical analysis of the algorithm's complexity
    as well as a benchmark with actual data. Before spending time and effort on performance
    improvement, we need to start with benchmark analysis to identify where we might
    be able to do things more efficiently. It's also important to confirm that the
    processing is correct before trying to optimize performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In some cases, we''ll need detailed analysis of specific functions or even
    Python operators. The `timeit` module can be helpful here. We might need to do
    something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The value computed for `m` can help us make a concrete comparison between distance
    computations. The `timeit` module will execute the given statement, `manhattan(d1,
    d2)`, after performing the one-time setup of some imports and creation of sample
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Iterators are both a performance boost and a potential way to clarify the overall
    design. They can be helpful with our case study because so much of the processing
    iterates over large collections of data.
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter looked at a design pattern that seems ubiquitous in Python, the
    iterator. The Python iterator concept is a foundation of the language and is used
    widely. In this chapter we examined a number of aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: Design patterns are good ideas we see repeated in software implementations,
    designs, and architectures. A good design pattern has a name, and a context where
    it's usable. Because it's only a pattern, not reusable code, the implementation
    details will vary each time the pattern is followed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Iterator` protocol is one of the most powerful design patterns because
    it provides a consistent way to work with data collections. We can view strings,
    tuples, lists, sets, and even files as iterable collections. A mapping contains
    a number of iterable collections including the keys, the values, and the items
    (key and value pairs.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List, set, and dictionary comprehensions are short, pithy summaries of how to
    create a new collection from an existing collection. They involve a source iterable,
    an optional filter, and a final expression to define the objects in the new collection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generator functions build on other patterns. They let us define iterable objects
    that have map and filter capabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you don't use comprehensions in your daily coding very often, the first thing
    you should do is search through some existing code and find some `for` loops.
    See whether any of them can be trivially converted to a generator expression or
    a list, set, or dictionary comprehension.
  prefs: []
  type: TYPE_NORMAL
- en: Test the claim that list comprehensions are faster than `for` loops. This can
    be done with the built-in `timeit` module. Use the help documentation for the
    `timeit.timeit` function to find out how to use it. Basically, write two functions
    that do the same thing, one using a list comprehension, and one using a `for` loop
    to iterate over several thousand items. Pass each function into `timeit.timeit`,
    and compare the results. If you're feeling adventurous, compare generators and
    generator expressions as well. Testing code using `timeit` can become addictive,
    so bear in mind that code does not need to be hyperfast unless it's being executed
    an immense number of times, such as on a huge input list or file.
  prefs: []
  type: TYPE_NORMAL
- en: Play around with generator functions. Start with basic iterators that require
    multiple values (mathematical sequences are canonical examples; the Fibonacci
    sequence is overused if you can't think of anything better). Try some more advanced
    generators that do things such as take multiple input lists and somehow yield
    values that merge them. Generators can also be used on files; can you write a
    simple generator that shows lines that are identical in two files?
  prefs: []
  type: TYPE_NORMAL
- en: Extend the log processing exercise to replace the `WARNING` filter with a time
    range filter; all the messages between Jan 26, 2015 11:25:46 and Jan 26, 2015
    11:26:15, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Once you can find `WARNING` lines or lines within a specific time, combine the
    two filters to select only the warnings within the given time. You can use an
    `and` condition within a single generator, or combine multiple generators, in
    effect building an `and` condition. Which seems more adaptable to changing requirements?
  prefs: []
  type: TYPE_NORMAL
- en: When we presented the class `WarningReformat(Iterator[Tuple[str, ...]]):` example
    of an iterator, we made a questionable design decision. The `__init__()` method
    accepted an open file as an argument value and the `__next__()` method used `readline()`
    on that file. What if we change this slightly and create an explicit iterator
    object that we use inside another iterator?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: If we make this change, then `__next__()` can use `line = next(self.insequence)`
    instead of `line = self.insequence.readline()`. Switching from `object.readline()`
    to `next(object)` is an interesting generalization. Does it change anything about
    the `extract_and_parse_2()` function? Does it permit us to use generator expressions
    along with the `WarningReformat` iterator?
  prefs: []
  type: TYPE_NORMAL
- en: Take this one further step. Refactor the `WarningReformat` class into two separate
    classes, one to filter for `WARN` and a separate class to parse and reformat each
    line of the input log. Rewrite the `extract_and_parse_2()` function using instances
    of these two classes. Which is "better"? What metric did you use to evaluate "better"?
  prefs: []
  type: TYPE_NORMAL
- en: The case study summarized the *k*-NN algorithm as a kind of comprehension to
    compute distance values, sort and pick the *k* nearest. The case study didn't
    talk much about the partitioning algorithm to separate training data from test
    data. This, too, seems like it might work out as a pair of list comprehensions.
    There's an interesting problem here, though. We'd like to create two lists, reading
    the source exactly once. This isn't easily done with list comprehensions. However,
    look at the `itertools` module for some possible designs. Specifically, the `itertools.tee()`
    function will provide multiple iterables from a single source.
  prefs: []
  type: TYPE_NORMAL
- en: Look at the recipes section of the `itertools` module. How can the `itertools.partition()`
    function be used to partition data?
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned that design patterns are useful abstractions that
    provide best-practice solutions for common programming problems. We covered our
    first design pattern, the iterator, as well as numerous ways that Python uses
    and abuses this pattern for its own nefarious purposes. The original iterator
    pattern is extremely object-oriented, but it is also rather ugly and verbose to
    code around. However, Python's built-in syntax abstracts the ugliness away, leaving
    us with a clean interface to these object-oriented constructs.
  prefs: []
  type: TYPE_NORMAL
- en: Comprehensions and generator expressions can combine container construction
    with iteration in a single line. Generator functions can be constructed using
    the `yield` syntax.
  prefs: []
  type: TYPE_NORMAL
- en: We'll cover several more design patterns in the next two chapters.
  prefs: []
  type: TYPE_NORMAL
