- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: The Iterator Pattern
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迭代器模式
- en: 'We''ve discussed how many of Python''s built-ins and idioms seem, at first
    blush, to fly in the face of object-oriented principles, but are actually providing
    access to real objects under the hood. In this chapter, we''ll discuss how the `for` loop,
    which seems so structured, is actually a lightweight wrapper around a set of object-oriented
    principles. We''ll also see a variety of extensions to this syntax that automatically
    create even more types of object. We will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了Python的许多内置函数和惯用法乍一看似乎与面向对象原则相悖，但实际上它们在底层提供了对真实对象的访问。在本章中，我们将讨论看似结构化的`for`循环实际上是如何围绕一组面向对象原则进行轻量级封装的。我们还将看到对这个语法的各种扩展，这些扩展可以自动创建更多类型的对象。我们将涵盖以下主题：
- en: What design patterns are
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是设计模式
- en: The iterator protocol – one of the most powerful design patterns
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代协议 – 最强大的设计模式之一
- en: List, set, and dictionary comprehensions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列表、集合和字典推导式
- en: Generator functions, and how they build on other patterns
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器函数，以及它们如何建立在其他模式之上
- en: The case study for this chapter will revisit the algorithms for partitioning
    sample data into testing and training subsets to see how the iterator design pattern
    applies to this part of the problem.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的案例研究将重新审视将样本数据划分为测试集和训练集的算法，以了解迭代器设计模式如何应用于该问题的这一部分。
- en: We'll start with an overview of what design patterns are and why they're so
    important.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先概述什么是设计模式以及为什么它们如此重要。
- en: Design patterns in brief
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简要设计模式
- en: When engineers and architects decide to build a bridge, or a tower, or a building,
    they follow certain principles to ensure structural integrity. There are various
    possible designs for bridges (suspension and cantilever, for example), but if
    the engineer doesn't use one of the standard designs, and doesn't have a brilliant
    new design, it is likely the bridge they design will collapse.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当工程师和建筑师决定建造一座桥梁、一座塔楼或一栋建筑时，他们会遵循某些原则以确保结构完整。桥梁（例如，悬索桥和悬臂桥）有各种可能的设计，但如果工程师不使用标准设计，也没有一个出色的全新设计，那么他们设计的桥梁很可能会倒塌。
- en: Design patterns are an attempt to bring this same formal definition for correctly
    designed structures to software engineering. There are many different design patterns
    to solve different general problems. Design patterns are applied to solve a common
    problem faced by developers in some specific situation. The design pattern is
    a suggestion as to the ideal solution for that problem, in terms of object-oriented
    design. What's central to a pattern is that it is reused often in unique contexts.
    One clever solution is a good idea. Two similar solutions might be a coincidence.
    Three or more reuses of an idea and it starts to look like a repeating pattern.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 设计模式是试图将这种相同的正式定义应用于正确设计的结构，并将其引入软件工程。存在许多不同的设计模式来解决不同的通用问题。设计模式被应用于解决开发者在某些特定情况下面临的一些常见问题。设计模式是对该问题的理想解决方案的建议，从面向对象设计的角度出发。一个模式的核心在于它在独特的环境中经常被重用。一个巧妙的解决方案是一个好主意。两个相似解决方案可能是巧合。一个想法被重复使用三次或更多，它开始看起来像是一个重复的模式。
- en: Knowing design patterns and choosing to use them in our software does not, however,
    guarantee that we are creating a *correct* solution. In 1907, the Québec Bridge (to
    this day, the longest cantilever bridge in the world, just short of a kilometer
    long) collapsed before construction was completed, because the engineers who designed
    it grossly underestimated the weight of the steel used to construct it. Similarly,
    in software development, we may incorrectly choose or apply a design pattern,
    and create software that *collapses* under normal operating situations or when stressed
    beyond its original design limits.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，了解设计模式并选择在软件中使用它们，并不能保证我们正在创建一个 *正确* 的解决方案。在1907年，魁北克桥（时至今日，世界上跨度最长的悬臂桥，长度接近一公里）在建设完成前倒塌，因为设计它的工程师们极大地低估了用于建造它的钢材重量。同样，在软件开发中，我们可能会错误地选择或应用设计模式，从而创建出在正常操作情况下或超出原始设计极限时 *崩溃* 的软件。
- en: Any one design pattern proposes a set of objects interacting in a specific way
    to solve a general problem. The job of the programmer is to recognize when they
    are facing a specific version of such a problem, then to choose and adapt the
    general pattern to their precise needs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 任何一种设计模式都提出了一组以特定方式相互作用的对象来解决一个普遍问题。程序员的任务是识别他们何时面临这种问题的特定版本，然后选择并调整通用模式以满足他们的具体需求。
- en: In this chapter, we'll look deeply at the iterator design pattern. This pattern
    is so powerful and pervasive that the Python developers have provided multiple
    syntaxes to access the object-oriented principles underlying the pattern. We will
    be covering other design patterns in the next two chapters. Some of them have
    language support and some don't, but none of them are so intrinsically a part
    of the Python coder's daily life as the iterator pattern.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨迭代器设计模式。这个模式非常强大且普遍，以至于Python开发者提供了多种语法来访问模式背后的面向对象原则。我们将在接下来的两章中介绍其他设计模式。其中一些有语言支持，而另一些则没有，但没有任何一个模式像迭代器模式那样，与Python程序员日常生活的内在联系如此紧密。
- en: Iterators
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迭代器
- en: 'In typical design pattern parlance, an **iterator** is an object with a `next()` method
    and a `done()` method; the latter returns `True` if there are no items left in
    the sequence. In a programming language without built-in support for iterators,
    the iterator would be used like this:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的设计模式术语中，一个**迭代器**是一个具有`next()`方法和`done()`方法的对象；后者在序列中没有剩余项时返回`True`。在没有内置迭代器支持的编程语言中，迭代器会被这样使用：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In Python, iteration is available across many language features, so the method
    gets a special name, `__next__`. This method can be accessed using the `next(iterator)` built-in.
    Rather than a `done()` method, Python's iterator protocol raises the `StopIteration`
    exception to notify the client that the iterator has completed. Finally, we have
    the much more readable `for item in iterator:` syntax to actually access items
    in an iterator instead of messing around with a `while` statement. Let's look
    at each these in more detail.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，迭代可以在许多语言特性中使用，因此该方法有一个特殊的名称，`__next__`。这个方法可以通过内置的`next(iterator)`来访问。而不是使用`done()`方法，Python的迭代器协议通过抛出`StopIteration`异常来通知客户端迭代器已经完成。最后，我们还有更易读的`for
    item in iterator:`语法，实际上用来访问迭代器中的项目，而不是与`while`语句纠缠。让我们更详细地看看这些内容。
- en: The iterator protocol
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迭代协议
- en: The `Iterator` abstract base class, in the `collections.abc` module, defines
    the *iterator* protocol in Python. This definition is also referenced by the `typing`
    module to provide suitable type hints. At the foundation, any `Collection` class
    definition must be `Iterable`. To be `Iterable` means implementing an `__iter__()`
    method; this method creates an `Iterator` object.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`Iterator` 抽象基类，位于 `collections.abc` 模块中，定义了 Python 中的 *迭代器* 协议。此定义也被 `typing`
    模块引用，以提供合适的类型提示。在基础层面，任何 `Collection` 类定义都必须是 `Iterable`。要成为 `Iterable`，意味着实现一个
    `__iter__()` 方法；此方法创建一个 `Iterator` 对象。'
- en: '![Diagram  Description automatically generated](img/B17070_10_01.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B17070_10_01.png)'
- en: 'Figure 10.1: The abstractions for Iterable'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1：Iterable的抽象
- en: As mentioned, an `Iterator` class must define a `__next__()` method that the `for` statement
    (and other features that support iteration) can call to get a new element from
    the sequence. In addition, every `Iterator` class must also fulfill the `Iterable` interface.
    This means an `Iterator` will also provide an `__iter__()` method.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，一个`Iterator`类必须定义一个`__next__()`方法，该方法是`for`语句（以及其他支持迭代的特性）可以调用来从序列中获取新元素的方法。此外，每个`Iterator`类还必须实现`Iterable`接口。这意味着`Iterator`也将提供一个`__iter__()`方法。
- en: 'This might sound a bit confusing, so have a look at the following example.
    Note that this is a very verbose way to solve this problem. It explains iteration
    and the two protocols in question, but we''ll be looking at several more readable
    ways to get this effect later in this chapter:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能听起来有些令人困惑，所以请看一下下面的例子。请注意，这是一种非常冗长的解决问题的方式。它解释了迭代和所涉及的两个协议，但我们在本章后面将探讨几种更易读的方式来实现这一效果：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This example defines a `CapitalIterable` class whose job is to loop over each
    of the words in a string and output them with the first letter capitalized. We
    formalized this by using the `Iterable[str]` type hint as a superclass to make
    it clear what our intention was. Most of the work of this iterable class is delegated
    to the `CapitalIterator` implementation. One way to interact with this iterator
    is as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例定义了一个`CapitalIterable`类，其任务是遍历字符串中的每个单词，并将它们以首字母大写的方式输出。我们通过使用`Iterable[str]`类型提示作为超类来形式化这一点，以明确我们的意图。这个可迭代类的大部分工作都委托给了`CapitalIterator`实现。与这个迭代器交互的一种方式如下：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This example first constructs an iterable, assigning it to a variable with the
    boringly obvious name of `iterable`. It then retrieves a `CapitalIterator` instance
    from the `iterable` object. The distinction may need explanation; the iterable
    is an object with elements that can be iterated over. Normally, these elements
    can be looped over multiple times, maybe even at the same time or in overlapping
    code. The iterator, on the other hand, represents a specific location in that
    iterable; some of the items have been consumed and some have not. Two different
    iterators might be at different places in the list of words, but any one iterator
    can mark only one place.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子首先构建了一个可迭代对象，并将其赋值给一个名字平淡无奇的变量`iterable`。然后从`iterable`对象中检索出一个`CapitalIterator`实例。这种区别可能需要解释；可迭代对象是一个具有可迭代元素的对象。通常，这些元素可以被多次循环遍历，甚至可能同时或重叠地进行。另一方面，迭代器代表可迭代对象中的特定位置；一些项目已经被消费，而一些还没有。两个不同的迭代器可能在单词列表的不同位置，但任何一个迭代器只能标记一个位置。
- en: Each time `next()` is called on the iterator, it returns another token from
    the iterable, in order, and updates its internal state to point to the next item.
    Eventually, the iterator will be exhausted (won't have any more elements to return),
    in which case a `StopIteration` exception is raised, and we break out of the `while`
    statement.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 每次在迭代器上调用`next()`时，它都会按顺序返回可迭代对象中的另一个标记，并更新其内部状态以指向下一个项目。最终，迭代器将耗尽（没有更多元素可以返回），在这种情况下，将引发`StopIteration`异常，然后我们跳出`while`语句。
- en: 'Python has a simpler syntax for constructing an iterator from an iterable:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Python 从可迭代对象构造迭代器的语法更简单：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see, the `for` statement, in spite of not looking remotely object-oriented,
    is actually a shortcut to some fundamentally object-oriented design principles.
    Keep this in mind as we discuss comprehensions, as they, too, appear to be the
    polar opposite of an object-oriented tool. Yet, they use the same iteration protocol
    as `for` statements and are another kind of shortcut.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，尽管`for`语句看起来与面向对象毫无关联，但实际上它是通往一些基本面向对象设计原则的捷径。在我们讨论生成器表达式时，请记住这一点，因为它们似乎也是面向对象工具的对立面。然而，它们与`for`语句使用相同的迭代协议，并且是另一种快捷方式。
- en: 'The number of iterable classes in Python is large. We''re not surprised when
    strings, tuples, and lists are iterable. A set, clearly, must be iterable, even
    if the order of elements may be difficult to predict. A mapping will iterate over
    the keys by default; other iterators are available. A file iterates over the available
    lines. A regular expression has a method, `finditer()`, that is an iterator over
    each instance of a matching substring that it can find. The `Path.glob()` method
    will iterate over matching items in a directory. The `range()` object is also
    an iterator. You get the idea: anything even vaguely collection-like will support
    some kind of iterator.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Python中可迭代的类数量很大。当字符串、元组和列表是可迭代的时，我们并不感到惊讶。显然，集合也必须是可迭代的，即使元素的顺序可能难以预测。映射默认会遍历键；其他迭代器也是可用的。文件会遍历可用的行。正则表达式有一个`finditer()`方法，它是一个迭代器，遍历它可以找到的每个匹配子串的实例。`Path.glob()`方法会遍历目录中的匹配项。`range()`对象也是一个迭代器。你明白了：任何稍微有点集合样式的对象都将支持某种类型的迭代器。
- en: Comprehensions
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解
- en: Comprehensions are simple, but powerful, syntaxes that allow us to transform
    or filter an iterable object in as little as one line of code. The resultant object
    can be a perfectly normal list, set, or dictionary, or it can be a *generator
    expression* that can be efficiently consumed while keeping just one element in
    memory at a time.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 理解是简单但强大的语法，它允许我们用一行代码将可迭代对象进行转换或过滤。结果对象可以是一个完全正常的列表、集合或字典，也可以是一个*生成器表达式*，在保持每次只存储一个元素的同时，可以高效地消费。
- en: List comprehensions
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 列推导式
- en: List comprehensions are one of the most powerful tools in Python, so people
    tend to think of them as advanced. They're not. Indeed, we've taken the liberty
    of littering previous examples with comprehensions, assuming you would understand
    them. While it's true that advanced programmers use comprehensions a lot, it's
    not because they're advanced. It's because a comprehension is so fundamental to
    Python, it can handle many of the most common operations in application software.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 列表推导是 Python 中最强大的工具之一，因此人们往往认为它们是高级的。其实并非如此。实际上，我们在之前的例子中已经自由地使用了推导，假设你们能够理解它们。虽然高级程序员确实大量使用推导，但这并不是因为它们高级。而是因为推导在
    Python 中非常基础，它可以处理应用软件中许多最常见的操作。
- en: 'Let''s have a look at one of those common operations; namely, converting a
    list of items into a list of related items. Specifically, let''s assume we just
    read a list of strings from a file, and now we want to convert it to a list of
    integers. We know every item in the list is an integer, and we want to do some
    activity (say, calculate an average) on those numbers. Here''s one simple way
    to approach it:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些常见操作之一；即，将一个项目列表转换为相关项目列表。具体来说，假设我们刚刚从一个文件中读取了一个字符串列表，现在我们想要将其转换为整数列表。我们知道列表中的每个项目都是一个整数，并且我们想要对这些数字进行一些活动（比如，计算平均值）。这里有实现这一目标的一种简单方法：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This works fine and it''s only three lines of code. If you aren''t used to
    comprehensions, you may not even think it looks ugly! Now, look at the same code
    using a list comprehension:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这工作得很好，而且只有三行代码。如果你不习惯使用列表推导式，你可能甚至觉得它看起来不丑！现在，看看使用列表推导式的相同代码：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We're down to one line and, importantly for performance, we've dropped an `append`
    method call for each item in the list. Overall, it's pretty easy to tell what's
    going on, even if you're not used to comprehension syntax.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只剩下一行代码，并且对于性能来说，我们为列表中的每个项目去掉了`append`方法的调用。总体来说，即使你不习惯理解这种语法，也很容易看出发生了什么。
- en: The square brackets indicate, as always, that we're creating a list. Inside
    this list is a `for` clause that iterates over each item in the input sequence.
    The only thing that may be confusing is what's happening between the list's opening
    brace and the start of the `for` statement. Whatever expression is provided here
    is applied to *each* of the items in the input list. The item in question is referenced
    by the `num` variable from the `for` clause. So, this expression applies the `int` function
    to each element and stores the resulting integer in the new list.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 方括号始终表示我们正在创建一个列表。在这个列表内部有一个`for`循环，它会遍历输入序列中的每个项目。可能让人感到困惑的是列表开括号和`for`语句开始之间的内容。这里提供的任何表达式都会应用于输入列表中的*每个*项目。所讨论的项目通过`for`循环中的`num`变量来引用。因此，这个表达式将`int`函数应用于每个元素，并将结果整数存储在新的列表中。
- en: Terminology-wise, we call this a **mapping**. We are applying the result expression,
    `int(num)` in this example, to map values from the source iterable to create a
    resulting iterable list.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从术语上讲，我们称这为**映射**。在这个例子中，我们应用结果表达式`int(num)`，将源可迭代对象中的值映射到创建的结果可迭代列表中。
- en: That's all there is to a basic list comprehension. Comprehensions are highly
    optimized, making them far faster than `for` statements when processing a large
    number of items. When used wisely, they're also more readable. These are two compelling
    reasons to use them widely.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 基本列表推导式的全部内容就是这些。推导式经过高度优化，在处理大量项目时比`for`语句要快得多。当明智地使用时，它们也更易于阅读。这就是广泛使用它们的两个强有力的理由。
- en: 'Converting one list of items into a related list isn''t the only thing we can
    do with a list comprehension. We can also choose to exclude certain values by
    adding an `if` statement inside the comprehension. We call this a **filter**.
    Have a look:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个项目列表转换为相关列表并不是列表推导所能做的唯一事情。我们还可以选择通过在推导中添加一个`if`语句来排除某些值。我们称这为**过滤器**。看看这个例子：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The essential difference between this example and the previous one is the `if
    len(num) < 3` clause. This extra code excludes any strings with more than two
    characters. The `if` clause is applied to each element **before** the final `int()` function,
    so it's testing the length of a string. Since our input strings are all integers
    at heart, it excludes any number over 99.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一个示例相比，本质区别在于 `if len(num) < 3` 这条语句。这段额外的代码排除了任何超过两个字符的字符串。`if` 语句应用于每个元素**在**最终
    `int()` 函数之前，因此它是在测试字符串的长度。由于我们的输入字符串本质上都是整数，它排除了任何大于99的数字。
- en: A list comprehension can be used to map input values to output values, applying
    a filter along the way to include or exclude any values that meet a specific condition.
    A great many algorithms involve mapping and filtering operations.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 列推导式可以将输入值映射到输出值，同时在过程中应用过滤器以包含或排除满足特定条件的任何值。许多算法都涉及映射和过滤操作。
- en: Any iterable can be the input to a list comprehension. In other words, anything
    we can wrap in a `for` statement can also be used as the source for a comprehension.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 任何可迭代的对象都可以作为列表推导式的输入。换句话说，我们可以用`for`循环包裹的任何东西也可以用作推导式的来源。
- en: 'For example, text files are iterable; each call to `__next__()` on the file''s
    iterator will return one line of the file. We can examine the lines of a text
    file by naming the open file in the `for` clause of a list comprehension. We can
    then use the `if` clause to extract interesting lines of text. This example finds
    a subset of lines in a test file:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，文本文件是可迭代的；对文件迭代器调用`__next__()`将返回文件的一行。我们可以通过在列表推导式的`for`子句中指定打开的文件来检查文本文件的行。然后，我们可以使用`if`子句来提取有趣的文本行。以下示例在测试文件中找到行的一个子集：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this example, we've added some whitespace to make the comprehension more
    readable (list comprehensions don't *have* to fit on one physical line even though
    they're one logical line). This example creates a list of lines that have the
    "`>>>`" prompt in them. The presence of "`>>>`" suggests there might be a doctest
    example in this file. The list of lines has `rstrip()` applied to remove trailing
    whitespace, like the `\n` that ends each line of text returned by the iterator.
    The resulting list object, `examples`, suggests some of the test cases that can
    be found within the code. (This isn't as clever as doctest's own parser.)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们添加了一些空白来使理解更加易读（列表推导式**不必**适应一行物理空间，即使它们是逻辑上的一行）。此示例创建了一个包含"`>>>`"提示符的行列表。"`>>>`"的存在表明这个文件中可能有一个doctest示例。对行列表应用了`rstrip()`方法，以移除尾随空白，例如迭代器返回的每行文本末尾的`\n`。结果列表对象`examples`暗示了一些可以在代码中找到的测试用例。（这并不像doctest自己的解析器那样聪明。）
- en: 'Let''s extend this example to capture the line numbers for each example with
    a "`>>>`" prompt in it. This is a common requirement, and the built-in `enumerate()`
    function helps us pair a number with each item provided by the iterator:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个例子扩展一下，以便捕获包含"`>>>`"提示符的每个示例的行号。这是一个常见的需求，内置的`enumerate()`函数帮助我们将一个数字与迭代器提供的每个项目配对：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `enumerate()` function consumes an iterable, providing an iterable sequence
    of two-tuples of a number and the original item. If the line passes our "`>>>`"
    test, we'll create a two-tuple of the number and the cleaned-up text. We've done
    some sophisticated processing in – effectively – one line of code. Essentially,
    though, it's a filter and a mapping. First it extracts tuples from the source,
    then it filters the lines that match the given `if` clause, then it evaluates
    the `(number, line.rstrip())` expression to create resulting tuples, and finally,
    collects it all into a list object. The ubiquity of this iterate-filter-map-collect
    pattern drives the idea behind a list comprehension.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`enumerate()` 函数消耗一个可迭代对象，提供一个由数字和原始项组成的元组的可迭代序列。如果该行通过了我们的 "`>>>`" 测试，我们将创建一个包含数字和清理后的文本的两个元组。我们在实际上只使用了一行代码就完成了一些复杂的处理。本质上，它是一个过滤和映射的过程。首先，它从源中提取元组，然后过滤掉与给定
    `if` 子句匹配的行，接着评估 `(number, line.rstrip())` 表达式以创建结果元组，最后将所有内容收集到一个列表对象中。这种迭代-过滤-映射-收集模式的普遍性推动了列表推导背后的思想。'
- en: Set and dictionary comprehensions
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集合和字典推导式
- en: Comprehensions aren't restricted to lists. We can use a similar syntax with
    braces to create sets and dictionaries as well. Let's start with sets. One way
    to create a set is to wrap a list comprehension in the `set()` constructor, which
    converts it to a set. But why waste memory on an intermediate list that gets discarded,
    when we can create a set directly?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 理解并不局限于列表。我们还可以使用类似的语法，用大括号创建集合和字典。让我们从集合开始。创建集合的一种方法是将列表推导式包裹在`set()`构造函数中，将其转换为集合。但为什么要在中间列表上浪费内存，而这个列表最终会被丢弃，当我们可以直接创建一个集合时？
- en: 'Here''s an example that uses a named tuple to model author/title/genre triples,
    and then retrieves a set of all the authors that write in a specific genre:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个使用命名元组来建模作者/标题/类型三元组的示例，然后检索所有在特定类型中写作的作者集合：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We''ve defined a small library of instances of the `Book` class. We can create
    a set from each of these objects by using a set comprehension. It looks a lot
    like a list comprehension, but uses `{}` instead of `[]`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了一个`Book`类实例的小型库。我们可以通过使用集合推导来从这些对象中创建一个集合。它看起来很像列表推导，但使用`{}`而不是`[]`：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The highlighted set comprehension sure is short in comparison to the demo-data
    setup! If we were to use a list comprehension, of course, Terry Pratchett would
    have been listed twice. As it is, the nature of sets removes the duplicates, and
    we end up with the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 与演示数据设置相比，高亮显示的集合理解确实要短得多！如果我们使用列表理解，当然，特里·普拉切特会被列出两次。实际上，集合的性质消除了重复项，我们最终得到以下结果：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that sets don't have a defined ordering, so your output may differ from
    this example. For testing purposes, we'll sometimes set the `PYTHONHASHSEED` environment
    variable to impose an order. This introduces a tiny security vulnerability, so
    it's only suitable for testing.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，集合没有定义的顺序，因此您的输出可能与这个示例不同。出于测试目的，我们有时会将`PYTHONHASHSEED`环境变量设置为强制一个顺序。这引入了一个微小的安全漏洞，因此仅适用于测试。
- en: 'Still using braces, we can introduce a colon to make `key:value` pairs required
    to create a dictionary comprehension. For example, it may be useful to quickly
    look up the author or genre in a dictionary if we know the title. We can use a
    dictionary comprehension to map titles to `books` objects:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然使用花括号，我们可以引入一个冒号来创建必需的 `key:value` 对来构建字典推导式。例如，如果我们知道书名，快速查找作者或类型在字典中可能很有用。我们可以使用字典推导式将书名映射到
    `books` 对象：
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now, we have a dictionary, and can look up books by title using the normal syntax,
    `fantasy_titles['Nightwatch']`. We've created a high-performance index from a
    lower-performance sequence.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个字典，可以使用正常语法通过书名查找书籍，例如`fantasy_titles['Nightwatch']`。我们已经从一个低性能序列创建了一个高性能索引。
- en: In summary, comprehensions are not advanced Python, nor are they features that
    subvert object-oriented programming. They are a more concise syntax for creating
    a list, set, or dictionary from an existing iterable source of data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，列表推导不是高级Python，也不是颠覆面向对象编程的特性。它们是从现有的可迭代数据源创建列表、集合或字典的更简洁语法。
- en: Generator expressions
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成器表达式
- en: Sometimes we want to process a new sequence without pulling a new list, set,
    or dictionary into system memory. If we're iterating over items one at a time,
    and don't actually care about having a complete container (such as a list or dictionary)
    created, a container is a waste of memory. When processing one item at a time,
    we only need the current object available in memory at any one moment. But when
    we create a container, all the objects have to be stored in that container before
    we start processing them.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候我们希望在不对系统内存中拉取新的列表、集合或字典的情况下处理一个新的序列。如果我们逐个迭代项目，并且实际上并不关心创建一个完整的容器（例如列表或字典），那么容器就是内存的浪费。当我们逐个处理项目时，我们只需要在任何时刻内存中可用的当前对象。但是，当我们创建一个容器时，所有对象都必须在开始处理之前存储在该容器中。
- en: 'For example, consider a program that processes log files. A very simple log
    might contain information in this format:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个处理日志文件的程序。一个非常简单的日志可能包含以下格式的信息：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Log files for popular web servers, databases, or email servers can contain many
    gigabytes of data (one of the authors once had to clean nearly two terabytes of
    logs off a misbehaving system). If we want to process each line in the log, we
    can't use a list comprehension; it would create a list containing every line in
    the file. This probably wouldn't fit in RAM and could bring the computer to its
    knees, depending on the operating system.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 流行网络服务器、数据库或电子邮件服务器的日志文件可能包含数GB的数据（其中一位作者曾经不得不从一个行为异常的系统上清理近两TB的日志）。如果我们想处理日志中的每一行，我们不能使用列表推导；它会创建一个包含文件中每一行的列表。这很可能无法适应RAM，并且可能会根据操作系统使计算机瘫痪。
- en: If we used a `for` statement on the log file, we could process one line at a
    time before reading the next one into memory. Wouldn't be nice if we could use
    comprehension syntax to get the same effect?
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在日志文件上使用`for`语句，我们就可以逐行处理，在读取下一行到内存之前。如果我们可以使用理解语法来达到同样的效果，那岂不是很好？
- en: This is where generator expressions come in. They use the same syntax as comprehensions,
    but they don't create a final container object. We call them **lazy**; they reluctantly
    produce values on demand. To create a generator expression, wrap the comprehension
    in `()` instead of `[]` or `{}`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是生成器表达式发挥作用的地方。它们使用与列表推导式相同的语法，但不会创建一个最终的容器对象。我们称它们为**懒加载**；它们会根据需求不情愿地产生值。要创建一个生成器表达式，将推导式用括号`()`而不是方括号`[]`或大括号`{}`括起来。
- en: 'The following code parses a log file in the previously presented format and
    outputs a new log file that contains only the `WARNING` lines:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码解析了之前展示格式的日志文件，并输出一个只包含`WARNING`行的新的日志文件：
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We've opened the `sample.log` file, a file perhaps too large to fit in memory.
    A generator expression will filter out the warnings (in this case, it uses the `if` syntax
    and leaves the line unmodified). This is lazy, and doesn't really do anything
    until we consume its output. We can open another file as a subset. The final `for`
    statement consumes each individual line from the `warning_lines` generator. At
    no time is the full log file read into memory; the processing happens one line
    at a time.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经打开了`sample.log`文件，这个文件可能太大而无法全部装入内存。一个生成器表达式将过滤掉警告（在这种情况下，它使用`if`语法并保留该行未修改）。这是懒加载的，实际上直到我们消费其输出之前并不会做任何事情。我们可以打开另一个文件作为子集。最后的`for`语句消费`warning_lines`生成器中的每一行。在任何时候，完整的日志文件都不会被读入内存；处理是逐行进行的。
- en: 'If we run it on our sample file, the resulting `warnings.log` file looks like
    this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在我们的样本文件上运行它，生成的`warnings.log`文件看起来是这样的：
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Of course, with a short input file, we could have safely used a list comprehension,
    doing all the processing in memory. When the file is millions of lines long, the
    generator expression will have a huge impact on both memory and speed.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，对于一个小型的输入文件，我们可以安全地使用列表推导式，在内存中完成所有处理。当文件有数百万行长时，生成器表达式将对内存和速度产生巨大影响。
- en: The core of a comprehension is the generator expression. Wrapping a generator
    in `[]` creates a list. Wrapping a generator in `{}` creates a set. Using `{}`
    and `:` to separate keys and values creates a dictionary. Wrapping a generator
    in `()` is still a generator expression, not a tuple.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 理解的核心是生成器表达式。将生成器用`[]`括起来创建一个列表。将生成器用`{}`括起来创建一个集合。使用`{}`和`:`分隔键和值创建一个字典。将生成器用`()`括起来仍然是生成器表达式，而不是元组。
- en: Generator expressions are frequently most useful inside function calls. For
    example, we can call `sum`, `min`, or `max` on a generator expression instead
    of a list, since these functions process one object at a time. We're only interested
    in the aggregate result, not any intermediate container.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器表达式通常在函数调用中最为有用。例如，我们可以对生成器表达式调用`sum`、`min`或`max`，而不是列表，因为这些函数一次处理一个对象。我们只对汇总结果感兴趣，而不是任何中间容器。
- en: In general, of the four options, a generator expression should be used whenever
    possible. If we don't actually need a list, set, or dictionary, but simply need
    to filter or apply a mapping to items in a sequence, a generator expression will
    be most efficient. If we need to know the length of a list, or sort the result,
    remove duplicates, or create a dictionary, we'll have to use the comprehension
    syntax and create a resulting collection.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，在四个选项中，应尽可能使用生成器表达式。如果我们实际上不需要列表、集合或字典，而只是需要过滤或对序列中的项目应用映射，生成器表达式将是最有效的。如果我们需要知道列表的长度，或者对结果进行排序、移除重复项或创建字典，我们就必须使用推导式语法并创建一个结果集合。
- en: Generator functions
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器函数
- en: Generator functions embody the essential features of a generator expression,
    which is the generalization of a comprehension. The generator function syntax
    looks even less object-oriented than anything we've seen, but we'll discover that
    once again, it is a syntax shortcut to create a kind of iterator object. It helps
    us build processing following the standard iterator-filter-mapping pattern.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器函数体现了生成器表达式的本质特征，它是理解的一种推广。生成器函数的语法看起来甚至比我们见过的任何东西都更非面向对象，但我们将再次发现，它是一种创建迭代对象类型的语法快捷方式。它帮助我们按照标准的迭代器-过滤-映射模式构建处理过程。
- en: Let's take the log file example a little further. If we want to decompose the
    log into columns, we'll have to do a more significant transformation as part of
    the mapping step. This will involve a regular expression to find the timestamp,
    the severity word, and the message as a whole. We'll look at a number of solutions
    to this problem to show how generators and generator functions can be applied
    to create the objects we want.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步探讨日志文件示例。如果我们想要将日志分解成列，我们将在映射步骤中进行更显著的转换。这需要使用正则表达式来查找时间戳、严重性词以及整个消息。我们将探讨几个解决这个问题的方案，以展示如何应用生成器和生成函数来创建我们想要的对象。
- en: 'Here''s a version, avoiding generator expressions entirely:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个完全避免使用生成器表达式的版本：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We''ve defined a regular expression to match three groups:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个正则表达式来匹配三个组：
- en: The complex date string, `(\w\w\w \d\d, \d\d\d\d \d\d:\d\d:\d\d),` which is
    a generalization of strings like "`Apr 05, 2021 20:04:41`".
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂的日期字符串，`(\w\w\w \d\d, \d\d\d\d \d\d:\d\d:\d\d),` 是像 "`Apr 05, 2021 20:04:41`"
    这样的字符串的泛化。
- en: The severity level, `(\w+)`, which matches a run of letters, digits, or underscores.
    This will match words like INFO and DEBUG.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 严重程度级别，`(\w+)`，匹配一串字母、数字或下划线。这将匹配像INFO和DEBUG这样的单词。
- en: An optional message, `(.*)`, which will collect all characters to the end of
    the line.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个可选的消息，`(.*)`，它将收集到行尾的所有字符。
- en: This pattern is assigned to the `pattern` variable. As an alternative, we could
    also use `split(' ')` to break the line into space-separated words; the first
    four words are the date, the next word is the severity, and all the remaining
    words are the message. This isn't as flexible as defining a regular expression.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此模式被分配给`pattern`变量。作为替代，我们也可以使用`split(' ')`将行分割成空格分隔的单词；前四个单词是日期，下一个单词是严重性，所有剩余的单词是消息。这不如定义正则表达式灵活。
- en: The decomposition of the line into groups involves two steps. First, we apply
    `pattern.match()` to the line of text to create a `Match` object. Then we interrogate
    the `Match` object for the sequence of groups that matched. We have a `cast(Match[str],
    pattern.match(line))` to tell **mypy** that every line will create a `Match` object.
    The type hint for `re.match()` is `Optional[Match]` because it returns a `None`
    when there's no `Match`. We're using `cast()` to make the claim that every line
    will match, and if it doesn't match, we want this function to raise an exception.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 将行分解成组涉及两个步骤。首先，我们对文本行应用 `pattern.match()` 来创建一个 `Match` 对象。然后我们查询 `Match` 对象以获取匹配的组序列。我们使用
    `cast(Match[str], pattern.match(line))` 来告诉 **mypy** 每一行都会创建一个 `Match` 对象。`re.match()`
    的类型提示是 `Optional[Match]`，因为它在没有匹配时返回 `None`。我们使用 `cast()` 来声明每一行都会匹配，如果它没有匹配，我们希望这个函数抛出一个异常。
- en: This deeply nested function seems maintainable, but so many levels of indent
    in so few lines is kind of ugly. More alarmingly, if there is some irregularity
    in the file, and we want to handle the case where the `pattern.match(line)` returns
    `None`, we'd have to include another `if` statement, leading to even deeper levels
    of nesting. Deeply nested conditional processing leads to statements where the
    conditions under which they are executed can be obscure.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这个深度嵌套的函数看起来是可维护的，但在如此少的行数中却有着如此多的缩进级别，看起来有点丑陋。更令人担忧的是，如果文件中存在一些不规则性，而我们又想处理`pattern.match(line)`返回`None`的情况，我们就必须包含另一个`if`语句，从而导致更深层次的嵌套。深度嵌套的条件处理会导致执行它们的条件变得模糊不清。
- en: The reader has to mentally integrate all of the preceding `if` statements to
    work out the condition. This can be a problem with this kind of solution.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 读者必须心理上整合所有前面的`if`语句来计算出条件。这种解决方案可能会出现这样的问题。
- en: 'Now let''s consider a truly object-oriented solution, without any shortcuts:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑一个真正面向对象的解决方案，没有任何捷径：
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We''ve defined a formal `WarningReformat` iterator that emits the three-tuple
    of the date, warning, and message. We''ve used a type hint of `tuple[str, ...]`
    because it matches the output from the `self.pattern.match(line).groups()` expression:
    it''s a sequence of strings, with no constraint on how many will be present. The
    iterator is initialized with a `TextIO` object, something file-like that has a
    `readline()` method.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个正式的 `WarningReformat` 迭代器，它输出日期、警告和消息的三元组。我们使用了类型提示 `tuple[str, ...]`，因为它与
    `self.pattern.match(line).groups()` 表达式的输出相匹配：它是一个字符串序列，没有对存在数量的限制。迭代器使用一个 `TextIO`
    对象初始化，这是一个类似文件的对象，具有 `readline()` 方法。
- en: This `__next__()` method reads lines from the file, discarding any lines that
    are not `WARNING` lines. When we encounter a `WARNING` line, we parse it and return
    the three-tuple of strings.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 `__next__()` 方法从文件中读取行，丢弃任何不是 `WARNING` 行的行。当我们遇到一个 `WARNING` 行时，我们解析它并返回一个包含三个字符串的三元组。
- en: The `extract_and_parse_2()` function uses an instance of the `WarningReformat`
    class in a `for` statement; this will evaluate the `__next__()` method repeatedly
    to process the subsequent `WARNING` line. When we run out of lines, the `WarningReformat`
    class raises a `StopIteration` exception to tell the function statement we're
    finished iterating. It's pretty ugly compared to the other examples, but it's
    also powerful; now that we have a class in our hands, we can do whatever we want
    with it.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`extract_and_parse_2()` 函数在 `for` 循环中使用 `WarningReformat` 类的一个实例；这将反复评估 `__next__()`
    方法以处理后续的 `WARNING` 行。当我们用完行时，`WarningReformat` 类会引发一个 `StopIteration` 异常来告诉函数语句我们已经完成了迭代。与其它示例相比，这看起来相当丑陋，但它也很强大；现在我们手中有了这个类，我们可以用它来做任何我们想做的事情。'
- en: 'With that background behind us, we finally get to see true generators in action.
    This next example does *exactly* the same thing as the previous one: it creates
    an object with a `__next__()` method that raises `StopIteration` when it''s out
    of inputs:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在具备这些背景知识之后，我们终于能够看到真正的生成器在实际中的应用。接下来的这个例子与上一个例子**完全相同**：它创建了一个具有`__next__()`方法的对象，当输入耗尽时，该方法会引发`StopIteration`异常：
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `yield` statement in the `warning_filters()` function is the key to generators.
    When Python sees `yield` in a function, it takes that function and wraps it up
    in an object that follows the `Iterator` protocol, not unlike the class defined
    in our previous example. Think of the `yield` statement as similar to the `return` statement;
    it returns a line. Unlike `return`, however, the function is only suspended. When
    it is called again (via `next()`), it will start where it left off – on the line
    after the `yield` statement – instead of at the beginning of the function. In
    this example, there is no line *a**fter* the `yield` statement, so it jumps to
    the next iteration of the `for` statement. Since the `yield` statement is inside
    an `if` statement, it only yields lines that contain `WARNING`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在`warning_filters()`函数中的`yield`语句是生成器的关键。当Python在函数中看到`yield`时，它会将该函数包装在一个遵循`Iterator`协议的对象中，这与我们之前示例中定义的类类似。将`yield`语句想象成类似于`return`语句；它返回一行。然而，与`return`不同，函数只是被挂起。当它再次被调用（通过`next()`）时，它将从上次离开的地方开始——在`yield`语句之后的行——而不是从函数的开始处。在这个例子中，`yield`语句之后没有行，所以它跳到`for`语句的下一个迭代。由于`yield`语句在`if`语句内部，它只产生包含`WARNING`的行。
- en: 'While it looks like this is just a function looping over the lines, it is actually
    creating a special type of object, a generator object:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然看起来这只是一个遍历行数的函数，但实际上它正在创建一种特殊类型的对象，即生成器对象：
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: All the function does is create and return a generator object. In this example,
    an empty list was provided, and a generator was built. The generator object has
    `__iter__()` and `__next__()` methods on it, just like the one we created from
    a class definition in the previous example. (Using the `dir()` built-in function
    on it will reveal what else is part of a generator.) Whenever the `__next__()`
    method is called, the generator runs the function until it finds a `yield` statement.
    It then suspends execution, retaining the current state, and returning the value
    from `yield`. The next time the `__next__()` method is called, it restores the
    state and picks up execution where it left off.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 所有该函数所做的只是创建并返回一个生成器对象。在这个例子中，提供了一个空列表，并构建了一个生成器。生成器对象上具有`__iter__()`和`__next__()`方法，就像我们在上一个例子中从类定义创建的那样。（使用内置的`dir()`函数可以揭示生成器还包括哪些内容。）每当调用`__next__()`方法时，生成器会运行函数，直到找到`yield`语句。然后它暂停执行，保留当前状态，并返回`yield`的值。下次调用`__next__()`方法时，它会恢复状态，并从上次离开的地方继续执行。
- en: 'This generator function is nearly identical to this generator expression:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这个生成器函数几乎与这个生成器表达式相同：
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can see how these various patterns align. The generator expression has all
    the elements of the statements, slightly compressed, and in a different order:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这些各种模式是如何对齐的。生成器表达式包含了语句的所有元素，略微压缩，并且顺序不同：
- en: '![Diagram  Description automatically generated](img/B17070_10_02.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B17070_10_02.png)'
- en: 'Figure 10.2: Generator functions compared with generator expressions'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2：生成器函数与生成器表达式的比较
- en: A comprehension, then, is a generator wrapped in `[]` or `{}` to create a concrete
    object. In some cases, it can make sense to use `list()`, `set()`, or `dict()`
    as a wrapper around a generator. This is helpful when we're considering replacing
    the generic collection with a customized collection of our own. Changing `list()`
    into `MySpecialContainer()` seems to make the change more apparent.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个理解（comprehension）是一个被`[]`或`{}`包裹的生成器，用于创建一个具体对象。在某些情况下，使用`list()`、`set()`或`dict()`作为生成器的外包装是有意义的。当我们考虑用我们自己的定制集合替换通用集合时，这很有帮助。将`list()`改为`MySpecialContainer()`似乎使这种变化更加明显。
- en: The generator expression has the advantage of being short and appearing right
    where it's needed. The generator function has a name and parameters, meaning it
    can be reused. More importantly, a generator function can have multiple statements
    and more complex processing logic in the cases where statements are needed. One
    common reason for switching from a generator expression to a function is to add
    exception handling.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器表达式具有简洁且直接出现在所需位置的优势。生成器函数有一个名称和参数，这意味着它可以被重用。更重要的是，在需要语句的情况下，生成器函数可以包含多个语句和更复杂的处理逻辑。从生成器表达式切换到函数的一个常见原因是为了添加异常处理。
- en: Yield items from another iterable
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从另一个可迭代对象中产生项目
- en: Often, when we build a generator function, we end up in a situation where we
    want to yield data from another iterable object, possibly a list comprehension
    or generator expression we constructed inside the generator, or perhaps some external
    items that were passed into the function. We'll look at how to do this with the
    `yield from` statement.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当我们构建一个生成器函数时，我们可能会遇到想要从另一个可迭代对象中产生数据的情况，这可能是一个我们在生成器内部构建的列表推导式或生成器表达式，或者可能是传递给函数的一些外部项目。我们将探讨如何使用`yield
    from`语句来实现这一点。
- en: Let's adapt the generator example a bit so that instead of accepting an input
    file, it accepts the name of a directory. The idea is to keep our existing warnings
    filter generator in place, but tweak the structure of the functions that use it.
    We'll operate on iterators as both input and result; this way the same function
    could be used regardless of whether the log lines came from a file, memory, the
    web, or another iterator.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对生成器示例进行一点修改，使其不再接受输入文件，而是接受一个目录的名称。想法是保持我们现有的警告过滤器生成器不变，但调整使用它的函数的结构。我们将对迭代器进行操作，作为输入和结果；这样，无论日志行来自文件、内存、网络还是另一个迭代器，都可以使用相同的函数。
- en: 'This version of the code illustrates a new `file_extract()` generator. This
    does some basic setup before yielding information from the `warnings_filter()`
    generator:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的这个版本演示了一个新的`file_extract()`生成器。在从`warnings_filter()`生成器提供信息之前，它进行了一些基本的设置：
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Our top-level function `extract_and_parse_d()` has a slight change to use the
    `file_extract()` function instead of opening a file and applying the `warnings_filter()`
    to one file. The `file_extract()` generator will yield all of the `WARNING` lines
    from *all* of the files provided in the argument value.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的高级函数 `extract_and_parse_d()` 进行了一点点修改，现在使用 `file_extract()` 函数而不是打开一个文件并对一个文件应用
    `warnings_filter()`。`file_extract()` 生成器将产生所有在参数值中提供的 *所有* 文件中的 `WARNING` 行。
- en: The `yield from` syntax is a useful shortcut when writing chained generators.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`yield from` 语法在编写链式生成器时是一个有用的快捷方式。'
- en: 'What''s central in this example is the laziness of each of the generators involved.
    Consider what happens when the `extract_and_parse_d()` function, the client, makes a
    demand:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，关键的是每个生成器的懒惰性。考虑当`extract_and_parse_d()`函数，即客户端，提出需求时会发生什么：
- en: The client evaluates `file_extract(log_files)`. Since this is in a `for` statement,
    there's an `__iter__()` method evaluation.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 客户端评估 `file_extract(log_files)`。由于这在一个 `for` 循环语句中，因此会进行 `__iter__()` 方法评估。
- en: The `file_extract()` generator gets an iterator from the `path_iter` iterable,
    and uses this to get the next `Path` instance. The `Path` object is used to create
    a file object that's provided to the `warnings_filter()` generator.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`file_extract()` 生成器从 `path_iter` 可迭代对象中获取一个迭代器，并使用它来获取下一个 `Path` 实例。`Path`
    对象用于创建一个提供给 `warnings_filter()` 生成器的文件对象。'
- en: The `warnings_filter()` generator uses the file's iterator over lines to read
    until it finds a `WARNING` line, which it parses, yielding exactly one tuple.
    The fewest number of lines were read to find this line.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`warnings_filter()` 生成器使用文件的行迭代器读取，直到找到 `WARNING` 行，然后解析该行，并生成一个元组。找到此行所需读取的行数最少。'
- en: The `file_extract()` generator is yielding from the `warnings_filter()` generator,
    so the single tuple is provided to the ultimate client, the `extract_and_parse_d()`
    function.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`file_extract()` 生成器从 `warnings_filter()` 生成器中产生，因此单个元组被提供给最终客户端，即 `extract_and_parse_d()`
    函数。'
- en: The `extract_and_parse_d()` function writes the single tuple to the open CSV
    file, and then demands another tuple. This request goes to `file_extract()`, which
    pushes the demand down to `warnings_filter()`, which pushes the demand to an open
    file to provide lines until a `WARNING` line is found.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`extract_and_parse_d()` 函数将单个元组写入打开的 CSV 文件，然后要求另一个元组。这个请求发送到 `file_extract()`，它将请求传递给
    `warnings_filter()`，`warnings_filter()` 将请求传递到打开的文件，以提供行，直到找到 `WARNING` 行。'
- en: Each generator is lazy and provides one response, doing the least amount of
    work it can get away with to produce the result. This means that a directory with
    a huge number of giant log files is processed by having one open log file, and
    one current line being parsed and processed. It won't fill memory no matter how
    large the files are.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 每个生成器都是懒惰的，只提供一个响应，尽可能少做工作以产生结果。这意味着一个包含大量巨型日志文件的目录通过只打开一个日志文件，解析并处理当前行来处理。无论文件有多大，它都不会占用内存。
- en: We've seen how generator functions can provide data to other generator functions.
    We can do this with ordinary generator expressions, also. We'll make some small
    changes to the `warnings_filter()` function to show how we can create a stack
    of generator expressions.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到生成器函数如何向其他生成器函数提供数据。我们也可以用普通的生成器表达式做到这一点。我们将对`warnings_filter()`函数做一些小的修改，以展示我们如何创建一个生成器表达式的堆栈。
- en: Generator stacks
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成器堆栈
- en: 'The generator function (and the generator expression) for `warnings_filter`
    makes an unpleasant assumption. The use of `cast()` makes a claim to **mypy**
    that''s – perhaps – a bad claim to make. Here''s the example:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器函数（以及生成器表达式）对于`warnings_filter`做出了一个令人不快的假设。使用`cast()`对**mypy**提出了一种——或许——不恰当的主张。以下是一个示例：
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The use of `cast()` is a way of claiming the `pattern.match()` will always yield
    a `Match[str]` object. This isn't a great assumption to make. Someone may change
    the format of the log file to include a multiple-line message, and our `WARNING`
    filter would crash every time we encountered a multi-line message.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `cast()` 是一种断言 `pattern.match()` 总是会返回一个 `Match[str]` 对象的方式。这不是一个很好的假设。有人可能会更改日志文件的格式，以包含多行消息，而我们的
    `WARNING` 过滤器每次遇到多行消息时都会崩溃。
- en: 'Here''s a message that would cause problems followed by a message that''s easy
    to process:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一条可能引起问题的信息，随后是一条易于处理的信息：
- en: '[PRE23]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The first line has the word `WARN` in a multi-line message that will break our
    assumption about lines that contain the word `WARN`. We need to handle this with
    a little more care.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行在一个多行消息中包含单词`WARN`，这将打破我们对包含单词`WARN`的行的假设。我们需要更加小心地处理这个问题。
- en: We can rewrite this generator expression to create a generator function, and
    add an assignment statement (to save the `Match` object) and an `if` statement
    to further decompose the filtering. We can use the walrus operator `:=` to save
    the `Match` object, also.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个生成器表达式重写为一个生成器函数，并添加一个赋值语句（用于保存`Match`对象）以及一个`if`语句来进一步分解过滤过程。我们还可以使用
    Walrus 操作符 `:=` 来保存`Match`对象。
- en: 'We could reframe the generator expression as the following generator function:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将生成器表达式重构成以下生成器函数：
- en: '[PRE24]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As we noted above, this complex filtering tends toward deeply nested `if` statements,
    which can create logic that''s difficult to summarize. In this case, the two conditions
    aren''t terribly complex. An alternative is to change this into a series of map
    and filter stages, each of which does a separate, small transformation on the
    input. We can decompose the matching and filtering into the following:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们上面所提到的，这种复杂的过滤往往趋向于深层嵌套的`if`语句，这可能会创建难以总结的逻辑。在这种情况下，这两个条件并不特别复杂。一个替代方案是将这转换为一系列的映射和过滤阶段，每个阶段都对输入进行单独的、小的转换。我们可以将匹配和过滤分解为以下步骤：
- en: Map the source line to an `Optional[Match[str]]` object using the `pattern.match()`
    method.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `pattern.match()` 方法将源行映射到 `Optional[Match[str]]` 对象。
- en: Filter to reject any `None` objects, passing only good `Match` objects and applying the
    `groups()` method to create a `List[str]`.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤掉任何`None`对象，只传递好的`Match`对象，并应用`groups()`方法创建一个`List[str]`。
- en: Filter the strings to reject the non-`WARN` lines, and pass the `WARN` lines.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤字符串以拒绝非`WARN`行，并传递`WARN`行。
- en: 'Each of these stages is a generator expression following the standard pattern.
    We can expand the `warnings_filter` expression into a stack of three expressions:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 每个这些阶段都是一个遵循标准模式的生成器表达式。我们可以将`warnings_filter`表达式展开成三个表达式的堆栈：
- en: '[PRE25]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: These expressions are, of course, utterly lazy. The final `warnings_filter`
    uses the iterable, `group_iter`. This iterable gets matches from another generator,
    `possible_match_iter`, which gets source text lines from the `source` object,
    an iterable source of lines. Since each of these generators is getting items from
    another lazy iterator, there's only one line of data being processed through the
    `if` clause and the final expression clause at each stage of this process.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表达式当然是极其懒惰的。最后的`warnings_filter`使用了可迭代对象`group_iter`。这个可迭代对象从另一个生成器`possible_match_iter`获取匹配项，而`possible_match_iter`则从`source`对象获取源文本行，`source`对象是一个行可迭代的来源。由于这些生成器中的每一个都在从另一个懒惰迭代器获取项目，因此在处理过程的每个阶段，只有一行数据通过`if`子句和最终表达式子句进行处理。
- en: Note that we can exploit the surrounding `()` to break each expression into
    multiple lines. This can help show the map or filter operation that's embodied
    in each expression.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们可以利用周围的`()`将每个表达式拆分成多行。这有助于展示每个表达式中体现的映射或过滤操作。
- en: 'We can inject additional processing as long as it fits this essential mapping-and-filtering
    design pattern. Before moving on, we''re going to switch to a slightly more friendly
    regular expression for locating lines in our log file:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 只要符合这个基本的映射-过滤设计模式，我们就可以注入额外的处理。在继续之前，我们将切换到一个稍微更友好的正则表达式，用于定位日志文件中的行：
- en: '[PRE26]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This regular expression is broken into three adjacent strings; Python will automatically
    concatenate string literals. The expression uses three named groups. The date-time
    stamp, for example, is group number one, a hard-to-remember bit of trivia. The
    `?P<dt>` inside the `()` means the `groupdict()` method of a `Match` object will
    have the key `dt` in the resulting dictionary. As we introduce more processing
    steps, we'll need to be much more clear about the intermediate results.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这个正则表达式被拆分为三个相邻的字符串；Python 会自动连接字符串字面量。该表达式使用了三个命名组。例如，日期时间戳是第一组，这是一些难以记住的琐事。`()`
    内的 `?P<dt>` 表示 `Match` 对象的 `groupdict()` 方法将在结果字典中有一个键 `dt`。随着我们引入更多的处理步骤，我们需要对中间结果有更清晰的了解。
- en: 'Here''s an image of the regular expression that can sometimes be helpful:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了一个有时有帮助的正则表达式图像：
- en: '![](img/B17070_10_03.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17070_10_03.png)'
- en: 'Figure 10.3: Log line regular expression diagram'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3：日志行正则表达式图
- en: Let's expand this example to transform the date-time stamp to another format.
    This involves injecting a transformation from the input format to the desired
    output format. We can do this in one big gulp, or we can do it in a series of
    small sips.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个例子扩展到将日期时间戳转换为另一种格式。这涉及到从输入格式到所需输出格式的转换。我们可以一次性完成，也可以分多次小步骤进行。
- en: 'This sequence of steps makes it easier to add or change one individual step
    without breaking the entire pipeline of processing:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这一系列步骤使得在不破坏整个处理流程的情况下添加或更改单个步骤变得更加容易：
- en: '[PRE27]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We've created two additional stages. One parses the input time to create a Python
    `datetime` object; the second stage formats the `datetime` object as an ISO. Breaking
    the transformation down into small steps lets us treat each mapping operation
    and each filtering operating as discrete, separate steps. We can add, change,
    and delete with a little more flexibility when we create these smaller, easier-to-understand
    steps. The idea is to isolate each transformation into a separate object, described
    by a generator expression.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了两个额外的阶段。第一个阶段将输入时间解析为 Python `datetime` 对象；第二个阶段将 `datetime` 对象格式化为 ISO
    格式。将转换过程分解成小步骤，使我们能够将每个映射操作和每个过滤操作视为独立的、分开的步骤。在创建这些更小、更容易理解的步骤时，我们可以有更多的灵活性来添加、更改和删除。想法是将每个转换隔离成单独的对象，由生成器表达式描述。
- en: The result of the `dt_iter` expression is an iterable over anonymous tuples.
    This is a place where a `NamedTuple` can add clarity. See *Chapter 7*, *Python
    Data Structures*, for more information on `NamedTuple`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`dt_iter` 表达式的结果是匿名元组的可迭代对象。这是一个`NamedTuple`可以增加清晰度的位置。有关`NamedTuple`的更多信息，请参阅*第7章*，*Python数据结构*。'
- en: 'We have an additional way to look at these transformational steps, using the
    built-in `map()` and `filter()` functions. These functions provide features similar
    to generator expressions, using another, slightly different syntax:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有另一种方式来观察这些转换步骤，即使用内置的 `map()` 和 `filter()` 函数。这些函数提供了与生成器表达式类似的功能，但使用了一种稍微不同的语法：
- en: '[PRE28]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The lambda objects are anonymous functions. Each lambda is a callable object
    with parameters and a single expression that is evaluated and returned. There's
    no name and no statements in the body of a lambda. Each stage in this pipeline
    is a discrete mapping or filtering operation. While we can combine mapping and
    filtering into a single `map(lambda ..., filter(lambda ..., source))`, this can
    be too confusing to be helpful.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 对象是无名函数。每个 lambda 都是一个具有参数的单个表达式，该表达式被评估并返回的可调用对象。Lambda 的主体中没有名称和语句。这个管道中的每个阶段都是一个离散的映射或过滤操作。虽然我们可以将映射和过滤组合成一个单一的
    `map(lambda ..., filter(lambda ..., source))`，但这可能会过于混乱，反而没有帮助。
- en: The `possible_match_iter` applies the `pattern.match()` to each line. The `good_match_iter`
    uses the special `filter(None, source)` that passes non-`None` objects, and rejects
    `None` objects. The `group_iter` uses a lambda to evaluate `m.groups()` for each
    object, `m`, in `good_match_iter`. The `warnings_iter` will filter the `group_iter`
    results, keeping only the `WARN` lines, and rejecting all others. The `dt_iter`
    and the final `warnings_filter` expressions perform a conversion from the source
    datetime format to a generic `datetime` object, followed by formatting the `datetime`
    object in a different string format.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`possible_match_iter` 对每一行应用 `pattern.match()`。`good_match_iter` 使用特殊的 `filter(None,
    source)` 过滤器，只传递非 `None` 对象，并拒绝 `None` 对象。`group_iter` 使用 lambda 表达式评估 `good_match_iter`
    中每个对象 `m` 的 `m.groups()`。`warnings_iter` 将过滤 `group_iter` 的结果，只保留 `WARN` 行，并拒绝所有其他行。`dt_iter`
    和最终的 `warnings_filter` 表达式将源日期时间格式转换为通用的 `datetime` 对象，然后以不同的字符串格式格式化 `datetime`
    对象。'
- en: We've seen a number of ways of approaching a complex map-filter problem. We
    can write nested `for` and `if` statements. We can create explicit `Iterator`
    subclass definitions. We can create iterator-based objects using function definitions
    that include the `yield` statement. This provides us the formal interface of the
    `Iterator` class without the lengthy boilerplate required to define `__iter__()`
    and `__next__()` methods. Additionally, we can use generator expressions and even
    comprehensions to apply the iterator design pattern in a number of common contexts.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了处理复杂映射-过滤器问题的多种方法。我们可以编写嵌套的`for`和`if`语句。我们可以创建显式的`Iterator`子类定义。我们还可以使用包含`yield`语句的函数定义来创建基于迭代器的对象。这为我们提供了`Iterator`类的正式接口，而不需要定义`__iter__()`和`__next__()`方法所需的冗长样板代码。此外，我们还可以使用生成器表达式和列表推导式，将迭代器设计模式应用于多种常见场景。
- en: The iterator pattern is a foundational aspect of Python programming. Every time
    we work with a collection, we'll be iterating through the items, and we'll be
    using an iterator. Because iteration is so central, there are a variety of ways
    of tackling the problem. We can use `for` statements, generator functions, generator
    expressions, and we can build our own iterator classes.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代模式是 Python 编程的基础要素。每次我们处理一个集合时，我们都会遍历其中的项目，并且会使用迭代器。因为迭代如此核心，所以有各种方法来解决这个问题。我们可以使用
    `for` 语句、生成器函数、生成器表达式，并且我们可以构建自己的迭代器类。
- en: Case study
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究
- en: Python makes extensive use of iterators and iterable collections. This underlying
    aspect appears in many places. Each `for` statement makes implicit use of this.
    When we use functional programming techniques, such as generator expressions,
    and the `map()`, `filter()`, and `reduce()` functions, we're exploiting iterators.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Python广泛使用迭代器和可迭代集合。这一基本特性在许多地方都有体现。每个`for`语句都隐式地使用了这一点。当我们使用函数式编程技术，如生成器表达式，以及`map()`、`filter()`和`reduce()`函数时，我们正在利用迭代器。
- en: Python has an `itertools` module full of additional iterator-based design patterns.
    This is worthy of study because it provides many examples of common operations
    that are readily available using built-in constructs.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Python 拥有一个充满额外基于迭代器设计模式的 `itertools` 模块。这值得研究，因为它提供了许多使用内置构造函数即可轻松使用的常见操作的示例。
- en: 'We can apply these concepts in a number of places in our case study:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在我们的案例研究中将这些建议应用于多个地方：
- en: Partitioning all the original samples into testing and training subsets.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有原始样本划分为测试集和训练集。
- en: Testing a particular *k* and distance hyperparameter set by classifying all
    the test cases.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过对所有测试用例进行分类来测试特定的*k*和距离超参数集。
- en: The *k*-nearest neighbors (*k*-NN) algorithm itself and how it locates the *k* nearest
    neighbors from all the training samples.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k*-最近邻（*k*-NN）算法本身以及它是如何从所有训练样本中定位到最近的*k*个邻居的。'
- en: The common aspect of these three processing examples is the "for all" aspect
    of each one. We'll take a little side-trip into the math behind comprehensions
    and generator functions. The math isn't terribly complex, but the following section
    can be thought of as deep background. After this digression, we'll dive into partitioning
    data into training and testing subsets using the iterator concepts.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个处理示例的共同点是每个示例的“对所有”方面。我们将稍微偏离一下，探讨理解列表推导和生成器函数背后的数学原理。这个数学并不特别复杂，但接下来的部分可以被视为深入背景。在这次离题之后，我们将深入探讨使用迭代器概念将数据划分为训练集和测试集。
- en: The Set Builder background
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集合构造背景
- en: Formally, we can summarize operations like partitioning, testing, and even locating
    nearest neighbors with a logic expression. Some developers like the formality
    of it because it can help describe the processing without forcing a specific Python
    implementation.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 从严格意义上讲，我们可以用逻辑表达式来总结诸如分区、测试甚至定位最近邻等操作。一些开发者喜欢它的正式性，因为它可以在不强制特定Python实现的情况下帮助描述处理过程。
- en: 'Here''s the essential rule for partitioning, for example. This involves a "for
    all" condition that describes the elements of a set of samples, *S*:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是分区的基本规则，例如。这涉及到一个“对所有”条件，它描述了样本集合 *S* 的元素：
- en: '![](img/B17070_10_001.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17070_10_001.png)'
- en: In other words, for all *s* in the universe of available samples, *S*, the value
    of *s* is either in the training set, *R*, or the testing set, *E*. This summarizes
    the result of a successful partition of the data. It doesn't describe an algorithm,
    directly, but having this rule can help us be sure we haven't missed something
    important.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，对于所有在可用样本宇宙中的样本 *s*，即样本集 *S*，样本 *s* 的值要么在训练集 *R* 中，要么在测试集 *E* 中。这总结了数据成功划分的结果。这并不是直接描述一个算法，但拥有这个规则可以帮助我们确信我们没有遗漏任何重要的东西。
- en: 'We can also summarize a performance metric for testing. The recall metric has
    a "for all" implied by the *∑* construct:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以为测试总结一个性能指标。召回率指标隐含了由 *∑* 构造的“对所有”的含义：
- en: '![](img/B17070_10_002.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17070_10_002.png)'
- en: The quality score, *q*, is the sum for all *e* in the testing set, *E*, of 1
    where the `knn()` classifier applied to *e* matches the species for *e*, `s(e)`,
    otherwise 0\. This can map nicely to a Python generator expression.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 质量分数，*q*，是测试集*E*中所有*e*的求和，其中`knn()`分类器应用于*e*时匹配到*e*的物种`s(e)`为1，否则为0。这可以很好地映射到一个Python生成器表达式。
- en: 'The *k*-NN algorithm involves a bit more complexity in its definition. We can
    think of it as a partitioning problem. We need to start with a collection of ordered
    pairs. Each pair is the distance from an unknown, *u*, to a training sample, *r*,
    summarized as ![](img/B17070_10_003.png). As we saw in *Chapter 3*, there are
    a number of ways to compute this distance. This has to be done for all training
    samples, *r,* in the universe of training samples, *R*:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-NN算法在其定义上涉及更多的复杂性。我们可以将其视为一个划分问题。我们需要从一个有序对的集合开始。每一对表示未知点*u*到训练样本*r*的距离，总结为![图片](img/B17070_10_003.png)。正如我们在*第3章*中看到的，有几种方法可以计算这个距离。这必须对所有训练样本*r*在训练样本集*R*的宇宙中完成：'
- en: '![](img/B17070_10_004.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17070_10_004.png)'
- en: 'Then we need to partition these distances into two subsets, *N*, and *F* (near
    and far) such that all distances in *N* are less than or equal to all distances
    in *F*:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要将这些距离分为两个子集，*N* 和 *F*（近和远），使得 *N* 中的所有距离都小于或等于 *F* 中的所有距离：
- en: '![](img/B17070_10_005.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17070_10_005.png)'
- en: We also need to be sure the number of elements in the near set, *N,* is equal
    to the desired number of neighbors, *k*.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要确保近邻集中的元素数量，即*N*，等于期望的邻居数量，即*k*。
- en: This final formalism exposes an interesting nuance to the computation. What
    if there are more than *k* neighbors with the same distance metric? Should *all* of
    the equidistant training samples be included in voting? Or should we arbitrarily
    slice exactly *k* of the equidistant samples? If we "arbitrarily" slice, what's
    the exact rule that gets used for choosing among the equidistant training samples?
    Does the selection rule even matter? These could be significant issues, and they're
    outside the scope of this book.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这种最终形式揭示了计算中的一个有趣细微差别。如果有超过 *k* 个邻居具有相同的距离度量呢？是否应该将所有等距的训练样本都包含在投票中？或者我们应该任意地选择恰好
    *k* 个等距样本？如果我们“任意”地切割，选择等距训练样本时使用的确切规则是什么？选择规则甚至重要吗？这些问题可能是重要的问题，但它们超出了本书的范围。
- en: The example later in the chapter uses the `sorted()` function, which tends to
    preserve the original order. Can this lead to a bias to our classifier when confronted
    with equidistant choices? This, too, may be a significant issue, and it's also
    outside the scope of this book.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 本章后面的例子使用了`sorted()`函数，该函数倾向于保留原始顺序。这会不会导致我们在面对等距选择时对分类器产生偏差？这同样可能是一个重要问题，而且它也不在本书的讨论范围之内。
- en: Given a little bit of set theory, we can tackle the ideas of partitioning the
    data, and computing the *k* nearest neighbors, making use of the common iterator
    features. We'll start with the partitioning algorithm's implementation in Python.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一点集合论知识，我们可以处理数据划分和计算*k*最近邻的想法，利用常见的迭代器特性。我们将从Python中划分算法的实现开始。
- en: Multiple partitions
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多分区
- en: Our goal is to separate testing and training data. There's a tiny bump in the
    road, however, called **deduplication**. The statistical measures of overall quality
    rely on the training and testing sets being independent; this means we need to
    avoid duplicate samples being split between testing and training sets. Before
    we can create testing and training partitions, we need to find any duplicates.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是将测试数据和训练数据分开。然而，在道路上有一个小小的障碍，称为**去重**。整体质量的统计指标依赖于训练集和测试集的独立性；这意味着我们需要避免重复样本在测试集和训练集之间被分割。在我们能够创建测试和训练分区之前，我们需要找到任何重复的样本。
- en: We can't – easily – compare each sample with all of the other samples. For a
    large set of samples, this may take a very long time. A pool of ten thousand samples
    would lead to 100 million checks for duplication. This isn't practical. Instead,
    we can partition our data into subgroups where the values for all the measured
    features are *likely* to be equal. Then, from those subgroups, we can choose testing
    and training samples. This lets us avoid comparing every sample with all of the
    other samples to look for duplicates.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法轻松地将每个样本与其他所有样本进行比较。对于大量样本集，这可能需要非常长的时间。一万样本的集合会导致一亿次的重复检查。这并不实用。相反，我们可以将我们的数据划分为子组，其中所有测量特征的值*很可能*是相等的。然后，从这些子组中，我们可以选择测试和训练样本。这使我们能够避免将每个样本与其他所有样本进行比较以寻找重复项。
- en: 'If we use Python''s internal hash values, we can create buckets containing
    samples that may have equal values. In Python, if items are equal, they must have
    the same integer hash value. The inverse is not true: items may coincidentally
    have the same hash value, but may not actually be equal.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用 Python 的内部哈希值，我们可以创建包含可能具有相等值的样本的桶。在 Python 中，如果项目相等，它们必须具有相同的整数哈希值。反之则不成立：项目可能偶然具有相同的哈希值，但实际上并不相等。
- en: 'Formally, we can say this:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 正式来说，我们可以这样说：
- en: '![](img/B17070_10_006.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17070_10_006.png)'
- en: That is, if two objects in Python, *a* and *b*, are equal, they must also have
    the same hash value ![](img/B17070_10_007.png). The reverse is not true because
    equality is more than a simple hash value check; it's possible that ![](img/B17070_10_008.png);
    the hash values may be the same, but the underlying objects aren't actually equal.
    We call this a "hash collision" of two unequal values.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，如果Python中的两个对象 *a* 和 *b* 相等，它们也必须具有相同的哈希值 ![](img/B17070_10_007.png)。反之则不成立，因为相等不仅仅是简单的哈希值检查；可能存在 ![](img/B17070_10_008.png)；哈希值可能相同，但底层对象实际上并不相等。我们称这种情况为“两个不相等值的哈希冲突”。
- en: 'Continuing this thought, the following is a matter of definition for modulo:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 继续这个思路，以下是对模运算定义的问题：
- en: '![](img/B17070_10_009.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17070_10_009.png)'
- en: If two values are equal, they are also equal to any modulo of those values.
    When we want to know if `a == b`, we can ask if `a % 2 == b % 2`; if both numbers
    are odd or both numbers are even, then there's a chance `a` and `b` could be equal.
    If one number is even and the other is odd, there's no way they can be equal.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个值相等，它们也等于这些值的任何模数。当我们想知道 `a == b` 是否成立时，我们可以询问 `a % 2 == b % 2`；如果两个数都是奇数或都是偶数，那么
    `a` 和 `b` 可能相等。如果一个是偶数而另一个是奇数，它们就不可能相等。
- en: For complex objects, we can use `hash(a) % m == hash(b) % m`. If the two hash
    values, modulo *m*, are the same, then the hash values could be the same, and
    the two objects, `a` and `b`, could also be equal. We know it's possible for several
    objects to have the same hash value, and even more objects to have the same hash
    value modulo *m*.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 对于复杂对象，我们可以使用 `hash(a) % m == hash(b) % m`。如果两个哈希值，模 *m* 后相同，那么这两个哈希值可能是相同的，并且两个对象
    `a` 和 `b` 也可能是相等的。我们知道多个对象可能具有相同的哈希值，甚至更多对象在模 *m* 后具有相同的哈希值。
- en: While this doesn't tell us if two items are equal, this technique limits the
    domain of objects required for exact equality comparison to very small pools of
    a few items instead of the entire set of all samples. We can avoid duplicates
    if we avoid splitting up one of these subgroups.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这并不能告诉我们两个项目是否相等，但这项技术将精确比较所需对象的域限制在非常小的几个项目池中，而不是所有样本的整个集合。如果我们避免拆分这些子组之一，我们就可以避免重复。
- en: 'Here''s a view of seven samples broken into three subgroups based on their
    hash codes modulo 3\. Most of the subgroups have items that are potentially equal,
    but actually aren''t equal. One of the groups has an actual duplicate sample:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是七个样本的视图，根据它们的哈希码模3分为三个子组。大多数子组中的项目在潜在上是相等的，但实际上并不相等。其中一个组实际上有一个重复的样本：
- en: '![Diagram  Description automatically generated](img/B17070_10_04.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B17070_10_04.png)'
- en: 'Figure 10.4: Partitioning sample data to locate duplicates'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4：将样本数据分区以定位重复项
- en: To find the duplicate sample, we don't need to compare each sample against the
    other six. We can look within each subgroup and compare a few samples to see if
    they happen to be duplicates.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到重复的样本，我们不需要将每个样本与其他六个样本进行比较。我们可以在每个子组内部查找，并比较几个样本，看看它们是否恰好是重复的。
- en: The idea behind this deduplication approach is to separate the entire suite
    of samples into sixty buckets where the samples have equal hash values, modulo
    sixty. Samples in the same bucket *could* be equal, and as a simple expedient,
    we can treat them as equal. Samples in separate buckets have different hash values
    and cannot possibly be equal.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这种去重方法的理念是将整个样本集分成六十个桶，其中样本的哈希值相等，模六十。同一桶中的样本*可能*相等，作为一个简单的权宜之计，我们可以将它们视为相等。不同桶中的样本具有不同的哈希值，不可能相等。
- en: We can avoid having duplicate samples in both testing and training by using
    an entire bucket's set of samples together. That way, the duplicates are either
    all testing or all training, but never split.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一起使用整个桶的样本集来避免在测试和训练中存在重复样本。这样，重复的样本要么全部用于测试，要么全部用于训练，但绝不会分开。
- en: 'Here''s a partition function that first creates 60 separate buckets for samples.
    Then, some fraction of the buckets are allocated for testing, the rest for training.
    Specifically, 12, 15, or 20 buckets out of 60 are about 20%, 25%, or 33% of the
    population. Here''s an implementation that deduplicates as it partitions into
    testing and training subsets:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个分区函数，它首先为样本创建60个独立的桶。然后，其中一部分桶被分配用于测试，其余的用于训练。具体来说，60个桶中的12、15或20个桶大约占人口的20%、25%或33%。以下是一个在将数据集划分为测试集和训练集时进行去重的实现示例：
- en: '[PRE29]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'There are three steps in this partitioning:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个划分过程中有三个步骤：
- en: We create sixty separate lists of samples that – because the hashes are equal
    – may have duplicates. We keep these batches together to avoid splitting duplicates
    so they're in both testing and training subsets.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了六十个独立的样本列表，由于哈希值相同，这些列表中可能存在重复。我们将这些批次放在一起，以避免将重复样本分割到测试集和训练集两个子集中。
- en: We build two lists of iterators. Each list has an iterator over a subset of
    the buckets. The `training_rule()` function is used to make sure we get 12/60,
    15/60, or 20/60 buckets in testing, and the rest in training. Since each of these
    iterators is lazy, these lists of iterators can be used to accumulate samples.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们构建了两个迭代器列表。每个列表都有一个对桶子子集的迭代器。`training_rule()` 函数用于确保我们在测试中获取到 12/60、15/60
    或 20/60 的桶子，其余的用于训练。由于这些迭代器都是懒加载的，因此这些迭代器列表可以用来累积样本。
- en: Finally, we use the `itertools.chain` to consume values from a sequence of generators.
    A chain of iterators will consume the items from each of the various individual
    bucket-level iterators to create the two final partitions of samples.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用`itertools.chain`来从生成器的序列中消费值。一系列迭代器将消费来自各个单独的桶级别迭代器的项目，以创建两个最终的样本分区。
- en: Note that the type hint for `ModuloDict` defines a subtype of the generic `DefaultDict`.
    It provides a key of `int` and the value will be a `list[KnownSample]` instances.
    We've provided this named type to avoid repeating the long definition of the dictionaries
    we'll be working with.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到`ModuloDict`的类型提示定义了一个泛型`DefaultDict`的子类型。它提供了一个`int`类型的键，值将是一个`list[KnownSample]`实例的列表。我们提供了这个命名类型，以避免重复我们将会使用的字典的冗长定义。
- en: 'The `itertools.chain()` is a pretty clever kind of iterator. It consumes data
    from other iterators. Here''s an example:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`itertools.chain()` 是一种相当聪明的迭代器。它从其他迭代器中消耗数据。以下是一个示例：'
- en: '[PRE30]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We created two `range()` objects, `p1`, and `p2`. A chain object will be an
    iterator, and we used the `list()` function to consume all the values.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了两个 `range()` 对象，`p1` 和 `p2`。链对象将是一个迭代器，我们使用了 `list()` 函数来消费所有值。
- en: The steps above can create a large mapping as an intermediate data structure.
    It also creates sixty generators, but these don't require very much memory. The
    final two lists contain references to the same `Sample` objects as the partitioning
    dictionary. The good news is the mapping is temporary and only exists during this
    function.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的步骤可以创建一个大型映射作为中间数据结构。它还创建了六十个生成器，但这些不需要太多的内存。最后的两个列表包含了对分区字典中相同的`Sample`对象的引用。好消息是，这个映射是临时的，并且只存在于这个函数期间。
- en: This function also depends on a `training_rule()` function. This function has
    a type hint of `Callable[[int], bool]`. Given the index value for a partition
    (a value from 0 to 59, inclusive), we can assign it to a testing or training partition.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数还依赖于一个`training_rule()`函数。此函数的类型提示为`Callable[[int], bool]`。给定一个分区（从0到59的值，包括0和59），我们可以将其分配给测试或训练分区。
- en: 'We can use different implementations to get to 80%, 75%, or 66% testing data.
    For example:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用不同的实现方法来获取80%、75%或66%的测试数据。例如：
- en: '[PRE31]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The above lambda object will perform a 75% training and 25% testing split.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 上述 lambda 对象将执行 75% 的训练和 25% 的测试分割。
- en: Once we've partitioned the data, we can use iterators for classifying samples
    as well as testing the quality of our classification process.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对数据进行分区，我们就可以使用迭代器来对样本进行分类，以及测试我们分类过程的质量。
- en: Testing
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试
- en: 'The testing process can also be defined as a higher-order function, a function
    that accepts a function as a parameter value. We can summarize the testing effort
    as a map-reduce problem. Given a `Hyperparameter` with a *k* value and a distance
    algorithm, we need to use an iterator for the following two steps:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 测试过程也可以被定义为一个高阶函数，即一个接受函数作为参数值的函数。我们可以将测试工作量总结为一个映射-归约问题。给定一个具有*k*值的`超参数`和距离算法，我们需要使用迭代器来完成以下两个步骤：
- en: A function classifies all test samples, mapping each test sample to 1 if the
    classification was correct or 0 for an incorrect classification. This is the map
    part of map-reduce.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数将所有测试样本分类，如果分类正确则将每个测试样本映射到1，如果分类错误则映射到0。这是map-reduce中的映射部分。
- en: A function computes a summary with the count of correct values from the long
    sequence of actual classified samples. This is the reduce part of map-reduce.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数通过计算实际分类样本长序列中正确值的数量来生成一个摘要。这是map-reduce中的reduce部分。
- en: Python provides high-level functions for these map and reduce operations. This
    allows us to focus on the details of the mapping and ignore the boilerplate part
    of iterating over the data items.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Python 为这些映射和归约操作提供了高级函数。这使得我们能够专注于映射的细节，而忽略遍历数据项的样板代码部分。
- en: Looking forward to the next section, we'll want to refactor the `Hyperparameter`
    class to split the classifier algorithm into a separate, standalone function.
    We'll make the classifier function a **Strategy** that we provide when we create
    an instance of the `Hyperparameter` class. Doing this means we can more easily
    experiment with some alternatives. We'll look at three different ways to approach
    refactoring a class.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 期待下一节内容，我们将想要重构`Hyperparameter`类，将其分类器算法拆分为一个独立的、单独的函数。我们将使分类器函数成为一个**策略**，在我们创建`Hyperparameter`类的实例时提供。这样做意味着我们可以更容易地尝试一些替代方案。我们将探讨三种不同的方法来重构一个类。
- en: 'Here''s one definition that relies on an external classifier function:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个依赖于外部分类器函数的定义：
- en: '[PRE32]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The `test()` method uses two mapping operations and a reduce operation. First,
    we define a generator that will map each testing sample to a `ClassifiedKnownSample`
    object. This object has the original sample and the results of the classification.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`test()` 方法使用了两个映射操作和一个归约操作。首先，我们定义了一个生成器，它将每个测试样本映射到一个 `ClassifiedKnownSample`
    对象。这个对象包含原始样本和分类的结果。'
- en: Second, we define a generator that will map each `ClassifiedKnownSample` object
    to a 1 (for a test that matched the expected species) or a 0 (for a test that
    failed). This generator depends on the first generator to provide values.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们定义了一个生成器，它将每个`ClassifiedKnownSample`对象映射到1（对于匹配预期物种的测试）或0（对于失败的测试）。这个生成器依赖于第一个生成器来提供值。
- en: 'The actual work is the summation: this consumes values from the second generator.
    The second generator consumes objects from the first generator. This technique
    can minimize the volume of data in memory at any one time. It also decomposes
    a complex algorithm into two separate steps, allowing us to make changes as necessary.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 实际工作是对这些值的求和：这会从第二个生成器中获取值。第二个生成器从第一个生成器中获取对象。这种技术可以最小化在任何时刻内存中的数据量。它还将一个复杂的算法分解为两个独立的步骤，使我们能够根据需要做出更改。
- en: There's an optimization available here, also. The value of `t.classification` in
    the second generator is `self.classify(t.sample.sample)`. It's possible to reduce
    this to a single generator and eliminate creating intermediate `ClassifiedKnownSample`
    objects.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这里也有一个优化方法。第二个生成器中`t.classification`的值是`self.classify(t.sample.sample)`。可以将它简化为一个生成器，并消除创建中间`ClassifiedKnownSample`对象的过程。
- en: 'Here''s how the test operation looks. We can build a `Hyperparameter` instance
    using a function for distance, `manhattan()`, and a classifier function, `k_nn_1()`:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是测试操作的外观。我们可以使用距离函数`manhattan()`和分类器函数`k_nn_1()`来构建一个`Hyperparameter`实例：
- en: '[PRE33]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We'll look at the implementations of various classifiers in the next two sections.
    We'll start with the base definition, `k_nn_1()`, and then look at one based on
    the `bisect` module next.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的两节中探讨各种分类器的实现。我们将从基础定义`k_nn_1()`开始，然后接着查看基于`bisect`模块的一个实现。
- en: The essential k-NN algorithm
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本的k-NN算法
- en: 'We can summarize the *k*-NN algorithm as having the following steps:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将*k*-近邻算法总结为以下步骤：
- en: Create a list of all (distance, training sample) pairs.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建所有（距离，训练样本）对的列表。
- en: Sort these in ascending order.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按升序排列这些。
- en: Pick to the first *k*, which will be the *k* nearest neighbors.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择第一个*k*，它将是最近的*k*个邻居。
- en: Chose the mode (the highest frequency) label for the *k* nearest neighbors.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择距离最近的邻居（最高频率）的模式标签。
- en: 'The implementation would look like this:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 实现方式如下：
- en: '[PRE34]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: While clear, this does accumulate a large number of distance values in the `distances`
    list object, when only *k* are actually needed. The `sorted()` function consumes
    the source generator and creates a (potentially large) list of intermediate values.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然清晰，但这确实会在`distances`列表对象中累积大量的距离值，而实际上只需要**k**个。`sorted()`函数消耗了源生成器，并创建了一个（可能很大的）中间值列表。
- en: One of the high-cost parts of this specific *k*-NN algorithm is sorting the
    entire set of training data after the distances have been computed. We summarize
    the complexity with the description as an *O(n log n)* operation. A way to avoid
    cost is to avoid sorting the entire set of distance computations.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特定的 *k*-NN 算法中成本较高的部分是在计算距离之后对整个训练数据集进行排序。我们用描述性的方式将其复杂度总结为 *O(n log n)* 操作。避免成本的一种方法是不对整个距离计算集进行排序。
- en: '*Steps 1* to *3* can be optimized to keep only the *k* smallest distance values.
    We can do this by using the `bisect` module to locate the position in a sorted
    list where a new value can be inserted. If we only keep values that are smaller
    than the *k* values in the list, we can avoid a lengthy sort.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤 1* 到 *步骤 3* 可以优化，只保留 *k* 个最小的距离值。我们可以通过使用 `bisect` 模块来定位在有序列表中插入新值的位置。如果我们只保留小于列表中 *k* 个值的那些值，就可以避免进行冗长的排序。'
- en: k-NN using the bisect module
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用二分模块的k-NN
- en: 'Here''s an alternative implementation of *k*-NN that tries to avoid sorting
    all of the distance computations:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提供了一个避免对所有距离计算进行排序的 *k*-NN 的替代实现：
- en: 'For each training sample:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个训练样本：
- en: Compute the distance from this training sample to the unknown sample.
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算此训练样本到未知样本的距离。
- en: If it's greater than the last of the *k* nearest neighbors seen so far, discard
    the new distance.
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果它大于迄今为止看到的最近的k个邻居中的最后一个，则丢弃新的距离。
- en: Otherwise, find a spot among the *k* values; insert the new item; truncate the
    list to length *k*.
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 否则，在 *k* 值中找到一个位置；插入新项目；截断列表至长度 *k*。
- en: Find the frequencies of result values among the *k* nearest neighbors.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最近的**k**个邻居中找到结果值的频率。
- en: Choose the mode (the highest frequency) among the *k* nearest neighbors.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最近的**k**个邻居中选择模式（最高频率）。
- en: 'If we seed the list of *k* nearest neighbors with floating point infinity values,
    ![](img/B17070_10_011.png) to mathematicians, `float("inf")` in Python, then the
    first few computed distances, *d*, will be kept because ![](img/B17070_10_012.png).
    After the first *k* distances have been computed, the remaining distances must
    be smaller than one of the *k* neighbor''s distances to be relevant:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将**k**个最近邻的列表初始化为浮点无穷大值，![](img/B17070_10_011.png)对于数学家来说，Python中的`float("inf")`，那么前几个计算出的距离**d**将被保留，因为![](img/B17070_10_012.png)。在计算了前**k**个距离之后，剩余的距离必须小于**k**个邻居中任意一个的距离才能被认为是相关的：
- en: '[PRE35]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Instead of sorting all distances into a big list, we''re inserting (and removing)
    one distance from a much smaller list. After the first *k* distances are computed,
    this algorithm involves two kinds of state change: a new item is inserted into
    the *k* nearest neighbors, and the most distant of the *k+1* neighbors is removed.
    While this doesn''t change the overall complexity in a dramatic way, these are
    relatively inexpensive operations when performed on a very small list of only
    *k* items.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是将所有距离排序到一个大列表中，而是从一个远小得多的列表中插入（和删除）一个距离。在计算了前 *k* 个距离之后，此算法涉及两种状态变化：一个新的项目被插入到最近的
    *k* 个邻居中，并且最远的 *k+1* 个邻居之一被移除。虽然这并没有以戏剧性的方式改变整体复杂度，但当在只有 *k* 个项目的非常小的列表上执行时，这些操作相对来说是低成本的。
- en: k-NN using the heapq module
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 heapq 模块实现的 k-NN
- en: We have yet another trick up our sleeve. We can use the `heapq` module to maintain
    a sorted list of items. This lets us implement the sorting operation as each item
    is placed into the overall list. This doesn't reduce the general complexity of
    the processing, but it replaces two inexpensive insert and pop operations with *potentially* less
    expensive insert operations.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有另一个锦囊妙计。我们可以使用`heapq`模块来维护一个排序后的项目列表。这使得我们能够在每个项目被放入整体列表时执行排序操作。这并不会降低处理的一般复杂性，但它将两个低成本的插入和弹出操作替换为*可能*更经济的插入操作。
- en: The idea is to start with an empty list and insert items into the list, ensuring
    that (a) the items are kept in order, and (b) the item at the head of the list
    always has the least distance. The heap queue algorithm can maintain an upper
    bound on the size of the queue. Keeping only *k* items should also reduce the
    volume of data required in memory.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: We can then pop *k* items from the heap to retrieve the nearest neighbors.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以随后从堆中弹出*k*个元素以检索最近的邻居。
- en: '[PRE36]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This is elegantly simple. It's not, however, remarkably fast. It turns out that
    the cost of computing the distances outweighs the cost savings from using a more
    advanced heap queue to reduce the number of items being sorted.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常简洁优雅。然而，它并不特别快速。结果证明，计算距离的成本超过了使用更高级的堆队列来减少待排序项目数量的成本节约。
- en: Conclusion
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: 'We can compare these distinct *k*-NN algorithms by providing a consistent set
    of training and test data. We''ll use a function like the following:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过提供一组一致的训练和测试数据来比较这些不同的**k**-NN算法。我们将使用如下函数：
- en: '[PRE37]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We've created a consistent `Hyperparameter` instance. Each instance has a common
    value of *k* and a common distance function; they have a distinct classifier algorithm.
    We can execute the `test()` method and display the time required.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个一致的`超参数`实例。每个实例都有一个共同的值*k*和一个共同的距离函数；它们有一个不同的分类算法。我们可以执行`test()`方法并显示所需的时间。
- en: 'A `main()` function can use this to examine the various classifiers:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `main()` 函数可以使用此功能来检查各种分类器：
- en: '[PRE38]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We've applied each of the classifiers to a consistent set of data. We haven't
    shown the `a_lot_of_data()` function. This creates two lists of `TrainingKnownSample`
    and `TestingKnownSample` instances. We've left this as an exercise for the reader.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将每个分类器应用于一组一致的数据。我们没有展示`a_lot_of_data()`函数。这创建了两个`TrainingKnownSample`和`TestingKnownSample`实例的列表。我们将这留作读者的练习。
- en: 'Here are the performance results comparing these alterative *k*-NN algorithms:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是这些替代 *k*-NN 算法的性能比较结果：
- en: '| algorithm | test quality | time |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 测试质量 | 时间 |'
- en: '| k_nn_1 | q= 241/ 1000 | 6.553s |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| k_nn_1 | q= 241/ 1000 | 6.553s |'
- en: '| k_nn_b | q= 241/ 1000 | 3.992s |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| k_nn_b | q= 241/ 1000 | 3.992s |'
- en: '| k_nn_q | q= 241/ 1000 | 5.294s |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| k_nn_q | q= 241/ 1000 | 5.294s |'
- en: The test quality is the number of correct test cases. The number is low because
    the data is completely random, and a correct classification rate of about 25%
    is what's expected if our random data uses four different species names.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 测试质量是正确测试用例的数量。这个数字很低，因为数据是完全随机的，如果我们的随机数据使用四种不同的物种名称，预期的正确分类率大约是25%。
- en: The original algorithm, `k_nn_1`, is the slowest, something we suspected. This
    provides the necessary evidence that optimization of this may be necessary. The `bisect` based
    processing, row `k_nn_b` in the table, suggests that working with a small list
    outweighs the costs of performing the bisect operation many times. The `heapq` processing
    time, row `k_nn_h`, was better than the original algorithm, but only by about
    20%.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 原始算法 `k_nn_1` 是最慢的，这是我们怀疑的。这提供了必要的证据，表明对此进行优化可能是必要的。基于 `bisect` 的处理，表格中的行 `k_nn_b`，表明使用小列表的工作成本超过了多次执行二分查找操作的成本。`heapq`
    的处理时间，行 `k_nn_h`，比原始算法要好，但仅好约 20%。
- en: It's important to do both a theoretical analysis of the algorithm's complexity
    as well as a benchmark with actual data. Before spending time and effort on performance
    improvement, we need to start with benchmark analysis to identify where we might
    be able to do things more efficiently. It's also important to confirm that the
    processing is correct before trying to optimize performance.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在对算法的复杂度进行理论分析的同时，使用实际数据进行基准测试也是非常重要的。在投入时间和精力进行性能改进之前，我们需要从基准分析开始，以确定我们可能在哪些方面做得更加高效。在尝试优化性能之前，确认处理过程正确也同样重要。
- en: 'In some cases, we''ll need detailed analysis of specific functions or even
    Python operators. The `timeit` module can be helpful here. We might need to do
    something like the following:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们需要对特定的函数或甚至Python运算符进行详细分析。在这里，`timeit`模块可能会有所帮助。我们可能需要做如下操作：
- en: '[PRE39]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The value computed for `m` can help us make a concrete comparison between distance
    computations. The `timeit` module will execute the given statement, `manhattan(d1,
    d2)`, after performing the one-time setup of some imports and creation of sample
    data.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 计算出的 `m` 值可以帮助我们具体比较距离计算。`timeit` 模块将在执行一次性的导入设置和创建样本数据之后，执行给定的语句 `manhattan(d1,
    d2)`。
- en: Iterators are both a performance boost and a potential way to clarify the overall
    design. They can be helpful with our case study because so much of the processing
    iterates over large collections of data.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代器既是一种性能提升，也是一种阐明整体设计的潜在方法。在我们的案例研究中，迭代器可能很有帮助，因为大部分的处理都是在大数据集上迭代进行的。
- en: Recall
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回忆
- en: 'This chapter looked at a design pattern that seems ubiquitous in Python, the
    iterator. The Python iterator concept is a foundation of the language and is used
    widely. In this chapter we examined a number of aspects:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了在Python中似乎无处不在的设计模式——迭代器。Python的迭代器概念是语言的基础，并且被广泛使用。在本章中，我们考察了多个方面：
- en: Design patterns are good ideas we see repeated in software implementations,
    designs, and architectures. A good design pattern has a name, and a context where
    it's usable. Because it's only a pattern, not reusable code, the implementation
    details will vary each time the pattern is followed.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计模式是我们看到在软件实现、设计和架构中反复出现的好想法。一个好的设计模式有一个名称，以及一个它可用的上下文。因为它只是一个模式，而不是可重用的代码，所以每次遵循该模式时，实现细节都会有所不同。
- en: The `Iterator` protocol is one of the most powerful design patterns because
    it provides a consistent way to work with data collections. We can view strings,
    tuples, lists, sets, and even files as iterable collections. A mapping contains
    a number of iterable collections including the keys, the values, and the items
    (key and value pairs.)
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`迭代器`协议是其中最强大的设计模式之一，因为它提供了一种一致的方式来处理数据集合。我们可以将字符串、元组、列表、集合，甚至文件视为可迭代的集合。映射包含多个可迭代的集合，包括键、值以及项（键值对）。'
- en: List, set, and dictionary comprehensions are short, pithy summaries of how to
    create a new collection from an existing collection. They involve a source iterable,
    an optional filter, and a final expression to define the objects in the new collection.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列表、集合和字典推导式是创建新集合的简洁总结。它们涉及一个源可迭代对象、一个可选的过滤器，以及一个最终表达式来定义新集合中的对象。
- en: Generator functions build on other patterns. They let us define iterable objects
    that have map and filter capabilities.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器函数建立在其他模式之上。它们让我们能够定义具有映射和过滤功能的可迭代对象。
- en: Exercises
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: If you don't use comprehensions in your daily coding very often, the first thing
    you should do is search through some existing code and find some `for` loops.
    See whether any of them can be trivially converted to a generator expression or
    a list, set, or dictionary comprehension.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你平时编码中很少使用列表推导，你应该首先搜索一些现有的代码，找到一些`for`循环。看看它们是否可以轻易地转换成生成器表达式或列表、集合或字典推导。
- en: Test the claim that list comprehensions are faster than `for` loops. This can
    be done with the built-in `timeit` module. Use the help documentation for the
    `timeit.timeit` function to find out how to use it. Basically, write two functions
    that do the same thing, one using a list comprehension, and one using a `for` loop
    to iterate over several thousand items. Pass each function into `timeit.timeit`,
    and compare the results. If you're feeling adventurous, compare generators and
    generator expressions as well. Testing code using `timeit` can become addictive,
    so bear in mind that code does not need to be hyperfast unless it's being executed
    an immense number of times, such as on a huge input list or file.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 测试列表推导式是否比`for`循环更快的说法。这可以通过内置的`timeit`模块来完成。使用`timeit.timeit`函数的帮助文档来了解如何使用它。基本上，编写两个执行相同功能的函数，一个使用列表推导式，另一个使用`for`循环遍历数千个项目。将每个函数传递给`timeit.timeit`，并比较结果。如果你喜欢冒险，也可以比较生成器和生成器表达式。使用`timeit`测试代码可能会变得上瘾，所以请记住，除非代码被大量执行，例如在巨大的输入列表或文件上，否则代码不需要非常快。
- en: Play around with generator functions. Start with basic iterators that require
    multiple values (mathematical sequences are canonical examples; the Fibonacci
    sequence is overused if you can't think of anything better). Try some more advanced
    generators that do things such as take multiple input lists and somehow yield
    values that merge them. Generators can also be used on files; can you write a
    simple generator that shows lines that are identical in two files?
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试使用生成器函数。从需要多个值的简单迭代器开始（数学序列是典型的例子；如果你想不到更好的例子，斐波那契数列可能会被过度使用）。尝试一些更高级的生成器，它们可以执行诸如接受多个输入列表并以某种方式合并它们的值等操作。生成器也可以用于文件；你能编写一个简单的生成器，显示两个文件中相同的行吗？
- en: Extend the log processing exercise to replace the `WARNING` filter with a time
    range filter; all the messages between Jan 26, 2015 11:25:46 and Jan 26, 2015
    11:26:15, for example.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 将日志处理练习扩展到用时间范围过滤器替换`WARNING`过滤器；例如，所有在2015年1月26日11:25:46和2015年1月26日11:26:15之间的消息。
- en: Once you can find `WARNING` lines or lines within a specific time, combine the
    two filters to select only the warnings within the given time. You can use an
    `and` condition within a single generator, or combine multiple generators, in
    effect building an `and` condition. Which seems more adaptable to changing requirements?
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你能找到`WARNING`行或特定时间内的行，将这两个过滤器结合起来，以只选择给定时间内的警告。你可以在单个生成器内使用`and`条件，或者组合多个生成器，实际上构建一个`and`条件。哪种方式似乎更能适应不断变化的需求？
- en: When we presented the class `WarningReformat(Iterator[Tuple[str, ...]]):` example
    of an iterator, we made a questionable design decision. The `__init__()` method
    accepted an open file as an argument value and the `__next__()` method used `readline()`
    on that file. What if we change this slightly and create an explicit iterator
    object that we use inside another iterator?
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们展示了类 `WarningReformat(Iterator[Tuple[str, ...]]):` 的迭代器示例时，我们做出了一些值得商榷的设计决策。`__init__()`
    方法接受一个打开的文件作为参数值，而 `__next__()` 方法则在该文件上使用 `readline()`。如果我们稍作改变，创建一个显式的迭代器对象，并在另一个迭代器内部使用它，会怎样呢？
- en: '[PRE40]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: If we make this change, then `__next__()` can use `line = next(self.insequence)`
    instead of `line = self.insequence.readline()`. Switching from `object.readline()`
    to `next(object)` is an interesting generalization. Does it change anything about
    the `extract_and_parse_2()` function? Does it permit us to use generator expressions
    along with the `WarningReformat` iterator?
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们进行这个更改，那么 `__next__()` 可以使用 `line = next(self.insequence)` 而不是 `line = self.insequence.readline()`。从
    `object.readline()` 切换到 `next(object)` 是一个有趣的推广。这会改变 `extract_and_parse_2()` 函数的任何内容吗？这让我们能够与
    `WarningReformat` 迭代器一起使用生成器表达式吗？
- en: Take this one further step. Refactor the `WarningReformat` class into two separate
    classes, one to filter for `WARN` and a separate class to parse and reformat each
    line of the input log. Rewrite the `extract_and_parse_2()` function using instances
    of these two classes. Which is "better"? What metric did you use to evaluate "better"?
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 再进一步。将`WarningReformat`类重构为两个独立的类，一个用于过滤`WARN`，另一个用于解析和重新格式化输入日志的每一行。使用这两个类的实例重写`extract_and_parse_2()`函数。哪个更好？你使用了哪些指标来评估“更好”？
- en: The case study summarized the *k*-NN algorithm as a kind of comprehension to
    compute distance values, sort and pick the *k* nearest. The case study didn't
    talk much about the partitioning algorithm to separate training data from test
    data. This, too, seems like it might work out as a pair of list comprehensions.
    There's an interesting problem here, though. We'd like to create two lists, reading
    the source exactly once. This isn't easily done with list comprehensions. However,
    look at the `itertools` module for some possible designs. Specifically, the `itertools.tee()`
    function will provide multiple iterables from a single source.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 案例研究将*k*-NN算法总结为一种计算距离值、排序并选择最近的*k*个的方法。案例研究并没有过多地讨论用于将训练数据与测试数据分开的分区算法。这也可能像一对列表推导式那样工作。然而，这里有一个有趣的问题。我们希望创建两个列表，并且只读取源数据一次。这并不容易通过列表推导式完成。然而，查看`itertools`模块可能会有一些可能的设计。具体来说，`itertools.tee()`函数将从一个单一源提供多个可迭代对象。
- en: Look at the recipes section of the `itertools` module. How can the `itertools.partition()`
    function be used to partition data?
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 查看`itertools`模块的食谱部分。如何使用`itertools.partition()`函数来分割数据？
- en: Summary
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned that design patterns are useful abstractions that
    provide best-practice solutions for common programming problems. We covered our
    first design pattern, the iterator, as well as numerous ways that Python uses
    and abuses this pattern for its own nefarious purposes. The original iterator
    pattern is extremely object-oriented, but it is also rather ugly and verbose to
    code around. However, Python's built-in syntax abstracts the ugliness away, leaving
    us with a clean interface to these object-oriented constructs.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解到设计模式是有用的抽象，为常见的编程问题提供了最佳实践解决方案。我们介绍了我们的第一个设计模式——迭代器，以及Python如何以各种方式使用和滥用这个模式来实现其自身的邪恶目的。原始的迭代器模式非常面向对象，但编写起来既丑陋又啰嗦。然而，Python的内建语法抽象掉了这种丑陋，为我们提供了面向对象构造的整洁接口。
- en: Comprehensions and generator expressions can combine container construction
    with iteration in a single line. Generator functions can be constructed using
    the `yield` syntax.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 理解和生成器表达式可以将容器构建与迭代结合在一行中。可以使用`yield`语法来构建生成器函数。
- en: We'll cover several more design patterns in the next two chapters.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的两章中介绍更多设计模式。
