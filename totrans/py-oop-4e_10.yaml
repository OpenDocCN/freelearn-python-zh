- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: The Iterator Pattern
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迭代器模式
- en: 'We''ve discussed how many of Python''s built-ins and idioms seem, at first
    blush, to fly in the face of object-oriented principles, but are actually providing
    access to real objects under the hood. In this chapter, we''ll discuss how the `for` loop,
    which seems so structured, is actually a lightweight wrapper around a set of object-oriented
    principles. We''ll also see a variety of extensions to this syntax that automatically
    create even more types of object. We will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了Python的许多内置函数和惯用法乍一看似乎与面向对象原则相悖，但实际上它们在底层提供了对真实对象的访问。在本章中，我们将讨论看似结构化的`for`循环实际上是如何围绕一组面向对象原则进行轻量级封装的。我们还将看到对这个语法的各种扩展，这些扩展可以自动创建更多类型的对象。我们将涵盖以下主题：
- en: What design patterns are
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是设计模式
- en: The iterator protocol – one of the most powerful design patterns
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代协议 – 最强大的设计模式之一
- en: List, set, and dictionary comprehensions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列表、集合和字典推导式
- en: Generator functions, and how they build on other patterns
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器函数，以及它们如何建立在其他模式之上
- en: The case study for this chapter will revisit the algorithms for partitioning
    sample data into testing and training subsets to see how the iterator design pattern
    applies to this part of the problem.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的案例研究将重新审视将样本数据划分为测试集和训练集的算法，以了解迭代器设计模式如何应用于该问题的这一部分。
- en: We'll start with an overview of what design patterns are and why they're so
    important.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先概述什么是设计模式以及为什么它们如此重要。
- en: Design patterns in brief
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简要设计模式
- en: When engineers and architects decide to build a bridge, or a tower, or a building,
    they follow certain principles to ensure structural integrity. There are various
    possible designs for bridges (suspension and cantilever, for example), but if
    the engineer doesn't use one of the standard designs, and doesn't have a brilliant
    new design, it is likely the bridge they design will collapse.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当工程师和建筑师决定建造一座桥梁、一座塔楼或一栋建筑时，他们会遵循某些原则以确保结构完整。桥梁（例如，悬索桥和悬臂桥）有各种可能的设计，但如果工程师不使用标准设计，也没有一个出色的全新设计，那么他们设计的桥梁很可能会倒塌。
- en: Design patterns are an attempt to bring this same formal definition for correctly
    designed structures to software engineering. There are many different design patterns
    to solve different general problems. Design patterns are applied to solve a common
    problem faced by developers in some specific situation. The design pattern is
    a suggestion as to the ideal solution for that problem, in terms of object-oriented
    design. What's central to a pattern is that it is reused often in unique contexts.
    One clever solution is a good idea. Two similar solutions might be a coincidence.
    Three or more reuses of an idea and it starts to look like a repeating pattern.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 设计模式是试图将这种相同的正式定义应用于正确设计的结构，并将其引入软件工程。存在许多不同的设计模式来解决不同的通用问题。设计模式被应用于解决开发者在某些特定情况下面临的一些常见问题。设计模式是对该问题的理想解决方案的建议，从面向对象设计的角度出发。一个模式的核心在于它在独特的环境中经常被重用。一个巧妙的解决方案是一个好主意。两个相似解决方案可能是巧合。一个想法被重复使用三次或更多，它开始看起来像是一个重复的模式。
- en: Knowing design patterns and choosing to use them in our software does not, however,
    guarantee that we are creating a *correct* solution. In 1907, the Québec Bridge (to
    this day, the longest cantilever bridge in the world, just short of a kilometer
    long) collapsed before construction was completed, because the engineers who designed
    it grossly underestimated the weight of the steel used to construct it. Similarly,
    in software development, we may incorrectly choose or apply a design pattern,
    and create software that *collapses* under normal operating situations or when stressed
    beyond its original design limits.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，了解设计模式并选择在软件中使用它们，并不能保证我们正在创建一个 *正确* 的解决方案。在1907年，魁北克桥（时至今日，世界上跨度最长的悬臂桥，长度接近一公里）在建设完成前倒塌，因为设计它的工程师们极大地低估了用于建造它的钢材重量。同样，在软件开发中，我们可能会错误地选择或应用设计模式，从而创建出在正常操作情况下或超出原始设计极限时 *崩溃* 的软件。
- en: Any one design pattern proposes a set of objects interacting in a specific way
    to solve a general problem. The job of the programmer is to recognize when they
    are facing a specific version of such a problem, then to choose and adapt the
    general pattern to their precise needs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 任何一种设计模式都提出了一组以特定方式相互作用的对象来解决一个普遍问题。程序员的任务是识别他们何时面临这种问题的特定版本，然后选择并调整通用模式以满足他们的具体需求。
- en: In this chapter, we'll look deeply at the iterator design pattern. This pattern
    is so powerful and pervasive that the Python developers have provided multiple
    syntaxes to access the object-oriented principles underlying the pattern. We will
    be covering other design patterns in the next two chapters. Some of them have
    language support and some don't, but none of them are so intrinsically a part
    of the Python coder's daily life as the iterator pattern.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨迭代器设计模式。这个模式非常强大且普遍，以至于Python开发者提供了多种语法来访问模式背后的面向对象原则。我们将在接下来的两章中介绍其他设计模式。其中一些有语言支持，而另一些则没有，但没有任何一个模式像迭代器模式那样，与Python程序员日常生活的内在联系如此紧密。
- en: Iterators
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迭代器
- en: 'In typical design pattern parlance, an **iterator** is an object with a `next()` method
    and a `done()` method; the latter returns `True` if there are no items left in
    the sequence. In a programming language without built-in support for iterators,
    the iterator would be used like this:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的设计模式术语中，一个**迭代器**是一个具有`next()`方法和`done()`方法的对象；后者在序列中没有剩余项时返回`True`。在没有内置迭代器支持的编程语言中，迭代器会被这样使用：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In Python, iteration is available across many language features, so the method
    gets a special name, `__next__`. This method can be accessed using the `next(iterator)` built-in.
    Rather than a `done()` method, Python's iterator protocol raises the `StopIteration`
    exception to notify the client that the iterator has completed. Finally, we have
    the much more readable `for item in iterator:` syntax to actually access items
    in an iterator instead of messing around with a `while` statement. Let's look
    at each these in more detail.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，迭代可以在许多语言特性中使用，因此该方法有一个特殊的名称，`__next__`。这个方法可以通过内置的`next(iterator)`来访问。而不是使用`done()`方法，Python的迭代器协议通过抛出`StopIteration`异常来通知客户端迭代器已经完成。最后，我们还有更易读的`for
    item in iterator:`语法，实际上用来访问迭代器中的项目，而不是与`while`语句纠缠。让我们更详细地看看这些内容。
- en: The iterator protocol
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迭代协议
- en: The `Iterator` abstract base class, in the `collections.abc` module, defines
    the *iterator* protocol in Python. This definition is also referenced by the `typing`
    module to provide suitable type hints. At the foundation, any `Collection` class
    definition must be `Iterable`. To be `Iterable` means implementing an `__iter__()`
    method; this method creates an `Iterator` object.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`Iterator` 抽象基类，位于 `collections.abc` 模块中，定义了 Python 中的 *迭代器* 协议。此定义也被 `typing`
    模块引用，以提供合适的类型提示。在基础层面，任何 `Collection` 类定义都必须是 `Iterable`。要成为 `Iterable`，意味着实现一个
    `__iter__()` 方法；此方法创建一个 `Iterator` 对象。'
- en: '![Diagram  Description automatically generated](img/B17070_10_01.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B17070_10_01.png)'
- en: 'Figure 10.1: The abstractions for Iterable'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1：Iterable的抽象
- en: As mentioned, an `Iterator` class must define a `__next__()` method that the `for` statement
    (and other features that support iteration) can call to get a new element from
    the sequence. In addition, every `Iterator` class must also fulfill the `Iterable` interface.
    This means an `Iterator` will also provide an `__iter__()` method.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，一个`Iterator`类必须定义一个`__next__()`方法，该方法是`for`语句（以及其他支持迭代的特性）可以调用来从序列中获取新元素的方法。此外，每个`Iterator`类还必须实现`Iterable`接口。这意味着`Iterator`也将提供一个`__iter__()`方法。
- en: 'This might sound a bit confusing, so have a look at the following example.
    Note that this is a very verbose way to solve this problem. It explains iteration
    and the two protocols in question, but we''ll be looking at several more readable
    ways to get this effect later in this chapter:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能听起来有些令人困惑，所以请看一下下面的例子。请注意，这是一种非常冗长的解决问题的方式。它解释了迭代和所涉及的两个协议，但我们在本章后面将探讨几种更易读的方式来实现这一效果：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This example defines a `CapitalIterable` class whose job is to loop over each
    of the words in a string and output them with the first letter capitalized. We
    formalized this by using the `Iterable[str]` type hint as a superclass to make
    it clear what our intention was. Most of the work of this iterable class is delegated
    to the `CapitalIterator` implementation. One way to interact with this iterator
    is as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例定义了一个`CapitalIterable`类，其任务是遍历字符串中的每个单词，并将它们以首字母大写的方式输出。我们通过使用`Iterable[str]`类型提示作为超类来形式化这一点，以明确我们的意图。这个可迭代类的大部分工作都委托给了`CapitalIterator`实现。与这个迭代器交互的一种方式如下：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This example first constructs an iterable, assigning it to a variable with the
    boringly obvious name of `iterable`. It then retrieves a `CapitalIterator` instance
    from the `iterable` object. The distinction may need explanation; the iterable
    is an object with elements that can be iterated over. Normally, these elements
    can be looped over multiple times, maybe even at the same time or in overlapping
    code. The iterator, on the other hand, represents a specific location in that
    iterable; some of the items have been consumed and some have not. Two different
    iterators might be at different places in the list of words, but any one iterator
    can mark only one place.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子首先构建了一个可迭代对象，并将其赋值给一个名字平淡无奇的变量`iterable`。然后从`iterable`对象中检索出一个`CapitalIterator`实例。这种区别可能需要解释；可迭代对象是一个具有可迭代元素的对象。通常，这些元素可以被多次循环遍历，甚至可能同时或重叠地进行。另一方面，迭代器代表可迭代对象中的特定位置；一些项目已经被消费，而一些还没有。两个不同的迭代器可能在单词列表的不同位置，但任何一个迭代器只能标记一个位置。
- en: Each time `next()` is called on the iterator, it returns another token from
    the iterable, in order, and updates its internal state to point to the next item.
    Eventually, the iterator will be exhausted (won't have any more elements to return),
    in which case a `StopIteration` exception is raised, and we break out of the `while`
    statement.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 每次在迭代器上调用`next()`时，它都会按顺序返回可迭代对象中的另一个标记，并更新其内部状态以指向下一个项目。最终，迭代器将耗尽（没有更多元素可以返回），在这种情况下，将引发`StopIteration`异常，然后我们跳出`while`语句。
- en: 'Python has a simpler syntax for constructing an iterator from an iterable:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Python 从可迭代对象构造迭代器的语法更简单：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see, the `for` statement, in spite of not looking remotely object-oriented,
    is actually a shortcut to some fundamentally object-oriented design principles.
    Keep this in mind as we discuss comprehensions, as they, too, appear to be the
    polar opposite of an object-oriented tool. Yet, they use the same iteration protocol
    as `for` statements and are another kind of shortcut.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，尽管`for`语句看起来与面向对象毫无关联，但实际上它是通往一些基本面向对象设计原则的捷径。在我们讨论生成器表达式时，请记住这一点，因为它们似乎也是面向对象工具的对立面。然而，它们与`for`语句使用相同的迭代协议，并且是另一种快捷方式。
- en: 'The number of iterable classes in Python is large. We''re not surprised when
    strings, tuples, and lists are iterable. A set, clearly, must be iterable, even
    if the order of elements may be difficult to predict. A mapping will iterate over
    the keys by default; other iterators are available. A file iterates over the available
    lines. A regular expression has a method, `finditer()`, that is an iterator over
    each instance of a matching substring that it can find. The `Path.glob()` method
    will iterate over matching items in a directory. The `range()` object is also
    an iterator. You get the idea: anything even vaguely collection-like will support
    some kind of iterator.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Python中可迭代的类数量很大。当字符串、元组和列表是可迭代的时，我们并不感到惊讶。显然，集合也必须是可迭代的，即使元素的顺序可能难以预测。映射默认会遍历键；其他迭代器也是可用的。文件会遍历可用的行。正则表达式有一个`finditer()`方法，它是一个迭代器，遍历它可以找到的每个匹配子串的实例。`Path.glob()`方法会遍历目录中的匹配项。`range()`对象也是一个迭代器。你明白了：任何稍微有点集合样式的对象都将支持某种类型的迭代器。
- en: Comprehensions
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解
- en: Comprehensions are simple, but powerful, syntaxes that allow us to transform
    or filter an iterable object in as little as one line of code. The resultant object
    can be a perfectly normal list, set, or dictionary, or it can be a *generator
    expression* that can be efficiently consumed while keeping just one element in
    memory at a time.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 理解是简单但强大的语法，它允许我们用一行代码将可迭代对象进行转换或过滤。结果对象可以是一个完全正常的列表、集合或字典，也可以是一个*生成器表达式*，在保持每次只存储一个元素的同时，可以高效地消费。
- en: List comprehensions
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 列推导式
- en: List comprehensions are one of the most powerful tools in Python, so people
    tend to think of them as advanced. They're not. Indeed, we've taken the liberty
    of littering previous examples with comprehensions, assuming you would understand
    them. While it's true that advanced programmers use comprehensions a lot, it's
    not because they're advanced. It's because a comprehension is so fundamental to
    Python, it can handle many of the most common operations in application software.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 列表推导是 Python 中最强大的工具之一，因此人们往往认为它们是高级的。其实并非如此。实际上，我们在之前的例子中已经自由地使用了推导，假设你们能够理解它们。虽然高级程序员确实大量使用推导，但这并不是因为它们高级。而是因为推导在
    Python 中非常基础，它可以处理应用软件中许多最常见的操作。
- en: 'Let''s have a look at one of those common operations; namely, converting a
    list of items into a list of related items. Specifically, let''s assume we just
    read a list of strings from a file, and now we want to convert it to a list of
    integers. We know every item in the list is an integer, and we want to do some
    activity (say, calculate an average) on those numbers. Here''s one simple way
    to approach it:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些常见操作之一；即，将一个项目列表转换为相关项目列表。具体来说，假设我们刚刚从一个文件中读取了一个字符串列表，现在我们想要将其转换为整数列表。我们知道列表中的每个项目都是一个整数，并且我们想要对这些数字进行一些活动（比如，计算平均值）。这里有实现这一目标的一种简单方法：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This works fine and it''s only three lines of code. If you aren''t used to
    comprehensions, you may not even think it looks ugly! Now, look at the same code
    using a list comprehension:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这工作得很好，而且只有三行代码。如果你不习惯使用列表推导式，你可能甚至觉得它看起来不丑！现在，看看使用列表推导式的相同代码：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We're down to one line and, importantly for performance, we've dropped an `append`
    method call for each item in the list. Overall, it's pretty easy to tell what's
    going on, even if you're not used to comprehension syntax.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只剩下一行代码，并且对于性能来说，我们为列表中的每个项目去掉了`append`方法的调用。总体来说，即使你不习惯理解这种语法，也很容易看出发生了什么。
- en: The square brackets indicate, as always, that we're creating a list. Inside
    this list is a `for` clause that iterates over each item in the input sequence.
    The only thing that may be confusing is what's happening between the list's opening
    brace and the start of the `for` statement. Whatever expression is provided here
    is applied to *each* of the items in the input list. The item in question is referenced
    by the `num` variable from the `for` clause. So, this expression applies the `int` function
    to each element and stores the resulting integer in the new list.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 方括号始终表示我们正在创建一个列表。在这个列表内部有一个`for`循环，它会遍历输入序列中的每个项目。可能让人感到困惑的是列表开括号和`for`语句开始之间的内容。这里提供的任何表达式都会应用于输入列表中的*每个*项目。所讨论的项目通过`for`循环中的`num`变量来引用。因此，这个表达式将`int`函数应用于每个元素，并将结果整数存储在新的列表中。
- en: Terminology-wise, we call this a **mapping**. We are applying the result expression,
    `int(num)` in this example, to map values from the source iterable to create a
    resulting iterable list.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从术语上讲，我们称这为**映射**。在这个例子中，我们应用结果表达式`int(num)`，将源可迭代对象中的值映射到创建的结果可迭代列表中。
- en: That's all there is to a basic list comprehension. Comprehensions are highly
    optimized, making them far faster than `for` statements when processing a large
    number of items. When used wisely, they're also more readable. These are two compelling
    reasons to use them widely.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 基本列表推导式的全部内容就是这些。推导式经过高度优化，在处理大量项目时比`for`语句要快得多。当明智地使用时，它们也更易于阅读。这就是广泛使用它们的两个强有力的理由。
- en: 'Converting one list of items into a related list isn''t the only thing we can
    do with a list comprehension. We can also choose to exclude certain values by
    adding an `if` statement inside the comprehension. We call this a **filter**.
    Have a look:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个项目列表转换为相关列表并不是列表推导所能做的唯一事情。我们还可以选择通过在推导中添加一个`if`语句来排除某些值。我们称这为**过滤器**。看看这个例子：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The essential difference between this example and the previous one is the `if
    len(num) < 3` clause. This extra code excludes any strings with more than two
    characters. The `if` clause is applied to each element **before** the final `int()` function,
    so it's testing the length of a string. Since our input strings are all integers
    at heart, it excludes any number over 99.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一个示例相比，本质区别在于 `if len(num) < 3` 这条语句。这段额外的代码排除了任何超过两个字符的字符串。`if` 语句应用于每个元素**在**最终
    `int()` 函数之前，因此它是在测试字符串的长度。由于我们的输入字符串本质上都是整数，它排除了任何大于99的数字。
- en: A list comprehension can be used to map input values to output values, applying
    a filter along the way to include or exclude any values that meet a specific condition.
    A great many algorithms involve mapping and filtering operations.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 列推导式可以将输入值映射到输出值，同时在过程中应用过滤器以包含或排除满足特定条件的任何值。许多算法都涉及映射和过滤操作。
- en: Any iterable can be the input to a list comprehension. In other words, anything
    we can wrap in a `for` statement can also be used as the source for a comprehension.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 任何可迭代的对象都可以作为列表推导式的输入。换句话说，我们可以用`for`循环包裹的任何东西也可以用作推导式的来源。
- en: 'For example, text files are iterable; each call to `__next__()` on the file''s
    iterator will return one line of the file. We can examine the lines of a text
    file by naming the open file in the `for` clause of a list comprehension. We can
    then use the `if` clause to extract interesting lines of text. This example finds
    a subset of lines in a test file:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this example, we've added some whitespace to make the comprehension more
    readable (list comprehensions don't *have* to fit on one physical line even though
    they're one logical line). This example creates a list of lines that have the
    "`>>>`" prompt in them. The presence of "`>>>`" suggests there might be a doctest
    example in this file. The list of lines has `rstrip()` applied to remove trailing
    whitespace, like the `\n` that ends each line of text returned by the iterator.
    The resulting list object, `examples`, suggests some of the test cases that can
    be found within the code. (This isn't as clever as doctest's own parser.)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s extend this example to capture the line numbers for each example with
    a "`>>>`" prompt in it. This is a common requirement, and the built-in `enumerate()`
    function helps us pair a number with each item provided by the iterator:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个例子扩展一下，以便捕获包含"`>>>`"提示符的每个示例的行号。这是一个常见的需求，内置的`enumerate()`函数帮助我们将一个数字与迭代器提供的每个项目配对：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `enumerate()` function consumes an iterable, providing an iterable sequence
    of two-tuples of a number and the original item. If the line passes our "`>>>`"
    test, we'll create a two-tuple of the number and the cleaned-up text. We've done
    some sophisticated processing in – effectively – one line of code. Essentially,
    though, it's a filter and a mapping. First it extracts tuples from the source,
    then it filters the lines that match the given `if` clause, then it evaluates
    the `(number, line.rstrip())` expression to create resulting tuples, and finally,
    collects it all into a list object. The ubiquity of this iterate-filter-map-collect
    pattern drives the idea behind a list comprehension.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Set and dictionary comprehensions
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集合和字典推导式
- en: Comprehensions aren't restricted to lists. We can use a similar syntax with
    braces to create sets and dictionaries as well. Let's start with sets. One way
    to create a set is to wrap a list comprehension in the `set()` constructor, which
    converts it to a set. But why waste memory on an intermediate list that gets discarded,
    when we can create a set directly?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example that uses a named tuple to model author/title/genre triples,
    and then retrieves a set of all the authors that write in a specific genre:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We''ve defined a small library of instances of the `Book` class. We can create
    a set from each of these objects by using a set comprehension. It looks a lot
    like a list comprehension, but uses `{}` instead of `[]`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The highlighted set comprehension sure is short in comparison to the demo-data
    setup! If we were to use a list comprehension, of course, Terry Pratchett would
    have been listed twice. As it is, the nature of sets removes the duplicates, and
    we end up with the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that sets don't have a defined ordering, so your output may differ from
    this example. For testing purposes, we'll sometimes set the `PYTHONHASHSEED` environment
    variable to impose an order. This introduces a tiny security vulnerability, so
    it's only suitable for testing.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'Still using braces, we can introduce a colon to make `key:value` pairs required
    to create a dictionary comprehension. For example, it may be useful to quickly
    look up the author or genre in a dictionary if we know the title. We can use a
    dictionary comprehension to map titles to `books` objects:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now, we have a dictionary, and can look up books by title using the normal syntax,
    `fantasy_titles['Nightwatch']`. We've created a high-performance index from a
    lower-performance sequence.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: In summary, comprehensions are not advanced Python, nor are they features that
    subvert object-oriented programming. They are a more concise syntax for creating
    a list, set, or dictionary from an existing iterable source of data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Generator expressions
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes we want to process a new sequence without pulling a new list, set,
    or dictionary into system memory. If we're iterating over items one at a time,
    and don't actually care about having a complete container (such as a list or dictionary)
    created, a container is a waste of memory. When processing one item at a time,
    we only need the current object available in memory at any one moment. But when
    we create a container, all the objects have to be stored in that container before
    we start processing them.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider a program that processes log files. A very simple log
    might contain information in this format:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Log files for popular web servers, databases, or email servers can contain many
    gigabytes of data (one of the authors once had to clean nearly two terabytes of
    logs off a misbehaving system). If we want to process each line in the log, we
    can't use a list comprehension; it would create a list containing every line in
    the file. This probably wouldn't fit in RAM and could bring the computer to its
    knees, depending on the operating system.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: If we used a `for` statement on the log file, we could process one line at a
    time before reading the next one into memory. Wouldn't be nice if we could use
    comprehension syntax to get the same effect?
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: This is where generator expressions come in. They use the same syntax as comprehensions,
    but they don't create a final container object. We call them **lazy**; they reluctantly
    produce values on demand. To create a generator expression, wrap the comprehension
    in `()` instead of `[]` or `{}`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code parses a log file in the previously presented format and
    outputs a new log file that contains only the `WARNING` lines:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We've opened the `sample.log` file, a file perhaps too large to fit in memory.
    A generator expression will filter out the warnings (in this case, it uses the `if` syntax
    and leaves the line unmodified). This is lazy, and doesn't really do anything
    until we consume its output. We can open another file as a subset. The final `for`
    statement consumes each individual line from the `warning_lines` generator. At
    no time is the full log file read into memory; the processing happens one line
    at a time.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run it on our sample file, the resulting `warnings.log` file looks like
    this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Of course, with a short input file, we could have safely used a list comprehension,
    doing all the processing in memory. When the file is millions of lines long, the
    generator expression will have a huge impact on both memory and speed.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: The core of a comprehension is the generator expression. Wrapping a generator
    in `[]` creates a list. Wrapping a generator in `{}` creates a set. Using `{}`
    and `:` to separate keys and values creates a dictionary. Wrapping a generator
    in `()` is still a generator expression, not a tuple.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Generator expressions are frequently most useful inside function calls. For
    example, we can call `sum`, `min`, or `max` on a generator expression instead
    of a list, since these functions process one object at a time. We're only interested
    in the aggregate result, not any intermediate container.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: In general, of the four options, a generator expression should be used whenever
    possible. If we don't actually need a list, set, or dictionary, but simply need
    to filter or apply a mapping to items in a sequence, a generator expression will
    be most efficient. If we need to know the length of a list, or sort the result,
    remove duplicates, or create a dictionary, we'll have to use the comprehension
    syntax and create a resulting collection.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Generator functions
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generator functions embody the essential features of a generator expression,
    which is the generalization of a comprehension. The generator function syntax
    looks even less object-oriented than anything we've seen, but we'll discover that
    once again, it is a syntax shortcut to create a kind of iterator object. It helps
    us build processing following the standard iterator-filter-mapping pattern.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Let's take the log file example a little further. If we want to decompose the
    log into columns, we'll have to do a more significant transformation as part of
    the mapping step. This will involve a regular expression to find the timestamp,
    the severity word, and the message as a whole. We'll look at a number of solutions
    to this problem to show how generators and generator functions can be applied
    to create the objects we want.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a version, avoiding generator expressions entirely:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We''ve defined a regular expression to match three groups:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: The complex date string, `(\w\w\w \d\d, \d\d\d\d \d\d:\d\d:\d\d),` which is
    a generalization of strings like "`Apr 05, 2021 20:04:41`".
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The severity level, `(\w+)`, which matches a run of letters, digits, or underscores.
    This will match words like INFO and DEBUG.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An optional message, `(.*)`, which will collect all characters to the end of
    the line.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This pattern is assigned to the `pattern` variable. As an alternative, we could
    also use `split(' ')` to break the line into space-separated words; the first
    four words are the date, the next word is the severity, and all the remaining
    words are the message. This isn't as flexible as defining a regular expression.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: The decomposition of the line into groups involves two steps. First, we apply
    `pattern.match()` to the line of text to create a `Match` object. Then we interrogate
    the `Match` object for the sequence of groups that matched. We have a `cast(Match[str],
    pattern.match(line))` to tell **mypy** that every line will create a `Match` object.
    The type hint for `re.match()` is `Optional[Match]` because it returns a `None`
    when there's no `Match`. We're using `cast()` to make the claim that every line
    will match, and if it doesn't match, we want this function to raise an exception.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: This deeply nested function seems maintainable, but so many levels of indent
    in so few lines is kind of ugly. More alarmingly, if there is some irregularity
    in the file, and we want to handle the case where the `pattern.match(line)` returns
    `None`, we'd have to include another `if` statement, leading to even deeper levels
    of nesting. Deeply nested conditional processing leads to statements where the
    conditions under which they are executed can be obscure.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: The reader has to mentally integrate all of the preceding `if` statements to
    work out the condition. This can be a problem with this kind of solution.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s consider a truly object-oriented solution, without any shortcuts:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We''ve defined a formal `WarningReformat` iterator that emits the three-tuple
    of the date, warning, and message. We''ve used a type hint of `tuple[str, ...]`
    because it matches the output from the `self.pattern.match(line).groups()` expression:
    it''s a sequence of strings, with no constraint on how many will be present. The
    iterator is initialized with a `TextIO` object, something file-like that has a
    `readline()` method.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: This `__next__()` method reads lines from the file, discarding any lines that
    are not `WARNING` lines. When we encounter a `WARNING` line, we parse it and return
    the three-tuple of strings.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: The `extract_and_parse_2()` function uses an instance of the `WarningReformat`
    class in a `for` statement; this will evaluate the `__next__()` method repeatedly
    to process the subsequent `WARNING` line. When we run out of lines, the `WarningReformat`
    class raises a `StopIteration` exception to tell the function statement we're
    finished iterating. It's pretty ugly compared to the other examples, but it's
    also powerful; now that we have a class in our hands, we can do whatever we want
    with it.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'With that background behind us, we finally get to see true generators in action.
    This next example does *exactly* the same thing as the previous one: it creates
    an object with a `__next__()` method that raises `StopIteration` when it''s out
    of inputs:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `yield` statement in the `warning_filters()` function is the key to generators.
    When Python sees `yield` in a function, it takes that function and wraps it up
    in an object that follows the `Iterator` protocol, not unlike the class defined
    in our previous example. Think of the `yield` statement as similar to the `return` statement;
    it returns a line. Unlike `return`, however, the function is only suspended. When
    it is called again (via `next()`), it will start where it left off – on the line
    after the `yield` statement – instead of at the beginning of the function. In
    this example, there is no line *a**fter* the `yield` statement, so it jumps to
    the next iteration of the `for` statement. Since the `yield` statement is inside
    an `if` statement, it only yields lines that contain `WARNING`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'While it looks like this is just a function looping over the lines, it is actually
    creating a special type of object, a generator object:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: All the function does is create and return a generator object. In this example,
    an empty list was provided, and a generator was built. The generator object has
    `__iter__()` and `__next__()` methods on it, just like the one we created from
    a class definition in the previous example. (Using the `dir()` built-in function
    on it will reveal what else is part of a generator.) Whenever the `__next__()`
    method is called, the generator runs the function until it finds a `yield` statement.
    It then suspends execution, retaining the current state, and returning the value
    from `yield`. The next time the `__next__()` method is called, it restores the
    state and picks up execution where it left off.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'This generator function is nearly identical to this generator expression:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can see how these various patterns align. The generator expression has all
    the elements of the statements, slightly compressed, and in a different order:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17070_10_02.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Generator functions compared with generator expressions'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: A comprehension, then, is a generator wrapped in `[]` or `{}` to create a concrete
    object. In some cases, it can make sense to use `list()`, `set()`, or `dict()`
    as a wrapper around a generator. This is helpful when we're considering replacing
    the generic collection with a customized collection of our own. Changing `list()`
    into `MySpecialContainer()` seems to make the change more apparent.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: The generator expression has the advantage of being short and appearing right
    where it's needed. The generator function has a name and parameters, meaning it
    can be reused. More importantly, a generator function can have multiple statements
    and more complex processing logic in the cases where statements are needed. One
    common reason for switching from a generator expression to a function is to add
    exception handling.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Yield items from another iterable
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often, when we build a generator function, we end up in a situation where we
    want to yield data from another iterable object, possibly a list comprehension
    or generator expression we constructed inside the generator, or perhaps some external
    items that were passed into the function. We'll look at how to do this with the
    `yield from` statement.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Let's adapt the generator example a bit so that instead of accepting an input
    file, it accepts the name of a directory. The idea is to keep our existing warnings
    filter generator in place, but tweak the structure of the functions that use it.
    We'll operate on iterators as both input and result; this way the same function
    could be used regardless of whether the log lines came from a file, memory, the
    web, or another iterator.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'This version of the code illustrates a new `file_extract()` generator. This
    does some basic setup before yielding information from the `warnings_filter()`
    generator:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Our top-level function `extract_and_parse_d()` has a slight change to use the
    `file_extract()` function instead of opening a file and applying the `warnings_filter()`
    to one file. The `file_extract()` generator will yield all of the `WARNING` lines
    from *all* of the files provided in the argument value.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: The `yield from` syntax is a useful shortcut when writing chained generators.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'What''s central in this example is the laziness of each of the generators involved.
    Consider what happens when the `extract_and_parse_d()` function, the client, makes a
    demand:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: The client evaluates `file_extract(log_files)`. Since this is in a `for` statement,
    there's an `__iter__()` method evaluation.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `file_extract()` generator gets an iterator from the `path_iter` iterable,
    and uses this to get the next `Path` instance. The `Path` object is used to create
    a file object that's provided to the `warnings_filter()` generator.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `warnings_filter()` generator uses the file's iterator over lines to read
    until it finds a `WARNING` line, which it parses, yielding exactly one tuple.
    The fewest number of lines were read to find this line.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `file_extract()` generator is yielding from the `warnings_filter()` generator,
    so the single tuple is provided to the ultimate client, the `extract_and_parse_d()`
    function.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `extract_and_parse_d()` function writes the single tuple to the open CSV
    file, and then demands another tuple. This request goes to `file_extract()`, which
    pushes the demand down to `warnings_filter()`, which pushes the demand to an open
    file to provide lines until a `WARNING` line is found.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each generator is lazy and provides one response, doing the least amount of
    work it can get away with to produce the result. This means that a directory with
    a huge number of giant log files is processed by having one open log file, and
    one current line being parsed and processed. It won't fill memory no matter how
    large the files are.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: We've seen how generator functions can provide data to other generator functions.
    We can do this with ordinary generator expressions, also. We'll make some small
    changes to the `warnings_filter()` function to show how we can create a stack
    of generator expressions.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Generator stacks
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The generator function (and the generator expression) for `warnings_filter`
    makes an unpleasant assumption. The use of `cast()` makes a claim to **mypy**
    that''s – perhaps – a bad claim to make. Here''s the example:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The use of `cast()` is a way of claiming the `pattern.match()` will always yield
    a `Match[str]` object. This isn't a great assumption to make. Someone may change
    the format of the log file to include a multiple-line message, and our `WARNING`
    filter would crash every time we encountered a multi-line message.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a message that would cause problems followed by a message that''s easy
    to process:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The first line has the word `WARN` in a multi-line message that will break our
    assumption about lines that contain the word `WARN`. We need to handle this with
    a little more care.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: We can rewrite this generator expression to create a generator function, and
    add an assignment statement (to save the `Match` object) and an `if` statement
    to further decompose the filtering. We can use the walrus operator `:=` to save
    the `Match` object, also.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'We could reframe the generator expression as the following generator function:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As we noted above, this complex filtering tends toward deeply nested `if` statements,
    which can create logic that''s difficult to summarize. In this case, the two conditions
    aren''t terribly complex. An alternative is to change this into a series of map
    and filter stages, each of which does a separate, small transformation on the
    input. We can decompose the matching and filtering into the following:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Map the source line to an `Optional[Match[str]]` object using the `pattern.match()`
    method.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filter to reject any `None` objects, passing only good `Match` objects and applying the
    `groups()` method to create a `List[str]`.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filter the strings to reject the non-`WARN` lines, and pass the `WARN` lines.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each of these stages is a generator expression following the standard pattern.
    We can expand the `warnings_filter` expression into a stack of three expressions:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: These expressions are, of course, utterly lazy. The final `warnings_filter`
    uses the iterable, `group_iter`. This iterable gets matches from another generator,
    `possible_match_iter`, which gets source text lines from the `source` object,
    an iterable source of lines. Since each of these generators is getting items from
    another lazy iterator, there's only one line of data being processed through the
    `if` clause and the final expression clause at each stage of this process.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Note that we can exploit the surrounding `()` to break each expression into
    multiple lines. This can help show the map or filter operation that's embodied
    in each expression.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'We can inject additional processing as long as it fits this essential mapping-and-filtering
    design pattern. Before moving on, we''re going to switch to a slightly more friendly
    regular expression for locating lines in our log file:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This regular expression is broken into three adjacent strings; Python will automatically
    concatenate string literals. The expression uses three named groups. The date-time
    stamp, for example, is group number one, a hard-to-remember bit of trivia. The
    `?P<dt>` inside the `()` means the `groupdict()` method of a `Match` object will
    have the key `dt` in the resulting dictionary. As we introduce more processing
    steps, we'll need to be much more clear about the intermediate results.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an image of the regular expression that can sometimes be helpful:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17070_10_03.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Log line regular expression diagram'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Let's expand this example to transform the date-time stamp to another format.
    This involves injecting a transformation from the input format to the desired
    output format. We can do this in one big gulp, or we can do it in a series of
    small sips.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'This sequence of steps makes it easier to add or change one individual step
    without breaking the entire pipeline of processing:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We've created two additional stages. One parses the input time to create a Python
    `datetime` object; the second stage formats the `datetime` object as an ISO. Breaking
    the transformation down into small steps lets us treat each mapping operation
    and each filtering operating as discrete, separate steps. We can add, change,
    and delete with a little more flexibility when we create these smaller, easier-to-understand
    steps. The idea is to isolate each transformation into a separate object, described
    by a generator expression.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: The result of the `dt_iter` expression is an iterable over anonymous tuples.
    This is a place where a `NamedTuple` can add clarity. See *Chapter 7*, *Python
    Data Structures*, for more information on `NamedTuple`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'We have an additional way to look at these transformational steps, using the
    built-in `map()` and `filter()` functions. These functions provide features similar
    to generator expressions, using another, slightly different syntax:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The lambda objects are anonymous functions. Each lambda is a callable object
    with parameters and a single expression that is evaluated and returned. There's
    no name and no statements in the body of a lambda. Each stage in this pipeline
    is a discrete mapping or filtering operation. While we can combine mapping and
    filtering into a single `map(lambda ..., filter(lambda ..., source))`, this can
    be too confusing to be helpful.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: The `possible_match_iter` applies the `pattern.match()` to each line. The `good_match_iter`
    uses the special `filter(None, source)` that passes non-`None` objects, and rejects
    `None` objects. The `group_iter` uses a lambda to evaluate `m.groups()` for each
    object, `m`, in `good_match_iter`. The `warnings_iter` will filter the `group_iter`
    results, keeping only the `WARN` lines, and rejecting all others. The `dt_iter`
    and the final `warnings_filter` expressions perform a conversion from the source
    datetime format to a generic `datetime` object, followed by formatting the `datetime`
    object in a different string format.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: We've seen a number of ways of approaching a complex map-filter problem. We
    can write nested `for` and `if` statements. We can create explicit `Iterator`
    subclass definitions. We can create iterator-based objects using function definitions
    that include the `yield` statement. This provides us the formal interface of the
    `Iterator` class without the lengthy boilerplate required to define `__iter__()`
    and `__next__()` methods. Additionally, we can use generator expressions and even
    comprehensions to apply the iterator design pattern in a number of common contexts.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: The iterator pattern is a foundational aspect of Python programming. Every time
    we work with a collection, we'll be iterating through the items, and we'll be
    using an iterator. Because iteration is so central, there are a variety of ways
    of tackling the problem. We can use `for` statements, generator functions, generator
    expressions, and we can build our own iterator classes.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Case study
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python makes extensive use of iterators and iterable collections. This underlying
    aspect appears in many places. Each `for` statement makes implicit use of this.
    When we use functional programming techniques, such as generator expressions,
    and the `map()`, `filter()`, and `reduce()` functions, we're exploiting iterators.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Python has an `itertools` module full of additional iterator-based design patterns.
    This is worthy of study because it provides many examples of common operations
    that are readily available using built-in constructs.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply these concepts in a number of places in our case study:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning all the original samples into testing and training subsets.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing a particular *k* and distance hyperparameter set by classifying all
    the test cases.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *k*-nearest neighbors (*k*-NN) algorithm itself and how it locates the *k* nearest
    neighbors from all the training samples.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The common aspect of these three processing examples is the "for all" aspect
    of each one. We'll take a little side-trip into the math behind comprehensions
    and generator functions. The math isn't terribly complex, but the following section
    can be thought of as deep background. After this digression, we'll dive into partitioning
    data into training and testing subsets using the iterator concepts.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: The Set Builder background
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Formally, we can summarize operations like partitioning, testing, and even locating
    nearest neighbors with a logic expression. Some developers like the formality
    of it because it can help describe the processing without forcing a specific Python
    implementation.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the essential rule for partitioning, for example. This involves a "for
    all" condition that describes the elements of a set of samples, *S*:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17070_10_001.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: In other words, for all *s* in the universe of available samples, *S*, the value
    of *s* is either in the training set, *R*, or the testing set, *E*. This summarizes
    the result of a successful partition of the data. It doesn't describe an algorithm,
    directly, but having this rule can help us be sure we haven't missed something
    important.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also summarize a performance metric for testing. The recall metric has
    a "for all" implied by the *∑* construct:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17070_10_002.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: The quality score, *q*, is the sum for all *e* in the testing set, *E*, of 1
    where the `knn()` classifier applied to *e* matches the species for *e*, `s(e)`,
    otherwise 0\. This can map nicely to a Python generator expression.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'The *k*-NN algorithm involves a bit more complexity in its definition. We can
    think of it as a partitioning problem. We need to start with a collection of ordered
    pairs. Each pair is the distance from an unknown, *u*, to a training sample, *r*,
    summarized as ![](img/B17070_10_003.png). As we saw in *Chapter 3*, there are
    a number of ways to compute this distance. This has to be done for all training
    samples, *r,* in the universe of training samples, *R*:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17070_10_004.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: 'Then we need to partition these distances into two subsets, *N*, and *F* (near
    and far) such that all distances in *N* are less than or equal to all distances
    in *F*:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17070_10_005.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: We also need to be sure the number of elements in the near set, *N,* is equal
    to the desired number of neighbors, *k*.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: This final formalism exposes an interesting nuance to the computation. What
    if there are more than *k* neighbors with the same distance metric? Should *all* of
    the equidistant training samples be included in voting? Or should we arbitrarily
    slice exactly *k* of the equidistant samples? If we "arbitrarily" slice, what's
    the exact rule that gets used for choosing among the equidistant training samples?
    Does the selection rule even matter? These could be significant issues, and they're
    outside the scope of this book.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: The example later in the chapter uses the `sorted()` function, which tends to
    preserve the original order. Can this lead to a bias to our classifier when confronted
    with equidistant choices? This, too, may be a significant issue, and it's also
    outside the scope of this book.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Given a little bit of set theory, we can tackle the ideas of partitioning the
    data, and computing the *k* nearest neighbors, making use of the common iterator
    features. We'll start with the partitioning algorithm's implementation in Python.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Multiple partitions
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our goal is to separate testing and training data. There's a tiny bump in the
    road, however, called **deduplication**. The statistical measures of overall quality
    rely on the training and testing sets being independent; this means we need to
    avoid duplicate samples being split between testing and training sets. Before
    we can create testing and training partitions, we need to find any duplicates.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: We can't – easily – compare each sample with all of the other samples. For a
    large set of samples, this may take a very long time. A pool of ten thousand samples
    would lead to 100 million checks for duplication. This isn't practical. Instead,
    we can partition our data into subgroups where the values for all the measured
    features are *likely* to be equal. Then, from those subgroups, we can choose testing
    and training samples. This lets us avoid comparing every sample with all of the
    other samples to look for duplicates.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use Python''s internal hash values, we can create buckets containing
    samples that may have equal values. In Python, if items are equal, they must have
    the same integer hash value. The inverse is not true: items may coincidentally
    have the same hash value, but may not actually be equal.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, we can say this:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17070_10_006.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: That is, if two objects in Python, *a* and *b*, are equal, they must also have
    the same hash value ![](img/B17070_10_007.png). The reverse is not true because
    equality is more than a simple hash value check; it's possible that ![](img/B17070_10_008.png);
    the hash values may be the same, but the underlying objects aren't actually equal.
    We call this a "hash collision" of two unequal values.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing this thought, the following is a matter of definition for modulo:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17070_10_009.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: If two values are equal, they are also equal to any modulo of those values.
    When we want to know if `a == b`, we can ask if `a % 2 == b % 2`; if both numbers
    are odd or both numbers are even, then there's a chance `a` and `b` could be equal.
    If one number is even and the other is odd, there's no way they can be equal.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: For complex objects, we can use `hash(a) % m == hash(b) % m`. If the two hash
    values, modulo *m*, are the same, then the hash values could be the same, and
    the two objects, `a` and `b`, could also be equal. We know it's possible for several
    objects to have the same hash value, and even more objects to have the same hash
    value modulo *m*.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: While this doesn't tell us if two items are equal, this technique limits the
    domain of objects required for exact equality comparison to very small pools of
    a few items instead of the entire set of all samples. We can avoid duplicates
    if we avoid splitting up one of these subgroups.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a view of seven samples broken into three subgroups based on their
    hash codes modulo 3\. Most of the subgroups have items that are potentially equal,
    but actually aren''t equal. One of the groups has an actual duplicate sample:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17070_10_04.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: Partitioning sample data to locate duplicates'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: To find the duplicate sample, we don't need to compare each sample against the
    other six. We can look within each subgroup and compare a few samples to see if
    they happen to be duplicates.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind this deduplication approach is to separate the entire suite
    of samples into sixty buckets where the samples have equal hash values, modulo
    sixty. Samples in the same bucket *could* be equal, and as a simple expedient,
    we can treat them as equal. Samples in separate buckets have different hash values
    and cannot possibly be equal.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: We can avoid having duplicate samples in both testing and training by using
    an entire bucket's set of samples together. That way, the duplicates are either
    all testing or all training, but never split.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a partition function that first creates 60 separate buckets for samples.
    Then, some fraction of the buckets are allocated for testing, the rest for training.
    Specifically, 12, 15, or 20 buckets out of 60 are about 20%, 25%, or 33% of the
    population. Here''s an implementation that deduplicates as it partitions into
    testing and training subsets:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'There are three steps in this partitioning:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: We create sixty separate lists of samples that – because the hashes are equal
    – may have duplicates. We keep these batches together to avoid splitting duplicates
    so they're in both testing and training subsets.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We build two lists of iterators. Each list has an iterator over a subset of
    the buckets. The `training_rule()` function is used to make sure we get 12/60,
    15/60, or 20/60 buckets in testing, and the rest in training. Since each of these
    iterators is lazy, these lists of iterators can be used to accumulate samples.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we use the `itertools.chain` to consume values from a sequence of generators.
    A chain of iterators will consume the items from each of the various individual
    bucket-level iterators to create the two final partitions of samples.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that the type hint for `ModuloDict` defines a subtype of the generic `DefaultDict`.
    It provides a key of `int` and the value will be a `list[KnownSample]` instances.
    We've provided this named type to avoid repeating the long definition of the dictionaries
    we'll be working with.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'The `itertools.chain()` is a pretty clever kind of iterator. It consumes data
    from other iterators. Here''s an example:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We created two `range()` objects, `p1`, and `p2`. A chain object will be an
    iterator, and we used the `list()` function to consume all the values.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: The steps above can create a large mapping as an intermediate data structure.
    It also creates sixty generators, but these don't require very much memory. The
    final two lists contain references to the same `Sample` objects as the partitioning
    dictionary. The good news is the mapping is temporary and only exists during this
    function.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: This function also depends on a `training_rule()` function. This function has
    a type hint of `Callable[[int], bool]`. Given the index value for a partition
    (a value from 0 to 59, inclusive), we can assign it to a testing or training partition.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use different implementations to get to 80%, 75%, or 66% testing data.
    For example:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The above lambda object will perform a 75% training and 25% testing split.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Once we've partitioned the data, we can use iterators for classifying samples
    as well as testing the quality of our classification process.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Testing
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The testing process can also be defined as a higher-order function, a function
    that accepts a function as a parameter value. We can summarize the testing effort
    as a map-reduce problem. Given a `Hyperparameter` with a *k* value and a distance
    algorithm, we need to use an iterator for the following two steps:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: A function classifies all test samples, mapping each test sample to 1 if the
    classification was correct or 0 for an incorrect classification. This is the map
    part of map-reduce.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function computes a summary with the count of correct values from the long
    sequence of actual classified samples. This is the reduce part of map-reduce.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python provides high-level functions for these map and reduce operations. This
    allows us to focus on the details of the mapping and ignore the boilerplate part
    of iterating over the data items.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Looking forward to the next section, we'll want to refactor the `Hyperparameter`
    class to split the classifier algorithm into a separate, standalone function.
    We'll make the classifier function a **Strategy** that we provide when we create
    an instance of the `Hyperparameter` class. Doing this means we can more easily
    experiment with some alternatives. We'll look at three different ways to approach
    refactoring a class.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s one definition that relies on an external classifier function:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The `test()` method uses two mapping operations and a reduce operation. First,
    we define a generator that will map each testing sample to a `ClassifiedKnownSample`
    object. This object has the original sample and the results of the classification.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Second, we define a generator that will map each `ClassifiedKnownSample` object
    to a 1 (for a test that matched the expected species) or a 0 (for a test that
    failed). This generator depends on the first generator to provide values.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'The actual work is the summation: this consumes values from the second generator.
    The second generator consumes objects from the first generator. This technique
    can minimize the volume of data in memory at any one time. It also decomposes
    a complex algorithm into two separate steps, allowing us to make changes as necessary.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: There's an optimization available here, also. The value of `t.classification` in
    the second generator is `self.classify(t.sample.sample)`. It's possible to reduce
    this to a single generator and eliminate creating intermediate `ClassifiedKnownSample`
    objects.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how the test operation looks. We can build a `Hyperparameter` instance
    using a function for distance, `manhattan()`, and a classifier function, `k_nn_1()`:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We'll look at the implementations of various classifiers in the next two sections.
    We'll start with the base definition, `k_nn_1()`, and then look at one based on
    the `bisect` module next.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: The essential k-NN algorithm
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can summarize the *k*-NN algorithm as having the following steps:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Create a list of all (distance, training sample) pairs.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort these in ascending order.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pick to the first *k*, which will be the *k* nearest neighbors.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chose the mode (the highest frequency) label for the *k* nearest neighbors.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The implementation would look like this:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: While clear, this does accumulate a large number of distance values in the `distances`
    list object, when only *k* are actually needed. The `sorted()` function consumes
    the source generator and creates a (potentially large) list of intermediate values.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: One of the high-cost parts of this specific *k*-NN algorithm is sorting the
    entire set of training data after the distances have been computed. We summarize
    the complexity with the description as an *O(n log n)* operation. A way to avoid
    cost is to avoid sorting the entire set of distance computations.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '*Steps 1* to *3* can be optimized to keep only the *k* smallest distance values.
    We can do this by using the `bisect` module to locate the position in a sorted
    list where a new value can be inserted. If we only keep values that are smaller
    than the *k* values in the list, we can avoid a lengthy sort.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: k-NN using the bisect module
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here''s an alternative implementation of *k*-NN that tries to avoid sorting
    all of the distance computations:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'For each training sample:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the distance from this training sample to the unknown sample.
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If it's greater than the last of the *k* nearest neighbors seen so far, discard
    the new distance.
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Otherwise, find a spot among the *k* values; insert the new item; truncate the
    list to length *k*.
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the frequencies of result values among the *k* nearest neighbors.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the mode (the highest frequency) among the *k* nearest neighbors.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If we seed the list of *k* nearest neighbors with floating point infinity values,
    ![](img/B17070_10_011.png) to mathematicians, `float("inf")` in Python, then the
    first few computed distances, *d*, will be kept because ![](img/B17070_10_012.png).
    After the first *k* distances have been computed, the remaining distances must
    be smaller than one of the *k* neighbor''s distances to be relevant:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Instead of sorting all distances into a big list, we''re inserting (and removing)
    one distance from a much smaller list. After the first *k* distances are computed,
    this algorithm involves two kinds of state change: a new item is inserted into
    the *k* nearest neighbors, and the most distant of the *k+1* neighbors is removed.
    While this doesn''t change the overall complexity in a dramatic way, these are
    relatively inexpensive operations when performed on a very small list of only
    *k* items.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: k-NN using the heapq module
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have yet another trick up our sleeve. We can use the `heapq` module to maintain
    a sorted list of items. This lets us implement the sorting operation as each item
    is placed into the overall list. This doesn't reduce the general complexity of
    the processing, but it replaces two inexpensive insert and pop operations with *potentially* less
    expensive insert operations.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to start with an empty list and insert items into the list, ensuring
    that (a) the items are kept in order, and (b) the item at the head of the list
    always has the least distance. The heap queue algorithm can maintain an upper
    bound on the size of the queue. Keeping only *k* items should also reduce the
    volume of data required in memory.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: We can then pop *k* items from the heap to retrieve the nearest neighbors.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This is elegantly simple. It's not, however, remarkably fast. It turns out that
    the cost of computing the distances outweighs the cost savings from using a more
    advanced heap queue to reduce the number of items being sorted.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can compare these distinct *k*-NN algorithms by providing a consistent set
    of training and test data. We''ll use a function like the following:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We've created a consistent `Hyperparameter` instance. Each instance has a common
    value of *k* and a common distance function; they have a distinct classifier algorithm.
    We can execute the `test()` method and display the time required.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'A `main()` function can use this to examine the various classifiers:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We've applied each of the classifiers to a consistent set of data. We haven't
    shown the `a_lot_of_data()` function. This creates two lists of `TrainingKnownSample`
    and `TestingKnownSample` instances. We've left this as an exercise for the reader.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the performance results comparing these alterative *k*-NN algorithms:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '| algorithm | test quality | time |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
- en: '| k_nn_1 | q= 241/ 1000 | 6.553s |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
- en: '| k_nn_b | q= 241/ 1000 | 3.992s |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
- en: '| k_nn_q | q= 241/ 1000 | 5.294s |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
- en: The test quality is the number of correct test cases. The number is low because
    the data is completely random, and a correct classification rate of about 25%
    is what's expected if our random data uses four different species names.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: The original algorithm, `k_nn_1`, is the slowest, something we suspected. This
    provides the necessary evidence that optimization of this may be necessary. The `bisect` based
    processing, row `k_nn_b` in the table, suggests that working with a small list
    outweighs the costs of performing the bisect operation many times. The `heapq` processing
    time, row `k_nn_h`, was better than the original algorithm, but only by about
    20%.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: It's important to do both a theoretical analysis of the algorithm's complexity
    as well as a benchmark with actual data. Before spending time and effort on performance
    improvement, we need to start with benchmark analysis to identify where we might
    be able to do things more efficiently. It's also important to confirm that the
    processing is correct before trying to optimize performance.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'In some cases, we''ll need detailed analysis of specific functions or even
    Python operators. The `timeit` module can be helpful here. We might need to do
    something like the following:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The value computed for `m` can help us make a concrete comparison between distance
    computations. The `timeit` module will execute the given statement, `manhattan(d1,
    d2)`, after performing the one-time setup of some imports and creation of sample
    data.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Iterators are both a performance boost and a potential way to clarify the overall
    design. They can be helpful with our case study because so much of the processing
    iterates over large collections of data.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter looked at a design pattern that seems ubiquitous in Python, the
    iterator. The Python iterator concept is a foundation of the language and is used
    widely. In this chapter we examined a number of aspects:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Design patterns are good ideas we see repeated in software implementations,
    designs, and architectures. A good design pattern has a name, and a context where
    it's usable. Because it's only a pattern, not reusable code, the implementation
    details will vary each time the pattern is followed.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Iterator` protocol is one of the most powerful design patterns because
    it provides a consistent way to work with data collections. We can view strings,
    tuples, lists, sets, and even files as iterable collections. A mapping contains
    a number of iterable collections including the keys, the values, and the items
    (key and value pairs.)
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List, set, and dictionary comprehensions are short, pithy summaries of how to
    create a new collection from an existing collection. They involve a source iterable,
    an optional filter, and a final expression to define the objects in the new collection.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generator functions build on other patterns. They let us define iterable objects
    that have map and filter capabilities.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you don't use comprehensions in your daily coding very often, the first thing
    you should do is search through some existing code and find some `for` loops.
    See whether any of them can be trivially converted to a generator expression or
    a list, set, or dictionary comprehension.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Test the claim that list comprehensions are faster than `for` loops. This can
    be done with the built-in `timeit` module. Use the help documentation for the
    `timeit.timeit` function to find out how to use it. Basically, write two functions
    that do the same thing, one using a list comprehension, and one using a `for` loop
    to iterate over several thousand items. Pass each function into `timeit.timeit`,
    and compare the results. If you're feeling adventurous, compare generators and
    generator expressions as well. Testing code using `timeit` can become addictive,
    so bear in mind that code does not need to be hyperfast unless it's being executed
    an immense number of times, such as on a huge input list or file.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Play around with generator functions. Start with basic iterators that require
    multiple values (mathematical sequences are canonical examples; the Fibonacci
    sequence is overused if you can't think of anything better). Try some more advanced
    generators that do things such as take multiple input lists and somehow yield
    values that merge them. Generators can also be used on files; can you write a
    simple generator that shows lines that are identical in two files?
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Extend the log processing exercise to replace the `WARNING` filter with a time
    range filter; all the messages between Jan 26, 2015 11:25:46 and Jan 26, 2015
    11:26:15, for example.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Once you can find `WARNING` lines or lines within a specific time, combine the
    two filters to select only the warnings within the given time. You can use an
    `and` condition within a single generator, or combine multiple generators, in
    effect building an `and` condition. Which seems more adaptable to changing requirements?
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: When we presented the class `WarningReformat(Iterator[Tuple[str, ...]]):` example
    of an iterator, we made a questionable design decision. The `__init__()` method
    accepted an open file as an argument value and the `__next__()` method used `readline()`
    on that file. What if we change this slightly and create an explicit iterator
    object that we use inside another iterator?
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: If we make this change, then `__next__()` can use `line = next(self.insequence)`
    instead of `line = self.insequence.readline()`. Switching from `object.readline()`
    to `next(object)` is an interesting generalization. Does it change anything about
    the `extract_and_parse_2()` function? Does it permit us to use generator expressions
    along with the `WarningReformat` iterator?
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Take this one further step. Refactor the `WarningReformat` class into two separate
    classes, one to filter for `WARN` and a separate class to parse and reformat each
    line of the input log. Rewrite the `extract_and_parse_2()` function using instances
    of these two classes. Which is "better"? What metric did you use to evaluate "better"?
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: The case study summarized the *k*-NN algorithm as a kind of comprehension to
    compute distance values, sort and pick the *k* nearest. The case study didn't
    talk much about the partitioning algorithm to separate training data from test
    data. This, too, seems like it might work out as a pair of list comprehensions.
    There's an interesting problem here, though. We'd like to create two lists, reading
    the source exactly once. This isn't easily done with list comprehensions. However,
    look at the `itertools` module for some possible designs. Specifically, the `itertools.tee()`
    function will provide multiple iterables from a single source.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Look at the recipes section of the `itertools` module. How can the `itertools.partition()`
    function be used to partition data?
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned that design patterns are useful abstractions that
    provide best-practice solutions for common programming problems. We covered our
    first design pattern, the iterator, as well as numerous ways that Python uses
    and abuses this pattern for its own nefarious purposes. The original iterator
    pattern is extremely object-oriented, but it is also rather ugly and verbose to
    code around. However, Python's built-in syntax abstracts the ugliness away, leaving
    us with a clean interface to these object-oriented constructs.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Comprehensions and generator expressions can combine container construction
    with iteration in a single line. Generator functions can be constructed using
    the `yield` syntax.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: We'll cover several more design patterns in the next two chapters.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
