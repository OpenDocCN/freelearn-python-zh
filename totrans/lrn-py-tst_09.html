<html><head></head><body>
  <div><div><div><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Other Tools and Techniques</h1></div></div></div><p>We've covered the core elements of testing in Python, but there are a number of peripheral methods and tools that will make your life easier. In this chapter, we're going to go through several of them in brief.</p><p>In this chapter, we're going to:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Discuss code coverage and how to get a code coverage report from Nose</li><li class="listitem" style="list-style-type: disc">Discuss continuous integration and Buildbot</li><li class="listitem" style="list-style-type: disc">Learn how to integrate automated testing with Git, Mercurial, Bazaar, and Subversion</li></ul></div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec63"/>Code coverage</h1></div></div></div><p>Tests tell you <a id="id394" class="indexterm"/>when the code you're testing doesn't work the way you thought it would, but they don't tell you a thing about the code you're not testing. They don't even tell you that the code you're not testing isn't being tested.</p><p>Code coverage is a technique to address that shortcoming. A code coverage tool watches while your tests are running, and keeps track of which lines of code are (and aren't) executed. After the tests have run, the tool will give you a report describing how well your tests cover the whole body of code.</p><p>It's desirable to have the coverage approach 100 percent, as you probably figured out already. Be careful not to focus on the coverage number too intensely, though, because it can be a bit misleading. Even if your tests execute every line of code in the program, they can easily not test everything that needs to be tested. This means that you can't take 100 percent coverage as certain proof that your tests are complete. On the other hand, there are times when some code really, truly doesn't need to be covered by the tests—some debugging support code, for example, or code generated by a user interface builder—and so less than 100 percent coverage is often completely acceptable.</p><p>Code coverage is a tool to give you an insight into what your tests are doing, and what they might be overlooking. It's not the definition of a good test suite.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec71"/>Installing coverage.py</h2></div></div></div><p>We're going to be <a id="id395" class="indexterm"/>working with a module called <code class="literal">coverage.py</code>, which is—unsurprisingly—a code coverage tool for Python.</p><p>Since <code class="literal">coverage.py</code> isn't built in to Python, we're going to need to download and install it. You can <a id="id396" class="indexterm"/>download the latest version from the Python Package Index at <a class="ulink" href="http://pypi.python.org/pypi/coverage">http://pypi.python.org/pypi/coverage</a>, but it will probably <a id="id397" class="indexterm"/>be easier simply to type the following from the command line:</p><div><pre class="programlisting"><strong>python3 -m pip install --user coverage</strong></pre></div><p>We're going to walk through the steps of using <code class="literal">coverage.py</code> here, but if you want more information you <a id="id398" class="indexterm"/>can find it on the <code class="literal">coverage.py</code> home page at <a class="ulink" href="http://nedbatchelder.com/code/coverage/">http://nedbatchelder.com/code/coverage/</a>.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec72"/>Using coverage.py with Nose</h2></div></div></div><p>We're going to <a id="id399" class="indexterm"/>create a little toy code module with tests, and then apply <code class="literal">coverage.py</code> to find out how much of the code the tests actually use.</p><p>Put the following <a id="id400" class="indexterm"/>test code into <code class="literal">test_toy.py</code>. There are <a id="id401" class="indexterm"/>several problems with these tests, which we'll discuss later, but they ought to run:</p><div><pre class="programlisting">from unittest import TestCase
import toy

class test_global_function(TestCase):
    def test_positive(self):
        self.assertEqual(toy.global_function(3), 4)

    def test_negative(self):
        self.assertEqual(toy.global_function(-3), -2)

    def test_large(self):
        self.assertEqual(toy.global_function(2**13), 2**13 + 1)

class test_example_class(TestCase):
    def test_timestwo(self):
        example = toy.Example(5)
        self.assertEqual(example.timestwo(), 10)</pre></div><p>What we have here is a couple of <code class="literal">TestCase</code> classes with some very basic tests in them. These tests wouldn't be of much use in a real-world situation, but all we need them for is to illustrate how the code coverage tool works.</p><p>Put the following code into <code class="literal">toy.py</code>. Notice the <code class="literal">if __name__ == '__main__'</code> clause at the bottom; we haven't dealt with one of these in a while, so I'll remind you that the code inside that block <a id="id402" class="indexterm"/>runs <code class="literal">doctest</code> if we were to run the module <a id="id403" class="indexterm"/>with Python <code class="literal">toy.py</code>:</p><div><pre class="programlisting">def global_function(x):
    r"""
    &gt;&gt;&gt; global_function(5)
    6
    """
    return x + 1

class Example:
    def __init__(self, param):
        self.param = param

    def timestwo(self):
        return self.param * 2

    def __repr__(self):
        return 'Example({!r})'.format(self.param)

if __name__ == '__main__':
    import doctest
    doctest.testmod()</pre></div><p>Here, we have the code that satisfies the tests we just wrote. Like the tests themselves, this code wouldn't be of much use, but it serves as an illustration.</p><p>Go ahead and run Nose. It should find the tests, run them, and report that all is well. The problem is that some of the code isn't ever tested. Let's run the tests again, only this time we'll tell Nose to use <code class="literal">coverage.py</code> to measure coverage while it's running the tests:</p><div><pre class="programlisting"><strong>python -m nose --with-coverage --cover-erase</strong></pre></div><p>This should give <a id="id404" class="indexterm"/>us an error report that looks like this:</p><div><pre class="programlisting"><strong>.....</strong>
<strong>Name    Stmts   Miss  Cover   Missing</strong>
<strong>-------------------------------------</strong>
<strong>toy        12      3    75%   16, 19-20</strong>
<strong>----------------------------------------------------------------------</strong>
<strong>Ran 5 tests in 0.053s</strong>

<strong>OK</strong></pre></div><p>The dots at the top indicate passing tests, and the <code class="literal">OK</code> at the bottom says that the testing procedure worked as expected, but the part in between is new. That's our coverage report. Apparently, our tests only cover three quarters of our code: out of the 12 statement lines in <code class="literal">toy.py</code>, three didn't get executed. These lines were <code class="literal">16</code> and <code class="literal">19</code> through <code class="literal">20</code>.</p><div><div><h3 class="title"><a id="tip18"/>Tip</h3><p>The range <code class="literal">19-20</code> isn't any more useful than writing <code class="literal">19</code>, <code class="literal">20</code> would have been, but larger contiguous groups of lines are reported in the same way. That's a lot easier to parse, visually, than a soup of separate line numbers would be, especially when it's a range like <code class="literal">361-947</code>.</p></div></div><p>When we passed <code class="literal">--with-coverage</code> and <code class="literal">--cover-erase</code> as command-line parameters to Nose, what did they do? Well, <code class="literal">--with-coverage</code> is pretty straightforward: it told Nose to look for <code class="literal">coverage.py</code> and to use it while the tests get executed. That's just what we wanted. The second parameter, <code class="literal">--cover-erase</code>, tells Nose to forget about any coverage information that was acquired during previous runs. By default, coverage information is aggregated across all of the uses of <code class="literal">coverage.py</code>. This allows you to run a set of tests using <a id="id405" class="indexterm"/>different testing frameworks or mechanisms, and then check the cumulative coverage. You still want to erase the data from previous test <a id="id406" class="indexterm"/>runs at the beginning of <a id="id407" class="indexterm"/>this process, though, and the <code class="literal">--cover-erase</code> command line is how you tell Nose to tell <code class="literal">coverage.py</code> that you're starting a new.</p><div><div><h3 class="title"><a id="tip19"/>Tip</h3><p>Nose, being an integrated testing system, often renders the need to aggregate coverage information that is negligible. You'll almost always want <code class="literal">--cover-erase</code> when you invoke Nose with coverage enabled, so you should consider adding <code class="literal">cover-erase=1</code> to your Nose configuration file, as discussed in previous chapters.</p></div></div><p>Another useful Nose command-line option is <code class="literal">--cover-package=PACKAGE</code>, which limits the coverage report to the specific package you're interested in. It didn't show up in our toy because we didn't import anything, but normally the coverage report includes every module or package that has code executed while your tests are running. The percentage of the standard library that is covered by your tests is usually not useful information. It can be convenient to limit the report to the things you actually want to know.</p><p>So, back to our coverage report. The missing lines were line <code class="literal">16</code> and lines <code class="literal">19</code> through <code class="literal">20</code>. Looking back at our code, we see that line <code class="literal">16</code> is the <code class="literal">__repr__</code> method. We really should have tested that, so the coverage check has revealed a hole in our tests that we should fix. Lines <code class="literal">19</code> and <code class="literal">20</code> are just code to run <code class="literal">doctest</code>, though. They're not something that we ought to be using under production conditions, so we can just ignore that coverage hole.</p><p>Code coverage can't detect problems with the tests themselves, in most cases. In the previous test code, the test for the <code class="literal">timestwo</code> method violates the isolation of units and invokes two different methods of <code class="literal">example_class</code>. Since one of the methods is the constructor, this might <a id="id408" class="indexterm"/>be acceptable, but the coverage checker isn't in a position to even see that there might be a problem. All it saw was more lines of code being covered. That's not a problem—it's how a coverage checker ought to work—but <a id="id409" class="indexterm"/>it's something to keep in mind. Coverage <a id="id410" class="indexterm"/>is useful, but high coverage doesn't equal good tests.</p></div></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec64"/>Version control integration</h1></div></div></div><p>Most version <a id="id411" class="indexterm"/>control systems have the ability to run a program you've written in response to various events, as a way of customizing the version control system's <a id="id412" class="indexterm"/>behavior. These programs are commonly called <strong>hooks</strong>.</p><p>You can do all kinds of things by installing the right hook programs, but we're only going to focus on one use. We can make the version control program automatically run our tests, when we commit a new version of the code to the version control repository.</p><p>This is a fairly nifty trick, because this makes it difficult for test-breaking bugs to get into the repository unnoticed. Somewhat like code coverage, though, there's potential for trouble if it becomes a matter of policy rather than simply being a tool to make your life easier.</p><p>In most systems, you can write the hooks so that it's impossible to commit code that breaks tests. This might sound like a good idea at first, but it's really not. One reason for this is that one of the major purposes of a version control system is communication between developers, and interfering with that tends to be unproductive in the long run. Another reason is that it prevents anybody from committing partial solutions to problems, which means that things tend to get dumped into the repository in big chunks. Big commits are a problem because they make it hard to keep track of what changed, which adds to the confusion. There are better ways to make sure that you always have a working code base socked <a id="id413" class="indexterm"/>away somewhere, such as version control branches.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec73"/>Git</h2></div></div></div><p>Git has become the most <a id="id414" class="indexterm"/>widely used distributed version control system, so we'll start there. By virtue of its being distributed, and thus decentralized, each Git user can control their own hooks. Cloning a repository will not clone the hooks for that repository.</p><p>If you don't have Git <a id="id415" class="indexterm"/>installed and don't plan to use it, you can skip this section.</p><p>Git hooks are stored in the <code class="literal">.git/hooks/</code> subdirectory of the repository, each in its own file. The ones that we're interested in are the <code class="literal">pre-commit</code> hook and the <code class="literal">prepare-commit-msg</code> hook, either of which would potentially be suitable to our purposes.</p><p>All Git hooks are programs that Git executes automatically at a specific time. If a program named <code class="literal">pre-commit</code> exists in the <code class="literal">hooks</code> directory, it is run before a commit happens to check whether the commit is valid. If a program named <code class="literal">prepare-commit-msg</code> exists in the <code class="literal">hooks</code> directory, it is run to modify the default commit message that is presented to the user.</p><p>So, the <code class="literal">pre-commit</code> <a id="id416" class="indexterm"/>hook is the one we want if we want the failed tests to abort the commit (which is acceptable with Git, though I still don't recommend it because there's a command-line option, <code class="literal">--no-verify</code>, that allows the user to commit even if the tests fail). We can also run the tests from <code class="literal">pre-commit</code> and print the error report to the screen, while allowing the commit, regardless of the result, by simply producing a zero result code after we invoke Nose, instead of passing on the Nose result code.</p><p>If we want to get fancier and add the test report to the commit message, or include it in the commit message file that will be shown to the user without actually adding it to the commit message, we need the <code class="literal">prepare‑commit‑msg</code> hook instead. This is what we're going to do in our example.</p><div><div><div><div><h3 class="title"><a id="ch09lvl3sec07"/>Example test-runner hook</h3></div></div></div><p>As I mentioned, Git hooks are programs, which means that we can write them in Python. If you place the <a id="id417" class="indexterm"/>following code in a file named <code class="literal">.git/hooks/prepare-commit-msg</code> (and make it executable) within one of your <a id="id418" class="indexterm"/>Git repositories, your Nose test suite will be run before each commit, and the test report will be presented to you when you are prompted for a commit message, but commented out so as to not actually end up in the Git log. If the tests convince you that you don't want to commit yet, all you have to do is leave the message blank to abort the commit.</p><div><div><h3 class="title"><a id="tip20"/>Tip</h3><p>In Windows, a file named <code class="literal">prepare-commit-msg</code> won't be executable. You'll need to name the actual hook program <code class="literal">prepare-commit-msg.py</code> and create a batch file named <code class="literal">prepare-commit-msg.bat</code> containing the following (assuming you have Python's program directory in the <code class="literal">PATH</code> environment variable):</p><div><pre class="programlisting">@echo offpythonw prepare-commit-msg.py</pre></div><p>This is the first time I've mentioned the <code class="literal">pythonw</code> command. It's a Windows-specific version of the Python interpreter with only one difference from the normal Python program: it does not open a terminal window for text-mode interactions. When a program is run via <code class="literal">pythonw</code> on Windows, nothing is visible to the user unless the program intentionally creates a user interface.</p></div></div><p>So, without <a id="id419" class="indexterm"/>further ado, here is the Python program for a Git <code class="literal">prepare-commit-msg</code> hook that integrates Nose:</p><div><pre class="programlisting">#!/usr/bin/env python3
from sys import argv
from subprocess import check_output, CalledProcessError, STDOUT

PYTHON = ['pythonw', 'python']
NOSE = ['-m', 'nose', '--with-coverage', '--cover-erase']

lines = ['', '# Nose test report:']

report = None

try:
    for command in PYTHON:
        try:
            report = check_output([command] + NOSE,
                                  stderr=STDOUT,
                                  universal_newlines=True)
        except FileNotFoundError:
            pass
        else:
            break
except CalledProcessError as x:
    report = x.output

if report is None:
    lines.append('#    Unable to run Python.')
else:
    for line in report.splitlines():
        if not line:
            lines.append('')
        else:
            lines.append('# ' + line)

with open(argv[1], 'r') as f:
    lines.append(f.read())

with open(argv[1], 'w') as f:
    f.write('\n'.join(lines))</pre></div><p>Now, whenever <a id="id420" class="indexterm"/>you run a Git <code class="literal">commi</code>t command, you'll get a Nose report:</p><div><pre class="programlisting"><strong>git commit -a</strong></pre></div></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec74"/>Subversion</h2></div></div></div><p>Subversion is the most <a id="id421" class="indexterm"/>popular freely available centralized version control system. There is a single server tasked with keeping track of <a id="id422" class="indexterm"/>everybody's changes, and this server also handles running hooks. This means that there is a single set of hooks that applies to everybody, probably under the control of a system administrator.</p><p>If you don't have Subversion installed and don't plan on using it, you can skip this section.</p><p>Subversion hooks are stored in files in the <code class="literal">hooks</code> subdirectory of the server's repository. Because Subversion operates on a centralized, client-server architecture, we're going to need both a client and a server setup for this example. They can both be on the same computer, but they'll be in different directories.</p><p>Before we can work with concrete examples, we need a Subversion server. You can create one by making a new directory called <code class="literal">svnrepo</code>, and executing the following command:</p><div><pre class="programlisting"><strong>$ svnadmin create svnrepo/</strong></pre></div><p>Now, we need to configure the server to accept commits from us. To do this, we open the file called <code class="literal">conf/passwd</code>, and add the following line at the bottom:</p><div><pre class="programlisting">testuser = testpass</pre></div><p>Then we need to edit <code class="literal">conf/svnserve.conf</code>, and change the line reading:</p><div><pre class="programlisting"># password-db = passwd</pre></div><p>into the following:</p><div><pre class="programlisting">password-db = passwd</pre></div><p>The Subversion server needs to be running before we can interact with it. This is done by making sure that we're in the <code class="literal">svnrepo</code> directory and then run the command:</p><div><pre class="programlisting"><strong>svnserve -d -r ..</strong></pre></div><p>Next, we need to import some test code into the Subversion repository. Make a directory and place the <a id="id423" class="indexterm"/>following (simple and silly) code into it in a <a id="id424" class="indexterm"/>file called <code class="literal">test_simple.py</code>:</p><div><pre class="programlisting">from unittest import TestCase

class test_simple(TestCase):
    def test_one(self):
        self.assertNotEqual("Testing", "Hooks")

    def test_two(self):
        self.assertEqual("Same", "Same")</pre></div><p>You can perform the import by executing the following command:</p><div><pre class="programlisting"><strong>$ svn import --username=testuser --password=testpass svn://localhost/svnrepo/</strong></pre></div><div><div><h3 class="title"><a id="note23"/>Note</h3><p>Subversion needs to know which text editor you want to use. If the preceding command fails, you probably need to tell it explicitly. You can do this by setting the <code class="literal">SVN_EDITOR</code> environment variable to the program path of the editor you prefer.</p></div></div><p>That command is likely to print out a gigantic, scary message about remembering passwords. In spite of the warnings, just say yes.</p><p>Now that we've got the code imported, we need to check out a copy of it to work on. We can do this with the following command:</p><div><pre class="programlisting"><strong>$ svn checkout --username=testuser --password=testpass svn://localhost/svnrepo/ svn</strong></pre></div><div><div><h3 class="title"><a id="tip22"/>Tip</h3><p>From here on, in this example, we're going to be assuming that the Subversion server is running in a Unix-like environment (the clients might be running on Windows, but that doesn't matter for our purposes). The reason for this is that the details of the <code class="literal">post-commit</code> hook are significantly different on systems that don't have a Unix style shell scripting language, although the concepts remain the same.</p></div></div><p>The following code goes into a file called <code class="literal">hooks/post-commit</code> inside the Subversion server's repository. The <code class="literal">svn update</code> and <code class="literal">svn checkout</code> lines have been wrapped around to fit on the <a id="id425" class="indexterm"/>page. In actual use, this wrapping <a id="id426" class="indexterm"/>should not be present:</p><div><pre class="programlisting">#!/bin/sh
REPO="$1"

if /usr/bin/test -e "$REPO/working"; then
    /usr/bin/svn update --username=testuser --password=testpass "$REPO/working/";
else
    /usr/bin/svn checkout --username=testuser --password=testpass svn://localhost/svnrepo/ "$REPO/working/";
fi

cd "$REPO/working/"

exec /usr/bin/nosetests</pre></div><p>Use the <code class="literal">chmod +x post-commit</code> command to make the hook executable.</p><p>Change to the <code class="literal">svn</code> checkout directory and edit <code class="literal">test_simple.py</code> to make one of the tests fail. We do this because, if all the tests pass, Subversion won't show us anything to indicate that they were run at all. We only get feedback if they fail:</p><div><pre class="programlisting">from unittest import TestCase

class test_simple(TestCase):
    def test_one(self):
        self.assertNotEqual("Testing", "Hooks")

    def test_two(self):
        self.assertEqual("Same", "Same!")</pre></div><p>Now commit the changes using the following command:</p><div><pre class="programlisting"><strong>$ svn commit --username=testuser --password=testpass</strong></pre></div><p>Notice that the commit triggered the execution of Nose, and that, if any of the tests fail, Subversion shows us the errors.</p><p>Because Subversion <a id="id427" class="indexterm"/>has one central set of hooks, they can be <a id="id428" class="indexterm"/>applied automatically to anybody who uses the repository.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec75"/>Mercurial</h2></div></div></div><p>Like Git, Mercurial is a distributed version control system with hooks that are managed by each user <a id="id429" class="indexterm"/>individually. Mercurial's hooks themselves, though, take a rather different form.</p><p>If you don't have <a id="id430" class="indexterm"/>Mercurial installed and don't plan to use it, you can skip this section.</p><p>Mercurial hooks can go in several different places. The two most useful are in your personal configuration file and in your repository configuration file.</p><p>Your personal configuration file is <code class="literal">~/.hgrc</code> on Unix-like systems, and <code class="literal">%USERPROFILE%\Mercurial.ini</code> (which usually means <code class="literal">C:\Documents and Settings\&lt;username&gt;\Mercurial.ini</code>) on Windows-based systems.</p><p>Your repository configuration file is stored in a subdirectory of the repository, specifically, <code class="literal">.hg/hgrc</code>, on all systems.</p><p>We're going to use the repository configuration file to store the hook, which means that the first thing we have to do is have a repository to work with. Make a new directory somewhere convenient, and execute the following command in it:</p><div><pre class="programlisting"><strong>$ hg init</strong></pre></div><p>One side-effect of this command is that a <code class="literal">.hg</code> subdirectory gets created. Change to this directory, and then create a text file called <code class="literal">hgrc</code> containing the following text:</p><div><pre class="programlisting">[hooks]
commit = python3 -m nose</pre></div><p>In the repository directory (that is, the parent of the <code class="literal">.hg</code> directory), we need some tests for Nose to run. Create a file called <code class="literal">test_simple.py</code> containing the following, admittedly silly, tests:</p><div><pre class="programlisting">from unittest import TestCase

class test_simple(TestCase):
    def test_one(self):
        self.assertNotEqual("Testing", "Hooks")

    def test_two(self):
        self.assertEqual("Same", "Same")</pre></div><p>Run the following commands to add the test file and commit it to the repository:</p><div><pre class="programlisting"><strong>$ hg add</strong>
<strong>$ hg commit</strong></pre></div><p>Notice that the commit triggered a run-through of the tests. Since we put the hook in the repository <a id="id431" class="indexterm"/>configuration file, it will only take effect on commits to this repository. If we'd instead put it into your personal configuration file, it <a id="id432" class="indexterm"/>would be called when you committed to any repository.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec76"/>Bazaar</h2></div></div></div><p>Like Git and <a id="id433" class="indexterm"/>Mercurial, Bazaar is a distributed version control system, and the individual users can control the hooks that apply to their own repositories. If you don't have Bazaar installed and don't plan to use it, you can skip this section.</p><p>Bazaar hooks go in your <a id="id434" class="indexterm"/>plugins directory. On Unix-like systems, that's <code class="literal">~/.bazaar/plugins/</code>, while on Windows, it's <code class="literal">C:\Documents and Settings\&lt;username&gt;\Application Data\Bazaar\&lt;version&gt;\plugins\</code>. In either case, you might have to create the plugins subdirectory, if it doesn't already exist.</p><p>Bazaar hooks are always written in Python, which is nice but, as I write this, they're always written in Python 2, not Python 3. This means that the hook code presented in this section is Python 2 code. Place the following code into a file called <code class="literal">run_nose.py</code> in the plugins directory:</p><div><pre class="programlisting">from bzrlib import branch
from os.path import join, sep
from os import chdir
from subprocess import call

def run_nose(local, master, old_num, old_id, new_num, new_id):
    try:
        base = local.base
    except AttributeError:
        base = master.base

    if not base.startswith('file://'):
        return

    try:
        chdir(join(sep, *base[7:].split('/')))
    except OSError:
        return

    call(['nosetests'])

branch.Branch.hooks.install_named_hook('post_commit',
                                       run_nose,
                                       'Runs Nose after each commit')</pre></div><p>Bazaar hooks are written in Python, so we've written our hook as a function called <code class="literal">run_nose</code>. Our <code class="literal">run_nose</code> function checks in order to make sure that the repository we're working on is local, and <a id="id435" class="indexterm"/>then it changes directories into the repository and runs Nose. We registered <code class="literal">run_nose</code> as a hook by calling <code class="literal">branch.Branch.hooks.install_named_hook</code>.</p><p>From now on, any <a id="id436" class="indexterm"/>time you commit to a Bazaar repository, Nose will search for and run whatever tests it can find within that repository. Note that this applies to any and all local repositories, as long as you're logged in to the same account <a id="id437" class="indexterm"/>on your computer.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec65"/>Automated continuous integration</h1></div></div></div><p>Automated continuous integration tools are a step beyond using a version control hook to run your tests when you <a id="id438" class="indexterm"/>commit code to the repository. Instead of running your test suite once, an automated continuous integration system compiles your code (if need be) and runs your tests many times, in many different environments.</p><p>An automated continuous integration system might, for example, run your tests under Python versions 3.2, 3.3, and 3.4 on each of Windows, Linux, and Mac OS X. This not only lets you know about errors in your code, but also about the unexpected problems caused by the external environment. It's nice to know when that last patch broke the program on Windows, even though it worked like a charm on your Linux box.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec77"/>Buildbot</h2></div></div></div><p>Buildbot is a popular <a id="id439" class="indexterm"/>automated continuous integration tool. Using Buildbot, you can create a network of "build slaves" that will check your code each time you commit it to your repository. This network can be quite large, and it can be distributed around the Internet, so Buildbot works even for projects with lots of developers spread around the world.</p><p>Buildbot's home page is at <a class="ulink" href="http://buildbot.net/">http://buildbot.net/</a>. By following links from this site, you can find the <a id="id440" class="indexterm"/>manual and download the latest version of the tool. At the time of <a id="id441" class="indexterm"/>writing, installing Buildbot with Python 3.x was slightly more complicated than just <code class="literal">pip install buildbot buildbot-slave</code>; thanks to some of the install files being targeted at Python 2.x. It's actually completely fine to install it with Python 2.x, and probably easier to deal with. Even if it's installed in Python 2.x, Buildbot can run your Python 3.x code inside the Python 3.x interpreter.</p><p>Buildbot operates in one of <a id="id442" class="indexterm"/>two modes, <code class="literal">buildmaster</code> and <code class="literal">buildslave</code>. A <code class="literal">buildmaster</code> mode manages a network of buildslaves, while the <code class="literal">buildslave</code> mode run the tests in their assorted environments.</p><div><div><div><div><h3 class="title"><a id="ch09lvl3sec08"/>Setup</h3></div></div></div><p>To set up a <code class="literal">buildmaster</code> mode, create a directory for it to operate in and then run the following command:</p><div><pre class="programlisting"><strong>$ buildbot create-master &lt;directory&gt;</strong></pre></div><p>In the preceding <a id="id443" class="indexterm"/>command, <code class="literal">&lt;directory&gt;</code> is the directory you just created for <code class="literal">buildbot</code> to work in.</p><p>Similarly, to set up a <code class="literal">buildslave</code> mode, create a directory for it to operate in and then run the command:</p><div><pre class="programlisting"><strong>$ buildslave create-slave &lt;directory&gt; &lt;host:port&gt; &lt;name&gt; &lt;password&gt;</strong></pre></div><p>In the preceding command, <code class="literal">&lt;directory&gt;</code> is the directory you just created for the <code class="literal">buildbot</code> to work in, <code class="literal">&lt;host:port&gt;</code> is the Internet host and port where the <code class="literal">buildmaster</code> can be found, and <code class="literal">&lt;name&gt;</code> and <code class="literal">&lt;password&gt;</code> are the login information that identifies this <code class="literal">buildslave</code> to the <code class="literal">buildmaster</code>. All of this information (except the directory) is determined by the operator of the <code class="literal">buildmaster</code>.</p><p>You should edit <code class="literal">&lt;directory&gt;/info/admin</code> and <code class="literal">&lt;directory&gt;/info/host</code> to contain the e-mail address you want associated with this <code class="literal">buildslave</code> and a description of the <code class="literal">buildslave</code> operating environment, respectively.</p><p>On both the <code class="literal">buildmaster</code> and the <code class="literal">buildslave</code>, you'll need to start up the <code class="literal">buildbot</code> background process. To do this, use the command:</p><div><pre class="programlisting"><strong>$ buildbot start &lt;directory&gt;</strong></pre></div><p>In the preceding command, the directory is the directory you set up as the <code class="literal">buildmaster</code>.</p><p>Configuring a <code class="literal">buildmaster</code> is a significant topic in itself, and one that we're not going to address in detail. It's fully described in Buildbot's own documentation. We will provide a simple<a id="id444" class="indexterm"/> configuration file, though, for reference and quick setup. This particular configuration file assumes that you're using Git, but it is not significantly different for other version control systems. The following code goes in the master's <code class="literal">&lt;directory&gt;/master.cfg</code> file:</p><div><pre class="programlisting"># -*- python -*-
# ex: set syntax=python:

c = BuildmasterConfig = {}

c['projectName'] = "&lt;replace with project name&gt;"
c['projectURL'] = "&lt;replace with project url&gt;"
c['buildbotURL'] = "http://&lt;replace with master url&gt;:8010/"

c['status'] = []
from buildbot.status import html
c['status'].append(html.WebStatus(http_port=8010,
                                  allowForce=True))

c['slavePortnum'] = 9989

from buildbot.buildslave import BuildSlave
c['slaves'] = [
    BuildSlave("bot1name", "bot1passwd"),
    ]

from buildbot.changes.pb import PBChangeSource
c['change_source'] = PBChangeSource()

from buildbot.scheduler import Scheduler
c['schedulers'] = []
c['schedulers'].append(Scheduler(name="all", branch=None,
                                 treeStableTimer=2 * 60,
                                 builderNames=["buildbot-full"]))

from buildbot.process import factory
from buildbot.steps.source.git import Git
from buildbot.steps.shell import Test
f1 = factory.BuildFactory()
f1.addStep(Git(repourl="&lt;replace with repository url&gt;"))
f1.addStep(Test(command = ['python3', '-m' 'nose']))

b1 = {'name': "buildbot-full",
      'slavename': "bot1name",
      'builddir': "full",
      'factory': f1,
      }
c['builders'] = [b1]</pre></div><p>We just set up <a id="id445" class="indexterm"/>Buildbot to run our tests whenever it notices that our source code has been unchanged for two hours.</p><p>We told it to run the tests by adding a build step that runs <code class="literal">nose</code>:</p><div><pre class="programlisting">f1.addStep(Test(command = ['python3', '-m' 'nose']))</pre></div><p>We told it to wait for the source code to be unchanged for two hours by adding a build scheduler:</p><div><pre class="programlisting">c['schedulers'].append(Scheduler(name="all", branch=None,
                                 treeStableTimer=2 * 60,
                                 builderNames=["buildbot-full"]))</pre></div><p>To make effective use of this Buildbot configuration, you also need to install a version control hook that notifies Buildbot of changes. Generically, this can be done by calling the <code class="literal">buildbot sendchange</code> shell command from the hook.</p><p>Once you have <code class="literal">buildmaster</code> and <code class="literal">buildslave</code> configured, have hooked <code class="literal">buildbot</code> into your <a id="id446" class="indexterm"/>version control system, and have started <code class="literal">buildmaster</code> and <code class="literal">buildslave</code>, you're in business.</p></div><div><div><div><div><h3 class="title"><a id="ch09lvl3sec09"/>Using Buildbot</h3></div></div></div><p>You'll be able to <a id="id447" class="indexterm"/>see a report of the Buildbot status in your web browser, by navigating to the <code class="literal">buildbotURL</code> that you configured in the <code class="literal">master.cfg</code> file. One of the most useful reports is the so-called waterfall view. If the most recent commit passes the tests, you should see something similar to the following screenshot:</p><div><img src="img/3211OS_09_01.jpg" alt="Using Buildbot"/></div><p>On the other hand, when the commit fails to pass the tests, you'll see the screenshot as follows:</p><div><img src="img/3211OS_09_02.jpg" alt="Using Buildbot"/></div><p>Either way, you'll also see a history of earlier versions, whether or not they passed the tests as well as who made the changes and when, and what the output of the test command looked like.</p><p>You'll see similar <a id="id448" class="indexterm"/>information for each of the buildslaves, which means that, when the tests pass on some systems and fail on others, you'll know which system configurations are having the problem.</p></div></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec66"/>Summary</h1></div></div></div><p>We learned a lot in this chapter about code coverage and plugging our tests into the other automation systems we use while writing software.</p><p>Specifically, we covered what code coverage is and what it can tell us about our tests. We learned how to run Nose automatically when our version control software detects changes in the source code, and how to set up the Buildbot automated continuous integration system.</p><p>Now that we've learned about code coverage, version control hooks, and automated continuous integration, you're ready to tackle more or less anything. Congratulations!</p></div></div>
</body></html>