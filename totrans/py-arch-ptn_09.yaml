- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Event-Driven Structures
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 事件驱动结构
- en: Request-response is not the only software architecture that can be used in a
    system. There can also be requests that don't require an immediate response. Perhaps
    there's no interest in a response, as the task can be done without the caller
    being required to wait, or perhaps it takes a long time and the caller doesn't
    want to be waiting for it. In any case, there's the option to, from the point
    of view of the caller, just send a message and proceed.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 请求-响应不是在系统中可以使用的唯一软件架构。也可以有不需要立即响应的请求。也许没有兴趣在响应，因为任务可以在调用者不需要等待的情况下完成，或者可能需要很长时间，而调用者不想等待。无论如何，从调用者的角度来看，有选择只是发送消息并继续进行。
- en: 'This message is called an *event*, and there are multiple uses for this kind
    of system. In this chapter, we will introduce the concept, and we will describe
    in detail one of the most popular uses of it: creating asynchronous tasks that
    are executed in the background while the caller of the task continues uninterrupted.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这条消息被称为*事件*，这类系统有多种用途。在本章中，我们将介绍这一概念，并详细描述其中最流行的用途之一：创建在任务调用者不间断的情况下在后台执行的后台异步任务。
- en: In the chapter, we will describe the basics of asynchronous tasks, including
    the details of queueing systems and how to generate automatically scheduled tasks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将描述异步任务的基础，包括排队系统的细节以及如何生成自动计划的任务。
- en: We will use Celery as an example of a popular task manager in Python that has
    multiple capabilities. We will show specific examples of how to perform common
    tasks. We will also explore Celery Flower, a tool that creates a web interface
    to monitor and control Celery and has an HTTP API that allows you to control that
    interface, including sending new tasks to execute.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以Celery为例，它是Python中一个具有多种功能的流行任务管理器。我们将展示如何执行常见任务的特定示例。我们还将探索Celery Flower，这是一个创建Web界面的工具，用于监控和控制Celery，并具有HTTP
    API，允许您控制该界面，包括发送新任务以执行。
- en: 'In this chapter, we''ll cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Sending events
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发送事件
- en: Asynchronous tasks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异步任务
- en: Subdividing tasks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务细分
- en: Scheduled tasks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计划任务
- en: Queue effects
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 队列效应
- en: Celery
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Celery
- en: Let's start by describing the basics of event-driven systems.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先描述事件驱动系统的基础。
- en: Sending events
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发送事件
- en: Event-driven structures are based on the fire-and-forget principle. Instead
    of sending data and waiting until the other part returns a response, it just sends
    data and continues executing.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 事件驱动结构基于“发射后不管”的原则。不是发送数据并等待另一部分返回响应，而是发送数据并继续执行。
- en: This makes it different from the request-response architecture that we saw in
    the previous chapter. A request-response process will wait until an appropriate
    response is generated. Meanwhile, the execution of more code will stop, as the
    new data produced by the external system is required to continue.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得它与我们在上一章中看到的请求-响应架构有所不同。请求-响应过程将等待直到生成适当的响应。同时，更多代码的执行将停止，因为需要外部系统产生的新数据来继续。
- en: In an event-driven system, there's no response data, at least not in the same
    sense. Instead, an event containing the request will be sent, and the task will
    just continue. Some minimal information could be returned to ensure that the event
    can be tracked later.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在事件驱动系统中，没有响应数据，至少不是在相同的意义上。相反，将发送包含请求的事件，任务将继续进行。可以返回一些最小信息以确保事件可以被跟踪。
- en: Event-driven systems can be implemented with request-response servers. This
    doesn't make them a pure request-response system. For example, a RESTful API that
    creates an event and returns an event ID. Any work is not done yet, and the only
    detail returned is an identifier to be able to check the status of any follow-up
    tasks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 事件驱动系统可以使用请求-响应服务器实现。这并不意味着它们是纯请求-响应系统。例如，一个创建事件并返回事件ID的RESTful API。任何工作尚未完成，唯一返回的细节是一个标识符，以便能够检查任何后续任务的状态。
- en: This is not the only option, as this event ID may be produced locally, or even
    not be produced at all.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是唯一的选择，因为这个事件ID可能是本地生成的，甚至可能根本不生成。
- en: The difference is that the task itself won't be done in the same moment, so
    getting back from generating the event will be very fast. The event, once generated,
    will travel to a different system that will transmit it towards its destination.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 差别在于任务本身不会在同一时刻完成，因此从生成事件返回将非常快。一旦生成，事件将前往不同的系统，该系统将把它传输到目的地。
- en: This system is called a *bus* and works to make messages flow through the system.
    An architecture can use a single bus that acts as a central place to send messages
    across systems, or it can use multiple ones.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个系统被称为*总线*，其工作原理是使消息在系统中流动。一个架构可以使用一个充当发送消息到系统中央位置的单个总线，或者可以使用多个总线。
- en: In general, it's advisable to use a single bus to communicate all the systems.
    There are multiple tools that allow us to implement multiple logical partitions,
    so the messages are routed to and from the right destinations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，建议使用单个总线来通信所有系统。有多种工具允许我们实现多个逻辑分区，因此消息被路由到和从正确的目的地。
- en: Each of the events will be inserted into a *queue*. A queue is a logical FIFO
    system that will transmit the events from the entry point to the defined next
    stage. At that point, another module will receive the event and process it.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 每个事件都将被插入到一个*队列*中。队列是一个逻辑FIFO系统，它将从入口点传输事件到定义的下一阶段。在那个点上，另一个模块将接收事件并处理它。
- en: This new system is listening to the queue and extracts all the received events
    to process them. This worker can't communicate directly with the sender of the
    event through the same channel, but it can interact with other elements, like
    shared databases or exposed endpoints, and can even send more events into queues
    to further process the results.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新系统正在监听队列，并提取所有接收到的事件以进行处理。这个工作员不能通过相同的通道直接与事件发送者通信，但它可以与其他元素交互，如共享数据库或公开的端点，甚至可以向队列发送更多事件以进一步处理结果。
- en: The systems at each end of the queue are called the *publisher* and the *subscriber*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 队列两端的系统被称为*发布者*和*订阅者*。
- en: Multiple subscribers can tend the same queue, and they'll be extracting events
    in parallel. Multiple publishers can also produce events into the same queue.
    The capacity of the queue will be described by the number of events that can be
    processed, and enough subscribers should be provided so the queue can be processed
    quickly enough.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 多个订阅者可以管理同一个队列，并且他们会并行提取事件。多个发布者也可以将事件发布到同一个队列。队列的容量将由可以处理的事件数量来描述，并且应该提供足够的订阅者，以便队列能够快速处理。
- en: Typical tools that can work as a bus are RabbitMQ, Redis, and Apache Kafka.
    While it is possible to use a tool "as is," there are multiple libraries that
    will help you work with these tools to create your own way of handling sending
    messages.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 可以作为总线的典型工具有RabbitMQ、Redis和Apache Kafka。虽然可以使用工具“原样”使用，但有多种库可以帮助你使用这些工具以创建自己的处理发送消息的方式。
- en: Asynchronous tasks
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异步任务
- en: A simple event-driven system is one that allows you to execute asynchronous
    tasks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的基于事件的系统允许你执行异步任务。
- en: The events produced by an event-driven system describe a particular task to
    execute. Normally, each task will require some time to execute, which makes it
    impractical to be executed directly as part of the publisher code flow.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由基于事件的系统产生的事件描述了要执行的特殊任务。通常，每个任务都需要一些时间来执行，这使得它作为发布者代码流的一部分直接执行变得不切实际。
- en: The typical example is a web server that needs to respond to the user in a reasonable
    time. Some HTTP timeouts can produce errors if an HTTP request takes too long,
    and generally it is not a great experience to respond in more than a second or
    two.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的例子是一个需要合理时间内响应用户的Web服务器。如果HTTP请求耗时过长，一些HTTP超时可能会产生错误，通常在超过一秒或两秒后响应并不是一个好的体验。
- en: These operations that take a long time may involve tasks like encoding video
    into a different resolution, analyzing images with a complex algorithm, sending
    1,000 emails to customers, deleting a million registers in bulk, copying data
    from an external database into a local one, generating reports, or pulling data
    from multiple sources.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这些耗时操作可能包括将视频编码为不同的分辨率、使用复杂算法分析图像、向客户发送1,000封电子邮件、批量删除一百万个注册信息、将数据从外部数据库复制到本地数据库、生成报告或从多个来源提取数据。
- en: The solution is to send an event to handle this task, generate a task ID, and
    return the task ID immediately. The event will be sent to a message queue that
    will deliver it to a back-end system. The back-end system will then execute the
    task, which can take as long as it needs to execute.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是发送一个事件来处理这个任务，生成一个任务ID，并立即返回任务ID。事件将被发送到消息队列，然后将其传递到后端系统。后端系统将执行任务，该任务可能需要执行很长时间。
- en: Meanwhile, the task ID can be used to monitor the progress of the execution.
    The back-end task will update the status of the execution in shared storage, like
    a database, so when it's completed, the web front-end can inform the user. This
    shared storage can also store any produced results that may be interesting.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，可以使用任务ID来监控执行进度。后端任务将在共享存储中，如数据库中，更新执行状态，因此当它完成时，Web前端可以通知用户。这个共享存储还可以存储任何可能有趣的结果。
- en: '![Diagram'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![图'
- en: Description automatically generated](img/B17580_07_01.png)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17580_07_01.png)
- en: 'Figure 7.1: The flow of an event'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：事件流程
- en: Because the status of the task is stored in a database that's accessible by
    the front-end web server, the user can ask for the status of the task at any point
    by identifying it through the task ID.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因为任务状态存储在一个前端Web服务器可以访问的数据库中，用户可以通过任务ID在任何时候请求任务的状态。
- en: '![Diagram, schematic'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '![图，示意图'
- en: Description automatically generated](img/B17580_07_02.png)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17580_07_02.png)
- en: 'Figure 7.2: Checking the progress of an async task with shared storage'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：使用共享存储检查异步任务的进度
- en: The back-end system can produce intermediate updates if necessary, showing when
    25% or 50% of the task has been completed. This will need to be stored in the
    same shared storage.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 后端系统如果需要，可以产生中间更新，显示任务完成25%或50%的时间。这将需要存储在相同的共享存储中。
- en: This process is a simplification, though. The queue is usually capable of returning
    whether a task has been finished or not. The shared storage/database will be required
    only if the task is required to return some data. A database works fine for small
    results, but if big elements like documents are produced as part of the task,
    this may not be a valid option and a different kind of storage may be required.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个过程是一种简化，但队列通常能够返回任务是否已完成。只有在任务需要返回一些数据时，才需要共享存储/数据库。对于小结果，数据库运行良好，但如果任务生成了像文档这样的大元素，这可能不是一个有效的选项，可能需要不同类型的存储。
- en: For example, if a task is to generate a report, the back-end will store it in
    document storage like AWS S3 so it's available to be downloaded by the user later.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果任务是要生成一个报告，后端会将其存储在文档存储中，如AWS S3，以便用户稍后可以下载。
- en: A shared database is not the only way to be sure that the web server front-end
    is capable of receiving information. The web server can expose an internal API
    that allows the back-end to send back information. This is, to all effects, the
    same as sending the data to a different external service. The back-end will need
    to access the API, configure it, and perhaps be authenticated. The API can be
    created exclusively for the back-end or can be an API for general usage that also
    accepts the specific data that the back-end system will produce.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 共享数据库不是确保Web服务器前端能够接收信息的唯一方式。Web服务器可以公开一个内部API，允许后端发送回信息。这在所有效果上等同于将数据发送到不同的外部服务。后端需要访问API，配置它，并可能需要进行认证。API可以专门为后端创建，也可以是一个通用API，它也接受后端系统将产生的特定数据。
- en: Sharing access to a database between two different systems can be difficult,
    as the database will need to be in sync for both systems. We need to detach the
    systems so they can be deployed independently and without breaking backward compatibility.
    Any change in the schema will require extra care to ensure that the system can
    perform at any point, without interruption. Exposing an API and keeping the database
    under the full control of the front-end service is a good solution, but keep in
    mind that requests originating from the back-end will compete with external requests,
    so we need enough capacity for both.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个不同的系统之间共享数据库的访问可能很困难，因为数据库需要同步才能满足两个系统。我们需要将系统分离，以便它们可以独立部署，而不会破坏向后兼容性。任何对模式的更改都需要额外的注意，以确保系统在任何时候都能正常运行，不会中断。公开一个API并保持数据库完全受前端服务的控制是一个好方法，但请注意，来自后端的请求将与外部请求竞争，因此我们需要足够的容量来满足两者。
- en: In this case, all the information, task IDs, statuses, and results can remain
    inside the web server's internal storage.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，所有信息、任务ID、状态和结果都可以保留在Web服务器内部存储中。
- en: Remember that the queue is likely to store the task ID and the status of the
    task. This may be replicated for convenience in the internal storage.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，队列可能会存储任务ID和任务状态。为了方便，这些信息可能在内部存储中进行了复制。
- en: '![Diagram'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![图'
- en: Description automatically generated](img/B17580_07_03.png)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17580_07_03.png)
- en: 'Figure 7.3: Sending back information to the source service'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3：将信息发送回源服务
- en: Remember that this API doesn't have to be directed to the same front-end. It
    can also call any other service, internal or external, generating a complex flow
    between elements. It even creates its own events that will be reintroduced into
    the queue to produce other tasks.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，这个API不必指向同一个前端。它也可以调用任何其他服务，无论是内部还是外部，从而在元素之间生成复杂的流程。它甚至创建自己的事件，这些事件将被重新引入队列以产生其他任务。
- en: Subdividing tasks
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务细分
- en: It's entirely possible to generate more tasks from an initial one. This is done
    by creating the right event inside a task and sending it to the right queue.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 从初始任务生成更多任务是完全可能的。这是通过在任务内部创建正确的事件并将其发送到正确的队列来完成的。
- en: This allows a single task to distribute its load and parallelize its action.
    For example, if a task generates a report and sends it by email to a group of
    recipients, the task can first generate the report and then send the emails in
    parallel by creating new tasks that will focus only on creating the emails and
    attaching the report.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许单个任务分配其负载并并行化其操作。例如，如果一个任务生成报告并通过电子邮件发送给一组收件人，该任务可以先生成报告，然后通过创建仅专注于创建电子邮件和附加报告的新任务来并行发送电子邮件。
- en: This spreads the load over multiple workers, speeding up the process. Another
    advantage is that individual tasks will be shorter, which makes them easier to
    control, monitor, and operate.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这将负载分散到多个工作者上，加快了处理速度。另一个优点是，单个任务将更短，这使得它们更容易控制、监控和操作。
- en: Some task managers may permit the creation of workflows where tasks are distributed,
    and their results are returned and combined. This can be used in some cases, but
    in practice it is less useful than it initially appears, as it introduces extra
    waiting and we can end up with the task taking a longer time.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一些任务管理器可能允许创建工作流，其中任务被分配，并且它们的结果被返回和合并。在某些情况下可以使用此功能，但在实践中，它不如最初看起来那么有用，因为它引入了额外的等待时间，我们最终可能会发现任务花费了更长的时间。
- en: But easy wins are bulk tasks performing similar actions on multiple elements
    without the need to combine the results, which are quite commonly encountered.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 但容易获得的成功是批量任务，在多个元素上执行类似操作，而不需要合并结果，这在实践中相当常见。
- en: Keep in mind, though, that this will make the initial task finish quickly, making
    the initial task's ID status a bad way to check whether the whole operation has
    been completed. The initial task may return the IDs of the new tasks if they need
    to be monitored.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，这将使初始任务快速完成，使得初始任务的ID状态检查整个操作是否完成不是一个好方法。如果需要监控，初始任务可能会返回新任务的ID。
- en: The process can be repeated, if necessary, with subtasks creating their own
    subtasks. Some tasks may require creating huge amounts of information in the background,
    so subdividing them may make sense, but it will also increase the complexity of
    following the flow of the code, so use this technique sparingly and only when
    it creates a clear advantage.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，可以重复此过程，子任务创建自己的子任务。某些任务可能需要在后台创建大量信息，因此细分它们可能是有意义的，但这也将增加跟踪代码流程的复杂性，因此请谨慎使用此技术，并且仅在它创造明显优势时使用。
- en: Scheduled tasks
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计划任务
- en: Asynchronous tasks don't need to be generated directly by a frontend and direct
    action by a user, but can also be set to run at specific times, through a schedule.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 异步任务不需要由前端直接生成，也不需要用户的直接操作，但也可以设置为在特定时间运行，通过一个计划。
- en: Some examples of scheduled tasks include generating daily reports during night
    hours, updating information hourly via an external API, precaching values so they
    are quickly available later, generating a schedule for next week at the start
    of the week, and sending reminder emails every hour.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一些计划任务的例子包括在夜间生成每日报告、每小时通过外部API更新信息、预先缓存值以便稍后快速可用、在周初生成下周的日程表，以及每小时发送提醒电子邮件。
- en: Most task queues will allow the generation of scheduled tasks, indicating it
    clearly in their definition, so they will be triggered automatically.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数任务队列都允许生成计划任务，并在其定义中明确指出，因此它们将自动触发。
- en: We will see later in the chapter how to generate a scheduled task for Celery.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面看到如何为Celery生成计划任务。
- en: Some scheduled tasks can be quite big, such as each night sending emails to
    thousands of recipients. It's very useful to divide a scheduled task, so a small
    scheduled task is triggered just to add all the individual tasks to the queue
    that will be processed later. This distributes the load and allows the task to
    finish earlier, making full use of the system.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一些计划任务可能相当大，例如每晚向数千个收件人发送电子邮件。将计划任务分解是非常有用的，这样就可以触发一个小型的计划任务，仅用于将所有单个任务添加到稍后处理的队列中。这分散了负载，并允许任务更早完成，充分利用系统。
- en: In the example of sending emails, a single task triggers every night, reading
    the configuration and creating a new task for each email found. Then the new tasks
    will receive the email, compose the body by pulling from external information,
    and send it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在发送电子邮件的例子中，每晚都会触发一个单独的任务，读取配置并为每个找到的电子邮件创建一个新任务。然后，新任务将接收电子邮件，通过从外部信息中提取来编写正文，并发送它。
- en: Queue effects
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 队列效应
- en: An important element of asynchronous tasks is the effect that introducing a
    queue may have. As we've seen, the background tasks are slow, meaning that any
    worker running them will be busy for some time.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 异步任务的一个重要元素是引入队列可能产生的影响。正如我们所见，后台任务运行缓慢，这意味着运行它们的任何工人都会忙一段时间。
- en: Meanwhile, more tasks can be introduced, which may mean that the queue starts
    building up.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，可以引入更多任务，这可能导致队列开始积累。
- en: '![Diagram'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![图'
- en: Description automatically generated with low confidence](img/B17580_07_04.png)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 描述由低置信度自动生成](img/B17580_07_04.png)
- en: 'Figure 7.4: Single queue'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4：单个队列
- en: On the one hand, this can be a capacity problem. If the number of workers is
    not sufficient to handle the average number of tasks introduced in the queue,
    the queue will build up until it reaches its limit, and new tasks will be rejected.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，这可能是一个容量问题。如果工人的数量不足以处理队列中引入的平均任务数，队列将积累到其极限，新的任务将被拒绝。
- en: But typically, the load doesn't work like a constant influx of tasks. Instead,
    there are times when there are no tasks to execute, and other times when there's
    a sudden spike in the number of tasks to be executed, filling the queue. Also,
    there's a need to calculate the right number of workers to keep running to be
    sure that the waiting period for those spikes, where a task gets delayed because
    all the workers are busy, is not causing problems.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 但通常，负载不会像恒定的任务流入那样工作。相反，有时没有任务要执行，而其他时候，任务的数量会突然增加，填满队列。此外，还需要计算正确的工人数量，以确保在所有工人忙碌时任务延迟的等待期不会造成问题。
- en: Calculating the "right" amount of workers can be difficult, but with a bit of
    trial and error a "good enough" number can be obtained. There's a mathematical
    tool to deal with it, queueing theory, which calculates it based on several parameters.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 计算正确的工人数量可能很困难，但通过一些尝试和错误，可以得到一个“足够好”的数量。有一个数学工具可以处理它，即排队论，它基于几个参数进行计算。
- en: In any case, these days resources for each worker are cheap and it's not imperative
    to generate the exact number of workers, as long as it's close enough so that
    any possible spike can be processed in a reasonable amount of time.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，如今每个工人的资源都很便宜，不需要精确生成工人的数量，只要足够接近，以便任何可能的峰值都能在合理的时间内处理。
- en: You can learn more about queueing theory at [http://people.brunel.ac.uk/~mastjjb/jeb/or/queue.html](http://people.brunel.ac.uk/~mastjjb/jeb/or/queue.html).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[http://people.brunel.ac.uk/~mastjjb/jeb/or/queue.html](http://people.brunel.ac.uk/~mastjjb/jeb/or/queue.html)了解更多关于排队论的信息。
- en: An extra difficulty, as we saw with scheduled tasks, is that at a specific time,
    a considerable number of tasks can be triggered at the same time. This can saturate
    the queue at a particular time, requiring perhaps an hour to digest all the tasks,
    for example, creating daily reports, ingesting new updates in an external API
    every 4 hours, or aggregating data for the week.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个额外的困难，正如我们通过计划任务所看到的，是在特定时间，可能同时触发大量任务。这可能在特定时间饱和队列，可能需要一个小时来处理所有任务，例如，创建每日报告，每4小时在外部API中摄取新更新，或汇总一周的数据。
- en: This means that, for example, if 100 tasks to create background reports are
    added, they will block a task to generate a report sent by a user, which will
    produce a bad experience. The user will have to wait for far too long if they
    ask for the report a few minutes after the scheduled tasks were fired.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，例如，如果添加了100个创建背景报告的任务，它们将阻塞一个由用户发送的报告生成任务，这将产生不良体验。如果用户在预定任务启动后几分钟内请求报告，他们不得不等待很长时间。
- en: A possible solution is to use multiple queues, with different workers pulling
    from them.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能的解决方案是使用多个队列，不同的工人从它们中提取任务。
- en: '![Diagram'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图](#)'
- en: Description automatically generated](img/B17580_07_05.png)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17580_07_05.png)
- en: 'Figure 7.5: Priority and background queue'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5：优先级和背景队列
- en: This makes those different tasks go to different workers, making it possible
    to reserve capacity for certain tasks to run uninterrupted. In our example, the
    background reports can go to their own dedicated workers, and the user reports
    have their own workers as well. This, though, wastes capacity. If the background
    reports run only once a day, once the 100 tasks are processed, the workers will
    be idle for the rest of the day, even if there's a long queue in the worker serving
    the user reports.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得不同的任务被分配到不同的工人，使得为某些任务预留容量以不间断运行成为可能。在我们的例子中，背景报告可以分配到它们自己的专用工人，用户报告也有自己的工人。然而，这会浪费容量。如果背景报告每天只运行一次，一旦100个任务被处理，工人将在剩余的时间里空闲，即使用户报告服务的工人队列很长。
- en: Instead of that, a mixed approach can be used.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 而是采用混合方法。
- en: '![Diagram'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图](#)'
- en: Description automatically generated](img/B17580_07_06.png)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B17580_07_06.png)
- en: 'Figure 7.6: Regular worker pulling from multiple queues'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6：普通工人从多个队列中提取任务
- en: In this case, the user report worker will continue with the same approach, but
    the background report worker will pull tasks from both queues. In this case, we
    limit the capacity for background reports, but at the same time, we increase it
    for the user report tasks when there's available capacity.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，用户报告的工人将继续使用相同的方法，但背景报告的工人将同时从两个队列中提取任务。在这种情况下，我们限制背景报告的容量，但同时，当有可用容量时，我们增加用户报告任务的容量。
- en: We reserve capacity for the user report tasks, which are priority, and make
    the rest of the workers pull from all available tasks, including priority and
    non-priority tasks.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为用户报告任务预留容量，这些任务是优先的，并让其他工人从所有可用的任务中提取，包括优先和非优先任务。
- en: 'To be able to divide work into these two queues, the tasks need to be divided
    carefully:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够将这些任务分配到这两个队列，需要对任务进行仔细划分：
- en: '*Priority tasks*. They are started on behalf of the user. They are time sensitive.
    They are fast to execute, so latency is important.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优先任务*。代表用户启动。它们对时间敏感。执行速度快，因此延迟很重要。'
- en: '*Background tasks*. Normally started by automated systems and scheduled tasks.
    They are less time sensitive. They can run for long periods, so higher latency
    is easier to accept.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*后台任务*。通常由自动化系统和计划任务启动。它们对时间不太敏感。可以长时间运行，因此更高的延迟更容易接受。'
- en: The balance between them should be maintained. If too many tasks are labeled
    as priority, the queue will be quickly filled, rendering it pointless.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 两者之间的平衡应该保持。如果太多任务被标记为优先级，队列将很快被填满，变得毫无意义。
- en: There's always the temptation to generate multiple queues to set up different
    priorities and reserve capacity for each of them. This is normally not a good
    idea, as they will waste capacity. The most efficient system is one with a single
    queue, as all capacity will be always used. There is a problem of priority, though,
    as it makes some tasks take too long. More than two queues overcomplicates and
    risks wasting capacity where many workers are idle most of the time, while other
    queues are filled. The simplicity of two queues helps develop the discipline of
    deciding between only two options and makes it easy to understand why we want
    multiple queues.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 总是会有创建多个队列以设置不同优先级并为每个队列预留容量的诱惑。这通常不是一个好主意，因为它们会浪费容量。最有效率的系统是单队列系统，因为所有容量都将始终被使用。然而，也存在优先级问题，因为它使得某些任务耗时过长。超过两个队列会过于复杂，并可能导致许多工人大部分时间空闲，而其他队列却满载。两个队列的简单性有助于培养在两种选项之间做出决定的纪律，并使人们容易理解为什么我们想要多个队列。
- en: The number of priority workers can be tweaked based on the number and frequency
    of spikes and expected turnaround time. Only enough priority workers to cover
    regular traffic at the times where there are big spikes in background tasks are
    required, as long as those spikes are predictable.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 根据峰值数量和频率以及预期的周转时间，可以调整优先级工人的数量。只要这些峰值是可预测的，就只需要足够的优先级工人来覆盖在后台任务大峰值期间的正常流量。
- en: Good metrics are critical for monitoring and understanding the behavior of the
    queue. We will talk more about metrics in *Chapter 13*,*Metrics*.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的指标对于监控和理解队列的行为至关重要。我们将在*第13章*，*指标*中更多地讨论指标。
- en: An alternative is to generate a priority system based on specific priorities,
    like numbers. That way, a task with priority 3 will be executed before a task
    with priority 2, and that before a task with priority 1, and so on. The great
    advantage of having priorities is that the workers can be working all the time,
    without wasting any capacity.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是基于特定的优先级（如数字）生成优先级系统。这样，优先级为3的任务将在优先级为2的任务之前执行，然后是优先级为1的任务，依此类推。拥有优先级的巨大优势是工人可以一直工作，而不会浪费任何容量。
- en: 'But this approach has some problems:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 但这种方法也有一些问题：
- en: A lot of queue backends don't support it efficiently. To keep a queue sorted
    by priority costs more than just assigning tasks to a plain queue. In practice,
    it may not produce as good results as you expect, requiring many tweaks and adjustments.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多队列后端不支持其高效执行。为了按优先级对队列进行排序，所需的成本远高于仅将任务分配到普通队列。在实践中，可能不会产生你预期的那么好的结果，可能需要许多调整和修改。
- en: It means you need to deal with priority inflation. It's very easy for teams
    to start increasing the priority of tasks over time, especially if multiple teams
    are involved. The decision on what task should return first could get complicated
    and pressure can grow the priority numbers over time.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这意味着你需要处理优先级膨胀。随着时间的推移，团队开始增加任务的优先级很容易，尤其是如果涉及多个团队。关于哪个任务应该首先返回的决定可能会变得复杂，随着时间的推移，压力可能会使优先级数字增加。
- en: While it can appear that a sorted queue is ideal, the simplicity of two levels
    (priority and background) makes it very easy to understand the system and generates
    easy expectations when developing and creating new tasks. It's way easier to tweak
    and understand and will generate better results with less work.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然看起来排序队列是理想的，但两个级别（优先级和背景）的简单性使得理解系统非常容易，并在开发和创建新任务时产生简单的期望。它更容易调整和理解，并且可以以更少的努力产生更好的结果。
- en: Single code for all workers
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 所有工人的单一代码
- en: When having different workers pulling from different queues, the worker could
    have different codebases, making one with priority tasks and another with background
    tasks.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当有不同工人从不同的队列中提取时，工人可能有不同的代码库，一个处理优先级任务，另一个处理背景任务。
- en: Note that for this to work, it will require strict separation of tasks. More
    about this a bit later.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为了使这可行，需要严格分离任务。关于这一点，我们稍后会详细讨论。
- en: 'This is generally not advisable, as it will differentiate the codebase and
    require maintaining two code bases in parallel, with some problems:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是不建议的，因为它将区分代码库，并需要并行维护两个代码库，存在一些问题：
- en: It's likely that some tasks or task parts will be either priority or background,
    depending on what system or user triggers them. For example, reports that can
    be either produced on the fly for a user, or daily as part of a batch process
    to finally send them by mail. The report generation should remain common, so any
    change is applied to both.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些任务或任务部分可能是优先级或背景，这取决于触发它们的系统或用户。例如，报告可以是即时为用户生成，也可以作为批量处理的一部分每天生成，最终通过邮件发送。报告生成应保持通用，以便任何更改都应用于两者。
- en: Handling two codebases instead of one is more inconvenient. A big part of the
    general code is shared, so updates will need to be run independently.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理两个代码库而不是一个会更不方便。大部分通用代码是共享的，因此更新需要独立运行。
- en: A unique codebase can handle all kinds of tasks. That makes it possible to have
    a worker that handles both priority and background tasks. Two codebases will require
    strict task separation, not using the extra capacity available in the background
    workers to help with priority tasks.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个独特的代码库可以处理所有类型的任务。这使得有可能有一个工人可以处理优先级和背景任务。两个代码库将需要严格的任务分离，不使用背景工人中可用的额外容量来帮助处理优先级任务。
- en: It is better to use a single worker when building, and through the configuration
    decide to receive messages from one queue or both. This simplifies the architecture
    for local development and testing.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建时使用单个工人数更好，并通过配置决定从单个队列或两个队列接收消息。这简化了本地开发和测试的架构。
- en: This may not be adequate when the nature of the tasks may create conflicts.
    For example, if some of the tasks require big dependencies or specialized hardware
    (as could be the case with some AI-related tasks) this may require that specific
    tasks run in dedicated workers, making it impractical for them to share the same
    codebase. These cases are rare, and unless they are encountered, it's better to
    try to consolidate and use the same worker for all tasks.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务性质可能产生冲突时，这可能是不够的。例如，如果一些任务需要大依赖项或专用硬件（例如某些与人工智能相关的任务），这可能需要特定任务在专用工人数上运行，使得它们共享相同的代码库变得不切实际。这些情况很少见，除非遇到，否则最好尝试合并并使用相同的工人数处理所有任务。
- en: Cloud queues and workers
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云队列和工人数
- en: The main characteristic of cloud computing is that services can be started and
    stopped dynamically, allowing us to use only the resources required at a particular
    moment. This allows the system to increase and decrease capacity quickly.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算的主要特点是服务可以动态启动和停止，使我们能够仅使用特定时刻所需的资源。这使得系统可以快速增加和减少容量。
- en: In cloud environments, it's possible that the number of workers extracting events
    from a queue can be modified. That alleviates the problems with resourcing that
    we discussed above. Do we have a full queue? Increase the number of workers on
    demand! Ideally, we could even spawn a single worker for each event that spawns
    a task, making the system infinitely scalable.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在云环境中，可能需要修改从队列中提取事件的工人数。这缓解了我们上面讨论的资源问题。我们是否有满队列？按需增加工人数！理想情况下，我们甚至可以为每个触发任务的每个事件生成一个单独的工人数，从而使系统无限可扩展。
- en: 'This, obviously, is easier said than done, as there are some issues with trying
    to dynamically create workers on the spot:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，说起来容易做起来难，因为尝试动态创建现场工人数有一些问题：
- en: The start-up time can add significant time to the execution of the task, even
    to the point of being longer than the execution time of the task itself. Depending
    on how heavy the creation of a worker is, starting it can take a significant amount
    of time.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动时间可能会给任务的执行增加显著的时间，甚至可能比任务的执行时间还要长。根据创建工人数的重量，启动它可能需要相当长的时间。
- en: In the traditional cloud setting, the lowest granularity required to start a
    new virtual server, which is relatively heavy, takes at least a couple of minutes.
    With newer tools, such as containers, this can be sped up sensibly, but the underlying
    principle will remain, as at some point in time a new virtual server will need
    to be spawned.
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在传统的云设置中，启动新的虚拟服务器所需的最小粒度相对较重，至少需要几分钟。使用较新的工具，如容器，这可以合理地加快速度，但基本原理将保持不变，因为最终某个时间点将需要生成新的虚拟服务器。
- en: A single new virtual worker may be too big for a single worker, making it inefficient
    to spawn one for each task. Again, containerized solutions can help by making
    it easier to separate between creating a new container and requiring spinning
    up a new virtual server in the cloud service.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个新的虚拟工人数可能对单个工人数来说太大，为每个任务生成一个可能效率低下。再次强调，容器化解决方案可以通过简化创建新容器和需要启动云服务中的新虚拟服务器之间的区别来帮助解决这个问题。
- en: Any cloud service should have limits. Each new worker created costs money and
    cloud services can get very expensive if scaled up without control. Without certain
    control on the cost side of things, this can grow to be a problem due to high,
    unexpected costs. Normally this can happen by accident, with some explosion of
    workers due to some problem on the system, but there's also a security attack,
    called Cash Overflow, aimed at making a service run as expensively as possible
    to force the owner of the service to stop it or even bankrupt them.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何云服务都应该有限制。每个新创建的工人数都会产生费用，如果没有控制地扩展，云服务可能会变得非常昂贵。如果没有对成本方面的某些控制，这可能会因为高昂且意外的费用而成为一个问题。通常这种情况可能是意外发生的，由于系统中的某些问题导致工人数激增，但还有一种名为“现金溢出”的安全攻击，旨在使服务尽可能昂贵地运行，迫使服务所有者停止服务甚至破产。
- en: Because of these problems, normally a solution will need to work in sort of
    a batched way, allowing extra space to grow and generating extra virtual servers
    only when they are required to reduce the queue. In the same way, when the extra
    capacity is not required any more, it will be removed.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Extra care should be taken to be sure that all the workers located in the same
    virtual server are idle before stopping it. This is done automatically by stopping
    the servers gracefully, so they'll finish any remaining tasks, start no new ones,
    and finish when everything is done.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'The process should be similar to this:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17580_07_07.png)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.7: Starting up a new server'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Knowing exactly when a new server should be spawned depends greatly on the requirements
    for latency, traffic, and the speed of creating a new server (if the server starts
    quickly, perhaps it can be less aggressive in scaling up).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: A good starting point is to create a new server each time the queue has a number
    of tasks equal to or greater than the number of workers in a single server. That
    triggers a new server that will be able to handle those tasks. If the creation
    is triggered with fewer tasks than that, it will create a server that is not quite
    filled. If the start-up time is very long, this can be reduced to ensure that
    the new server is up before there's a significant queue building up. But this
    will require experimentation and testing for a specific system.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Celery
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Celery is the most popular task queue created in Python. It allows us to create
    new tasks easily and can handle the creation of the events that trigger new tasks.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Celery requires to work to set up a *broker*, which will be used as a queue
    to handle the messages.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: In Celery parlance, the broker is the message queue, while the *backend* is
    reserved for interacting with a storage system to return information.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: The code that creates the message will add it to the broker, and the broker
    will pass it to one of the connected workers. When everything happens with Python
    code, where the `celery` package can be installed, it's simple to operate. We'll
    see later how to operate it in other cases.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Celery can use multiple systems as brokers. The most popular are Redis and RabbitMQ.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: In our examples, we will use Redis as it can be used for the broker and the
    backend, and it's widely available in cloud systems. It's also quite scalable
    and handles big loads easily.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Using a backend is optional, as tasks don't need to define a return value, and
    it's very common that asynchronous tasks don't directly return response data other
    than the status of the task. The key word here is "directly"; sometimes, a task
    will generate an external result that can be accessible, but not through the Celery
    system.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Some examples of these values are reports that can be stored in other storage
    facilities, emails sent during task processing, and pre-caching of values, where
    there is not a direct result, but there's new data generated and stored in other
    places.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: The returning value needs also to be small enough that it can be stored in the
    system working as the backend. Also, if strong persistence is used, it's recommended
    that a database is used as the backend.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the example present on GitHub: [https://github.com/PacktPublishing/Python-Architecture-Patterns/tree/main/chapter_07_event_driven/celery_example](https://github.com/PacktPublishing/Python-Architecture-Patterns/tree/main/chapter_07_event_driven/celery_example).
    We will use the example to create a task to retrieve, from an external API, pending
    `TO DO` actions by some users, and generate an email to send as a reminder.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Remember to install the required dependencies by running `pip install -r requirements.txt`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at the code.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Celery
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code is divided into two files: `celery_tasks.py`, which describes the
    tasks, and `start_task.py`, which connects with the queue and enqueues a task.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'At the start of each, we need to configure the broker to use. In this case,
    we will use a Redis server running in the `localhost`:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As a prerequisite, we need to set up a Redis server running in our expected
    `localhost` address. An easy way of doing so, if you have Docker installed, is
    to start a container:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This starts the standard Redis container that will expose the service over the
    standard port, 6379\. That will connect automatically with the previous broker
    URL of `redis://localhost`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: This is all the configuration that's required, and it will allow both sides,
    the publisher and the subscriber, to connect to the queue.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Celery worker
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use [https://jsonplaceholder.typicode.com/](https://jsonplaceholder.typicode.com/)
    to simulate calling an external API. This testing site exposes an accessible REST
    endpoint to retrieve some mock information. You can see their definition, but
    basically, we will access the `/todos` and `/users` endpoints. The `/todos` endpoint
    exposes actions stored by the users, so we will query them to retrieve pending
    actions, and combine this with the information in the `/users` endpoint.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: The `celery_tasks.py` worker defines a main task, `obtain_info`, and a secondary
    task, `send_email`. The first one pulls the information from the API and decides
    what emails need to be sent. The second then sends the email.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The sending of the email is just mocked to avoid complicating the system and
    needing to handle mocked email addresses. It's left as an exercise for the reader.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'The file starts with the configuration of the queue and imports:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `logger` definition permits the use of native Celery logs that will be streamed
    into the Celery configuration for logs. By default, this is the standard output.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the `obtain_info` task. Note the `@app.task` that defines
    the function as a Celery task:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We wrap the function with `INFO` logs to provide context to the task execution.
    First, it calls the `/todos` endpoint on this line, which then goes through each
    task independently, skipping any completed task.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, it checks the information for the user and puts it into the `info` variable.
    Because this information can be used multiple times in the same loop, it is cached
    in the `users` dictionary. Once the info is cached, it''s not asked for again:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The individual task data is added to a list created to store all the tasks for
    a user. The `task_reminders` dictionary is created as a `defaultdict(list)`, meaning
    that the first time a particular `user_id` is accessed, if it's not present, it
    will be initialized as an empty list, allowing a new element to be appended.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, the stored elements in `task_reminders` are iterated to compose the
    resulting email:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Two follow-up functions are called: `obtain_user_info` and `compose_email`.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '`obtain_user_info` retrieves the information directly from the `/users/{user_id}`
    endpoint and returns it:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`compose_email` takes the information in the task list, which includes a group
    of `user_info, task_info`, extracts the title information for each `task_info`,
    then the email from the matched `user_info`, and then calls the `send_email` task:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As you can see, the `send_email` task includes a `.delay` call, which enqueues
    this task with the appropriate parameters. `send_email` is another Celery task.
    It is very simple as we are just mocking the email delivery. It just logs its
    parameters:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Triggering tasks
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `start_task.py` script contains all the code to trigger the task. This is
    a simple script that imports the task from the other file.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that it inherits all the configuration from `celery_tasks.py` when doing
    the import.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Importantly, it calls the task with `.delay()`. This sends the task to the queue
    so the worker can pull it out and execute it.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Note that if you call the task directly with `obtain_info()`, you'll execute
    the code directly, instead of submitting the task to the queue.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Let's see now how both files interact.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Connecting the dots
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To be able to set both parts, the publisher and the consumer, first start the
    worker calling style:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Note**: Some of the modules used, such as Celery, might not be compatible
    with Windows systems. More information can be found at [https://docs.celeryproject.org/en/stable/faq.html#does-celery-support-windows](https://docs.celeryproject.org/en/stable/faq.html#does-celery-support-windows).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'This starts the `celery_tasks` module (the `celery_tasks.py` file) with the
    `-A` parameter. It sets the log level to `INFO` and starts three workers with
    the `-c 3` parameter. It will display a starting log similar to this one:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Note that it displays the two available tasks, `obtain_info` and `send_email`.
    In another window, we can send tasks calling the `start_task.py` script:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This will trigger the task in the Celery worker, producing logs (edited for
    clarity and brevity). We will explain the logs in the next paragraphs.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Because we started three different workers, the logs are intertwined. Pay attention
    to the first task, which corresponds to `obtain_info`. This task has been executed
    in the worker `ForkPoolWorker-2` in our execution.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: While this task is being executed, the `send_email` tasks are also being enqueued
    and executed by the other workers.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: At the end of the execution, there's a log showing the time it has taken, in
    seconds.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: If only one worker is involved, the tasks will be run consecutively, making
    it easier to differentiate between tasks.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: We can see how the `send_email` tasks start before the end of the `obtain_info`
    task, and that there are still `send_email` tasks running after the end of the
    `obtain_info` task, showing how the tasks are running independently.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Scheduled tasks
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inside Celery, we can also generate tasks with a certain schedule, so they can
    be triggered automatically at the proper time.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, we need to define a task and a schedule. We defined them in the `celery_scheduled_tasks.py`
    file. Let''s take a look:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This file starts with the same configuration as the previous example, and we
    define a small, simple task that just displays when it is executed.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The interesting bit comes later, as the schedule is configured in the `app.conf.beat_schedule`
    parameter. We created two entries.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The first one defines an execution of the proper task every 15 seconds. The
    task needs to include the module name (`celery_scheduled_tasks`). The `schedule`
    parameter is defined in seconds. The `args` parameter contains any parameter to
    pass for the execution. Note that it's defined as a list of parameters. In this
    case, we create a tuple with a single entry, as there's only one argument.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: The second entry defines the schedule instead as a crontab entry.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This `crontab` object, which is passed as the `schedule` parameter, executes
    the task once every two minutes. Crontab entries are very flexible and allow for
    a wide range of possible actions.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples are as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '| Crontab entry | Description |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: '| `crontab()` | Execute every minute, the lowest possible resolution |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '| `crontab(minute=0)` | Execute every hour, at minute 0 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| `crontab(minute=15)` | Execute hourly, at minute 15 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| `crontab(hour=0, minute=0)` | Execute daily, at midnight (in your time zone)
    |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| `crontab(hour=6, minute=30, day_of_week=''monday'')` | Execute every Monday,
    at 6:30 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| `crontab(hour=''*/8'', minute=0)` | Execute every hour divisible by 8 (0,
    8, 16). Three times a day, at minute 0 in each case |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| `crontab(day_of_month=1, hour=0, minute=0)` | Execute on the first of each
    month, at midnight |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '| `crontab(minute=''*/2'')` | Execute every minute divisible by 2\. Once every
    two minutes |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: There are more options, including relating the time to solar times, like dawn
    and dusk, or custom schedulers, but most use cases will be perfectly fine either
    once every X seconds or with a crontab definition.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check the full documentation here: [https://docs.celeryproject.org/en/stable/userguide/periodic-tasks.html#starting-the-scheduler](https://docs.celeryproject.org/en/stable/userguide/periodic-tasks.html#starting-the-scheduler).'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'To start the scheduler, we need to start a specific worker, the `beat` worker:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We start the `celery_scheduled_tasks` worker in the usual way.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'But you can see that there''s still no incoming tasks. We need to start `celery
    beat`, which is a specific worker that inserts the tasks in the queue:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Once `celery beat` is started, you''ll start seeing the tasks being scheduled
    and executed as expected:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'You can see that both kinds of tasks are scheduled accordingly. In this log,
    check the times and see that they are 15 seconds apart:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The other task happens exactly every 2 minutes. Note that the first execution
    may not be totally precise. In this case, the schedule was triggered in the later
    seconds of 15:12 and still got executed later than that. In any case, it will
    be within the 1-minute resolution window of the crontab.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: When creating periodic tasks, keep in mind the different priorities, as we described
    previously in the chapter.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: It is good practice to use a periodic task as a "heartbeat" to check that the
    system is working correctly. This task can be used to monitor that the tasks in
    the system are flowing as expected, with no big delays or problems.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: This leads to the way of monitoring how the different tasks are being executed,
    in a better way than just by checking the logs.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Celery Flower
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Obtaining good monitoring in Celery is important if you want to understand the
    executed tasks and find and fix problems. A good tool for that is Flower, which
    enhances Celery by adding a real-time monitoring web page that allows you to control
    Celery through the web page and through an HTTP API.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: You can check the whole documentation at [https://flower.readthedocs.io/en/latest/](https://flower.readthedocs.io/en/latest/).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: It's also very easy to set up and integrate with Celery. First, we need to be
    sure that the `flower` package is installed. The package is included in the `requirements.txt`
    after the previous step, but if it's not, you can install it independently using
    `pip3`.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Once it is installed, you can start `flower` with the following command:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The command is very similar to starting the Celery workers, but includes the
    definition of the broker using Redis, as we saw before, with `--broker=redis://localhost`,
    and specifying the port to expose, `--port=5555`.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: The interface is exposed in `http://localhost:5555`.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17580_07_08.png)
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.8: Celery Flower interface'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'The front page shows the different workers in the system. Note that it shows
    the number of active tasks, as well as processed tasks. In this case, we have
    11 tasks corresponding to a whole run of `start_task.py`. You can go to the **Tasks**
    tab to see the details of each of the tasks executed, which looks like this:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17580_07_09.png)
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.9: Tasks page'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: You can see information such as the input parameters, the state of the task,
    the name of the task, and how long it ran for.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Each Celery process will appear independently, even if it's capable of running
    multiple workers. You can check its parameters on the **Worker** page. See the
    **Max concurrency** parameter.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17580_07_10.png)
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.10: Worker page'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: From here, you can also review and change the configuration of the number of
    workers per Celery process, set rate limits, and more.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Flower HTTP API
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A great addition from Flower is the HTTP API, which allows us to control Flower
    through HTTP calls. This enables the automatic control of the system and allows
    us to trigger the tasks directly with an HTTP request. This can be used to call
    the tasks in any programming language, and greatly increases the flexibility of
    Celery.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'The URL to call a task asynchronously is the following:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'It requires a POST, and the arguments of the call should be included in the
    body. For example, make a call with `curl`:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The task is executed in the worker:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Using the same API, the status of the task can be retrieved with a GET request:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'For example:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note the `state` parameter, which here shows the task is finished successfully,
    but it will return `PENDING` if it's not done yet.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: This can be used to poll the status of the task until it's completed or it shows
    an error, as we described earlier in the chapter.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have seen what event-driven structures are. We started with
    a general discussion about how events can be used to create different flows than
    the traditional request-response structure. We talked about how the events are
    introduced into queues to be transmitted to other systems. We introduced the idea
    of a publisher and a subscriber to introduce or extract events from that queue.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'We described how this structure could be used to act on asynchronous tasks:
    tasks that run in the background and allow other elements of the interface to
    respond quickly. We described how dividing asynchronous tasks into smaller ones
    can help increase throughput by taking advantage of having multiple subscribers
    that can execute these smaller tasks. We described how tasks can be added automatically
    at certain times to allow the execution of predetermined tasks periodically.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: As the introduction of tasks can happen with great variability, we discussed
    some important details of how queues work, the different problems that we can
    encounter, and strategies to deal with them. We talked about how a simple strategy
    for a background queue and a priority queue works in most scenarios and warned
    about overcomplicating it. We also explained that, in the same spirit, it's better
    to keep the code synchronized among all workers, even in cases when the queues
    may be different. We also briefly touched on the capabilities of cloud computing
    as applied to asynchronous workers.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 由于任务的引入可能会有很大的变化，我们讨论了一些关于队列如何工作的重要细节，我们可能遇到的不同问题以及处理它们的策略。我们讨论了在大多数情况下，一个简单的背景队列和优先队列的策略是如何工作的，并警告不要过度复杂化。我们还解释了，在同样的精神下，最好在所有工作者之间保持代码同步，即使在队列可能不同的情况下。我们还简要提到了云计算在异步工作者中的应用能力。
- en: We explained how to use Celery, a popular task manager, to create asynchronous
    tasks. We covered setting up the different elements, including the back-end broker,
    how to define a proper worker, and how to generate tasks from a different service.
    We included a section on how to create scheduled tasks in Celery as well.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解释了如何使用流行的任务管理器Celery来创建异步任务。我们涵盖了设置不同元素的过程，包括后端代理、如何定义合适的工作者以及如何从不同的服务生成任务。我们还包含了一个关于如何在Celery中创建计划任务的章节。
- en: We presented Celery Flower, a complement for Celery that includes a web interface
    with which we can monitor and control Celery. It also includes an HTTP API that
    allows us to create tasks by sending HTTP requests, allowing any programming language
    to interact with our Celery system.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了Celery Flower，它是Celery的一个补充，包括一个网页界面，通过它可以监控和控制Celery。它还包含一个HTTP API，允许我们通过发送HTTP请求来创建任务，使得任何编程语言都可以与我们的Celery系统交互。
