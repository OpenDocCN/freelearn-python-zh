<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Creating Projects with the Raspberry Pi Camera Module</h1></div></div></div><p>In this chapter, we will cover the following topics:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Getting started with the Raspberry Pi camera module</li><li class="listitem" style="list-style-type: disc">Using the camera with Python</li><li class="listitem" style="list-style-type: disc">Generating a time-lapse video</li><li class="listitem" style="list-style-type: disc">Creating a stop frame animation</li><li class="listitem" style="list-style-type: disc">Making a QR code reader</li><li class="listitem" style="list-style-type: disc">Discovering and experimenting with OpenCV</li><li class="listitem" style="list-style-type: disc">Color detection with OpenCV</li><li class="listitem" style="list-style-type: disc">Performing motion tracking with OpenCV</li></ul></div><div><div><div><div><h1 class="title"><a id="ch08lvl1sec58"/>Introduction</h1></div></div></div><p>The Raspberry Pi camera module is a special add-on of the Raspberry Pi that makes use of the <strong>Camera Serial Interface</strong> (<strong>CSI</strong>) <strong>connector</strong>. This connects directly to the GPU core of the Raspberry Pi processor, allowing images to be captured directly on the unit.</p><p>We shall create a basic <strong>graphical user interface</strong> (<strong>GUI</strong>) using the <code class="literal">tkinter</code> library we used in <a class="link" href="ch03.html" title="Chapter 3. Using Python for Automation and Productivity">Chapter 3</a>, <em>Using Python for Automation and Productivity</em>, and <a class="link" href="ch04.html" title="Chapter 4. Creating Games and Graphics">Chapter 4</a>, <em>Creating Games and Graphics</em>. This will form the basis of the following three examples where we extend the GUI with additional controls so that we can put the camera to various uses for a range of different projects.</p><p>Finally, we set up the powerful <strong>Open Computer Vision</strong> (<strong>OpenCV</strong>) library to perform some advanced image processing. We will learn the basics of OpenCV and use it to track objects based on their color or detect movement.</p><div><div><h3 class="title"><a id="tip18"/>Tip</h3><p>This chapter uses the Raspberry Pi camera module, which is available from most retailers listed in the <em>Makers, hobbyists, and Raspberry Pi specialists</em> section of the <a class="link" href="apa.html" title="Appendix A. Hardware and Software List">Appendix</a>, <em>Hardware and Software List</em>.</p></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec59"/>Getting started with the Raspberry Pi camera module</h1></div></div></div><p>We will start by installing and <a class="indexterm" id="id564"/>setting up the Raspberry Pi <a class="indexterm" id="id565"/>camera module; then we will create a small camera GUI that enables us to preview and take photos. The first GUI we will create is shown in the following image:</p><div><img alt="Getting started with the Raspberry Pi camera module" src="img/6623OT_08_001.jpg"/><div><p>A basic camera GUI for the Raspberry Pi camera module</p></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec155"/>Getting ready</h2></div></div></div><p>The Raspberry Pi camera <a class="indexterm" id="id566"/>module consists of a camera mounted on a small <strong>Printed Circuit Board</strong> (<strong>PCB</strong>)<a class="indexterm" id="id567"/> attached to a small <a class="indexterm" id="id568"/>ribbon cable. The ribbon cable can be attached directly to the CSI port of the Raspberry Pi board (marked as <strong>S5</strong>, the port is located between the USB and the HDMI port on the Raspberry Pi). The following image shows the Raspberry Pi camera module:</p><div><img alt="Getting ready" src="img/6623OT_08_002.jpg"/><div><p>The Raspberry Pi camera module</p></div></div><p>The Raspberry Pi Foundation <a class="indexterm" id="id569"/>provides detailed instructions (and a video) on how to install the camera at <a class="ulink" href="http://www.raspberrypi.org/archives/3890">http://www.raspberrypi.org/archives/3890</a>; carry out the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">First, fit the camera as shown in the following image (ensure that you have disconnected the Raspberry Pi from any power sources first):<div><img alt="Getting ready" src="img/6623OT_08_003.jpg"/><div><p>The ribbon connector for the camera module is located next to the HDMI socket</p></div></div><p>To fit the<a class="indexterm" id="id570"/> ribbon cable into the CSI socket, you <a class="indexterm" id="id571"/>need to gently lift up and loosen the tab of the ribbon socket. Insert the ribbon into the slot with the metal contacts facing towards the HDMI port. Take care not to bend or fold the ribbon cable, and ensure that it is seated firmly and level in the socket before pushing the tab back into place.</p></li><li class="listitem">Finally, enable the camera. You can do this via the Raspberry Pi Configuration GUI on the Raspbian desktop (open this via the <strong>Interfaces</strong> menu).<div><img alt="Getting ready" src="img/6623OT_08_004.jpg"/><div><p>Enable the Raspberry Pi camera via the Interfaces tab in the Raspberry Pi Configuration screen</p></div></div></li></ol></div><p>Alternatively, you can do this via the command line, using <code class="literal">raspi-config</code>. Use <code class="literal">sudo raspi-config</code> to run it, find the menu entry for <strong>Enable Camera</strong>, and enable it. You will be prompted to reboot afterwards.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec156"/>How to do it…</h2></div></div></div><p>You can use two <a class="indexterm" id="id572"/>programs that are also installed as part of the <a class="indexterm" id="id573"/>upgrade—<code class="literal">raspivid</code> and <code class="literal">raspistill</code>—to test the camera.</p><p>To take a single picture, use the following command (<code class="literal">-t 0</code> takes the picture immediately):</p><div><pre class="programlisting">
<strong>raspistill -o image.jpg -t 0</strong>
</pre></div><p>To take a short, 10-second video in the H.264 format, use the following command (the <code class="literal">-t</code> value is in milliseconds):</p><div><pre class="programlisting">
<strong>raspivid -o video.h264 -t 10000</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec157"/>How it works…</h2></div></div></div><p>The full documentation of the <a class="indexterm" id="id574"/>camera and the <code class="literal">raspivid</code> and <code class="literal">raspistill</code> utilities is <a class="indexterm" id="id575"/>available on the Raspberry Pi site at <a class="ulink" href="http://www.raspberrypi.org/wp-content/uploads/2013/07/RaspiCam-Documentation.pdf">http://www.raspberrypi.org/wp-content/uploads/2013/07/RaspiCam-Documentation.pdf</a>.</p><div><div><h3 class="title"><a id="tip19"/>Tip</h3><p>To get more<a class="indexterm" id="id576"/> information on each of the programs, you can use the <code class="literal">less</code> command to view the instructions (use <code class="literal">q</code> to quit) as shown:</p><div><pre class="programlisting">
<strong>raspistill &gt; less</strong>
<strong>raspivid &gt; less</strong>
</pre></div><p>Each command<a class="indexterm" id="id577"/> provides full control of the camera settings, such as exposure, white balance, sharpness, contrast, brightness, and the resolution.</p></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec60"/>Using the camera with Python</h1></div></div></div><p>The <a class="indexterm" id="id578"/>camera module on the Raspberry Pi is more<a class="indexterm" id="id579"/> than just a standard webcam. Since we have full access to the controls and settings from within our own programs, it allows us to take control and create our own camera applications.</p><p>In this chapter, we will use the Python module called <code class="literal">picamera</code> created by Dave Hughes to control the camera module, which performs all the functions <code class="literal">raspivid</code> and <code class="literal">raspistill</code> support.</p><p>See <a class="ulink" href="http://picamera.readthedocs.org">http://picamera.readthedocs.org</a> for additional documentation and lots of  useful examples.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec158"/>Getting ready</h2></div></div></div><p>The Raspberry Pi camera module should be connected and installed as detailed in the previous section.</p><p>In addition, we will also need to install the Python 3 Pillow Library (the details of how to do this have been covered in the <em>Displaying photo information in an application </em>recipe in <a class="link" href="ch03.html" title="Chapter 3. Using Python for Automation and Productivity">Chapter 3</a>, <em>Using Python for Automation and Productivity</em>).</p><p>Now, install <code class="literal">picamera</code> for Python 3 using the following command:</p><div><pre class="programlisting">
<strong>sudo apt-get install python3-picamera</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec159"/>How to do it…</h2></div></div></div><div><ol class="orderedlist arabic"><li class="listitem">Create the following <code class="literal">cameraGUI.py</code> script that shall contain the main class for the GUI:<div><pre class="programlisting">#!/usr/bin/python3
#cameraGUI.py
import tkinter as TK
from PIL import Image
import subprocess
import time
import datetime
import picamera as picam

class SET():
  PV_SIZE=(320,240)
  NORM_SIZE=(2592,1944)
  NO_RESIZE=(0,0)
  PREVIEW_FILE="PREVIEW.gif"
  TEMP_FILE="PREVIEW.ppm"

class cameraGUI(TK.Frame):
  def run(cmd):
    print("Run:"+cmd)
    subprocess.call([cmd], shell=True)
  def camCapture(filename,size=SET.NORM_SIZE):
    with picam.PiCamera() as camera:
      camera.resolution = size
      print("Image: %s"%filename)
      camera.capture(filename)
  def getTKImage(filename,previewsize=SET.NO_RESIZE):
    encoding=str.split(filename,".")[1].lower()
    print("Image Encoding: %s"%encoding)
    try:
      if encoding=="gif" and previewsize==SET.NO_RESIZE:
        theTKImage=TK.PhotoImage(file=filename)
      else:
        imageview=Image.open(filename)
        if previewsize!=SET.NO_RESIZE:
          imageview.thumbnail(previewsize,Image.ANTIALIAS)
        imageview.save(SET.TEMP_FILE,format="ppm")
        theTKImage=TK.PhotoImage(file=SET.TEMP_FILE)
    except IOError:
      print("Unable to get: %s"%filename)
    return theTKImage
  def timestamp():
    ts=time.time() 
    tstring=datetime.datetime.fromtimestamp(ts)
    return tstring.strftime("%Y%m%d_%H%M%S")

  def __init__(self,parent):
    self.parent=parent
    TK.Frame.__init__(self,self.parent)
    self.parent.title("Camera GUI")
    self.previewUpdate = TK.IntVar()
    self.filename=TK.StringVar()
    self.canvas = TK.Canvas(self.parent,
                            width=SET.PV_SIZE[0],
                            height=SET.PV_SIZE[1])
    self.canvas.grid(row=0,columnspan=4)
    self.shutterBtn=TK.Button(self.parent,text="Shutter",
                                    command=self.shutter)
    self.shutterBtn.grid(row=1,column=0)
    exitBtn=TK.Button(self.parent,text="Exit",
                             command=self.exit)
    exitBtn.grid(row=1,column=3)
    previewChk=TK.Checkbutton(self.parent,text="Preview",
                              variable=self.previewUpdate)
    previewChk.grid(row=1,column=1)
    labelFilename=TK.Label(self.parent,
                           textvariable=self.filename)
    labelFilename.grid(row=2,column=0,columnspan=3)
    self.preview()
  def msg(self,text):
    self.filename.set(text)
    self.update()
  def btnState(self,state):
    self.shutterBtn["state"] = state
  def shutter(self):
    self.btnState("disabled")
    self.msg("Taking photo...")
    self.update()
    if self.previewUpdate.get() == 1:
      self.preview()
    else:
      self.normal()
    self.btnState("active")
  def normal(self):
    name=cameraGUI.timestamp()+".jpg"
    cameraGUI.camCapture(name,SET.NORM_SIZE)
    self.updateDisp(name,previewsize=SET.PV_SIZE)
    self.msg(name)
  def preview(self):
    cameraGUI.camCapture(SET.PREVIEW_FILE,SET.PV_SIZE)
    self.updateDisp(SET.PREVIEW_FILE)
    self.msg(SET.PREVIEW_FILE)
  def updateDisp(self,filename,previewsize=SET.NO_RESIZE):
    self.msg("Loading Preview...")
    self.myImage=cameraGUI.getTKImage(filename,previewsize)
    self.theImage=self.canvas.create_image(0,0,
                                  anchor=TK.NW,
                                  image=self.myImage)
    self.update()
  def exit(self):
    exit()
#End</pre></div></li><li class="listitem">Next, create<a class="indexterm" id="id580"/> the<a class="indexterm" id="id581"/> following <code class="literal">cameraGUI1normal.py</code> file to use the GUI:<div><pre class="programlisting">#!/usr/bin/python3
#cameraGUI1normal.py
import tkinter as TK
import cameraGUI as GUI

root=TK.Tk()
root.title("Camera GUI")
cam=GUI.cameraGUI(root)
TK.mainloop()
#End</pre></div></li><li class="listitem">Run the example with the following command:<div><pre class="programlisting">
<strong>python3 cameraGUI1normal.py</strong>
</pre></div></li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec160"/>How it works…</h2></div></div></div><p>In the <code class="literal">cameraGUI.py</code> file, we use a class called <code class="literal">SET</code> to contain the settings for the application (you will see in the following example why this is particularly helpful and allows us to keep all of the references to the settings in one place).</p><p>We will define a base class called <code class="literal">cameraGUI</code> (so we can attach <code class="literal">Tkinter</code> objects to it), which inherits a <code class="literal">TK.Frame</code> class. The <code class="literal">cameraGUI</code> class will contain all the methods to create the Tkinter application, including laying out the controls and providing all the required functions.</p><p>We define the following three utility functions for the class to use:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">run()</code>: This function will allow us to send commands to be run on the command line using <code class="literal">subprocess.call</code> (we will use <code class="literal">subprocess.call</code> in the following examples to perform video encoding and other applications).</li><li class="listitem" style="list-style-type: disc"><code class="literal">getTKImage()</code>: This function will allow us to create a <code class="literal">TK.PhotoImage</code> object suitable to display on the Tkinter canvas. The Tkinter canvas is unable to directly display JPG images, so we use the<a class="indexterm" id="id582"/> <strong>Pillow library</strong> (<strong>PIL</strong>) to resize it for display and convert it into a <strong>PPM</strong> file (the <strong>Portable PixMap</strong> format, which<a class="indexterm" id="id583"/> supports more colors than GIF). Since this conversion and resize process can take a few seconds, we will use GIF images to provide a quick camera preview images.</li><li class="listitem" style="list-style-type: disc"><code class="literal">timestamp()</code>: This function will allow us to generate a timestamp string that we can use to automatically name any images we take.</li></ul></div><p>Within the <a class="indexterm" id="id584"/>class initializer (<code class="literal">__init__()</code>), we define all the <a class="indexterm" id="id585"/>control variables, generate all the GUI objects and controls we want to use, and use the <code class="literal">grid()</code> functions to position the objects. The layout of the GUI is shown in the following image:</p><div><img alt="How it works…" src="img/6623OT_08_005.jpg"/><div><p>The layout of the camera GUI</p></div></div><p>We define the following control variables:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">self.previewUpdate</code>: This is linked to the status of the <strong>Preview</strong> checkbox (<code class="literal">previewChk</code>)</li><li class="listitem" style="list-style-type: disc"><code class="literal">self.filename</code>: This is linked to text displayed by the <code class="literal">labelFilename</code> widget</li></ul></div><p>We also link<a class="indexterm" id="id586"/> the <strong>Shutter</strong> button (<code class="literal">shutterBtn</code>) to <code class="literal">self.shutter()</code>, which will be called whenever the <strong>Shutter</strong> button is pressed, and <a class="indexterm" id="id587"/>the <strong>Exit</strong> button (<code class="literal">exitBtn</code>) to the  <code class="literal">self.exit()</code> function.</p><p>Finally, in the <code class="literal">__init__()</code> function, we call <code class="literal">self.preview()</code>, which will ensure that <strong>Camera GUI</strong> takes a picture and displays it as soon as the application has started.</p><p>When the <strong>Shutter</strong> button is pressed, <code class="literal">self.shutter()</code> is called. This calls <code class="literal">this.btnState("disabled")</code> to disable the <strong>Shutter</strong> button while we are taking new pictures. This prevents any pictures being taken while the camera is already in use. When the rest of the actions have been completed, <code class="literal">this.btnState("active")</code> is used to re-enable the button.</p><p>The <code class="literal">self.shutter()</code> function will call either the <code class="literal">self.normal()</code> or <code class="literal">self.preview()</code> function, depending on the status of the <strong>Preview</strong> checkbox (by getting the value of <code class="literal">self.previewUpdate</code>).</p><p>The <code class="literal">cameraGUI.camCapture()</code> function uses <code class="literal">pycamera</code> to create a camera object, set the resolution, and capture an image using the required filename. The <code class="literal">self.preview()</code> function takes an image called <code class="literal">PREVIEW_FILE</code> with a resolution of <code class="literal">PV_SIZE</code> as defined in the <code class="literal">SET</code> class.</p><p>Next, <code class="literal">self.updateDisp(PREVIEW_FILE)</code> is called and will use <code class="literal">cameraGUI.getTKImage()</code> to open the generated <code class="literal">PREVIEW.gif</code> file as a <code class="literal">TK.PhotoImage</code> object and apply it to the <code class="literal">Canvas</code> object in the GUI. We now call <code class="literal">self.update()</code>, which is a function inherited from the <code class="literal">TK.Frame</code> class; <code class="literal">self.update()</code> will allow the Tkinter display to be updated (in this case, with the new image). Finally, the <code class="literal">self.preview()</code> function will also call <code class="literal">self.msg()</code>, which will update the <code class="literal">self.filename</code> value with the filename of the image being displayed (<code class="literal">PREVIEW.gif</code>). Again, this also uses <code class="literal">self.update()</code> to update the display.</p><p>If the <strong>Preview</strong> checkbox is unchecked, then the <code class="literal">self.shutter()</code> function will call <code class="literal">self.normal()</code>. However, this time it will take a much larger 2,592 x 1,944 (5 megapixel) JPG image with the filename set to the latest <code class="literal">&lt;timestamp&gt;</code> value obtained from <code class="literal">self.timestamp()</code>. The resultant image is also resized and converted to a PPM image so it can be loaded as a <code class="literal">TK.PhotoImage</code> object that will be displayed in the application window.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec161"/>There's more…</h2></div></div></div><p>The camera application makes use of class structures to organize the code and make it easy to extend. In the following sections, we explain the types of methods and functions we have defined to allow this.</p><p>The Raspberry Pi can also make use of standard USB cameras or webcams. Alternatively, we can use additional Video4Linux drivers to allow the camera module to work like a standard webcam.</p><div><div><div><div><h3 class="title"><a id="ch08lvl3sec62"/>Class member and static functions</h3></div></div></div><p>The <code class="literal">cameraGUI</code> class has two<a class="indexterm" id="id588"/> types of function defined. First, we define some static methods (<code class="literal">run()</code>, <code class="literal">getTKImage()</code>, and <code class="literal">timestamp()</code>). These methods are tied to the class rather than to a specific instance; this means that we can use them <a class="indexterm" id="id589"/>without referring to a particular <code class="literal">cameraGUI</code> object but to the class itself. This is useful to define utility functions that are related to the class, since they may be useful in other parts of the program as well and may not need to access the data/objects contained within a <code class="literal">cameraGUI</code> object. The functions can be called using <code class="literal">cameraGUI.run("command")</code>.</p><p>Next, we define the class member functions that, as in the previous classes we have used, include a reference to <code class="literal">self</code>. This means that they are only accessible to instances of the class (objects of the type <code class="literal">cameraGUI</code>) and can use the data contained within the object (using the <code class="literal">self</code> reference).</p></div><div><div><div><div><h3 class="title"><a id="ch08lvl3sec63"/>Using a USB webcam instead</h3></div></div></div><p>The Raspberry Pi <a class="indexterm" id="id590"/>camera module is not the only way you can add a camera to the Raspberry Pi; in most cases, you can use a USB webcam as well. The current Raspberry Pi Raspbian image should detect the most common webcam devices automatically when you plug them in; however, the support can vary.</p><p>To determine if your webcam has been detected, check to see if the following device file has been created on your system by running the following command:</p><div><pre class="programlisting">
<strong>ls /dev/video*</strong>
</pre></div><p>If detected successfully, you will see <code class="literal">/dev/video0</code> or something similar, which is the reference you will use to access your webcam.</p><p>Install a suitable image capture program, such as <code class="literal">fswebcam</code>, using the following command:</p><div><pre class="programlisting">
<strong>sudo apt-get install fswebcam</strong>
</pre></div><p>You can test it with the following command:</p><div><pre class="programlisting">
<strong>fswebcam -d /dev/video0 -r 320x240 testing.jpg</strong>
</pre></div><p>Or alternatively, you can test it using <code class="literal">dd</code> as follows:</p><div><pre class="programlisting">
<strong>dd if=/dev/video0 of=testing.jpeg bs=11M count=1</strong>
</pre></div><div><div><h3 class="title"><a id="note76"/>Note</h3><p>Webcams<a class="indexterm" id="id591"/> can require additional power from the USB ports of the Raspberry Pi; if you get errors, you may find that using a powered USB hub helps. For a list of supported devices and for troubleshooting, see the Raspberry Pi wiki page at <a class="ulink" href="http://elinux.org/RPi_USB_Webcams">http://elinux.org/RPi_USB_Webcams</a>.</p></div></div><p>In the previous example, change the following functions in the <code class="literal">cameraGUI</code> class as follows:</p><div><ol class="orderedlist arabic"><li class="listitem">Remove <code class="literal">camCapture()</code> and remove <code class="literal">import picamera as picam </code>from the start of the file.</li><li class="listitem">Within <code class="literal">normal()</code>, replace <code class="literal">cameraGUI.camCapture(name,SET.NORM_SIZE)</code> with the following:<div><pre class="programlisting">    cameraGUI.run(SET.CAM_PREVIEW+SET.CAM_OUTPUT+
                  SET.PREVIEW_FILE)</pre></div></li><li class="listitem">Within<a class="indexterm" id="id592"/> <code class="literal">preview()</code>, replace <code class="literal">cameraGUI.camCapture(SET.PREVIEW_FILE,SET.PV_SIZE)</code> with the following:<div><pre class="programlisting">    cameraGUI.run(SET.CAM_NORMAL+SET.CAM_OUTPUT+name)</pre></div></li><li class="listitem">Within the <code class="literal">SET</code> class, define the following variables:<div><pre class="programlisting">CAM_OUTPUT=" "
CAM_PREVIEW="fswebcam -d /dev/video0 -r 320x240"
CAM_NORMAL="fswebcam -d /dev/video0 -r 640x480"</pre></div></li></ol></div><p>By making the previous changes to the <code class="literal">cameraGUI</code> class, the connected USB webcam will take the images instead.</p></div><div><div><div><div><h3 class="title"><a id="ch08lvl3sec64"/>Additional drivers for the Raspberry Pi camera</h3></div></div></div><p>Video4Linux drivers<a class="indexterm" id="id593"/> are available for the Raspberry Pi camera<a class="indexterm" id="id594"/> module. While these additional drivers are not quite official yet, it is likely that they will be included in the Raspbian image when they are. For more details, see <a class="ulink" href="http://www.linux-projects.org/uv4l/">http://www.linux-projects.org/uv4l/</a>.</p><p>The driver will allow you to use the camera module like you would a USB webcam, as a <code class="literal">/dev/video*</code> device, although you will not need this for the examples in this chapter.</p><p>Perform the following steps to install the additional drivers:</p><div><ol class="orderedlist arabic"><li class="listitem">First, download the <code class="literal">apt</code> keys and add the source to the <code class="literal">apt</code> sources list. You can do this with the following commands:<div><pre class="programlisting">
<strong>wget http://www.linux-projects.org/listing/uv4l_repo/lrkey.asc</strong>
<strong>sudo apt-key add ./lrkey.asc</strong>
<strong>sudo nano /etc/apt/souces.list </strong>
</pre></div></li><li class="listitem">Add the following into the file (on a single line):<div><pre class="programlisting">
<strong>deb http://www.linux-projects.org/listing/uv4l_repo/raspbian/ wheezy main</strong>
</pre></div></li><li class="listitem">Install the drivers with the following commands:<div><pre class="programlisting">
<strong>sudo apt-get update</strong>
<strong>sudo apt-get install uv4l uv4l-raspicam</strong>
</pre></div></li><li class="listitem">To use the <code class="literal">uv4l</code> driver, load it using the following command (on a single line):<div><pre class="programlisting">uv4l --driver raspicam --auto-video_nr --width 640 –height480 --encoding jpeg</pre></div></li></ol></div><p>The Raspberry Pi will then be accessible through <code class="literal">/dev/video0</code> (depending on whether you have other video <a class="indexterm" id="id595"/>devices installed). It can be used with <a class="indexterm" id="id596"/>standard webcam programs.</p></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec162"/>See also</h2></div></div></div><p>For more examples on using the Tkinter library, see <a class="link" href="ch03.html" title="Chapter 3. Using Python for Automation and Productivity">Chapter 3</a>, <em>Using Python for Automation and Productivity</em>, and <a class="link" href="ch04.html" title="Chapter 4. Creating Games and Graphics">Chapter 4</a>, <em>Creating Games and Graphics</em>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec61"/>Generating a time-lapse video</h1></div></div></div><p>Having a camera attached to<a class="indexterm" id="id597"/> a computer provides us with a great way to take pictures at controlled intervals and automatically process them into a video to create a time-lapse sequence. The <code class="literal">pycamera</code> Python module has a special <code class="literal">capture_continuous()</code> function that will create a series of images. For the time-lapse video, we will specify the time between each image and the total number of images that need to be taken. To help the user, we will also calculate the overall duration of the video to provide an indication of how long it will take.</p><p>We shall add to our previous GUI interface to provide controls to run time lapses and also automatically generate a video clip from the results. The GUI will now look similar to the following screenshot:</p><div><img alt="Generating a time-lapse video" src="img/6623OT_08_006.jpg"/><div><p>The time-lapse application</p></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec163"/>Getting ready</h2></div></div></div><p>You will need everything <a class="indexterm" id="id598"/>set up as it was for the previous example, including the  <code class="literal">cameraGUI.py</code> file that we created in the same directory and <code class="literal">pycamera</code>, which we installed. We shall also use <code class="literal">mencoder</code>, which will allow us to take the time-lapse images and combine them into a video clip.</p><p>To install <code class="literal">mencoder</code>, use <code class="literal">apt-get</code>, as shown in the following command:</p><div><pre class="programlisting">
<strong>sudo apt-get install mencoder</strong>
</pre></div><p>An explanation of the command-line options can be found in the <code class="literal">mencoder</code> man pages.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec164"/>How to do it…</h2></div></div></div><p>Create <code class="literal">timelapseGUI.py</code> in the same directory as <code class="literal">cameraGUI.py</code> by performing the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Start by importing <a class="indexterm" id="id599"/>the supporting modules (including <code class="literal">cameraGUI</code>) as shown in the following code snippet:<div><pre class="programlisting">#!/usr/bin/python3
#timelapseGUI.py
import tkinter as TK
from tkinter import messagebox
import cameraGUI as camGUI
import time</pre></div></li><li class="listitem">Extend the <code class="literal">cameraGUI.SET</code> class with settings for the time lapse and encoding as follows:<div><pre class="programlisting">class SET(camGUI.SET):
  TL_SIZE=(1920,1080)
  ENC_PROG="mencoder -nosound -ovc lavc -lavcopts"
  ENC_PROG+=" vcodec=mpeg4:aspect=16/9:vbitrate=8000000"
  ENC_PROG+=" -vf scale=%d:%d"%(TL_SIZE[0],TL_SIZE[1])
  ENC_PROG+=" -o %s -mf type=jpeg:fps=24 mf://@%s"
  LIST_FILE="image_list.txt"</pre></div></li><li class="listitem">Extend the main <code class="literal">cameraGUI</code> class with an additional function to perform the time lapse as follows:<div><pre class="programlisting">class cameraGUI(camGUI.cameraGUI):
  def camTimelapse(filename,size=SET.TL_SIZE,
                    timedelay=10,numImages=10):
    with camGUI.picam.PiCamera() as camera:
      camera.resolution = size
      for count, name in \
            enumerate(camera.capture_continuous(filename)):
        print("Timelapse: %s"%name)
        if count == numImages:
          break
        time.sleep(timedelay)</pre></div></li><li class="listitem">Add the extra controls for the time-lapse GUI as shown in the following code snippet:<div><pre class="programlisting">  def __init__(self,parent):
    super(cameraGUI,self).__init__(parent)
    self.parent=parent
    TK.Frame.__init__(self,self.parent,background="white")
    self.numImageTL=TK.StringVar()
    self.peroidTL=TK.StringVar()
    self.totalTimeTL=TK.StringVar()
    self.genVideoTL=TK.IntVar()
    labelnumImgTK=TK.Label(self.parent,text="TL:#Images")
    labelperoidTK=TK.Label(self.parent,text="TL:Delay")
    labeltotalTimeTK=TK.Label(self.parent,
                              text="TL:TotalTime")
    self.numImgSpn=TK.Spinbox(self.parent,
                       textvariable=self.numImageTL,
                       from_=1,to=99999,
                       width=5,state="readonly",
                       command=self.calcTLTotalTime)
    self.peroidSpn=TK.Spinbox(self.parent,
                       textvariable=self.peroidTL,
                       from_=1,to=99999,width=5,
                       command=self.calcTLTotalTime)
    self.totalTime=TK.Label(self.parent,
                       textvariable=self.totalTimeTL)
    self.TLBtn=TK.Button(self.parent,text="TL GO!",
                             command=self.timelapse)
    genChk=TK.Checkbutton(self.parent,text="GenVideo",
                             command=self.genVideoChk,
                             variable=self.genVideoTL)
    labelnumImgTK.grid(row=3,column=0)
    self.numImgSpn.grid(row=4,column=0)
    labelperoidTK.grid(row=3,column=1)
    self.peroidSpn.grid(row=4,column=1)
    labeltotalTimeTK.grid(row=3,column=2)
    self.totalTime.grid(row=4,column=2)
    self.TLBtn.grid(row=3,column=3)
    genChk.grid(row=4,column=3)
    self.numImageTL.set(10)
    self.peroidTL.set(5)
    self.genVideoTL.set(1)
    self.calcTLTotalTime()</pre></div></li><li class="listitem">Add supporting <a class="indexterm" id="id600"/>functions to calculate the settings and handle the time lapse as follows:<div><pre class="programlisting">  def btnState(self,state):
    self.TLBtn["state"] = state
    super(cameraGUI,self).btnState(state)
  def calcTLTotalTime(self):
    numImg=float(self.numImageTL.get())-1
    peroid=float(self.peroidTL.get())
    if numImg&lt;0:
      numImg=1
    self.totalTimeTL.set(numImg*peroid)
  def timelapse(self):
    self.msg("Running Timelapse")
    self.btnState("disabled")
    self.update()
    self.tstamp="TL"+cameraGUI.timestamp()
    cameraGUI.camTimelapse(self.tstamp+'{counter:03d}.jpg',
                           SET.TL_SIZE,
                           float(self.peroidTL.get()),
                           int(self.numImageTL.get()))
    if self.genVideoTL.get() == 1:
      self.genTLVideo()
    self.btnState("active")
    TK.messagebox.showinfo("Timelapse Complete",
                           "Processing complete")
    self.update()</pre></div></li><li class="listitem">Add supporting functions to handle and generate the time-lapse video as follows:<div><pre class="programlisting">  def genTLVideo(self):
    self.msg("Generate video...")
    cameraGUI.run("ls "+self.tstamp+"*.jpg &gt; "
                                +SET.LIST_FILE)
    cameraGUI.run(SET.ENC_PROG%(self.tstamp+".avi",
                                      SET.LIST_FILE))
    self.msg(self.tstamp+".avi")
#End</pre></div></li><li class="listitem">Next, create<a class="indexterm" id="id601"/> the following <code class="literal">cameraGUI2timelapse.py</code> script to use the GUI:<div><pre class="programlisting">#!/usr/bin/python3
#cameraGUI2timelapse.py
import tkinter as TK
import timelapseGUI as GUI

root=TK.Tk()
root.title("Camera GUI")
cam=GUI.cameraGUI(root)
TK.mainloop()
#End</pre></div></li></ol></div><p>We import <code class="literal">timelapseGUI</code> instead of <code class="literal">cameraGUI</code>; this will add the <code class="literal">timelapseGUI</code> module to the <code class="literal">cameraGUI</code> script.</p><p>Run the example with the following command:</p><div><pre class="programlisting">
<strong>python3 cameraGUI2timelapse.py</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec165"/>How it works…</h2></div></div></div><p>The <code class="literal">timelapseGUI.py</code> script allows us to take the classes defined in <code class="literal">cameraGUI.py</code> and extend them. The<a class="indexterm" id="id602"/> previous <code class="literal">cameraGUI</code> class inherits all of the content of the <code class="literal">TK.Frame</code> class, and using the same technique we can also inherit the <code class="literal">SET</code> and <code class="literal">cameraGUI</code> classes in our application.</p><p>We add some additional settings to the <code class="literal">SET</code> class to provide the settings for <code class="literal">mencoder</code> (to encode the video).</p><p>We shall extend the basic <code class="literal">cameraGUI</code> class by inheriting from <code class="literal">camGUI.cameraGUI</code> and defining a new version of <code class="literal">__init__()</code> for the class. Using <code class="literal">super()</code>, we can include the functionality from the original <code class="literal">__init__()</code> function and then define the extra controls we want to add to the GUI. The extended GUI is shown in the following screenshot:</p><div><img alt="How it works…" src="img/6623OT_08_007.jpg"/><div><p>The time-lapse GUI layout that extends the base camera GUI</p></div></div><p>We define the following <a class="indexterm" id="id603"/>control variables:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">self.numImageTL</code>: This is linked to the value of the <code class="literal">numImgSpn</code> spinbox control to specify the number of images we want to take in our time lapse (and also provide the <code class="literal">numimages</code> value for <code class="literal">camTimelapse</code>).</li><li class="listitem" style="list-style-type: disc"><code class="literal">self.peroidTL</code>: This is linked to the value of the <code class="literal">peroidSpn</code> spinbox control; it determines how many seconds there should be between the time-lapse images (and also provides the <code class="literal">timedelay</code> value for <code class="literal">camTimelapse</code>).</li><li class="listitem" style="list-style-type: disc"><code class="literal">self.totalTimeTL</code>: This is linked to the <code class="literal">totalTime</code> label object. It is calculated using the number of images and the <code class="literal">timedelay</code> time between each to indicate how long the time lapse will run for.</li><li class="listitem" style="list-style-type: disc"><code class="literal">self.genVideoTL</code>: This controls the state of the <code class="literal">genChk</code> checkbox control. It is used to determine whether the video has been generated after the time-lapse images have been taken.</li></ul></div><p>We link both of the spinbox controls to <code class="literal">self.calcTLTotalTime()</code> so that when they are changed, the <code class="literal">totalTimeTL</code> value is also updated (although it is not called if they are edited directly). We link <code class="literal">genChk</code> to <code class="literal">self.genVideoChk()</code> and <code class="literal">TLBtn</code> to <code class="literal">self.timelapse()</code>.</p><p>Finally, we specify the positions of the controls using <code class="literal">grid()</code> and set some defaults for the time-lapse settings.</p><p>The <code class="literal">self.genVideoChk()</code> function is<a class="indexterm" id="id604"/> called when the <code class="literal">genChk</code> checkbox is ticked or cleared. This allows us to inform the user of the effect that this checkbox has by generating a pop-up message box to say if the video will be generated at the end of the time lapse or if just images will be created.</p><p>When the <strong>TL GO!</strong> button is pressed (<code class="literal">TLBtn</code>), <code class="literal">self.timelapse()</code> is called; this will disable the <strong>Shutter</strong> and <strong>TL GO!</strong> buttons (since we have also extended the <code class="literal">self.btnState()</code> function). The <code class="literal">self.timelapse()</code> function will also set the <code class="literal">self.tstamp</code> value so the same timestamp can be used for the images and the resulting video file (if generated).</p><p>The time lapse is run using the <code class="literal">camTimelapse()</code> function as shown in the following code:</p><div><pre class="programlisting">def camTimelapse(filename,size=SET.TL_SIZE,
                    timedelay=10,numImages=10):
    with camGUI.picam.PiCamera() as camera:
      camera.resolution = size
      for count, name in \
            enumerate(camera.capture_continuous(filename)):
        print("Timelapse: %s"%name)
        if count == numImages:
          break
        time.sleep(timedelay)</pre></div><p>We create a new <code class="literal">PiCamera</code> object, set the image resolution, and start a <code class="literal">for…in</code> loop for <code class="literal">capture_continuous()</code>. Each time an image is taken, we print the filename and then wait for the required <code class="literal">timedelay</code> value. Finally, when the required number of images have been taken, we break out of the loop and continue.</p><p>Once this is complete, we check the value of <code class="literal">self.genVideoTL</code> to determine if we want to generate the video (which is handled by <code class="literal">genTLVideo()</code>).</p><p>To generate the video, we first run the following command to create an <code class="literal">image_list.txt</code> file of the images:</p><div><pre class="programlisting">
<strong>ls &lt;self.tstamp&gt;*.jpg &gt; image_list.txt</strong>
</pre></div><p>Then we run <code class="literal">mencoder</code> with<a class="indexterm" id="id605"/> the suitable settings (see the <code class="literal">mencoder</code> man pages for what each item does) to create an MPEG4-encoded (8 Mbps) AVI file with 24 <code class="literal">frames per second (fps)</code> from the list of time-lapse images. The equivalent command (defined by <code class="literal">ENC_PROG</code>) is as follows:</p><div><pre class="programlisting">
<strong>mencoder -nosound -ovc lavc \</strong>
<strong> -lavcopts vcodec=mpeg4:aspect=16/9:vbitrate=8000000 \</strong>
<strong> -vf scale=1920:1080 -o &lt;self.tstamp&gt;.avi \</strong>
<strong> -mf type=jpeg:fps=24 mf://@image_list.txt</strong>
</pre></div><div><div><h3 class="title"><a id="tip21"/>Tip</h3><p>Long commands can be split into several lines on the command terminal by using the <code class="literal">\</code> character. This allows you to continue writing the command on another line, only executing it when you finish a line without the <code class="literal">\</code> character.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec166"/>There's more…</h2></div></div></div><p>This chapter uses methods such as class inheritance and function overriding to structure and reuse our code in a number of different ways. When used correctly, these methods could enable us to design complex systems in a logical and flexible way.</p><p>Additionally, when generating your own time-lapse sequences, you can opt to switch off the LED on the camera module or make use of the low-light version of the Raspberry Pi camera: the NoIR camera.</p><div><div><div><div><h3 class="title"><a id="ch08lvl3sec65"/>Class inheritance and function overriding</h3></div></div></div><p>In the previous example, we <a class="indexterm" id="id606"/>used some clever coding in order to reuse our original <code class="literal">cameraGUI</code> class <a class="indexterm" id="id607"/>and create a plugin file that extends its features.</p><p>The class name does not have to be the same as <code class="literal">cameraGUI</code> (we just use it in this case so we can swap out the additional GUI components just by changing the file we import). In fact, we could define one basic class that contains several general functions and then extend the class by inheritance into a number of subclasses; here, each subclass defines specific behaviors, functions, and data. The extending and structuring of the subclasses is shown in the following diagram:</p><div><img alt="Class inheritance and function overriding" src="img/6623OT_08_008.jpg"/><div><p>This diagram shows how classes can be extended and structured</p></div></div><p>To illustrate this, we will take a non-code example in which we have written a general recipe for preparing a cake. You can then extend the <code class="literal">basicCake</code> recipe by inheriting all the <code class="literal">basicCake</code> elements and add some additional steps (equivalent code functions) to perhaps add icing/frosting on top to make an <code class="literal">icedCake(basicCake)</code> class. We did this with our <code class="literal">SET</code> class by<a class="indexterm" id="id608"/> adding additional items to an existing class (we just chose not to change the name).</p><p>We can also add in some <a class="indexterm" id="id609"/>additional elements to the existing steps (add some currants at the <code class="literal">addIngredients</code> step and create <code class="literal">currantCake(basicCake)</code>). We have done this using the <code class="literal">super()</code> function in our code by adding additional parts to the <code class="literal">__init__()</code> function. For example, we would use <code class="literal">super(basicCake.self).addIngredients()</code> to include all the steps in the <code class="literal">addIngredients()</code> function defined in the <code class="literal">basicCake</code> class, and then add an extra step to include currants. The advantage is if we then change the basic cake ingredients, it will also flow through to all the other classes.</p><p>You could even override some of the original functions by replacing them with new ones; for instance, you could replace the original recipe for <code class="literal">basicCake</code> with one to make <code class="literal">chocolateCake(basicCake)</code> while still using the same instructions to cook, and so on. We can do this by defining replacement functions with the same names without using <code class="literal">super()</code>.</p><p>Using structured design in this way can become very powerful since we can easily create many variants of the same sort of object but have all the common elements defined in the same place. This has many advantages when it comes to testing, developing, and maintaining large and complex systems. The key here is to take an overall view of your project and try to identify the common elements before you begin. You will find that the better the structure you have, the easier it is to develop and improve it.</p><p>For more information <a class="indexterm" id="id610"/>on this, it is worth reading up on object-oriented design methods <a class="indexterm" id="id611"/>and how to use <strong>Unified Modelling Language</strong> (<strong>UML</strong>)<a class="indexterm" id="id612"/> to help you describe and understand your system.</p></div><div><div><div><div><h3 class="title"><a id="ch08lvl3sec66"/>Disabling the camera LED</h3></div></div></div><p>If you want to create <a class="indexterm" id="id613"/>time-lapse videos at night or next to windows, you may notice that the red camera LED (which lights up for every shot) adds unwanted light or reflections. Fortunately, the camera LED can be controlled through the GPIO. The LED is controlled using <code class="literal">GPIO.BCM</code> Pin 5; unfortunately, there isn't an equivalent <code class="literal">GPIO.BOARD</code> pin number for it.</p><p>To add it to a Python script, use the following code:</p><div><pre class="programlisting">import RPi.GPIO as GPIO

GPIO.cleanup()
GPIO.setmode(GPIO.BCM)
CAMERALED=5 #GPIO using BCM numbering
GPIO.setup(CAMERALED, GPIO.OUT)
GPIO.output(CAMERALED,False)</pre></div><p>Alternatively, you could use the LED for something else, for example, as an indicator as part of a delay timer that provides a countdown and warning that the camera is about to take an image.</p></div><div><div><div><div><h3 class="title"><a id="ch08lvl3sec67"/>Pi NoIR – taking night shots</h3></div></div></div><p>There is also a variant of the Raspberry Pi camera module available called <a class="indexterm" id="id614"/>
<strong>Pi NoIR</strong>. This version of the camera is the same as the original, except that the internal infrared filter has been removed. Among other things, this allows you to use infrared lighting to illuminate areas at night time (just like most night security cameras do) and see everything that is happening in the dark!</p><p>
<em>The MagPi</em> Issue 18 (<a class="ulink" href="https://www.raspberrypi.org/magpi/">https://www.raspberrypi.org/magpi/</a>) has<a class="indexterm" id="id615"/> published an excellent feature explaining the other uses of the Pi NoIR camera module.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec62"/>Creating a stop frame animation</h1></div></div></div><p>Stop frame (or stop motion) animation<a class="indexterm" id="id616"/> is the process of taking a series of still images of items, while making very small movements (typically of an easily moveable object such as a doll or plasticine model) in each frame. When the frames are assembled into a video, the small movements combine to produce an animation.</p><div><img alt="Creating a stop frame animation" src="img/6623OT_08_009.jpg"/><div><p>Multiple images can be combined into an animation</p></div></div><p>Traditionally, such animations were made by taking hundreds or even thousands of individual photos on a film camera (such as a Cine Super 8 movie camera) and then sending the film off to be developed and playing back the results some weeks later. Despite the inspiring creations by Nick Park at Aardman Animations, including Wallace and Gromit (which are full-length, stop frame animation films), this was a hobby that was a little out of reach for most.</p><p>In the modern digital age, we can take multiple images quickly and easily with the luxury of reviewing the results almost instantly. Now anyone can try their hand at their own animated masterpieces with very little cost or effort.</p><p>We will extend our original <strong>Camera GUI</strong> with some extra features that will allow us to create our own stop frame animations. It will allow us to take images and try them out in a sequence before generating a finished video for us.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec167"/>Getting ready</h2></div></div></div><p>The software setup for this<a class="indexterm" id="id617"/> example will be the same as the previous time-lapse example. Again, we will need <code class="literal">mencoder</code> to be installed and we need the <code class="literal">cameraGUI.py</code> file in the  same directory.</p><p>You will also need something to animate, ideally something you can put in different poses, like the two dolls shown in the following image:</p><div><img alt="Getting ready" src="img/6623OT_08_010.jpg"/><div><p>Two potential stars for our stop frame animation</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec168"/>How to do it…</h2></div></div></div><p>Create <code class="literal">animateGUI.py</code> in the<a class="indexterm" id="id618"/> same directory as <code class="literal">cameraGUI.py</code> by performing the  following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Start by importing the supporting modules (including <code class="literal">cameraGUI</code>), as shown in the following code:<div><pre class="programlisting">#!/usr/bin/python3
#animateGUI.py
import tkinter as TK
from tkinter import messagebox
import time
import os
import cameraGUI as camGUI</pre></div></li><li class="listitem">Extend the <code class="literal">cameraGUI.SET</code> class with settings for the image size and encoding as follows:<div><pre class="programlisting">class SET(camGUI.SET):
  TL_SIZE=(1920,1080)
  ENC_PROG="mencoder -nosound -ovc lavc -lavcopts"
  ENC_PROG+=" vcodec=mpeg4:aspect=16/9:vbitrate=8000000"
  ENC_PROG+=" -vf scale=%d:%d"%(TL_SIZE[0],TL_SIZE[1])
  ENC_PROG+=" -o %s -mf type=jpeg:fps=24 mf://@%s"
  LIST_FILE="image_list.txt"</pre></div></li><li class="listitem">Extend the <a class="indexterm" id="id619"/>main <code class="literal">cameraGUI</code> class with the functions required for the animation as follows:<div><pre class="programlisting">class cameraGUI(camGUI.cameraGUI):
  def diff(a, b):
    b = set(b)
    return [aa for aa in a if aa not in b]
  def __init__(self,parent):
    super(cameraGUI,self).__init__(parent)
    self.parent=parent
    TK.Frame.__init__(self,self.parent,
                      background="white")
    self.theList = TK.Variable()
    self.imageListbox=TK.Listbox(self.parent,
                   listvariable=self.theList,
                       selectmode=TK.EXTENDED)
    self.imageListbox.grid(row=0, column=4,columnspan=2,
                              sticky=TK.N+TK.S+TK.E+TK.W)
    yscroll = TK.Scrollbar(command=self.imageListbox.yview,
                                        orient=TK.VERTICAL)
    yscroll.grid(row=0, column=6, sticky=TK.N+TK.S)
    self.imageListbox.configure(yscrollcommand=yscroll.set)
    self.trimBtn=TK.Button(self.parent,text="Trim",
                                  command=self.trim)
    self.trimBtn.grid(row=1,column=4)
    self.speed = TK.IntVar()
    self.speed.set(20)
    self.speedScale=TK.Scale(self.parent,from_=1,to=30,
                                  orient=TK.HORIZONTAL,
                                   variable=self.speed,
                                   label="Speed (fps)")
    self.speedScale.grid(row=2,column=4)
    self.genBtn=TK.Button(self.parent,text="Generate",
                                 command=self.generate)
    self.genBtn.grid(row=2,column=5)
    self.btnAniTxt=TK.StringVar()
    self.btnAniTxt.set("Animate")
    self.animateBtn=TK.Button(self.parent,
              textvariable=self.btnAniTxt,
                      command=self.animate)
    self.animateBtn.grid(row=1,column=5)
    self.animating=False
    self.updateList()</pre></div></li><li class="listitem">Add functions to<a class="indexterm" id="id620"/> list the images that were taken and remove them from the list using the following code snippet:<div><pre class="programlisting">  def shutter(self):
    super(cameraGUI,self).shutter()
    self.updateList()

  def updateList(self):
    filelist=[]
    for files in os.listdir("."):
      if files.endswith(".jpg"):
        filelist.append(files)
    filelist.sort()
    self.theList.set(tuple(filelist))
    self.canvas.update()

  def generate(self):
    self.msg("Generate video...")
    cameraGUI.run("ls *.jpg &gt; "+SET.LIST_FILE)
    filename=cameraGUI.timestamp()+".avi"
    cameraGUI.run(SET.ENC_PROG%(filename,SET.LIST_FILE))
    self.msg(filename)
    TK.messagebox.showinfo("Encode Complete",
                           "Video: "+filename)
  def trim(self):
    print("Trim List")
    selected = map(int,self.imageListbox.curselection())
    trim=cameraGUI.diff(range(self.imageListbox.size()),
                                                selected)
    for item in trim:
      filename=self.theList.get()[item]
      self.msg("Rename file %s"%filename)
      #We could delete os.remove() but os.rename() allows
      #us to change our minds (files are just renamed).
      os.rename(filename,
                filename.replace(".jpg",".jpg.bak"))
      self.imageListbox.selection_clear(0,
                      last=self.imageListbox.size())
    self.updateList()</pre></div></li><li class="listitem">Include functions to perform the test animation using the image list as follows:<div><pre class="programlisting">  def animate(self):
    print("Animate Toggle")
    if (self.animating==True):
      self.btnAniTxt.set("Animate")
      self.animating=False
    else:
      self.btnAniTxt.set("STOP")
      self.animating=True
      self.doAnimate()

  def doAnimate(self):
    imageList=[]
    selected = self.imageListbox.curselection()
    if len(selected)==0:
      selected=range(self.imageListbox.size())
    print(selected)
    if len(selected)==0:
      TK.messagebox.showinfo("Error",
                      "There are no images to display!")
      self.animate()
    elif len(selected)==1:
      filename=self.theList.get()[int(selected[0])]
      self.updateDisp(filename,SET.PV_SIZE)
      self.animate()
    else:
      for idx,item in enumerate(selected):
        self.msg("Generate Image: %d/%d"%(idx+1,
                                        len(selected)))
        filename=self.theList.get()[int(item)]
        aImage=cameraGUI.getTKImage(filename,SET.PV_SIZE)
        imageList.append(aImage)
      print("Apply Images")
      canvasList=[]
      for idx,aImage in enumerate(imageList):
        self.msg("Apply Image: %d/%d"%(idx+1,
                                       len(imageList)))
        canvasList.append(self.canvas.create_image(0, 0,
                                  anchor=TK.NW,
                                  image=imageList[idx],
                                  state=TK.HIDDEN))
      self.cycleImages(canvasList)

  def cycleImages(self,canvasList):
    while (self.animating==True):
      print("Cycle Images")
      for idx,aImage in enumerate(canvasList):
        self.msg("Cycle Image: %d/%d"%(idx+1,
                                  len(canvasList)))
        self.canvas.itemconfigure(canvasList[idx],
                                  state=TK.NORMAL)
        if idx&gt;=1:
          self.canvas.itemconfigure(canvasList[idx-1],
                                      state=TK.HIDDEN)
        elif len(canvasList)&gt;1:
          self.canvas.itemconfigure(
                        canvasList[len(canvasList)-1],
                                      state=TK.HIDDEN)
        self.canvas.update()
        time.sleep(1/self.speed.get())
#End</pre></div></li><li class="listitem">Next, create the following <code class="literal">cameraGUI3animate.py</code> file to use the GUI:<div><pre class="programlisting">#!/usr/bin/python3
#cameraGUI3animate.py
import tkinter as TK
import animateGUI as GUI

#Define Tkinter App
root=TK.Tk()
root.title("Camera GUI")
cam=GUI.cameraGUI(root)
TK.mainloop()
#End</pre></div></li><li class="listitem">Run the <a class="indexterm" id="id621"/>example with the following command:<div><pre class="programlisting">
<strong>python3 cameraGUI3animate.py</strong>
</pre></div></li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec169"/>How it works…</h2></div></div></div><p>Once again, we create a new class based on the original <code class="literal">cameraGUI</code> class. This time, we define the following GUI with six extra controls:</p><div><img alt="How it works…" src="img/6623OT_08_011.jpg"/><div><p>The animation GUI layout</p></div></div><p>We create a <a class="indexterm" id="id622"/>listbox control (<code class="literal">imageListbox</code>) that will contain a list of the <code class="literal">.jpg</code> images in the current directory (<code class="literal">self.theList</code>). This control has a vertical scroll bar (<code class="literal">yscroll</code>) linked to it to allow easy scrolling of the list, and <code class="literal">selectmode=TK.EXTENDED</code> is used to allow multiple selections using <em>Shift</em> and <em>Ctrl</em> (for block and group selections).</p><p>Next, we add a <strong>Trim</strong> button (<code class="literal">timeBtn</code>) that will call <code class="literal">self.trim()</code>. This will remove any items that have not been selected in the list. We use <code class="literal">curselection()</code> to get a list of the currently selected items from the <code class="literal">imageListbox</code> control. The <code class="literal">curselection()</code> function normally returns a list of indexes that are numerical strings, so we use <code class="literal">map(int,...)</code> to convert the result into a list of integers.</p><p>We use this list to get all the indexes that have not been selected using our utility <code class="literal">diff(a,b)</code> function. The function compares a full list of indexes against the selected ones and returns any that haven't been selected.</p><p>The <code class="literal">self.trim()</code> function uses <code class="literal">os.rename()</code> to change the filename extensions from <code class="literal">.jpg</code> to <code class="literal">.jpg.bak</code> for all the non-selected images. We could delete them using <code class="literal">os.remove()</code>, but we only really want to rename them to stop them from appearing in the list and final video. The list is repopulated using <code class="literal">self.updateList()</code>, which updates <code class="literal">self.theList</code> with a list of all the <code class="literal">.jpg</code> files available.</p><p>We add a scale control (<code class="literal">speedScale</code>) that is linked to <code class="literal">self.speed</code> and is used to control the playback speed of the animation test. As earlier, we add a <strong>Generate</strong> button (<code class="literal">genBtn</code>) that calls <code class="literal">self.generate()</code>.</p><p>Finally, we add the <strong>Animate</strong> button (<code class="literal">animateBtn</code>). The text for the button is linked to  <code class="literal">self.btnAniTxt</code> (making it easy to change within our program), and when pressed, the button calls <code class="literal">self.animate()</code>.</p><div><div><h3 class="title"><a id="note77"/>Note</h3><p>We override the original <code class="literal">shutter()</code> function from the original <code class="literal">cameraGUI</code> script by adding a call to <code class="literal">self.updateList()</code>. This ensures that after an image has been taken, the list of images is updated with the new image automatically. Again, we use <code class="literal">super()</code> to ensure that the original functionality is also performed.</p></div></div><p>The <code class="literal">animate()</code> function (called by clicking on the <strong>Animate</strong> button) allows us to test a selection of images to <a class="indexterm" id="id623"/>see whether they will make a good animation or not. When the button is clicked on, we change the text of the button to <strong>STOP</strong>, the <code class="literal">self.animating</code> flag to <strong>True</strong> (to indicate that the animation mode is running), and call <code class="literal">doAnimate()</code>.</p><p>The <code class="literal">doAnimate()</code> function first gets a list of currently selected images in the <code class="literal">imageListbox</code> control, generates a list of <code class="literal">TK.PhotoImage</code> objects, and attaches them to the <code class="literal">self.canvas</code> object in the GUI. However, if only one image has been selected, we display it directly using <code class="literal">self.updateDisp()</code>. Alternatively, if no images have been selected, it will try to use them all (unless the list is empty, in which case it will inform the user that there are no images to animate). When we have more than one <code class="literal">TK.PhotoImage</code> object linked to the canvas, we can loop through them using the <code class="literal">cycleImages()</code> function.</p><p>The <code class="literal">TK.PhotoImage</code> objects are all created with their states set to <code class="literal">TK.HIDDEN</code>, which means they are not visible on the canvas. To produce the animation effect, the <code class="literal">cycleImages()</code> function will set each image to <code class="literal">TK.NORMAL</code> and then <code class="literal">TK.HIDDEN</code> again, allowing each frame to be displayed for 1 divided by <code class="literal">self.speed</code> (the fps value set by the Scale control) seconds before showing the next.</p><p>The <code class="literal">cycleImages()</code> function will perform the animation as long as <code class="literal">self.animating</code> is <strong>True</strong>, that is, until the <code class="literal">animateBtn</code> object is clicked on again.</p><p>Once the user is happy with their animation, they can generate the video using the <strong>Generate</strong> button (<code class="literal">genBtn</code>). The <code class="literal">generate()</code> function will call <code class="literal">mencoder</code> to generate the final video of all the images in the <code class="literal">imageListbox</code> control. </p><p>If you really want to get into producing animations, you should consider adding some extra features to help you, such as the ability to duplicate and reorder frames. You may want to add some manual<a class="indexterm" id="id624"/> adjustments for the camera to avoid white balance and lighting fluctuations caused by the automatic settings of the camera.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec170"/>There's more…</h2></div></div></div><p>The camera module is ideal for close-up photography due to its small size and ability to be remotely controlled. By using small lenses or adding hardware controls, you could make a purpose-built animation machine.</p><div><div><div><div><h3 class="title"><a id="ch08lvl3sec68"/>Improving the focus</h3></div></div></div><p>The Raspberry Pi camera<a class="indexterm" id="id625"/> lens has been designed mainly for middle to long distance photography, and it therefore has trouble focusing on objects that are closer than 25 cm (10 inches). However, using some basic lenses, we can adjust the effective focal length and make it more suitable for macro photography. You can use add-on lenses that are available for mobile phones or credit card-style magnifier lenses to adjust the focus, as shown in the following images:</p><div><img alt="Improving the focus" src="img/6623OT_08_012.jpg"/><div><p>An add-on macro lens (right) and a credit card magnifier (left) can improve the focus of close-up items</p></div></div></div><div><div><div><div><h3 class="title"><a id="ch08lvl3sec69"/>Creating a hardware shutter</h3></div></div></div><p>Of course, while it is <a class="indexterm" id="id626"/>useful to have a display available to review the images taken, it is often useful to be able to simply press a physical button to take an image. Fortunately, this is just a matter of attaching a button (and resistor) to a GPIO pin, as we have done previously (see the <em>Responding to a button</em> recipe in <a class="link" href="ch06.html" title="Chapter 6. Using Python to Drive Hardware">Chapter 6</a>, <em>Using Python to Drive Hardware</em>), and creating suitable GPIO control code to call our <code class="literal">cameraGUI.camCapture()</code> function. The code for this is as follows:</p><div><pre class="programlisting">#!/usr/bin/python3
#shutterCam.py
import RPi.GPIO as GPIO
import cameraGUI as camGUI
import time


GPIO.setmode(GPIO.BOARD)
CAMERA_BTN=12 #GPIO Pin 12
GPIO.setup(CAMERA_BTN,GPIO.IN,pull_up_down=GPIO.PUD_UP)
count=1
try:
  while True:
    btn_val = GPIO.input(CAMERA_BTN)
    #Take photo when Pin 12 at 0V
    if btn_val==False:
      camGUI.cameraGUI.camCapture("Snap%03d.jpg"%count,
                                   camGUI.SET.NORM_SIZE)
      count+=1
    time.sleep(0.1)
finally:
  GPIO.cleanup()
#End</pre></div><p>The previous code will <a class="indexterm" id="id627"/>take a picture when the button is pressed. The following diagram shows the connections and circuit diagram required to achieve this:</p><div><img alt="Creating a hardware shutter" src="img/6623OT_08_013.jpg"/><div><p>The button (and 1K ohm resistor) should be connected between pins 12 and 6 (GND)</p></div></div><p>You don't even have to <a class="indexterm" id="id628"/>stop here since you can add buttons and switches for any of the controls or settings for the camera if you want to. You can even use other hardware (such as infrared sensors and so on) to trigger the camera to take an image or video.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec63"/>Making a QR code reader</h1></div></div></div><p>You have probably seen QR <a class="indexterm" id="id629"/>codes in various places, and perhaps even used a few to pick up links from posters or adverts. However, they can be far more useful if you make your own. The following example discusses how we can use the Raspberry Pi to read QR codes and the hidden content (or even link to an audio file or video).</p><p>This could be used to create your own personalized Raspberry Pi QR code jukebox, perhaps as an aid for children to provide solutions to math problems, or even to play an audio file of you reading your kid's favorite book as they follow along page by page. The following screenshot is an example of a QR code:</p><div><img alt="Making a QR code reader" src="img/6623OT_08_014.jpg"/><div><p>You can use QR codes to make magical self-reading books</p></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec171"/>Getting ready</h2></div></div></div><p>This example requires a setup similar to the previous examples (except we won't need <code class="literal">mencoder</code> this time). We will need to install<a class="indexterm" id="id630"/> <strong>ZBar</strong>, which is a cross-platform QR code and barcode reader, and <strong>flite</strong>
<a class="indexterm" id="id631"/> (a text-to-speech utility that we used in <a class="link" href="ch06.html" title="Chapter 6. Using Python to Drive Hardware">Chapter 6</a>, <em>Using Python to Drive Hardware</em>).</p><p>To install ZBar and flite, use <code class="literal">apt-get</code> as shown in the following command:</p><div><pre class="programlisting">
<strong>sudo apt-get install zbar-tools flite</strong>
</pre></div><div><div><h3 class="title"><a id="tip22"/>Tip</h3><p>There are Python 2.7 libraries available for Zbar, but they are not currently compatible with Python 3. Zbar also includes a real-time scanner (<code class="literal">zbarcam</code>) that uses video input to detect barcodes and QR codes automatically. Unfortunately, this isn't compatible with the Raspberry Pi camera either.</p><p>This isn't a big problem for us since we can use the <code class="literal">zbarimg</code> program directly to detect the QR codes from images taken with <code class="literal">picamera</code>.</p></div></div><p>Once you have the software installed, you will need some QR codes to scan (see the <em>There's more…</em> section in <em>Generating QR codes</em>) and some suitably named MP3 files (these could be recordings of you reading the pages of a book or music tracks).</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec172"/>How to do it…</h2></div></div></div><p>Create the <a class="indexterm" id="id632"/>following <code class="literal">qrcodeGUI.py</code> script in the same directory as <code class="literal">cameraGUI.py</code>:</p><div><pre class="programlisting">#!/usr/bin/python3
#qrcodeGUI.py
import tkinter as TK
from tkinter import messagebox
import subprocess
import cameraGUI as camGUI

class SET(camGUI.SET):
  QR_SIZE=(640,480)
  READ_QR="zbarimg "

class cameraGUI(camGUI.cameraGUI):
  def run_p(cmd):
    print("RunP:"+cmd)
    proc=subprocess.Popen(cmd,shell=True,stdout=subprocess.PIPE)
    result=""
    for line in proc.stdout:
      result=str(line,"utf-8")
    return result
  def __init__(self,parent):
    super(cameraGUI,self).__init__(parent)
    self.parent=parent
    TK.Frame.__init__(self,self.parent,background="white")
    self.qrScan=TK.IntVar()
    self.qrRead=TK.IntVar()
    self.qrStream=TK.IntVar()
    self.resultQR=TK.StringVar()
    self.btnQrTxt=TK.StringVar()
    self.btnQrTxt.set("QR GO!")
    self.QRBtn=TK.Button(self.parent,textvariable=self.btnQrTxt,
                                              command=self.qrGet)
    readChk=TK.Checkbutton(self.parent,text="Read",
                               variable=self.qrRead)
    streamChk=TK.Checkbutton(self.parent,text="Stream",
                                 variable=self.qrStream)
    labelQR=TK.Label(self.parent,textvariable=self.resultQR)
    readChk.grid(row=3,column=0)
    streamChk.grid(row=3,column=1)
    self.QRBtn.grid(row=3,column=3)
    labelQR.grid(row=4,columnspan=4)
    self.scan=False
  def qrGet(self):
    if (self.scan==True):
      self.btnQrTxt.set("QR GO!")
      self.btnState("active")
      self.scan=False
    else:
      self.msg("Get QR Code")
      self.btnQrTxt.set("STOP")
      self.btnState("disabled")
      self.scan=True
      self.qrScanner()
  def qrScanner(self):
    found=False
    while self.scan==True:
      self.resultQR.set("Taking image...")
      self.update()
      cameraGUI.camCapture(SET.PREVIEW_FILE,SET.QR_SIZE)
      self.resultQR.set("Scanning for QRCode...")
      self.update()
      #check for QR code in image
      qrcode=cameraGUI.run_p(SET.READ_QR+SET.PREVIEW_FILE)
      if len(qrcode)&gt;0:
        self.msg("Got barcode: %s"%qrcode)
        qrcode=qrcode.strip("QR-Code:").strip('\n')
        self.resultQR.set(qrcode)
        self.scan=False
        found=True
      else:
        self.resultQR.set("No QRCode Found")
    if found:
      self.qrAction(qrcode)
      self.btnState("active")
      self.btnQrTxt.set("QR GO!")
    self.update()
  def qrAction(self,qrcode):
    if self.qrRead.get() == 1:
      self.msg("Read:"+qrcode)
      cameraGUI.run("sudo flite -t '"+qrcode+"'")
    if self.qrStream.get() == 1:
      self.msg("Stream:"+qrcode)
      cameraGUI.run("omxplayer '"+qrcode+"'")
    if self.qrRead.get() == 0 and self.qrStream.get() == 0:
      TK.messagebox.showinfo("QR Code",self.resultQR.get())
#End</pre></div><p>Next, create a copy <a class="indexterm" id="id633"/>of <code class="literal">cameraGUItimelapse.py</code> or <code class="literal">cameraGUIanimate.py</code> and call it <code class="literal">cameraGUIqrcode.py</code>. Again, make sure you import the new file for the GUI using the following code:</p><div><pre class="programlisting">import qrcodeGUI as GUI</pre></div><p>The GUI with QR code will look as shown in the following screenshot:</p><div><img alt="How to do it…" src="img/6623OT_08_015.jpg"/><div><p>The QR code GUI</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec173"/>How it works…</h2></div></div></div><p>The new <code class="literal">qrcodeGUI.py</code> file adds the <strong>Read</strong> and <strong>Play</strong> checkbox controls and a button control to start scanning <a class="indexterm" id="id634"/>for QR codes. When <strong>QR GO!</strong> is clicked on, <code class="literal">self.qrGet()</code> will start a cycle of taking images and checking the result with <code class="literal">zbarimg</code>. If <code class="literal">zbarimg</code> finds a QR code in the image, then the scanning will stop and the result will be displayed. Otherwise, it will continue to scan until the <strong>STOP</strong> button is clicked on. While the scanning is taking place, the text for <code class="literal">QRBtn</code> is changed to <strong>STOP</strong>.</p><p>In order to capture the output of <code class="literal">zbarimg</code>, we have to change how we run the command slightly. To do this, we define <code class="literal">run_p()</code>, which uses the following code:</p><div><pre class="programlisting">proc=subprocess.Popen(cmd,shell=True,stdout=subprocess.PIPE)</pre></div><p>This returns <code class="literal">stdout</code> as part of the <code class="literal">proc</code> object, which contains the output of the <code class="literal">zbarimg</code> program. We then extract the resulting QR code that was read from the image (if one was found).</p><p>When <strong>Read</strong> is selected, <code class="literal">flite</code> is used to read out the QR code, and if <strong>Play</strong> is selected, <code class="literal">omxplayer</code> is used to play the file (assuming the QR code contains a suitable link).</p><p>For the best results, it is recommended that you take a preview shot first to ensure that you have lined up the target QR code correctly before running the QR scanner.</p><div><img alt="How it works…" src="img/6623OT_08_016.jpg"/><div><p>Example QR code page markers (page001.mp3 and page002.mp3)</p></div></div><p>The previous QR codes contain <code class="literal">page001.mp3</code> and <code class="literal">page002.mp3</code>. These QR codes allow us to play files with the same name if placed in the same directory as our script. You can generate your own QR codes by following the instructions in the <em>There's more…</em> section in this recipe.</p><p>You could even use the <a class="indexterm" id="id635"/>book's ISBN barcode to select a different directory of MP3s based on the barcode read; the barcode allows you to reuse the same set of page-numbered QR codes for any book you like.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec174"/>There's more…</h2></div></div></div><p>To make use of the previous example, you can use the example in the next section to generate a range of QR codes to use.</p><div><div><div><div><h3 class="title"><a id="ch08lvl3sec70"/>Generating QR codes</h3></div></div></div><p>You can create <a class="indexterm" id="id636"/>QR codes using <strong>PyQRCode</strong> (see <a class="ulink" href="https://pypi.python.org/pypi/PyQRCode">https://pypi.python.org/pypi/PyQRCode</a> for more information).</p><p>You can install PyQRCode using the PIP Python manager as follows (see the <em>Getting ready</em> section of the <em>Displaying photo information in an application</em> recipe in <a class="link" href="ch03.html" title="Chapter 3. Using Python for Automation and Productivity">Chapter 3</a>, <em>Using Python for Automation and Productivity</em>):</p><div><pre class="programlisting">
<strong>sudo pip-3.2 install pyqrcode</strong>
</pre></div><p>To encode QR codes in the PNG format, PyQrCode uses <a class="indexterm" id="id637"/>PyPNG (<a class="ulink" href="https://github.com/drj11/pypng">https://github.com/drj11/pypng</a>), which can be installed with the following command:</p><div><pre class="programlisting">
<strong>sudo pip-3.2 install pypng</strong>
</pre></div><p>Use the following <code class="literal">generateQRCodes.py</code> script to generate QR codes to link to files, such as the <code class="literal">page001.mp3</code> and <code class="literal">page002.mp3</code> files that you have recorded:</p><div><pre class="programlisting">#!/usr/bin/python3
#generateQRCodes.py
import pyqrcode
valid=False
print("QR-Code generator")
while(valid==False):
    inputpages=input("How many pages?")
    try:
      PAGES=int(inputpages)
      valid=True
    except ValueError:
      print("Enter valid number.")
      pass
print("Creating QR-Codes for "+str(PAGES)+" pages:")
for i in range(PAGES):
  file="page%03d"%(i+1)
  qr_code = pyqrcode.create(file+".mp3")
  qr_code.png(file+".png")
  print("Generated QR-Code for "+file)
print("Completed")
#End</pre></div><p>Run this code using the following command:</p><div><pre class="programlisting">
<strong>python3 generateQRCodes.py</strong>
</pre></div><p>The previous code <a class="indexterm" id="id638"/>will create a set of QR codes that can be used to activate the required MP3 file and read the page out loud (or play the file that you have linked to it).</p></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec175"/>See also</h2></div></div></div><p>The <strong>Open Source Computer Vision</strong> (<strong>OpenCV</strong>)<a class="indexterm" id="id639"/> project is a very powerful <a class="indexterm" id="id640"/>image and video processing engine; more details are available at <a class="ulink" href="http://opencv.org">http://opencv.org</a>.</p><p>By combining the camera with OpenCV, the Raspberry Pi is able to recognize and interact with its environment.</p><p>An excellent example of this is Samuel Matos's RS4 OpenCV Self-balancing Robot<a class="indexterm" id="id641"/> (<a class="ulink" href="http://roboticssamy.blogspot.pt">http://roboticssamy.blogspot.pt</a>) that can seek out and respond to various custom signs; the camera module can be used to navigate and control the robot.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec64"/>Discover and experiment with OpenCV</h1></div></div></div><p>The OpenCV<a class="indexterm" id="id642"/> library is an extensive library aimed at providing real-time computer vision processing across multiple platforms. Essentially, if you want to do any serious image processing, object recognition, or analysis, then OpenCV is a perfect place to get started.</p><p>Fortunately, the latest release of OpenCV (version 3) has added support for interfacing via Python 3. Although performing real-time video processing can often require a computer with a powerful CPU, it will run on relativity limited devices such as the original Raspberry Pi (version 1). Using the more powerful Raspberry Pi 2 is highly recommended for the following recipes.</p><p>The concepts and underlying methods behind image and video processing can get rather complicated. This first recipe shall demonstrate how to work with OpenCV and most importantly provide an easy way to visualize various stages that may be used for processing images.</p><div><img alt="Discover and experiment with OpenCV" src="img/6623OT_08_017.jpg"/><div><p>When performing tests with the camera ensure you have suitable test subjects available</p></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec176"/>Getting ready</h2></div></div></div><p>The OpenCV <a class="indexterm" id="id643"/>library is written in C++ and needs to be compiled before we can use it on the Raspberry Pi. To do this, we will need to install all the required packages and then download a release from the OpenCV Git repository. OpenCV can require around 2.5 GB of space while it compiles; however, a standard installation of Raspbian from NOOBS can use around 5.5 GB. This means there may be insufficient space available on an 8 GB SD card. It may be possible to squeeze OpenCV onto a smaller SD card (by installing a custom Raspbian image or utilizing a USB flash device); however, to avoid complications, it is recommended you use at least a 16 GB SD card to compile and install OpenCV on.</p><p>Additionally, while the majority of recipes in this book can be run using SSH and X11-forwarding over a network connection, the OpenCV display window appears to function far more effectively if you are connected to a local screen (via HDMI) and controlled directly with a local input devices.</p><p>Installing OpenCV is quite a long process, but I feel the results are well worth the effort:</p><div><ol class="orderedlist arabic"><li class="listitem">Ensure that the Raspberry Pi is as up to date as possible, using the following commands:<div><pre class="programlisting">sudo apt-get update
sudo apt-get upgrade
sudo rpi-update</pre></div></li><li class="listitem">And perform a reboot to apply the changes:<div><pre class="programlisting">
<strong>sudo reboot</strong>
</pre></div></li><li class="listitem">Before we compile OpenCV, we need to install a number of dependencies to support the build process:<div><pre class="programlisting">sudo apt-get install build-essential cmake pkg-config
sudo apt-get install python2.7-dev python3-dev</pre></div></li><li class="listitem">We also need to install a number of supporting libraries and packages used by OpenCV (we may not use all of these, but they form part of the build process). These will also provide support for a wide range of image and video formats from within OpenCV:<div><pre class="programlisting">
<strong>sudo apt-get install libjpeg-dev libtiff5-dev libjasper-dev libpng12-dev</strong>
<strong>sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev</strong>
<strong>sudo apt-get install libxvidcore-dev libx264-dev</strong>
<strong>sudo apt-get install libgtk2.0-dev</strong>
</pre></div></li><li class="listitem">We can also install NumPy, which is very useful when manipulating image arrays within OpenCV, <strong>Automatically Tuned Linear Algebra Software</strong> (<strong>ATLAS</strong>), and GFortran for additional mathematic functionality:<div><pre class="programlisting">
<strong>sudo apt-get install python3-numpy</strong>
<strong>sudo apt-get install libatlas-base-dev gfortran</strong>
</pre></div></li><li class="listitem">Now that we have<a class="indexterm" id="id644"/> the supporting packages, we can download OpenCV and OpenCV Contributed (extra modules) directly from GitHub. We will also create a build location ready for the next step:<div><pre class="programlisting">
<strong>cd ~</strong>
<strong>wget -O opencv.zip https://github.com/Itseez/opencv/archive/3.0.0.zip</strong>
<strong>unzip opencv.zip</strong>
<strong>wget -O opencv_contrib.zip https://github.com/Itseez/opencv_contrib/archive/3.0.0.zip</strong>
<strong>unzip opencv_contrib.zip</strong>
<strong>cd opencv-3.0.0</strong>
<strong>mkdir build</strong>
<strong>cd build</strong>
</pre></div><div><div><h3 class="title"><a id="note78"/>Note</h3><p>
<strong>Note</strong>: You can download the latest version using the following links and selecting a specific release tag; however, you may find you require additional dependencies or modules for the package to compile successfully. Ensure you select the same release for<a class="indexterm" id="id645"/> OpenCV and the contributed modules.</p><p>
<a class="ulink" href="https://github.com/Itseez/opencv/">https://github.com/Itseez/opencv/</a>
</p><p>
<a class="ulink" href="https://github.com/Itseez/opencv_contrib/">https://github.com/Itseez/opencv_contrib/</a>
</p></div></div></li><li class="listitem">The <code class="literal">make</code> file can be created using the following commands. This takes around 10 minutes to finish (see the following screenshot):<div><pre class="programlisting">
<strong>cmake -D CMAKE_BUILD_TYPE=RELEASE \</strong>
<strong>  -D CMAKE_INSTALL_PREFIX=/usr/local \</strong>
<strong>  -D INSTALL_C_EXAMPLES=ON \</strong>
<strong>  -D INSTALL_PYTHON_EXAMPLES=ON \</strong>
<strong>  -D OPENCV_EXTRA_MODULES_PATH=~/opencv_contrib-3.0.0/modules \</strong>
<strong>  -D BUILD_EXAMPLES=ON ..</strong>
</pre></div><div><img alt="Getting ready" src="img/6623OT_08_018.jpg"/><div><p>Ensure that the Python 2.7 and Python 3 sections match this screenshot</p></div></div></li><li class="listitem">We are now ready to compile OpenCV; be warned this will take a considerable amount of time to complete. Fortunately, if you have to stop the process or if there is a problem, you can resume the <code class="literal">make</code> command, checking and skip ping any components that have already been completed. To restart the <code class="literal">make</code> from the start, use <code class="literal">make clean</code> to clear the build and start afresh.</li></ol></div><div><div><h3 class="title"><a id="note79"/>Note</h3><p>
<strong>Note</strong>: By using all four processing cores on the Raspberry Pi 2, the build time can be reduced to just over an hour. Use the <code class="literal">–j4</code> switch to enable the four cores, which will allow multiple jobs to be run during the build process.</p></div></div><p>The build can take<a class="indexterm" id="id646"/> almost three hours to complete. If you have the Raspbian desktop loaded or you are running other tasks in the background, it is recommended you log out to the command line and stop any additional jobs, otherwise the process may take even longer to complete.</p><p>For a Raspberry Pi 1, use a single-threaded <code class="literal">make</code> job with the following command:</p><div><pre class="programlisting">
<strong>make</strong>
</pre></div><p>For a Raspberry Pi 2, enable up to four simultaneous jobs by using the following command:</p><div><pre class="programlisting">
<strong>make -j4</strong>
</pre></div><div><img alt="Getting ready" src="img/6623OT_08_019.jpg"/><div><p>A completed build should look like this</p></div></div><p>With OpenCV <a class="indexterm" id="id647"/>compiled successfully, it can be installed:</p><div><pre class="programlisting">
<strong>sudo make install</strong>
</pre></div><p>Now that is all completed, we can quickly test that OpenCV is now available to use with Python 3. Run the following command to open the Python 3 terminal:</p><div><pre class="programlisting">
<strong>python3</strong>
</pre></div><p>At the Python 3 terminal, enter the following:</p><div><pre class="programlisting">
<strong>import cv2</strong>
<strong>cv2.__version__</strong>
</pre></div><p>This will display the version of the OpenCV you have just installed!</p><div><div><h3 class="title"><a id="note80"/>Note</h3><p>
<strong>Note</strong>: The OpenCV library is updated regularly, which can cause problems with the build process. Therefore, if you have issues, the Py Image Search website (<a class="ulink" href="http://www.pyimagesearch.com">http://www.pyimagesearch.com</a>) is an excellent resource that contains the latest guides and video tutorials for installing OpenCV on the Raspberry Pi.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec177"/>How to do it…</h2></div></div></div><p>For our first OpenCV<a class="indexterm" id="id648"/> test, we will use it to display a captured image. Create the following <code class="literal">openimage.py</code> file:</p><div><pre class="programlisting">#!/usr/bin/python3
#openimage.py
import cv2

# Load a color image in grayscale
img = cv2.imread('testimage.jpg',0)
cv2.imshow('Frame',img)
cv2.waitKey(0)
cv2.destroyAllWindows()</pre></div><p>Ensure before you run the script, that you capture an image to display using the following command:</p><div><pre class="programlisting">
<strong>raspistill -o testimage.jpg -w 640 -h 480</strong>
</pre></div><p>Run the script with the following command:</p><div><pre class="programlisting">
<strong>python3 openimage.py</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec178"/>How it works…</h2></div></div></div><p>The simple test program starts<a class="indexterm" id="id649"/> by importing OpenCV (<code class="literal">cv2</code>) and loading the image using <code class="literal">cv2.imread()</code>. We then use <code class="literal">cv2.imshow()</code> to display our image (<code class="literal">img</code>) in an image box with the title <code class="literal">'Frame'</code>. We then wait for a press of any key (<code class="literal">cv2.waitKey(0)</code>) before closing the display window.</p><div><img alt="How it works…" src="img/6623OT_08_020.jpg"/><div><p>The image is displayed in a standard frame as a grayscale image</p></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec65"/>Color detection with OpenCV</h1></div></div></div><p>We shall begin experimenting with OpenCV<a class="indexterm" id="id650"/> by performing some basic operations on live image<a class="indexterm" id="id651"/> data. In this recipe, we shall perform some basic image processing to allow detection of different colored objects and track their location on screen.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec179"/>Getting ready</h2></div></div></div><p>In addition to the setup from the previous recipe, you will need a suitable colored object to track. For example, a small colored ball, a suitable colored mug, or a pencil with a square of colored paper taped to it is ideal. The example should allow you to detect the location (indicated by a color spot) of blue, green, red, magenta (pink) or yellow objects.</p><div><img alt="Getting ready" src="img/6623OT_08_021.jpg"/><div><p>We can use OpenCV to detect colored objects in an image</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec180"/>How to do it…</h2></div></div></div><p>Create the <a class="indexterm" id="id652"/>following<a class="indexterm" id="id653"/> <code class="literal">opencv_display.py</code> script:</p><div><pre class="programlisting">#!/usr/bin/python3
#opencv_display.py
from picamera.array import PiRGBArray
from picamera import PiCamera
import time
import cv2

import opencv_color_detect as PROCESS  

def show_images(images,text,MODE):          
  # show the frame
  cv2.putText(images[MODE], "%s:%s" %(MODE,text[MODE]), (10,20),
              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2)
  cv2.imshow("Frame", images[MODE])

def begin_capture():
  # initialize the camera and grab a reference to the raw camera capture
  camera = PiCamera()
  camera.resolution = (640, 480)
  camera.framerate = 50
  camera.hflip = True

  rawCapture = PiRGBArray(camera, size=(640, 480))
 
  # allow the camera to warmup
  time.sleep(0.1)
  print("Starting camera...")
  MODE=0
  
  # capture frames from the camera
  for frame in camera.capture_continuous(rawCapture, format="bgr", use_video_port=True):
    # capture any key presses
    key = cv2.waitKey(1) &amp; 0xFF
 
	# grab the raw NumPy array representing the image
    images, text = PROCESS.process_image(frame.array,key)

    #Change display mode or quit
    if key == ord("m"):
      MODE=MODE%len(images)
    elif key == ord("q"):
      print("Quit")
      break

  #Display the output images
    show_images(images,text,MODE)

  # clear the stream in preparation for the next frame
    rawCapture.truncate(0)

begin_capture()
#End</pre></div><p>Create the following <code class="literal">opencv_color_detect.py</code> script <a class="indexterm" id="id654"/>in the same<a class="indexterm" id="id655"/> directory as <code class="literal">opencv_display.py</code>:</p><div><pre class="programlisting">#!/usr/bin/python3
#opencv_color_detect.py
import cv2
import numpy as np

BLUR=(5,5)
threshold=0
#Set the BGR color thresholds
THRESH_TXT=["Blue","Green","Red","Magenta","Yellow"]
THRESH_LOW=[[80,40,0],[40,80,0],[40,00,80],[80,0,80],[0,80,80]]
THRESH_HI=[[220,100,80],[100,220,80],[100,80,220],[220,80,220],[80,220,220]]

def process_image(raw_image,control):
  global threshold
  text=[]
  images=[]

  #Switch color threshold
  if control == ord("c"):
    threshold=(threshold+1)%len(THRESH_LOW)
  #Display contour and hierarchy details
  elif control == ord("i"):
    print("Contour: %s"%contours)
    print("Hierarchy: %s"%hierarchy)

  #Keep a copy of the raw image
  text.append("Raw Image %s"%THRESH_TXT[threshold])
  images.append(raw_image)
  
  #Blur the raw image
  text.append("with Blur...%s"%THRESH_TXT[threshold])
  images.append(cv2.blur(raw_image, BLUR))

  #Set the color thresholds
  lower = np.array(THRESH_LOW[threshold],dtype="uint8")
  upper = np.array(THRESH_HI[threshold], dtype="uint8")

  text.append("with Threshold...%s"%THRESH_TXT[threshold])
  images.append(cv2.inRange(images[-1], lower, upper))

  #Find contours in the threshold image
  text.append("with Contours...%s"%THRESH_TXT[threshold])
  images.append(images[-1].copy())
  image, contours, hierarchy = cv2.findContours(images[-1],
                                                cv2.RETR_LIST,
                                                cv2.CHAIN_APPROX_SIMPLE)

  #Display contour and hierarchy details
  if control == ord("i"):
    print("Contour: %s"%contours)
    print("Hierarchy: %s"%hierarchy)

  #Find the contour with maximum area and store it as best_cnt
  max_area = 0
  best_cnt = 1
  for cnt in contours:
    area = cv2.contourArea(cnt)
    if area &gt; max_area:
      max_area = area
      best_cnt = cnt

  #Find the centroid of the best_cnt and draw a circle there
  M = cv2.moments(best_cnt)
  cx,cy = int(M['m10']/M['m00']), int(M['m01']/M['m00'])
        
  if max_area&gt;0:
    cv2.circle(raw_image,(cx,cy),8,(THRESH_HI[threshold]),-1)
    cv2.circle(raw_image,(cx,cy),4,(THRESH_LOW[threshold]),-1)

  return(images,text)
#End</pre></div><p>To run the <a class="indexterm" id="id656"/>example, use the following command:</p><div><pre class="programlisting">
<strong>python3 opencv_display.py</strong>
</pre></div><p>Use the <em>M</em> key to cycle<a class="indexterm" id="id657"/> through the available display modes, the <em>C</em> key to change the particular color we want to detect (blue, green, red, magenta or yellow), and the <em>I</em> key to display information about the detected contours and hierarchy data.</p><div><img alt="How to do it…" src="img/6623OT_08_022.jpg"/><div><p>The raw image (top left) is processed with Blur (top right), Threshold (bottom left) and Contour (bottom right) operations.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec181"/>How it works…</h2></div></div></div><p>The first <a class="indexterm" id="id658"/>script (<code class="literal">opencv_display.py</code>) provides us with a common<a class="indexterm" id="id659"/> base for running our OpenCV examples from. The script consists of two functions, <code class="literal">begin_capture()</code> and <code class="literal">show_images()</code>.</p><p>The <code class="literal">begin_capture()</code> function sets up the PiCamera to take continuous frames (at 50 fps and a resolution of 640x480), converting them into a raw image format suitable for OpenCV to process. We use relatively low resolution images here since we do not need a lot of detail to perform the kind of processing we are aiming for. In fact, the smaller the images the less memory they use and the less intensive the processing that we need to perform is.</p><p>By using the <code class="literal">camera.capture_continuous()</code> function of the PiCamera library, we will get an image frame ready for us to process. We shall pass each new frame to the <code class="literal">process_image()</code> function, which will be provided by the <code class="literal">opencv_color_detect.py</code> file along with any captured key presses (to allow the user a little control). The <code class="literal">process_image()</code> function (which we will go through in further detail later on) returns two arrays (images and text).</p><p>We pass both the<a class="indexterm" id="id660"/> images and text arrays to the <code class="literal">show_images()</code> function, along <a class="indexterm" id="id661"/>with the selected <code class="literal">MODE</code> (which is controlled by the user pressing the <code class="literal">M</code> key to cycle through them). Within the <code class="literal">show_images()</code> function we use the text for the given MODE and use <code class="literal">putText()</code> to add it to the image we are displaying (again, whichever image corresponds to the selected <code class="literal">MODE</code>). Finally, we display the image in a separate window using <code class="literal">cv2.imshow()</code>.</p><div><img alt="How it works…" src="img/6623OT_08_023.jpg"/><div><p>The raw image is displayed by the script (including the tracking marker)</p></div></div><p>All the real fun is contained within the <code class="literal">opencv_color_detect.py</code> script, which performs all the required image processing to our raw video stream. The aim is to simplify the source image and then identify the middle of any area that matches our required color.</p><div><div><h3 class="title"><a id="tip23"/>Tip</h3><p>
<strong>Note</strong>: The script purposely retains each stage of the processing so you can see the effect of each step on the previous image yourself. This is by far the best way to understand how we can go from a standard video image to something that the computer is able to comprehend. To achieve this, we use an array to collect the images as we produce them (using <code class="literal">images.append()</code> to add each new image and we use a <em>Pythonic</em> way to refer to the last item in an array, the <code class="literal">[-1]</code> notation. In other programming languages this would produce an error, but with Python it is perfectly acceptable to use negative numbers to count backwards from the end of an array (so it follows -1 is the first item from the end of the array, and -2 would be the second from the end).</p></div></div><p>The <code class="literal">process_image()</code> image function shall produce four different images (which we provide<a class="indexterm" id="id662"/> references to in our <code class="literal">images</code> array). In the first image<a class="indexterm" id="id663"/> we simply keep a copy of our raw image (displayed as <code class="literal">0: Raw Image [Color]</code>). Since this is a full color untouched image, this shall be the image with which we shall show the location of the detected object (this is added at the end of function).</p><p>The next image we produce is a blurred version of the original (displayed as <code class="literal">1: with Blur…[Color]</code>) by using the <code class="literal">cv2.blur()</code> function with the <code class="literal">BLUR</code> tuple to specify the amount on the (<em>x,y</em>) axes. By slightly blurring the image, we hope to eliminate any unnecessary detail or erroneous noise in the image; this is ideal since we are only interested in large blocks of color, so fine detail is irrelevant.</p><p>The third image (displayed as <code class="literal">2:with Threshold…[color]</code>) is the result of applying the given upper and lower threshold using the <code class="literal">cv2.inRange()</code> function. This produces a simple black and white image, where any parts of the image that are between the upper and lower color thresholds are displayed in white. Hopefully, you will be able to clearly see your test object as you move it in front of the camera as a large white patch. You can check this image to ensure that your background is not confused with your target object. If the threshold image is mostly white then try a different color target, moving the camera to a different location, or adjusting the colors used in the threshold arrays (<code class="literal">THRESH_LOW/HI</code>).</p><div><div><h3 class="title"><a id="note81"/>Note</h3><p>Note: The color mapping<a class="indexterm" id="id664"/> used in this example is OpenCV's<a class="indexterm" id="id665"/> <strong>BGR</strong> format. This means that the pixel colors are stored as an array of three integers, for Blue, Green, and Red. The color thresholds are therefore specified in this format; this is contrary to the more typical RGB color format used for example in HTML web colors.</p></div></div><p>The last image provides the final piece of the puzzle; displayed as <code class="literal">3:with Contours...[color]</code>, it shows the result of the <code class="literal">cv2.findContours()</code> function. OpenCV will calculate the contours in the image. This will discover all the edges of the shapes that were in the threshold image and return them in a list (contours). Each individual contour is an array of the (<em>x,y</em>) coordinates of the boundary points of each shape in the image.</p><div><div><h3 class="title"><a id="tip24"/>Tip</h3><p>
<strong>Note</strong>: The contours are applied directly to the supplied image by the <code class="literal">cv2.findContours()</code> function, which is why we make a copy of the threshold image (using <code class="literal">images[-1].copy()</code>) so we can see both steps in our process. We also use <code class="literal">cv2.CHAIN_APPROX_SIMPLE</code>, which attempts to simplify the stored coordinates so any points that aren't needed are skipped (for example, any along a straight line can be removed as long as we have the start and end points). Alternatively, we could use <code class="literal">cv2.CHAIN_APPROX_NONE</code>, which keeps all the points.</p></div></div><p>We can use the<a class="indexterm" id="id666"/> list of contours to determine the area of each; in our <a class="indexterm" id="id667"/>case, we are most interested in the largest one (which will hopefully contain the object we are tracking as the largest area of the image that has colors within the given thresholds). We shall use <code class="literal">cv2.contourArea()</code> on each contour discovered to calculate the area and keep whichever one ends up being the largest.</p><p>Finally, we can list the moments, which are a list of numbers that provide a mathematical approximation of the shape. The moments provide us with a simple calculation to obtain the centroid of the shape. The centroid is like the <em>center of mass</em> of the shape; for example, if it was made out of a flat solid piece of material, it would be the point at which you could balance it on the end of your finger.</p><p>
<em>cx, cy = M['m10'] / M['m00'], M['m01'] / M['m00'])</em>
</p><p>We display a small marker (consisting of the upper and lower threshold colors) using the calculated coordinates to indicate the detected location of the object.</p><div><img alt="How it works…" src="img/6623OT_08_024.jpg"/><div><p>The location of the object is marked with a colored spot as it tracked within the image</p></div></div><p>For additional information about OpenCV's contours and <a class="indexterm" id="id668"/>moments, see OpenCV-Python Tutorials (<a class="ulink" href="http://goo.gl/eP9Cn3">http://goo.gl/eP9Cn3</a>).</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec182"/>There's more…</h2></div></div></div><p>This recipe allows us to track an object by detecting the required colors within the camera frame, which will provide a relative <em>x</em> and <em>y</em> position of the object.</p><p>We can mount the Raspberry Pi camera on a movable platform, for example a rover/bug robot platform (like the ones described in <a class="link" href="ch09.html" title="Chapter 9. Building Robots">Chapter 9</a>) or by using a servo-controlled tilt and pan camera mount (as shown in the following image).</p><div><img alt="There's more…" src="img/6623OT_08_025.jpg"/><div><p>The Raspberry Pi camera can be controlled using a servo mount</p></div></div><p>By combining the camera input and the object coordinates, we can make the Raspberry Pi track the object wherever it goes. If we detect that the object has moved to one side of the camera frame, we can use the Raspberry Pi hardware control to re-center the object within the camera frame (either by steering the robot or tilting and panning the camera).</p><div><img alt="There's more…" src="img/6623OT_08_026.jpg"/><div><p>The object has been detected in the top-right of the screen, so turn the camera to the right and up to follow the object</p></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec66"/>Performing motion tracking with OpenCV</h1></div></div></div><p>While it is <a class="indexterm" id="id669"/>useful to be able to track objects of a specific<a class="indexterm" id="id670"/> color, sometimes we are just interested in the actual motion taking place. This is particularly true when the objects we wish to track could blend in with the background.</p><div><div><h3 class="title"><a id="note82"/>Note</h3><p>
<strong>Note</strong>: Security cameras often use IR detectors<a class="indexterm" id="id671"/> to act as triggers; however, these rely upon detecting a change in detected heat across the sensor. This means they will not work if the object does not give off additional heat relative to the background and they will not track the direction of the motion.</p><p>
<a class="ulink" href="https://learn.adafruit.com/pir-passive-infrared-proximity-motion-sensor/how-pirs-work">https://learn.adafruit.com/pir-passive-infrared-proximity-motion-sensor/how-pirs-work</a>
</p></div></div><p>The following recipe will demonstrate how OpenCV can be used to detect motion and also provide a record of where the object has been over a period of time.</p><div><img alt="Performing motion tracking with OpenCV" src="img/6623OT_08_027.jpg"/><div><p>The motion of an object within the frame is traced on screen, allowing the pattern of movement to be recorded and studied</p></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec183"/>Getting ready</h2></div></div></div><p>The following <a class="indexterm" id="id672"/>script will allow us to track an object and <a class="indexterm" id="id673"/>display its path on the screen. For this task, I have volunteered our family tortoise; however, any object that moves can be used.</p><div><img alt="Getting ready" src="img/6623OT_08_028.jpg"/><div><p>Our tortoise made an excellent test subject; it was very interesting to see where she wandered during the day</p></div></div><p>The setup in this<a class="indexterm" id="id674"/> case works particularly well for <a class="indexterm" id="id675"/>the following reasons. Firstly, since the tortoise is of a similar color to the background we can't use the previous method of color detection (unless we stuck some markers on her). Secondly, the tortoise house has a useful shelf above it, allowing the Raspberry Pi and camera to be mounted directly above it. Finally, the enclosure is artificially lit, so other than the movement of the tortoise, the image observed should remain relatively constant during our testing. When performing this task with external factors such as natural light, you may find that they interfere with the moment detection (making it difficult to determine what is changes due to movement compared to changes in the environment – see the <em>There's more...</em> section for tips on overcoming this).</p><p>The rest of the setup will be the same as the previous OpenCV recipe (see <em>Color detection with OpenCV</em>).</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec184"/>How to do it…</h2></div></div></div><p>Create the following script, called <code class="literal">opencv_detect_motion.py</code>:</p><div><pre class="programlisting">#!/usr/bin/python3
#opencv_motion_detect.py
import cv2
import numpy as np

GAUSSIAN=(21,21)

imageBG=None
gray=True

movement=[]
AVG=2
avgX=0
avgY=0
count=0

def process_image(raw_image,control):
  global imageBG
  global count,avgX,avgY,movement,gray

  text=[]
  images=[]
  reset=False

  #Toggle Gray and reset background
  if control == ord("g"):
    if gray:
      gray=not gray
    reset=True
    print("Toggle Gray")
  #Reset the background image
  elif control == ord("r"):
    reset=True
    
  #Clear movement record and reset background
  if reset:
    print("Reset Background")
    imageBG=None
    movement=[]

  #Keep a copy of the raw image
  text.append("Raw Image")
  images.append(raw_image)

  if gray:
    raw_image=cv2.cvtColor(raw_image,cv2.COLOR_BGR2GRAY)

  #Blur the raw image
  text.append("with Gaussian Blur...")
  images.append(cv2.GaussianBlur(raw_image, GAUSSIAN))

  #Initialise background
  if imageBG is None:
    imageBG=images[-1]

  text.append("with image delta...")  
  images.append(cv2.absdiff(imageBG,images[-1]))

  text.append("with threshold mask...")                
  images.append(cv2.threshold(images[-1], 25, 255,
                             cv2.THRESH_BINARY)[1])

  text.append("with dilation...")                
  images.append(cv2.dilate(images[-1],None, iterations=3))

  #Find contours
  if not gray:
    #Require gray image to find contours
    text.append("with dilation gray...")
    images.append(cv2.cvtColor(images[-1],cv2.COLOR_BGR2GRAY))
  text.append("with contours...")
  images.append(images[-1].copy())
  aimage, contours, hierarchy = cv2.findContours(images[-1],
                                                 cv2.RETR_LIST,
                                                 cv2.CHAIN_APPROX_SIMPLE)

  #Display contour and hierarchy details
  if control == ord("i"):
    print("Contour: %s"%contours)
    print("Hierarchy: %s"%hierarchy)

  #Determine the area of each of the contours
  largest_area=0
  found_contour=None
  for cnt in contours:
    area = cv2.contourArea(cnt)
    #Find which one is largest
    if area &gt; largest_area:
      largest_area=area
      found_contour=cnt


  if found_contour != None:
    #Find the centre of the contour
    M=cv2.moments(found_contour)
    cx,cy=int(M['m10']/M['m00']),int(M['m01']/M['m00'])
    #Calculate the average
    if count&lt;AVG:
      avgX=(avgX+cx)/2
      avgY=(avgY+cy)/2
      count=count+1
    else:
      movement.append((int(avgX),int(avgY)))
      avgX=cx
      avgY=cy
      count=0

  #Display
  if found_contour != None:
    cv2.circle(images[0],(cx,cy),10,(255,255,255),-1)
  if len(movement) &gt; 1:
    for i,j in enumerate(movement):
      if i&gt;1:
        cv2.line(images[0],movement[i-1],movement[i],(255,255,255))
    
  return(images,text)  
#End</pre></div><p>Next, find the following line in the <code class="literal">opencv_display.py</code> file (from the previous recipe):</p><div><pre class="programlisting">
<strong>import opencv_color_detect as PROCESS  </strong>
</pre></div><p>to the following:</p><div><pre class="programlisting">
<strong>import opencv_motion_detect as PROCESS</strong>
</pre></div><p>To run the example, use the following command:</p><div><pre class="programlisting">
<strong>python3 opencv_display.py</strong>
</pre></div><p>Use the <em>M</em> key to cycle through the available display modes, the <em>G</em> key to toggle gray scale mode, the <em>I</em> key to display information about the detected contours and hierarchy data, and the <em>B</em> key to reset the image we set as the background.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec185"/>How it works…</h2></div></div></div><p>The principle <a class="indexterm" id="id676"/>behind this motion detection method <a class="indexterm" id="id677"/>is elegantly simple. First, we take an initial image as our golden image (where no moment is taking place); we shall treat this as our static background. Now we simply compare any subsequent image to this original background image. If there is any significant difference from the first image, we assume the difference is due to movement. Once we have detected motion, we will then generate a trace of the movement over time and display it on the frame.</p><div><img alt="How it works…" src="img/6623OT_08_029.jpg"/><div><p>The golden image (right) is a gray-scale version of the raw image (left) with a Gaussian blur applied</p></div></div><p>When the script is run we ensure that the reset flag is set to <code class="literal">True</code>, which ensures we use the first image captured as the golden image (also, if the user presses <em>R</em> we allow the golden image to be refreshed with a new image). We also detect if the user presses <em>G</em>, which will switch between processing the image in grayscale or in color. The default is grayscale since this is more efficient to process and the colors do not matter when detecting motion (but it is interesting to see the result of the same processing when the images are still in color too).</p><p>Just like the <a class="indexterm" id="id678"/>previous recipe, we will keep a copy of each<a class="indexterm" id="id679"/> image to allow better understanding of each stage in the process. The first image that is displayed is <code class="literal">0:Raw Image</code>, which is a direct copy of the camera image (we will overlay the detected motion on this image).</p><p>In the next image, <code class="literal">1:with Gaussian Blur…</code>, we use <code class="literal">cv2.GaussianBlur(raw_image, GAUSSIAN, 0)</code>, providing a smoothed out version of the original (hopefully removing Gaussian noise from the image). Like the <code class="literal">blur</code> function, we provide the image to be processed and the <em>x,y</em> magnitudes (which for the Gaussian algorithm have to be positive and odd).</p><div><div><h3 class="title"><a id="note83"/>Note</h3><p>Note: You can compare the Gaussian Blur with the standard blur method by inserting the following code (just before the Gaussian Blur section) and cycling between the modes:</p><div><pre class="programlisting">  text.append("with Low Blur...")
  images.append(cv2.blur(raw_image, (5,5))
  text.append("with High Blur...")
  images.append(cv2.blur(raw_image, (30,30))</pre></div><p>The background image is set using this blurred image (if it has not been set previously or it has been reset).</p></div></div><p>We use <code class="literal">cv2.absdiff(imageBG,images[-1])</code> to determine what differences there are between the <code class="literal">imageBG</code> (the original background image) and the latest Gaussian blurred image to provide <code class="literal">2:with image delta...</code>.</p><div><img alt="How it works…" src="img/6623OT_08_030.jpg"/><div><p>This image (inverted here to make it clearer) shows the difference from the golden image. The tortoise has moved near the middle of the image</p></div></div><p>Next, we apply a <a class="indexterm" id="id680"/>binary threshold mask (displayed as <code class="literal">3:with threshold mask…</code>), which will set any pixel between the upper (255) and <a class="indexterm" id="id681"/>lower (25) threshold to 255, resulting in a black and white image displaying the main areas of movement.</p><div><img alt="How it works…" src="img/6623OT_08_031.jpg"/><div><p>A threshold filter is applied to the delta image, highlighting the largest changes in the image</p></div></div><p>Now, we dilate the threshold image (displayed as <code class="literal">4:with dilation…</code>) using <code class="literal">cv2.dilate(images[-1], None, iterations=3)</code>. The <code class="literal">dilate</code> operation works by growing the white section of the image by a pixel in each iteration. By using <code class="literal">None</code> as the second parameter, we are setting the kernel to use a default value (alternatively, an array of 0s and 1s can be used to fully control how the dilation is applied).</p><div><img alt="How it works…" src="img/6623OT_08_032.jpg"/><div><p>The dilated image grows the spots of detected movement</p></div></div><p>We use the <code class="literal">cv2.contours()</code> function, like we did in the previous recipe, to detect the outline of<a class="indexterm" id="id682"/> the detected shapes; the result is <a class="indexterm" id="id683"/>displayed as <code class="literal">5:with contours…</code>. We must convert the image to grayscale, if it isn't already, since this function works best with a binary image (an image that is black and white).</p><div><img alt="How it works…" src="img/6623OT_08_033.jpg"/><div><p>The area of the contours are calculated and used to determine the location of the main area of movement</p></div></div><p>As before, we calculate the area of each contour and discover which is the largest by using <code class="literal">cv2.contourAera()</code>. Then we determine the coordinates of the middle of the selected contour by finding the moments (via <code class="literal">cv2.moments()</code>). Finally, we add these coordinates to the moment array so we can display a trace of the detected movement on our original image.</p><p>Additionally, to trace relatively slow-moving objects we can also average several of the detected coordinates to provide a smoother trace of movement.</p><p>As mentioned<a class="indexterm" id="id684"/> at the start, external factors can <a class="indexterm" id="id685"/>interfere with this simple algorithm where even subtle changes in the environment can cause errors in the movement detection. Fortunately, techniques such as applying long term averaging to the background image (rather than a single onetime snapshot) will cause any gradual changes, such as lighting, to be incorporated into the background image.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec186"/>There's more…</h2></div></div></div><p>Although we have only briefly touched on a small aspect of the OpenCV library, it should be clear that it is perfect for use with the Raspberry Pi. We have seen OpenCV provides some very powerful processing with relative ease and the Raspberry Pi (particularly the Raspberry Pi model 2) is an ideal platform on which to run it.</p><p>As you can imagine, it simply isn't practical to cover everything OpenCV is able to do within a few examples, but I hope it has at least whetted your appetite (and provided you with a ready-to-go setup from which you can experiment with and create your own projects).</p><p>Fortunately, not only are there lots of tutorials and guides available online, but there are also several books that cover OpenCV in great detail; in particular, the following Packt books are recommended:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><em>OpenCV Computer Vision with Python</em> by <em>Joseph Howse</em></li><li class="listitem" style="list-style-type: disc"><em>Raspberry Pi Computer Vision Programming</em> by <em>Ashwin Pajankar</em></li></ul></div><p>In the last two examples, I've attempted to keep the code as brief as possible while ensuring it is easy to observe the inner workings behind the recipe. It should be very easy to adapt them or add your own simply by importing different modules with your own <code class="literal">process_images()</code> function in.</p><p>For more ideas and projects, there is an excellent list on the following site:</p><p>
<a class="ulink" href="http://www.intorobotics.com/20-hand-picked-raspberry-pi-tutorials-in-computer-vision/">http://www.intorobotics.com/20-hand-picked-raspberry-pi-tutorials-in-computer-vision/</a>
</p></div></div></body></html>