["```py\n$ pip install pyprof2calltree\n\n```", "```py\n$ sudo apt-get install kcachegrind\n\n```", "```py\n$ brew install qcachegrind\n\n```", "```py\n$ pyprof2calltree -o [output-file-name] -i input-file.prof\n\n```", "```py\nfrom xml.etree import ElementTree\nfrom cProfile import Profile\nimport pstats\nxml_content = '<a>\\n' + '\\t<b/><c><d>text</d></c>\\n' * 100 + '</a>'\nprofiler = Profile()\nprofiler.runctx(\n\"ElementTree.fromstring(xml_content)\",\nlocals(), globals())\n\nfrom pyprof2calltree import convert, visualize\nstats = pstats.Stats(profiler)\nvisualize(stats)      # run kcachegrind\n```", "```py\nimport cProfile\nimport pstats\nimport sys\n\nfrom tweetStats import build_twit_stats\n\nprofiler = cProfile.Profile()\nprofiler.enable()\n\nbuild_twit_stats()\nprofiler.create_stats()\nstats = pstats.Stats(profiler)\nstats.strip_dirs().sort_stats('cumulative').dump_stats('tweet-stats.prof') #saves the stats into a file called tweet-stats.prof, instead of printing them into stdout\n\n```", "```py\n$pyprof2calltree -i tweet-stats.prof -k\n\n```", "```py\nprofiler = cProfile.Profile()\nprofiler.enable()\n__start__()\nprofiler.create_stats()\nstats = pstats.Stats(profiler)\nstats.strip_dirs().sort_stats('cumulative').dump_stats('inverted-index-stats.prof')\n```", "```py\n$ pyprof2calltree -i inverted-index-stats.prof -k\n\n```", "```py\ndef getWords(content, filename, wordIndexDict):\n  currentOffset = 0\n  for line in content:\n    localWords = line.split()\n    for (idx, word) in enumerate(localWords):\n      currentOffset = getOffsetUpToWord(localWords, idx) + currentOffset \n      wordIndexDict[word].append([filename, currentOffset])\n  return wordIndexDict\n```", "```py\ndef getWords(content, filename, wordIndexDict):\n  currentOffset = 0\n  prevLineLength = 0\n  for lineIndex, line in enumerate(content):\n    lastOffsetUptoWord = 0\n    localWords = line.split()\n\n    if lineIndex > 0:\n      prevLineLength += len(content[lineIndex - 1]) + 1\n    for idx, word in enumerate(localWords):\n      if idx > 0:\n        lastOffsetUptoWord += len(localWords[idx-1])\n      currentOffset = lastOffsetUptoWord + idx +  1 + prevLineLength\n\n      wordIndexDict[word].append([filename, currentOffset])\n```", "```py\n$ apt-get install python-profiler python-wxgtk2.8 python-setuptools\n\n```", "```py\n$ pip install  SquareMap RunSnakeRun\n\n```", "```py\n$ python -m cProfile -o inverted-index-cprof.prof inverted-index.py\n$ runsnake inverted-index-cprof.prof\n\n```", "```py\ndef lowest_common_multiplier(arg1, arg2):\n    i = max(arg1, arg2)\n    while i < (arg1 * arg2):\n        if i % min(arg1,arg2) == 0:\n            return i\n        i += max(arg1,arg2)\n    return(arg1 * arg2)\n\nprint lowest_common_multiplier(41391237, 2830338)\n```", "```py\n$ python -m cProfile -o lcm.prof lcm.py\n\n```", "```py\n$ runsnake lcm.prof\n\n```", "```py\ndef lowest_common_multiplier(arg1, arg2):\n    i = max(arg1, arg2)\n    _max = i\n    _min = min(arg1,arg2)\n    while i < (arg1 * arg2):\n        if i % _min == 0:\n            return i\n        i += _max\n    return(arg1 * arg2)\n\nprint lowest_common_multiplier(41391237, 2830338)\n```", "```py\nimport re\nimport sys\n\n#Turns a list of entries from the index file into a dictionary indexed\n#by words\ndef list2dict(l):\n  retDict = {}\n  for item in l:\n    lineParts = item.split(',')\n    word = lineParts.pop(0)\n    data = ','.join(lineParts)\n    indexDataParts = re.findall('\\(([a-zA-Z0-9\\./, ]{2,})\\)' ,data)\n    retDict[word] = indexDataParts\n  return retDict\n\n#Load the index's content into memory and parse itdef loadIndex():\n  indexFilename = \"./index-file.txt\"\n  with open(indexFilename, 'r') as fh: \n    indexLines = []\n    for line in fh:\n      indexLines.append(line)\n    index = list2dict(indexLines)\n\n    return index\n\n#Reads the content of a file, takes care of fixing encoding issues with utf8 and removes unwanted characters (the ones we didn't want when generating the index)\ndef readFileContent(filepath):\n    with open(filepath, 'r') as f:\n    return [x.replace(\",\", \"\").replace(\".\",\"\").replace(\"\\t\",\"\").replace(\"\\r\",\"\").replace(\"|\",\"\").strip(\" \") for x in f.read().decode(\"utf-8-sig\").encode(\"utf-8\").split( '\\n' )]\ndef findMatch(results):\n  matches = []\n  for r in results:\n    parts = r.split(',')\n    filepath = parts.pop(0)\n    fileContent = ' '.join(readFileContent(filepath))\n    for offset in parts:\n      ioffset = int(offset)\n      if ioffset > 0:\n        ioffset -= 1\n      matchLine = fileContent[ioffset:(ioffset + 100)]\n      matches.append(matchLine)\n  return matches\n\n#Search for the word inside the index\ndef searchWord(w):\n  index = None\n  index = loadIndex()\n  result = index.get(w)\n  if result:\n    return findMatch(result)\n  else:\n      return []\n\n#Let the user define the search word...\nsearchKey = sys.argv[1] if len(sys.argv) > 1 else None\nif searchKey is None: #if there is none, we output a usage message\n print \"Usage: python search.py <search word>\"\nelse: #otherwise, we search\n  results = searchWord(searchKey)\n  if not results:\n      print \"No results found for '%s'\" % (searchKey)\n  else:\n      for r in results:\n      print r\n```", "```py\n$ python -m cProfile -o search.prof search.py John\n\n```", "```py\ndef loadIndex():\n  indexFilename = \"./index-file.txt\"\n  with open(indexFilename, 'r') as fh:\n    #instead of looping through every line to append it into an array, we use the readlines method which does that already\n    indexLines = fh.readlines()\n    index = list2dict(indexLines)\n    return index\n```", "```py\n  retDict = {}\n  for item in l:\n    lineParts = item.split(',(')\n    word = lineParts[0]\n    ndexDataParts = [x.replace(\")\",\"\") for x in lineParts[1:]]\n  retDict[word] = indexDataParts\n  return retDict\n```", "```py\nimport sys\n\n#Turns a list of entries from the index file into a dictionary indexed\n#by words\ndef list2dict(l):\n  retDict = {}\n  for item in l:\n    lineParts = item.split(',(')\n  word = lineParts[0]\n    indexDataParts = [x.replace(\")\",\"\") for x in lineParts[1:]]\n  retDict[word] = indexDataParts\n  return retDict\n\n#Load the index's content into memory and parse it\ndef loadIndex():\n  indexFilename = \"./index-file.txt\"\n  with open(indexFilename, 'r') as fh:\n    #instead of looping through every line to append it into an array, we use the readlines method which does that already\n    indexLines = fh.readlines()\n    index = list2dict(indexLines)\n    return index\n\n#Reads the content of a file, takes care of fixing encoding issues with utf8 and removes unwanted characters (the ones we didn't want when generating the index)#\ndef readFileContent(filepath):\n    with open(filepath, 'r') as f:\n    return [x.replace(\",\", \"\").replace(\".\",\"\").replace(\"\\t\",\"\").replace(\"\\r\",\"\").replace(\"|\",\"\").strip(\" \") for x in f.read().decode(\"utf-8-sig\").encode(\"utf-8\").split( '\\n' )]\n\ndef findMatch(results):\n  matches = []\n  for r in results:\n    parts = r.split(',')\n\n    filepath = parts[0]\n    del parts[0]\n    fileContent = ' '.join(readFileContent(filepath))\n    for offset in parts:\n      ioffset = int(offset)\n      if ioffset > 0:\n        ioffset -= 1\n      matchLine = fileContent[ioffset:(ioffset + 100)]\n      matches.append(matchLine)\n  return matches\n\n#Search for the word inside the index\ndef searchWord(w):\n  index = None\n  index = loadIndex()\n  result = index.get(w)\n  if result:\n    return findMatch(result)\n  else:\n    return []\n\n#Let the user define the search word...\nsearchKey = sys.argv[1] if len(sys.argv) > 1 else None\n\nif searchKey is None: #if there is none, we output a usage message\n  print \"Usage: python search.py <search word>\"\nelse: #otherwise, we search\n  results = searchWord(searchKey)\n  if not results:\n    print \"No results found for '%s'\" % (searchKey)\n  else:\n    for r in results:\n    print r\n```"]