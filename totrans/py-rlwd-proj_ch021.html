<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<meta charset="utf-8"/>
<meta content="pandoc" name="generator"/>
<title>ch021.xhtml</title>

<!-- kobo-style -->
<style id="koboSpanStyle" type="text/css" xmlns="http://www.w3.org/1999/xhtml">.koboSpan { -webkit-text-combine: inherit; }</style>
</head>
<body epub:type="bodymatter">

<h1 data-number="21">Chapter 17<br/>
Next Steps</h1>
<p>The journey from raw data to useful information has only begun. There are often many more steps to getting insights that can be used to support enterprise decision-making. From here, the reader needs to take the initiative to extend these projects, or consider other projects. Some readers will want to demonstrate their grasp of Python while others will go more deeply into the area of exploratory data analysis.</p>
<p>Python is used for so many different things that it seems difficult to even suggest a direction for deeper understanding of the language, the libraries, and the various ways Python is used.</p>
<p>In this chapter, we’ll touch on a few more topics related to exploratory data analysis. The projects in this book are only a tiny fraction of the kinds of problems that need to be solved on a daily basis.</p>
<p>Every analyst needs to balance the time between understanding the enterprise data being processed, searching for better ways to model the data, and effective ways to present the results. Each of these areas is a large domain of knowledge and skills.</p>
<p>We’ll start with a review of the architecture underlying the sequence of projects in this book. </p>

<h2 data-number="21.1">17.1  Overall data wrangling</h2>
<p>The applications and notebooks are designed around the following multi-stage architecture:</p>
<ul>
<li><p>Data acquisition</p></li>
<li><p>Inspection of data</p></li>
<li><p>Cleaning data; this includes validating, converting, standardizing, and saving intermediate results</p></li>
<li><p>Summarizing, and the start of modeling data</p></li>
<li><p>Creating deeper analysis and more sophisticated statistical models</p></li>
</ul>
<p>The stages fit together as shown in <a href="#17.1"><em>Figure 17.1</em></a>.</p>
<figure class="IMG---Figure">
<img alt="Figure 17.1: Data Analysis Pipeline " src="img/file79.jpg"/>
<figcaption class="IMG---Caption">Figure 17.1: Data Analysis Pipeline </figcaption>
</figure>
<p>The last step in this pipeline isn’t — of course — final. In many cases, the project evolves from exploration to monitoring and maintenance. There will be a long tail where the model continues to be confirmed. Some enterprise management oversight is an essential part of this ongoing confirmation.</p>
<p>In some cases, the long tail is interrupted by a change. This may be reflected by a model’s inaccuracy. There may be a failure to pass basic statistical tests. Uncovering the change and the reasons for change is why enterprise management oversight is so essential to data analysis.</p>
<p>This long tail of analysis can last a long time. The responsibility may be passed from analyst to analyst. Stakeholders may come and go. An analyst often needs to spend precious time justifying an ongoing study that confirms the enterprise remains on course.</p>
<p>Other changes in enterprise processing or software will lead to outright failure in the analytical processing tools. The most notable changes are those to “upstream” applications. Sometimes these changes are new versions of software. Other times, the upstream changes are organizational in nature, and some of the foundational assumptions about the enterprise need to change. As the data sources change, the data acquisition part of this pipeline must also change. In some cases, the cleaning, validating, and standardizing must also change.</p>
<p>Because of the rapid pace of change in the supporting tools — Python, JupyterLab, Matplotlib, etc. — it becomes essential to rebuild and retest these analytic applications periodically. The version numbers in <code>requirements.txt</code> files must be checked against Anaconda distributions, conda-forge, and the PyPI index. The tempo and nature of the changes make this maintenance task an essential part of any well-engineered solution.</p>
<p>The idea of enterprise oversight and management involvement is sometimes dubbed ‘‘decision support.” We’ll look briefly at how data analysis and modeling is done as a service to decision-makers. </p>


<h2 data-number="21.2">17.2  The concept of “decision support”</h2>
<p>The core concept behind all data processing, including analytics and modeling, is to help some person make a decision. Ideally, a good decision will be based on sound data.</p>
<p>In many cases, decisions are made by software. Sometimes the decisions are simple rules that identify bad data, incomplete processes, or invalid actions. In other cases, the decisions are more nuanced, and we apply the term “artificial intelligence” to the software making the decision.</p>
<p>While many kinds of software applications make many automated decisions, a person is still — ultimately — responsible for those decisions being correct and consistent. This responsibility may be implemented as a person reviewing a periodic summary of decisions made.</p>
<p>This responsible stakeholder needs to understand the number and types of decisions being made by application software. They need to confirm the automated decisions reflect sound data as well as the stated policy, the governing principles of the enterprise, and any legal frameworks in which the enterprise operates.</p>
<p>This suggests a need for a meta-analysis and a higher level of decision support. The operational data is used to create a model that can make decisions. The results of the decisions become a dataset about the decision-making process; this is subject to analysis and modeling to confirm the proper behavior of the operational model.</p>
<p>In all cases, the ultimate consumer is the person who needs the data to decide if a process is working correctly or there are defects that need correction.</p>
<p>This idea of multiple levels of data processing leads to the idea of carefully tracking data sources to understand the meaning and any transformations applied to that data. We’ll look at metadata topics, next. </p>


<h2 data-number="21.3">17.3  Concept of metadata and provenance</h2>
<p>The description of a dataset includes three important aspects:</p>
<ul>
<li><p>The syntax or physical format and logical layout of the data</p></li>
<li><p>The semantics, or meaning, of the data</p></li>
<li><p>The provenance, or the origin and transformations applied to the data</p></li>
</ul>
<p>The physical format of a dataset is often summarized using the name of a well-known file format. For example, the data may be in CSV format. The order of columns in a CSV file may change, leading to a need to have headings or some metadata describing the logical layout of the columns within a CSV file.</p>
<p>Much of this information can be enumerated in JSON schema definitions.</p>
<p>In some cases, the metadata might be yet another CSV file that has column numbers, preferred data types, and column names. We might have a secondary CSV file that looks like the following example:</p>
<pre class="source-code">1,height,height in inches
2,weight,weight in pounds
3,price,price in dollars</pre>
<p>This metadata information describes the contents of a separate CSV file with the relevant data in it. This can be transformed into a JSON schema to provide a uniform metadata notation.</p>
<p>The provenance metadata has a more complicated set of relationships. The PROV model (see <a class="url" href="https://www.w3.org/TR/prov-overview/">https://www.w3.org/TR/prov-overview/</a>) describes a model that includes <strong>Entity</strong>, <strong>Agent</strong>, and <strong>Activity</strong>, which create or influence the creation of data. Within the PROV model, there are a number of relationships, including <strong>Generation </strong>and <strong>Derivation</strong>, that have a direct impact on the data being analyzed.</p>
<p>There are several ways to serialize the information. The PROV-N standard provides a textual representation that’s relatively easy to read. The PROV-O standard defines an OWL ontology that can be used to describe the provenance of data. Ontology tools can query the graph of relationships to help an analyst better understand the data being analyzed.</p>
<p>The reader is encouraged to look at <a class="url" href="https://pypi.org/project/prov/">https://pypi.org/project/prov/</a> for a Python implementation of the PROV standard for describing data provenance.</p>
<p>In the next section, we’ll look at additional data modeling and machine learning applications. </p>


<h2 data-number="21.4">17.4  Next steps toward machine learning</h2>
<p>We can draw a rough boundary between statistical modeling and machine learning. This is a hot topic of debate because — viewed from a suitable distance — all statistical modeling can be described as machine learning.</p>
<p>In this book, we’ve drawn a boundary to distinguish methods based on algorithms that are finite, definite, and effective. For example, the process of using the linear least squares technique to find a function that matches data is generally reproducible with an exact closed-form answer that doesn’t require tuning hyperparameters.</p>
<p>Even within our narrow domain of “statistical modeling,” we can encounter data sets for which linear least squares don’t behave well. One notable assumption of the least squares estimates, for example, is that the independent variables are all known exactly. If the <em>x </em>values are subject to observational error, a more sophisticated approach is required.</p>
<div><div><p>The boundary between “statistical modeling” and “machine learning” isn’t a crisp, simple distinction.</p>
</div>
</div>
<p>We’ll note one characteristic feature of machine learning: tuning hyperparameters. The exploration of hyperparameters can become a complex side topic for building a useful model. This feature is important because of the jump in the computing cost between a statistical model and a machine learning model that requires hyperparameter tuning.</p>
<p>Here are two points on a rough spectrum of computational costs:</p>
<ul>
<li><p>A statistical model may be created by a finite algorithm to reduce the data to a few parameters including the coefficients of a function that fits the data.</p></li>
<li><p>A machine learning model may involve a search through alternative hyperparameter values to locate a combination that produces a model passes some statistical tests for utility.</p></li>
</ul>
<p>The search through hyperparameter values often involves doing substantial computation to create each variation of a model. Then doing additional substantial computations to measure the accuracy and general utility of the model. These two steps are iterated for various hyperparameter values, looking for the best model. This iterative search can make some machine learning approaches computationally intensive.</p>
<p>This overhead and hyperparameter search is not a universal feature of machine learning. For the purposes of this book, it’s where the author drew a line to limit the scope, complexity, and cost of the projects.</p>
<p>You are strongly encouraged to continue your projects by studying the various linear models available in scikit-learn. See <a class="url" href="https://scikit-learn.org/stable/modules/linear_model.html">https://scikit-learn.org/stable/modules/linear_model.html</a>.</p>
<p>The sequence of projects in this book is the first step toward creating a useful understanding from raw data.</p>


</body>
</html>
