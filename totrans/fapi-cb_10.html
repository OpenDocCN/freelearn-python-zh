<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-325"><a id="_idTextAnchor328"/>10</h1>
<h1 id="_idParaDest-326"><a id="_idTextAnchor329"/>Integrating FastAPI with other Python Libraries</h1>
<p>In this chapter, we will delve into the process of expanding the capabilities <a id="_idIndexMarker642"/>of <strong class="bold">FastAPI</strong> by integrating it with <a id="_idIndexMarker643"/>other <strong class="bold">Python</strong> libraries. By harnessing the power of external tools and libraries, you can enhance the functionality of your FastAPI applications and unlock new possibilities for creating dynamic and feature-rich web services.</p>
<p>Throughout this chapter, you will learn how to integrate FastAPI with a diverse range of Python libraries, each serving a different purpose and offering unique functionalities. From taking advantage of advanced natural language processing capabilities with <strong class="bold">Cohere</strong> and <strong class="bold">LangChain</strong> to integrating real-time communication features with <strong class="bold">gRPC</strong> and <strong class="bold">GraphQL</strong>, you will discover how to harness the full potential of FastAPI in conjunction with other popular Python tools.</p>
<p>By integrating FastAPI with other Python libraries, you will be able to build sophisticated web applications that go beyond<a id="_idIndexMarker644"/> simple <strong class="bold">REST APIs</strong>. Whether you are developing a chatbot powered by natural language processing or integrating <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models for<a id="_idIndexMarker645"/> intelligent decision-making, the possibilities are endless.</p>
<p>By the end of this chapter, you will be equipped with the knowledge and skills to effectively leverage external tools and resources, enabling you to build sophisticated and feature-rich APIs that meet the needs of your users.</p>
<p>This chapter includes the following recipes:</p>
<ul>
<li>Integrating FastAPI with gRPC</li>
<li>Connecting FastAPI with GraphQL</li>
<li>Using ML models with Joblib</li>
<li>Integrating FastAPI with Cohere</li>
<li>Integrating FastAPI with LangChain</li>
</ul>
<h1 id="_idParaDest-327"><a id="_idTextAnchor330"/>Technical requirements</h1>
<p>To follow the recipes in this chapter, it is important to have a good understanding of FastAPI. Additionally, since this chapter demonstrates how to integrate FastAPI with external libraries, having a basic knowledge of each library can be beneficial.</p>
<p>However, we will provide external links for you to review any of the concepts that are used in the recipes. You can also refer back to this chapter whenever you need to integrate a technology with FastAPI.</p>
<p>The code used in the chapter is hosted on GitHub at <a href="https://github.com/PacktPublishing/FastAPI-Cookbook/tree/main/Chapter10">https://github.com/PacktPublishing/FastAPI-Cookbook/tree/main/Chapter10</a>.</p>
<p>It is recommended to set up a virtual environment for the project in the project root folder to efficiently manage dependencies and maintain project isolation.</p>
<p>For each recipe, you can install all the dependencies at once within your virtual environment by using the <code>requirements.txt</code> file provided in the GitHub repository in the following project folder:</p>
<pre class="console">
$ pip install –r requirements.txt</pre> <p>Let’s start delving into this recipe and discovering the potential of coupling FastAPI with external libraries.</p>
<h1 id="_idParaDest-328"><a id="_idTextAnchor331"/>Integrating FastAPI with gRPC</h1>
<p>gRPC is a high-performance, open<a id="_idIndexMarker646"/> source universal <strong class="bold">Remote Procedure Call</strong> (<strong class="bold">RPC</strong>) framework<a id="_idIndexMarker647"/> originally <a id="_idIndexMarker648"/>developed by Google. It is designed to be efficient, lightweight, and interoperable across different programming languages and platforms. Integrating FastAPI with gRPC allows you to leverage the power of RPC for building efficient, scalable, and maintainable APIs.</p>
<p>The recipe will show how to build a gateway between a REST client and a gRPC server by using FastAPI.</p>
<h2 id="_idParaDest-329"><a id="_idTextAnchor332"/>Getting ready</h2>
<p>To follow the recipe, it can be beneficial to have some previous knowledge of protocol buffers. You can have a look at the official documentation at <a href="https://protobuf.dev/overview/">https://protobuf.dev/overview/</a>.</p>
<p>Also, we will use the proto3 version to define the <code>.proto</code> files. You can check the language guide at <a href="https://protobuf.dev/programming-guides/proto3/">https://protobuf.dev/programming-guides/proto3/</a>.</p>
<p>We will create a dedicated root project folder for the recipe called <code>grpc_gateway</code>.</p>
<p>Aside from <code>fastapi</code> and <code>uvicorn</code>, you also need to install the <code>grpcio</code> and <code>grpcio-tools</code> packages . You can do this by using the <code>requirements.txt</code> file provided in the GitHub repository<a id="_idIndexMarker649"/> in your environment or by explicitly specifying the packages with the <code>pip</code> command in your environment as follows:</p>
<pre class="console">
$ pip install fastapi uvicorn grpcio grpcio-tools</pre> <p>Before starting with the recipe, let’s build a basic gRPC server with one method that takes a message <a id="_idIndexMarker650"/>from<a id="_idIndexMarker651"/> the client and sends back a message as well by following these steps.</p>
<ol>
<li>Under the root project let’s create a <code>grpcserver.proto</code> file containing the definition of our server as follows:<pre class="source-code">
syntax = "proto3";
service GrpcServer{
    rpc GetServerResponse(Message)
    returns (MessageResponse) {}
}</pre></li> <li>In the same file, we will define the <code>Message</code> and <code>MessageResponse</code> messages as follows:<pre class="source-code">
message Message{
string message = 1;
}
message MessageResponse{
string message = 1;
bool received = 2;
}</pre><p class="list-inset">From the <code>.proto</code> file we have just created, we can automatically generate the Python code necessary to integrate the service and gRPC client as well with a proto compiler.</p></li> <li>Then, from<a id="_idIndexMarker652"/> the <a id="_idIndexMarker653"/>command line terminal, run the following command:<pre class="source-code">
<strong class="bold">$ python -m grpc_tools.protoc \</strong>
<strong class="bold">--proto_path=. ./grpcserver.proto \</strong>
<strong class="bold">--python_out=. \</strong>
<code>grpcserver_pb2_grpc.py</code> and <code>grpcserver_pb2.py</code>. The <code>grpcserver_pb2_grpc.py</code> file contains the class to build the server a support function and a <code>stub</code> class that will be used by the client, while the <code>grpcserver_pb2.py</code> module contains the classes that define the messages. In our case, these are <code>Message</code> and <code>MessageResponse</code>.</p></li> <li>Now let’s write a script to run the gRPC server. Let’s create a file called <code>grpc_server.py</code> and define the server class as follows:<pre class="source-code">
from grpcserver_pb2 import MessageResponse
from grpcserver_pb2_grpc import GrpcServerServicer
class Service(GrpcServerServicer):
    async def GetServerResponse(
        self, request, context
    ):
        message = request.message
        logging.info(f"Received message: {message}")
        result = (
            "Hello I am up and running, received: "
            f"{message}"
        )
        result = {
            "message": result,
            "received": True,
        }
        return MessageResponse(**result)</pre></li> <li>Then we will define <a id="_idIndexMarker654"/>the<a id="_idIndexMarker655"/> function to run the server on the localhost on port <code>50015</code> as follows:<pre class="source-code">
import grpc
from grpcserver_pb2_grpc import (
    add_GrpcServerServicer_to_server
)
async def serve():
    server = grpc.aio.server()
    add_GrpcServerServicer_to_server(
        Service(), server
    )
    server.add_insecure_port("[::]:50051")
    logging.info("Starting server on port 50051")
    await server.start()
    await server.wait_for_termination()</pre></li> <li>We then close<a id="_idIndexMarker656"/> the<a id="_idIndexMarker657"/> script by running the <code>serve</code> function into the event loop:<pre class="source-code">
import asyncio
import logging
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(serve())</pre></li> </ol>
<p>This is all we need to build the gRPC server. Now we can run the script from the command line:</p>
<pre class="console">
<strong class="bold">$ python ./grpc_server.py</strong></pre> <p>If everything has been set up correctly you will see the following log message on the terminal:</p>
<pre class="console">
INFO:root:Starting server on port 50051</pre> <p>With the gRPC server running, we can now create our gateway by leveraging FastAPI.</p>
<h2 id="_idParaDest-330"><a id="_idTextAnchor333"/>How to do it…</h2>
<p>We will create a FastAPI application with one <code>GET /grpc</code> endpoint that will take a message as a parameter, forward the request to the gRPC server, and return the message from the gRPC server to the client. Let’s go through the following steps to build a basic gateway application.</p>
<ol>
<li>Under the project root folder, let’s create a folder called <code>app</code> with a <code>main.py</code> module containing the server as follows:<pre class="source-code">
from fastapi import FastAPI
app = FastAPI()</pre></li> <li>Now let’s create <a id="_idIndexMarker658"/>the <a id="_idIndexMarker659"/>response class schema with Pydantic that will reflect the <code>MessageResponse</code> proto class as follows:<pre class="source-code">
from pydantic import BaseModel
class GRPCResponse(BaseModel):
    message: str
    received: bool</pre></li> <li>Then we will initialize the <code>grpc_channel</code>object, which is an abstraction layer for gRPC calls containing the gRPC service URL, as follows:<pre class="source-code">
grpc_channel = grpc.aio.insecure_channel(
    "localhost:50051"
)</pre></li> <li>Finally, we can create our endpoint as follows:<pre class="source-code">
@app.get("/grpc")
async def call_grpc(message: str) -&gt; GRPCResponse:
    async with grpc_channel as channel:
        grpc_stub = GrpcServerStub(channel)
        response = await grpc_stub.GetServerResponse(
            Message(message=message)
        )
        return response</pre></li> </ol>
<p>Once we have created our FastAPI application, let’s spin up the server from the command line:</p>
<pre class="console">
$ uvicorn app.main:app</pre> <p>Open the interactive documentation at <code>http://localhost:8000/docs</code> and you will see the new endpoint that will take a message parameter and return the response from the gRPC server. If<a id="_idIndexMarker660"/> you<a id="_idIndexMarker661"/> try to call it, you will also see the log message for the call on the gRPC server terminal.</p>
<p>You have successfully set up a REST-gRPC gateway by using FastAPI!</p>
<h2 id="_idParaDest-331"><a id="_idTextAnchor334"/>There’s more…</h2>
<p>We have created a gateway that supports Unary RPC, which is a simple RPC that resembles a normal function call. It involves sending a single request, which is defined in the <code>.proto</code> file, to the server and receiving a single response back from the server. However, there are various types of RPC implementations available that allow for the streaming of messages from the client to the server or from the server to the client, as well as ones that allow for bidirectional communication.</p>
<p>Creating a REST gateway using FastAPI is a relatively easy task. For more information on how to implement different types of gRPC in Python, you can refer to the following article: <a href="https://www.velotio.com/engineering-blog/grpc-implementation-using-python">https://www.velotio.com/engineering-blog/grpc-implementation-using-python</a>. Once you have mastered these concepts, you can easily integrate them into FastAPI and build a complete gateway for gRPC services.</p>
<h2 id="_idParaDest-332"><a id="_idTextAnchor335"/>See also</h2>
<p>You can dive deeper<a id="_idIndexMarker662"/> into<a id="_idIndexMarker663"/> protocol buffer and how you can use it in Python code in the official documentation:</p>
<ul>
<li><em class="italic">Protocol Buffer Python Generated </em><em class="italic">Code</em>: <a href="https://protobuf.dev/reference/python/python-generated/">https://protobuf.dev/reference/python/python-generated/</a></li>
</ul>
<p>You check more on how to implement gRPC in Python code at the gRPC official documentation page:</p>
<ul>
<li><em class="italic">gRPC Python </em><em class="italic">Tutorial</em>: <a href="https://grpc.io/docs/languages/python/basics/">https://grpc.io/docs/languages/python/basics/</a></li>
</ul>
<p>Also, have a look at the examples on GitHub:</p>
<ul>
<li><em class="italic">gRPC Python GitHub </em><em class="italic">Examples</em>: <a href="https://github.com/grpc/grpc/tree/master/examples/python">https://github.com/grpc/grpc/tree/master/examples/python</a></li>
</ul>
<h1 id="_idParaDest-333"><a id="_idTextAnchor336"/>Connecting FastAPI with GraphQL</h1>
<p>GraphQL is a query<a id="_idIndexMarker664"/> language for APIs and a runtime for executing <a id="_idIndexMarker665"/>queries. It provides an efficient, powerful, and flexible alternative to traditional REST APIs by allowing clients to specify exactly what data they need. Integrating FastAPI with GraphQL enables you to build APIs that are highly customizable and capable of handling complex data requirements. In this recipe, we will see how to connect FastAPI with GraphQL to query a user database, allowing you to create GraphQL schemas, define resolvers, and expose a GraphQL endpoint in your FastAPI application.</p>
<h2 id="_idParaDest-334"><a id="_idTextAnchor337"/>Getting ready</h2>
<p>To follow the recipe, it can be beneficial to ensure you already have some basic knowledge about GraphQL. You can have a look at the official documentation at <a href="https://graphql.org/learn/">https://graphql.org/learn/</a>.</p>
<p>In the GitHub repository folder of this chapter, there is a folder named <code>graphql</code>, which we will consider as the root project folder. To implement GraphQL, we will be utilizing the Strawberry library. Please ensure that you have it installed in your environment along with FastAPI. You can install it by using the <code>requirements.txt</code> file located in the project root of the repository or by using the <code>pip</code> command by running the following:</p>
<pre class="console">
$ pip install fastapi uvicorn strawberry-graphql[fastapi]</pre> <p>Once the installation is complete, we can start the recipe.</p>
<h2 id="_idParaDest-335"><a id="_idTextAnchor338"/>How to do it…</h2>
<p>Let’s create a basic GraphQL endpoint that retrieves users from a specific country in a database. Let’s do it through the following steps.</p>
<ol>
<li>Let’s create a <code>database.py</code> module containing a list of users that we will use as a<a id="_idIndexMarker666"/> database<a id="_idIndexMarker667"/> source. Define the <code>User</code> class as follows:<pre class="source-code">
from pydantic import BaseModel
class User(BaseModel):
    id: int
    username: str
    phone_number: str
    country: str</pre></li> <li>You can then write a <code>users_db</code> object, which will be a list of <code>User</code> class objects, or copy the one from the respective <code>database.py</code> file on the GitHub repository at <a href="https://raw.githubusercontent.com/PacktPublishing/FastAPI-Cookbook/main/Chapter10/graphql/database.py">https://raw.githubusercontent.com/PacktPublishing/FastAPI-Cookbook/main/Chapter10/graphql/database.py</a>.<p class="list-inset">It will look like this:</p><pre class="source-code">
users_db: list[User] = [
    User(
        id=1,
        username="user1",
        phone_number="1234567890",
        country="USA",
    ),
# other users
]</pre><p class="list-inset">We will use this list as a database for our simple query.</p></li> <li>In a separate module called <code>graphql_utils.py</code>, we will define the query. But first, let’s<a id="_idIndexMarker668"/> define the model returned by the <a id="_idIndexMarker669"/>query as follows:<pre class="source-code">
import strawberry
@strawberry.type
class User:
    username: str
    phone_number: str
    country: str</pre></li> <li>Then we will define the query as follows:<pre class="source-code">
@strawberry.type
class Query:
    @strawberry.field
    def users(
        self, country: str | None
    ) -&gt; list[User]:
        return [
            User(
                username=user.username,
                phone_number=user.phone_number,
                country=user.country,
            )
            for user in users_db
            if user.country == country
        ]</pre><p class="list-inset">The query takes a country as an argument and returns all the users for the country.</p></li> <li>Now, in the<a id="_idIndexMarker670"/> same <a id="_idIndexMarker671"/>file, let’s create the GraphQL schema with the FastAPI router:<pre class="source-code">
from strawberry.fastapi import GraphQLRouter
schema = strawberry.Schema(Query)
graphql_app = GraphQLRouter(schema)</pre><p class="list-inset">The last line will create a <code>fastapi.Router</code> instance that will handle the endpoint.</p></li> <li>Let’s finalize the endpoint by adding the router to the main FastAPI instance in a separate <code>main.py</code> module as follows:<pre class="source-code">
from fastapi import FastAPI
from graphql_utils import graphql_app
app = FastAPI()
app.include_router(graphql_app, prefix="/graphql")</pre><p class="list-inset">We added the endpoint to the FastAPI instance and defined the <code>/</code><code>graphql</code> path.</p></li> </ol>
<p>This is all you need to setup a GraphQl endpoint within FastAPI.</p>
<p>To explore the potential of the endpoint, let’s run the server from the command line:</p>
<pre class="console">
<code>http://localhost:8000/graphql</code>. You will see an interactive page for the endpoint. The page is divided into two panels. The left contains the query editor and the right will show you the response.</p>
<p>Try to make <a id="_idIndexMarker672"/>the<a id="_idIndexMarker673"/> following GraphQL query:</p>
<pre class="source-code">
{
  users(country: "USA") {
    username
    country
    phoneNumber
  }
}</pre> <p>You will see the result on the right panel, which will look something like this:</p>
<pre class="source-code">
{
  "data": {
    "users": [
      {
        "username": "user1",
        "country": "USA",
        "phoneNumber": "1234567890"
      }
    ]
  }
}</pre> <p>You have learned how to create an interactive GraphQL endpoint. By combining RESTful endpoints with GraphQL, the potential for data querying and modification can be greatly expanded. Real-world scenarios may involve using REST endpoints to modify the database by adding, modifying, or removing records. GraphQL can then be used to query the database<a id="_idIndexMarker674"/> and<a id="_idIndexMarker675"/> extract valuable insights.</p>
<h2 id="_idParaDest-336"><a id="_idTextAnchor339"/>See also</h2>
<p>You can consult the FastAPI official documentation on how to integrate GraphQL:</p>
<ul>
<li><em class="italic">FastAPI GraphQL </em><em class="italic">Documentation</em>: <a href="https://fastapi.tiangolo.com/how-to/graphql/">https://fastapi.tiangolo.com/how-to/graphql/</a></li>
</ul>
<p>Also, in the Strawberry documentation, you can find a dedicated page on FastAPI integration:</p>
<ul>
<li><em class="italic">Integrate FastAPI with </em><em class="italic">Strawberry</em>: <a href="https://strawberry.rocks/docs/integrations/fastapi">https://strawberry.rocks/docs/integrations/fastapi</a></li>
</ul>
<h1 id="_idParaDest-337"><a id="_idTextAnchor340"/>Using ML models with Joblib</h1>
<p>ML models are <a id="_idIndexMarker676"/>powerful tools for data analysis, prediction, and <a id="_idIndexMarker677"/>decision-making in various applications. FastAPI provides a robust framework for building web services, making it an ideal choice for deploying ML models in production environments. In this recipe, we will see how to integrate an ML model with FastAPI using <strong class="bold">Joblib</strong>, a popular library for model serialization and deserialization in Python.</p>
<p>We will develop an AI-powered doctor application that can diagnose diseases by analyzing the symptoms provided.</p>
<p class="callout-heading">Warning</p>
<p class="callout">Note that the diagnoses provided by the AI doctor should not be trusted in real-life situations, as it is not reliable.</p>
<h2 id="_idParaDest-338"><a id="_idTextAnchor341"/>Getting ready</h2>
<p>Prior knowledge of ML is not mandatory but having some can be useful to help you follow the recipe.</p>
<p>We will apply the recipe to a new project, so create a folder named <code>ai_doctor</code> that we will use as the project root folder.</p>
<p>To ensure that you have all the necessary packages in your environment, you can install them using the <code>requirements.txt</code> file provided in the GitHub repository or from the command line:</p>
<pre class="console">
$ pip install fastapi[all] joblib scikit-learn</pre> <p>We will download the model from the Hugging Face Hub, a centralized hub hosting pre-trained ML models that are ready to be used.</p>
<p>We will use the <code>human-disease-prediction</code> model, which is a relatively lightweight linear logistic regression model developed with the <code>scikit-learn</code> package. You can check it out at the following link: <a href="https://huggingface.co/AWeirdDev/human-disease-prediction">https://huggingface.co/AWeirdDev/human-disease-prediction</a>.</p>
<p>To download it, we<a id="_idIndexMarker678"/> will leverage the provided <code>huggingface_hub</code> Python<a id="_idIndexMarker679"/> package, so make sure you have it in your environment by running the following:</p>
<pre class="console">
$ pip install huggingface_hub</pre> <p>Once the installation is complete, we can proceed with building our AI doctor.</p>
<h2 id="_idParaDest-339"><a id="_idTextAnchor342"/>How to do it…</h2>
<p>Let’s follow these steps to create our AI doctor:</p>
<ol>
<li>Let’s start by writing the code to accommodate the ML model. In the project root folder, let's create the <code>app</code> folder containing a module called <code>utils.py</code>. In the module, we will declare a <code>symptoms_list</code> list containing all the symptoms accepted by the model. You can download the file directly from the GitHub repository at <a href="https://raw.githubusercontent.com/PacktPublishing/FastAPI-Cookbook/main/Chapter10/ai_doctor/app/utils.py">https://raw.githubusercontent.com/PacktPublishing/FastAPI-Cookbook/main/Chapter10/ai_doctor/app/utils.py</a>.<p class="list-inset">You can find the complete list on the model’s documentation page at <a href="https://huggingface.co/AWeirdDev/human-disease-prediction">https://huggingface.co/AWeirdDev/human-disease-prediction</a>.</p></li>
<li>Still in the <code>app</code> folder, let’s create the <code>main.py</code> module that will contain the <code>FastAPI</code> server class object and the endpoint. To incorporate the model into our application, we will utilize the FastAPI lifespan feature.<p class="list-inset">We can define <a id="_idIndexMarker680"/>the <a id="_idIndexMarker681"/>lifespan context manager as follows:</p><pre class="source-code">
from fastapi import FastAPI
from contextlib import asynccontextmanager
ml_model = {}
REPO_ID = "AWeirdDev/human-disease-prediction"
FILENAME = "sklearn_model.joblib"
@asynccontextmanager
async def lifespan(app: FastAPI):
    ml_model["doctor"] = joblib.load(
        hf_hub_download(
            repo_id=REPO_ID, filename=FILENAME
        )
    )
    yield
    ml_model.clear()</pre><p class="list-inset">The <code>lifespan</code> context manager serves as middleware and carries out operations before and after server start and shutdown. It retrieves the model from the Hugging Face Hub and stores it in the <code>ml_model</code> dictionary, so it to be used across the endpoints without the need to reload it every time it is called.</p></li> <li>Once it has been defined, we need to pass it to the <code>FastAPI</code> object class as follows:<pre class="source-code">
app = FastAPI(
    title="AI Doctor",
    lifespan=lifespan
)</pre></li> <li>Now we need <a id="_idIndexMarker682"/>to<a id="_idIndexMarker683"/> create the endpoint that will take the symptoms as parameters and return the diagnosis.<p class="list-inset">The idea is to return each symptom as a path parameter. Since we have <code>132</code> possible symptoms, we will create the parameters object dynamically with Pydantic and restrict our model to the first ten symptoms. In the <code>main.py</code> file, let’s create the <code>Symptoms</code> class used to accept the parameters with the <code>pydantic.create_model</code> function as follows:</p><pre class="source-code">
from pydantic import create_model
from app.utils import symptoms_list
query_parameters = {
    symp: (bool, False)
    for symp in symptoms_list[:10]
}
Symptoms = create_model(
    "Symptoms", **query_params
)</pre><p class="list-inset">We now have all that we need to create our <code>GET /</code><code>diagnosis</code> endpoint.</p></li> <li>Let’s create<a id="_idIndexMarker684"/> our <a id="_idIndexMarker685"/>endpoint as follows:<pre class="source-code">
@app.get("/diagnosis")
async def get_diagnosis(
    symptoms: Annotated[Symptoms, Depends()],
):
    array = [
        int(value)
        for _, value in symptoms.model_dump().items()
    ]
    array.extend(
        # adapt array to the model's input shape
        [0] * (len(symptoms_list) - len(array))
    )
    len(symptoms_list)
    diseases = ml_model["doctor"].predict([array])
    return {
        "diseases": [disease for disease in diseases]
    }</pre></li> </ol>
<p>To test it, as usual, spin up the server with <code>uvicorn</code> from the command line by running the following:</p>
<pre class="console">
$ uvicorn app.main:app</pre> <p>Open the interactive documentation from the browser at <code>http://localhost:8000/docs</code>. You will see the only <code>GET /diagnosis</code> endpoint and you will be able to select the symptoms. Try to select some of them and get your diagnosis from the AI doctor you have just created.</p>
<p>You have just created a FastAPI application that integrates an ML model. You can use the same model for different endpoints, but you can also integrate multiple models within the<a id="_idIndexMarker686"/> same <a id="_idIndexMarker687"/>application with the same strategy.</p>
<h2 id="_idParaDest-340"><a id="_idTextAnchor343"/>See also</h2>
<p>You can check the guidelines on how to integrate an ML model into FastAPI on the official<a id="_idIndexMarker688"/> documentation page:</p>
<ul>
<li><em class="italic">Lifespan </em><em class="italic">Events</em>: <a href="https://fastapi.tiangolo.com/advanced/events/?h=machine+learning#use-case">https://fastapi.tiangolo.com/advanced/events/?h=machine+learning#use-case</a></li>
</ul>
<p>You can have a look at the<a id="_idIndexMarker689"/> Hugging Face Hub platform documentation at the link:</p>
<ul>
<li><em class="italic">Hugging Face Hub </em><em class="italic">Documentation</em>: <a href="https://huggingface.co/docs/hub/index">https://huggingface.co/docs/hub/index</a></li>
</ul>
<p>Take a moment to <a id="_idIndexMarker690"/>explore the capabilities of the <code>scikit-learn</code> package by referring to the official documentation:</p>
<ul>
<li><em class="italic">Scikit-learn </em><em class="italic">Documentation</em>: <a href="https://scikit-learn.org/stable/">https://scikit-learn.org/stable/</a></li>
</ul>
<h1 id="_idParaDest-341"><a id="_idTextAnchor344"/>Integrating FastAPI with Cohere</h1>
<p>Cohere offers <a id="_idIndexMarker691"/>powerful language models and APIs that<a id="_idIndexMarker692"/> enable developers to build sophisticated AI-powered applications capable of understanding and generating human-like text.</p>
<p>State-of-the-art language models, such as the <strong class="bold">Generative Pre-trained Transformer</strong> (<strong class="bold">GPT</strong>) series, have <a id="_idIndexMarker693"/>revolutionized how machines comprehend and generate natural language. These models, which are trained on vast amounts of text data, deeply understand human language patterns, semantics, and context.</p>
<p>By leveraging Cohere AI’s models, developers can empower their applications to engage in natural language conversations, answer queries, generate creative content, and perform a wide range of language-related tasks.</p>
<p>In this recipe, we will create an AI-powered chatbot using FastAPI and Cohere that suggests Italian cuisine recipes based on user queries.</p>
<h2 id="_idParaDest-342"><a id="_idTextAnchor345"/>Getting ready</h2>
<p>Before starting the recipe, you will need a Cohere account and an API key.</p>
<p>You can create your account at the page <a href="https://dashboard.cohere.com/welcome/login">https://dashboard.cohere.com/welcome/login</a> by clicking the <strong class="bold">Sign up</strong> button at the top. At the time of writing, you can create an account by using your existing GitHub or Google account.</p>
<p>Once logged in, you <a id="_idIndexMarker694"/>will see a welcome page and a platform <a id="_idIndexMarker695"/>menu on the left with some options. Click on <strong class="bold">API keys</strong> to access the API menu.</p>
<p>By default, you will have a trial key that is free of charge, but it is rate limited and it cannot be used for commercial purposes. For the recipe, it will be largely sufficient.</p>
<p>Now create the project root folder called <code>chef_ai</code> and store your API key in a file called <code>.env</code> under the project root folder as follows:</p>
<pre class="source-code">
COHERE_API_KEY="your-cohere-api-key"</pre> <p class="callout-heading">Warning</p>
<p class="callout">If you develop your project with a versioning system control such as Git, for example, make sure to not track any API keys. If you have done this already, even unintentionally, revoke the key from the Cohere API keys page and generate a new one.</p>
<p>Aside from the API key, make sure that you also have all the required packages in your environment. We will need <code>fastapi</code>, <code>uvicorn</code>, <code>cohere</code>, and <code>python-dotenv</code>. This last package will enable importing environment variables from the <code>.</code><code>env</code> file.</p>
<p>You can install all the packages with the <code>requirements.txt</code> file provided in the GitHub repository in the <code>chef_ai</code> project folder by running the following:</p>
<pre class="console">
$ pip install -r requirements.txt</pre> <p>Alternatively you can install them one by one:</p>
<pre class="console">
$ pip install fastapi uvicorn cohere python-dotenv</pre> <p>Once the installation is complete, we can dive into the recipe and create our “chef de cuisine” assistant.</p>
<h2 id="_idParaDest-343"><a id="_idTextAnchor346"/>How to do it…</h2>
<p>We will create<a id="_idIndexMarker696"/> our<a id="_idIndexMarker697"/> chef cuisine assistant by using a message completion chat. Chat completion models take a list of messages as input and return a model-generated message as output. The first message to provide is <a id="_idIndexMarker698"/>the <strong class="bold">system message</strong>.</p>
<p>A system message defines how a chatbot behaves in a conversation, such as adopting a specific tone or acting as a specialist such as a senior UX designer or software engineer. In our case, the system message will tell the chatbot to behave like a chef de cuisine.</p>
<p>Let’s create an endpoint to call our chat through the following steps:</p>
<ol>
<li>Let’s create a <code>handlers.py</code> module under the project root and import the Cohere API key from the <code>.</code><code>env</code> file:<pre class="source-code">
from dotenv import load_dotenv
load_dotenv()</pre></li> <li>Let’s write the system message as follows:<pre class="source-code">
SYSTEM_MESSAGE = (
    "You are a skilled Italian top chef "
    "expert in Italian cuisine tradition "
    "that suggest the best recipes unveiling "
    "tricks and tips from Grandma's Kitchen"
    "shortly and concisely."
)</pre></li> <li>Define the Cohere asynchronous client as follows:<pre class="source-code">
from cohere import AsyncClient
client = AsyncClient()</pre></li> <li>Before creating the function the generate the message, let’s import the required modules as:<pre class="source-code">
from cohere import ChatMessage
from cohere.core.api_error import ApiError
from fastapi import HTTPException</pre></li> <li>Then, we <a id="_idIndexMarker699"/>can<a id="_idIndexMarker700"/> define the function to generate our message:<pre class="source-code">
async def generate_chat_completion(
    user_query=" ", messages=[]
) -&gt; str:
    try:
        response = await client.chat(
            message=user_query,
            model="command-r-plus",
            preamble=SYSTEM_MESSAGE,
            chat_history=messages,
        )
        messages.extend(
            [
                ChatMessage(
                    role="USER", message=user_query
                ),
                ChatMessage(
                    role="CHATBOT",
                    message=response.text,
                ),
            ]
        )
        return response.text
    except ApiError as e:
        raise HTTPException(
            status_code=e.status_code, detail=e.body
        )</pre><p class="list-inset">The function will take in input the user query and the messages previously exchanged during the <a id="_idIndexMarker701"/>conversation. If <a id="_idIndexMarker702"/>the response is returned with no errors, the messages list is updated with the new interaction, otherwise an HTTPException error is raised.</p><p class="list-inset">We utilized <code>main.py</code> module, located under the project root folder, we can start defining the <code>messages</code> list in the application state at the startup with the lifespan context manager:<pre class="source-code">
from contextlib import asynccontextmanager
from fastapi import FastAPI
@asynccontextmanager
async def lifespan(app: FastAPI):
    yield {"messages": []}</pre></li> <li>We then pass the <code>lifespan</code> context manager to the app object as:<pre class="source-code">
app = FastAPI(
    title="Chef Cuisine Chatbot App",
    lifespan=lifespan,
)</pre></li> <li>Finally, we <a id="_idIndexMarker703"/>can <a id="_idIndexMarker704"/>create our endpoint as follows:<pre class="source-code">
from typing import Annotated
from fastapi import Body, Request
from handlers import generate_chat_completion
@app.post("/query")
async def query_chat_bot(
    request: Request,
    query: Annotated[str, Body(min_length=1)],
) -&gt; str:
    answer = await generate_chat_completion(
        query, request.state.messages
    )
    return answer</pre><p class="list-inset">We enforce a minimum length for the query message <code>(Body(min_length=1))</code> to prevent the model from returning an error response.</p></li> </ol>
<p>You have just created an endpoint that interacts with our chef de cuisine chatbot.</p>
<p>To test it, spin up the server with <code>uvicorn</code>:</p>
<pre class="console">
<strong class="bold">$ uvicorn main:app</strong></pre> <p>Open the interactive documentation and start testing the endpoint. For example, you can prompt the model with a message such as the following:</p>
<pre class="source-code">
"Hello, could you suggest a quick recipe for lunch to be prepared in less than one hour?"</pre> <p>Read the answer, then try asking the bot to replace some ingredients and continue the chat. Once you <a id="_idIndexMarker705"/>have <a id="_idIndexMarker706"/>completed your recipe, enjoy your meal!</p>
<p class="callout-heading">Exercise</p>
<p class="callout">We have created a chatbot endpoint to interact with our assistant. However, for real-life applications, it can be useful to have an endpoint that returns all the messages exchanged.</p>
<p class="callout">Create a <code>GET /messages</code> endpoint that returns all the messages in a formatted way.</p>
<p class="callout">Also create an endpoint <code>POST /restart-conversation</code> that will flush all the messages and restart the conversation without any previous messages.</p>
<h2 id="_idParaDest-344"><a id="_idTextAnchor347"/>See also</h2>
<p>You can have a look at the Cohere quickstart on building a chatbot on the official documentation page:</p>
<ul>
<li><em class="italic">Building a </em><em class="italic">Chatbot</em>: <a href="https://docs.cohere.com/docs/building-a-chatbot">https://docs.cohere.com/docs/building-a-chatbot</a></li>
</ul>
<p>In production<a id="_idIndexMarker707"/> environment, depending on the project’s needs<a id="_idIndexMarker708"/> and budget, you might want to choose from the several models available. You can see an overview of the models provided by Cohere here:</p>
<ul>
<li><em class="italic">Models </em><em class="italic">Overview</em>: <a href="https://docs.cohere.com/docs/models">https://docs.cohere.com/docs/models</a></li>
</ul>
<h1 id="_idParaDest-345"><a id="_idTextAnchor348"/>Integrating FastAPI with LangChain</h1>
<p>LangChain is a<a id="_idIndexMarker709"/> versatile <a id="_idIndexMarker710"/>interface for nearly<a id="_idIndexMarker711"/> any <strong class="bold">Large Language Model</strong> (<strong class="bold">LLM</strong>) that allows developers to create LLM applications and integrate them with external data sources and software workflows. It was launched in October 2022 and quickly became a top open source project on GitHub.</p>
<p>We will use LangChain and FastAPI to create an AI-powered assistant for an electronic goods store that provides recommendations and helps users.</p>
<p>We will set <a id="_idIndexMarker712"/>up a <strong class="bold">Retrieval-Augmented Generation</strong> (<strong class="bold">RAG</strong>) application, which involves empowering the model with personalized data to be trained. In this particular case, that would be a document of frequently asked questions.</p>
<p>This recipe will guide you through the process of integrating FastAPI with LangChain to create dynamic and interactive AI assistants that enhance the customer shopping experience.</p>
<h2 id="_idParaDest-346"><a id="_idTextAnchor349"/>Getting ready</h2>
<p>Before starting the recipe, you will need a Cohere API key. If you don’t have it, you can check the <em class="italic">Getting ready</em> section of the <em class="italic">Integrating FastAPI with </em><em class="italic">Cohere</em> recipe.</p>
<p>Create a project directory called <code>ecotech_RAG</code> and place the API key within a <code>.env</code> file, labeled as <code>COHERE_API_KEY</code>.</p>
<p>Previous knowledge of LLM and RAG is not required but having it would help.</p>
<p>Aside from the <code>fastapi</code> and <code>uvicorn</code> packages, you will need to install <code>python-dotenv</code> and the packages related to LangChain. You can do this by using <code>requirements.txt</code> or by installing them with <code>pip</code> as follows:</p>
<pre class="console">
$ pip install fastapi uvicorn python-dotenv
$ pip install langchain
$ pip install langchain-community langchain-cohere
$ pip install chromadb unstructured</pre> <p>Once the installation is complete, we can start building our AI shop assistant.</p>
<h2 id="_idParaDest-347"><a id="_idTextAnchor350"/>How to do it…</h2>
<p>We are going to create an application with a single endpoint that interacts with an LLM from Cohere.</p>
<p>The idea behind LangChain is to provide a series of interconnected modules, forming a chain to establish a workflow linking the user query with the model output.</p>
<p>We will split <a id="_idIndexMarker713"/>the<a id="_idIndexMarker714"/> process of creating the endpoint to interact with the RAG AI assistant into the following steps:</p>
<ol>
<li>Defining the prompts</li>
<li>Ingesting and vectorizing the documents</li>
<li>Building the model chain</li>
<li>Creating the endpoint</li>
</ol>
<p>Let’s start building our AI-powered assistant.</p>
<h3>Defining the prompts</h3>
<p>Like for the previous <a id="_idIndexMarker715"/>recipe, we will utilize a chat model that takes a list message as input. For this specific use case, however, we will supply the model with two messages: the system message and the user message. LangChain includes template objects for specific messages. Here are the steps to set up our prompts:</p>
<ol>
<li>Under the root project, create a module called <code>prompting.py</code>. Let’s start the module by defining a template message that will be used as the system message:<pre class="source-code">
template: str = """
    You are a customer support Chatbot.
    You assist users with general inquiries
    and technical issues.
    You will answer to the question:
    {question}
    Your answer will only be based on the knowledge
    of the context below you are trained on.
    -----------
    {context}
    -----------
    if you don't know the answer,
    you will ask the user
    to rephrase the question or
    redirect the user the support@ecotech.com
    always be friendly and helpful
    at the end of the conversation,
    ask the user if they are satisfied
    with the answer if yes,
    say goodbye and end the conversation
    """</pre><p class="list-inset">This is a common <a id="_idIndexMarker716"/>prompt for customer assistants that contains two variables: <code>question</code> and <code>context</code>. Those variables will be required to query the model.</p></li> <li>With that template, we can define the system message as follows:<pre class="source-code">
from langchain.prompts import (
    SystemMessagePromptTemplate,
)
system_message_prompt = (
    SystemMessagePromptTemplate.from_template(
        template
    )
)</pre></li> <li>The user <a id="_idIndexMarker717"/>message does not require specific context and can be defined as follows:<pre class="source-code">
from langchain.prompts import (
    HumanMessagePromptTemplate,
)
human_message_prompt = (
    HumanMessagePromptTemplate.from_template(
        template="{question}",
    )
)</pre></li> <li>Then we can group both messages under the dedicated chat message <code>template</code> object as follows:<pre class="source-code">
from langchain.prompts import ChatPromptTemplate
chat_prompt_template = (
    ChatPromptTemplate.from_messages(
        [system_message_prompt, human_message_prompt]
    )
)</pre></li> </ol>
<p>This is all we<a id="_idIndexMarker718"/> need to set up the prompt object to query our model.</p>
<h3>Ingesting and vectorizing the documents</h3>
<p>Our assistant will <a id="_idIndexMarker719"/>answer user questions by analyzing the documents we will provide to the model. Let’s create a <code>docs</code> folder under the project root that will contain the documents. First, download the <code>faq_ecotech.txt</code> file from the GitHub repository in the <code>ecotech_RAG/docs</code> project folder and save it in the local <code>docs</code> folder.</p>
<p>You can download it directly at <a href="https://raw.githubusercontent.com/PacktPublishing/FastAPI-Cookbook/main/Chapter10/ecotech_RAG/docs/faq_ecotech.txt">https://raw.githubusercontent.com/PacktPublishing/FastAPI-Cookbook/main/Chapter10/ecotech_RAG/docs/faq_ecotech.txt</a>.</p>
<p>Alternatively, you can create your own FAQ file. Just ensure that each question and answer is separated by one empty line.</p>
<p>The information contained in the file will be used by our assistant to help the customers. However, to retrieve the information, we will need to split our documents into chunks and store them as vectors to optimize searching based on similarity.</p>
<p>To split the documents, we will use a character-based text splitter. To store chunks, we will use Chroma DB, an in-memory vector database.</p>
<p>Then, let’s create a <code>documents.py</code> module that will contain the <code>load_documents</code> helper function that will upload the files into a variable as follows:</p>
<pre class="source-code">
from langchain.text_splitter import (
    CharacterTextSplitter,
)
from langchain_core.documents.base import Document
from langchain_community.document_loaders import (
    DirectoryLoader,
)
from langchain_community.vectorstores import Chroma
async def load_documents(
    db: Chroma,
):
    text_splitter = CharacterTextSplitter(
        chunk_size=100, chunk_overlap=0
    )
    raw_documents = DirectoryLoader(
        "docs", "*.txt"
    ).load()
    chunks = text_splitter.split_documents(
        raw_documents
    )
    await db.aadd_documents(chunks)</pre> <p>The <code>DirectoryLoader</code> class uploads the content of all the <code>.txt</code> files from the <code>docs</code> folder, then<a id="_idIndexMarker720"/> the <code>text_splitter</code> object reorganizes the documents into document chunks of <code>100</code> characters that will be then added to the <code>Chroma</code> database.</p>
<p>By utilizing the vectorized database alongside the user query, we can retrieve the relevant context to feed into our model, which will analyze the most significant portion.</p>
<p>We can write a function for this called <code>get_context</code> as follows:</p>
<pre class="source-code">
def get_context(
    user_query: str, db: Chroma
) -&gt; str:
    docs = db.similarity_search(user_query)
    return "\n\n".join(
        doc.page_content for doc in docs
    )</pre> <p>The documents have to be stored and vectorized in numerical representations called embedding. This can be done with Chroma, an AI-native vector database.</p>
<p>Then, through a similarity search operation (<code>db.similaratiry_search</code>) between the user query and the document chunks, we can retrieve the relevant content to pass as context to the model.</p>
<p>We have now <a id="_idIndexMarker721"/>retrieved the context to provide in the chat model system message template.</p>
<h3>Building the model chain</h3>
<p>Once we have <a id="_idIndexMarker722"/>defined the mechanism to retrieve the context, we can build the chain model. Let’s build it through the following steps:</p>
<ol>
<li>Let’s create a new module called <code>model.py</code>. Since we will use Cohere, we will upload the environment variables from the <code>.env</code> file with the <code>dotenv</code> package as follows:<pre class="source-code">
from dotenv import load_dotenv
load_dotenv()</pre></li> <li>Then we will define the model we are going to use:<pre class="source-code">
from langchain_cohere import ChatCohere
model = ChatCohere(model="command-r-plus")</pre><p class="list-inset">We will use the same module we used in the previous recipe, Command R+.</p></li> <li>Now we can gather the pieces we have created to leverage the power of LangChain by creating the chain pipeline to query the model as follows:<pre class="source-code">
from langchain.schema import StrOutputParser
from prompting import chat_prompt_template
chain = (
    chat_prompt_template | model | StrOutputParser()
)</pre></li> </ol>
<p>We will <a id="_idIndexMarker723"/>use the chain object to create our endpoint to expose through the API.</p>
<h3>Creating the endpoint</h3>
<p>We will make the <code>app</code> object <a id="_idIndexMarker724"/>instance with the endpoint in the <code>main.py</code> module under the project root folder. As always, let’s follow these steps to create it:</p>
<ol>
<li>The operation of loading the documents can be quite CPU-intensive, especially in real-life applications. Therefore, we will define a lifespan context manager to execute this process only at server startup. The <code>lifespan</code> function will be structured as follows:<pre class="source-code">
from contextlib import asynccontextmanager
from fastapi import FastAPI
from langchain_cohere import CohereEmbeddings
from langchain_community.vectorstores import Chroma
from documents import load_documents
@asynccontextmanager
async def lifespan(app: FastAPI):
    db = Chroma(
        embedding_function=CohereEmbeddings()
    )
    await load_documents(db)
    yield {"db": db}</pre></li> <li>We can then pass it to the FastAPI object as follows:<pre class="source-code">
app = FastAPI(
    title="Ecotech AI Assistant",
    lifespan=lifespan
)</pre></li> <li>Now, we<a id="_idIndexMarker725"/> can define a <code>POST /message</code> endpoint as follows:<pre class="source-code">
from typing import Annotated
from fastapi import Body, Request
from documents import get_context
from model import chain
@app.post("/message")
async def query_assistant(
    request: Request,
    question: Annotated[str, Body()],
) -&gt; str:
    context = get_context(question, request.state.db)
    response = await chain.ainvoke(
        {
            "question": question,
            "context": context,
        }
    )
    return response</pre></li> <li>The endpoint will accept a body string text as input and will return the response from the model as a string based on the documents provided in the <code>docs</code> folder at startup.</li>
</ol>
<p>To test it, you can spin up the server from the following command:</p>
<pre class="console">
$ uvicorn main:app</pre> <p>Once the server<a id="_idIndexMarker726"/> has started, open the interactive documentation at <code>http://localhost:8000/docs</code> and you will see the <code>POST /message</code> endpoint we just created.</p>
<p>Try first to send a message that is not related to the assistance, something like the following:</p>
<pre class="source-code">
"What is the capital of Belgium ?"</pre> <p>You will receive an answer like this:</p>
<pre class="source-code">
"I apologize, but I cannot answer that question as it is outside of my knowledge base. I am an FAQ chatbot trained to answer specific questions related to EcoTech Electronics, including our product compatibility with smart home systems, international shipping costs, and promotions for first-time customers. If you have any questions related to these topics, I'd be happy to help! Otherwise, for general inquiries, you can reach out to our support team at support@ecotech.com. Is there anything else I can assist you with today regarding EcoTech Electronics?"</pre> <p>Then try to ask, for example, the following:</p>
<pre class="source-code">
"What kind of payments do you accept?"</pre> <p>You will get your assistance answer, which should be something like this:</p>
<pre class="source-code">
"We want to make sure your shopping experience with us is as smooth and secure as possible. For online purchases, we currently accept major credit cards: Visa, Mastercard, and American Express. You also have the option to pay through PayPal, which offers an additional layer of security and convenience. \n\nThese payment methods are integrated into our straightforward online checkout process, ensuring a quick and efficient transaction. \n\nAre there any specific payment methods you are interested in using, or do you have any further questions about our accepted forms of payment? We want to ensure your peace of mind and a great overall experience shopping with us. \n\nAre you satisfied with the answer?"</pre> <p>You can double check that the answer is in line with what is written in the FAQ document in the <code>docs</code> folder.</p>
<p>You have just<a id="_idIndexMarker727"/> implemented a RAG AI-powered assistant with LangChain and FastAPI. You will now be able to implement your own AI assistant for your application.</p>
<p class="callout-heading">Exercise</p>
<p class="callout">We have implemented the endpoint to interact with the chat model that will answer based on the document provided. However, real-life API applications will allow the addition of new documents interactively.</p>
<p class="callout">Create a new <code>POST /document</code> endpoint that will add a file in the <code>docs</code> folder and reload the documents in the code.</p>
<p class="callout">Have a look at the <em class="italic">Working with file uploads and downloads</em> recipe in <a href="B21025_02.xhtml#_idTextAnchor052"><em class="italic">Chapter 2</em></a>, <em class="italic">Working with Data</em>, to see how to upload files in FastAPI.</p>
<h2 id="_idParaDest-348"><a id="_idTextAnchor351"/>See also</h2>
<p>You can have a look<a id="_idIndexMarker728"/> at the quickstart in the LangChain documentation:</p>
<ul>
<li><em class="italic">LangChain </em><em class="italic">Quickstart</em>: <a href="https://python.langchain.com/v0.1/docs/get_started/quickstart/">https://python.langchain.com/v0.1/docs/get_started/quickstart/</a></li>
</ul>
<p>We have used Chroma, a vector database largely used for ML applications. Feel free to have a look at the <a id="_idIndexMarker729"/>documentation:</p>
<ul>
<li><em class="italic">Chroma</em>: <a href="https://docs.trychroma.com/">https://docs.trychroma.com/</a></li>
</ul>
</div>
</div></body></html>