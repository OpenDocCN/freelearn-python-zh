["```py\n        class Reverse_Seq:\n            def __init__(self, data_in):\n                self.data = data_in\n                self.index = len(data_in) # Go to last element\n            def __iter__(self):\n                return self  # Needed to use __next__()\n            def __next__(self):\n                if self.index == 0: # No more elements\n                    raise StopIteration  # Manually stop the iterator\n                self.index = self.index - 1  # Go to previous element in sequence\n                return self.data[self.index] # Return element at index\n```", "```py\n        def my_generator(x):\n            while x:\n                x -= 1\n                yield x\n```", "```py\n        mygen = my_generator(5)\n```", "```py\n        next(mygen)\n```", "```py\nl = [x for x in foo if x % 2 == 0]\n```", "```py\ng = (x for x in foo if x % 2 == 0)\n```", "```py\nfor i in g:\n```", "```py\n        def cor():\n            hi = yield \"Hello\"\n            yield hi\n```", "```py\n        cor = cor()\n```", "```py\n        print(next(cor))\n```", "```py\n        print(cor.send(\"World\"))\n```", "```py\n        def coroutine(funct):\n            def wrapper(*args, **kwargs):\n                cor = funct(*args, **kwargs)\n                next(cor)\n                return cor\n            return wrapper\n```", "```py\n        import asyncio\n\n        async def compute(x, y):\n            print(\"Compute %s + %s ...\" % (x, y))\n            await asyncio.sleep(1.0)\n            return x + y\n\n        async def print_sum(x, y):\n            result = await compute(x, y)\n            print(\"%s + %s = %s\" % (x, y, result))\n\n        loop = asyncio.get_event_loop()\n        loop.run_until_complete(print_sum(1, 2))\n        loop.close()\n```", "```py\n        import asyncio\n\n        async def factorial(name, number):\n            f = 1\n            for i in range(2, number+1):\n                print(\"Task %s: Compute factorial(%s)...\" % (name, i))\n            await asyncio.sleep(1)\n            f *= i\n            print(\"Task %s: factorial(%s) = %s\" % (name, number, f))\n\n        loop = asyncio.get_event_loop()\n        loop.run_until_complete(asyncio.gather(\n            factorial(\"A\", 2),\n            factorial(\"B\", 3),\n            factorial(\"C\", 4),\n        ))\n        loop.close()\n```", "```py\n        import os\n```", "```py\n        def child():\n            print(\"Child {} calling\".format(os.getpid()))\n            os._exit(0)\n```", "```py\n        def parent():\n            for i in range(10):\n                newchild = os.fork()\n                if newchild == 0:\n                    child()\n                else:\n                    print(\"Parent {parent} calling. Creating child {child}\".format(parent=os.getpid(), child=newchild))\n            i += 1\n```", "```py\n        import urllib.request\n        import urllib.error\n        import time\n\n        def single_thread_retrieval():\n            start_time = time.time()\n            urls = [\"https://www.python.org\", \n                    \"https://www.google.com\", \n                    \"https://www.techdirt.com\",\n                    \"https://www.facebook.com\",\n                    \"https://www.ibm.com\",\n                    \"https://www.dell.com\",\n                    \"https://www.amd.com\",\n                    \"https://www.yahoo.com\",\n                    \"https://www.microsoft.com\",\n                    \"https://www.apache.org\"]\n            try:\n                for url in urls:\n                    urllib.request.urlopen(url)\n            except urllib.error.HTTPError:\n                pass\n            return time.time() - start_time\n```", "```py\n        import statistics\n\n        times = []\n\n        def avg_time(func, val):\n            for num in range(val):\n                times.append(func)\n        return statistics.mean(times)\n```", "```py\n        import time \n        import threading \n        import queue \n        import urllib.request, urllib.error \n\n        class Receiver(threading.Thread): \n            def __init__(self, queue): \n                threading.Thread.__init__(self)\n                self._queue = queue \n\n            def run(self):\n                while True: \n                    url = self._queue.get() \n                    if isinstance(url, str) and url == 'quit':\n                        break\n                    try:\n                        urllib.request.urlopen(url)\n                    except urllib.error.HTTPError:\n                        pass\n```", "```py\n        def Creator():\n            urls = [\"https://www.python.org\", \n                    \"https://www.google.com\", \n                    \"https://www.techdirt.com\",\n                    \"https://www.facebook.com\",\n                    \"https://www.ibm.com\",\n                    \"https://www.dell.com\",\n                    \"https://www.amd.com\",\n                    \"https://www.yahoo.com\",\n                    \"https://www.microsoft.com\",\n                    \"https://www.apache.org\"]\n\n            cue = queue.Queue()\n            worker_threads = build_worker_pool(cue, 4)\n            start_time = time.time()\n```", "```py\n            for url in urls: \n                cue.put(url) \n\n            for worker in worker_threads:\n                cue.put('quit')\n            for worker in worker_threads:\n                worker.join()\n\n            print('Done! Time taken: {}'.format(time.time() - start_time))\n\n        def build_worker_pool(cue, size):\n            workers = []\n            for _ in range(size):\n                worker = Receiver(cue)\n                worker.start() \n                workers.append(worker)\n            return workers\n\n        if __name__ == '__main__':\n            Creator()\n```", "```py\n        import urllib.request, urllib.error \n        from multiprocessing.dummy import Pool \n        import time\n\n        start_time = time.time()\n\n        urls = [\"https://www.python.org\", \n                \"https://www.google.com\", \n                \"https://www.techdirt.com\",\n                \"https://www.facebook.com\",\n                \"https://www.ibm.com\",\n                \"https://www.dell.com\",\n                \"https://www.amd.com\",\n                \"https://www.yahoo.com\",\n                \"https://www.microsoft.com\",\n                \"https://www.apache.org\"]\n\n        # Make the Pool of workers\n        pool = Pool(4) \n\n        # Open the urls in their own process\n        try:\n            pool.map(urllib.request.urlopen, urls)\n        except urllib.error.HTTPError:\n            pass\n\n        #close the pool and wait for the work to finish \n        pool.close() \n        pool.join() \n\n        print('Done! Time taken: {}'.format(time.time() - start_time))\n```", "```py\npip install comath\n```"]