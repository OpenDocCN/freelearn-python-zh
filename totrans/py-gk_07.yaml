- en: '*Chapter 5*: Testing and Automation with Python'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Software testing is the process of validating an application or a program as
    per user requirements or desired specifications and evaluating the software for
    scalability and optimization goals. Validating software as a real user takes a
    long time and is not an efficient use of human resources. Moreover, testing is
    not performed only one or two times, but it is a continuous process as a part
    of software development. To rescue the situation, test automation is recommended
    for all sorts of testing. **Test automation** is a set of programs written to
    validate an application's behavior using different scenarios as input to these
    programs. For professional software development environments, it is a must that
    automation tests get executed every time the source code is updated (also called
    a **commit operation**) into a central repository.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will study different approaches to automated testing, followed
    by looking at different types of testing frameworks and libraries that are available
    for Python applications. Then, we will focus on unit testing and will look into
    different ways of implementing unit testing in Python. Next, we will study the
    usefulness of **test-driven development** (**TDD**) and the right way to implement
    it. Finally, we will focus on automated **continuous integration** (**CI**) and
    will look into the challenges of implementing it robustly and efficiently. This
    chapter will help you understand the concepts of automated testing in Python at
    various levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding various levels of testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with Python test frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing TDD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing automated CI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of this chapter, you will not only understand different types of
    test automation but will also be able to write unit tests using one of the two
    popular test frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These are the technical requirements for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: You need to have installed Python 3.7 or later on your computer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need to register an account with Test PyPI and create an **application programming
    interface** (**API**) token under your account.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sample code for this chapter can be found at [https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter05](https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter05).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding various levels of testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Testing is performed at various levels based on the application type, its complexity
    level, and the role of the team that is working on the application. The different
    levels of testing include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: System testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acceptance testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These different levels of testing are applied in the order shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Different levels of testing during software development'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_05_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – Different levels of testing during software development
  prefs: []
  type: TYPE_NORMAL
- en: These testing levels are described in the next subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unit testing is a type of testing that is focused on the smallest possible unit
    level. A unit corresponds to a unit of code that can be a function in a module
    or a method in a class, or it can be a module in an application. A unit test executes
    a single unit of code in isolation and validates that the code is working as expected.
    Unit testing is a technique used by developers to identify bugs at the early stages
    of code development and fix them as part of the first iteration of the development
    process. In Python, unit testing mainly targets a particular class or module without
    involving dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests are developed by application developers and can be performed at any
    time. Unit testing is a kind of `pyunit` (`unittest`), `pytest`, `doctest`, `nose`,
    and a few others.
  prefs: []
  type: TYPE_NORMAL
- en: Integration testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Integration testing is about testing individual units of a program collectively
    in the form of a group. The idea behind this type of testing is to test the combination
    of different functions or modules of an application together to validate the interfaces
    between the components and the data exchange between them.
  prefs: []
  type: TYPE_NORMAL
- en: Integration testing is typically done by testers and not by developers. This
    type of testing starts after the unit testing process, and the focus of this testing
    is to identify the integration problem when different modules or functions are
    used together. In some cases, integration testing requires external resources
    or data that may not be possible to provide in a development environment. This
    limitation can be managed by using mock testing, which provides replacement mock
    objects for external or internal dependencies. The mock objects simulate the behavior
    of the real dependencies. Examples of mock testing can be sending an email or
    making a payment using a credit card.
  prefs: []
  type: TYPE_NORMAL
- en: Integration testing is a kind of **black-box testing**. The libraries and the
    tools used for integration testing are pretty much the same as for unit testing,
    with a difference that the boundaries of tests are pushed further out to include
    multiple units in a single test.
  prefs: []
  type: TYPE_NORMAL
- en: System testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The boundaries of system testing are further pushed out to the system level,
    which may be a full-blown module or an application. This type of testing validates
    the application functionality from an **end-to-end** (**E2E**) perspective.
  prefs: []
  type: TYPE_NORMAL
- en: System tests are also developed by testers but after completing the integration
    testing process. We can say that integration testing is a prerequisite for system
    testing; otherwise, a lot of effort will be repeated while performing system testing.
    System testing can identify potential problems but does not pinpoint the location
    of the problem. The exact root cause of the problem is typically identified by
    integration testing or even by adding more unit tests.
  prefs: []
  type: TYPE_NORMAL
- en: System testing is also a type of black-box testing and can leverage the same
    libraries that are available for integration testing.
  prefs: []
  type: TYPE_NORMAL
- en: Acceptance testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Acceptance testing is end-user testing before accepting the software for day-to-day
    use. Acceptance testing is not commonly a candidate for automation testing, but
    it is worth using automation for acceptance testing in situations where application
    users have to interact with the product using an API. This testing is also called
    **user acceptance testing** (**UAT**). This type of testing can be easily mixed
    up with system testing but it is different in that it ensures the usability of
    the application from a real user''s point of view. There are also further two
    types of acceptance testing: **factory acceptance testing** (**FAT**) and **operational
    acceptance testing** (**OAT**). The former is more popular from a hardware point
    of view, and the latter is performed by the operation teams, who are responsible
    for using the product in production environments.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we also hear about **alpha** and **beta** testing. These are also
    user-level testing approaches and are not meant for test automation. Alpha testing
    is performed by developers and internal staff to emulate actual user behavior.
    Beta testing is performed by customers or actual users for early feedback before
    declaring **general availability** (**GA**) of the software.
  prefs: []
  type: TYPE_NORMAL
- en: We also use the term **regression testing** in software development. This is
    basically the execution of tests every time we make a change in the source code
    or any internal or external dependency changes. This practice ensures that our
    product is performing in the same way as it was before making a change. Since
    regression testing is repeated many times, automating the tests is a must for
    this type of testing.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will investigate how to build test cases using the test
    frameworks in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Python test frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Python comes with standard as well as third-party libraries for test automation.
    The most popular frameworks are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pytest`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unittest`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doctest`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nose`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These frameworks can be used for unit testing as well as for integration and
    system testing. In this section, we will evaluate two of these frameworks: `unittest`,
    which is part of the Python standard library, and `pytest`, which is available
    as an external library. The focus of this evaluation will be on building test
    cases (mainly unit tests) using these two frameworks, although the integration
    and system tests can also be built using the same libraries and design patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start writing any test cases, it is important to understand what
    a test case is. In the context of this chapter and book, we can define a test
    case as a way of validating the outcomes of a particular behavior of a programming
    code as per the expected results. The development of a test case can be broken
    down into the following four stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Arrange**: This is a stage where we prepare the environment for our test
    cases. This does not include any action or validation step. In the test automation
    community, this stage is more commonly known as preparing **test fixtures**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Act**: This is the action stage that triggers the system we want to test.
    This action stage results in a change in the system behavior, and the changed
    state of the system is something we want to evaluate for validation purposes.
    Note that we do not validate anything at this stage.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Assert**: At this stage, we evaluate the results of the *act* stage and validate
    the results against the expected outcome. Based on this validation, the test automation
    tools mark the test case as failed or passed. In most of the tools, this validation
    is achieved using built-in *assert* functions or statements.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Cleanup**: At this stage, the environment is cleaned up to make sure the
    other tests are not impacted by the status changes caused by the *act* stage.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The core stages of a test case are *act* and *assert*. The *arrange* and *cleanup*
    stages are optional but highly recommended. These two stages mainly provide software
    test fixtures. A test fixture is a type of equipment or device or software that
    provides an environment to test a device or a machine or software consistently.
    The term *test fixture* is used in the same context for unit testing and integration
    testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The test frameworks or libraries provide helper methods or statements to facilitate
    the implementation of these stages conveniently. In the next sections, we will
    evaluate the `unittest` and the `pytest` frameworks for the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to build base-level test cases for act and assert stages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build test cases with test fixtures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build test cases for exception and error validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to run test cases in bulk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to include and exclude test cases in execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These topics not only cover the development of a variety of test cases but also
    include different ways to execute them. We will start our evaluation with the
    `unittest` framework.
  prefs: []
  type: TYPE_NORMAL
- en: Working with the unittest framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before starting to discuss practical examples with the `unittest` framework
    or library, it is important to introduce a few terms and traditional method names
    related to unit testing and, in particular, to the `unittest` library. This terminology
    is used more or less by all test frameworks and is outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Test case**: A test or test case or test method is a set of code instructions
    that are based on a comparison of the current condition versus the post-execution
    conditions after executing a unit of application code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test suite**: A test suite is a collection of test cases that may have common
    pre-conditions, initialization steps, and perhaps the same cleanup steps. This
    foments reusability of test automation code and reduced execution time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test runner**: This is a Python application that executes the tests (unit
    tests), validates all the assertions defined in the code, and gives the results
    back to us as a success or a failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Setup**: This is a special method in a test suite that will be executed before
    each test case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setupClass`: This is a special method in a test suite that will be executed
    only once at the start of the execution of tests in a test suite.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`teardown`: This is another special method in a test suite that is executed
    after completion of every test regardless of whether the test passes or fails.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`teardownClass`: This is another special method in a test suite that is executed
    only once when all the tests in a suite are completed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To write test cases using the `unittest` library, we are required to implement
    the test cases as instance methods of a class that must be inherited from the
    `TestCase` base class. The `TestCase` class comes with several methods to facilitate
    writing as well as executing the test cases. These methods are grouped into three
    categories, which are discussed next:'
  prefs: []
  type: TYPE_NORMAL
- en: '`setUp`, `tearDown`, `setupClass`, `teardownClass`, `run`, `skipTest`, `skipTestIf`,
    `subTest`, and `debug`. These tests are used by the test runner to execute a piece
    of code before or after a test case or running a set of test cases, running a
    test, skipping a test, or running any block of code as a sub-test. In our test
    case implementation class, we can override these methods. The exact details of
    these methods are available as part of the Python documentation at [https://docs.python.org/3/library/unittest.html](https://docs.python.org/3/library/unittest.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validation methods** (assert methods): These methods are used to implement
    test cases to check for success or failure conditions and report success or failures
    for a test case automatically. These methods'' name typically starts with an *assert*
    prefix. The list of assert methods is very long. We provide commonly used assert
    methods here as examples:![Figure 5.2 – A few examples of assert methods of the
    TestCase class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '](img/B17189_05_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 5.2 – A few examples of assert methods of the TestCase class
  prefs: []
  type: TYPE_NORMAL
- en: '`failureException`: This attribute provides an exception raised by a test method.
    This exception can be used as a superclass to define a custom failure exception
    with additional information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'b) `longMessage`: This attribute determines what to do with a custom message
    that is passed as an argument with an `assert` method. If the value of this attribute
    is set to `True`, the message is appended to the standard failure message. If
    this attribute is set to `false`, a custom message replaces the standard message.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `countTestCases()`: This method returns the number of tests attached to
    a test object.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) `shortDescription()`: This method returns a description of a test method
    if there is any description added, using a docstring.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We have reviewed the main methods of the `TestCase` class in this section. In
    the next section, we will explore how to use `unittest` to build unit tests for
    a sample module or an application.
  prefs: []
  type: TYPE_NORMAL
- en: Building test cases using the base TestCase class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `unittest` library is a standard Python testing framework that is highly
    inspired by the **JUnit** framework, a popular testing framework in the Java community.
    Unit tests are written in separate Python files and it is recommended to make
    the files part of the main project. As we discussed in [*Chapter 2*](B17189_02_Final_PG_ePub.xhtml#_idTextAnchor086),
    *Using Modularization to Handle Complex Projects*, in the *Building a package*
    section, the **Python Packaging Authority** (**PyPA**) guidelines recommend having
    a separate folder for tests when building packages for a project or a library.
    In our code examples for this section, we will follow a similar structure to the
    one shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In our first code example, we will build a test suite for the `add` function
    in the `myadd.py` module, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'It is important to understand that there can be more than one test case for
    the same piece of code (an `add` function, in our case). For the `add` function,
    we implemented four test cases by varying the values of input parameters. Next
    is a code sample with four test cases for the `add` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'All the key points of the preceding test suite are discussed next, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: To implement unit tests using the `unittest` framework, we need to import a
    standard library with the same name, `unittest`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to import the module or modules we want to test in our test suite. In
    this case, we imported the `add` function from the `myadd.py` module using the
    relative import approach (see the *Importing modules* section of [*Chapter 2*](B17189_02_Final_PG_ePub.xhtml#_idTextAnchor086),
    *Using Modularization to Handle Complex Projects,* for details)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will implement a test suite class that is inherited from the `unittest.Testcase`
    base class. The test cases are implemented in the subclass, which is the `MyAddTestSuite`
    class in this case. The `unittest.Testcase` class constructor can take a method
    name as an input that can be used to run the test cases. By default, there is
    a `runTest` method already implemented that is used by the test runner to execute
    the tests. In a majority of the cases, we do not need to provide our own method
    or re-implement the `runTest` method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To implement a test case, we need to write a method that starts with the `test`
    prefix and is followed by an underscore. This helps the test runner to look for
    the test cases to be executed. Using this naming convention, we added four methods
    to our test suite.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In each test-case method, we used a special `assertEqual` method, which is available
    from the base class. This method represents the assert stage of a test case and
    is used to decide if our test will be declared as passed or failed. The first
    parameter of this method is the expected results of the unit test, the second
    parameter is the value that we get after executing the code under test, and the
    third parameter (optional) is the message to be provided in the report in case
    the test is failed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the test suite, we added the `unittest.main` method to trigger
    the test runner to run the `runTest` method, which makes it easy to execute the
    tests without using the commands at the console. This `main` method (a `TestProgram`
    class under the hood) will first discover all the tests to be executed and then
    execute the tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Unit tests can be run using a command such as `Python -m unittest <test suite
    or module>`, but the code examples we provide in this chapter will assume that
    we are running the test cases using the PyCharm **integrated development environment**
    (**IDE**).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we will build the next level of test cases using the test fixtures.
  prefs: []
  type: TYPE_NORMAL
- en: Building test cases with test fixtures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have discussed `setUp` and `tearDown` methods that are run automatically
    by test runners before and after executing a test case. These methods (along with
    the `setUpClass` and `tearDownClass` methods) provide the test fixtures and are
    useful to implement the unit tests efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will revise the implementation of our `add` function. In the new
    implementation, we will make this unit of code a part of the `MyAdd` class. We
    are also handling the situation by throwing a `TypeError` exception in case the
    input arguments are invalid. Next is the complete code snippet with the new `add`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous section, we built test cases using only the act stage and the
    assert stage. In this section, we will revise the previous code example by adding
    `setUp` and `tearDown` methods. Next is the test suite for this `myAdd` class,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In this test suite, we added or changed the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We added a `setUp` method in which we created a new instance of the `MyAdd`
    class and saved its reference as an instance attribute. This means we will be
    creating a new instance of the `MyAdd` class *before* we execute any test case.
    This may not be ideal for this test suite, as a better approach could be to use
    the `setUpClass` method and create a single instance of the `MyAdd` class for
    the whole test suite, but we have implemented it this way for illustration purposes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also added a `tearDown` method. To demonstrate how to implement it, we simply
    called the destructor (using the `del` function) on the `MyAdd` instance that
    we created in the `setUp` method. As with the `setUp` method, the `tearDown` method
    is executed *after* each test case. If we intend to use the `setUpClass` method,
    there is an equivalent method for teardown, which is `tearDownClass`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will present code examples that will build test cases
    to handle a `TypeError` exception.
  prefs: []
  type: TYPE_NORMAL
- en: Building test cases with error handling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous code examples, we only compared the test-case results with the
    expected results. We did not consider any exception handling such as what would
    be the behavior of our program if the wrong types of arguments were passed as
    input to our `add` function. The unit tests have to cover these aspects of the
    programming as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next code example, we will build test cases to handle errors or exceptions
    which are expected from a unit of code. For this example, we will use the same
    `add` function, which throws a `TypeError` exception if the argument is not a
    number. The test cases will be built by passing non-numeric arguments to the `add`
    function. The next code snippet shows the test cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, we added two additional test cases to the `test_add3.py`
    module. These test cases use the `assertRaises` method to validate if a particular
    type of exception is thrown or not. In our test cases, we used a single letter
    (`a`) or two letters (`a` and `b`) as arguments for the two test cases. In both
    cases, we are expecting the intended exception (`TypeError`) to be thrown. It
    is important to note the arguments of the `assertRaises` method. This method expects
    only the method or function name as a second argument. The parameters of the method
    or function have to be passed separately as arguments of the `assertRaises` function.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have executed multiple test cases under a single test suite. In the
    next section, we will discuss how we can run multiple test suites simultaneously,
    using the command line and also programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: Executing multiple test suites
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we built test cases for each unit of code, the number of test cases (unit
    test cases) grows very quickly. The idea of using test suites is to bring modularity
    into the test-case development. Test suites also make it easier to maintain and
    extend the test cases as we add more functionality to an application. The next
    aspect that comes to our mind is how to execute multiple test suites through a
    master script or a workflow. CI tools such as Jenkins provides such functionality
    out of the box. Test frameworks such as `unittest`, `nose`, or `pytest` also provide
    similar features.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will build a simple calculator application (a `MyCalc`
    class) with `add`, `subtract`, `multiply`, and `divide` methods in it. Later,
    we will add one test suite for each method in this class. This way, we will add
    four test suites for this calculator application. A directory structure is important
    in implementing the test suites and test cases. For this application, we will
    use the following directory structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Directory structure for the mycalc application and test suites
    associated with this application'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_05_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 – Directory structure for the mycalc application and test suites
    associated with this application
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python code is written in the `mycalc.py` module and the test suite files
    (`test_mycalc*.py`) are shown next. Note that we show only one test case in each
    test suite in the code examples shown next. In reality, there will be multiple
    test cases in each test suite. We will start with the calculator functions in
    the `mycalc.py` file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we have a test suite to test the `add` function in the `test_mycalc_add.py`
    file, as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we have a test suite to test the `subtract` function in the `test_mycalc_subtract.py`
    file, as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we have a test suite to test the `multiply` function in the `test_mycalc_multiply.py`
    file, as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we have a test suite to test the `divide` function in the `test_mycalc_divide.py`
    file, as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We have the sample application code and all four test suites'' code. The next
    aspect is how to execute all the test suites in one go. One easy way to do this
    is by using the `discover` keyword. In our example case, we will run the following
    command from the top of the project to discover and execute all test cases in
    all the four test suites that are available in the `tests_mycalc` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will be executed recursively, which means it can discover the
    test cases in sub-directories as well. The other (optional) parameters can be
    used to select a set of test cases for execution, and these are described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-v`: To make the output verbose.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-s`: Start directory for the discovery of test cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-p`: Pattern to use for searching the test files. The default is `test*.py`,
    but it can be changed by this parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-t`: This is a top-level directory of the project. If not specified, the start
    directory is the top-level directory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Although the command-line option of running multiple test suites is simple
    and powerful, we sometimes need to control the way we run selected tests from
    different test suites that may be in different locations. This is where loading
    and executing the test cases through the Python code is handy. The next code snippet
    is an example of how to load the test suites from a class name, find the test
    cases in each of the suites, and then run them using the `unittest` test runner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we have covered building test cases using the `unittest` library.
    In the next section, we will work with the `pytest` library.
  prefs: []
  type: TYPE_NORMAL
- en: Working with the pytest framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The test cases written using the `unittest` library are easier to read and manage,
    especially if you are coming from a background of using JUnit or other similar
    frameworks. But for large-scale Python applications, the `pytest` library stands
    out as one of the most popular frameworks, mainly because of its ease of use in
    implementation and its ability to extend for complex testing requirements. In
    the case of the `pytest` library, there is no requirement to extend the unit test
    class from any base class; in fact, we can write the test cases without even implementing
    any class.
  prefs: []
  type: TYPE_NORMAL
- en: '`pytest` is an open source framework. The `pytest` test framework can auto-discover
    tests, just as with the `unittest` framework, if the filename has a `test` prefix,
    and this discovery format is configurable. The `pytest` framework includes the
    same level of functionality as it is provided by the `unittest` framework for
    writing unit tests. In this section, we will focus on discussing the features
    that are different or additional in the `pytest` framework.'
  prefs: []
  type: TYPE_NORMAL
- en: Building test cases without a base class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To demonstrate how to write unit test cases using the `pytest` library, we
    will revise our `myadd2.py` module by implementing the `add` function without
    a class. This new `add` function will add two numbers and throw an exception if
    the *numbers* are not passed as arguments. The test-case code using the `pytest`
    framework is shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'And the test cases'' module is shown next, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We only showed two test cases for the `test_myadd3.py` module as the other
    test cases will be similar to the first two test cases. These additional test
    cases are available with this chapter''s source code under the GitHub directory.
    A couple of key differences in the test case implementation are outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: There is no requirement to implement test cases under a class, and we can implement
    test cases as class methods without inheriting them from any base class. This
    is a key difference in comparison to the `unittest` library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `assert` statements are available as a keyword for validation of any condition
    to declare whether a test passed or failed. Separating `assert` keywords from
    the conditional statement makes assertions in test cases very flexible and customizable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is also important to mention that the console output and the reporting is
    more powerful with the `pytest` framework. As an example, the console output of
    executing test cases using the `test_myadd3.py` module is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will investigate how to validate expected errors using the `pytest`
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Building test cases with error handling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Writing test cases to validate the throwing of an expected exception or error
    is different in the `pytest` framework as compared to writing such test cases
    in the `unittest` framework. The `pytest` framework utilizes the context manager
    for exception validation. In our `test_myadd3.py` test module, we already added
    two test cases for exception validation. An extract of the code in the `test_myadd3.py`
    module with the two test cases is shown next, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: To validate the exception, we are using the `raises` function of the `pytest`
    library to indicate what sort of exception is expected by running a certain unit
    of code (`add('a', 5)` in our first test case). In the second test case, we used
    a `match` argument to validate the message that is set when an exception is thrown.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss how to use markers with the `pytest` framework.
  prefs: []
  type: TYPE_NORMAL
- en: Building test cases with pytest markers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `pytest` framework is equipped with markers that allow us to attach metadata
    or define different categories for our test cases. This metadata can be used for
    many purposes, such as including or excluding certain test cases. The markers
    are implemented using the `@pytest.mark` decorator.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `pytest` framework provides a few built-in markers, with the most popular
    ones being described next:'
  prefs: []
  type: TYPE_NORMAL
- en: '`skip`: The test runner will skip a test case unconditionally when this marker
    is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skipif`: This marker is used to skip a test based on a conditional expression
    that is passed as an argument to this marker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`xfail`: This marker is used to ignore an expected failure in a test case.
    It is used with a certain condition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`parametrize`: This marker is used to perform multiple calls to the test case
    with different values as arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To demonstrate the use of the first three markers, we rewrite our `test_add3.py`
    module by adding markers with the test-case functions. The revised test-case module
    (`test_add4.py`) is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We used the `skip` marker unconditionally for the first test case. This will
    ignore the test case. For the second test case, we used the `skipif` marker with
    a condition of a Python version greater than 3.6\. For the last test case, we
    deliberately raised an exception, and we used the `xfail` marker to ignore this
    type of exception if the system platform is Windows. This type of marker is helpful
    for ignoring errors in test cases if they are expected for a certain condition,
    such as the operating system in this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'The console output from the execution of the test cases is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will discuss the use of the `parametrize` marker with the `pytest`
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Building test cases with parametrization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In all previous code examples, we built test-case functions or methods without
    passing any parameters to them. But for many test scenarios, we need to run the
    same test case by varying the input data. In a classical approach, we run multiple
    test cases that are different only in terms of the input data we used for them.
    Our previous example of `test_myadd3.py` shows how to implement test cases using
    this classical approach. A recommended approach for such type of testing is to
    use `pytest` will execute our test case as many times as the number of permutations
    is in the table or the dictionary. A real-world example of DDT is to validate
    the behavior of a login feature of an application by using a variety of users
    with valid and invalid credentials.
  prefs: []
  type: TYPE_NORMAL
- en: In the `pytest` framework, DDT can be implemented using parametrization with
    the `pytest` marker. By using the `parametrize` marker, we can define which input
    argument we need to pass and also the test dataset we need to use. The `pytest`
    framework will automatically execute the test-case function multiple times as
    per the number of entries in the test data provided with the `parametrize` marker.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate how to use the `parametrize` marker for DDT, we will revise our
    `myadd4.py` module for the test cases of the `add` function. In the revised code,
    we will have only one test-case function but different test data to be used for
    the input parameters, as illustrated in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'For the `parametrize` marker, we used three parameters, which are described
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Test-case arguments**: We provide a list of arguments to be passed to our
    test function in the same order as defined with the test-case function definition.
    Also, the test data we need to provide in the next argument will follow the same
    order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data**: The test data to be passed will be a list of different sets of input
    arguments. The number of entries in the test data will determine how many times
    the test case will be executed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ids`: This is an optional parameter that is mainly attaching a friendly tag
    to different test datasets we provided in the previous argument. These **identifier**
    (**ID**) tags will be used in the output report to identify different executions
    of the same test case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The console output for this test-case execution is shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This console output shows us how many times the test case is executed and with
    which test data. The test cases built using the `pytest` markers are concise and
    easy to implement. This saves a lot of time and enables us to write more test
    cases (by varying data only) in a short time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will discuss another important feature of the `pytest` library: fixtures.'
  prefs: []
  type: TYPE_NORMAL
- en: Building test cases with pytest fixtures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the `pytest` framework, the test fixtures are implemented using Python decorators
    `(@pytest.fixture`). The implementation of test fixtures in the `pytest` framework
    is very powerful as compared to the other frameworks for the following key reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Fixtures in the `pytest` framework provide high scalability. We can define a
    generic setup or fixtures (methods) that can be reused across functions, classes,
    modules, and packages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fixture implementation of the `pytest` framework is modular in nature. We can
    use one or more fixtures with a test case. A fixture can use one or many other
    fixtures as well, just as we use functions to call other functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each test case in a test suite will have the flexibility to use the same or
    a different set of fixtures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can create fixtures in the `pytest` framework with a scope set for them.
    The default scope is `function`, which means the fixture will be executed before
    every function (test case). Other scope options are `module`, `class`, `package`,
    or `session`. These are defined briefly next:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a) `Function`: The fixture is destroyed after executing a test case.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `Module`: The fixture is destroyed after executing the last test case in
    a module.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `Class`: The fixture is destroyed after executing the last test case in
    a class.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) `Package`: The fixture is destroyed after executing the last test case in
    a package.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'e) `Session`: The fixture is destroyed after executing the last test case in
    a test session.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `pytest` framework has a few useful built-in fixtures that can be used out
    of the box, such as `capfd` to capture output to the file descriptors, `capsys`
    to capture output to `stdout` and `stderr`, `request` to provide information on
    the requesting test function, and `testdir` to provide a temporary test directory
    for test executions.
  prefs: []
  type: TYPE_NORMAL
- en: Fixtures in the `pytest` framework can be used to reset or tear down at the
    end of a test case as well. We will discuss this later on in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next code example, we will build test cases for our `MyCalc` class using
    custom fixtures. The sample code for `MyCalc` is already shared in the *Executing
    multiple test suites* section. The implementation of a test fixture and test cases
    is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In this test-suite example, these are the key points of discussion:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We created two fixtures: `my_calc` and `test_data`. The `my_calc` fixture is
    set with a scope set to `module` because we want it to be executed only once to
    provide an instance of the `MyCalc` class. The `test_data` fixture is using the
    default scope (`function`), which means it will be executed before every method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the test cases (`test_add` and `test_subtract`), we used the fixtures as
    input arguments. The name of the argument has to match the fixture function name.
    The `pytest` framework automatically looks for a fixture with the name used as
    an argument for a test case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code example we discussed is using a fixture as the setup function. A question
    we may want to ask is: *How we can achieve teardown functionality with the pytest
    fixtures?* There are two approaches available for implementing the teardown functionality,
    and these are discussed next.'
  prefs: []
  type: TYPE_NORMAL
- en: Using yield instead of a return statement
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With this approach, we write some code mainly for setup purposes, use a `yield`
    statement instead of `return`, and then write code for teardown purposes after
    the `yield` statement. If we have a test suite or module with many fixtures used
    in it, the `pytest` test runner will execute each fixture (as per the evaluated
    order of execution) till the `yield` statement is encountered. As soon as the
    test-case execution is completed, the `pytest` test runner triggers the execution
    of all fixtures that are yielded and executes the code that is written after the
    `yield` statement. The use of a yield-based approach is clean in the sense that
    the code is easy to follow and maintain. Therefore, it is a recommended approach.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a finalizer method using the request fixture
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'With this approach, we have to consider three steps to write a teardown method,
    outlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We have to use a `request` object in our fixtures. The `request` object can
    be provided using the built-in fixture with the same name.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will define a `teardown` method, separately or as a part of the fixture implementation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will provide the `teardown` method as a callable method to the request object
    using the `addfinalizer` method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To illustrate both approaches with code examples, we will modify our previous
    implementation of the fixtures. In the revised code, we will implement the `my_calc`
    fixture using a `yield` approach and the `data_set` fixture using an `addfinalizer`
    approach. Here is the revised code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note that there is no real need for teardown functionality for these example
    fixtures, but we added them for illustration purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Using `nose` and `doctest` for test automation is similar to using the `unittest`
    and `pytest` frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss a TDD approach to software development.
  prefs: []
  type: TYPE_NORMAL
- en: Executing TDD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TDD is a well-known practice in software engineering. This is a software development
    approach in which test cases are written first before writing any code for a required
    feature in an application. Here are the three simple rules of TDD:'
  prefs: []
  type: TYPE_NORMAL
- en: Do not write any functional code unless you write a unit test that is failing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not write any additional code in the same test more than you need to make
    the test fail.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not write any functional code more than what is needed to pass a failing
    test.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These TDD rules also drive us to follow a famous three-phase approach of software
    development called **Red, Green, Refactor**. The phases are repeated continuously
    for TDD. These three phases are shown in *Figure 5.4* and are described next.
  prefs: []
  type: TYPE_NORMAL
- en: Red
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this phase, the first step is to write a test without having any code to
    test. The test will obviously fail in this case. We will not try to write a complete
    test case but only write enough code to fail the test.
  prefs: []
  type: TYPE_NORMAL
- en: Green
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this phase, the first step is to write the code until an already written
    test passes. Again, we will only write enough code to pass the test. We will run
    all tests to make sure previously written tests also pass.
  prefs: []
  type: TYPE_NORMAL
- en: Refactor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this phase, we should consider improving the quality of the code, which means
    making the code easy to read and use optimization—for example, any hardcoded values
    have to be removed. Running the tests after each refactoring cycle is also recommended.
    The outcome of the refactor phase is clean code. We can repeat the cycle by adding
    more test scenarios and adding code to make the new test pass, and this cycle
    must be repeated until a feature is developed.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to understand that TDD is neither a testing nor a design approach.
    It is an approach to developing software according to specifications that are
    defined by writing test cases first.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the three phases of TDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – TDD, also known as Red, Green, Refactor'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_05_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.4 – TDD, also known as Red, Green, Refactor
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will introduce the role of test automation in the CI
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing automated CI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CI is a process that combines the benefits of both automated testing and version
    control systems to achieve a fully automated integration environment. With a CI
    development approach, we integrate our code into a shared repository frequently.
    Every time we add our code to a repository, the following two processes are expected
    to kick in:'
  prefs: []
  type: TYPE_NORMAL
- en: An automated build process starts to validate that the newly added code is not
    breaking anything from a compilation or syntax point of view.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An automated test execution starts to verify that the existing, as well as new
    functionality is as per the test cases defined.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The different steps and phases of the CI process are depicted in the following
    diagram. Although we have shown the build phase in this flowchart, it is not a
    required phase for Python-based projects as we can execute integration tests without
    compiled code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Phases of CI testing'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_05_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.5 – Phases of CI testing
  prefs: []
  type: TYPE_NORMAL
- en: To build a CI system, we need to have a stable distributed version control and
    a tool that can be used to implement workflow for testing the whole application
    through a series of test suites. There are several commercial and open source
    software tools available that provide CI and **continuous delivery** (**CD**)
    functionality. These tools are designed for easy integration with a source control
    system and with a test automation framework. A few popular tools available for
    CI are *Jenkins*, *Bamboo*, *Buildbot*, *GitLab CI*, *CircleCI,* and *Buddy*.
    Details of these tools appear in the *Further reading* section, for those of you
    who are interested to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: The obvious benefits of this automated CI are to detect errors quickly and fix
    them more conveniently right at the beginning. It is important to understand that
    CI is not about bug fixing, but it definitely helps to identify bugs easily and
    get them fixed promptly.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced different levels of testing for software applications.
    We also evaluated two test frameworks (`unittest` and `pytest`) that are available
    for Python test automation. We learned how to build basic- and advanced-level
    test cases using these two frameworks. Later in the chapter, we introduced the
    TDD approach and its clear benefits for software development. Finally, we touched
    base on the topic of CI, which is a key step in delivering software using **agile**
    and **development-operations** (**devops**) models.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is useful for anyone who wants to start writing unit tests for
    their Python application. The code examples provided provide a good starting point
    for us to write test cases using any test framework.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore different tricks and tips for developing
    applications in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Is unit testing a form of white-box or black-box testing?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When should we use mock objects?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which methods are used to implement test fixtures with the `unittest` framework?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is TDD different from CI?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When should we use DDT?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Learning Python Testing*, by *Daniel Arbuckle*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Test-Driven Development with Python*, by *Harry J.W. Percival*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Expert Python Programming*, by *Michał Jaworski and Tarek Ziadé*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*unittest* framework details are available with the Python documentation at
    [https://docs.python.org/3/library/unittest.html](https://docs.python.org/3/library/unittest.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: White-box testing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mock objects help simulate the behavior of external or internal dependencies.
    By using mock objects, we can focus on writing tests for validating functional
    behavior.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`setUp`, `tearDown`, `setUpClass`, `tearDownClass`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: TDD is an approach to developing software by writing the test cases first. CI
    is a process in which all the tests are executed every time we build a new release.
    There is no direct relationship between TDD and CI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DDT is used when we have to do functional testing with several permutations
    of input parameters. For example, if we are required to test an API endpoint with
    a different combination of arguments, we can leverage DDT.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
