- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What's Next?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we have discussed the design and development of microservices
    written in Python using the Quart framework. We have built a monolithic application
    from which to work, and covered strategies to migrate from that architecture to
    one that makes the best use of microservices, along with the potential errors
    that could arise and how to avoid them. We have also learned about deploying our
    application to cloud providers using container-based services.
  prefs: []
  type: TYPE_NORMAL
- en: However, this is not the end of the story, and there are other topics that are
    beneficial to learn more about. There is always going to be more room for improvement
    in our automation and tooling to help services keep up to date, more questions
    to answer about performance and capacity management that our monitoring and logging
    can help with, and considerations about how to scale and change our deployment
    architecture to improve the service's reliability and availability. Finally, we
    need to remember that – unless writing code for a hobby – the software itself
    is not the end goal, and we must keep our promises to the people who need the
    software.
  prefs: []
  type: TYPE_NORMAL
- en: Automation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We briefly discussed **Terraform** as a way to automate the creation of cloud-based
    resources, and there is a lot more to learn about this tool as well as others
    that can automate some of the work involved in running a service.
  prefs: []
  type: TYPE_NORMAL
- en: To configure inside an instance, configuration management tools such as **Ansible**,
    **Chef**, and **Puppet** allow you to copy files, change file contents, install
    packages, and set up a computer how you like it in a repeatable, predictable manner.
  prefs: []
  type: TYPE_NORMAL
- en: Building operating system images for your own environment can be done with HashiCorp's
    **Packer**, which lets you use the configuration management tools above to create
    operating system images for use in AWS, GCP, VMware, or Docker, among many others.
  prefs: []
  type: TYPE_NORMAL
- en: Even if your infrastructure is small, using automation to create and maintain
    it is still valuable. In the event of a disaster, you are a few short commands
    away from recreating your entire suite of applications, instead of weeks of painstaking
    work.
  prefs: []
  type: TYPE_NORMAL
- en: 'When creating infrastructure as code, it''s very easy to accidentally create
    a new monolith, responsible for creating and maintaining every component. If that
    is a deliberate, considered choice then it will work well, but it''s also worth
    remembering the other principles of privilege separation and ease of maintenance
    that come with separating out the features into smaller projects. Here are some
    relevant links:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Terraform: [https://www.terraform.io/](https://www.terraform.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ansible: [https://www.ansible.com/](https://www.ansible.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chef: [https://www.chef.io/](https://www.chef.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Puppet: [https://puppet.com/](https://puppet.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When an application needs to do more work, the historical approach has been
    to run the application on a bigger computer. Give it more memory, more CPU cores,
    and even more disk space. This does not increase the application's reliability,
    as it still relies on a single computer, and it comes with added complications
    once your application is large enough that there simply aren't any computers large
    enough to run it on.
  prefs: []
  type: TYPE_NORMAL
- en: Giving a program a larger computer to run on is called scaling vertically. By
    contrast, scaling horizontally is the approach of using many smaller computers.
    We came across this idea when discussing deploying on container-based services
    and increasing the number of instances that our Docker swarm used. An application
    must have a replicated, scalable idea of its current state to operate in this
    way, for client sessions, shopping basket contents, and anything else that a visitor
    would expect to be persistent between different pages of a website.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices allow you to scale an application much more easily, although it
    is important to remember that every component communicates with other microservices
    and that an increased load in one area will have consequences in others.
  prefs: []
  type: TYPE_NORMAL
- en: Careful monitoring will allow you to discover the bottlenecks in the overall
    system, and so prioritize which area needs the most urgent work in order to give
    the system more capacity.
  prefs: []
  type: TYPE_NORMAL
- en: Content Delivery Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some of the content our applications deliver does not change very often, such
    as HTML pages, JavaScript, images, and video streams. **Content Delivery Networks**
    (**CDNs**) aim to provide static content that is distributed around the world.
    Acting either as a layer in front of your application or alongside it, they can
    provide cacheable content to clients much more quickly than a customized service.
    Some CDNs will also allow you to dynamically scale images and video based on the
    client and its network quality, or provide protection against distributed denial
    of service attacks, making them a valuable tool for any web-based service.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-cloud deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When assessing the risks involved in running a service, it's easy to come to
    the realization that your organization is completely dependent on one cloud provider.
    A common desire to improve redundancy is to deploy services to multiple providers
    and spread the workload across Azure, GCP, Amazon, and others. This might seem
    like a great idea, but it also introduces a lot of complexity as different providers
    have different feature sets available, will need unique security arrangements,
    and be unable to share storage and secrets management.
  prefs: []
  type: TYPE_NORMAL
- en: While `Terraform` can help with this situation, it is often more achievable
    to aim for multiple regions within the same provider, and if several cloud providers
    are really required, to separate what's running in them based on how things interact.
    It's far easier to put a completely independent service somewhere else. There
    are parallels with the strategic approach and splitting a monolith into microservices,
    as a successful migration requires a clean interface between different components
    and well-structured isolation of concerns and requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Lambda Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lambda, or Cloud Functions, is a type of serverless deployment intended for
    small, short-lived tasks that can scale up and down very rapidly. While asynchronous
    frameworks have limited support in this area in 2021, they are widely used with
    synchronous code as the way they are run means that the responsiveness is controlled
    by the sheer number of them that can run simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: Expanding monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapter 5*, *Splitting the Monolith*, we discussed monitoring and collecting
    metrics to record what an application is doing. Measurements can tell some of
    the story and give a picture involving a count, a size, or time passing. To get
    even more information, we can use logging services to record messages our application
    produces.
  prefs: []
  type: TYPE_NORMAL
- en: If you have set up a Linux server, you may be familiar with the logs that pass
    through `rsyslog` and end up in a file that exists in `/var/log`. In a cloud service,
    and especially in a container, logging locally is far less useful, as we would
    have to then investigate all the running containers and cloud instances to discover
    what was happening. Instead, we can use a centralized logging service.
  prefs: []
  type: TYPE_NORMAL
- en: This could be done using tools such as AWS CloudWatch or Google's Cloud Logging,
    but it's also possible to run services such as `Splunk` or `Logstash`. The latter
    is part of a popular open source trio of tools called the `ELK` stack, as it contains
    Elasticsearch, Logstash, and Kibana, to collect, search, and visualize logged
    data. Using these tools, all the logs from the systems and applications can end
    up in a single place and be easily examined.
  prefs: []
  type: TYPE_NORMAL
- en: Using structured logging techniques, it is also straightforward to annotate
    all the log entries to easily determine which microservice produced them, and
    so to better correlate events. A centralized logging service will allow you to
    connect the dots between errors in one component and reports from a separate area.
    At the same time, each microservice being more isolated means that any impact
    they have on other components should be through the designed interfaces, instead
    of being side effects due to resource constraints on the same server, or in the
    same process tree.
  prefs: []
  type: TYPE_NORMAL
- en: The ELK stack is a great starting point for collecting large numbers of logs
    and metrics, and you can discover more about it at [https://www.elastic.co/what-is/elk-stack](https://www.elastic.co/what-is/elk-stack).
  prefs: []
  type: TYPE_NORMAL
- en: Making promises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When writing software, we are often not doing so in isolation, but instead to
    help our company or open source project achieve a goal. Relying on our intuition
    to tell us whether we're doing a good job is often misleading, as our instinct
    is affected by all the different biases humans have. Instead, we must measure
    – collect numbers, watch for patterns, and analyze data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate how well our software is doing, both to ourselves and to others,
    there are three levels we can think about. The first is the list of possible things
    we can measure, and these are known as **Service-Level Indicators** (**SLIs**).
    As developers, it is easy to come up with a list of technology-related SLIs, such
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: The API response time in milliseconds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A count of the different HTTP status codes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of bytes transferred in each request
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, it is vitally important to include organization-level indicators as
    well, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: How long an online shop's check-out process takes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many potential customers abandon a purchase during check-out
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The financial cost of running a service, especially one that automatically scales
    up and down
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Both types of indicators, when used together, can make for very useful reports
    and dashboards for an organization, but you also don''t want to be constantly
    checking on things – there is other work to be done! A **Service-Level Objective**
    (**SLO**) sets a threshold or alert value on top of an SLI, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Fewer than 1% of HTTP status codes must indicate a server error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rate of completed check-out operations must exceed 75%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Users can successfully complete at least 99.9% of their requests without an
    error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What should we do if an SLO is not met? That's where **Service-Level Agreements**
    (**SLAs**) come in. SLAs are a contract – official or otherwise – between the
    providers of a service and the people using it and describe what should happen
    when an SLO is not met.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example covering all the levels:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Service-level indicator: The number of HTTP 500 errors recorded'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Service-level objective: The HTTP 500 errors should not be more than 1% of
    the total requests'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Service-level agreement: A site reliability engineer is alerted and affected
    customers are informed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating SLOs helps developers and product team members understand what's important
    about an application and lets us demonstrate to everyone involved that the application
    is doing what it is meant to do.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As software developers we never stop improving our skills and knowledge, trying
    out new technologies and architectures, and building on the work of many others.
    Our profession's core skill is approaching a situation in a rational and methodical
    manner, breaking down each part of the problem into manageable chunks, and making
    sure that we – and others who have a stake in our work – can make sense of it
    all.
  prefs: []
  type: TYPE_NORMAL
- en: The microservices approach uses the same techniques in systems design, making
    each component easier to reason about and investigate. Like many approaches, it
    works very well when it is done with careful consideration, rather than a desire
    to follow a fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Designing applications well takes a combination of knowledge, skill, and experience,
    and we hope that this book has contributed to the expertise that you bring to
    your work, whether it's paid, volunteering, or a hobby.
  prefs: []
  type: TYPE_NORMAL
