<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Implementing Concurrency</h1>
            </header>

            <article>
                
<p>So far, we have explored how to measure and improve the performance of programs by reducing the number of operations performed by the CPU through clever algorithms and more efficient machine code. In this chapter, we will shift our focus to programs where most of the time is spent waiting for resources that are much slower than the CPU, such as persistent storage and network resources.</p>
<p>Asynchronous programming is a programming paradigm that helps to deal with slow and unpredictable resources (such as users) and is widely used to build responsive services and user interfaces. In this chapter, we will show you how to program asynchronously in Python using techniques such as coroutines and reactive programming.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>The memory hierarchy</li>
<li>Callbacks</li>
<li>Futures</li>
<li>Event loops</li>
<li>Writing coroutines with <kbd>asyncio</kbd></li>
<li>Converting synchronous code to asynchronous code</li>
<li>Reactive programming with RxPy</li>
<li>Working with observables</li>
<li>Building a memory monitor with RxPY</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Asynchronous programming</h1>
            </header>

            <article>
                
<p>Asynchronous programming is a way of dealing with slow and unpredictable resources. Rather than waiting idle for resources to become available, asynchronous programs are able to handle multiple resources concurrently and efficiently. Programming in an asynchronous way can be challenging because it is necessary to deal with external requests that can arrive in any order, may take a variable amount of time, or may fail unpredictably. In this section, we will introduce the topic by explaining the main concepts and terminology as well as by giving an idea of how asynchronous programs work.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Waiting for I/O</h1>
            </header>

            <article>
                
<p>A modern computer employs different kinds of memory to store data and perform operations. In general, a computer possesses a combination of expensive memory that is capable of operating at fast speeds and cheaper, and more abundant memory that operates at lower speeds and is used to store a larger amount of data.</p>
<p>The memory hierarchy is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full image-border" height="90" src="assets/B06440_06CHPNO_01.png" width="528"/></div>
<p>At the top of the memory hierarchy are the CPU registers. Those are integrated in the CPU and are used to store and execute machine instructions. Accessing data in a register generally takes one clock cycle. This means that if the CPU operates at 3 GHz, the time it takes to access one element in a CPU register is in the order of 0.3 nanoseconds.</p>
<p>At the layer just below the <strong>registers</strong>, you can find the CPU cache, which is comprised of multiple levels and is integrated in the processor. The <strong>cache</strong> operates at a slightly slower speed than the <strong>registers</strong> but within the same order of magnitude.</p>
<p>The next item in the hierarchy is the main memory (<strong>RAM</strong>), which holds much more data but is slower than the cache. Fetching an item from memory can take a few hundred clock cycles.</p>
<p>At the bottom layer, you can find persistent storage, such as a rotating disks (HDD) and <strong>Solid State Drives</strong> (<strong>SSD</strong>). These devices hold the most data and are orders of magnitude slower than the main memory. An HDD may take a few milliseconds to seek and retrieve an item, while an SSD is substantially faster and takes only a fraction of a millisecond.</p>
<p>To put the relative speed of each memory type into perspective, if you were to have the CPU with a clock speed of about one second, a register access would be equivalent to picking up a pen from the table. A cache access will be equivalent to picking up a book from the shelf. Moving higher in the hierarchy, a RAM access will be equivalent to loading up the laundry (about twenty x slower than the cache). When we move to persistent storage, things are quite a bit different. Retrieving an element from an SSD will be equivalent to doing a four day trip, while retrieving an element from an HDD can take up to six months! The times can stretch even further if we move on to access resources over the network.</p>
<p>From the preceding example, it should be clear that accessing data from storage and other I/O devices is much slower compared to the CPU; therefore, it is very important to handle those resources so that the CPU is never stuck waiting aimlessly. This can be accomplished by carefully designing software capable of managing multiple, ongoing requests at the same time.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Concurrency</h1>
            </header>

            <article>
                
<p>Concurrency is a way to implement a system that is able to deal with multiple requests at the same time. The idea is that we can move on and start handling other resources while we wait for a resource to become available. Concurrency works by splitting a task into smaller subtasks that can be executed out of order so that multiple tasks can be partially advanced without waiting for the previous tasks to finish.  </p>
<p>As a first example, we will describe how to implement concurrent access to a slow network resource. Let's say we have a web service that takes the square of a number, and the time between our request and the response will be approximately one second.  We can implement the <kbd>network_request</kbd> function that takes a number and returns a dictionary that contains information about the success of the operation and the result. We can simulate such services using the <kbd>time.sleep</kbd> function, as follows:</p>
<pre>
    import time<br/><br/>    def network_request(number):<br/>        time.sleep(1.0)<br/>        return {"success": True, "result": number ** 2}
</pre>
<p>We will also write some additional code that performs the request, verifies that the request was successful, and prints the result. In the following code, we define the <kbd>fetch_square</kbd> function and use it to calculate the square of the number two using a call to <kbd>network_request</kbd>:</p>
<pre>
    def fetch_square(number):<br/>        response = network_request(number)<br/>        if response["success"]:<br/>            print("Result is: {}".format(response["result"]))<br/><br/>    fetch_square(2)<br/>    # Output:<br/>    # Result is: 4
</pre>
<p>Fetching a number from the network will take one second because of the slow network. What if we want to calculate the square of multiple numbers? We can call <kbd>fetch_square</kbd>, which will start a network request as soon as the previous one is done:</p>
<pre>
    fetch_square(2)<br/>    fetch_square(3)<br/>    fetch_square(4)<br/>    # Output:<br/>    # Result is: 4<br/>    # Result is: 9<br/>    # Result is: 16
</pre>
<p>The previous code will take three seconds to run, but it's not the best we can do. Waiting for the previous result to finish is unnecessary as we can technically submit multiple requests at and wait for them parallely.</p>
<p>In the following diagram, the three tasks are represented as boxes. The time spent by the CPU processing and submitting the request is in orange while the waiting times are in blue. You can see how most of the time is spent waiting for the resources while our machine sits idle without doing anything else:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full image-border" height="76" src="assets/B06440_06CHPNO_02.png" width="317"/></div>
<p>Ideally, we would like to start other new task while we are waiting for the already submitted tasks to finish. In the following figure, you can see that as soon as we submit our request in <strong>fetch_square(2)</strong>, we can start preparing for <strong>fetch_square(3)</strong> and so on. This allows us to reduce the CPU waiting time and to start processing the results as soon as they become available:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full image-border" height="120" src="assets/B06440_06CHPNO_03.png" width="267"/></div>
<p>This strategy is made possible by the fact that the three requests are completely independent, and we don't need to wait for the completion of a previous task to start the next one. Also, note how a single CPU can comfortably handle this scenario. While distributing the work on multiple CPUs can further speedup the execution, if the waiting time is large compared to the processing times, the speedup will be minimal.</p>
<p>To implement concurrency, it is necessary to think and code differently; in the following sections, we'll demonstrate techniques and best practices to implement robust concurrent applications.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Callbacks</h1>
            </header>

            <article>
                
<p>The code we have seen so far blocks the execution of the program until the resource is available. The call responsible for the waiting is <kbd>time.sleep</kbd>. To make the code start working on other tasks, we need to find a way to avoid blocking the program flow so that the rest of the program can go on with the other tasks.</p>
<p>One of the simplest ways to accomplish this behavior is through callbacks. The strategy is quite similar to what we do when we request a cab.</p>
<p>Imagine that you are at a restaurant and you've had a few drinks. It's raining outside, and you'd rather not take the bus; therefore, you request a taxi and ask them to call when they're outside so that you can come out, and you don't have to wait in the rain.</p>
<p>What you did in this case is request a taxi (that is, the slow resource) but instead of waiting outside until the taxi arrives, you provide your number and instructions (callback) so that you can come outside when they're ready and go home.</p>
<p>We will now show how this mechanism can work in code. We will compare the blocking code of <kbd>time.sleep</kbd> with the equivalent non-blocking code of <kbd>threading.Timer</kbd>.</p>
<p>For this example, we will write a function, <kbd>wait_and_print</kbd>, that will block the program execution for one second and then print a message:</p>
<pre>
    def wait_and_print(msg):<br/>        time.sleep(1.0)<br/>        print(msg)
</pre>
<p>If we want to write the same function in a non-blocking way, we can use the <kbd>threading.Timer</kbd> class. We can initialize a <kbd>threading.Timer</kbd> instance by passing the amount of time we want to wait and a callback. A <strong>callback</strong> is simply a function that will be called when the timer expires. Note that we have to also call the <kbd>Timer.start</kbd> method to activate the timer:</p>
<pre>
    import threading<br/><br/>    def wait_and_print_async(msg):<br/>        def callback():<br/>            print(msg)<br/><br/>        timer = threading.Timer(1.0, callback)<br/>        timer.start()
</pre>
<p>An important feature of the <kbd>wait_and_print_async</kbd> function is that none of the statements are blocking the execution flow of the program.</p>
<div class="packt_tip">How is <kbd>threading.Timer</kbd> capable of waiting without blocking?<br/>
The strategy used by <kbd>threading.Timer</kbd> involves starting a new thread that is able to execute code in parallel. If this is confusing, don't worry, we will explore threading and parallel programming in detail in the following chapters.</div>
<p>This technique of registering callbacks for execution in response to certain events is commonly called the <em>Hollywood principle</em>. This is because, after an audition for a role at Hollywood, you may be told "<q>Don't call us, we'll call you</q>", meaning that they won't tell you if they chose you for the role immediately, but they'll call you in case they do.</p>
<p>To highlight the difference between the blocking and non-blocking version of <kbd>wait_and_print</kbd>, we can test and compare the execution of the two versions. In the output comments, the waiting periods are indicated by <kbd>&lt;wait...&gt;</kbd>:</p>
<pre>
    # Syncronous<br/>    wait_and_print("First call")<br/>    wait_and_print("Second call")<br/>    print("After call")<br/>    # Output:<br/>    # &lt;wait...&gt;<br/>    # First call  <br/>    # &lt;wait...&gt;<br/>    # Second call<br/>    # After call<br/>    # Async<br/>    wait_and_print_async("First call async")<br/>    wait_and_print_async("Second call async")<br/>    print("After submission")<br/>    # Output:<br/>    # After submission <br/>    # &lt;wait...&gt;<br/>    # First call<br/>    # Second call
</pre>
<p>The synchronous version behaves in a very familiar way. The code waits for a second, prints <kbd>First call</kbd>, waits for another second, and then prints the <kbd>Second call</kbd> and <kbd>After call</kbd> messages.</p>
<p>In the asynchronous version, <kbd>wait_and_print_async</kbd> <em>submits  (</em>rather than <em>execute</em>)<em> </em> those calls and moves on <em>immediately</em>. You can see this mechanism in action by acknowledging that the <kbd>"After submission"</kbd> message is printed immediately.</p>
<p>With this in mind, we can explore a slightly more complex situation by rewriting our <kbd>network_request</kbd> function using callbacks. In the following code, we define the <kbd>network_request_async</kbd> function. The biggest difference between <kbd>network_request_async</kbd> and its blocking counterpart is that <kbd>network_request_async</kbd> <em>doesn't return anything</em>. This is because we are merely submitting the request when <kbd>network_request_async</kbd> is called, but the value is available only when the request is completed.</p>
<p>If we can't return anything, how do we pass the result of the request? Rather than returning the value, we will pass the result as an argument to the <kbd>on_done</kbd> callback.</p>
<p>The rest of the function consists of submitting a callback (called <kbd>timer_done</kbd>) to the <kbd>timer.Timer</kbd> class that will call <kbd>on_done</kbd> when it's ready:</p>
<pre>
    def network_request_async(number, on_done):<br/><br/>        def timer_done():<br/>            on_done({"success": True, <br/>                     "result": number ** 2})<br/><br/>        timer = threading.Timer(1.0, timer_done)<br/>        timer.start()
</pre>
<p>The usage of <kbd>network_request_async</kbd> is quite similar to <kbd>timer.Timer</kbd>; all we have to do is pass the number we want to square and a callback that will receive the result <em>when it's ready</em>. This is demonstrated in the following snippet:</p>
<pre>
    def on_done(result):<br/>        print(result)<br/><br/>    network_request_async(2, on_done)
</pre>
<p>Now, if we submit multiple network requests, we note that the calls get executed concurrently and do not block the code:</p>
<pre>
    network_request_async(2, on_done)<br/>    network_request_async(3, on_done)<br/>    network_request_async(4, on_done)<br/>    print("After submission")
</pre>
<p>In order to use <kbd>network_request_async</kbd> in <kbd>fetch_square</kbd>, we need to adapt the code to use asynchronous constructs. In the following code, we modify <kbd>fetch_square</kbd> by defining and passing the <kbd>on_done</kbd> callback to <kbd>network_request_async</kbd>:</p>
<pre>
    def fetch_square(number):<br/>        def on_done(response):<br/>            if response["success"]:<br/>                print("Result is: {}".format(response["result"]))<br/><br/>        network_request_async(number, on_done)
</pre>
<p>You may have noted that the asynchronous code is significantly more convoluted than its synchronous counterpart. This is due to the fact that we are required to write and pass a callback every time we need to retrieve a certain result, causing the code to become nested and hard to follow.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Futures</h1>
            </header>

            <article>
                
<p>Futures are a more convenient pattern that can be used to keep track of the results of asynchronous calls. In the preceding code, we saw that rather than returning values, we accept callbacks and pass the results when they are ready. It is interesting to note that, so far, there is no easy way to track the status of the resource.</p>
<p>A <strong>future</strong> is an abstraction that helps us keep track of the requested resources and that we are waiting to become available. In Python, you can find a future implementation in the <kbd>concurrent.futures.Future</kbd> class. A <kbd>Future</kbd> instance can be created by calling its constructor with no arguments:</p>
<pre>
    fut = Future()<br/>    # Result:<br/>    # &lt;Future at 0x7f03e41599e8 state=pending&gt;
</pre>
<p>A future represents a value that is not yet available. You can see that its string representation reports the current status of the result which, in our case, is still pending. In order to make a result available, we can use the <kbd>Future.set_result</kbd> method:</p>
<pre>
    fut.set_result("Hello")<br/>    # Result:<br/>    # &lt;Future at 0x7f03e41599e8 state=finished returned str&gt;<br/><br/>    fut.result()<br/>    # Result:<br/>    # "Hello"
</pre>
<p>You can see that once we set the result, the <kbd>Future</kbd> will report that the task is finished and can be accessed using the <kbd>Future.result</kbd> method. It is also possible to subscribe a callback to a future so that, as soon as the result is available, the callback is executed. To attach a callback, it is sufficient to pass a function to the <kbd>Future.add_done_callback</kbd> method. When the task completes, the function will be called with the <kbd>Future</kbd> instance as its first argument and the result can be retrieved using the <kbd>Future.result()</kbd> method:</p>
<pre>
    fut = Future()<br/>    fut.add_done_callback(lambda future: print(future.result(), flush=True))<br/>    fut.set_result("Hello")<br/>    # Output:<br/>    # Hello
</pre>
<p>To get a grasp on how futures can be used in practice, we will adapt the <kbd>network_request_async</kbd> function to use futures. The idea is that, this time, instead of returning nothing, we return a <kbd>Future</kbd> that will keep track of the result for us. Note two things:</p>
<ul>
<li>We don't need to accept an <kbd>on_done callback</kbd> as callbacks can be connected later using the <kbd>Future.add_done_callback</kbd> method. Also, we pass the generic <kbd>Future.set_result</kbd> method as the callback for <kbd>threading.Timer</kbd>.</li>
<li>This time we are able to return a value, thus making the code a bit more similar to the blocking version we saw in the preceding section:</li>
</ul>
<pre>
    from concurrent.futures import Future<br/><br/>    def network_request_async(number):<br/>        future = Future()<br/>        result = {"success": True, "result": number ** 2}<br/>        timer = threading.Timer(1.0, lambda: future.set_result(result))<br/>        timer.start()<br/>        return future<br/><br/>    fut = network_request_async(2)
</pre>
<div class="packt_infobox">Even though we instantiate and manage futures directly in these examples; in practical applications, the futures are handled by frameworks. </div>
<p>If you execute the preceding code, nothing will happen as the code only consists of preparing and returning a <kbd>Future</kbd> instance. To enable further operation of the future results, we need to use the <kbd>Future.add_done_callback</kbd> method. In the following code, we adapt the <kbd>fetch_square</kbd> function to use futures:</p>
<pre>
    def fetch_square(number):<br/>        fut = network_request_async(number)<br/><br/>        def on_done_future(future):<br/>            response = future.result()<br/>            if response["success"]:<br/>                print("Result is: {}".format(response["result"]))<br/>        <br/>        fut.add_done_callback(on_done_future)
</pre>
<p>The code still looks quite similar to the callback version. Futures are a different and slightly more convenient way of working with callbacks. Futures are also advantageous, because they can <span>keep track of the resource status,</span> cancel (unschedule) scheduled tasks, and handle exceptions more naturally.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Event loops</h1>
            </header>

            <article>
                
<p>So far, we have implemented parallelism using OS threads. However, in many asynchronous frameworks, the coordination of concurrent tasks is managed by an <strong>event loop</strong>.</p>
<p>The idea behind an event loop is to continuously monitor the status of the various resources (for example, network connections and database queries) and trigger the execution of callbacks when events take place (for example, when a resource is ready or when a timer expires).</p>
<div class="packt_infobox"><span class="packt_screen">Why not just stick to threading?</span><br/>
Events loops are sometimes preferred as every unit of execution never runs at the same time as another and this can simplify dealing with shared variables, data structures, and resources. Read the next chapter for more details about parallel execution and its shortcomings.</div>
<p>As a first example, we will implement a thread-free version of <kbd>threading.Timer</kbd>. We can define a <kbd>Timer</kbd> class that will take a timeout and implement the <kbd>Timer.done</kbd> method that returns <kbd>True</kbd> if the timer has expired:</p>
<pre>
    class Timer:<br/>    <br/>        def __init__(self, timeout):<br/>            self.timeout = timeout<br/>            self.start = time.time()<br/>    <br/>        def done(self):<br/>            return time.time() - self.start &gt; self.timeout
</pre>
<p>To determine whether the timer has expired, we can write a loop that continuously checks the timer status by calling the <kbd>Timer.done</kbd> method. When the timer expires, we can print a message and exit the cycle:</p>
<pre>
    timer = Timer(1.0)<br/><br/>    while True:<br/>        if timer.done():<br/>            print("Timer is done!")<br/>            break
</pre>
<p>By implementing the timer in this way, the flow of execution is never blocked and we can, in principle, do other work inside the while loop.</p>
<div class="packt_infobox">Waiting for events to happen by continuously polling using a loop is commonly termed as <em>busy-waiting</em>.</div>
<p>Ideally, we would like to attach a custom function that executes when the timer goes off, just like we did in <kbd>threading.Timer</kbd>. To do this, we can implement a method, <kbd>Timer.on_timer_done</kbd>, that will accept a callback to be executed when the timer goes off:</p>
<pre>
    class Timer:<br/>       # ... previous code <br/>       def on_timer_done(self, callback):<br/>            self.callback = callback
</pre>
<p>Note that <kbd>on_timer_done</kbd> merely stores a reference to the callback. The entity that monitors the event and executes the callback is the loop. This concept is demonstrated as follows. Rather than using the print function, the loop will call <kbd>timer.callback</kbd> when appropriate:</p>
<pre>
    timer = Timer(1.0)<br/>    timer.on_timer_done(lambda: print("Timer is done!"))<br/><br/>    while True:<br/>        if timer.done():<br/>            <strong>timer.callback()</strong><br/>            break
</pre>
<p>As you can see, an asynchronous framework is starting to take place. All we did outside the loop was define the timer and the callback, while the loop took care of monitoring the timer and executing the associated callback. We can further extend our code by implementing support for multiple timers.</p>
<p>A natural way to implement multiple timers is to add a few <kbd>Timer</kbd> instances to a list and modify our event loop to periodically check all the timers and dispatch the callbacks when required. In the following code, we define two timers and attach a callback to each of them. Those timers are added to a list, <kbd>timers</kbd>, that is continuously monitored by our event loop. As soon as a timer is done, we execute the callback and remove the event from the list:</p>
<pre>
    timers = []<br/><br/>    timer1 = Timer(1.0)<br/>    timer1.on_timer_done(lambda: print("First timer is done!"))<br/><br/>    timer2 = Timer(2.0)<br/>    timer2.on_timer_done(lambda: print("Second timer is done!"))<br/><br/>    timers.append(timer1)<br/>    timers.append(timer2)<br/><br/>    while True:<br/>        for timer in timers:<br/>            if timer.done():<br/>                timer.callback()<br/>                timers.remove(timer)<br/>        # If no more timers are left, we exit the loop <br/>        if len(timers) == 0:<br/>            break
</pre>
<p>The main restriction of an event loop is, since the flow of execution is managed by a continuously running loop, that it <strong>never uses blocking calls</strong>. If we use any blocking statement (such as <kbd>time.sleep</kbd>) inside the loop, you can imagine how the event monitoring and callback dispatching will stop until the blocking call is done.</p>
<p>To avoid this, rather than using a blocking call, such as <kbd>time.sleep</kbd>, we let the event loop detect and execute the callback when the resource is ready. By not blocking the execution flow, the event loop is free to monitor multiple resources in a concurrent way.</p>
<div class="packt_infobox">The notification for events is usually implemented through operating system calls (such as the <kbd>select</kbd> Unix tool) that will resume the execution of the program whenever an event is ready (in contrast to busy-waiting).</div>
<p>The Python standard libraries include a very convenient event loop-based concurrency framework, <kbd>asyncio</kbd>, which will be the topic of the next section.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">The asyncio framework</h1>
            </header>

            <article>
                
<p>By now, you should have a solid foundation of how concurrency works, and how to use callbacks and futures. We can now move on and learn how to use the <kbd>asyncio</kbd> package present in the standard library since version 3.4. We will also explore the brand new <kbd>async</kbd>/<kbd>await</kbd> syntax to deal with asynchronous programming in a very natural way.</p>
<p>As a first example, we will see how to retrieve and execute a simple callback using <kbd>asyncio</kbd>. The <kbd>asyncio</kbd> loop can be retrieved by calling the <kbd>asyncio.get_event_loop()</kbd> function. We can schedule a callback for execution using  <kbd>loop.call_later</kbd> that takes a delay in seconds and a callback. We can also use the <kbd>loop.stop</kbd> method to halt the loop and exit the program.  To start processing the scheduled call, it is necessary to start the loop, which can be done using <kbd>loop.run_forever</kbd>. The following example demonstrates the usage of these basic methods by scheduling a callback that will print a message and halt the loop:</p>
<pre>
    import asyncio<br/><br/>    loop = asyncio.get_event_loop()<br/><br/>    def callback():<br/>        print("Hello, asyncio")<br/>        loop.stop()<br/><br/>    loop.call_later(1.0, callback)<br/>    loop.run_forever()
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Coroutines</h1>
            </header>

            <article>
                
<p>One of the main problems with callbacks is that they require you to break the program execution into small functions that will be invoked when a certain event takes place. As we saw in the earlier sections, callbacks can quickly become cumbersome.</p>
<p>Coroutines are another, perhaps a more natural, way to break up the program execution into chunks. They allow the programmer to write code that resembles synchronous code but will execute asynchronously. You may think of a coroutine as a function that can be stopped and resumed. A basic example of coroutines is generators.</p>
<p>Generators can be defined in Python using the <kbd>yield</kbd> statement inside a function. In the following example, we implement the <kbd>range_generator</kbd> function, which produces and returns values from <kbd>0</kbd> to <kbd>n</kbd>. We also add a print statement to log the internal state of the generator:</p>
<pre>
    def range_generator(n):<br/>        i = 0<br/>        while i &lt; n:<br/>            print("Generating value {}".format(i))<br/>            yield i<br/>            i += 1
</pre>
<p>When we call the <kbd>range_generator</kbd> function, the code is not executed immediately. Note that nothing is printed to output when the following snippet is executed. Instead, a <em>generator object</em> is returned:</p>
<pre>
    generator = range_generator(3)<br/>    generator<br/>    # Result:<br/>    # &lt;generator object range_generator at 0x7f03e418ba40&gt;
</pre>
<p>In order to start pulling values from a generator, it is necessary to use the <kbd>next</kbd> function:</p>
<pre>
    next(generator)<br/>    # Output:<br/>    # Generating value 0<br/><br/>    next(generator)<br/>    # Output:<br/>    # Generating value 1
</pre>
<p>Note that every time we invoke <kbd>next</kbd>, the code runs until it encounters the next <kbd>yield</kbd> statement and it is necessary to issue another <kbd>next</kbd> statement to resume the generator execution. You can think of a <kbd>yield</kbd> statement as a breakpoint where we can stop and resume execution (while also maintaining the internal state of the generator). This ability of stopping and resuming execution can be leveraged by the event loop to allow for concurrency. </p>
<p>It is also possible to <em>inject</em> (rather than <em>extract)</em> values in the generator through the <kbd>yield</kbd> statement. In the following example, we declare a function parrot that will repeat each message that we send. To allow a generator to receive a value, you can assign yield to a variable (in our case, it is <kbd>message = yield</kbd>). To insert values in the generator, we can use the <kbd>send</kbd> method. In the Python world, a generator that can also receive values is called a <em>generator-based coroutine</em>:</p>
<pre>
    def parrot():<br/>        while True:<br/>            message = yield<br/>            print("Parrot says: {}".format(message))<br/><br/>    generator = parrot()<br/>    generator.send(None)<br/>    generator.send("Hello")<br/>    generator.send("World")
</pre>
<p>Note that we also need to issue a <kbd>generator.send(None)</kbd> before we can start sending messages; this is done to bootstrap the function execution and bring us to the first <kbd>yield</kbd> statement. Also, note that there is an infinite loop inside <kbd>parrot</kbd>; if we implement this without using generators, we will get stuck running the loop forever!</p>
<p>With this in mind, you can imagine how an event loop can partially progress several of these generators without blocking the execution of the whole program. You can also imagine how a generator can be advanced only when some resource is ready, therefore eliminating the need for a callback.</p>
<p>It is possible to implement coroutines in <kbd>asyncio</kbd> using the <kbd>yield</kbd> statement. However, Python supports the definition of powerful coroutines using a more intuitive syntax since version 3.5.</p>
<p>To define a coroutine with <kbd>asyncio</kbd>, you can use the <kbd>async def</kbd> statement:</p>
<pre>
    async def hello():<br/>        print("Hello, async!")<br/><br/>    coro = hello()<br/>    coro<br/>    # Output:<br/>    # &lt;coroutine object hello at 0x7f314846bd58&gt;
</pre>
<p>As you can see, if we call the <kbd>hello</kbd> function, the function body is not executed immediately, but a <em>coroutine object</em> is returned. The <kbd>asyncio</kbd> coroutines do not support <kbd>next</kbd>, but they can be easily run in the <kbd>asyncio</kbd> event loop using the <kbd>run_until_complete</kbd> method:</p>
<pre>
    loop = asyncio.get_event_loop()<br/>    loop.run_until_complete(coro)
</pre>
<div class="packt_infobox">Coroutines defined with the <kbd>async def</kbd> statement are also called <em>native coroutines</em>.</div>
<p>The <kbd>asyncio</kbd>  module provides resources (called <em>awaitables</em>) that can be requested inside coroutines through the <kbd>await</kbd> syntax. For example, if we want to wait for a certain time and then execute a statement, we can use the <kbd>asyncio.sleep</kbd> function:</p>
<pre>
    async def wait_and_print(msg):<br/>        await asyncio.sleep(1)<br/>        print("Message: ", msg)<br/>    <br/>    loop.run_until_complete(wait_and_print("Hello"))
</pre>
<p>The result is beautiful, clean code. We are writing perfectly functional asynchronous code without all the ugliness of callbacks!</p>
<div class="packt_tip">You may have noted how <kbd>await</kbd> provides a breakpoint for the event loop so that, as it wait for the resource, the event loop can move on and concurrently manage other coroutines.</div>
<p>Even better, coroutines are also <kbd>awaitable</kbd>, and we can use the <kbd>await</kbd> statement to chain coroutines asynchronously. In the following example, we rewrite the <kbd>network_request</kbd> function, which we defined earlier, by replacing the call to <kbd>time.sleep</kbd> with <kbd>asyncio.sleep</kbd>:</p>
<pre>
    async def network_request(number):<br/>         <strong>await asyncio.sleep(1.0)</strong><br/>         return {"success": True, "result": number ** 2}
</pre>
<p>We can follow up by reimplementing <kbd>fetch_square</kbd>. As you can see, we can await <kbd>network_request</kbd> directly without needing additional futures or callbacks.</p>
<pre>
    async def fetch_square(number):<br/>         response = await network_request(number)<br/>         if response["success"]:<br/>             print("Result is: {}".format(response["result"]))
</pre>
<p>The coroutines can be executed individually using <kbd>loop.run_until_complete</kbd>:</p>
<pre>
    loop.run_until_complete(fetch_square(2))<br/>    loop.run_until_complete(fetch_square(3))<br/>    loop.run_until_complete(fetch_square(4))
</pre>
<p>Running tasks using <kbd>run_until_complete</kbd> is fine for testing and debugging. However, our program will be started with <kbd>loop.run_forever</kbd> most of the times, and we will need to submit our tasks while the loop is already running.</p>
<p><kbd>asyncio</kbd> provides the <kbd>ensure_future</kbd> function, which schedules coroutines (as well as futures) for execution. <kbd>ensure_future</kbd> can be used by simply passing the coroutine we want to schedule. The following code will schedule multiple calls to <kbd>fetch_square</kbd> that will be executed concurrently:</p>
<pre>
    asyncio.ensure_future(fetch_square(2))<br/>    asyncio.ensure_future(fetch_square(3))<br/>    asyncio.ensure_future(fetch_square(4))<br/><br/>    loop.run_forever()<br/>    # Hit Ctrl-C to stop the loop
</pre>
<p>As a bonus, when passing a coroutine, the <kbd>asyncio.ensure_future</kbd> function will return a <kbd>Task</kbd> instance (which is a subclass of <kbd>Future</kbd>) so that we can take advantage of the await syntax without having to give up the resource tracking capabilities of regular futures.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Converting blocking code into non-blocking code</h1>
            </header>

            <article>
                
<p>While <kbd>asyncio</kbd> supports connecting to resources in an asynchronous way, it is required to use blocking calls in certain cases. This happens, for example, when third-party APIs exclusively expose blocking calls (for example, many database libraries), but also when executing long-running computations. In this subsection, we will learn how to deal with blocking APIs and make them compatible with <kbd>asyncio</kbd>.</p>
<p>An effective strategy for dealing with blocking code is to run it in a separate thread. Threads are implemented at the <strong>Operating System</strong> (<strong>OS</strong>) level and allow parallel execution of blocking code. For this purpose, Python provides the <kbd>Executor</kbd> interface designed to run tasks in a separate thread and to monitor their progress using futures.</p>
<p>You can initialize a <kbd>ThreadPoolExecutor</kbd> by importing it from the <kbd>concurrent.futures</kbd> module. The executor will spawn a collection of threads (called <kbd>workers</kbd>) that will wait to execute whatever task we throw at them. Once a function is submitted, the executor will take care of dispatching its execution to an available worker thread and keep track of the result. The <kbd>max_workers</kbd> argument can be used to select the number of threads.</p>
<p>Note that the executor will not destroy a thread once a task is completed. By doing so, it reduces the cost associated with the creation and destruction of threads. </p>
<p>In the following example, we create a <kbd>ThreadPoolExecutor</kbd> with three workers, and we submit a <kbd>wait_and_return</kbd> function that will block the program execution for one second and return a message string. We then use the <kbd>submit</kbd> method to schedule its execution:</p>
<pre>
    from concurrent.futures import ThreadPoolExecutor<br/>    executor = ThreadPoolExecutor(max_workers=3)<br/><br/>    def wait_and_return(msg):<br/>        time.sleep(1)<br/>        return msg<br/><br/>    executor.submit(wait_and_return, "Hello. executor")<br/>    # Result:<br/>    # &lt;Future at 0x7ff616ff6748 state=running&gt;
</pre>
<p>The <kbd>executor.submit</kbd> method immediately schedules the function and returns a future. It is possible to manage the execution of tasks in <kbd>asyncio</kbd> using the <kbd>loop.run_in_executor</kbd> method, which works quite similarly to <kbd>executor.submit</kbd>:</p>
<pre>
    fut = loop.run_in_executor(executor, wait_and_return, "Hello, asyncio <br/>    executor")<br/>    # &lt;Future pending ...more info...&gt;
</pre>
<p>The <kbd>run_in_executor</kbd> method will also return an <kbd>asyncio.Future</kbd> instance that can be awaited from other code, the main difference being that the future will not be run until we start the loop. We can run and obtain the response using <kbd>loop.run_until_complete</kbd>:</p>
<pre>
    loop.run_until_complete(fut)<br/>    # Result:<br/>    # 'Hello, executor'
</pre>
<p>As a practical example, we can use this technique to implement concurrent fetching of several web pages. To do this, we will import the popular (blocking) <kbd>requests</kbd> library and run the <kbd>requests.get</kbd> function in the executor:</p>
<pre>
    import requests<br/><br/>    async def fetch_urls(urls):<br/>        responses = []<br/>        for url in urls:<br/>            responses.append(await loop.run_in_executor<br/>                                (executor, requests.get, url))<br/>        return responses<br/><br/>    loop.run_until_complete(fetch_ruls(['http://www.google.com', <br/>                                        'http://www.example.com',<br/>                                        'http://www.facebook.com']))<br/>    # Result<br/>    # []
</pre>
<p>This version of <kbd>fetch_url</kbd> will not block the execution and allow other coroutines in <kbd>asyncio</kbd> to run; however, it is not optimal as the function will not fetch a URL in parallel. To do this, we can use <kbd>asyncio.ensure_future</kbd> or employ the <kbd>asyncio.gather</kbd> convenience function that will submit all the coroutines at once and gather the results as they come. The usage of <kbd>asyncio.gather</kbd> is demonstrated here:</p>
<pre>
    def fetch_urls(urls):<br/>        return asyncio.gather(*[loop.run_in_executor<br/>                                 (executor, requests.get, url) <br/>                                 for url in urls])
</pre>
<div class="packt_infobox packt_tip">The number of URLs you can fetch in parallel with this method will be dependent on the number of worker threads you have. To avoid this limitation, you should use a natively non-blocking library, such as <kbd>aiohttp</kbd>.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Reactive programming</h1>
            </header>

            <article>
                
<p>Reactive programming is a paradigm that aims at building better concurrent systems. Reactive applications are designed to comply with the requirements exemplified by the reactive manifesto:</p>
<ul>
<li><strong>Responsive</strong>:  The system responds immediately to the user.</li>
<li><strong>Elastic</strong>: The system is capable of handling different levels of load and is able to adapt to accommodate increasing demands.</li>
<li><strong>Resilient</strong>: The system deals with failure gracefully. This is achieved by modularity and avoiding having a single point of failure.</li>
<li><strong>Message driven</strong>: The system should not block and take advantage of events and messages. A message-driven application helps achieve all the previous requirements.</li>
</ul>
<p>As you can see, the intent of reactive systems is quite noble, but how exactly does reactive programming work? In this section, we will learn about the principles of reactive programming using the RxPy library.</p>
<div class="packt_infobox">The RxPy library is part of ReactiveX (<a href="http://reactivex.io/">http://reactivex.io/</a>), which is a project that implements reactive programming tools for a large variety of languages.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Observables</h1>
            </header>

            <article>
                
<p>As the name implies, the main idea of reactive programming is to <em>react</em> to events. In the preceding section, we saw some examples of this idea with callbacks; you subscribe to them and the callback is executed as soon as the event takes place.</p>
<p>In reactive programming, this idea is expanded by thinking of events as streams of data. This can be exemplified by showing examples of such streams in RxPy. A data stream can be created from an iterator using the <kbd>Observable.from_iterable</kbd> factory method, as follows:</p>
<pre>
    from rx import Observable<br/>    obs = Observable.from_iterable(range(4))
</pre>
<p>In order to receive data from <kbd>obs</kbd>, we can use the <kbd>Observable.subscribe</kbd> method, which will execute the function we pass for each value that the data source emits:</p>
<pre>
    obs.subscribe(print)<br/>    # Output:<br/>    # 0<br/>    # 1<br/>    # 2<br/>    # 3
</pre>
<p>You may have noted that observables are ordered collections of items just like lists or, more generally, iterators. This is not a coincidence.</p>
<div class="packt_infobox">The term observable comes from the combination of observer and iterable. An <em>observer</em> is an object that reacts to changes of the variable it observes, while an <em>iterable</em> is an object that is capable of producing and keeping track of an iterator.</div>
<p>In Python, iterators are objects that define the <kbd>__next__</kbd> method, and whose elements can be extracted by calling <kbd>next</kbd>. An iterator can generally be obtained by a collection using <kbd>iter</kbd>; then we can extract elements using <kbd>next</kbd> or a <kbd>for</kbd> loop. Once an element is consumed from the iterator, we can't go back. We can demonstrate its usage by creating an iterator from a list:</p>
<pre>
    collection = list([1, 2, 3, 4, 5])<br/>    iterator = iter(collection)<br/><br/>    print("Next")<br/>    print(next(iterator))<br/>    print(next(iterator))<br/><br/>    print("For loop")<br/>    for i in iterator:<br/>         print(i)<br/><br/>    # Output:<br/>    # Next<br/>    # 1<br/>    # 2<br/>    # For loop<br/>    # 3<br/>    # 4<br/>    # 5
</pre>
<p>You can see how, every time we call <kbd>next</kbd> or we iterate, the iterator produces a value and advances. In a sense, we are <em>pulling</em> results from the iterator.</p>
<div class="packt_tip packt_infobox">Iterators sound a lot like generators; however, they are more general. In Python, generators are returned by functions that use yield expressions. As we saw, generators support <kbd>next</kbd>, therefore, they are a special class of iterators.</div>
<p>Now you can appreciate the contrast between an iterator and an observable.  An observable<em>pushes</em> a stream of data to us whenever it's ready, but that's not everything. An observable is also able to tell us when there is an error and where there is no more data. In fact, it is possible to register further callbacks to the <kbd>Observable.subscribe</kbd> method. In the following example, we create an observable and register callbacks to be called using <kbd>on_next</kbd> whenever the next item is available and using the <kbd>on_completed</kbd> argument when there is no more data:</p>
<pre>
    obs = Observable.from_iter(range(4))<br/>    obs.subscribe(on_next=lambda x: print(on_next="Next item: {}"),<br/>                  on_completed=lambda: print("No more data"))<br/>    # Output:<br/>    # Next element: 0<br/>    # Next element: 1<br/>    # Next element: 2<br/>    # Next element: 3<br/>    # No more data
</pre>
<p>This analogy with the iterator is important because we can use the same techniques that can be used with iterators to handle streams of events.</p>
<p>RxPy provides operators that can be used to create, transform, filter, and group observables. The power of reactive programming lies in the fact that those operations return other observables that can be conveniently chained and composed together. For a quick taste, we will demonstrate the usage of the <kbd>take</kbd> operator.</p>
<p>Given an observable, <kbd>take</kbd> will return a new observable that will stop after <kbd>n</kbd> items. Its usage is straightforward:</p>
<pre>
    obs = Observable.from_iterable(range(100000))<br/>    obs2 = obs.take(4)<br/><br/>    obs2.subscribe(print)<br/>    # Output:<br/>    # 0<br/>    # 1<br/>    # 2<br/>    # 3
</pre>
<p>The collection of operations implemented in RxPy is varied and rich, and can be used to build complex applications using these operators as building blocks.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Useful operators</h1>
            </header>

            <article>
                
<p>In this subsection, we will explore operators that transform the elements of a source observable in some way. The most prominent member of this family of operators is the familiar <kbd>map</kbd>, which emits the elements of the source observable after applying a function to them. For example, we may use <kbd>map</kbd> to calculate the square of a sequence of numbers:</p>
<pre>
    (Observable.from_iterable(range(4))<br/>               .map(lambda x: x**2)<br/>               .subscribe(print))<br/>    # Output:<br/>    # 0<br/>    # 1<br/>    # 4<br/>    # 9
</pre>
<p>Operators can be represented with marble diagrams that help us better understand how the operator works, especially when taking into account the fact that elements can be emitted over a region of time. In a marble diagram, a data stream (in our case, an observable) is represented by a solid line. A circle (or other shape) identifies a value emitted by the observable, an <strong>X</strong> symbol represents an error, and a vertical line represents the end of the stream.</p>
<p>In the following figure, we can see the marble diagram of <strong>map</strong>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full image-border" height="167" src="assets/B06440_06CHPNO_04.png" width="333"/></div>
<p>The source observable is placed at the top of the diagram, the transformation is placed in the middle, and the resulting observable is placed at the bottom.</p>
<p>Another example of a transformation is <kbd>group_by</kbd>, which sorts the items into groups based on a key. The <kbd>group_by</kbd> operator takes a function that extracts a key when given an element and produces an observable for each key with the elements associated to it.</p>
<p>The <kbd>group_by</kbd> operation can be expressed more clearly using a marble diagram. In the following figure, you can see how <kbd>group_by</kbd> emits two observables. Additionally, the items are dynamically sorted into groups <em>as soon as they are emitted</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full image-border" height="177" src="assets/B06440_06CHPNO_05.png" width="240"/></div>
<p>We can further understand how <kbd>group_by</kbd> works with a simple example. Let's say that we want to group the number according to the fact that they're even or odd. We can implement this using <kbd>group_by</kbd> by passing the <kbd>lambda x: x % 2</kbd> expression as a key function, which will return <kbd>0</kbd> if the number is even and <kbd>1</kbd> if the number is odd:</p>
<pre>
    obs = (Observable.from_range(range(4))<br/>                     .group_by(lambda x: x % 2))
</pre>
<p>At this point, if we subscribe and print the content of <kbd>obs</kbd>, actually two observables are printed:</p>
<pre>
    obs.subscribe(print)<br/>    # &lt;rx.linq.groupedobservable.GroupedObservable object at 0x7f0fba51f9e8&gt;<br/>    # &lt;rx.linq.groupedobservable.GroupedObservable object at 0x7f0fba51fa58&gt;
</pre>
<p>You can determine the group key using the <kbd>key</kbd> attribute. To extract all the even numbers, we can take the first observable (corresponding to a key equal to 0) and subscribe to it. In the following code, we show how this works:</p>
<pre>
    obs.subscribe(lambda x: print("group key: ", x.key))<br/>    # Output:<br/>    # group key:  0<br/>    # group key:  1<br/>    obs.take(1).subscribe(lambda x: x.subscribe(print))<br/>    # Output:<br/>    # 0<br/>    # 2
</pre>
<p>With <kbd>group_by</kbd>, we introduced an observable that emits other observables. This turns out to be quite a common pattern in reactive programming, and there are functions that allow you to combine different observables.</p>
<p>Two useful tools for combining observables are <kbd>merge_all</kbd> and <kbd>concat_all</kbd>. Merge takes multiple observables and produces a single observable that contains the element of the two observables in the order they are emitted. This is better illustrated using a marble diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full image-border" height="111" src="assets/B06440_06CHPNO_06.png" width="217"/></div>
<p><kbd>merge_all</kbd> can be compared to a similar operator, <kbd>concat_all</kbd>, which returns a new observable that emits the elements of all the elements of the first observable, followed by the elements of the second observable and so on. The marble diagram for <kbd>concat_all</kbd> is presented here:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full image-border" height="109" src="assets/B06440_06CHPNO_07.png" width="215"/></div>
<p>To demonstrate the usage of these two operators, we can apply those operations to the observable of observables returned by <kbd>group_by</kbd>. In the case of <kbd>merge_all</kbd>, the items are returned in the same order as they were initially (remember that <kbd>group_by</kbd> emits elements in the two groups as they come):</p>
<pre>
    obs.merge_all().subscribe(print)<br/>    # Output<br/>    # 0<br/>    # 1<br/>    # 2<br/>    # 3
</pre>
<p>On the other hand, <kbd>concat_all</kbd> first returns the even elements and then the odd elements as it waits for the first observable to complete, and then starts emitting the elements of the second observable. This is demonstrated in the following snippet. In this specific example, we also applied a function, <kbd>make_replay</kbd>; this is needed because, by the time the "even" stream is consumed, the elements of the second stream have already been produced and will not be available to <kbd>concat_all</kbd>. This concept will become much clearer after reading the <em>Hot and cold observables</em> section:</p>
<pre>
    def make_replay(a):<br/>        result = a.replay(None)<br/>        result.connect()<br/>        return result<br/><br/>    obs.map(make_replay).concat_all().subscribe(print)<br/>    # Output<br/>    # 0<br/>    # 2<br/>    # 1<br/>    # 3
</pre>
<p>This time around, the even numbers are printed first, followed by the odd numbers. </p>
<div class="packt_tip">RxPy also provides the  <kbd>merge</kbd> and <kbd>concat</kbd> operations that can be used to combine individual observables</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Hot and cold observables</h1>
            </header>

            <article>
                
<p>In the preceding section, we learned how to create an observable using the <kbd>Observable.from_iterable</kbd> method. RxPy provides many other tools to create more interesting event sources.</p>
<p><kbd>Observable.interval</kbd> takes a time interval in milliseconds, <kbd>period</kbd>, and will create an observable that emits a value every time the period has passed. The following line of code can be used to define an observable, <kbd>obs</kbd>, that will emit a number, starting from zero, every second. We use the <kbd>take</kbd> operator to limit the timer to four events:</p>
<pre>
    obs = Observable.interval(1000)<br/>    obs.take(4).subscribe(print)<br/>    # Output:<br/>    # 0<br/>    # 1<br/>    # 2<br/>    # 3
</pre>
<p>A very important fact about <kbd>Observable.interval</kbd> is that the timer doesn't start until we subscribe. We can observe this by printing both the index and the delay from when the timer starts definition using <kbd>time.time()</kbd>, as follows:</p>
<pre>
    import time<br/><br/>    start = time.time()<br/>    obs = Observable.interval(1000).map(lambda a: <br/>                                           (a, time.time() - start))<br/><br/>    # Let's wait 2 seconds before starting the subscription<br/>    time.sleep(2)<br/>    obs.take(4).subscribe(print)<br/>    # Output:<br/>    # (0, 3.003735303878784)<br/>    # (1, 4.004871129989624)<br/>    # (2, 5.005947589874268)<br/>    # (3, 6.00749135017395)
</pre>
<p>As you can see, the first element (corresponding to a <kbd>0</kbd> index) is produced after three seconds, which means that the timer started when we issue the <kbd>subscribe(print)</kbd> method.</p>
<p>Observables, such as <kbd>Observable.interval</kbd>, are called <em>lazy</em> because they start producing values only when requested (think of them as vending machines, which won't dispense food unless we press the button). In Rx jargon, these kind of observables are called <strong>cold</strong>. A property of cold observables is that, if we attach two subscribers, the interval timer will be started multiple times. This is quite evident from the following example. Here, we add a new subscription 0.5 seconds after the first, and you can see how the output of the two subscriptions come at different times:</p>
<pre>
    start = time.time()<br/>    obs = Observable.interval(1000).map(lambda a: <br/>                                           (a, time.time() - start))<br/><br/>    # Let's wait 2 seconds before starting the subscription<br/>    time.sleep(2)<br/>    obs.take(4).subscribe(lambda x: print("First subscriber: <br/>                                             {}".format(x)))<br/>    time.sleep(0.5)<br/>    obs.take(4).subscribe(lambda x: print("Second subscriber: <br/>                                             {}".format(x)))<br/>    # Output:<br/>    # First subscriber: (0, 3.0036110877990723)<br/>    # Second subscriber: (0, 3.5052847862243652)<br/>    # First subscriber: (1, 4.004414081573486)<br/>    # Second subscriber: (1, 4.506155252456665)<br/>    # First subscriber: (2, 5.005316972732544)<br/>    # Second subscriber: (2, 5.506817102432251)<br/>    # First subscriber: (3, 6.0062034130096436)<br/>    # Second subscriber: (3, 6.508296489715576)
</pre>
<p>Sometimes we may not want this behavior as we may want multiple subscribers to subscribe to the same data source. To make the observable produce the same data, we can delay the data production and ensure that all the subscribers will get the same data using the <kbd>publish</kbd> method.</p>
<p>Publish will transform our observable into a <kbd>ConnectableObservable</kbd>, which won't start pushing data immediately, but only when we call the <kbd>connect</kbd> method. The usage of <kbd>publish</kbd> and <kbd>connect</kbd> is demonstrated in the following snippet:</p>
<pre>
    start = time.time()<br/>    obs = Observable.interval(1000).map(lambda a: (a, time.time() - <br/>    start))<strong>.publish()</strong><br/>    obs.take(4).subscribe(lambda x: print("First subscriber: <br/>                                             {}".format(x)))<br/><strong>    obs.connect() # Data production starts here<br/></strong><br/>    time.sleep(2)<br/>    obs.take(4).subscribe(lambda x: print("Second subscriber: <br/>                                             {}".format(x)))<br/>    # Output:<br/>    # First subscriber: (0, 1.0016899108886719)<br/>    # First subscriber: (1, 2.0027990341186523)<br/>    # First subscriber: (2, 3.003532648086548)<br/>    # Second subscriber: (2, 3.003532648086548)<br/>    # First subscriber: (3, 4.004265308380127)<br/>    # Second subscriber: (3, 4.004265308380127)<br/>    # Second subscriber: (4, 5.005320310592651)<br/>    # Second subscriber: (5, 6.005795240402222)
</pre>
<p>In the preceding example, you can see how we first issue <kbd>publish</kbd>, then we subscribe the first subscriber and, finally, we issue <kbd>connect</kbd>. When <kbd>connect</kbd> is issued, the timer will start producing data. The second subscriber joins the party late and, in fact, won't receive the first two messages but will start receiving data from the third and so on. Note how, this time around, the subscribers share the exact same data. This kind of data source, where data is produced independently of the subscribers, is called <strong>hot</strong>.</p>
<p>Similar to <kbd>publish</kbd>, you can use the <kbd>replay</kbd> method that will produce the data <em>from the beginning</em> for each new subscriber. This is illustrated in the following example that, which is identical to the preceding one except that we replaced <kbd>publish</kbd> with <kbd>replay</kbd>:</p>
<div class="output_area">
<div class="output_subarea output_text output_stream output_stdout">
<pre>
    import time<br/><br/>    start = time.time()<br/>    obs = Observable.interval(1000).map(lambda a: (a, time.time() - <br/>    start))<strong>.replay(None)</strong><br/>    obs.take(4).subscribe(lambda x: print("First subscriber: <br/>                                             {}".format(x)))<br/><strong>    obs.connect()</strong><br/><br/>    time.sleep(2)<br/>    obs.take(4).subscribe(lambda x: print("Second subscriber: <br/>                                             {}".format(x)))<br/><br/>    First subscriber: (0, 1.0008857250213623)<br/>    First subscriber: (1, 2.0019824504852295)<br/><strong>    Second subscriber: (0, 1.0008857250213623)<br/>    Second subscriber: (1, 2.0019824504852295)<br/></strong>    First subscriber: (2, 3.0030810832977295)<br/>    Second subscriber: (2, 3.0030810832977295)<br/>    First subscriber: (3, 4.004604816436768)<br/>    Second subscriber: (3, 4.004604816436768)
</pre></div>
</div>
<p>You can see how, this time around, even though the second subscriber arrives late to the party, it is still given all the items that have been given out so far.</p>
<p>Another way of creating hot observables is through the <kbd>Subject</kbd> class. <kbd>Subject</kbd> is interesting because it's capable of both receiving and pushing data, and thus it can be used to manually <em>push</em> items to an observable. Using <kbd>Subject</kbd> is very intuitive; in the following code, we create a <kbd>Subject</kbd> and subscribe to it. Later, we push values to it using the <kbd>on_next</kbd> method; as soon as we do that, the subscriber is called:</p>
<pre>
    s = Subject()<br/>    s.subscribe(lambda a: print("Subject emitted value: {}".format(x))<br/>    s.on_next(1)<br/>    # Subject emitted value: 1<br/>    s.on_next(2)<br/>    # Subject emitted value: 2
</pre>
<p>Note that <kbd>Subject</kbd> is another example of hot observables.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Building a CPU monitor</h1>
            </header>

            <article>
                
<p>Now that we have a grasp on the main reactive programming concepts, we can implement a sample application. In this subsection, we will implement a monitor that will give us real-time information about our CPU usage and is capable of detecting spikes.</p>
<div class="packt_infobox">The complete code for the CPU monitor can be found in the <kbd>cpu_monitor.py</kbd> file.</div>
<p>As a first step, let's implement a data source. We will use the <kbd>psutil</kbd> module that provides a function, <kbd>psutil.cpu_percent</kbd>, that returns the latest available CPU usage as a percent (and doesn't block):</p>
<pre>
    import psutil<br/>    psutil.cpu_percent()<br/>    # Result: 9.7
</pre>
<p>Since we are developing a monitor, we would like to sample this information over a few time intervals. To accomplish this we can use the familiar <kbd>Observable.interval</kbd> , followed by <kbd>map</kbd> just like we did in the previous section. Also, we would like to make this observable <em>hot</em> as, for this application, all subscribers should receive a single source of data; to make <kbd>Observable.interval</kbd> hot,  we can use the <kbd>publish</kbd> and <kbd>connect</kbd> methods. The full code for the creation of the <kbd>cpu_data</kbd> observable is as follows</p>
<pre>
    cpu_data = (Observable<br/>                .interval(100) # Each 100 milliseconds<br/>                .map(lambda x: psutil.cpu_percent())<br/>                .publish())<br/>    cpu_data.connect() # Start producing data
</pre>
<p>We can test our monitor by printing a sample of 4 items</p>
<pre>
    cpu_data.take(4).subscribe(print)<br/>    # Output:<br/>    # 12.5<br/>    # 5.6<br/>    # 4.5<br/>    # 9.6
</pre>
<p>Now that our main data source is in place, we can implement a monitor visualization using <kbd>matplotlib</kbd>. The idea is to create a plot that contains a fixed amount of measurements and, as new data arrives, we include the newest measurement and remove the oldest one. This is commonly referred to as a <em>moving window</em> and is better understood with an illustration. In the following figure, our <kbd>cpu_data</kbd> stream is represented as a list of numbers. The first plot is produced as soon as we have the first four numbers and, each time a new number arrives, we shift the window by one position and update the plot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full image-border" height="137" src="assets/B06440_06CHPNO_08.png" width="204"/></div>
<p>To implement this algorithm, we can write a function, called <kbd>monitor_cpu</kbd>, that will create and update our plotting window. The function will do the following things:</p>
<ul>
<li>Initialize an empty plot and set up the correct plot limits.</li>
<li>Transform our <kbd>cpu_data</kbd> observable to return a moving window over the data. This can be accomplished using the <kbd>buffer_with_count</kbd> operator, which will take the number of points in our window, <kbd>npoints</kbd>, as parameters and the shift as <kbd>1</kbd>.</li>
<li>Subscribe to this new data stream and update the plot with the incoming data.</li>
</ul>
<p>The complete code for the function is shown here and, as you can see, is extremely compact. Take some time to run the function and play with the parameters:</p>
<pre>
    import numpy as np<br/>    from matplotlib import pyplot as plt<br/><br/>    def monitor_cpu(npoints):<br/>        lines, = plt.plot([], [])<br/>        plt.xlim(0, npoints) <br/>        plt.ylim(0, 100) # 0 to 100 percent<br/><br/>        <strong>cpu_data_window = cpu_data.buffer_with_count(npoints, 1)</strong><br/><br/>        def update_plot(cpu_readings):<br/>            lines.set_xdata(np.arange(npoints))<br/>            lines.set_ydata(np.array(cpu_readings))<br/>            plt.draw()<br/><br/>        <strong>cpu_data_window.subscribe(update_plot)</strong><br/><br/>        plt.show()
</pre>
<p>Another feature we may want to develop is, for example, an alert that triggers when the CPU has been high for a certain amount of time as this may indicate that some of the  processes in our machine are working very hard. This can be accomplished by combining <kbd>buffer_with_count</kbd> and <kbd>map</kbd>. We can take the CPU stream and a window, and then we will test whether all items have a value higher than twenty percent usage (in a quad-core CPU that corresponds to about one processor working at hundred percent) in the map function. If all the points in the window have a higher than <span>twenty percent</span> usage, we display a warning in our plot window.</p>
<p>The implementation of the new observable can be written as follows and will produce an observable that emits <kbd>True</kbd> if the CPU has high usage, and <kbd>False</kbd> otherwise:</p>
<pre>
    alertpoints = 4    <br/>    high_cpu = (cpu_data<br/>                .buffer_with_count(alertpoints, 1)<br/>                .map(lambda readings: <strong>all(r &gt; 20 for r in readings)</strong>))
</pre>
<p>Now that the <kbd>high_cpu</kbd> observable is ready, we can create a <kbd>matplotlib</kbd> label and subscribe to it for updates:</p>
<pre>
    label = plt.text(1, 1, "normal")<br/>    def update_warning(is_high):<br/>        if is_high:<br/>            label.set_text("high")<br/>        else:<br/>            label.set_text("normal")<br/>    high_cpu.subscribe(update_warning)
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>Asynchronous programming is useful when our code deals with slow and unpredictable resources, such as I/O devices and networks. In this chapter, we explored the fundamental concepts of concurrency and asynchronous programming and how to write concurrent code with the <kbd>asyncio</kbd> and RxPy libraries.</p>
<p><kbd>asyncio</kbd> coroutines are an excellent choice when dealing with multiple, interconnected resources as they greatly simplify the code logic by cleverly avoiding callbacks. Reactive programming is also very good in these situations, but it truly shines when dealing with streams of data that are common in real-time applications and user interfaces.</p>
<p>In the next two chapters, we will learn about parallel programming and how to achieve impressive performance gain by taking advantage of multiple cores and multiple machines.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>