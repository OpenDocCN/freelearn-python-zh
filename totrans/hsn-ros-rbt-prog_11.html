<html><head></head><body>
        

                            
                    <h1 class="header-title">Virtual SLAM and Navigation Using Gazebo</h1>
                
            
            
                
<p>In this chapter, you will be introduced to the concepts and components of robot navigation. Using <strong>SLAM</strong> (short for <strong>Simultaneous Localization and Mapping</strong>) techniques, you will be able to execute autonomous navigation with GoPiGo3. This chapter deals with advanced topics in simulation. Hence, it is essential that you have understood the concepts of the previous chapter, where we gave you the basics to interact with a virtual robot in Gazebo.</p>
<p>SLAM is a technique used in robotics to explore and map an unknown environment while estimating the pose of the robot itself. As it moves all around, it will be acquiring structured information of the surroundings by processing the raw data coming from its sensors.</p>
<p>You will explore this concept with a practical approach using the digital twin of GoPiGo3, neatly understanding why a SLAM implementation is required for proper navigation. The simulation will be run in Gazebo, the ROS native simulation tool with a physics engine that offers realistic results.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Dynamic simulation using Gazebo</li>
<li>Components in navigation</li>
<li>Robot perception and SLAM</li>
<li>Practicing SLAM and navigation with GoPiGo3</li>
</ul>
<p>By covering these topics, you will get more familiar with the Gazebo environment. You will understand the concepts of navigation and SLAM and how they relate to each other. With a very practical approach, you will learn to run SLAM and navigation tasks in Gazebo with a virtual model of a robot.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>To summarize and clarify the purposes of the steps that we'll take in this chapter dealing with the virtual robot, and in the next chapter regarding the physical GoPiGo3, the following list shows all these sensors and actuators we are going to work with, as well as the sections of the previous chapters that have dealt with each one:</p>
<ul>
<li><strong>Distance sensor</strong>: In <a href="0b20bdff-f1dc-42e8-ae83-fc290da31381.xhtml" target="_blank">Chapter 6</a>, <em>Programming in ROS – Commands and Tools</em>, the <em>Case study 1: publishing and reading the distance sensor</em><em> </em>section taught you how to use the distance sensor under ROS with the physical robot.</li>
<li><strong>Line follower</strong>. See the following list for assembly and unit-testing instructions.</li>
<li><strong>IMU sensor</strong>. See the following list for assembly and unit-testing instructions.</li>
<li><strong>Pi camera</strong>: In <a href="0b20bdff-f1dc-42e8-ae83-fc290da31381.xhtml" target="_blank"/><a href="0b20bdff-f1dc-42e8-ae83-fc290da31381.xhtml" target="_blank">Chapter 6</a>, <em>Programming in ROS – Commands and Tools</em>, the <em>Case Study 1: Publishing and reading the distance sensor</em><em> </em>section taught you how to use the Pi camera under ROS with the physical robot.</li>
<li><strong>Drive motors and encoders</strong>: In the previous chapter, the <em>Case study 3: Remote control using the keyboard </em>section taught you first how to use these items in ROS with the physical robot, and then how to implement a differential drive controller under the Gazebo simulator in ROS.</li>
</ul>
<p>For all of these, you have the following:</p>
<ul>
<li>Assembly instructions, which can be found in the <em>Deep dive into the electromechanics</em><em> </em>section of <a href="9bb411d1-934c-4497-aad4-7ad770d3783c.xhtml">Chapter 1</a>, <em>Assembling the Robot</em></li>
<li>Unit testing instructions, which can found in the <em>Unit testing of sensors and drives</em><em> </em>section of <a href="7a2b1b82-c666-42df-9f10-9777eabe82df.xhtml">Chapter 2</a>, <em>Unit Testing of GoPiGo3</em>, where the provided software taught you how to deal with unit tests using Python</li>
</ul>
<p>For optimal and easy-to-understand coverage of the topic of SLAM, we will implement a 360º-coverage <strong>Laser Distance Sensor</strong> (<strong>LDS</strong>)<strong> </strong>in the virtual robot. There are low-cost versions of this sensor technology, such as <strong>EAI YDLIDAR X4</strong> (available at<strong> </strong><a href="https://es.aliexpress.com/item/32908156152.html">https://www.aliexpress.com/item/32908156152.html</a>), which is the one we will make use of in the next chapter.</p>
<p>In this chapter, we will make use of the code located in the <kbd>Chapter8_Virtual_SLAM</kbd> folder at <a href="https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter8_Virtual_SLAM">https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter8_Virtual_SLAM</a>. Copy its files to the ROS workspace to have them available, and leave the rest outside of the <kbd>src</kbd> folder. This way, you will have a cleaner ROS environment:</p>
<pre><strong>$ cp -R ~/Hands-On-ROS-for-Robotics-Programming/Chapter8_Virtual_SLAM ~/catkin_ws/src/</strong></pre>
<p>The code contains two new ROS packages as follows:</p>
<ul>
<li><kbd>gopigo3_description</kbd>, which contains the URDF model plus the SDF (Gazebo tags) for a complete, dynamic simulation. This package provides the <kbd>gopigo3_rviz.launch</kbd> launch file to interactively visualize the model in RViz.</li>
<li><kbd>virtual_slam</kbd> contains the virtual robot simulation itself, plus the launch files needed to run SLAM in Gazebo.</li>
</ul>
<p>Then, rebuild the workspace so that it is known to your ROS installation:</p>
<pre><strong>$ cd ~/catkin_ws</strong><br/><strong>$ catkin_make</strong></pre>
<p>Check that the packages have been correctly installed by selecting them and listing the files:</p>
<pre><strong>$ rospack list | grep gopigo3</strong></pre>
<p>Then you need to make some installation and configuration to run the exercises, as follows.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">ROS navigation packages</h1>
                
            
            
                
<p>The following steps provide the installation instructions for ROS Kinetic, the version running in Ubuntu 16.04:</p>
<ol>
<li>First, let's prepare your machine with the required ROS packages needed for the navigation stack:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ sudo apt install ros-kinetic-navigation ros-kinetic-amcl ros-kinetic-map-server ros-kinetic-move-base ros-kinetic-urdf ros-kinetic-xacro ros-kinetic-compressed-image-transport ros-kinetic-rqt-image-view</strong></pre>
<ol start="2">
<li>In <strong>ROS Kinetic</strong>, you can install <kbd>slam_gmapping</kbd> from binaries:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong>$ sudo apt-get install ros-kinetic-slam-gmapping</strong></pre>
<p class="mce-root">This installs the <kbd>gmapping</kbd> and <kbd>openslam_gmapping</kbd> packages. If working with ROS Melodic (that is, you are in Ubuntu 18.04):</p>
<ul>
<li class="mce-root">Install the corresponding versions for Melodic:</li>
</ul>
<pre style="padding-left: 60px"><strong>$ sudo apt install ros-melodic-navigation ros-melodic-amcl ros-melodic-map-server ros-melodic-move-base ros-melodic-urdf ros-melodic-xacro ros-melodic-compressed-image-transport ros-melodic-rqt-image-view</strong></pre>
<ul>
<li>And finally the <kbd>slam_gmapping</kbd> package, that the time of writing is already available in its binary version:</li>
</ul>
<pre style="padding-left: 60px"><strong>sudo apt-get install ros-melodic-slam-gmapping</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">ROS master running on the local computer</h1>
                
            
            
                
<p> Since, in this chapter, you will only be using your local machine, you need to reconfigure the ROS master URI so that it does not point to the robot but to your local computer. Then, open your local <kbd>.bashrc</kbd> file and comment out the line at the end that specifies the URL where the ROS master can be found:</p>
<pre><strong>$ nano ~./bashrc</strong><br/><strong>   ...</strong><br/><strong>   export ROS_HOSTNAME=rosbot.local</strong><br/><strong>  # THIS LINE IS NOW A COMMENT # export ROS_MASTER_URI=http://gopigo3.local:11311</strong></pre>
<p>Close all Terminals, open a new one, and check the <kbd>ROS_MASTER_URI</kbd> variable:</p>
<pre><strong>$ echo $ROS_MASTER_URI</strong><br/><strong>    http://localhost:11311</strong></pre>
<p>You should find that the environment variable has reverted to the default server (<kbd>localhost</kbd>) and default port (<kbd>11311</kbd>). Now, we are ready to switch to the virtual robot.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Dynamic simulation using Gazebo</h1>
                
            
            
                
<p>In the previous chapter, you performed a very basic version of navigation, where the feedback to the robot about its environment always came from you as a human operator. For example, you saw that GoPiGo3 is advancing to an obstacle, so you made it turn left or right to avoid it.</p>
<p>This section takes you one step forward in remote control by providing feedback not only from your human vision, but also from robotic sensors. More precisely, GoPiGo3 will provide data from the Pi camera and from its distance sensor. The goal is that you can teleoperate it more precisely by getting as high-quality sensor data as possible. You may be able to guess at least two common scenarios in the real world where this kind of manual teleoperation is key for the execution of a planned task:</p>
<ul>
<li><strong>Surgical robot teleoperation</strong>: Where an expert surgeon can carry out a surgical operation without being present in the operating room where the patient is being attended to.</li>
<li><strong>Teleoperated rescue robots</strong>: This used in accidents where human cannot access the location on their own, such as a ravine between mountains in the occurrence of a flood, or disasters where direct human presence is to be avoided, for example in a nuclear disaster where the level of radioactivity is so high that an exposed human could absorb a dose of deadly radiation in a few minutes.</li>
</ul>
<p>Having these keys in mind, you should understand this section not only as a prior learning step before entering into autonomous navigation, but also as a motivational introduction to a common way of working with teleoperated robots in the real world.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Adding sensors to the GoPiGo3 model</h1>
                
            
            
                
<p>Up to now, you should have equipped your virtual robot with a differential drive controller that provides the capability to convert velocity commands into rotations of the left and right wheels. We need to complete the model with some sort of perception of the environment. For this, we will add controllers for two common sensors, a two-dimensional camera and an LDS. The first corresponds to the Pi camera of your physical robot, while the second is the unidirectional distance sensor of the GoPiGo3 kit.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Camera model</h1>
                
            
            
                
<div><p>You can add the solid of the camera as usual with <kbd>&lt;visual&gt;</kbd> tags, but since it is a commercial device, you can a get better look by using a realistic three-dimensional CAD model supplied by the manufacturer or made by someone else in the open source community. The URDF definition is as follows:</p>
</div>
<div><pre>&lt;link name="camera"&gt;<br/>  &lt;visual&gt;<br/>    &lt;origin xyz="0.25 0 0.05" rpy="0 1.570795 0" /&gt;<br/>    &lt;geometry&gt;<br/>      &lt;mesh filename="package://virtual_slam/meshes/<strong>piCamera.stl</strong>" scale="0.5 0.5 0.5"/&gt;<br/>    &lt;/geometry&gt;<br/>  &lt;/visual&gt;<br/>  ...<br/>&lt;/link&gt;<br/><br/>&lt;joint name="joint_camera" type="fixed"&gt;<br/>    &lt;parent link="base_link"/&gt;<br/>    &lt;child link="camera"/&gt;<br/>    &lt;origin xyz="0 0 0" rpy="0 0 0" /&gt; <br/>    &lt;axis xyz="1 0 0" /&gt;<br/>&lt;/joint&gt;<br/></pre>
<p>We can see two blocks in the preceding snippet: the <kbd>&lt;link&gt;</kbd> element to specify the solid, and the <kbd>&lt;joint&gt;</kbd> block to attach the camera to the robot chassis. Since the camera is rigidly attach to the body, we specify the <kbd>type="fixed"&gt;</kbd> to model such a characteristic.</p>
<p>Regarding the <kbd>&lt;link&gt;</kbd> element, we introduce the <kbd>&lt;mesh&gt;</kbd> tag to import the geometry from a CAD DAE filetype, marked in bold in the preceding snippet. The following screenshot shows the CAD model of the camera:</p>
</div>
<div><img src="img/9e760d9a-31b7-489b-888f-bdc204d1a3f7.png" style="width:19.75em;height:17.92em;"/></div>
<p>Then we add the camera technical features using a <kbd>&lt;gazebo&gt;</kbd> tag:</p>
<div><pre>&lt;gazebo reference="camera"&gt;<br/>  &lt;<strong>sensor </strong>type="<strong>camera</strong>" name="<strong>camera1</strong>"&gt;<br/>    &lt;update_rate&gt;30.0&lt;/update_rate&gt;<br/>    &lt;<strong>camera name="front"</strong>&gt;<br/>      &lt;horizontal_fov&gt;1.3962634&lt;/horizontal_fov&gt;<br/>      &lt;image&gt;<br/>        &lt;width&gt;800&lt;/width&gt;<br/>        &lt;height&gt;800&lt;/height&gt;<br/>        &lt;format&gt;R8G8B8&lt;/format&gt;<br/>      &lt;/image&gt;<br/>    &lt;clip&gt;<br/>      &lt;near&gt;0.02&lt;/near&gt;<br/>      &lt;far&gt;300&lt;/far&gt;<br/>    &lt;/clip&gt;<br/>    &lt;/<strong>camera</strong>&gt;<br/>    &lt;!-- <strong>plugin "camera_controller"</strong> filename="libgazebo_ros_camera.so" --&gt;<br/>  &lt;/<strong>sensor</strong>&gt;<br/>&lt;/gazebo&gt;</pre></div>
<div><p>The <kbd>&lt;update_rate&gt;</kbd> tag specifies that the sensor is read at a frequency of 30 Hz, that is, it takes 30 images per second. Finally, we add the Gazebo plugin that emulates the behavior of the camera. The following snippet is what substitutes the commented line that referred to <kbd>plugin "camera_controller"</kbd><strong> </strong>in the preceding code block:</p>
<pre>      &lt;<strong>plugin</strong> name="camera_controller" filename="<strong>libgazebo_ros_camera.so</strong>"&gt;<br/>        &lt;alwaysOn&gt;<strong>true</strong>&lt;/alwaysOn&gt;<br/>        &lt;updateRate&gt;0.0&lt;/updateRate&gt;<br/>        &lt;cameraName&gt;<strong>gopigo/camera1</strong>&lt;/cameraName&gt;<br/>        &lt;imageTopicName&gt;<strong>image_raw</strong>&lt;/imageTopicName&gt;<br/>        &lt;cameraInfoTopicName&gt;<strong>camera_info</strong>&lt;/cameraInfoTopicName&gt;<br/>        &lt;frameName&gt;camera&lt;/frameName&gt;<br/>        &lt;hackBaseline&gt;0.07&lt;/hackBaseline&gt;<br/>        &lt;distortionK1&gt;0.0&lt;/distortionK1&gt;<br/>        &lt;distortionK2&gt;0.0&lt;/distortionK2&gt;<br/>        &lt;distortionK3&gt;0.0&lt;/distortionK3&gt;<br/>        &lt;distortionT1&gt;0.0&lt;/distortionT1&gt;<br/>        &lt;distortionT2&gt;0.0&lt;/distortionT2&gt;<br/>      &lt;/<strong>plugin</strong>&gt;<br/></pre></div>
<div><p>The controller for the camera is in the <kbd>libgazebo_ros_camera.so</kbd> file, so what you provide within this block are the technical specifications of the camera you are using. Setting <kbd>&lt;updateRate&gt;</kbd> to <kbd>0.0</kbd> means that Gazebo should take the refreshment rate from the preceding <kbd>&lt;sensor&gt;</kbd> tag, that is, 30 Hz. As specified (see fields in bold letters), camera images will be published in the <kbd>/gopigo/camera1/image_raw</kbd> topic.</p>
<p>Launch the ROS visualization tool to check that the model is properly built. Since <strong>RViz</strong> only represents its visual features—it does not include any physical simulation engine—it is a much lighter environment than Gazebo and you have available all the options to check every aspect of the appearance of the model:</p>
</div>
<pre><strong>$ roslaunch gopigo3_description gopigo3_basic_rviz.launch</strong></pre>
<p>This launch file is very similar to the one you used in <a href="742e6846-70e4-4bd4-8576-f3e4f445df3f.xhtml">Chapter 4</a>, <em>Creating the Virtual Two-Wheeled ROS Robot</em>. The following screenshot shows the result you should see:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/752416bd-2335-4e05-ad11-39114c43716d.png" style="width:27.50em;height:17.33em;"/></p>
<p>In the next section, you will do a practical exercise to see how the camera works with Gazebo.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Simulating the camera</h1>
                
            
            
                
<p>Follow these steps for the simulation:</p>
<ol>
<li>Let's first place the robot in Gazebo the same way we did in the previous chapter and enable remote control with the keyboard:</li>
</ol>
<pre style="padding-left: 60px"><strong>T1 $ roslaunch virtual_slam gopigo3_basic_world.launch <br/>T2 $ rosrun key_teleop key_teleop.py /key_vel:=/cmd_vel</strong></pre>
<p style="padding-left: 60px"><kbd>key_teleop</kbd> allows you to remotely control the GoPiGo3 with the arrow keys of your keyboard.</p>
<ol start="2">
<li>Now, launch a node from the <kbd>image_view</kbd> package that comes preinstalled with ROS:</li>
</ol>
<div><div><pre style="padding-left: 60px"><strong>T3 $ rosrun image_view image_view image:=/gopigo/camera1/image_raw</strong></pre>
<p>We are remapping the <kbd>image</kbd> topic so that the node takes its data from the camera node topic, <kbd>/gopigo/camera1/image_raw</kbd>. This topic is defined in the preceding snippet of the camera controller plugin with the combination of the <kbd>&lt;imageTopicName&gt;</kbd> and<br/>
<kbd>&lt;cameraInfoTopicName&gt;</kbd> tags. Teleoperate the robot with the arrow keys and you will see the subjective view in the image window:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1405 image-border" src="img/4d34617c-f8e6-4f0f-ae1a-e178edec74d1.png" style="width:60.17em;height:31.75em;"/></p>
</div>
<p class="CDPAlignLeft CDPAlign">The background window corresponds to Gazebo (launched from Terminal <kbd>T1</kbd>) and there you can see the virtual robot looking at the traffic cones. The subjective view is shown in the left window (<kbd>T2</kbd>), provided by the Pi camera image live feed using the <kbd>image_view</kbd> package. Finally, the left-bottom window (<kbd>T3</kbd>) is the one you need to select to be able to move the robot with the arrow keys of the keyboard. We have used them to place the robot in front of the traffic cones, as shown in the preceding screenshot.</p>
</div>
<p>At this point, let's obtain the ROS graph with the well-known command, <kbd>rqt_graph</kbd>, and have a look at how the topic remapping for the image is handled:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1406 image-border" src="img/93620544-f345-4af9-8041-866578548eea.png" style="width:65.17em;height:12.17em;"/></p>
<p>Thanks to the mapping argument, <kbd>image:=/gopigo/camera1/image_raw</kbd>, the <kbd>image</kbd> topic of the <kbd>image_view</kbd> package remains implicit and just the <kbd>/gopigo/camera1/image_raw</kbd> is visible.</p>
<p>Are you aware how quick and easy it is to deliver a robot behavior when you are using prebuilt ROS modules and your custom robot definition? In the next section, we will cover these same steps for the second sensor.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Distance sensor</h1>
                
            
            
                
<p>We add the solid model of this sensor under the <kbd>&lt;visual&gt;</kbd> tag by following the same procedure we covered for the camera. The URDF definition is as follows:</p>
<div><pre>&lt;joint name="<strong>distance_sensor_solid_joint</strong>" type="fixed"&gt;<br/>    &lt;axis xyz="0 1 0" /&gt;<br/>    &lt;origin rpy="0 0 0" xyz="0 0 0" /&gt;<br/>    &lt;parent link="base_link"/&gt;<br/>    &lt;child link="distance_sensor_solid"/&gt;<br/>  &lt;/joint&gt;<br/><br/>  &lt;link name="<strong>distance_sensor_solid</strong>"&gt;<br/>    &lt;visual&gt;<br/>      &lt;origin xyz="0.2 0 0.155" rpy="1.570795 0 1.570795" /&gt;<br/>      &lt;geometry&gt;<br/>        &lt;mesh filename="package://gopigo3_description/meshes/IR_Sensor_Sharp_GP2Y_solid.stl" scale="0.005 0.005 0.005"/&gt;<br/>      &lt;/geometry&gt;<br/>      &lt;material name="red"/&gt;<br/>    &lt;/visual&gt;<br/>    ...<br/>  &lt;/link&gt;</pre>
<p>We can see two blocks in the preceding snippet: the <kbd>&lt;link&gt;</kbd> element to specify the solid, and the <kbd>&lt;joint&gt;</kbd> block to attach the sensor body to the robot chassis. Since the distance sensor is rigidly attach to the robot chassis, we specify the <kbd>type="fixed"&gt;</kbd> to model this characteristic. The solid model that we are using is shown in the following screenshot. In this case, we use a CAD model in STL format and reference it from the <kbd>&lt;mesh&gt;</kbd> tag:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/da1def45-c0f9-4659-9a4a-a0f00ab1e0aa.png" style="width:37.17em;height:28.92em;"/></p>
</div>
<p>We will base the sensor itself in another solid, since, if you do this with the solid in the preceding screenshot, you will see in Gazebo that the distance rays are blocked by the solid, and so the sensor will always produce a zero value for distance. So, we are going to explain to you a trick with which you can separate the solid model of the sensor from the sensing point, located at the origin of the link frame:</p>
<pre>&lt;joint name="<strong>distance_sensor_joint</strong>" type="fixed"&gt;<br/>    &lt;axis xyz="0 1 0" /&gt;<br/>    &lt;origin rpy="0 0 0" xyz="0.245 0 0.13" /&gt;<br/>    &lt;parent link="base_link"/&gt;<br/>    &lt;child link="distance_sensor"/&gt;<br/>&lt;/joint&gt;<br/>&lt;link name="<strong>distance_sensor</strong>"&gt;<br/>    &lt;visual&gt;<br/>         &lt;origin xyz="0 0 0" rpy="0 0 0"/&gt;<br/>         &lt;geometry&gt;<br/>             &lt;box size="0.01 0.01 0.01"/&gt;<br/>         &lt;/geometry&gt;<br/>         &lt;material name="red"/&gt;<br/>    &lt;/visual&gt;<br/>&lt;/link&gt;</pre>
<p>This snippet creates a box of 10 cm x 10 cm and place it in the coordinates specified by the <kbd>&lt;joint&gt;</kbd> tag. </p>
<p>Then we add the sensor technical features using a <kbd>&lt;gazebo&gt;</kbd> tag, which you can see refers to the <kbd>distance_sensor</kbd> link defined in the preceding snippet (not <kbd>distance_sensor_solid</kbd>):</p>
<div><pre>&lt;gazebo reference="<strong>distance_sensor</strong>"&gt; <br/>   &lt;<strong>sensor</strong> type="<strong>ray</strong>" name="<strong>laser_distance</strong>"&gt;<br/>      &lt;visualize&gt;<strong>true</strong>&lt;/visualize&gt;<br/>      &lt;update_rate&gt;10&lt;/update_rate&gt;<br/>      &lt;ray&gt;<br/>         ...<br/>         &lt;range&gt;<br/>            &lt;min&gt;0.01&lt;/min&gt;<br/>            &lt;max&gt;3&lt;/max&gt;<br/>            &lt;resolution&gt;0.01&lt;/resolution&gt;<br/>         &lt;/range&gt;<br/>      &lt;/ray&gt;<br/>      &lt;!-- <strong>plugin</strong> filename="<strong>libgazebo_ros_range.so</strong>" name="gazebo_ros_ir" --&gt;<br/>    &lt;/sensor&gt; <br/>   &lt;/gazebo&gt;</pre></div>
<div><p>The <kbd>&lt;update_rate&gt;</kbd> tag specifies that the sensor is read at a frequency of 10 Hz, and the <kbd>&lt;range&gt;</kbd> tag sets measured distance values between 10 cm and 3 m at 1 cm resolution.</p>
</div>
<div><p>The <kbd>&lt;visualize&gt;<strong>true</strong>&lt;/visualize&gt;</kbd> tag block allows you to see in Gazebo the laser ray of the distance sensor covering the <kbd>&lt;range&gt;</kbd> limits explained here; that is, its detection coverage reaches up to 3 meters.</p>
</div>
<p>Finally, we add the Gazebo plugin that emulates the behavior of the distance sensor. The following snippet is what substitutes the commented line referring to <kbd>plugin "gazebo_ros_ir"</kbd><strong> </strong>in the preceding code block:</p>
<div><pre>       &lt;<strong>plugin</strong> filename="<strong>libgazebo_ros_range.so</strong>" name="gazebo_ros_ir"&gt;<br/>         &lt;gaussianNoise&gt;0.005&lt;/gaussianNoise&gt;<br/>         &lt;alwaysOn&gt;true&lt;/alwaysOn&gt;<br/>         &lt;updateRate&gt;0.0&lt;/updateRate&gt;<br/>             &lt;topicName&gt;<strong>gopigo/distance_sensor</strong>&lt;/topicName&gt;<br/>             &lt;frameName&gt;<strong>distance_sensor</strong>&lt;/frameName&gt;<br/>         &lt;radiation&gt;INFRARED&lt;/radiation&gt;<br/>         &lt;fov&gt;0.02&lt;/fov&gt;<br/>       &lt;/plugin&gt; </pre></div>
<div><p>The controller for the distance sensor is in the <kbd>libgazebo_ros_range.so</kbd> file, so what you provide within this block are the technical specifications of the sensor you are using. Setting the <kbd>&lt;updateRate&gt;</kbd> tag to <kbd>0.0</kbd> means that Gazebo should take the refreshment rate from the preceding <kbd>&lt;sensor&gt;</kbd> tag, that is, 10 Hz. As specified (see fields in bold letters), range values will be published in the <kbd>/sensor/ir_front</kbd> topic.</p>
<p>Launch the ROS visualization tool to check that the model is properly built. Since <strong>RViz</strong> only represents its visual features, it is a much lighter environment than Gazebo and you have available all the options to check every aspect of the appearance of the model:</p>
</div>
<pre><strong>$ roslaunch gopigo3_description gopigo3_basic_rviz.launch</strong></pre>
<p>In the following screenshot, you can see the result together with the camera that we included earlier:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/e9d6f056-2e71-49b1-8202-463f99b4f90f.png" style="width:50.42em;height:35.83em;"/></p>
<p>In the next section, you will do a practical exercise to see how it works with the distance sensor under Gazebo.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Simulating the distance sensor</h1>
                
            
            
                
<p>This test includes both the distance sensor and the two-dimensional camera. Run the example by using four Terminals, as indicated in the following code:</p>
<pre><strong>T1 $ roslaunch virtual_slam gopigo3_basic_world.launch</strong><br/><strong>T2 $ rosrun key_teleop key_teleop.py /key_vel:=/cmd_vel</strong><br/><strong>T3 $ rostopic echo /gopigo/distance_sensor</strong><br/><strong>T4 $ rosrun image_view image_view image:=/gopigo/camera1/image_raw</strong></pre>
<p>The following screenshot is a composed view of the result you should obtain:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1407 image-border" src="img/a86a6a72-eed5-4b33-931f-f7bc71c827da.png" style="width:64.25em;height:25.58em;"/></p>
<p>In the preceding screenshot, you can find the following components:</p>
<ul>
<li>The central window is the Gazebo one, where you can see GoPiGo3, an obstacle, and the rays of the distance sensor.</li>
<li>The top-left gray window is the one we need to have selected so that arrow-key pushes are received as <kbd>/cmd_vel</kbd> topic messages for remote control.</li>
<li>The bottom-left black window shows in real time the messages transmitted to the topic of the distance sensor, that is, <kbd>/gopigo/distance_sensor</kbd>. The current distance to the obstacle is found in the <kbd>range</kbd> field, with a value of 1.13 m.</li>
<li>The right window shows the live view seen by the robot thanks to its two-dimensional camera, received in the<kbd>/gopigo/camera1/image_raw</kbd> topic.</li>
</ul>
<p>You can manually drive from one side of the scene to the other without crashing into any of the furniture. You plan—as a human—the optimal trajectory, and execute it to bring the robot to the destination goal while avoiding the obstacles. What you have done yourself previously is what the robot now has to do itself, performing as well as possible. This task is known as <strong>navigation</strong> and is what we are going to cover in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Components in navigation</h1>
                
            
            
                
<p>Navigation is the movement of a robot from the current position to a target location following a planned trajectory. This ability in a robot means that it is capable of determining its position at any point along the trajectory, as well as to setting up a plan of action given a representation of the environment, such as a map. We should also add the ability to avoid dynamic obstacles or others that were not present when the map was built for the first time.</p>
<p>There are four components to consider when building the navigation ability:</p>
<ul>
<li class="mce-root">A map of the environment, preexisting and given to the robot as an input, or built by its own means using the sensory data that it collects with its sensors. This whole process, that is, data acquisition plus interpretation, constitutes what we call the capability of robot perception. One well-known technique that takes advantage of robot perception is known as SLAM, as discussed earlier.</li>
<li>Real-time pose, understood as the ability of a robot to locate itself in terms of position and rotation (together referred to as pose) with respect to a fixed frame of reference in the environment. The typical technique in robotics for obtaining pose is known as dead reckoning, in which the current pose is estimated relative to the previous one plus internal odometry data—coming from the rotary encoders of the motors—and IMU sensor data to reduce the error of these calculations. Both of them are present in the GoPiGo3. </li>
<li>Robot perception, which arises from the combination of sensor data plus its interpretation, making the robot aware of the objects and obstacles that are around. In the GoPiGo3, the sensors that contribute to perception are the distance sensor, the Pi camera, and the LDS.</li>
</ul>
<ul>
<li>Path planning and execution, which includes the calculation of the optimal path and its execution so that the robot can achieve the target location. Since the map does not include all the details of the environment and there can be dynamic obstacles, the path planning should also be dynamic. Its algorithm will be better as it will be able to adapt to the varying conditions in the environment.</li>
</ul>
<p>Next, we will cover the costmap, a key concept on top of which navigation is based.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Costmaps for safe navigation</h1>
                
            
            
                
<p>The costmap for robot navigation arises from the combination of the robot's pose, estimated from the odometry data (encoders) and the IMU sensor, the perception of objects and obstacles in the environment using the distance sensor and LDS, and the <strong>occupancy grid map</strong> (<strong>OGM</strong>) obtained from the SLAM technique.</p>
<p>These sources of information provide as output a joint measurement of obstacle areas, probable collisions, and the movable area for the robot. There is a global costmap and a local one. The global one accounts for the navigation path using the fixed map obtained through SLAM, while the local version allows the robot to deal with the fine-grained details of its immediate environment to move around obstacles and avoid collisions.</p>
<p>The costmap, be it local or global, is measured in a range of 8 bits, that is, a value from 0 to 255 in each cell of the grid occupancy map. A zero value means a free area, and 255 is an occupied area. Values near 255 account for collision areas, while intermediate values range from low collision probabilities (0-127) to high (128-252). </p>
<p>In the next section, we will finally deal with SLAM, the technique that is at the core of robot navigation. As a starting point, we will complete the setup of the GoPiGo3 perception capability with the integration of an LDS.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Robot perception and SLAM</h1>
                
            
            
                
<p>The most straightforward way to implement robot navigation in ROS is by using an LDS that provides 360° coverage, allowing the robot to be aware of all the objects and obstacles around it.</p>
<p>In the introduction to this chapter, we identified the <strong>EAI YDLIDAR X4 </strong>as a low-cost option that can be integrated with our physical robot. That will be covered in the next chapter, while in the present one we will develop its virtual model to be integrated in Gazebo.</p>
<p>The next subsection extends the virtual GoPiGo3 that we've worked on in this chapter to include this very model of LDS. Afterward, we will deploy a quick SLAM example to get an overview of what this functionality can provide to robot navigation.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Adding a Laser Distance Sensor (LDS)</h1>
                
            
            
                
<p>The process to add the sensor is similar to what we did for the distance sensor in the previous section. Follow these steps to do it:</p>
<ol>
<li>We add the solid model of this sensor under the <kbd>&lt;visual&gt;</kbd> tag by following the same procedure we covered for the previous sensors. The URDF definition is as follows:</li>
</ol>
<div><pre style="padding-left: 60px">  &lt;link name="<strong>base_scan</strong>"&gt;<br/>    &lt;visual name="<strong>sensor_body</strong>"&gt;<br/>      &lt;origin xyz="0 0 0" rpy="0 0 0" /&gt;<br/>      &lt;geometry&gt;<br/>        &lt;mesh filename="package://gopigo3_description/meshes/TB3_lds-01.stl" scale="0.003 0.003 0.003"/&gt; <br/>      &lt;/geometry&gt;<br/>      &lt;material name="yellow"/&gt;<br/>    &lt;/visual&gt;<br/>    &lt;visual name="<strong>support</strong>"&gt;<br/>      &lt;origin xyz="0 0 -0.0625" rpy="0 0 0" /&gt;<br/>      &lt;geometry&gt;<br/>        &lt;cylinder length="0.12" radius="0.1" /&gt;<br/>      &lt;/geometry&gt;<br/>    &lt;/visual&gt; <br/>  &lt;/link&gt;</pre>
<p style="padding-left: 60px">We can see two <kbd>&lt;visual&gt;</kbd> blocks within the <kbd>&lt;link&gt;</kbd> element in the preceding snippet: <kbd>sensor_body</kbd> is the LDS itself, and <kbd>support</kbd> creates the physical interface between the sensor and the robot chassis. The solid model that we are using for the sensor body is the one shown in the following screenshot, which consists of a CAD model in STL format referenced from the <kbd>&lt;mesh&gt;</kbd> tag:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/47254f3d-453d-4cfe-b185-c5d91356bac0.png" style="width:34.67em;height:31.25em;"/></p>
<ol start="2">
<li>Next, we add a <kbd>&lt;joint&gt;</kbd> element of <kbd>&lt;type="fixed"&gt;</kbd> to attach the sensor assembly to the robot chassis:</li>
</ol>
<pre style="padding-left: 60px">  &lt;joint name="<strong>scan_joint</strong>" type="fixed"&gt;<br/>    &lt;parent link="base_link"/&gt;<br/>    &lt;child link="base_scan"/&gt;<br/>    &lt;origin xyz="-0.1 0 0.25" rpy="0 0 0"/&gt;<br/>  &lt;/joint&gt;</pre></div>
<ol start="3">
<li>Then we add the sensor technical features using a <kbd>&lt;gazebo&gt;</kbd> tag that you can see refers to the <kbd>distance_sensor</kbd> link defined in the preceding snippet (not <kbd>distance_sensor_solid</kbd>):</li>
</ol>
<div><pre style="padding-left: 60px">  &lt;gazebo reference="base_scan"&gt;<br/>    &lt;sensor type="ray" name="lds_lfcd_sensor"&gt;<br/>      &lt;visualize&gt;<strong>true</strong>&lt;/visualize&gt;<br/>      &lt;update_rate&gt;5&lt;/update_rate&gt;<br/>      &lt;ray&gt;<br/>        &lt;scan&gt;<br/>          &lt;horizontal&gt; <strong>&lt;samples&gt;721&lt;/samples&gt;</strong> ... &lt;/horizontal&gt;<br/>        &lt;/scan&gt;<br/>        &lt;range&gt;<br/>          &lt;min&gt;0.12&lt;/min&gt;<br/>          &lt;max&gt;10&lt;/max&gt;<br/>          &lt;resolution&gt;0.015&lt;/resolution&gt;<br/>        &lt;/range&gt;<br/>      &lt;/ray&gt;<br/>        &lt;!-- plugin name="gazebo_ros_lds_lfcd_controller" filename="<strong>libgazebo_ros_laser.so</strong>" --&gt;<br/>    &lt;/sensor&gt;<br/>  &lt;/gazebo&gt;</pre></div>
<p style="padding-left: 60px">The <kbd>&lt;range&gt;</kbd> tag sets measured distance values between 12 cm and 10 m, as can be found in the technical specification of the EAI YDLIDAR X4. Pay special attention to the <kbd>&lt;visualize&gt;true&lt;/visualize&gt;</kbd> tag, since, with a sensor like this, with 360º vision, the screen will be filled with rays to show the angle range that it covers. It is recommended to set this to <kbd>false</kbd> once you have visually checked that the sensor is working properly.</p>
<p>The <kbd>&lt;visualize&gt;true&lt;/visualize&gt;</kbd> tag block has the same meaning and effect for the distance sensor, as explained in the previous section when we built its model, in the <em>Distance sensor</em> subsection. The only difference is that the LDS covers all angles with 360º coverage, tracing as many rays as the number of samples specified inside the <kbd>&lt;samples&gt;</kbd> tag.</p>
<p style="padding-left: 60px">The <kbd>&lt;update_rate&gt;</kbd> tag specifies that the sensor is read at a frequency of 5 Hz, but the specification of the LDS is 5,000 Hz. Why don't we put the actual value? This is for CPU usage reasons:</p>
<p style="padding-left: 60px">Bear in mind that, if we set the reading frequency at its actual physical capability, it will take 5,000 samples per second, and each sample is a vector of 720 points.</p>
<p>Since LDS covers all possible directions, to get 720 rays evenly spaced at 0.5º, you have put one more sample, that is, 721, since 0º and 360º are actually the same angle.</p>
<p style="padding-left: 60px">Each point will be characterized by two float values (64 bits), so each sample needs 720 x 2 x 64 = 92160 bits = 11 Kb. Since there would be 5,000 samples, we would need a bandwidth of 53 Mb/s. That's a huge value to be managed by a Raspberry Pi CPU.</p>
<p style="padding-left: 60px">Since the robot will move at low speed, there is no need to have such a high-frequency reading, so we can limit it to only 5 Hz, which will have no impact on the robot behavior. This will require only 55 Kb/s of bandwidth, 1,000 times lower than what the sensor can provide.</p>
<p style="padding-left: 60px">This is a clear example of why you should not directly introduce the specifications of sensors within Gazebo, since it can impact the performance of the simulation. You need to critically analyze each sensor and decide what parameters to set in its virtual controller so that it reproduces the actual behavior well, while not unnecessarily overloading the CPU.</p>
<ol start="4">
<li>The next step is to add the Gazebo plugin that emulates the behavior of the distance sensor. The following snippet is what substitutes the commented line referring to <kbd>plugin "gazebo_ros_lds_lfcd_controller"</kbd><strong> </strong>in the preceding code block:</li>
</ol>
<pre>      &lt;plugin name="gazebo_ros_lds_lfcd_controller" filename="<strong>libgazebo_ros_laser.so</strong>"&gt;<br/>        &lt;topicName&gt;<strong>/gopigo/scan</strong>&lt;/topicName&gt;<br/>        &lt;frameName&gt;<strong>base_scan</strong>&lt;/frameName&gt;<br/>      &lt;/plugin&gt;</pre>
<p style="padding-left: 60px">The controller for the distance sensor is in the <kbd>libgazebo_ros_laser_range.so</kbd> file, so what you provide within this block are the technical specifications of the sensor for which you want to override the values provided in the <kbd>&lt;sensor&gt;</kbd> tag in the preceding snippet. As specified (see fields in bold letters), the range values will be published in the <kbd>/gopigo/scan</kbd> topic.</p>
<ol start="5">
<li>Finally, launch the ROS visualization tool to check that the model is properly built. Since RViz only represents its visual features, it is a much lighter environment than Gazebo and you have available all the options to check every aspect of the appearance of the model:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ roslaunch gopigo3_description gopigo3_rviz.launch</strong></pre>
<p style="padding-left: 60px">In the following screenshot, you can see the result together with the camera that we included earlier:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/191d5439-d7ed-4e6e-8f3c-bd7346cdfbfc.png" style="width:41.75em;height:32.17em;"/></p>
<p>In the next subsection, you will do a practical exercise to see how it works the laser distance sensor under Gazebo.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Simulating the LDS</h1>
                
            
            
                
<p>After including the LDS model in the virtual robot, we can proceed to see how it works by running the simulation in Gazebo:</p>
<ol>
<li>Execute the following commands in separate Terminals to see the sensor in action:</li>
</ol>
<pre style="padding-left: 60px"><strong>T1 $ roslaunch virtual_slam gopigo3_world.launch</strong><br/><strong>T2 $ rosrun key_teleop key_teleop.py /key_vel:=/cmd_vel</strong><br/><strong>T3 $ rostopic echo /gopigo/scan</strong></pre>
<p style="padding-left: 60px">In <kbd>T3</kbd>, you will see a large feed of data, since each <kbd>LaserScan</kbd> message contains 720 points to cover the 360° view around the sensor.</p>
<ol start="2">
<li>To test this sensor, it is better to use a Python script that makes the robot wander in the environment while avoiding the obstacles. To do this, we have implemented the following rules in our script:</li>
</ol>
<ul>
<li style="list-style-type: none">
<ul>
<li>If there is no obstacle, move forward at a reference speed of 0.8 m/s.</li>
<li>If the range provided by the distance sensor is lower than 2 meters, go back and rotate counter-clockwise until avoiding the obstacle.</li>
<li>Since the distance sensor throws unidirectional measurements, we should check the measurements from the LDS to find if there are obstacles to the sides, and the threshold should be lower than 1.6 meters. If obstacles are detected, go back and rotate counter-clockwise faster to avoid the obstacle and not get stuck on it.</li>
</ul>
</li>
</ul>
<p style="padding-left: 60px">This simple algorithm is implemented in the <kbd>wanderAround.py</kbd> script, and can be found under the <kbd>./virtual_slam/scripts/wanderAround.py</kbd> folder.</p>
<ol start="3">
<li>Now, give it a try, and enjoy watching how the GoPiGo3 goes from one side of the world to the other while avoiding obstacles. The sequence to run is the following:</li>
</ol>
<pre style="padding-left: 60px"><strong>T1 $ roslaunch virtual_slam gopigo3_world.launch</strong><br/><strong>T2 $ rosrun virtual_slam wanderAround.py</strong></pre>
<p style="padding-left: 30px">The following screenshot shows the robot wandering around:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/75cee70f-3dd8-4cac-b221-d52f1228a2f7.png" style="width:44.75em;height:32.83em;"/></p>
<p>To finish this section, we will briefly cover the key concepts of the SLAM theory so that you know what's under the hood when we proceed in the last section of the chapter, covering the practical part of this implementation of robot navigation.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">SLAM concepts</h1>
                
            
            
                
<p>SLAM allows the robot to build a map of the environment using the following two sources of information:</p>
<ul>
<li>Robot pose estimation, coming from the internal odometry (rotary encoders) and IMU sensor data</li>
<li>Distance to objects, obstacles and walls, coming from distance sensors, the LDS in particular</li>
</ul>
<p>In its most basic version, a map includes two-dimensional information, while in more advanced applications using industrial-grade LIDAR sensors, a richer map is built using three-dimensional information from LIDAR and/or from three-dimensional cameras. For the purpose of our learning path, we will deal with the two-dimensional OGM, also very common in ROS projects.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Occupancy Grid Map (OGM)</h1>
                
            
            
                
<p>Take the example of a square room with four static obstacles inside. The following diagram shows the map generated using SLAM in ROS (you will later learn how to generate it yourself):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1408 image-border" src="img/7e521a1a-e44d-40dd-a401-bda67cd033c1.png" style="width:26.92em;height:23.08em;"/></p>
<p>In such a two-dimensional map, the free areas and occupied areas are drawn in different intensities of gray in 8-bit format (0-255 range, as was already mentioned earlier when describing the costmaps). Then, the occupancy probability for each cell is obtained as the difference between 255 and the intensity value, divided by 255. This means the following:</p>
<ul>
<li>White areas (255 value) give a 0% probability; that is, there is no obstacle in them.</li>
<li>Black areas (0 value) give a 100% probability; that is, they are occupied.</li>
</ul>
<p>This probability distribution allows a costmap to be built that helps the robot to determine which trajectory to select to achieve the target location. When published to ROS, the occupancy probabilities translate into integer values between 0 (0% probability, that is, free space) and 100 (100%, that is, occupied space). A value of -1 is assigned to unknown areas. Map information is stored using two files:</p>
<ul>
<li>A <kbd>.pgm</kbd> format file, known as portable graymap format.</li>
<li>A <kbd>.yaml</kbd> file containing the configuration of the map. See the following example of its content:</li>
</ul>
<div><pre style="padding-left: 60px">image: ./test_map.pgm<br/>resolution: 0.010000<br/>origin: [-20.000000, -20.000000, 0.000000]<br/>negate: 0<br/>occupied_thresh: 0.65<br/>free_thresh: 0.196</pre>
<p>The most interesting parameters are the last two:</p>
<ul>
<li> <kbd>occupied_thresh = 0.65</kbd> means that a cell is considered as occupied if its probability is above 65%.</li>
<li><kbd>free_thresh = 0.196</kbd> establishes the threshold value below which the cell is considered free, that is, 19.6%.</li>
</ul>
<p>Given the size in pixels of the image, it is straightforward to infer the physical dimension of the cells in the map. This value is indicated by the <kbd>resolution</kbd> parameter, that is, 0.01 meter/pixel.</p>
</div>


            

            
        
    

        

                            
                    <h1 class="header-title">The SLAM process</h1>
                
            
            
                
<p>Building the map using a Gazebo simulation involves employing the following workflow:</p>
<ol>
<li>Launch the robot model within a modeled environment.</li>
<li>Launch the mapping ROS package.</li>
<li>Launch a special visualization in RViz that lets us see the areas the robot is scanning as it moves.</li>
<li>Teleoperate the robot to make it cover as much as possible of the surface of the virtual environment.</li>
<li>Once the exploration is finished, save the map, generating the two files in the formats indicated in the preceding section, that is, <kbd>.pgm</kbd> and <kbd>.yaml</kbd>.</li>
</ol>
<p>Having finished this information acquisition phase, we are ready for the robot to try and successfully complete a navigation task.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The navigation process</h1>
                
            
            
                
<p>Once your robot has generated a map, it will use it to plan a path to a given target destination. The process of executing such a plan is called navigation, and involves the following steps:</p>
<ol>
<li>Launch the robot model within the modeled environment. This step is the same as the first step in the SLAM process described earlier.</li>
<li>Provide the costmap that the robot built before. Bear in mind that the map is a characteristic of the environment, not of the robot. Hence, you can build the map with one robot and use the same map in navigation for any other robot you put in the same environment.</li>
<li>Set up the navigation algorithm. We will use the <strong>Adaptive Monte Carlo Localization</strong> (<strong>AMCL</strong>) algorithm, the most common choice for effective navigation. It is out of the scope of the book to describe such algorithms, but useful references are provided in the further reading section at the end of the chapter.</li>
<li>Launch a RViz visualization that will let you visualize the robot in the environment and easily mark the target pose (position and orientation) that it should achieve.</li>
<li>Let the robot navigate autonomously to the target location. At this point, you can relax and enjoy watching how the GoPiGo3 drives to the indicated position while avoiding the obstacles and minimizing the distance it has to cover.</li>
</ol>
<p>Should you want the robot to navigate to another location, you just have to indicate it in RViz once it has reached the previous target. Now it is time to see the preceding two processes—SLAM and navigation—in action. That is the scope of the last section of this chapter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Practising SLAM and navigation with the GoPiGo3</h1>
                
            
            
                
<p>Like it was mentioned at the end of the previous section, we are going to run an end-to-end example of SLAM and navigation with GoPiGo3. The first process deals with building a map of the environment using SLAM. Let's retrace the steps listed in the preceding section and see how to execute each of them in ROS.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Exploring the environment to build a map using SLAM</h1>
                
            
            
                
<p>Let's follow these steps to build the map of a simple Gazebo world called <kbd>stage_2.world</kbd>:</p>
<ol>
<li>Launch the robot model within a modeled environment by running the following line of code:</li>
</ol>
<div><pre style="padding-left: 60px"><strong>T1 $ roslaunch virtual_slam gopigo3_world.launch world:=stage_2.world</strong><br/></pre></div>
<p style="padding-left: 60px">This command launches Gazebo and places the GoPiGo3 model in the middle of it, as shown in the following screenshot:</p>
<div><img src="img/1f637221-af4b-4b01-98af-6f5bcfd49e48.png" style="width:25.17em;height:24.17em;"/></div>
<p style="padding-left: 60px">The environment consists of a square space with four static obstacles. The two-dimensional map we used in the <em>Occupancy Grid Map (OGM)</em> subsection of the previous section corresponds to this Gazebo world, whose filename is <kbd>stage_2.world</kbd>.</p>
<p>You can see that this world is by far simpler than the one we used in the first part of the chapter (there is an even simpler environment without the obstacles, named <kbd>stage_1.world</kbd>). We use this to illustrate the navigation concepts with a minimal setup for better understanding.<br/>
<br/>
It is left as an exercise for the reader to repeat this process with the Gazebo world from the first <em>Dynamic simulation using Gazebo</em> section. To do so, just omit the <kbd>world</kbd> argument so that it takes the default specified within the launch file. The command to execute this simulation is <kbd>$ roslaunch virtual_slam gopigo3_world.launch</kbd></p>
<p style="padding-left: 60px">Finally, take into account that we can specify any other environment we want to use in Gazebo by setting the <kbd>world</kbd> parameter to the filename of the one selected (available worlds are located inside the <kbd>./virtual_slam/worlds</kbd> folder of the code of this chapter):</p>
<div><pre style="padding-left: 60px"><strong>T1 $ roslaunch virtual_slam gopigo3_world.launch world:= &lt;OTHER_WORLD.world&gt;</strong></pre></div>
<ol start="2">
<li>Launch the SLAM mapping ROS package, including an RViz visualization that superimposes the virtual model of the robot with the actual scan data:</li>
</ol>
<div><pre style="padding-left: 60px"><strong>T2 $ roslaunch virtual_slam gopigo3_slam.launch</strong></pre></div>
<p style="padding-left: 60px">The appearance of the RViz window is shown in the following screenshot, where you can see together the virtual robot and the scan data (green points) in real time. The light-gray-colored areas are what the robot is actually perceiving with its LDS sensor, while the non colored areas (shadow spaces behind the obstacles) are not yet known by the GoPiGo3:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/028c1384-3570-4e8f-891a-9e7a56253871.png" style="width:21.00em;height:20.92em;"/></p>
<p style="padding-left: 60px">In the next step, we will explore the full environment to build the map.</p>
<ol start="3">
<li>Teleoperate the robot to make it cover as much as possible of the surface of the current Gazebo world. Let's do this as usual with the teleoperation package:</li>
</ol>
<div><pre style="padding-left: 60px"><strong>T3 $ rosrun key_teleop key_teleop.py /key_vel:=/cmd_vel</strong></pre></div>
<p style="padding-left: 60px">As you move the robot, the LDS sensor will acquire scan data from the unknown areas, and you will receive feedback in the RViz window:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/bf884e89-b642-45b0-9a26-11396d67ef2a.png" style="width:21.75em;height:20.42em;"/></p>
<p style="padding-left: 60px">In the preceding screenshot, you can see that, after wandering in the environment, only the bottom-left part is not scanned. Then move the robot to that location, and, as soon as you have all the space filled with a homogeneous color (light gray), proceed to step 4 in order to save the map.</p>
<ol start="4">
<li>Once you've finished the exploration, save the map, generating two files of the formats indicated in the preceding <em>SLAM process</em> subsection, that is, <kbd>.pgm</kbd> and <kbd>.yaml</kbd>:</li>
</ol>
<div><pre style="padding-left: 60px"><strong>T4 $ rosrun map_server map_saver -f ~/catkin_ws/map_stage_2</strong></pre></div>
<p style="padding-left: 60px">You will get two files in the root folder of your workspace: <kbd>map_stage_2.pgm</kbd> and <kbd>map_stage_2.yaml</kbd>.</p>
<p>The appearance of the generated map is shown in the preceding <em>Occupancy Grid Map (OGM)</em> subsection.</p>
<p>Provided with the map, we are ready to perform robot navigation with the GoPiGo3.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Driving along a planned trajectory using navigation</h1>
                
            
            
                
<p>First, close all open Terminals. Then, as in the SLAM process, let's proceed step by step to perform some navigation:</p>
<ol>
<li>Launch the robot model within the modeled environment. This step is the same as the first step in the SLAM process:</li>
</ol>
<pre style="padding-left: 60px"><strong>T1 $ roslaunch virtual_slam gopigo3_world.launch</strong></pre>
<ol start="2">
<li>Set up the navigation algorithm and launch RViz. We will use AMCL, the most common choice for effective navigation. It is out of the scope of the book to describe such algorithm, but you are provided with useful references in the<em> Further reading </em>section at the end of the chapter.</li>
</ol>
<p style="padding-left: 60px">In this step, we also provide the costmap that the robot built before. To do this, you just have to reference the <kbd>.yaml</kbd> map file you created before. Make sure that the corresponding <kbd>.pgm</kbd> file has the same name and is placed in the same location. This point is specified in the <kbd>roslaunch</kbd> command through the <kbd>map_file</kbd> argument:</p>
<div><pre style="padding-left: 60px"><strong>T2 $ roslaunch virtual_slam gopigo3_navigation.launch map_file:=$HOME/catkin_ws/map_stage_2.yaml</strong></pre></div>
<ol start="3">
<li>The RViz window, shown in the following screenshot, lets you visualize the robot in the environment and mark the target pose (position and orientation) that it should achieve:</li>
</ol>
<div><img src="img/50a5ea86-a012-47f8-b2f8-f2dec49c6eaa.png" style="width:25.17em;height:24.83em;"/></div>
<p>Find the 2D Nav Goal button at the top-right of the RViz window. You will use it to mark the target location to which the robot should navigate.</p>
<p style="padding-left: 60px">First of all, you have to tell the robot that this is the initial pose by pressing the <strong>2D Pose Estimate </strong>button. Then, mark it on screen (in this particular case, it isn't necessary, since the initial pose is the same as the one the robot had when it started to build the map in the preceding subsection).</p>
<p style="padding-left: 60px">Afterward, you can press <strong>2D Nav Goal</strong> button and set the target to the <em>bottom-left corner</em> by clicking the left mouse button. Release the mouse when the arrow has the desired orientation. After releasing, the robot will compute the path to follow and start navigating autonomously:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/0bb18a70-d0d2-40ba-8653-1d4807050717.png" style="width:24.83em;height:23.92em;"/></p>
<p style="padding-left: 60px">The preceding screenshot shows the first instant of the navigation plan execution. The orientation of the red arrow tells the GoPiGo3 in what direction it should stay facing once it has arrived at the target, and the curved line going from the robot to the target is the planned path. Since it has a map of the environment available, the robot is able to plan a path that avoids the obstacles. Wait a few seconds and you will see how the robot reaches the target without any external help.</p>
<ol start="4">
<li>Once the robot has arrived, press the <strong>2D Nav Goal</strong> button again and then mark the top-right corner. The following screenshot shows the first instant of the execution of the next navigation plan:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/d094c479-4c53-4f8a-be7b-78af194f3844.png" style="width:24.58em;height:23.58em;"/></p>
<p style="padding-left: 60px">You can see the new planned path and how this takes into account the presence of obstacles along the way to avoid collisions. The blue square around the robot represents the local window for obstacle avoidance planning. This is used by the <strong>Dynamic Window Approach</strong> (<strong>DWA</strong>) method, which generates a local path that efficiently evades the obstacles. The DWA method performs the calculations taking into account the robot's dynamics, in particular, its limited velocity and acceleration.</p>
<p>The AMCL algorithm for robot navigation generates the global path to reach the target based on the provided map, while the DWA method calculates the local path that accounts for the local conditions the robot may find near it. The latter provides the capability to deal both with obstacles present in the map, and also dynamic ones, such as people crossing the robot's path, for example. The <em>global path</em> and <em>local path</em> combine together to produce <em>highly autonomous robot navigation</em>.</p>
<p style="padding-left: 60px">The following screenshot shows the GoPiGo3 in the final instant before reaching the goal. Appreciate how, at this point, the DWA window also includes the target:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/26c3de76-b819-42f1-a2bc-fd0c6d5d478f.png" style="width:25.17em;height:24.50em;"/></p>
<ol start="5">
<li class="CDPAlignLeft CDPAlign">Finally, you should frame robot navigation within a sequence of tasks that the robot has to complete, one after the other, in order to achieve the goal that has been set by the user. Taking the examples of the navigation paths that we have used already in this chapter for explanation purposes, imagine a scenario in which the GoPiGo3 has to pick up an object in location <em>A</em> (the left-bottom corner) and deliver it to location <em>B</em> (the upper-right corner). In this case, the sequence of tasks would be as follows:
<ol>
<li>Navigate to location <em>A</em>.</li>
<li>Pick up the piece at location <em>A</em>.</li>
<li>Navigate to location <em>B</em>.</li>
<li>Drop off the piece at location <em>B</em>.</li>
</ol>
</li>
</ol>
<p>Conceptually, it is easy, right? But in this chapter, we have only covered the basics to accomplish tasks 1 and 3. Later, in <a href="3bf944de-e0f8-4e78-a38b-47796c91185b.xhtml" target="_blank">Chapter 10</a>, <em>Applying Machine Learning in Robotics</em>, you will be given the technical background on <strong>object recognition</strong> so that you can also program tasks 2 and 4. More precisely, it will be in the <em>A methodology to programmatically apply ML in Robotics </em>section where we will provide you with this insight.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>This chapter has introduced you to the master task of robot navigation. SLAM and navigation are complex matters and active research topics in robotics. So, this chapter has given you a taste of how to implement it so that you can quickly understand its mechanics without entering into details of the algorithms and the mathematics behind.</p>
<p>We expect to have aroused your curiosity on this topic. Now you are prepared to carry out the same task in the real world with the physical GoPiGo3. In the next chapter, you will perform the navigation and SLAM tasks with the physical robot.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Questions</h1>
                
            
            
                
<ol>
<li>Where are the sensor specifications included within a Gazebo SDF file?</li>
</ol>
<p style="padding-left: 60px">A) Outside of a <kbd>&lt;gazebo&gt;</kbd> tag<br/>
B) Within a <kbd>&lt;joint&gt;</kbd> tag<br/>C) Within a <kbd>&lt;sensor&gt;</kbd> tag</p>
<ol start="2">
<li>Regarding the controller specification of a sensor in Gazebo, what is the most relevant parameter in terms of CPU usage while running the simulation?</li>
</ol>
<p style="padding-left: 60px">A) The scan distance, because the larger the sensor range is, the more bandwidth consumption the CPU performs.<br/>
B) The angular scan, since the greater the angular resolution, the more bandwidth consumption is required to store the readings in the RAM.<br/>C) The maximum sensor frequency, because they are so high in real sensors that they easily can overload the CPU.</p>
<ol start="3">
<li>Where are the sensor mechanical properties included within a Gazebo description of the robot?</li>
</ol>
<p style="padding-left: 60px">A) Outside of a <kbd>&lt;gazebo&gt;</kbd> tag<br/>
B) Within a <kbd>&lt;joint&gt;</kbd> tag<br/>C) Within a <kbd>&lt;sensor&gt;</kbd> tag</p>
<ol start="4">
<li>What does the SLAM technique provide to a robot?</li>
</ol>
<p style="padding-left: 60px">A) A method to avoid moving obstacles in the environment<br/>
B) A method to build a map of the environment<br/>C) A method to avoid static and moving obstacles in the environment</p>
<ol start="5">
<li>How do you operationally specify a navigation goal to a robot?</li>
</ol>
<p style="padding-left: 60px">A) Tell it the target location and orientation<br/>
B) Set a target location in a two-dimensional map of the environment<br/>
C) Mark the borders of the area where the robot is expected to navigate to</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Further reading</h1>
                
            
            
                
<p>To delve deeper into the concepts explained in this chapter, you can check out the following references:</p>
<ul>
<li>Adaptive Monte Carlo Localization (AMCL), at <a href="http://roboticsknowledgebase.com/wiki/state-estimation/adaptive-monte-carlo-localization/">http://roboticsknowledgebase.com/wiki/state-estimation/adaptive-monte-carlo-localization/</a></li>
<li><em>Particle Filters in Robotics</em>, Proceedings of Uncertainty in AI (UAI), Thrun S. (2002), at <a href="http://robots.stanford.edu/papers/thrun.pf-in-robotics-uai02.pdf">http://robots.stanford.edu/papers/thrun.pf-in-robotics-uai02.pdf</a></li>
<li><em>SLAM for Dummies</em>, A Tutorial Approach to Simultaneous Localization and Mapping, Riisgaard S, at <a href="http://zyzx.haust.edu.cn/moocresource/data/081503/U/802/pdfs/soren_project.pdf">http://zyzx.haust.edu.cn/moocresource/data/081503/U/802/pdfs/soren_project.pdf</a></li>
<li><em>Robot Perception for Indoor Navigation</em>, Endres, F. (2015), Albert-Ludwigs-Universitat Freiburg, at<a href="https://d-nb.info/1119716993/34"> https://d-nb.info/1119716993/34</a></li>
</ul>


            

            
        
    </body></html>