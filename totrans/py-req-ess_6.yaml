- en: Chapter 6. Web Scraping with Python Requests and BeautifulSoup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have become experts in how to communicate with the Web through `Requests`.
    Everything progressed flamboyantly while working with the APIs. However, there
    are some conditions where we need to be aware of API folklore.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing that concerns us is not all web services have built an API for
    the sake of their third-party customers. Also, there is no statute that the API
    should be maintained perfectly. Even tech giants such as Google, Facebook, and
    Twitter tend to change their APIs abruptly without prior notice. So, it's better
    to understand that it is not always the API that comes to the rescue when we are
    looking for some vital information from a web resource.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of **web scraping** stands as a savior when we really turn imperative
    to access some information from a web resource that does not maintain an API.
    In this chapter, we will discuss tricks of the trade to extract information from
    web resources by following all the principles of web scraping.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin, let''s get to know some important concepts that will help
    us to reach our goal. Take a look at the response content format of a request,
    which will introduce us to a particular type of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, the response content is rendered in the form of semistructured
    data, which is represented using HTML tags; this in turn helps us to access the
    information about the different sections of a web page individually.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's get to know the different types of data that the Web generally deals
    with.
  prefs: []
  type: TYPE_NORMAL
- en: Types of data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In most cases, we deal with three types of data when working with web sources.
    They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Structured data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unstructured data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semistructured Data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structured data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Structured data is a type of data that exists in an organized form. Normally,
    structured data has a predefined format and it is machine readable. Each piece
    of data that lies in structured data has a relation with every other data as a
    specific format is imposed on it. This makes it easier and faster to access different
    parts of data. The structured data type helps in mitigating redundant data while
    dealing with huge amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: Databases always contain structured data, and SQL techniques can be used to
    access data from them. We can regard census records as an example of structured
    data. They contain information about the date of birth, gender, place, income,
    and so on, of the people of a country.
  prefs: []
  type: TYPE_NORMAL
- en: Unstructured data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In contrast to structured data, unstructured data either misses out on a standard
    format or stays unorganized even though a specific format is imposed on it. Due
    to this reason, it becomes difficult to deal with different parts of the data.
    Also, it turns into a tedious task. To handle unstructured data, different techniques
    such as text analytics, Natural Language Processing (NLP), and data mining are
    used. Images, scientific data, text-heavy content (such as newspapers, health
    records, and so on), come under the unstructured data type.
  prefs: []
  type: TYPE_NORMAL
- en: Semistructured data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Semistructured data is a type of data that follows an irregular trend or has
    a structure which changes rapidly. This data can be a self described one, it uses
    tags and other markers to establish a semantic relationship among the elements
    of the data. Semistructured data may contain information that is transferred from
    different sources. **Scraping** is the technique that is used to extract information
    from this type of data. The information available on the Web is a perfect example
    of semistructured data.
  prefs: []
  type: TYPE_NORMAL
- en: What is web scraping?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In simple words, web scraping is the process of extracting desired data from
    a web resource. This method involves different procedures such as interacting
    with the web resource, choosing the appropriate data, obtaining information from
    the data, and converting the data to the desired format. With all the previous
    methods considered, a major spotlight will be thrown on the process of pulling
    the required data from the semistructured data.
  prefs: []
  type: TYPE_NORMAL
- en: Dos and don'ts of web scraping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scraping a web resource is not always welcomed by the owners. Some companies
    put a restriction on using bots against them. It''s etiquette to follow certain
    rules while scraping. The following are the dos and don''ts of web scraping:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Do refer to the terms and conditions**: The first thing that should come
    to our mind before we begin scraping is terms and conditions. Do visit the website''s
    terms and conditions page and get to know whether they prohibit scraping from
    their site. If so, it''s better to back off.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Don''t bombard the server with a lot of requests**: Every website runs on
    a server that can serve only a specific amount of workload. It is equivalent to
    being rude if we bombard the server with lots of requests in a specific span of
    time, which may result in sever breakdown. Wait for some time between requests
    instead of bombarding the server with too many requests at once.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: Some sites put a restriction on the maximum number of requests processed per
    minute and will ban the request sender's IP address if this is not adhered to.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Do track the web resource from time to time**: A website doesn''t always
    stay the same. According to its usability and the requirement of users, they tend
    to change from time to time. If any alteration has taken place in the website,
    our code to scrape may fail. Do remember to track the changes made to the site,
    modify the scrapper script, and scrape accordingly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predominant steps to perform web scraping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Generally, the process of web scraping requires the use of different tools
    and libraries such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Chrome DevTools or FireBug Add-on**: This can be used to pinpoint the pieces
    of information in an HTML/XML page.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HTTP libraries**: These can be used to interact with the server and to pull
    a response document. An example of this is `python-requests`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Web scraping tools**: These are used to pull data from a semistructured document.
    Examples include `BeautifulSoup` or `Scrappy`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The overall picture of web scraping can be observed in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the URL(s) of the web resource to perform the web scraping task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use your favorite HTTP client/library to pull the semistructured document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before extracting the desired data, discover the pieces of data that are in
    semistructured format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Utilize a web scraping tool to parse the acquired semistructured document into
    a more structured one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw the desired data that we are hoping to use. That's all, we are done!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Key web scraping tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While pulling the required data from a semistructured document, we perform
    various tasks. The following are the basic tasks that we adopt for scraping:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Searching a semistructured document:** Accessing a particular element or
    a specific type of element in a document can be accomplished using its `tag` name
    and `tag` attributes, such as `id`, `class`, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Navigating within a semistructured document**: We can navigate through a
    web document to pull different types of data in four ways, which are navigating
    down, navigating sideways, navigating up, and navigating back and forth. We can
    get to know more about these in detail later in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modifying a semistructured document:** By modifying the `tag` name or the
    `tag` attributes of a document, we can streamline and pull the required data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is BeautifulSoup?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `BeautifulSoup` library is a simple yet powerful web scraping library. It
    has the capability to extract the desired data when provided with an HTML or XML
    document. It is charged with some superb methods, which help us to perform web
    scraping tasks effortlessly.
  prefs: []
  type: TYPE_NORMAL
- en: Document parsers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Document parsers aid us in parsing and serializing the semistructured documents
    that are written using HTML5, lxml, or any other markup language. By default,
    `BeautifulSoup` has Python's standard `HTMLParser` object. If we are dealing with
    different types of documents, such as HTML5 and lxml, we need to install them
    explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, our prime focus will be laid only on particular parts of the
    library, which help us to understand the techniques to develop a practical scraping
    bot that we will build at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Installing `BeautifulSoup` is pretty straightforward. We can use `pip` to install
    it with ease:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Whenever we intend to scrape a web resource using `BeautifulSoup`, we need
    to create a `BeautifulSoup` object for it. The following are the commands to do
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Objects in BeautifulSoup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `BeautifulSoup` object parses the given HTML/XML document and converts it
    into a tree of Python objects, which are discussed in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Tags
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The word "tag" represents an HTML/XML tag in the provided document. Each `tag`
    object has a name and a lot of attributes and methods. The following example showcases
    the way to deal with a `tag` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to access the type, name, and attributes of the `BeautifulSoup` object,
    with `soup`, that we created in the preceding example, use the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For accessing the `tag type`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For accessing the `tag name`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For accessing the `tag` attribute (`'id'` in the given html string)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: BeautifulSoup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The object that gets created when we intend to scrape a web resource is called
    a `BeautifulSoup` object. Put simply, it is the complete document that we are
    planning to scrape. This can be done using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: NavigableString
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A `NavigableString` object represents the contents of `tag`. We use the `.string`
    attribute of the `tag` object to access it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Comments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `comment` object illustrates the comment part of the web document. The
    following lines of code exemplify a `comment` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Web scraping tasks related to BeautifulSoup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As cited in the previous section of *Key web scraping tasks*, `BeautifulSoup`
    always follows those basic tasks in the process of web scraping. We can get to
    know these tasks in detail with the help of a practical example, using an HTML
    document. We will be using the following HTML document that is `scraping_example.html`,
    as an example through out the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To give a crystal clear understanding of the preceding web document, we showcased
    it as a document tree. The following diagram represents the preceding HTML document:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Web scraping tasks related to BeautifulSoup](img/B03661_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: When we create the `BeautifulSoup` object for the previously shown web document,
    it will result in a tree of Python objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform different tasks with the previous document, `scraping_example.html`,
    we need to create a `BeautifulSoup` object. To create it, open the Python shell
    and run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: From now, we will use the preceding `BeautifulSoup` object to execute different
    tasks. Let's perform the web scraping tasks on the `scraping_example.html` document
    and get an overall idea on all the tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Searching the tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To identify the different tags in an HTML/XML document, we need to search the
    whole document. In similar situations, we can use `BeautifulSoup` methods such
    as `find`, `find_all`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the syntax to search the whole document to identify the tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '`find(name, attributes, recursive, text, **kwargs)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: This is the first occurring tag name that appears in the process of
    discovery. It can be a string, a regular expression, a list, a function, or the
    value `True`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`find_all(name, attributes, recursive, text, limit, **kwargs)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: This is used to access specific types of tags with their name. It can
    be a string, a regular expression, a list, a function, or the value `True`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`limit`: This is the maximum number of results in the output.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The common attributes for the preceding two methods are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`attributes`: These are the attributes of an HTML/XML tag.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recursive`: This takes a Boolean value. If it is set to `True`, the `BeautifulSoup`
    library checks all the children of a specific tag. Vice versa, if it is set to
    `false`, the `BeautifulSoup` library checks the child at the next level only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text`: This parameter identifies tags that consist of the string content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Navigating within the tree
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different tasks are involved in navigating the document tree with the `Beautifulsoup4`
    module; they are discussed in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Navigating down
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can access a particular element's data by moving down in a document. If we
    consider the document tree in the previous figure, we can access different elements
    by moving downward from the top element—`html`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every element can be accessed using its `tag` name. Here is a way to access
    the contents of the `html` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the ways in which we can access the elements of the preceding document
    tree by navigating down. In order to access the `title` element, we should go
    from top to bottom, that is, from `html` to `head` and from `head` to `title`,
    as shown in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, you can access the `meta` element, as shown in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Navigating sideways
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To access the siblings in a document tree, we should navigate sideways. The
    `BeautifulSoup` library provides various `tag` object properties such as `.next_sibling`,
    `.previous_sibling`, `.next_siblings`, and `.previous_siblings`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you look at the preceding diagram containing the document tree, the different
    siblings at different levels of the tree, when navigated sideways, are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`head` and `body`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`div1`, `div2`, and `div3`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the document tree, the `head` tag is the first child of `html`, and `body`
    is the next child of `html`. In order to access the children of the `html` tag,
    we can use its `children` property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To access the next sibling of `head` element we can use `.find_next_sibling`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To access the previous sibling of `body`, we can use `.find_previous_sibling`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Navigating up
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can access a particular element's parent by moving toward the top of the
    document tree. The `BeautifulSoup` library provides two properties—`.parent` and
    `.parents`—to access the first parent of the `tag` element and all its ancestors,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Navigating back and forth
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To access the previously parsed element, we navigate back in the node of a
    tree, and to access the immediate element that gets parsed next, we navigate forward
    in the node of a tree. To deal with this, the `tag` object provides the `.find_previous_element`
    and `.find_next_element` properties, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Modifying the Tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The BeautifulSoup library also facilitates us to make changes to the web document
    according to our requirements. We can alter a tag's properties using its attributes,
    such as the `.name`, `.string`, and `.append()` method. We can also add new tags
    and strings to an existing tag with the help of the `.new_string()` and `.new_tag()`
    methods. There are also other methods, such as `.insert()`, `.insert_before()`,
    `.insert_after()`, and so on, to make various modifications to the document tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of changing the `title` tag''s `.string` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before modifying the `title` tag the title contents are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the way to modify the contents of a `title` tag:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After the modifications the contents of the `tilte` tag looks like this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Building a web scraping bot – a practical example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point of time, our minds got enlightened with all sorts of clues to
    scrape the Web. With all the information acquired, let's look at a practical example.
    Now, we will create a web scraping bot, which will pull a list of words from a
    web resource and store them in a JSON file.
  prefs: []
  type: TYPE_NORMAL
- en: Let's turn on the scraping mode!
  prefs: []
  type: TYPE_NORMAL
- en: The web scraping bot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, the web scraping bot is an automated script that has the capability to
    extract words from a website named majortests.com. This website consists of various
    tests and **Graduate Record Examinations** (**GRE**) word lists. With this web
    scraping bot, we will scrape the previously mentioned website and create a list
    of GRE words and their meanings in a JSON file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image is the sample page of the website that we are going to
    scrape:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The web scraping bot](img/B03661_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Before we kick start the scraping process, let''s revise the dos and don''t
    of web scraping as mentioned in the initial part of the chapter. Believe it or
    not they will definitely leave us in peace:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Do refer to the terms and conditions**: Yes, before scraping majortests.com,
    refer to the terms and conditions of the site and obtain the necessary legal permissions
    to scrape it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Don''t bombard the server with a lot of requests**: Keeping this in mind,
    for every request that we are going to send to the website, a delay has been instilled
    using Python''s `time.sleep` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Do track the web resource from time to time**: We ensured that the code runs
    perfectly with the website that is running on the server. Do check the site once
    before starting to scrape, so that it won''t break the code. This can be made
    possible by running some unit tests, which conform to the structure we expected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's start the implementation by following the steps to scrape that we
    discussed previously.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the URL or URLs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step in web scraping is to identify the URL or a list of URLs that
    will result in the required resources. In this case, our intent is to find all
    the URLs that result in the expected list of GRE words. The following is the list
    of the URLs of the sites that we are going to scrape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.majortests.com/gre/wordlist_01](http://www.majortests.com/gre/wordlist_01),'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.majortests.com/gre/wordlist_02](http://www.majortests.com/gre/wordlist_02),'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.majortests.com/gre/wordlist_03](http://www.majortests.com/gre/wordlist_03),
    and so on'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our aim is to scrape words from nine such URLs, for which we found a common
    pattern. This will help us to crawl all of them. The common URL pattern for all
    those URLs is written using Python''s `string` object, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`http://www.majortests.com/gre/wordlist_0%d`'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our implementation, we defined a method called `generate_urls`, which will
    generate the required list of URLs using the preceding URL string. The following
    snippet demonstrates the process in a Python shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Using an HTTP client
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will use the `requests` module as an HTTP client to get the web resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the `get_resource` function takes `url` as an argument
    and uses the `requests` module to get the resource.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the pieces of data to scrape
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, it is time to analyze and classify the contents of the web page. The content
    in this context is a list of words with their definitions. In order to identify
    the elements of the words and their definitions, we used Chrome DevTools. The
    perceived information of the elements (HTML elements) can help us to identify
    the word and its definition, which can be used in the process of scraping.
  prefs: []
  type: TYPE_NORMAL
- en: 'To carry this out open the URL ([http://www.majortests.com/gre/wordlist_01](http://www.majortests.com/gre/wordlist_01))
    in the Chrome browser and access the **Inspect element** option by right-clicking
    on the web page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Discovering the pieces of data to scrape](img/B03661_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding image, we can identify the structure of the word list, which
    appears in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'By looking at the parts of the previously referred to web page, we can interpret
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Each web page consists of a word list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every word list has many word groups that are defined in the same `div` tag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the words in a word group are described in a table having the class attribute—`wordlist`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each and every table row (`tr`) in the table represents a word and its definition
    using the `th` and `td` tags, respectively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing a web scraping tool
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s use `BeautifulSoup4` as a web scraping tool to parse the obtained web
    page contents that we received using the `requests` module in one of the previous
    steps. By following the preceding interpretations, we can direct `BeautifulSoup`
    to access the required content of the web page and deliver it as an object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding lines of code, the `make_soup` method takes the `html` content
    in the form of a string and returns a `BeautifulSoup` object.
  prefs: []
  type: TYPE_NORMAL
- en: Drawing the desired data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `BeautifulSoup` object that we obtained in the previous step is used to
    extract the required words and their definitions from it. Now, with the methods
    available in the `BeautifulSoup` object, we can navigate through the obtained
    HTML response, and then we can extract the list of words and their definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding lines of code, `get_words_from_soup` takes a `BeautifulSoup`
    object and then looks for all the words contained in the `wordlists` class using
    the instance's `find_all()` method, and then returns a dictionary of words.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dictionary of words obtained previously will be saved in a JSON file using
    the following `helper` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'On the whole, the process can be depicted in the following program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the content of the `words.json` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about different types of data that we encountered
    with web sources and tweaked some ideas. We came to know about the need for web
    scraping, the legal issues, and the goodies that it offers. Then, we jumped deep
    into web scraping tasks and their potential. You learned about a new library called
    `BeautifulSoup`, and its ins and outs, with examples.
  prefs: []
  type: TYPE_NORMAL
- en: We came to know the capabilities of `BeautifulSoup` in depth and worked on some
    examples to get a clear idea on it. At last, we created a practical scraping bot
    by applying the knowledge that we gained from the previous sections, which enlightened
    us with an experience to scrape a website in real time.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about the Flask microframework and we will
    build an application using it by following the best practices.
  prefs: []
  type: TYPE_NORMAL
