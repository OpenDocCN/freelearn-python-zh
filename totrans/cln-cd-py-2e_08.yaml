- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Unit Testing and Refactoring
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单元测试和重构
- en: 'The ideas explored in this chapter are fundamental pillars in the global context
    of the book because of their importance to our ultimate goal: to write better
    and more maintainable software.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨的思想在全球范围内是本书的基本支柱，因为它们对我们最终目标的重要性：编写更好、更易于维护的软件。
- en: Unit tests (and any form of automatic tests, for that matter) are critical to
    software maintainability, and therefore something that cannot be missing from
    any quality project. It is for that reason that this chapter is dedicated exclusively
    to aspects of automated testing as a key strategy, to safely modify the code,
    and iterate over it, in incrementally better versions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试（以及任何形式的自动测试）对于软件的可维护性至关重要，因此任何质量项目都不能缺少。正因为如此，本章专门致力于自动化测试作为关键策略的各个方面，以确保安全地修改代码，并逐步迭代出更好的版本。
- en: 'After this chapter, we will have gained more insight into the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章之后，我们将对以下内容有更深入的了解：
- en: Why automated tests are critical for a project's success
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么自动化测试对于项目的成功至关重要
- en: How unit tests work as a heuristic of the quality of the code
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单元测试如何作为代码质量的启发式方法
- en: What frameworks and tools are available to develop automated tests and set up
    quality gates
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用于开发自动化测试和设置质量门框架和工具
- en: Taking advantage of unit tests to understand the domain problem better and document
    code
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用单元测试更好地理解领域问题并记录代码
- en: Concepts related to unit testing, such as test-driven development
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与单元测试相关的概念，例如测试驱动开发
- en: In the previous chapters, we have seen Python-specific traits and how we can
    leverage them to achieve more maintainable code. We have also explored how general
    design principles of software engineering can be applied to Python using its peculiarities.
    Here we'll also revisit an important concept of software engineering, such as
    automatic testing, but with the use of tools, some of them available in the standard
    library (such as the `unittest` module), and some others that are external packages
    (such as `pytest`). We begin this journey by exploring how software design relates
    to unit testing.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到了Python特定的特性以及我们如何利用它们来实现更易于维护的代码。我们还探讨了软件工程的通用设计原则如何应用于Python，利用其特性。在这里，我们也将回顾软件工程的一个重要概念，如自动化测试，但使用工具，其中一些是标准库中可用的（如`unittest`模块），还有一些是外部包（如`pytest`）。我们开始这段旅程，通过探索软件设计如何与单元测试相关联。
- en: Design principles and unit testing
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计原则和单元测试
- en: In this section, we are first going to take a look at unit testing from a conceptual
    point of view. We will revisit some of the software engineering principles we
    discussed in the previous chapter to get an idea of how this is related to clean
    code.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先将从概念上审视单元测试。我们将回顾上一章中讨论的一些软件工程原则，以了解这与清洁代码的关系。
- en: After that, we will discuss in more detail how to put these concepts into practice
    (at the code level), and what frameworks and tools we can make use of.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将更详细地讨论如何将这些概念付诸实践（在代码层面），以及我们可以利用哪些框架和工具。
- en: First, we quickly define what unit testing is about. Unit tests are code in
    charge of validating other parts of the code. Normally, anyone would be tempted
    to say that unit tests validate the "core" of the application, but such a definition
    regards unit tests as secondary, which is not the way they are thought of in this
    book. Unit tests are core, and a critical component of the software and they should
    be treated with the same considerations as the business logic.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们快速定义单元测试是什么。单元测试是负责验证其他代码部分的代码。通常，任何人都会倾向于说单元测试验证应用的“核心”，但这种定义将单元测试视为次要的，而这并不是本书中对它们的看法。单元测试是核心，是软件的一个关键组成部分，它们应该与业务逻辑一样受到同样的考虑。
- en: 'A unit test is a piece of code that imports parts of the code with the business
    logic, and exercises its logic, asserting several scenarios with the idea of guaranteeing
    certain conditions. There are some traits that unit tests must have, such as:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试是一段代码，它导入包含业务逻辑的部分代码，并对其逻辑进行练习，通过断言几个场景来确保某些条件。单元测试必须具备一些特性，例如：
- en: 'Isolation: Unit tests should be completely independent from any other external
    agent, and they have to focus only on the business logic. For this reason, they
    do not connect to a database, they don''t perform HTTP requests, and so on. Isolation
    also means that the tests are independent among themselves: they must be able
    to run in any order, without depending on any previous state.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隔离：单元测试应该完全独立于任何其他外部代理，并且它们必须只关注业务逻辑。因此，它们不会连接到数据库，不会执行HTTP请求等。隔离还意味着测试之间是独立的：它们必须能够以任何顺序运行，而不依赖于任何先前的状态。
- en: 'Performance: Unit tests must run quickly. They are intended to be run multiple
    times, repeatedly.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能：单元测试必须运行得快。它们旨在多次重复运行。
- en: 'Repeatability: Unit tests should be able to objectively assess the status of
    the software in a deterministic way. This means the results yielded by the tests
    should be repeatable. Unit tests assess the status of the code: if a test fails,
    it must keep on failing until the code is fixed. If a test passes, and no changes
    in the code are made, it should continue to pass. Tests shouldn''t be flaky or
    randomized.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可重复性：单元测试应该能够以确定性的方式客观评估软件的状态。这意味着测试产生的结果应该是可重复的。单元测试评估代码的状态：如果测试失败，它必须持续失败，直到代码被修复。如果测试通过，并且代码没有变化，它应该继续通过。测试不应该是不稳定的或随机的。
- en: 'Self-validating: The execution of a unit test determines its result. There
    should be no extra step required to interpret the unit test (much less manual
    intervention).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自验证：单元测试的执行决定了其结果。不应需要额外的步骤来解释单元测试（更不用说手动干预）。
- en: More concretely, in Python, this means that we will have new `*.py` files where
    we are going to place our unit tests, and they are going to be called by some
    tool. These files will have `import` statements, to take what we need from our
    business logic (what we intend to test), and inside this file, we program the
    tests themselves. Afterward, a tool will collect our unit tests and run them,
    giving a result.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，在Python中，这意味着我们将有新的`*.py`文件，我们将在这里放置我们的单元测试，并且它们将被某些工具调用。这些文件将包含`import`语句，以从我们的业务逻辑（我们打算测试的内容）中获取所需的内容，并在该文件内部，我们编写测试本身。之后，一个工具将收集我们的单元测试并运行它们，给出结果。
- en: This last part is what self-validation actually means. When the tool calls our
    files, a Python process will be launched, and our tests will be running on it.
    If the tests fail, the process will have exited with an error code (in a Unix
    environment, this can be any number other than `0`). The standard is that the
    tool runs the test, and prints a dot (`.`) for every successful test; an `F` if
    the test failed (the condition of the test was not satisfied), and an `E` if there
    was an exception.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这最后部分就是自验证的实际意义。当工具调用我们的文件时，将启动一个Python进程，我们的测试将在其上运行。如果测试失败，进程将以错误代码退出（在Unix环境中，这可以是除`0`以外的任何数字）。标准是工具运行测试，并为每个成功的测试打印一个点（`.`）；如果测试失败（测试条件未满足），则打印`F`；如果有异常，则打印`E`。
- en: A note about other forms of automated testing
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于其他形式自动测试的注意事项
- en: Unit tests are intended to verify very small units of code, for example, a function,
    or a method. We want our unit tests to reach a very detailed level of granularity,
    testing as much code as possible. To test something bigger, such as a class, we
    would not want to use just unit tests, but rather a test suite, which is a collection
    of unit tests. Each one of them will be testing something more specific, like
    a method of that class.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试旨在验证非常小的代码单元，例如，一个函数或一个方法。我们希望我们的单元测试达到非常详细粒度，尽可能多地测试代码。要测试更大的东西，比如一个类，我们不想只使用单元测试，而应该使用测试套件，这是一个单元测试的集合。每个测试都将测试更具体的东西，比如那个类的方法。
- en: Unit tests aren't the only available mechanism of automatic testing, and we
    shouldn't expect them to catch all possible errors. There are also *acceptance*
    and *integration* tests, both beyond the scope of this book.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试不是唯一的自动测试机制，我们不应该期望它们捕获所有可能的错误。还有*验收*和*集成*测试，这两者都不在本书的范围之内。
- en: In an integration test, we want to test multiple components at once. In this
    case, we want to validate if collectively, they work as expected. In this case,
    it is acceptable (more than that, desirable) to have side effects, and to forget
    about isolation, meaning that we will want to issue HTTP requests, connect to
    databases, and so on. While we'd want our integration tests to actually run as
    the production code would, there are some dependencies we would still want to
    avoid. For example, if your service connects to another external dependency via
    the Internet, then that part would indeed be omitted.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在集成测试中，我们希望同时测试多个组件。在这种情况下，我们希望验证它们是否集体地按预期工作。在这种情况下，允许（甚至更希望）有副作用，并忘记隔离，这意味着我们希望发出HTTP请求，连接到数据库等。虽然我们希望我们的集成测试实际上像生产代码那样运行，但还有一些依赖关系我们仍然希望避免。例如，如果你的服务通过互联网连接到另一个外部依赖项，那么这部分确实会被省略。
- en: Let's say you have your application that uses a database and connects to some
    other internal services. The application will have different configuration files
    for different environments, and of course, in production you'll have the configuration
    set for the real services. However, for an integration test, you'll want to mock
    the database with a Docker container that's built specifically for those tests,
    and this will be configured in a specific configuration file. As for the dependencies,
    you'll want to mock them with Docker services, whenever that's possible.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个使用数据库并连接到一些其他内部服务的应用程序。该应用程序将为不同的环境有不同的配置文件，当然，在生产环境中，你将设置用于真实服务的配置。然而，对于集成测试，你将希望使用专门为这些测试构建的Docker容器来模拟数据库，这将在特定的配置文件中进行配置。至于依赖项，你将希望尽可能使用Docker服务来模拟它们。
- en: Mocking as part of unit testing will be covered later on in this chapter. When
    it comes to mocking dependencies to perform *component* testing, this will be
    covered in *Chapter 10*, *Clean Architecture*, when we mention components in the
    context of software architecture.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章稍后部分将介绍将模拟作为单元测试的一部分。当涉及到对组件进行测试时，模拟依赖关系的内容将在第10章“清洁架构”中介绍，那时我们将从软件架构的角度提到组件。
- en: An acceptance test is an automated form of testing that tries to validate the
    system from the perspective of a user, typically executing use cases.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接受测试是一种自动化的测试形式，试图从用户的角度验证系统，通常执行用例。
- en: 'These last two forms of testing lose another nice trait compared to unit tests:
    speed. As you can imagine, they will take more time to run, and therefore they
    will be run less frequently.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与单元测试相比，这两种测试形式失去了一个很好的特性：速度。正如你可以想象的那样，它们将需要更多的时间来运行，因此它们将运行得较少。
- en: 'In a good development environment, the programmer will have the entire test
    suite and will run unit tests all the time, repeatedly, while making changes to
    the code, iterating, refactoring, and so on. Once the changes are ready, and the
    pull request is open, the continuous integration service will run the build for
    that branch, where the unit tests will run as long as the integration or acceptance
    tests that might exist. Needless to say, the status of the build should be successful
    (green) before merging, but the important part is the difference between the kinds
    of tests: we want to run unit tests all the time, and those tests that take longer
    less frequently.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个好的开发环境中，程序员将拥有整个测试套件，并在修改代码、迭代、重构等过程中不断重复运行单元测试。一旦更改准备就绪，并且拉取请求已打开，持续集成服务将为该分支运行构建，其中单元测试将一直运行，直到存在集成或接受测试。不用说，构建的状态在合并之前应该是成功的（绿色），但重要的是测试类型之间的差异：我们希望一直运行单元测试，而那些运行时间较长的测试则运行得较少。
- en: For this reason, we want to have a lot of small unit tests, and a few automated
    tests, strategically designed to cover as much as possible of where the unit tests
    could not reach (the use of the database, for instance).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望拥有大量的单元测试，以及一些策略性地设计的自动化测试，以尽可能覆盖单元测试无法触及的地方（例如数据库的使用）。
- en: Finally, a word to the wise. Remember that this book encourages pragmatism.
    Besides these definitions given, and the points made about unit tests at the beginning
    of the section, the reader has to keep in mind that the best solution according
    to your criteria and context should predominate. Nobody knows your system better
    than you, which means if, for some reason, you have to write a unit test that
    needs to launch a Docker container to test against a database, go for it. As we
    have repeatedly remembered throughout the book, *practicality beats purity*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，给明智的人一句话。记住，这本书鼓励实用主义。除了这些定义和本节开头关于单元测试的要点之外，读者必须记住，根据您的标准和环境，最佳解决方案应该占主导地位。没有人比您更了解您的系统，这意味着如果出于某种原因，您必须编写一个需要启动Docker容器以测试数据库的单元测试，那就去做吧。正如我们在本书中反复提醒的那样，*实用性胜过纯粹性*。
- en: Unit testing and agile software development
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单元测试和敏捷软件开发
- en: In modern software development, we want to deliver value constantly, and as
    quickly as possible. The rationale behind these goals is that the earlier we get
    feedback, the less the impact, and the easier it will be to change. These are
    not new ideas at all; some of them resemble principles from decades ago, and others
    (such as the idea of getting feedback from stakeholders as soon as possible and
    iterating upon it) you can find in essays such as **The Cathedral and the Bazaar**
    (abbreviated as **CatB**).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代软件开发中，我们希望不断交付价值，并且尽可能快地交付。这些目标背后的逻辑是，我们越早得到反馈，影响就越小，改变就越容易。这些根本不是新想法；其中一些类似于几十年前的原则，而另一些（如尽快从利益相关者那里获得反馈并在此基础上迭代的思想）你可以在像**《大教堂与市集》**（缩写为**CatB**）这样的文章中找到。
- en: Therefore, we want to be able to respond effectively to changes, and for that,
    the software we write will have to change. As I mentioned in previous chapters,
    we want our software to be adaptable, flexible, and extensible.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望能够有效地应对变化，为此，我们编写的软件将必须发生变化。正如我在前面的章节中提到的，我们希望我们的软件具有适应性、灵活性和可扩展性。
- en: The code alone (regardless of how well written and designed it is) cannot guarantee
    us that it's flexible enough to be changed, if there's no formal proof that it
    will keep on running correctly after it was modified.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 代码本身（无论编写和设计得有多好）不能保证我们它足够灵活以进行更改，如果没有正式的证明它在修改后仍然可以正确运行。
- en: Let's say we design a piece of software following the SOLID principles, and
    in one part we actually have a set of components that comply with the open/closed
    principle, meaning that we can easily extend them without affecting too much existing
    code. Assume further that the code is written in a way that favors refactoring,
    so we could change it as required. What's to say that when we make these changes,
    we aren't introducing any bugs? How do we know that existing functionality is
    preserved (and there are no regressions)? Would you feel confident enough releasing
    that to your users? Will they believe that the new version works just as expected?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们按照SOLID原则设计一款软件，在某个部分我们实际上有一组符合开闭原则的组件，这意味着我们可以轻松地扩展它们，而不会对现有代码造成太大影响。进一步假设代码是以有利于重构的方式编写的，因此我们可以根据需要对其进行更改。那么，当我们进行这些更改时，我们是否在引入任何错误呢？我们如何知道现有功能是否得到保留（并且没有回归）？您是否足够自信将此版本发布给用户？他们会相信新版本能按预期工作吗？
- en: 'The answer to all of these questions is that we can''t be sure unless we have
    a formal proof of it. And unit tests are just that: formal proof that the program
    works according to the specifications.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些问题的答案是我们不能确定，除非我们有正式的证明。而单元测试正是这样：正式证明程序按照规格工作。
- en: Unit (or automated) tests, therefore, work as a safety net that gives us the
    confidence to work on our code. Armed with these tools, we can efficiently work
    on our code, and therefore this is what ultimately determines the velocity (or
    capacity) of the team working on the software product. The better the tests, the
    more likely it is that we can deliver value quickly without being stopped by bugs
    every now and then.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，单元测试（或自动化测试）就像一个安全网，它给了我们信心去修改代码。有了这些工具，我们可以高效地工作，因此这最终决定了软件产品团队的工作速度（或容量）。测试越好，我们能够快速交付价值而不被错误频繁阻止的可能性就越大。
- en: Unit testing and software design
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单元测试和软件设计
- en: This is the other face of the coin when it comes to the relationship between
    the main code and unit testing. Besides the pragmatic reasons explored in the
    previous section, it comes down to the fact that good software is testable software.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到主代码和单元测试之间的关系时，这是硬币的另一面。除了上一节中探讨的实用主义原因之外，这归结于好的软件是可测试的软件的事实。
- en: '**Testability** (the quality attribute that determines how easy to test software
    is) is not just nice to have, but a driver for clean code.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**可测试性**（决定软件测试难易程度的质量属性）不仅是一个好东西，而且是编写干净代码的驱动力。'
- en: Unit tests aren't just something complementary to the main code base, but rather
    something that has a direct impact and real influence on how the code is written.
    There are many levels of this, from the very beginning, when we realize that the
    moment we want to add unit tests for some parts of our code, we have to change
    it (resulting in a better version of it), to its ultimate expression (explored
    near the end of this chapter) when the entire code (the design) is driven by the
    way it's going to be tested via **test-driven design**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试不仅仅是主代码库的补充，而是一种对代码编写方式有直接影响和实际影响的因素。这有很多层次，从一开始，当我们意识到我们想要为代码的某些部分添加单元测试时，我们必须对其进行更改（从而得到一个更好的版本），到其最终的表达（在本章末尾附近探讨）时，整个代码（设计）都是通过将要进行的测试方式（**测试驱动设计**）来驱动的。
- en: Starting off with a simple example, I'll show you a small use case in which
    tests (and the need to test our code) lead to improvements in the way our code
    ends up being written.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个简单的例子开始，我将向您展示一个小的用例，其中测试（以及测试我们代码的需要）导致我们代码编写方式的改进。
- en: 'In the following example, we will simulate a process that requires sending
    metrics to an external system about the results obtained at each particular task
    (as always, details won''t make any difference as long as we focus on the code).
    We have a `Process` object that represents a task on the domain problem, and it
    uses a `metrics` client (an external dependency and therefore something we don''t
    control) to send the actual metrics to the external entity (this could be sending
    data to `syslog`, or `statsd`, for instance):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将模拟一个需要将每个特定任务获得的结果发送到外部系统的过程（正如通常一样，只要我们专注于代码，细节就不会有任何影响）。我们有一个`Process`对象，它代表领域问题上的一个任务，并使用`metrics`客户端（一个外部依赖项，因此我们无法控制）将实际指标发送到外部实体（这可能是指向`syslog`或`statsd`发送数据等）：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the simulated version of the third-party client, we put the requirement
    that the parameters provided must be of string type. Therefore, if the `result`
    of the `run_process` method is not a string, we might expect it to fail, and indeed
    it does:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三方客户端的模拟版本中，我们设定了必须提供字符串类型参数的要求。因此，如果`run_process`方法的`result`不是字符串，我们可能会预期它将失败，而且确实如此：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Remember that this validation is out of our hands and we cannot change the code,
    so we must provide the method with parameters of the correct type before proceeding.
    But since this is a bug we detected, we first want to write a unit test to make
    sure it will not happen again. We do this to prove that we fixed the issue, and
    to protect against this bug in the future, regardless of how many times the code
    is changed.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，这种验证超出了我们的控制范围，我们无法更改代码，所以在继续之前，我们必须提供正确类型的参数。但既然这是我们发现的错误，我们首先想编写一个单元测试来确保它不会再次发生。我们这样做是为了证明我们修复了问题，并且为了防止未来再次出现这个错误，无论代码更改多少次。
- en: It would be possible to test the code as is by mocking the client of the `Process`
    object (we will see how to do so in the *Mock objects* section, when we explore
    the tools for unit testing), but doing so runs more code than is needed (notice
    how the part we want to test is nested in the code). Moreover, it's good that
    the method is relatively small, because if it weren't, the test would have to
    run even more undesired parts that we might also need to mock. This is another
    example of good design (small, cohesive functions or methods), that relates to
    testability.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过模拟`Process`对象的客户端来测试代码（我们将在探讨单元测试工具的*模拟对象*部分中看到如何这样做），但这样做会运行比所需的更多代码（注意我们想要测试的部分是如何嵌套在代码中的）。此外，方法相对较小是个好事，因为如果不是这样，测试将不得不运行更多我们不希望运行的未指定部分，我们可能也需要对这些部分进行模拟。这是另一个关于良好设计（小而内聚的函数或方法）的例子，它与可测试性相关。
- en: 'Finally, we decide not to go to much trouble and test just the part that we
    need to, so instead of interacting with the `client` directly on the `main` method,
    we delegate to a `wrapper` method, and the new class looks like this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们决定不必过于麻烦，只测试我们需要测试的部分，所以不是直接在`main`方法上与`client`交互，而是委托给一个`wrapper`方法，新的类看起来是这样的：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this case, we opted for creating our own version of the `client` for metrics,
    that is, a wrapper around the third-party library one we used to have. To do this,
    we place a class that (with the same interface) will make the conversion of the
    types accordingly.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们选择创建我们自己的`client`版本用于指标，即围绕我们曾经使用过的第三方库的一个包装器。为此，我们放置一个具有相同接口的类，它将相应地进行类型转换。
- en: This way of using composition resembles the adapter design pattern (we'll explore
    design patterns in the next chapter, so, for now, it's just an informative message),
    and since this is a new object in our domain, it can have its respective unit
    tests. Having this object will make things simpler to test, but more importantly,
    now that we look at it, we realize that this is probably the way the code should
    have been written in the first place. Trying to write a unit test for our code
    made us realize that we were missing an important abstraction entirely!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这种使用组合的方式类似于适配器设计模式（我们将在下一章探讨设计模式，所以现在就先作为一个信息提示），由于这是我们领域中的新对象，它可以有自己的相应单元测试。拥有这个对象将使测试变得更加简单，但更重要的是，现在我们来看它，我们意识到这可能是代码最初就应该编写的方式。尝试为我们的代码编写单元测试让我们意识到我们完全遗漏了一个重要的抽象！
- en: 'Now that we have separated the method as it should be, let''s write the actual
    unit test for it. The details pertaining to the `unittest` module used in this
    example will be explored in more detail in the part of the chapter where we explore
    testing tools and libraries, but for now, reading the code will give us a first
    impression on how to test it, and it will make the previous concepts a little
    less abstract:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将方法分离成应该的样子，让我们为它编写实际的单元测试。关于本例中使用的`unittest`模块的详细信息将在探讨测试工具和库的部分进行更详细的探讨，但就现在而言，阅读代码将给我们一个如何测试的第一印象，并且会使之前的概念更加具体：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`Mock` is a type that''s available in the `unittest.mock` module, which is
    a convenient object to ask about all sorts of things. For example, in this case,
    we''re using it in place of the third-party library (mocked into the boundaries
    of the system, as commented on in the next section) to check that it''s called
    as expected (and once again, we''re not testing the library itself, only that
    it is called correctly). Notice how we run a call like the one in our `Process`
    object, but we expect the parameters to be converted to strings.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`Mock`是`unittest.mock`模块中的一个类型，它是一个方便的对象，可以询问各种各样的事情。例如，在这种情况下，我们用它来代替第三方库（如下一节注释所述，模拟到系统的边界）以检查它是否按预期调用（并且再次，我们不是测试库本身，只是测试它是否正确调用）。注意我们运行了一个像我们的`Process`对象中的调用，但我们期望参数被转换为字符串。'
- en: 'This is an example of how a unit test helps us in terms of the design of our
    code: by trying to test the code, we came up with a better version of it. We can
    go even further and say that this test isn''t good enough, because of how the
    unit test is overriding an internal collaborator of the wrapper client in the
    second line. In an attempt to fix this, we might say that the actual client must
    be provided by a parameter (using dependency injection), instead of creating it
    in its initialization method. And once again, the unit test made us think of a
    better implementation.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个单元测试如何帮助我们改进代码设计的例子：通过尝试测试代码，我们得到了一个更好的版本。我们可以更进一步地说，这个测试还不够好，因为单元测试在第二行中覆盖了包装器客户端的内部协作者。为了解决这个问题，我们可能会说，实际的客户端必须通过参数（使用依赖注入）提供，而不是在初始化方法中创建它。而且，单元测试再次让我们想到了一个更好的实现。
- en: The corollary of the previous example should be that the testability of a piece
    of code also speaks to its quality. In other words, if the code is hard to test,
    or its tests are complicated, then it probably needs to be improved.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 之前例子的推论应该是，代码的可测试性也反映了其质量。换句话说，如果代码难以测试，或者其测试复杂，那么它可能需要改进。
- en: '"There are no tricks to writing tests; there are only tricks to writing testable
    code"'
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “编写测试没有技巧；只有编写可测试代码的技巧”
- en: ''
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – Miško Hevery
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: – 米什科·赫维
- en: Defining the boundaries of what to test
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义要测试的边界
- en: Testing requires effort. And if we are not careful when deciding what to test,
    we will never end testing, hence wasting a lot of effort without achieving much.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 测试需要付出努力。如果我们决定测试什么时不小心，我们永远不会结束测试，从而浪费了大量努力而没有取得多少成果。
- en: We should scope the testing to the boundaries of our code. If we don't, we would
    have to also test the dependencies (external/third-party libraries or modules)
    in our code, and then their respective dependencies, and so on in a never-ending
    journey. It's not our responsibility to test dependencies, so we can assume that
    these projects have tests of their own. It would be enough just to test that the
    correct calls to external dependencies are done with the correct parameters (and
    that might even be an acceptable use of patching), but we shouldn't put more effort
    in than that.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该将测试范围限定在我们的代码边界内。如果我们不这样做，我们就必须测试代码中的依赖项（外部/第三方库或模块），然后是它们各自的依赖项，如此等等，形成一个永无止境的旅程。测试依赖项不是我们的责任，因此我们可以假设这些项目有自己的测试。只需测试正确的外部依赖项是否以正确的参数调用（这可能甚至可以接受使用修补），但我们不应该投入比这更多的努力。
- en: This is another instance where good software design pays off. If we have been
    careful in our design, and clearly defined the boundaries of our system (that
    is, we designed toward interfaces, instead of concrete implementations that will
    change, hence inverting the dependencies over external components to reduce temporal
    coupling), then it will be much easier to mock these interfaces when writing unit
    tests.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这又是一个良好的软件设计带来回报的例子。如果我们已经谨慎地进行了设计，并清楚地定义了系统的边界（也就是说，我们设计的是接口，而不是将改变的具体实现，从而将外部组件的依赖关系反转以减少时间耦合），那么在编写单元测试时模拟这些接口将会容易得多。
- en: In good unit testing, we want to patch on the boundaries of our system and focus
    on the core functionality to be exercised. We don't test external libraries (third-party
    tools installed via `pip`, for instance), but instead, we check that they are
    called correctly. When we explore `mock` objects later on in this chapter, we
    will review techniques and tools for performing these types of assertion.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在良好的单元测试中，我们希望针对系统的边界进行修补，并关注要测试的核心功能。我们不测试外部库（例如通过`pip`安装的第三方工具），而是检查它们是否被正确调用。当我们在本章后面探索`mock`对象时，我们将回顾执行这些类型断言的技术和工具。
- en: Tools for testing
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试工具
- en: There are a lot of tools we can use for writing our unit tests, all of them
    with pros and cons and serving different purposes. I'll present the two most common
    libraries used for unit testing in Python. They cover most (if not all) use cases,
    and they're very popular, so knowing how to use them comes in handy.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用来编写单元测试的工具有很多，它们各有优缺点，服务于不同的目的。我将介绍Python中用于单元测试的两个最常见库。它们涵盖了大多数（如果不是所有）用例，并且非常受欢迎，因此了解如何使用它们非常有用。
- en: Along with testing frameworks and test running libraries, it's often common
    to find projects that configure code coverage, which they use as quality metrics.
    Since coverage (when used as a metric) is misleading, after seeing how to create
    unit tests, we'll discuss why it's not to be taken lightly.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 除了测试框架和测试运行库之外，通常还会发现配置代码覆盖率的项目，它们将其用作质量指标。由于覆盖率（当用作指标时）具有误导性，在了解如何创建单元测试之后，我们将讨论为什么它不应被轻视。
- en: The next section starts by introducing the main libraries we're going to use
    in this chapter for unit testing.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将从介绍本章中我们将要使用的用于单元测试的主要库开始。
- en: Frameworks and libraries for unit testing
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单元测试的框架和库
- en: 'In this section, we will discuss two frameworks for writing and running unit
    tests. The first one, `unittest`, is available in the standard library of Python,
    while the second one, `pytest`, has to be installed externally via `pip`:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论两个用于编写和运行单元测试的框架。第一个框架是`unittest`，它包含在Python的标准库中，而第二个框架`pytest`则需要通过`pip`外部安装：
- en: '`unittest`: [https://docs.python.org/3/library/unittest.html](https://docs.python.org/3/library/unittest.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unittest`: [https://docs.python.org/3/library/unittest.html](https://docs.python.org/3/library/unittest.html)'
- en: '`pytest`: [https://docs.pytest.org/en/latest/](https://docs.pytest.org/en/latest/)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytest`: [https://docs.pytest.org/en/latest/](https://docs.pytest.org/en/latest/)'
- en: When it comes to covering testing scenarios for our code, `unittest` alone will
    most likely suffice, since it has plenty of helpers. However, for more complex
    systems on which we have multiple dependencies, connections to external systems,
    and probably the need to patch objects, define fixtures, and parameterize test
    cases, then `pytest` looks like a more complete option.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到为我们的代码覆盖测试场景时，仅使用`unittest`可能就足够了，因为它有大量的辅助工具。然而，对于具有多个依赖项、与外部系统连接以及可能需要修补对象、定义固定值和参数化测试用例的更复杂系统，`pytest`看起来是一个更完整的选项。
- en: We will use a small program as an example to show you how it could be tested
    using both options, which, in the end, will help us to get a better picture of
    how the two of them compare.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个小程序作为示例，展示如何使用两种选项进行测试，这最终将帮助我们更好地了解这两个库的比较。
- en: 'The example demonstrating testing tools is a simplified version of a version
    control tool that supports code reviews in merge requests. We will start with
    the following criteria:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 展示测试工具的示例是一个支持合并请求中代码审查的版本控制工具的简化版本。我们将从以下标准开始：
- en: A merge request is `rejected` if at least one person disagrees with the changes.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果至少有一个人反对更改，合并请求将被“拒绝”。
- en: If nobody has disagreed, and the merge request is good for at least two other
    developers, it's `approved`.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果没有人反对，并且合并请求至少对其他两位开发者来说是好的，它就是“批准”的。
- en: In any other case, its status is `pending`.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在任何其他情况下，其状态是“挂起”。
- en: 'And here is what the code might look like:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码可能的样子：
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Using this code as a base, let's see how it can be unit tested using both of
    the libraries presented in this chapter. The idea is not only to learn about how
    to use each library, but also to identify some differences.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以此代码为基础，让我们看看如何使用本章中介绍的两种库进行单元测试。这个想法不仅是为了了解如何使用每个库，而且是为了识别一些差异。
- en: unittest
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: unittest
- en: The `unittest` module is a great option with which to start writing unit tests
    because it provides a rich API to write all kinds of testing conditions, and since
    it's available in the standard library, it's quite versatile and convenient.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`unittest`模块是一个很好的起点，用于编写单元测试，因为它提供了一个丰富的API来编写各种测试条件，并且由于它包含在标准库中，因此它非常灵活和方便。'
- en: The `unittest` module is based on the concepts of JUnit (from Java), which,
    in turn, is also based on the original ideas of unit testing that come from Smalltalk
    (perhaps this is the reason behind the naming convention of the methods on this
    module), so it's object-oriented in nature. For this reason, tests are written
    through classes, where the checks are verified by methods, and it's common to
    group tests by scenarios in classes.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`unittest` 模块基于JUnit（来自Java）的概念，而JUnit又基于来自Smalltalk的单元测试的原始想法（这可能是这个模块上方法命名惯例背后的原因），因此它本质上是面向对象的。因此，测试是通过类编写的，检查是通过方法验证的，通常在类中按场景分组测试。'
- en: To start writing unit tests, we have to create a test class that inherits from
    `unittest.TestCase`, and define the conditions we want to stress on its methods.
    These methods should start with `test_*`, and can internally use any of the methods
    inherited from `unittest.TestCase` to check conditions that must hold true.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始编写单元测试，我们必须创建一个继承自`unittest.TestCase`的测试类，并定义我们想要在其方法上施加的条件。这些方法应该以`test_`开头，并且可以内部使用从`unittest.TestCase`继承的任何方法来检查必须成立的条件。
- en: 'Some examples of conditions we might want to verify for our case are as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能想要验证的一些条件示例如下：
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The API for unit testing provides many useful methods for comparison, the most
    common one being `assertEqual(<actual>, <expected>[, message])`, which can be
    used to compare the result of the operation against the value we were expecting,
    optionally using a message that will be shown in the case of an error.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试的API提供了许多有用的比较方法，最常见的是`assertEqual(<实际>, <预期>[, message])`，它可以用来比较操作的结果与我们期望的值，可选地使用在出错时显示的消息。
- en: I named the parameters using the order (`<actual>, <expected>`), because that's
    the order I've found most of the times in my experience. Even though I believe
    this is the most common form (as a convention) to use in Python, there are no
    recommendations or guidelines regarding this. In fact, some projects (such as
    gRPC) use the inverse form (`<expected>,` `<actual>`), and this is actually a
    convention in other languages (for example, Java and Kotlin). The key is to be
    consistent and respect the form that's already been used in your project.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Another useful testing method allows us to check whether a certain exception
    was raised or not (`assertRaises`).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: When something exceptional happens, we raise an exception in our code to prevent
    further processing under the wrong assumptions, and also to inform the caller
    that something is wrong with the call as it was performed. This is the part of
    the logic that ought to be tested, and that's what this method is for.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that we are now extending our logic a little bit further to allow users
    to close their merge requests, and once this happens, we don't want any more votes
    to take place (it wouldn't make sense to evaluate a merge request once this was
    already closed). To prevent this from happening, we extend our code, and we raise
    an exception on the unfortunate event that someone tries to cast a vote on a closed
    merge request.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'After adding two new statuses (`OPEN` and `CLOSED`), and a new `close()` method,
    we modify the previous methods for the voting to handle this check first:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we want to check that this validation indeed works. For this, we''re going
    to use the `asssertRaises` and `assertRaisesRegex` methods:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The former will expect that the exception provided is raised when calling the
    callable in the second argument, with the arguments (`*args` and `**kwargs`) on
    the rest of the function, and if that's not the case it will fail, saying that
    the exception that was expected to be raised wasn't. The latter does the same,
    but it also checks that the exception that was raised contains the message matching
    the regular expression that was provided as a parameter. Even if the exception
    is raised, but with a different message (not matching the regular expression),
    the test will fail.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Try to check for the error message, as not only will the exception, as an extra
    check, be more accurate and ensure that it is actually the exception we want that
    is being triggered, it will check whether another one of the same types got there
    by chance.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'Note how these methods can be used as context managers as well. In its first
    form (the one used in previous examples), the method takes the exception, then
    the callable, and finally the list of arguments to use in that callable). But
    we could also pass the exception as a parameter of the method, use it as a context
    manager, and evaluate our code inside the block of that context manager, in this
    format:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This second form is generally more useful (and sometimes, the only option);
    for example, if the logic we need to test can't be expressed as a single callable.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, you'll notice that we need to run the same test case, but with
    different data. Instead of repeating, and generating duplicated tests, we can
    build a single one and exercise its condition with different values. This is called
    **parameterized tests**, and we'll start exploring these in the next section.
    Later on, we'll revisit parameterized tests with `pytest`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Parameterized tests
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now, we would like to test how the threshold acceptance for the merge request
    works, just by providing data samples of what the `context` looks like without
    needing the entire `MergeRequest` object. We want to test the part of the `status`
    property that is after the line that checks whether it's closed, but independently.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'The best way to achieve this is to separate that component into another class,
    use composition, and then move on to test this new abstraction with its own test
    suite:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'With these changes, we can run the tests again and verify that they pass, meaning
    that this small refactor didn''t break anything of the current functionality (unit
    tests ensure regression). With this, we can proceed with our goal to write tests
    that are specific to the new class:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, in the `setUp()` method, we define the data fixture to be used throughout
    the tests. In this case, it's not actually needed, because we could have put it
    directly on the method, but if we expect to run some code before any test is executed,
    this is the place to write it, because this method is called once before every
    test is run.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: In this particular case, we could have defined this tuple as a class attribute,
    because it's a constant (static) value. If we needed to run some code, and perform
    some computation (such as building objects or using a factory), then the `setUp()`
    method is our only alternative.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: By writing this new version of the code, the parameters under the code being
    tested are clearer and more compact.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'To simulate that we''re running all of the parameters, the test iterates over
    all the data, and exercises the code with each instance. One interesting helper
    here is the use of `subTest`, which in this case we use to mark the test condition
    being called. If one of these iterations failed, `unittest` would report it with
    the corresponding value of the variables that were passed to the `subTest` (in
    this case, it was named `context`, but any series of keyword arguments would work
    just the same). For example, one error occurrence might look like this:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If you choose to parameterize tests, try to provide the context of each instance
    of the parameters with as much information as possible to make debugging easier.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind parameterized tests is to run the same test condition over different
    sets of data. The idea is that you first identify the equivalence classes of the
    data to test upon, and then you pick the value's representative of each class
    (more details on this later in the chapter). Then you'd like to know for which
    equivalence class your test failed, and the context provided by the `subTest`
    context manager is helpful in this case.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: pytest
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pytest is a great testing framework and can be installed via `pip install pytest`.
    One difference with respect to `unittest` is that, while it's still possible to
    classify test scenarios in classes and create object-oriented models of our tests,
    this is not actually mandatory, and it's possible to write unit tests with less
    boilerplate by just checking the conditions we want to verify in simple functions
    with the `assert` statement.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: By default, making comparisons with an `assert` statement will be enough for
    `pytest` to identify a unit test and report its result accordingly. More advanced
    uses, such as those seen in the previous section, are also possible, but they
    require the use of specific functions from the package.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: A nice feature is that the `pytests` command will run all the tests that it
    can discover, even if they were written with `unittest`. This compatibility makes
    it easier to transition from `unittest` to `pytest` gradually.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Basic test cases with pytest
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The conditions we tested in the previous section can be rewritten in simple
    functions with `pytest`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples with simple assertions are as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Boolean equality comparisons don''t require more than a simple `assert` statement,
    whereas other kinds of checks, such as the ones for the exceptions, do require
    that we use some functions:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this case, `pytest.raises` is the equivalent of `unittest.TestCase.assertRaises`,
    and it also accepts that it be called both as a method and as a context manager.
    If we want to check the message of the exception, instead of a different method
    (such as `assertRaisesRegex`), the same function has to be used, but as a context
    manager, and by providing the `match` parameter with the expression we would like
    to identify.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '`pytest` will also wrap the original exception into a custom one that can be
    expected (by checking some of its attributes, such as `.value`, for instance)
    in case we want to check for more conditions, but this use of the function covers
    the vast majority of cases.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Parameterized tests
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Running parameterized tests with `pytest` is better, not only because it provides
    a cleaner API, but also because each combination of the test with its parameters
    generates a new test case (a new function).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: To work with this, we have to use the `pytest.mark.parametrize` decorator on
    our test. The first parameter of the decorator is a string indicating the names
    of the parameters to pass to the `test` function, and the second has to be iterable
    with the respective values for those parameters.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice how the body of the testing function is reduced to one line (after removing
    the internal `for` loop, and its nested context manager), and the data for each
    test case is correctly isolated from the body of the function, making it easier
    to extend and maintain:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Use `@pytest.mark.parametrize` to eliminate repetition, keep the body of the
    test as cohesive as possible, and make the parameters (test inputs or scenarios)
    that the code must support explicitly.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: An important recommendation when using parametrization is that each parameter
    (every iteration) should correspond to only one testing scenario. That means you
    should not mix different test conditions into the same parameter. If you need
    to test for the combination of different parameters, then use different parameterizations
    stacked up. Stacking up this decorator will create as many test conditions as
    the cartesian product of all the values in the decorators.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a test configured like this:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Will run for the values `(x=1, y=a)`, `(x=1, y=b)`, `(x=2, y=a)`, and `(x=2,
    y=b)`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: This is a better approach as each test is smaller, and each parametrization
    more specific (cohesive). It will allow you to stress the code with the explosion
    of all the possible combinations in an easier way.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Data parameters work well when you have the data you need to test, or you know
    how to build it easily, but in some cases, you need specific objects to be constructed
    for a test, or you find yourself writing or building the same objects repeatedly.
    To help with this, we can use fixtures, as we will see in the next section.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Fixtures
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the great things about `pytest` is how it facilitates creating reusable
    features so that we can feed our tests with data or objects to test more effectively
    and without repetition.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we might want to create a `MergeRequest` object in a particular
    state and use that object in multiple tests. We define our object as a fixture
    by creating a function and applying the `@pytest.fixture` decorator. The tests
    that want to use that fixture will have to have a parameter with the same name
    as the function that''s defined, and `pytest` will make sure that it''s provided:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Remember that tests affect the main code as well, so the principles of clean
    code apply to them as well. In this case, the **Don't Repeat Yourself** (**DRY**)
    principle that we explored in previous chapters appears once again, and we can
    achieve it with the help of `pytest` fixtures.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Besides creating multiple objects or exposing data that will be used throughout
    the test suite, it's also possible to use them to set up some conditions, for
    example, to globally patch some functions that we don't want to be called, or
    when we want patch objects to be used instead.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Code coverage
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Tests runners support coverage plugins (to be installed via `pip`) that provide
    useful information about what lines in the code have been executed as tests ran.
    This information is of great help so that we know which parts of the code need
    to be covered by tests, as well identifying improvements to be made (both in the
    production code and in the tests). What I mean by this is that detecting lines
    of our production code that are uncovered will force us to write a test for that
    part of the code (because remember that code that doesn''t have tests should be
    considered broken). In that attempt of covering the code, several things can happen:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: We might realize we were missing a test scenario completely.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll try to come up with more unit tests or unit tests that cover more lines
    of code.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll try to simplify our production code, removing redundancies, and making
    it more compact, meaning it's easier to be covered.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We might even realize that the lines of code we're trying to cover are unreachable
    (perhaps there was a mistake in the logic) and can be safely removed.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep in mind that even though these are positive points, coverage should never
    be a target, only a metric. This means trying to achieve a high coverage, just
    to reach 100%, won't be productive or effective. We should understand code coverage
    as a unit to identify obvious parts of the code that need testing and see how
    we can improve that. We can, however, set a minimum threshold of say 80% (a generally
    accepted value) as the minimum level of desired coverage to know that the project
    has a reasonable number of tests.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, thinking that a high degree of code coverage is a sign of a healthy
    code base is also dangerous: keep in mind that most of the coverage tools will
    report on production lines of code that have been executed. That a line has been
    called doesn''t mean that it has been properly tested (only that it ran). A single
    statement might encapsulate multiple logical conditions, each of which needs to
    be tested separately.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Don't be misguided by a high degree of code coverage, and keep thinking about
    ways for testing the code, including those lines that are already covered.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: One of the most widely used libraries for this is `coverage` ([https://pypi.org/project/coverage/](https://pypi.org/project/coverage/)).
    We'll explore how to set up this tool in the next section.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Setting up rest coverage
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the case of `pytest`, we can install the `pytest-cov` package. Once installed,
    when the tests are run, we have to tell the `pytest` runner that `pytest-cov`
    will also run, and which package (or packages) should be covered (among other
    parameters and configurations).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: This package supports multiple configurations, including different sorts of
    output formats, and it's easy to integrate it with any CI tool, but among all
    these features, a highly recommended option is to set the flag that will tell
    us which lines haven't been covered by tests yet, because this is what's going
    to help us diagnose our code and allow us to start writing more tests.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'To show you an example of what this would look like, use the following command:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will produce an output similar to the following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, it's telling us that there is a line that doesn't have unit tests so that
    we can take a look and see how to write a unit test for it. This is a common scenario
    where we realize that to cover those missing lines, we need to refactor the code
    by creating smaller methods. As a result, our code will look much better, as in
    the example we saw at the beginning of this chapter.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: The problem lies in the inverse situation—can we trust the high coverage? Does
    this mean our code is correct? Unfortunately, having good test coverage is a necessary
    but insufficient condition for clean code. Not having tests for parts of the code
    is clearly something bad. Having tests is actually very good, but we can only
    say this for the tests that do exist. However, we don't know much about what tests
    we are missing, and we might be missing lots of conditions even when code coverage
    is high.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: These are some of the caveats of test coverage, which we will mention in the
    next section.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Caveats of test coverage
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Python is interpreted and, at a very high level, coverage tools take advantage
    of this to identify the lines that were interpreted (run) while the tests were
    running. It will then report this at the end. The fact that a line was interpreted
    does not mean that it was properly tested, and this is why we should be careful
    about reading the final coverage report and trusting what it says.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: This is actually true for any language. The fact that a line was exercised does
    not mean at all that it was stressed with all its possible combinations. The fact
    that all branches run successfully with the provided data only means that the
    code supported that combination, but it doesn't tell us anything about any other
    possible combinations of parameters that would make the program crash (fuzzy testing).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Use coverage as a tool to find blind spots in the code, but not as a metric
    or target goal.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this with a simple example, consider the following code:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, let''s say we write the following test for it:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: If we run the tests with coverage, the report will give us a flashy 100% of
    coverage. Needless to say, we're missing a test for half of the conditions of
    the single statement that executed. Even more troubling is the fact that since
    the `else` clause of the statement didn't run, we don't know in which ways our
    code might break (to make this example even more exaggerated, imagine there was
    an incorrect statement, such as `1/0` instead of the string `"odd"`, or that there's
    a function call).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Arguably, we might go a step further and think that this is only the "happy
    path" because we're providing good values to the function. But what about incorrect
    types? How should the function defend against that?
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: As you see, even a single and innocent-looking statement might trigger lots
    of questions and testing conditions that we need to be prepared for.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: It's a good idea to check how covered our code is, and even configure code coverage
    thresholds as part of the CI build, but we have to keep in mind that this is just
    another tool for us. And just like previous tools that we have explored (linters,
    code checkers, formatters, and suchlike), it's useful only in the context of more
    tools and a good environment prepared for a clean code base.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Another tool that will help us in our testing efforts is the use of mock objects.
    We explore these in the next section.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Mock objects
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are cases where our code is not the only thing that will be present in
    the context of our tests. After all, the systems we design and build have to do
    something real, and that usually means connecting to external services (databases,
    storage services, external APIs, cloud services, and so on). Because they need
    to have those side effects, they're inevitable. As much as we abstract our code,
    program toward interfaces, and isolate code from external factors to minimize
    side effects, they will be present in our tests, and we need an effective way
    to handle that.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '`Mock` objects are one of the best tactics used to protect our unit tests against
    undesirable side effects (as seen earlier in this chapter). Our code might need
    to perform an HTTP request or send a notification email, but we surely don''t
    want that to happen in our unit tests. Unit tests should target the logic of our
    code, and run quickly, as we want to run them quite often, which means we cannot
    afford latency. Therefore, real unit tests don''t use any actual service—they
    don''t connect to any database, they don''t issue HTTP requests, and basically,
    they do nothing other than exercise the logic of the production code.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: We need tests that do such things, but they aren't units. Integration tests
    are supposed to test functionality with a broader perspective, almost mimicking
    the behavior of a user. But they aren't fast. Because they connect to external
    systems and services, they take longer and are more expensive to run. In general,
    we would like to have lots of unit tests that run quickly in order to run them
    all the time and have integration tests run less often (for instance, on any new
    merge request).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: While mock objects are useful, abusing them ranges between a code smell or an
    anti-pattern. This is the first issue we discuss in the next section, before moving
    into the details of using mocks.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: A fair warning about patching and mocks
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: I said before that unit tests help us write better code, because the moment
    we start thinking about how to test our code, we'll realize how it can be improved
    to make it testable. And usually, as the code becomes more testable, it becomes
    cleaner (more cohesive, granular, divided into smaller, components, and so on).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting gain is that testing will help us notice code smells in
    parts where we thought our code was correct. One of the main warnings that our
    code has code smells is whether we find ourselves trying to monkey patch (or mock)
    a lot of different things just to cover a simple test case.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: The `unittest` module provides a tool for patching our objects at `unittest.mock.patch`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Patching means that the original code (given by a string denoting its location
    at import time) will be replaced by something else, other than its original code.
    If no replacement object is provided, the default is a standard mock object that
    will simply accept all method calls or attributes is asked about.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: The patching function replaces the code at runtime and has the disadvantage
    that we are losing contact with the original code that was there in the first
    place, making our tests a little shallower. It also carries performance considerations
    because of the overhead that imposes modifying objects in the interpreter at runtime,
    and it's something that might require future changes if we refactor our code and
    move things around (because the strings declared in the patching function will
    no longer be valid).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Using monkey patching or mocks in our tests might be acceptable, and by itself
    it doesn't represent an issue. On the other hand, abuse in monkey patching is
    indeed a red flag telling us that something has to be improved in our code.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the same way that encountering difficulties while testing a
    function might give us the idea that that function is probably too big and should
    be broken down into smaller pieces, trying to test a piece of code that requires
    a very invasive monkey patch should tell us that perhaps the code is relying too
    heavily on hard dependencies, and that dependency injection should be used instead.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Using mock objects
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In unit testing terminology, there are several types of object that fall into
    the category named **test doubles**. A test double is a type of object that will
    take the place of a real one in our test suite for different kinds of reasons
    (maybe we don't need the actual production code, but just a dummy object would
    work, or maybe we can't use it because it requires access to services or it has
    side effects that we don't want in our unit tests, and so on).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: There are different types of test double, such as dummy objects, stubs, spies,
    or mocks.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Mocks are the most general type of object, and since they''re quite flexible
    and versatile, they are appropriate for all cases without needing to go into much
    detail about the rest of them. It is for this reason that the standard library
    also includes an object of this kind, and it is common in most Python programs.
    That''s the one we are going to be using here: `unittest.mock.Mock`.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: A **mock** is a type of object created to a specification (usually resembling
    the object of a production class) and some configured responses (that is, we can
    tell the mock what it should return upon certain calls, and what its behavior
    should be). The `Mock` object will then record, as part of its internal status,
    how it was called (with what parameters, how many times, and so on), and we can
    use that information to verify the behavior of our application at a later stage.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: In the case of Python, the `Mock` object that's available from the standard
    library provides a nice API to make all sorts of behavioral assertions, such as
    checking how many times the mock was called, with what parameters, and so on.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Types of mocks
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The standard library provides `Mock` and `MagicMock` objects in the `unittest.mock`
    module. The former is a test double that can be configured to return any value
    and will keep track of the calls that were made to it. The latter does the same,
    but it also supports magic methods. This means that, if we have written idiomatic
    code that uses magic methods (and parts of the code we are testing will rely on
    that), it's likely that we will have to use a `MagicMock` instance instead of
    just a `Mock`.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'Trying to use `Mock` when our code needs to call magic methods will result
    in an error. See the following code for an example of this:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We want to test this function; however, another test needs to call the `author_by_id`
    function. For some reason, since we''re not testing that function, any value provided
    to that function (and returned) will be good:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As anticipated, this will not work:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Using `MagicMock` instead will work. We can even configure the magic method
    of this type of mock to return something we need in order to control the execution
    of our test:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: A use case for test doubles
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To see a possible use of mocks, we need to add a new component to our application
    that will be in charge of notifying the merge request of the `status` of the `build`.
    When a `build` is finished, this object will be called with the ID of the merge
    request and the `status` of the `build`, and it will update the `status` of the
    merge request with this information by sending an HTTP `POST` request to a particular
    fixed endpoint:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This class has many side effects, but one of them is an important external dependency
    that is hard to surmount. If we try to write a test over it without modifying
    anything, it will fail with a connection error as soon as it tries to perform
    the HTTP connection.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: As a testing goal, we just want to make sure that the information is composed
    correctly, and that library requests are being called with the appropriate parameters.
    Since this is an external dependency, we don't want to test the `requests` module;
    just checking that it's called correctly will be enough.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Another problem we will face when trying to compare data being sent to the library
    is that the class is calculating the current timestamp, which is impossible to
    predict in a unit test. Patching `datetime` directly is not possible, because
    the module is written in C. Some external libraries that can do that (`freezegun`,
    for example), but they come with a performance penalty, and for this example,
    this would be overkill. Therefore, we opt to wrap the functionality we want in
    a static method that we will be able to patch.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have established the points that need to be replaced in the code,
    let''s write the unit test:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: First, we use `mock.patch` as a decorator to replace the `requests` module.
    The result of this function will create a `mock` object that will be passed as
    a parameter to the test (named `mock_requests` in this example). Then, we use
    this function again, but this time as a context manager to change the return value
    of the method of the class that computes the date of the `build`, replacing the
    value with one we control, which we will use in the assertion.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Once we have all of this in place, we can call the class method with some parameters,
    and then we can use the `mock` object to check how it was called. In this case,
    we are using the method to see whether `requests.post` was indeed called with
    the parameters as we wanted them to be composed.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: This is a nice feature of mocks—not only do they put some boundaries around
    all external components (in this case to prevent actually sending some notifications
    or issuing HTTP requests), but they also provide a useful API to verify the calls
    and their parameters.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: While, in this case, we were able to test the code by setting the respective
    `mock` objects in place, it's also true that we had to patch quite a lot in proportion
    to the total lines of code for the main functionality. There is no rule about
    the ratio of pure productive code being tested versus how many parts of that code
    we have to mock, but certainly, by using common sense, we can see that, if we
    had to patch quite a lot of things in the same parts, something is not clearly
    abstracted, and it looks like a code smell.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'The patching of external dependencies can be used in combination with fixtures
    to apply some global configurations. For example, it''s usually a good idea to
    prevent all the unit tests from performing HTTP calls, so within the subdirectory
    for unit tests, we can add a fixture in the configuration file of `pytest` (`tests/unit/conftest.py`):'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This function will be invoked automatically in all unit tests (because of `autouse=True`),
    and when it does, it will patch the `post` function in the `requests` module.
    This is just an idea you can adapt to your projects to add some extra safety and
    make sure your unit tests are free of side effects.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore how to refactor code to overcome this issue.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Refactoring
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refactoring means changing the structure of the code by rearranging its internal
    representation without modifying its external behavior.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: One example would be if you identify a class that has lots of responsibilities
    and very long methods, and then decide to change it by using smaller methods,
    creating new internal collaborators, and distributing responsibilities into new,
    smaller objects. As you do that, you're careful not to change the original interface
    of that class, keep all its public methods as before, and not change any signature.
    To an external observer of that class, it might look like nothing happened (but
    we know otherwise).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '**Refactoring** is a critical activity in software maintenance, yet something
    that can''t be done (at least not correctly) without having unit tests. This is
    because, as each change gets made, we need to know that our code is still correct.
    In a sense, you can think of our unit tests as the "external observer" for our
    code, making sure the contract doesn''t break.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Every now and then, we need to support a new feature or use our software in
    unintended ways. The only way to accommodate such requirements is by first refactoring
    our code, to make it more generic or flexible.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Typically, when refactoring our code, we want to improve its structure and make
    it better, sometimes more generic, more readable, or more flexible. The challenge
    is to achieve these goals while at the same time preserving the exact same functionality
    it had prior to the modifications that were made. This constraint of having to
    support the same functionalities as before, but with a different version of the
    code, implies that we need to run regression tests on code that was modified.
    The only cost-effective way of running regression tests is if those tests are
    automatic. The most cost-effective version of automatic tests is unit testing.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Evolving our code
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous example, we were able to separate out the side effects from
    our code to make it testable by patching those parts of the code that depended
    on things we couldn't control on the unit test. This is a good approach since,
    after all, the `mock.patch` function comes in handy for these sorts of tasks and
    replaces the objects we tell it to, giving us back a `Mock` object.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: The downside of that is that we have to provide the path of the object we are
    going to mock, including the module, as a string. This is a bit fragile, because
    if we refactor our code (let's say we rename the file or move it to some other
    location), all the places with the patch will have to be updated, or the test
    will break.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: In the example, the fact that the `notify()` method directly depends on an implementation
    detail (the `requests` module) is a design issue; that is, it is taking its toll
    on the unit tests as well with the aforementioned fragility that is implied.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'We still need to replace those methods with doubles (mocks), but if we refactor
    the code, we can do it in a better way. Let''s separate these methods into smaller
    ones, and most importantly inject the dependency rather than keep it fixed. The
    code now applies the dependency inversion principle, and it expects to work with
    something that supports an interface (in this example, an implicit one), such
    as the one that the `requests` module provides:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We separate the methods (note how notify is now compose `+` deliver), make `compose_payload()`
    a new method (so that we can replace, without the need to patch the class), and
    require the `transport` dependency to be injected. Now that `transport` is a dependency,
    it is much easier to change that object for any double we want.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'It is even possible to expose a fixture of this object, with the doubles replaced
    as required:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As mentioned in the first chapter, the goal of having clean code is to have
    maintainable code, code that we can refactor so that it can evolve and extend
    to more requirements. To this end, tests are a great help. But since tests are
    so important, we also need to refactor them so that they can also maintain their
    relevance and usefulness as the code evolves. This is the topic of discussion
    of the next section.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Production code isn't the only one that evolves
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We keep saying that unit tests are as important as production code. And if we
    are careful enough with the production code to create the best possible abstraction,
    why wouldn't we do the same for unit tests?
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: If the code for unit tests is as important as the main code, then it's wise
    to design it with extensibility in mind and make it as maintainable as possible.
    After all, this is the code that will have to be maintained by an engineer other
    than its original author, so it has to be readable.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: The reason why we pay so much attention to the code's flexibility is that we
    know requirements change and evolve over time, and eventually, as domain business
    rules change, our code will have to change as well to support these new requirements.
    Since the production code changed to support new requirements, in turn, the testing
    code will have to change as well to support the newer version of the production
    code.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: In one of the first examples we used, we created a series of tests for the merge
    request object, trying different combinations and checking the status at which
    the merge request was left. This is a good first approach, but we can do better
    than that.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Once we understand the problem better, we can start creating better abstractions.
    With this, the first idea that comes to mind is that we can create a higher-level
    abstraction that checks for particular conditions. For example, if we have an
    object that is a test suite that specifically targets the `MergeRequest` class,
    we know its functionality will be limited to the behavior of this class (because
    it should comply to the SRP), and therefore we could create specific testing methods
    on this testing class. These will only make sense for this class, but that will
    be helpful in reducing a lot of boilerplate code.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of repeating assertions that follow the exact same structure, we can
    create a method that encapsulates this and reuse it across all of the tests:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If something changes with how we check the status of a merge request (or let's
    say we want to add extra checks), there is only one place (the `assert_approved()`
    method) that will have to be modified. More importantly, by creating these higher-level
    abstractions, the code that started as merely unit tests starts to evolve into
    what could end up being a testing framework with its own API or domain language,
    making testing more declarative.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: More about testing
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the concepts we have revisited so far, we know how to test our code, think
    about our design in terms of how it is going to be tested, and configure the tools
    in our project to run the automated tests that will give us some degree of confidence
    regarding the quality of the software we have written.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: If our confidence in the code is determined by the unit tests written on it,
    how do we know that they are enough? How could we be sure that we have been through
    enough on the test scenarios and that we are not missing some tests? Who says
    that these tests are correct? Meaning, who tests the tests?
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: The first part of the question, about being thorough in terms of the tests we
    write, is answered by going beyond in our testing efforts through property-based
    testing.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: The second part of the question might have multiple answers from different points
    of view, but we are going to briefly mention mutation testing as a means of determining
    that our tests are indeed correct. In this sense, we are thinking that the unit
    tests check our main productive code, and this works as a control for the unit
    tests as well.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Property-based testing
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Property-based testing consists of generating data for tests cases to find scenarios
    that will make the code fail, which weren't covered by our previous unit tests.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: The main library for this is `hypothesis` which, configured along with our unit
    tests, will help us find problematic data that will make our code fail.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: We can imagine that what this library does is find counterexamples for our code.
    We write our production code (and unit tests for it!), and we claim it's correct.
    Now, with this library, we define a `hypothesis` that must hold for our code,
    and if there are some cases where our assertions don't hold, `hypothesis` will
    provide a set of data that causes the error.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: The best thing about unit tests is that they make us think harder about our
    production code. The best thing about `hypothesis` is that it makes us think harder
    about our unit tests.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Mutation testing
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We know that tests are the formal verification method we have to ensure that
    our code is correct. And what makes sure that the test is correct? The production
    code, you might think, and yes, in a way this is correct. We can think of the
    main code as a counterbalance for our tests.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: The point in writing unit tests is that we are protecting ourselves against
    bugs and testing for failure scenarios we don't want to happen in production.
    It's good that the tests pass, but it would be bad if they pass for the wrong
    reasons. That is, we can use unit tests as an automatic regression tool—if someone
    introduces a bug in the code, later on, we expect at least one of our tests to
    catch it and fail. If this doesn't happen, either there is a test missing, or
    the ones we had are not doing the right checks.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: This is the idea behind mutation testing. With a mutation testing tool, the
    code will be modified to new versions (called **mutants**) that are variations
    of the original code, but with some of its logic altered (for example, operators
    are swapped, conditions are inverted).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: A good test suite should catch these mutants and kill them, in which case it
    means we can rely on the tests. If some mutants survive the experiment, it's usually
    a bad sign. Of course, this is not entirely precise, so there are intermediate
    states we might want to ignore.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'To quickly show you how this works and to allow you to get a practical idea
    of this, we are going to use a different version of the code that computes the
    status of a merge request based on the number of approvals and rejections. This
    time, we have changed the code for a simple version that, based on these numbers,
    returns the result. We have moved the enumeration with the constants for the statuses
    to a separate module so that it now looks more compact:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'And now will we add a simple unit test, checking one of the conditions and
    its expected `result`:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, we will install `mutpy`, a mutation testing tool for Python, with `pip
    install mutpy`, and tell it to run the mutation testing for this module with these
    tests. The following code runs for different cases, which are distinguished by
    changing the `CASE` environment variable:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'If you run the previous command for case 2 (which is also possible to run as
    `make mutation CASE=2`), the result is going to look something similar to this:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This is a good sign. Let''s take a particular instance to analyze what happened.
    One of the lines on the output shows the following mutant:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Notice that this mutant consists of the original version with the operator changed
    in line `11` (`>` for `<`), and the result is telling us that this mutant was
    killed by the tests. This means that with this version of the code (let's imagine
    that someone makes this change by mistake), then the result of the function would
    have been `APPROVED`, and since the test expects it to be `REJECTED`, it fails,
    which is a good sign (the test caught the bug that was introduced).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Mutation testing is a good way to assure the quality of the unit tests, but
    it requires some effort and careful analysis. By using this tool in complex environments,
    we will have to take some time analyzing each scenario. It is also true that it
    is expensive to run these tests because it requires multiple runs of different
    versions of the code, which might take up too many resources and may take longer
    to complete. However, it would be even more expensive to have to make these checks
    manually and will require much more effort. Not doing these checks at all might
    be even riskier because we would be jeopardizing the quality of the tests.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Common themes in testing
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I'd like to briefly touch on some topics that are usually good to keep in mind
    when thinking of ways of how to test our code because they're recurrent and helpful.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'These are points you''ll usually want to think about when trying to come up
    with tests for the code because they lead to ruthless testing. When you''re writing
    unit tests, your mindset has to be all about breaking the code: you want to make
    sure you find errors so that you can fix them, and that they don''t slip into
    production (which will be much worse).'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Boundaries or limit values
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Boundary values are usually a great source of trouble in the code, so that's
    probably a good starting place. Take a look at the code and inspect for conditions
    set around some values. Then, add tests to make sure you include these values.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in a line of code such as this:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Add explicit tests for the zero, because this seems to be a special case in
    the code.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: More generally, in a condition that checks for a range of values, check both
    ends of the interval. If the code deals with data structures (such as a list or
    a stack), check for an empty list, or a full stack, and make sure the indexes
    are always set correctly, even for values on their limits.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Classes of equivalence
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An equivalence class is a partition over a set, such that all elements in that
    partition are equivalent with respect to some function. Because all elements inside
    this partition are equivalent, we only need one of them as a representative in
    order to test that condition.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'To give a simple example, let''s recap our previous code used in the section
    to demonstrate code coverage:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Here, the function has a single `if` statement and is returning different data
    depending on that condition.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'If we wanted to simplify the testing for this function by stipulating that
    the set of values for input testing, `S`, is the set of integers, we could argue
    that it can be partitioned into two: even and odd numbers.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Because this code does something for even numbers, and something else for odd
    ones, we can say that these are our testing conditions. Namely, we only need one
    element of each sub-set to test the entire condition, no more than that. In other
    words, testing with 2 is the same as testing with 4 (the same logic is exercised
    in both cases), so we don't need both, but only one (any) of them. The same goes
    for 1, and 3 (or any other odd number).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: We can separate these representative elements into different parameters, and
    run the same test by using the `@pytest.mark.parametrize` decorator. The important
    thing is to make sure we cover all the cases, and that we're not repeating elements
    (that is, that we're not adding two different parametrizations with elements of
    the same partition, because that doesn't add any value).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'Testing by classes of equivalence has two benefits: on the one hand, we test
    effectively by not repeating new values that don''t add anything to our testing
    scenario, and on the other hand, if we exhaust all classes, then we have good
    coverage of the scenarios to test for.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Edge cases
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, try to add specific tests for all edge cases you can think of. This
    pretty much depends on the business logic and the peculiarities of the code you're
    writing, and there's some overlap with the idea of testing around boundary values.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: For example, if part of your code deals with dates, make sure you test for leap
    years, the 29^(th) of February, and in or around the new year.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have assumed we're writing the tests after the code. This is a typical
    case. After all, most of the time, you'll find yourself working on an already
    existing code base, rather than starting it from scratch.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: There's an alternative, which is writing the test prior to the code. That might
    be because you're starting a new project or feature, and you want to see what
    it will look like before writing the actual production code. Or it might be because
    there's a defect on the code base, and you first want to write a test to reproduce
    it, before jumping into the fix. This is called **Test-Driven Design** (**TDD**)
    and is discussed in the next section.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: A brief introduction to test-driven development
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are entire books dedicated only to TDD, so it would not be realistic to
    try and cover this topic comprehensively in this book. However, it's such an important
    topic that it has to be mentioned.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind TDD is that tests should be written before production code in
    a way that the production code is only written to respond to tests that are failing
    due to that missing implementation of the functionality.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple reasons why we would like to write the tests first and then
    the code. From a pragmatic point of view, we would be covering our production
    code quite accurately. Since all of the production code was written to respond
    to a unit test, it would be highly unlikely that there are tests missing for functionality
    (that doesn't mean that there is 100% coverage of course, but at least all the
    main functions, methods, or components will have their respective tests, even
    if they aren't completely covered).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow is simple and, at a high level, consist of three steps:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Write a unit test that describes how the code should behave. That can either
    be new functionality that still doesn't exist or current code that is broken,
    in which case the test describes the desired scenario. Running this test for the
    first time must fail.
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make the minimal changes in the code to make that test pass. The test should
    now pass.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Improve (refactor) the code and run the test again, making sure it still works.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This cycle has been popularized as the famous **red-green-refactor**, meaning
    that in the beginning, the tests fail (red), then we make them pass (green), and
    then we proceed to refactor the code and iterate it.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unit testing is a really interesting and deep topic, but more importantly, it
    is a critical part of the clean code. Ultimately, unit tests are what determine
    the quality of the code. Unit tests often act as a mirror for the code—when the
    code is easy to test, it's clear and correctly designed, and this will be reflected
    in the unit tests.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: The code for the unit tests is as important as production code. All principles
    that apply to production code also apply to unit tests. This means that they should
    be designed and maintained with the same effort and thoughtfulness. If we don't
    care about our unit tests, they will start to have problems and become defective
    (or problematic) and, as a result of that, useless. If this happens, and they
    are hard to maintain, they become a liability, which makes things even worse,
    because people will tend to ignore them or disable them entirely. This is the
    worst scenario because once this happens, the entire production code is in jeopardy.
    Moving forward blindly (without unit tests) is a recipe for disaster.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, Python provides many tools for unit testing, both in the standard library
    and available through `pip`. They are of great help and investing time in configuring
    them pays off in the long run.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how unit tests work as the formal specification of the program,
    and the proof that a piece of software works according to the specification, and
    we also learned that when it comes to discovering new testing scenarios, there
    is always room for improvement and we can always create more tests. In this sense,
    expanding our unit tests with different approaches (such as property-based testing
    or mutation testing) is a good investment.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll learn about design patterns and their applicability
    in Python.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is a list of information you can refer to:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'The `unittest` module of the Python standard library contains comprehensive
    documentation on how to start building a test suite: [https://docs.python.org/3/library/unittest.html](https://docs.python.org/3/library/unittest.html)'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hypothesis: [https://hypothesis.readthedocs.io/en/latest/](https://hypothesis.readthedocs.io/en/latest/)'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pytest''s` official documentation: [https://docs.pytest.org/en/latest/](https://docs.pytest.org/en/latest/)'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Cathedral and the Bazaar: Musings on Linux and Open Source by an Accidental
    Revolutionary (CatB)*, written by Eric S. Raymond (publisher: O''Reilly Media,
    1999)'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refactoring: [https://refactoring.com/](https://refactoring.com/)'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The art of software testing*, written by *Glenford J. Myers* (publisher: Wiley;
    3^(rd) edition, November 8, 2011)'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Writing testable code: [https://testing.googleblog.com/2008/08/by-miko-hevery-so-you-decided-to.html](https://testing.googleblog.com/2008/08/by-miko-hevery-so-you-decided-to.html)'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
