["```py\n<table> \n<tr id=\"places_national_flag__row\"><td class=\"w2p_fl\"><label for=\"places_national_flag\"      id=\"places_national_flag__label\">National Flag:</label></td>\n<td class=\"w2p_fw\"><img src=\"img/gb.png\" /></td><td           class=\"w2p_fc\"></td></tr> \n... \n<tr id=\"places_neighbours__row\"><td class=\"w2p_fl\"><label for=\"places_neighbours\"      id=\"places_neighbours__label\">Neighbours: </label></td><td class=\"w2p_fw\"><div><a href=\"/iso/IE\">IE </a></div></td><td class=\"w2p_fc\"></td></tr></table>\n\n```", "```py\n>>> import re \n>>> from chp1.advanced_link_crawler import download \n>>> url = 'http://example.webscraping.com/view/UnitedKingdom-239' \n>>> html = download(url) \n>>> re.findall(r'<td class=\"w2p_fw\">(.*?)</td>', html) \n['<img src=\"img/gb.png\" />', \n  '244,820 square kilometres', \n  '62,348,447', \n  'GB', \n  'United Kingdom', \n  'London', \n  '<a href=\"/continent/EU\">EU</a>', \n  '.uk', \n  'GBP', \n  'Pound', \n  '44', \n  '@# #@@|@## #@@|@@# #@@|@@## #@@|@#@ #@@|@@#@ #@@|GIR0AA', \n  '^(([A-Z]d{2}[A-Z]{2})|([A-Z]d{3}[A-Z]{2})|([A-Z]{2}d{2}     [A-Z]{2})|([A-Z]{2}d{3}[A-Z]{2})|([A-Z]d[A-Z]d[A-Z]{2})       |([A-Z]{2}d[A-Z]d[A-Z]{2})|(GIR0AA))$', \n  'en-GB,cy-GB,gd', \n  '<div><a href=\"/iso/IE\">IE </a></div>']\n\n```", "```py\n>>> re.findall('<td class=\"w2p_fw\">(.*?)</td>', html)[1] \n'244,820 square kilometres'\n\n```", "```py\n>>> re.findall('<tr id=\"places_area__row\"><td class=\"w2p_fl\"><label for=\"places_area\" id=\"places_area__label\">Area: </label></td><td class=\"w2p_fw\">(.*?)</td>', html) \n['244,820 square kilometres']\n\n```", "```py\n>>> re.findall('''<tr id=\"places_area__row\">.*?<tds*class=[\"']w2p_fw[\"']>(.*?)</td>''', html) ['244,820 square kilometres']\n\n```", "```py\n pip install beautifulsoup4\n\n```", "```py\n        <ul class=country> \n            <li>Area \n            <li>Population \n        </ul>\n\n```", "```py\n>>> from bs4 import BeautifulSoup \n>>> from pprint import pprint\n>>> broken_html = '<ul class=country><li>Area<li>Population</ul>' \n>>> # parse the HTML \n>>> soup = BeautifulSoup(broken_html, 'html.parser') \n>>> fixed_html = soup.prettify() \n>>> pprint(fixed_html)\n\n<ul class=\"country\">\n <li>\n  Area\n  <li>\n   Population\n  </li>\n </li>\n</ul>\n\n```", "```py\npip install html5lib\n\n```", "```py\n>>> soup = BeautifulSoup(broken_html, 'html5lib') \n>>> fixed_html = soup.prettify() \n>>> pprint(fixed_html)\n<html>\n   <head>\n   </head>\n   <body>\n     <ul class=\"country\">\n       <li>\n         Area\n       </li>\n       <li>\n         Population\n       </li>\n     </ul>\n   </body>\n</html>\n\n```", "```py\n>>> ul = soup.find('ul', attrs={'class':'country'}) \n>>> ul.find('li')  # returns just the first match \n<li>Area</li> \n>>> ul.find_all('li')  # returns all matches \n[<li>Area</li>, <li>Population</li>]\n\n```", "```py\n>>> from bs4 import BeautifulSoup \n>>> url = 'http://example.webscraping.com/places/view/United-Kingdom-239' \n>>> html = download(url) \n>>> soup = BeautifulSoup(html)   \n>>> # locate the area row \n>>> tr = soup.find(attrs={'id':'places_area__row'}) \n>>> td = tr.find(attrs={'class':'w2p_fw'})  # locate the data element\n>>> area = td.text  # extract the text from the data element\n>>> print(area) \n244,820 square kilometres\n\n```", "```py\n>>> from lxml.html import fromstring, tostring\n>>> broken_html = '<ul class=country><li>Area<li>Population</ul>' \n>>> tree = fromstring(broken_html)  # parse the HTML  \n>>> fixed_html = tostring(tree, pretty_print=True) \n>>> print(fixed_html) \n<ul class=\"country\"> \n    <li>Area</li> \n    <li>Population</li> \n</ul>\n\n```", "```py\npip install cssselect\n\n```", "```py\n>>> tree = fromstring(html) \n>>> td = tree.cssselect('tr#places_area__row > td.w2p_fw')[0] \n>>> area = td.text_content() \n>>> print(area) \n244,820 square kilometres\n\n```", "```py\nSelect any tag: * \nSelect by tag <a>: a \nSelect by class of \"link\": .link  \nSelect by tag <a> with class \"link\": a.link \nSelect by tag <a> with ID \"home\": a#home \nSelect by child <span> of tag <a>: a > span \nSelect by descendant <span> of tag <a>: a span \nSelect by tag <a> with attribute title of \"Home\": a[title=Home]\n\n```", "```py\n>>> tree = fromstring(html)\n>>> area = tree.xpath('//tr[@id=\"places_area__row\"]/td[@class=\"w2p_fw\"]/text()')[0]\n>>> print(area)\n244,820 square kilometres\n\n```", "```py\n>>> table = tree.xpath('//table')[0]\n>>> table.getchildren()\n[<Element tr at 0x7f525158ec78>,\n <Element tr at 0x7f52515ad638>,\n <Element tr at 0x7f52515ad5e8>,\n <Element tr at 0x7f52515ad688>,\n <Element tr at 0x7f52515ad728>,\n...]\n\n```", "```py\n>>> prev_sibling = table.getprevious()\n>>> print(prev_sibling)\nNone\n>>> next_sibling = table.getnext()\n>>> print(next_sibling)\n<Element div at 0x7f5252fe9138>\n>>> table.getparent()\n<Element form at 0x7f52515ad3b8>\n\n```", "```py\nFIELDS = ('area', 'population', 'iso', 'country', 'capital', 'continent', 'tld', 'currency_code', 'currency_name', 'phone', 'postal_code_format', 'postal_code_regex', 'languages', 'neighbours') \n\nimport re \ndef re_scraper(html): \n    results = {} \n    for field in FIELDS: \n        results[field] = re.search('<tr id=\"places_%s__row\">.*?<td class=\"w2p_fw\">(.*?)</td>' % field, html).groups()[0] \n    return results \n\nfrom bs4 import BeautifulSoup \ndef bs_scraper(html): \n    soup = BeautifulSoup(html, 'html.parser') \n    results = {} \n    for field in FIELDS: \n        results[field] = soup.find('table').find('tr',id='places_%s__row' % field).find('td',                  class_='w2p_fw').text \n    return results \n\nfrom lxml.html import fromstring\ndef lxml_scraper(html): \n    tree = fromstring(html) \n    results = {} \n    for field in FIELDS: \n        results[field] = tree.cssselect('table > tr#places_%s__row > td.w2p_fw' % field)[0].text_content() \n    return results \n\ndef lxml_xpath_scraper(html):\n    tree = fromstring(html)\n    results = {}\n    for field in FIELDS:\n        results[field] = tree.xpath('//tr[@id=\"places_%s__row\"]/td[@class=\"w2p_fw\"]' % field)[0].text_content()\n    return results\n\n```", "```py\nimport time\nimport re\nfrom chp2.all_scrapers import re_scraper, bs_scraper, \n    lxml_scraper, lxml_xpath_scraper\nfrom chp1.advanced_link_crawler import download\n\nNUM_ITERATIONS = 1000 # number of times to test each scraper\nhtml = download('http://example.webscraping.com/places/view/United-Kingdom-239')\n\nscrapers = [\n   ('Regular expressions', re_scraper),\n   ('BeautifulSoup', bs_scraper),\n   ('Lxml', lxml_scraper),\n   ('Xpath', lxml_xpath_scraper)]\n\nfor name, scraper in scrapers:\n    # record start time of scrape\n    start = time.time()\n    for i in range(NUM_ITERATIONS):\n        if scraper == re_scraper:\n            re.purge()\n        result = scraper(html)\n        # check scraped result is as expected\n        assert result['area'] == '244,820 square kilometres'\n    # record end time of scrape and output the total\n    end = time.time()\n    print('%s: %.2f seconds' % (name, end - start))\n\n```", "```py\n    $ python chp2/test_scrapers.py \n    Regular expressions: 1.80 seconds\n    BeautifulSoup: 14.05 seconds\n    Lxml: 3.08 seconds\n    Xpath: 1.07 seconds\n\n```", "```py\ndef link_crawler(..., scrape_callback=None): \n    ... \n    data = [] \n    if scrape_callback: \n        data.extend(scrape_callback(url, html) or []) \n        ...\n\n```", "```py\ndef scrape_callback(url, html): \n    fields = ('area', 'population', 'iso', 'country', 'capital',\n              'continent', 'tld', 'currency_code', 'currency_name',\n              'phone', 'postal_code_format', 'postal_code_regex',\n              'languages', 'neighbours')\n    if re.search('/view/', url): \n        tree = fromstring(html) \n        all_rows = [\n            tree.xpath('//tr[@id=\"places_%s__row\"]/td[@class=\"w2p_fw\"]' % field)[0].text_content()\n            for field in fields] \n        print(url, all_rows)\n\n```", "```py\n>>> from chp2.advanced_link_crawler import link_crawler, scrape_callback\n>>> link_crawler('http://example.webscraping.com', '/(index|view)/', scrape_callback=scrape_callback)\n\n```", "```py\nDownloading: http://example.webscraping.com/view/Botswana-30\nhttp://example.webscraping.com/view/Botswana-30 ['600,370 square kilometres', '2,029,307', 'BW', 'Botswana', 'Gaborone', 'AF', '.bw', 'BWP', 'Pula', '267', '', '', 'en-BW,tn-BW', 'ZW ZA NA ']\n\n```", "```py\nimport csv \nimport re\nfrom lxml.html import fromstring\nclass CsvCallback: \n    def __init__(self): \n        self.writer = csv.writer(open('../data/countries.csv', 'w')) \n        self.fields = ('area', 'population', 'iso', 'country', \n                       'capital', 'continent', 'tld', 'currency_code', 'currency_name', \n                       'phone', 'postal_code_format', 'postal_code_regex', \n                       'languages', 'neighbours') \n        self.writer.writerow(self.fields) \n\n    def __call__(self, url, html): \n        if re.search('/view/', url): \n            tree = fromstring(html)\n            all_rows = [\n                tree.xpath(\n                  '//tr[@id=\"places_%s__row\"]/td[@class=\"w2p_fw\"]' % field)[0].text_content()\n                for field in self.fields] \n             self.writer.writerow(all_rows)\n\n```", "```py\n>>> from chp2.advanced_link_crawler import link_crawler\n>>> from chp2.csv_callback import CsvCallback\n>>> link_crawler('http://example.webscraping.com/', '/(index|view)', max_depth=-1, scrape_callback=CsvCallback())\n\n```", "```py\nwswp/\n|-- code/\n|    |-- chp1/\n|    |    + (code files from chp 1)\n|    +-- chp2/\n|         + (code files from chp 2)\n|-- data/\n|    + (generated data files)\n|-- README.md\n+-- .gitignore\n\n```"]