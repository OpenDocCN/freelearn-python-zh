- en: '*Chapter 9*: Concurrent Web Requests'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will focus on concurrently making web requests. Intuitively, making
    requests to a web page to collect information about it is independent of applying
    the same task to another web page. This means that concurrency, specifically threading
    in this case, can be a powerful tool that provides a significant speedup in this
    process. In this chapter, we will learn about the fundamentals of web requests
    and how to interact with websites using Python. We will also learn how concurrency
    can help us make multiple requests efficiently. Finally, we will look at several
    good practices regarding web requests.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this chapter serves as a practical exercise for us to become more comfortable
    with concurrency in Python, which will help you tackle future concurrent programming
    projects with more confidence.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The basics of web requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The requests module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrent web requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The problem with timeouts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good practices in making web requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The basics of web requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The worldwide capacity to generate data is estimated to double in size every
    2 years. Even though there is an interdisciplinary field known as **data science**
    that is entirely dedicated to studying data, almost every programming task in
    software development also has something to do with collecting and analyzing data.
    A significant part of this is, of course, **data collection**. However, the data
    that we need for our applications is sometimes not stored nicely and cleanly in
    a database – sometimes, we need to collect the data we need from web pages.
  prefs: []
  type: TYPE_NORMAL
- en: For example, **web scraping** is a data extraction method that automatically
    makes requests to web pages and downloads specific information. Web scraping allows
    us to comb through numerous websites and collect any data we need systematically
    and consistently. The collected data can be analyzed later by our applications
    or simply saved on our computers in various formats. An example of this would
    be Google, which maintains and runs numerous web scrapers of its own to find and
    index web pages for its search engines.
  prefs: []
  type: TYPE_NORMAL
- en: The Python language itself provides several good options for applications of
    this kind. In this chapter, we will mainly work with the `requests` module to
    make client-side web requests from our Python programs. However, before we look
    into this module in more detail, we need to understand some web terminology to
    be able to effectively design our applications.
  prefs: []
  type: TYPE_NORMAL
- en: HTML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`.html` file extension. In an HTML document, text is surrounded and delimited
    by tags, written in angle brackets; that is, `<p>`, `<img>`, `<i>`, and so on.
    These tags typically consist of pairs – an opening tag and a closing tag – indicating
    the styling or the nature of the data included inside.'
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to include other forms of media in HTML code, such as images
    or videos. Numerous other tags are used in common HTML documents. Some specify
    a group of elements that share some common characteristics, such as `<id></id>`
    and `<class></class>`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of HTML code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Sample HTML code ](img/Figure_9.1_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Sample HTML code
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, detailed knowledge of what each HTML tag accomplishes is not required
    for us to make effective web requests. As we will see later in this chapter, the
    more essential part of making web requests is the ability to interact with web
    pages efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: HTTP requests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a typical communication process on the web, HTML text is the data that is
    to be saved and/or further processed. This kind of data needs to be collected
    from web pages, but how can we go about doing that? Most of the communication
    is done via the internet – more specifically, the **World Wide Web** (**WWW**)
    – and this utilizes the **Hypertext Transfer Protocol** (**HTTP**). In HTTP, request
    methods are used to convey the information of what data is being requested and
    should be sent back from a server.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, when you type `packtpub.com` in your browser, the browser sends
    a request method via HTTP to the Packt website''s main server, asking for data
    from the website. Now, if both your internet connection and Packt''s server are
    working well, then your browser will receive a response from the server, as shown
    in the following diagram. This response will be in the form of an HTML document,
    which will be interpreted by your browser, and your browser will display the corresponding
    HTML output on the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Diagram of HTTP communication ](img/Figure_9.2_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Diagram of HTTP communication
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, request methods are defined as verbs that indicate the desired action
    to be performed while the HTTP client (web browsers) and the server communicate
    with each other: `GET`, `HEAD`, `POST`, `PUT`, `DELETE`, and so on. Of these methods,
    `GET` and `POST` are two of the most common request methods that are used in web
    scraping applications; their functionality is described here:'
  prefs: []
  type: TYPE_NORMAL
- en: The `GET` method requests specific data from the server. This method only retrieves
    data and has no other effect on the server and its databases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `POST` method sends data in a specific form that is accepted by the server.
    This data could be, for example, a message to a bulletin board, mailing list,
    or newsgroup, information to be submitted to a web form, or an item to be added
    to a database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All general-purpose HTTP servers that we commonly see on the internet are required
    to implement at least the `GET` (and `HEAD`) method, while the `POST` method is
    considered optional.
  prefs: []
  type: TYPE_NORMAL
- en: HTTP status code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is not always the case that when a web request is made and sent to a web
    server, the server will process the request and return the requested data without
    fail. Sometimes, the server might be completely down or already busy interacting
    with other clients and therefore unresponsive to a new request; sometimes, the
    client itself makes bad requests to a server (for example, incorrectly formatted
    or malicious requests).
  prefs: []
  type: TYPE_NORMAL
- en: As a way to categorize these problems as well as provide the most information
    possible during the communication resulting from a web request, HTTP requires
    servers to respond to each request from its clients with an **HTTP response status
    code**. A status code is typically a three-digit number that indicates the specific
    characteristics of the response that the server sends back to a client.
  prefs: []
  type: TYPE_NORMAL
- en: 'In total, there are five large categories of HTTP response status codes, indicated
    by the first digit of the code. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1xx (informational status code)**: The request was received, and the server
    is processing it. For example, **100** means that the request header has been
    received and that the server is waiting for the request body; **102** indicates
    that the request is currently being processed (this is used for large requests
    and to prevent clients from timing out).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2xx (successful status code)**: The request was successfully received, understood,
    and processed by the server. For example, **200** means the request was successfully
    fulfilled; **202** indicates that the request has been accepted for processing,
    but the processing itself is not complete.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3xx (redirectional status code)**: Additional actions need to be taken so
    that the request can be successfully processed. For example, **300** means that
    there are multiple options regarding how the response from the server should be
    processed (for example, giving the client multiple video format options when a
    video file is to be downloaded); **301** indicates that the server has been moved
    permanently and all requests should be directed to another address (provided in
    the response from the server).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**4xx (error status code for the client)**: The request was incorrectly formatted
    by the client and could not be processed. For example, **400** means that the
    client sent in a bad request (for example, a syntax error or the size of the request
    is too large); **404** (arguably the most well-known status code) indicates that
    the request method is not supported by the server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**5xx (error status code for the server)**: The request, although valid, could
    not be processed by the server. For example, **500** means that there is an internal
    server error in which an unexpected condition was encountered; **504** (Gateway
    Timeout) means that the server, which was acting as a gateway or a proxy, did
    not receive a response from the final server in time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A lot more can be said about these status codes, but it is already sufficient
    for us to keep the big five categories previously mentioned in mind when making
    web requests from Python. If you would like to find more specific information
    about these or other status codes, the **Internet Assigned Numbers Authority**
    (**IANA**) maintains the official registry of HTTP status codes. Now, let's start
    learning about making web requests in Python.
  prefs: []
  type: TYPE_NORMAL
- en: The requests module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `requests` module allows its users to make and send HTTP request methods.
    In the applications that we will be considering, it is mainly used to make contact
    with the server of the web pages we want to extract data from and obtain the response
    for the server.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: According to the official documentation of the module, the use of Python 3 is
    `requests`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the module on your computer, run one of the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: These commands should install `requests` and any other required dependencies
    (`idna`, `certifi`, `urllib3`, and so on) for you if your system does not have
    those already. After this, run `import requests` in a Python interpreter to confirm
    that the module has been installed successfully. Next, we will use `requests`
    to build the sequential, non-concurrent version of our program.
  prefs: []
  type: TYPE_NORMAL
- en: Making a request in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s look at an example usage of the module, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we are using the `requests` module to download the HTML code
    of a web page; that is, `www.google.com`. The `requests.get()` method sends a
    `GET` request method to `url` and we store the response in the `res` variable.
    After checking the status and headers of the response by printing them out, we
    create a file called `google.html` and write the HTML code, which is stored in
    the response text, to the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the program (assuming that your internet is working and that
    the Google server is not down), you should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The response had a `200` status code, which we know means that the request has
    been completed. The header of the response, which is stored in `res.headers`,
    also contains further specific information regarding the response. For example,
    we can see the date and time the request was made, that the content of the response
    is text and HTML, and that the total length of the content is `4958`.
  prefs: []
  type: TYPE_NORMAL
- en: The data that was sent from the server was also written to the `google.html`
    file. When you open this file in a text editor, you will be able to see the HTML
    code of the web page that we have downloaded using `requests`. On the other hand,
    if you use a web browser to open the file, you will see how **most** of the information
    from the original web page is now being displayed through a downloaded offline
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following is how Google Chrome interprets the HTML file on
    my system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Downloaded HTML opened offline ](img/Figure_9.3_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Downloaded HTML opened offline
  prefs: []
  type: TYPE_NORMAL
- en: There is other information that is stored on the server that the web pages of
    that server refer to. This means that not all of the information that an online
    web page provides can be downloaded via a `GET` request, and this is why offline
    HTML code sometimes fails to contain all of the information available on the online
    web page that it was downloaded from. (For example, the downloaded HTML code in
    the preceding screenshot does not display the Google icon correctly.)
  prefs: []
  type: TYPE_NORMAL
- en: Running a ping test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the basic knowledge of HTTP requests and the `requests` module in Python
    in hand, we will, for the remaining portion of this chapter, tackle the central
    problem of running a **ping test**. A ping test is a procedure in which you test
    the communication between your system and specific web servers, simply by requesting
    each of the servers in question. By considering the HTTP response status code
    (potentially) returned by the server, the test is used to evaluate either the
    internet connection of your system or the availability of the servers.
  prefs: []
  type: TYPE_NORMAL
- en: Ping tests are quite common among web administrators, who usually have to manage
    a large number of websites simultaneously. It is a good tool to quickly identify
    pages that are unexpectedly unresponsive or down. Many tools provide you with
    powerful options regarding ping tests and, in this chapter, we will be designing
    a ping test application that can concurrently send multiple web requests at the
    same time.
  prefs: []
  type: TYPE_NORMAL
- en: To simulate different HTTP response status codes to be sent back to our program,
    we will be using [httpstat.us](http://httpstat.us), a website that can generate
    various status codes and is commonly used to test how applications that make web
    requests can handle varying responses. Specifically, to use a request that will
    return a `200` status code in a program, we can simply send the request [httpstat.us/200](http://httpstat.us/200);
    the same applies to other status codes. In our ping test program, we will have
    a list of [httpstat.us](http://httpstat.us) URLs with different status codes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this program, the `ping()` function takes in a URL and attempts to make a
    `GET` request to the site. Then, it prints out the content of the response returned
    by the server. In our main program, we have a list of different status codes that
    we mentioned earlier, each of which we will go through and call the `ping()` function
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final output, after running the preceding example, should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that our ping test program was able to obtain the corresponding
    responses from the server. However, our current program is purely sequential,
    and we would like to implement a concurrent version of it. We will do this in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent web requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the context of concurrent programming, we can see that the process of making
    a request to a web server and obtaining the returned response is independent of
    the same procedure for a different web server. This is to say that we could apply
    concurrency and parallelism to our ping test application to speed up our execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the concurrent ping test applications that we are designing, multiple HTTP
    requests will be made to the server simultaneously and the corresponding responses
    will be sent back to our program, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Parallel HTTP requests ](img/Figure_9.4_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Parallel HTTP requests
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned previously, concurrency and parallelism have significant applications
    in web development, and most servers nowadays can handle a large number of requests
    at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how we can make multiple web requests at the same time, with
    the help of `threading`.
  prefs: []
  type: TYPE_NORMAL
- en: Spawning multiple threads
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To apply concurrency, we can simply use the `threading` module that we have
    been discussing to create separate threads to handle different web requests. Let''s
    take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we are including the sequential logic from the previous example
    to process our URL list so that we can compare the speed improvement when we apply
    threading to our ping test program. We are also creating a thread to ping each
    of the URLs in our URL list using the `threading` module; these threads will be
    executed independently from each other. The time it takes to process the URLs
    both sequentially and concurrently is also tracked using methods from the `time`
    module.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run the program, your output should be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: While the specific time that the sequential logic and threading logic takes
    to process all the URLs might be different from system to system, there should
    still be a clear distinction between the two. Specifically, here, we can see that
    the threading logic was almost six times faster than the sequential logic (which
    corresponds to the fact that we had six threads processing six URLs in parallel).
    There is no doubt, then, that concurrency can provide a significant speedup for
    our ping test application and for the process of making web requests in general.
  prefs: []
  type: TYPE_NORMAL
- en: Refactoring request logic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The current version of our ping test application works as intended, but we
    can improve its readability by refactoring the logic where we make web requests
    in a thread class. Consider the `MyThread` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, `MyThread` inherits from the `threading.Thread` class and
    contains two additional attributes: `url` and `result`. The `url` attribute holds
    the URL that the thread instance should process; the response that''s returned
    from the web server to that thread will be written to the `result` attribute (in
    the `run()` function).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Outside of this class, we can simply loop through the URL list and create and
    manage the threads accordingly, while not having to worry about the request logic
    in the main program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are now storing the responses in the `result` attribute of the
    `MyThread` class, instead of directly printing them out, as we did in the old
    `ping()` function from the previous examples. This means that, after making sure
    that all the threads have finished, we will need to loop through the threads one
    more time and print out those responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refactoring the request logic should not greatly affect the performance of
    our current program; we are keeping track of the execution speed to see if this
    is the case. If you execute the program, you will obtain an output similar to
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Just as we expected, we are still achieving a significant speedup from the sequential
    version of the program with this refactored request logic. Again, our main program
    is now more readable, and further adjustments to the request logic (as we will
    see in the next section) can simply be directed to the `MyThread` class, without
    affecting the rest of the program.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our program can now make concurrent web requests to specific sites and display
    the returned status code. However, there is a problem common in working with web
    requests that our program cannot handle yet: timeouts. We will learn how to address
    this in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: The problem with timeouts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will explore a potential improvement we can make to our
    ping test application: handling **timeouts**. Timeouts typically occur when the
    server takes an unusually long time to process a specific request, and the connection
    between the server and its client is terminated.'
  prefs: []
  type: TYPE_NORMAL
- en: In the context of a ping test application, we will be implementing a customized
    threshold for the timeout. Recall that a ping test is used to determine whether
    specific servers are still responsive, so we can specify in our program that,
    if a request takes more than our timeout threshold for the server to respond,
    we will categorize that specific server with a timeout.
  prefs: []
  type: TYPE_NORMAL
- en: Support from httpstat.us and simulation in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to different options for status codes, the [httpstat.us](http://httpstat.us)
    website also provides us with a way to simulate a delay in its response when we
    send in requests. Specifically, we can customize the delay time (in milliseconds)
    with a query argument in our `GET` request. For example, [httpstat.us/200?sleep=5000](http://httpstat.us/200?sleep=5000)
    will return a response after a 5-second delay.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see how a delay like this would affect the execution of our program.
    Consider the following program, which contains the current request logic of our
    ping test application but has a different URL list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have a URL that will take around 20 seconds to return a response. Considering
    that we will block the main program until all the threads finish their execution
    (with the `join()` method), our program will most likely appear to be hanging
    for 20 seconds before any response is printed out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the program to experience this for yourself. A 20-second delay will occur
    (which will make the execution take significantly longer to finish) and we will
    obtain the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Let's say that 20 seconds is too long of a response time, and we cannot afford
    to wait for a request that long. So, we would like to implement some logic that
    can handle long waiting times like this.
  prefs: []
  type: TYPE_NORMAL
- en: Timeout specifications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overall, an efficient ping test application should not be waiting for responses
    from its websites for a long time; it should have a set threshold for a timeout
    that, if a server fails to return a response under that threshold, the application
    will deem that server non-responsive. So, we need to implement a way to keep track
    of how much time has passed since a request has been sent to a server. We will
    do this by counting down from the timeout threshold. Once that threshold has been
    passed, all the responses (whether they've returned yet or not) will be printed
    out.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we will also be keeping track of how many requests are still pending
    and have not had their responses returned. We will be using the `is_alive()` method
    from the `threading.Thread` class to indirectly determine whether a response has
    been returned for a specific request. If, at one point, the thread that's processing
    a specific request is alive, we can conclude that that specific request is still
    pending.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following `process_requests()` function first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This function takes in a list of threads that we have been using to make web
    requests in the previous examples, as well as an optional argument specifying
    the timeout threshold. Inside this function, we have an inner function, `alive_count()`,
    which returns the count of the threads that are still alive at the time of the
    function call.
  prefs: []
  type: TYPE_NORMAL
- en: In the `process_requests()` function, so long as there are threads that are
    currently alive and processing requests, we will allow the threads to continue
    with their execution (this is done in the `while` loop with the double condition).
    The `UPDATE_INTERVAL` variable, as you can see, specifies how often we check for
    this condition. If either condition fails (if there are no alive threads left
    or if the threshold timeout is passed), then we will proceed with printing out
    the responses (even if some might have not been returned).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s turn our attention to the new `MyThread` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This class is almost identical to the one we considered in the previous example,
    except that the initial value for the `result` attribute is a message indicating
    a timeout. In the case that we discussed earlier, where the timeout threshold
    specified in the `process_requests()` function is passed, this initial value will
    be used when the responses are printed out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s consider our main program in `example6.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Here, in our URL list, we have a request that would take 4 seconds and another
    that would take 20 seconds, aside from the ones that would respond immediately.
    As the timeout threshold that we are using is 5 seconds, theoretically, we should
    be able to see that the 4-second-delay request will successfully obtain a response,
    while the 20-second-delay one will not.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is another point to be made about this program: `process_requests()`
    function, if the timeout threshold is passed while there is still at least one
    thread being processed, then the function will proceed to print out the `result`
    attribute of each thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This means that we do not block our program until all of the threads have finished
    their execution by using the `join()` function, which means the program can simply
    move forward if the timeout threshold is reached. However, this also means that
    the threads themselves do not terminate at this point. The 20-second-delay request,
    specifically, will still most likely be running after our program exits out of
    the `process_requests()` function.
  prefs: []
  type: TYPE_NORMAL
- en: If the thread that's processing this request is not a daemon thread (as we know,
    daemon threads execute in the background and never terminate), it will block the
    main program from finishing until the thread itself finishes. By making this thread,
    and any other thread, a daemon thread, we allow the main program to finish as
    soon as it executes the last line of its instructions, even if there are threads
    still running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see this program in action. Execute this code; your output should be
    similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it took around 5 seconds for our program to finish this time.
    This is because it spent 5 seconds waiting for the threads that were still running
    and, as soon as the 5-second threshold was passed, the program printed out the
    results. Here, we can see that the result from the 20-second-delay request was
    simply the default value of the `result` attribute of the `MyThread` class, while
    the rest of the requests were able to obtain the correct response from the server
    (including the 4-second-delay request since it had enough time to obtain the response).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you would like to see the effect of non-daemon threads, which we discussed
    earlier, simply comment out the corresponding line of code in our main program,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: You will see that the main program will hang for around 20 seconds as the non-daemon
    thread processing the 20-second-delay request is still running, before being able
    to finish its execution (even though the output that's produced will be identical).
  prefs: []
  type: TYPE_NORMAL
- en: Good practices in making web requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a few aspects of making concurrent web requests that require careful
    consideration and implementation. In this section, we will be going over those
    aspects and some of the best practices that you should use when developing your
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the terms of service and data-collecting policies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unauthorized data collection has been the topic of discussion in the technology
    world for the past few years, and it will continue to be for a long time – and
    for good reasons too. So, it is extremely important for developers who are making
    automated web requests in their applications to look for websites' policies on
    data collecting. You can find these policies in their terms of service or similar
    documents. When in doubt, it is generally a good rule of thumb to contact the
    website directly to ask for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Error handling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Errors are something that no one can easily avoid in the field of programming,
    and this is especially true when making web requests. Errors in these programs
    can include making bad requests (invalid requests or even bad internet connections),
    mishandling downloaded HTML code, or unsuccessfully parsing HTML code. So, it
    is important to make use of `try...except` blocks and other error-handling tools
    in Python to avoid crashing your application. Avoiding crashes is especially important
    if your code/applications are used in production and larger applications.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, in concurrent web scraping, it might be possible for some threads
    to collect data successfully, while others fail. By implementing error-handling
    functionalities in multithreaded parts of your program, you can make sure that
    a failed thread will not be able to crash the entirety of your program and ensure
    that successful threads can still return their results.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is important to note that `try...except` block in our program that
    will catch all errors that occur in the program's execution, and no further information
    regarding the errors can be obtained; this practice is also known as error swallowing.
    It's highly recommended that you have some specific error handling code in a program
    so that not only can appropriate actions be taken with regards to that specific
    error, but other errors that have not been taken into account might also reveal
    themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Update your program regularly
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is quite common for websites to change their request-handling logic, as well
    as their displayed data, regularly. If a program that makes requests to a website
    has considerably inflexible logic to interact with the server of the website (for
    example, structuring its requests in a specific format, only handling one kind
    of response, and so on), then if and when the website alters the way it handles
    its client requests, the program will most likely stop functioning correctly.
    This situation happens frequently with web scraping programs that look for data
    in specific HTML tags; when the HTML tags are changed, these programs will fail
    to find their data.
  prefs: []
  type: TYPE_NORMAL
- en: This practice is implemented to prevent automated data collecting programs from
    functioning. The only way to keep using a website that has recently changed its
    request-handling logic is to analyze the updated protocols and alter our programs
    accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Avoid making a large number of requests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each time one of the programs that we have been discussing runs, it makes HTTP
    requests to a server that manages the site that you'd like to extract data from.
    This process happens significantly more frequently and over a shorter amount of
    time in a concurrent program, where multiple requests are being submitted to that
    server.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned previously, servers nowadays can handle multiple requests simultaneously
    with ease. However, to avoid having to overwork and overconsume resources, servers
    are also designed to stop answering requests that come in too frequently. The
    websites of big tech companies, such as Amazon or Twitter, look for large amounts
    of automated requests that are made from the same IP address and implement different
    response protocols; some requests might be delayed, some might be refused a response,
    or the IP address might even be banned from making further requests for a specific
    amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly, making repeated, heavy-duty requests to servers is a form of
    hacking a website. In **Denial of Service** (**DoS**) and **Distributed Denial
    of Service** (**DDoS**) attacks, a very large number of requests are made at the
    same time to the server, flooding the bandwidth of the targeted server with traffic.
    As a result, normal, non-malicious requests from other clients are denied because
    the servers are busy processing the concurrent requests, as illustrated in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Illustration of a DDoS attack ](img/Figure_9.5_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – Illustration of a DDoS attack
  prefs: []
  type: TYPE_NORMAL
- en: So, it is important to space out the concurrent requests that your application
    makes to a server so that the application will not be considered an attacker and
    be potentially banned or treated as a malicious client. This could be as simple
    as limiting the maximum number of threads/requests that can be implemented at
    a time in your program or pausing the threading for a specific amount of time
    (for example, using the `time.sleep()` function) before sending a request to the
    server.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the basics of HTML and web requests. The two
    most common web requests are `GET` and `POST` requests. There are five main categories
    of HTTP response status codes, each indicating a different concept regarding the
    communication between the server and its client. By considering the status codes
    that are received from different websites, we can write a ping test application
    that effectively checks the responsiveness of those websites.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency can be applied to the problem of making multiple web requests simultaneously
    via threading to provide a significant improvement in application speed. However,
    it is important to keep several considerations in mind when making concurrent
    web requests.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, the exercise we just went through in this chapter will prove useful
    in helping us approach the general problem of converting a sequential program
    into its concurrent version. The simple ping test that we have built could also
    be extended to have more complex behaviors and functionalities. In the next chapter,
    we will consider a similar procedure for the application of image processing.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is HTML?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are HTTP requests?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are HTTP response status codes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the `requests` module help with making web requests?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a ping test and how is one typically designed?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is concurrency applicable in making web requests?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the considerations that need to be made while developing applications
    that make concurrent web requests?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Automate the Boring Stuff with Python: Practical Programming for Total Beginners*,
    Al. Sweigart, No Starch Press, 2015'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Web Scraping with Python*, Richard Lawson, Packt Publishing Ltd, 2015'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Instant Web Scraping with Java*, Ryan Mitchell, Packt Publishing Ltd, 2013'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
