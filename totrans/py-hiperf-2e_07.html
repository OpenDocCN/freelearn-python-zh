<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Parallel Processing</h1>
            </header>

            <article>
                
<p>With parallel processing by using multiple cores, you can increase the amount of calculations your program can do in a given time frame without needing a faster processor. The main idea is to divide a problem into independent subunits and use multiple cores to solve those subunits in parallel.</p>
<p>Parallel processing is necessary to tackle large-scale problems. Companies produce massive quantities of data every day that need to be stored in multiple computers and analyzed. Scientists and engineers run parallel code on supercomputers to simulate massive systems.</p>
<p>Parallel processing allows you to take advantage of multicore CPUs as well as GPUs that work extremely well with highly parallel problems. In this chapter, we will cover the following topics:</p>
<ul>
<li>A brief introduction to the fundamentals of parallel processing</li>
<li>Illustrating how to parallelize simple problems with the <kbd>multiprocessing</kbd> Python library</li>
<li>Using the simple <kbd>ProcessPoolExecutor</kbd> interface</li>
<li>Parallelizing our programs using multithreading with the help of Cython and OpenMP</li>
<li>Achieving parallelism automatically with Theano and Tensorflow</li>
<li>Executing code on a GPU with Theano, Tensorflow, and Numba</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Introduction to parallel programming</h1>
            </header>

            <article>
                
<p>In order to parallelize a program, it is necessary to divide the problem into subunits that can run independently (or almost independently) from each other.</p>
<p>A problem where the subunits are totally independent from each other is called <strong>embarrassingly parallel</strong>. An element-wise operation on an array is a typical example--the operation needs to only know the element it is handling at the moment. Another example is our particle simulator. Since there are no interactions, each particle can evolve independently from the others. Embarrassingly parallel problems are very easy to implement and perform very well on parallel architectures.</p>
<p>Other problems may be divided into subunits but have to share some data to perform their calculations. In those cases, the implementation is less straightforward and can lead to performance issues because of the communication costs.</p>
<p class="CDPAlignLeft CDPAlign">We will illustrate the concept with an example. Imagine that you have a particle simulator, but this time the particles attract other particles within a certain distance (as shown in the following figure). To parallelize this problem, we divide the simulation box into regions and assign each region to a different processor. If we evolve the system one step at a time, some particles will interact with particles in a neighboring region. To perform the next iteration, communication with the new particle positions of the neighboring region is required.</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full image-border" height="254" src="assets/B06440_07CHPNO_01.png" width="254"/></div>
<p>Communication between processes is costly and can seriously hinder the performance of parallel programs. There exist two main ways to handle data communication in parallel programs:</p>
<ul>
<li><strong>Shared memory</strong></li>
<li><strong>Distributed memory</strong></li>
</ul>
<p>In shared memory, the subunits have access to the same memory space. The advantage of this approach is that you don't have to explicitly handle the communication as it is sufficient to write or read from the shared memory. However, problems arise when multiple processes try to access and change the same memory location at the same time. Care should be taken to avoid such conflict using synchronization techniques.</p>
<p>In the distributed memory model, each process is completely separated from the others and possesses its own memory space. In this case, communication is handled explicitly between the processes. The communication overhead is typically costlier compared to shared memory as data can potentially travel through a network interface.</p>
<p>One common way to achieve parallelism with the shared memory model is <strong>threads</strong>. Threads are independent subtasks that originate from a process and share resources, such as memory. This concept is further illustrated in the following figure. Threads produce multiple execution context and share the same memory space, while processes provide multiple execution context that possess their own memory space and communication has to be handled explicitly.</p>
<p><img class="aligncenter size-full image-border" height="210" src="assets/B06440_07CHPNO_02.png" width="539"/></p>
<p>Python can spawn and handle threads, but they can't be used to increase performance; due to the Python interpreter design, only one Python instruction is allowed to run at a time--this mechanism is called <strong>Global Interpreter Lock</strong> (<strong>GIL</strong>). What happens is that each time a thread executes a Python statement, the thread acquires a lock and, when the execution is completed, the same lock is released. Since the lock can be acquired only by one thread at a time, other threads are prevented from executing Python statements while some other thread holds the lock.</p>
<p>Even though the GIL prevents parallel execution of Python instructions, threads can still be used to provide concurrency in situations where the lock can be released, such as in time-consuming I/O operations or in C extensions.</p>
<div class="packt_infobox">Why not remove the GIL? In past years, many attempts have been made, including the most recent gilectomy experiment. First, removing the GIL is not an easy task and requires modification of most of the Python data structures. Additionally, such fine-grained locking can be costly and may introduce substantial performance loss in single-threaded programs. Despite this, some Python implementations (notable examples are Jython and IronPython) do not use the GIL.</div>
<p>The GIL can be completely sidestepped using processes instead of threads. Processes don't share the same memory area and are independent from each other--each process has its own interpreter. Processes have a few disadvantages: starting up a new process is generally slower than starting a new thread, they consume more memory, and inter-process communication can be slow. On the other hand, processes are still very flexible, and they scale better as they can be distributed on multiple machines.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Graphic processing units</h1>
            </header>

            <article>
                
<p>Graphic processing units are special processors designed for computer graphics applications. Those applications usually require processing the geometry of a 3D scene and output an array of pixel to the screen. The operations performed by GPUs involve array and matrix operations on floating point numbers.</p>
<p>GPUs are designed to run this graphics-related operation very efficiently, and they achieve this by adopting a highly parallel architecture. Compared to a CPU, a GPU has many more (thousands) of small processing units. GPUs are intended to produce data at about 60 frames per second, which is much slower than the typical response time of a CPU, which possesses higher clock speeds.</p>
<p>GPUs possess a very different architecture from a standard CPU and are specialized for computing floating point operations. Therefore, to compile programs for GPUs, it is necessary to utilize special programming platforms, such as CUDA and OpenCL.</p>
<p><span><strong>Compute Unified Device Architecture</strong> (<strong>CUDA</strong>) is a proprietary NVIDIA technology. It provides an API that can be accessed from other languages. CUDA provides the NVCC tool that can be used to compile GPU programs written in a language similar to C (CUDA C) as well as numerous libraries that implement highly optimized mathematical routines.</span></p>
<p><strong>OpenCL</strong> is an open technology with the ability of writing parallel programs that can be compiled for a variety of target devices (CPUs and GPUs of several vendors) and is a good option for non-NVIDIA devices.</p>
<p>GPU programming sounds wonderful on paper. However, don't throw away your CPU yet. GPU programming is tricky and only specific use cases benefit from the GPU architecture. Programmers need to be aware of the costs incurred in memory transfers to and from the main memory and how to implement algorithms to take advantage of the GPU architecture.</p>
<p>Generally, GPUs are great at increasing the amount of operations you can perform per unit of time (also called <strong>throughput</strong>); however, they require more time to prepare the data for processing. In contrast, CPUs are much faster at producing an individual result from scratch (also called <strong>latency</strong>).</p>
<p>For the right problem, GPUs provide extreme (10 to 100 times) speedup. For this reason, they often constitute a very inexpensive (the same speedup will require hundreds of CPUs) solution to improve the performance of numerically intensive applications. We will illustrate how to execute some algorithms on a GPU in the <em>Automatic Parallelism</em> section.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Using multiple processes</h1>
            </header>

            <article>
                
<p>The standard <kbd>multiprocessing</kbd> module can be used to quickly parallelize simple tasks by spawning several processes, while avoiding the GIL problem. Its interface is easy to use and includes several utilities to handle task submission and synchronization.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">The Process and Pool classes</h1>
            </header>

            <article>
                
<p>You can create a process that runs independently by subclassing <kbd>multiprocessing.Process</kbd>. You can extend the <kbd>__init__</kbd> method to initialize resources, and you can write the portion of the code that will be executed in a subprocess by implementing the <kbd>Process.run</kbd> method. In the following code, we define a <kbd>Process</kbd> class that will wait for one second and print its assigned <kbd>id</kbd>:</p>
<pre>
    import multiprocessing <br/>    import time <br/><br/>    class Process(multiprocessing.Process): <br/>        def __init__(self, id): <br/>            super(Process, self).__init__() <br/>            self.id = id <br/><br/>        def run(self): <br/>            time.sleep(1) <br/>            print("I'm the process with id: {}".format(self.id))
</pre>
<p>To spawn the process, we have to instantiate the <kbd>Process</kbd> class and call the <kbd>Process.start</kbd> method. Note that you don't directly call <kbd>Process.run</kbd>; the call to <kbd>Process.start</kbd> will create a new process that, in turn, will call the <kbd>Process.run</kbd> method. We can add the following lines at the end of the preceding snippet to create and start the new process:</p>
<pre>
    if __name__ == '__main__': <br/>        p = Process(0) <br/>        p.start()
</pre>
<p>The instructions after <kbd>Process.start</kbd> will be executed immediately without waiting for the <kbd>p</kbd> process to finish. To wait for the task completion, you can use the <kbd>Process.join</kbd> method, as follows:</p>
<pre>
    if __name__ == '__main__': <br/>       p = Process(0) <br/>       p.start() <br/>       p.join()
</pre>
<p>We can launch four different processes that will run parallely in the same way. In a serial program, the total required time will be four seconds. Since the execution is concurrent, the resulting wallclock time will be of one second. In the following code, we create four processes that will execute concurrently:</p>
<pre>
    if __name__ == '__main__': <br/>        processes = Process(1), Process(2), Process(3), Process(4) <br/>        [p.start() for p in processes]
</pre>
<p>Note that the order of the execution for parallel processes is unpredictable and ultimately depends on how the OS schedules their execution. You can verify this behavior by executing the program multiple times; the order will likely be different between runs.</p>
<p>The <kbd>multiprocessing</kbd> module exposes a convenient interface that makes it easy to assign and distribute tasks to a set of processes that reside in the <kbd>multiprocessing.Pool</kbd> class.</p>
<p>The <kbd>multiprocessing.Pool</kbd> class spawns a set of processes--called <strong>workers</strong>--and lets us submit tasks through the <kbd>apply</kbd>/<kbd>apply_async</kbd> and <kbd>map</kbd>/<kbd>map_async</kbd> methods.</p>
<p>The <kbd>Pool.map</kbd> method applies a function to each element of a list and returns the list of results. Its usage is equivalent to the built-in (serial) <kbd>map</kbd>.</p>
<p>To use a parallel map, you should first initialize a <kbd>multiprocessing.Pool</kbd> object. It takes the number of workers as its first argument; if not provided, that number will be equal to the number of cores in the system. You can initialize a <kbd>multiprocessing.Pool</kbd> object in the following way:</p>
<pre>
    pool = multiprocessing.Pool() <br/>    pool = multiprocessing.Pool(processes=4)
</pre>
<p>Let's see <kbd>pool.map</kbd> in action. If you have a function that computes the square of a number, you can map the function to the list by calling <kbd>Pool.map</kbd> and passing the function and the list of inputs as arguments, as follows:</p>
<pre>
    def square(x): <br/>        return x * x <br/><br/>    inputs = [0, 1, 2, 3, 4] <br/>    outputs = pool.map(square, inputs)
</pre>
<p>The <kbd>Pool.map_async</kbd> function is just like <kbd>Pool.map</kbd> but returns an <kbd>AsyncResult</kbd> object instead of the actual result. When we call  <kbd>Pool.map</kbd>, the execution of the main program is stopped until all the workers are finished processing the result. With <kbd>map_async</kbd>, the <kbd>AsyncResult</kbd> object is returned immediately without blocking the main program and the calculations are done in the background. We can then retrieve the result using the <kbd>AsyncResult.get</kbd> method at any time, as shown in the following lines:</p>
<pre>
    outputs_async = pool.map_async(square, inputs) <br/>    outputs = outputs_async.get()
</pre>
<p><kbd>Pool.apply_async</kbd> assigns a task consisting of a single function to one of the workers. It takes the function and its arguments and returns an <kbd>AsyncResult</kbd> object. We can obtain an effect similar to <kbd>map</kbd> using <kbd>apply_async</kbd>, as shown:</p>
<pre>
    results_async = [pool.apply_async(square, i) for i in range(100))] <br/>    results = [r.get() for r in results_async]
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">The Executor interface</h1>
            </header>

            <article>
                
<p>From version 3.2 onward, it is possible to execute Python code in parallel using the <kbd>Executor</kbd> interface provided in the <kbd>concurrent.futures</kbd> module. We already saw the <kbd>Executor</kbd> interface in action in the previous chapter, when we used <kbd>ThreadPoolExecutor</kbd> to perform multiple tasks concurrently. In this subsection, we'll demonstrate the usage of the <kbd>ProcessPoolExecutor</kbd> class.</p>
<p><kbd>ProcessPoolExecutor</kbd> exposes a very lean interface, at least when compared to the more featureful <kbd>multiprocessing.Pool</kbd>. A <kbd>ProcessPoolExecutor</kbd> can be instantiated, similar to <kbd>ThreadPoolExecutor</kbd>, by passing a number of worker threads using the <kbd>max_workers</kbd> argument (by default, <kbd>max_workers</kbd> will be the number of CPU cores available). The main methods available to the <kbd>ProcessPoolExecutor</kbd> are <kbd>submit</kbd> and <kbd>map</kbd>.</p>
<p>The <kbd>submit</kbd> method will take a function and return a <kbd>Future</kbd> (see the last chapter) that will keep track of the execution of the submitted function. The method map works similarly to the <kbd>Pool.map</kbd> function, except that it returns an iterator rather than a list:</p>
<pre>
    from concurrent.futures import ProcessPoolExecutor<br/><br/>    executor = ProcessPoolExecutor(max_workers=4)<br/>    fut = executor.submit(square, 2)<br/>    # Result:<br/>    # &lt;Future at 0x7f5b5c030940 state=running&gt;<br/><br/>    result = executor.map(square, [0, 1, 2, 3, 4])<br/>    list(result)<br/>    # Result:<br/>    # [0, 1, 4, 9, 16]
</pre>
<p>To extract the result from one or more <kbd>Future</kbd> instances, you can use the <kbd>concurrent.futures.wait</kbd> and <kbd>concurrent.futures.as_completed</kbd> functions. The <kbd>wait</kbd> function accepts a list of <kbd>future</kbd> and will block the execution of the programs until all the futures have completed their execution. The result can then be extracted using the <kbd>Future.result</kbd> method. The <kbd>as_completed</kbd> function also accepts a function but will, instead, return an iterator over the results:</p>
<pre>
    from concurrent.futures import wait, as_completed<br/><br/>    fut1 = executor.submit(square, 2)<br/>    fut2 = executor.submit(square, 3)<br/>    wait([fut1, fut2])<br/>    # Then you can extract the results using fut1.result() and fut2.result()<br/><br/>    results = as_completed([fut1, fut2])<br/>    list(results)<br/>    # Result:<br/>    # [4, 9]
</pre>
<p>Alternatively, you can generate futures using the <kbd>asyncio.run_in_executor</kbd> function and manipulate the results using all the tools and syntax provided by the <kbd>asyncio</kbd> libraries so that you can achieve concurrency and parallelism at the same time.  </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Monte Carlo approximation of pi</h1>
            </header>

            <article>
                
<p>As an example, we will implement a canonical, embarrassingly parallel program--the <strong>Monte Carlo approximation of pi</strong>. Imagine that we have a square of size 2 units; its area will be 4 units. Now, we inscribe a circle of 1 unit radius in this square; the area of the circle will be <em>pi * r^2</em>. By substituting the value of <em>r</em> in the previous equation, we get that the numerical value for the area of the circle is <em>pi * (1)^2 = pi</em>. You can refer to the following figure for a graphical representation.</p>
<p>If we shoot a lot of random points on this figure, some points will fall into the circle, which we'll call <strong>hits, </strong>while the remaining points, <strong>misses, </strong>will be outside the circle. The area of the circle will be proportional to the number of hits, while the area of the square will be proportional to the total number of shots. To get the value of <em>pi</em>, it is sufficient to divide the area of the circle (equal to <em>pi</em>) by the area of the square (equal to 4):</p>
<pre>
    hits/total = area_circle/area_square = pi/4 <br/>    pi = 4 * hits/total
</pre>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full image-border" height="193" src="assets/image_07_003.png" width="207"/></div>
<p>The strategy we will employ in our program will be as follows:</p>
<ul>
<li>Generate a lot of uniformly random (<em>x</em>, <em>y</em>) numbers in the range (<strong>-1</strong>, <strong>1</strong>)</li>
<li>Test whether those numbers lie inside the circle by checking whether <em>x**2 + y**2</em> &lt;= <em>1</em></li>
</ul>
<p>The first step when writing a parallel program is to write a serial version and verify that it works. In a real-world scenario, you also want to leave the parallelization as the last step of your optimization process. First, we need to identify the slow parts, and second, parallelization is time-consuming and gives you <em>at most</em> a speedup equal to the number of processors. The implementation of the serial program is as follows:</p>
<pre>
    import random <br/><br/>    samples = 1000000 <br/>    hits = 0 <br/><br/>    for i in range(samples): <br/>        x = random.uniform(-1.0, 1.0) <br/>        y = random.uniform(-1.0, 1.0) <br/><br/>        if x**2 + y**2 &lt;= 1: <br/>            hits += 1 <br/>     <br/>    pi = 4.0 * hits/samples
</pre>
<p>The accuracy of our approximation will improve as we increase the number of samples. You can note that each loop iteration is independent from the other--this problem is embarrassingly parallel.</p>
<p>To parallelize this code, we can write a function, called <kbd>sample</kbd>, that corresponds to a single hit-miss check. If the sample hits the circle, the function will return <kbd>1</kbd>; otherwise, it will return <kbd>0</kbd>. By running <kbd>sample</kbd> multiple times and summing the results, we'll get the total number of hits. We can run <kbd>sample</kbd> over multiple processors with <kbd>apply_async</kbd> and get the results in the following way:</p>
<pre>
    def sample(): <br/>        x = random.uniform(-1.0, 1.0) <br/>        y = random.uniform(-1.0, 1.0) <br/><br/>        if x**2 + y**2 &lt;= 1: <br/>            return 1 <br/>        else: <br/>            return 0 <br/><br/>    pool = multiprocessing.Pool() <br/>    results_async = [pool.apply_async(sample) for i in range(samples)] <br/>    hits = sum(r.get() for r in results_async)
</pre>
<p>We can wrap the two versions in the <kbd>pi_serial</kbd> and <kbd>pi_apply_async</kbd> functions (you can find their implementation in the <kbd>pi.py</kbd> file) and benchmark the execution speed, as follows:</p>
<pre>
<strong>$ time python -c 'import pi; pi.pi_serial()'</strong><br/><strong>real    0m0.734s</strong><br/><strong>user    0m0.731s</strong><br/><strong>sys     0m0.004s</strong><br/><strong>$ time python -c 'import pi; pi.pi_apply_async()'</strong><br/><strong>real    1m36.989s</strong><br/><strong>user    1m55.984s</strong><br/><strong>sys     0m50.386</strong>
</pre>
<p>As shown in the earlier benchmark, our first parallel version literally cripples our code. The reason is that the time spent doing the actual calculation is small compared to the overhead required to send and distribute the tasks to the workers.</p>
<p>To solve the issue, we have to make the overhead negligible compared to the calculation time. For example, we can ask each worker to handle more than one sample at a time, thus reducing the task communication overhead. We can write a <kbd>sample_multiple</kbd> function that processes more than one hit and modifies our parallel version by dividing our problem by 10; more intensive tasks are shown in the following code:</p>
<pre>
    def sample_multiple(samples_partial): <br/>        return sum(sample() for i in range(samples_partial)) <br/><br/>    n_tasks = 10 <br/>    chunk_size = samples/n_tasks <br/>    pool = multiprocessing.Pool() <br/>    results_async = [pool.apply_async(sample_multiple, chunk_size) <br/>                     for i in range(n_tasks)] <br/>    hits = sum(r.get() for r in results_async)
</pre>
<p>We can wrap this in a function called <kbd>pi_apply_async_chunked</kbd> and run it as follows:</p>
<pre>
<strong>$ time python -c 'import pi; pi.pi_apply_async_chunked()'</strong><br/><strong>real    0m0.325s</strong><br/><strong>user    0m0.816s</strong><br/><strong>sys     0m0.008s</strong>
</pre>
<p>The results are much better; we more than doubled the speed of our program. You can also notice that the <kbd>user</kbd> metric is larger than <kbd>real</kbd>; the total CPU time is larger than the total time because more than one CPU worked at the same time. If you increase the number of samples, you will note that the ratio of communication to calculation decreases, giving even better speedups.</p>
<p>Everything is nice and simple when dealing with embarrassingly parallel problems. However, sometimes you have to share data between processes.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Synchronization and locks</h1>
            </header>

            <article>
                
<p>Even if <kbd>multiprocessing</kbd> uses processes (with their own independent memory), it lets you define certain variables and arrays as shared memory. You can define a shared variable using <kbd>multiprocessing.Value</kbd>, passing its data type as a string (<kbd>i</kbd> integer, <kbd>d</kbd> double, <kbd>f</kbd> float, and so on). You can update the content of the variable through the <kbd>value</kbd> attribute, as shown in the following code snippet:</p>
<pre>
    shared_variable = multiprocessing.Value('f') <br/>    shared_variable.value = 0
</pre>
<p>When using shared memory, you should be aware of concurrent accesses. Imagine that you have a shared integer variable and each process increments its value multiple times. You will define a process class as follows:</p>
<pre>
    class Process(multiprocessing.Process): <br/><br/>        def __init__(self, counter): <br/>            super(Process, self).__init__() <br/>            self.counter = counter <br/><br/>        def run(self): <br/>            for i in range(1000): <br/>                self.counter.value += 1
</pre>
<p>You can initialize the shared variable in the main program and pass it to <kbd>4</kbd> processes, as shown in the following code:</p>
<pre>
    def main(): <br/>        counter = multiprocessing.Value('i', lock=True) <br/>        counter.value = 0 <br/><br/>        processes = [Process(counter) for i in range(4)] <br/>        [p.start() for p in processes] <br/>        [p.join() for p in processes] # processes are done <br/>        print(counter.value)
</pre>
<p>If you run this program (<kbd>shared.py</kbd> in the code directory), you will note that the final value of <kbd>counter</kbd> is not 4000, but it has random values (on my machine, they are between 2000 and 2500). If we assume that the arithmetic is correct, we can conclude that there's a problem with the parallelization.</p>
<p>What happens is that multiple processes are trying to access the same shared variable at the same time. The situation is best explained by looking at the following figure. In a serial execution, the first process reads (the number <kbd>0</kbd>), increments it, and writes the new value (<kbd>1</kbd>); the second process reads the new value (<kbd>1</kbd>), increments it, and writes it again (<kbd>2</kbd>).</p>
<p>In the parallel execution, the two processes read (<kbd>0</kbd>), increment it, and write the value (<kbd>1</kbd>) at the same time, leading to a wrong answer:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full image-border" height="195" src="assets/B06440_07CHPNO_04.png" width="388"/></div>
<p class="CDPAlignLeft CDPAlign"><br/>
To solve this problem, we need to synchronize the access to this variable so that only one process at a time can access, increment, and write the value on the shared variable. This feature is provided by the <kbd>multiprocessing.Lock</kbd> class. A lock can be acquired and released through the <kbd>acquire</kbd> method and <kbd>release</kbd>, or using the lock as a context manager. Since the lock can be acquired by only one process at a time, this method prevents multiple processes from executing the protected section of code at the same time.</p>
<p>We can define a global <span>lock</span> and use it as a context manager to restrict the access to the counter, as shown in the following code snippet:</p>
<pre>
    lock = multiprocessing.Lock() <br/><br/>    class Process(multiprocessing.Process): <br/><br/>        def __init__(self, counter): <br/>            super(Process, self).__init__() <br/>            self.counter = counter <br/><br/>        def run(self): <br/>            for i in range(1000): <br/>                with lock: # acquire the lock <br/>                    self.counter.value += 1 <br/>                # release the lock
</pre>
<p>Synchronization primitives, such as locks, are essential to solve many problems, but they should be kept to a minimum to improve the performance of your program.</p>
<div class="packt_infobox">The <kbd>multiprocessing</kbd> module includes other communication and synchronization tools; you can refer to the official documentation at <span class="URLPACKT"><a href="http://docs.python.org/3/library/multiprocessing.html">http://docs.python.org/3/library/multiprocessing.html</a> for a complete reference.</span></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Parallel Cython with OpenMP</h1>
            </header>

            <article>
                
<p>Cython provides a convenient interface to perform shared-memory parallel processing through <strong>OpenMP</strong>. This lets you write extremely efficient parallel code directly in Cython without having to create a C wrapper.</p>
<p>OpenMP is a specification and an API designed to write multithreaded, parallel programs. The OpenMP specification includes a series of C preprocessor directives to manage threads and provides communication patterns, load balancing, and other synchronization features. Several C/C++ and Fortran compilers (including GCC) implement the OpenMP API.</p>
<p>We can introduce the Cython parallel features with a small example. Cython provides a simple API based on OpenMP in the <kbd>cython.parallel</kbd> module. The simplest way to achieve parallelism is through <kbd>prange</kbd>, which is a construct that automatically distributes loop operations in multiple threads.</p>
<p>First of all, we can write the serial version of a program that computes the square of each element of a NumPy array in the <kbd>hello_parallel.pyx</kbd> file. We define a function, <kbd>square_serial</kbd>, that takes a buffer as input and populates an output array with the squares of the input array elements; <kbd>square_serial</kbd> is shown in the following code snippet:</p>
<pre>
    import numpy as np <br/><br/>    def square_serial(double[:] inp): <br/>        cdef int i, size <br/>        cdef double[:] out <br/>        size = inp.shape[0] <br/>        out_np = np.empty(size, 'double') <br/>        out = out_np <br/><br/>        for i in range(size): <br/>            out[i] = inp[i]*inp[i] <br/><br/>        return out_np
</pre>
<p>Implementing a parallel version of the loop over the array elements involves substituting the <kbd>range</kbd> call with <kbd>prange</kbd>. There's a caveat--to use <kbd>prange</kbd>, it is necessary that the body of the loop is interpreter-free. As already explained, we need to release the GIL and, since interpreter calls generally acquire the GIL, they need to be avoided to make use of threads.</p>
<p>In Cython, you can release the GIL using the <kbd>nogil</kbd> context, as follows:</p>
<pre>
    with nogil: <br/>        for i in prange(size): <br/>            out[i] = inp[i]*inp[i]
</pre>
<p>Alternatively, you can use the option <kbd>nogil=True</kbd> of <kbd>prange</kbd> that will automatically wrap the loop body in a <kbd>nogil</kbd> block:</p>
<pre>
    for i in prange(size, nogil=True): <br/>        out[i] = inp[i]*inp[i]
</pre>
<p>Attempts to call Python code in a <kbd>prange</kbd> block will produce an error. Prohibited operations include function calls, objects initialization, and so on. To enable such operations in a <kbd>prange</kbd> block (you may want to do so for debugging purposes), you have to re-enable the GIL using the <kbd>with gil</kbd> statement:</p>
<pre>
    for i in prange(size, nogil=True): <br/>        out[i] = inp[i]*inp[i] <br/>        with gil:   <br/>            x = 0 # Python assignment
</pre>
<p>We can now test our code by compiling it as a Python extension module. To enable OpenMP support, it is necessary to change the <kbd>setup.py</kbd> file so that it includes the compilation option <kbd>-fopenmp</kbd> . This can be achieved by using the <kbd>distutils.extension.Extension</kbd> class in <kbd>distutils</kbd> and passing it to <kbd>cythonize</kbd>. The complete <kbd>setup.py</kbd> file is as follows:</p>
<pre>
    from distutils.core import setup <br/>    from distutils.extension import Extension <br/>    from Cython.Build import cythonize <br/><br/>    hello_parallel = Extension('hello_parallel', <br/>                               ['hello_parallel.pyx'], <br/>                                extra_compile_args=['-fopenmp'], <br/>                                extra_link_args=['-fopenmp']) <br/><br/>    setup( <br/>       name='Hello', <br/>       ext_modules = cythonize(['cevolve.pyx', hello_parallel]), <br/>    )
</pre>
<p>Using <kbd>prange</kbd>, we can easily parallelize the Cython version of our <kbd>ParticleSimulator</kbd>. The following code contains the <kbd>c_evolve</kbd> function of the <kbd>cevolve.pyx</kbd> Cython module that was written in <a href="ce893a62-a46c-4575-8163-01921cf8bb7b.xhtml">Chapter 4</a><span class="ChapterrefPACKT">, <em>C Performance with Cython</em></span>:</p>
<pre>
    def c_evolve(double[:, :] r_i,double[:] ang_speed_i, <br/>                 double timestep,int nsteps): <br/><br/>        # cdef declarations <br/><br/>        for i in range(nsteps): <br/>            for j in range(nparticles): <br/>                # loop body
</pre>
<p>First, we will invert the order of the loops so that the outermost loop will be executed in parallel (each iteration is independent from the other). Since the particles don't interact with each other, we can change the order of iteration safely, as shown in the following snippet:</p>
<pre>
        for j in range(nparticles): <br/>            for i in range(nsteps): <br/><br/>                # loop body
</pre>
<p>Next, we will replace the <kbd>range</kbd> call of the outer loop with  <kbd>prange</kbd> and remove calls that acquire the GIL. Since our code was already enhanced with static types, the <kbd>nogil</kbd> option can be applied safely as follows:</p>
<pre>
    for j in prange(nparticles, nogil=True)
</pre>
<p>We can now compare the functions by wrapping them in the benchmark function to assess any performance improvement:</p>
<pre>
    In [3]: %timeit benchmark(10000, 'openmp') # Running on 4 processors<br/>    1 loops, best of 3: 599 ms per loop <br/>    In [4]: %timeit benchmark(10000, 'cython') <br/>    1 loops, best of 3: 1.35 s per loop
</pre>
<p>Interestingly, we achieved a 2x speedup by writing a parallel version using <kbd>prange</kbd>.  </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Automatic parallelism</h1>
            </header>

            <article>
                
<p>As we mentioned earlier, normal Python programs have trouble achieving thread parallelism because of the GIL. So far, we worked around this problem using separate processes; starting a process, however, takes significantly more time and memory than starting a thread.</p>
<p>We also saw that sidestepping the Python environment allowed us to achieve a 2x speedup on an already fast Cython code. This strategy allowed us to achieve lightweight parallelism but required a separate compilation step. In this section, we will further explore this strategy using special libraries that are capable of automatically translating our code into a parallel version for efficient execution.</p>
<p>Examples of packages that implement automatic parallelism are the (by now) familiar JIT compilers  <kbd>numexpr</kbd> and Numba. Other packages have been developed to automatically optimize and parallelize array and matrix-intensive expressions, which are crucial in specific numerical and machine learning applications.</p>
<p><strong>Theano</strong> is a project that allows you to define a mathematical expression on arrays (more generally, <em>tensors</em>), and compile them to a fast language, such as C or C++. Many of the operations that Theano implements are parallelizable and can run on both CPU and GPU.</p>
<p><strong>Tensorflow</strong> is another library that, similar to Theano, is targeted towards expression of array-intensive mathematical expression but, rather than translating the expressions to specialized C code, executes the operations on an efficient C++ engine.</p>
<p>Both Theano and Tensorflow are ideal when the problem at hand can be expressed in a chain of matrix and element-wise operations (such as <em>neural networks</em>).</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Getting started with Theano</h1>
            </header>

            <article>
                
<p>Theano is somewhat similar to a compiler but with the added bonuses of being able to express, manipulate, and optimize mathematical expressions as well as run code on CPU and GPU. Since 2010, Theano has improved release after release and has been adopted by several other Python projects as a way to automatically generate efficient computational models on the fly.</p>
<p>In Theano, you first <em>define</em> the function you want to run by specifying variables and transformation using a pure Python API. This specification will then be compiled to machine code for execution.</p>
<p>As a first example, let's examine how to implement a function that computes the square of a number. The input will be represented by a scalar variable, <kbd>a</kbd>, and then we will transform it to obtain its square, indicated by <kbd>a_sq</kbd>. In the following code, we will use the <kbd>T.scalar</kbd> function to define the variable and use the normal <kbd>**</kbd> operator to obtain a new variable:</p>
<pre>
    import theano.tensor as T<br/>    import theano as th<br/>    a = T.scalar('a')<br/>    a_sq = a ** 2<br/>    print(a_sq)<br/>    # Output:<br/>    # Elemwise{pow,no_inplace}.0
</pre>
<p>As you can see, no specific value is computed and the transformation we apply is purely symbolic. In order to use this transformation, we need to generate a function. To compile a function, you can use the <kbd>th.function</kbd> utility that takes a list of the input variables as its first argument, and the output transformation (in our case <kbd>a_sq</kbd>) as its second argument:</p>
<pre>
    compute_square = th.function([a], a_sq)
</pre>
<p>Theano will take some time and translate the expression to efficient C code and compile it, all in the background! The return value of <kbd>th.function</kbd> will be a ready-to-use Python function and its usage is demonstrated in the next line of code:</p>
<pre>
    compute_square(2)<br/>    4.0
</pre>
<p>Unsurprisingly, <kbd>compute_square</kbd> correctly returns the input value squared. Note, however, that the return type is not an integer (like the input type) but a floating point number. This is because the Theano default variable type is <kbd>float64</kbd>. you can verify that by inspecting the <kbd>dtype</kbd> attribute of the <kbd>a</kbd> variable:</p>
<pre>
    a.dtype<br/>    # Result: <br/>    # float64
</pre>
<p>The Theano behavior is very different compared to what we saw with Numba. Theano doesn't compile generic Python code and, also, doesn't do any type inference; defining Theano functions requires a more precise specification of the types involved.</p>
<p>The real power of Theano comes from its support for array expressions. Defining a one-dimensional vector can be done with the <kbd>T.vector</kbd> function; the returned variable supports broadcasting operations with the same semantics of NumPy arrays. For instance, we can take two vectors and compute the element-wise sum of their squares, as follows:</p>
<pre>
    a = T.vector('a')<br/>    b = T.vector('b')<br/>    ab_sq = a**2 + b**2<br/>    compute_square = th.function([a, b], ab_sq)<br/><br/>    compute_square([0, 1, 2], [3, 4, 5])<br/>    # Result:<br/>    # array([  9.,  17.,  29.])
</pre>
<p>The idea is, again, to use the Theano API as a mini-language to combine various Numpy array expressions will be compiled to efficient machine code.</p>
<div class="packt_infobox">One of the selling points of Theano is its ability to perform arithmetic simplifications and automatic gradient calculations. For more information, refer to the official documentation (<a href="http://deeplearning.net/software/theano/introduction.html">http://deeplearning.net/software/theano/introduction.html</a>).</div>
<p>To demonstrate Theano functionality on a familiar use case, we can implement our parallel calculation of pi again. Our function will take a collection of two random coordinates as input and return the <kbd>pi</kbd> estimate. The input random numbers will be defined as vectors named <kbd>x</kbd> and <kbd>y</kbd>, and we can test their position inside the circle using standard element-wise operation that we will store in the <kbd>hit_test</kbd> variable:</p>
<pre>
    x = T.vector('x')<br/>    y = T.vector('y')<br/><br/>    hit_test = x ** 2 + y ** 2 &lt; 1
</pre>
<p>At this point, we need to count the number of <kbd>True</kbd> elements in <kbd>hit_test</kbd>, which can be done taking its sum (it will be implicitly cast to integer).  To obtain the pi estimate, we finally need to calculate the ratio of hits versus the total number of trials. The calculation is illustrated in the following code block:</p>
<pre>
    hits = hit_test.sum()<br/>    total = x.shape[0]<br/>    pi_est = 4 * hits/total
</pre>
<p>We can benchmark the execution of the Theano implementation using <kbd>th.function</kbd> and the <kbd>timeit</kbd> module. In our test, we will pass two arrays of size 30,000 and use the <kbd>timeit.timeit</kbd> utility to execute the <kbd>calculate_pi</kbd> function multiple times:</p>
<pre>
    calculate_pi = th.function([x, y], pi_est)<br/><br/>    x_val = np.random.uniform(-1, 1, 30000)<br/>    y_val = np.random.uniform(-1, 1, 30000)<br/><br/>    import timeit<br/>    res = timeit.timeit("calculate_pi(x_val, y_val)", <br/>    "from __main__ import x_val, y_val, calculate_pi", number=100000)<br/>    print(res)<br/>    # Output:<br/>    # 10.905971487998613
</pre>
<p>The serial execution of this function takes about 10 seconds. Theano is capable of automatically parallelizing the code by implementing element-wise and matrix operations using specialized packages, such as OpenMP and the <strong>Basic Linear Algebra Subprograms</strong> (<strong>BLAS</strong>) linear algebra routines. Parallel execution can be enabled using configuration options.</p>
<p>In Theano, you can set up configuration options by modifying variables in the <kbd>theano.config</kbd> object at import time. For example, you can issue the following commands to enable OpenMP support:</p>
<pre>
<strong>import theano<br/>theano.config.openmp = True<br/>theano.config.openmp_elemwise_minsize = 10</strong>
</pre>
<p>The parameters relevant to OpenMP are as follows:</p>
<ul>
<li><kbd>openmp_elemwise_minsize</kbd>: This is an integer number that represents the minimum size of the arrays where element-wise parallelization should be enabled (the overhead of the parallelization can harm performance for small arrays)</li>
<li><kbd>openmp</kbd>: This is a Boolean flag that controls the activation of OpenMP compilation (it should be activated by default)</li>
</ul>
<p>Controlling the number of threads assigned for OpenMP execution can be done by setting the <kbd>OMP_NUM_THREADS</kbd> environmental variable before executing the code.</p>
<p>We can now write a simple benchmark to demonstrate the OpenMP usage in practice. In a file <kbd>test_theano.py</kbd>, we will put the complete code for the pi estimation example:</p>
<pre>
    # File: test_theano.py<br/>    import numpy as np<br/>    import theano.tensor as T<br/>    import theano as th<br/>    th.config.openmp_elemwise_minsize = 1000<br/>    th.config.openmp = True<br/><br/>    x = T.vector('x')<br/>    y = T.vector('y')<br/><br/>    hit_test = x ** 2 + y ** 2 &lt;= 1<br/>    hits = hit_test.sum()<br/>    misses = x.shape[0]<br/>    pi_est = 4 * hits/misses<br/><br/>    calculate_pi = th.function([x, y], pi_est)<br/><br/>    x_val = np.random.uniform(-1, 1, 30000)<br/>    y_val = np.random.uniform(-1, 1, 30000)<br/><br/>    import timeit<br/>    res = timeit.timeit("calculate_pi(x_val, y_val)", <br/>                        "from __main__ import x_val, y_val, <br/>                        calculate_pi", number=100000)<br/>    print(res)
</pre>
<p>At this point, we can run the code from the command line and assess the scaling with an increasing number of threads by setting the <kbd>OMP_NUM_THREADS</kbd> environment variable:</p>
<pre>
    $ <strong>OMP_NUM_THREADS=1</strong> python test_theano.py<br/>    10.905971487998613<br/>    $ <strong>OMP_NUM_THREADS=2</strong> python test_theano.py<br/>    7.538279129999864<br/>    $ <strong>OMP_NUM_THREADS=3</strong> python test_theano.py<br/>    9.405846934998408<br/>    $ <strong>OMP_NUM_THREADS=4</strong> python test_theano.py<br/>    14.634153957000308
</pre>
<p>Interestingly, there is a small speedup when using two threads, but the performance degrades quickly as we increase their number. This means that for this input size, it is not advantageous to use more than two threads as the price you pay to start new threads and synchronize their shared data is higher than the speedup that you can obtain from the parallel execution. </p>
<p>Achieving good parallel performance can be tricky as it will depend on the specific operations and how they access the underlying data. As a general rule, measuring the performance of a parallel program is crucial and obtaining substantial speedups is a work of trial and error.</p>
<p>As an example, we can see that the parallel performance quickly degrades using a slightly different code. In our hit test, we used the <kbd>sum</kbd> method directly and relied on the explicit casting of the <kbd>hit_tests</kbd> Boolean array. If we make the cast explicit, Theano will generate a slightly different code that benefits less from multiple threads. We can modify the <kbd>test_theano.py</kbd> file to verify this effect:</p>
<pre>
    # Older version<br/>    # hits = hit_test.sum()<br/>    hits = <strong>hit_test.astype('int32').sum()</strong>
</pre>
<p>If we rerun our benchmark, we see that the number of threads does not affect the running time significantly. Despite that, the timings improved considerably as compared to the original version:</p>
<pre>
    $ <strong>OMP_NUM_THREADS=1</strong> python test_theano.py<br/>    5.822126664999814<br/>    $ <strong>OMP_NUM_THREADS=2</strong> python test_theano.py<br/>    5.697357518001809<br/>    $ <strong>OMP_NUM_THREADS=3</strong> python test_theano.py <br/>    5.636914656002773<br/>    $ <strong>OMP_NUM_THREADS=4</strong> python test_theano.py<br/>    5.764030176000233
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Profiling Theano</h1>
            </header>

            <article>
                
<p>Given the importance of measuring and analyzing performance, Theano provides powerful and informative profiling tools. To generate profiling data, the only modification needed is the addition of the <kbd>profile=True</kbd> option to <kbd>th.function</kbd>:</p>
<pre>
    calculate_pi = th.function([x, y], pi_est, <strong>profile=True</strong>)
</pre>
<p>The profiler will collect data as the function is being run (for example, through <kbd>timeit</kbd> or direct invocation). The profiling summary can be printed to output by issuing the <kbd>summary</kbd> command, as follows:</p>
<pre>
    calculate_pi.profile.summary()
</pre>
<p>To generate profiling data, we can rerun our script after adding the <kbd>profile=True</kbd> option (for this experiment, we will set the <kbd>OMP_NUM_THREADS</kbd> environmental variable to 1). Also, we will revert our script to the version that performed the casting of <kbd>hit_tests</kbd> implicitly.</p>
<div class="packt_tip">You can also set up profiling globally using the <kbd>config.profile</kbd> option.</div>
<p>The output printed by <kbd>calculate_pi.profile.summary()</kbd> is quite long and informative. A part of it is reported in the next block of text. The output is comprised of three sections that refer to timings sorted by <kbd>Class</kbd>, <kbd>Ops</kbd>, and <kbd>Apply</kbd>. In our example, we are concerned with <kbd>Ops</kbd>, which roughly maps to the functions used in the Theano compiled code. As you can see, roughly 80% of the time is spent in taking the element-wise square and sum of the two numbers, while the rest of the time is spent calculating the sum:</p>
<pre>
Function profiling<br/>==================<br/>  Message: test_theano.py:15<br/><br/>... other output<br/>Time in 100000 calls to Function.__call__: 1.015549e+01s<br/>... other output<br/><br/>Class<br/>---<br/>&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; &lt;#apply&gt; &lt;Class name&gt;<br/>.... timing info by class<br/><br/><strong>Ops<br/>---</strong><br/>&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; &lt;#apply&gt; &lt;Op name&gt;<br/><strong>  80.0%    80.0%       6.722s       6.72e-05s     C     100000        1   Elemwise{Composite{LT((sqr(i0) + sqr(i1)), i2)}}</strong><br/>  19.4%    99.4%       1.634s       1.63e-05s     C     100000        1   Sum{acc_dtype=int64}<br/>   0.3%    99.8%       0.027s       2.66e-07s     C     100000        1   Elemwise{Composite{((i0 * i1) / i2)}}<br/>   0.2%   100.0%       0.020s       2.03e-07s     C     100000        1   Shape_i{0}<br/>   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)<br/><br/>Apply<br/>------<br/>&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;#call&gt; &lt;id&gt; &lt;Apply name&gt;<br/>... timing info by apply
</pre>
<p>This information is consistent with what was found in our first benchmark. The code went from about 11 seconds to roughly 8 seconds when two threads were used. From these numbers, we can analyze how the time was spent.</p>
<p>Out of these 11 seconds, 80% of the time (about 8.8 seconds) was spent doing element-wise operations. This means that, in perfectly parallel conditions, the increase in speed by adding two threads will be 4.4 seconds. In this scenario, the theoretical execution time will be 6.6 seconds. Considering that we obtained a timing of about 8 seconds, it looks like there is some extra overhead (1.4 seconds) for the thread usage.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Tensorflow</h1>
            </header>

            <article>
                
<p>Tensorflow is another library designed for fast numerical calculations and automatic parallelism. It was released as an open source project by Google in 2015. Tensorflow works by building mathematical expressions similar to Theano, except that the computation is not compiled to machine code but is executed on an external engine written in C++. Tensorflow supports execution and deployment of parallel codes on one or more CPUs and GPUs.</p>
<p>The usage of Tensorflow is quite similar to that of Theano. To create a variable in Tensorflow, you can use the <kbd>tf.placeholder</kbd> function that takes a data type as input:</p>
<pre>
    import tensorflow as tf<br/><br/>    a = tf.placeholder('float64')
</pre>
<p>Tensorflow mathematical expressions can be expressed quite similarly to Theano, except for a few different naming conventions as well as a more restricted support for the NumPy semantics.</p>
<p>Tensorflow doesn't compile functions to C and then machine code like Theano, but serializes the defined mathematical functions (the data structure containing variables and transformations is called <strong>computation graph</strong>) and executes them on specific devices. The configuration of devices and context can be done using the <kbd>tf.Session</kbd> object.</p>
<p>Once the desired expression is defined, a <kbd>tf.Session</kbd> needs to be initialized and can be used to execute computation graphs using the <kbd>Session.run</kbd> method. In the following example, we demonstrate the usage of the Tensorflow API to implement a simple element-wise sum of squares:</p>
<pre>
    a = tf.placeholder('float64')<br/>    b = tf.placeholder('float64')<br/>    ab_sq = a**2 + b**2<br/><br/>    with tf.Session() as session:<br/>        result = session.run(ab_sq, feed_dict={a: [0, 1, 2], <br/>                                               b: [3, 4, 5]})<br/>        print(result)<br/>    # Output:<br/>    # array([  9.,  17.,  29.])
</pre>
<p>Parallelism in Tensorflow is achieved automatically by its smart execution engine, and it generally works well without much fiddling. However, note that it is mostly suited for deep learning workloads that involve the definition of complex functions that use a lot of matrix multiplications and calculate their gradient.</p>
<p>We can now replicate the estimation of the pi example using Tensorflow capabilities and benchmark its execution speed and parallelism against the Theano implementation. What we will do is this:</p>
<ul>
<li>Define our <kbd>x</kbd> and <kbd>y</kbd> variables and perform a hit test using broadcasted operations.</li>
<li>Calculate the sum of <kbd>hit_tests</kbd> using the <kbd>tf.reduce_sum</kbd> function.</li>
<li>Initialize a <kbd>Session</kbd> object with the <kbd>inter_op_parallelism_threads</kbd> and <kbd>intra_op_parallelism_threads</kbd> configuration options. These options control the number of threads used for different classes of parallel operations. <span class="typ">Note</span> <span class="pln">that the first</span> <kbd><span class="typ">Session</span></kbd> <span class="pln">created with such options </span><span class="pln">sets the</span><span class="pln"> number of threads for the whole script (even future <kbd>Session</kbd> instances).</span></li>
</ul>
<p>We can now write a script name, <kbd>test_tensorflow.py</kbd>, containing the following code. Note that the number of threads is passed as the first argument of the script (<kbd>sys.argv[1]</kbd>):</p>
<pre>
    import tensorflow as tf<br/>    import numpy as np<br/>    import time<br/>    import sys<br/><br/>    NUM_THREADS = int(sys.argv[1])<br/>    samples = 30000<br/><br/>    print('Num threads', NUM_THREADS)<br/>    x_data = np.random.uniform(-1, 1, samples)<br/>    y_data = np.random.uniform(-1, 1, samples)<br/><br/>    x = tf.placeholder('float64', name='x')<br/>    y = tf.placeholder('float64', name='y')<br/><br/>    hit_tests = x ** 2 + y ** 2 &lt;= 1.0<br/>    hits = tf.reduce_sum(tf.cast(hit_tests, 'int32'))<br/><br/>    with tf.Session<br/>        (config=tf.ConfigProto<br/>            (inter_op_parallelism_threads=NUM_THREADS,<br/>             intra_op_parallelism_threads=NUM_THREADS)) as sess:<br/>        start = time.time()<br/>        for i in range(10000):<br/>            sess.run(hits, {x: x_data, y: y_data})<br/>        print(time.time() - start)
</pre>
<p>If we run the script multiple times with different values of <kbd>NUM_THREADS</kbd>, we see that the performance is quite similar to Theano and that the speedup increased by parallelization is quite modest:</p>
<pre>
<strong>    $ python test_tensorflow.py 1</strong><br/>    13.059704780578613<br/><strong>    $ python test_tensorflow.py 2</strong><br/>    11.938535928726196<br/><strong>    $ python test_tensorflow.py 3</strong><br/>    12.783955574035645<br/><strong>    $ python test_tensorflow.py 4</strong><br/>    12.158143043518066
</pre>
<p>The main advantage of using software packages such as Tensorflow and Theano is the support for parallel matrix operations that are commonly used in machine learning algorithms. This is very effective because those operations can achieve impressive performance gains on GPU hardware that is designed to perform these operations with high throughput. </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Running code on a GPU</h1>
            </header>

            <article>
                
<p>In this subsection, we will demonstrate the usage of a GPU with Theano and Tensorflow. As an example, we will benchmark the execution of a very simple matrix multiplication on the GPU and compare it to its running time on a CPU.</p>
<div class="packt_infobox">The code in this subsection requires the possession of a GPU. For learning purposes, it is possible to use the Amazon EC2 service (<a href="https://aws.amazon.com/ec2">https://aws.amazon.com/ec2</a>) to request a GPU-enabled instance.</div>
<p>The following code performs a simple matrix multiplication using Theano. We use the <kbd>T.matrix</kbd> function to initialize a two-dimensional array, and then we use the <kbd>T.dot</kbd> method to perform the matrix multiplication:</p>
<pre>
    from theano import function, config<br/>    import theano.tensor as T<br/>    import numpy as np<br/>    import time<br/><br/>    N = 5000<br/><br/>    A_data = np.random.rand(N, N).astype('float32')<br/>    B_data = np.random.rand(N, N).astype('float32')<br/><br/>    A = T.matrix('A')<br/>    B = T.matrix('B')<br/><br/>    f = function([A, B], T.dot(A, B))<br/><br/>    start = time.time()<br/>    f(A_data, B_data)<br/><br/>    print("Matrix multiply ({}) took {} seconds".format(N, time.time() - start))<br/>    print('Device used:', config.device)
</pre>
<p>It is possible to ask Theano to execute this code on a GPU by setting the <kbd>config.device=gpu</kbd> option. For added convenience, we can set up the configuration value from the command line using the <kbd>THEANO_FLAGS</kbd> environmental variable, shown as follows. After copying the previous code in the <kbd>test_theano_matmul.py</kbd> file, we can benchmark the execution time by issuing the following command:</p>
<pre>
<strong>    $ THEANO_FLAGS=device=gpu python test_theano_gpu.py </strong><br/>    Matrix multiply (5000) took 0.4182612895965576 seconds<br/>    Device used: gpu
</pre>
<p>We can analogously run the same code on the CPU using the <kbd>device=cpu</kbd> configuration option:</p>
<pre>
<strong>    $ THEANO_FLAGS=device=cpu python test_theano.py</strong> <br/>    Matrix multiply (5000) took 2.9623231887817383 seconds<br/>    Device used: cpu
</pre>
<p>As you can see, the GPU is 7.2 times faster than the CPU version for this example!</p>
<p>For comparison, we may benchmark equivalent code using Tensorflow. The implementation of a Tensorflow version is reported in the next code snippet. The main differences with the Theano version are as follows:</p>
<ul>
<li>The usage of the <kbd>tf.device</kbd> config manager that serves to specify the target device (<kbd>/cpu:0</kbd> or <kbd>/gpu:0</kbd>)</li>
<li>The matrix multiplication is performed using the <kbd>tf.matmul</kbd> operator:</li>
</ul>
<pre>
    import tensorflow as tf<br/>    import time<br/>    import numpy as np<br/>    N = 5000<br/><br/>    A_data = np.random.rand(N, N)<br/>    B_data = np.random.rand(N, N)<br/><br/>    # Creates a graph.<br/><br/><strong>    with tf.device('/gpu:0'):</strong><br/>        A = tf.placeholder('float32')<br/>        B = tf.placeholder('float32')<br/><br/>        C = tf.matmul(A, B)<br/><br/>    with tf.Session() as sess:<br/>        start = time.time()<br/>        sess.run(C, {A: A_data, B: B_data})<br/>        print('Matrix multiply ({}) took: {}'.format(N, time.time() - start))
</pre>
<p>If we run the <kbd>test_tensorflow_matmul.py</kbd> script with the appropriate <kbd>tf.device</kbd> option, we obtain the following timings:</p>
<pre>
<strong>    # Ran with tf.device('/gpu:0')</strong><br/>    Matrix multiply (5000) took: 1.417285680770874<br/><br/><strong>    # Ran with tf.device('/cpu:0')</strong><br/>    Matrix multiply (5000) took: 2.9646761417388916 
</pre>
<p>As you can see, the performance gain is substantial (but not as good as the Theano version) in this simple case.</p>
<p>Another way to achieve automatic GPU computation is the now familiar Numba. With Numba, it is possible to compile Python code to programs that can be run on a GPU. This flexibility allows for advanced GPU programming as well as more simplified interfaces. In particular, Numba makes extremely easy-to-write, GPU-ready, generalized universal functions.</p>
<p>In the next example, we will demonstrate how to write a universal function that applies an exponential function on two numbers and sums the results. As we already saw in <a href="3797e4bb-99b8-4ba2-bb17-f757078b1d2b.xhtml">Chapter 5</a>, <em>Exploring Compilers</em> this can be accomplished using the <kbd>nb.vectorize</kbd> function (we'll also specify the <kbd>cpu</kbd> target explicitly):</p>
<pre>
    import numba as nb<br/>    import math<br/>    @nb.vectorize(target='cpu')<br/>    def expon_cpu(x, y):<br/>        return math.exp(x) + math.exp(y)
</pre>
<p>The <kbd>expon_cpu</kbd> universal function can be compiled for the GPU device using the <kbd>target='cuda'</kbd> option. Also, note that it is necessary to specify the input types for CUDA universal functions. The implementation of <kbd>expon_gpu</kbd> is as follows:</p>
<pre>
    @nb.vectorize(['float32(float32, float32)'], target='cuda')<br/>    def expon_gpu(x, y):<br/>        return math.exp(x) + math.exp(y)
</pre>
<p>We can now benchmark the execution of the two functions by applying the functions on two arrays of size 1,000,000. Also, note that we execute the function before measuring the timings to trigger the Numba just-in-time compilation:</p>
<pre>
    import numpy as np<br/>    import time<br/><br/>    N = 1000000<br/>    niter = 100<br/><br/>    a = np.random.rand(N).astype('float32')<br/>    b = np.random.rand(N).astype('float32')<br/><br/>    # Trigger compilation<br/>    expon_cpu(a, b)<br/>    expon_gpu(a, b)<br/><br/>    # Timing<br/>    start = time.time()<br/>    for i in range(niter):<br/>       expon_cpu(a, b)<br/>    print("CPU:", time.time() - start)<br/><br/>    start = time.time()<br/>    for i in range(niter): <br/>        expon_gpu(a, b) <br/>    print("GPU:", time.time() - start) <br/>    # <strong>Output:</strong><br/>    # <strong>CPU: 2.4762887954711914</strong><br/>    # <strong>GPU: 0.8668839931488037</strong>
</pre>
<p>Thanks to the GPU execution, we were able to achieve a 3x speedup over the CPU version. Note that transferring data on the GPU is quite expensive; therefore, GPU execution becomes advantageous only for very large arrays.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>Parallel processing is an effective way to improve performance on large datasets. Embarrassingly parallel problems are excellent candidates for parallel execution that can be easily implemented to achieve good performance scaling.</p>
<p>In this chapter, we illustrated the basics of parallel programming in Python. We learned how to circumvent Python threading limitation by spawning processes using the tools available in the Python standard library. We also explored how to implement a multithreaded program using Cython and OpenMP.</p>
<p>For more complex problems, we learned how to use the Theano, Tensorflow, and Numba packages to automatically compile array-intensive expressions for parallel execution on CPU and GPU devices.</p>
<p>In the next chapter, we will learn how to write and execute parallel programs on multiple processors and machines using libraries such as dask and PySpark.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>