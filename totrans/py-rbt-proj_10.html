<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Making a Guard Robot</h1>
                </header>
            
            <article>
                
<p class="mce-root">I am sure you must have seen the movie <em>I, Robot</em> or <em>Chappie</em>. After watching the movie, a lot of people would be intrigued by the idea of making a robot that would work to protect and guard you. However, the security systems that are the state of the art can hardly be classified as a robot. In this chapter, we will take a step ahead in the lane of vision processing and make a guard robot. Its purpose would be to guard your gate and if an unknown person comes over to the gate it would start to trigger an alarm. However, the interesting thing is that the robot would not trigger any alarm if a known person comes home. What's more is that it would clear the way and get out of the door area to let you in. Once you are inside, it will automatically be back in its position to guard and get back to work yet again.</p>
<p class="mce-root">How cool would that be? So let's get going and make this robot a reality.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Face detection</h1>
                </header>
            
            <article>
                
<p><span>Now, before we go ahead and detect faces, we need to tell the robot what a face is and what it looks like. Raspberry Pi does not know how exactly to classify a face from a pumpkin. So firstly, we would be using a dataset to tell the robot what our face looks like; thereafter, we will start recognizing the faces as we go. So let's go ahead and see how to do it.<br/></span></p>
<p><span>Firstly, you need to install a dependency called Haar-cascade. This is a cascade-dependent algorithm that is used to detect objects rapidly. To do this, go ahead and run the following syntax on your terminal:</span></p>
<pre><strong>git clone https://github.com/opencv/opencv/tree/master/data/haarcascades</strong></pre>
<p>This will save the <kbd>haarcascades</kbd> file onto your Raspberry Pi and you will be ready to use it. Once you are done, see the following code but write it over your Raspberry only after you have seen the following explanation line by line:</p>
<pre>import cv2<br/>import numpy as np<br/><br/>face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')<br/><br/>cap = cv2.VideoCapture(0)<br/><br/>while True:<br/><br/>        ret, img = cap.read()<br/>        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br/>        faces = face_cascade.detectMultiScale(gray)<br/>       <br/>        for (x,y,w,h) in faces:<br/>           cv2.rectangle(img, (x,y), (x+w, y+h), (255,0,0), 2)<br/><br/>        cv2.imshow('img',img)<br/><br/>        k = cv2.waitKey(1) &amp; 0xff<br/>        if k == ord(‘q’):<br/>                break<br/><br/>cap.release()<br/>cv2.destroyAllWindows()</pre>
<p>Now, this might look like something out of our world and pretty much every thing is new, so let's understand what we are doing here:</p>
<pre>face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')</pre>
<p>Once we have installed Haar-cascade, we are basically taking in the data which is already trained onto our Raspberry Pi. In this line, we are opening a classifier, which is reading the data from a file named <kbd>haarcascade_frontalface_default.xml</kbd>. This is the file that will tell Raspberry Pi whether the image captured is a frontal face or not. This file has a trained dataset to enable the Raspberry to do so. Now, we are using a function of OpenCV called <kbd>CascadeClassifier()</kbd>, which uses this learned data from the file mentioned and then classifies the images:</p>
<pre>cap = cv2.VideoCapture(0)</pre>
<p>This will capture the video from the camera with the port number <kbd>0</kbd>. So whenever the data needs to be captured, the variable <kbd>cap</kbd> can be used instead of writing the whole program.</p>
<pre>        ret, img = cap.read()</pre>
<p>We have understood this line in the previous chapter. It is simply capturing the image from the camera and saving it in the variable called <kbd>img</kbd> and then <kbd>ret</kbd> will return true if the capture is done or false if there is an error.</p>
<pre>        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</pre>
<p>We have used this line previously as well. What it is doing is, it is simply converting the captured image using the <kbd>cv2.cvtColour()</kbd> <span>function. T</span>he arguments passed in it are the following <kbd>img</kbd>, which will basically tell which image needs to be converted. Thereafter, <kbd>cv2.COLOR_BGR2GRAY</kbd> will tell from which image type it has to be converted into what.</p>
<pre>        faces = face_cascade.detectMultiScale(gray)</pre>
<p>The <kbd>face_cascade.detectMultiScale()</kbd> function is a function of <kbd>face_cascade</kbd>. It detects the objects of various sizes and creates a rectangle of a similar size around it. The values returned to the variable faces would be the <kbd>x</kbd> and <kbd>y</kbd> coordinates of the object detected along with the width and height of the object as well. Hence, we need to define the size and position of the detected object.</p>
<pre>        for (x,y,w,h) in faces:<br/>           cv2.rectangle(img, (x,y), (x+w, y+h), (255,0,0), 2)</pre>
<p>In the previous line of code, we have taken the values of the position and the height of the rectangle. However, we still haven't drawn one in the actual picture. What this <kbd>for</kbd> loop will do is, it'll add a rectangle to the image using the <kbd>cv2.rectangle()</kbd> <span>function.</span> <kbd>img</kbd> is telling which image needs to be worked on. <kbd>(x,y)</kbd> is defining the starting coordinates of the position of the object. The value <kbd>(x+w, y+h)</kbd> is defining the end point of the rectangle. The value <kbd>(255,0,0)</kbd> is defining the color and argument <kbd>2</kbd> is defining the thickness of the line.</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/ec518dd5-ca51-4f9f-8be4-289922921837.png" style="width:39.25em;height:23.92em;"/></div>
<pre>        cv2.imshow('img',img)</pre>
<p>In this line of code, we simply use the <kbd>imshow()</kbd> <span>function </span>to give the final output, which will have the image overlayed by the rectangle we just drew. This will indicate that we have successfully identified the image. The <kbd>'img'</kbd> <span>argument </span>will tell the name of the window and the second <kbd>img</kbd> will tell the function which image needs to be shown.</p>
<pre>        k = cv2.waitKey(1) &amp; 0xff<br/>        if k == ord(‘q’):<br/>                break</pre>
<p>This line is simply waiting to take the key press of <kbd>q</kbd>. Whenever the user presses the <kbd>q</kbd> <span>key, </span>the <kbd>if</kbd> statement would become true, which will thereby break the infinite loop.</p>
<pre>cap.release()<br/>cv2.destroyAllWindows()</pre>
<p>Finally, we are releasing the cameras using <kbd>cap.release()</kbd> and then closing all the windows using <kbd>cv2.destoryAllWindows()</kbd>.</p>
<p>Now, go ahead and run the code and see whether it is able to detect your face or not. Good luck!</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Knowing the face</h1>
                </header>
            
            <article>
                
<p>All right, we have detected the face by using a few lines of code, but I would not consider it to be a very big victory as we were fighting using the sword made by other developers. The learning set imported was a generic face learned set. However, in this chapter, we will go ahead and make our very own learning set to recognize a specific human face. This is really a cool thing and I'm sure you will love doing it.</p>
<p>So, let's get started. As you did earlier, go through the explanation first and then write the code so that you understand it very well.</p>
<p>Firstly, we are using the program to capture the images of the object that needs to be detected. In our case, this object will be a person and his face. So, let's see what we need to do:</p>
<pre>import cv2<br/>import numpy as np<br/><br/>faceDetect = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')<br/>cam = cv2.VideoCapture(0)<br/><br/>sampleNum = 0<br/><br/>id = raw_input('enter user id')<br/><br/>while True:<br/>        ret,img = cam.read()<br/>        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br/>        faces = faceDetect.detectMultiScale(gray,1.3,5)<br/><br/>        for (x,y,w,h) in faces:<br/>                sampleNum = sampleNum + 1<br/>                cv2.imwrite("dataSet/User."+str(id)+"."+str(sampleNum)+".jpg",  gray[y:y+h, x:x+w])<br/><br/>                cv2.rectangle(img, (x,y), (x+w,y+h), (0,0,255), 2)<br/>                cv2.waitKey(100)<br/>        cv2.imshow("Face", img)<br/>        cv2.waitKey(1)<br/>        if sampleNum&gt;20:<br/><br/>                break<br/>cam.release()<br/>cv2.destroyAllWindows()</pre>
<p>Here is the explanation:</p>
<pre>faceDetect = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')<br/>cam = cv2.VideoCapture(0)</pre>
<p>As you have seen earlier, we are using the preceding two lines of code to import the learned dataset and also to start the camera.</p>
<pre>id = raw_input('enter user id')</pre>
<p>As we will be training the system to learn a specific face, it is very important that the program knows who it is detecting either by name or ID. This will help us clarify who we have detected. So, to go ahead and detect a person by face, we need to provide his ID, which is being done in the following code:</p>
<pre>        ret,img = cam.read()<br/>        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</pre>
<p>This is exactly the same as we did in the previous section of code. Refer to it if you need explanation.</p>
<pre>        faces = faceDetect.detectMultiScale(gray,1.3,5)</pre>
<p>Now, this line of code might also sound to you like a repetition; however, there is an addition to it. In this function, two arguments have been passed instead of one. The first is <kbd>grey</kbd> and the second is the minimum and maximum size of the object that can be detected. This is important to make sure that the object detected is big enough for the learning process to happen.</p>
<pre>        for (x,y,w,h) in faces:<br/>                sampleNum = sampleNum + 1<br/>                cv2.imwrite("dataSet/User."+str(id)+"."+str(sampleNum)+".jpg",  gray[y:y+h, x:x+w])</pre>
<p>Here, we are using the same <kbd>for</kbd> loop to perform the following condition. So the loop will only be true when the face is detected. Whenever it is detected, the <kbd>sampleNum</kbd> <span>variable </span>would be incremented by <kbd>1</kbd> by counting the number of faces detected. Further, to capture the images onto our system, we need the following line of code: </p>
<pre>cv2.inwrite('dataSet/User."+str(id)+"."+str(sampleNum)+".jpg",gray[y:y+h,x:x+w])</pre>
<p>What it does is, it simply saves the image onto a folder by the name <kbd>dataSet/User</kbd>. It is very important to be able to make a folder by this name yourself. If you don't do this, then it would go haywire when it does not find the folder where it is supposed to save. <kbd>+str(id)</kbd> will save the name by the ID of the person and increment it with the number of samples counted using <kbd>+str(sampleNum)</kbd>. Furthermore, we have mentioned that the image would be saved by the format <kbd>.jpg</kbd> and finally <kbd>gray[y:y+h, x:x+w]</kbd> is selecting the part of the image that contains the face.</p>
<p>The rest of the program beyond this point is self explanatory and I suspect you can understand it on your own. In very simple English, this would save the images in a folder and will keep doing so, until it reaches 20 images.</p>
<p>Now that we have captured the images, it's time to make the system learn the images and understand how to recognize them. To do this, we need to install something called the <kbd>pillow</kbd> library. Installing it is easy. All you need to do is, write the following line in the terminal:</p>
<pre><strong><span>sudo -H </span>pip install pillow</strong></pre>
<p>This <kbd>pillow</kbd> library would help us read the dataset. We will understand more in a moment. Once this is installed, let's go ahead and see how we are doing the learning part. So go ahead and understand the following code and then let's get going:</p>
<pre>import os<br/>import cv2<br/>import numpy as np<br/>from PIL import Image<br/><br/>recognizer = cv2.face.LBPHFaceRecognizer_create()<br/><br/>path = 'dataSet'<br/><br/>def getImageID(path):<br/>    imagePaths = [os.path.join(path,f) for f in os.listdir(path)]<span><br/><br/></span>    faces=[]<br/>    IDs=[]<br/><br/>    for imagePath in imagePaths:<br/>        faceImg = Image.open(imagePath).convert('L')<br/><br/>       faceNp = np.array(faceImg, 'unit8')<br/><br/>        ID = int(os.path.split(imagePath)[-1].split('.')[1])<br/><br/>        faces.append(faceNp)<br/>        print ID<br/>        IDs.append(ID)<br/><br/>    return IDs, faces<br/><br/>Ids, faces = getImageID(path)<br/>recognizer.train(faces, np.array(Ids))<br/>recognizer.save('recognizer/trainningData.yml')<br/><br/>cv2.destroyAllWindows()</pre>
<p>Alien might be the word which might come in your head after seeing this code, but it sure won't be alien after you have gone through this explanation. So let's see:</p>
<pre>recognizer = cv2.face.LBPHFaceRecognizer_create()</pre>
<p>It is creating a recognizer using the <kbd>cv2.face.LBPHFaceRecognizer_create()</kbd><span> function.</span></p>
<pre>path = 'dataSet'</pre>
<p>This line here tells where the captured data is stored in Raspberry Pi. We have <span>already </span>made a folder by this name and it contains the images that we have stored previously.</p>
<pre>def getImageID(path):<br/>    imagePaths = [os.path.join(path,f) for f in os.listdir(path)]</pre>
<p>Here we are defining the function named <kbd>getImageID(path)</kbd>.</p>
<p><span>This join function will join the path with <kbd>f</kbd>. Now, <kbd>f</kbd> is a variable containing the filename as it loops through the list of files inside the folder defined as path using <kbd>os.listdir(path)</kbd></span>.</p>
<pre>    for imagePath in imagePaths:<br/>        faceImg = Image.open(imagePath).convert('L')</pre>
<p>The <kbd>for</kbd> loop in this line will be true for every image that we have and will run the code inside of it. What <kbd>Image.open(imagePath).convert('L')</kbd> does is, it simply covers the image in monochrome format. Using this line, every image that we are having would be converted into monochrome.</p>
<pre>faceNp = np.array(faceImg, 'unit8')</pre>
<p>OpenCV works with <kbd>numpy</kbd> array; hence, we need to convert the images to the desired format. To do this, we are using a <kbd>faceNp</kbd> <span>variable </span>to call the <kbd>np.array()</kbd> <span>function. T</span>his function converts the images into <kbd>numpy</kbd> array of name <kbd>faceImg</kbd> and with 8-bit integer value, as we have passed the argument <kbd>unit8</kbd>.</p>
<pre>        ID = int(os.path.split(imagePath)[-1].split('.')[1])</pre>
<p>In this line, we are using a variable to call the <kbd>int()</kbd> <span>function, </span>which will split the path name for the images being captured. Why are we doing this? This is done to extract the ID number from the actual filename. Hence, we are doing this using the following function:</p>
<pre>        faces.append(faceNp)<br/>        print ID<br/>        IDs.append(ID)</pre>
<p>Here, using <kbd>faces.append(faceNp)</kbd>, we are adding the data into the array by the name of <kbd>faces</kbd> and the data being added is <kbd>faceNp</kbd>. Then, we are printing the <kbd>ID</kbd> of that image.<br/>
<br/>
Once done, <kbd>IDs.append(ID)</kbd> will add the <kbd>ID</kbd> to the array <kbd>IDs</kbd>. This whole process is being done as the function that we would be using for training would only take in the values in the form of an array. So we have to convert the entire data in the form of an array and fetch it to the trainer.<br/>
<br/>
So the entire explanation so far was defining the function named <kbd>getImageId(Path)</kbd>.</p>
<pre>    return IDs, faces</pre>
<p>Now this line would return the values of <kbd>IDs</kbd> of the faces which will be further used to train the dataset.</p>
<pre>Ids, faces = getImageID(path)<br/>recognizer.train(faces, np.array(Ids))<br/>recognizer.save('recognizer/trainningData.yml')</pre>
<p>In the first line here, the <kbd>getImageID(path)</kbd> <span>function </span>would take in the path of the any image and return the <kbd>Ids</kbd> of the images. Then, <kbd>faces</kbd> would have the array data of the images.</p>
<p>Finally, <span>in</span> <kbd>recognizer.train(faces, np.array(Ids))</kbd>, we are using a function of <kbd>recognizer</kbd> called <kbd>train</kbd> to train the system based on their images. The arguments passed here are that <kbd>faces</kbd> has the array of the images. Furthermore, <kbd>np.array(Ids)</kbd> is the array, which is returned by the function defined by the name <kbd>getImageID()</kbd>.</p>
<p>Once the system is trained using the following program, we would save the trained data to a file. This is done using the <kbd>recognizer.save()</kbd> <span>function. T</span>he argument passed in it is the name and the extension of the file saved.</p>
<p>It can be a little intense and sometimes confusing as well. However, it will seem easy once you do it. Now, it's time that you go ahead and make the system learn about your face and its data.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Recognizing the face</h1>
                </header>
            
            <article>
                
<p>Now that we have learned how to make our system learn, it's time to use that learned data and recognize the face. So without much talking, let's go ahead and understand how this would be done:</p>
<pre>import numpy as np<br/>import cv2<br/><br/>faceDetect = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')<br/><br/>cam = cv2.VideoCapture(0)<br/>rec = cv2.face.LBPHFaceRecognizer_create()<br/><br/>rec.read("recognizer/trainningData.yml")<br/>id = 0<br/>font = cv2.FONT_HERSHEY_SIMPLEX<br/><br/>while True:<br/><br/>   ret, img = cam.read()<br/>   gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)<br/>   faces = faceDetect.detectMultiScale(gray,1.3,5)<br/><br/>   for (x,y,w,h) in faces:<br/>       cv2.rectangle(img, (x,y), (x+w, y+h), (0,0,255), 2)<br/>       id, conf = rec.predict(gray[y:y+h, x:x+w])<br/><br/>       if id==1:<br/>           id = "BEN"<br/>       cv2.putText(img, str(id), (x,y+h),font,2, (255,0,0),1,)<br/><br/>    cv2.imshow("face", img)<br/><br/>    if cv2.waitKey(1)==ord('q'):<br/>       break<br/><br/>cam.release()<br/>cv2.destroyAllWindows()</pre>
<p>In this code, there are not a lot of new things that you might encounter. It is very similar to the first code that we started with in this chapter. Essentially, it is also doing the same work. The only difference is that it recognizes the person by his ID. So, let's see what is different and how well it performs.</p>
<p>Now, most of the code is repetitive. So instead of going through all of it, I will only touch on the ones which are new. Here we go:</p>
<pre>font = cv2.FONT_HERSHEY_SIMPLEX</pre>
<p>Like last time, we were drawing a rectangle over an identified image. However, this time there are places where overlay has to be done with some text as well. So here we are choosing the font that we need to use in this program:</p>
<pre>id, conf = rec.predict(gray[y:y+h, x:x+w])</pre>
<p>With this line, the prediction is taking place using the function of the recognizer called <kbd>predict()</kbd>. This predicts the images and returns the <kbd>id</kbd> of the detected image.</p>
<pre>       if id==1:<br/>           id = "BEN"</pre>
<p>Now, finally, if the <kbd>id</kbd> is equal to <kbd>1</kbd>, then the value of <kbd>id</kbd> would be changed to <kbd>BEN</kbd>.</p>
<pre>     cv2.putText(img, str(id), (x,y+h),font,2, (255,0,0),1,)</pre>
<p>The <kbd>putText()</kbd> <span>function </span>will put a text on the detected object. The definition of each of the arguments is as follows:</p>
<ul>
<li><kbd>img</kbd>: This is the image on which the text has to be put.</li>
<li><kbd>str(id)</kbd>: This is the string that needs to be printed, in our case it would print the ID of the person.</li>
<li><kbd>(x, y+h)</kbd>: This is the position where the text would be printed.</li>
<li><kbd>font</kbd>: This is the font of the printed text. In our case, it would be the value of the font defined earlier.</li>
<li><kbd>2</kbd>: This is the font scale, that is, how big the font would be. This can be similar to magnification.</li>
<li><kbd>(255,0,0)</kbd>: This is the color of the font.</li>
<li><kbd>1</kbd>: This is the thickness of the font.</li>
</ul>
<p>With this program, we would be able to find out if the learning set is working as per our requirement. Once you have written the code, try identifying the people with it and see whether it works accurately. If the accuracy is not satisfactory, then you may want to take more than 20 samples for learning. Conduct a few trials and I am sure you would reach perfection very soon.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Making the guard robot</h1>
                </header>
            
            <article>
                
<p>Now that we have understood how the learning works and how the learned data can be used to identify people, it's time to put it to use. As the name of the chapter suggests, we will be making a guard robot using this technology. Now, let's have a look at the following program. Before you start programming, take out the robotic vehicle you had and make the connection like we did before in <a href="17f222ca-2716-430f-a87b-0b37352f5ae0.xhtml">Chapter 6</a>, <em>Bluetooth-Controlled Robotic Car</em>. Attach the motor, motor driver, and Raspberry Pi. Once you have done that, then write the following code. This code is utilizing all the learning of the previous programs in the chapter and we would be able to distinguish between an intruder and a resident of the house based on vision processing. So let's get going:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/a9a88f57-272e-415d-a2b4-222f672bcaf4.png"/></div>
<pre>import numpy as np<br/>import cv2<br/>Import RPi.GPIO as GPIO<br/><br/>Motor1F = 20<br/>Motor1R = 21<br/>Motor2F = 2<br/>Motor2R = 3<br/>Buzzer = 24<br/><br/>GPIO.setmode(GPIO.BCM)  <br/>GPIO.setwarnings(False)<br/>GPIO.setup(Motor1a,GPIO.OUT)<br/>GPIO.setup(Motor1b,GPIO.OUT)<br/>GPIO.setup(Motor2a,GPIO.OUT)<br/>GPIO.setup(Motor2b,GPIO.OUT)<br/>GPIO.setup(Buzzer, GPIO.OUT)<br/><br/>def forward():<br/><br/>        GPIO.output(Motor1F,1)<br/>        GPIO.output(Motor1R,0)<br/>        GPIO.output(Motor2F,1)<br/>        GPIO.output(Motor2R,0)<br/><br/>def backward():<br/><br/>        GPIO.output(Motor1F,0)<br/>        GPIO.output(Motor1R,1)<br/>        GPIO.output(Motor2F,0)<br/>        GPIO.output(Motor2R,1)<br/><br/>def stop():<br/><br/>        GPIO.output(Motor1F,0)<br/>        GPIO.output(Motor1R,0)<br/>        GPIO.output(Motor2F,0)<br/>        GPIO.output(Motor2R,0)<br/><br/>faceDetect = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')<br/><br/>cam = cv2.VideoCapture(0)<br/>rec = cv2.face.LBPHFaceRecognizer_create()<br/>rec.read("recognizer/trainningData.yml")<br/><br/>id = 0<br/>font = cv2.FONT_HERSHEY_SIMPLEX<br/><br/>while True:<br/><br/> ret, img = cam.read()<br/> gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)<br/> faces = faceDetect.detectMultiScale(gray,1.3,5)<br/><br/> for (x,y,w,h) in faces:<br/>     cv2.rectangle(img, (x,y), (x+w, y+h), (0,0,255), 2)<br/>     id, conf = rec.predict(gray[y:y+h, x:x+w])<br/><br/>     if id==1:<br/>         id = "BEN"<br/><br/>         forward()<br/>         time.sleep(1)<br/>         stop()<br/>         time.sleep(5)<br/>         backward()<br/>         time.sleep(1)<br/><br/>     else :<br/><br/>         GPIO.output(Buzzer, 1)<br/>         time.sleep(5)<br/><br/>     cv2.putText(img, str(id), (x,y+h),font,2, (255,0,0),1,cv2.LINE_AA)<br/>     cv2.imshow("face", img)<br/><br/> id = 0 <br/> if cv2.waitKey(1)==ord('q'):<br/> break<br/><br/>cam.release()<br/>cv2.destroyAllWindows()</pre>
<p>As always, we will only be looking at the peculiar changes in the program, a lot of it will be carried over from the previous chapter. So we will try not to repeat the explanation unless necessary.</p>
<pre>def forward():<br/><br/>        GPIO.output(Motor1a,0)<br/>        GPIO.output(Motor1b,1)<br/>        GPIO.output(Motor2a,0)<br/>        GPIO.output(Motor2b,1)<br/><br/>def backward():<br/><br/>        GPIO.output(Motor1a,1)<br/>        GPIO.output(Motor1b,0)<br/>        GPIO.output(Motor2a,1)<br/>        GPIO.output(Motor2b,0)<br/><br/>def stop():<br/><br/>        GPIO.output(Motor1a,0)<br/>        GPIO.output(Motor1b,0)<br/>        GPIO.output(Motor2a,0)<br/>        GPIO.output(Motor2b,0)</pre>
<p>Just as a recap, we are defining two functions, namely, <kbd>backwards</kbd>, <kbd>reverse</kbd>, and <kbd>stop</kbd>. These functions will help us move the vehicle in the direction in which we want.</p>
<pre>faceDetect = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')</pre>
<p>This line is importing the previously learned dataset by the name of <kbd>harrcascade_frontalface_default.xml</kbd>. This will help us recognize any face that comes in front of the camera.</p>
<pre> for (x,y,w,h) in faces:<br/>     cv2.rectangle(img, (x,y), (x+w, y+h), (0,0,255), 2)<br/>     id, conf = rec.predict(gray[y:y+h, x:x+w])<br/><br/>     if id==1:<br/>         id = "BEN"<br/><br/>         forward()<br/>         time.sleep(1)<br/>         stop()<br/>         time.sleep(5)<br/>         backward()<br/>         time.sleep(1)<br/><br/>     else :<br/><br/>         GPIO.output(Buzzer, 1)<br/>         time.sleep(5)</pre>
<p>In this piece of the code, we are identifying the face and taking decisions based on it. As we have done earlier, if the face is detected, then its corresponding ID would be given by the program. However, no ID would be given if the face is not detected by the previously learned dataset in which any face could be detected. Hence, according to the program, if the <kbd>id == 1</kbd>, then the robotic vehicle would move forward moving away from the path, thereafter it would stop for <kbd>5</kbd> seconds and get back to where it was earlier. In case the ID generated is anything except <kbd>1</kbd>, then the Buzzer would be turned on for <kbd>5</kbd> seconds alerting the user.</p>
<p>By using this system, anyone who is identified can be let inside the premises; however, if the person is not identified, then the alarm would be triggered.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have learned how to detect objects using a prelearned dataset. We also learned how to make our very own learned dataset for a specific object. Finally, we have used all of that learning to make a guard robot, who will guard our home using the power of vision processing.</p>
<p> </p>


            </article>

            
        </section>
    </div></body></html>