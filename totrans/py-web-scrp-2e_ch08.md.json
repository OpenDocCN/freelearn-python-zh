["```py\npip install scrapy\n\n```", "```py\n$ scrapy\n Scrapy 1.3.3 - no active project\n\nUsage:\n scrapy <command> [options] [args]\n\nAvailable commands:\n bench    Run quick benchmark test\n commands \n fetch    Fetch a URL using the Scrapy downloader\n...\n\n```", "```py\n$ scrapy startproject example\n$ cd example\n\n```", "```py\n    scrapy.cfg \n    example/ \n        __init__.py   \n        items.py\n        middlewares.py   \n        pipelines.py   \n        settings.py   \n        spiders/ \n            __init__.py \n\n```", "```py\n# -*- coding: utf-8 -*- \n# Define here the models for your scraped items\n#\n# See documentation in:\n# http://doc.scrapy.org/en/latest/topics/items.html\n\nimport scrapy \n\nclass ExampleItem(scrapy.Item): \n    # define the fields for your item here like: \n    # name = scrapy.Field() \n    pass \n\n```", "```py\nclass CountryItem(scrapy.Item): \n    name = scrapy.Field() \n    population = scrapy.Field() \n\n```", "```py\n    $ scrapy genspider country example.webscraping.com --template=crawl\n\n```", "```py\n# -*- coding: utf-8 -*-\nimport scrapy \nfrom scrapy.linkextractors import LinkExtractor \nfrom scrapy.spiders import CrawlSpider, Rule \n\nclass CountrySpider(CrawlSpider): \n    name = 'country' \n    allowed_domains = ['example.webscraping.com'] \n    start_urls = ['http://example.webscraping.com']\n\n    rules = ( \n        Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True), \n    ) \n\n    def parse_item(self, response): \n        i = {} \n        #i['domain_id'] = response.xpath('//input[@id=\"sid\"]/@value').extract() \n        #i['name'] = response.xpath('//div[@id=\"name\"]').extract() \n        #i['description'] = response.xpath('//div[@id=\"description\"]').extract() \n        return i \n\n```", "```py\nCONCURRENT_REQUESTS_PER_DOMAIN = 1 \nDOWNLOAD_DELAY = 5 \n\n```", "```py\n    $ scrapy crawl country -s LOG_LEVEL=ERROR\n$\n\n```", "```py\nfrom example.items import CountryItem\n    ...\n\n    rules = ( \n        Rule(LinkExtractor(allow=r'/index/'), follow=True), \n        Rule(LinkExtractor(allow=r'/view/'), callback='parse_item') \n    ) \n\n    def parse_item():\n        i = CountryItem()\n        ...\n\n```", "```py\n$ scrapy crawl country -s LOG_LEVEL=DEBUG\n...\n2017-03-24 11:52:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET     http://example.webscraping.com/view/Belize-23> (referer: http://example.webscraping.com/index/2)\n2017-03-24 11:52:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.webscraping.com/view/Belgium-22> (referer: http://example.webscraping.com/index/2)\n2017-03-24 11:52:53 [scrapy.extensions.logstats] INFO: Crawled 40 pages (at 10 pages/min), scraped 0 items (at 0 items/min)\n2017-03-24 11:52:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.webscraping.com/user/login?_next=%2Findex%2F0> (referer: http://example.webscraping.com/index/0)\n2017-03-24 11:53:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.webscraping.com/user/register?_next=%2Findex%2F0> (referer: http://example.webscraping.com/index/0)\n... \n\n```", "```py\n    rules = ( \n        Rule(LinkExtractor(allow=r'/index/', deny=r'/user/'), follow=True), \n        Rule(LinkExtractor(allow=r'/view/', deny=r'/user/'), callback='parse_item') \n    ) \n\n```", "```py\n2017-03-24 11:56:03 [scrapy.crawler] INFO: Received SIG_SETMASK, shutting down gracefully. Send again to force \n\n```", "```py\n$ scrapy shell http://example.webscraping.com/view/United-Kingdom-239 ...\n[s] Available Scrapy objects:\n[s] scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n[s] crawler    <scrapy.crawler.Crawler object at 0x7fd18a669cc0>\n[s] item       {}\n[s] request    <GET http://example.webscraping.com/view/United-Kingdom-239>\n[s] response   <200 http://example.webscraping.com/view/United-Kingdom-239>\n[s] settings   <scrapy.settings.Settings object at 0x7fd189655940>\n[s] spider     <CountrySpider 'country' at 0x7fd1893dd320>\n[s] Useful shortcuts:\n[s] fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n[s] fetch(req)                  Fetch a scrapy.Request and update local objects \n[s] shelp()                     Shell help (print this help)\n[s] view(response)              View response in a browser\nIn [1]: \n\n```", "```py\nIn [1]: response.url \nOut[1]:'http://example.webscraping.com/view/United-Kingdom-239' \nIn [2]: response.status \nOut[2]: 200 \n\n```", "```py\nIn [3]: response.css('tr#places_country__row td.w2p_fw::text') \n[<Selector xpath=u\"descendant-or-self:: \n tr[@id = 'places_country__row']/descendant-or-self:: \n */td[@class and contains( \n concat(' ', normalize-space(@class), ' '), \n ' w2p_fw ')]/text()\" data=u'United Kingdom'>] \n\n```", "```py\nIn [4]: name_css = 'tr#places_country__row td.w2p_fw::text' \n\nIn [5]: response.css(name_css).extract() \nOut[5]: [u'United Kingdom'] \n\nIn [6]: pop_xpath = '//tr[@id=\"places_population__row\"]/td[@class=\"w2p_fw\"]/text()' \n\nIn [7]: response.xpath(pop_xpath).extract()\nOut[7]: [u'62,348,447']\n\n```", "```py\ndef parse_item(self, response): \n    item = CountryItem() \n    name_css = 'tr#places_country__row td.w2p_fw::text' \n    item['name'] = response.css(name_css).extract() \n    pop_xpath = '//tr[@id=\"places_population__row\"]/td[@class=\"w2p_fw\"]/text()'\n    item['population'] = response.xpath(pop_xpath).extract() \n    return item\n\n```", "```py\nclass CountrySpider(CrawlSpider): \n    name = 'country' \n    start_urls = ['http://example.webscraping.com/'] \n    allowed_domains = ['example.webscraping.com'] \n    rules = ( \n        Rule(LinkExtractor(allow=r'/index/', deny=r'/user/'), follow=True), \n        Rule(LinkExtractor(allow=r'/view/', deny=r'/user/'), callback='parse_item') \n    ) \n\n    def parse_item(self, response): \n        item = CountryItem() \n        name_css = 'tr#places_country__row td.w2p_fw::text' \n        item['name'] = response.css(name_css).extract()\n        pop_xpath = '//tr[@id=\"places_population__row\"]/td[@class=\"w2p_fw\"]/text()'\n        item['population'] = response.xpath(pop_xpath).extract() \n        return item\n\n```", "```py\n$ scrapy crawl country --output=../../../data/scrapy_countries.csv -s LOG_LEVEL=INFO 2017-03-24 14:20:25 [scrapy.extensions.logstats] INFO: Crawled 277 pages (at 10 pages/min), scraped 249 items (at 9 items/min)\n2017-03-24 14:20:42 [scrapy.core.engine] INFO: Closing spider (finished)\n2017-03-24 14:20:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 158580,\n 'downloader/request_count': 280,\n 'downloader/request_method_count/GET': 280,\n 'downloader/response_bytes': 944210,\n 'downloader/response_count': 280,\n 'downloader/response_status_count/200': 280,\n 'dupefilter/filtered': 61,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2017, 3, 24, 13, 20, 42, 792220),\n 'item_scraped_count': 252,\n 'log_count/INFO': 35,\n 'request_depth_max': 26,\n 'response_received_count': 280,\n 'scheduler/dequeued': 279,\n 'scheduler/dequeued/memory': 279,\n 'scheduler/enqueued': 279,\n 'scheduler/enqueued/memory': 279,\n 'start_time': datetime.datetime(2017, 3, 24, 12, 52, 25, 733163)}\n2017-03-24 14:20:42 [scrapy.core.engine] INFO: Spider closed (finished) \n\n```", "```py\nname,population \nAfghanistan,\"29,121,286\" \nAntigua and Barbuda,\"86,754\" \nAntarctica,0 \nAnguilla,\"13,254\" \nAngola,\"13,068,161\" \nAndorra,\"84,000\" \nAmerican Samoa,\"57,881\" \nAlgeria,\"34,586,184\" \nAlbania,\"2,986,952\" \nAland Islands,\"26,711\" \n... \n\n```", "```py\n$ scrapy crawl country -s LOG_LEVEL=DEBUG -s JOBDIR=../../../data/crawls/country\n...\n2017-03-24 13:41:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.webscraping.com/view/Anguilla-8> (referer: http://example.webscraping.com/)\n2017-03-24 13:41:54 [scrapy.core.scraper] DEBUG: Scraped from <200 http://example.webscraping.com/view/Anguilla-8>\n{'name': ['Anguilla'], 'population': ['13,254']}\n2017-03-24 13:41:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.webscraping.com/view/Angola-7> (referer: http://example.webscraping.com/)\n2017-03-24 13:41:59 [scrapy.core.scraper] DEBUG: Scraped from <200 http://example.webscraping.com/view/Angola-7>\n{'name': ['Angola'], 'population': ['13,068,161']}\n2017-03-24 13:42:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.webscraping.com/view/Andorra-6> (referer: http://example.webscraping.com/)\n2017-03-24 13:42:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://example.webscraping.com/view/Andorra-6>\n{'name': ['Andorra'], 'population': ['84,000']}\n^C2017-03-24 13:42:10 [scrapy.crawler] INFO: Received SIG_SETMASK, shutting down gracefully. Send again to force \n...\n[country] INFO: Spider closed (shutdown)\n\n```", "```py\n$ ls ../../../data/crawls/country/\nrequests.queue requests.seen spider.state\n\n```", "```py\n$ scrapy crawl country -s LOG_LEVEL=DEBUG -s JOBDIR=../../../data/crawls/country\n...\n2017-03-24 13:49:49 [scrapy.core.engine] INFO: Spider opened\n2017-03-24 13:49:49 [scrapy.core.scheduler] INFO: Resuming crawl (13 requests scheduled)\n2017-03-24 13:49:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2017-03-24 13:49:49 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2017-03-24 13:49:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.webscraping.com/robots.txt> (referer: None)\n2017-03-24 13:49:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.webscraping.com/view/Cameroon-40> (referer: http://example.webscraping.com/index/3)\n2017-03-24 13:49:54 [scrapy.core.scraper] DEBUG: Scraped from <200 http://example.webscraping.com/view/Cameroon-40>\n{'name': ['Cameroon'], 'population': ['19,294,149']}\n...\n\n```", "```py\n$ docker run -v ~/portia_projects:/app/data/projects:rw -p 9001:9001 scrapinghub/portia:portia-2.0.7\nUnable to find image 'scrapinghub/portia:portia-2.0.7' locally\nlatest: Pulling from scrapinghub/portia ...\n2017-03-28 12:57:42.711720 [-] Site starting on 9002\n2017-03-28 12:57:42.711818 [-] Starting factory <slyd.server.Site instance at 0x7f57334e61b8>\n\n```", "```py\npip install scrapely\n\n```", "```py\n\n>>> from scrapely import Scraper\n>>> s = Scraper()\n>>> train_url = 'http://example.webscraping.com/view/Afghanistan-1'\n>>> s.train(train_url, {'name': 'Afghanistan', 'population': '29,121,286'})\n>>> test_url = 'http://example.webscraping.com/view/United-Kingdom-239'\n>>> s.scrape(test_url)\n[{u'name': [u'United Kingdom'], u'population': [u'62,348,447']}]\n\n```"]