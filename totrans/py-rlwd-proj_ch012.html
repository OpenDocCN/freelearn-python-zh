<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<meta charset="utf-8"/>
<meta content="pandoc" name="generator"/>
<title>ch012.xhtml</title>

<!-- kobo-style -->

<style id="koboSpanStyle" type="text/css" xmlns="http://www.w3.org/1999/xhtml">.koboSpan { -webkit-text-combine: inherit; }</style>
</head>
<body epub:type="bodymatter">

<h1 data-number="12">Chapter 8<br/>
Project 2.5: Schema and Metadata</h1>
<p>It helps to keep the data schema separate from the various applications that share the schema. One way to do this is to have a separate module with class definitions that all of the applications in a suite can share. While this is helpful for a simple project, it can be awkward when sharing data schema more widely. A Python language module is particularly difficult for sharing data outside the Python environment.</p>
<p>This project will define a schema in JSON Schema Notation, first by building <code>pydantic</code> class definitions, then by extracting the JSON from the class definition. This will allow you to publish a formal definition of the data being created. The schema can be used by a variety of tools to validate data files and assure that the data is suitable for further analytical use.</p>
<p>The schema is also useful for diagnosing problems with data sources. Validator tools like <code>jsonschema</code> can provide detailed error reports that can help identify changes in source data from bug fixes or software updates.</p>
<p>This chapter will cover a number of skills related to data inspection techniques:</p>
<ul>
<li><p>Using the <strong>Pydantic </strong>module for crisp, complete definitions</p></li>
<li><p>Using JSON Schema to create an exportable language-independent definition that anyone can use</p></li>
<li><p>Creating test scenarios to use the formal schema definition</p></li>
</ul>
<p>We’ll start by looking at the reasons why a formal schema is helpful. </p>

<h2 data-number="12.1">8.1  Description</h2>
<p>Data validation is a common requirement when moving data between applications. It is extremely helpful to have a clear definition of what constitutes valid data. It helps even more when the definition exists outside a particular programming language or platform.</p>
<p>We can use the JSON Schema (<a class="url" href="https://json-schema.org">https://json-schema.org</a>) to define a schema that applies to the intermediate documents created by the acquisition process. Using JSON Schema enables the confident and reliable use of the JSON data format.</p>
<p>The JSON Schema definition can be shared and reused within separate Python projects and with non-Python environments, as well. It allows us to build data quality checks into the acquisition pipeline to positively affirm the data really fit the requirements for analysis and processing.</p>
<p>Additional metadata provided with a schema often includes the provenance of the data and details on how attribute values are derived. This isn’t a formal part of a JSON Schema, but we can add some details to the JSON Schema document that includes provenance and processing descriptions.</p>
<p>The subsequent data cleaning projects should validate the input documents using a source schema. Starting with <a href="ch013.xhtml#x1-2080009"><em>Chapter</em><em> 9</em></a>, <a href="ch013.xhtml#x1-2080009"><em>Project 3.1: Data Cleaning Base</em> <em>Application</em></a>, the applications should validate their output using the target analytic schema. It can seem silly to have an application both create sample records and also validate those records against a schema. What’s important is the schema will be shared, and evolve with the needs of consumers of the data. The data acquisition and cleaning operations, on the other hand, evolve with the data sources. It is all too common for an ad hoc solution to a data problem to seem good but create invalid data.</p>
<p>It rarely creates new problems to validate inputs as well as outputs against a visible, agreed-upon schema. There will be some overhead to the validation operation, but much of the processing cost is dominated by the time to perform input and output, not data validation.</p>
<p>Looking forward to <a href="ch016.xhtml#x1-27600012"><em>Chapter</em><em> 12</em></a>, <a href="ch016.xhtml#x1-27600012"><em>Project 3.8: Integrated Data Acquisition Web</em> <em>Service</em></a>, we’ll see additional uses for a formally-defined schema. We’ll also uncover a small problem with using JSON Schema to describe ND JSON documents. For now, we’ll focus on the need to use JSON Schema to describe data.</p>
<p>We’ll start by adding some modules to make it easier to create JSON Schema documents. </p>


<h2 data-number="12.2">8.2  Approach</h2>
<p>First, we’ll need some additional modules. The <code>jsonschema</code> module defines a validator that can be used to confirm a document matches the defined schema.</p>
<p>Additionally, the <strong>Pydantic </strong>module provides a way to create class definitions that can emit JSON Schema definitions, saving us from having to create the schema manually. In most cases, manual schema creation is not terribly difficult. For some cases, though, the schema and the validation rules might be challenging to write directly, and having Python class definitions available can simplify the process.</p>
<p>This needs to be added to the <code>requirements-dev.txt</code> file so other developers know to install it.</p>
<p>When using <strong>conda </strong>to manage virtual environments, the command might look like the following:</p>
<div><div><pre class="console">% conda install jsonschema pydantic</pre>
</div>
</div>
<p>When using other tools to manage virtual environments, the command might look like the following:</p>
<pre class="console">% python -m pip install jupyterlab</pre>
<p>The JSON Schema package requires some supplemental type stubs. These are used by the <strong>mypy </strong>tool to confirm the application is using types consistently. Use the following command to add stubs:</p>
<div><div><pre class="console">% mypy --install-types</pre>
</div>
</div>
<p>Additionally, the <code>pydantic</code> package includes a <strong>mypy </strong>plug-in that will extend the type-checking capabilities of <strong>mypy</strong>. This will spot more nuanced potential problems with classes defined using <code>pydantic</code>.</p>
<p>To enable the plugin, add <code>pydantic.mypy</code> to the list of plugins in the <strong>mypy </strong>configuration file, <code>mypy.ini</code>. The <code>mypy.ini</code> file should look like this:</p>
<pre class="console">[mypy]
plugins = pydantic.mypy</pre>
<p>(This file goes in the root of the project directory.)</p>
<p>This plugin is part of the <strong>pydantic </strong>download, and is compatible with <strong>mypy</strong> versions starting with 0.910.</p>
<p>With these two packages, we can define classes with details that can be used to create JSON Schema files. Once we have a JSON Schema file, we can use the schema definition to confirm that sample data is valid.</p>
<p>For more information on <strong>Pydantic</strong>, see <a class="url" href="https://docs.pydantic.dev">https://docs.pydantic.dev</a>.</p>
<p>The core concept is to use <strong>Pydantic </strong>to define dataclasses with detailed field definitions. These definitions can be used for data validation in Python. The definition can also be used to emit a JSON Schema document to share with other projects.</p>
<p>The schema definitions are also useful for defining an OpenAPI specification. In <a href="ch016.xhtml#x1-27600012"><em>Chapter</em><em> 12</em></a>, <a href="ch016.xhtml#x1-27600012"><em>Project 3.8: Integrated Data Acquisition Web Service</em></a>, we’ll turn to creating a web service that provides data. The OpenAPI specification for this service will include the schema definitions from this project.</p>
<p>The use of <strong>Pydantic </strong>isn’t required. It is, however, very convenient for creating a schema that can be described via JSON Schema. It saves a great deal of fussing with details in JSON syntax.</p>
<p>We’ll start with using <strong>Pydantic </strong>to create a useful data model module. This will extend the data models built for projects in earlier chapters. </p>

<h3 data-number="12.2.1">8.2.1  Define Pydantic classes and emit the JSON Schema</h3>
<p>We’ll start with two profound modifications to the data model definitions used in earlier chapters. One change is to switch from the <code>dataclasses</code> module to the <code>pydantic.dataclasses</code> module. Doing this creates the need to explicitly use <code>dataclasses.field</code> for individual field definitions. This is generally a small change to an <code>import</code> statement to use <code>from</code><code> pydantic.dataclasses</code><code> import</code><code> dataclass</code>. The dataclasses <code>field()</code> function will need some changes, also, to add additional details used by <strong>pydantic</strong>. The changes should be completely transparent to the existing application; all tests will pass after these changes.</p>
<p>The second change is to add some important metadata to the classes. Where the <code>dataclasses.field(...)</code> definition is used, the <code>metadata={}</code> attribute can be added to include a dictionary with JSON Schema attributes like the description, title, examples, valid ranges of values, etc. For other fields, the <code>pydantic.Field()</code> function must be used to provide a title, description, and other constraints on the field. This will generate a great deal of metadata for us.</p>
<p>See <a class="url" href="https://docs.pydantic.dev/usage/schema/#field-customization">https://docs.pydantic.dev/usage/schema/#field-customization</a> for the wide variety of field definition details available.</p>
<div><div><pre class="source-code">from pydantic import Field
from pydantic.dataclasses import dataclass

@dataclass
class SeriesSample:
    """
    An individual sample value.
    """
    x: float = Field(title="The x attribute", ge=0.0)
    y: float = Field(title="The y attribute", ge=0.0)

@dataclass
class Series:
    """
    A named series with a collection of values.
    """
    name: str = Field(title="Series name")
    samples: list[SeriesSample] = Field(title="Sequence of samples
      in this series")</pre>
</div>
</div>
<p>We’ve provided several additional details in this model definition module. The details include:</p>
<ul>
<li><p>Docstrings on each class. These will become descriptions in the JSON Schema.</p></li>
<li><p>Fields for each attribute. These, too, become descriptions in the JSON Schema.</p></li>
<li><p>For the <code>x</code> and <code>y</code> attributes of the <code>SeriesSample</code> class definition, we added a <code>ge</code> value. This is a range specification, requiring the values to be greater than or equal to zero.</p></li>
</ul>
<p>We’ve also made extremely profound changes to the model: we’ve moved from the source data description — which was a number of <code>str</code> values — to the target data description, using <code>float</code> values.</p>
<p>What’s central here is that we have two variations on each model:</p>
<ul>
<li><p><strong>Acquisition</strong>: This is the data as we find it ”in the wild.” In the examples in this book, some variations of source data are text-only, forcing us to use <code>str</code> as a common type. Some data sources will have data in more useful Python objects, permitting types other than <code>str</code>.</p></li>
<li><p><strong>Analysis</strong>: This is the data used for further analysis. These data sets can use native Python objects. For the most part, we’ll focus on objects that are easily serialized to JSON. The exception will be date-time values, which don’t readily serialize to JSON, but require some additional conversion from a standard ISO text format.</p></li>
</ul>
<p>The class examples shown above do not <em>replace </em>the <code>model</code> module in our applications. They form a second model of more useful data. The recommended approach is to change the initial acquisition model’s module name from <code>model</code> to <code>acquisition_model</code> (or perhaps the shorter <code>source_model</code>). This property describes the model with mostly string values as the source. This second model is the <code>analysis_model</code>.</p>
<p>The results of the initial investigation into the data can provide narrower and more strict constraints for the analysis model class definitions. See <a href="ch011.xhtml#x1-1610007"><em>Chapter</em><em> 7</em></a>, <a href="ch011.xhtml#x1-1610007"><em>Data Inspection Features</em></a> for a number of inspections that can help to reveal expected minima and maxima for attribute values.</p>
<p>The <strong>Pydantic </strong>library comes with a large number of customized data types that can be used to describe data values. See <a class="url" href="https://docs.pydantic.dev/usage/types/">https://docs.pydantic.dev/usage/types/</a> for documentation. Using the <code>pydantic</code> types can be simpler than defining an attribute as a string, and trying to create a regular expression for valid values.</p>
<p>Note that validation of source values isn’t central to <strong>Pydantic</strong>. When Python objects are provided, it’s entirely possible for the <strong>Pydantic </strong>module to perform a successful data conversion where we might have hoped for an exception to be raised. A concrete example is providing a Python <code>float</code> object to a field that requires an <code>int</code> value. The <code>float</code> object will be converted; an exception will <em>not </em>be raised. If this kind of very strict validation of Python objects is required, some additional programming is needed.</p>
<p>In the next section, we’ll create a JSON Schema definition of our model. We can either export the definition from the class definition, or we can craft the JSON manually. </p>


<h3 data-number="12.2.2">8.2.2  Define expected data domains in JSON Schema notation</h3>
<p>Once we have the class definition, we can then export a schema that describes the class. Note that the <strong>Pydantic </strong>dataclass is a wrapper around an underlying <code>pydantic.BaseModel</code> subclass definition.</p>
<p>We can create a JSON Schema document by adding the following lines to the bottom of the module:</p>
<div><div><pre class="source-code">from pydantic import schema_of
import json

if __name__ == "__main__":
    schema = schema_of(Series)
    print(json.dumps(schema, indent=2))</pre>
</div>
</div>
<p>These lines turn the data definition module into a script that writes the JSON Schema definition to the standard output file.</p>
<p>The <code>schema_of()</code> function will extract a schema from the dataclass created in the previous section. (See <a href="#x1-1980001"><em>Define Pydantic classes and emit the JSON Schema</em></a>.) The underlying <code>pydantic.BaseModel</code> subclass also has a <code>schema()</code> method that will transform the class definition into a richly-detailed JSON Schema definition. When working with <strong>pydantic </strong>dataclasses, the <code>pydantic.BaseModel</code> isn’t directly available, and the <code>schema_of()</code> function must be used.</p>
<p>When executing the terminal command <code>python</code><code> src/analysis_model.py</code>, the schema is displayed.</p>
<p>The output begins as follows:</p>
<div><div><pre class="source-code">{
  "title": "Series",
  "description": "A named series with a collection of values.",
  "type": "object",
  "properties": {
    "name": {
      "title": "Series name",
      "type": "string"
    },
    "samples": {
      "title": "Sequence of samples in this series",
      "type": "array",
      "items": {
        "\$ref": "#/definitions/SeriesSample"
      }
    }
  },
  "required": [
    "name",
    "samples"
  ],
  ...
}</pre>
</div>
</div>
<p>We can see that the title matches the class name. The description matches the docstring. The collection of properties matches the attributes’ names in the class. Each of the property definitions provides the type information from the dataclass.</p>
<p>The <code>$ref</code> item is a reference to another definition provided later in the JSON Schema. This use of references makes sure the other class definition is separately visible, and is available to support this schema definition.</p>
<p>A very complex model may have a number of definitions that are shared in multiple places. This <code>$ref</code> technique normalizes the structure so only a single definition is provided. Multiple references to the single definition assure proper reuse of the class definition.</p>
<p>The JSON structure may look unusual at first glance, but it’s not frighteningly complex. Reviewing <a class="url" href="https://json-schema.org">https://json-schema.org</a> will provide information on how best to create JSON Schema definitions without using the <strong>Pydantic</strong> module. </p>


<h3 data-number="12.2.3">8.2.3  Use JSON Schema to validate intermediate files</h3>
<p>Once we have a JSON Schema definition, we can provide it to other stakeholders to be sure they understand the data required or the data provided. We can also use the JSON Schema to create a validator that can examine a JSON document and determine if the document really does match the schema.</p>
<p>We can do this with a <code>pydantic</code> class definition. There’s a <code>parse_obj()</code> method that will examine a dictionary to create an instance of the given <code>pydantic</code> class could be built. The <code>parse_raw()</code> method can parse a string or bytes object to create an instance of the given class.</p>
<p>We can also do this with the <code>jsonschema</code> module. We’ll look at this as an alternative to <code>pydantic</code> to show how sharing the JSON Schema allows other applications to work with a formal definition of the analysis model.</p>
<p>First, we need to create a validator from the schema. We can dump the JSON into a file and then load the JSON back from the file. We can also save a step by creating a validator directly from the <strong>Pydantic</strong>-created JSON Schema. Here’s the short version:</p>
<div><div><pre class="source-code">from pydantic import schema_of
from jsonschema.validators import Draft202012Validator
from analysis_model import *

schema = schema_of(SeriesSample)
validator = Draft202012Validator(schema)</pre>
</div>
</div>
<p>This creates a validator using the latest version of JSON Schema, the 2020 draft. (The project is on track to become a standard, and has gone through a number of drafts as it matures.)</p>
<p>Here’s how we might write a function to scan a file to be sure the NDJSON documents all properly fit the defined schema:</p>
<div><div><pre class="source-code">def validate_ndjson_file(
        validator: Draft202012Validator,
        source_file: TextIO
) -&gt; Counter[str]:
    counts: Counter[str] = Counter()
    for row in source_file:
        document = json.loads(row)
        if not validator.is_valid(document):
            errors = list(validator.iter_errors(document))
            print(document, errors)
            counts[’faulty’] += 1
        else:
            counts[’good’] += 1
    return counts</pre>
</div>
</div>
<p>This function will read each NDJSON document from the given source file. It will use the given validator to see if the document has problems or is otherwise valid. For faulty documents, it will print the document and the entire list of validation errors.</p>
<p>This kind of function can be embedded into a separate script to check files.</p>
<p>We can, similarly, create the schema for the source model, and use JSON Schema (or <strong>Pydantic</strong>) to validate source files before attempting to process them.</p>
<p>We’ll turn to the more complete validation and cleaning solution in <a href="ch013.xhtml#x1-2080009"><em>Chapter</em><em> 9</em></a>, <a href="ch013.xhtml#x1-2080009"><em>Project 3.1: Data Cleaning Base Application</em></a>. This project is one of the foundational components of the more complete solution.</p>
<p>We’ll look at the deliverables for this project in the next section. </p>



<h2 data-number="12.3">8.3  Deliverables</h2>
<p>This project has the following deliverables:</p>
<ul>
<li><p>A <code>requirements.txt</code> file that identifies the tools used, usually <code>pydantic==1.10.2</code> and <code>jsonschema==4.16.0</code>.</p></li>
<li><p>Documentation in the <code>docs</code> folder.</p></li>
<li><p>The JSON-format files with the source and analysis schemas. A separate <code>schema</code> directory is the suggested location for these files.</p></li>
<li><p>An acceptance test for the schemas.</p></li>
</ul>
<p>We’ll look at the schema acceptance test in some detail. Then we’ll look at using schema to extend other acceptance tests. </p>

<h3 data-number="12.3.1">8.3.1  Schema acceptance tests</h3>
<p>To know if the schema is useful, it is essential to have acceptance test cases. As new sources of data are integrated into an application, and old sources of data mutate through ordinary bug fixes and upgrades, files will change. The new files will often cause problems, and the root cause of the problem will be the unexpected file format change.</p>
<p>Once a file format change is identified, the smallest relevant example needs to be transformed into an acceptance test. The test will — of course — fail. Now, the data acquisition pipeline can be fixed knowing there is a precise definition of done.</p>
<p>To start with, the acceptance test suite should have an example file that’s valid and an example file that’s invalid.</p>
<p>As we noted in <a href="ch008.xhtml#x1-780004"><em>Chapter</em><em> 4</em></a>, <a href="ch008.xhtml#x1-780004"><em>Data Acquisition Features: Web APIs and Scraping</em></a>, we can provide a large block of text as part of a Gherkin scenario. We can consider something like the following scenario:</p>
<div><div><pre class="source-code">Scenario: Valid file is recognized.
    Given a file "example_1.ndjson" with the following content
        """
        {"x": 1.2, "y": 3.4}
        {"x": 5.6, "y": 7.8}
        """
    When the schema validation tool is run with the analysis schema
    Then the output shows 2 good records
    And the output shows 0 faulty records</pre>
</div>
</div>
<p>This allows us to provide the contents for an NDJSON file. The HTML extract command is quite long. The content is available as the <code>context.text</code> parameter of the step definition function. See <a href="ch008.xhtml#x1-1050004"><em>Acceptance tests</em></a> for more examples of how to write the step definitions to create a temporary file to be used for this test case.</p>
<p>Scenarios for faulty records are also essential, of course. It’s important to be sure the schema definition will reject invalid data. </p>


<h3 data-number="12.3.2">8.3.2  Extended acceptance testing</h3>
<p>In <em>Chapters 3</em>, <em>4</em>, and <em>5</em>, we wrote acceptance tests that — generally — looked at log summaries of the application’s activity to be sure it properly acquired source data. We did not write acceptance tests that specifically looked at the data.</p>
<p>Testing with a schema definition permits a complete analysis of each and every field and record in a file. The completeness of this check is of tremendous value.</p>
<p>This means that we can add some additional Then steps to existing scenarios. They might look like the following:</p>
<div><div><pre class="source-code">    # Given (shown earlier)...
    # When (shown earlier)...
    Then the log has an INFO line with "header: [’x’, ’y’]"
    And log has INFO line with "Series_1 count: 11"
    And log has INFO line with "Series_2 count: 11"
    And log has INFO line with "Series_3 count: 11"
    And log has INFO line with "Series_4 count: 11"
    And the output directory files are valid
        using the "schema/Anscombe_Source.json" schema</pre>
</div>
</div>
<p>The additional ”Then the output directory files are valid...” line requires a step definition that must do the following things:</p>
<ol>
<li><div><p>Load the named JSON Schema file and build a <code>Validator</code>.</p>
</div></li>
<li><div><p>Use the <code>Validator</code> object to examine each line of the ND JSON file to be sure they’re valid.</p>
</div></li>
</ol>
<p>This use of the schema as part of the acceptance test suite will parallel the way data suppliers and data consumers can use the schema to assure the data files are valid.</p>
<p>It’s important to note the schema definition given earlier in this chapter (in <a href="#x1-1980001"><em>Define Pydantic classes and emit the JSON Schema</em></a>) was the output from a future project’s data cleaning step. The schema shown in that example is not the output from the previous data acquisition applications.</p>
<p>To validate the output from data acquisition, you will need to use the model for the various data acquisition projects in <em>Chapters 3</em>, <em>4</em>, and <em>5</em>. This will be <strong>very</strong> similar to the example shown earlier in this chapter. While similar, it will differ in a profound way: it will use <code>str</code> instead of <code>float</code> for the series sample attribute values. </p>



<h2 data-number="12.4">8.4  Summary</h2>
<p>This chapter’s projects have shown examples of the following features of a data acquisition application:</p>
<ul>
<li><p>Using the Pydantic module for crisp, complete definitions</p></li>
<li><p>Using JSON Schema to create an exportable language-independent definition that anyone can use</p></li>
<li><p>Creating test scenarios to use the formal schema definition</p></li>
</ul>
<p>Having formalized schema definitions permits recording additional details about the data processing applications and the transformations applied to the data.</p>
<p>The docstrings for the class definitions become the descriptions in the schema. This permits writing details on data provenance and transformation that are exposed to all users of the data.</p>
<p>The JSON Schema standard permits recording examples of values. The <strong>Pydantic </strong>package has ways to include this metadata in field definitions, and class configuration objects. This can be helpful when explaining odd or unusual data encodings.</p>
<p>Further, for text fields, JSONSchema permits including a format attribute that can provide a regular expression used to validate the text. The <strong>Pydantic</strong> package has first-class support for this additional validation of text fields.</p>
<p>We’ll return to the details of data validation in <a href="ch013.xhtml#x1-2080009"><em>Chapter</em><em> 9</em></a>, <a href="ch013.xhtml#x1-2080009"><em>Project 3.1: Data</em> <em>Cleaning Base Application</em></a> and <a href="ch014.xhtml#x1-22900010"><em>Chapter</em><em> 10</em></a>, <a href="ch014.xhtml#x1-22900010"><em>Data Cleaning Features</em></a>. In those chapters, we’ll delve more deeply into the various <strong>Pydantic </strong>validation features. </p>


<h2 data-number="12.5">8.5  Extras</h2>
<p>Here are some ideas for you to add to this project. </p>

<h3 data-number="12.5.1">8.5.1  Revise all previous chapter models to use Pydantic</h3>
<p>The previous chapters used <code>dataclass</code> definitions from the <code>dataclasses</code> module. These can be shifted to use the <code>pydantic.dataclasses</code> module. This should have minimal impact on the previous projects.</p>
<p>We can also shift all of the previous acceptance test suites to use a formal schema definition for the source data.</p>
<p></p>


<h3 data-number="12.5.2">8.5.2  Use the ORM layer</h3>
<p>For SQL extracts, an ORM can be helpful. The <code>pydantic</code> module lets an application create Python objects from intermediate ORM objects. This two-layer processing seems complex but permits detailed validation in the <strong>Pydantic </strong>objects that aren’t handled by the database.</p>
<p>For example, a database may have a numeric column without any range provided. A <strong>Pydantic </strong>class definition can provide a field definition with <code>ge</code> and <code>le</code> attributes to define a range. Further, <strong>Pydantic </strong>permits the definition of a unique data type with unique validation rules that can be applied to database extract values.</p>
<p>First, see <a class="url" href="https://docs.sqlalchemy.org/en/20/orm/">https://docs.sqlalchemy.org/en/20/orm/</a> for information on the SQLAlchemy ORM layer. This provides a class definition from which SQL statements like <code>CREATE</code><code> TABLE</code>, <code>SELECT</code>, and <code>INSERT</code> can be derived.</p>
<p>Then, see the <a class="url" href="https://docs.pydantic.dev/usage/models/#orm-mode-aka-arbitrary-class-instances">https://docs.pydantic.dev/usage/models/#orm-mode-aka-arbitrary-class-instances</a> ”ORM Mode (aka Arbitrary Class Instances)” section of the <strong>Pydantic</strong> documentation for ways to map a more useful class to the intermediate ORM class.</p>
<p>For legacy data in a quirky, poorly-designed database, this can become a problem. For databases designed from the beginning with an ORM layer, on the other hand, this can be a simplification to the SQL. </p>



</body>
</html>
