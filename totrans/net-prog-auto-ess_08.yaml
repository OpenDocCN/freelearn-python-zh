- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaling Your Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how to interact with network devices, we should start thinking
    about building a solution that scales. But why do we need to scale our code? You
    might be thinking that the answer is obvious and it is just because it will allow
    your solution to grow easily as your network grows. But scaling is not just about
    going up but scaling down too. So, scaling your code means that you are going
    to build a solution that can follow demand easily, saving resources when not required
    and using more when required.
  prefs: []
  type: TYPE_NORMAL
- en: You should consider adding scaling capabilities adding scaling capabilities
    to your network automation solution before writing the code. It should be planned
    during design time and then executed during development time. The scaling capabilities
    have to be one of the requirements for building your solution. It also should
    be a clear milestone during implementation and testing.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to check some techniques used today to scale your
    code up and down effectively. This will allow your solution to adapt easily to
    follow network growth and, if necessary, easily scale down to save resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with multitasking, threads, and coroutines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding schedulers and job dispatchers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using microservices and containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you should have enough information to choose the
    best scaling solution for your code.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The source code described in this chapter is stored in the GitHub repository
    at [https://github.com/PacktPublishing/Network-Programming-and-Automation-Essentials/tree/main/Chapter08](https://github.com/PacktPublishing/Network-Programming-and-Automation-Essentials/tree/main/Chapter08).
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with multitasking, threads, and coroutines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multitasking, as the name suggests, is the capability of doing several tasks
    at the same time. In computers, a task is also known as a job or a process and
    there are different techniques for running tasks at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: The capability to run code at the same time allows your system to scale up and
    down whenever necessary. If you have to communicate with more network devices,
    just run more code in parallel; if you need fewer devices, just run less code.
    That will enable your system to scale up and down.
  prefs: []
  type: TYPE_NORMAL
- en: But running code in parallel will have an impact on the available machine resources,
    and some of these resources will be limited by how your code is consuming them.
    For instance, if your code is using the network interface to download files, and
    running one single line of code is already consuming 50 Mbps of the network interface
    (which is 50% of the 100 Mbps interface), it is not advised to run multiple lines
    of code in parallel to increase the speed, as the limitation is on the network
    interface and not in the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Other factors are also important to be considered when running code in parallel,
    that is, the other shared resources besides the network, such as the CPU, disk,
    and memory. In some cases, bottlenecks in the disk might cause more limitations
    for code parallelism than the CPU, especially using disks mounted over the network.
    In other cases, a large program consuming lots of memory would block the execution
    of any other program running in parallel because of a lack of free memory. Therefore,
    the resources that your process will touch and how they interact will have an
    impact on how much parallelism is possible.
  prefs: []
  type: TYPE_NORMAL
- en: One thing we should clarify here is the term **I/O**, which is an acronym for
    computer **input/output**. I/O is used to designate any communication between
    the CPU of the machine and the external world, such as accessing the disk, writing
    to memory, or sending data to the network. If your code requires lots of external
    access and it is, most of the time, waiting to receive a response from external
    communication, we normally say the code is **I/O bound**. An example of slow I/O
    can be found when accessing remote networks and, in some cases, remote disks.
    On the other hand, if your code requires more CPU computation than I/O, we normally
    say the code is **CPU bound**. Most network automation systems will be I/O bound
    because of network device access.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now investigate a few techniques to run code at the same time in Go and
    Python.
  prefs: []
  type: TYPE_NORMAL
- en: Multiprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In computers, when a program is loaded in memory to run, it’s called a process.
    The program can be either a script or a binary file, but it is normally represented
    by one single file. This file will be loaded into memory and it is seen by the
    operating system as a process. The capability of running multiple processes at
    the same time is called multiprocessing, and it is normally managed by the operating
    system.
  prefs: []
  type: TYPE_NORMAL
- en: The number of CPUs of the hardware where the processes are running is irrelevant
    to the multiprocessing capability. The operating system is responsible for allocating
    the CPU time for all processes that are in memory and ready to run. However, as
    the number of CPUs, speed of the CPU, and memory are limited, the number of processes
    that can run at the same time will also be limited. Normally, it depends on the
    size of the process and how much CPU it consumes.
  prefs: []
  type: TYPE_NORMAL
- en: In most computer languages, multiprocessing is implemented using the `fork()`
    system call implemented by the operating system to create a complete copy of the
    currently running process.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s investigate how we can use multiprocessing in Go and Python.
  prefs: []
  type: TYPE_NORMAL
- en: Multiprocessing in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Python, multiprocessing is accomplished by the standard library called `multiprocessing`.
    Full documentation on Python `multiprocessing` can be found at [docs.python.org/3/library/multiprocessing](http://docs.python.org/3/library/multiprocessing).
  prefs: []
  type: TYPE_NORMAL
- en: In the first example, we are going to use the operating system program called
    `ping` to target one network node. Then, we are going to make it parallel for
    multiple targets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example for one target network node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'It is important to note that calling `ping` from Python is not efficient. It
    will cause more overhead because Python will have to invoke an external program
    that resides in the filesystem. In order to make the example more efficient, we
    need to use the ICMP `echo request` and receive an ICMP `echo reply` from the
    Python network sockets, instead of invoking an external program such as `ping`.
    One solution is to use the Python third-party library called `pythonping` ([https://pypi.org/project/pythonping/](https://pypi.org/project/pythonping/)).
    But there is one caveat: the `ping` program has `setuid` to allow ICMP packets
    to be sent by a non-privileged user. Thus, in order to run with `pythonping`,
    you need admin/root privileges (accomplished in Linux using `sudo`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the same example using `pythonping` for one target network
    node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this program should generate the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to send ICMP requests to multiple targets, you will have to send
    them sequentially one after the other. However, a better solution would be to
    run them in parallel using the `multiprocessing` Python library. The following
    is an example of four targets using `multiprocessing`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run the preceding program, you should get an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note that the response of each target does not depend on the response of others.
    Therefore, the output should always be in order from low latency to high latency.
    In the preceding example, `google.com` finished first, showing a latency of just
    45.31 ms.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: It is important to call `multiprocessing` inside the `main()` function, or a
    function that is called from `main()`. Also, make sure `main()` can be safely
    imported by a Python interpreter (use `__name__` ). You can find more details
    on why at [https://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming](https://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming).
  prefs: []
  type: TYPE_NORMAL
- en: Python has additional methods to invoke code parallelism besides the preceding
    example using `Process()`, called `multiprocessing.Pool()` and `multiprocessing.Queue()`.
    The `Pool()` class is used to instantiate a pool of workers that can do a job
    without the need to communicate with each other. The `Queue()` class is used when
    communication between processes is required. More on that can be found at [https://docs.python.org/3/library/multiprocessing.html](https://docs.python.org/3/library/multiprocessing.html).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how we can use multiprocessing in Go.
  prefs: []
  type: TYPE_NORMAL
- en: Multiprocessing in Go
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To create processes from a program, you need to create a copy of the data of
    the current running program to a new process. That is what Python’s `multiprocessing`
    does. However, Go implements parallelism very differently. Go was designed to
    work with routines similar to coroutines, they are called goroutines, which manage
    parallelism at runtime. As goroutines are much more efficient, there is no need
    to implement multiprocessing natively in Go.
  prefs: []
  type: TYPE_NORMAL
- en: Note that using the `exec` library, by calling `exec.Command()` and then `Cmd.Start()`
    and `Cmd.Wait()`, will allow you to create multiple processes at the same time,
    but it is a call to the operating system to execute an external program. Therefore,
    it is not considered native multiprocessing and is not efficient.
  prefs: []
  type: TYPE_NORMAL
- en: For these reasons, we don’t have an example of multi-processing in Go.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see now how we do multithreading.
  prefs: []
  type: TYPE_NORMAL
- en: Multithreading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In computer languages, a thread is a smaller part of a process, which can have
    one or multiple threads. The memory is shared between the threads in the same
    process, in contrast with a process that does not share memory with another process.
    Therefore, a thread is known as a lightweight process because it requires less
    memory, and communication between threads within a process is faster. In consequence,
    spawning new threads is much faster in comparison with new processes.
  prefs: []
  type: TYPE_NORMAL
- en: A CPU with multithreading capability is a CPU that has the ability to run multiple
    threads in a single core by providing instruction-level parallelism or thread-level
    parallelism. This capability is also known as **Simultaneous** **Multithreading**
    (**SMT**).
  prefs: []
  type: TYPE_NORMAL
- en: One example of SMT is the Intel CPU i9-10900K, which has 10 cores and the capability
    to run 2 threads at the same time per core, which allows up to 20 simultaneous
    threads. Intel has created a trademark name for SMT, which they call **hyper-threading**.
    Normally, AMD and Intel x86 CPU architectures can run up to two threads per core.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the Oracle SPARC M8 processor has 32 cores that can run 8 threads
    each, allowing a staggering number of 256 simultaneous threads. More on this amazing
    CPU can be found at [https://www.oracle.com/us/products/servers-storage/sparc-m8-processor-ds-3864282.pdf](https://www.oracle.com/us/products/servers-storage/sparc-m8-processor-ds-3864282.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: But for the CPU to perform its best using threads, two other requirements are
    necessary, an operating system that allows CPU-level multithreading and a computer
    language that allows the creation of simultaneous threads.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how we can use multithreading in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Multithreading in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multithreading is the Achilles’ heel of Python. The main reason is that the
    Python interpreter called CPython (discussed in [*Chapter 6*](B18165_06.xhtml#_idTextAnchor166))
    uses a **Global Interpreter Lock** (**GIL**) to make it thread-safe. This has
    a consequence of not allowing Python code to run multiple threads at the same
    time in a multithread CPU. GIL also adds overhead and using multithreading might
    cause the program to run slower in comparison with multiprocessing when more CPU
    work is required.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in Python, multithreading is not recommended for programs that are
    CPU bound. For network and other I/O-bound programs, multithreading might be faster
    to spawn and easier to communicate with and save runtime memory. But it is important
    to note that only one thread will run at a time using the CPython interpreter,
    so if you require true parallelism, use the `multiprocessing` library instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, the standard library offers multithreading by using the library
    called `threading`. So, let’s create one example using multithreading in Python
    by taking the same targets for ICMP tests used in the code example in the previous
    section. The following is the same example using ICMP but using threading:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of running the preceding program will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the output is quite similar using the `threading` and `multiprocessing`
    libraries, but which one runs faster?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now run a test program to compare the speed of using `threading` and `multiprocessing`
    for the ICMP tests. The source code of this program is included in the GitHub
    repository for this chapter. The name of the program is `performance-thread-process-example.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the output of this program running for 10, 20, 50, and 100 ICMP probes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding output, running multithreading in Python for a certain
    number of threads might be faster. However, as we get close to the number 50,
    it becomes less effective and runs much slower. It is important to notice that
    this will depend on where you are running your code. The Python interpreter running
    on Windows is different from in Linux or even in macOS, but the general idea is
    the same: more threads mean more overhead for the GIL.'
  prefs: []
  type: TYPE_NORMAL
- en: The recommendation is not to use Python multithreading unless you are spawning
    a small number of threads and are not CPU bound.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Because of the CPython GIL, it is not possible to run parallel threads in Python.
    Therefore, if your program is CPU bound and requires CPU parallelism, the way
    to go is to use the `multiprocessing` library instead of the `threading` library.
    More details can be found at [docs.python.org/3/library/threading](http://docs.python.org/3/library/threading).
  prefs: []
  type: TYPE_NORMAL
- en: But if you still want to use Python with multithreading, there are other Python
    interpreters that might provide some capability. One example is `threading` module.
    With PyPy-STM, it is possible for simultaneous threads to run, but you will have
    to use the `transaction` module, specifically the `TransactionQueue` class. More
    on multithreading using PyPy-STM can be found at [doc.pypy.org/en/latest/stm.html#user-guide](http://doc.pypy.org/en/latest/stm.html#user-guide).
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see how we can do multithreading in Go.
  prefs: []
  type: TYPE_NORMAL
- en: Multithreading in Go
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Writing code that scales in Go does not require the creation of threads or processes.
    Go implements parallelism very efficiently using goroutines, which are presented
    as threads to the operating system by the Go runtime. Goroutines will be explained
    in more detail in the following section, which talks about coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: We will also see how we can run multiple lines of code at the same time using
    coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The term *coroutine* was coined back in 1958 by Melvin Conway and Joel Erdwinn.
    Then, the idea was officially introduced in a paper published in the *ACM* magazine
    in 1963.
  prefs: []
  type: TYPE_NORMAL
- en: Despite being very old, the adoption of the term came later with some modern
    computer languages. Coroutines are essentially code that can be suspended. The
    concept is like a thread (in multithreading), because it is a small part of the
    code, and has local variables and its own stack. But the main difference between
    threads and coroutines in a multitasking system is threads can run in parallel
    and coroutines are collaborative. Some like to describe the difference as the
    same as between task concurrency and task parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the definition taken from *Oracle Multithreaded* *Programming Guide*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*In a multithreaded process on a single processor, the processor can switch
    execution resources between threads, resulting in concurrent execution. Concurrency
    indicates that more than one thread is making progress, but the threads are not
    actually running simultaneously. The switching between threads happens quickly
    enough that the threads might appear to run simultaneously. In the same multithreaded
    process in a shared-memory multiprocessor environment, each thread in the process
    can run concurrently on a separate processor, resulting in parallel execution,
    which is true* *simultaneous execution.*'
  prefs: []
  type: TYPE_NORMAL
- en: The source can be found at [https://docs.oracle.com/cd/E36784_01/html/E36868/mtintro-6.html](https://docs.oracle.com/cd/E36784_01/html/E36868/mtintro-6.html).
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s now check how we can use coroutines in Python and then in Go.
  prefs: []
  type: TYPE_NORMAL
- en: Adding coroutines in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Python has recently added coroutines to the standard library. They are part
    of the module called `asyncio`. Because of that, you won’t find this capability
    for older versions of Python; you need at least Python version 3.7.
  prefs: []
  type: TYPE_NORMAL
- en: But when do we use coroutines in Python? The best fit is when you require lots
    of parallel tasks that are I/O bound, such as a network. For CPU-bound applications,
    it is always recommended to use `multiprocessing` instead.
  prefs: []
  type: TYPE_NORMAL
- en: In comparison to `threading`, `asyncio` is more useful for our network automation
    work, because it is I/O bound and scales up more than using `threading`. In addition,
    it is even lighter than threads and processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s then create the same ICMP probe test using coroutines in Python. The
    following is an example of the code for the same network targets used in previous
    examples (you can find this code in `Chapter08/Python/asyncio-example.py` in the
    GitHub repo of the book):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the preceding program example will generate the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note that now, the first ping reply to be printed is not actually the one that
    has the least latency, which shows the program is running sequentially, following
    the order of the `TARGETS` variable in the loop. That means the `asyncio` coroutines
    are not being suspended to allow others to run when they are blocked. Therefore,
    this is not a good example of using coroutines if we want to scale up. This is
    because the library used in the example is `pythonping`, which is not `asyncio`
    compatible and is not suspending the coroutine when it is waiting for the network
    ICMP response.
  prefs: []
  type: TYPE_NORMAL
- en: We added this example to show how bad it is to use `asyncio` with coroutines
    that have code that is incompatible with `asyncio`. To fix this issue, let’s now
    use a third-party library for the ICMP probe that is compatible with `asyncio`,
    which is called `aioping`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code only shows the change on the import to add `aioping` instead
    of `pythonping` and the change on the `myping()` function, where we added an `await`
    statement before the `ping()`function. The other difference is that `aioping`
    works with the exception called `TimeoutError` to detect a non-response of an
    ICMP request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The complete program with the fixes shown previously can be found in the GitHub
    repository of this book at `Chapter08/Python/asyncio-example-fixed.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run this code now with the fix, it should show something like the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note that, now, the output is based on how fast the targets answer the ICMP
    request and the output does not follow the `TARGETS` list order like in the previous
    example.
  prefs: []
  type: TYPE_NORMAL
- en: The important difference in the preceding code is the usage of `await` before
    `ping`, which indicates to the Python `asyncio` module that the coroutine may
    stop and allow another coroutine to run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you may be wondering whether you could, instead of using the new library,
    `aioping`, just add `await` to the previous example in front of the `ping` statement
    in the `pythonping` library. But that will not work and will generate the following
    exception:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: That is because the `pythonping` library is not compatible with the `asyncio`
    module.
  prefs: []
  type: TYPE_NORMAL
- en: Use `asyncio` whenever you need to have lots of tasks running because it is
    very cheap to use coroutines as a task, much faster and lighter than processes
    and threads. However, it requires that your application be I/O bound to take advantage
    of the concurrency of the coroutines. Access to network devices is a good example
    of a slow I/O-bound application and may be a perfect fit for our network automation
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: To allow efficient use of coroutines in Python, you have to make sure that the
    coroutine is suspending execution when there is a wait in I/O (such as a network)
    to allow other coroutines to run. This is normally indicated by the `asyncio`
    statement called `await`. Indeed, using the third-party library in your coroutine
    needs to be compatible with `asyncio`. As the `asyncio` module is quite new, there
    are not many third-party libraries that are compatible with `asyncio`. Without
    this compatibility, your code will run coroutines sequentially instead of concurrently,
    and using `asyncio` will not be a good idea.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how coroutines can be used in Go.
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines in Go
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Go language is special and shines best when it requires code to scale up
    with performance, and that is accomplished in Go using goroutines.
  prefs: []
  type: TYPE_NORMAL
- en: Goroutines are not the same as coroutines, because they can run like threads
    in parallel. But they are not like threads either, because they are much smaller
    (starting with only 8 KB for Go version 1.4) and use channels for communication.
    This may be confusing at first, but I promise you that goroutines are not difficult
    to understand and use. Indeed, they are easier to understand and use compared
    to coroutines in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Since Go version 1.14, goroutines are implemented using asynchronously preemptible
    scheduling. That means the tasks are no longer in the control of the developer
    and are entirely managed by Go’s runtime (you can find details at [https://go.dev/doc/go1.14#runtime](https://go.dev/doc/go1.14#runtime)).
    Go’s runtime is responsible for presenting to the operating system the threads
    that are going to run, which can run simultaneously in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: Go’s runtime is responsible for creating and destroying the threads that correspond
    to a goroutine. These operations would be much heavier when implemented by the
    operating system using a native multithreading language, but in Go, they are light
    as Go’s runtime maintains a pool of threads for the goroutines. The fact that
    Go’s runtime controls the mapping between goroutines and threads makes the operating
    system completely unaware of goroutines.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, Go doesn’t use coroutines, but instead uses goroutines, which are
    not the same and are more like a blend between coroutines and threads, with better
    performance than the two.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now go through a simple example of an ICMP probe using goroutines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this program, it should output something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: To use a goroutine, you just need to add `go` in front of the function you want
    to call as a goroutine. The `go` statement indicates that the function can be
    executed in the background with its own stack and variables. The program then
    executes the line after the `go` statement and continues the flow normally without
    waiting for anything to return from the goroutine. As the ICMP probe request takes
    a few milliseconds to receive an ICMP response, the program will exit before it
    can print anything by the goroutines. Therefore, we need to add a sleep time of
    3 seconds before finishing the program to make sure all the goroutines that send
    ICMP requests have received and printed the results. Otherwise, you won’t be able
    to see any output, because the program will end before the goroutines finish printing
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to wait until the goroutines end, Go has mechanisms to communicate
    and wait until they end. One simple one is using `sync.WaitGroup`. Let’s now rewrite
    our previous example, removing the sleep time and adding `WaitGroup` to wait for
    the goroutines to finish. The following is the same example that waits until all
    goroutines end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: If you run the preceding code, it should end faster than the previous one because
    it does not sleep for 3 seconds; it only waits until all goroutines end, which
    should be less than half a second.
  prefs: []
  type: TYPE_NORMAL
- en: To allow `sync.WaitGroup` to work, you have to set a value to it at the beginning
    using `Add()`. In the preceding example, it adds `4`, which is the number of goroutines
    that will run. Then, you pass the pointer of the variable to each goroutine function
    (`&wg`), which will be marked as `Done()` as the function ends using `defer` (explained
    in [*Chapter 7*](B18165_07.xhtml#_idTextAnchor183)).
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, we did not generate any communication between the
    goroutines, as they use the terminal to print. We only passed a pointer to the
    workgroup variable, called `wg`. If you want to communicate between goroutines,
    you can do that by using `channel`, which can be unidirectional or bidirectional.
  prefs: []
  type: TYPE_NORMAL
- en: 'More on goroutines can be found at the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Google I/O 2012 - Go Concurrency* *Patterns*: [https://www.youtube.com/watch?v=f6kdp27TYZs](https://www.youtube.com/watch?v=f6kdp27TYZs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Google I/O 2013 – Advanced Go Concurrency* *Patterns*: [https://www.youtube.com/watch?v=QDDwwePbDtw](https://www.youtube.com/watch?v=QDDwwePbDtw)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More documentation on Goroutines can be found at [go.dev/doc/effective_go#concurrency](http://go.dev/doc/effective_go#concurrency)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before going to the next section, let’s summarize how we scale up in Python
    and Go. In Python, to make the right choice, use *Figure 8**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Python decision-making for scaling your code](img/B18165_08_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Python decision-making for scaling your code
  prefs: []
  type: TYPE_NORMAL
- en: The diagram in *Figure 8**.1* shows which Python library to use when scaling
    your code. If CPU bound, use `multiprocessing`. If you have too many connections
    with slow I/O, use `asyncio`, and if the number of connections is small, use `threading`.
  prefs: []
  type: TYPE_NORMAL
- en: For Go, there is only one option, which is a goroutine. Easy answer!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now check how we can scale the system using schedulers and dispatchers.
  prefs: []
  type: TYPE_NORMAL
- en: Adding schedulers and job dispatchers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A scheduler is a system that selects a job for running, where a **job** can
    be understood as a program or part of code that requires running. A dispatcher,
    on the other hand, is the system that takes the job and places it in the execution
    queue of a machine. They are complementary, and in some cases, they are treated
    as the same system. So, for the purpose of this section, we are going to talk
    about some systems that can do both scheduling and dispatching jobs.
  prefs: []
  type: TYPE_NORMAL
- en: The main objective of using systems that can schedule and dispatch jobs is to
    gain scale by adding machines that can run more jobs in parallel. It is kind of
    similar to a single program using multiprocessing but with the difference that
    the new processes are being executed on another machine.
  prefs: []
  type: TYPE_NORMAL
- en: You could do lots of work to improve the performance of your program, but in
    the end, you will be bound by the machine’s limitations, and if your application
    is CPU bound, it will be limited by the number of cores, the number of concurrent
    threads, and the speed of the CPU used. You could work hard to improve the performance
    of your code, but to grow more, the only solution is to add more CPU hardware,
    which can be accomplished by adding machines. The group of machines that are dedicated
    to running jobs for schedulers and dispatchers is normally called a **cluster**.
  prefs: []
  type: TYPE_NORMAL
- en: A cluster of machines that are ready to run jobs in parallel can be installed
    locally or can be installed in separate locations. The distance between the machines
    in a cluster adds latency to the communication between the machines and delays
    data synchronization. Quick synchronization between machines may or may not be
    relevant, depending on how fast the results are required and how they need to
    be combined if they depend on each other in time. Quicker results might require
    a local cluster. A more relaxed time frame for getting results would allow clusters
    to be located further apart.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now discuss how we can use a classical scheduler and dispatcher.
  prefs: []
  type: TYPE_NORMAL
- en: Using classical schedulers and dispatchers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The classical scheduler and dispatcher would be any system that takes one job,
    deploys it to a machine in the cluster, and executes it. In a classical case,
    the job is just a program that is ready to run on a machine. The program can be
    written in any language; however, there are differences in how the installation
    would be compared between Python and Go.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s investigate what the differences are between using a cluster ready to
    run Python scripts and ready to run Go-compiled code.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for using Go and Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the program was written in Python, it is required that all machines in the
    cluster have the Python interpreter version that is compatible with the Python
    code. For instance, if code was written for Python 3.10, the CPython interpreter
    to be installed must be at least version 3.10\. Another important point here is
    that all third-party libraries used in the Python script will also have to be
    installed on all machines. The version of each third-party library must be compatible
    with the Python script, as newer versions of a particular third-party library
    might break the execution of your code. You might need to maintain a table somewhere
    containing the versions of each third-party library used to avoid wrong machine
    updates. In conclusion, using Python complicates your cluster installation, management,
    and updates a lot.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, using the Go language is much simpler for deploying in a
    cluster. You just need to compile the Go program to the same CPU architecture
    that the code will run. All third-party libraries used will be added to the compiled
    code. The versions of each third-party library used in your program will be controlled
    automatically by your local development environment with the `go.sum` and `go.mod`
    files. In summary, you don’t need to install an interpreter on the machines and
    don’t need to worry about installing or updating any third-party libraries, which
    is much simpler.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now see a few examples of a scheduler and dispatcher for a cluster of
    machines.
  prefs: []
  type: TYPE_NORMAL
- en: Using Nomad
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Nomad is an implementation for job scheduling that was built using Go and is
    supported by a company called HashiCorp ([https://www.nomadproject.io/](https://www.nomadproject.io/)).
    Nomad is also very popular for scheduling and launching Docker containers in a
    cluster of machines, as we are going to see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: With Nomad, you are able to define a job by writing a configuration file that
    describes the job and how you want to run it. The job description can be written
    in any formatted file, such as YAML or TOML, but the default supported format
    is **HCL**. Once you have the job description completed, it is then translated
    to JSON, which will be used on the Nomad API (more details can be found at [https://developer.hashicorp.com/nomad/docs/job-specification](https://developer.hashicorp.com/nomad/docs/job-specification)).
  prefs: []
  type: TYPE_NORMAL
- en: Nomad supports several task drivers, which allows you to schedule different
    kinds of programs. If you are using a Go-compiled program, you will have to use
    the `Fork/Exec` driver (further details are available at [https://developer.hashicorp.com/nomad/docs/drivers/exec](https://developer.hashicorp.com/nomad/docs/drivers/exec)).
    Using the `Fork/Exec` driver, you can execute any program, including Python scripts,
    but with the caveat of having all third-party libraries and the Python interpreter
    previously installed on all machines of the cluster, which is not managed by Nomad
    and must be done on your own separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a job specification for an ICMP probe program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note that the preceding program example is called `icmp-probe` and would have
    to accept the operating system environment variable as input. In our example,
    the variable is called `TARGETS`.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have defined your job, you can dispatch it by issuing the `nomad job
    dispatch <job-description-file>` command (more details can be found at [developer.hashicorp.com/nomad/docs/commands/job/dispatch](http://developer.hashicorp.com/nomad/docs/commands/job/dispatch)).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now check how we could use another popular scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: Using Cronsun
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cronsun is another scheduler and dispatcher that works in a similar way to the
    popular Unix cron but for multiple machines. The goal of Cronsun is to be easy
    and simple for managing jobs on lots of machines. It is developed in the Go language
    but can also launch jobs in any language by invoking a shell on the remote machine,
    such as in the Nomad `Fork/Exec` driver (more details can be found at [https://github.com/shunfei/cronsun](https://github.com/shunfei/cronsun)).
    It also has a graphical interface that allows easy visualization of the running
    jobs. Cronsun was built and designed based on another Go third-party package,
    called `robfig/cron` ([https://github.com/robfig/cron](https://github.com/robfig/cron)).
  prefs: []
  type: TYPE_NORMAL
- en: Using Cronsun, you will be able to launch several jobs on multiple machines,
    but there is no machine cluster management like in Nomad. Another important point
    is Cronsun does not work with Linux containers, so it purely focuses on executing
    Unix shell programs in the remote machine by doing process forking.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now look at a more complex scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: Using DolphinScheduler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DolphinScheduler is a complete system for scheduling and dispatching jobs that
    is supported by the Apache Software Foundation. It has many more features compared
    to Nomad and Cronsun, with workflow capabilities that allow a job to wait for
    input from another job before executing. It also has a graphical interface that
    helps to visualize running jobs and dependencies (more details can be found at
    [https://dolphinscheduler.apache.org/](https://dolphinscheduler.apache.org/)).
  prefs: []
  type: TYPE_NORMAL
- en: Although DolphinScheduler is primarily written in Java, it can dispatch jobs
    in Python and Go. It is much more complex and has many capabilities that might
    not be necessary for your requirements to scale up.
  prefs: []
  type: TYPE_NORMAL
- en: There are several other job schedulers and dispatchers that you could use, but
    some of them are used for specific languages, such as Quartz.NET, used for .NET
    applications ([https://www.quartz-scheduler.net/](https://www.quartz-scheduler.net/)),
    and Bree, used for Node.js applications ([https://github.com/breejs/bree](https://github.com/breejs/bree)).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see now how we can use big data schedulers and dispatchers for carrying
    out computation at scale in network automation.
  prefs: []
  type: TYPE_NORMAL
- en: Working with big data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are specific applications that require lots of CPU for data processing.
    These applications require a system that allows running code with very specialized
    algorithms focused on data analysis. These are normally referred to as systems
    and applications for **big data**.
  prefs: []
  type: TYPE_NORMAL
- en: Big data is a collection of datasets that are too large to be analyzed on just
    one computer. It is a field that is dominated by data scientists, data engineers,
    and artificial intelligence engineers. The reason is that they normally analyze
    a lot of data to extract information, and their work requires a system that scales
    up a lot in terms of CPU processing. Such scale can only be achieved by using
    systems that can schedule and dispatch jobs over many computers in a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm model used for big data is called **MapReduce**. A MapReduce programming
    model is used to implement analysis on large datasets using an algorithm that
    runs on several machines in a cluster. Originally, the term MapReduce was related
    to a Google product, but now it is a term used for programs that deal with big
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The original paper published by Jeffrey Dean and Sanjay Ghemawat called *MapReduce:
    Simplified Data Processing on Large Clusters* is a good reference and good reading
    to dive deeper into the subject. The paper is public and can be downloaded from
    the Google Research page at [https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how we can use big data in our network automation.
  prefs: []
  type: TYPE_NORMAL
- en: Big data and network automation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Big data is used in network automation to help with traffic engineering and
    optimization. MapReduce is used to calculate better traffic paths over a combination
    of traffic demands and routing paths. Traffic demands are collected and stored
    using the IP source and IP destination, then MapReduce is used to calculate a
    traffic demand matrix. For this work, routing and traffic information is collected
    from all network devices using BGP, SNMP, and a flow-based collection such as
    `sflow`, `ipfix`, or `netflow`. The data collected is normally big and real-time
    results are required to allow for proper network optimization and traffic engineering
    on time.
  prefs: []
  type: TYPE_NORMAL
- en: One example would be the collection of IP data flow from the transit routers
    and peering routers (discussed in [*Chapter 1*](B18165_01.xhtml#_idTextAnchor015)).
    This flow information would then be analyzed in conjunction with the routing information
    obtained from the routers. Then, a better routing policy can be applied in real
    time to select better external paths or network interfaces that are less congested.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now investigate some popular systems that can be used for big data.
  prefs: []
  type: TYPE_NORMAL
- en: Using systems for big data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The two most popular open source systems for big data are **Apache Hadoop**
    ([https://hadoop.apache.org/](https://hadoop.apache.org/)) and **Apache Spark**
    ([https://spark.apache.org/](https://spark.apache.org/)). Both systems are supported
    and maintained by the Apache Software Foundation ([https://www.apache.org/](https://www.apache.org/))
    and are used to build large cluster systems to run big data.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between Hadoop and Spark is related to how they perform big data
    analysis. Hadoop is used for batch job scheduling without real-time requirements.
    It uses more disk capacity and the response time is more relaxed, so the cluster
    machines don’t need to be local, and the machines need to have large disks. On
    the other hand, Spark uses more memory and less disk space, the machines need
    to be located closer, and the response time is more predictable, therefore it
    is used for real-time applications.
  prefs: []
  type: TYPE_NORMAL
- en: For our network automation on traffic analysis, either option can be used, but
    Spark would be preferred for faster and more periodic results. Hadoop would be
    used to generate monthly and daily reports, but not to interact with real-time
    routing policies.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now look at a common problem with having your own cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Resource allocation and cloud services
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the problems with using Hadoop and Spark is that you will need to create
    your own cluster of machines. That means installing and maintaining the hardware
    and operating system software. But that is not the main problem. The main problem
    is that resource utilization will vary throughout the day and the year.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, imagine you are using a big data system in your company to calculate
    the best path for a particular group of routers during the day. The problem is
    the collected data to be analyzed will change; in busy hours, you will need more
    CPU processing to calculate compared to non-busy hours. The difference can be
    hundreds of CPUs, which will lead to lots of idle CPU hours at the end of the
    month.
  prefs: []
  type: TYPE_NORMAL
- en: How do you solve this issue? By using a cloud-based service provider to allocate
    machines for your cluster. With it, you can add and remove machines during the
    day and throughout the week, allowing growth when needed and releasing computing
    power when not needed. One example is to use AWS’ product called **Elastic MapReduce**
    (**EMR**), which can be used with easy machine allocation for your cluster, scaling
    up and down by software (more details can be found at [https://aws.amazon.com/emr/](https://aws.amazon.com/emr/)).
    Similar services can be obtained from other cloud service providers, such as Google,
    Oracle, or Microsoft.
  prefs: []
  type: TYPE_NORMAL
- en: One important point to observe is that big data systems do not allow running
    any program or language, but only code that has the MapReduce concept capabilities.
    So, it is much more specific compared to Nomad or Cronsun, and focuses only on
    data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now check how we can scale using microservices and Linux containers.
  prefs: []
  type: TYPE_NORMAL
- en: Using microservices and containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When software is built based on a combination of small, independent services,
    we normally say the software was built using microservices architecture. Microservices
    architecture is a way to develop applications by combining small services that
    might belong or not to the same software development team.
  prefs: []
  type: TYPE_NORMAL
- en: The success of this approach is due to the isolation between each service, which
    is accomplished by using Linux containers (described in [*Chapter 2*](B18165_02.xhtml#_idTextAnchor041)).
    Using Linux containers is a good way to isolate memory, CPU, networks, and disks.
    Each Linux container can’t interact with other Linux containers in the same host
    unless a pre-defined communication channel is established. The communication channels
    of a service have to use well-documented APIs.
  prefs: []
  type: TYPE_NORMAL
- en: The machine that runs microservices is normally called a **container host**
    or just a host. A host can have multiple microservices that may or may not communicate
    with each other. A combination of hosts is called a cluster of container hosts.
    Some orchestration software is able to spawn several copies of a service in one
    host or different hosts. Using microservices architecture is a good way to scale
    your system.
  prefs: []
  type: TYPE_NORMAL
- en: One very popular place to build and publish a microservice is **Docker** ([https://www.docker.com/](https://www.docker.com/)).
    A Docker container is normally referred to as a service that is built using a
    Linux container. A Docker host is where a Docker container can run, and in a similar
    way, a Docker cluster is a group of hosts that can run Docker containers.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see now how we can use Docker containers to scale our code.
  prefs: []
  type: TYPE_NORMAL
- en: Building a scalable solution by example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s build a solution using microservices architecture by creating our own
    Docker container and then launching it multiple times. Our service has a few requirements,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It needs to have an API to accept requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The API needs to accept a list of targets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ICMP probe will be sent to each target to verify latency concurrently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The API will respond using HTTP plain text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each service can accept up to 1,000 targets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The timeout for each ICMP probe must be 2 seconds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on these requirements, let’s write some code that will be used in our
    service.
  prefs: []
  type: TYPE_NORMAL
- en: Writing the service code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the previous requirements, let’s write some code in Go to build our service.
    We are going to use the Go third-party package for ICMP that we used before in
    this chapter called `go-ping/ping`, and `sync.WaitGroup` to wait for the goroutines
    to end.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break the code into two blocks. The second block of code is as follows,
    describing the `probeTargets()` and `main()` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding block represents the last two functions of our service. In the
    `main()` function, we just need to call `http.HandleFunc`, passing the API reference
    used for the `GET` method and the name of the function that will be invoked. Then,
    `http.ListenAndServe` is called using port `9900` to listen for API requests.
    Note that `log.Fatal` is used with `ListenAndServe` because it should never end
    unless it has a problem. The following is an API `GET` client request example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The preceding API request will call `probeTargets()`, which will run the loop
    invoking the goroutines (called `probe()`) two times, which will send ICMP requests
    to `google.com` and `cisco.com`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now have a look at the last block of code containing the `probe()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `probe()` function does not return a value, a `log` message, or
    a print message. All messages, including errors, are returned to the HTTP client
    requesting the ICMP probes. To allow the return to the client, we have to use
    the `fmt.Fprintf()` function, passing the reference `w`, which points to an `http.ResponseWriter`
    type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we continue with our example, let’s make a modification to our `main()`
    function to allow reading the port number from the operating system environment
    variable. So, the service can be called with different port numbers when being
    invoked, just needing to change the operating system environment variable called
    `PORT`, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Let’s now build our Docker container using a Dockerfile.
  prefs: []
  type: TYPE_NORMAL
- en: Building our Docker container
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To build the Docker container, we are going to use Dockerfile definitions. Then,
    we just need to run `docker build` to create our container. Before you install
    the Docker engine in your environment, check the documentation on how to install
    it at [https://docs.docker.com/engine/install/](https://docs.docker.com/engine/install/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the Dockerfile used in our example of an ICMP probe service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'To build the Docker container, you just need to run `docker build . –t probe-service`.
    After running the build, you should be able to see the image by using the `docker
    image` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The Docker container name is `probe-service` and you can run the service by
    using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'To listen to a different port, you need to set the `PORT` environment variable.
    An example for port `7700` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that you could map different host ports to port `9900` if you want to
    run multiple services in the same host without changing the port that the container
    listens to. You just need to specify a different port for the host when mapping,
    as in the following example running three services on the same machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the preceding three commands will start three services on the host
    ports: `9001`, `9002`, and `9003`. The service inside the container still uses
    port `9900`. To check the services running in a host, use the `docker ps` command,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output shows that there are three services running on the host,
    listening to ports `9001`, `9002`, and `9003`. You can access the APIs for each
    of them and probe up to 3,000 targets, 1,000 per service.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now see how we can automate launching multiple services using Docker Compose.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up using Docker Compose
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using Docker Compose will help you to add services that will run at the same
    time without needing to invoke the `docker run` command. In our example, we are
    going to use Docker Compose to launch five ICMP probe services. The following
    is the Docker Compose file example in YAML format (described in [*Chapter 4*](B18165_04.xhtml#_idTextAnchor100),):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the services, just type `docker compose up –d`, and to stop them, just
    run `docker compose down`. The following is an example of the output of the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s see how we can scale up using multiple machines with a Docker container.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up with clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To scale even more, you could set up a cluster of Docker host containers. This
    will allow you to launch thousands of services, allowing our ICMP probe service
    to scale to millions of targets. You could build the cluster yourself by managing
    a group of machines and running the services, or you could use a system to do
    all that for you.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now investigate a few systems that are used to manage and launch services
    for a cluster of machines running container services.
  prefs: []
  type: TYPE_NORMAL
- en: Using Docker Swarm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'With **Docker Swarm**, you are able to launch containers on several machines.
    It is easy to use because it only requires installing Docker. Once you have installed
    it, it is easy to create a Docker Swarm cluster. You just have to run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have started the first Docker Swarm host, it will then take the lead
    place, and to add another host, you just need to use the `docker swarm join` command.
    To avoid any host joining the Docker Swarm cluster, a token is used. The preceding
    example starts with `SWMTKN-1`. Note that a host in a Docker Swarm cluster is
    also called a **node**. So, let’s add more nodes to our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have four nodes in the cluster, with `host-1` as the leader. You can
    check the status of the cluster nodes by typing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have your cluster, you can launch a service by running the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, we just launched a Swarm service called `probe` using
    the `probe-service` image, the same image used in previous examples. Note that
    we’ve only launched one replica to later show how easy it is to scale up. Let’s
    check now how the service is installed by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now scale up for 10 probes by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if you check the service, it will show 10 replicas, as in the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also check where each replica is running by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the output of the preceding command, there are 10 probes running
    as replicas on nodes `host-1`, `host-2`, `host-3`, and `host-4`. You can also
    specify where you want the replica to run, among other parameters. In this example,
    we are able to scale up our ICMP probe service to 10,000 targets by using 4 hosts.
  prefs: []
  type: TYPE_NORMAL
- en: One important point we missed on these commands was allocating the ports to
    listen for our replicas. As replicas can run in the same host, they can’t use
    the same port. We then need to make sure each replica is assigned with a different
    port number to listen to. A client accessing our `probe-service` cluster needs
    to know the IP addresses of the hosts and the port numbers that are listening
    before connecting for requests.
  prefs: []
  type: TYPE_NORMAL
- en: A better and more controlled way to deploy Docker Swarm is to use a YAML configuration
    file like we did when using Docker Compose. More details on the configuration
    file for Docker Swarm can be found at [https://github.com/docker/labs/blob/master/beginner/chapters/votingapp.md](https://github.com/docker/labs/blob/master/beginner/chapters/votingapp.md).
  prefs: []
  type: TYPE_NORMAL
- en: More documentation on Docker Swarm can be found at [https://docs.docker.com/engine/swarm/swarm-mode/](https://docs.docker.com/engine/swarm/swarm-mode/).
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s investigate how to use multiple hosts using Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Using Kubernetes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Kubernetes is perhaps one of the most popular systems to manage the microservices
    architecture in a cluster. Its popularity is also due to it being backed by the
    **Cloud Native Computing Foundation** ([https://www.cncf.io/](https://www.cncf.io/)),
    which is part of the **Linux Foundation** ([https://www.linuxfoundation.org/](https://www.linuxfoundation.org/)).
    Large companies use Kubernetes, such as Amazon, Google, Apple, Cisco, and Huawei,
    among others.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes provides many more capabilities than Docker Swarm, such as service
    orchestration, load-balancing, service monitoring, self-healing, and auto-scaling
    by traffic, among other features. Despite the large community and vast capabilities,
    you might not want to use Kubernetes if your requirement is simple and needs to
    scale in large quantities. Kubernetes provides a lot of capabilities that might
    be an overhead to your development. For our `probe-service`, I would not recommend
    using Kubernetes, because it is too complex for our purposes of ICMP probing targets.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use a Docker Compose file to configure Kubernetes, which is done
    by using a service translator such as Kompose ([https://kompose.io/](https://kompose.io/)).
    More details can be found at [https://kubernetes.io/docs/tasks/configure-pod-container/translate-compose-kubernetes/](https://kubernetes.io/docs/tasks/configure-pod-container/translate-compose-kubernetes/).
  prefs: []
  type: TYPE_NORMAL
- en: If you want to start using Kubernetes, you can find plenty of examples and documentation
    on the internet. The best place to start is at [https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now check how we can use another cluster based on Nomad.
  prefs: []
  type: TYPE_NORMAL
- en: Using Nomad
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Nomad is also used to implement Docker clustering ([https://www.nomadproject.io/](https://www.nomadproject.io/)).
    Nomad also has several capabilities that are comparable to Kubernetes, such as
    monitoring, self-healing, and auto-scaling. However, the features list is not
    as long and complete as Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, why would we use Nomad instead of Kubernetes? There are three main reasons
    that you might want to use Nomad, as listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Simpler to deploy and easy to configure in comparison to Kubernetes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes can scale up to 5,000 nodes with 300,000 containers. Nomad, on the
    other hand, is able to scale to 10,000 nodes and more than 2 million containers
    ([https://www.javelynn.com/cloud/the-two-million-container-challenge/](https://www.javelynn.com/cloud/the-two-million-container-challenge/)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can support other services besides Linux containers, such as **QEMU** virtual
    machines, Java, Unix processes, and Windows containers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More documentation on Nomad can be found at [https://developer.hashicorp.com/nomad/docs](https://developer.hashicorp.com/nomad/docs).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now have a brief look at how to use microservice architectures provided
    by cloud service providers.
  prefs: []
  type: TYPE_NORMAL
- en: Using cloud service providers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are also proprietary solutions that are provided by cloud service providers,
    such as **Azure Container Instances**, **Google Kubernetes Engine** (**GKE**),
    and Amazon **Elastic Container Service** (**ECS**). The advantage of using a cloud
    service provider is you don’t need physical machines in your infrastructure to
    create the cluster. There are also products where you don’t even need to care
    about the cluster and the nodes in it, such as a product from Amazon called AWS
    Fargate. With AWS Fargate, you just need the Docker container published in a Docker
    registry and a service specification without the need to specify the nodes or
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this section has given you a good idea of how to scale your code by using
    Linux containers and host clustering. Microservice architecture is a hot topic
    that has gotten lots of attention from developers and cloud service providers
    in recent years. Several acronyms might be used to describe this technology, but
    we have covered the basics here. You now have enough knowledge to dive even deeper
    into this subject.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has shown you a good summary of how you can improve and use systems
    to scale your code. We also demonstrated how we can use standard and third-party
    libraries to add capabilities to our code to scale.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you are probably much more familiar with the technologies that you could
    use to interact with large networks. You are now in a better position to choose
    a language, a library, and a system that will support your network automation
    to scale to handle thousands or even millions of network devices.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to cover how to test your code and your system,
    which will allow you to build solutions for network automation that are less prone
    to failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: Testing, Hands-On, and Going Forward'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The third part of the book will discuss what has to be considered when building
    a framework for testing your code and how to do so, We will do some real hands-on
    testing and, finally, describe what to do to move forward in the network automation
    realm. We will provide the details on creating a testing framework and do hands-on
    work using an emulated network, which will help to put into practice all the information
    learned in previous parts.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B18165_09.xhtml#_idTextAnchor209), *Network Code Testing Frameworks*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B18165_10.xhtml#_idTextAnchor227), *Hands-On and Going Forward*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
