["```py\n    gcloud –version\n    ```", "```py\n    gcloud components install app-engine-python\n    ```", "```py\n    gcloud init\n    ```", "```py\n    gcloud projects create time-wsproj\n    ```", "```py\n    gcloud config set project time-wsproj\n    ```", "```py\n    gcloud app create                   #for default project\n    gcloud app create --project=time-wsproj  #for specific project\n    ```", "```py\n    gcloud services list --available | grep cloudbuild\n    #output will be like: NAME: cloudbuild.googleapis.com\n    #Cloud SDK command to enable this service\n    gcloud services enable cloudbuild.googleapis.com\n    ```", "```py\n    gcloud beta commands for the first time, you will be prompted to install the beta component. You should go ahead and install it. If you are already using a Cloud SDK version with a billing component included for GA, you can skip using the beta keyword or use the appropriate commands, as per the Cloud SDK release documentation.\n    ```", "```py\n    gcloud services list --available | grep cloudbilling\n    #output will be: NAME: cloudbilling.googleapis.com\n    #command to enable this service\n    gcloud services enable cloudbilling.googleapis.com\n    ```", "```py\ngcloud projects describe time-wsproj\n```", "```py\ncreateTime: '2021-06-05T12:03:31.039Z'\nlifecycleState: ACTIVE\nname: time-wsproj\nprojectId: time-wsproj\nprojectNumber: '539807460484'\n```", "```py\nruntime: python38\n```", "```py\nfrom flask import Flask\nfrom datetime import date, datetime\n# If 'entrypoint' is not defined in app.yaml, App Engine will look #for an app variable. This is the case in our YAML file\napp = Flask(__name__)\n@app.route('/')\ndef welcome():\n    return 'Welcome Python Geek! Use appropriate URI for date       and time'\n@app.route('/date')\ndef today():\n    today = date.today()\n    return \"{date:\" + today.strftime(\"%B %d, %Y\") + '}'\n@app.route('/time')\ndef time():\n    now = datetime.now()\n    return \"{time:\" + now.strftime(\"%H:%M:%S\") + '}'\nif __name__ == '__main__':\n    # For local testing\n    app.run(host='127.0.0.1', port=8080, debug=True)\n```", "```py\nFlask==2.0.1\n```", "```py\npython -m venv myenv\nsource myenv/bin/activate\n```", "```py\npip install -r requirements.txt\n```", "```py\npython main.py\n```", "```py\n* Serving Flask app 'main' (lazy loading)\n* Environment: production\n   WARNING: This is a development server. Do not use it in a      production deployment.\n   Use a production WSGI server instead.\n* Debug mode: on\n* Running on http://127.0.0.1:8080/ (Press CTRL+C to quit)\n* Restarting with stat\n* Debugger is active!\n* Debugger PIN: 668-656-035\n```", "```py\ngcloud app deploy\n```", "```py\ngcloud app browse\n```", "```py\ngcloud app versions list\n```", "```py\ngcloud app versions stop <version id>\ngcloud app versions start <version id>\ngcloud app versions delete <version id>\n```", "```py\n[Final PColletcion] = ([Initial Input PCollection] |     [First PTransform] | [Second PTransform] | [Third PTransform])\n```", "```py\n#pipeline1.py: Separate strings from a PCollection\nimport apache_beam as beam\nwith beam.Pipeline() as pipeline:\n  subjects = (\n      pipeline\n      | 'Subjects' >> beam.Create([\n          'English Maths Science French Arts', ])\n      | 'Split subjects' >> beam.FlatMap(str.split)\n      | beam.Map(print))\n```", "```py\n#pipeline2.py: Separate subjects with grade from a PCollection\nimport apache_beam as beam\ndef my_format(sub, marks):\n    yield '{}\\t{}'.format(sub,marks)\nwith beam.Pipeline() as pipeline:\n  plants = (\n      pipeline\n      | 'Subjects' >> beam.Create([\n      ('English','A'),\n      ('Maths', 'B+'),\n      ('Science', 'A-'),\n      ('French', 'A'),\n      ('Arts', 'A+'),\n      ])\n      | 'Format subjects with marks' >> beam.FlatMapTuple(my_        format)\n      | beam.Map(print))\n```", "```py\n#pipeline3.py: Read data from a file and give results back to another file\nimport apache_beam as beam\nfrom apache_beam.io import WriteToText, ReadFromText\nwith beam.Pipeline() as pipeline:\n    lines = pipeline | ReadFromText('sample1.txt')\n    subjects = (\n      lines\n      | 'Subjects' >> beam.FlatMap(str.split))\n    subjects | WriteToText(file_path_prefix='subjects', \n                           file_name_suffix='.txt',\n                           shard_name_template='')\n```", "```py\n    #pipeline4.py(part 1): Using argument for a pipeline\n    import re, argparse, apache_beam as beam\n    from apache_beam.io import WriteToText, ReadFromText\n    from apache_beam.options.pipeline_options import   PipelineOptions\n    class WordParsingDoFn(beam.DoFn):\n      def process(self, element):\n        return re.findall(r'[\\w\\']+', element, re.UNICODE)\n    def run(argv=None):\n        parser = argparse.ArgumentParser()\n        parser.add_argument(\n            '--input',\n            dest='input',\n            default='sample1.txt',\n            help='Input file to process.')\n        parser.add_argument(\n            '--output',\n            dest='output',\n            default='subjects',\n            help='Output file to write results to.')\n        parser.add_argument(\n            '--extension',\n            dest='ext',\n            default='.txt',\n            help='Output file extension to use.')\n        known_args, pipeline_args = parser.parse_known_      args(argv)\n    ```", "```py\n    #pipeline4.py(part 2): under the run method\n        pipeline_args.extend([\n            '--runner=DirectRunner',\n            '--job_name=demo-local-job',\n        ])\n        pipeline_options = PipelineOptions(pipeline_args)\n    ```", "```py\n    #sample1.txt) are parsed and are put as one word in one line in the output file. \n    ```", "```py\n    gsutil mb gs://<bucket name>\n    ```", "```py\n    pip install apache-beam[gcp]\n    ```", "```py\n# wordcount.py(part 1): count words in a text file\nimport argparse, os, re, apache_beam as beam\nfrom apache_beam.io import ReadFromText, WriteToText\nfrom apache_beam.options.pipeline_options import PipelineOptions\nfrom apache_beam.options.pipeline_options import SetupOptions\ndef run(argv=None, save_main_session=True):\n  os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"some folder/    key.json\"\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      '--input',\n      dest='input',\n      default='gs://muasif/input/sample.txt',\n      help='Input file to process.')\n  parser.add_argument(\n      '--output',\n      dest='output',\n      default='gs://muasif/input/result',\n      help='Output file to write results to.')\n  known_args, pipeline_args = parser.parse_known_args(argv)\n```", "```py\n# wordcount.py (part 2): under the run method\n  pipeline_args.extend([\n      '--runner=DataflowRunner',\n      '--project=word-count-316612',\n      '--region=us-central1',\n      '--staging_location=gs://muasif/staging',\n      '--temp_location=gs://muasif/temp',\n      '--job_name=my-wordcount-job',\n  ])\n  pipeline_options = PipelineOptions(pipeline_args)\n  pipeline_options.view_as(SetupOptions).\\\n      save_main_session = save_main_session\n```", "```py\n# wordcount.py (part 3): under the run method\n  with beam.Pipeline(options=pipeline_options) as p:\n    lines = p | ReadFromText(known_args.input)\n    # Count the occurrences of each word.\n    counts = (\n        lines\n        | 'Split words' >> (\n            beam.FlatMap(\n                lambda x: re.findall(r'[A-Za-z\\']+', x)).\n                with_output_types(str))\n        | 'Pair with 1' >> beam.Map(lambda x: (x, 1))\n        | 'Group & Sum' >> beam.CombinePerKey(sum))\n    def format_result(word_count):\n      (word, count) = word_count\n      return '%s: %s' % (word, count)\n    output = counts | 'Format' >> beam.Map(format_result)\n    output | WriteToText(known_args.output)\n```", "```py\npython wordcount.py \\\n    --project word-count-316612 \\\n    --region us-central1 \\\n    --input gs://muasif/input/sample.txt \\\n    --output gs://muasif/output/results \\\n    --runner DataflowRunner \\\n    --temp_location gs://muasif/temp \\\n    --staging_location gs://muasif/staging\n```"]