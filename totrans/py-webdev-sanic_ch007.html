<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en-US">
<head>
<meta charset="utf-8"/>
<meta name="generator" content="packt"/>
<title>6 Operating outside the Request Handler</title>



</head>
<body>

<h1 data-number="7">6 Operating outside the Request Handler</h1>
<p>The basic building block of application development within Sanic is the request handler, which is sometimes known as a “route handler”. Those terms can be used interchangeably and mean the same thing. It is the function that Sanic runs when a request has been routed to your application to be handled and responded to. This is where business logic and HTTP logic combine to allow the developer to dictate how responses should be delivered back to the client. It is the obvious place to start when learning how to build with Sanic.</p>
<p>However, request handlers alone do not provide enough power to create a polished application experience. In order to build out an application that is polished and professional, we must break outside the handler to see what other tools Sanic has to offer. It is time to think about the HTTP request/response cycle as not being confined to a single function. We will broaden our scope so that responding to a request is not the responsibility of just the handler, but of the entire application. We have already gotten a taste of this when we caught a glimpse of middleware.</p>
<p>In this chapter, we are going to cover the following topics:</p>
<ul>
<li>Making use of ctx</li>
<li>Altering requests and responses with middleware</li>
<li>Leveraging signals for intra-worker communication</li>
<li>Mastering HTTP connections</li>
<li>Implementing Exception handling</li>
<li>Background task processing</li>
</ul>
<p>Of course, not all projects will need features like these, but when used in the right place they can be extremely powerful. Have you ever worked on a DIY project around your home and not quite had the right tools for the job? It can be super frustrating and inefficient when you need a Phillips-head screwdriver, but all you have are flat head screwdrivers. Not having the right tool for the job can make your task harder, but it also sometimes decreases the quality of the work that you can perform.</p>
<p>Think of the features that we explore in this Chapter as tools. There is a common saying you may have heard that says: “<em>If you are holding a hammer, then every problem looks like a nail</em>.” Lucky for us, we have a bunch of tools and our job now is to learn how to use them. We are about to go explore the Sanic tool belt and see what kinds of problems we can solve.</p>

<h2 data-number="7.1">Technical requirements</h2>
<p>In this chapter, you should have the same tools available as in the previous Chapters at your disposal in order to be able to follow along with the examples (IDE, modern Python, and curl).</p>
<p>You can access source code for this chapter on GitHub: <a href="https://github.com/PacktPublishing/Web-Development-with-Sanic/tree/main/chapters/06">https://github.com/PacktPublishing/Web-Development-with-Sanic/tree/main/chapters/06</a>.</p>


<h2 data-number="7.2">Making use of ctx</h2>
<p>Before we begin with the tool belt, there is one more concept that we must become familiar with. It is fairly ubiquitous in Sanic, and you will see it in a lot of places. I am talking about: <code>ctx</code>. What is it?</p>
<p>It is stands for <em>context</em>. These <code>ctx</code> objects can be found in a number of places, and it is impractical to build without making good use of them. What they enable is the passing of state from one location in your application to another. They exist for your own usage as a developer, and you should feel free to use them however you wish. That is to say that the <code>ctx</code> objects are yours to add information to without worrying about name collisions or otherwise impacting the operation of Sanic.</p>
<p>The most common example that comes to mind is your database connection object. You create it once, but you want to have access to it in many places. How does this work?</p>
<pre><code>@app.before_server_start
async def setup_db(app, loop):
    app.ctx.db = await setup_my_db()</code></pre>
<p>Now, anywhere you can access the application instance, you can access the db instance. For example, you can access it inside a function somewhere:</p>
<pre><code>from sanic import Sanic
async def some_function_somewhere():
    app = Sanic.get_app()
    await app.ctx.db.execute(...)
Or, perhaps you need it in your route handler:
bp = Blueprint(&quot;auth&quot;)
@bp.post(&quot;/login&quot;)
async def login(request: Request):
    session_id = await request.app.ctx.db.execute(...)
    ...</code></pre>
<p>Here is a list of all of the locations that have a <code>ctx</code> object:</p>
<table>
<tbody>
<tr class="odd">
<td><strong>Object</strong></td>
<td><strong>Description</strong></td>
<td><strong>Example</strong></td>
</tr>
<tr class="even">
<td>Sanic</td>
<td>Available during the entire lifetime of your worker instance. It is worker specific, meaning that if you run multiple workers, it will <em>not</em> keep them synchronized. Best used for connection management, or other things that need to be made available throughout the lifetime of the application instance.</td>
<td><code>app.ctx</code></td>
</tr>
<tr class="odd">
<td>Blueprint</td>
<td>Available on a Blueprint instance as long as the blueprint exists. This might be helpful if you have some specific data that needs to be available for the entire worker lifetime, but you want to control its access to anything attached to that particular Blueprint.</td>
<td><code>bp.ctx</code></td>
</tr>
<tr class="even">
<td>Request</td>
<td>Available for the duration of a single HTTP request. Helpful for adding details in middleware, and then making it available in the handler or other middleware. Common uses include session IDs and user instances.</td>
<td><code>request.ctx</code></td>
</tr>
<tr class="odd">
<td>ConnInfo</td>
<td>Available for the duration of an entire HTTP connection (potentially multiple requests). Be careful with this one particularly if you use a proxy. It usually should not be used for sensitive information.</td>
<td><code>request.conn_info.ctx</code></td>
</tr>
<tr class="even">
<td>Route</td>
<td>Available on the Route and Signal instances. This is the one exception where Sanic actually does store some details on the ctx object.</td>
<td><code>request.route.ctx</code></td>
</tr>
</tbody>
</table>
Table 6.1 - Sanic features with a <code>ctx </code> object
<p>We will continue to come back to <code>ctx </code>objects often. They are a very important concept in Sanic to allow the passing of arbitrary data and objects. Not all of them are created equal, and you will likely find yourself using <code>app.ctx </code>and <code>request.ctx </code>much more often than any of the others.</p>
<p>Now that we have this basic building block behind us, we will see what it actually looks like to pass these objects around. In the next section regarding middleware, we will see how the Request object—and therefor also the <code>request.ctx</code>—can be accessed in multiple places from your application.</p>


<h2 data-number="7.3">Altering requests and responses with middleware</h2>
<p>If you have been following along with the book up until now, the concept of middleware should be familiar. This is the first tool in the tool belt that you should become familiar with.</p>
<p>Middleware are snippets of code that can be run before and after route handlers. Middleware comes in two varieties: request and response.</p>

<h3 data-number="7.3.1">Request middleware</h3>
<p>The request middleware executes in the order in which it was declared, before the route handler.</p>
<pre><code>@app.on_request
async def one(request):
    print(&quot;one&quot;)
@app.on_request
async def two(request):
    print(&quot;two&quot;)
@app.get(&quot;/&quot;)
async def handler(request):
    print(&quot;three&quot;)
    return text(&quot;done&quot;)</code></pre>
<p>When we try and reach this endpoint, we should see the following in the terminal:</p>
<pre><code>one
two
three
(sanic.access)[INFO][127.0.0.1:47194]: GET http://localhost:7777/  200 4</code></pre>
<p>But, this only tells part of the story. Sometimes we may need to add some additional logic to only <em>parts</em> of our application. Let’s pretend we are working on building an e-commerce application. Like other online stores, we will need to build a shopping cart that holds products that are going to be purchased. For the sake of our example, we will imagine that when the user logs in, we create the cart in our database and store a reference to it in a cookie. Something like this:</p>
<pre><code>@app.post(&quot;/login&quot;)
async def login(request):
    user = await do_some_fancy_login_stuff(request)
    cart = await generate_shopping_cart(request)
    response = text(f&quot;Hello {user.name}&quot;)
    response.cookies[&quot;cart&quot;] = cart.uid
    return response</code></pre>
<p>Do not get too tied up in the details here. The point is that on every subsequent request, there will be a cookie called cart that we can use to fetch data from our database.</p>
<p>Now, suppose that we want all endpoints on our <code>/cart</code> path to have access to the shopping cart. We might have endpoints for adding items, removing items, changing quantities, and so on. However, we will always need access to the cart. Rather than repeating the logic in every handler, we can do it once on the Blueprint. Adding middleware to all the routes on a single Blueprint looks and functions similarly to application wide middleware.</p>
<pre><code>bp = Blueprint(&quot;ShoppingCart&quot;, url_prefix=&quot;/cart&quot;)
@bp.on_request
async def fetch_cart(request):
    cart_id = request.cookies.get(&quot;cart&quot;)
    request.ctx.cart = await fetch_shopping_cart(cart_id)
@bp.get(&quot;/&quot;)
async def get_cart(request):
    print(request.ctx.cart)
    ...</code></pre>
<p>As we would expect, every endpoint that is attached to the <code>ShoppingCart</code> Blueprint will fetch the cart before it runs the handler. I am sure you can see the value in this sort of pattern. Where you can identify a group of routes that need a similar functionality, sometimes it is best to pull that out into middleware. This is a good time to also point out that this works also with Blueprint Groups. We could change the middleware to this and have the same impact:</p>
<pre><code>group = Blueprint.group(bp)
@group.on_request
async def fetch_cart(request):
    cart_id = request.cookies.get(&quot;cart&quot;)
    request.ctx.cart = await fetch_shopping_cart(cart_id)</code></pre>
<p>Just as we would expect, endpoints that are within that Blueprint Group will now have the shopping cart accessible to them.</p>
<p>Knowing that we can execute middleware both application-wide and blueprint-specific leads to an interesting question: in what order are they applied? No matter the order in which they are declared, all application-wide middleware will <em>always</em> run before the blueprint-specific middleware. To illustrate this point, we will use an example that mixes the two types.</p>
<pre><code>bp = Blueprint(&quot;Six&quot;, url_prefix=&quot;/six&quot;)
@app.on_request
async def one(request):
    request.ctx.numbers = []
    request.ctx.numbers.append(1)
@bp.on_request
async def two(request):
    request.ctx.numbers.append(2)
@app.on_request
async def three(request):
    request.ctx.numbers.append(3)
@bp.on_request
async def four(request):
    request.ctx.numbers.append(4)
@app.on_request
async def five(request):
    request.ctx.numbers.append(5)
@bp.on_request
async def six(request):
    request.ctx.numbers.append(6)
@app.get(&quot;/&quot;)
async def app_handler(request):
    return json(request.ctx.numbers)
@bp.get(&quot;/&quot;)
async def bp_handler(request):
    return json(request.ctx.numbers)
app.blueprint(bp)</code></pre>
<p>As you can see in this example, we interspersed declaring application and blueprint middleware by alternating between them: first application, then blueprint, etc. While the code lists the functions in sequential order (1, 2, 3, 4, 5, 6), our output will not be. You should be able to anticipate how our endpoints will respond with the application numbers appended before the blueprint numbers. Sure enough, that is the case:</p>
<pre><code>$ curl localhost:7777     
[1,3,5]
$ curl localhost:7777/six
[1,3,5,2,4,6]</code></pre>
<p>It is also really helpful to point out that since middleware is just passing along the <code>Request </code>object, subsequent middleware has access to whatever changes earlier middleware performed. In this example, we created the list of numbers in one, which was then available to all of the middleware.</p>


<h3 data-number="7.3.2">Response middleware</h3>
<p>On the other side of the HTTP lifecycle, we have response middleware. The same rules for request middleware apply:</p>
<ul>
<li>It is executed based upon the order of declaration, <em>although, it is reverse order!</em></li>
<li>Response middleware can be both application-wide or blueprint-specific</li>
<li>All application-wide middleware will run before any blueprint-specific middleware</li>
</ul>
<p>In the last section, we counted from 1 through 6 using middleware. We will take the exact same code (order is important!), but change from request to response:</p>
<pre><code>bp = Blueprint(&quot;Six&quot;, url_prefix=&quot;/six&quot;)
@app.on_response
async def one(request, response):
    request.ctx.numbers = []
    request.ctx.numbers.append(1)
@bp.on_response
async def two(request, response):
    request.ctx.numbers.append(2)
@app.on_response
async def three(request, response):
    request.ctx.numbers.append(3)
@bp.on_response
async def four(request, response):
    request.ctx.numbers.append(4)
@app.on_response
async def five(request, response):
    request.ctx.numbers.append(5)
@bp.on_response
async def six(request, response):
    request.ctx.numbers.append(6)
@app.get(&quot;/&quot;)
async def app_handler(request):
    return json(request.ctx.numbers)
@bp.get(&quot;/&quot;)
async def bp_handler(request):
    return json(request.ctx.numbers)</code></pre>
<p>Now, when we hit our endpoint, we will see a different order:</p>
<pre><code>$ curl localhost:7777
500 — Internal Server Error
===========================
&#39;types.SimpleNamespace&#39; object has no attribute &#39;numbers&#39;
AttributeError: &#39;types.SimpleNamespace&#39; object has no attribute &#39;numbers&#39; while handling path /
Traceback of __main__ (most recent call last):
  AttributeError: &#39;types.SimpleNamespace&#39; object has no attribute &#39;numbers&#39;
    File /path/to/sanic/app.py, line 777, in handle_request
    response = await response
    File /path/to/server.py, line 48, in app_handler
    return json(request.ctx.numbers)</code></pre>
<p>Uh oh, what happened? Well, since we did not define our <code>ctx.numbers</code> container until the response middleware, it was not available inside the handlers. Let’s make a quick change. We will create that object inside of a request middleware. For the sake of our example, we also will return None from the request handler, and instead create our response from our last middleware. In this example, the last middleware to respond will be the first Blueprint response middleware declared.</p>
<pre><code>@bp.on_response
async def complete(request, response):
    return json(request.ctx.numbers)
@app.on_request
async def zero(request):
request.ctx.numbers = []
@app.on_response
async def one(request, response):
    request.ctx.numbers.append(1)
@bp.on_response
async def two(request, response):
    request.ctx.numbers.append(2)
@app.on_response
async def three(request, response):
    request.ctx.numbers.append(3)
@bp.on_response
async def four(request, response):
    request.ctx.numbers.append(4)
@app.on_response
async def five(request, response):
    request.ctx.numbers.append(5)
@bp.on_response
async def six(request, response):
    request.ctx.numbers.append(6)
@bp.get(&quot;/&quot;)
async def bp_handler(request):
    request.ctx.numbers = []
    return json(&quot;blah blah blah&quot;)</code></pre>
<p>Take a close look at the above. We still have a mixture of application and blueprint middleware. We create the numbers container inside of the handler. Also, it is important to note that we are using the exact same ordering that we used for the request middleware that yielded: 1, 3, 5, 2, 4, 6. The changes here are merely to show us how response middleware reverses its order. Can you guess what order our numbers will be in? Let’s check:</p>
<pre><code>$ curl localhost:7777/six
[5,3,1,6,4,2]</code></pre>
<p>First, all of the application-wide response middleware runs (in reverse order of declaration). Second, all of the blueprint-specific middleware runs (in reverse order of declaration). Keep this distinction in mind when you are creating your response middleware if they are interconnected.</p>
<p>Whereas a common use-case for request middleware is to add some data to the request object for further processing, this is not so practical for response middleware. Our above example is a bit odd and impractical. What then is response middleware good for? The most common use case is probably setting headers and cookies.</p>
<p>Here is a simple (and very common) use case:</p>
<pre><code>@app.on_response
async def add_correlation_id(request: Request, response: HTTPResponse):
    header_name = request.app.config.REQUEST_ID_HEADER
    response.headers[header_name] = request.id</code></pre>
<p>Why would you want to do this? Many web APIs use what is known as a <em>correlation ID</em> to help identify individual requests. This is helpful for logging purposes, for tracking a request as it trickles through various systems in your stack, and also for clients that are consuming your API to keep track of what is happening. Sanic latches onto this principal and will set the <code>request.id</code> automatically for you. This value will either be the incoming correlation ID from the incoming request headers, or a unique value generated per request. By default, Sanic will generate a UUID for this value. You usually will not need to worry about this unless you want to use something other than a UUID for correlating web requests. If you are interested in how you can override Sanic’s logic for generating these, checkout <em>Chapter 11</em>, <em>A complete real-world example</em>.</p>
<p>Coming back to our above example, we see that we are simply grabbing that value and appending it to our response headers. We can now see it in action:</p>
<pre><code>$ curl localhost:7777 -i 
HTTP/1.1 200 OK
X-Request-ID: 1e3f9c46-1b92-4d33-80ce-cca532e2b93c
content-length: 9
connection: keep-alive
content-type: text/plain; charset=utf-8
Hello, world.</code></pre>
<p>This small snippet is something I would highly encourage you to add to all of your applications. It is extremely beneficial when you pair it with request ID logging. This is also something we will add into our application in <em>Chapter 11</em>.</p>


<h3 data-number="7.3.3">Responding early (or late) with middleware</h3>
<p>When we explored the response middleware ordering example from the last section, did you notice something peculiar happening with our responses? Did you see this:</p>
<pre><code>@bp.on_response
async def complete(request, response):
    return json(request.ctx.numbers)
...
@bp.get(&quot;/&quot;)
async def bp_handler(request):
    request.ctx.numbers = []
    return json(&quot;blah blah blah&quot;)</code></pre>
<p>We had a non-sensical response from the handler, but it was not returned. That is because in our middleware we returned an <code>HTTPResponse</code> object. Whenever you return a value from middleware–whether request or response–Sanic will assume that you are trying to end HTTP lifecycle and return immediately. Therefore, you should <em>never</em> return anything from middleware that is:</p>
<ul>
<li>Not an <code>HTTPResponse</code> object</li>
<li>Not intended to interrupt the HTTP lifecycle</li>
</ul>
<p>This rule, however, does not apply to None values. You can still use return <code>None</code> if you simply want to halt execution of the middleware.</p>
<pre><code>@app.on_request
async def check_for_politeness(request: Request):
    if &quot;please&quot; in request.headers:
        return None
    return text(&quot;You must say please&quot;)</code></pre>
<p>Let’s see how this plays out now:</p>
<pre><code>$ curl localhost:7777/show-me-the-money                
You must say please
$ curl localhost:7777/show-me-the-money -H &quot;Please: With a cherry on top&quot;</code></pre>
<p>In the second request, it was allowed to proceed because it had the correct header. Therefore, we can see that returning <code>None </code>is also acceptable from middleware. If you are familiar with using continue inside of a Python loop, it has roughly the same impact: halt execution, and move onto the next step.</p>
<blockquote>
<p><strong>Important note</strong></p>
<p>Even though we were looking for the value <code>please </code>in the request headers, we were able to pass <code>Please </code>and for it to still work since headers are always case-insensitive.</p>
</blockquote>


<h3 data-number="7.3.4">Middleware and streaming responses</h3>
<p>There is one more <em>gotcha</em> that you should know about middleware. Remember how we simply said that the middleware basically wraps before and after the route handler? This is not entirely true.</p>
<p>Truthfully, the middleware wraps the generation of the response. Since this <em>usually</em> happens in the return statement of a handler, that is why we take the simplistic approach.</p>
<p>This point can be easily seen if we revisit our Chapter 5 example with our streaming handler. Here is where we started:</p>
<pre><code>@app.get(&quot;/&quot;)
async def handler(request: Request):
    resp = await request.respond()
    for _ in range(4):
        await resp.send(b&quot;Now I&#39;m free, free-falling&quot;)
        await asyncio.sleep(1)
    await resp.eof()</code></pre>
<p>Let’s add some print statements and some middleware so we can examine the order of execution.</p>
<pre><code>@app.get(&quot;/&quot;)
async def handler(request: Request):
    print(&quot;before respond()&quot;)
    resp = await request.respond()
    print(&quot;after respond()&quot;)
    for _ in range(4):
        print(&quot;sending&quot;)
        await resp.send(b&quot;Now I&#39;m free, free-falling&quot;)
        await asyncio.sleep(1)
    print(&quot;cleanup&quot;)
    await resp.eof()
    print(&quot;done&quot;)
@app.on_request
async def req_middleware(request):
    print(&quot;request middleware&quot;)
@app.on_response
async def resp_middleware(request, response):
    print(&quot;response middleware&quot;)</code></pre>
<p>Now we will hit the endpoint, and look at our terminal logs:</p>
<pre><code>request middleware
before respond()
response middleware
after respond()
sending
(sanic.access)[INFO][127.0.0.1:49480]: GET http://localhost:7777/  200 26
sending
sending
sending
cleanup
done</code></pre>
<p>As we would expect, the request middleware runs first, and then we begin the route handler. But, the response middleware runs immediately after we call: <code>request.respond()</code>. For most use cases of response middleware (like adding headers), this should not matter. It will, however, pose a problem if you absolutely must execute some bit of code <em>after</em> the route handler is complete. If this is the case, then your solution is to use signals, which we will explore later in this Chapter.</p>
<p>But first, we are going to explore signals, which sometimes are a great replacement for middleware. While middleware essentially is a tool that allows us to extend business logic outside the confines of the route handler, and to share it among different endpoints, we will learn that signals are more like breakpoints that allow us to interject code into Sanic.</p>



<h2 data-number="7.4">Leveraging signals for intra-worker communication</h2>
<p>In general, Sanic tries to make it possible for developers to extend its capabilities to create custom solutions. This is the reason that when interfacing with Sanic, there are a number of options to inject custom classes to overtake, change, or otherwise extend its functionality. For example, did you know that you could swap out its HTTP protocol to essentially turn Sanic into an FTP server (or any other TCP-based protocol)? Or, maybe you want to extend the router capabilities?</p>
<p>These sorts of customizations are rather quite advanced. We will not cover them in this book since for most use cases it is the equivalent of hanging a picture nail on your wall with a sledgehammer.</p>
<p>The Sanic team introduced signals as a method to extend the functionality of the platform in a more user-friendly format. Very intentionally, setting up a signal handler looks and feels like a route handler:</p>
<pre><code>@app.signal(&quot;http.lifecycle.begin&quot;)
async def connection_begin(conn_info):
    print(&quot;Hello from http.lifecycle.begin&quot;)</code></pre>
<p>You may be asking: what exactly is this, and how can I use it? In this example, we learn that <code>http.lifecycle.begin</code> is an event name. When Sanic opens an HTTP connection to a client, it dispatches this signal. Sanic will then look to see if there are any handlers waiting for it and run them. Therefore, all we did was setup a handler to attach to that event. We will dig a little more into the pre-defined events in this Chapter. But first, let’s take a closer examination at the structure and operation of signals.</p>

<h3 data-number="7.4.1">Signal definitions</h3>
<p>All signals are defined by their event name, which is composed of three segments. We just saw a signal event called: <code>http.lifecycle.begin</code>. Obviously, the three segments are <code>http</code>, <code>lifecycle</code>, and <code>begin</code>. An event will <em>only</em> ever have three segments.</p>
<p>This is important to know because even though Sanic ships with a bunch of signals out of the box, it also allows us to create our own signals along the way. Therefore, we will need to follow the pattern. It is helpful to think of the first segment as a namespace, the middle as a reference, and the last as an action. Sort of like this:</p>
<pre><code>namespace.reference.action</code></pre>
<p>Thinking in these terms helps me conceptualize them. I like to think of them like routes. In fact, they actually are! Under the hood, Sanic deals with signal handlers the same way as it does with route handlers because they inherit from the same base class.</p>
<p>If a signal is essentially a route, does that mean it can look for dynamic path parameters too? Yes! Check this out:</p>
<pre><code>@app.signal(&quot;http.lifecycle.&lt;foo&gt;&quot;)
async def handler(**kwargs):
    print(&quot;Hello!!!&quot;)</code></pre>
<p>Go hit any route in your application now, and we should see the following in our terminal:</p>
<pre><code>[DEBUG] Dispatching signal: http.lifecycle.begin
Hello!!!
[DEBUG] Dispatching signal: http.lifecycle.read_head
Hello!!!
[DEBUG] Dispatching signal: http.lifecycle.request
Hello!!!
[DEBUG] Dispatching signal: http.lifecycle.handle
Hello!!!
request middleware
response middleware
[DEBUG] Dispatching signal: http.lifecycle.response
Hello!!!
[INFO][127.0.0.1:39580]: GET http://localhost:7777/  200 20
[DEBUG] Dispatching signal: http.lifecycle.send
Hello!!!
[DEBUG] Dispatching signal: http.lifecycle.complete
Hello!!!</code></pre>
<p>Before continuing on to see what signals are available, there is one more thing we need to be aware of: condition. The <code>app.signal()</code> method accepts a keyword argument called condition that can help in limiting the events that match on it. Only an event that is dispatched with the same condition will be executed.</p>
<p>We will look at a concrete example.</p>
<ol>
<li><p>Start by adding some request middleware.</p>
<pre><code>@app.on_request
async def req_middleware(request):
    print(&quot;request middleware&quot;)</code></pre></li>
<li><p>Then add a signal to attach to our middleware (this is a built-in as we will see later).</p>
<pre><code>@app.signal(&quot;http.middleware.before&quot;)
async def handler(**kwargs):
    print(&quot;Hello!!!&quot;)</code></pre></li>
<li><p>Now, let’s go take a look at our terminal after we hit an endpoint:</p>
<pre><code>[DEBUG] Dispatching signal: http.middleware.before
request middleware</code></pre>
<p>Hmmm, we see that the signal was dispatched, and that our middleware ran, but our signal handlers did not. Why? The <code>http.middleware.*</code> events are special in that they will only match when a specific <strong>condition</strong> is met. Therefore, we need to amend our signal definition to include the required condition.</p></li>
<li><p>Change your signal to add the condition like this:</p>
<pre><code>@app.signal(&quot;http.middleware.before&quot;, condition={&quot;attach_to&quot;: &quot;request&quot;})
async def handler(**kwargs):
    print(&quot;Hello!!!&quot;)</code></pre></li>
<li><p>Hit the endpoint again. We should now see the text as anticipated.</p>
<pre><code>[DEBUG] Dispatching signal: http.middleware.before
Hello!!!
request middleware</code></pre></li>
</ol>
<p>Conditions are something that you can also add to your custom signal dispatches (keep reading ahead to the <em>Custom signals</em> section to learn more). It would look like this:</p>
<pre><code>app.dispatch(&quot;custom.signal.event&quot;, condition={&quot;foo&quot;: &quot;bar&quot;})</code></pre>
<p>Most signal use cases will not need this approach. However, if you find the need for additional control on signal dispatching, it might just be the right tool for the job. Let’s turn our attention back to Sanic’s built-in signals and see what other events we can attach signals to.</p>


<h3 data-number="7.4.2">Using built-in signals</h3>
<p>There are a number of built-in signals that we can use. Take a look at the tables below and dog-ear this page in the book. I highly encourage you to come back to this table often and look at your options when trying to solve a problem. While the implementations and usages we come up with in this book may be small, it is your job to learn the process so you can more effectively solve your own application needs.</p>
<p>First, are the signals related to routing. They will execute on every request.</p>
<table>
<tbody>
<tr class="odd">
<td><strong>Event name</strong></td>
<td><strong>Arguments</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="even">
<td><code>http.routing.before</code></td>
<td><code>request</code></td>
<td>When Sanic is ready to resolve the incoming path to a route</td>
</tr>
<tr class="odd">
<td><code>http.routing.after</code></td>
<td><code>request, route, kwargs, handler</code></td>
<td>Immediately after a route has been found</td>
</tr>
</tbody>
</table>
Table 6.2 - Available built-in routing signals
<p>Second, we have the signals that are specifically related to the request/response lifecycle.</p>
<table>
<tbody>
<tr class="odd">
<td><strong>Event name</strong></td>
<td><strong>Arguments</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="even">
<td><code>http.lifecycle.begin</code></td>
<td><code>conn_info</code></td>
<td>When an HTTP connection is established</td>
</tr>
<tr class="odd">
<td><code>http.lifecycle.read_head</code></td>
<td><code>head</code></td>
<td>After an HTTP head is read, but before it is parsed</td>
</tr>
<tr class="even">
<td><code>http.lifecycle.request</code></td>
<td><code>request</code></td>
<td>Immediately upon the creation of a Request object</td>
</tr>
<tr class="odd">
<td><code>http.lifecycle.handle</code></td>
<td><code>request</code></td>
<td>Before Sanic begins to handle a request</td>
</tr>
<tr class="even">
<td><code>http.lifecycle.read_body</code></td>
<td><code>body</code></td>
<td>Every time bytes are read from a request body</td>
</tr>
<tr class="odd">
<td><code>http.lifecycle.exception</code></td>
<td><code>request, exception</code></td>
<td>When an exception is raised in a route handler or middleware</td>
</tr>
<tr class="even">
<td><code>http.lifecycle.response</code></td>
<td><code>request, response</code></td>
<td>Just before a response is sent</td>
</tr>
<tr class="odd">
<td><code>http.lifecycle.send</code></td>
<td><code>data</code></td>
<td>Everytime date is sent to an HTTP transport</td>
</tr>
<tr class="even">
<td><code>http.lifecycle.complete</code></td>
<td><code>conn_info</code></td>
<td>When an HTTP connection is closed</td>
</tr>
</tbody>
</table>
Table 6.3 - Available built-in request/response lifecycle signals
<p>Third, we have the events that wrap around each middleware handler. These are not likely signals that you will use often. Instead, they primarily exist for the benefit of Sanic plugin developers.</p>
<table>
<tbody>
<tr class="odd">
<td><strong>Event name</strong></td>
<td><strong>Arguments</strong></td>
<td><strong>Conditions</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="even">
<td><code>http.middleware.before</code></td>
<td><code>request, response</code></td>
<td><code>{"attach_to": "request"} or {"attach_to": "response"}</code></td>
<td>Before each middleware runs</td>
</tr>
<tr class="odd">
<td><code>http.middleware.after</code></td>
<td><code>request, response</code></td>
<td><code>{"attach_to": "request"} or {"attach_to": "response"}</code></td>
<td>After each middleware runs</td>
</tr>
</tbody>
</table>
Table 6.4 - Available built-in middleware signals
<p>Finally, we have the server events. These signals are a one-to-one match with the listener events. Although you can call them as a signal, there is a convenient decorator for each of them as indicated in the descriptions.</p>
<table>
<tbody>
<tr class="odd">
<td><strong>Event name</strong></td>
<td><strong>Arguments</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="even">
<td><code>server.init.before</code></td>
<td><code>app, loop</code></td>
<td>Before a server starts up (equivalent to <code>app.before_server_start</code> )</td>
</tr>
<tr class="odd">
<td><code>server.init.after</code></td>
<td><code>app, loop</code></td>
<td>After a server starts up (equivalent to <code>app.after_server_start</code> )</td>
</tr>
<tr class="even">
<td><code>server.shutdown.before</code></td>
<td><code>app, loop</code></td>
<td>Before a server shuts down (equivalent to <code>app.before_server_stop</code> )</td>
</tr>
<tr class="odd">
<td><code>server.shutdown.after</code></td>
<td><code>app, loop</code></td>
<td>After a server shuts down (equivalent to <code>app.after_server_stop</code> )</td>
</tr>
</tbody>
</table>
Table 6.5 - Available built-in server lifecycle signals
<p>I want to share an anecdote that exemplifies the power of signals. I do a lot of support for Sanic users. If you have spent any time looking over the community resources (either the Forums or the Discord server), you likely have seen me helping developers solve their problems. I really do enjoy this aspect of being involved in OSS.</p>
<p>On one occasion I was asked by someone who was having trouble with middleware. The goal was to use response middleware to log out helpful information about responses as they were being delivered from the server. The problem is that when an exception is raised in middleware, it will halt the rest of the middleware from running. Therefore, this individual was not able to log every response. The requests that raised an exception in other response middleware never made it to the logger. The solution—as you have probably guessed—was to use signals. In particular, the <code>http.lifecycle.response</code> event worked perfectly for this use case.</p>
<p>To show the point, here is some code:</p>
<ol>
<li><p>Setup two middleware, one for logging and one for causing an exception. Remember, they need to be in reverse order from how you want them to run:</p>
<pre><code>@app.on_response
async def log_response(request, response):
    logger.info(&quot;some information for your logs&quot;)
@app.on_response
async def something_bad_happens_here(request, response):
    raise InvalidUsage(&quot;Uh oh&quot;)</code></pre></li>
<li>When we hit any endpoint, <code>log_response</code> will <em>NEVER</em> be run.</li>
<li><p>To solve, change <code>log_response</code> from middleware into a signal (which is as easy as changing the decorator):</p>
<pre><code>@app.signal(&quot;http.lifecycle.response&quot;)
async def log_response(request, response):
    logger.info(&quot;some information for your logs&quot;)</code></pre></li>
<li><p>Now, when we access the endpoint and experience the exception, we still get our logs as expected:</p>
<pre><code>[ERROR] Exception occurred in one of response middleware handlers
Traceback (most recent call last):
  File &quot;/home/adam/Projects/Sanic/sanic/sanic/request.py&quot;, line 183, in respond
    response = await self.app._run_response_middleware(
  File &quot;_run_response_middleware&quot;, line 22, in _run_response_middleware
    from ssl import Purpose, SSLContext, create_default_context
  File &quot;/tmp/p.py&quot;, line 23, in something_bad_happens_here
    raise InvalidUsage(&quot;Uh oh&quot;)
sanic.exceptions.InvalidUsage: Uh oh
[DEBUG] Dispatching signal: http.lifecycle.response
[INFO] some information for your logs
[INFO][127.0.0.1:40466]: GET http://localhost:7777/  200 3</code></pre></li>
</ol>
<p>We can also use this exact same signal to solve one of our earlier problems. Remember when we were examining response middleware and had somewhat surprising results with a streaming handler? If you go back to earlier in this Chapter, we noticed that response middleware actually was called when the response object was created, not after the handler completed. We could use <code>http.lifecycle.response</code> to wrap up after our lyrics are done streaming.</p>
<pre><code>@app.signal(&quot;http.lifecycle.response&quot;)
async def http_lifecycle_response(request, response):
    print(&quot;Finally... the route handler is over&quot;)</code></pre>
<p>This might be another good time for you to put the book down and do some exploration. Go back to that earlier example with the streaming handler and play around with some of these signals. Take a look at the arguments they receive and think about how you might make usage of them. It is also, of course, important to understand the order in which they are dispatched.</p>
<p>After you complete that, we will take a look at creating custom signals and events.</p>


<h3 data-number="7.4.3">Custom signals</h3>
<p>So far, we have been looking specifically at the built-in signals. They are sort of a narrow implementation of what Sanic signals have to offer. While it is helpful to think of them as breakpoints that allow us to insert functionality into Sanic itself, in truth there is a more general concept at play.</p>
<p>Signals allow for intra-application communication. Because they can be dispatched asynchronously as background tasks, it can become a convenient method for one part of your application to inform another that something has happened. This introduces another important concept of signals: they can be dispatched as inline or as tasks.</p>
<p>So far, every single example we have seen with the built-in signals is inline. That is to say that Sanic will halt the processing of a request until the signals complete. This is how we are able to add functionality into the lifecycle while maintaining a consistent flow.</p>
<p>This might not always be desirable. In fact, often times when you want to implement your own solution with custom signals, having them run as a background task gives the application the ability to continue responding to the request, while it goes and does something else.</p>
<p>Let’s take logging for example. Imagine that we are back in our example where we are building an ecommerce application. We want to augment our access logs to include information about the authenticated use (if any), and the number of items they have in their shopping cart. Let’s take our earlier middleware example and convert it to signals:</p>
<ol>
<li><p>We need to create a signal to pull the user and shopping cart information onto our request object. Again, we just need to change the first line.</p>
<pre><code>@app.signal(&quot;http.lifecycle.handle&quot;)
async def fetch_user_and_cart(request):
    cart_id = request.cookies.get(&quot;cart&quot;)
    session_id = request.cookies.get(&quot;session&quot;)
    request.ctx.cart = await fetch_shopping_cart(cart_id)
    request.ctx.user = await fetch_user(session_id)</code></pre></li>
<li><p>For the sake of our example, we want to throw together some quick models and fake getters like this:</p>
<pre><code>@dataclass
class Cart:
    items: List[str]
@dataclass
class User:
    name: str
async def fetch_shopping_cart(cart_id):
    return Cart([&quot;chocolate bar&quot;, &quot;gummy bears&quot;])
async def fetch_user(session_id):
    return User(&quot;Adam&quot;)</code></pre></li>
<li><p>This will be enough to get our example operational, but we want to be able to see it. For now, we will add a route handler that just outputs our <code>request.ctx</code>:</p>
<pre><code>@app.get(&quot;/&quot;)
async def route_handler(request: Request):
    return json(request.ctx.__dict__)</code></pre></li>
<li><p>We should now see our fake user and cart are available as expected:</p>
<pre><code>$ curl localhost:7777 -H &#39;Cookie: cart=123&amp;session_id=456&#39;
{
  &quot;cart&quot;: {
    &quot;items&quot;: [
      &quot;chocolate bar&quot;,
      &quot;gummy bears&quot;
    ]
  },
  &quot;user&quot;: {
    &quot;name&quot;: &quot;Adam&quot;
  }
}</code></pre></li>
<li><p>Since we want to use our own access logs, we should turn off Sanic’s access logs. Back in Chapter 2, we decided we were going to run all of our examples like this:</p>
<pre><code>$ sanic server:app -p 7777 --debug --workers=2</code></pre>
<p>We are going to change that now. Add <code>--no-access-logs</code>:</p>
<pre><code>$ sanic server:app -p 7777 --debug --workers=2 --no-access-logs</code></pre></li>
<li><p>Now, we are going to add our own request logger. But, to illustrate the point we are trying to make, we will manually make our signal take a while to respond:</p>
<pre><code>@app.signal(&quot;http.lifecycle.handle&quot;)
async def access_log(request):
    await asyncio.sleep(3)
    name = request.ctx.user.name
    count = len(request.ctx.cart.items)
    logger.info(f&quot;Request from {name}, who has a cart with {count} items&quot;)</code></pre></li>
<li><p>When you access the endpoint, you will see the following in your logs. You also should experience a delay before the logging appears, and also before your response is delivered.</p>
<pre><code>[DEBUG] Dispatching signal: http.lifecycle.request
[DEBUG] Dispatching signal: http.lifecycle.handle
[INFO] Request from Adam, who has a cart with 2 items</code></pre></li>
<li><p>To fix this, we will create a custom signal for our logger, and dispatch the event from <code>fetch_user_and_cart</code>. Let’s make the following changes:</p>
<pre><code>@app.signal(&quot;http.lifecycle.request&quot;)
async def fetch_user_and_cart(request):
    cart_id = request.cookies.get(&quot;cart&quot;)
    session_id = request.cookies.get(&quot;session&quot;)
    request.ctx.cart = await fetch_shopping_cart(cart_id)
    request.ctx.user = await fetch_user(session_id)
    await request.app.dispatch(
        &quot;olives.request.incoming&quot;,
        context={&quot;request&quot;: request},
        inline=True,
    )
@app.signal(&quot;olives.request.incoming&quot;)
async def access_log(request):
    await asyncio.sleep(3)
    name = request.ctx.user.name
    count = len(request.ctx.cart.items)
    logger.info(f&quot;Request from {name}, who has a cart with {count} items&quot;)</code></pre></li>
<li>This time when we go and access the endpoint, there are two things you need to pay attention to. First, your response should return almost immediately. The delayed response we experienced earlier should be gone. Second, the delay in the access log should remain.</li>
</ol>
<p>What we have effectively done here is taken any IO wait time in the logging away from the request cycle. To do this we created a custom signal. That signal was called <code>olives.request.incoming</code>. There is nothing special about this. It is entirely arbitrary. The only requirements, as we discussed, is that it has three parts.</p>
<p>To execute the signal, we just need to call <code>app.dispatch</code> with the same name:</p>
<pre><code>await app.dispatch(&quot;one.two.three&quot;)</code></pre>
<p>Because we wanted to have access to the Request object in <code>access_log</code>, we used the optional argument context to pass the object.</p>
<p>So, why did the <code>http.lifecycle.handle</code> signal delay the response, but <code>olives.request.incoming</code> did not? Because the former was executed <em>inline</em> and the latter as a background task. Under the hood, Sanic calls dispatch with <code>inline=True</code>. Go ahead and add that to the custom dispatch to see how that impacts the response. Once again, both the logging and the response are now delayed. You should use this when you want your application to pause on the dispatch until all signals attached to it are done running. If that order is not important, you will achieve more performance if you leave it out.</p>
<p>There are a few more arguments that dispatch takes that might be helpful for you. Here is the function signature:</p>
<pre><code>def dispatch(
    event: str,
    *,
    condition: Optional[Dict[str, str]] = None,
    context: Optional[Dict[str, Any]] = None,
    fail_not_found: bool = True,
    inline: bool = False,
    reverse: bool = False,
):</code></pre>
<p>The arguments that this function accepts are as follows:</p>
<ul>
<li><code>condition</code>: Used as seen with the middleware signals to control additional matching (we saw this as used by the <code>http.middleware.*</code> signals)</li>
<li><code>context</code>: Arguments that should be passed to the signal</li>
<li><code>fail_not_found</code>: What if you dispatch an event that does not exist? Should it raise an exception or fail silently?</li>
<li><code>inline</code>: Run in a task or not as discussed already</li>
<li><code>reverse</code>: When there are multiple signals on an event, what order should they run in?</li>
</ul>


<h3 data-number="7.4.4">Waiting on events</h3>
<p>The last helpful thing about dispatching a signal event is that they also can be used like asyncio events to block until it is dispatched. The use case for this is different than dispatching. When you dispatch a signal, you are causing some other operation to occur, usually in a background task. You should wait on a signal event when you want to pause an existing task until that event happens. This means that it will block the currently existing task, whether that is a background task or the actual request that is being handled.</p>
<p>The easiest way to show this is with a super simple loop that runs constantly in your application.</p>
<ol>
<li><p>Setup your loop like this. Notice that we are using <code>app.event</code> with our event name. For simplicity, we are using a built-in signal event, but it could also be a custom. In order to work, we would just need an app.signal to be registered with the same name.</p>
<pre><code>async def wait_for_event(app: Sanic):
    while True:
        print(&quot;&gt; waiting&quot;)
        await app.event(&quot;http.lifecycle.request&quot;)
        print(&quot;&gt; event found&quot;)
@app.after_server_start
async def after_server_start(app, loop):
    app.add_task(wait_for_event(app))</code></pre></li>
<li><p>Now, when we hit our endpoint, we should see this in the logs:</p>
<pre><code>&gt; waiting
[INFO] Starting worker [165193]
[DEBUG] Dispatching signal: http.lifecycle.request
&gt; event found
&gt; waiting</code></pre></li>
</ol>
<p>This might be a helpful tool especially if your application uses websockets. You might, for example, want to keep track of the number of open sockets. Feel free to turn back to the websockets example and see if you can integrate some events and signals into your implementation.</p>
<p>One more helpful use case is where you have a number of things that need to happen in your endpoint before you respond. You want to push off some work to a signal, but ultimately it does need to be complete before responding.</p>
<p>We could do something like this. Setup the following handlers and signals.</p>
<pre><code>@app.signal(&quot;registration.email.send&quot;)
async def send_registration_email(email, request):
    await asyncio.sleep(3)
    await request.app.dispatch(&quot;registration.email.done&quot;)
@app.post(&quot;/register&quot;)
async def handle_registration(request):
    await do_registration()
    await request.app.dispatch(
        &quot;registration.email.send&quot;,
        context={
            &quot;email&quot;: &quot;alice@bob.co&quot;,
            &quot;request&quot;: request,
        },
    )
    await do_something_else_while_email_is_sent()
    print(&quot;Waiting for email send to complete&quot;)
    await request.app.event(&quot;registration.email.done&quot;)
    print(&quot;Done.&quot;)
    return text(&quot;Registration email sent&quot;)</code></pre>
<p>Now when we look at the terminal, we should see this:</p>
<pre><code>do_registration
Sending email
do_something_else_while_email_is_sent
Waiting for email send to complete
Done.</code></pre>
<p>Since we know that sending the email will be an expensive operation, we send that off to the background while continue processing the request. By using app.event, we were able to wait for the <code>registration.email.done</code> event to be dispatched before responding that the email in fact had been sent.</p>
<p>One thing that you should make note of, in this example there is not actually a signal attached to <code>registration.email.done</code>. Out of the box, Sanic will complain and raise an exception. If you would like to use this pattern, you have three options.</p>
<ol>
<li><p>Register a signal.</p>
<pre><code>@app.signal(&quot;registration.email.done&quot;)
async def noop():
    ...</code></pre></li>
<li><p>Since we do not need to actually execute anything, we do not actually need a handler:</p>
<pre><code>app.add_signal(None, &quot;registration.email.done&quot;)</code></pre></li>
<li><p>Tell Sanic to automatically create all events when there is a dispatch, regardless of whether there is a registered signal:</p>
<pre><code>app.config.EVENT_AUTOREGISTER = True</code></pre></li>
</ol>
<p>Now that we know there are a number of ways to control the execution of business logic within an HTTP lifecycle, we will next explore some other things we can do to exploit our new found tools.</p>



<h2 data-number="7.5">Mastering HTTP connections</h2>
<p>Earlier in Chapter 4, we discussed how the HTTP lifecycle represented a conversation between a client and a server. The client requests information, and the server responds. In particular, we likened it to a video chat with bi-directional communication. Let’s dig into this analogy a little deeper to expand our understanding of HTTP and Sanic.</p>
<p>Rather than thinking about an HTTP request as the video chat, it is better to think of it as an individual conversation, or better yet, a single question and answer. Something like this:</p>
<p><strong>Client</strong>: Hi, my session ID is 123456, and my shopping cart ID is 987654. Can you tell me what other items I can buy?</p>
<p><strong>Server</strong>: Hi Adam, you have pure olive oil, and extra virgin olive oil in your cart already. You can add: balsamic vinegar or red wine vinegar.</p>
<p>Sanic is a “performant” web framework because it is capable of having these conversations with multiple clients at the same time. While it is fetching the results for one client, it can begin conversations with other clients:</p>
<p><strong>Client 1</strong>: What products do you sell?</p>
<p><strong>Client 2</strong>: How much does a barrel of olive oil cost?</p>
<p><strong>Client 3</strong>: What is the meaning of life?</p>
<p>By being capable of corresponding within multiple video chat sessions simultaneously, the server has become more efficient at responding. But, what happens when one client has multiple questions? To start and stop the video chat for each “conversation” would be time consuming and costly.</p>
<p><em>Start video chat</em></p>
<p><strong>Client</strong>: Here are my credentials, can I login?</p>
<p><strong>Server</strong>: Hi Adam, nice to see you again, here is a session ID: 123456. Goodbye.</p>
<p><em>Stop video chat</em></p>
<p><em>Start video chat</em></p>
<p><strong>Client</strong>: Hi, my session ID is 123456. Can I update my profile information?</p>
<p><strong>Server</strong>: Oops, Bad Request. Looks like you did not send me the right data. Goodbye.</p>
<p><em>Stop video chat</em></p>
<p>Every time that the video chat starts and stops we are wasting time and resources. HTTP/1.1 sought to solve this problem by introducing persistent connections. This is accomplished with the Keep-Alive header. We do not need to worry specifically about how this header works from the client or server. Sanic will take care of responding appropriately.</p>
<p>What we do need to understand is that it exists, and that it includes a timeout. This means that Sanic will not close the connection to the client if another request comes within some timeout period.</p>
<p><em>Start video chat</em></p>
<p><strong>Client</strong>: Here are my credentials, can I login?</p>
<p><strong>Server</strong>: Hi Adam, nice to see you again, here is a session ID: 123456.</p>
<p><strong>Server</strong>: <em>waiting…</em></p>
<p><strong>Server</strong>: <em>waiting…</em></p>
<p><strong>Server</strong>: <em>waiting…</em></p>
<p><strong>Server</strong>: Goodbye.</p>
<p><em>Stop video chat</em></p>
<p>We have now created an efficiency within a single video chat to allow for multiple conversations.</p>
<p>There are two practical concerns we need to think about here: (1) how long should the server wait? And, (2) can we make the connection more efficient?</p>

<h3 data-number="7.5.1">Keep-Alive within Sanic</h3>
<p>Sanic will keep HTTP connections alive by default. This makes operations more performant as we saw earlier. There may, however, be instances where this is undesirable. Perhaps you <em>never</em> want to keep the connections open. If you know that your application will never handle more than one request per client, then perhaps it is wasteful to use precious memory to keep open a connection that will never be reused. To turn it off, just set a configuration value on your application instance:</p>
<pre><code>app.config.KEEP_ALIVE = False</code></pre>
<p>As you can probably guess, even the most basic web applications will never fall into this category. Therefore, even though we have the ability to turn off keep-alive, you probably should not.</p>
<p>What you are more likely going to want to change is the timeout. By default, Sanic will keep connections open for five seconds. This may not seem long, but it should be long enough for most use cases without being wasteful. This is, however, Sanic just making a complete guess. You are more likely to know and understand the needs of your application, and you should feel free to tune this number to your needs. How? Again, with a simple configuration value:</p>
<pre><code>app.config.KEEP_ALIVE_TIMEOUT = 60</code></pre>
<p>To give you some context, here is a snippet from the Sanic User Guide that provides some insight how other systems operate:</p>
<p><em>“Apache httpd server default keepalive timeout = 5 seconds</em></p>
<p><em>Nginx server default keepalive timeout = 75 seconds</em></p>
<p><em>Nginx performance tuning guidelines uses keepalive = 15 seconds</em></p>
<p><em>IE (5-9) client hard keepalive limit = 60 seconds</em></p>
<p><em>Firefox client hard keepalive limit = 115 seconds</em></p>
<p><em>Opera 11 client hard keepalive limit = 120 seconds</em></p>
<p><em>Chrome 13+ client keepalive limit &gt; 300+ seconds”</em></p>
<p><a href="https://sanicframework.org/en/guide/deployment/configuration.html#keep-alive-timeout">https://sanicframework.org/en/guide/deployment/configuration.html#keep-alive-timeout</a></p>
<p>How do you know if you should increase the timeout? If you are building a single-page application where your API is meant to power a JS frontend, there is a high likelihood that your browser will make a lot of requests. This is generally the nature of how these frontend applications work. This would be especially true if you expect users to click a button, browse through some content, and click some more. The first thing that comes to my mind would be a web portal type application where a single user might need to make dozens of calls within a minute, but they might be spaced out by some interval of browsing time. In this case, increasing the timeout to reflect the expected usage might make sense.</p>
<p>This does not mean that you should increase it too far. First, as we see above browsers generally have a limit to the maximum they will hold a connection open. Second, going too far with connection length can be wasteful and harmful to your memory performance. It is a balance that you are after. There is no one good answer, so you may need to experiment to see what works.</p>


<h3 data-number="7.5.2">Caching data per connection</h3>
<p>If you are thinking about ways that you might exploit some of these tools for your applications needs, you might have noticed a potential efficiency you can create. Back at the beginning of this Chapter there is a table that lists all of the context (<code>ctx</code>) objects that are available to you in Sanic. One of them is connection specific.</p>
<p>This means that not only are you able to create stateful requests, but you can also add state into a single connection. Our simple example will be a counter.</p>
<ol>
<li><p>Start by creating a counter when connection is established. We will use a signal for this:</p>
<pre><code>from itertools import count
@app.signal(&quot;http.lifecycle.begin&quot;)
async def setup_counter(conn_info):
    conn_info.ctx._counter = count()</code></pre></li>
<li><p>Next, we will increment the counter on every request using middleware:</p>
<pre><code>@app.on_request
async def increment(request):
    request.conn_info.ctx.count = next(request.conn_info.ctx._counter)</code></pre></li>
<li><p>Then we will output that in our request body so we can see what this looks like:</p>
<pre><code>@app.get(&quot;/&quot;)
async def handler(request):
    return json({&quot;request_number&quot;: request.conn_info.ctx.count})</code></pre></li>
<li><p>Now, we will issue multiple requests using curl. To do that, we just give it the URL multiple times:</p>
<pre><code>$ curl localhost:7777 localhost:7777
{&quot;request_number&quot;:0}
{&quot;request_number&quot;:1}</code></pre></li>
</ol>
<p>This is of course a trivial example, and we could get that information from Sanic easily enough:</p>
<pre><code>@app.get(&quot;/&quot;)
async def handler(request):
    return json(
        {
            &quot;request_number&quot;: request.conn_info.ctx.count,
            &quot;sanic_count&quot;: request.protocol.state[&quot;requests_count&quot;],
        },
    )</code></pre>
<p>This could be extremely useful if you have some data that might be expensive to obtain, but want it available for all requests. Coming back to our earlier roleplay model, it would be as if your server fetched some details when the video chat started. Now, every time the client asks a question, the server already has the details on hand in cache.</p>
<blockquote>
<p><strong>Important note</strong></p>
<p>This does come with a warning. If your application is exposed through a proxy, it could be connection pooling. That is to say that the proxy could be taking requests from differing clients and bundling them together in one connection. Think of this as if your video chat session was not in someone’s private home, but were instead in the foyer of a large university dormitory. Anyone could walk up to the single video chat session and ask a question. You might not be guaranteed to have the same person all the time. Therefore, before you expose any sort of sensitive details on this object, you must know that it will be safe. Your best practice might just be to keep the sensitive details on <code>request.ctx</code>.</p>
</blockquote>


<h3 data-number="7.5.3">Handling exceptions like a pro</h3>
<p>In an ideal world, our applications would never fail and users would never submit bad information. All endpoints would return a <code>200 OK</code> response all the time. This is, of course, pure fantasy, and no web application could be complete if it did not address the possibility of failures. In real life, our code will have bugs, there will be edge cases not addressed, and users will send us bad data and misuse the application. In short: our application will fail. Therefore, we must think about this constantly.</p>
<p>Sanic does, of course, provide some default handling for us. It includes a few different styles of exception handlers (HTML, JSON, and text), and can be used both in production and development. It is of course unopinionated, and therefore likely inadequate for a decently sized application. We will talk more about the fallback error handling in the <em>Fallback handling</em> section later. As we just learned, handling exceptions in an application is critical to the quality (and ultimately security) of a web application. We will now learn more about how to do that in Sanic.</p>



<h2 data-number="7.6">Implementing proper Exception handling</h2>
<p>Before we look at how to handle exceptions with Sanic, it is important to consider that a failure to properly address this could become a security problem. The obvious way would be through an inadvertent disclosure of sensitive information. This is known as <em>leaking</em>. This occurs when an exception is raised (by mistake of on purpose by the user) and your application reports back exposing details about how the application is built, or the data stored.</p>
<p>In a real-world worst-case scenario, I once had an old forgotten endpoint that no longer worked in one of my web applications. No one used it anymore and I simply forgot that it existed or was even still live. The problem was that the endpoint did not have proper exception handling and errors were directly reported as they occurred. That means even “<em>Failure to connect to database XYZ using username ABC and password EFG</em>”messages were flowing right to anyone that accessed the endpoint. Oops.</p>
<p>Therefore, even though we do not discuss security concerns in general until Chapter 7, it does extend into the current exploration into exception handling. There are two main concerns here: providing exception messages with tracebacks or other implementation details, and incorrectly using 400 series responses.</p>

<h3 data-number="7.6.1">Bad exception messages</h3>
<p>While developing, it is super helpful to have as much information about your request as possible. This is why it would be desirable to have exception messages and tracebacks in your responses. When you are building your applications in debug mode, you will get all of these details. But make sure you turn it off in production! Just like I wish my applications only served <code>200 OK</code> response all the time, I wish I never stumbled onto a website that accidentally leaked debug information to me. It happens out there in the wild, so be careful not to fall into that mistake.</p>
<p>What is perhaps more common is failing to properly consider the content of the errors when responding. When writing messages that will reach the end user, keep in mind that you do not want to accidentally disclose implementation details.</p>


<h3 data-number="7.6.2">Misusing statuses</h3>
<p>Closely related to bad exceptions are exceptions that leak information about your application. Imagine if your bank website had an endpoint that was: <code>/accounts/id/123456789</code>. They do their due diligence and properly protect the endpoint so that only you can access it. That is not a problem. But, what happens to someone that cannot access it? What happens when I try to access your bank account? Obviously I would get a 401 Unauthorized because it is not my account. However, as soon as you do that, the bank is now acknowledging that 123456789 is a legitimate account number. Therefore, I <em>highly</em> encourage you to use the below chart and commit it to memory.</p>
<table>
<tbody>
<tr class="odd">
<td><strong>Status</strong></td>
<td><strong>Description</strong></td>
<td><strong>Sanic Exception</strong></td>
<td><strong>When to use</strong></td>
</tr>
<tr class="even">
<td>400</td>
<td>Bad Request</td>
<td><code>InvalidUsage</code></td>
<td>When any user submits data in an unexpected form or they otherwise did something your application does not intend to handle</td>
</tr>
<tr class="odd">
<td>401</td>
<td>Unauthorized</td>
<td><code>Unauthorized</code></td>
<td>When an unknown user has not been authenticated. In other words, you do not know who the user is.</td>
</tr>
<tr class="even">
<td>403</td>
<td>Forbidden</td>
<td><code>Forbidden</code></td>
<td>When a known user does not have permissions to do something on a <em>KNOWN</em> resource</td>
</tr>
<tr class="odd">
<td>404</td>
<td>Not Found</td>
<td><code>NotFound</code></td>
<td>When any user attempts access on a hidden resource</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
Table 6.6 - Sanic exceptions for common 400 series HTTP responses
<p>Perhaps the biggest failure here is when people inadvertently expose the existence of a hidden resource with a 401 or 403. Your bank should have instead sent me a 404 and directed me to a “page not found” response. This is not to say that you should always favor a 404. But it is to your benefit from a security perspective to think about who could be accessing the information, and what they should or should not know about it. Then, you can decide which error response is appropriate.</p>


<h3 data-number="7.6.3">Responses through raising an exception</h3>
<p>One of the most convenient things about exception handling in Sanic is that it is relatively trivial to get started. Remember, we are just coding a Python script here, and you should treat it like you might anything else. What should you do when something goes wrong? Raise an exception! Here is an example.</p>
<ol>
<li><p>Make a simple handler, we will ignore the return value here since we do not need it to prove our point. Use your imagination for what could be beyond the <code>...</code>.</p>
<pre><code>@app.post(&quot;/cart)
async def add_to_cart(request):
    if &quot;name&quot; not in request.json:
        raise InvalidUsage(&quot;You forgot to send a product name&quot;)
    ...</code></pre></li>
<li><p>Next, we will submit some JSON to the endpoint leaving out the name property. Make sure to use <code>-i</code> so we can inspect the response headers.</p>
<pre><code>$ curl localhost:7777/cart -X POST -d &#39;{}&#39; -i
HTTP/1.1 400 Bad Request
content-length: 83
connection: keep-alive
content-type: text/plain; charset=utf-8
400 — Bad Request
=================
You forgot to send a product name</code></pre></li>
</ol>
<p>Take note how we received a 400 response but did not actually return a response from the handler. This is because if you raise any exception from <code>sanic.exceptions</code> they <em>could</em> be used to return an appropriate status code. Furthermore, you will find that many of the exceptions in that module (like <code>InvalidUsage</code>) have a default <code>status_code</code>. This is why when you raise <code>InvalidUsage </code>Sanic will respond with a 400. You could of course override the status code by passing a different on. Let’s see how that would work:</p>
<ol>
<li><p>Setup this endpoint and change <code>status_code </code>to something other than 400:</p>
<pre><code>@app.post(&quot;/coffee&quot;)
async def teapot(request):
    raise InvalidUsage(&quot;Hmm...&quot;, status_code=418)</code></pre></li>
<li><p>Now, let’s access it:</p>
<pre><code>$ curl localhost:777/coffee -X POST -i      
HTTP/1.1 418 I&#39;m a teapot
content-length: 58
connection: keep-alive
content-type: text/plain; charset=utf-8
418 — I&#39;m a teapot
==================
Hmm...</code></pre></li>
</ol>
<p>As you can see, we passed the 418 status code to the exception. Sanic took that code and properly converted it to the appropriate HTTP response: <code>418 I'm a teapot</code>. Yes, that is a real HTTP response. Don’t believe me? Look it up in RFC 7168 § 2.3.3. <a href="https://datatracker.ietf.org/doc/html/rfc7168#section-2.3.3">https://datatracker.ietf.org/doc/html/rfc7168#section-2.3.3</a></p>
<p>Here is a reference of all of the built-in exceptions and their associated response codes:</p>
<table>
<tbody>
<tr class="odd">
<td><strong>Exception</strong></td>
<td><strong>Status</strong></td>
</tr>
<tr class="even">
<td><code>HeaderNotFound</code></td>
<td>400 Bad Request</td>
</tr>
<tr class="odd">
<td><code>InvalidUsage</code></td>
<td>400 Bad Request</td>
</tr>
<tr class="even">
<td><code>Unauthorized</code></td>
<td>401 Unauthorized</td>
</tr>
<tr class="odd">
<td><code>Forbidden</code></td>
<td>403 Forbidden</td>
</tr>
<tr class="even">
<td><code>FileNotFound</code></td>
<td>404 Not Found</td>
</tr>
<tr class="odd">
<td><code>NotFound</code></td>
<td>404 Not Found</td>
</tr>
<tr class="even">
<td><code>MethodNotSupported</code></td>
<td>405 Method Not Allowed</td>
</tr>
<tr class="odd">
<td><code>RequestTimeout</code></td>
<td>408 Request Timeout</td>
</tr>
<tr class="even">
<td><code>PayloadTooLarge</code></td>
<td>413 Request Entity Too Large</td>
</tr>
<tr class="odd">
<td><code>ContentRangeError</code></td>
<td>416 Request Range Not Satisfiable</td>
</tr>
<tr class="even">
<td><code>InvalidRangeType</code></td>
<td>416 Request Range Not Satisfiable</td>
</tr>
<tr class="odd">
<td><code>HeaderExpectationFailed</code></td>
<td>417 Expectation Failed</td>
</tr>
<tr class="even">
<td><code>ServerError</code></td>
<td>500 Internal Server Error</td>
</tr>
<tr class="odd">
<td><code>URLBuildError</code></td>
<td>500 Internal Server Error</td>
</tr>
<tr class="even">
<td><code>ServiceUnavailable</code></td>
<td>503 Service Unavailable</td>
</tr>
</tbody>
</table>
Table 6.4 Sanic exceptions with built in HTTP responses
<p>It is therefore a really good practice to make usage of these status codes. An obvious example might be when you are looking up something in your database that does not exist:</p>
<pre><code>@app.get(&quot;/product/&lt;product_id:uuid&gt;&quot;)
async def product_details(request, product_id):
    try:
        product = await Product.query(product_id=product_id)
    except DoesNotExist:
        raise NotFound(&quot;No product found&quot;)</code></pre>
<p>Using the Sanic exceptions is perhaps one of the easiest solutions to getting appropriate responses back to the users.</p>
<p>We could of course go one step further. We can make our own custom exceptions that subclass from the Sanic exceptions to leverage the same capability.</p>
<ol>
<li><p>Create an exception that subclasses one of the existing Sanic exceptions:</p>
<pre><code>from sanic.exceptions import InvalidUsage
class MinQuantityError(InvalidUsage):
    ...</code></pre></li>
<li><p>Raise it when appropriate:</p>
<pre><code>@app.post(&quot;/cart&quot;)
async def add_to_cart(request):
    if request.json[&quot;qty&quot;] &lt; 5:
        raise MinQuantityError(
            &quot;Sorry, you must purchase at least 5 of this item&quot;
        )</code></pre></li>
<li><p>See the error when we have a bad request (less than 5 items):</p>
<pre><code>$ curl localhost:777/cart -X POST -d &#39;{&quot;qty&quot;: 1}&#39; -i
HTTP/1.1 400 Bad Request
content-length: 98
connection: keep-alive
content-type: text/plain; charset=utf-8
400 — Bad Request
=================
Sorry, you must purchase at least 5 of this item</code></pre></li>
</ol>
<p>Using and reusing exceptions that inherit from <code>SanicException </code>is highly encouraged. It not only is a good practice because it provides a consistent and clean mechanism for organizing your code, it makes it easy to provide the appropriate HTTP responses.</p>
<p>So far throughout this book, when we have hit an exception with our client (like in the last example), we have received a nice textual representation of that error. In the next section, we will learn about the other types of exception output, and how we can control it.</p>


<h3 data-number="7.6.4">Fallback handling</h3>
<p>Let’s face it: formatting exceptions is mundane. There is little doubt that using our skills we have learned so far that we could build our own set of exception handlers. We know how to use templates, catch exceptions, and return HTTP responses with an error status. But creating those take time and a lot of boilerplate code.</p>
<p>Which is why it is nice that Sanic offers three (3) different exception handlers: HTML, JSON, and plain text. For the most part, the examples in this book have used the plain text handlers only because it has been a more suitable form for presenting information in a book. Let’s go back to our example where we raised a <code>NotFound </code>error and see what it might look like with each of the three types of handlers.</p>

<h4 data-number="7.6.4.1">HTML</h4>
<ol>
<li><p>Setup our endpoint to raise the exception:</p>
<pre><code>@app.get(&quot;/product/&lt;product_name:slug&gt;&quot;)
async def product_details(request, product_name):
    raise NotFound(&quot;No product found&quot;)</code></pre></li>
<li><p>Tell Sanic to use HTML formatting. We will look more into configurations in Chapter 8. For now, we will just set the value right after our Sanic instance:</p>
<pre><code>app = Sanic(__name__)
app.config.FALLBACK_ERROR_FORMAT = &quot;html&quot;</code></pre></li>
<li>Open up a web browser and go to our endpoint. You should see something like this:</li>
</ol>
<figure>
<img src="img/file7.png" alt="Figure 6.1 - Example 404 page showing what the default 404 Not Found HTML page looks like in Sanic" /><figcaption aria-hidden="true">Figure 6.1 - Example 404 page showing what the default 404 Not Found HTML page looks like in Sanic</figcaption>
</figure>


<h4 data-number="7.6.4.2">JSON</h4>
<ol>
<li><p>Use the same setup as before, but change the fallback format to <code>json</code>.</p>
<pre><code>app.config.FALLBACK_ERROR_FORMAT = &quot;html&quot;</code></pre></li>
<li><p>This time we will access the endpoint with curl:</p>
<pre><code>$ curl localhost:7777/product/missing-product
{
  &quot;description&quot;: &quot;Not Found&quot;,
  &quot;status&quot;: 404,
  &quot;message&quot;: &quot;No product found&quot;
}</code></pre></li>
</ol>
<p>Instead of nicely formatted HTML that we saw with the previous example, our exception has been formatted into JSON. This is more appropriate if your endpoint will—for example—be used by a Javascript browser application.</p>


<h4 data-number="7.6.4.3">Text</h4>
<ol>
<li><p>Again using the same setup, we will change the fallback format to <code>text</code>.</p>
<pre><code>app.config.FALLBACK_ERROR_FORMAT = &quot;text&quot;</code></pre></li>
<li><p>We will again use curl to access the endpoint:</p>
<pre><code>$ curl localhost:7777/product/missing-product
404 — Not Found
===============
No product found</code></pre></li>
</ol>
<p>As you can see, there are three convenient formatters for our exceptions that may be appropriate in different circumstances.</p>


<h4 data-number="7.6.4.4">Auto</h4>
<p>The previous three examples used <code>FALLBACK_ERROR_FORMAT</code> to show that there are three types of built-in error formats. There is a fourth option for setting FALLBACK_ERROR_FORMAT: <code>auto</code>. It would look like this.</p>
<pre><code>app.config.FALLBACK_ERROR_FORMAT = &quot;auto&quot;</code></pre>
<p>When the format is set to <code>auto</code>, Sanic will look at the the routing handler and the incoming request to determine what is likely to be the most appropriate handler to use. For example, if a route handler always uses the <code>text()</code> response object, then Sanic will assume that you want the exceptions to also be formatted in <code>text </code>format. The same applies to <code>html()</code> and <code>json()</code> responses.</p>
<p>Sanic will even go one step further than that when in <code>auto </code>mode. It will analyze the incoming request to look at the headers to make sure that what it <em>thinks</em> is correct and matches with what the client said that it wants to receive.</p>


<h4 data-number="7.6.4.5">Manual override per route</h4>
<p>The last option we have is to set the error format on an individual route inside of the route definition. This would allow us to be specific and deviate from the fallback option if needed.</p>
<ol>
<li><p>Consider the example where we set the fallback to <code>html</code>.</p>
<pre><code>app.config.FALLBACK_ERROR_FORMAT = &quot;html&quot;</code></pre></li>
<li><p>Let’s now change our route definition from the beginning of this section to look like the following with a specific defined <code>error_format</code>:</p>
<pre><code>@app.get(&quot;/product/&lt;product_name:slug&gt;&quot;, error_format=&quot;text&quot;)
async def product_details(request, product_name):
    raise NotFound(&quot;No product found&quot;)</code></pre></li>
<li><p>As you might already be able to guess, we will <em>not</em> see a formatted HTML page, but instead will see the plain text from earlier.</p>
<pre><code>$ curl localhost:7777/product/missing-product
404 — Not Found
===============
No product found</code></pre></li>
</ol>



<h3 data-number="7.6.5">Catching exceptions</h3>
<p>Although Sanic conveniently handles a lot of exceptions for us, it goes without saying that it cannot anticipate every error that could be raised in an application. We therefore need to think about how we want to handle exceptions that come from outside of Sanic. Or, rather, how to handle exceptions that are not manually raised by our application using one of the Sanic exceptions that conveniently adds a response code.</p>
<p>Returning to our ecommerce example, let’s imagine that we are using a third-party vendor for handling our credit card transactions. They have conveniently provided us with a module that we can use to process credit cards. When something goes wrong, their module will raise a <code>CreditCardError</code>. Our job now is to make sure that our application is ready to handle this error.</p>
<p>Before we do that, however, let’s see why this is important.</p>
<ol>
<li><p>Imagine that this is our endpoint:</p>
<pre><code>@app.post(&quot;/cart/complete&quot;)
async def complete_transaction(request):
    ...
    await submit_payment(...)
    ...</code></pre></li>
<li><p>Now, we access the endpoint, and if there is an error we get this response:</p>
<pre><code>$ curl localhost:7777/cart/complete -X POST
500 — Internal Server Error
============================
The server encountered an internal error and cannot complete your request.</code></pre></li>
</ol>
<p>That is not a very helpful message. If we look at our logs, however, we might see this:</p>
<pre><code>[ERROR] Exception occurred while handling uri: &#39;http://localhost:7777/cart/complete&#39;
Traceback (most recent call last):
  File &quot;handle_request&quot;, line 83, in handle_request
    &quot;&quot;&quot;
  File &quot;/path/to/server.py&quot;, line 19, in complete_transaction
    await submit_payment(...)
  File &quot;/path/to/server.py&quot;, line 13, in submit_payment
    raise CreditCardError(&quot;Expiration date must be in format: MMYY&quot;)
CreditCardError: Expiration date must be in format: MMYY
[INFO][127.0.0.1:58334]: POST http://localhost:7777/cart/complete  500 144</code></pre>
<p>That error looks potentially far more helpful to our users.</p>
<p>One solution could, of course, just be to catch the exception and return the response that we want:</p>
<pre><code>@app.post(&quot;/cart/complete&quot;)
async def complete_transaction(request):
    ...
    try:
        await submit_payment(...)
    except CreditCardError as e:
        return text(str(e), status=400)
    ...</code></pre>
<p>This pattern is not ideal, however. It would require a lot of extra code when we need to catch every potential exception in various locations in the application to cast them to responses. This also would turn our code into a giant mess of try/except blocks and make things harder to read, and ultimately maintain. In short, it would go against some of the development principles we established early on in this book.</p>
<p>A better solution would be to add an application-wide exception handler. This tells Sanic that anytime this exception bubbles up, it should catch it and respond in a certain way. It looks very much like a route handler:</p>
<pre><code>@app.exception(CreditCardError)
async def handle_credit_card_errors(request, exception):
    return text(str(exception), status=400)</code></pre>
<p>Sanic has now registered this as an exception handler, and will use it anytime that the <code>CreditCardError </code>is raised. Of course, this handler is super simplistic, but you might imagine that it could be used for: extra logging, providing request context, sending out an emergency alert notification to your devops team at 3am, and so on.</p>
<blockquote>
<p><strong>TIP</strong></p>
<p>Error handlers are not limited to your application instance. Just like other regular route handlers, they can be registered on your Blueprint instances to be able to customize error handling for a specific subset of your application.</p>
</blockquote>
<p>Exception handling is an incredibly important part of application development. It is an immediate differentiator between amateur applications and professional application. We now know how we can use exceptions to provide not only helpful messages to our users, but also to provide proper HTTP response codes. We now move on to another topic (background processing) that can really help to take your applications to the next level.</p>



<h2 data-number="7.7">Background processing</h2>
<p>There comes a time in the development of most applications where the developers or users start to notice the application is feeling a bit slow. There are some operations that seem to take a long time and it is harming the usability of the rest of the application. It could be computationally expensive, or it could be because of a network operation reaching out to another system.</p>
<p>Let’s imagine that you are in this scenario. You have built a great application and an endpoint that allows users to generate a PDF report with the click of a button showing all kinds of fancy data and graphs. The problem is that to retrieve all the data and then crunch the numbers seems to take twenty (20) seconds. That’s an eternity for a HTTP request! After spending time to squeeze as much performance out of the report generator as you can, you are finally at the conclusion that it runs as fast as it can. What can you do?</p>
<p>Push it to the background.</p>
<p>When we say “background processing” what we really mean is a solution that allows the current request to complete without having finalized whatever it needs to be done. In this example, it would mean completing the request that <em>starts</em> the report generation before it is actually finished. Whenever and wherever you can, I recommend pushing work to the background. Earlier in the <em>Waiting on events</em> section of this Chapter we saw a use case for sending out registration emails in the background. Indeed the usage of signals as described earlier is a form of background processing. It is, however, not the only tool Sanic provides.</p>

<h3 data-number="7.7.1">Adding tasks to the loop</h3>
<p>As you may already know, one of the cornerstones of the <code>asyncio </code>library are tasks. They are essentially the unit of processing that is responsible for running asynchronous work on the loop. If the concept of a task or the task loop are still foreign to you, it might be a good time to do a little research on the Internet before continuing on.</p>
<p>In the typical scenario, you can generate a task by getting access to the event loop, and then calling <code>create_task</code> as seen here:</p>
<pre><code>import asyncio
async def something():
...
async def main():
loop = asyncio.get_running_loop()
loop.create_task(something())</code></pre>
<p>This is probably not new to you, but what this does is start running <code>something </code>in a task outside of the current one.</p>
<p>Sanic adds a simple interface for creating tasks, as shown here:</p>
<pre><code>async def something():
...
app.add_task(something)</code></pre>
<p>This is probably the simplest form of background processing, and is a pattern that you should get comfortable using. Why use this over <code>create_task</code>? There are three reasons:</p>
<ul>
<li>It is easier since you do not need to fetch the loop</li>
<li>It can be used in the global scope before the loop has started</li>
<li>It can be called or not called, and also with or without the application instance as an argument</li>
</ul>
<p>To illustrate the flexibility, contrast the previous example with this:</p>
<pre><code>from sanic import Sanic
from my_app import something
app = Sanic(“MyAwesomeApp”)
app.add_task(something(app))</code></pre>
<blockquote>
<p><strong>TIP</strong></p>
<p>If the task is not called like the first example, Sanic will introspect the function to see if it expects the <code>app </code>instance as an argument, and inject it.</p>
</blockquote>
<p>Asyncio tasks are very helpful, but sometimes you need a more robust solution. Let’s see what our other options are.</p>


<h3 data-number="7.7.2">Integrating with an outside service</h3>
<p>If there is work to be done by your application, but it is outside of the scope of your API for whatever reason, you might want to turn to an off-the-shelf solution. This comes in the form of another service that is running somewhere else. The job of your web API now is to feed work into that service.</p>
<p>In the Python world, the classic framework for this kind of work is Celery. It is of course not the only option, but since this book is not about deciding what to use, we will show Celery as an example because it is widely used and known. In short, Celery is a platform with workers that read messages from a queue. Some client is responsible for pushing work to the queue, and when a worker receives the message, it executes the work.</p>
<p>For Celery to operate, it runs a process on a machine somewhere. It has a set of known operations that it can perform (that are also called “tasks”). To initiate a task, an outside client needs to connect to it through a broker, and send instructions to run the task. A basic implementation might look like this.</p>
<ol>
<li><p>We setup a client to be able to communicate with the process. A common place to put this is on the <code>application.ctx</code> to make it usable anywhere in the application.</p>
<pre><code>from celery import Celery
@app.before_server_start 
def setup_celery(app, _):
    app.ctx.celery = Celery(...)</code></pre></li>
<li><p>To use it, we simply call the client from the route handler to push some work to Celery.</p>
<pre><code>@app.post(&quot;/start_task&quot;) 
async def start_task(request): 
    task = request.app.ctx.celery.send_task(
        &quot;execute_slow_stuff&quot;, 
        kwargs=request.json
    )
    return text(f&quot;Started task with {task.id=}&quot;, status=202)</code></pre></li>
</ol>
<p>An important thing to point out here is that we are using a <code>202 Accepted </code>status to tell whoever made the request that the operation has been accepted for processing. No guarantee is being made that it is done, or will be done.</p>
<p>After examining Celery, you may be thinking that it is overkill for your needs. But, <code>app.add_task</code> does not seem like enough. Next we look at how you could develop your own in-process queue system.</p>


<h3 data-number="7.7.3">Designing an in-process task queue</h3>
<p>Sometimes the <em>obvious</em> goldilocks solution for your needs is to build something entirely confined to Sanic. It will be easier to manage if you have only one service to worry about instead of multiples. You may still want to keep the idea of “workers” and a “task queue” without the overhead required in implementing a service like Celery. So, let’s build something that hopefully you can use as a launching point for something even more amazing in your applications. Before we get started, you can checkout the final code product in the GitHub repo at: ___.</p>
<p>Before we go any further, let’s change the name from “task queue” to “job queue”. We do not want to confuse ourselves with asyncio tasks for example. For the rest of this section, the word “task” will relate to an asyncio task.</p>
<p>To begin, we will develop a set of needs for our job queue.</p>
<ul>
<li>There should be one or more “workers” that are capable of executing jobs outside of the request/response cycle.</li>
<li>They should execute jobs in a first-in-first-out order.</li>
<li>Completion order of jobs is not important (for example, job A starts before job B, but it does not matter which one finishes first).</li>
<li>We should be able to check on the state of a job.</li>
</ul>
<p>Our strategy to achieve this will be to build out a framework where we have a “worker” that that is itself a background task. Its job will be to look for jobs inside of a common queue and execute them. The concept is very similar to Celery, except we are handling it all within our Sanic application with asyncio tasks. We are going to walk through the source to accomplish this, but not all of it. Implementation details not relevant to this discussion will be skipped here. For full details, please refer to the source code in the GitHub repository: ____.</p>
<ol>
<li><p>To begin, let’s setup a very simple application with a single blueprint.</p>
<pre><code>from sanic import Sanic
from job.blueprint import bp
app = Sanic(__name__)
app.config.NUM_TASK_WORKERS = 3
app.blueprint(bp)</code></pre></li>
<li><p>That blueprint will be the location where we will attach some listeners and our endpoints.</p>
<pre><code>from sanic import Blueprint
from job.startup import (
setup_task_executor,
setup_job_fetch,
register_operations,
)
from job.view import JobListView, JobDetailView
bp = Blueprint(&quot;JobQueue&quot;, url_prefix=&quot;/job&quot;)
bp.after_server_start(setup_job_fetch)
bp.after_server_start(setup_task_executor)
bp.after_server_start(register_operations)
bp.add_route(JobListView.as_view(), &quot;&quot;)
bp.add_route(JobDetailView.as_view(), &quot;/&lt;uid:uuid&gt;&quot;)</code></pre>
<p>As you can see, we have three listeners that we need to run: <code>setup_job fetch</code>, <code>setup_task_executor</code>, and <code>register_operations</code>. We also have two views: one is a list view and the other a detail view. Let’s take each of these items in turn to see what they are.</p></li>
<li><p>Since we want to store the state of our tasks, we need some sort of a datastore. To keep things really simple, I created a file-based database called <code>FileBackend</code>.</p>
<pre><code>async def setup_job_fetch(app, _):
app.ctx.jobs = FileBackend(&quot;./db&quot;)</code></pre></li>
<li><p>The functionality of this job management system will be driven from our job queue, which will be implemented with <code>asyncio.Queue</code>. So, we next need to setup our queue and workers.</p>
<pre><code>async def setup_task_executor(app, _):
app.ctx.queue = asyncio.Queue(maxsize=64)
for x in range(app.config.NUM_TASK_WORKERS):
name = f&quot;Worker-{x}&quot;
print(f&quot;Starting up executor: {name}&quot;)
app.add_task(worker(name, app.ctx.queue, app.ctx.jobs))</code></pre>
<p>After creating our queue, we create one or more background tasks. As you can see, we are simply using Sanic’s <code>add_task </code>method to create a task from the <code>worker </code>function. We will see that function in just a moment.</p></li>
<li><p>The last listener we need will setup an object that will be used to hold all of our potential operations.</p>
<pre><code>async def register_operations(app, _):
app.ctx.registry = OperationRegistry(Hello)</code></pre>
<p>To remind you, and <code>Operation </code>will be something that we want to run in the background. In this example, we have one operation: <code>Hello</code>. Before looking at an operation, let’s look at the two views.</p></li>
<li><p>The list view will have a POST call that is responsible for pushing a new job into the Queue. You can also imagine that this would be an appropriate place to make an endpoint that listed all of the existing jobs (paginated of course). First, it will need to get some data from the request:</p>
<pre><code>class JobListView(HTTPMethodView):
async def post(self, request):
operation = request.json.get(&quot;operation&quot;)
kwargs = request.json.get(&quot;kwargs&quot;, {})
if not operation:
raise InvalidUsage(&quot;Missing operation&quot;)</code></pre>
<p>Here, we perform some very simple data validation. In a real-world scenario, you might want to do some more to make sure that the request JSON conforms to what you are expecting.</p></li>
<li><p>After validating the data, we can push information about the job to the queue.</p>
<pre><code>uid = uuid.uuid4()
await request.app.ctx.queue.put(
{
&quot;operation&quot;: operation,
&quot;uid&quot;: uid,
&quot;kwargs&quot;: kwargs,
}
)
return json({&quot;uid&quot;: str(uid)}, status=202)</code></pre>
<p>We created a UUID. This unique identifier will be used both in storing the job in our database, and retrieving information about it later. Also, it is important to point out that we are using the <code>202 Accepted</code> response since it is the most appropriate form.</p></li>
<li><p>The detail view is very simple. Using the unique identifier, we simply look it up in the database and return it.</p>
<pre><code>class JobDetailView(HTTPMethodView):
async def get(self, request, uid: uuid.UUID):
data = await request.app.ctx.jobs.fetch(uid)
return json(data)</code></pre></li>
<li><p>Coming back to our <code>Hello </code>operation, we will build it now:</p>
<pre><code>import asyncio
from .base import Operation
class Hello(Operation):
async def run(self, name=&quot;world&quot;):
message = f&quot;Hello, {name}&quot;
print(message)
await asyncio.sleep(10)
print(&quot;Done.&quot;)
return message</code></pre>
<p>As you can see, it is a simple object that has a <code>run </code>method. That method will be called by the worker when running a <code>Job</code>.</p></li>
<li><p>The worker is really nothing more than an async function. Its job will be to run a never ending loop. Inside that loop it will wait until there is a job in the queue.</p>
<pre><code>async def worker(name, queue, backend):
while True:
job = await queue.get()
if not job:
break
size = queue.qsize()
print(f&quot;[{name}] Running {job}. {size} in queue.&quot;)</code></pre></li>
<li><p>Once it has the information about how to run a job, it needs to create a job instance, and execute it.</p>
<pre><code>job_instance = await Job.create(job, backend)
async with job_instance as operation:
await job_instance.execute(operation)</code></pre></li>
</ol>
<p>A couple final things to say about this solution: one of its biggest faults is that it has no recovery. If your application crashes or restarts, there is no way to continue processing a job that had already begun. In a true task management process, this is usually an important feature. Therefore, in the GitHub repository in addition to the source used to build this solution, you will find source code for a “subprocess” task queue. I will not walk you through the steps to build it since it is largely a similar exercise with a lot of the same code. However, it differs from this solution in two important ways: it does have the ability to recover and restart an unfinished job, and instead of running in asyncio tasks, it leverages Sanic’s process management listeners to create a subprocess using multiprocessing techniques. Please take some time to look through the source code there as you continue to learn and work your way through this book.</p>



<h2 data-number="7.8">Summary</h2>
<p>In my opinion, one of the biggest leaps that you can make as an application developer is devising strategies to abstract a solution to a problem, and to reuse that solution in multiple places. If you have ever heard of the DRY (Don’t Repeat Yourself) principle, this is what I mean. Applications are seldom ever “complete.” We develop them, maintain them, and change them. If we have too much repetitive code, or code that is too tightly coupled to a single use case, then it becomes more difficult to change it or adapt it to different use cases. Learning to generalize our solutions mitigates this problem.</p>
<p>In Sanic, this means taking logic out of the route handlers. It is best if we can minimize the amount of code in the individual handlers, and instead place that code in other locations where it can be reused by other endpoints. Did you notice how the route handlers in the final example in <em>Designing an in-process task queue</em> had not more than a dozen lines? While the exact length is not important, it is helpful to keep these clean and short and place your logic somewhere else.</p>
<p>Perhaps one of the biggest takeaways from this Chapter should be that there is usually not a single way to do something. Often we can use a mixture of these methodologies to achieve our goal. It is then the job of the application developer to look at the tool belt and decide which tool is best for any given situation.</p>
<p>For this reason, as a Sanic developer you should learn how to devise strategies to respond to web requests outside of the route handler. In this chapter, we learned about some tools to help you accomplish this using middleware; built-in and custom signals; connection management; exception handling; and background processing. Again, think of these as your core tools in your toolbelt. Got a screw that needs tightening? Pull out your middleware. Need to drill a hole in some wood? Time to grab the drill off the shelf. The more familiar you become with basic building blocks like these in Sanic, the greater your understanding will be in how to piece together a professional grade application.</p>
<p>It is your job now to play with these and internalize them on your way to becoming a better developer.</p>
<p>We have scratched the surface of security-related issues. In the next Chapter, we will take a closer look at how we can protect our Sanic applications.h</p>


</body>
</html>
