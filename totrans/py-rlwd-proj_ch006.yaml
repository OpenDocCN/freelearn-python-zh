- en: Chapter 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章
- en: Overview of the Projects
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 项目概述
- en: Our general plan is to craft analytic, decision support modules and applications.
    These applications support decision-making by providing summaries of available
    data to the stakeholders. Decision-making spans a spectrum from uncovering new
    relationships among variables to confirming that data variation is random noise
    within narrow limits. The processing will start with acquiring data and moving
    it through several stages until statistical summaries can be presented.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的一般计划是构建分析、决策支持模块和应用。这些应用通过向利益相关者提供可用数据的摘要来支持决策。决策的范围从揭示变量之间新的关系到确认数据变化在狭窄范围内是随机的噪声。处理将从获取数据并通过几个阶段移动开始，直到可以展示统计摘要。
- en: The processing will be decomposed into several stages. Each stage will be built
    as a core concept application. There will be subsequent projects to add features
    to the core application. In some cases, a number of features will be added to
    several projects all combined into a single chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 处理将被分解为几个阶段。每个阶段将作为一个核心概念应用来构建。将会有后续项目来增加核心应用的功能。在某些情况下，一些功能将被添加到几个项目中，所有这些项目都合并到一个章节中。
- en: The stages are inspired by the **Extract-Transform-Load** (**ETL**) architectural
    pattern. The design in this book expands on the ETL design with a number of additional
    steps. The words have been changed because the legacy terminology can be misleading.
    These features – often required for real-world pragmatic applications — will be
    inserted as additional stages in a pipeline.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段设计灵感来源于**提取-转换-加载**（**ETL**）架构模式。本书中的设计在ETL设计的基础上增加了多个额外步骤。由于旧术语可能具有误导性，因此对这些词进行了更改。这些特性——通常对于现实世界的实用应用是必需的——将作为管道中的额外阶段插入。
- en: Once the data is cleaned and standardized, then the book will describe some
    simple statistical models. The analysis will stop there. You are urged to move
    to more advanced books that cover AI and machine learning.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被清理和标准化，本书将描述一些简单的统计模型。分析将在这里停止。建议您转向更高级的书籍，这些书籍涵盖了人工智能和机器学习。
- en: There are 22 distinct projects, many of which build on previous results. It’s
    not required to do all of the projects in order. When skipping a project, however,
    it’s important to read the description and deliverables for the project being
    skipped. This can help to more fully understand the context for the later projects.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 有22个不同的项目，其中许多基于先前结果。不需要按顺序完成所有项目。但是，在跳过一个项目时，阅读该项目描述和交付成果非常重要。这有助于更全面地理解后续项目的背景。
- en: 'This chapter will cover our overall architectural approach to creating a complete
    sequence of data analysis programs. We’ll use the following multi-stage approach:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍我们创建完整数据分析程序序列的整体架构方法。我们将采用以下多阶段方法：
- en: Data acquisition
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据采集
- en: Inspection of data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据检查
- en: Cleaning data; this includes validating, converting, standardizing, and saving
    intermediate results
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清洗数据；这包括验证、转换、标准化和保存中间结果
- en: Summarizing, and modeling data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结和建模数据
- en: Creating more sophisticated statistical models
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建更复杂的统计模型
- en: The stages fit together as shown in [*Figure 2.1*](#2.1).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段组合方式如图[*图2.1*](#2.1)所示。
- en: '![Figure 2.1: Data Analysis Pipeline ](img/file7.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1：数据分析管道](img/file7.jpg)'
- en: 'Figure 2.1: Data Analysis Pipeline'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：数据分析管道
- en: A central idea behind this is *separation of concerns*. Each stage is a distinct
    operation, and each stage may evolve independently of the others. For example,
    there may be multiple sources of data, leading to several distinct data acquisition
    implementations, each of which creates a common internal representation to permit
    a single, uniform inspection tool.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这背后的一个核心思想是*关注点的分离*。每个阶段都是一个独立的操作，每个阶段可以独立于其他阶段发展。例如，可能有多个数据来源，导致几个不同的数据采集实现，每个实现都创建一个共同的内部表示，以便使用单个、统一的检查工具。
- en: Similarly, data cleansing problems seem to arise almost randomly in organizations,
    leading to a need to add distinct validation and standardization operations. The
    idea is to allocate responsibility for semantic special cases and exceptions in
    this stage of the pipeline.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，数据清洗问题似乎在组织中几乎随机出现，导致需要添加独特的验证和标准化操作。这个想法是在管道的这个阶段分配对语义特殊情况和异常的责任。
- en: One of the architectural ideas is to mix automated applications and a few manual
    JupyterLab notebooks into an integrated whole. The notebooks are essential for
    troubleshooting questions or problems. For elegant reports and presentations,
    notebooks are also very useful. While Python applications can produce tidy PDF
    files with polished reporting, it seems a bit easier to publish a notebook with
    analysis and findings.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 建筑理念之一是将自动化应用程序和几个手动JupyterLab笔记本混合成一个整体。笔记本对于解决故障问题或问题至关重要。对于优雅的报告和演示，笔记本也非常有用。虽然Python应用程序可以生成整洁的PDF文件和完善的报告，但似乎发布带有分析和发现内容的笔记本要容易一些。
- en: We’ll start with the acquisition stage of processing.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从处理阶段的获取阶段开始。
- en: 2.1 General data acquisition
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 通用数据获取
- en: All data analysis processing starts with the essential step of acquiring the
    data from a source.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据分析处理都始于从源获取数据的必要步骤。
- en: 'The above statement seems almost silly, but failures in this effort often lead
    to complicated rework later. It’s essential to recognize that data exists in these
    two essential forms:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 上述声明似乎有些荒谬，但在此方面的失败往往会导致后续复杂的返工。认识到数据存在这两种基本形式至关重要：
- en: Python objects, usable in analytic programs. While the obvious candidates are
    numbers and strings, this includes using packages like **Pillow** to operate on
    images as Python objects. A package like **librosa** can create objects representing
    audio data.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用于分析程序的Python对象。虽然明显的候选者是数字和字符串，但这包括使用**Pillow**等包以Python对象的形式操作图像。像**librosa**这样的包可以创建表示音频数据的对象。
- en: 'A serialization of a Python object. There are many choices here:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python对象的序列化。这里有很多选择：
- en: Text. Some kind of string. There are numerous syntax variants, including CSV,
    JSON, TOML, YAML, HTML, XML, etc.
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本。某种字符串。有无数的语法变体，包括CSV、JSON、TOML、YAML、HTML、XML等。
- en: Pickled Python Objects. These are created by the `pickle` module.
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pickled Python对象。这些是由`pickle`模块创建的。
- en: Binary Formats. Tools like Protobuf can serialize native Python objects into
    a stream of bytes. Some YAML extensions, similarly, can serialize an object in
    a binary format that isn’t text. Images and audio samples are often stored in
    compressed binary formats.
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二进制格式。像Protobuf这样的工具可以将原生Python对象序列化为字节流。类似地，一些YAML扩展可以将对象序列化为非文本的二进制格式。图像和音频样本通常以压缩的二进制格式存储。
- en: The format for the source data is — almost universally — not fixed by any rules
    or conventions. Writing an application based on the assumption that source data
    is **always** a CSV-format file can lead to problems when a new format is required.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 源数据的格式——几乎普遍地——不是由任何规则或惯例固定。基于假设源数据**总是**是CSV格式文件的假设编写应用程序，当需要新的格式时可能会导致问题。
- en: It’s best to treat all input formats as subject to change. The data — once acquired
    — can be saved in a common format used by the analysis pipeline, and independent
    of the source format (we’ll get to the persistence in [*Clean, validate,* *standardize,
    and persist*](#x1-510004)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最好将所有输入格式视为可能发生变化。一旦获取数据，就可以将其保存为分析管道使用的通用格式，并且独立于源格式（我们将在[*清洁、验证、标准化和持久化*](#x1-510004)中讨论持久化）。
- en: 'We’ll start with Project 1.1: ”Acquire Data”. This will build the Data Acquisition
    Base Application. It will acquire CSV-format data and serve as the basis for adding
    formats in later projects.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从项目1.1：“获取数据”开始。这将构建数据获取基础应用程序。它将获取CSV格式数据，并作为在后续项目中添加格式的依据。
- en: There are a number of variants on how data is acquired. In the next few chapters,
    we’ll look at some alternative data extraction approaches.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 数据获取的方法有很多种。在接下来的几章中，我们将探讨一些替代的数据提取方法。
- en: 2.2 Acquisition via Extract
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 通过提取获取
- en: 'Since data formats are in a constant state of flux, it’s helpful to understand
    how to add and modify data formats. These projects will all build on Project 1.1
    by adding features to the base application. The following projects are designed
    around alternative sources for data:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据格式处于不断变化的状态，了解如何添加和修改数据格式很有帮助。这些项目都将基于项目1.1，通过向基础应用程序添加功能来构建。以下项目围绕数据的不同来源设计：
- en: 'Project 1.2: ”Acquire Web Data from an API”. This project will acquire data
    from web services using JSON format.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目1.2：“从API获取Web数据”。此项目将使用JSON格式从网络服务获取数据。
- en: 'Project 1.3: ”Acquire Web Data from HTML”. This project will acquire data from
    a web page by scraping the HTML.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目1.3：“从HTML获取Web数据”。此项目将通过抓取HTML从网页获取数据。
- en: 'Two separate projects are part of gathering data from a SQL database:'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个独立的项目是收集来自SQL数据库的数据的一部分：
- en: 'Project 1.4: ”Build a Local Database”. This is a necessary sidebar project
    to build a local SQL database. This is necessary because SQL databases accessible
    by the public are a rarity. It’s more secure to build our own demonstration database.'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目1.4：“构建本地数据库”。这是一个必要的辅助项目，用于构建本地SQL数据库。这是必要的，因为公众可访问的SQL数据库是罕见的。构建我们自己的演示数据库更安全。
- en: 'Project 1.5: ”Acquire Data from a Local Database”. Once a database is available,
    we can acquire data from a SQL extract.'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目1.5：“从本地数据库获取数据”。一旦数据库可用，我们就可以从SQL提取中获取数据。
- en: These projects will focus on data represented as text. For CSV files, the data
    is text; an application must convert it to a more useful Python type. HTML pages,
    also, are pure text. Sometimes, additional attributes are provided to suggest
    the text should be treated as a number. A SQL database is often populated with
    non-text data. To be consistent, the SQL data should be serialized as text. The
    acquisition applications all share a common approach of working with text.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些项目将专注于以文本形式表示的数据。对于CSV文件，数据是文本；应用程序必须将其转换为更有用的Python类型。HTML页面也是纯文本。有时，还提供了额外的属性，表明文本应被视为数字。SQL数据库通常填充了非文本数据。为了保持一致，SQL数据应序列化为文本。获取应用程序都采用处理文本的共同方法。
- en: 'These applications will also minimize the transformations applied to the source
    data. To process the data consistently, it’s helpful to make a shift to a common
    format. As we’ll see in [*Chapter** 3*](ch007.xhtml#x1-560003), [*Project 1.1:
    Data Acquisition Base Application*](ch007.xhtml#x1-560003) the NDJSON format provides
    a useful structure that can often be mapped back to source files.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些应用还将最小化对源数据应用的转换。为了一致地处理数据，转向一个通用格式是有帮助的。正如我们将在[*第3章*](ch007.xhtml#x1-560003)中看到的，[*项目1.1：数据获取基础应用*](ch007.xhtml#x1-560003)，NDJSON格式提供了一个有用的结构，这通常可以映射回源文件。
- en: After acquiring new data, it’s prudent to do a manual inspection. This is often
    done a few times at the start of application development. After that, inspection
    is only done to diagnose problems with the source data. The next few chapters
    will cover projects to inspect data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在获取新数据后，进行手动检查是谨慎的做法。这通常在应用开发初期进行几次。之后，检查仅用于诊断源数据的问题。接下来的几章将涵盖检查数据的项目。
- en: 2.3 Inspection
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 检查
- en: Data inspection needs to be done when starting development. It’s essential to
    survey new data to be sure it really is what’s needed to solve the user’s problems.
    A common frustration is incomplete or inconsistent data, and these problems need
    to be exposed as soon as possible to avoid wasting time and effort creating software
    to process data that doesn’t really exist.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 数据检查需要在开发初期进行。确保新数据确实是解决用户问题的必要条件至关重要。常见的挫折是不完整或不一致的数据，这些问题需要尽快揭露，以避免浪费时间和精力创建处理不存在数据的软件。
- en: Additionally, data is inspected manually to uncover problems. It’s important
    to recognize that data sources are in a constant state of flux. As applications
    evolve and mature, the data provided for analysis will change. In many cases,
    data analytics applications discover other enterprise changes after the fact via
    invalid data. It’s important to understand the evolution via good data inspection
    tools.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据还通过手动检查来揭露问题。重要的是要认识到数据源处于不断变化的状态。随着应用的演变和成熟，用于分析的数据将发生变化。在许多情况下，数据分析应用通过无效数据在事后发现其他企业变化。了解通过良好的数据检查工具的演变是很重要的。
- en: Inspection is an inherently manual process. Therefore, we’re going to use JupyterLab
    to create notebooks to look at the data and determine some basic features.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 检查是一个本质上手动的过程。因此，我们将使用JupyterLab创建笔记本来查看数据并确定一些基本特征。
- en: In rare cases where privacy is important, developers may not be allowed to do
    data inspection. More privileged people — with permission to see payment card
    or healthcare details — may be part of data inspection. This means an inspection
    notebook may be something created by a developer for use by stakeholders.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在隐私至关重要的罕见情况下，开发者可能不允许进行数据检查。更有特权的人——拥有查看支付卡或医疗细节的权限——可能参与数据检查。这意味着检查笔记本可能是由开发者创建的，供利益相关者使用。
- en: In many cases, a data inspection notebook can be the start of a fully-automated
    data cleansing application. A developer can extract notebook cells as functions,
    building a module that’s usable from both notebook and application. The cell results
    can be used to create unit test cases.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，数据检查笔记本可以是完全自动化数据清洗应用程序的开始。开发者可以将笔记本单元提取为函数，构建一个既可以从笔记本也可以从应用程序中使用的模块。单元结果可以用来创建单元测试用例。
- en: 'The stage in the pipeline requires a number of inspection projects:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 管道中的这一阶段需要多个检查项目：
- en: 'Project 2.1: ”Inspect Data”. This will build a core data inspection notebook
    with enough features to confirm that some of the acquired data is likely to be
    valid.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目2.1：“检查数据”。这将构建一个核心数据检查笔记本，具有足够的特性来确认一些获取的数据可能是有效的。
- en: 'Project 2.2: ”Inspect Data: Cardinal Domains”. This project will add analysis
    features for measurements, dates, and times. These are cardinal domains that reflect
    measures and counts.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目2.2：“检查数据：基数域”。此项目将为测量、日期和时间添加分析功能。这些是反映测量和计数的基数域。
- en: 'Project 2.3: ”Inspect Data: Nominal and Ordinary Domains”. This project will
    add analysis features for text or coded numeric data. This includes nominal data
    and ordinal numeric domains. It’s important to recognize that US Zip Codes are
    digit strings, not numbers.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目2.3：“检查数据：名义和普通域”。此项目将为文本或编码数值数据添加分析功能。这包括名义数据和有序数值域。重要的是要认识到美国邮政编码是数字字符串，而不是数字。
- en: 'Project 2.4: ”Inspect Data: Reference Data”. This notebook will include features
    to find reference domains when working with data that has been normalized and
    decomposed into subsets with references via coded ”key” values.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目2.4：“检查数据：参考数据”。此笔记本将包括在处理已标准化并分解为具有编码“键”值子集的数据时查找参考域的功能。
- en: 'Project 2.5: ”Define a Reusable Schema”. As a final step, it can help define
    a formal schema, and related metadata, using the JSON Schema standard.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目2.5：“定义可重用架构”。作为最后一步，它可以帮助使用JSON Schema标准定义正式架构和相关元数据。
- en: While some of these projects seem to be one-time efforts, they often need to
    be written with some care. In many cases, a notebook will need to be reused when
    there’s a problem. It helps to provide adequate explanations and test cases to
    help refresh someone’s memory on details of the data and what are known problem
    areas. Additionally, notebooks may serve as examples for test cases and the design
    of Python classes or functions to automate cleaning, validating, or standardizing
    data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些项目似乎是一次性的努力，但它们通常需要谨慎编写。在许多情况下，当出现问题时，需要重复使用笔记本。提供充分的解释和测试用例有助于刷新人们对数据细节和已知问题区域的记忆。此外，笔记本可以作为测试用例和自动化清洗、验证或标准化数据的Python类或函数设计的示例。
- en: After a detailed inspection, we can then build applications to automate cleaning,
    validating, and normalizing the values. The next batch of projects will address
    this stage of the pipeline.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 经过详细检查后，我们就可以构建应用程序来自动化清洗、验证和标准化值。下一批项目将解决这个管道阶段的问题。
- en: 2.4 Clean, validate, standardize, and persist
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 清洗、验证、标准化和持久化
- en: Once the data is understood in a general sense, it makes sense to write applications
    to clean up any serialization problems, and perform more formal tests to be sure
    the data really is valid. One frustratingly common problem is receiving duplicate
    files of data; this can happen when scheduled processing was disrupted somewhere
    else in the enterprise, and a previous period’s files were reused for analysis.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦从一般意义上理解了数据，编写应用程序来清理任何序列化问题，并执行更正式的测试以确保数据确实有效是有意义的。一个令人沮丧的常见问题是收到重复的数据文件；这可能会发生在企业中的某个地方预定处理被打断，并且之前时期的数据被重新用于分析。
- en: The validation testing is sometimes part of cleaning. If the data contains any
    unexpected invalid values, it may be necessary to reject it. In other cases, known
    problems can be resolved as part of analytics by replacing invalid data with valid
    data. An example of this is US Postal Codes, which are (sometimes) translated
    into numbers, and the leading zeros are lost.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 验证测试有时是清理的一部分。如果数据包含任何意外的无效值，可能需要拒绝它。在其他情况下，已知问题可以作为分析的一部分通过用有效数据替换无效数据来解决。一个例子是美国邮政编码，它们（有时）被转换为数字，并且前导零丢失。
- en: 'These stages in the data analysis pipeline are described by a number of projects:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据分析管道阶段由多个项目描述：
- en: 'Project 3.1: ”Clean Data”. This builds the data cleaning base application.
    The design details can come from the data inspection notebooks.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目3.1：“清洗数据”。这构建了数据清洗基础应用程序。设计细节可以来自数据检查笔记本。
- en: 'Project 3.2: ”Clean and Validate”. These features will validate and convert
    numeric fields.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目3.2：“清洗和验证”。这些功能将验证并转换数值字段。
- en: 'Project 3.3: ”Clean and Validate Text and Codes”. The validation of text fields
    and numeric coded fields requires somewhat more complex designs.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目3.3：“清洗和验证文本和代码”。验证文本字段和数值编码字段需要更复杂的设计。
- en: 'Project 3.4: ”Clean and Validate References”. When data arrives from separate
    sources, it is essential to validate references among those sources.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目3.4：“清洗和验证引用”。当数据从不同的来源到达时，验证这些来源之间的引用是至关重要的。
- en: 'Project 3.5: ”Standardize Data”. Some data sources require standardizing to
    create common codes and ranges.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目3.5：“标准化数据”。某些数据源需要标准化以创建通用的代码和范围。
- en: 'Project 3.6: ”Acquire and Clean Pipeline”. It’s often helpful to integrate
    the acquisition, cleaning, validating, and standardizing into a single pipeline.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目3.6：“获取和清洗管道”。将获取、清洗、验证和标准化集成到单个管道中通常很有帮助。
- en: 'Project 3.7: ”Acquire, Clean, and Save”. One key architectural feature of this
    pipeline is saving intermediate files in a common format, distinct from the data
    sources.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目3.7：“获取、清洗和保存”。此管道的一个关键架构特性是将中间文件保存为通用格式，与数据源不同。
- en: 'Project 3.8: ”Data Provider Web Service”. In many enterprises, an internal
    web service and API are expected as sources for analytic data. This project will
    wrap the data acquisition pipeline into a RESTful web service.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目3.8：“数据提供者Web服务”。在许多企业中，内部Web服务和API被视为分析数据的来源。此项目将数据获取管道封装为RESTful Web服务。
- en: In these projects, we’ll transform the text values from the acquisition applications
    into more useful Python objects like integers, floating-point values, decimal
    values, and date-time values.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些项目中，我们将从获取应用程序中转换文本值到更有用的Python对象，如整数、浮点值、十进制值和日期时间值。
- en: Once the data is cleaned and validated, the exploration can continue. The first
    step is to summarize the data, again, using a Jupyter notebook to create readable,
    publishable reports and presentations. The next chapters will explore the work
    of summarizing data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗和验证完成后，探索可以继续。第一步是总结数据，再次使用Jupyter笔记本创建可读的、可发布的报告和演示文稿。下一章将探讨总结数据的工作。
- en: 2.5 Summarize and analyze
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 总结和分析
- en: Summarizing data in a useful form is more art than technology. It can be difficult
    to know how best to present information to people to help them make more valuable,
    or helpful decisions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以有用的形式总结数据更多的是艺术而非技术。可能很难知道如何最好地向人们展示信息以帮助他们做出更有价值或更有帮助的决策。
- en: 'There are a few projects to capture the essence of summaries and initial analysis:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个项目用于捕捉总结和初步分析的核心：
- en: 'Project 4.1: ”A Data Dashboard”. This notebook will show a number of visual
    analysis techniques.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目4.1：“数据仪表板”。这个笔记本将展示多种可视化分析技术。
- en: 'Project 4.2: ”A Published Report”. A notebook can be saved as a PDF file, creating
    a report that’s easily shared.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目4.2：“已发布的报告”。可以将笔记本保存为PDF文件，创建一个易于分享的报告。
- en: The initial work of summarizing and creating shared, published reports sets
    the stage for more formal, automated reporting. The next set of projects builds
    modules that provide deeper and more sophisticated statistical models.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 总结和创建共享、发布的报告的初步工作为更正式、自动化的报告奠定了基础。下一组项目将构建提供更深入和更复杂统计模型的模块。
- en: 2.6 Statistical modeling
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 统计建模
- en: 'The point of data analysis is to digest raw data and present information to
    people to support their decision-making. The previous stages of the pipeline have
    prepared two important things:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析的目的在于消化原始数据，向人们展示信息以支持他们的决策。管道的先前阶段准备了两个重要的事情：
- en: Raw data has been cleaned and standardized to provide data that are relatively
    easy to analyze.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始数据已经过清洗和标准化，以提供相对容易分析的数据。
- en: The process of inspecting and summarizing the data has helped analysts, developers,
    and, ultimately, users understand what the information means.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查和总结数据的过程帮助分析师、开发人员和最终用户理解信息的意义。
- en: The confluence of data and deeper meaning creates significant value for an enterprise.
    The analysis process can continue as more formalized statistical modeling. This,
    in turn, may lead to artificial intelligence (AI) and machine learning (ML) applications.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'The processing pipeline includes these projects to gather summaries of individual
    variables as well as combinations of variables:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'Project 5.1: ”Statistical Model: Core Processing”. This project builds the
    base application for applying statistical models and saving parameters about the
    data. This will focus on summaries like mean, median, mode, and variance.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Project 5.2: ”Statistical Model: Relationships”. It’s common to want to know
    the relationships among variables. This includes measures like correlation among
    variables.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This sequence of stages produces high-quality data and provides ways to diagnose
    and debug problems with data sources. The sequence of projects will illustrate
    how automated solutions and interactive inspection can be used to create useful,
    timely, insightful reporting and analysis.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 2.7 Data contracts
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will touch on data contracts at various stages in this pipeline. This application’s
    data acquisition, for example, may have a formalized contract with a data provider.
    It’s also possible that an informal data contract, in the form of a schema definition,
    or an API is all that’s available.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'In [*Chapter** 8*](ch012.xhtml#x1-1950008), [*Project 2.5: Schema and Metadata*](ch012.xhtml#x1-1950008)
    we’ll consider some schema publication concerns. In [*Chapter** 11*](ch015.xhtml#x1-26400011),
    [*Project 3.7: Interim Data Persistence*](ch015.xhtml#x1-26400011) we’ll consider
    the schema provided to downstream applications. These two topics are related to
    a formal data contract, but this book won’t delve deeply into data contracts,
    how they’re created, or how they might be used.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 2.8 Summary
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This data analysis pipeline moves data from sources through a series of stages
    to create clean, valid, standardized data. The general flow supports a variety
    of needs and permits a great deal of customization and extension.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: For developers with an interest in data science or machine learning, these projects
    cover what is sometimes called the ”data wrangling” part of data science or machine
    learning. It can be a significant complication as data is understood and differences
    among data sources are resolved and explored. These are the — sometimes difficult
    — preparatory steps prior to building a model that can be used for AI decision-making.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: For readers with an interest in the web, this kind of data processing and extraction
    is part of presenting data via a web application API or website. Project 3.7 creates
    a web server, and will be of particular interest. Because the web service requires
    clean data, the preceding projects are helpful for creating data that can be published.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: For folks with an automation or IoT interest, *Part 2* explains how to use Jupyter
    Notebooks to gather and inspect data. This is a common need, and the various steps
    to clean, validate, and standardize data become all the more important when dealing
    with real-world devices subject to the vagaries of temperature and voltage.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对自动化或物联网感兴趣的人，**第2部分**解释了如何使用Jupyter Notebooks收集和检查数据。这是一个常见需求，当处理受温度和电压变化影响的现实世界设备时，清理、验证和标准化数据的各个步骤变得更加重要。
- en: 'We’ve looked at the following multi-stage approach to doing data analysis:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经研究了以下多阶段方法来进行数据分析：
- en: Data Acquisition
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据采集
- en: Inspection of Data
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据检查
- en: Clean, Validate, Standardize, and Persist
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理、验证、标准化和持久化
- en: Summarize and Analyze
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概括和分析
- en: Create a Statistical Model
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建统计模型
- en: This pipeline follows the Extract-Transform-Load (ETL) concept. The terms have
    been changed because the legacy words are sometimes misleading. Our acquisition
    stage overlaps with what is understood as the ”Extract” operation. For some developers,
    Extract is limited to database extracts; we’d like to go beyond that to include
    other data source transformations. Our cleaning, validating, and standardizing
    stages are usually combined into the ”Transform” operation. Saving the clean data
    is generally the objective of ”Load”; we’re not emphasizing a database load, but
    instead, we’ll use files.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此管道遵循提取-转换-加载（ETL）概念。术语已经改变，因为旧术语有时会误导。我们的采集阶段与通常所说的“提取”操作重叠。对于一些开发者来说，“提取”仅限于数据库提取；我们希望超越这一点，包括其他数据源转换。我们的清理、验证和标准化阶段通常合并为“转换”操作。保存干净数据通常是“加载”的目标；我们不是强调数据库加载，而是将使用文件。
- en: 'Throughout the book, we’ll describe each project’s objective and provide the
    foundation of a sound technical approach. The details of the implementation are
    up to you. We’ll enumerate the deliverables; this may repeat some of the information
    from [*Chapter** 1*](ch005.xhtml#x1-170001), [*Project Zero: A Template for Other
    Projects*](ch005.xhtml#x1-170001). The book provides a great deal of information
    on acceptance test cases and unit test cases — the definition of done. By covering
    the approach, we’ve left room for you to design and implement the needed application
    software.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们将描述每个项目的目标，并提供一个可靠的技术方法的基础。实施细节由你决定。我们将列出可交付成果；这可能会重复一些来自[*第1章*](ch005.xhtml#x1-170001)、[*项目零：其他项目的模板*](ch005.xhtml#x1-170001)的信息。本书提供了大量关于验收测试用例和单元测试用例的信息——完成的标准。通过涵盖这种方法，我们为你留下了设计和实现所需应用程序软件的空间。
- en: In the next chapter, we’ll build the first data acquisition project. This will
    work with CSV-format files. Later projects will work with database extracts and
    web services.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将构建第一个数据采集项目。这将使用CSV格式的文件。后续的项目将使用数据库提取和Web服务。
