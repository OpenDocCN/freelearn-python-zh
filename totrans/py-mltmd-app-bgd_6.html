<html><head></head><body>
  <div><div><div><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Audio Controls and Effects</h1></div></div></div><div><blockquote class="blockquote"><p>In the previous chapter, the focus was on learning fundamentals of audio processing. It introduced us to the GStreamer multimedia framework. We applied this knowledge to develop some frequently needed audio processing tools. In this chapter, we will go one step further by developing tools for adding audio effects, mixing audio tracks, creating custom music tracks, and so on.</p></blockquote></div><p>In this chapter, we shall:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Learn how to control a streaming audio.</li><li class="listitem" style="list-style-type: disc">Spice up the audio by adding effects such as fading, echo, and panorama.</li><li class="listitem" style="list-style-type: disc">Work on a project where a custom music track will be created by combining different audio clips.</li><li class="listitem" style="list-style-type: disc">Add visualization effect to a streaming audio.</li></ul></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Mix two audio streams into a single track. For example, mix an audio containing only a<em> vocal track</em> with an audio containing only<em> background music track</em>.</li></ul></div><p>So let's get on with it.</p><div><div><div><div><h1 class="title"><a id="ch06lvl1sec01"/>Controlling playback</h1></div></div></div><p>In an audio player, various options such as Play, Pause, Stop, and so on, provide a way to control the streaming audio. Such playback controls also find use in other audio processing techniques. We have already used some of the playback controls in<a class="link" href="ch05.html" title="Chapter 5. Working with Audios"> Chapter 5</a>,<em> Working with Audios</em>. In this chapter, we will study some more controlling options.<a id="id245" class="indexterm"/>
</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec01"/>Play</h2></div></div></div><p>In the previous chapter, we developed a preliminary command-line audio player using GStreamer. The audio streaming can be started by instructing the GStreamer pipeline to begin the flow of audio data. This was achieved by the following code:<a id="id246" class="indexterm"/>
</p><div><pre class="programlisting">self.pipeline.set_state(gst.STATE_PLAYING)
</pre></div><p>With the above instruction, the audio will be streamed until the end of the stream is reached. Refer to the code in the<em> Playing Audio</em> section of<a class="link" href="ch05.html" title="Chapter 5. Working with Audios"> Chapter 5</a>,<em> Working with Audios</em> to see what the surrounding code looks like. If you develop a user interface for a simple audio player, the "Play" button can be connected to a method that will set the state of pipeline to<code class="literal"> gst.STATE_PLAYING</code>.<a id="id247" class="indexterm"/>
</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec02"/>Pause/resume</h2></div></div></div><p>The streaming audio can be paused temporarily by setting the GStreamer pipeline state to<code class="literal"> gst.STATE_PAUSED</code>. Pausing music in an audio player is another commonly performed operation. But this also finds use while doing some special audio processing.<a id="id248" class="indexterm"/>
</p></div></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec02"/>Time for action -  pause and resume a playing audio stream</h1></div></div></div><p>We will now review a very simple example demonstrating various playback control techniques. The same example will be used in the next few sections. This exercise will be an ideal preparation while working on the project 'Extract Audio Using Playback Controls'. So let's get started!<a id="id249" class="indexterm"/>
</p><div><ol class="orderedlist arabic"><li class="listitem">Download the file<code class="literal"> PlaybackControlExamples.py</code> from the Packt website. This file has all the necessary code that illustrates various playback controls. The overall class and its methods are illustrated below for reference. See the source file to know more about each of these methods.<div><pre class="programlisting">class AudioPlayer:
def __init__(self):
pass
def constructPipeline(self):
pass
def connectSignals(self):
pass
def decodebin_pad_added(self, decodebin, pad ):
pass
def play(self):
pass
def runExamples(self):
pass
def runPauseExample(self):
pass
def runStopExample(self):
pass
def runSeekExample(self):
pass
def okToRunExamples(self):
pass
def message_handler(self, bus, message):
pass
</pre></div></li><li class="listitem">The overall code layout is very similar to the code developed in the Playing audio section of Chapter 5, Working with Audios. Thus, we will just review some of the newly added methods relevant to this section.<a id="id250" class="indexterm"/></li><li class="listitem">Here is the code for<code class="literal"> self.play</code> method.<div><pre class="programlisting">1 def play(self):
2 self.is_playing = True
3 self.player.set_state(gst.STATE_PLAYING)
4 self.position = None
5 while self.is_playing:
6 time.sleep(0.5)
7 try:
9 self.position = (
10 self.player.query_position(gst.FORMAT_TIME,
11 None) [0] )
16 except gst.QueryError:
17 # The pipeline has probably reached
18 # the end of the audio, (and thus has 'reset' itself.
19 # So, it may be unable to query the current position.
20 # In this case, do nothing except to reset
21 # self.position to None.
22 self.position = None
23
24 if not self.position is None:
25 #Convert the duration into seconds.
26 self.position = self.position/gst.SECOND
27 print "\n Current playing time: ",
28 self.position
29
30 self.runExamples()
31 evt_loop.quit()
</pre></div></li><li class="listitem">Inside the while loop, on line 9, the current position of the streaming audio is queried using the query_position call. This is an API method of GStreamer Pipeline object. When the pipeline approaches the end of the stream, it may throw an error while querying the current position. Therefore, we catch the exception gst.QueryError, in the try-except block. The time.sleep call is important before entering the try-except block. It ensures that the position is queried every 0.5 seconds. If you remove this call, the next code will be executed for each incremental tiny step. From a performance standpoint this is unnecessary. The current position thus obtained is expressed in nanoseconds, Thus, if the time is say 0.1 seconds, it is obtained as 100 000 000 nanoseconds. To convert it into seconds, it is divided by a GStreamer constant gst.SECOND. On line 30, the main method that runs various audio control examples is called.<a id="id251" class="indexterm"/></li><li class="listitem">Let's see the code in<code class="literal"> self.runExamples</code> method now.<div><pre class="programlisting">1 def runExamples(self):
2
3 if not self.okToRunExamples():
4 return
5
6 # The example will be roughly be run when the streaming
7 # crosses 5 second mark.
8 if self.position &gt;= 5 and self.position &lt; 8:
9 if self.pause_example:
10 self.runPauseExample()
11 elif self.stop_example:
12 self.runStopExample()
13 elif self.seek_example:
14 self.runSeekExample()
15 # this flag ensures that an example is run
16 # only once.
17 self.ranExample = True
</pre></div></li><li class="listitem">The method self.okToRunExamples does some preliminary error checking and ensures that the total streaming duration is greater than 20 seconds. This method will not be discussed here. When the current track position reaches 5 seconds, one of the examples is run. Which example to run is determined by the corresponding boolean flag. For instance, if self.pause_example flag is set to True, it will run the code that will 'pause' the audio stream. Likewise for the other examples. These three flags are initialized to False in the __init__ method.</li><li class="listitem">The last method we will review is<code class="literal"> self.runPauseExample</code>.<div><pre class="programlisting">1 def runPauseExample(self):
2 print ("\n Pause example: Playback will be paused"
3 " for 5 seconds and will then be resumed...")
4 self.player.set_state(gst.STATE_PAUSED)
5 time.sleep(5)
6 print "\n .. OK now resuming the playback"
7 self.player.set_state(gst.STATE_PLAYING)
</pre></div></li><li class="listitem">The streaming audio is paused by the call on line 4. The time.sleep call will keep the audio paused for 5 seconds and then the audio playback is resumed by the call on line 7.</li><li class="listitem">Make sure to set the flag<code class="literal"> self.pause_example</code> to True in the<code class="literal"> __init__</code> method and specify the proper audio file path for the variable for<code class="literal"> self.inFileLocation</code>. Then run this example from the command prompt as:<a id="id252" class="indexterm"/><div><pre class="programlisting">$python PlaybackControlExamples.py
</pre></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: none">The audio will be played for the first 5 seconds. It will be then paused for another 5 seconds and finally the playback will be resumed.</li></ul></div></li></ol></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec03"/>What just happened?</h2></div></div></div><p>With the help of a simple example, we learned how to pause a streaming audio. We also saw how the current position of the streaming audio is queried. This knowledge will be used in a project later in this chapter.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec04"/>Stop</h2></div></div></div><p>Setting the state of the GStreamer pipeline to<code class="literal"> gst.STATE_NULL</code> stops the audio streaming. Recall the<code class="literal"> message_handler</code> method explained in the Playing Audio section of the previous chapter. We made use of this state when the end of stream message was put on the<code class="literal"> bus</code>. In the file<code class="literal"> PlaybackControlExamples.py</code>, the following code stops the streaming of the audio.<a id="id253" class="indexterm"/>
</p><div><pre class="programlisting">def runStopExample(self):
print ("\n STOP example: Playback will be STOPPED"
" and then the application will be terminated.")
self.player.set_state(gst.STATE_NULL)
self.is_playing = False
</pre></div><p>In this file, set the flag<code class="literal"> self.stop_example</code> to<code class="literal"> True</code> and then run the program from the command line to see this illustration.<a id="id254" class="indexterm"/>
</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec05"/>Fast-forward/rewind</h2></div></div></div><p>Fast-forwarding or rewinding a track simply means that the current position on the audio track being played is shifted to some other position. This is also called seeking a position on a track. The<code class="literal"> pipeline</code> element of GStreamer defines an API method,<code class="literal"> seek_simple</code>, that facilitates jumping to a specified position on the track in a streaming audio. In the file<code class="literal"> PlabackControlExamples.py</code>, this is illustrated by the following method.<a id="id255" class="indexterm"/>
</p><div><pre class="programlisting">def runSeekExample(self):
print ("\n SEEK example: Now jumping to position at 15 seconds"
"the audio will continue to stream after this")
self.player.seek_simple(gst.FORMAT_TIME,
gst.SEEK_FLAG_FLUSH,
15*gst.SECOND)
self.player.set_state(gst.STATE_PAUSED)
print "\n starting playback in 2 seconds.."
time.sleep(2)
self.player.set_state(gst.STATE_PLAYING)
</pre></div><p>When this method is called, the current audio position is shifted to a position corresponding to 15 seconds duration on the audio track. The highlighted lines of code are the key. The<code class="literal"> seek_simple</code> method takes three arguments. The first argument,<code class="literal"> gst.FORMAT_TIME</code>, represents the time on the track. The second argument,gst.SEEK_GLAG_FLUSH, is a 'seek flag'. It tells the pipeline to clear the currently playing audio data. In other words it instructs to flush the pipeline. This makes the seek operation faster according to the documentation. There are several other seek flags. Refer to the GStreamer documentation to know more about these flags. The third argument specifies the time on the track that will be the new 'current position' of the streaming audio. This time I specified in nanoseconds and so, it is multiplied by a constant<code class="literal"> gst.SECOND</code>. Note that pipeline should be in playing state, before calling<code class="literal"> seek_simple</code> method.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec06"/>Project: extract audio using playback controls</h2></div></div></div><p>In the last chapter, we learned how to use<code class="literal"> gnonlin</code> plugin to extract a piece of audio.<code class="literal"> Gnonlin</code> made our job very easy. In this project, we will see another way of extracting the audio files, by applying basic audio processing techniques using GStreamer. We will use some of the audio playback controls just learned. This project will serve as a refresher on various fundamental components of GStreamer API.<a id="id256" class="indexterm"/>
</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec03"/>Time for action -  MP3 cutter from basic principles</h1></div></div></div><p>Let's create an MP3 cutter from 'basic principles'. That is we won't be using<code class="literal"> gnonlin</code> to do this. In this project, we will apply knowledge about seeking a track playing, pausing the pipeline along with the basic audio processing operations.<a id="id257" class="indexterm"/>
</p><p>This utility can be run from the command line as:</p><div><pre class="programlisting">$python AudioCutter_Method2.py [options]
</pre></div><p>Where, the<code class="literal"> [options]</code> are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">--input_file:</code> The input audio file in MP3 format from which a piece of audio needs to be cut.</li><li class="listitem" style="list-style-type: disc"><code class="literal">--output_file:</code> The output file path where the extracted audio will be saved. This needs to be in MP3 format.</li><li class="listitem" style="list-style-type: disc"><code class="literal">--start_time:</code> The position in seconds on the original track. This will be the starting position of the audio to be extracted.</li><li class="listitem" style="list-style-type: disc"><code class="literal">--end_time:</code> The position in seconds on the original track. This will be the end position of the extracted audio.</li></ul></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">--verbose_mode:</code> Prints useful information such as current position of the track (in seconds) while extracting the audio. By default, this flag is set to<code class="literal"> False</code>.</li></ul></div><div><ol class="orderedlist arabic"><li class="listitem">Download the file<code class="literal"> AudioCutter_Method2.py</code> from the Packt website. We will discuss only the most important methods here. You can refer to the source code in this file for developing the rest of the code.</li><li class="listitem">We will start as usual, by defining a class with empty methods.<div><pre class="programlisting">import os, sys, time
import thread
import gobject
from optparse import OptionParser
import pygst
pygst.require("0.10")
import gst
class AudioCutter:
def __init__(self):
pass
def constructPipeline(self):
pass
def decodebin_pad_added(self, decodebin, pad):
pass
def connectSignals(self):
pass
def run(self):
pass
def extractAudio(self):
pass
def processArgs(self):
pass
def printUsage(self):
pass
def printFinalStatus(self):
pass
def message_handler(self, bus, message):
pass
audioCutter = AudioCutter()
thread.start_new_thread(audioCutter.run, ())
gobject.threads_init()
evt_loop = gobject.MainLoop()
evt_loop.run()
</pre></div></li><li class="listitem">As you can see, the overall structure and the method names are very much consistent with the MP3 cutter example in earlier chapters. Instead of method<code class="literal"> gnonlin_pad_added</code> we have<code class="literal"> decodebin_pad_added</code> which indicates we are going to capture the<code class="literal"> pad_added</code> signal for the<code class="literal"> decodebin</code>. Also, there are new methods<code class="literal"> run</code> and<code class="literal"> extractAudio</code>. We will discuss these in detail.<a id="id258" class="indexterm"/></li><li class="listitem">Now let's review the constructor of the class.<div><pre class="programlisting">1 def __init__(self):
2 self.start_time = None
3 self.end_time = None
4 self.is_playing = False
5 self.seek_done = False
6 self.position = 0
7 self.duration = None
8 #Flag used for printing purpose only.
9 self.error_msg = ''
10 self.verbose_mode = False
11
12 self.processArgs()
13 self.constructPipeline()
14 self.connectSignals()
</pre></div></li><li class="listitem">The<code class="literal"> __init__</code> method calls methods to process user input and then constructs the GStreamer pipeline by calling the<code class="literal"> constructPipeline()</code> method. This is similar to what we have seen in several examples earlier.<a id="id259" class="indexterm"/></li><li class="listitem">Think about this. To extract an audio, what elements do you need? We need all the elements used in audio conversion utility developed in last chapter. Note that in this example we are saving the output in the same audio format as the input. Let's try to construct an initial pipeline.<div><pre class="programlisting">1 def constructPipeline(self):
2 self.pipeline = gst.Pipeline()
3 self.fakesink = gst.element_factory_make("fakesink")
4 filesrc = gst.element_factory_make("filesrc")
5 filesrc.set_property("location", self.inFileLocation)
6
7 autoaudiosink = gst.element_factory_make(
8 "autoaudiosink")
9
10 self.decodebin = gst.element_factory_make("decodebin")
11
12 self.audioconvert = gst.element_factory_make(
13 "audioconvert")
14
15 self.encoder = gst.element_factory_make("lame",
16 "mp3_encoder")
17
18 self.filesink = gst.element_factory_make("filesink")
19 self.filesink.set_property("location",
20 self.outFileLocation)
21
22 self.pipeline.add(filesrc, self.decodebin,
23 self.audioconvert,
24 self.encoder, self.fakesink)
25
26 gst.element_link_many(filesrc, self.decodebin)
27 gst.element_link_many(self.audioconvert,
28 self.encoder, self.fakesink)
</pre></div></li><li class="listitem">We are already familiar with most of the elements included in this pipeline. The pipeline looks identical to the one in audio conversion utility except for the sink element. Notice that the<code class="literal"> filesink</code> element is created on line 18. But it is not added to the pipeline! Instead we have added a<code class="literal"> fakesink</code> element. Can you guess why? This is an extraction utility. We just need to save a portion of an input audio file. The start position of the extracted portion may not be the start position of the original track. Thus, at this time, we will not add the<code class="literal"> filesink</code> to the pipeline.<a id="id260" class="indexterm"/></li><li class="listitem">Next write the<code class="literal"> AudioCutter.run</code> method.<div><pre class="programlisting">1 def run(self):
2 self.is_playing = True
3 print "\n Converting audio. Please be patient.."
4 self.pipeline.set_state(gst.STATE_PLAYING)
5 time.sleep(1)
6 while self.is_playing:
7 self.extractAudio()
8 self.printFinalStatus()
9 evt_loop.quit()
</pre></div></li><li class="listitem">On line 4, we apply one of the playback control commands to instruct the pipeline to 'begin'. The state of the input audio is set to<code class="literal"> STATE_PLAYING</code>. As seen earlier, the flag<code class="literal"> self.is_playing</code> is changed in the<code class="literal"> message_handler</code> method. In the<code class="literal"> while</code> loop, the workhorse method<code class="literal"> self.extractAudio()</code> is called. The rest of the code is self-explanatory.</li><li class="listitem">Now we will review the method that does the job of cutting the piece of input audio. Let us first see the important things considered in<code class="literal"> extractAudio()</code> method. Then it will be very easy to understand the code. This following illustration lists these important things.<div><img src="img/0165_06_01.jpg" alt="Time for action - MP3 cutter from basic principles"/></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: none">Important steps considered in AudioCutter.extractAudio() method appear in the preceding image.</li></ul></div></li><li class="listitem">To extract a piece of audio from the input, the flow of data through the pipeline needs to be 'started'. Then, we need to jump to a position in the input audio that corresponds to the start position of the audio file to be extracted. Once the start position is identified, the GStreamer pipeline needs to be tweaked so that there is a<code class="literal"> filesink</code> element. The<code class="literal"> filesink</code> will specify the output audio file. After setting the pipeline, we need to begin the flow of data. When the user-specified end position is reached, the program execution should stop. Now let's write the code.<a id="id261" class="indexterm"/><div><pre class="programlisting">1 def extractAudio(self):
2 if not self.seek_done:
3 time.sleep(0.1)
4 self.duration = \
5 self.pipeline.query_duration(gst.FORMAT_TIME,
6 None) [0]
7 self.duration = self.duration/gst.SECOND
</pre></div><div><pre class="programlisting">8
9 if self.start_time &gt; self.duration:
10 print "\n start time specified" \
11 " is more than the total audio duration"\
12 " resetting the start time to 0 sec"
13 self.start_time = 0.0
14
15 self.pipeline.seek_simple(gst.FORMAT_TIME,
16 gst.SEEK_FLAG_FLUSH,
17 self.start_time*gst.SECOND)
18
19 self.pipeline.set_state(gst.STATE_PAUSED)
20 self.seek_done = True
21 self.pipeline.remove(self.fakesink)
22
23 self.pipeline.add(self.filesink)
24 gst.element_link_many(self.encoder, self.filesink)
25 self.pipeline.set_state(gst.STATE_PLAYING)
26
27 time.sleep(0.1)
28 try:
29 self.position = self.pipeline.query_position(
30 gst.FORMAT_TIME, None)[0]
31 self.position = self.position/gst.SECOND
32 except gst.QueryError:
33 # The pipeline has probably reached
34 # the end of the audio, (and thus has 'reset' itself)
35 if self.duration is None:
36 self.error_msg = "\n Error cutting the audio
37 file.Unable to determine the audio duration."
38 self.pipeline.set_state(gst.STATE_NULL)
39 self.is_playing = False
40 if ( self.position &lt;= self.duration and
41 self.position &gt; (self.duration - 10) ):
42 # Position close to the end of file.
43 # Do nothing to avoid a possible traceback.
44 #The audio cutting should work
45 pass
46 else:
47 self.error_msg =" Error cutting the audio file"
48 self.pipeline.set_state(gst.STATE_NULL)
49 self.is_playing = False
50
51 if not self.end_time is None:
52 if self.position &gt;= self.end_time:
53 self.pipeline.set_state(gst.STATE_NULL)
54 self.is_playing = False
55
56 if self.verbose_mode:
57 print "\n Current play time: =", self.position
</pre></div></li><li class="listitem">The code block between lines 3 to 25 is executed only once, when the program enters this method for the first time. The flag<code class="literal"> self.seek_done</code> ensures it is executed only once. This is an important piece of code that does the steps 2 to 5 represented by rectangular blocks in the above illustration. Let's review this code in detail now.<a id="id262" class="indexterm"/></li><li class="listitem">On line 3, we ask the program to wait for 0.1 seconds by<code class="literal"> time.sleep</code> call. This is necessary for the next line of code that queries the total duration of the playback. The API method query duration returns the total duration of the playback. The argument<code class="literal"> gst.FORMAT_TIME</code> ensures that the return value is in time format (nanoseconds). To get it in seconds, we divide it by<code class="literal"> gst.SECOND</code>.</li><li class="listitem">Next, on lines 15-17, we jump to the position on the input audio track pertaining to the user-supplied argument<code class="literal"> self.start_time</code>. Note that the time argument in the method<code class="literal"> seek_simple</code> needs to be in nanoseconds. So it is multiplied by<code class="literal"> gst.SECOND</code>.</li><li class="listitem">On line 19, the<code class="literal"> gst.STATE_PAUSED</code> call pauses the flow of data in the pipeline. The<code class="literal"> fakesink</code> element is removed from the pipeline with<code class="literal"> self.pipline.remove</code> call. This also unlinks it from the pipeline. Then the<code class="literal"> self.filesink</code> element is added and linked in the pipeline on lines 23 and 24. With this, we are all set to start playing the audio file again. Here onwards, the audio data will be saved to the audio file indicated by the<code class="literal"> filesink</code> element.<a id="id263" class="indexterm"/></li><li class="listitem">On line 27, the current position being played is queried. Note that this is done in a try-except block to avoid any possible error while querying the position when the audio is very near to the end of the file. When<code class="literal"> self.position</code> reaches the specified<code class="literal"> self.end_time</code>, the data flow through the pipeline is stopped by the<code class="literal"> gst.STATE_NULL</code> call.</li><li class="listitem">Write other methods such as<code class="literal"> decodebin_pad_added, connectSignals</code>. The source code can be found in the file<code class="literal"> AudioCutter_Method2.py</code>.</li><li class="listitem">We are now all set to run the program. Run it from the command line by specifying the appropriate arguments mentioned at the beginning of this section.</li></ol></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec07"/>What just happened?</h2></div></div></div><p>By applying fundamental audio processing techniques, we developed an MP3 cutter utility. This is just another way of extracting audio. We accomplished this task by making use of various playback controls learned in earlier sections.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec04"/>Adjusting volume</h1></div></div></div><p>One of the most common audio operations we perform is to adjust the volume level of a playing audio. Suppose you have a collection of your favourite songs on your computer. You have been adding songs to this collection from various sources over the years and have created a 'playlist' so that you can listen to them one after the other. But some of the songs start much louder than the others. Of course you can adjust the volume every time such songs start playing but that's not what you would like to do is it?? You want to fix this, but how? Let's learn how!<a id="id264" class="indexterm"/>
</p><p>The<code class="literal"> volume</code> element in GStreamer can be used to control the volume of the streaming audio. It is classified as a type of audio filter. Run<code class="literal"> gst-inspect-0.10</code> command on<code class="literal"> volume</code> to know more details about its properties.</p><p>How will you adjust volume using the command-line version of GStreamer? Here is the command on Windows XP that accomplishes this. You should use forward slashes as the backward slashes are not parsed properly by the 'location' property.</p><div><pre class="programlisting">$gstlaunch-0.10 filesrc location=/path/to/audio.mp3 ! decodebin ! Audioconvert ! volume volume=0.8 ! autoaudiosink
</pre></div><p>This pipeline is very similar to the audio playing example. All we did was to add a<code class="literal"> volume</code> element after<code class="literal"> audioconvert</code>.</p></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec05"/>Time for action -  adjusting volume</h1></div></div></div><p>Now let's develop a Python example of modifying volume of an audio file. We will write a utility that can take an input audio file and write the output file with increased or decreased level of the default volume. The utility will support writing audio files with MP3 format. If you need some other formats, you can extend this application. Refer to the Audio Converter project we did in the previous chapter.</p><div><ol class="orderedlist arabic"><li class="listitem">Download the file<code class="literal"> AudioEffects.py</code> from Packt website. It has the source code for this example as well as for the<em> Fading effect</em>.</li><li class="listitem">Write the constructor of the class<code class="literal"> AudioEffects</code>.<div><pre class="programlisting">1 def __init__(self):
2 self.is_playing = False
3 # Flag used for printing purpose only.
4 self.error_msg = ''
5 self.fade_example = False
6 self.inFileLocation = "C:/AudioFiles/audio1.mp3"
7 self.outFileLocation = (
8 "C:/AudioFiles/audio1_out.mp3" )
9
10 self.constructPipeline()
11 self.connectSignals()
</pre></div></li><li class="listitem">The flag<code class="literal"> self.fade_example</code> should be set to<code class="literal"> False</code> in this example. You can ignore it for now. It will be used in the<em> Fading effects</em> section. Specify appropriate input and output audio file paths on lines 6 and 8.<a id="id265" class="indexterm"/></li><li class="listitem">We will review the<code class="literal"> self.constructPipeline()</code> method next.<div><pre class="programlisting">1 def constructPipeline(self):
2 self.pipeline = gst.Pipeline()
3
4 self.filesrc = gst.element_factory_make("filesrc")
5 self.filesrc.set_property("location",
6 self.inFileLocation)
7
8 self.decodebin = gst.element_factory_make("decodebin")
9 self.audioconvert = gst.element_factory_make(
10 "audioconvert")
11 self.encoder = gst.element_factory_make("lame")
12
13 self.filesink = gst.element_factory_make("filesink")
14 self.filesink.set_property("location",
15 self.outFileLocation)
16
17 self.volume = gst.element_factory_make("volume")
18 self.volumeLevel = 2.0
19
20 if self.fade_example:
21 self.setupVolumeControl()
22 else:
23 self.volume.set_property("volume",
24 self.volumeLevel)
25
26
27 self.pipeline.add(self.filesrc,
28 self.decodebin,
29 self.audioconvert,
30 self.volume,
31 self.encoder,
32 self.filesink)
33
34 gst.element_link_many( self.filesrc, self.decodebin)
35 gst.element_link_many(self.audioconvert,
36 self.volume,
37 self.encoder,
38 self.filesink)
</pre></div></li><li class="listitem">Various GStreamer elements are created the usual way. On line 17, the volume element is created.</li><li class="listitem">The<code class="literal"> volume</code> element has a "volume" property. This determines the level of volume in the streaming audio. By default, this has a value of 1.0 which indicates 100% of the current default volume of the audio. A value of 0.0 indicates no volume. A value greater than 1.0 will make the audio louder than the original level. Let's set this level as 2.0, which means the resultant volume will be louder than the original. The rest of the code in this method adds and links elements in the GStreamer pipeline.<a id="id266" class="indexterm"/></li><li class="listitem">Review the rest of the code from the file mentioned earlier. It is self- explanatory.</li><li class="listitem">Run the program on the command prompt as:</li><li class="listitem">Play the resultant audio and compare its default sound level with the original audio.<div><pre class="programlisting">$python AudioEffects.py
</pre></div></li></ol></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec08"/>What just happened?</h2></div></div></div><p>With a very simple illustration, we learned how to change the default sound level of an audio file. What if you want to have varying sound levels at certain points in the audio? We will discuss that very soon, in the<em> Fading effects</em> section.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec06"/>Audio effects</h1></div></div></div><p>One adds spices for improved taste to food, similarly, to enhance the music or any sound we add audio effects. There is a wide range of audio effect plugins available in GStreamer. We will discuss some of the commonly used audio effects in the coming sections.<a id="id267" class="indexterm"/>
</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec09"/>Fading effects</h2></div></div></div><p>Fading is a gradual increase or decrease in the volume level of an audio. Fading-out means gradually decreasing the volume of the audio file as it approaches the end. Typically, at the end, the volume level is set as 0. On similar lines, fade-in effect gradually increases the volume level from the beginning of an audio. In this chapter, we will learn how to add fade-out effect to an audio. Once we learn that, it is trivial to implement fade-in effects.<a id="id268" class="indexterm"/>
</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec07"/>Time for action -  fading effects</h1></div></div></div><p>Let's add fade-out effect to an input audio. We will use the same source file as used in the<em> Adjusting volume</em> section.<a id="id269" class="indexterm"/>
</p><div><ol class="orderedlist arabic"><li class="listitem">If you haven't already, download the file<code class="literal"> AudioEffects.py</code> that has the source code for this example.</li><li class="listitem">In the<code class="literal"> __init__</code> method of this class, you will need to do one small change. Set the flag<code class="literal"> self.fade_example</code> to<code class="literal"> True</code> so that it now runs the code that adds fade-out effect.</li><li class="listitem">We already reviewed the<code class="literal"> self.constructPipeline()</code> method in<em> Adjusting volume</em> section. It calls the method<code class="literal"> self.setupVolumeControl()</code>.<div><pre class="programlisting">1 def setupVolumeControl(self):
2 self.volumeControl = gst.Controller(self.volume,
3 "volume")
4 self.volumeControl.set("volume", 0.0*gst.SECOND,
5 self.volumeLevel)
6 self.volumeControl.set_interpolation_mode("volume",
7 gst.INTERPOLATE_LINEAR)
</pre></div></li><li class="listitem">The GStreamer<code class="literal"> Controller</code> object is created on line 2. It is a light-weight object that provides a way to control various properties of GStreamer objects. In this case, it will be used to adjust the 'volume' property of<code class="literal"> self.volume</code>. The set method of the<code class="literal"> Controller</code> takes three arguments, namely, the property that needs to be controlled ("volume"), the time on the audio track at which it needs to be changed, and the new value of that property (self.volumeLevel). Here, the volume level at the beginning of the audio is set<code class="literal"> self.volumeLevel</code>. Next, the interpolation mode is set for the<code class="literal"> volume</code> property being adjusted by the<code class="literal"> Controller</code> object. Here, we ask the<code class="literal"> self.volumeControl</code> to linearly change the volume from its earlier value to the new value as the audio track progresses. For example, if the sound level at the beginning is set as 1.0 and at 30 seconds it is set as 0.5, the volume levels between 0 to 30 seconds on the track will be linearly interpolated. In this case it will linearly decrease from level 1.0 at 0 seconds to level 0.5 at 30 seconds.<div><div><h3 class="title"><a id="tip15"/>Tip</h3><p>The GStreamer documentation suggests that<code class="literal"> Controller.set_interpolation_mode</code> is deprecated (but is still backward compatible in the version 0.10.5 which is used in this book). See a 'TODO' comment in file<code class="literal"> AudioEffects.py</code>.</p></div></div></li><li class="listitem">In order to add a fade-out effect towards the end, first we need to get the total duration of the audio being played. We can query the duration only after the audio has been set for playing (example, when it is in<code class="literal"> gst.STATE_PLAYING</code> mode). This is done in<code class="literal"> self.play()</code> method.<div><pre class="programlisting">def play(self):
self.is_playing = True
self.pipeline.set_state(gst.STATE_PLAYING)
if self.fade_example:
self.addFadingEffect()
while self.is_playing:
time.sleep(1)
self.printFinalStatus()
evt_loop.quit()
</pre></div></li><li class="listitem">Once the pipeline's state is set to<code class="literal"> gst.STATE_PLAYING</code>, the<code class="literal"> self.addFadingEffects()</code> method will be called as shown by the highlighted line of code.<a id="id270" class="indexterm"/></li><li class="listitem">We will review this method now.<div><pre class="programlisting">1 def addFadingEffect(self):
2 # Fist make sure that we can add the fading effect!
3 if not self.is_playing:
4 print ("\n Error: unable to add fade effect"
5 "addFadingEffect() called erroniously")
6 return
7
8 time.sleep(0.1)
9 try:
10 duration = (
11 self.pipeline.query_duration(gst.FORMAT_TIME,
12 None) [0] )
13 #Convert the duration into seconds.
14 duration = duration/gst.SECOND
15 except gst.QueryError:
16 # The pipeline has probably reached
17 # the end of the audio, (and thus has 'reset' itself)
18 print ("\n Error: unable to determine duration."
19 "Fading effect not added." )
20 return
21
22 if duration &lt; 4:
23 print ("ERROR: unable to add fading effect."
24 "\n duration too short.")
25 return
26
27 fade_start = duration - 4
28 fade_volume = self.volumeLevel
29 fade_end = duration
30
31 self.volumeControl.set("volume",
32 fade_start * gst.SECOND,
33 fade_volume)
34
35 self.volumeControl.set("volume",
36 fade_end * gst.SECOND,
37 fade_volume*0.01)
</pre></div></li><li class="listitem">First we ensure that duration of the audio being played can be computed without any errors. This is done by the code block 2-24. Next, the<code class="literal"> fade_start</code> time is defined. At this control point the fade-out effect will begin. The fade-out will start 4 seconds before the end of the audio. The volume will linearly decrease from<code class="literal"> fade_start</code> time to<code class="literal"> fade_end</code> time. The fade_volume is the reference volume level when the fade-out begins. On lines 30 and 34 we actually set these fade timing and volume parameters for<code class="literal"> self.volumeController</code> , the<code class="literal"> Controller</code> object that adjusts the volume. The gradual decrease in the volume level is achieved by the<code class="literal"> gst.INTERPOLATE_LINEAR</code>, discussed in an earlier step.</li><li class="listitem">Develop or review the remaining code using the reference file<code class="literal"> AudioEffects.py</code>. Make sure to specify appropriate input and output audio paths for variables<code class="literal"> self.inFileLocation</code> and<code class="literal"> self.outFileLocation</code> respectively. Then run the program from the command line as:<a id="id271" class="indexterm"/></li><li class="listitem">This should create the output audio file, with a fade-out effect that begins 4 seconds before the end of the file.<div><pre class="programlisting">$python AudioEffects.py
</pre></div></li></ol></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec10"/>What just happened?</h2></div></div></div><p>We learned how to add a fading effect to an audio file using GStreamer multimedia framework. We used the same GStreamer pipeline as the one used in the<em> Adjusting volume</em> section, but this time, the volume level was controlled using the<code class="literal"> Controller</code> object in GStreamer. The technique we just learned will come handy while working on project 'Combining Audio Clips ' later in this chapter.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec11"/>Have a go hero add fade-in effect</h2></div></div></div><p>This is going to be straightforward. We added a fade-out effect earlier. Now extend this utility by adding a fade-in effect to the input audio. Use a total fade duration of 4 seconds. The<code class="literal"> fade_start</code> time in this case will be 0 seconds. Try the interpolation mode as<code class="literal"> gst.INTERPOLATE_CUBIC</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec12"/>Echo echo echo...</h2></div></div></div><p>Echo is a reflection of a sound heard a short time period after the original sound. In audio processing, to achieve this effect the input audio signal is recorded and then played back after the specified 'delay time' with a specified intensity. An echo effect can be added using the<code class="literal"> audioecho</code> plugin in GStreamer. The audio echo plugin should be available by default in your GStreamer installation. Check this by running the following command:<a id="id272" class="indexterm"/>
</p><div><pre class="programlisting">$gst-inspect-0.10 audioecho
</pre></div><p>If it is not available, you will need to install it separately. Refer to the GStreamer website for installation instructions.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec08"/>Time for action -  adding echo effect</h1></div></div></div><p>Let's write code to add an echo effect to an input audio. The code is very similar to the one in the<code class="literal"> AudioEffects.py</code> file discussed in earlier section. Just to simplify the matter, we will use the code in file<code class="literal"> EchoEffect.py</code> file for easier understanding. Later, you can easily integrate this with the code in<code class="literal"> AudioEffects.py</code>.<a id="id273" class="indexterm"/>
</p><div><ol class="orderedlist arabic"><li class="listitem">Download the file<code class="literal"> EchoEffect.py</code> that has the source code to add audio echo effect. The file contains class<code class="literal"> AudioEffects</code> whose constructor has the following code.<div><pre class="programlisting">def __init__(self):
self.is_playing = False
# Flag used for printing purpose only.
self.error_msg = ''
#the flag that determines whether to use
# a gst Controller object to adjust the
# intensity of echo while playing the audio.
self.use_echo_controller = False
self.inFileLocation = "C:/AudioFiles/audio1.mp3"
self.outFileLocation = "C:/AudioFiles/audio1_out.mp3"
self.constructPipeline()
self.connectSignals()
</pre></div></li><li class="listitem">It is similar to the __init__ method discussed in the Fading Effects section. One difference here is the flag self.use_echo_controller. If it is set to True, the GStreamer Controller object will be used to adjust certain echo properties while the audio is being streamed. We will first see how a simple echo effect can be implemented and then discuss the echo control details. Specify the appropriate values for audio file path variables self.inFileLocation and self.outFileLocation.</li><li class="listitem">Let's build the GStreamer pipeline.<a id="id274" class="indexterm"/><div><pre class="programlisting">1 def constructPipeline(self):
2 self.pipeline = gst.Pipeline()
3
4 self.filesrc = gst.element_factory_make("filesrc")
5 self.filesrc.set_property("location",
6 self.inFileLocation)
7
8 self.decodebin = gst.element_factory_make("decodebin")
9
10 self.audioconvert = gst.element_factory_make(
11 "audioconvert")
12 self.audioconvert2 = gst.element_factory_make(
13 "audioconvert")
14
15 self.encoder = gst.element_factory_make("lame")
16
17 self.filesink = gst.element_factory_make("filesink")
18 self.filesink.set_property("location",
19 self.outFileLocation)
20
21 self.echo = gst.element_factory_make("audioecho")
22 self.echo.set_property("delay", 1*gst.SECOND)
23 self.echo.set_property("feedback", 0.3)
24
25 if self.use_echo_controller:
26 self.setupEchoControl()
27 else:
28 self.echo.set_property("intensity", 0.5)
29
30 self.pipeline.add(self.filesrc,self.decodebin,
31 self.audioconvert,
32 self.echo,
33 self.audioconvert2,
34 self.encoder,
35 self.filesink)
36
37 gst.element_link_many( self.filesrc, self.decodebin)
38 gst.element_link_many(self.audioconvert,
39 self.echo,
40 self.audioconvert2,
44 self.encoder,
45 self.filesink)
</pre></div></li><li class="listitem">The audioecho element is created on line 21. The property delay specifies the duration after which the echo sound will be played. We specify it as 1 second, and you can increase or decrease this value further. The echo feedback value is set as 0.3. On line 28, the intensity property is set to 0.5. It can be set in a range 0.0 to 1.0 and determines the sound intensity of the echo. Thus, if you set it to 0.0, the echo won't be heard.</li><li class="listitem">Notice that there are two<code class="literal"> audioconvert</code> elements. The first<code class="literal"> audioconvert</code> converts the decoded audio stream into a playable format input to the<code class="literal"> self.echo</code> element. Similarly on the other end of the echo element, we need<code class="literal"> audioconvert</code> element to process the audio format after the echo effect has been added. This audio is then encoded in MP3 format and saved to the location specified by<code class="literal"> self.filesink</code>.<a id="id275" class="indexterm"/></li><li class="listitem">Run the program from the command line as:<div><pre class="programlisting">$python EchoEffect.py
</pre></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: none">If you play the output file, the echo sound will be audible throughout the playback duration.</li></ul></div></li><li class="listitem">Now we will add a feature that will allow us to add echo effect only for a certain duration of the audio track. In the<code class="literal"> __init__</code> method, set the flag<code class="literal"> self.use_echo_controller</code> to<code class="literal"> True</code>.</li><li class="listitem">We will now review the method<code class="literal"> self.setupEchoControl()</code> which is called in<code class="literal"> self.constructPipeline()</code>.<div><pre class="programlisting">def setupEchoControl(self):
self.echoControl = gst.Controller(self.echo, "intensity")
self.echoControl.set("intensity", 0*gst.SECOND, 0.5)
self.echoControl.set("intensity", 4*gst.SECOND, 0.0)
</pre></div></li><li class="listitem">Setting up<code class="literal"> gst.Controller</code> object is very similar to the one developed in the<em> Fading effects</em> section. Here, we ask the<code class="literal"> Controller</code> object,<code class="literal"> self.echoControl</code>, to control the property 'intensity' of the<code class="literal"> audioecho</code> element,<code class="literal"> self.echo</code>. At the beginning of the playback (0 seconds), we set the echo intensity as<code class="literal"> 0.5</code>. We add another control point at 4 seconds during the playback and set the<code class="literal"> intensity</code> level as<code class="literal"> 0.0</code>. What this effectively means is that we don't want to hear any echo after the first 4 seconds of the audio playback! .</li><li class="listitem">Run the program again from the command line as:<a id="id276" class="indexterm"/><div><pre class="programlisting">$python EchoEffect.py
</pre></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: none">Note that the only change done here is the value of flag self.use_echo_controller is set to True. Play the output file; the echo sound will be audible only for the first 4 seconds during the playback.</li></ul></div></li></ol></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec13"/>What just happened?</h2></div></div></div><p>We learned how to add echo to an audio clip. To accomplish this, the<code class="literal"> audioecho</code> element was added and linked in the GStreamer pipeline. We also learned how to selectively add echo effect to the audio using GStreamer<code class="literal"> Controller</code> object.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec14"/>Have a go hero add Reverberation Effect</h2></div></div></div><p>Suppose you are in a theater. When an actor at the center stage talks, the sound waves are reflected from the surfaces of the theater before reaching your ears. Thus what you hear is a bunch of these reflected sounds. This is known as reverberation effect. According to the<code class="literal"> audioecho</code> plugin documentation, if you set the<code class="literal"> delay</code> property to a value of less than 0.2 seconds in<code class="literal"> audioecho</code> element, it produces a reverberation effect. Try setting different values for<code class="literal"> delay</code>, less than 0.2 seconds and see how it affects the output audio. Note, this argument is taken as an integer. Therefore, specify this value in nanoseconds. For example specify 0.05 seconds as<code class="literal"> 50000000</code> instead of<code class="literal"> 0.05*gst.SECOND</code>. This is illustrated below.<a id="id277" class="indexterm"/>
</p><div><pre class="programlisting">self.echo.set_property("delay", 50000000)
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec15"/>Panning/panorama</h2></div></div></div><p>The stereo panorama effect can be added to a sound by using<code class="literal"> audiopanorama</code> plugin (part of<code class="literal"> audiofx</code> plugin). This plugin should be available by default in your GStreamer installation. Use<code class="literal"> gst-inspect-0.10</code> to verify it is there and also to know more about its properties. Download the file<code class="literal"> PanoramaEffect.py</code> from the Packt website. This file is more or less identical to<code class="literal"> AudioEffects.py</code> or<code class="literal"> EchoEffect.py</code>. The following is a code snippet from the<code class="literal"> self.contructPipeline</code> method in file<code class="literal"> PanoramaEffect.py</code>
<a id="id278" class="indexterm"/>
</p><div><pre class="programlisting">1 # Stereo panorama effect
2 self.panorama = gst.element_factory_make("audiopanorama")
3 self.panorama.set_property("panorama", 1.0)
4
5
6 self.pipeline.add(self.filesrc,
7 self.decodebin,
8 self.audioconvert,
9 self.panorama,
10 self.encoder,
11 self.filesink)
12
13
14 gst.element_link_many( self.filesrc, self.decodebin)
15 gst.element_link_many(self.audioconvert,
16 self.panorama,
17 self.encoder,
18 self.filesink)
</pre></div><p>We have discussed the following many times. Let's go over the code once again as a refresher… just in case you missed it earlier. The code block 6-11 adds all the elements to the GStreamer pipeline. Notice that we call<code class="literal"> gst.element_link_many</code> twice. Do you recall why? The first call on line 14 makes a connection between<code class="literal"> self.filesrc</code> and<code class="literal"> self.decodebin</code>. There is one important point to note when we make a second call to<code class="literal"> gst.element_link_many</code>. Notice that we have not linked<code class="literal"> self.decodebin</code> with<code class="literal"> self.audioconvert</code>. This is because<code class="literal"> self.decodebin</code> implements dynamic pads. So we connect it at the runtime, using the callback method,<code class="literal"> decodebin_pad_added</code>.</p><p>You can review the rest of the code from this file. The<code class="literal"> audiopanorama</code> element is created on line 2 in the code snippet. The<code class="literal"> panorama</code> property can have a value in the range<code class="literal"> -1.0</code> to<code class="literal"> 1.0</code>. If you have stereo speakers connects, the sound will entirely come from the left speaker if a value of<code class="literal"> -1.0</code> is specified. Likewise, a value of<code class="literal"> 1.0</code> will make the sound come from right speaker only. In the above code snippet, we instruct the program to exclusively use the right speaker for audio streaming. The audio will be streamed from both speakers if the value is in-between these two limits. Each speaker's contribution will be determined by actual value.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec16"/>Have a go hero control panorama effect and more...</h2></div></div></div><p>'Move' the sound around! Add a GStreamer<code class="literal"> Controller</code> object to adjust the<code class="literal"> panorama</code> property of the<code class="literal"> self.panorama</code> element. This is similar to what we did in<code class="literal"> EchoEffect.py</code>. Add some control points in the audio stream as done earlier, and specify different values for the<code class="literal"> panorama</code> property.<a id="id279" class="indexterm"/>
</p><p>Integrate this feature with the code in<code class="literal"> AudioEffects.py</code> discussed earlier in this chapter.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec09"/>Project: combining audio clips</h1></div></div></div><p>It is time for a project! In this project, we will create a single audio file, which has custom audio clips appended one after the other. Here, we will apply several of the things learned in earlier section, and also in the previous chapter on audio processing.<a id="id280" class="indexterm"/>
</p><p>Creating a new audio file, which is a combination of several audio tracks of your choice involves the following steps:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">First thing we need are the audio files that need to be included. Depending upon our requirement, we may need only a small portion of an audio track. So we will develop a general application considering this possibility. This is illustrated in the time-line illustrated earlier.</li><li class="listitem" style="list-style-type: disc">Next, we need to make sure that these audio pieces are played in a specified order.</li><li class="listitem" style="list-style-type: disc">There should be a 'blank' or a 'silent' audio in-between the two audio pieces.</li><li class="listitem" style="list-style-type: disc">Next, we will also implement audio fade-out effect for each of the pieces in the track. This will ensure that the audio doesn't end abruptly.</li></ul></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec17"/>Media 'timeline' explained</h2></div></div></div><p>Before we begin this project, it is important to understand the concept of a timeline. A timeline can be viewed as the overall representation of a path where you can control the time for which an individual audio clip is played.</p><p>In this project, since we are saving the resultant audio, it is the same as the total playback time of the resultant audio. In this timeline, we can specify 'when' an audio needs to be played and how long it needs to be played. This is better explained with the illustration below. Consider a timeline with a total duration of 250 seconds. This is represented by the central thick line with circles at the end. Suppose there are three audio clips, namely,<code class="literal"> Media #1, Media #2</code> and<code class="literal"> Media #3</code> as indicated in the illustration. We wish to include a portion of each of these audio clips in the main timeline (the audio file to be saved). In the main media timeline, the audio between 0 seconds to 80 second represents a portion from<code class="literal"> Media #1</code>. It corresponds to the audio between 30 seconds to 110 seconds in<code class="literal"> Media #1</code>. Likewise, audio between 90 to 200 seconds on main media timeline represents a chunk from<code class="literal"> Media #2</code> and so on. Thus, we can tweak the priority and position of the individual audio clips on the main media timeline to create the desired audio output.</p><div><img src="img/0165_06_02.jpg" alt="Media 'timeline' explained"/></div><p>Main media timeline is represented with multiple media tracks in the preceding image.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec10"/>Time for action -  creating custom audio by combining clips</h1></div></div></div><p>Let's develop an application where we will combine multiple audio clips into a single audio file.<a id="id281" class="indexterm"/>
</p><div><ol class="orderedlist arabic"><li class="listitem">Download the file<code class="literal"> CombiningAudio.py</code>. This file contains all the code necessary to run this application. As done earlier, we will discuss only the most important methods in this class.</li><li class="listitem">Write the following code.<a id="id282" class="indexterm"/><div><pre class="programlisting">1 import os, sys, time
2 import thread
3 import gobject
4 from optparse import OptionParser
5
6 import pygst
7 pygst.require("0.10")
8 import gst
9
10 class AudioMerger:
11 def __init__(self):
12 pass
13 def constructPipeline(self):
14 pass
15 def addFadingEffect(self):
16 pass
17 def setupFadeBin(self):
18 pass
19 def addGnlFileSources(self):
20 pass
21 def gnonlin_pad_added(self, gnonlin_elem, pad):
22 pass
23 def run(self):
24 pass
25 def connectSignals(self):
26 pass
27 def printUsage(self):
28 pass
29 def printFinalStatus(self):
30 pass
31 def message_handler(self, bus, message):
32 pass
33 #Run the program
34 audioMerger = AudioMerger()
35 thread.start_new_thread(audioMerger.run, ())
36 gobject.threads_init()
37 evt_loop = gobject.MainLoop()
38 evt_loop.run()
</pre></div></li><li class="listitem">The overall structure of the code is identical to several other examples in this book. We will expand some of the class methods such as addFadingEffect, setupFadeBin in the next steps.</li><li class="listitem">Now, let's review the<code class="literal"> constructPipeline</code> method.<a id="id283" class="indexterm"/><div><pre class="programlisting">1 def constructPipeline(self):
2 self.pipeline = gst.Pipeline()
3 self.composition = (
4 gst.element_factory_make("gnlcomposition") )
5
6 # Add audio tracks to the gnl Composition
7 self.addGnlFileSources()
8
9 self.encoder = gst.element_factory_make("lame",
10 "mp3_encoder")
11 self.filesink = gst.element_factory_make("filesink")
12 self.filesink.set_property("location",
13 self.outFileLocation)
14
15 # Fade out the individual audio pieces
16 # when that audio piece is approaching end
17 self.addFadingEffect()
18
19 self.pipeline.add(self.composition,
20 self.fadeBin,
21 self.encoder,
22 self.filesink)
23
24 gst.element_link_many(self.fadeBin,
25 self.encoder,
26 self.filesink)
</pre></div></li><li class="listitem">We used functionality such as gnlcomposition, gnlcontroller, and so on while implementing audio fading effects in an earlier section. These modules will be used in this project as well. On line 7, all the audio clips we wish to include are added to the timeline or gnlcomposition. We will review this method later. Note that the gnlcomposition uses dynamic pads. The pad-added signal is connected in self.connectSignals. On line 17, a fading effect is set up for the audio clips. This ensures smooth termination of individual audio clips in the timeline. Finally, the code block between lines 19 to 26 constructs the pipeline and links various GStreamer elements in the pipeline. Let's review other important methods in this class one by one.</li><li class="listitem">The method<code class="literal"> self.addGnlFileSources</code> does multiple things. It adds the audio clips to the main timeline in a desired order. This method also ensures that there is some 'breathing space' or a blank audio of a short duration in between any two audio clips. Write the following method.<a id="id284" class="indexterm"/><div><pre class="programlisting">1 def addGnlFileSources(self):
2 #Parameters for gnlfilesources
3 start_time_1 = 0
4 duration_1 = 20
5 media_start_time_1 = 20
6 media_duration_1 = 20
7 inFileLocation_1 = "C:/AudioFiles/audio1.mp3"
8
9 start_time_2 = duration_1 + 3
10 duration_2 = 30
11 media_start_time_2 = 20
12 media_duration_2 = 30
13 inFileLocation_2 ="C:/AudioFiles/audio2.mp3"
14
15 #Parameters for blank audio between 2 tracks
16 blank_start_time = 0
17 blank_duration = start_time_2 + duration_2 + 3
18
19 # These timings will be used for adding fade effects
20 # See method self.addFadingEffect()
21 self.fade_start_1 = duration_1 - 3
22 self.fade_start_2 = start_time_2 + duration_2 - 3
23 self.fade_end_1 = start_time_1 + duration_1
24 self.fade_end_2 = start_time_2 + duration_2
25
26 filesrc1 = gst.element_factory_make("gnlfilesource")
27 filesrc1.set_property("uri",
28 "file:///" + inFileLocation_1)
29 filesrc1.set_property("start", start_time_1*gst.SECOND)
30 filesrc1.set_property("duration",
31 duration_1 * gst.SECOND )
32 filesrc1.set_property("media-start",
33 media_start_time_1*gst.SECOND)
34 filesrc1.set_property("media-duration",
35 media_duration_1*gst.SECOND)
36 filesrc1.set_property("priority", 1)
37
38 # Setup a gnl source that will act like a blank audio
39 # source.
40 gnlBlankAudio= gst.element_factory_make("gnlsource")
41 gnlBlankAudio.set_property("priority", 4294967295)
42 gnlBlankAudio.set_property("start",blank_start_time)
43 gnlBlankAudio.set_property("duration",
44 blank_duration * gst.SECOND)
45
46 blankAudio = gst.element_factory_make("audiotestsrc")
47 blankAudio.set_property("wave", 4)
48 gnlBlankAudio.add(blankAudio)
49
50 filesrc2 = gst.element_factory_make("gnlfilesource")
51 filesrc2.set_property("uri",
52 "file:///" + inFileLocation_2)
53 filesrc2.set_property("start",
54 start_time_2 * gst.SECOND)
55 filesrc2.set_property("duration",
56 duration_2 * gst.SECOND )
57 filesrc2.set_property("media-start",
58 media_start_time_2*gst.SECOND)
59 filesrc2.set_property("media-duration",
60 media_duration_2*gst.SECOND)
61 filesrc2.set_property("priority", 2)
63
63 self.composition.add(gnlBlankAudio)
64 self.composition.add(filesrc1)
65 self.composition.add(filesrc2)
</pre></div></li><li class="listitem">First we declare various parameters needed to put the audio clips in the main timeline. Here, the audio clips are mostly the gnlfilesource elements whereas the timeline is the total length of the output audio track. This parameter setting is done by the code between lines 3 to 13. In this example, we are combining only two audio clips. Replace the audio file paths on lines 7 and 13 with the appropriate file paths on your machine.<a id="id285" class="indexterm"/><div><div><h3 class="title"><a id="tip16"/>Tip</h3><p>Important note for Windows users: Make sure to specify the file path with forward slashes '/' as shown on line 13 of the code snippet. If the path is specified as, for instance,<code class="literal"> C:\AudioFiles\audio2.mp3</code>, the '\a' is treated differently by GStreamer! A workaround could be to normalize the path or to always use forward slashes while specifying the path. In this case<code class="literal"> C:/AudioFiles/audio2.mp3</code>.</p></div></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: none">The first media file will be placed for 20 seconds on the main timeline. The total duration of the audio is specified by the parameter media_duration_1. The parameter media_start_1 specifies the actual time of the first audio file which will be the start_time_1 on the main timeline. The basic concept behind timeline is explained earlier in this section. Try tweaking a few parameters to get a good grasp of how the timeline works. For the second audio, notice how the start_time_2 is specified. It is equal to duration_1 + 3. A time of 3 seconds is added so that there is a 'sound of silence' between two tracks. You can change this to a silent duration of your choice.</li></ul></div></li><li class="listitem">Next, the parameters necessary for the blank audio are defined. In general, the<code class="literal"> gnlcomposition</code> will 'play' the blank audio when nothing else is being played (this is with the assumption that a proper<code class="literal"> priority</code> is set). We define the total duration of this silent track sufficiently long enough, longer than the combined duration of all the audio clips, so that this track is 'available to play' when the time comes. Note that<code class="literal"> gnlcomposition</code> won't play the silent track for its complete duration! It is just so that we have a long enough track that can be played at various points. In this project, we are only using two audio files. So, it is not really necessary to set blank duration parameter as greater than or equal to the total timeline duration. It is okay if we just have it for 3 seconds. But imagine that you have more than 2 audio clips. The silent audio will be played between tracks 1 and 2 but then it won't be available for tracks between 2 and 3! If we were to have 3 audio tracks, then the blank audio duration can be set as illustrated in the following code snippet and by adding another<code class="literal"> gnlfilesource</code> to the<code class="literal"> self.composition</code>. You can also test the resultant audio file by specifying<code class="literal"> blank_duration = 3</code>. In that case, there won't be a silent track between audio clips 2 and 3!<a id="id286" class="indexterm"/><div><pre class="programlisting">start_time_3 = start_time_2 + duration_2 + 3
duration_3 = 30
media_start_time_3 = 0
media_duration_3 = 30
inFileLocation_3 ="C:\AudioFiles\audio3.mp3"
# Parameters for blank audio between 2 tracks
blank_start_time = 0
blank_duration = start_time_3 + duration_3 + 3
</pre></div></li><li class="listitem">The code between lines 19 to 24 sets up some instance variables needed to add fade-out effect to the individual audio clips in the<code class="literal"> gnlcomposition</code>. These will be used in the<code class="literal"> self.addFadingEffect</code> method.</li><li class="listitem">The code blocks 26-36 and 50-61 define the<code class="literal"> gnlfilesource</code> elements to be added to the<code class="literal"> self.composition</code> along with their properties. We have already learned about<code class="literal"> gnlfilesource</code>, so these code blocks should be self-explanatory. However, see the code on lines 36 ad 61? Here we set the priority of the audio clips in the main timeline. It is important step. If you don't define the priority, by default, each<code class="literal"> gnlsource</code> will have highest priority indicated by value '0'. This is a little bit tricky. It is best explained by tweaking certain values and actually playing the output audio! Let's keep it simple for now. See the next 'Have a go Hero' section that asks you to experiment a few things related to the<code class="literal"> priority</code>.</li><li class="listitem">Let's review the code block 40-44. Here, a<code class="literal"> gnlsource</code> (and not<code class="literal"> gnlfilesource)</code> is created on line 40. We call it<code class="literal"> gnlBlankAudio</code>. Line 41 is very important. It tells the program to consider this element last. That is,<code class="literal"> gnlBlankAudio</code> is set with the least possible priority among the elements added to the<code class="literal"> gnlcomposition</code>. This ensures that the blank piece of audio is played only between the tracks and not as an audio clip of its own. Whenever the start point of the next audio in the<code class="literal"> gnlcomposition</code> approaches, it will push the<code class="literal"> gnlBlankAudio</code> to a backseat and start playing this new audio clip instead. This is because the other audio clips are set at a higher<code class="literal"> priority</code> than the<code class="literal"> gnlBlankAudio</code>. You might be wondering what the value<code class="literal"> 4294967295</code> for<code class="literal"> priority</code> signifies. If you run<code class="literal"> gst-inspect-0.10</code> command on<code class="literal"> gnlsource</code> you will notice that the<code class="literal"> priority</code> has a range from<code class="literal"> 0</code> to4294967295. Thus the least possible priority level is<code class="literal"> 4294967295</code>. In this example, we can get away with the priority level of<code class="literal"> 3</code> because we have specified the<code class="literal"> blank_duration</code> parameter appropriately. But, suppose you don't know beforehand what<code class="literal"> blank_duration</code> should be and you set it to a large number. In this case, if you have set the priority of<code class="literal"> gnlBlankAudio</code> as<code class="literal"> 3</code>, at the end of the output audio it will play the remaining portion of the<code class="literal"> gnlBlankAudio</code>. Thus, the total track duration will be unnecessarily increased. Instead, if you use priority as<code class="literal"> 4294967295</code>, it won't play the surplus portion of the blank audio. If you have multiple of audio tracks and if their number is not known to begin with, the least priority level we are using is the safest value for the blank audio clip. As mentioned earlier, the following priority for<code class="literal"> gnlBlankAudio</code> should work as well.<div><pre class="programlisting">gnlBlankAudio.set_property("priority", 3)
</pre></div></li><li class="listitem">On line 46, an<code class="literal"> audiotestsrc</code> element is created. This plugin should be available in your installation of GStreamer. This plugin can be used to generate several elementary audio signals such as a sine waveform, a silent wave form, and so on. Run<code class="literal"> gst-inspect-0.10</code> on<code class="literal"> audiotestsrc</code> to see what types of audio signals it can generate. The type of audio signal we need can be specified by the 'wave' property of<code class="literal"> audiotestsrc</code> . The value of 4 for<code class="literal"> wave</code> property corresponds to a silence waveform. A value of 3 generates triangle wave forms and so on. On line 48, the<code class="literal"> audiotestsrc</code> element is added to the<code class="literal"> gnlsource</code> element (gnlBlankAudio). This simply means that when we start playing the<code class="literal"> gnlcomposition</code>, the silent audio pertaining<code class="literal"> gnlsource</code> element is generated using<code class="literal"> audiotestsrc</code> element within it.</li><li class="listitem">Finally, the code between lines 63-65 adds the<code class="literal"> gnlfilesource</code> and<code class="literal"> gnlsource</code> elements to the<code class="literal"> self.composition</code>.</li><li class="listitem">Now we will quickly review the method<code class="literal"> self.addFadingEffect()</code>.<div><pre class="programlisting">1 def addFadingEffect(self):
2 self.setupFadeBin()
3
4 #Volume control element
5 self.volumeControl = gst.Controller(self.volume,
6 "volume")
7 self.volumeControl.set_interpolation_mode("volume",
8 gst.INTERPOLATE_LINEAR)
9
10 fade_time = 20
11 fade_volume = 0.5
12 fade_end_time = 30
13
14 reset_time = self.fade_end_1 + 1
15
16 self.volumeControl.set("volume",
17 self.fade_start_1 * gst.SECOND,
18 1.0)
19 self.volumeControl.set("volume",
20 self.fade_end_1 * gst.SECOND,
21 fade_volume*0.2)
22 self.volumeControl.set("volume",
23 reset_time * gst.SECOND,
24 1.0)
25 self.volumeControl.set("volume",
26 self.fade_start_2 * gst.SECOND,
27 1.0)
28 self.volumeControl.set("volume",
29 self.fade_end_2 * gst.SECOND,
30 fade_volume*0.2)
</pre></div></li><li class="listitem">In<em> Fading effects</em> section, we added fade-out effect to an audio file. In that section individual elements such as audio convert and volume were added and linked in the main pipeline. Here, we will follow a different way, so as to learn a few more things in GStreamer. We will create a GStreamer<code class="literal"> bin</code> element to add the fade-out effect to the audio clips. You can choose to do it the old way, but creating a<code class="literal"> bin</code> provides a certain level of abstraction. The<code class="literal"> bin</code> element is created by the highlighted line of code. We will review that method next. The rest of the code in this method is very similar to the one developed earlier. The<code class="literal"> self.volumeControl</code> is a GStreamer<code class="literal"> Controller</code> element. We specify volume at appropriate time intervals in the timeline to implement fade-out effect for the individual audio clips. It is important to adjust the level of volume back to the original one after each<code class="literal"> fade_end</code> time. This ensures that the next clip starts with an appropriate level of volume. This is achieved by code between lines 22-24.</li><li class="listitem">Now let's see how to construct a GStreamer bin element for the fading effect.<div><pre class="programlisting">1 def setupFadeBin(self):
2 self.audioconvert = gst.element_factory_make(
3 "audioconvert")
4 self.volume = gst.element_factory_make("volume")
5 self.audioconvert2 = gst.element_factory_make(
6 "audioconvert")
7
8 self.fadeBin = gst.element_factory_make("bin",
9 "fadeBin")
10 self.fadeBin.add(self.audioconvert,
11 self.volume,
12 self.audioconvert2)
13
14 gst.element_link_many(self.audioconvert,
15 self.volume,
16 self.audioconvert2)
17
18 # Create Ghost pads for fadeBin
19 sinkPad = self.audioconvert.get_pad("sink")
20 self.fadeBinSink = gst.GhostPad("sink", sinkPad)
21 self.fadeBinSrc = (
22 gst.GhostPad("src", self.audioconvert2.get_pad("src")) )
23
24 self.fadeBin.add_pad(self.fadeBinSink)
25 self.fadeBin.add_pad(self.fadeBinSrc)
</pre></div></li><li class="listitem">On lines 2-6, we define the elements necessary to change volume of an audio in a GStreamer pipeline. This is nothing new. On line 8, we create<code class="literal"> self.fadeBin</code>, a GStreamer bin element. A<code class="literal"> bin</code> is a container that manages the element objects added to it. The essential elements are added to this bin on line 10. The elements are then linked the same way we link elements in a GStreamer pipeline. The bin itself is pretty much set up. But there is one more important thing. We need to ensure that this bin can be linked with other elements in a GStreamer pipeline. For that we need to create ghost pads.</li><li class="listitem">Recall what a<code class="literal"> ghost pad</code> is from the last chapter. A<code class="literal"> bin</code> element is an 'abstract element'. It doesn't have<code class="literal"> pads</code> of its own. But in order to work like an element, it needs<code class="literal"> pads</code> to connect to the other elements within the pipeline. So the<code class="literal"> bin</code> uses a<code class="literal"> pad</code> of an element within it as if it was its own<code class="literal"> pad.</code> This is called a ghost pad. Thus the<code class="literal"> ghost pads</code> are used to connect an appropriate element inside a<code class="literal"> bin</code>. It enables using a<code class="literal"> bin</code> object as an abstract element in a GStreamer pipeline. We create two<code class="literal"> ghost pads</code>. One as<code class="literal"> src pad</code> and one as<code class="literal"> sink pad</code>. It is done by the code on lines 19-22. Note that we use<code class="literal"> sink pad</code> of<code class="literal"> self.audioconvert</code> as the<code class="literal"> sink ghost pad</code> of the bin and<code class="literal"> src pad</code> of<code class="literal"> self.audioconvert2</code> as<code class="literal"> src ghost pad</code>. Which pad to use as src or sink is decided by how we link elements within the bin. Looking at the code between lines 14 to 17 will make it clear. Finally, the<code class="literal"> ghost pads</code> are added to the<code class="literal"> self.fadeBin</code> on lines 24 and 25.</li><li class="listitem">The method<code class="literal"> self.gnonlin_pad_added()</code> gets called whenever the<code class="literal"> pad-added</code> signal is emitted for<code class="literal"> self.composition</code>. Notice that<code class="literal"> compatible_pad</code> in this method is obtained from<code class="literal"> self.fadeBin</code>.<div><pre class="programlisting">def gnonlin_pad_added(self, gnonlin_elem, pad):
caps = pad.get_caps()
compatible_pad = \
self.fadeBin.get_compatible_pad(pad, caps)
pad.link(compatible_pad)
</pre></div></li><li class="listitem">Develop the rest of the methods by reviewing the code in file<code class="literal"> CombiningAudio.py</code>. Be sure to specify appropriate input and output audio file locations. Once all the pieces are in place, run the program as:<div><pre class="programlisting">python CombiningAudio.py
</pre></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: none">This should create the output audio file containing audio clips combined together!</li></ul></div></li></ol></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec18"/>What just happened?</h2></div></div></div><p>In this project we developed a cool application that can combine two or more audio clips into a single audio file. To accomplish this, we used many audio processing techniques learned in earlier sections and the previous chapter on audio processing. We made use of various elements from<code class="literal"> gnonlin</code> plugin such as<code class="literal"> gnlcomposition, gnlfilesource</code>, and<code class="literal"> gnlsource</code> . We learned how to create and link a GStreamer<code class="literal"> bin</code> container to represent the fade-out effect as an abstract element in the pipeline. Among other things, we learned how to insert a blank audio in-between audio clips.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec19"/>Have a go hero change various properties of 'gnlfilesource'</h2></div></div></div><p>In the earlier<em> Time for action</em> section, we set priority property for the two<code class="literal"> gnlfilesource</code> elements added to the<code class="literal"> gnlcomposition</code>. Tweak the<code class="literal"> start</code> and the<code class="literal"> priority</code> properties of the two<code class="literal"> gnlfilesource</code> elements to see what happens to the output audio. For example, swap the priority of two<code class="literal"> gnlfilesource</code> elements and change the<code class="literal"> start_time_2</code> to<code class="literal"> duration_1</code>, and see what happens. Notice how it affects the playback of the first audio clip!</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec11"/>Audio mixing</h1></div></div></div><p>Imagine that you have some instrumental music files in your collection. You have a hidden desire to become a playback singer and you wish to sing these songs with the background music. What will you do? Well, the simplest thing to do is to put on headphones and play any instrumental music. Then sing along and record your vocal. OK, what's next? You need to mix the instrumental music and your own vocal together to get what you want!<a id="id287" class="indexterm"/>
</p><p>Let's see how to mix two audio tracks together. The<code class="literal"> interleave</code> is a GStreamer plugin that facilitates mixing of two audio tracks. It merges multiple mono channel input audios into a single audio stream in a non-contiguous fashion. This plugin should be available in your default GStreamer installation.</p></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec12"/>Time for action -  mixing audio tracks</h1></div></div></div><p>Let's write a utility that can mix two audio streams together.</p><div><ol class="orderedlist arabic"><li class="listitem">Download the file<code class="literal"> AudioMixer.py</code> which contains the source code for this utility.</li><li class="listitem">Now we will review the<code class="literal"> constructPipeline</code> method. The API method<code class="literal"> gst.parse_launch()</code> explained in the previous chapter will be used here.<div><pre class="programlisting">1 def constructPipeline(self):
2 audio1_str = (" filesrc location=%s ! "
3 "decodebin ! audioconvert ! "
4 % (self.inFileLocation_1) )
5
6 audio2_str = ( " filesrc location=%s "
7 "! decodebin ! audioconvert ! "
8 %(self.inFileLocation_2) )
9
10 interleave_str = ( "interleave name=mix ! "
11 " audioconvert ! lame ! "
12 " filesink location=%s"%self.outFileLocation )
13
14 queue_str = " ! queue ! mix."
15
16 myPipelineString = (
17 interleave_str + audio1_str + queue_str +
18 audio2_str + queue_str )
19
20 self.pipeline = gst.parse_launch(myPipelineString)
</pre></div></li><li class="listitem">The<code class="literal"> audio1_str</code> and<code class="literal"> audio2_str</code> are the portions of the main pipeline strings. Each of these contain<code class="literal"> filesrc</code> ,<code class="literal"> decodebin</code>, and<code class="literal"> audioconvert</code> elements. The<code class="literal"> filesrc</code> provides the location of respective input audio files. By now, we very well know what this portion of a GStreamer pipeline does.</li><li class="listitem">On lines 10-12, the<code class="literal"> interleave_str</code> defines another portion of the main pipeline string. The data output from the<code class="literal"> interleave</code> element needs to be converted into a format expected by the encoder element. The encoder is then connected to the<code class="literal"> filesink</code> element where the output audio will be stored.<a id="id288" class="indexterm"/></li><li class="listitem">As mentioned earlier, the<code class="literal"> interleave</code> merges multiple audio channels into a single audio stream. In this case, the<code class="literal"> interleave</code> element reads in data from two different audio streams via queue elements.<p>The sink pad of the queue element is linked with the audioconvert element. The queue element is a buffer to which the audio data from the audioconvert is written. Then this data is further read by the interleave element. This linkage within the GStreamer pipeline can be represented by the following string "audioconvert ! queue ! mix.". Note that the dot '.' after 'mix' is important. It is a part of the syntax when gst.parse_launch is used.
</p></li><li class="listitem">To summarize, the data streamed from the portions of the pipeline,<code class="literal"> audio1_str</code> and<code class="literal"> audio2_str</code>, will be ultimately read by the<code class="literal"> interleave</code> via 'queue' elements and then it will follow the rest of the pipeline represented by<code class="literal"> interleave_str</code>.<p>On line 20, the pipeline string is fed to gst.parse_launch to create a GStreamer pipeline instance.
</p></li><li class="listitem">Review the rest of the code from the source file<code class="literal"> AudioMixer.py</code>. Change the input and output audio file path strings represented by<code class="literal"> self.inFileLocation_1, self.inFileLocation_2</code>, and<code class="literal"> self.outFileLocation</code>. Then run the code as:<div><pre class="programlisting">$python AudioMixer.py
</pre></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: none">This should create the interleaved audio output. If you play this audio file, you will hear both the audio clips playing at once. Try selecting only a single audio channel, such as "Left" channel or "Right" channel. In this case, you will notice that each of these audio clips is sent stored on a separate channel. For example, if you play only the left channel, only one of these audio clips will be heard, so would be the case for the other channel.</li></ul></div></li></ol></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec20"/>What just happened?</h2></div></div></div><p>Using<code class="literal"> interleave</code> element, we merged two audio tracks to create an interleaved audio. This can be used as an audio mixing utility. We learned how to use<code class="literal"> queue</code> element as an audio data buffer which is then read by the interleave element.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec13"/>Visualizing an audio track</h1></div></div></div><p>Most of the popular audio players provide a feature to 'visualize' the audio being played. This visualization effect is typically generated on the fly and is synchronized with the audio signal. Typically, the visualizer responds to changes in audio frequency and volume level among other properties. These changes are then shown by use of animated graphics. GStreamer provides certain plugins to visualize a track. The 'monoscope' visualization plugin is generally available in the default GStreamer installation. It displays a highly stabilized waveform of the streaming audio. Make sure that the GStreamer installation has the<code class="literal"> monoscope</code> plugin by running the<code class="literal"> gst-inspect-0.10</code> command. There are several other popular plugins such as<code class="literal"> goom</code> and<code class="literal"> libvisual</code>. But these are not available by default in the GStreamer binary installed on Windows XP. You can install these plugins and try using these to add visualization effects.<a id="id289" class="indexterm"/>
</p></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec14"/>Time for action -  audio visualizer</h1></div></div></div><p>The visualization effect can be added to the streaming audio using different techniques. We will use the simplest approach of all to develop a Music Visualizer utility.<a id="id290" class="indexterm"/>
</p><p>Here, we will be using the<code class="literal"> playbin</code> plugin of GStreamer. Recall that the<code class="literal"> playbin</code> was first used in the<em> Playing an audio from a Website</em> section of the Working with Audios chapter. This plugin provides a higher level audio /video player and it should be available in the default GStreamer installation.</p><div><ol class="orderedlist arabic"><li class="listitem">Download the file<code class="literal"> MusicVisualizer.py</code> from the Packt website. This is a small program. The class methods are represented below. Look at the code from this file for more details.<div><pre class="programlisting">class AudioPlayer:
def __init__(self):
pass
def connectSignals(self):
pass
def play(self):
pass
def message_handler(self, bus, message):
pass
</pre></div></li><li class="listitem">Most of the code is identical to the one illustrated in the Playing audio from a website section of the previous chapter. The only difference here is the constructor of the class where various properties of the playbin element are defined.<p>Now let's review the constructor of the class AudioPlayer.
</p><div><pre class="programlisting">1 def __init__(self):
2 self.is_playing = False
3 inFileLocation = "C:/AudioFiles/audio1.mp3"
4
5 #Create a playbin element
6 self.player = gst.element_factory_make("playbin")
7
8 # Create the audio visualization element.
9 self.monoscope = gst.element_factory_make("monoscope")
10 self.player.set_property("uri",
11 "file:///" + inFileLocation)
12 self.player.set_property("vis-plugin", self.monoscope)
13 self.connectSignals()
</pre></div></li><li class="listitem">Modify the<code class="literal"> inFileLocation</code> on line 3 to match an audio file path on your computer. On line 6 and 8, the<code class="literal"> playbin</code> and<code class="literal"> monoscope</code> elements are created. The latter is a plugin that enables audio visualization. On line 12, we set the value for property<code class="literal"> vis-plugin</code> as the<code class="literal"> monoscope</code> element created earlier. The<code class="literal"> vis-plugin</code> stands for 'visualization plugin' that the<code class="literal"> playbin</code> element should use to visualize the music.<a id="id291" class="indexterm"/></li><li class="listitem">That's all! You can review the rest of the code from the file<code class="literal"> MusicVisualizer.py</code>. Now run the program from the command line as:<div><pre class="programlisting">$python MusicVisualizer.py
</pre></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: none">This should start playing the input audio file and at the same time, it should also pop up a small window where you can 'visualize' the streaming audio.</li></ul></div><div><div><h3 class="title"><a id="tip17"/>Tip</h3><p>Note: The overall performance of this application may depend on the number of processes running at the time this program is run. It may also depend on the specifications of your computer such as processor speed.</p></div></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: none">Here, the stable audio waveform will be shown as the music plays. The following shows a snapshot of this visualization window at two different timeframes.</li></ul></div><div><img src="img/0165_6_3.jpg" alt="Time for action - audio visualizer"/></div><div><img src="img/0165_6_4.jpg" alt="Time for action - audio visualizer"/></div><p>Snapshots at some random timeframes using Music Visualizer using 'monoscope' are depicted here.</p></li></ol></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec21"/>What just happened?</h2></div></div></div><p>We used the GStreamer plugins<code class="literal"> playbin</code> and<code class="literal"> monoscope</code> to develop an audio visualization utility for a streaming audio. The<code class="literal"> monoscope</code> element provided a way to visualize highly stable audio waveforms.<a id="id292" class="indexterm"/>
</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec22"/>Have a go hero use other visualization plugins</h2></div></div></div><p>To illustrate visualization effects for an audio, the<code class="literal"> monoscope</code> plugin was used. If you have some other visualization plugins available in the GStreamer installation, use those to create different visualization effects. The following are some of the plugins that can be used for this purpose:<code class="literal"> goom, goom2k1, libvisual</code>, and<code class="literal"> synaesthesia</code>. The audio visualization accomplished by<code class="literal"> synaesthesia</code> plugin is shown in the next illustration.</p><div><img src="img/0165_6_5.jpg" alt="Have a go hero use other visualization plugins"/></div><div><img src="img/0165_6_6.jpg" alt="Have a go hero use other visualization plugins"/></div><p>Music Visualizer using 'synaesthesia': Snapshots at some random timeframes is depicted here.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec15"/>Summary</h1></div></div></div><p>We learned a lot in this chapter about various audio enhancement and control techniques. The GStreamer multimedia framework was used to accomplish this. We specifically covered:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Audio controls: How to control the streaming of an audio data. With the help of coding illustrations, we learned about playback controls such as play, pause, seek, and stop. These controls were then used in a project where a portion of an audio was extracted.</li><li class="listitem" style="list-style-type: disc">Adding effects: Enhancing the audio by adding audio effects such as fade-in, echo/reverberation, and so on.</li><li class="listitem" style="list-style-type: disc">Non-linear audio editing: How to combine two or more audio streams into a single track. This was done in one of the projects we undertook.</li><li class="listitem" style="list-style-type: disc">Audio mixing technique to merge multiple mono channel audio streams into a single interleaved audio.</li></ul></div><p>Additionally, we also learned techniques such as visualizing an audio. This concludes our discussion on audio processing in Python using GStreamer framework.</p><p>In the next chapter, we will learn how to process videos using Python.</p></div></div>
</body></html>