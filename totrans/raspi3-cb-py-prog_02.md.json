["```py\nfrom sklearn.datasets import fetch_20newsgroups \ncategory_mapping = {'misc.forsale': 'Sellings', 'rec.motorcycles': 'Motorbikes', \n        'rec.sport.baseball': 'Baseball', 'sci.crypt': 'Cryptography', \n        'sci.space': 'OuterSpace'} \n\ntraining_content = fetch_20newsgroups(subset='train', \ncategories=category_mapping.keys(), shuffle=True, random_state=7) \n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer \n\nvectorizing = CountVectorizer() \ntrain_counts = vectorizing.fit_transform(training_content.data) \nprint \"nDimensions of training data:\", train_counts.shape \n```", "```py\nfrom sklearn.naive_bayes import MultinomialNB \nfrom sklearn.feature_extraction.text import TfidfTransformer \n\ninput_content = [ \n    \"The curveballs of right handed pitchers tend to curve to the left\", \n    \"Caesar cipher is an ancient form of encryption\", \n    \"This two-wheeler is really good on slippery roads\" \n] \n\ntfidf_transformer = TfidfTransformer() \ntrain_tfidf = tfidf_transformer.fit_transform(train_counts) \n```", "```py\nclassifier = MultinomialNB().fit(train_tfidf, training_content.target) \ninput_counts = vectorizing.transform(input_content) \ninput_tfidf = tfidf_transformer.transform(input_counts) \n```", "```py\ncategories_prediction = classifier.predict(input_tfidf) \n```", "```py\nfor sentence, category in zip(input_content, categories_prediction): \n    print 'nInput:', sentence, 'nPredicted category:',  \n            category_mapping[training_content.target_names[category]] \n```", "```py\nfrom nltk.tokenize import sent_tokenize\n```", "```py\ntokenize_list_sent = sent_tokenize(text)\nprint \"nSentence tokenizer:\" \nprint tokenize_list_sent \n```", "```py\nfrom nltk.tokenize import word_tokenize \nprint \"nWord tokenizer:\" \nprint word_tokenize(text) \n```", "```py\nfrom nltk.tokenize import WordPunctTokenizer \nword_punct_tokenizer = WordPunctTokenizer() \nprint \"nWord punct tokenizer:\" \nprint word_punct_tokenizer.tokenize(text) \n```", "```py\nfrom nltk.stem.porter import PorterStemmer \nfrom nltk.stem.lancaster import LancasterStemmer \nfrom nltk.stem.snowball import SnowballStemmer \n```", "```py\nwords = ['ability', 'baby', 'college', 'playing', 'is', 'dream', 'election', 'beaches', 'image', 'group', 'happy'] \n```", "```py\nstemmers = ['PORTER', 'LANCASTER', 'SNOWBALL'] \n```", "```py\nstem_porter = PorterStemmer() \nstem_lancaster = LancasterStemmer() \nstem_snowball = SnowballStemmer('english') \n```", "```py\nformatted_row = '{:>16}' * (len(stemmers) + 1) \nprint 'n', formatted_row.format('WORD', *stemmers), 'n' \n```", "```py\nfor word in words:\n  stem_words = [stem_porter.stem(word), \n  stem_lancaster.stem(word), \n  stem_snowball.stem(word)] \n  print formatted_row.format(word, *stem_words) \n```", "```py\nimport numpy as np \nfrom nltk.corpus import brown \n```", "```py\n# Split a text into chunks \ndef splitter(content, num_of_words): \n   words = content.split(' ') \n   result = [] \n```", "```py\n   current_count = 0 \n   current_words = []\n```", "```py\n   for word in words: \n     current_words.append(word) \n     current_count += 1 \n```", "```py\n     if current_count == num_of_words: \n       result.append(' '.join(current_words)) \n       current_words = [] \n       current_count = 0 \n```", "```py\n       result.append(' '.join(current_words)) \n       return result \n```", "```py\nif __name__=='__main__': \n  # Read the data from the Brown corpus \n  content = ' '.join(brown.words()[:10000]) \n```", "```py\n  # Number of words in each chunk \n  num_of_words = 1600 \n```", "```py\n  chunks = [] \n  counter = 0 \n```", "```py\n  num_text_chunks = splitter(content, num_of_words) \n  print \"Number of text chunks =\", len(num_text_chunks) \n```", "```py\nimport numpy as np \nfrom nltk.corpus import brown \nfrom chunking import splitter \n```", "```py\nif __name__=='__main__': \n        content = ' '.join(brown.words()[:10000]) \n```", "```py\n    num_of_words = 2000 \n    num_chunks = [] \n    count = 0 \n    texts_chunk = splitter(content, num_of_words) \n```", "```py\n    for text in texts_chunk: \n      num_chunk = {'index': count, 'text': text} \n      num_chunks.append(num_chunk) \n      count += 1\n```", "```py\n  from sklearn.feature_extraction.text      \n  import CountVectorizer\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer \nvectorizer = CountVectorizer(min_df=5, max_df=.95) \nmatrix = vectorizer.fit_transform([num_chunk['text'] for num_chunk in num_chunks]) \n```", "```py\nvocabulary = np.array(vectorizer.get_feature_names()) \nprint \"nVocabulary:\" \nprint vocabulary \n```", "```py\nprint \"nDocument term matrix:\" \nchunks_name = ['Chunk-0', 'Chunk-1', 'Chunk-2', 'Chunk-3', 'Chunk-4'] \nformatted_row = '{:>12}' * (len(chunks_name) + 1) \nprint 'n', formatted_row.format('Word', *chunks_name), 'n' \n```", "```py\nfor word, item in zip(vocabulary, matrix.T): \n# 'item' is a 'csr_matrix' data structure \n result = [str(x) for x in item.data] \n print formatted_row.format(word, *result)\n```"]