<html><head></head><body>
<div><div><div><h1 id="_idParaDest-33"><em class="italic"><a id="_idTextAnchor032"/>Chapter 2</em>: Pure Python Optimizations</h1>
			<p>As mentioned in the previous chapter, one of the most effective ways of improving the performance of applications is through the use of better algorithms and data structures. The Python standard library provides a large variety of ready-to-use algorithms and data structures that can be directly incorporated into your applications. With the tools learned from this chapter, you will be able to use the right algorithm for the task and achieve massive speed gains.</p>
			<p>Even though many algorithms have been around for quite a while, they are especially relevant in today's world as we continuously produce, consume, and analyze ever-increasing amounts of data. Buying a larger server or micro-optimizing can work for some time, but achieving better scaling through algorithmic improvement can solve the problem once and for all.</p>
			<p>In this chapter, we will learn how to achieve better scaling using standard algorithms and data structures. More advanced use cases where we take advantage of third-party libraries will also be covered. We will also learn about tools to implement caching, a technique used to achieve faster response times by sacrificing some space in memory or on disk.</p>
			<p>The list of topics to be covered in this chapter is as follows:</p>
			<ul>
				<li>Using the right algorithms and data structures</li>
				<li>Improved efficiency with caching and memoization</li>
				<li>Efficient iteration with comprehensions and generators</li>
			</ul>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor033"/>Technical requirements</h1>
			<p>You will find the code files for this chapter here: <a href="https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter02">https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter02</a>.</p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor034"/>Using the right algorithms and data structures</h1>
			<p>Algorithmic improvements are especially effective in increasing performance because they typically allow the application to scale better with increasingly large inputs.</p>
			<p>Algorithm running times<a id="_idIndexMarker087"/> can be classified according to their computational complexity, a characterization of the resources required to perform a task. </p>
			<p>Such classification is expressed through the <em class="italic">Big O notation</em>, an upper bound on the operations required to execute the task, which usually depends on the input size. Specifically, Big O notation<a id="_idIndexMarker088"/> describes how the runtime or memory requirement of an algorithm grows in terms of the input size. For this reason, a lower Big O denotes a more efficient algorithm, which is what we aim for.</p>
			<p>For example, incrementing each element of a list can be implemented using a <code>for</code> loop, as follows:</p>
			<pre>    input = list(range(10))
    for i, _ in enumerate(input):
        input[i] += 1 </pre>
			<p>If the operation does not depend on the size of the input (for example, accessing the first element of a list), the algorithm is said to take constant, or <em class="italic">O</em>(1), time. This means that, no matter how much data we have, the time to run the algorithm will always be the same.</p>
			<p>In this simple algorithm, the <code>input[i] += 1</code> operation will be repeated <code>10</code> times, which is the size of the input. If we double the size of the input array, the number of operations will increase proportionally. Since the number of operations is proportional to the input size, this algorithm is said to take <em class="italic">O</em>(<em class="italic">N</em>) time, where <em class="italic">N</em> is the size of the input array.</p>
			<p>In some instances, the running time<a id="_idIndexMarker089"/> may depend on the structure of the input (for example, if the collection is sorted or contains many duplicates). In these cases, an algorithm may have different best-case, average-case, and worst-case running times. Unless stated otherwise, the running times presented in this chapter are considered to be average running times.</p>
			<p>In this section, we will examine the running times of algorithms and data structures that are implemented in the Python standard<a id="_idIndexMarker090"/> library and understand how improving running times results in massive gains and<a id="_idIndexMarker091"/> allows us to solve large-scale problems with elegance.</p>
			<p>You can find the code used to run the benchmarks in this chapter in the <code>Algorithms.ipynb</code> notebook, which can be opened using <em class="italic">Jupyter</em>.</p>
			<p>First, we will examine <strong class="bold">lists</strong> and <strong class="bold">deques</strong>.</p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/>Lists and deques</h2>
			<p>Python lists are ordered collections<a id="_idIndexMarker092"/> of elements and, in Python, are implemented as resizable arrays. An <strong class="bold">array</strong> is a basic data structure<a id="_idIndexMarker093"/> that consists of a series of contiguous memory locations, and each location contains a reference to a Python object.</p>
			<p>Lists shine in accessing, modifying, and appending elements. Accessing or modifying an element involves fetching the object reference from the appropriate position of the underlying array and has <em class="italic">O</em>(1) complexity. Appending an element is also very fast. When an empty list is created, an array of fixed size is allocated and, as we insert elements, the slots in the array are gradually filled up. Once all the slots are occupied, the list needs to increase the size of its underlying array, thus triggering a memory reallocation that can take <em class="italic">O</em>(<em class="italic">N</em>) time. Nevertheless, those memory allocations are infrequent, and the time complexity for the append operation is referred to as amortized <em class="italic">O</em>(1) time.</p>
			<p>The list operations that may have efficiency problems are those that add or remove elements at the beginning (or somewhere in the middle) of the list. When an item is inserted or removed from the beginning of a list, all the subsequent elements of the array need to be shifted by a position, thus taking <em class="italic">O</em>(<em class="italic">N</em>) time.</p>
			<p>In the following table, the timings for different operations on a list of 10,000 size are shown; you can see how insertion and removal performances vary quite dramatically if performed at the beginning or the end of the list:</p>
			<div><div><img src="img/B17499_Table_2.1.jpg" alt="Table 1.1 – The speed of different list operations " width="1650" height="332"/>
				</div>
			</div>
			<p class="figure-caption">Table 1.1 – The speed of different list operations</p>
			<p>In some cases, it is necessary to efficiently perform the insertion or removal of elements both at the beginning and the end of the collection. Python provides a data structure with those properties in the <code>collections.deque</code> class. The word <em class="italic">deque</em> stands for <strong class="bold">double-ended queue</strong> because this data structure<a id="_idIndexMarker094"/> is designed to efficiently<a id="_idIndexMarker095"/> put and remove elements at the beginning and the end<a id="_idIndexMarker096"/> of the collection, as it is in the case of queues. In Python, deques are implemented as doubly linked lists.</p>
			<p>Deques, in addition to <code>pop</code> and <code>append</code>, expose the <code>popleft</code> and <code>appendleft</code> methods that have <em class="italic">O</em>(1) running time:</p>
			<div><div><img src="img/B17499_Table_2.2.jpg" alt="Table 1.2 – The speed of different deque operations " width="1650" height="394"/>
				</div>
			</div>
			<p class="figure-caption">Table 1.2 – The speed of different deque operations</p>
			<p>Despite these advantages, deques should not be used to replace regular lists in most cases. The efficiency gained by the <code>appendleft</code> and <code>popleft</code> operations comes at a cost – accessing an element in the middle of a deque is an <em class="italic">O</em>(N) operation, as shown in the following table:</p>
			<div><div><img src="img/B17499_Table_2.3.jpg" alt="Table 1.3 – The inefficiency of deques in accessing the middle element " width="1650" height="279"/>
				</div>
			</div>
			<p class="figure-caption">Table 1.3 – The inefficiency of deques in accessing the middle element</p>
			<p>Searching for an item in a list is generally an <em class="italic">O</em>(<em class="italic">N</em>) operation and is performed using the <code>list.index</code> method. A simple way to speed up searches in lists is to keep the array sorted and perform a binary search using the <code>bisect</code> module.</p>
			<p>The <code>bisect</code> module allows<a id="_idIndexMarker097"/> fast searches on sorted arrays. The <code>bisect.bisect</code> function can be used on a sorted<a id="_idIndexMarker098"/> list to find the index to place an element while maintaining the array in sorted order. In the following example, we can see that if we want to insert the <code>3</code> element in the array while keeping <code>collection</code> in sorted order, we should put <code>3</code> in the third position (which corresponds to index <code>2</code>):</p>
			<pre>    insert bisect
    collection = [1, 2, 4, 5, 6]
    bisect.bisect(collection, 3)
    # Result: 2</pre>
			<p>This function uses the binary search algorithm that has <em class="italic">O</em>(<em class="italic">log</em>(<em class="italic">N</em>)) running time. Such a running time is exceptionally fast and, basically, means that your running time will increase by a constant amount every time you <em class="italic">double</em> your input size. This means that if, for example, your program takes <code>1</code> second to run on an input of <code>1000</code> size, it will take <code>2</code> seconds to process an input of <code>2000</code> size, <code>3</code> seconds to process an input of <code>4000</code> size, and so on. If you had <code>100</code> seconds, you could theoretically process an input of <code>10^33</code> size, which is larger than the number of atoms in your body!</p>
			<p>If the value we are trying to insert is already present in the list, the <code>bisect.bisect</code> function will return the location <em class="italic">after</em> the already present value. Therefore, we can use the <code>bisect.bisect_left</code> variant, which returns the correct index in the following way (taken from<a id="_idIndexMarker099"/> the module documentation at <a href="https://docs.python.org/3.5/library/bisect.html">https://docs.python.org/3.5/library/bisect.html</a>):</p>
			<pre>    def index_bisect(a, x):
      'Locate the leftmost value exactly equal to x'
      i = bisect.bisect_left(a, x)
      if i != len(a) and a[i] == x:
      return i
      raise ValueError</pre>
			<p>In the following table, you can<a id="_idIndexMarker100"/> see how the running time of the <code>bisect</code> solution is barely affected<a id="_idIndexMarker101"/> by these input sizes, making it a suitable solution when searching through very large collections:</p>
			<div><div><img src="img/B17499_Table_2.4.jpg" alt="" width="1650" height="200"/>
				</div>
			</div>
			<p class="figure-caption">Table 1.4 – The efficiency of the bisect function</p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor036"/>Dictionaries</h2>
			<p><strong class="bold">Dictionaries</strong> are extremely versatile and extensively<a id="_idIndexMarker102"/> used in the Python language, for example, in package-, module-, and class-level namespaces, as well as object and class annotations. Dictionaries are implemented as hash maps and are very good at element insertion, deletion, and access; all these operations have an average <em class="italic">O</em>(1) time complexity.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">In Python versions up to 3.5, dictionaries are <em class="italic">unordered collections</em>. Since Python 3.6, dictionaries are capable of maintaining their elements by order of insertion.</p>
			<h3 id="_idParaDest-38">Hash map</h3>
			<p>A <code>hash</code> function; Python implements <code>hash</code> functions for several data types. As a demonstration, the generic function to obtain hash codes is <code>hash</code>. In the following example, we show you how to obtain the hash code when given the <code>"hello"</code> string:</p>
			<pre>    hash("hello")
    # Result: -1182655621190490452
    # To restrict the number to be a certain range you can
      use
    # the modulo (%) operator
      hash("hello") % 10
    # Result: 8</pre>
			<p>Hash maps can be tricky to implement because they need to handle collisions that happen when two different objects have the same hash code. However, all the complexity is elegantly hidden behind the implementation and the default collision resolution works well in most real-world scenarios.</p>
			<p>The access to, insertion, and removal<a id="_idIndexMarker104"/> of an item in a dictionary scales as <em class="italic">O</em>(1) with the size of the dictionary. However, note that the computation of the <code>hash</code> function still needs to happen and, for strings, the computation scales with the length of the string. As string keys are usually relatively small, this doesn't constitute a problem in practice.</p>
			<p>A dictionary can be used to efficiently count unique elements in a list. In this example, we define the <code>counter_dict</code> function that takes a list and returns a dictionary containing the number of occurrences of each value in the list:</p>
			<pre>    def counter_dict(items): 
        counter = {} 
        for item in items: 
            if item not in counter: 
                counter[item] = 1 
            else: 
                counter[item] += 1 
        return counter</pre>
			<p>The code can be somewhat simplified using <code>collections.defaultdict</code>, which can be used to produce dictionaries where each new key is automatically assigned a default value. In the following code, the <code>defaultdict(int)</code> call produces a dictionary where every<a id="_idIndexMarker105"/> new element is automatically assigned a zero value and can be used to streamline the counting:</p>
			<pre>    from collections import defaultdict
    def counter_defaultdict(items):
        counter = defaultdict(int)
        for item in items:
            counter[item] += 1
        return counter</pre>
			<p>The <code>collections</code> module also includes a <code>Counter</code> class that can be used for the same purpose with a single line of code:</p>
			<pre>    from collections import Counter
    counter = Counter(items)</pre>
			<p>Speed-wise, all these ways of counting have the same time complexity, but the <code>Counter</code> implementation is the most efficient, as shown in the following table:</p>
			<div><div><img src="img/B17499_Table_2.5.jpg" alt="Table 1.5 – The different methods of computing counters " width="1650" height="255"/>
				</div>
			</div>
			<p class="figure-caption">Table 1.5 – The different methods of computing counters</p>
			<h4>Building an in-memory search index using a hash map</h4>
			<p>Dictionaries can be used to quickly search for a word in a list of documents, similar to a search engine. In this subsection, we will learn how to build<a id="_idIndexMarker106"/> an inverted index based on a dictionary of lists. Let's say we have a collection<a id="_idIndexMarker107"/> of four documents:</p>
			<pre>    docs = ["the cat is under the table",
            "the dog is under the table",
            "cats and dogs smell roses",
            "Carla eats an apple"]</pre>
			<p>A simple way to retrieve all the documents that match a query is to scan each document and test for the presence of a word. For example, if we want to look up the documents where the word <code>table</code> appears, we can employ the following filtering operation:</p>
			<pre>    matches = [doc for doc in docs if "table" in doc]</pre>
			<p>This approach is simple and works well when we have one-off queries; however, if we need to query the collection very often, it can be beneficial to optimize querying time. Since the per-query cost of the linear scan is <em class="italic">O</em>(<em class="italic">N</em>), you can imagine that better scaling will allow us to handle much larger document collections.</p>
			<p>A better strategy is to spend some time preprocessing the documents so that they are easier to find<a id="_idIndexMarker108"/> at query time. We can build a structure, called the <code>"table"</code> will be associated with the <code>"the cat is under the table"</code> and <code>"the dog is under the table"</code> documents; they correspond to <code>0</code> and <code>1</code> indices.</p>
			<p>Such a mapping can be implemented by going over our collection of documents and storing in a dictionary the index of the documents where that term appears. The implementation is similar to the <code>counter_dict</code> function, except that, instead of incrementing a counter, we are growing the list of documents that match the current term:</p>
			<pre>    # Building an index
    index = {}
    for i, doc in enumerate(docs):
        # We iterate over each term in the document
        for word in doc.split():
            # We build a list containing the indices 
            # where the term appears
            if word not in index:
                index[word] = [i]
            else:
                index[word].append(i)</pre>
			<p>Once we have built our index, doing a query involves a simple dictionary lookup. For example, if we want to return all the documents containing the <code>table</code> term, we can simply query the index and retrieve the corresponding documents:</p>
			<pre>    results = index["table"]
    result_documents = [docs[i] for i in results]</pre>
			<p>Since all it takes to query our collection is dictionary access, the index can handle queries with <em class="italic">O</em>(1) time complexity! Thanks to the inverted index, we are now able to query any number of documents (as long as they fit in memory) in constant time. Needless to say, indexing is a technique widely<a id="_idIndexMarker109"/> used to quickly retrieve data, not only in search engines but also in databases<a id="_idIndexMarker110"/> and any system that requires fast searches.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Building an inverted index<a id="_idIndexMarker111"/> is an expensive operation and requires you to encode every possible query. This is a substantial drawback, but the benefits are great, and it may be worthwhile to pay the price in terms of decreased flexibility.</p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor037"/>Sets</h2>
			<p><strong class="bold">Sets</strong> are unordered collections of elements, with the additional restriction<a id="_idIndexMarker112"/> that the elements must be unique. The main use cases where sets are a good choice are membership tests (testing whether an element is present in the collection) and, unsurprisingly, set operations such as union, difference, and intersection.</p>
			<p>In Python, sets are implemented using a hash-based algorithm, just like dictionaries; therefore, the time complexities for addition, deletion, and testing for membership scale as <em class="italic">O</em>(1) with the size of the collection.</p>
			<p>Sets contain only <em class="italic">unique elements</em>. An immediate use case of sets is the removal of duplicates from a collection, which can be accomplished by simply passing the collection through the <code>set</code> constructor, as follows:</p>
			<pre>    # create a list that contains duplicates
    x = list(range(1000)) + list(range(500))
    # the set *x_unique* will contain only 
    # the unique elements in x
    x_unique = set(x)</pre>
			<p>The time complexity for removing duplicates is <em class="italic">O</em>(N), as it requires reading the input and putting each element in the set.</p>
			<p>Sets offer a number of operations, such as union, intersection, and difference. The union of two sets is a new set containing all the elements of both the sets; the intersection is a new set that contains only the elements in common between the two sets, and the difference is a new set containing the elements of the first set that are not contained in the second set. The time complexities for these operations are shown in the following table. Note that since we have two different input sizes, we will use the letter <code>S</code> to indicate the size of the first set (called <code>s</code>), and <code>T</code> to indicate the size of the second set (called <code>t</code>):</p>
			<p class="figure-caption"><img src="img/B17499_Table_2.6.png" alt="Table 1.6 – The running time of set operations " width="842" height="266"/></p>
			<p class="figure-caption">Table 1.6 – The running time of set operations</p>
			<p>An application of set operations is, for example, Boolean queries. Going back to the inverted index example of the previous subsection, we may want to support queries that include multiple terms. For example, we may want to search for all the documents that contain the words <code>cat</code> and <code>table</code>. This kind of query can be efficiently computed by taking the intersection between the set of documents containing <code>cat</code> and the set of documents containing <code>table</code>.</p>
			<p>In order to efficiently support<a id="_idIndexMarker113"/> those operations, we can change our indexing code so that each term is associated with a set of documents (rather than a list). After applying this change, calculating more advanced queries is a matter of applying the right set operation. In the following code, we show the inverted index based on sets and the query using set operations:</p>
			<pre>    # Building an index using sets
    index = {}
    for i, doc in enumerate(docs):
        # We iterate over each term in the document
        for word in doc.split():
            # We build a set containing the indices 
            # where the term appears
            if word not in index:
                index[word] = {i}
            else:
                index[word].add(i)
    
    # Querying the documents containing both "cat" and
        "table"
    index['cat'].intersection(index['table'])</pre>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor038"/>Heaps</h2>
			<p><strong class="bold">Heaps</strong> are data structures designed to quickly find<a id="_idIndexMarker114"/> and extract the maximum (or minimum) value in a collection. A typical use case for heaps is to process a series of incoming tasks in order of maximum priority.</p>
			<p>You can theoretically use a sorted list using the tools in the <code>bisect</code> module; however, while extracting the maximum value will take <em class="italic">O</em>(1) time (using <code>list.pop</code>), insertion will still take <em class="italic">O</em>(N) time (remember that, even if finding the insertion point takes <em class="italic">O</em>(<em class="italic">log</em>(<em class="italic">N</em>)) time, inserting an element in the middle of a list is still an <em class="italic">O</em>(<em class="italic">N</em>) operation). A heap is a more efficient data structure that allows for the insertion and extraction of maximum values with <em class="italic">O</em>(<em class="italic">log</em>(<em class="italic">N</em>)) time complexity.</p>
			<p>In Python, heaps are built using the procedures contained in the <code>heapq</code> module on an underlying list. For example, if we have a list of 10 elements, we can reorganize it into a heap with the <code>heapq.heapify</code> function:</p>
			<pre>    import heapq
    collection = [10, 3, 3, 4, 5, 6]
    heapq.heapify(collection)</pre>
			<p>To perform the insertion and extraction operations on the heap, we can use the <code>heapq.heappush</code> and <code>heapq.heappop</code> functions. The <code>heapq.heappop</code> function will extract the minimum value in the collection in <em class="italic">O</em>(<em class="italic">log</em>(<em class="italic">N</em>)) time and can be used in the following way:</p>
			<pre>    heapq.heappop(collection)
    # Returns: 3</pre>
			<p>Similarly, you can push the <code>1</code> integer with the <code>heapq.heappush</code> function, as follows:</p>
			<pre>    heapq.heappush(collection, 1)</pre>
			<p>Another easy-to-use option is the <code>queue.PriorityQueue</code> class, which as a bonus, is thread- and process-safe. The <code>PriorityQueue</code> class can be filled up with elements using the <code>PriorityQueue.put</code> method, while <code>PriorityQueue.get</code> can be used to extract the minimum value in the collection:</p>
			<pre>    from queue import PriorityQueue
    queue = PriorityQueue()
    for element in collection:
        queue.put(element)
    queue.get()
    # Returns: 3</pre>
			<p>If the maximum element is required, a simple trick is to multiply each element of the list by <code>-1</code>. In this way, the order of the elements will be inverted. Also, if you want to associate an object (for example, a task to run) with each number (which can represent the priority), you can insert tuples of the <code>(number, object)</code> form; the comparison operator for the tuple<a id="_idIndexMarker115"/> will be ordered with respect to its first element, as shown in the following example:</p>
			<pre>    queue = PriorityQueue()
    queue.put((3, "priority 3"))
    queue.put((2, "priority 2"))
    queue.put((1, "priority 1"))
    queue.get()
    # Returns: (1, "priority 1")</pre>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor039"/>Tries</h2>
			<p>A perhaps less popular data structure, but very useful in practice, is the <strong class="bold">trie</strong> (sometimes called <strong class="bold">prefix tree</strong>). Tries are extremely fast at<a id="_idIndexMarker116"/> matching a list of strings<a id="_idIndexMarker117"/> against a prefix. This is especially useful when implementing features such as <em class="italic">search as you type</em> and <em class="italic">autocompletion</em>, where the list of available completions is very large and short response times are required.</p>
			<p>Unfortunately, Python does not include a trie implementation in its standard library; however, many efficient implementations are readily available through <em class="italic">PyPI</em>. The one we will use in this subsection is <code>patricia-trie</code>, a single-file, pure Python implementation of trie. As an example, we will use <code>patricia-trie</code> to perform the task of finding the longest prefix in a set of strings (just like autocompletion).</p>
			<p>Here, we can demonstrate how fast a trie is able to search through a list of strings. In order to generate a large amount of unique random strings, we can define a function, <code>random_string</code>. The <code>random_string</code> function will return a string composed of random uppercase characters and, while there is a chance to get duplicates, we can greatly reduce the probability of duplicates to the point of being negligible if we make the string<a id="_idIndexMarker118"/> long enough. The implementation of the <code>random_string</code> function is shown as follows:</p>
			<pre>    from random import choice
    from string import ascii_uppercase
    def random_string(length):
     """Produce a random string made of *length* uppercase \
        ascii characters"""
     return ''.join(choice(ascii_uppercase) for i in \
       range(length))</pre>
			<p>We can build a list of random strings and time how fast it searches for a prefix (in our case, the <code>"AA"</code> string) using the <code>str.startswith</code> function:</p>
			<pre>    strings = [random_string(32) for i in range(10000)]
    matches = [s for s in strings if s.startswith('AA')]</pre>
			<p>List comprehension and <code>str.startwith</code> are already very optimized operations and, on this small dataset, the search takes only a millisecond or so:</p>
			<pre>    %timeit [s for s in strings if s.startswith('AA')]
    1000 loops, best of 3: 1.76 ms per loop</pre>
			<p>Now, let's try using a trie for the same<a id="_idIndexMarker119"/> operation. In this example, we will use the <code>patricia-trie</code> library that is installable through <code>pip</code>. The <code>patricia.trie</code> class implements a variant of the trie data structure with an interface similar to a dictionary. We can initialize our trie by creating a dictionary from our list of strings, as follows:</p>
			<pre>    from patricia import trie
    strings_dict = {s:0 for s in strings} 
    # A dictionary where all values are 0
    strings_trie = trie(**strings_dict)</pre>
			<p>To query <code>patricia-trie</code> for a matching prefix, we can use the <code>trie.iter</code> method, which returns an iterator over the matching strings:</p>
			<pre>    matches = list(strings_trie.iter('AA'))</pre>
			<p>Now that we know how to initialize and query a trie, we can time the operation:</p>
			<pre>    %timeit list(strings_trie.iter('AA'))
    10000 loops, best of 3: 60.1 µs per loop</pre>
			<p>The timing for this input size is <strong class="bold">60.1 µs</strong>, which is about 30 times faster (1.76 ms = 1760 µs) than linear search! This impressive speedup is because of the better computational complexity of the trie prefix search. Querying a trie has an <em class="italic">O</em>(<em class="italic">S</em>) time complexity, where <em class="italic">S</em> is the length of the longest string in the collection, while the time complexity of a simple linear scan is <em class="italic">O</em>(<em class="italic">N</em>), where <em class="italic">N</em> is the size of the collection. </p>
			<p>Note that if we want to return all the prefixes that match, the running time will be proportional to the number of results that match the prefix. Therefore, when designing timing benchmarks, care must be taken to ensure that we are always returning the same number of results.</p>
			<p>The scaling properties of a trie versus a linear scan for datasets of different sizes that contains 10 prefix matches are shown in the following table:</p>
			<div><div><img src="img/B17499_Table_2.7.jpg" alt="Table 1.7 – The running time of a trie versus a linear scan " width="1635" height="220"/>
				</div>
			</div>
			<p class="figure-caption">Table 1.7 – The running time of a trie versus a linear scan</p>
			<p>An interesting fact is that the implementation of <code>patricia-trie</code> is actually a single Python file; this clearly shows how simple and powerful a clever algorithm can be. For extra features and performance, other C-optimized trie libraries are also available, such as <code>datrie</code> and <code>marisa-trie</code>.</p>
			<p>At this point, we have considered most of the important data structures that are native to Python. Appropriate use of these data structures will speed up your application by a significant degree. In addition to data structures, there are other computing techniques and concepts that we could utilize in order to make our programs even faster<a id="_idTextAnchor040"/>. In the next section, we will take a look<a id="_idIndexMarker120"/> at caching and memoization, which are common practices when you expect the same computations to be repeated multiple times.</p>
			<h1 id="_idParaDest-42"><a id="_idTextAnchor041"/>Improved efficiency with caching and memoization</h1>
			<p><strong class="bold">Caching</strong> is a great technique used to improve<a id="_idIndexMarker121"/> the performance of a wide range of applications. The idea behind caching is to store expensive<a id="_idIndexMarker122"/> results in a temporary location, called a <strong class="bold">cache</strong>, that can be located in memory, on disk, or in a remote location.</p>
			<p>Web applications make extensive use of caching. In a web application, it is often the case that multiple users request a certain page at the same time. In this case, instead of recomputing the page for each user, the web application can compute it once and serve the user the already rendered page. Ideally, caching also needs a mechanism for invalidation so that if the page needs to be updated, we can recompute it before serving it again. Intelligent caching allows web applications to handle the increasing number of users with fewer resources. Caching can also be done preemptively, such as the later sections of a video getting buffered when watching a video online.</p>
			<p>Caching is also used to improve the performance<a id="_idIndexMarker123"/> of certain algorithms. A great example is computing the Fibonacci sequence. Since computing the next number in the Fibonacci sequence requires the previous number in the sequence, you can store and reuse previous results, dramatically improving the running time. Storing and reusing<a id="_idIndexMarker124"/> the results of the previous function calls in an application is usually termed <strong class="bold">memoization</strong> and is one of the forms of caching. Several other algorithms can take advantage of memoization<a id="_idIndexMarker125"/> to gain impressive performance improvements, and this programming technique is commonly referred to as <strong class="bold">dynamic programming</strong>, where you aim to solve a large problem<a id="_idIndexMarker126"/> by breaking it into smaller ones.</p>
			<p>The benefits of caching, however, do not come for free. What we are actually doing is sacrificing some space to improve the speed of the application. Additionally, if the cache is stored in a location on the network, we may incur transfer costs and the general time needed for communication. You should evaluate when it is convenient to use a cache and how much space you are willing to trade for an increase in speed.</p>
			<p>Given the usefulness of this technique, the Python standard library includes a simple in-memory cache out of the box in the <code>functools</code> module. The <code>functools.lru_cache</code> decorator can be used to easily cache the results of a function.</p>
			<p>In the following example, we create a function, <code>sum2</code>, that prints a statement and returns the sum of two numbers. By running the function twice, you can see that the first time the <code>sum2</code> function is executed, the <code>"Calculating ..."</code> string is produced, while the second time, the result is returned without running the function:</p>
			<pre>    from functools import lru_cache
    @lru_cache()
    def sum2(a, b):
        print("Calculating {} + {}".format(a, b))
        return a + b
    print(sum2(1, 2))
    # Output: 
    # Calculating 1 + 2
    # 3
    print(sum2(1, 2))
    # Output: 
    # 3</pre>
			<p>The <code>lru_cache</code> decorator also provides<a id="_idIndexMarker127"/> other basic features. To restrict the size of the cache, you can set the number of elements that we intend to maintain through the <code>max_size</code> argument. If we want our cache size to be unbounded, we can specify a value of <code>None</code>. An example of <code>max_size</code> usage is shown here:</p>
			<pre>    @lru_cache(max_size=16)
    def sum2(a, b):
        ...</pre>
			<p>In this way, as we execute <code>sum2</code> with different arguments, the cache will reach a maximum size of <code>16</code> and, as we keep requesting more calculations, new values will replace older values in the cache. The <code>lru</code> prefix originates from this strategy, which means <em class="italic">least recently used</em>.</p>
			<p>The <code>lru_cache</code> decorator also adds extra functionalities<a id="_idIndexMarker128"/> to the decorated function. For example, it is possible<a id="_idIndexMarker129"/> to examine the cache performance using the <code>cache_info</code> method, and it is possible to reset the cache using the <code>cache_clear</code> method, as follows:</p>
			<pre>    sum2.cache_info()
    # Output: CacheInfo(hits=0, misses=1, maxsize=128,
      currsize=1)
    sum2.cache_clear()</pre>
			<p>As an example, we can see how a problem, such as computing the Fibonacci series, may benefit from caching. We can define a <code>fibonacci</code> function and time its execution:</p>
			<pre>    def fibonacci(n):
        if n &lt; 1:
            return 1
        else:
            return fibonacci(n - 1) + fibonacci(n - 2)
    # Non-memoized version
    %timeit fibonacci(20)
    100 loops, best of 3: 5.57 ms per loop</pre>
			<p>The execution takes 5.57 milliseconds, which is very long. The scaling of the function written in this way has poor performance; the previously computed Fibonacci sequences are not reused, causing this algorithm to have an exponential scaling of roughly <em class="italic">O</em>(<em class="italic">2N</em>).</p>
			<p>Caching can improve<a id="_idIndexMarker130"/> this algorithm by storing and reusing the already-computed Fibonacci numbers. To implement the cached version, it is sufficient to apply the <code>lru_cache</code> decorator to the original <code>fibonacci</code> function. Also, to design a proper<a id="_idIndexMarker131"/> benchmark, we need to ensure that a new cache is instantiated<a id="_idIndexMarker132"/> for every run; to do this, we can use the <code>timeit.repeat</code> function, as shown in the following example:</p>
			<pre>    import timeit
    setup_code = '''
    from functools import lru_cache
    from __main__ import fibonacci
    fibonacci_memoized = lru_cache(maxsize=None)(fibonacci)'''
    results = timeit.repeat('fibonacci_memoized(20)',
                            setup=setup_code,
                            repeat=1000,
                            number=1)
    print("Fibonacci took {:.2f} us".format(min(results)))
    # Output: Fibonacci took 0.01 us</pre>
			<p>Even though we changed the algorithm by adding a simple decorator, the running time now is much less than a microsecond. The reason is, thanks to caching, we now have a linear time algorithm instead of an exponential one.</p>
			<p>The <code>lru_cache</code> decorator can be used to implement<a id="_idIndexMarker133"/> simple in-memory caching<a id="_idIndexMarker134"/> in your application. For more advanced use cases, third-party<a id="_idIndexMarker135"/> modules can be used for more powerful implementation and on-disk caching.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>Joblib</h2>
			<p>A simple library that, among other things, provides a simple on-disk cache is <code>joblib</code>. The package<a id="_idIndexMarker136"/> can be used in a similar way as <code>lru_cache</code>, except that the results will be stored on disk and will persist between runs.</p>
			<p>The <code>joblib</code> module can be installed from PyPI using the <code>pip install joblib</code> command.</p>
			<p>The <code>joblib</code> module provides the <code>Memory</code> class, which can be used to memoize functions using the <code>Memory.cache</code> decorator:</p>
			<pre>    from joblib import Memory
    memory = Memory(cachedir='/path/to/cachedir')
    @memory.cache
    def sum2(a, b):
        return a + b</pre>
			<p>The function will behave similarly to <code>lru_cache</code>, with the exception that the results will be stored on disk in the directory specified by the <code>cachedir</code> argument during <code>Memory</code> initialization. Additionally, the cached results will persist over subsequent runs!</p>
			<p>The <code>Memory.cache</code> method also allows limiting recomputation to only when certain arguments change, and the resulting decorated function supports basic functionalities to clear and analyze the cache. </p>
			<p>Perhaps the best <code>joblib</code> feature is, thanks to intelligent hashing algorithms, providing efficient memoization<a id="_idIndexMarker137"/> of functions that operate on <code>numpy</code> arrays, which is particularly useful in scientific and engineering applications.</p>
			<p>At this point, we have seen that by using caching and memoization, our program can reuse computations in an efficient way. Another common strategy to improve running time is to utilize specifically designed techniques as appropriate. In the next section, we will see how we can take advantage of comprehensions and generators when working with Python loops.</p>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor043"/>Efficient iteration with comprehensions and generators</h1>
			<p>In this section, we will explore a few simple strategies to speed up Python loops using <code>for</code> loops, as they are designed<a id="_idIndexMarker138"/> to avoid many unnecessary computational overheads<a id="_idIndexMarker139"/> during their construction. Another reason to use this construct is readability; even if the speedup over a standard loop is modest, the comprehension and generator syntax is more compact and (most of the time) more intuitive.</p>
			<p>In the following example, we can see that both the list comprehension and generator expressions are faster than an explicit loop when combined with the <code>sum</code> function:</p>
			<pre>    def loop(): 
        res = [] 
        for i in range(100000): 
            res.append(i * i) 
        return sum(res) 
    def comprehension(): 
        return sum([i * i for i in range(100000)]) 
    def generator(): 
        return sum(i * i for i in range(100000)) 
    %timeit loop() 
    100 loops, best of 3: 16.1 ms per loop 
    %timeit comprehension() 
    100 loops, best of 3: 10.1 ms per loop 
    %timeit generator() 
    100 loops, best of 3: 12.4 ms per loop </pre>
			<p>Just like lists, it is possible to use <code>dict</code> comprehension to build dictionaries slightly more efficiently<a id="_idIndexMarker140"/> and compactly, as shown in the<a id="_idIndexMarker141"/> following code:</p>
			<pre>    def loop(): 
        res = {} 
        for i in range(100000): 
            res[i] = i
        return res
    def comprehension(): 
        return {i: i for i in range(100000)}
    %timeit loop() 
    100 loops, best of 3: 13.2 ms per loop 
    %timeit comprehension() 
    100 loops, best of 3: 12.8 ms per loop</pre>
			<p>Efficient looping (especially in terms of memory) can be implemented using iterators and functions such as <code>filter</code> and <code>map</code>. As an example, consider the problem of applying a series of operations to a list using list comprehension and then taking the maximum value:</p>
			<pre>    def map_comprehension(numbers):
        a = [n * 2 for n in numbers]
        b = [n ** 2 for n in a]
        c = [n ** 0.33 for n in b]
        return max(c)</pre>
			<p>The problem with this approach is that for every list comprehension, we are allocating a new list, increasing memory<a id="_idIndexMarker142"/> usage. Instead of using list comprehension, we can employ generators. Generators<a id="_idIndexMarker143"/> are objects that, when iterated upon, compute a value on the fly and return the result.</p>
			<p>For example, the <code>map</code> function takes two arguments – a function and an iterator – and returns a generator that applies the function to every element of the collection. The important point is that the operation happens only <em class="italic">while we are iterating</em>, and not when <code>map</code> is invoked!</p>
			<p>We can rewrite the previous function using <code>map</code> and by creating intermediate generators, rather than lists, thus saving memory by computing the values on the fly:</p>
			<pre>    def map_normal(numbers):
        a = map(lambda n: n * 2, numbers)
        b = map(lambda n: n ** 2, a)
        c = map(lambda n: n ** 0.33, b)
        return max(c)</pre>
			<p>We can profile the memory of the two solutions using the <code>memory_profiler</code> extension from an IPython session. The extension provides a small utility, <code>%memit</code>, that will help us evaluate the memory usage of a Python statement in a way similar to <code>%timeit</code>, as illustrated in the following details:</p>
			<pre>    %load_ext memory_profiler
    numbers = range(1000000)
    %memit map_comprehension(numbers)
    peak memory: 166.33 MiB, increment: 102.54 MiB
    %memit map_normal(numbers)
    peak memory: 71.04 MiB, increment: 0.00 MiB</pre>
			<p>As you can see, the memory used by the first <a id="_idIndexMarker144"/>version is <code>102.54 MiB</code>, while the second version consumes <code>0.00 MiB</code>! For those who are interested, more functions that return generators<a id="_idIndexMarker145"/> can be found in the <code>itertools</code> module, which provides a set of utilities designed to handle common iteration patterns.</p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor044"/>Summary</h1>
			<p>Algorithmic optimization can improve how your application scales as we process increasingly large data. In this chapter, we demonstrated use cases and running times of the most common data structures available in Python, such as lists, deques, dictionaries, heaps, and tries. We also covered caching, a technique that can be used to trade some space, in memory or on disk, in exchange for the increased responsiveness of an application. We also demonstrated how to get modest speed gains by replacing <code>for</code> loops with fast constructs, such as list comprehensions and generator expressions.</p>
			<p>Overall, we have seen that by utilizing a specifically designed data structure or technique that is appropriate in certain situations, the efficiency of our program can be greatly improved. The topics covered in this chapter offer us the ability to do just that across a wide range of use cases.</p>
			<p>In the subsequent chapters, we will learn how to improve performance further using numerical libraries such as <code>numpy</code> and how to write extension modules in a lower-level language with the help of <em class="italic">Cython</em>.</p>
			<h1 id="_idParaDest-46"><a id="_idTextAnchor045"/>Questions</h1>
			<ol>
				<li>Identify the best/most appropriate from the data structures covered in this chapter concerning the following use cases:<ol><li>Mapping items to another <em class="italic">set</em> of items (<em class="italic">set</em> being used in the most general sense)</li><li>Accessing, modifying, and appending elements</li><li>Maintaining a collection of unique elements</li><li>Keeping track of the minimum/maximum of a <em class="italic">set</em> (in the most general sense)</li><li>Appending and removing elements at the endpoints of a <em class="italic">sequence</em> (in the most general sense).</li><li>Fast searching according to some similarity criterion (for example, being used by autocompletion engines).</li></ol></li>
				<li>What is the difference between caching and memoization?</li>
				<li>Why are comprehensions and generators (in most situations) more preferred than explicit <code>for</code> loops?</li>
				<li>Consider the problem of representing a pairwise association between a set of letters and a set of numbers (for example, a  2, b  1, c  3, and so on), where we need to look at what number a given letter is associated with in our application.<ol><li>Is a list an appropriate data structure for this task, and if not, what is?</li><li>What if each number represented the number of instances of a given letter in a text document? What would the best data structure for this task be?</li></ol></li>
			</ol>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor046"/>Further reading</h1>
			<ul>
				<li>Lazy evaluation in Python: <a href="https://towardsdatascience.com/what-is-lazy-evaluation-in-python-9efb1d3bfed0">https://towardsdatascience.com/what-is-lazy-evaluation-in-python-9efb1d3bfed0</a></li>
				<li>The <code>yield</code> statement in Python: <a href="https://realpython.com/introduction-to-python-generators/">https://realpython.com/introduction-to-python-generators/</a></li>
			</ul>
		</div>
	</div>
</div>
</body></html>