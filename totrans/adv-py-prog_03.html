<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer023">
			<h1 id="_idParaDest-33"><em class="italic"><a id="_idTextAnchor032"/>Chapter 2</em>: Pure Python Optimizations</h1>
			<p>As mentioned in the previous chapter, one of the most effective ways of improving the performance of applications is through the use of better algorithms and data structures. The Python standard library provides a large variety of ready-to-use algorithms and data structures that can be directly incorporated into your applications. With the tools learned from this chapter, you will be able to use the right algorithm for the task and achieve massive speed gains.</p>
			<p>Even though many algorithms have been around for quite a while, they are especially relevant in today's world as we continuously produce, consume, and analyze ever-increasing amounts of data. Buying a larger server or micro-optimizing can work for some time, but achieving better scaling through algorithmic improvement can solve the problem once and for all.</p>
			<p>In this chapter, we will learn how to achieve better scaling using standard algorithms and data structures. More advanced use cases where we take advantage of third-party libraries will also be covered. We will also learn about tools to implement caching, a technique used to achieve faster response times by sacrificing some space in memory or on disk.</p>
			<p>The list of topics to be covered in this chapter is as follows:</p>
			<ul>
				<li>Using the right algorithms and data structures</li>
				<li>Improved efficiency with caching and memoization</li>
				<li>Efficient iteration with comprehensions and generators</li>
			</ul>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor033"/>Technical requirements</h1>
			<p>You will find the code files for this chapter here: <a href="https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter02">https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter02</a>.</p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor034"/>Using the right algorithms and data structures</h1>
			<p>Algorithmic improvements are especially effective in increasing performance because they typically allow the application to scale better with increasingly large inputs.</p>
			<p>Algorithm running times<a id="_idIndexMarker087"/> can be classified according to their computational complexity, a characterization of the resources required to perform a task. </p>
			<p>Such classification is expressed through the <em class="italic">Big O notation</em>, an upper bound on the operations required to execute the task, which usually depends on the input size. Specifically, Big O notation<a id="_idIndexMarker088"/> describes how the runtime or memory requirement of an algorithm grows in terms of the input size. For this reason, a lower Big O denotes a more efficient algorithm, which is what we aim for.</p>
			<p>For example, incrementing each element of a list can be implemented using a <strong class="source-inline">for</strong> loop, as follows:</p>
			<p class="source-code">    input = list(range(10))</p>
			<p class="source-code">    for i, _ in enumerate(input):</p>
			<p class="source-code">        input[i] += 1 </p>
			<p>If the operation does not depend on the size of the input (for example, accessing the first element of a list), the algorithm is said to take constant, or <em class="italic">O</em>(1), time. This means that, no matter how much data we have, the time to run the algorithm will always be the same.</p>
			<p>In this simple algorithm, the <strong class="source-inline">input[i] += 1</strong> operation will be repeated <strong class="source-inline">10</strong> times, which is the size of the input. If we double the size of the input array, the number of operations will increase proportionally. Since the number of operations is proportional to the input size, this algorithm is said to take <em class="italic">O</em>(<em class="italic">N</em>) time, where <em class="italic">N</em> is the size of the input array.</p>
			<p>In some instances, the running time<a id="_idIndexMarker089"/> may depend on the structure of the input (for example, if the collection is sorted or contains many duplicates). In these cases, an algorithm may have different best-case, average-case, and worst-case running times. Unless stated otherwise, the running times presented in this chapter are considered to be average running times.</p>
			<p>In this section, we will examine the running times of algorithms and data structures that are implemented in the Python standard<a id="_idIndexMarker090"/> library and understand how improving running times results in massive gains and<a id="_idIndexMarker091"/> allows us to solve large-scale problems with elegance.</p>
			<p>You can find the code used to run the benchmarks in this chapter in the <strong class="source-inline">Algorithms.ipynb</strong> notebook, which can be opened using <em class="italic">Jupyter</em>.</p>
			<p>First, we will examine <strong class="bold">lists</strong> and <strong class="bold">deques</strong>.</p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/>Lists and deques</h2>
			<p>Python lists are ordered collections<a id="_idIndexMarker092"/> of elements and, in Python, are implemented as resizable arrays. An <strong class="bold">array</strong> is a basic data structure<a id="_idIndexMarker093"/> that consists of a series of contiguous memory locations, and each location contains a reference to a Python object.</p>
			<p>Lists shine in accessing, modifying, and appending elements. Accessing or modifying an element involves fetching the object reference from the appropriate position of the underlying array and has <em class="italic">O</em>(1) complexity. Appending an element is also very fast. When an empty list is created, an array of fixed size is allocated and, as we insert elements, the slots in the array are gradually filled up. Once all the slots are occupied, the list needs to increase the size of its underlying array, thus triggering a memory reallocation that can take <em class="italic">O</em>(<em class="italic">N</em>) time. Nevertheless, those memory allocations are infrequent, and the time complexity for the append operation is referred to as amortized <em class="italic">O</em>(1) time.</p>
			<p>The list operations that may have efficiency problems are those that add or remove elements at the beginning (or somewhere in the middle) of the list. When an item is inserted or removed from the beginning of a list, all the subsequent elements of the array need to be shifted by a position, thus taking <em class="italic">O</em>(<em class="italic">N</em>) time.</p>
			<p>In the following table, the timings for different operations on a list of 10,000 size are shown; you can see how insertion and removal performances vary quite dramatically if performed at the beginning or the end of the list:</p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/B17499_Table_2.1.jpg" alt="Table 1.1 – The speed of different list operations " width="1650" height="332"/>
				</div>
			</div>
			<p class="figure-caption">Table 1.1 – The speed of different list operations</p>
			<p>In some cases, it is necessary to efficiently perform the insertion or removal of elements both at the beginning and the end of the collection. Python provides a data structure with those properties in the <strong class="source-inline">collections.deque</strong> class. The word <em class="italic">deque</em> stands for <strong class="bold">double-ended queue</strong> because this data structure<a id="_idIndexMarker094"/> is designed to efficiently<a id="_idIndexMarker095"/> put and remove elements at the beginning and the end<a id="_idIndexMarker096"/> of the collection, as it is in the case of queues. In Python, deques are implemented as doubly linked lists.</p>
			<p>Deques, in addition to <strong class="source-inline">pop</strong> and <strong class="source-inline">append</strong>, expose the <strong class="source-inline">popleft</strong> and <strong class="source-inline">appendleft</strong> methods that have <em class="italic">O</em>(1) running time:</p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B17499_Table_2.2.jpg" alt="Table 1.2 – The speed of different deque operations " width="1650" height="394"/>
				</div>
			</div>
			<p class="figure-caption">Table 1.2 – The speed of different deque operations</p>
			<p>Despite these advantages, deques should not be used to replace regular lists in most cases. The efficiency gained by the <strong class="source-inline">appendleft</strong> and <strong class="source-inline">popleft</strong> operations comes at a cost – accessing an element in the middle of a deque is an <em class="italic">O</em>(N) operation, as shown in the following table:</p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B17499_Table_2.3.jpg" alt="Table 1.3 – The inefficiency of deques in accessing the middle element " width="1650" height="279"/>
				</div>
			</div>
			<p class="figure-caption">Table 1.3 – The inefficiency of deques in accessing the middle element</p>
			<p>Searching for an item in a list is generally an <em class="italic">O</em>(<em class="italic">N</em>) operation and is performed using the <strong class="source-inline">list.index</strong> method. A simple way to speed up searches in lists is to keep the array sorted and perform a binary search using the <strong class="source-inline">bisect</strong> module.</p>
			<p>The <strong class="source-inline">bisect</strong> module allows<a id="_idIndexMarker097"/> fast searches on sorted arrays. The <strong class="source-inline">bisect.bisect</strong> function can be used on a sorted<a id="_idIndexMarker098"/> list to find the index to place an element while maintaining the array in sorted order. In the following example, we can see that if we want to insert the <strong class="source-inline">3</strong> element in the array while keeping <strong class="source-inline">collection</strong> in sorted order, we should put <strong class="source-inline">3</strong> in the third position (which corresponds to index <strong class="source-inline">2</strong>):</p>
			<p class="source-code">    insert bisect</p>
			<p class="source-code">    collection = [1, 2, 4, 5, 6]</p>
			<p class="source-code">    bisect.bisect(collection, 3)</p>
			<p class="source-code">    # Result: 2</p>
			<p>This function uses the binary search algorithm that has <em class="italic">O</em>(<em class="italic">log</em>(<em class="italic">N</em>)) running time. Such a running time is exceptionally fast and, basically, means that your running time will increase by a constant amount every time you <em class="italic">double</em> your input size. This means that if, for example, your program takes <strong class="source-inline">1</strong> second to run on an input of <strong class="source-inline">1000</strong> size, it will take <strong class="source-inline">2</strong> seconds to process an input of <strong class="source-inline">2000</strong> size, <strong class="source-inline">3</strong> seconds to process an input of <strong class="source-inline">4000</strong> size, and so on. If you had <strong class="source-inline">100</strong> seconds, you could theoretically process an input of <strong class="source-inline">10^33</strong> size, which is larger than the number of atoms in your body!</p>
			<p>If the value we are trying to insert is already present in the list, the <strong class="source-inline">bisect.bisect</strong> function will return the location <em class="italic">after</em> the already present value. Therefore, we can use the <strong class="source-inline">bisect.bisect_left</strong> variant, which returns the correct index in the following way (taken from<a id="_idIndexMarker099"/> the module documentation at <a href="https://docs.python.org/3.5/library/bisect.html">https://docs.python.org/3.5/library/bisect.html</a>):</p>
			<p class="source-code">    def index_bisect(a, x):</p>
			<p class="source-code">      'Locate the leftmost value exactly equal to x'</p>
			<p class="source-code">      i = bisect.bisect_left(a, x)</p>
			<p class="source-code">      if i != len(a) and a[i] == x:</p>
			<p class="source-code">      return i</p>
			<p class="source-code">      raise ValueError</p>
			<p>In the following table, you can<a id="_idIndexMarker100"/> see how the running time of the <strong class="source-inline">bisect</strong> solution is barely affected<a id="_idIndexMarker101"/> by these input sizes, making it a suitable solution when searching through very large collections:</p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B17499_Table_2.4.jpg" alt="" width="1650" height="200"/>
				</div>
			</div>
			<p class="figure-caption">Table 1.4 – The efficiency of the bisect function</p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor036"/>Dictionaries</h2>
			<p><strong class="bold">Dictionaries</strong> are extremely versatile and extensively<a id="_idIndexMarker102"/> used in the Python language, for example, in package-, module-, and class-level namespaces, as well as object and class annotations. Dictionaries are implemented as hash maps and are very good at element insertion, deletion, and access; all these operations have an average <em class="italic">O</em>(1) time complexity.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">In Python versions up to 3.5, dictionaries are <em class="italic">unordered collections</em>. Since Python 3.6, dictionaries are capable of maintaining their elements by order of insertion.</p>
			<h3 id="_idParaDest-38">Hash map</h3>
			<p>A <strong class="bold">hash map</strong> is a data structure that associates a<a id="_idIndexMarker103"/> set of key-value pairs. The principle behind hash maps is to assign a specific index to each key so that its associated value can be stored in an array. The index can be obtained through the use of a <strong class="source-inline">hash</strong> function; Python implements <strong class="source-inline">hash</strong> functions for several data types. As a demonstration, the generic function to obtain hash codes is <strong class="source-inline">hash</strong>. In the following example, we show you how to obtain the hash code when given the <strong class="source-inline">"hello"</strong> string:</p>
			<p class="source-code">    hash("hello")</p>
			<p class="source-code">    # Result: -1182655621190490452</p>
			<p class="source-code">    # To restrict the number to be a certain range you can</p>
			<p class="source-code">      use</p>
			<p class="source-code">    # the modulo (%) operator</p>
			<p class="source-code">      hash("hello") % 10</p>
			<p class="source-code">    # Result: 8</p>
			<p>Hash maps can be tricky to implement because they need to handle collisions that happen when two different objects have the same hash code. However, all the complexity is elegantly hidden behind the implementation and the default collision resolution works well in most real-world scenarios.</p>
			<p>The access to, insertion, and removal<a id="_idIndexMarker104"/> of an item in a dictionary scales as <em class="italic">O</em>(1) with the size of the dictionary. However, note that the computation of the <strong class="source-inline">hash</strong> function still needs to happen and, for strings, the computation scales with the length of the string. As string keys are usually relatively small, this doesn't constitute a problem in practice.</p>
			<p>A dictionary can be used to efficiently count unique elements in a list. In this example, we define the <strong class="source-inline">counter_dict</strong> function that takes a list and returns a dictionary containing the number of occurrences of each value in the list:</p>
			<p class="source-code">    def counter_dict(items): </p>
			<p class="source-code">        counter = {} </p>
			<p class="source-code">        for item in items: </p>
			<p class="source-code">            if item not in counter: </p>
			<p class="source-code">                counter[item] = 1 </p>
			<p class="source-code">            else: </p>
			<p class="source-code">                counter[item] += 1 </p>
			<p class="source-code">        return counter</p>
			<p>The code can be somewhat simplified using <strong class="source-inline">collections.defaultdict</strong>, which can be used to produce dictionaries where each new key is automatically assigned a default value. In the following code, the <strong class="source-inline">defaultdict(int)</strong> call produces a dictionary where every<a id="_idIndexMarker105"/> new element is automatically assigned a zero value and can be used to streamline the counting:</p>
			<p class="source-code">    from collections import defaultdict</p>
			<p class="source-code">    def counter_defaultdict(items):</p>
			<p class="source-code">        counter = defaultdict(int)</p>
			<p class="source-code">        for item in items:</p>
			<p class="source-code">            counter[item] += 1</p>
			<p class="source-code">        return counter</p>
			<p>The <strong class="source-inline">collections</strong> module also includes a <strong class="source-inline">Counter</strong> class that can be used for the same purpose with a single line of code:</p>
			<p class="source-code">    from collections import Counter</p>
			<p class="source-code">    counter = Counter(items)</p>
			<p>Speed-wise, all these ways of counting have the same time complexity, but the <strong class="source-inline">Counter</strong> implementation is the most efficient, as shown in the following table:</p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B17499_Table_2.5.jpg" alt="Table 1.5 – The different methods of computing counters " width="1650" height="255"/>
				</div>
			</div>
			<p class="figure-caption">Table 1.5 – The different methods of computing counters</p>
			<h4>Building an in-memory search index using a hash map</h4>
			<p>Dictionaries can be used to quickly search for a word in a list of documents, similar to a search engine. In this subsection, we will learn how to build<a id="_idIndexMarker106"/> an inverted index based on a dictionary of lists. Let's say we have a collection<a id="_idIndexMarker107"/> of four documents:</p>
			<p class="source-code">    docs = ["the cat is under the table",</p>
			<p class="source-code">            "the dog is under the table",</p>
			<p class="source-code">            "cats and dogs smell roses",</p>
			<p class="source-code">            "Carla eats an apple"]</p>
			<p>A simple way to retrieve all the documents that match a query is to scan each document and test for the presence of a word. For example, if we want to look up the documents where the word <strong class="source-inline">table</strong> appears, we can employ the following filtering operation:</p>
			<p class="source-code">    matches = [doc for doc in docs if "table" in doc]</p>
			<p>This approach is simple and works well when we have one-off queries; however, if we need to query the collection very often, it can be beneficial to optimize querying time. Since the per-query cost of the linear scan is <em class="italic">O</em>(<em class="italic">N</em>), you can imagine that better scaling will allow us to handle much larger document collections.</p>
			<p>A better strategy is to spend some time preprocessing the documents so that they are easier to find<a id="_idIndexMarker108"/> at query time. We can build a structure, called the <strong class="bold">inverted index</strong>, that associates each word in our collection with the list of documents where that word is present. In our earlier example, the word <strong class="source-inline">"table"</strong> will be associated with the <strong class="source-inline">"the cat is under the table"</strong> and <strong class="source-inline">"the dog is under the table"</strong> documents; they correspond to <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> indices.</p>
			<p>Such a mapping can be implemented by going over our collection of documents and storing in a dictionary the index of the documents where that term appears. The implementation is similar to the <strong class="source-inline">counter_dict</strong> function, except that, instead of incrementing a counter, we are growing the list of documents that match the current term:</p>
			<p class="source-code">    # Building an index</p>
			<p class="source-code">    index = {}</p>
			<p class="source-code">    for i, doc in enumerate(docs):</p>
			<p class="source-code">        # We iterate over each term in the document</p>
			<p class="source-code">        for word in doc.split():</p>
			<p class="source-code">            # We build a list containing the indices </p>
			<p class="source-code">            # where the term appears</p>
			<p class="source-code">            if word not in index:</p>
			<p class="source-code">                index[word] = [i]</p>
			<p class="source-code">            else:</p>
			<p class="source-code">                index[word].append(i)</p>
			<p>Once we have built our index, doing a query involves a simple dictionary lookup. For example, if we want to return all the documents containing the <strong class="source-inline">table</strong> term, we can simply query the index and retrieve the corresponding documents:</p>
			<p class="source-code">    results = index["table"]</p>
			<p class="source-code">    result_documents = [docs[i] for i in results]</p>
			<p>Since all it takes to query our collection is dictionary access, the index can handle queries with <em class="italic">O</em>(1) time complexity! Thanks to the inverted index, we are now able to query any number of documents (as long as they fit in memory) in constant time. Needless to say, indexing is a technique widely<a id="_idIndexMarker109"/> used to quickly retrieve data, not only in search engines but also in databases<a id="_idIndexMarker110"/> and any system that requires fast searches.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Building an inverted index<a id="_idIndexMarker111"/> is an expensive operation and requires you to encode every possible query. This is a substantial drawback, but the benefits are great, and it may be worthwhile to pay the price in terms of decreased flexibility.</p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor037"/>Sets</h2>
			<p><strong class="bold">Sets</strong> are unordered collections of elements, with the additional restriction<a id="_idIndexMarker112"/> that the elements must be unique. The main use cases where sets are a good choice are membership tests (testing whether an element is present in the collection) and, unsurprisingly, set operations such as union, difference, and intersection.</p>
			<p>In Python, sets are implemented using a hash-based algorithm, just like dictionaries; therefore, the time complexities for addition, deletion, and testing for membership scale as <em class="italic">O</em>(1) with the size of the collection.</p>
			<p>Sets contain only <em class="italic">unique elements</em>. An immediate use case of sets is the removal of duplicates from a collection, which can be accomplished by simply passing the collection through the <strong class="source-inline">set</strong> constructor, as follows:</p>
			<p class="source-code">    # create a list that contains duplicates</p>
			<p class="source-code">    x = list(range(1000)) + list(range(500))</p>
			<p class="source-code">    # the set *x_unique* will contain only </p>
			<p class="source-code">    # the unique elements in x</p>
			<p class="source-code">    x_unique = set(x)</p>
			<p>The time complexity for removing duplicates is <em class="italic">O</em>(N), as it requires reading the input and putting each element in the set.</p>
			<p>Sets offer a number of operations, such as union, intersection, and difference. The union of two sets is a new set containing all the elements of both the sets; the intersection is a new set that contains only the elements in common between the two sets, and the difference is a new set containing the elements of the first set that are not contained in the second set. The time complexities for these operations are shown in the following table. Note that since we have two different input sizes, we will use the letter <strong class="source-inline">S</strong> to indicate the size of the first set (called <strong class="source-inline">s</strong>), and <strong class="source-inline">T</strong> to indicate the size of the second set (called <strong class="source-inline">t</strong>):</p>
			<p class="figure-caption"><img src="image/B17499_Table_2.6.png" alt="Table 1.6 – The running time of set operations " width="842" height="266"/></p>
			<p class="figure-caption">Table 1.6 – The running time of set operations</p>
			<p>An application of set operations is, for example, Boolean queries. Going back to the inverted index example of the previous subsection, we may want to support queries that include multiple terms. For example, we may want to search for all the documents that contain the words <strong class="source-inline">cat</strong> and <strong class="source-inline">table</strong>. This kind of query can be efficiently computed by taking the intersection between the set of documents containing <strong class="source-inline">cat</strong> and the set of documents containing <strong class="source-inline">table</strong>.</p>
			<p>In order to efficiently support<a id="_idIndexMarker113"/> those operations, we can change our indexing code so that each term is associated with a set of documents (rather than a list). After applying this change, calculating more advanced queries is a matter of applying the right set operation. In the following code, we show the inverted index based on sets and the query using set operations:</p>
			<p class="source-code">    # Building an index using sets</p>
			<p class="source-code">    index = {}</p>
			<p class="source-code">    for i, doc in enumerate(docs):</p>
			<p class="source-code">        # We iterate over each term in the document</p>
			<p class="source-code">        for word in doc.split():</p>
			<p class="source-code">            # We build a set containing the indices </p>
			<p class="source-code">            # where the term appears</p>
			<p class="source-code">            if word not in index:</p>
			<p class="source-code">                index[word] = {i}</p>
			<p class="source-code">            else:</p>
			<p class="source-code">                index[word].add(i)</p>
			<p class="source-code">    </p>
			<p class="source-code">    # Querying the documents containing both "cat" and</p>
			<p class="source-code">        "table"</p>
			<p class="source-code">    index['cat'].intersection(index['table'])</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor038"/>Heaps</h2>
			<p><strong class="bold">Heaps</strong> are data structures designed to quickly find<a id="_idIndexMarker114"/> and extract the maximum (or minimum) value in a collection. A typical use case for heaps is to process a series of incoming tasks in order of maximum priority.</p>
			<p>You can theoretically use a sorted list using the tools in the <strong class="source-inline">bisect</strong> module; however, while extracting the maximum value will take <em class="italic">O</em>(1) time (using <strong class="source-inline">list.pop</strong>), insertion will still take <em class="italic">O</em>(N) time (remember that, even if finding the insertion point takes <em class="italic">O</em>(<em class="italic">log</em>(<em class="italic">N</em>)) time, inserting an element in the middle of a list is still an <em class="italic">O</em>(<em class="italic">N</em>) operation). A heap is a more efficient data structure that allows for the insertion and extraction of maximum values with <em class="italic">O</em>(<em class="italic">log</em>(<em class="italic">N</em>)) time complexity.</p>
			<p>In Python, heaps are built using the procedures contained in the <strong class="source-inline">heapq</strong> module on an underlying list. For example, if we have a list of 10 elements, we can reorganize it into a heap with the <strong class="source-inline">heapq.heapify</strong> function:</p>
			<p class="source-code">    import heapq</p>
			<p class="source-code">    collection = [10, 3, 3, 4, 5, 6]</p>
			<p class="source-code">    heapq.heapify(collection)</p>
			<p>To perform the insertion and extraction operations on the heap, we can use the <strong class="source-inline">heapq.heappush</strong> and <strong class="source-inline">heapq.heappop</strong> functions. The <strong class="source-inline">heapq.heappop</strong> function will extract the minimum value in the collection in <em class="italic">O</em>(<em class="italic">log</em>(<em class="italic">N</em>)) time and can be used in the following way:</p>
			<p class="source-code">    heapq.heappop(collection)</p>
			<p class="source-code">    # Returns: 3</p>
			<p>Similarly, you can push the <strong class="source-inline">1</strong> integer with the <strong class="source-inline">heapq.heappush</strong> function, as follows:</p>
			<p class="source-code">    heapq.heappush(collection, 1)</p>
			<p>Another easy-to-use option is the <strong class="source-inline">queue.PriorityQueue</strong> class, which as a bonus, is thread- and process-safe. The <strong class="source-inline">PriorityQueue</strong> class can be filled up with elements using the <strong class="source-inline">PriorityQueue.put</strong> method, while <strong class="source-inline">PriorityQueue.get</strong> can be used to extract the minimum value in the collection:</p>
			<p class="source-code">    from queue import PriorityQueue</p>
			<p class="source-code">    queue = PriorityQueue()</p>
			<p class="source-code">    for element in collection:</p>
			<p class="source-code">        queue.put(element)</p>
			<p class="source-code">    queue.get()</p>
			<p class="source-code">    # Returns: 3</p>
			<p>If the maximum element is required, a simple trick is to multiply each element of the list by <strong class="source-inline">-1</strong>. In this way, the order of the elements will be inverted. Also, if you want to associate an object (for example, a task to run) with each number (which can represent the priority), you can insert tuples of the <strong class="source-inline">(number, object)</strong> form; the comparison operator for the tuple<a id="_idIndexMarker115"/> will be ordered with respect to its first element, as shown in the following example:</p>
			<p class="source-code">    queue = PriorityQueue()</p>
			<p class="source-code">    queue.put((3, "priority 3"))</p>
			<p class="source-code">    queue.put((2, "priority 2"))</p>
			<p class="source-code">    queue.put((1, "priority 1"))</p>
			<p class="source-code">    queue.get()</p>
			<p class="source-code">    # Returns: (1, "priority 1")</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor039"/>Tries</h2>
			<p>A perhaps less popular data structure, but very useful in practice, is the <strong class="bold">trie</strong> (sometimes called <strong class="bold">prefix tree</strong>). Tries are extremely fast at<a id="_idIndexMarker116"/> matching a list of strings<a id="_idIndexMarker117"/> against a prefix. This is especially useful when implementing features such as <em class="italic">search as you type</em> and <em class="italic">autocompletion</em>, where the list of available completions is very large and short response times are required.</p>
			<p>Unfortunately, Python does not include a trie implementation in its standard library; however, many efficient implementations are readily available through <em class="italic">PyPI</em>. The one we will use in this subsection is <strong class="source-inline">patricia-trie</strong>, a single-file, pure Python implementation of trie. As an example, we will use <strong class="source-inline">patricia-trie</strong> to perform the task of finding the longest prefix in a set of strings (just like autocompletion).</p>
			<p>Here, we can demonstrate how fast a trie is able to search through a list of strings. In order to generate a large amount of unique random strings, we can define a function, <strong class="source-inline">random_string</strong>. The <strong class="source-inline">random_string</strong> function will return a string composed of random uppercase characters and, while there is a chance to get duplicates, we can greatly reduce the probability of duplicates to the point of being negligible if we make the string<a id="_idIndexMarker118"/> long enough. The implementation of the <strong class="source-inline">random_string</strong> function is shown as follows:</p>
			<p class="source-code">    from random import choice</p>
			<p class="source-code">    from string import ascii_uppercase</p>
			<p class="source-code">    def random_string(length):</p>
			<p class="source-code">     """Produce a random string made of *length* uppercase \</p>
			<p class="source-code">        ascii characters"""</p>
			<p class="source-code">     return ''.join(choice(ascii_uppercase) for i in \</p>
			<p class="source-code">       range(length))</p>
			<p>We can build a list of random strings and time how fast it searches for a prefix (in our case, the <strong class="source-inline">"AA"</strong> string) using the <strong class="source-inline">str.startswith</strong> function:</p>
			<p class="source-code">    strings = [random_string(32) for i in range(10000)]</p>
			<p class="source-code">    matches = [s for s in strings if s.startswith('AA')]</p>
			<p>List comprehension and <strong class="source-inline">str.startwith</strong> are already very optimized operations and, on this small dataset, the search takes only a millisecond or so:</p>
			<p class="source-code">    %timeit [s for s in strings if s.startswith('AA')]</p>
			<p class="source-code">    1000 loops, best of 3: 1.76 ms per loop</p>
			<p>Now, let's try using a trie for the same<a id="_idIndexMarker119"/> operation. In this example, we will use the <strong class="source-inline">patricia-trie</strong> library that is installable through <strong class="source-inline">pip</strong>. The <strong class="source-inline">patricia.trie</strong> class implements a variant of the trie data structure with an interface similar to a dictionary. We can initialize our trie by creating a dictionary from our list of strings, as follows:</p>
			<p class="source-code">    from patricia import trie</p>
			<p class="source-code">    strings_dict = {s:0 for s in strings} </p>
			<p class="source-code">    # A dictionary where all values are 0</p>
			<p class="source-code">    strings_trie = trie(**strings_dict)</p>
			<p>To query <strong class="source-inline">patricia-trie</strong> for a matching prefix, we can use the <strong class="source-inline">trie.iter</strong> method, which returns an iterator over the matching strings:</p>
			<p class="source-code">    matches = list(strings_trie.iter('AA'))</p>
			<p>Now that we know how to initialize and query a trie, we can time the operation:</p>
			<p class="source-code">    %timeit list(strings_trie.iter('AA'))</p>
			<p class="source-code">    10000 loops, best of 3: 60.1 µs per loop</p>
			<p>The timing for this input size is <strong class="bold">60.1 µs</strong>, which is about 30 times faster (1.76 ms = 1760 µs) than linear search! This impressive speedup is because of the better computational complexity of the trie prefix search. Querying a trie has an <em class="italic">O</em>(<em class="italic">S</em>) time complexity, where <em class="italic">S</em> is the length of the longest string in the collection, while the time complexity of a simple linear scan is <em class="italic">O</em>(<em class="italic">N</em>), where <em class="italic">N</em> is the size of the collection. </p>
			<p>Note that if we want to return all the prefixes that match, the running time will be proportional to the number of results that match the prefix. Therefore, when designing timing benchmarks, care must be taken to ensure that we are always returning the same number of results.</p>
			<p>The scaling properties of a trie versus a linear scan for datasets of different sizes that contains 10 prefix matches are shown in the following table:</p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B17499_Table_2.7.jpg" alt="Table 1.7 – The running time of a trie versus a linear scan " width="1635" height="220"/>
				</div>
			</div>
			<p class="figure-caption">Table 1.7 – The running time of a trie versus a linear scan</p>
			<p>An interesting fact is that the implementation of <strong class="source-inline">patricia-trie</strong> is actually a single Python file; this clearly shows how simple and powerful a clever algorithm can be. For extra features and performance, other C-optimized trie libraries are also available, such as <strong class="source-inline">datrie</strong> and <strong class="source-inline">marisa-trie</strong>.</p>
			<p>At this point, we have considered most of the important data structures that are native to Python. Appropriate use of these data structures will speed up your application by a significant degree. In addition to data structures, there are other computing techniques and concepts that we could utilize in order to make our programs even faster<a id="_idTextAnchor040"/>. In the next section, we will take a look<a id="_idIndexMarker120"/> at caching and memoization, which are common practices when you expect the same computations to be repeated multiple times.</p>
			<h1 id="_idParaDest-42"><a id="_idTextAnchor041"/>Improved efficiency with caching and memoization</h1>
			<p><strong class="bold">Caching</strong> is a great technique used to improve<a id="_idIndexMarker121"/> the performance of a wide range of applications. The idea behind caching is to store expensive<a id="_idIndexMarker122"/> results in a temporary location, called a <strong class="bold">cache</strong>, that can be located in memory, on disk, or in a remote location.</p>
			<p>Web applications make extensive use of caching. In a web application, it is often the case that multiple users request a certain page at the same time. In this case, instead of recomputing the page for each user, the web application can compute it once and serve the user the already rendered page. Ideally, caching also needs a mechanism for invalidation so that if the page needs to be updated, we can recompute it before serving it again. Intelligent caching allows web applications to handle the increasing number of users with fewer resources. Caching can also be done preemptively, such as the later sections of a video getting buffered when watching a video online.</p>
			<p>Caching is also used to improve the performance<a id="_idIndexMarker123"/> of certain algorithms. A great example is computing the Fibonacci sequence. Since computing the next number in the Fibonacci sequence requires the previous number in the sequence, you can store and reuse previous results, dramatically improving the running time. Storing and reusing<a id="_idIndexMarker124"/> the results of the previous function calls in an application is usually termed <strong class="bold">memoization</strong> and is one of the forms of caching. Several other algorithms can take advantage of memoization<a id="_idIndexMarker125"/> to gain impressive performance improvements, and this programming technique is commonly referred to as <strong class="bold">dynamic programming</strong>, where you aim to solve a large problem<a id="_idIndexMarker126"/> by breaking it into smaller ones.</p>
			<p>The benefits of caching, however, do not come for free. What we are actually doing is sacrificing some space to improve the speed of the application. Additionally, if the cache is stored in a location on the network, we may incur transfer costs and the general time needed for communication. You should evaluate when it is convenient to use a cache and how much space you are willing to trade for an increase in speed.</p>
			<p>Given the usefulness of this technique, the Python standard library includes a simple in-memory cache out of the box in the <strong class="source-inline">functools</strong> module. The <strong class="source-inline">functools.lru_cache</strong> decorator can be used to easily cache the results of a function.</p>
			<p>In the following example, we create a function, <strong class="source-inline">sum2</strong>, that prints a statement and returns the sum of two numbers. By running the function twice, you can see that the first time the <strong class="source-inline">sum2</strong> function is executed, the <strong class="source-inline">"Calculating ..."</strong> string is produced, while the second time, the result is returned without running the function:</p>
			<p class="source-code">    from functools import lru_cache</p>
			<p class="source-code">    @lru_cache()</p>
			<p class="source-code">    def sum2(a, b):</p>
			<p class="source-code">        print("Calculating {} + {}".format(a, b))</p>
			<p class="source-code">        return a + b</p>
			<p class="source-code">    print(sum2(1, 2))</p>
			<p class="source-code">    # Output: </p>
			<p class="source-code">    # Calculating 1 + 2</p>
			<p class="source-code">    # 3</p>
			<p class="source-code">    print(sum2(1, 2))</p>
			<p class="source-code">    # Output: </p>
			<p class="source-code">    # 3</p>
			<p>The <strong class="source-inline">lru_cache</strong> decorator also provides<a id="_idIndexMarker127"/> other basic features. To restrict the size of the cache, you can set the number of elements that we intend to maintain through the <strong class="source-inline">max_size</strong> argument. If we want our cache size to be unbounded, we can specify a value of <strong class="source-inline">None</strong>. An example of <strong class="source-inline">max_size</strong> usage is shown here:</p>
			<p class="source-code">    @lru_cache(max_size=16)</p>
			<p class="source-code">    def sum2(a, b):</p>
			<p class="source-code">        ...</p>
			<p>In this way, as we execute <strong class="source-inline">sum2</strong> with different arguments, the cache will reach a maximum size of <strong class="source-inline">16</strong> and, as we keep requesting more calculations, new values will replace older values in the cache. The <strong class="source-inline">lru</strong> prefix originates from this strategy, which means <em class="italic">least recently used</em>.</p>
			<p>The <strong class="source-inline">lru_cache</strong> decorator also adds extra functionalities<a id="_idIndexMarker128"/> to the decorated function. For example, it is possible<a id="_idIndexMarker129"/> to examine the cache performance using the <strong class="source-inline">cache_info</strong> method, and it is possible to reset the cache using the <strong class="source-inline">cache_clear</strong> method, as follows:</p>
			<p class="source-code">    sum2.cache_info()</p>
			<p class="source-code">    # Output: CacheInfo(hits=0, misses=1, maxsize=128,</p>
			<p class="source-code">      currsize=1)</p>
			<p class="source-code">    sum2.cache_clear()</p>
			<p>As an example, we can see how a problem, such as computing the Fibonacci series, may benefit from caching. We can define a <strong class="source-inline">fibonacci</strong> function and time its execution:</p>
			<p class="source-code">    def fibonacci(n):</p>
			<p class="source-code">        if n &lt; 1:</p>
			<p class="source-code">            return 1</p>
			<p class="source-code">        else:</p>
			<p class="source-code">            return fibonacci(n - 1) + fibonacci(n - 2)</p>
			<p class="source-code">    # Non-memoized version</p>
			<p class="source-code">    %timeit fibonacci(20)</p>
			<p class="source-code">    100 loops, best of 3: 5.57 ms per loop</p>
			<p>The execution takes 5.57 milliseconds, which is very long. The scaling of the function written in this way has poor performance; the previously computed Fibonacci sequences are not reused, causing this algorithm to have an exponential scaling of roughly <em class="italic">O</em>(<em class="italic">2N</em>).</p>
			<p>Caching can improve<a id="_idIndexMarker130"/> this algorithm by storing and reusing the already-computed Fibonacci numbers. To implement the cached version, it is sufficient to apply the <strong class="source-inline">lru_cache</strong> decorator to the original <strong class="source-inline">fibonacci</strong> function. Also, to design a proper<a id="_idIndexMarker131"/> benchmark, we need to ensure that a new cache is instantiated<a id="_idIndexMarker132"/> for every run; to do this, we can use the <strong class="source-inline">timeit.repeat</strong> function, as shown in the following example:</p>
			<p class="source-code">    import timeit</p>
			<p class="source-code">    setup_code = '''</p>
			<p class="source-code">    from functools import lru_cache</p>
			<p class="source-code">    from __main__ import fibonacci</p>
			<p class="source-code">    fibonacci_memoized = lru_cache(maxsize=None)(fibonacci)'''</p>
			<p class="source-code">    results = timeit.repeat('fibonacci_memoized(20)',</p>
			<p class="source-code">                            setup=setup_code,</p>
			<p class="source-code">                            repeat=1000,</p>
			<p class="source-code">                            number=1)</p>
			<p class="source-code">    print("Fibonacci took {:.2f} us".format(min(results)))</p>
			<p class="source-code">    # Output: Fibonacci took 0.01 us</p>
			<p>Even though we changed the algorithm by adding a simple decorator, the running time now is much less than a microsecond. The reason is, thanks to caching, we now have a linear time algorithm instead of an exponential one.</p>
			<p>The <strong class="source-inline">lru_cache</strong> decorator can be used to implement<a id="_idIndexMarker133"/> simple in-memory caching<a id="_idIndexMarker134"/> in your application. For more advanced use cases, third-party<a id="_idIndexMarker135"/> modules can be used for more powerful implementation and on-disk caching.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>Joblib</h2>
			<p>A simple library that, among other things, provides a simple on-disk cache is <strong class="source-inline">joblib</strong>. The package<a id="_idIndexMarker136"/> can be used in a similar way as <strong class="source-inline">lru_cache</strong>, except that the results will be stored on disk and will persist between runs.</p>
			<p>The <strong class="source-inline">joblib</strong> module can be installed from PyPI using the <strong class="source-inline">pip install joblib</strong> command.</p>
			<p>The <strong class="source-inline">joblib</strong> module provides the <strong class="source-inline">Memory</strong> class, which can be used to memoize functions using the <strong class="source-inline">Memory.cache</strong> decorator:</p>
			<p class="source-code">    from joblib import Memory</p>
			<p class="source-code">    memory = Memory(cachedir='/path/to/cachedir')</p>
			<p class="source-code">    @memory.cache</p>
			<p class="source-code">    def sum2(a, b):</p>
			<p class="source-code">        return a + b</p>
			<p>The function will behave similarly to <strong class="source-inline">lru_cache</strong>, with the exception that the results will be stored on disk in the directory specified by the <strong class="source-inline">cachedir</strong> argument during <strong class="source-inline">Memory</strong> initialization. Additionally, the cached results will persist over subsequent runs!</p>
			<p>The <strong class="source-inline">Memory.cache</strong> method also allows limiting recomputation to only when certain arguments change, and the resulting decorated function supports basic functionalities to clear and analyze the cache. </p>
			<p>Perhaps the best <strong class="source-inline">joblib</strong> feature is, thanks to intelligent hashing algorithms, providing efficient memoization<a id="_idIndexMarker137"/> of functions that operate on <strong class="source-inline">numpy</strong> arrays, which is particularly useful in scientific and engineering applications.</p>
			<p>At this point, we have seen that by using caching and memoization, our program can reuse computations in an efficient way. Another common strategy to improve running time is to utilize specifically designed techniques as appropriate. In the next section, we will see how we can take advantage of comprehensions and generators when working with Python loops.</p>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor043"/>Efficient iteration with comprehensions and generators</h1>
			<p>In this section, we will explore a few simple strategies to speed up Python loops using <strong class="bold">comprehensions</strong> and <strong class="bold">generators</strong>. In Python, comprehension and generator expressions are fairly optimized operations and should be preferred in place of explicit <strong class="source-inline">for</strong> loops, as they are designed<a id="_idIndexMarker138"/> to avoid many unnecessary computational overheads<a id="_idIndexMarker139"/> during their construction. Another reason to use this construct is readability; even if the speedup over a standard loop is modest, the comprehension and generator syntax is more compact and (most of the time) more intuitive.</p>
			<p>In the following example, we can see that both the list comprehension and generator expressions are faster than an explicit loop when combined with the <strong class="source-inline">sum</strong> function:</p>
			<p class="source-code">    def loop(): </p>
			<p class="source-code">        res = [] </p>
			<p class="source-code">        for i in range(100000): </p>
			<p class="source-code">            res.append(i * i) </p>
			<p class="source-code">        return sum(res) </p>
			<p class="source-code">    def comprehension(): </p>
			<p class="source-code">        return sum([i * i for i in range(100000)]) </p>
			<p class="source-code">    def generator(): </p>
			<p class="source-code">        return sum(i * i for i in range(100000)) </p>
			<p class="source-code">    %timeit loop() </p>
			<p class="source-code">    100 loops, best of 3: 16.1 ms per loop </p>
			<p class="source-code">    %timeit comprehension() </p>
			<p class="source-code">    100 loops, best of 3: 10.1 ms per loop </p>
			<p class="source-code">    %timeit generator() </p>
			<p class="source-code">    100 loops, best of 3: 12.4 ms per loop </p>
			<p>Just like lists, it is possible to use <strong class="source-inline">dict</strong> comprehension to build dictionaries slightly more efficiently<a id="_idIndexMarker140"/> and compactly, as shown in the<a id="_idIndexMarker141"/> following code:</p>
			<p class="source-code">    def loop(): </p>
			<p class="source-code">        res = {} </p>
			<p class="source-code">        for i in range(100000): </p>
			<p class="source-code">            res[i] = i</p>
			<p class="source-code">        return res</p>
			<p class="source-code">    def comprehension(): </p>
			<p class="source-code">        return {i: i for i in range(100000)}</p>
			<p class="source-code">    %timeit loop() </p>
			<p class="source-code">    100 loops, best of 3: 13.2 ms per loop </p>
			<p class="source-code">    %timeit comprehension() </p>
			<p class="source-code">    100 loops, best of 3: 12.8 ms per loop</p>
			<p>Efficient looping (especially in terms of memory) can be implemented using iterators and functions such as <strong class="source-inline">filter</strong> and <strong class="source-inline">map</strong>. As an example, consider the problem of applying a series of operations to a list using list comprehension and then taking the maximum value:</p>
			<p class="source-code">    def map_comprehension(numbers):</p>
			<p class="source-code">        a = [n * 2 for n in numbers]</p>
			<p class="source-code">        b = [n ** 2 for n in a]</p>
			<p class="source-code">        c = [n ** 0.33 for n in b]</p>
			<p class="source-code">        return max(c)</p>
			<p>The problem with this approach is that for every list comprehension, we are allocating a new list, increasing memory<a id="_idIndexMarker142"/> usage. Instead of using list comprehension, we can employ generators. Generators<a id="_idIndexMarker143"/> are objects that, when iterated upon, compute a value on the fly and return the result.</p>
			<p>For example, the <strong class="source-inline">map</strong> function takes two arguments – a function and an iterator – and returns a generator that applies the function to every element of the collection. The important point is that the operation happens only <em class="italic">while we are iterating</em>, and not when <strong class="source-inline">map</strong> is invoked!</p>
			<p>We can rewrite the previous function using <strong class="source-inline">map</strong> and by creating intermediate generators, rather than lists, thus saving memory by computing the values on the fly:</p>
			<p class="source-code">    def map_normal(numbers):</p>
			<p class="source-code">        a = map(lambda n: n * 2, numbers)</p>
			<p class="source-code">        b = map(lambda n: n ** 2, a)</p>
			<p class="source-code">        c = map(lambda n: n ** 0.33, b)</p>
			<p class="source-code">        return max(c)</p>
			<p>We can profile the memory of the two solutions using the <strong class="source-inline">memory_profiler</strong> extension from an IPython session. The extension provides a small utility, <strong class="source-inline">%memit</strong>, that will help us evaluate the memory usage of a Python statement in a way similar to <strong class="source-inline">%timeit</strong>, as illustrated in the following details:</p>
			<p class="source-code">    %load_ext memory_profiler</p>
			<p class="source-code">    numbers = range(1000000)</p>
			<p class="source-code">    %memit map_comprehension(numbers)</p>
			<p class="source-code">    peak memory: 166.33 MiB, increment: 102.54 MiB</p>
			<p class="source-code">    %memit map_normal(numbers)</p>
			<p class="source-code">    peak memory: 71.04 MiB, increment: 0.00 MiB</p>
			<p>As you can see, the memory used by the first <a id="_idIndexMarker144"/>version is <strong class="source-inline">102.54 MiB</strong>, while the second version consumes <strong class="source-inline">0.00 MiB</strong>! For those who are interested, more functions that return generators<a id="_idIndexMarker145"/> can be found in the <strong class="source-inline">itertools</strong> module, which provides a set of utilities designed to handle common iteration patterns.</p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor044"/>Summary</h1>
			<p>Algorithmic optimization can improve how your application scales as we process increasingly large data. In this chapter, we demonstrated use cases and running times of the most common data structures available in Python, such as lists, deques, dictionaries, heaps, and tries. We also covered caching, a technique that can be used to trade some space, in memory or on disk, in exchange for the increased responsiveness of an application. We also demonstrated how to get modest speed gains by replacing <strong class="source-inline">for</strong> loops with fast constructs, such as list comprehensions and generator expressions.</p>
			<p>Overall, we have seen that by utilizing a specifically designed data structure or technique that is appropriate in certain situations, the efficiency of our program can be greatly improved. The topics covered in this chapter offer us the ability to do just that across a wide range of use cases.</p>
			<p>In the subsequent chapters, we will learn how to improve performance further using numerical libraries such as <strong class="source-inline">numpy</strong> and how to write extension modules in a lower-level language with the help of <em class="italic">Cython</em>.</p>
			<h1 id="_idParaDest-46"><a id="_idTextAnchor045"/>Questions</h1>
			<ol>
				<li>Identify the best/most appropriate from the data structures covered in this chapter concerning the following use cases:<ol><li>Mapping items to another <em class="italic">set</em> of items (<em class="italic">set</em> being used in the most general sense)</li><li>Accessing, modifying, and appending elements</li><li>Maintaining a collection of unique elements</li><li>Keeping track of the minimum/maximum of a <em class="italic">set</em> (in the most general sense)</li><li>Appending and removing elements at the endpoints of a <em class="italic">sequence</em> (in the most general sense).</li><li>Fast searching according to some similarity criterion (for example, being used by autocompletion engines).</li></ol></li>
				<li>What is the difference between caching and memoization?</li>
				<li>Why are comprehensions and generators (in most situations) more preferred than explicit <strong class="source-inline">for</strong> loops?</li>
				<li>Consider the problem of representing a pairwise association between a set of letters and a set of numbers (for example, a  2, b  1, c  3, and so on), where we need to look at what number a given letter is associated with in our application.<ol><li>Is a list an appropriate data structure for this task, and if not, what is?</li><li>What if each number represented the number of instances of a given letter in a text document? What would the best data structure for this task be?</li></ol></li>
			</ol>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor046"/>Further reading</h1>
			<ul>
				<li>Lazy evaluation in Python: <a href="https://towardsdatascience.com/what-is-lazy-evaluation-in-python-9efb1d3bfed0">https://towardsdatascience.com/what-is-lazy-evaluation-in-python-9efb1d3bfed0</a></li>
				<li>The <strong class="source-inline">yield</strong> statement in Python: <a href="https://realpython.com/introduction-to-python-generators/">https://realpython.com/introduction-to-python-generators/</a></li>
			</ul>
		</div>
	</div>
</div>
</body></html>