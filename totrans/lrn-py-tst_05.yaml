- en: Chapter 5. Structured Testing with unittest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `doctest` tool is flexible and extremely easy to use but, as we''ve noticed,
    it falls somewhat short when it comes to writing disciplined tests. That''s not
    to say that it''s impossible; we''ve seen that we can write well-behaved, isolated
    tests in `doctest`. The problem is that `doctest` doesn''t do any of that work
    for us. Fortunately, we have another testing tool on hand, a tool that requires
    a bit more structure in our tests, and provides a bit more support: `unittest`.'
  prefs: []
  type: TYPE_NORMAL
- en: The `unittest` module was designed based on the requirements of unit testing,
    but it's not actually limited to that. You can use unit test for integration and
    system testing, too.
  prefs: []
  type: TYPE_NORMAL
- en: Like `doctest`, `unittest` is a part of the Python standard library; thus, if
    you've got Python, you have unit test.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Writing tests within the `unittest` framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running our new tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at the features that make `unittest` a good choice for larger test suites
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start talking about new concepts and features, let's take a look at
    how to use `unittest` to express the ideas that we've already learned about. That
    way, we'll have something solid on which ground our new understanding.
  prefs: []
  type: TYPE_NORMAL
- en: We're going to revisit the `PID` class, or at least the tests for the `PID`
    class, from [Chapter 3](ch03.html "Chapter 3. Unit Testing with doctest"), *Unit
    Testing with doctest*. We're going to rewrite the tests so that they operate within
    the `unittest` framework.
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on, take a moment to refer back to the final version of the `pid.txt`
    file from [Chapter 3](ch03.html "Chapter 3. Unit Testing with doctest"), *Unit
    Testing with doctest*. We'll be implementing the same tests using the `unittest`
    framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new file called `test_pid.py` in the same directory as `pid.py`. Notice
    that this is a `.py` file: `unittest` tests are pure Python source code, rather
    than being plain text with source code embedded in it. This means that the tests
    will be less useful from a documentary point of view, but grants other benefits
    in exchange.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Insert the following code into your newly created `test_pid.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: It has been argued, sometimes with good reason, that unit tests should not contain
    more than one assertion. The idea is that each unit test should test one thing
    and one thing only, to further narrow down what the problem is, when the test
    fails. It's a good point but not something to be overly fanatic about, in my opinion.
    In cases like the preceding code, splitting each assertion out into its own test
    function will not produce any more informative error messages than we get in this
    way; it would just increase our overhead.
  prefs: []
  type: TYPE_NORMAL
- en: 'My rule of thumb is that a test function can have any number of trivial assertions,
    and at most one non-trivial assertion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, run the tests by typing the following on the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see output similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The basics](img/3211OS_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, what did we do there? There are several things to notice:'
  prefs: []
  type: TYPE_NORMAL
- en: First, all of the tests are their own methods of classes that inherit from `unittest.TestCase`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tests are named `test_<something>`, where `<something>` is a description
    to help you (and others who share the code) remember what the test is actually
    checking. This matters because `unittest` (and several other testing tools) use
    the name to differentiate tests from non-test methods. As a rule of thumb, your
    test method names and test module filenames should start with test.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because each test is a method, each test naturally runs in its own variable
    scope. Right here, we gain a big advantage from keeping the tests isolated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We inherited a bunch of `assert<Something>` methods from `TestCase`. These give
    us more flexible ways of checking whether values match, and provide more useful
    error reports, than Python's basic `assert` statement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We used `unittest.mock.patch` as a method decorator. In [Chapter 4](ch04.html
    "Chapter 4. Decoupling Units with unittest.mock"), *Decoupling Units with unittest.mock*,
    we used it as a context manager. Either way, it does the same thing: it replaces
    an object with a mock object, and then puts the original back. When used as a
    decorator, the replacement happens before the method runs, and the original is
    put back after the method is complete. That''s exactly what we need when our test
    is a method, so we''ll be doing it in this way quite a lot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We didn't patch over `time.time`, we patched `over pid.time`. This is because
    we're not reimporting the `pid` module for each test here. The `pid` module contains
    `from time import time`, which means that, when it is first loaded, the `time`
    function is referenced directly into the `pid` module's scope. From then on, changing
    `time.time` doesn't have any effect on `pid.time`, unless we change it and then
    reimport the `pid` module. Instead of going to all that trouble, we just patched
    `pid.time` directly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We didn't tell `unittest` which tests to run. Instead, we told it to discover
    them and it found the tests on its own and ran them automatically. This often
    works well and saves effort. We'll be looking at a more elaborate tool for test
    discovery and execution in [Chapter 6](ch06.html "Chapter 6. Running Your Tests
    with Nose"), *Running Your Tests with Nose*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `unittest` module prints out one dot for each successful test. It will give
    you more information for tests that fail, or raise an unexpected exception.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The actual tests we performed are the same ones that were written in `doctest`.
    So far, all we're seeing is a different way of expressing them.
  prefs: []
  type: TYPE_NORMAL
- en: Each test method embodies a single test of a single unit. This gives us a convenient
    way to structure our tests, grouping together related tests into the same class
    so that they're easier to find. You might have noticed that we used two test classes
    in the example. This was for organizational purposes in this case, although there
    can also be good practical reasons to separate your tests into multiple classes.
    We'll talk about that soon.
  prefs: []
  type: TYPE_NORMAL
- en: Putting each test into its own method means that each test executes in an isolated
    namespace, which makes it easier to keep `unittest`-style tests from interfering
    with each other, relative to `doctest`-style tests. This also means that `unittest`
    knows how many unit tests are in your test file, instead of simply knowing how
    many expressions there are (you might have noticed that `doctest` counts each
    `>>>` line as a separate test). Finally, putting each test in its own method means
    that each test has a name, which can be a valuable feature. When you run `unittest`,
    it will include the names of any failing tests in the error report.
  prefs: []
  type: TYPE_NORMAL
- en: Tests in `unittest` don't directly care about anything that isn't part of a
    call to one of the `TestCase` `assert` methods. This means that we don't have
    to be bothered about the return values of any functions we call or the results
    of any expressions we use, unless they're important to the test. This also means
    that we need to remember to write an assert describing every aspect of the test
    that we want to have checked. We'll go through the various assertion methods of
    `TestCase` shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Assertions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Assertions are the mechanism we use to tell `unittest` what the important outcomes
    of the test are. By using appropriate assertions, we can tell `unittest` exactly
    what to expect from each test.
  prefs: []
  type: TYPE_NORMAL
- en: The assertTrue method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we call `self.assertTrue(expression)`, we're telling `unittest` that the
    expression must be true in order for the test to be a success.
  prefs: []
  type: TYPE_NORMAL
- en: This is a very flexible assertion, since you can check for nearly anything by
    writing the appropriate Boolean expression. It's also one of the last assertions
    you should consider using, because it doesn't tell `unittest` anything about the
    kind of comparison you're making, which means that `unittest` can't tell you clearly
    what's gone wrong if the test fails.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the following test code containing two tests that are
    guaranteed to fail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: It might seem that the two tests are interchangeable, since they both test the
    same thing. Certainly they'll both fail (or, in the unlikely event that one equals
    two, they'll both pass), so why prefer one over the other?
  prefs: []
  type: TYPE_NORMAL
- en: Run the tests and see what happens (and also notice that the tests were not
    executed in the same order as we wrote them; the tests are totally independent
    of each other, so that's okay, right?).
  prefs: []
  type: TYPE_NORMAL
- en: 'Both the tests fail, as expected, but the test that uses `assertEqual` tells
    us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The other one says:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s pretty clear which of these outputs is more useful in this situation.
    The `assertTrue` test was able to correctly determine that the test should fail,
    but it didn''t know enough to report any useful information about why it failed.
    The `assertEqual` test, on the other hand, knew first of all that it was checking
    whether the two expressions were equal, and second it knew how to present the
    results so that they would be most useful: by evaluating each of the expressions
    that it was comparing and placing a `!=` symbol between the results. It tells
    us both which expectation failed, and what the relevant expressions evaluate to.'
  prefs: []
  type: TYPE_NORMAL
- en: The assertFalse method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `assertFalse` method will succeed when the `assertTrue` method will fail,
    and vice versa. It has the same limits in terms of producing useful output that
    `assertTrue` has, and the same flexibility in terms of being able to test nearly
    any condition.
  prefs: []
  type: TYPE_NORMAL
- en: The assertEqual method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the `assertTrue` discussion, the `assertEqual` assertion checks
    whether its two parameters are in fact equal, and reports a failure if they are
    not, along with the actual values of the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The assertNotEqual method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `assertNotEqual` assertion fails whenever the `assertEqual` assertion would
    have succeeded, and vice versa. When it reports a failure, its output indicates
    that the values of the two expressions are equal, and provides you with those
    values.
  prefs: []
  type: TYPE_NORMAL
- en: The assertAlmostEqual method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we've seen before, comparing floating point numbers can be troublesome. In
    particular, checking whether two floating point numbers are equal is problematic,
    because things that you might expect to be equal—things that, mathematically,
    are equal—may still end up differing down among the least significant bits. Floating
    point numbers only compare equal when every bit is the same.
  prefs: []
  type: TYPE_NORMAL
- en: To address this problem, `unittest` provides `assertAlmostEqual`, which checks
    whether the two floating point values are almost the same; a small amount of difference
    between them is tolerated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at this problem in action. If you take the square root of seven,
    and then square it, the result should be seven. Here''s a pair of tests that check
    this fact:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `test_square_root_of_seven_squared_incorrectly` method checks that ![The
    assertAlmostEqual method](img/3211OS_05_02.jpg), which is true in reality. In
    the more specialized number system available to computers, though, taking the
    square root of 7 and then squaring it doesn't quite get us back to 7, so this
    test will fail. We will look more closely at this in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: The `test_square_root_of_seven_squared` method checks ![The assertAlmostEqual
    method](img/3211OS_05_03.jpg), which even the computer will find to be true, so
    this test should pass.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, floating point numbers (the representation of real numbers used
    by computers) are not precise, because the majority of numbers on the real number
    line cannot be represented with a finite, non-repeating sequence of digits, much
    less than a mere 64 bits. Consequently, what you get back from evaluating the
    mathematical expression in the previous example is not quite seven. It's good
    enough for government work though—or practically any other sort of work as well—so
    we don't want our test to quibble over that tiny difference. Because of this,
    we should habitually use `assertAlmostEqual` and `assertNotAlmostEqual` when we're
    comparing floating point numbers with equality.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This problem doesn't generally carry over into other comparison operators. Checking
    whether one floating point number is less than the other, for example, is very
    unlikely to produce the wrong result due to insignificant errors. It's only in
    cases of equality that this problem bites us.
  prefs: []
  type: TYPE_NORMAL
- en: The assertNotAlmostEqual method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `assertNotAlmostEqual` assertion fails whenever the `assertAlmostEqual`
    assertion would have succeeded, and vice versa. When it reports a failure, its
    output indicates that the values of the two expressions are nearly equal, and
    provides you with those values.
  prefs: []
  type: TYPE_NORMAL
- en: The assertIs and assertIsNot methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `assertIs` and `assertIsNot` methods have the same relationship with Python's
    `is` operator that `assertEqual` and `assertNotEqual` have to Python's `==` operator.
    What this means is that they check whether the two operands are (or are not) exactly
    the same object.
  prefs: []
  type: TYPE_NORMAL
- en: The assertIsNone and assertIsNotNone methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `assertIsNone` and `assertIsNotNone` methods are like `assertIs` and `assertIsNot`,
    except that they accept only one parameter that they always compare to `None`,
    rather than accepting two parameters and comparing them to each other.
  prefs: []
  type: TYPE_NORMAL
- en: The assertIn and assertNotIn methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `assertIn` method is used for checking container objects such as dictionaries,
    tuples, lists, and sets. If the first parameter is contained in the second, the
    assertion passes. If not, the assertion fails. The `assertNotIn` method performs
    the inverse check.
  prefs: []
  type: TYPE_NORMAL
- en: The assertIsInstance and assertNotIsInstance methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `assertIsInstance` method checks whether the object passed as the first
    parameter is an instance of the class passed as the second parameter. The `assertNotIsInstance`
    method performs the opposite check, ensuring that the object is not an instance
    of the class.
  prefs: []
  type: TYPE_NORMAL
- en: The assertRaises method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As always, we need to make sure that our units correctly signal errors. Doing
    the right thing when they receive good inputs is only half the job; they need
    to do something reasonable when they receive bad inputs, as well.
  prefs: []
  type: TYPE_NORMAL
- en: The `assertRaises` method checks whether a callable raises a specified exception
    when passed a specified set of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A callable is a function, a method, a class, or an object of any arbitrary type
    that has a `__call__` method.
  prefs: []
  type: TYPE_NORMAL
- en: This assertion only works with callables, which means that you don't have a
    way of checking whether other sorts of expressions raise an expected exception.
    If that doesn't fit the needs of your test, it's possible to construct your own
    test using the `fail` method, described below.
  prefs: []
  type: TYPE_NORMAL
- en: To use `assertRaises`, first pass the expected exception to it, then the callable,
    and then the parameters that should be passed to the callable when it's invoked.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example test using `assertRaises`. This test ought to fail, because
    the callable won''t raise the expected exception. `''8ca2''` is a perfectly acceptable
    input to `int`, when you''re also passing `base = 16` to it. Notice that `assertRaises`
    will accept any number of positional or keyword arguments, and pass them on to
    the callable on invocation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this test, it fails (as we knew it would) because `int` didn''t
    raise the exception we told `assertRaises` to expect. The test fails and reports
    this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If an exception is raised, but it's not the one you told `unittest` to expect,
    then `unittest` considers that as an error. An error is different from a failure.
    A failure means that one of your tests has detected a problem in the unit being
    tested. An error means that there's a problem with the test itself.
  prefs: []
  type: TYPE_NORMAL
- en: The fail method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When all else fails, you can fall back on `fail`. When the code in your test
    calls fail, the test fails.
  prefs: []
  type: TYPE_NORMAL
- en: What good does that do? When none of the `assert` methods do what you need,
    you can instead write your checks in such a way that `fail` will be called if
    the test does not pass. This allows you to use the full expressiveness of Python
    to describe checks for your expectations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at an example. This time, we''re going to test on a less-than
    operation, which isn''t one of the operations directly supported by an `assert`
    method. Using fail, it''s easy to implement the test anyhow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If a particular comparison gets used repeatedly in your tests, you can write
    your own `assert` function for that comparison, using `fail` to report errors
    just as we did in the preceding example.
  prefs: []
  type: TYPE_NORMAL
- en: A couple of things to notice here. First of all, take note of the `not` in the
    `if` statement. Since we want to run `fail` if the test should *not* pass, but
    we're used to describing the circumstances when the test should succeed, a good
    way to write the test is to write the success condition, and then invert it with
    `not`. That way we can continue thinking in the way we're used to when we use
    `fail`. The second thing to note is that you can pass a message to fail when you
    call it; it will be printed out in `unittest` report of failed tests. If you choose
    your message carefully, it can be a big help.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you get it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Take a look at the following `doctest`. Can you work out how the equivalent
    `unittest` would look like?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'That `doctest` code tries to convert a string into an integer; if this conversion
    does not raise a `ValueError`, it reports an error. In `unittest`, that looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: How do you check whether two floating point numbers are equal in `unittest`?
    You should use the `assertAlmostEqual` method, so as not to get tripped by the
    floating point imprecision.
  prefs: []
  type: TYPE_NORMAL
- en: When would you choose to use `assertTrue`? How about `fail`? You would use `assertTrue`
    if none of the more specialized assertions suit your needs. You would use fail
    if you need maximum control when a test succeeds or fails.
  prefs: []
  type: TYPE_NORMAL
- en: Look back at some of the tests we wrote in the previous chapters, and translate
    them from `doctest` into `unittest`. Given what you already know of `unittest`,
    you should be able to translate any of the tests.
  prefs: []
  type: TYPE_NORMAL
- en: While you're doing this, think about the relative merits of `unittest` and `doctest`
    for each of the tests that you translate. The two systems have different strengths,
    so it makes sense that each will be the more appropriate choice for different
    situations. When is `doctest` the better choice, and when is `unittest`?
  prefs: []
  type: TYPE_NORMAL
- en: Test fixtures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `unittest` has an important and highly useful capability that `doctest`
    lacks. You can tell `unittest` how to create a standardized environment for your
    unit tests to run inside, and how to clean up that environment when it's done.
    This ability to create and later destroy a standardized test environment is a
    test fixture. While test fixtures don't actually make any tests possible that
    were impossible before, they can certainly make them shorter and less repetitive.
  prefs: []
  type: TYPE_NORMAL
- en: Example – testing database-backed units
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many programs need to access a database for their operation, which means that
    many of the units these programs are made of also access a database. The point
    is that the purpose of a database is to store information and make it accessible
    in other, arbitrary places; in other words, databases exist to break the isolation
    of units. The same problem applies to other information stores as well: for example,
    files in permanent storage.'
  prefs: []
  type: TYPE_NORMAL
- en: How do we deal with that? After all, just leaving the units that interact with
    the database untested is no solution. We need to create an environment where the
    database connection works as usual, but where any changes that are made do not
    last. There are a few different ways in which we can do this but, no matter what
    the details are, we need to set up the special database connection before each
    test that uses it, and we need to destroy any changes after each such test.
  prefs: []
  type: TYPE_NORMAL
- en: The `unittest` helps us do this by providing test fixtures via the `setUp` and
    `tearDown` methods of the `TestCase` class. These methods exist for us to override,
    with the default versions doing nothing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s some database-using code (let''s say it exists in a file called `employees.py`),
    for which we''re going to write tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The preceding code uses the sqlite3 database that ships with Python. Since the
    `sqlite3` interface is compatible with Python's DB-API 2.0, any database backend
    you find yourself using will have a similar interface to what you see here.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start off by importing the needed modules and introducing our `TestCase`
    subclass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We need a `setUp` method to create the environment that our tests depend on.
    In this case, that means creating a new database connection to an in-memory-only
    database, and populating that database with the needed tables and rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We need a `tearDown` method to undo whatever the `setUp` method did, so that
    each test can run in an untouched version of the environment. Since the database
    is only in memory, all we have to do is close the connection, and it goes away.
    The `tearDown` method may end up being much more complicated in other scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need the tests themselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We just used a `setUp` method in our `TestCase`, along with a matching `tearDown`
    method. Between them, these methods made sure that the environment in which the
    tests were executed was the one they needed (that was `setUp`'s job) and that
    the environment of each test was cleaned up after the test was run, so that the
    tests didn't interfere with each other (this was the job of `tearDown`). The `unittest`
    made sure that `setUp` was run once before each test method, and that `tearDown`
    was run once after each test method.
  prefs: []
  type: TYPE_NORMAL
- en: Because a test fixture—as defined by `setUp` and `tearDown`—gets wrapped around
    every test in a `TestCase` class, the `setUp` and `tearDown` methods for the `TestCase`
    classes that contain too many tests can get very complicated and waste a lot of
    time dealing with details that are unnecessary for some of the tests. You can
    avoid this problem by simply grouping together those tests that require specific
    aspects of the environment into their own `TestCase` classes. Give each `TestCase`
    an appropriate `setUp` and `tearDown`, only dealing with those aspects of the
    environment that are necessary for the tests it contains. You can have as many
    `TestCase` classes as you want, so there's no need to skimp on them when you're
    deciding which tests to group together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice how simple the `tearDown` method we used was. That''s usually a good
    sign: when the changes that need to be undone in the `tearDown` method are simple
    to describe, it often means that you can be sure of doing this perfectly. Since
    any imperfection of the `tearDown` method makes it possible for the tests to leave
    behind stray data that might alter how other tests behave, getting it right is
    important. In this case, all of our changes were confined inside the database,
    so getting rid of the database does the trick.'
  prefs: []
  type: TYPE_NORMAL
- en: We could have used a mock object for the database connection, instead. There's
    nothing wrong with that approach, except that, in this case, it would have been
    more effort for us. Sometimes mock objects are the perfect tool for the job, sometimes
    test fixtures save effort; sometimes you need both to get the job done easily.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter contained a lot of information about how to use the `unittest`
    framework to write your tests.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we covered how to use `unittest` to express concepts you were
    already familiar with from `doctest`; differences and similarities between `unittest`
    and doctest; how to use test fixtures to embed your tests in a controlled and
    temporary environment; and how to use the `unittest.mock` patch to decorate test
    methods to further control the environment the test executes inside.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll look at a tool called Nose that is capable of finding
    and running `doctest` tests, `unittest` tests, and ad hoc tests all in the same
    test run and of providing you with a unified test report.
  prefs: []
  type: TYPE_NORMAL
