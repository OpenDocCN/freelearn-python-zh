- en: Applying Machine Learning in Robotics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provides a hands-on introduction to **machine learning **(**ML**)
    in robotics. Although we assume that you have not yet worked in such a field,
    it will be helpful to have some background in statistics and data analytics. In
    any case, this chapter intends to be a gentle introduction to the topic, favoring
    intuition instead of complex mathematical formulations, and putting the focus
    on understanding the common concepts used in the field of ML.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we will devote the discussion to such concepts by providing
    specific examples of robots. This is somewhat original because most references
    and books on ML give examples oriented to data science. Hence, as you become more
    familiar with robotics, it should be easier for you to understand the concepts
    this way.
  prefs: []
  type: TYPE_NORMAL
- en: With the explanations about deep learning, you will understand how crucial this
    technique is for the robot to acquire knowledge of its surroundings through the
    processing of raw data coming from the robot's camera (2D and/or 3D) and specific
    distance sensors. With the specific example of object recognition explained in
    this chapter, you will learn how raw image data is processed in order to build
    robot's knowledge in the robot, making it capable to take smart actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the system for TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How ML is being applied in Robotics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ML pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A methodology to programmatically apply ML in robotics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning applied to robotics— computer vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concrete application we will do for GoPiGo3 deals with computer vision,
    the most common perception task in robotics. Equipped with this capability, the
    robot should be aware of the objects around itself, making it capable to interact
    with them. As a result of this chapter, we expect you to develop the basic insight
    of when and how to apply deep learning in robotics.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the examples in this chapter, we will use **TensorFlow** ([https://www.tensorflow.org/](https://www.tensorflow.org/)),
    the ML framework open-sourced by Google in 2015, which has become the big brother
    in the data science community because of all of the people involved as active
    developers or end users.
  prefs: []
  type: TYPE_NORMAL
- en: The main TensorFlow API is developed in Python and is the one we are going to
    use. To install it, we need to have the well-known `pip` Python package manager
    in our system. Even though it comes bundled with the Ubuntu OS, we provide the
    instructions for installing it. Later, we will cover the TensorFlow installation
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Let's first provide the path for the code of this chapter, and then describe
    the step-by-step procedure to configure your laptop with TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will make use of the code located in the `Chapter10_Deep_Learning_` folder
    at [https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter10_Deep_Learning_](https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter10_Deep_Learning_).
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy its files to the **ROS**(short for** Robot Operating System**) workspace
    to have them available and leave the rest outside the `src` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This way, you will have a cleaner ROS environment. As usual, you need to rebuild
    the workspace on the laptop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Then, let's start with the setup for TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the system for TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we will set up `pip`, the Python package manager and afterward the framework
    for performing ML, that is, TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Installing pip
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ubuntu distributions typically ship with `pip` preinstalled. Unless a Python
    library requests you to upgrade, you can stay with the same version. In any case,
    we recommend working with the latest one, as explained in the following.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the latest version
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section applies to the case in which you need to install `pip` or upgrade
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, remove the previous version if there is one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We do this because the Ubuntu repository may not have the latest version of
    `pip`. In the next step, you will access the original source to get all of the
    updates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the installation script and execute it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Check which version is installed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If it was already present in your system, you can easily upgrade using `pip`
    itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now, you are ready to proceed with the installation of the ML environment.
  prefs: []
  type: TYPE_NORMAL
- en: Installing TensorFlow and other dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenCV, the well-known and open source computer vision library ([https://opencv.org/](https://opencv.org/)),
    brings to ROS the capability of image processing. It is used by TensorFlow to
    deal with images that you will obtain from the robot camera. To install it in
    your system, you need the `pip` package manager that we explained earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `--user` flag ensures that the package is installed locally to the user
    home folder at `~/.local/lib/python2.7/site-packages`. Otherwise, it should be
    installed system-wide at the `/usr/local/lib/python2.7/dist-packages` path, as
    is the case of `pip` (in such cases, you should perform the installation with `sudo`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The OpenCV ROS bridge ([http://wiki.ros.org/cv_bridge](http://wiki.ros.org/cv_bridge)) ships
    with the full-stack installation of ROS. If, for some reason, the package is missing
    in your environment, you can easily install it with this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For the `<ROS_VERSION>` tag, use the `kinetic` value or `melodic` depending
    on the ROS distribution you have.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, install TensorFlow as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `--upgrade` flag gives you the advantage to update the package if it is
    already installed. If you are working in Ubuntu 16.04, TensorFlow V2 will throw
    compatibility issues. In such a case, install TensorFlow V1 as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In Ubuntu 18.04, you will be ready with the upgraded version of TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving better performance using the GPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alternatively, you could use the GPU version of TensorFlow to take advantage
    of this hardware on your laptop. The **GPU** (short for **Graphical Processing** **Unit**)
    card of your laptop is primarily used to power the display output on the screen.
    Therefore, it is very good at image processing.
  prefs: []
  type: TYPE_NORMAL
- en: As the kinds of calculations we need to do in ML are very similar (that is,
    floating-point, vector, and matrix operations), you can speed up the training
    and usage of your ML models by using the GPU for calculations instead of the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'By using the GPU, you may achieve at least a factor of 10 in speed calculation
    for using the CPU, even in the case of the cheapest GPU cards. Hence, the choice
    of GPU is worth it. The command to install the corresponding TensorFlow library
    in Ubuntu 18.04 is pretty simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, if you are working in Ubuntu 16.04, install TensorFlow V1 to avoid
    compatibility issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: With TensorFlow installed, being the normal version or the GPU-performant one,
    you are ready to use ML within ROS.
  prefs: []
  type: TYPE_NORMAL
- en: ML comes to robotics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML has its roots in statistical science. Remember when you have a cloud of points
    on an x-y frame and try to find the straight line that best fits all of them at
    the same time? This is what we call a linear regression and can be solved with
    a simple analytical formula. **Regression** is the first algorithm that you typically
    study when getting started with ML.
  prefs: []
  type: TYPE_NORMAL
- en: To acquire perspective, be aware that, before 1980, artificial intelligence
    and ML were part of the same corpora of knowledge. Then, artificial intelligence
    researchers focused their efforts on using logical, knowledge-based approaches,
    and ML kept the algorithmic approach, *regression* being the most basic and having
    neural network-based algorithms as its main bundle. Hence, this fact favored that
    ML evolved as a separated discipline.
  prefs: []
  type: TYPE_NORMAL
- en: Following path of the traditional research in neural networks in the '60s and
    '70s, ML kept on developing in this field. Then, its first golden age came in
    the '90s.
  prefs: []
  type: TYPE_NORMAL
- en: However, 25 years ago, the computer resources that a neural network required
    were not within the reach of normal PCs, since a huge amount of data needed to
    be processed to obtain accurate results. It was more than one decade later that
    computing capacity was available to everyone, and then problem-solving based on
    neural network algorithms finally became a commodity.
  prefs: []
  type: TYPE_NORMAL
- en: This fact brings us to the present boom of ML, where functionalities such as
    content recommendation (shops, films, and music) and facial/ object recognition
    (camera-based apps) are used ubiquitously in most modern smartphones.
  prefs: []
  type: TYPE_NORMAL
- en: On the other side, robots started their path in the industry by 1950, being at
    the beginning just mechanical devices that performed repetitive motions. As artificial
    intelligence and its accompanying discipline, ML, developed in parallel, practical
    results in these fields could be transferred, since robots were also powered by
    similar CPUs to those with which ML problems were solved. Then, robots gradually
    acquired the capability to better accomplish actions by being aware of their effects
    in the environment. Data came from the robot's camera and sensors provides feedback
    to the *learning system* that allowed it to perform better every time. This learning
    system is just an ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: And how different is robot learning from human learning? Well, our brain is
    far more efficient. To recognize for the first time whether an animal is a dog,
    a kid just needs four or five samples, while an ML algorithm needs hundreds to
    be accurate in its answers. This is the underlying reason why ML models used by
    robots need to be pretrained with lots of data so that the robot can respond—both
    **accurately** and in **real time**—with a smart action, that is, by picking an
    object from one location and moving it to another previously marked as the target
    (a typical problem in the logistics industry).
  prefs: []
  type: TYPE_NORMAL
- en: This task of identifying objects is what we will do in the practical example
    of this chapter. We will supply the robot with a trained model able to recognize
    different kinds of common objects (balls, mouses, keyboards, and so on) and will
    observe the response when putting it in front of several of these objects. Hence,
    let's keep on explaining the following concepts surrounding this practical example
    regarding the recognition of several kinds of objects.
  prefs: []
  type: TYPE_NORMAL
- en: Core concepts in ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before going into the use case of object recognition in images, let''s take
    a much simpler example, the prediction of the price of a house as a function of
    several independent variables: area, number of rooms, distance to the center of
    the city, population density, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, to have a working ML algorithm, we need an underlying model that, when
    fed with input data, can produce a prediction. The data has to be supplied according
    to the features, that is, independent variables, that we have selected for our
    model. Then, establishing the correspondence with our simple example, we can explain
    the several concepts involved in an ML problem:'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm is the computation as a whole, specified as a sequence of instructions
    or steps that are to be followed to produce a result. All of the instructions
    have to be unambiguous and the actor that is running the algorithm does not have
    to make any additional decision; all of them are covered by the algorithm, which
    specifies what to do at a certain point if a condition needs to be evaluated.
    Then, you can easily infer that an algorithm is something that can be programmed
    in a computer, no matter which language is used. In the case of the example of
    the prediction of the price of a house, the algorithm consists of applying the
    sequence of instructions given sample data—that is, area, number of rooms, and
    so on—to obtain a prediction of its price.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model provides an assumption of the analytical function to apply to the
    input data to obtain a prediction. For example, we can say that the model for
    the price of the house is a linear function of the inputs, that is, given an increment
    in the percentage of the area of the house leads to the same percentual increment
    in its predicted price. For the rest of the independent variables, the same reasoning
    would apply because we have assumed a linear dependence. The model is applied
    in some steps of the algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The features are the independent variables of our model, that is to say, the
    available data that you have to predict the price of a house. In our example,
    these are area, number of rooms, distance to the center of the city, and population
    density.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset is a structured data collection providing values for each of the
    selected features for a large number of items. In our example, the dataset should
    be a table in which each row contains the available data of a concrete house,
    and each column contains the values of each selected feature, that is, a surface
    column, number of rooms column, distance to the center of the city column, population
    density column, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When facing a new problem, the data scientist has to decide for all of these
    three elements: the algorithm, the model, and the features. The last topic, feature
    selection, is where there''s the added value that a human provides to solving
    an ML problem; the rest of the tasks are automated and accomplished by a computer.
    The next subsection explains in detail what features are and emphasizes the importance
    of their selection to obtain accurate predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting features in ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Features in ML constitute a set of characteristics that have to be selected
    by the user, and it is this selection upon which the dataset is built. The expertise
    for making a good feature selection is more a question of experience and insight
    than a structured process. Hence, a good data scientist is one who understands
    the problem and can decompose it in its essential parts to find what the relevant
    features are. These act as the independent variables from which accurate predictions
    can be made.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve an ML problem, it is crucial to perform the right feature selection.
    If you do not detect the relevant features, no matter how much data you put in
    the solver, you will never get a good prediction. As shown in the following diagram,
    we will feed the ML algorithm with a data collection to obtain a result, that
    is, a prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4835e387-64c0-47ef-b82e-3487f14183ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Data collection has been built according to the selected features. For example,
    if you decide to build the model for price prediction of the houses in a given
    city based on three features—area, number of rooms, and distance to the center
    of the city—for every new house you want to predict the price of, you will have
    to feed the algorithm with the specific values of such features, for example,
    85 square meters, 4 rooms, and 1.5 kilometers to the center of the city.
  prefs: []
  type: TYPE_NORMAL
- en: Next, it is crucial to understand how the values of these features are combined
    to obtain the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The ML pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Problem-solving is split into two parts. The first is training the model according
    to the pipeline shown in this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e8f3cab-07f2-491d-99e1-16ab3d6c723a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we are assuming a simple model where the output depends linearly on the
    values of the features, the goal of training consists of determining the weights
    to be applied to each of them to obtain the prediction. Let''s explain it with
    this mathematical formulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Price = W1 * area + W2 * nº rooms + W3 * distance*'
  prefs: []
  type: TYPE_NORMAL
- en: As you may infer, the weights, *W1*, *W2*, and *W3*, are the coefficients that
    multiply each feature. After making the sum of the three products, we obtain the
    predicted price. So, the training phase consists of finding the set of weights
    that best fit the dataset we have available. In the training set, the data contains
    both the features and the actual prices. Hence, by applying the algorithm of least
    square regression ([https://www.statisticshowto.datasciencecentral.com/least-squares-regression-line/](https://www.statisticshowto.datasciencecentral.com/least-squares-regression-line/)),
    we determine the set of values for *W1*, *W2*, and *W3* that best fit all of the
    actual prices supplied. This algorithm guarantees that the resulting equation
    is the one that provides the minimum global error for all of the items used for
    the training.
  prefs: []
  type: TYPE_NORMAL
- en: 'But we do not want to best fit only the supplied data since we already know
    these prices. We wish that the resulting equation also be the best fit for any
    other house for which we do not know the price. So, the way to validate such an
    equation is by using a different dataset, called the test set, from the one we
    used for training. The programmatic way to do this is by splitting the available
    data before performing the training. The typical approach is to make two random
    sets: one containing 70%-90% of the data for training and another with the remaining
    30-10% to perform the validation. This way, the training set provides us with
    the provisional best-fit weights, *W1*, *W2*, and *W3*, and the validation set
    is used to estimate how well our ML model is operationally defined as the least
    square error.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second part corresponds to the prediction itself, that is, when our ML
    algorithm is put in production in a real application. In the prediction (production)
    phase, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57329b5c-ad0c-4a9a-9fb9-8bfdbef7228b.png)'
  prefs: []
  type: TYPE_IMG
- en: The process of ML, in reality, is more a circular one than a linear one because,
    as we get more data for training, we can improve the calculation of the weights,
    and then rewrite the equation with the new set of coefficients, *W1*, *W2*, and *W3*.
    This way, ML is an iterative process that can improve the accuracy of predictions
    as more data is available and the model is retrained again and again.
  prefs: []
  type: TYPE_NORMAL
- en: From ML to deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will understand what deep learning is and how it relates
    to ML. And the most straightforward way to get this insight is by giving a quick
    overview of the most commonly used algorithms. Then, from that perspective, you
    could appreciate why deep learning is the most active area of research nowadays.
  prefs: []
  type: TYPE_NORMAL
- en: ML algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As pointed out in the preceding diagram and explanations, the algorithm is the
    central part of ML problem-solving. A data scientist has also to select which
    one to apply depending on the kind of problem they are facing. So, let's have
    a quick overview of the most commonly used algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Regression tries to find the curve that best fits a cloud of points, and it
    has been described in detail with the case of the prediction of house prices.
    In such a case, we have been talking about a linear dependency, but the algorithm
    can be generalized to any kind of curve that can be represented as a sum of dot
    products between coefficients (weights) and independent variables (features),
    that is, a polynomial. A common case is that of a term that is the square of a
    feature. In this case, the curve is a parabola and, mathematically, can be expressed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y = W1 * x + W2 * x² + W3 * 1*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s review this with a real-life example. Given an independent variable,
    the years of experience of a candidate, we wish to predict what their salary will
    be when applying for a job opportunity. You can easily understand that the dependence
    of the salary, at least during the first years of experience, does not follow
    a linear dependence, that is, a candidate with 2 years will not get twice the
    salary with respect to when he/she had one year of experience. Percentual increments
    in salary will be gradually higher as he/she accumulates more experience. This
    kind of relationship can be modeled as a parabola. Then, from the independent
    variable, *x*, and the salary, we define two features: *x* and *x²*.'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression is used in classification problems, a very common type in
    ML. In this case, we try to predict a binary classification such as pass/fail,
    win/lose, alive/dead, or healthy/sick. This algorithm can be understood as a special
    case of regression, where the predicted variable is categorical, that is, it can
    only take a finite set of values (two if it is a binary classification). The underlying
    model is a probability function and, given a value of the independent variable,
    if the resulting probability is greater than 50%, we predict pass, win, alive,
    or healthy, and if lower, the prediction is the other category, that is, fail,
    lose, dead, or sick.
  prefs: []
  type: TYPE_NORMAL
- en: Product recommendation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Product recommendation is the most used functionality in the consumer sector,
    for example, shopping, watching films, and readings books, taking as input user
    characteristics and well-rated items by other users with similar characteristics.
    There are several algorithms to implement this functionality such as collaborative
    filtering or featurized matrix factorization. If you are interested in this field,
    we provide good introduction references in the *Further reading* section at the
    end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Clusteringis a scenario where we have many items and we want to group them
    by similarity. In this case, items are unlabeled and we ask the algorithm to do
    two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Make a group of similar items.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Label these groups so that new items are both classified and labeled by the
    algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As an example, think of a collection of texts about many topics and you wish
    the algorithm to group similar texts and identify the main topic of each group,
    that is, label them: history, science, literature, philosophy, and so on. One
    of the classical algorithms for this scenario is the nearest neighbor method,
    where you define a metric, calculate it for each pair of items, and group together
    those pairs that are close enough (based on the defined metric). It can be though
    as a distance-like function that is computed between each set of two points.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A **multiclassification scenario**, where there are more than two categories—let''s
    say n—is addressed by solving n logistic regressions where each one performs a
    binary classification for each of the possible categories. For example, if we
    want to detect the dominant color in an image (of four possible categories: red,
    green, blue, or yellow), we can build a classifier consisting of four logistic
    regressions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Red/NOT red
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Green/NOT green
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blue/NOT blue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yellow/NOT yellow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There could be a fifth category, which we can call *unknown*, for cases in which
    the image is not classified in any of the red, green, blue, or yellow colors.
    Finally, this type of multi-logistic regression applied to images is the entrance
    door to the last algorithm, deep learning, on which we will focus from now until
    the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning is the most active research field in ML nowadays. The underlying
    model of this algorithm is a neural network whose way of working tries to mimic
    what the human brain does. Each neuron in the model performs a regression from
    its input with a special function, called **sigmoid**, that provides a sharp but
    continuous probability distribution of the output event. This function is the
    same as that of the probability function used in **logistic regression,** as described
    earlier. In this particular case of a neuron, if the resulting probability is
    greater than 50%, the neuron is activated and feeds another neuron or neurons
    downstream. If lower than 50%, the neuron is not active and hence it has negligible
    influence downstream.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we are going to provide more details about how deep learning works so
    that when you perform the practical exercise with GoPiGo3, you know what it is
    going on under the hood in ROS.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning and neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From now on, we will base our explanations on the practical example of the recognition
    of objects in images, which, in the case of the robot, will be supplied by the
    Raspberry Pi camera. In the following diagram, you can see a representation of
    a neural network that differentiates the three kinds of layers that there can
    be:'
  prefs: []
  type: TYPE_NORMAL
- en: The input layer is where we feed the dataset. Remember that such data has to
    be structured according to the selected features, that is, one neuron per feature.
    We will later discuss this particular and very common case of image datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hidden layer(s)—one or more—are the intermediate steps in the deep learning
    pipeline that extract more features so that the algorithm is more capable of discriminating
    between objects. These hidden features are implicit, and the end user does not
    necessarily need to know about them because their extraction is intrinsic (automatic)
    to the network structure itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output layer provides the prediction. Each neuron provides a logical 1
    if activated (a probability greater than 50%) or a 0 if not activated (lower than
    50%). So, the resulting probability in the output layer will be the answer with
    a certain probability:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/251eae31-f607-45ba-a251-52ce7c9e502e.png)'
  prefs: []
  type: TYPE_IMG
- en: License CC-BY-SA-2.5 source: https://commons.wikimedia.org/wiki/File:Neural_Network.gif
  prefs: []
  type: TYPE_NORMAL
- en: Following a sequential approach, let's explain how a neural network works by
    covering what each layer makes on the supplied input data.
  prefs: []
  type: TYPE_NORMAL
- en: The input layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is the first step of the deep learning pipeline, and the most common structure
    of this layer is to have as many input neurons (features) as three times the number
    of pixels the image has:'
  prefs: []
  type: TYPE_NORMAL
- en: For images of a size of 256 x 256 pixels, this means 65.536 pixels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In general, we will deal with color images, so each pixel will have three channels:
    red, blue, and green; each value stands for the intensity ranging from 0 to 255
    for 8 bits of color depth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, the number of features is *65.536 x 3 = 196.608* and the value of each
    feature will be a number between 0 and 255\. Each feature is represented with
    one neuron in the input layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Afterwards, the neural network is asked to answer this question: is there a
    cat in the picture? And the goal of the next layers is to extract the essential
    aspects of the image to answer that question.'
  prefs: []
  type: TYPE_NORMAL
- en: The hidden layer(s)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For understanding how this layer works, let''s go back to the regression algorithm we
    explain earlier. There, we expressed the predicted variable as a linear combination
    of features—area, number of rooms, and distance to the center multiplied by weights,
    respectively, *W1*, *W2*, and *W3*. Establishing the analogy with our neural network,
    the features would apply to the neurons and the weights to the edges that connect
    each pair of neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b9b28ae-5bc2-4294-90c5-a3a1156d488f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: https://commons.wikimedia.org/wiki/File:Artificial_neural_network_pso.png,
    Cyberbotics Ltd.'
  prefs: []
  type: TYPE_NORMAL
- en: CC BY-SA 3.0 https://creativecommons.org/licenses/by-sa/3.0
  prefs: []
  type: TYPE_NORMAL
- en: The value of each feature would be processed with the sigmoid function of its
    neuron (input layer; *j* neurons) to produce a probability value, *Sij*, which
    is then multiplied by the weight, *Wij*, of the edge that connects it to each
    neuron downstream (hidden layer ; *i* neurons). Hence, the feature input to this
    neuron, *i*, in the hidden layer is a sum of products, there being as many terms
    as neurons are connected to it upstream (input layer ; *j* neurons).
  prefs: []
  type: TYPE_NORMAL
- en: Such a result is the sum over *j* of all of the terms, *Sij*, with the index, *j*,
    which is an iterator that ranges over all of the neurons connected to *i* neurons
    in the input layer. The weights *Wij* of the edges connecting pairs of neurons
    are more properly called **hyperparameters**.
  prefs: []
  type: TYPE_NORMAL
- en: The neural structure of the hidden layers provides what we call intrinsic features,
    which are inherent properties of the network and do not have to be selected by
    the user (they are established by the designer of the neural network). What the
    user has to do is to train the network to obtain the best set of weights, *Wij*,
    that makes the network to as predictive as possible with the available dataset.
    Here is where the magic of deep learning resides because a well-designed architecture
    of layers can provide a very accurate predictive model. The downside is that you
    need a lot of data to get a well-trained network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recapping from the beginning, given an input image, you can calculate the feature
    input to the neurons of each layer, *Fi*, based on the probabilities from the
    previous layer, *Sij*, and the weights, *Wij*, of the edges connecting to neuron
    *i*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Fi = (sum over j) [Sij * Wij]*'
  prefs: []
  type: TYPE_NORMAL
- en: Proceeding downstream layer by layer, you can finally obtain the probabilities
    of the neurons of the output layer and, therefore, answer with the prediction
    of what the analyzed image contains.
  prefs: []
  type: TYPE_NORMAL
- en: As was mentioned earlier, and given that complexity of the network structure,
    you may guess that, for training such a model, you would need much more data than
    for traditional ML algorithms such as regression. More specially, what you have
    to calculate are the values of how many hyperparameters as edges connecting pairs
    of neurons there are. Once you achieve this milestone, you get a trained network
    that can be applied to unlabeled images to predict its content.
  prefs: []
  type: TYPE_NORMAL
- en: The output layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the question of our example, that is, there is a cat in the picture? yes
    if the image shows a cat, or not if it doesn''t. So we only need a neuron in the
    output layer, as shown in the diagram below. Then, if trained with many photos
    of cats, this network could classify an image to say whether it contains a cat
    (1) or not (0). An important point here is that the model should be able to identify the
    cat whatever position it occupies in the image, center, left, right, top, down,
    and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/38dc3fe0-59c4-4718-9f10-86034d2c1d88.png)'
  prefs: []
  type: TYPE_IMG
- en: Source: https://commons.wikimedia.org/wiki/File:NeuralNetwork.png
  prefs: []
  type: TYPE_NORMAL
- en: If we need to classify 10 kinds of objects (several types of pets, for example),
    we would need an output layer with 10 neurons. The result of the computation of
    the network would be a vector with 10 probabilities—each one linked to each neuron,
    and the one that provides the largest value (the closest to 100%) would tell us
    what kind of pet there is in the input image with more probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, you can make the network more complex and add more output neurons
    (and possibly more hidden layers) to obtain more details of the images. Consider
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify whether there is one cat or two or more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Identify characteristics of the face, such as whether the eyes and/or mouth
    are open or closed:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/58274728-a20d-4abb-bcfd-cb5a5a82d307.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: https://www.flickr.com/photos/55855622@N06/5173363938 by jeici1, License:
    CC BY 2.0'
  prefs: []
  type: TYPE_NORMAL
- en: This is a quite complex topic and beyond of the scope of this introductory chapter,
    whose goal is just to provide a descriptive understanding of what deep learning
    is and how it works. Anyway, the reader is encouraged to delve deeper into the
    topic, and for that, two didactic references are included in the *Further reading*
    section at the end of this chapter: *Intuitive Deep Learning Parts 1 and 2*.
  prefs: []
  type: TYPE_NORMAL
- en: From this point, we move to the practical part and start by stating a general
    methodology to tackle ML problems in robotics.
  prefs: []
  type: TYPE_NORMAL
- en: A methodology to programmatically apply ML in robotics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A specific aspect of ML is that robot responses have to happen in real time,
    without delays, so that the actions taken are effective. For example, if it finds
    an obstacle crossing the path it is following, we expect that it avoids it. To
    do so, obstacle identification has to occur as it appears in the robot's field
    of view. Hence, the subsequent action of avoiding the obstacle has to be taken
    immediately to avoid a crash.
  prefs: []
  type: TYPE_NORMAL
- en: We will support our methodology description with an end-to-end example that
    covers all that GoPiGo3 can do up to this point. Then, with this example, we expect
    that GoPiGo3 can carry a load on top of its chassis from its current location
    to a target location (a common case in garbage collector robots).
  prefs: []
  type: TYPE_NORMAL
- en: A general approach to application programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps involved in solving this challenge are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Determine what high-level tasks are involved.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: List the atomic tasks that, put together, are capable of accomplishing the high-level
    tasks. This is the level at which we create our program in ROS, writing node scripts
    and launch files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Program the robot application by adapting the algorithms of the high-level tasks
    to the specific situation we are trying to solve.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we provide a breakdown of each of these steps so that we can implement
    the functionality in the real robot:'
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the high-level tasks to be carried out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**SLAM**: This is **Simultaneous Localization and Mapping **(**SLAM**) to build
    a map of the actual environment.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Navigation**: Setting a target pose, GoPiGo3 can move autonomously until
    achieving it.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visual recognition**: GoPiGo3 can identify where it has to be placed so that
    the garbage it carries can be collected.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'List the atomic tasks that are involved in the example. Let''s say that, to
    be successful, GoPiGo3 has to be able to do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load a map of the environment.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate an optimum path to achieve the target location given the information
    from the map.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Start navigating toward the goal.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Avoid obstacles found along the path.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Stop if unexpected conditions are found in the environment that do not let it
    advance anymore. Then, ask for help.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: After receiving help, resume the path to the target location.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Recognize the garbage store entrance and stop at the exact position where a
    hoist will hook the loaded garbage.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Program the robot application. Each of the preceding atomic tasks will correspond
    to a ROS node script, which can be expressed as a launch file with just one `<node>`
    tag. Then, you have to put these seven nodes on a ROS graph and draw the edges
    that should connect pairs using topics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each published topic, you should determine which frequency the topic should
    be published with so that the robot can react quickly enough. For example, since
    the typical speed of GoPiGo3 is 1 m/s, we wish the scan distance to be updated
    10 times every 1 m traveled. This means that the robot will receive a perception
    update every 10 cm(=0.1 m) traveled and will be able to detect the presence of
    obstacles outside of a circumference of 0.1 m radius. The minimum publishing rate
    so that the robot can react to avoid the obstacle is calculated with this simple
    formula: *(1 m/s) /0.1 m = 10 Hz*.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For each topic a node is subscribed to the code should trigger a robot action
    that allows it to successfully adapt to such conditions in the environment. For
    example, given the topic providing distances around GoPiGo3, when its value is
    below a threshold, 20 cm, for example (you will see now where this number comes
    from), GoPiGo3 recalculates the local path to avoid the obstacle. We should select
    this threshold according to the 10 Hz rate of publishing we decided previously;
    remember that this rate came from the fact that the robot will receive a perception
    update every 10 cm traveling. Taking a safety factor of 2, the threshold is simply
    *10 cm * 2 = 20 cm*, providing room and time so that it avoids the obstacle.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: There's no need for ML currently now for atomic tasks 1 through 6\. But when
    it comes to aligning with the garbage stop entrance, GoPiGo3 needs to know not
    only its pose but also its relative position to the entrance, so that the hoist
    can successfully hook the loaded garbage.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating an ML task
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This node of step 7 formulates its functionality as* recognize the garbage
    store entrance and stop at the exact position where a hoist will hook the loaded
    garbage*. Hence, the Pi camera comes to the rescue and image recognition capability
    has to be included in the logic programming of this node. This logic can be briefly
    expressed as publishing the `cmd_vel` messages to robot differential drives that
    allow GoPiGo3 to be put right in place. So, it is a feedback mechanism between
    visual perception, that is, entrance shape alignment in the image or not, and
    a motion command to correct and center:'
  prefs: []
  type: TYPE_NORMAL
- en: If the entrance is shifted to the left in the image, the robot should rotate
    left.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And if deviated to the right, it should rotate right an angle proportional to
    the distance from the entrance to the center of the image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And your very first question should be: how can we integrate such an ML task
    with our robotic application? And the answer comes to enlighten how the ROS publish/subscribe
    mechanism is both powerful and simple at the same time. Its neutral nature allows
    us to integrate any kind of task that can be packaged into a black box by adhering
    to the following two rules:'
  prefs: []
  type: TYPE_NORMAL
- en: Input is supplied via a subscribed topic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output is delivered using a published topic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the concrete case of ML applied to center the robot in the entrance door,
    we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Input to the ML node (subscribed topic) is the image feed from the Pi camera.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output from the ML node (published topic) is the horizontal distance from the
    shape of the door to the center of the image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, the GoPiGo3 drive node takes that output topic as the data to determine
    which `cmd_vel` command should be sent to the motors. This establishes a feedback
    loop with the ML node that makes it possible that the robot position converges
    to get finally centered in the entrance door:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1e991b3-3f41-4469-b7b5-6eb06bbf72e2.png)'
  prefs: []
  type: TYPE_IMG
- en: The ML published topic, `object_position`, is an integer that provides the distance
    in pixels from the centroid of the object (entrance door) to the center of the
    image frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it is out of the scope of this chapter, it is good to know at this
    point that ROS provides other interaction mechanisms between nodes, and the programmer''s
    choice about which one to use depends on the specific functionality to be implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: A ROS service is the classical implementation of the server/client architecture.
    The client node (*drive node*) makes a request to the server node (*ML node*) and
    this performs the calculation (the distance in pixels from the entrance door to
    the center of the image frame). Then, the response is sent back to the client.
    The key difference with the publish/ subscribe mechanism is that this is not expecting
    to receive requests; it publishes messages at the rate set within the code of
    the node, independently, whether other nodes are listening or not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A ROS action is similar to a ROS service, that is, it provides a response to
    a request from a node, with the difference that, in this case, the client node
    does not block the execution (until it receives the answer). That is to say, it
    keeps executing other code instructions and, when it receives the response, the
    client triggers the programmed action (rotates the robot for alignment). This
    behavior is called asynchronous, unlike a ROS service, which is synchronous in
    nature, that is, it blocks the node execution until the response is received.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, let's dive into how to make GoPiGo3 aware of the objects it has around,
    and we will do this in the final section of this chapter where we will build a
    general ML node that is able to detect a wide range of object types.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning applied to robotics – computer vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The practical part of this chapter consists of operationally implementing the
    ML node described earlier. What we represented there as a black box is developed
    now as a ROS package that you may integrate with the functionalities you discovered
    in previous chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: The remote control in [Chapter 7](0653ab6b-8710-41e7-9c01-5024865e3e27.xhtml),
    *Robot Control and Simulation**,* for both the virtual robot in Gazebo and the
    physical GoPiGo3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robot navigation for a virtual robot in [Chapter 8](25ac032c-5bfe-47ff-aa5a-f178dbff7c57.xhtml),
    *Virtual SLAM and Navigation Using Gazebo*, and the physical GoPiGo3 in [Chapter
    9](7b6ae4e6-2cd1-44e5-8f11-459b83987f42.xhtml), *SLAM for Robot Navigation*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, we divide this section into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: The first section, *Object recognition in Gazebo*, provides you with the tools
    to integrate the ML node for image recognition in Gazebo so that, after finishing
    the practice, you may let your creativity fly to combine object recognition with
    any of the drive nodes from **remote control** or **robot navigation** and make
    the virtual robot smarter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second section, *Object recognition in the real world*, provides the same
    integration with the physical GoPiGo3 and you will discover the ML node black
    box is the same no matter where the images come from, that is, objects in Gazebo
    or the real world. The choice is made by you when linking the ML node subscription
    to images of any of those scenarios.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This procedure also gives an operational way to test a new robot application.
    Start with the validation in Gazebo, where you will mainly check that the developed
    code has no significant bugs and the robot works as expected; then, proceed with
    it to the real world—understand how all of the external variables that are not
    present in Gazebo act on the robot, see how it responds, and then decide which
    code refinements you need to make to get it to work.
  prefs: []
  type: TYPE_NORMAL
- en: Object recognition in Gazebo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To get the code, follow the instructions we provided at the beginning of this
    chapter under the section, *Technical requirements*. The exercise in Gazebo is
    going to be pretty simple and very effective at the same time. You will check
    how the virtual GoPiGo3 can recognize a common *tennis ball* from the image feed
    coming from the robot''s camera:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by spawning a model of the ball in Gazebo:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, launch a `rqt_image_view` node to watch the subjective view as perceived
    from the robot''s camera:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Click on the top-left empty box, and select ;`/gopigo/camera1/image_raw` topic.
    Then, you will see the subjective view of the robot as acquired by its front camera.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, spawn a model of the ball in Gazebo:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Bear in mind that the `models_spawn_library` package requires you to execute
    the launch file as superuser. As soon as the ball is spawned in Gazebo, the process
    finishes and `T3` is released.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, launch the remote control node so that you can control GoPiGo with the
    keyboard as usual:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This package was installed in [Chapter 7](0653ab6b-8710-41e7-9c01-5024865e3e27.xhtml),
    *Robot Control and Simulation*. If you did not install it, do so now. The source
    of this ROS package is at [https://github.com/ros-teleop/teleop_tools](https://github.com/ros-teleop/teleop_tools).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, launch the image recognition node and watch the screen output. Use
    `T3` where you already have `sudo` enabled:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You can get a more condensed feed by subscribing to the `/result` topic, which
    provides just the name of the recognized objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'See the composition of the following screenshots showing how the tennis ball
    is recognized in the Terminal window (bottom-left side):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ac87985-ba8e-41f4-9353-8d06e7198b33.png)'
  prefs: []
  type: TYPE_IMG
- en: Is it easy to replicate? We expect so. Now,let's proceed to repeat the process
    with the physical robot.
  prefs: []
  type: TYPE_NORMAL
- en: Object recognition in the real world
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, remember to point the ROS master URI to the robot as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Apply this for every new Terminal in the laptop, or include the line in the
    `.bashrc` file. The physical robot configuration is as shown here, with GoPiGo3
    in front of a small yellow ball:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce2fdd35-516e-4f94-ab6f-48371dbc66bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Run the following two commands in two independent Terminals in the Raspberry
    Pi:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The packages you are using in the preceding are the ones in [Chapter 6](0b20bdff-f1dc-42e8-ae83-fc290da31381.xhtml),
    *Programming in ROS- Commands and Tools*. So make sure you did not delete them,
    and if so, get them back. In the laptop is where you run the new packages to perform
    image recognition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `image_transport-` package (you can find its ROS wiki page at [http://wiki.ros.org/image_transport](http://wiki.ros.org/image_transport))
    is commonly used in ROS to provide transparent support for transmitting images
    in low-bandwidth compressed formats.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, `T1` makes `raspicam_node/image`—output from `r2`—available in raw format,
    that is, the `/raspicam_node/image_raw` topic, the output of `T1`. This facilitates
    the image feed, which can then be processed later by `start_image_recognition.launch`.
    At this point, it is very useful to look at the ROS graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b6c683c-4a22-47ca-afb4-6c0501ae747f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Remember that this visualization is launched with the `rqt_graph` command in
    another Terminal. Find that the transport operation is carried out by the `image_republisher_157...` node. Then,
    launch a `rqt_image_view` node to watch the subjective view as perceived through
    the Pi camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In the pop-up window, you have to select the `/raspicam_node/image_raw` topic to
    get the subjective view from the Pi camera.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, as we did in simulation, launch the image recognition node and subscribe
    to the `/result` topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The only difference for the Gazebo scenario is that you have to remap the topic
    supplied by the Pi camera with `raspicam_node`, to the topic named `rgb_image_topic`,
    which is the one accepted by the image recognition node.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have presented three different objects to the robot successively: the yellow
    ball, the mouse, and the monitor. Find out how the three of them are recognized
    by the robot in real time. Is it surprising?'
  prefs: []
  type: TYPE_NORMAL
- en: 'The yellow ball can be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c83af262-0045-4c88-8622-397ac4e6c7d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, the mouse can be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e7577bb-ef5c-4d6a-81c9-297f98f9988e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And, finally, the monitor can be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e976bec-8aaf-4ae4-b05e-19596bdbc209.png)'
  prefs: []
  type: TYPE_IMG
- en: If you have arrived at this point, you are in a good position to start creating
    advanced applications in ROS that integrate object recognition as an ability that
    uses GoPiGo3 to execute smart actions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provided a quick introduction to ML in robotics. We expect you
    to have acquired insight into what ML and deep learning are, qualitatively understood
    how a neural network processes images to recognize objects, and can operationally
    implement the algorithm in a simulated and/or physical robot.
  prefs: []
  type: TYPE_NORMAL
- en: ML is a very wide field and you should not expect nor really need to get an
    expert in the field. What you need to assimilate is the knowledge to integrate
    deep learning capabilities in your robots.
  prefs: []
  type: TYPE_NORMAL
- en: As you have seen in the practical case, we have used a pretrained model that
    covers common objects. Then, we have simply used this model and have not needed
    additional training. There are plenty of trained models on the web shared by data
    science companies and open source developers. You should spend time looking for
    these models, and only go to train your own models when the scenario that the
    robot is facing is so specific that general-purpose ML models do not cover it
    with decent accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In the final two chapters, we will focus on reinforcement learning, a task that
    is complementary to the deep learning technique described in this chapter. With
    the latter, the robot gets the perception of the environment, and with the former,
    it chains several actions oriented to a goal.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the task for solving ML that requires more experience and insight from
    the data scientist?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) The algorithm selection
  prefs: []
  type: TYPE_NORMAL
- en: B) The feature selection
  prefs: []
  type: TYPE_NORMAL
- en: C) The model
  prefs: []
  type: TYPE_NORMAL
- en: What is the relationship between ML and deep learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) ML covers many algorithms and deep learning only algorithms to find deep
    features.
  prefs: []
  type: TYPE_NORMAL
- en: B) Deep learning is a subset of ML.
  prefs: []
  type: TYPE_NORMAL
- en: C) Deep learning deals with all of the ML algorithms except neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: How should you integrate an ML task with a ROS application?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) You should train the model outside and then provide ROS with a file of results.
  prefs: []
  type: TYPE_NORMAL
- en: B) You have the choice of using publish/subscribe, a ROS service, or an action
    server.
  prefs: []
  type: TYPE_NORMAL
- en: C) You have to use the specific communication protocol of the ML model.
  prefs: []
  type: TYPE_NORMAL
- en: What is the main difference between the publish/subscribe mechanism and the
    ROS service mechanism?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) ROS service is synchronous while publish/subscribe is asynchronous.
  prefs: []
  type: TYPE_NORMAL
- en: B) ROS service is asynchronous while publish/subscribe is synchronous.
  prefs: []
  type: TYPE_NORMAL
- en: C) Publish/subscribe does not need to receive requests from other nodes in order
    to publish messages.
  prefs: []
  type: TYPE_NORMAL
- en: If the practical example explained in the *Deep learning applied to robotics
    – computer vision* section was carried out with a red ball instead of a yellow
    one, will the prediction with the same model we are using?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A) Yes, the color is not a feature for object shape recognition.
  prefs: []
  type: TYPE_NORMAL
- en: B) Yes, and in addition to identifying a ball, it will also tell that it is
    red.
  prefs: []
  type: TYPE_NORMAL
- en: C) It depends on whether the model was trained with balls of different colors.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To delve deeper into the concepts explained in this chapter, you can check
    out the following references:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A Brief History of ML:* [https://www.dataversity.net/a-brief-history-of-machine-learning](https://www.dataversity.net/a-brief-history-of-machine-learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Brief History of Robotics Since 1950:* [https://www.encyclopedia.com/science/encyclopedias-almanacs-transcripts-and-maps/brief-history-robotics-1950](https://www.encyclopedia.com/science/encyclopedias-almanacs-transcripts-and-maps/brief-history-robotics-1950)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ML for Recommender systems -Part 1 (algorithms, evaluation and cold start):* [https://medium.com/recombee-blog/machine-learning-for-recommender-systems-part-1-algorithms-evaluation-and-cold-start-6f696683d0ed](https://medium.com/recombee-blog/machine-learning-for-recommender-systems-part-1-algorithms-evaluation-and-cold-start-6f696683d0ed)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ML for Recommender systems - Part 2 (Deep Recommendation, Sequence Prediction,
    AutoML, and Reinforcement Learning in Recommendation):* [https://medium.com/recombee-blog/machine-learning-for-recommender-systems-part-2-deep-recommendation-sequence-prediction-automl-f134bc79d66b](https://medium.com/recombee-blog/machine-learning-for-recommender-systems-part-2-deep-recommendation-sequence-prediction-automl-f134bc79d66b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Intuitive Deep Learning Part 1a: Introduction to Neural Networks:* [https://medium.com/intuitive-deep-learning/intuitive-deep-learning-part-1a-introduction-to-neural-networks-d7b16ebf6b99](https://medium.com/intuitive-deep-learning/intuitive-deep-learning-part-1a-introduction-to-neural-networks-d7b16ebf6b99)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Intuitive Deep Learning Part 2: CNNs for Computer Vision:* [https://medium.com/intuitive-deep-learning/intuitive-deep-learning-part-2-cnns-for-computer-vision-24992d050a27](https://medium.com/intuitive-deep-learning/intuitive-deep-learning-part-2-cnns-for-computer-vision-24992d050a27)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Build your first Convolutional Neural Network to recognize images:* [https://medium.com/intuitive-deep-learning/build-your-first-convolutional-neural-network-to-recognize-images-84b9c78fe0ce](https://medium.com/intuitive-deep-learning/build-your-first-convolutional-neural-network-to-recognize-images-84b9c78fe0ce)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
