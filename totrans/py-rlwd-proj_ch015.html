<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<meta charset="utf-8"/>
<meta content="pandoc" name="generator"/>
<title>ch015.xhtml</title>

<!-- kobo-style -->
<style id="koboSpanStyle" type="text/css" xmlns="http://www.w3.org/1999/xhtml">.koboSpan { -webkit-text-combine: inherit; }</style>
</head>
<body epub:type="bodymatter">

<h1 data-number="15">Chapter 11<br/>
Project 3.7: Interim Data Persistence</h1>
<p>Our goal is to create files of clean, converted data we can then use for further analysis. To an extent, the goal of creating a file of clean data has been a part of all of the previous chapters. We’ve avoided looking deeply at the interim results of acquisition and cleaning. This chapter formalizes some of the processing that was quietly assumed in those earlier chapters. In this chapter, we’ll look more closely at two topics:</p>
<ul>
<li><p>File formats and data persistence</p></li>
<li><p>The architecture of applications</p></li>
</ul>
<p></p>

<h2 data-number="15.1">11.1  Description</h2>
<p>In the previous chapters, particularly those starting with <a href="ch013.xhtml#x1-2080009"><em>Chapter</em><em> 9</em></a>, <a href="ch013.xhtml#x1-2080009"><em>Project 3.1:</em> <em>Data Cleaning Base Application</em></a>, the question of ”persistence” was dealt with casually. The previous chapters all wrote the cleaned samples into a file in ND JSON format. This saved delving into the alternatives and the various choices available. It’s time to review the previous projects and consider the choice of file format for persistence.</p>
<p>What’s important is the overall flow of data from acquisition to analysis. The conceptual flow of data is shown in <a href="#11.1"><em>Figure 11.1</em></a>.</p>
<figure class="IMG---Figure">
<img alt="Figure 11.1: Data Analysis Pipeline " src="img/file51.jpg"/>
<figcaption class="IMG---Caption">Figure 11.1: Data Analysis Pipeline </figcaption>
</figure>
<p>This differs from the diagram shown in <a href="ch006.xhtml#x1-470002"><em>Chapter</em><em> 2</em></a>, <a href="ch006.xhtml#x1-470002"><em>Overview of the Projects</em></a>, where the stages were not quite as well defined. Some experience with acquiring and cleaning data helps to clarify the considerations around saving and working with data.</p>
<p>The diagram shows a few of the many choices for persisting interim data. A more complete list of format choices includes the following:</p>
<ul>
<li><p>CSV</p></li>
<li><p>TOML</p></li>
<li><p>JSON</p></li>
<li><p>Pickle</p></li>
<li><p>A SQL database</p></li>
<li><p>YAML</p></li>
</ul>
<p>There are others, but this list contains formats that enjoy a direct implementation in Python. Note that YAML is popular but isn’t a built-in feature of the Python standard library. Additional formats include protocol buffers ( <a class="url" href="https://protobuf.dev">https://protobuf.dev</a>) and Parquet ( <a class="url" href="https://parquet.apache.org">https://parquet.apache.org</a>). These two formats require a bit more work to define the structure before serializing and deserializing Python data; we’ll leave them out of this discussion.</p>
<p>The CSV format has two disadvantages. The most notable of these problems is the representation of all data types as simple strings. This means any type of conversion information must be offered in metadata outside the CSV file. The <strong>Pydantic </strong>package provides the needed metadata in the form of a class definition, making this format tolerable. The secondary problem is the lack of a deeper structure to the data. This forces the files to have a flat sequence of primitive attributes.</p>
<p>The JSON format doesn’t—directly—serialize datetime or timedelta objects. To make this work reliably, additional metadata is required to deserialize these types from supported JSON values like text or numbers. This missing feature is provided by the <strong>Pydantic </strong>package and works elegantly. A <code>datetime.datetime</code> object will serialize as a string, and the type information in the class definition is used to properly parse the string. Similarly, a <code>datetime.timedelta</code> is serialized as a float number but converted — correctly — into a <code>datetime.timedelta</code> based on the type information in the class definition.</p>
<p>The TOML format has one advantage over the JSON format. Specifically, the TOML format has a tidy way to serialize datetime objects, a capability the JSON library lacks. The TOML format has the disadvantage of not offering a direct way to put multiple TOML documents into a single file. This limits TOML’s ability to handle vast datasets. Using a TOML file with a simple array of values limits the application to the amount of data that can fit into memory.</p>
<p>The pickle format can be used with the <strong>Pydantic </strong>package. This format has the advantage of preserving all of the Python-type information and is also very compact. Unlike JSON, CSV, or TOML, it’s not human-friendly and can be difficult to read. The <code>shelve</code> module permits the building of a handy database file with multiple pickled objects that can be saved and reused. While it’s technically possible to execute arbitrary code when reading a pickle file, the pipeline of acquisition and cleansing applications does not involve any unknown agencies providing data of unknown provenance.</p>
<p>A SQL database is also supported by the <strong>Pydantic </strong>package by using an ORM model. This means defining two models in parallel. One model is for the ORM layer (for example, SQLAlchemy) to create table definitions. The other model, a subclass of <code>pydantic.BaseModel</code>, uses native <strong>Pydantic </strong>features. The <strong>Pydantic </strong>class will have a <code>from_orm()</code> method to create native objects from the ORM layer, performing validation and cleaning.</p>
<p>The YAML format offers the ability to serialize arbitrary Python objects, a capability that makes it easy to persist native Python objects. It also raises security questions. If care is taken to avoid working with uploaded YAML files from insecure sources, the ability to serialize arbitrary Python code is less of a potential security problem.</p>
<p>Of these file formats, the richest set of capabilities seems to be available via JSON. Since we’ll often want to record many individual samples in a single file, <strong>newline-delimited </strong>(<strong>ND</strong>) JSON seems to be ideal.</p>
<p>In some situations — particularly where spreadsheets will be used for analysis purposes — the CSV format offers some value. The idea of moving from a sophisticated Jupyter Notebook to a spreadsheet is not something we endorse. The lack of automated test capabilities for spreadsheets suggests they are not suitable for automated data processing. </p>


<h2 data-number="15.2">11.2  Overall approach</h2>
<p>For reference see <a href="ch013.xhtml#x1-2080009"><em>Chapter</em><em> 9</em></a>, <a href="ch013.xhtml#x1-2080009"><em>Project 3.1: Data Cleaning Base Application</em></a>, specifically <a href="ch013.xhtml#x1-2150002"><em>Approach</em></a>. This suggests that the <code>clean</code> module should have minimal changes from the earlier version.</p>
<p>A cleaning application will have several separate views of the data. There are at least four viewpoints:</p>
<ul>
<li><p>The source data. This is the original data as managed by the upstream applications. In an enterprise context, this may be a transactional database with business records that are precious and part of day-to-day operations. The data model reflects considerations of those day-to-day operations.</p></li>
<li><p>Data acquisition interim data, usually in a text-centric format. We’ve suggested using ND JSON for this because it allows a tidy dictionary-like collection of name-value pairs, and supports quite complex Python data structures. In some cases, we may perform some summarization of this raw data to standardize scores. This data may be used to diagnose and debug problems with upstream sources. It’s also possible that this data only exists in a shared buffer as part of a pipeline between an acquire and a cleaning application.</p></li>
<li><p>Cleaned analysis data, using native Python data types including <code>datetime</code>, <code>timedelta</code>, <code>int</code>, <code>float</code>, and <code>boolean</code>. These are supplemented with <strong>Pydantic </strong>class definitions that act as metadata for proper interpretation of the values. These will be used by people to support decision-making. They may be used to train AI models used to automate some decision-making.</p></li>
<li><p>The decision-maker’s understanding of the available information. This viewpoint tends to dominate discussions with users when trying to gather, organize, and present data. In many cases, the user’s understanding grows and adapts quickly as data is presented, leading to a shifting landscape of needs. This requires a great deal of flexibility to provide the right data to the right person at the right time.</p></li>
</ul>
<p>The <strong>acquire </strong>application overlaps with two of these models: it consumes the source data and produces an interim representation. The <strong>clean </strong>application also overlaps two of these models: it consumes the interim representation and produces the analysis model objects. It’s essential to distinguish these models and to use explicit, formal mappings between them.</p>
<p>This need for a clear separation and obvious mappings is the primary reason why we suggest including a “builder” method in a model class. Often we’ve called it something like <code>from_row()</code> or <code>from_dict()</code> or something that suggests the model instance is built from some other source of data via explicit assignment of individual attributes.</p>
<p>Conceptually, each model has a pattern similar to the one shown in the following snippet:</p>
<p>-</p>
<div><div><pre class="source-code">class Example:
        field_1: SomeElementType
        field_2: AnotherType

        @classmethod
        def from_source(cls, source: SomeRowType) -&gt; "Example":
                return Example(
                        field_1=transform_1(source),
                        field_2=transform_2(source),
                )</pre>
</div>
</div>
<p>The transformation functions, <code>transform1()</code> and <code>transform2()</code>, are often implicit when using <code>pydantic.BaseModel</code>. This is a helpful simplification of this design pattern. The essential idea, however, doesn’t change, since we’re often rearranging, combining, and splitting source fields to create useful data.</p>
<p>When the final output format is either CSV or JSON, there are two helpful methods of <code>pydantic.BaseModel</code>. These methods are <code>dict()</code> and <code>json()</code>. The <code>dict()</code> method creates a native Python dictionary that can be used by a <code>csv.DictWriter</code> instance to write CSV output. The <code>json()</code> method can be used directly to write data in ND JSON format. It’s imperative for ND JSON to make sure the <code>indent</code> value used by the <code>json.dump()</code> function is <code>None</code>. Any other value for the <code>indent</code> parameter will create multi-line JSON objects, breaking the ND JSON file format.</p>
<p>The <strong>acquire </strong>application often has to wrestle with the complication of data sources that are unreliable. The application should save some history from each attempt to acquire data and acquire only the ”missing” data, avoiding the overhead of rereading perfectly good data. This can become complicated if there’s no easy way to make a request for a subset of data.</p>
<p>When working with APIs, for example, there’s a <code>Last-Modified</code> header that can help identify new data. The <code>If-Modified-Since</code> header on a request can avoid reading data that’s unchanged. Similarly, the <code>Range</code> header might be supported by an API to permit retrieving parts of a document after a connection is dropped.</p>
<p>When working with SQL databases, some variants of the <code>SELECT</code> statement permit <code>LIMIT</code> and <code>OFFSET</code> clauses to retrieve data on separate pages. Tracking the pages of data can simplify restarting a long-running query.</p>
<p>Similarly, the <strong>clean </strong>application needs to avoid re-processing data in the unlikely event that it doesn’t finish and needs to be restarted. For very large datasets, this might mean scanning the previous, incomplete output to determine where to begin cleaning raw data to avoid re-processing rows.</p>
<p>We can think of these operations as being “idempotent” in the cases when they have run completely and correctly. We want to be able to run (and re-run) the <strong>acquire </strong>application without damaging intermediate result files. We also want an additional feature of adding to the file until it’s correct and complete. (This isn’t precisely the definition of “idempotent”; we should limit the term to illustrate that correct and complete files are not damaged by re-running an application.) Similarly, the <strong>clean </strong>application should be designed so it can be run — and re-run — until all problems are resolved without overwriting or reprocessing useful results. </p>

<h3 data-number="15.2.1">11.2.1  Designing idempotent operations</h3>
<p>Ideally, our applications present a UX that can be summarized as ”pick up where they left off.” The application will check for output files, and avoid destroying previously acquired or cleaned data.</p>
<p>For many of the carefully curated Kaggle data sets, there will be no change to the source data. A time-consuming download can be avoided by examining metadata via the Kaggle API to decide if a file previously downloaded is complete and still valid.</p>
<p>For enterprise data, in a constant state of flux, the processing must have an explicit ”as-of date” or ”operational date” provided as a run-time parameter. A common way to make this date (or date-and-time) evident is to make it part of a file’s metadata. The most visible location is the file’s name. We might have a file named <code>2023-12-31-manufacturing-orders.ndj</code>, where the as-of date is clearly part of the file name.</p>
<p>Idempotency requires programs in the data acquisition and cleaning pipeline to check for existing output files and avoid overwriting them unless an explicit command-line option permits overwriting. It also requires an application to read through the output file to find out how many rows it contains. This number of existing rows can be used to tailor the processing to avoid re-processing existing rows.</p>
<p>Consider an application that reads from a database to acquire raw data. The ”as-of-date” is 2022-01-18, for example. When the application runs and something goes wrong in the network, the database connection could be lost after processing a subset of rows. We’ll imagine the output file has 42 rows written before the network failure caused the application to crash.</p>
<p>When the log is checked and it’s clear the application failed, it can be re-run. The program can check the output directory and find the file with 42 rows, meaning the application is being run in recovery mode. There should be two important changes to behavior:</p>
<ul>
<li><p>Add a <code>LIMIT</code><code> -1</code><code> OFFSET</code><code> 42</code> clause to the <code>SELECT</code> statement to skip the 42 rows already retrieved. (For many databases, <code>LIMIT</code><code> -1</code><code> OFFSET</code><code> 0</code> will retrieve all rows; this can be used as a default value.)</p></li>
<li><p>Open the output file in ”append” mode to add new records to the end of the existing file.</p></li>
</ul>
<p>These two changes permit the application to be restarted as many times as required to query all of the required data.</p>
<p>For other data sources, there may not be a simple ”limit-offset” parameter in the query. This may lead to an application that reads and ignores some number of records before processing the remaining records. When the output file doesn’t exist, the offset before processing has a value of zero.</p>
<p>It’s important to handle date-time ranges correctly.</p>
<div><div><p>It’s imperative to make sure date and date-time ranges are properly <strong>half-open intervals</strong>. The starting date and time are included. The ending date and time are excluded.</p>
<p>Consider a weekly extract of data.</p>
<p>One range is 2023-01-14 to 2023-01-21. The 14th is included. The 21st is not included. The next week, the range is 2023-01-21 to 2023-01-28. The 21st is included in this extract.</p>
<p>Using half-open intervals makes it easier to be sure no date is accidentally omitted or duplicated.</p>
</div>
</div>
<p>Now that we’ve considered the approach to writing the interim data, we can look at the deliverables for this project. </p>



<h2 data-number="15.3">11.3  Deliverables</h2>
<p>The refactoring of existing applications to formalize the interim file formats leads to changes in existing projects. These changes will ripple through to unit test changes. There should not be any acceptance test changes when refactoring the data model modules.</p>
<p>Adding a ”pick up where you left off” feature, on the other hand, will lead to changes in the application behavior. This will be reflected in the acceptance test suite, as well as unit tests.</p>
<p>The deliverables depend on which projects you’ve completed, and which modules need revision. We’ll look at some of the considerations for these deliverables. </p>

<h3 data-number="15.3.1">11.3.1  Unit test</h3>
<p>A function that creates an output file will need to have test cases with two distinct fixtures. One fixture will have a version of the output file, and the other fixture will have no output file. These fixtures can be built on top of the <code>pytest.tmp_path</code> fixture. This fixture provides a unique temporary directory that can be populated with files needed to confirm that existing files are appended to instead of overwritten.</p>
<p>Some test cases will need to confirm that existing files were properly extended. Other test cases will confirm that the file is properly created when it didn’t exist. An edge case is the presence of a file of length zero — it was created, but no data was written. This can be challenging when there is no previous data to read to discover the previous state.</p>
<p>Another edge case is the presence of a damaged, incomplete row of data at the end of the file. This requires some clever use of the <code>seek()</code> and <code>tell()</code> methods of an open file to selectively overwrite the incomplete final record of the file. One approach is to use the <code>tell()</code> method before reading each sample. If an exception is raised by the file’s parser, seek to the last reported <code>tell()</code> position, and start writing there. </p>


<h3 data-number="15.3.2">11.3.2  Acceptance test</h3>
<p>The acceptance test scenarios will require an unreliable source of data. Looking back at <a href="ch008.xhtml#x1-780004"><em>Chapter</em><em> 4</em></a>, <a href="ch008.xhtml#x1-780004"><em>Data Acquisition Features: Web APIs and Scraping</em></a>, specifically <a href="ch008.xhtml#x1-910003"><em>Acceptance tests</em></a>, we can see the acceptance test suite involves using the <code>bottle</code> project to create a very small web service.</p>
<p>There are two aspects to the scenarios, each with different outcomes. The two aspects are:</p>
<ol>
<li><div><p>The service or database provides all results or it fails to provide a complete set of results.</p>
</div></li>
<li><div><p>The working files are not present — we could call this the “clean start” mode — or partial files exist and the application is working in recovery mode.</p>
</div></li>
</ol>
<p>Since each aspect has two alternatives, there are four combinations of scenarios for this feature:</p>
<ol>
<li><div><p>The existing scenario is where the working directory is empty and the API or database works correctly. All rows are properly saved.</p>
</div></li>
<li><div><p>A new scenario where the working directory is empty and the service or database returns a partial result. The returned rows are saved, but the results are marked as incomplete, perhaps with an error entry in the log.</p>
</div></li>
<li><div><p>A new scenario where the given working directory has partial results and the API or database works correctly. The new rows are appended to existing rows, leading to a complete result.</p>
</div></li>
<li><div><p>A new scenario where the given working directory has partial results and the service or database returns a partial result. The cumulative collection of rows are usable, but the results are still marked as incomplete.</p>
</div></li>
</ol>
<p>A version of the mock RESTful process can return some rows and even after that return 502 status codes. The database version of the incomplete results scenarios is challenging because SQLite is quite difficult to crash at run-time. Rather than try to create a version of SQLite that times out or crashes, it’s better to rely on unit testing with a mock database to be sure crashes are handled properly. The four acceptance test scenarios will demonstrate that working files are extended without being overwritten. </p>


<h3 data-number="15.3.3">11.3.3  Cleaned up re-runnable application design</h3>
<p>The final application with the ”pick-up-where-you-left-off” feature can be very handy for creating robust, reliable analytic tools. The question of ”what do we do to recover?” should involve little (or no) thought.</p>
<p>Creating “idempotent” applications, in general, permits rugged and reliable processing. When an application doesn’t work, the root cause must be found and fixed, and the application can be run again to finish the otherwise unfinished work from the failed attempt. This lets analysts focus on what went wrong — and fixing that — instead of having to figure out how to finish the processing. </p>



<h2 data-number="15.4">11.4  Summary</h2>
<p>In this chapter, we looked at two important parts of the data acquisition pipeline:</p>
<ul>
<li><p>File formats and data persistence</p></li>
<li><p>The architecture of applications</p></li>
</ul>
<p>There are many file formats available for Python data. It seems like newline delimited (ND) JSON is, perhaps, the best way to handle large files of complex records. It fits well with Pydantic’s capabilities, and the data can be processed readily by Jupyter Notebook applications.</p>
<p>The capability to retry a failed operation without losing existing data can be helpful when working with large data extractions and slow processing. It can be very helpful to be able to re-run the data acquisition without having to wait while previously processed data is processed again. </p>


<h2 data-number="15.5">11.5  Extras</h2>
<p>Here are some ideas for you to add to these projects. </p>

<h3 data-number="15.5.1">11.5.1  Using a SQL database</h3>
<p>Using a SQL database for cleaned analytical data can be part of a comprehensive database-centric data warehouse. The implementation, when based on <strong>Pydantic</strong>, requires the native Python classes as well as the ORM classes that map to the database.</p>
<p>It also requires some care in handling repeated queries for enterprise data. In the ordinary file system, file names can have processing dates. In the database, this is more commonly assigned to an attribute of the data. This means multiple time periods of data occupy a single table, distinguished by the ”as-of” date for the rows.</p>
<p>A common database optimization is to provide a “time dimension” table. For each date, the associated date of the week, fiscal weeks, month, quarter, and year is provided as an attribute. Using this table saves computing any attributes of a date. It also allows the enterprise fiscal calendar to be used to make sure that 13-week quarters are used properly, instead of the fairly arbitrary calendar month boundaries.</p>
<p>This kind of additional processing isn’t required but must be considered when thinking about using a relational database for analysis data.</p>
<p>This extra project can use SQLAlchemy to define an ORM layer for a SQLite database. The ORM layer can be used to create tables and write rows of analysis data to those tables. This permits using SQL queries to examine the analysis data, and possibly use complex <code>SELECT-GROUP</code><code> BY</code> queries to perform some analytic processing. </p>


<h3 data-number="15.5.2">11.5.2  Persistence with NoSQL databases</h3>
<p>There are many NoSQL databases available. A number of products like MongoDB use a JSON-based document store. Database engines like PostgreSQL and SQLite3 have the capability of storing JSON text in a column of a database table. We’ll narrow our focus onto JSON-based databases as a way to avoid looking at the vast number of databases available.</p>
<p>We can use SQLite3 BLOB columns to store JSON text, creating a NoSQL database using the SQLite3 storage engine.</p>
<p>A small table with two columns: <code>doc_id</code>, and <code>doc_text</code>, can create a NoSQL-like database. The SQL definition would look like this:</p>
<div><div><pre class="source-code">CREATE TABLE IF NOT EXISTS document(
        doc_id INTEGER PRIMARY KEY,
        doc_text BLOB
)</pre>
</div>
</div>
<p>This table will have a primary key column that’s populated automatically with integer values. It has a text field that can hold the serialized text of a JSON document.</p>
<p>The SQLite3 function <code>json()</code> should be used when inserting JSON documents:</p>
<div><div><pre class="source-code">INSERT INTO document(doc_text) VALUES(json(:json_text))</pre>
</div>
</div>
<p>This will confirm the supplied value of <code>json_text</code> is valid JSON, and will also minimize the storage, removing needless whitespace. This statement is generally executed with the parameter <code>{"json_text":</code><code> json.dumps(document)</code> to convert a native Python document into JSON text so it can then be persisted into the database.</p>
<p>The attributes of a JSON object can be interrogated using the SQLite <code>-&gt;&gt;</code> operator to extract a field from a JSON document. A query for a document with a named field that has a specific value will look like this:</p>
<div><div><pre class="source-code">SELECT doc_text FROM document WHERE doc_text -&gt;&gt; ’field’ = :value</pre>
</div>
</div>
<p>In the above SQL, the field’s name, <code>field</code>, is fixed as part of the SQL. This can be done when the schema is designed to support only a few queries. In the more general case, the field name might be provided as a parameter value, leading to a query like the following:</p>
<div><div><pre class="source-code">SELECT doc_text FROM document WHERE doc_text -&gt;&gt; :name = :value</pre>
</div>
</div>
<p>This query requires a small dictionary with the keys ”name” and ”value”, which will provide the field name and field value used to locate matching documents.</p>
<p>This kind of database design lets us write processing that’s similar to some of the capabilities of a document store without the overhead of installing a document store database. The JSON documents can be inserted into this document store. The query syntax uses a few SQL keywords as overhead, but the bulk of the processing can be JSON-based interrogation of documents to locate the desired subset of available documents.</p>
<p>The idea here is to use a JSON-based document store instead of a file in ND JSON format. The Document Store interface to SQLite3 should be a module that can be reused in a JupyterLab Notebook to acquire and analyze data. While unit tests are required for the database interface, there are a few changes to the acceptance test suite required to confirm this changed design. </p>



</body>
</html>
