- en: Chapter 8. Scaling Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will try to understand how we can make our program work
    for more inputs by making the program scalable. We will do this by both optimizing
    and adding computing power to the system. We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Going multithreaded
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using multiple processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going asynchronous
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling horizontally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The major reason a system is not able to scale is state. Events can change the
    state of a system permanently for both that request or further requests from that
    endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Normally state is stored in the database, and reactions to events are worked
    on sequentially, and changes to state due to events are then stored in DB.
  prefs: []
  type: TYPE_NORMAL
- en: 'Task can be computation intensive (CPU load) or IO bound in which system needs
    answers from some other entity. Here, `taskg` and `taskng` are GIL and nonGIL
    versions of a task. The `taskng` task is in C module compiled via SWIG by enabling
    its threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, I have created a test server that responds to requests after 1
    sec. To create a comparison for scenarios, we first create a simple serial program.
    As expected, both IO and CPU tasks time add up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Outputs can be easily summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling Python](img/B04885_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Going multithreaded
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Key 1: Using threads to process in parallel.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how threads can help us in improving performance. In Python, due
    to Global Interpreter Lock, only one thread runs at a given time. Also, context
    is switched as all of them are given a chance to run. Hence, this is load in addition
    to computation. Hence, CPU-intensive tasks should take the same or more time.
    IO tasks are not doing anything but waiting, so they will get the boost. In the
    following code segment, `threaded_iotask` and `threaded_cputask` are two functions
    that are executed using separate threads. The code is run for various values to
    get results. The process function invokes multiple threads for tasks and sums
    up the timings taken:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Plotting various results onscreen, we can easily see that threading is helping
    IO tasks but not CPU tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Going multithreaded](img/B04885_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As discussed, this is due to GIL. Our CPU task is defined in C, we can give
    up GIL to see whether that helps. The following is plot of run with no GIL for
    tasks. We can see that CPU tasks are now taking a lot less time than before. But,
    GIL is there for a reason. If we give up GIL, atomicity for data structures is
    not guaranteed as there may be two threads working on same data structure at a
    given time.
  prefs: []
  type: TYPE_NORMAL
- en: '![Going multithreaded](img/B04885_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Using multiple processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Key 2: Churning CPU-intensive tasks.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiple processes are helpful to fully utilize all CPU cores. It helps in
    CPU-intensive work as tasks are run in separate processes, and there is no GIL
    between actual working processes. The setup and communication cost between processes
    is higher than threads. In the following code section, `proc_iotask`, `proc_cputask`
    are processes that run them for various inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following diagram, we can see the multiple IO operations are getting
    a boost from multiprocessing. CPU tasks are also getting a boost from multiprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using multiple processes](img/B04885_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we compare all four: serial, threads, threads without GIL, and multiprocesses,
    we will observe that threads without GIL and multiprocesses are taking almost
    the same time. Also, serial and threads are taking the same time, which shows
    little benefit of using threads in CPU-intensive tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using multiple processes](img/B04885_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Going asynchronous
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Key 3: Being asynchronous for parallel execution.**'
  prefs: []
  type: TYPE_NORMAL
- en: We can also process more than one request by being asynchronous as well. In
    this method, instead of us polling for updates from objects, they tell us when
    they have a result. Hence, the main thread in the meantime can execute other stuff.
    Asyncio, Twisted, and Tornado are libraries in Python that can help us write such
    code. Asyncio, and Tornado are supported in Python 3, and some portions of Twisted
    also run on Python 3 as of now. Python 3.5 introduced the `async` and `await`
    keywords that helps write asynchronous code. The `async` keyword defines that
    the function is an asynchronous function and that the result may not be available
    right away. The `await` keyword waits until the results are captured and returns
    the result.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, `await` in the main function waits for all the results
    to be available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Plotting results on the graph, we can see that we got a boost on the IO portion,
    but for CPU-intensive work, it takes time similar to serial:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Going asynchronous](img/B04885_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'CPU tasks are blocking everything, hence, this is a bad design. We have to
    use either threads, or better multiprocessing to help in CPU-intensive tasks.
    To run tasks in threads or processes, we can use `ThreadPoolExecutor`, and `ProcessPoolExecutor`
    from the `concurrent.futures` package. The following is the code for `ThreadPoolExecutor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'For `ProcessPoolExecutor`, we have to use a multiprocessing queue to collect
    results back, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Plotting the results, we can see that threads are taking more or less the same
    time as without them, but they can still help make the program more responsive
    as the program will be able to perform other IO tasks in the meantime. Multiprocessing
    gives the max boost:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Going asynchronous](img/B04885_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Async systems are used mostly when IO is the main thing. As you can see, it
    is similar to serial for CPU. Let''s now take a look at which one is better, threading
    or `async`, for our scalable IO-based application. We used the same IO task but
    on higher loads. Asyncio gives failures and takes more time than threads. I tested
    this on Python 3.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Going asynchronous](img/B04885_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The last advice will be to look at other implementations as well, such as PyPy,
    Jython, IronPython, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling horizontally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we add further nodes to the application, it must add to the total processing
    power. To create frontend systems that perform more data transmission than computation,
    `async` frameworks are better suited. If we use PyPy, it will give a performance
    boost to the application. Code for Python 3 or Python 2 compatibility using six
    or other such libraries so that we can use anything available for optimization.
  prefs: []
  type: TYPE_NORMAL
- en: We can use message pack or JSON for message transfer. I prefer JSON for language
    agnostic and easy-text representation. Workers can be multiprocessing workers
    for CPU-bound tasks or thread-based for other scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: The system should not store the state but pass it with messages. Everything
    doesn't need to be in DB. We can take some things out when not necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'ZeroMQ (messageQueue): ZMQ is a wonderful library that acts as a glue to connect
    your programs together. It has connectors for almost all language. You can easily
    use multiple languages/frameworks to enable their communication with ZMQ and among
    themselves. It also provides tools to create various utilities. Let''s now look
    at how we can create a load-balanced worker system easily using ZMQ. In the following
    code snippet, we created a client (requester) that can ask for a result from a
    group of servers (workers) that are load balanced. In the following code, we can
    see the socket type is `DEALER`. Sockets in ZMQ can be thought of as mini servers.
    The `req` sockets do not actually transmit until they get a response for the previous
    one. `DEALER` and `ROUTER` sockets are better suited for real-life scenarios.
    The code for synchronization is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the code for servers or actual workers. We can have many of
    them and the load is distributed in a round-robin fashion among them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the result from the run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We can use the third-party package `Supervisord` to make workers restart on
    failure.
  prefs: []
  type: TYPE_NORMAL
- en: The real power of ZMQ is in creating network architecture and nodes as required
    by the project from simpler components. You can test the framework easily as it
    can support IPC, TCP, UDP, and many more protocols. They can also be used interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: There are other libraries/frameworks as well that can help a lot in this space,
    such as NSQ, Python parallel. Many projects go for RabbitMQ as the broker and
    AMQP as the protocol. Choosing good communication is very important for the design
    and scalability of a system, and it depends on the project requirement.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Making a program scalable is easy if we separate portions of program and use
    each part tuned for best performance. In this chapter, we saw how various portions
    of Python help in vertical as well as horizontal scaling. All this information
    must be taken into consideration when designing architecture of the application.
  prefs: []
  type: TYPE_NORMAL
