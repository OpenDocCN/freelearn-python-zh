<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Scrapy</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Scrapy</h1>
            </header>

            <article>
                
<p><strong>Scrapy</strong> is a popular web scraping and crawling framework utilizing high-level functionality to make scraping websites easier. In this chapter, we will get to know Scrapy by using it to scrape the example website, just as we did in <a href="py-web-scrp-2e_ch02.html" target="_blank"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Scraping the Data</em>. Then, we will cover <strong>Portia</strong>, which is an application based on Scrapy which allows you to scrape a website through a point and click interface.</p>
<p>In this chapter we will cover the following topics:</p>
<ul>
<li>Getting started with Scrapy</li>
<li>Creating a Spider</li>
<li>Comparing different spider types</li>
<li>Crawling with Scrapy</li>
<li>Visual Scraping with Portia</li>
<li>Automated Scraping with Scrapely</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Installing Scrapy</h1>
            </header>

            <article>
                
<p>Scrapy can be installed with the <kbd>pip</kbd> command, as follows:</p>
<pre><strong>pip install scrapy</strong>
</pre>
<p>Scrapy relies on some external libraries, so if you have trouble installing it there is additional information available on the official website at: <a href="http://doc.scrapy.org/en/latest/intro/install.html" target="_blank"><span class="URLPACKT">http://doc.scrapy.org/en/latest/intro/install.html</span></a>.</p>
<p>If Scrapy is installed correctly, a <kbd>scrapy</kbd> command will now be available in the terminal:</p>
<pre><strong>$ scrapy</strong><br/><strong>    Scrapy 1.3.3 - no active project</strong><br/><br/><strong>Usage:</strong><br/><strong>  scrapy &lt;command&gt; [options] [args]</strong><br/><br/><strong>Available commands:</strong><br/><strong>        bench    Run quick benchmark test</strong><br/><strong>        commands </strong><br/><strong>        fetch    Fetch a URL using the Scrapy downloader</strong><br/><strong>...</strong>
</pre>
<p>We will use the following commands in this chapter:</p>
<ul>
<li><kbd>startproject</kbd>: Creates a new project</li>
<li><kbd>genspider</kbd>: Generates a new spider from a template</li>
<li><kbd>crawl</kbd>: Runs a spider</li>
<li><kbd>shell</kbd>: Starts the interactive scraping console</li>
</ul>
<div class="packt_infobox">For detailed information about these and other commands available, refer to <a href="http://doc.scrapy.org/en/latest/topics/commands.html" target="_blank"><span class="URLPACKT">http://doc.scrapy.org/en/latest/topics/commands.html</span></a></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Starting a project</h1>
            </header>

            <article>
                
<p>Now that Scrapy is installed, we can run the <kbd>startproject</kbd> command to generate the default structure for our first Scrapy&#160;project.</p>
<p>To do this, open the terminal and navigate to the directory where you want to store your Scrapy project, and then run <kbd>scrapy startproject &lt;project name&gt;</kbd>. Here, we will use <kbd>example</kbd> for the project name:</p>
<pre><strong>$ scrapy startproject example</strong><br/><strong>$ cd example</strong>
</pre>
<p>Here are the files generated by the <kbd>scrapy</kbd> command:</p>
<pre>    scrapy.cfg <br/>    example/ <br/>        __init__.py   <br/>        items.py<br/>        middlewares.py   <br/>        pipelines.py   <br/>        settings.py   <br/>        spiders/ <br/>            __init__.py 
</pre>
<p>The important files for this chapter (and in general for Scrapy use) are as follows:</p>
<ul>
<li><kbd>items.py</kbd>: This file defines a model of the fields that will be scraped</li>
<li><kbd>settings.py</kbd>: This file defines settings, such as the user agent and crawl delay</li>
<li><kbd>spiders/</kbd>: The actual scraping and crawling code are stored in this directory</li>
</ul>
<p>Additionally, Scrapy uses <kbd>scrapy.cfg</kbd> for project configuration,&#160;<kbd>pipelines.py</kbd> to process the scraped fields and <kbd>middlewares.</kbd><kbd>py</kbd> to control request and response middleware, but they will not need to be modified for&#160;this example.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Defining a model</h1>
            </header>

            <article>
                
<p>By default, <kbd>example/items.py</kbd> contains the following code:</p>
<pre># -*- coding: utf-8 -*- <br/># Define here the models for your scraped items<br/>#<br/># See documentation in:<br/># http://doc.scrapy.org/en/latest/topics/items.html<br/><br/>import scrapy <br/><br/>class ExampleItem(scrapy.Item): <br/>    # define the fields for your item here like: <br/>    # name = scrapy.Field() <br/>    pass 
</pre>
<p>The <kbd>ExampleItem</kbd> class is a template which needs to be replaced with the details we'd like to extract from the example country page. For now, we will just scrape the country name and population, rather than all the country details. Here is an updated model to support this:</p>
<pre>class CountryItem(scrapy.Item): <br/>    name = scrapy.Field() <br/>    population = scrapy.Field() 
</pre>
<div class="packt_infobox">Full documentation for defining items is available at <a href="http://doc.scrapy.org/en/latest/topics/items.html" target="_blank"><span class="URLPACKT">http://doc.scrapy.org/en/latest/topics/items.html</span></a></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Creating a spider</h1>
            </header>

            <article>
                
<p>Now, we can build the actual crawling and scraping code, known as a <strong>spider</strong> in Scrapy. An initial template can be generated with the <kbd>genspider</kbd> command, which takes the name you want to call the spider, the domain, and an optional template:</p>
<pre>    <strong>$ scrapy genspider country example.webscraping.com --template=crawl</strong>
</pre>
<p>We used the built-in <kbd>crawl</kbd> template which&#160;utilizes the Scrapy library's&#160;<kbd>CrawlSpider</kbd>. A Scrapy&#160;<kbd>CrawlSpider</kbd>&#160;has special attributes and methods available&#160;when crawling the web rather than a simple&#160;scraping spider.</p>
<p>After running the <kbd>genspider</kbd> command, the following code is&#160;generated in <kbd>example/spiders/country.py</kbd>:</p>
<pre># -*- coding: utf-8 -*-<br/>import scrapy <br/>from scrapy.linkextractors import LinkExtractor <br/>from scrapy.spiders import CrawlSpider, Rule <br/><br/>class CountrySpider(CrawlSpider): <br/>    name = 'country' <br/>    allowed_domains = ['example.webscraping.com'] <br/>    start_urls = ['http://example.webscraping.com']<br/><br/>    rules = ( <br/>        Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True), <br/>    ) <br/><br/>    def parse_item(self, response): <br/>        i = {} <br/>        #i['domain_id'] = response.xpath('//input[@id="sid"]/@value').extract() <br/>        #i['name'] = response.xpath('//div[@id="name"]').extract() <br/>        #i['description'] = response.xpath('//div[@id="description"]').extract() <br/>        return i 
</pre>
<p>The initial lines import the required Scrapy libraries and encoding definition. Then, a class is created for the spider, which contains the following&#160;class attributes:</p>
<ul>
<li><kbd>name</kbd>: A&#160;string to identify the spider</li>
<li><kbd>allowed_domains</kbd>: A&#160;list of the domains that can be crawled -- if this isn't&#160;set,&#160;any domain can be crawled</li>
<li><kbd>start_urls</kbd>: A&#160;list of URLs to begin&#160;the crawl.&#160;</li>
<li><kbd>rules</kbd>: This attribute is a tuple of <kbd>Rule</kbd> objects defined by&#160;regular expressions which tell the crawler what links to follow and what links have useful content to scrape</li>
</ul>
<p>You will notice the&#160;defined&#160;<kbd>Rule</kbd> has a <kbd>callback</kbd> attribute which sets the callback to&#160;<kbd>parse_item</kbd>, the&#160;method defined just below. This method is the main data extraction method for&#160;<kbd>CrawlSpider</kbd> objects, and the generated Scrapy code within that method has an example of extracting content from the page.</p>
<p>Because Scrapy is a high-level framework, there is a lot going on here in only a few lines of code. The official documentation has further details about building spiders, and can be found at <a href="http://doc.scrapy.org/en/latest/topics/spiders.html" target="_blank"><span class="URLPACKT">http://doc.scrapy.org/en/latest/topics/spiders.html</span></a>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Tuning settings</h1>
            </header>

            <article>
                
<p>Before running the generated crawl spider, the Scrapy settings should be updated to avoid the spider being blocked. By default, Scrapy allows up to 16&#160;concurrent downloads for a domain with no delay between downloads, which is much faster than a real user would browse. This behavior is easy&#160;for a server to detect and block.</p>
<p>As mentioned in Chapter 1, the example website we are scraping is configured to temporarily block crawlers which consistently download at faster than one request per second, so the default settings would ensure&#160;our spider is&#160;blocked. Unless you are running the example website locally, I recommend adding these lines to <kbd>example/settings.py</kbd> so the crawler only downloads a single request per domain at a time with a reasonable 5 second delay between downloads:</p>
<pre>CONCURRENT_REQUESTS_PER_DOMAIN = 1 <br/>DOWNLOAD_DELAY = 5 
</pre>
<p>You can also search and find those settings in the documentation, modify and uncomment them with the above values. Note that Scrapy will not use this precise delay between requests, because this would also make a crawler easier to detect and block. Instead, it adds a random offset within this delay&#160;between requests.</p>
<div class="packt_infobox">For details about these settings and the many others available, refer to <a href="http://doc.scrapy.org/en/latest/topics/settings.html" target="_blank"><span class="URLPACKT">http://doc.scrapy.org/en/latest/topics/settings.html</span></a>.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Testing the spider</h1>
            </header>

            <article>
                
<p>To run a spider from the command line, the <kbd>crawl</kbd> command is used along with the name of the spider:</p>
<pre>    <strong>$ scrapy crawl country -s LOG_LEVEL=ERROR</strong><br/><strong>$</strong>
</pre>
<p>The script runs to completion with no output. Take note of the <kbd>-s LOG_LEVEL=ERROR</kbd> flag-this is a Scrapy setting and is equivalent to defining <kbd>LOG_LEVEL = 'ERROR'</kbd> in the <kbd>settings.py</kbd> file. By default, Scrapy will output all log messages to the terminal, so here the log level was raised to isolate error messages. Here, no output means our spider completed without error -- great!</p>
<p>In order to actually scrape some content from the pages, we need to add a few lines to the spider file. To ensure we can start building and extracting our items, we have to first start using our <kbd>CountryItem</kbd> and also update our crawler rules. Here is an updated version of the spider:</p>
<pre>from example.items import CountryItem<br/>    ...<br/><br/>    rules = ( <br/>        Rule(LinkExtractor(allow=r'/index/'), follow=True), <br/>        Rule(LinkExtractor(allow=r'/view/'), callback='parse_item') <br/>    ) <br/><br/>    def parse_item():<br/>        i = CountryItem()<br/>        ...
</pre>
<p>In order to extract structured data, we should use our&#160;<kbd>CountryItem</kbd> class which we created. In this added code, we are importing the class and instantiating an object as the <kbd>i</kbd> (or item) in our&#160;<kbd>parse_item</kbd> method.&#160;</p>
<p>Additionally, we need to add rules so our spider can find data and extract it. The default rule searched the url pattern <kbd>r'/Items'</kbd> which is not matched on the example site. Instead, we can create two new rules from what we know already about the site.&#160;The first rule will crawl the index pages and follow their links, and the second rule will crawl the country pages and pass the downloaded response to the <kbd>callback</kbd> function for scraping.</p>
<p>Let's see what happens when this improved spider is run with the log level set to <kbd>DEBUG</kbd> to show more crawling messages:</p>
<pre><strong>$ scrapy crawl country -s LOG_LEVEL=DEBUG</strong><br/><strong>...</strong><br/><strong>2017-03-24 11:52:42 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET     http://example.webscraping.com/view/Belize-23&gt; (referer: http://example.webscraping.com/index/2)</strong><br/><strong>2017-03-24 11:52:49 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://example.webscraping.com/view/Belgium-22&gt; (referer: http://example.webscraping.com/index/2)</strong><br/><strong>2017-03-24 11:52:53 [scrapy.extensions.logstats] INFO: Crawled 40 pages (at 10 pages/min), scraped 0 items (at 0 items/min)</strong><br/><strong>2017-03-24 11:52:56 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://example.webscraping.com/user/login?_next=%2Findex%2F0&gt; (referer: http://example.webscraping.com/index/0)</strong><br/><strong>2017-03-24 11:53:03 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://example.webscraping.com/user/register?_next=%2Findex%2F0&gt; (referer: http://example.webscraping.com/index/0)</strong><br/><strong>...<br/></strong>
</pre>
<p>This log output shows the index pages and countries are being crawled and duplicate links are filtered, which is handy. We can also see our installed middlewares and other important information output when we first start the crawler.</p>
<p>However, we also notice the spider is wasting resources by crawling the login and register forms linked from each web page, because they match the <kbd>rules</kbd> regular expressions. The login URL in the preceding command ends with <kbd>_next=%2Findex%2F1</kbd>, which is a URL encoding equivalent to <kbd>_next=/index/1</kbd>, defining a post-login redirect. To prevent these URLs from being crawled, we can use the <kbd>deny</kbd> parameter of the rules, which also expects a regular expression and will prevent crawling every&#160;matching URL.</p>
<p>Here is an updated version of code to prevent crawling the user login and registration forms by avoiding the URLs containing <kbd>/user/</kbd>:</p>
<pre>    rules = ( <br/>        Rule(LinkExtractor(allow=r'/index/', deny=r'/user/'), follow=True), <br/>        Rule(LinkExtractor(allow=r'/view/', deny=r'/user/'), callback='parse_item') <br/>    ) 
</pre>
<div class="packt_infobox"><span>Further documentation about how to use the LinkExtractor class is available at</span> <a href="http://doc.scrapy.org/en/latest/topics/link-extractors.html" target="_blank"><span class="URLPACKT">http://doc.scrapy.org/en/latest/topics/link-extractors.html</span></a><span>.</span></div>
<p>To stop the current crawl and restart with the new code, you can send a quit signal using <em>Ctrl</em> + <em>C</em> or <em>cmd</em> + <em>C</em>. You should then see a message similar to this one:</p>
<pre>2017-03-24 11:56:03 [scrapy.crawler] INFO: Received SIG_SETMASK, shutting down gracefully. Send again to force 
</pre>
<p><span>It will finish queued requests and then stop. You'll see some extra statistics and debugging at the end, which we will cover later in this section.</span></p>
<div class="packt_infobox">In addition to adding deny rules to the crawler, you can use the&#160;<kbd>process_links</kbd> argument for the&#160;<kbd>Rule</kbd> object. This allow you to create a function which iterates through the found links and makes any modifications (such as removing or adding parts of query strings). More information about crawling rules is available in the documentation:&#160;<a href="https://doc.scrapy.org/en/latest/topics/spiders.html#crawling-rules" target="_blank">https://doc.scrapy.org/en/latest/topics/spiders.html#crawling-rules</a></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Different Spider Types</h1>
            </header>

            <article>
                
<p>In this Scrapy example, we have utilized the Scrapy&#160;<kbd>CrawlSpider</kbd>, which is particularly useful when crawling a website or series of websites. Scrapy has several other spiders you may want to use depending on the site and your extraction needs. These spiders fall under the following categories:</p>
<ul>
<li><kbd>Spider</kbd>: A normal scraping spider. This is usually used for just scraping one type of page.</li>
<li><kbd>CrawlSpider</kbd>: A crawl spider; usually used for traversing a domain and scraping one (or several) types of pages from the pages it finds by crawling links.</li>
<li><kbd>XMLFeedSpider</kbd>: A spider which traverses an XML feed and extracts content from each node.</li>
<li><kbd>CSVFeedSpider</kbd>: Similar to the XML spider, but instead can parse CSV rows within the feed.</li>
<li><kbd>SitemapSpider</kbd>: A spider which can crawl a site with differing rules by first parsing the Sitemap.</li>
</ul>
<p>Each of these spiders are included in your default Scrapy installation, so you can&#160;access them whenever you may want to build a new web scraper. In this chapter, we'll finish building our first crawl spider as a&#160;first&#160;example of how to use Scrapy tools.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Scraping with the shell command</h1>
            </header>

            <article>
                
<p>Now that Scrapy can crawl the countries, we can&#160;define what data to&#160;scrape. To help test how to extract data from a web page, Scrapy comes with a handy command called <kbd>shell</kbd>&#160;which presents us with the Scrapy API via an Python or IPython interpreter.</p>
<p>We can call the command using the URL we would like to start with, like so:&#160;</p>
<pre><strong>$ scrapy shell http://example.webscraping.com/view/United-Kingdom-239<br/></strong>...<br/><strong>[s] Available Scrapy objects:</strong><br/><strong>[s] scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)</strong><br/><strong>[s] crawler    &lt;scrapy.crawler.Crawler object at 0x7fd18a669cc0&gt;</strong><br/><strong>[s] item       {}</strong><br/><strong>[s] request    &lt;GET http://example.webscraping.com/view/United-Kingdom-239&gt;</strong><br/><strong>[s] response   &lt;200 http://example.webscraping.com/view/United-Kingdom-239&gt;</strong><br/><strong>[s] settings   &lt;scrapy.settings.Settings object at 0x7fd189655940&gt;</strong><br/><strong>[s] spider     &lt;CountrySpider 'country' at 0x7fd1893dd320&gt;</strong><br/><strong>[s] Useful shortcuts:</strong><br/><strong>[s] fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)</strong><br/><strong>[s] fetch(req)                  Fetch a scrapy.Request and update local objects </strong><br/><strong>[s] shelp()                     Shell help (print this help)</strong><br/><strong>[s] view(response)              View response in a browser<br/>In [1]: </strong>
</pre>
<p>We can now query the <kbd>response</kbd>&#160;object to check what data is available.</p>
<pre><strong>In [1]: response.url </strong><br/><strong>Out[1]:'http://example.webscraping.com/view/United-Kingdom-239' <br/></strong><br/><strong>In [2]: response.status </strong><br/><strong>Out[2]: 200</strong> 
</pre>
<p>Scrapy uses <kbd>lxml</kbd> to scrape data, so we can use the same CSS selectors as those in <a href="py-web-scrp-2e_ch02.html" target="_blank"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Scraping the Data</em>:</p>
<pre><strong>In [3]: response.css('tr#places_country__row td.w2p_fw::text') </strong><br/><strong>[&lt;Selector xpath=u"descendant-or-self:: </strong><br/><strong>    tr[@id = 'places_country__row']/descendant-or-self:: </strong><br/><strong>    */td[@class and contains( </strong><br/><strong>    concat(' ', normalize-space(@class), ' '), </strong><br/><strong>    ' w2p_fw ')]/text()" data=u'United Kingdom'&gt;]</strong> 
</pre>
<p>The&#160;method returns a list with an <kbd>lxml</kbd> selector. You may also recognize some of the XPath syntax Scrapy and <kbd>lxml</kbd>&#160;use to select the item. As we learned in <a href="py-web-scrp-2e_ch02.html" target="_blank">Chapter 2</a>, &#160;<em>Scraping the Data</em>, <kbd>lxml</kbd> converts all CSS Selectors to XPath before extracting content.</p>
<p>In order to actually get the text from this country row, we must call&#160;the <kbd>extract()</kbd> method:</p>
<pre><strong>In [4]: name_css = 'tr#places_country__row td.w2p_fw::text' </strong><br/><br/><strong>In [5]: response.css(name_css).extract() </strong><br/><strong>Out[5]: [u'United Kingdom'] </strong><br/><br/><strong>In [6]: pop_xpath = '//tr[@id="places_population__row"]/td[@class="w2p_fw"]/text()' </strong><br/><br/><strong>In [7]: response.xpath(pop_xpath).extract()</strong><br/><strong>Out[7]: [u'62,348,447']</strong>
</pre>
<p>As we can see from the output above, the Scrapy&#160;<kbd>response</kbd> object can be parsed using both&#160;<kbd>css</kbd> and&#160;<kbd>xpath</kbd>, making it very versatile for getting obvious and harder-to-reach content.&#160;</p>
<p>These selectors can then be used in the <kbd>parse_item()</kbd> method generated earlier in <kbd>example/spiders/country.py</kbd>.&#160;Note we set attributes of the&#160;<kbd>scrapy.Item</kbd> object using dictionary syntax:</p>
<pre>def parse_item(self, response): <br/>    item = CountryItem() <br/>    name_css = 'tr#places_country__row td.w2p_fw::text' <br/>    item['name'] = response.css(name_css).extract() <br/>    pop_xpath = '//tr[@id="places_population__row"]/td[@class="w2p_fw"]/text()'<br/>    item['population'] = response.xpath(pop_xpath).extract() <br/>    return item
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Checking results</h1>
            </header>

            <article>
                
<p>Here is the completed version of our spider:</p>
<pre>class CountrySpider(CrawlSpider): <br/>    name = 'country' <br/>    start_urls = ['http://example.webscraping.com/'] <br/>    allowed_domains = ['example.webscraping.com'] <br/>    rules = ( <br/>        Rule(LinkExtractor(allow=r'/index/', deny=r'/user/'), follow=True), <br/>        Rule(LinkExtractor(allow=r'/view/', deny=r'/user/'), callback='parse_item') <br/>    ) <br/><br/>    def parse_item(self, response): <br/>        item = CountryItem() <br/>        name_css = 'tr#places_country__row td.w2p_fw::text' <br/>        item['name'] = response.css(name_css).extract()<br/>        pop_xpath = '//tr[@id="places_population__row"]/td[@class="w2p_fw"]/text()'<br/>        item['population'] = response.xpath(pop_xpath).extract() <br/>        return item
</pre>
<p>To save the results, we could&#160;define a Scrapy pipeline or set up an output setting in our <kbd>settings.py</kbd> file. However, Scrapy also provides a handy <kbd>--output</kbd> flag to easily save scraped items automatically in CSV, JSON, or XML format.</p>
<p>Here are the results when the final version of the spider is run with the output to a CSV file and the log level is set to <kbd>INFO</kbd>, to filter out less important messages:</p>
<pre><strong>$ scrapy crawl country --output=../../../data/scrapy_countries.csv -s LOG_LEVEL=INFO<br/></strong><strong>2017-03-24 14:20:25 [scrapy.extensions.logstats] INFO: Crawled 277 pages (at 10 pages/min), scraped 249 items (at 9 items/min)<br/>2017-03-24 14:20:42 [scrapy.core.engine] INFO: Closing spider (finished)<br/>2017-03-24 14:20:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:<br/>{'downloader/request_bytes': 158580,<br/> 'downloader/request_count': 280,<br/> 'downloader/request_method_count/GET': 280,<br/> 'downloader/response_bytes': 944210,<br/> 'downloader/response_count': 280,<br/> 'downloader/response_status_count/200': 280,<br/> 'dupefilter/filtered': 61,<br/> 'finish_reason': 'finished',<br/> 'finish_time': datetime.datetime(2017, 3, 24, 13, 20, 42, 792220),<br/> 'item_scraped_count': 252,<br/> 'log_count/INFO': 35,<br/> 'request_depth_max': 26,<br/> 'response_received_count': 280,<br/> 'scheduler/dequeued': 279,<br/> 'scheduler/dequeued/memory': 279,<br/> 'scheduler/enqueued': 279,<br/> 'scheduler/enqueued/memory': 279,<br/> 'start_time': datetime.datetime(2017, 3, 24, 12, 52, 25, 733163)}<br/>2017-03-24 14:20:42 [scrapy.core.engine] INFO: Spider closed (finished)<br/></strong>
</pre>
<p>At the end of the crawl, Scrapy outputs some statistics to give an indication of how the crawl performed. From these statistics, we know that 280&#160;web pages were crawled and 252 items were scraped, which is the expected number of countries in the database, so we know the crawler was able to find them all.</p>
<div class="packt_infobox">You need to run Scrapy spider and crawl commands from within the generated folder Scrapy creates (for our project this is the <kbd>example/</kbd> directory we created using the&#160;<kbd>startproject</kbd> command). The spiders use the&#160;<kbd>scrapy.cfg</kbd> and&#160;<kbd>settings.py</kbd> files to determine how and where to scrape and to set spider paths for crawling or scraping use.</div>
<p>To verify these countries were scraped correctly we can check the contents of <kbd>countries.csv</kbd>:</p>
<pre>name,population <br/>Afghanistan,"29,121,286" <br/>Antigua and Barbuda,"86,754" <br/>Antarctica,0 <br/>Anguilla,"13,254" <br/>Angola,"13,068,161" <br/>Andorra,"84,000" <br/>American Samoa,"57,881" <br/>Algeria,"34,586,184" <br/>Albania,"2,986,952" <br/>Aland Islands,"26,711" <br/>... 
</pre>
<p>As expected this CSV&#160;contains the name and population for each country. Scraping this data required writing less code than the original crawler built in <a href="py-web-scrp-2e_ch02.html" target="_blank"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Scraping the Data</em> because Scrapy provides&#160;high-level functionality and nice built-in features like built-in CSV writers.</p>
<p>In the following section on Portia we will re-implement this scraper writing even less code.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Interrupting and resuming a crawl</h1>
            </header>

            <article>
                
<p>Sometimes when scraping a website, it can be useful to pause the crawl and resume it at a later time without needing to start over from the beginning. For example, you may need to interrupt the crawl to reset your computer after a software update, or perhaps, the website you are crawling is returning errors and you want to continue the crawl later.</p>
<p>Conveniently, Scrapy comes with built-in support to pause and resume crawls without needing to modify our example spider. To enable this feature, we just need to define the <kbd>JOBDIR</kbd> setting with a&#160;directory where the current state of a crawl can be saved. Note separate directories must be used to save the state of multiple crawls.</p>
<p>Here is an example using this feature with our spider:</p>
<pre><strong>$ scrapy crawl country -s LOG_LEVEL=DEBUG -s JOBDIR=../../../data/crawls/country</strong><br/><strong>...</strong><br/><strong>2017-03-24 13:41:54 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://example.webscraping.com/view/Anguilla-8&gt; (referer: http://example.webscraping.com/)</strong><br/><strong>2017-03-24 13:41:54 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://example.webscraping.com/view/Anguilla-8&gt;</strong><br/><strong>{'name': ['Anguilla'], 'population': ['13,254']}</strong><br/><strong>2017-03-24 13:41:59 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://example.webscraping.com/view/Angola-7&gt; (referer: http://example.webscraping.com/)</strong><br/><strong>2017-03-24 13:41:59 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://example.webscraping.com/view/Angola-7&gt;</strong><br/><strong>{'name': ['Angola'], 'population': ['13,068,161']}</strong><br/><strong>2017-03-24 13:42:04 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://example.webscraping.com/view/Andorra-6&gt; (referer: http://example.webscraping.com/)</strong><br/><strong>2017-03-24 13:42:04 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://example.webscraping.com/view/Andorra-6&gt;</strong><br/><strong>{'name': ['Andorra'], 'population': ['84,000']}</strong><br/><strong>^C2017-03-24 13:42:10 [scrapy.crawler] INFO: Received SIG_SETMASK, shutting down gracefully. Send again to force</strong> <br/><strong>...</strong><br/><strong>[country] INFO: Spider closed (shutdown)</strong>
</pre>
<p>Here, we see a&#160;<kbd>^C</kbd>&#160;in the line that says <kbd>Received SIG_SETMASK</kbd> which is the same&#160;<em>Ctrl</em> + <em>C</em> <span class="KeyPACKT">or</span> <em>cmd</em> + <em>C</em> <span class="KeyPACKT">we used earlier in the chapter to stop our scraper.</span> To have Scrapy save the crawl state, you must wait here for the crawl to shut down gracefully and resist the temptation to enter <span class="KeyPACKT">the termination sequence</span>&#160;again to force immediate shutdown! The state of the crawl will now be saved in the data directory in&#160;<kbd>crawls/country</kbd>. We can see the saved files if we look in that directory (Note this command and directory syntax will need to be altered for Windows users):</p>
<pre><strong>$ ls ../../../data/crawls/country/</strong><br/><strong>requests.queue requests.seen spider.state</strong>
</pre>
<p>&#160;The crawl can be resumed by running the same command:</p>
<pre><strong>$ scrapy crawl country -s LOG_LEVEL=DEBUG -s JOBDIR=../../../data/crawls/country</strong><br/><strong>...</strong><br/><strong>2017-03-24 13:49:49 [scrapy.core.engine] INFO: Spider opened</strong><br/><strong>2017-03-24 13:49:49 [scrapy.core.scheduler] INFO: Resuming crawl (13 requests scheduled)</strong><br/><strong>2017-03-24 13:49:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)</strong><br/><strong>2017-03-24 13:49:49 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023</strong><br/><strong>2017-03-24 13:49:49 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://example.webscraping.com/robots.txt&gt; (referer: None)</strong><br/><strong>2017-03-24 13:49:54 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://example.webscraping.com/view/Cameroon-40&gt; (referer: http://example.webscraping.com/index/3)</strong><br/><strong>2017-03-24 13:49:54 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://example.webscraping.com/view/Cameroon-40&gt;</strong><br/><strong>{'name': ['Cameroon'], 'population': ['19,294,149']}</strong><br/><strong>...</strong>
</pre>
<p>The crawl now resumes from where it paused and continues as normal. This feature is not particularly useful for our example website because the number of pages to download is manageable. However, for larger websites which&#160;could take months to crawl, being able to pause and resume crawls is quite convenient.</p>
<div class="packt_infobox">There are some edge cases not covered here that can cause problems when resuming a crawl, such as expiring cookies and sessions. These are mentioned in the Scrapy documentation available at <a href="http://doc.scrapy.org/en/latest/topics/jobs.html" target="_blank"><span class="URLPACKT">http://doc.scrapy.org/en/latest/topics/jobs.html</span></a>.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Scrapy Performance Tuning</h1>
            </header>

            <article>
                
<p>If we check the initial full scrape of the example site and take a look at the start and end times, we can see the scrape took approximately&#160;1,697 seconds. If we calculate how many seconds per page (on average), that is ~6 seconds per page. Knowing we did not use the Scrapy concurrency features and fully aware that we also added a delay of ~5 seconds between requests, this means Scrapy is parsing and extracting data at around 1s per page (Recall from <a href="py-web-scrp-2e_ch02.html" target="_blank">Chapter 2</a>, <em>Scraping the Data</em>, that our fastest scraper using XPath took 1.07s). I gave a talk at PyCon 2014 comparing web scraping library speed, and&#160;even then, Scrapy was massively faster than any other scraping frameworks I could find. I was able to write a simple Google search scraper that was returning (on average) 100 requests a second. Scrapy has come a long way since then, and I always recommend it for the most performant Python scraping framework.</p>
<p>In addition to leveraging the concurrency Scrapy uses (via Twisted), Scrapy can be tuned to use things like page caches and other performance considerations (such as utilizing proxies to allow more concurrent requests to a single site). In order to install the cache, you should first read the cache middleware documentation (<a href="https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpcache" target="_blank">https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpcache</a>). You might have already seen in the&#160;<kbd>settings.py</kbd> file, there are several good examples of how to implement the proper cache settings. For implementing proxies, there are some great helper libraries (as Scrapy only gives access to a simple middleware class). The current most popular and updated library is&#160;<a href="https://github.com/aivarsk/scrapy-proxies" target="_blank">https://github.com/aivarsk/scrapy-proxies</a>, which&#160;has Python3 support and is fairly&#160;easy to integrate.&#160;</p>
<p>As always, libraries and recommended setup can change, so reading the latest Scrapy documentation should always be your first stop when it comes to checking performance and making spider changes.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Visual scraping with Portia</h1>
            </header>

            <article>
                
<p>Portia is a an open-source tool built on top of Scrapy that supports building a spider by clicking on the parts of a website which&#160;need to be scraped. This method can be more convenient than creating the CSS or XPath selectors manually.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Installation</h1>
            </header>

            <article>
                
<p>Portia is a powerful tool, and it depends on multiple external libraries for its functionality. It is also relatively new, so currently, the installation steps are somewhat involved. In case the installation is simplified in future, the latest documentation can be found at <a href="https://github.com/scrapinghub/portia#running-portia"><span class="URLPACKT">https://github.com/scrapinghub/portia#running-portia</span></a>. The current recommended way to run Portia is to use Docker (the open-source container framework). If you don't have Docker installed, you'll need to do so first by following the latest instructions (<a href="https://docs.docker.com/engine/installation/">https://docs.docker.com/engine/installation/</a>).</p>
<p>Once Docker is installed and running, you can pull the <kbd>scrapinghub</kbd> image and get started. First, you should be in the directory you'd like to create your new portia project and run the command like so:</p>
<pre><strong>$ docker run -v ~/portia_projects:/app/data/projects:rw -p 9001:9001 scrapinghub/portia:portia-2.0.7</strong><br/><strong>Unable to find image 'scrapinghub/portia:portia-2.0.7' locally</strong><br/><strong>latest: Pulling from scrapinghub/portia<br/></strong>...<br/><strong>2017-03-28 12:57:42.711720 [-] Site starting on 9002</strong><br/><strong>2017-03-28 12:57:42.711818 [-] Starting factory &lt;slyd.server.Site instance at 0x7f57334e61b8&gt;</strong>
</pre>
<div class="packt_infobox"><span>In the command, we created a new folder at</span> <kbd>~/portia_projects</kbd><span>. If you'd rather have your portia projects stored elsewhere, change the</span> <kbd>-v</kbd> <span>command to point to the absolute file path where you would like to store your portia files.</span></div>
<p>These last few lines show that the Portia website is now up and running. The site&#160;will now be accessible in your web browser at <kbd>http://localhost:9001/</kbd>.</p>
<p>Your initial screen should look similar to this:</p>
<p><img class="image-border" src="images/portia_home.png"/></p>
<div class="CDPAlignCenter CDPAlign"></div>
<p>If you have problems during installation it's worth checking the Portia Issues page at <a href="https://github.com/scrapinghub/portia/issues" target="_blank"><span class="URLPACKT">https://github.com/scrapinghub/portia/issues</span></a>, in case someone else has experienced the same problem and found a solution. In this book I have used the specific Portia image I used (<kbd>scrapinghub/portia:portia-2.0.7</kbd>), but you can also try using the latest official release&#160;<kbd><span>scrapinghub/portia</span></kbd>.</p>
<p>In addition, I recommend always using the latest recommended instructions as documented in the README file and Portia documentation, even if they differ from the ones covered in this section. Portia is under active development and instructions could change after&#160;the publication of this book.&#160;</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Annotation</h1>
            </header>

            <article>
                
<p>At the Portia start page, the page prompts you to enter a project. Once you enter that text, then there is a textbox to enter the URL of the website you want to scrape, such as <a href="http://example.webscraping.com"><span class="URLPACKT">http://example.webscraping.com</span></a>.</p>
<p>When you've typed that, Portia will then load the project view:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="images/portia_project.png"/></div>
<p>Once you click the&#160;<span class="packt_screen">New Spider</span>&#160;button, you will see the following Spider view:</p>
<p><img class="image-border" src="images/portia_spider_view.png"/></p>
<p>You will start to recognize some of the fields from the Scrapy spider we already built earlier in this chapter (such as start pages and link crawling rules).&#160;By default,&#160;the spider name is set to the domain (<span class="packt_screen">example.webscraping.com</span>), which can be modified by clicking on the labels.</p>
<p>Next,&#160;click on the "New Sample" button to start collecting data from the page:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="images/portia_select_area.png"/></div>
<p>Now when you roll over the different elements of the page, you will see them highlighted. You can also see the CSS&#160;selector in the Inspector tab to the right of the website area.</p>
<p>Because we want&#160;to scrape the population elements on the individual country pages, we first need to navigate from this homepage to the individual country pages. To do so, we first need to click "Close Sample" and then click on any country. When the country page loads, we can once again click "New Sample".</p>
<p>To start adding fields to our items for extraction, we can click on the population field. When we do, an item is added and we can see the extracted information:<br/>
<br/>
<img class="image-border" src="images/portia_add_field.png"/></p>
<p><br/>
We can rename the field by using the left text field&#160;area and simply typing in the new name "population". Then, we can click the "Add Field" button. To add more fields, we can do the same for the country name and any other fields&#160;we are interested in by first clicking on the large + button and then selecting the field values in the same way. The annotated fields will be highlighted in the web page and you can see the extracted data in the extracted items section.&#160;</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="images/portia_extracted_items.png"/></div>
<p>If you want to delete any fields, you can simply use the red - sign next to the field name. When the annotations are complete, click on the blue&#160;"Close sample"&#160;button at the top. If you then wanted to download the spider to run in a Scrapy project, you can do so by clicking the link next to the spider name:<br/>
<br/>
<img class="image-border" src="images/portia_download.png"/><br/>
<br/>
You can also see all of your spiders and the settings in the mounted folder <kbd>~/portia_projects</kbd>.&#160;</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Running the Spider</h1>
            </header>

            <article>
                
<p>If you are running Portia as a Docker container, you can run the <kbd>portiacrawl</kbd> command using the&#160;same Docker image. First, stop your current container using <em>Ctrl</em> + <em>C</em>. Then, you can run the following command:<br/>
<br/>
<kbd>docker run -i -t --rm -v ~/portia_projects:/app/data/projects:rw -v &lt;OUTPUT_FOLDER&gt;:/mnt:rw -p 9001:9001 scrapinghub/portia portiacrawl /app/data/projects/&lt;PROJECT_NAME&gt; example.webscraping.com -o /mnt/example.webscraping.com.jl</kbd><br/>
&#160;<br/>
Make sure to&#160;update the OUTPUT_FOLDER in an absolute path where you want to store your output files and PROJECT_NAME variables is the name you used when starting your project (mine was my_example_site). You should see output similar to output you notice when running Scrapy. You may notice error messages (this is due to not changing the download delay or parallel requests -- both of which can be done in the web interface by changing the project and spider settings). You can also pass extra settings to your spider when it is run using the <kbd>-s</kbd> flag. My command looks like this:<br/>
<kbd><span><br/>
docker run -i -t --rm -v ~/portia_projects:/app/data/projects:rw -v</span> <span>~/portia_output</span><span>:/mnt:rw -p 9001:9001 scrapinghub/portia portiacrawl /app/data/projects/my_example_site</span><span>&#160;example.webscraping.com -o /mnt/example.webscraping<kbd>.com.jl</kbd></span>-s CONCURRENT_REQUESTS_PER_DOMAIN=1 -s DOWNLOAD_DELAY=5</kbd></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Checking results</h1>
            </header>

            <article>
                
<p>When the spider is finished, you can check your results in the output folder you created:<br/>
<br/>
<kbd>$ head ~/portia_output/example.webscraping.com.jl<br/>
{"_type": "Example web scraping website1", "url": "http://example.webscraping.com/view/Antigua-and-Barbuda-10", "phone_code": ["+1-268"], "_template": "98ed-4785-8e1b", "country_name": ["Antigua and Barbuda"], "population": ["86,754"]}<br/>
{"_template": "98ed-4785-8e1b", "country_name": ["Antarctica"], "_type": "Example web scraping website1", "url": "http://example.webscraping.com/view/Antarctica-9", "population": ["0"]}<br/>
{"_type": "Example web scraping website1", "url": "http://example.webscraping.com/view/Anguilla-8", "phone_code": ["+1-264"], "_template": "98ed-4785-8e1b", "country_name": ["Anguilla"], "population": ["13,254"]}<br/>
...</kbd></p>
<p>Here are a few examples of the&#160;results of your scrape. As you can see, they are in JSON format. If you wanted to export in CSV&#160;format, you can simply change the output file name to end with&#160;<kbd>.csv</kbd>.</p>
<p>With just a few clicks on a site and a few instructions for Docker, you've scraped the example website!&#160;Portia is a handy tool to use, especially for straightforward websites, or if you need to collaborate with non-developers. On the other hand, for more complex websites,&#160;you always&#160;have the option to develop the Scrapy crawler directly in Python or use Portia to develop the first iteration and expand it using your own Python skills.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Automated scraping with Scrapely</h1>
            </header>

            <article>
                
<p>For scraping the annotated fields Portia uses a library called <strong>Scrapely (<a href="https://github.com/scrapy/scrapely" target="_blank">https://github.com/scrapy/scrapely</a>)</strong>, which is a useful open-source tool developed independently from&#160;Portia. Scrapely uses training data to build a model of what to scrape from a web page. The trained model can then be applied to scrape other web pages with the same structure.</p>
<p>You can install it using pip:</p>
<pre><strong>pip install scrapely</strong>
</pre>
<p>Here is an example to show how it works:</p>
<pre>  <br/><strong>&gt;&gt;&gt; from scrapely import Scraper</strong><br/><strong>&gt;&gt;&gt; s = Scraper()</strong><br/><strong>&gt;&gt;&gt; train_url = 'http://example.webscraping.com/view/Afghanistan-1'</strong><br/><strong>&gt;&gt;&gt; s.train(train_url, {'name': 'Afghanistan', 'population': '29,121,286'})</strong><br/><strong>&gt;&gt;&gt; test_url = 'http://example.webscraping.com/view/United-Kingdom-239'</strong><br/><strong>&gt;&gt;&gt; s.scrape(test_url)</strong><br/><strong>[{u'name': [u'United Kingdom'], u'population': [u'62,348,447']}]</strong>
</pre>
<p>First, Scrapely is given the data we want to scrape from the <kbd>Afghanistan</kbd> web page to train the model (here, the country name and population). This model is then applied to a&#160;different&#160;country page and Scrapely uses the trained&#160;model to correctly return the country name and population here as well.</p>
<p>This workflow allows scraping web pages without needing to know their structure, only the desired content you want to extract for the training case (or multiple training cases). This approach can be particularly useful if the content of a web page is static, but&#160;the layout is changing. For example, with a news website, the text of the published article will most likely not change, though the layout may be updated. In this case, Scrapely can then be retrained using the same data to generate a model for the new website structure. For this example to work properly, you would need to store your training data somewhere for reuse.</p>
<p>The example web page used here to test Scrapely is well structured with separate tags and attributes for each data type so Scrapely was able to correctly and easily train a model. For more complex web pages, Scrapely can fail to locate the content correctly. The Scrapely documentation warns you should "train with caution". As machine learning becomes faster and easier, perhaps a more robust automated web scraping library will be released; for now, it is still quite useful to know how to scrape a website directly using the techniques covered throughout this book.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>This chapter introduced Scrapy, a web scraping framework with many high-level features to improve efficiency when scraping websites. Additionally, we&#160;covered Portia, which provides a visual interface to generate Scrapy spiders. Finally, we tested Scrapely, the library used by Portia to scrape web pages automatically by first training&#160;a simple model.</p>
<p>In the next chapter, we will apply the skills learned so far to some real-world websites.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>
</body>
</html>