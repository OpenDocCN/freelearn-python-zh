<html><head></head><body>
<div><div><div><h1 id="_idParaDest-150"><em class="italic"><a id="_idTextAnchor141"/>Chapter 8</em>: Parallel Processing</h1>
			<p>With parallel processing using multiple cores, you can increase the number of calculations your program can do in a given time frame without needing a faster processor. The main idea is to divide a problem into independent subunits and use multiple cores to solve those subunits in parallel.</p>
			<p>Parallel processing is necessary to tackle large-scale problems. Every day, companies produce massive quantities of data that needs to be stored in multiple computers and analyzed. Scientists and engineers run parallel code on supercomputers to simulate massive systems.</p>
			<p>Parallel processing allows you to take advantage of multicore <strong class="bold">central processing units</strong> (<strong class="bold">CPUs</strong>) as well as <strong class="bold">graphics processing units</strong> (<strong class="bold">GPUs</strong>) that work extremely well with highly parallel problems. </p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Introduction to parallel programming</li>
				<li>Using multiple processes</li>
				<li>Parallel Cython with <strong class="bold">Open Multi-Processing</strong> (<strong class="bold">OpenMP</strong>)</li>
				<li>Automatic parallelism</li>
			</ul>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor142"/>Technical requirements</h1>
			<p>The code files for this chapter can be accessed through this link: <a href="https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter08">https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter08</a>.</p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor143"/>Introduction to parallel programming</h1>
			<p>To parallelize a program, it is necessary to divide<a id="_idIndexMarker621"/> the problem into subunits that can run independently (or almost independently) from each other.</p>
			<p>A problem where the subunits are totally independent<a id="_idIndexMarker622"/> of each other is called <em class="italic">embarrassingly parallel</em>. An element-wise operation on an array is a typical example—the operation needs to only know the element it is handling now. Another example is our particle simulator. Since there are no interactions, each particle can evolve independently from the others. Embarrassingly parallel problems are very easy to implement and perform very well on parallel architectures.</p>
			<p>Other problems may be divided into subunits but must share some data to perform their calculations. In those cases, the implementation is less straightforward and can lead to performance issues because of the communication costs.</p>
			<p>We will illustrate the concept with an example. Imagine that you have a particle simulator, but this time, the particles attract other particles within a certain distance (as shown in the following figure): </p>
			<div><div><img src="img/B17499_Figure_8.1.jpg" alt="Figure 8.1 – Illustration of a neighboring region " width="891" height="370"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Illustration of a neighboring region</p>
			<p>To parallelize this problem, we divide the simulation box into regions and assign each region to a different processor. If we evolve the system one step at a time, some particles will interact with particles in a neighboring region. To perform the next iteration, communication with the new particle positions of the neighboring region is required.</p>
			<p>Communication between processes is costly and can seriously hinder the performance of parallel programs. There exist two main ways to handle data communication in parallel programs: shared memory and distributed memory.</p>
			<p>In <strong class="bold">shared memory</strong>, the subunits have access to the same<a id="_idIndexMarker623"/> memory space. An advantage of this approach is that you don't have to explicitly handle the communication as it is sufficient<a id="_idIndexMarker624"/> to write or read from the shared memory. However, problems arise when multiple processes try to access and change the same memory location at the same time. Care should be taken to avoid such conflicts using synchronization techniques.</p>
			<p>In the <strong class="bold">distributed memory</strong> model, each process is completely separated<a id="_idIndexMarker625"/> from the others and possesses its own memory space. In this case, communication<a id="_idIndexMarker626"/> is handled explicitly between the processes. The communication overhead is typically costlier compared to shared memory as data can potentially travel through a network interface.</p>
			<p>One common way to achieve parallelism with the shared memory model is through <strong class="bold">threads</strong>. Threads are independent subtasks that originate<a id="_idIndexMarker627"/> from a process and share resources, such as memory. This concept is further illustrated in the following diagram: </p>
			<div><div><img src="img/B17499_Figure_8.2.jpg" alt="Figure 8.2 – Illustration of the difference between threads and processes " width="828" height="271"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – Illustration of the difference between threads and processes</p>
			<p>Threads produce<a id="_idIndexMarker628"/> multiple execution contexts and share the same memory space, while processes provide multiple execution contexts that possess their own memory space, and communication must be handled explicitly.</p>
			<p>Python can spawn and handle threads, but they can't be used to increase performance; due to the Python interpreter design, only one Python instruction<a id="_idIndexMarker629"/> is allowed to run at a time—this mechanism is called the <strong class="bold">Global Interpreter Lock</strong> (<strong class="bold">GIL</strong>). What happens is that each time a thread executes a Python statement, the thread acquires a lock and, when the execution is completed, the same lock is released. Since<a id="_idIndexMarker630"/> the lock can be acquired only by one thread at a time, other threads are prevented from executing Python statements while some other thread holds the lock.</p>
			<p>Even though the GIL prevents parallel execution of Python instructions, threads can still be used to provide concurrency in situations where the lock can be released, such as in time-consuming <strong class="bold">input/output</strong> (<strong class="bold">I/O</strong>) operations or in C extensions.</p>
			<p class="callout-heading">Why Not Remove the GIL?</p>
			<p class="callout">In past years, many attempts have been made, including the most recent <em class="italic">Gilectomy </em>experiment. First, removing the GIL is not an easy task and requires modification of most of the Python data<a id="_idIndexMarker631"/> structures. Additionally, such fine-grained locking can be costly and may introduce substantial performance loss in single-threaded programs. Despite this, some Python implementations (notable examples are Jython and IronPython) do not use the GIL.</p>
			<p>The GIL can be<a id="_idIndexMarker632"/> completely sidestepped using processes instead of threads. Processes don't share the same memory area and are independent of each other—each process has its own interpreter. Processes have a few disadvantages: starting up a new process is generally slower than starting a new thread, they consume more memory, and <strong class="bold">inter-process communication</strong> (<strong class="bold">IPC</strong>) can be slow. On the other hand, processes are still very flexible, and they scale better as they can be distributed on multiple machines.</p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor144"/>GPUs</h2>
			<p>GPUs are special processors designed<a id="_idIndexMarker633"/> for computer graphics applications. Those applications usually require processing the geometry of a <strong class="bold">three-dimensional</strong> (<strong class="bold">3D</strong>) scene and output<a id="_idIndexMarker634"/> an array of pixels to the screen. The operations performed by GPUs involve array and matrix operations on floating-point numbers.</p>
			<p>GPUs are designed to run this graphics-related operation very efficiently, and they achieve this by adopting a highly parallel architecture. Compared to a CPU, a GPU has many more (thousands) of small processing units. GPUs are intended to produce data at about 60 <strong class="bold">frames per second</strong> (<strong class="bold">FPS</strong>), which is much slower than the typical response time of a CPU, which possesses higher clock speeds.</p>
			<p>GPUs possess a very different architecture from a standard CPU and are specialized for computing floating-point operations. Therefore, to compile programs for GPUs, it is necessary to utilize special programming platforms, such as <strong class="bold">Compute Unified Device Architecture</strong> (<strong class="bold">CUDA</strong>) and <strong class="bold">Open Computing Language</strong> (<strong class="bold">OpenCL</strong>).</p>
			<p>CUDA is a proprietary<a id="_idIndexMarker635"/> NVIDIA<a id="_idIndexMarker636"/> technology. It provides an <strong class="bold">application programming interface</strong> (<strong class="bold">API</strong>) that can be accessed from other languages. CUDA provides the <strong class="bold">NVIDIA CUDA Compiler</strong> (<strong class="bold">NVCC</strong>) tool that can be used to compile<a id="_idIndexMarker637"/> GPU programs written in a language such as C (CUDA C), as well as numerous libraries that implement highly optimized mathematical routines.</p>
			<p><strong class="bold">OpenCL</strong> is an open technology with<a id="_idIndexMarker638"/> an ability to write parallel programs that can be compiled for a variety of target devices (CPUs and GPUs of several vendors) and is a good option for non-NVIDIA devices.</p>
			<p>GPU programming sounds wonderful on paper. However, don't throw away your CPU yet. GPU programming is tricky, and only specific use cases benefit from the GPU architecture. Programmers need to be aware of the costs incurred in memory transfers to and from the main memory and how to implement algorithms to take advantage of the GPU architecture.</p>
			<p>Generally, GPUs are great at increasing the number of operations<a id="_idIndexMarker639"/> you can perform per unit of time (also called <strong class="bold">throughput</strong>); however, they require more time to prepare the data for processing. In contrast, CPUs are much faster at producing an individual<a id="_idIndexMarker640"/> result from scratch (also called <strong class="bold">latency</strong>).</p>
			<p>For the right problem, GPUs provide extreme (10 to 100 times) speedup. For this reason, they often constitute a very inexpensive solution (the same speedup will require hundreds of CPUs) to improve the performance<a id="_idIndexMarker641"/> of numerically intensive applications. We will illustrate how to execute some<a id="_idIndexMarker642"/> algorithms on a GPU in the <em class="italic">Automatic parallelism</em> section.</p>
			<p>Having said that, we will begin our discussion on multiprocessing using standard processes in the next section.</p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor145"/>Using multiple processes</h1>
			<p>The standard <code>multiprocessing</code> module can be used to quickly parallelize simple tasks by spawning<a id="_idIndexMarker643"/> several processes while avoiding the GIL problem. Its interface is easy to use and includes several utilities to handle task submission and synchronization.</p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor146"/>The Process and Pool classes</h2>
			<p>You can create<a id="_idIndexMarker644"/> a process that runs independently by subclassing <code>multiprocessing.Process</code>. You can extend the <code>__init__</code> method to initialize<a id="_idIndexMarker645"/> resources, and you can write a portion of the code that<a id="_idIndexMarker646"/> will be executed in a subprocess<a id="_idIndexMarker647"/> by implementing the <code>Process.run</code> method. In the following code snippet, we define a <code>Process</code> class that will wait for 1 second and print its assigned <code>id</code> value:</p>
			<pre>    import multiprocessing 
    import time 
    class Process(multiprocessing.Process): 
        def __init__(self, id): 
            super(Process, self).__init__() 
            self.id = id 
        def run(self): 
            time.sleep(1) 
            print("I'm the process with id: 
              {}".format(self.id))</pre>
			<p>To spawn the process, we must instantiate<a id="_idIndexMarker648"/> the <code>Process</code> class and call the <code>Process.start</code> method. Note that you don't directly call <code>Process.run</code>; the call to <code>Process.start</code> will create a new process that, in turn, will call the <code>Process.run</code> method. We can<a id="_idIndexMarker649"/> add the following lines at the end of the<a id="_idIndexMarker650"/> preceding snippet to create and start<a id="_idIndexMarker651"/> the new process:</p>
			<pre>    if __name__ == '__main__': 
        p = Process(0) 
        p.start()</pre>
			<p class="callout-heading">The special __name__ Variable</p>
			<p class="callout">Note that we need to place any code that manages processes inside the <code>if __name__ == '__main__'</code> condition, as shown in the previous code snippet, to avoid many undesirable behaviors. All the code shown in this chapter will be assumed to follow this practice.</p>
			<p>The instructions after <code>Process.start</code> will be executed immediately without waiting for the <code>p</code> process to finish. To wait for the task completion, you can use the <code>Process.join</code> method, as follows:</p>
			<pre>    if __name__ == '__main__': 
       p = Process(0) 
       p.start() 
       p.join()</pre>
			<p>We can launch four different<a id="_idIndexMarker652"/> processes that will run in parallel in the same way. In a serial<a id="_idIndexMarker653"/> program, the total required time will be 4 seconds. Since the execution<a id="_idIndexMarker654"/> is concurrent, the resulting wall<a id="_idIndexMarker655"/> clock time will be of 1 second. In the following code snippet, we create four processes that will execute concurrently:</p>
			<pre>    if __name__ == '__main__': 
        processes = Process(1), Process(2), Process(3), 
          Process(4) 
        [p.start() for p in processes]</pre>
			<p>Note that the order of the execution for parallel processes is unpredictable and ultimately depends on how the <strong class="bold">operating system</strong> (<strong class="bold">OS</strong>) schedules this. You can verify this behavior by executing the program multiple times; the order will likely be different between runs.</p>
			<p>The <code>multiprocessing</code> module exposes a convenient interface that makes it easy to assign and distribute tasks to a set of processes that reside in the <code>multiprocessing.Pool</code> class.</p>
			<p>The <code>multiprocessing.Pool</code> class spawns<a id="_idIndexMarker656"/> a set of processes—called <em class="italic">workers</em>—and lets us submit tasks through the <code>apply</code>/<code>apply_async</code> and <code>map</code>/<code>map_async</code> methods.</p>
			<p>The <code>pool.map</code> method applies a function to each element of a list and returns a list of results. Its usage is equivalent to the built-in (serial) <code>map</code>.</p>
			<p>To use a parallel map, you should first initialize a <code>multiprocessing.Pool</code> object that takes the number of workers as its first argument; if not provided, that number will be equal to the number of cores<a id="_idIndexMarker657"/> in the system. You can initialize a <code>multiprocessing.Pool</code> object<a id="_idIndexMarker658"/> in the following way:</p>
			<pre>    pool = multiprocessing.Pool() 
    pool = multiprocessing.Pool(processes=4)</pre>
			<p>Let's see <code>pool.map</code> in action. If you have<a id="_idIndexMarker659"/> a function that computes the square of a number, you can<a id="_idIndexMarker660"/> map the function to the list by calling <code>pool.map</code> and passing the function and the list of inputs as arguments, as follows:</p>
			<pre>    def square(x): 
        return x * x 
    inputs = [0, 1, 2, 3, 4] 
    outputs = pool.map(square, inputs)</pre>
			<p>The <code>pool.map_async</code> function is just like <code>pool.map</code> but returns an <code>AsyncResult</code> object instead of the actual result. When we call <code>pool.map</code>, the execution of the main program is stopped until all the workers are finished processing the result. With <code>map_async</code>, the <code>AsyncResult</code> object is returned immediately without blocking the main program and the calculations are done in the background. We can then retrieve the result using the <code>AsyncResult.get</code> method at any time, as shown in the following lines of code:</p>
			<pre>    outputs_async = pool.map_async(square, inputs) 
    outputs = outputs_async.get()</pre>
			<p><code>pool.apply_async</code> assigns a task consisting of a single function to one of the workers. It takes the function and its arguments and returns an <code>AsyncResult</code> object. We can obtain an effect similar to <code>map</code> using <code>apply_async</code>, as shown here:</p>
			<pre>    results_async = [pool.apply_async(square, i) for i in \
      range(100))] 
    results = [r.get() for r in results_async]</pre>
			<p>To use the<a id="_idIndexMarker661"/> results computed<a id="_idIndexMarker662"/> and returned by these processes, we can simply<a id="_idIndexMarker663"/> access the data stored<a id="_idIndexMarker664"/> in <code>results</code>.</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor147"/>The Executor interface</h2>
			<p>From version 3.2 onward, it is possible to execute Python code in parallel using the <code>Executor</code> interface provided in the <code>concurrent.futures</code> module. We already saw the <code>Executor</code> interface in action in the previous<a id="_idIndexMarker665"/> chapter, when we used <code>ThreadPoolExecutor</code> to perform multiple tasks concurrently. In this<a id="_idIndexMarker666"/> subsection, we'll demonstrate the usage of the <code>ProcessPoolExecutor</code> class.</p>
			<p><code>ProcessPoolExecutor</code> exposes a very lean interface, at least when compared to the more featureful <code>multiprocessing.Pool</code>. A <code>ProcessPoolExecutor</code> class can be instantiated, similar to <code>ThreadPoolExecutor</code>, by passing a number of worker threads using the <code>max_workers</code> argument (by default, <code>max_workers</code> will be the number of CPU cores available). The main methods available to the <code>ProcessPoolExecutor</code> class are <code>submit</code> and <code>map</code>.</p>
			<p>The <code>submit</code> method will take a function and return a <code>Future</code> instance that will keep track of the execution of the submitted function. The <code>map</code> method works similarly to the <code>pool.map</code> function, except that it returns an iterator rather than a list. The code is illustrated in the following snippet:</p>
			<pre>    from concurrent.futures import ProcessPoolExecutor
    executor = ProcessPoolExecutor(max_workers=4)
    fut = executor.submit(square, 2)
    # Result:
    # &lt;Future at 0x7f5b5c030940 state=running&gt;
    result = executor.map(square, [0, 1, 2, 3, 4])
    list(result)
    # Result:
    # [0, 1, 4, 9, 16]</pre>
			<p>To extract the result from one or more <code>Future</code> instances, you can use the <code>concurrent.futures.wait</code> and <code>concurrent.futures.as_completed</code> functions. The <code>wait</code> function accepts a list of <code>future</code> instances and will block the execution of the programs<a id="_idIndexMarker667"/> until all the futures have completed their<a id="_idIndexMarker668"/> execution. The result can then be extracted using the <code>Future.result</code> method. The <code>as_completed</code> function also accepts a function but will, instead, return an iterator over the results. The code is illustrated in the following snippet:</p>
			<pre>    from concurrent.futures import wait, as_completed
    fut1 = executor.submit(square, 2)
    fut2 = executor.submit(square, 3)
    wait([fut1, fut2])
    # Then you can extract the results using fut1.result() 
      and fut2.result()
    results = as_completed([fut1, fut2])
    list(results)
    # Result:
    # [4, 9]</pre>
			<p>Alternatively, you can generate futures using the <code>asyncio.run_in_executor</code> function and manipulate the results using all the tools and syntax provided by the <code>asyncio</code> libraries so that<a id="_idIndexMarker669"/> you can achieve concurrency and parallelism at the same time.  </p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor148"/>Monte Carlo approximation of pi</h2>
			<p>As an example, we will implement<a id="_idIndexMarker670"/> a canonical, embarrassingly parallel program—<strong class="bold">the Monte Carlo approximation of pi</strong>. Imagine that we have a square of size 2 units; its area will be 4 units. Now, we inscribe<a id="_idIndexMarker671"/> a circle of 1 unit radius in this square; the area of the circle will be <img src="img/Formula_8.1_B17499.png" alt="" width="143" height="48"/>. By substituting the value of <em class="italic">r</em> in the previous equation, we get that the numerical value for the area of the circle is <img src="img/Formula_8.2_B17499.png" alt="" width="177" height="46"/> <em class="italic">= pi</em>. You can refer to the following screenshot for a graphical representation of this:</p>
			<div><div><img src="img/B17499_Figure_8.3.jpg" alt="Figure 8.3 – Illustration of our strategy of approximation of pi " width="811" height="324"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – Illustration of our strategy of approximation of pi</p>
			<p>If we shoot a lot of random points on this, some points will fall into the circle, which we'll call <em class="italic">hits</em>, while the remaining points, <em class="italic">misses</em>, will be outside the circle. The area of the circle will be proportional to the number of hits, while the area of the square will be proportional to the total number of shots. To get the value of pi, it is sufficient to divide the area of the circle (equal to <em class="italic">pi</em>) by the area of the square (equal to 4), as illustrated in the following code snippet:</p>
			<pre>    hits/total = area_circle/area_square = pi/4 
    pi = 4 * hits/total</pre>
			<p>The strategy we will employ in our program will be as follows:</p>
			<ul>
				<li>Generate a lot of uniformly random (<em class="italic">x</em>, <em class="italic">y</em>) numbers in the range (-1, 1).</li>
				<li>Test whether those numbers lie inside the circle by checking whether <em class="italic">x**2 + y**2 &lt;= 1</em>.</li>
			</ul>
			<p>The first step when writing a parallel<a id="_idIndexMarker672"/> program is to write a serial version and verify that it works. In a real-world scenario, you also want to leave parallelization as the last step of your optimization process—first, because we need to identify the slow parts, and second, parallelization is time-consuming and <em class="italic">gives you at most a speedup equal to the number of processors</em>. The implementation of the serial program is shown here:</p>
			<pre>    import random 
    samples = 1000000 
    hits = 0 
    for i in range(samples): 
        x = random.uniform(-1.0, 1.0) 
        y = random.uniform(-1.0, 1.0) 
        if x**2 + y**2 &lt;= 1: 
            hits += 1 
     
    pi = 4.0 * hits/samples</pre>
			<p>The accuracy of our approximation will improve as we increase the number of samples. Note that each loop iteration is independent of the other—this problem is embarrassingly parallel.</p>
			<p>To parallelize this code, we can<a id="_idIndexMarker673"/> write a function, called <code>sample</code>, that corresponds<a id="_idIndexMarker674"/> to a single hit-miss check. If the sample hits the circle, the function will return <code>1</code>; otherwise, it will return <code>0</code>. By running <code>sample</code> multiple times and summing the results, we'll get the total number of hits. We can run <code>sample</code> over multiple processors with <code>apply_async</code> and get the results in the following way:</p>
			<pre>    def sample(): 
        x = random.uniform(-1.0, 1.0) 
        y = random.uniform(-1.0, 1.0) 
        if x**2 + y**2 &lt;= 1: 
            return 1 
        else: 
            return 0 
    pool = multiprocessing.Pool() 
    results_async = [pool.apply_async(sample) for i in \
      range(samples)] 
    hits = sum(r.get() for r in results_async)</pre>
			<p>We can wrap the two versions<a id="_idIndexMarker675"/> in the <code>pi_serial</code> and <code>pi_apply_async</code> functions (you can find their implementation in the <code>pi.py</code> file) and benchmark the<a id="_idIndexMarker676"/> execution speed, as follows:</p>
			<pre>$ time python -c 'import pi; pi.pi_serial()'
real    0m0.734s
user    0m0.731s
sys     0m0.004s
$ time python -c 'import pi; pi.pi_apply_async()'
real    1m36.989s
user    1m55.984s
sys     0m50.386</pre>
			<p>As shown in the earlier benchmark, our first parallel version literally cripples our code, the reason being that the time spent doing the actual calculation is small compared to the overhead required to send and distribute the tasks to the workers.</p>
			<p>To solve the issue, we have to make the overhead negligible compared to the calculation time. For example, we can ask each worker to handle more than one sample at a time, thus reducing the task communication overhead. We can write a <code>sample_multiple</code> function that processes more than one hit and modifies our parallel version by dividing our problem<a id="_idIndexMarker677"/> by 10; more intensive tasks are shown<a id="_idIndexMarker678"/> in the following code snippet:</p>
			<pre>    def sample_multiple(samples_partial): 
        return sum(sample() for i inrange(samples_partial)) 
    n_tasks = 10 
    chunk_size = samples/n_tasks 
    pool = multiprocessing.Pool() 
    results_async = [pool.apply_async(sample_multiple, \
      chunk_size) for i in range(n_tasks)] 
    hits = sum(r.get() for r in results_async)</pre>
			<p>We can wrap this in a function called <code>pi_apply_async_chunked</code> and run it as follows:</p>
			<pre>$ time python -c 'import pi; pi.pi_apply_async_chunked()'
real    0m0.325s
user    0m0.816s
sys     0m0.008s</pre>
			<p>The results are much better; we more than doubled the speed of our program. You can also notice that the <code>user</code> metric is larger than <code>real</code>; the total CPU time is larger than the total time because more than one CPU worked at the same time. If you increase the number of samples, you will note that the ratio of communication to calculation decreases, giving even better speedups.</p>
			<p>Everything is nice and simple<a id="_idIndexMarker679"/> when dealing with embarrassingly parallel<a id="_idIndexMarker680"/> problems. However, you sometimes have to share data between processes.</p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor149"/>Synchronization and locks</h2>
			<p>Even if <code>multiprocessing</code> uses processes (with their own independent memory), it lets you define certain variables<a id="_idIndexMarker681"/> and arrays as shared memory. You can define a shared<a id="_idIndexMarker682"/> variable using <code>multiprocessing.Value</code>, passing its data type as a string (<code>i</code> for integer, <code>d</code> for double, <code>f</code> for float, and so on). You can update<a id="_idIndexMarker683"/> the content of the variable through the <code>value</code> attribute, as shown<a id="_idIndexMarker684"/> in the following code snippet:</p>
			<pre>    shared_variable = multiprocessing.Value('f') 
    shared_variable.value = 0</pre>
			<p>When using shared memory, you should be aware of concurrent access. Imagine that you have a shared integer variable, and each process increments its value multiple times. You will define a <code>Process</code> class, as follows:</p>
			<pre>    class Process(multiprocessing.Process): 
        def __init__(self, counter): 
            super(Process, self).__init__() 
            self.counter = counter 
        def run(self): 
            for i in range(1000): 
                self.counter.value += 1</pre>
			<p>You can initialize the shared variable in the main program and pass it to <code>4</code> processes, as shown in the following code snippet:</p>
			<pre>    def main(): 
        counter = multiprocessing.Value('i', lock=True) 
        counter.value = 0 
        processes = [Process(counter) for i in range(4)] 
        [p.start() for p in processes] 
        [p.join() for p in processes] # processes are done 
        print(counter.value)</pre>
			<p>If you run this program (<code>shared.py</code> in the code directory), you will note that the final value of <code>counter</code> is not <code>4000</code>, but it has random values (on my machine, they are between <code>2000</code> and <code>2500</code>). If we assume that the arithmetic is correct, we can conclude that there's a problem with the parallelization.</p>
			<p>What happens is that multiple<a id="_idIndexMarker685"/> processes are trying to access the same shared<a id="_idIndexMarker686"/> variable at the same time. The situation is best explained by looking<a id="_idIndexMarker687"/> at the following diagram. In a serial<a id="_idIndexMarker688"/> execution, the first process reads the number (<code>0</code>), increments it, and writes the new value (<code>1</code>); the second process reads the new value (<code>1</code>), increments it, and writes it again (<code>2</code>). </p>
			<p>In parallel execution, the two processes read the number (<code>0</code>), increment it, and write the value (<code>1</code>) at the same time, leading to a wrong answer:</p>
			<div><div><img src="img/B17499_Figure_8.4.jpg" alt="Figure 8.4 – Multiple processes accessing the same variable, leading to incorrect behavior " width="840" height="309"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.4 – Multiple processes accessing the same variable, leading to incorrect behavior</p>
			<p>To solve this problem, we need to synchronize the access to this variable so that only one process at a time can access, increment, and write the value on the shared variable. This feature is provided by the <code>multiprocessing.Lock</code> class. A lock can be acquired and released through the <code>acquire</code> and <code>release</code> methods respectively or by using the lock as a context manager. Since the lock can be acquired by only one process at a time, this method prevents multiple processes from executing the protected section of code at the same time.</p>
			<p>We can define a global<a id="_idIndexMarker689"/> lock and use<a id="_idIndexMarker690"/> it as a context manager to restrict<a id="_idIndexMarker691"/> access to the counter, as shown in the following<a id="_idIndexMarker692"/> code snippet:</p>
			<pre>    class Process(multiprocessing.Process): 
        def __init__(self, counter): 
            super(Process, self).__init__() 
            self.counter = counter 
        def run(self): 
            for i in range(1000): 
                with lock: # acquire the lock 
                    self.counter.value += 1 
                # release the lock</pre>
			<p>Synchronization primitives, such as locks, are essential to solving many problems, but they should be kept to a minimum to improve the performance of your program.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The <code>multiprocessing</code> module includes<a id="_idIndexMarker693"/> other communication and synchronization tools; you can refer to the official documentation at <a href="http://docs.python.org/3/library/multiprocessing.html">http://docs.python.org/3/library/multiprocessing.html</a> for a complete reference.</p>
			<p>In <a href="B17499_04_Final_SS_ePub.xhtml#_idTextAnchor068"><em class="italic">Chapter 4</em></a>, <em class="italic">C Performance with Cython</em>, we discussed Cython<a id="_idIndexMarker694"/> as a method of speeding up<a id="_idIndexMarker695"/> our programs. Cython itself also allows parallel<a id="_idIndexMarker696"/> processing via OpenMP, which<a id="_idIndexMarker697"/> we will examine next.</p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor150"/>Parallel Cython with OpenMP</h1>
			<p>Cython provides a convenient<a id="_idIndexMarker698"/> interface to perform shared-memory parallel processing through <em class="italic">OpenMP</em>. This lets you write extremely efficient parallel code directly<a id="_idIndexMarker699"/> in Cython without having to create a C wrapper.</p>
			<p>OpenMP is a specification<a id="_idIndexMarker700"/> and an API designed to write multithreaded, parallel programs. The OpenMP specification includes a series of C preprocessor directives to manage threads and provides communication patterns, load balancing, and other synchronization<a id="_idIndexMarker701"/> features. Several C/C++ and Fortran compilers (including the <strong class="bold">GNU Compiler Collection</strong> (<strong class="bold">GCC</strong>)) implement the OpenMP API.</p>
			<p>We can introduce the Cython parallel features with a small example. Cython provides a simple API based on OpenMP in the <code>cython.parallel</code> module. The simplest way to achieve parallelism is through <code>prange</code>, which is a construct that automatically distributes loop operations in multiple threads.</p>
			<p>First of all, we can write the serial version of a program that computes the square of each element of a NumPy array in the <code>hello_parallel.pyx</code> file. We define a function, <code>square_serial</code>, that takes a buffer as input and populates an output array with the squares of the input array elements; <code>square_serial</code> is shown in the following code snippet:</p>
			<pre>    import numpy as np 
    def square_serial(double[:] inp): 
        cdef int i, size 
        cdef double[:] out 
        size = inp.shape[0] 
        out_np = np.empty(size, 'double') 
        out = out_np 
        for i in range(size): 
            out[i] = inp[i]*inp[i] 
        return out_np</pre>
			<p>Implementing a parallel version of the loop over the array elements involves substituting the <code>range</code> call with <code>prange</code>. There's a caveat—to use <code>prange</code>, the body of the loop must be interpreter-free. As already explained, we need to release the GIL and, since interpreter calls generally<a id="_idIndexMarker702"/> acquire the GIL, they need to be<a id="_idIndexMarker703"/> avoided to make use of threads.</p>
			<p>In Cython, you can release the GIL using the <code>nogil</code> context, as follows:</p>
			<pre>    with nogil: 
        for i in prange(size): 
            out[i] = inp[i]*inp[i]</pre>
			<p>Alternatively, you can use the <code>nogil=True</code> option of <code>prange</code> that will automatically wrap the loop body in a <code>nogil</code> block, as follows:</p>
			<pre>    for i in prange(size, nogil=True): 
        out[i] = inp[i]*inp[i]</pre>
			<p>Attempts to call Python code in a <code>prange</code> block will produce an error. Prohibited operations include function calls, object initialization, and so on. To enable such operations in a <code>prange</code> block (you may want to do so for debugging purposes), you have to re-enable the GIL using a <code>with gil</code> statement, as follows:</p>
			<pre>    for i in prange(size, nogil=True): 
        out[i] = inp[i]*inp[i] 
        with gil:   
            x = 0 # Python assignment</pre>
			<p>We can now test our code by compiling it as a Python extension module. To enable OpenMP support, it is necessary<a id="_idIndexMarker704"/> to change the <code>setup.py</code> file so that<a id="_idIndexMarker705"/> it includes the <code>-fopenmp</code> compilation option. This can be achieved by using the <code>distutils.extension.Extension</code> class in <code>distutils</code> and passing it to <code>cythonize</code>. The complete <code>setup.py</code> file looks like this:</p>
			<pre>    from distutils.core import setup 
    from distutils.extension import Extension 
    from Cython.Build import cythonize 
    hello_parallel = Extension(
        'hello_parallel', 
        ['hello_parallel.pyx'], 
        extra_compile_args=['-fopenmp'], 
        extra_link_args=['-fopenmp']) 
    setup( 
       name='Hello', 
       ext_modules = cythonize(['cevolve.pyx', 
         hello_parallel]), 
    )</pre>
			<p>Using <code>prange</code>, we can easily parallelize the Cython version of our <code>ParticleSimulator</code> class. The following code snippet contains the <code>c_evolve</code> function of the <code>cevolve.pyx</code> Cython module that was written in <a href="B17499_04_Final_SS_ePub.xhtml#_idTextAnchor068"><em class="italic">Chapter 4</em></a>, <em class="italic">C Performance with Cython</em>:</p>
			<pre>    def c_evolve(double[:, :] r_i,double[:] ang_speed_i, \
                 double timestep,int nsteps): 
        # cdef declarations 
        for i in range(nsteps): 
            for j in range(nparticles): 
                # loop body</pre>
			<p>First, we will invert the order of the loops so that the outermost loop will be executed in parallel (each iteration is independent of the other). Since the particles don't interact with each other, we can change<a id="_idIndexMarker706"/> the order of iteration<a id="_idIndexMarker707"/> safely, as shown in the following code snippet:</p>
			<pre>        for j in range(nparticles): 
            for i in range(nsteps): 
                # loop body</pre>
			<p>Next, we will replace the <code>range</code> call of the outer loop with <code>prange</code> and remove calls that acquire the GIL. Since our code was already enhanced with static types, the <code>nogil</code> option can be applied safely, as follows:</p>
			<pre>    for j in prange(nparticles, nogil=True)</pre>
			<p>We can now compare the functions by wrapping them in the <code>benchmark</code> function to assess any performance improvement, as follows:</p>
			<pre>    In [3]: %timeit benchmark(10000, 'openmp') # Running on 
      4 processors
    1 loops, best of 3: 599 ms per loop 
    In [4]: %timeit benchmark(10000, 'cython') 
    1 loops, best of 3: 1.35 s per loop</pre>
			<p>Interestingly, we achieved a two-times speedup by writing a parallel version using <code>prange</code>.</p>
			<p>As we mentioned earlier, normal Python programs have trouble achieving thread parallelism because of the GIL. So far, we worked around this problem using separate processes; starting a process, however, takes significantly more time and memory than starting a thread.</p>
			<p>We also saw that sidestepping<a id="_idIndexMarker708"/> the Python environment allowed us<a id="_idIndexMarker709"/> to achieve a two-times speedup on already fast Cython code. This strategy allowed us to achieve lightweight parallelism but required a separate compilation step. In the next section, we will further explore this strategy using special libraries that are capable of automatically translating our code into a parallel version for efficient execution.</p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor151"/>Automatic parallelism</h1>
			<p>Examples of packages that<a id="_idIndexMarker710"/> implement automatic parallelism are the (by now) familiar <code>numexpr</code> and Numba. Other packages<a id="_idIndexMarker711"/> have been developed to automatically optimize and parallelize array and matrix-intensive expressions, which<a id="_idIndexMarker712"/> are crucial in specific numerical and <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) applications.</p>
			<p><strong class="bold">Theano</strong> is a project that allows you to define<a id="_idIndexMarker713"/> a mathematical expression on arrays (more generally, <em class="italic">tensors</em>), and compile them to a fast language, such as C or C++. Many of the operations that Theano implements are parallelizable and can run on both the CPU and GPU.</p>
			<p><strong class="bold">TensorFlow</strong> is another library that, similar to Theano, is targeted<a id="_idIndexMarker714"/> toward array-intensive mathematical expressions but, rather than translating the expressions to specialized C code, executes the operations on an efficient C++ engine.</p>
			<p>Both Theano and TensorFlow are ideal when the problem at hand can be expressed in a chain of matrix and element-wise operations (such as <em class="italic">neural networks</em>).</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor152"/>Getting started with Theano</h2>
			<p>Theano is somewhat similar<a id="_idIndexMarker715"/> to a compiler but with the added bonus of being able to express, manipulate, and optimize mathematical expressions as well as run code on the CPU and GPU. Since 2010, Theano has improved release after release and has been adopted by several other Python projects as a way to automatically generate efficient computational models on the fly.</p>
			<p>The package may be installed using the following command:</p>
			<pre>$pip install Theano</pre>
			<p>In Theano, you first define the function you want to run by specifying variables and transformation using a pure Python API. This specification will then be compiled into machine code for execution.</p>
			<p>As a first example, let's examine how to implement a function that computes the square of a number. The input will be represented by a scalar variable, <code>a</code>, and then we will transform it to obtain its square, indicated by <code>a_sq</code>. In the following code snippet, we will use the <code>T.scalar</code> function to define a variable and use the normal <code>**</code> operator to obtain a new variable:</p>
			<pre>    import theano.tensor as T
    import theano as th
    a = T.scalar('a')
    a_sq = a ** 2
    print(a_sq)
    # Output:
    # Elemwise{pow,no_inplace}.0</pre>
			<p>As you can see, no specific value is computed, and the transformation we apply is purely symbolic. In order to use this transformation, we need to generate a function. To compile a function, you can use the <code>th.function</code> utility that takes a list of the input variables as its first argument and the output transformation (in our case, <code>a_sq</code>) as its second argument, as illustrated in the following code snippet:</p>
			<pre>    compute_square = th.function([a], a_sq)</pre>
			<p>Theano will take some time and translate the expression to efficient C code and compile it, all in the background! The return value of <code>th.function</code> will be a ready-to-use Python function, and its usage is demonstrated in the next line of code:</p>
			<pre>    compute_square(2)
    4.0</pre>
			<p>Unsurprisingly, <code>compute_square</code> correctly returns the input value squared. Note, however, that the return type is not<a id="_idIndexMarker716"/> an integer (like the input type) but a floating-point number. This is because the Theano default variable type is <code>float64</code>. You can verify that by inspecting the <code>dtype</code> attribute of the <code>a</code> variable, as follows:</p>
			<pre>    a.dtype
    # Result: 
    # float64</pre>
			<p>The Theano behavior is very different compared to what we saw with Numba. Theano doesn't compile generic Python code and, also, doesn't do any type of inference; defining Theano functions requires a more precise specification of the types involved.</p>
			<p>The real power of Theano<a id="_idIndexMarker717"/> comes from its support for array expressions. Defining a <code>T.vector</code> function; the returned variable supports broadcasting operations with the same semantics of NumPy arrays. For instance, we can take two vectors and compute the element-wise sum of their squares, as follows:</p>
			<pre>    a = T.vector('a')
    b = T.vector('b')
    ab_sq = a**2 + b**2
    compute_square = th.function([a, b], ab_sq)
    compute_square([0, 1, 2], [3, 4, 5])
    # Result:
    # array([  9.,  17.,  29.])</pre>
			<p>The idea is, again, to use the Theano API as a mini-language to combine various NumPy array expressions that will be compiled as efficient machine code.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">One of the selling points of Theano is its ability to perform arithmetic simplifications and automatic gradient<a id="_idIndexMarker718"/> calculations. For more information, refer to the official documentation (<a href="https://theano-pymc.readthedocs.io/en/latest/">https://theano-pymc.readthedocs.io/en/latest/</a>).</p>
			<p>To demonstrate Theano functionality on a familiar use case, we can implement our parallel calculation of pi again. Our function will take a collection of two random coordinates as input and return the <code>pi</code> estimate. The input random numbers will be defined as vectors named <code>x</code> and <code>y</code>, and we can test their position inside the circle using a standard element-wise operation that we will store in the <code>hit_test</code> variable, as illustrated in the following code snippet:</p>
			<pre>    x = T.vector('x')
    y = T.vector('y')
    hit_test = x ** 2 + y ** 2 &lt; 1</pre>
			<p>At this point, we need to count<a id="_idIndexMarker719"/> the number of <code>True</code> elements in <code>hit_test</code>, which can be done by taking its sum (it will be implicitly cast to integer). To obtain the <code>pi</code> estimate, we finally need to calculate the ratio of hits versus the total number of trials. The calculation is illustrated in the following code snippet:</p>
			<pre>    hits = hit_test.sum()
    total = x.shape[0]
    pi_est = 4 * hits/total</pre>
			<p>We can benchmark the execution of the Theano implementation using <code>th.function</code> and the <code>timeit</code> module. In our test, we will pass two arrays of size <code>30000</code> and use the <code>timeit.timeit</code> utility to execute the <code>calculate_pi</code> function multiple times, as illustrated in the following code snippet:</p>
			<pre>    calculate_pi = th.function([x, y], pi_est)
    x_val = np.random.uniform(-1, 1, 30000)
    y_val = np.random.uniform(-1, 1, 30000)
    import timeit
    res = timeit.timeit("calculate_pi(x_val, y_val)", \
    "from __main__ import x_val, y_val, calculate_pi", \
      number=100000)
    print(res)
    # Output:
    # 10.905971487998613</pre>
			<p>The serial execution of this function takes about 10 seconds. Theano is capable of automatically parallelizing the code by implementing element-wise and matrix operations using<a id="_idIndexMarker720"/> specialized packages, such as OpenMP and the <strong class="bold">Basic Linear Algebra Subprograms</strong> (<strong class="bold">BLAS</strong>) linear algebra routines. Parallel execution can be enabled using configuration options.</p>
			<p>In Theano, you can set up configuration options by modifying variables in the <code>theano.config</code> object at import<a id="_idIndexMarker721"/> time. For example, you can issue the following commands to enable OpenMP support:</p>
			<pre>import theano
theano.config.openmp = True
theano.config.openmp_elemwise_minsize = 10</pre>
			<p>The parameters<a id="_idIndexMarker722"/> relevant to OpenMP are outlined here:</p>
			<ul>
				<li><code>openmp_elemwise_minsize</code>: This is an integer number that represents the minimum size of the arrays where element-wise parallelization should be enabled (the overhead of the parallelization can harm performance for small arrays).</li>
				<li><code>openmp</code>: This is a Boolean flag that controls the activation of OpenMP compilation (it should be activated by default).</li>
			</ul>
			<p>Controlling the number of threads assigned for OpenMP execution can be done by setting the <code>OMP_NUM_THREADS</code> environmental variable before executing the code.</p>
			<p>We can now write a simple benchmark<a id="_idIndexMarker723"/> to demonstrate OpenMP usage in practice. In a <code>test_theano.py</code> file, we will put the complete code for the <code>pi</code> estimation example, as follows:</p>
			<pre>    # File: test_theano.py
    import numpy as np
    import theano.tensor as T
    import theano as th
    th.config.openmp_elemwise_minsize = 1000
    th.config.openmp = True
    x = T.vector('x')
    y = T.vector('y')
    hit_test = x ** 2 + y ** 2 &lt;= 1
    hits = hit_test.sum()
    misses = x.shape[0]
    pi_est = 4 * hits/misses
    calculate_pi = th.function([x, y], pi_est)
    x_val = np.random.uniform(-1, 1, 30000)
    y_val = np.random.uniform(-1, 1, 30000)
    import timeit
    res = timeit.timeit("calculate_pi(x_val, y_val)", 
                        "from __main__ import x_val, y_val, 
                        calculate_pi", number=100000)
    print(res)</pre>
			<p>At this point, we can run the code from the command line and assess the scaling with an increasing number of threads<a id="_idIndexMarker724"/> by setting the <code>OMP_NUM_THREADS</code> environment variable, as follows:</p>
			<pre>    $ OMP_NUM_THREADS=1 python test_theano.py
    10.905971487998613
    $ OMP_NUM_THREADS=2 python test_theano.py
    7.538279129999864
    $ OMP_NUM_THREADS=3 python test_theano.py
    9.405846934998408
    $ OMP_NUM_THREADS=4 python test_theano.py
    14.634153957000308</pre>
			<p>Interestingly, there is a small speedup when using two threads, but the performance degrades quickly as we increase their number. This means that for this input size, it is not advantageous to use more than two threads as the price you pay to start new threads and synchronize their shared data is higher than the speedup that you can obtain from the parallel execution. </p>
			<p>Achieving good parallel performance can be tricky as this will depend on the specific operations and how they access the underlying data. As a general rule, measuring the performance of a parallel program is crucial, and obtaining substantial speedups is a work of trial and error.</p>
			<p>As an example, we can see that the parallel performance quickly degrades using slightly different code. In our hit test, we used the <code>sum</code> method directly and relied on the explicit casting of the <code>hit_tests</code> Boolean array. If we make the cast explicit, Theano will generate slightly different code that benefits less from multiple threads. We can modify the <code>test_theano.py</code> file to verify this effect, as follows:</p>
			<pre>    # Older version
    # hits = hit_test.sum()
    hits = hit_test.astype('int32').sum()</pre>
			<p>If we rerun our benchmark, we see that the number of threads does not affect the running time significantly, as illustrated<a id="_idIndexMarker725"/> here:</p>
			<pre>    $ OMP_NUM_THREADS=1 python test_theano.py
    5.822126664999814
    $ OMP_NUM_THREADS=2 python test_theano.py
    5.697357518001809
    $ OMP_NUM_THREADS=3 python test_theano.py 
    5.636914656002773
    $ OMP_NUM_THREADS=4 python test_theano.py
    5.764030176000233</pre>
			<p>Despite that, the timings improved considerably compared to the original version.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor153"/>Profiling Theano</h2>
			<p>Given the importance<a id="_idIndexMarker726"/> of measuring and analyzing performance, Theano provides powerful and informative profiling tools. To generate profiling data, the only modification needed is the addition of the <code>profile=True</code> option to <code>th.function</code>, as illustrated in the following code snippet:</p>
			<pre>    calculate_pi = th.function([x, y], pi_est, 
      profile=True)</pre>
			<p>The profiler will collect data as the function is being run (for example, through <code>timeit</code> or direct invocation). The profiling summary can be printed to the output by issuing the <code>summary</code> command, as follows:</p>
			<pre>    calculate_pi.profile.summary()</pre>
			<p>To generate profiling data, we can rerun our script after adding the <code>profile=True</code> option (for this experiment, we will set the <code>OMP_NUM_THREADS</code> environmental variable to <code>1</code>). Also, we will revert our script to the version that performed the casting of <code>hit_tests</code> implicitly.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can also set up profiling globally using the <code>config.profile</code> option.</p>
			<p>The output printed by <code>calculate_pi.profile.summary()</code> is quite long and informative. A part of it is reported in the next block of code. The output is comprised of three sections that refer to timings sorted by <code>Class</code>, <code>Ops</code>, and <code>Apply</code>. In our example, we are concerned with <code>Ops</code>, which roughly maps to the functions used in the Theano compiled code. As you can see here, roughly 80% of the time is spent in taking the element-wise square and sum<a id="_idIndexMarker727"/> of the two numbers, while the rest of the time is spent calculating the sum:</p>
			<pre>Function profiling
==================
  Message: test_theano.py:15
... other output
   Time in 100000 calls to Function.__call__: 1.015549e+01s
... other output
Class
---
&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; 
&lt;#call&gt; &lt;#apply&gt; &lt;Class name&gt;
.... timing info by class
Ops
---
&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; 
&lt;#apply&gt; &lt;Op name&gt;
  80.0%    80.0%       6.722s       6.72e-
05s     C     100000        1   Elemwise{Composite{LT((sqr(
i0) + sqr(i1)), i2)}}
  19.4%    99.4%       1.634s       1.63e-
05s     C     100000        1   Sum{acc_dtype=int64}
   0.3%    99.8%       0.027s       2.66e-
07s     C     100000        1   Elemwise{Composite{((i0 * 
i1) / i2)}}
   0.2%   100.0%       0.020s       2.03e-
07s     C     100000        1   Shape_i{0}
   ... (remaining 0 Ops account for   0.00%(0.00s) of the 
     runtime)
Apply
------
&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;#call&gt; &lt;id&gt; 
&lt;Apply name&gt;
... timing info by apply</pre>
			<p>This information is consistent with what was found in our first benchmark. The code went from about 11 seconds to roughly 8 seconds when two threads were used. From these numbers, we can analyze how the time was spent.</p>
			<p>Out of these 11 seconds, 80% of the time (about 8.8 seconds) was spent doing element-wise operations. This means<a id="_idIndexMarker728"/> that, in perfectly parallel conditions, the increase in speed by adding two threads will be 4.4 seconds. In this scenario, the theoretical execution time would be 6.6 seconds. Considering that we obtained a timing of about 8 seconds, it looks like there is some extra overhead (1.4 seconds) for the thread usage.</p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor154"/>TensorFlow</h2>
			<p>TensorFlow is another library<a id="_idIndexMarker729"/> designed for fast numerical calculations and automatic parallelism. It was released as an open source project by Google in 2015. TensorFlow works by building mathematical expressions similar to Theano, except that the computation is not compiled as machine code but is executed on an external engine written in C++. TensorFlow supports the execution and deployment of parallel code on one or more CPUs and GPUs.</p>
			<p>We can install TensorFlow using the following command:</p>
			<pre>$pip install tensorflow</pre>
			<p class="callout-heading">TensorFlow Version Compatibility</p>
			<p class="callout">Note that<a id="_idIndexMarker730"/> as the default option, TensorFlow 2.x will be installed without further specifications. However, since the number of users of TensorFlow 1.x is still considerable, the code we use next will follow the syntax of TensorFlow 1.x. You can either install version 1 by specifying <code>pip install tensorflow==1.15</code> or disable version 2's behavior using <code>import tensorflow.compat.v1 as tf; tf.disable_v2_behavior()</code> when importing the library, as shown next.</p>
			<p>The usage of TensorFlow is quite similar to that of Theano. To create a variable in TensorFlow, you can use the <code>tf.placeholder</code> function that takes a data type as input, as follows:</p>
			<pre>    import tensorflow.compat.v1 as tf
    tf.disable_v2_behavior()
    a = tf.placeholder('float64')</pre>
			<p>TensorFlow mathematical expressions can be expressed quite similarly to Theano, except for a few different naming conventions as well as more restricted support for the NumPy semantics.</p>
			<p>TensorFlow doesn't compile functions to C and then machine code like Theano does, but serializes the defined mathematical functions (the data structure containing variables<a id="_idIndexMarker731"/> and transformations is called a <code>tf.Session</code> object.</p>
			<p>Once the desired expression is defined, a <code>tf.Session</code> object needs to be initialized and can be used to execute computation graphs using the <code>Session.run</code> method. In the following example, we demonstrate<a id="_idIndexMarker732"/> the usage of the TensorFlow API to implement a simple element-wise sum of squares:</p>
			<pre>    a = tf.placeholder('float64')
    b = tf.placeholder('float64')
    ab_sq = a**2 + b**2
    with tf.Session() as session:
        result = session.run(ab_sq, feed_dict={a: [0, 1, \
          2], b: [3, 4, 5]})
        print(result)
    # Output:
    # array([  9.,  17.,  29.])</pre>
			<p>Parallelism in TensorFlow is achieved automatically by its smart execution engine, and it generally works well without much fiddling. However, note that it is mostly suited for <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) workloads that involve the definition<a id="_idIndexMarker733"/> of complex functions that use a lot of matrix multiplications and calculate their gradient.</p>
			<p>We can now replicate the estimation of pi example using TensorFlow capabilities and benchmark its execution speed and parallelism against the Theano implementation. What we will do is this:</p>
			<ul>
				<li>Define our <code>x</code> and <code>y</code> variables and perform a hit test using broadcasted operations.</li>
				<li>Calculate the sum of <code>hit_tests</code> using the <code>tf.reduce_sum</code> function.</li>
				<li>Initialize a <code>Session</code> object with the <code>inter_op_parallelism_threads</code> and <code>intra_op_parallelism_threads</code> configuration options. These options control the number of threads used for different classes of parallel operations. Note that the first <code>Session</code> instance created with such options sets the number of threads for the whole script (even future <code>Session</code> instances).</li>
			</ul>
			<p>We can now write a script name, <code>test_tensorflow.py</code>, containing the following code. Note that the number of threads<a id="_idIndexMarker734"/> is passed as the first argument of the script (<code>sys.argv[1]</code>):</p>
			<pre>    import tensorflow.compat.v1 as tf
    tf.disable_v2_behavior()
    import numpy as np
    import time
    import sys
    NUM_THREADS = int(sys.argv[1])
    samples = 30000
    print('Num threads', NUM_THREADS)
    x_data = np.random.uniform(-1, 1, samples)
    y_data = np.random.uniform(-1, 1, samples)
    x = tf.placeholder('float64', name='x')
    y = tf.placeholder('float64', name='y')
    hit_tests = x ** 2 + y ** 2 &lt;= 1.0
    hits = tf.reduce_sum(tf.cast(hit_tests, 'int32'))
    with tf.Session
        (config=tf.ConfigProto
            (inter_op_parallelism_threads=NUM_THREADS,
             intra_op_parallelism_threads=NUM_THREADS)) as \
               sess:
        start = time.time()
        for i in range(10000):
            sess.run(hits, {x: x_data, y: y_data})
        print(time.time() - start)</pre>
			<p>If we run the script multiple<a id="_idIndexMarker735"/> times with different values of <code>NUM_THREADS</code>, we see that the performance is quite similar to Theano and that the speedup increased by parallelization is quite modest, as illustrated here:</p>
			<pre>    $ python test_tensorflow.py 1
    13.059704780578613
    $ python test_tensorflow.py 2
    11.938535928726196
    $ python test_tensorflow.py 3
    12.783955574035645
    $ python test_tensorflow.py 4
    12.158143043518066</pre>
			<p>The main advantage of using software<a id="_idIndexMarker736"/> packages such as TensorFlow and Theano is the support for parallel matrix operations that are commonly used in ML algorithms. This is very effective because those operations can achieve impressive performance gains on GPU hardware that is designed to perform these operations with high throughput. </p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor155"/>Running code on a GPU</h2>
			<p>In this subsection, we will demonstrate<a id="_idIndexMarker737"/> the usage of a GPU with Theano and TensorFlow. As an example, we will benchmark the execution of very simple matrix multiplication on the GPU and compare it to its running time on a CPU.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The code in this subsection requires the possession of a GPU. For learning<a id="_idIndexMarker738"/> purposes, it is possible to use the Amazon <strong class="bold">Elastic Compute Cloud</strong> (<strong class="bold">EC2</strong>) service (<a href="https://aws.amazon.com/ec2">https://aws.amazon.com/ec2</a>) to request a GPU-enabled instance.</p>
			<p>The following code performs a simple matrix<a id="_idIndexMarker739"/> multiplication using Theano. We use the <code>T.matrix</code> function to initialize a <code>T.dot</code> method to perform the matrix multiplication:</p>
			<pre>    from theano import function, config
    import theano.tensor as T
    import numpy as np
    import time
    N = 5000
    A_data = np.random.rand(N, N).astype('float32')
    B_data = np.random.rand(N, N).astype('float32')
    A = T.matrix('A')
    B = T.matrix('B')
    f = function([A, B], T.dot(A, B))
    start = time.time()
    f(A_data, B_data)
    print("Matrix multiply ({}) took {} seconds".format(N, \
      time.time() - start))
    print('Device used:', config.device)</pre>
			<p>It is possible to ask Theano to execute this code on a GPU by setting the <code>config.device=gpu</code> option. For added convenience, we can set up the configuration value from the command<a id="_idIndexMarker740"/> line using the <code>THEANO_FLAGS</code> environmental variable, shown as follows. After copying the previous code into the <code>test_theano_matmul.py</code> file, we can benchmark the execution time by issuing the following command:</p>
			<pre>    $ THEANO_FLAGS=device=gpu python test_theano_gpu.py 
    Matrix multiply (5000) took 0.4182612895965576 seconds
    Device used: gpu</pre>
			<p>We can analogously run the same code on the CPU using the <code>device=cpu</code> configuration option, as follows:</p>
			<pre>    $ THEANO_FLAGS=device=cpu python test_theano.py 
    Matrix multiply (5000) took 2.9623231887817383 seconds
    Device used: cpu</pre>
			<p>As you can see, the <em class="italic">GPU is 7.2 times faster than the CPU</em> version for this example!</p>
			<p>For comparison, we may benchmark equivalent code using TensorFlow. The implementation of a TensorFlow version is shown in the next code snippet. The main differences with the Theano version are outlined here:</p>
			<ul>
				<li>The usage of the <code>tf.device</code> config manager that serves to specify the target device (<code>/cpu:0</code> or <code>/gpu:0</code>).</li>
				<li>The matrix multiplication is performed using the <code>tf.matmul</code> operator.</li>
			</ul>
			<p>This is illustrated<a id="_idIndexMarker741"/> in the following code snippet:</p>
			<pre>    import tensorflow as tf
    import time
    import numpy as np
    N = 5000
    A_data = np.random.rand(N, N)
    B_data = np.random.rand(N, N)
    # Creates a graph.
    with tf.device('/gpu:0'):
        A = tf.placeholder('float32')
        B = tf.placeholder('float32')
        C = tf.matmul(A, B)
    with tf.Session() as sess:
        start = time.time()
        sess.run(C, {A: A_data, B: B_data})
        print('Matrix multiply ({}) took: {}'.format(N, \
          time.time() - start))</pre>
			<p>If we run the <code>test_tensorflow_matmul.py</code> script with the appropriate <code>tf.device</code> option, we obtain the following timings:</p>
			<pre>    # Ran with tf.device('/gpu:0')
    Matrix multiply (5000) took: 1.417285680770874
    # Ran with tf.device('/cpu:0')
    Matrix multiply (5000) took: 2.9646761417388916 </pre>
			<p>As you can see, the performance gain is substantial (but not as good as the Theano version) in this simple case.</p>
			<p>Another way to achieve automatic GPU computation<a id="_idIndexMarker742"/> is the now-familiar Numba. With Numba, it is possible<a id="_idIndexMarker743"/> to compile Python code to programs that can be run on a GPU. This flexibility allows for advanced GPU programming as well as more simplified interfaces. In particular, Numba makes extremely easy-to-write, GPU-ready, generalized universal functions.</p>
			<p>In the next example, we will demonstrate how to write a universal function that applies an exponential function on two numbers and sums the results. As we already saw in <a href="B17499_05_Final_SS_ePub.xhtml#_idTextAnchor085"><em class="italic">Chapter 5</em></a>, <em class="italic">Exploring Compilers</em>, this can be accomplished using the <code>nb.vectorize</code> function (we'll also specify the <code>cpu</code> target explicitly). The code is shown here:</p>
			<pre>    import numba as nb
    import math
    @nb.vectorize(target='cpu')
    def expon_cpu(x, y):
        return math.exp(x) + math.exp(y)</pre>
			<p>The <code>expon_cpu</code> universal function can be compiled for the GPU device using the <code>target='cuda'</code> option. Also, note that it is necessary to specify the input types for CUDA universal functions. The implementation of <code>expon_gpu</code> is shown here:</p>
			<pre>    @nb.vectorize(['float32(float32, float32)'], 
      target='cuda')
    def expon_gpu(x, y):
        return math.exp(x) + math.exp(y)</pre>
			<p>We can now benchmark<a id="_idIndexMarker744"/> the execution of the two functions by applying the functions on two arrays of size <code>1000000</code>. Also, note in the following code snippet that we execute the function before measuring the timings to trigger the Numba JIT compilation:</p>
			<pre>    import numpy as np
    import time
    N = 1000000
    niter = 100
    a = np.random.rand(N).astype('float32')
    b = np.random.rand(N).astype('float32')
    # Trigger compilation
    expon_cpu(a, b)
    expon_gpu(a, b)
    # Timing
    start = time.time()
    for i in range(niter):
       expon_cpu(a, b)
    print("CPU:", time.time() - start)
    start = time.time()
    for i in range(niter): 
        expon_gpu(a, b) 
    print("GPU:", time.time() - start) 
    # Output:
    # CPU: 2.4762887954711914
    # GPU: 0.8668839931488037</pre>
			<p>Thanks to the GPU execution, we were able to achieve a three-times speedup over the CPU version. Note that transferring data on the GPU is quite expensive; therefore, GPU execution becomes advantageous<a id="_idIndexMarker745"/> only for very large arrays.</p>
			<p class="callout-heading">When To Use Which Package</p>
			<p class="callout">To close out this chapter, we will include a brief discussion regarding the parallel processing tools that we have examined thus far. First, we have seen how to use <code>multiprocessing</code> to manage multiple processes natively in Python. If you are using Cython, you may appeal to OpenMP to implement parallelism while being able to avoid working with C wrappers.</p>
			<p class="callout">Finally, we study Theano and TensorFlow as two packages that automatically compile array-centric code and parallelize the execution. While these two packages offer similar advantages when it comes to automatic parallelism, at the time of this writing, TensorFlow has gained significant popularity, especially within the DL community, where the parallelism of matrix multiplications is the norm.</p>
			<p class="callout">On the other hand, the active development of Theano stopped in 2018. While the package may still be utilized for automatic parallelism and DL uses, no new versions will be released. For this reason, TensorFlow is often preferred by Python programmers nowadays.</p>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor156"/>Summary</h1>
			<p>Parallel processing is an effective way to improve performance on large datasets. Embarrassingly parallel problems are excellent candidates for parallel execution that can be easily implemented to achieve good performance scaling.</p>
			<p>In this chapter, we illustrated the basics of parallel programming in Python. We learned how to circumvent Python threading limitations by spawning processes using the tools available in the Python standard library. We also explored how to implement a multithreaded program using Cython and OpenMP.</p>
			<p>For more complex problems, we learned how to use the Theano, TensorFlow, and Numba packages to automatically compile array-intensive expressions for parallel execution on CPU and GPU devices.</p>
			<p>In the next chapter, we will learn how to apply parallel programming techniques to build a hands-on application that makes and handles web requests concurrently.</p>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor157"/>Questions</h1>
			<ol>
				<li>Why doesn't running Python code across multiple threads offer any speedup? What is the alternative approach that we have discussed in this chapter?</li>
				<li>In the <code>multiprocessing</code> module, what is the difference between the <code>Process</code> and the <code>Pool</code> interface in terms of implementing multiprocessing?</li>
				<li>On a high level, how do libraries such as Theano and TensorFlow help in parallelizing Python code?</li>
			</ol>
		</div>
	</div>
</div>
</body></html>