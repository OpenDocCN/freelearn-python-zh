<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer143">
			<h1 id="_idParaDest-150"><em class="italic"><a id="_idTextAnchor141"/>Chapter 8</em>: Parallel Processing</h1>
			<p>With parallel processing using multiple cores, you can increase the number of calculations your program can do in a given time frame without needing a faster processor. The main idea is to divide a problem into independent subunits and use multiple cores to solve those subunits in parallel.</p>
			<p>Parallel processing is necessary to tackle large-scale problems. Every day, companies produce massive quantities of data that needs to be stored in multiple computers and analyzed. Scientists and engineers run parallel code on supercomputers to simulate massive systems.</p>
			<p>Parallel processing allows you to take advantage of multicore <strong class="bold">central processing units</strong> (<strong class="bold">CPUs</strong>) as well as <strong class="bold">graphics processing units</strong> (<strong class="bold">GPUs</strong>) that work extremely well with highly parallel problems. </p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Introduction to parallel programming</li>
				<li>Using multiple processes</li>
				<li>Parallel Cython with <strong class="bold">Open Multi-Processing</strong> (<strong class="bold">OpenMP</strong>)</li>
				<li>Automatic parallelism</li>
			</ul>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor142"/>Technical requirements</h1>
			<p>The code files for this chapter can be accessed through this link: <a href="https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter08">https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter08</a>.</p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor143"/>Introduction to parallel programming</h1>
			<p>To parallelize a program, it is necessary to divide<a id="_idIndexMarker621"/> the problem into subunits that can run independently (or almost independently) from each other.</p>
			<p>A problem where the subunits are totally independent<a id="_idIndexMarker622"/> of each other is called <em class="italic">embarrassingly parallel</em>. An element-wise operation on an array is a typical example—the operation needs to only know the element it is handling now. Another example is our particle simulator. Since there are no interactions, each particle can evolve independently from the others. Embarrassingly parallel problems are very easy to implement and perform very well on parallel architectures.</p>
			<p>Other problems may be divided into subunits but must share some data to perform their calculations. In those cases, the implementation is less straightforward and can lead to performance issues because of the communication costs.</p>
			<p>We will illustrate the concept with an example. Imagine that you have a particle simulator, but this time, the particles attract other particles within a certain distance (as shown in the following figure): </p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="image/B17499_Figure_8.1.jpg" alt="Figure 8.1 – Illustration of a neighboring region " width="891" height="370"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Illustration of a neighboring region</p>
			<p>To parallelize this problem, we divide the simulation box into regions and assign each region to a different processor. If we evolve the system one step at a time, some particles will interact with particles in a neighboring region. To perform the next iteration, communication with the new particle positions of the neighboring region is required.</p>
			<p>Communication between processes is costly and can seriously hinder the performance of parallel programs. There exist two main ways to handle data communication in parallel programs: shared memory and distributed memory.</p>
			<p>In <strong class="bold">shared memory</strong>, the subunits have access to the same<a id="_idIndexMarker623"/> memory space. An advantage of this approach is that you don't have to explicitly handle the communication as it is sufficient<a id="_idIndexMarker624"/> to write or read from the shared memory. However, problems arise when multiple processes try to access and change the same memory location at the same time. Care should be taken to avoid such conflicts using synchronization techniques.</p>
			<p>In the <strong class="bold">distributed memory</strong> model, each process is completely separated<a id="_idIndexMarker625"/> from the others and possesses its own memory space. In this case, communication<a id="_idIndexMarker626"/> is handled explicitly between the processes. The communication overhead is typically costlier compared to shared memory as data can potentially travel through a network interface.</p>
			<p>One common way to achieve parallelism with the shared memory model is through <strong class="bold">threads</strong>. Threads are independent subtasks that originate<a id="_idIndexMarker627"/> from a process and share resources, such as memory. This concept is further illustrated in the following diagram: </p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/B17499_Figure_8.2.jpg" alt="Figure 8.2 – Illustration of the difference between threads and processes " width="828" height="271"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – Illustration of the difference between threads and processes</p>
			<p>Threads produce<a id="_idIndexMarker628"/> multiple execution contexts and share the same memory space, while processes provide multiple execution contexts that possess their own memory space, and communication must be handled explicitly.</p>
			<p>Python can spawn and handle threads, but they can't be used to increase performance; due to the Python interpreter design, only one Python instruction<a id="_idIndexMarker629"/> is allowed to run at a time—this mechanism is called the <strong class="bold">Global Interpreter Lock</strong> (<strong class="bold">GIL</strong>). What happens is that each time a thread executes a Python statement, the thread acquires a lock and, when the execution is completed, the same lock is released. Since<a id="_idIndexMarker630"/> the lock can be acquired only by one thread at a time, other threads are prevented from executing Python statements while some other thread holds the lock.</p>
			<p>Even though the GIL prevents parallel execution of Python instructions, threads can still be used to provide concurrency in situations where the lock can be released, such as in time-consuming <strong class="bold">input/output</strong> (<strong class="bold">I/O</strong>) operations or in C extensions.</p>
			<p class="callout-heading">Why Not Remove the GIL?</p>
			<p class="callout">In past years, many attempts have been made, including the most recent <em class="italic">Gilectomy </em>experiment. First, removing the GIL is not an easy task and requires modification of most of the Python data<a id="_idIndexMarker631"/> structures. Additionally, such fine-grained locking can be costly and may introduce substantial performance loss in single-threaded programs. Despite this, some Python implementations (notable examples are Jython and IronPython) do not use the GIL.</p>
			<p>The GIL can be<a id="_idIndexMarker632"/> completely sidestepped using processes instead of threads. Processes don't share the same memory area and are independent of each other—each process has its own interpreter. Processes have a few disadvantages: starting up a new process is generally slower than starting a new thread, they consume more memory, and <strong class="bold">inter-process communication</strong> (<strong class="bold">IPC</strong>) can be slow. On the other hand, processes are still very flexible, and they scale better as they can be distributed on multiple machines.</p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor144"/>GPUs</h2>
			<p>GPUs are special processors designed<a id="_idIndexMarker633"/> for computer graphics applications. Those applications usually require processing the geometry of a <strong class="bold">three-dimensional</strong> (<strong class="bold">3D</strong>) scene and output<a id="_idIndexMarker634"/> an array of pixels to the screen. The operations performed by GPUs involve array and matrix operations on floating-point numbers.</p>
			<p>GPUs are designed to run this graphics-related operation very efficiently, and they achieve this by adopting a highly parallel architecture. Compared to a CPU, a GPU has many more (thousands) of small processing units. GPUs are intended to produce data at about 60 <strong class="bold">frames per second</strong> (<strong class="bold">FPS</strong>), which is much slower than the typical response time of a CPU, which possesses higher clock speeds.</p>
			<p>GPUs possess a very different architecture from a standard CPU and are specialized for computing floating-point operations. Therefore, to compile programs for GPUs, it is necessary to utilize special programming platforms, such as <strong class="bold">Compute Unified Device Architecture</strong> (<strong class="bold">CUDA</strong>) and <strong class="bold">Open Computing Language</strong> (<strong class="bold">OpenCL</strong>).</p>
			<p>CUDA is a proprietary<a id="_idIndexMarker635"/> NVIDIA<a id="_idIndexMarker636"/> technology. It provides an <strong class="bold">application programming interface</strong> (<strong class="bold">API</strong>) that can be accessed from other languages. CUDA provides the <strong class="bold">NVIDIA CUDA Compiler</strong> (<strong class="bold">NVCC</strong>) tool that can be used to compile<a id="_idIndexMarker637"/> GPU programs written in a language such as C (CUDA C), as well as numerous libraries that implement highly optimized mathematical routines.</p>
			<p><strong class="bold">OpenCL</strong> is an open technology with<a id="_idIndexMarker638"/> an ability to write parallel programs that can be compiled for a variety of target devices (CPUs and GPUs of several vendors) and is a good option for non-NVIDIA devices.</p>
			<p>GPU programming sounds wonderful on paper. However, don't throw away your CPU yet. GPU programming is tricky, and only specific use cases benefit from the GPU architecture. Programmers need to be aware of the costs incurred in memory transfers to and from the main memory and how to implement algorithms to take advantage of the GPU architecture.</p>
			<p>Generally, GPUs are great at increasing the number of operations<a id="_idIndexMarker639"/> you can perform per unit of time (also called <strong class="bold">throughput</strong>); however, they require more time to prepare the data for processing. In contrast, CPUs are much faster at producing an individual<a id="_idIndexMarker640"/> result from scratch (also called <strong class="bold">latency</strong>).</p>
			<p>For the right problem, GPUs provide extreme (10 to 100 times) speedup. For this reason, they often constitute a very inexpensive solution (the same speedup will require hundreds of CPUs) to improve the performance<a id="_idIndexMarker641"/> of numerically intensive applications. We will illustrate how to execute some<a id="_idIndexMarker642"/> algorithms on a GPU in the <em class="italic">Automatic parallelism</em> section.</p>
			<p>Having said that, we will begin our discussion on multiprocessing using standard processes in the next section.</p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor145"/>Using multiple processes</h1>
			<p>The standard <strong class="source-inline">multiprocessing</strong> module can be used to quickly parallelize simple tasks by spawning<a id="_idIndexMarker643"/> several processes while avoiding the GIL problem. Its interface is easy to use and includes several utilities to handle task submission and synchronization.</p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor146"/>The Process and Pool classes</h2>
			<p>You can create<a id="_idIndexMarker644"/> a process that runs independently by subclassing <strong class="source-inline">multiprocessing.Process</strong>. You can extend the <strong class="source-inline">__init__</strong> method to initialize<a id="_idIndexMarker645"/> resources, and you can write a portion of the code that<a id="_idIndexMarker646"/> will be executed in a subprocess<a id="_idIndexMarker647"/> by implementing the <strong class="source-inline">Process.run</strong> method. In the following code snippet, we define a <strong class="source-inline">Process</strong> class that will wait for 1 second and print its assigned <strong class="source-inline">id</strong> value:</p>
			<p class="source-code">    import multiprocessing </p>
			<p class="source-code">    import time </p>
			<p class="source-code">    class Process(multiprocessing.Process): </p>
			<p class="source-code">        def __init__(self, id): </p>
			<p class="source-code">            super(Process, self).__init__() </p>
			<p class="source-code">            self.id = id </p>
			<p class="source-code">        def run(self): </p>
			<p class="source-code">            time.sleep(1) </p>
			<p class="source-code">            print("I'm the process with id: </p>
			<p class="source-code">              {}".format(self.id))</p>
			<p>To spawn the process, we must instantiate<a id="_idIndexMarker648"/> the <strong class="source-inline">Process</strong> class and call the <strong class="source-inline">Process.start</strong> method. Note that you don't directly call <strong class="source-inline">Process.run</strong>; the call to <strong class="source-inline">Process.start</strong> will create a new process that, in turn, will call the <strong class="source-inline">Process.run</strong> method. We can<a id="_idIndexMarker649"/> add the following lines at the end of the<a id="_idIndexMarker650"/> preceding snippet to create and start<a id="_idIndexMarker651"/> the new process:</p>
			<p class="source-code">    if __name__ == '__main__': </p>
			<p class="source-code">        p = Process(0) </p>
			<p class="source-code">        p.start()</p>
			<p class="callout-heading">The special __name__ Variable</p>
			<p class="callout">Note that we need to place any code that manages processes inside the <strong class="source-inline">if __name__ == '__main__'</strong> condition, as shown in the previous code snippet, to avoid many undesirable behaviors. All the code shown in this chapter will be assumed to follow this practice.</p>
			<p>The instructions after <strong class="source-inline">Process.start</strong> will be executed immediately without waiting for the <strong class="source-inline">p</strong> process to finish. To wait for the task completion, you can use the <strong class="source-inline">Process.join</strong> method, as follows:</p>
			<p class="source-code">    if __name__ == '__main__': </p>
			<p class="source-code">       p = Process(0) </p>
			<p class="source-code">       p.start() </p>
			<p class="source-code">       p.join()</p>
			<p>We can launch four different<a id="_idIndexMarker652"/> processes that will run in parallel in the same way. In a serial<a id="_idIndexMarker653"/> program, the total required time will be 4 seconds. Since the execution<a id="_idIndexMarker654"/> is concurrent, the resulting wall<a id="_idIndexMarker655"/> clock time will be of 1 second. In the following code snippet, we create four processes that will execute concurrently:</p>
			<p class="source-code">    if __name__ == '__main__': </p>
			<p class="source-code">        processes = Process(1), Process(2), Process(3), </p>
			<p class="source-code">          Process(4) </p>
			<p class="source-code">        [p.start() for p in processes]</p>
			<p>Note that the order of the execution for parallel processes is unpredictable and ultimately depends on how the <strong class="bold">operating system</strong> (<strong class="bold">OS</strong>) schedules this. You can verify this behavior by executing the program multiple times; the order will likely be different between runs.</p>
			<p>The <strong class="source-inline">multiprocessing</strong> module exposes a convenient interface that makes it easy to assign and distribute tasks to a set of processes that reside in the <strong class="source-inline">multiprocessing.Pool</strong> class.</p>
			<p>The <strong class="source-inline">multiprocessing.Pool</strong> class spawns<a id="_idIndexMarker656"/> a set of processes—called <em class="italic">workers</em>—and lets us submit tasks through the <strong class="source-inline">apply</strong>/<strong class="source-inline">apply_async</strong> and <strong class="source-inline">map</strong>/<strong class="source-inline">map_async</strong> methods.</p>
			<p>The <strong class="source-inline">pool.map</strong> method applies a function to each element of a list and returns a list of results. Its usage is equivalent to the built-in (serial) <strong class="source-inline">map</strong>.</p>
			<p>To use a parallel map, you should first initialize a <strong class="source-inline">multiprocessing.Pool</strong> object that takes the number of workers as its first argument; if not provided, that number will be equal to the number of cores<a id="_idIndexMarker657"/> in the system. You can initialize a <strong class="source-inline">multiprocessing.Pool</strong> object<a id="_idIndexMarker658"/> in the following way:</p>
			<p class="source-code">    pool = multiprocessing.Pool() </p>
			<p class="source-code">    pool = multiprocessing.Pool(processes=4)</p>
			<p>Let's see <strong class="source-inline">pool.map</strong> in action. If you have<a id="_idIndexMarker659"/> a function that computes the square of a number, you can<a id="_idIndexMarker660"/> map the function to the list by calling <strong class="source-inline">pool.map</strong> and passing the function and the list of inputs as arguments, as follows:</p>
			<p class="source-code">    def square(x): </p>
			<p class="source-code">        return x * x </p>
			<p class="source-code">    inputs = [0, 1, 2, 3, 4] </p>
			<p class="source-code">    outputs = pool.map(square, inputs)</p>
			<p>The <strong class="source-inline">pool.map_async</strong> function is just like <strong class="source-inline">pool.map</strong> but returns an <strong class="source-inline">AsyncResult</strong> object instead of the actual result. When we call <strong class="source-inline">pool.map</strong>, the execution of the main program is stopped until all the workers are finished processing the result. With <strong class="source-inline">map_async</strong>, the <strong class="source-inline">AsyncResult</strong> object is returned immediately without blocking the main program and the calculations are done in the background. We can then retrieve the result using the <strong class="source-inline">AsyncResult.get</strong> method at any time, as shown in the following lines of code:</p>
			<p class="source-code">    outputs_async = pool.map_async(square, inputs) </p>
			<p class="source-code">    outputs = outputs_async.get()</p>
			<p><strong class="source-inline">pool.apply_async</strong> assigns a task consisting of a single function to one of the workers. It takes the function and its arguments and returns an <strong class="source-inline">AsyncResult</strong> object. We can obtain an effect similar to <strong class="source-inline">map</strong> using <strong class="source-inline">apply_async</strong>, as shown here:</p>
			<p class="source-code">    results_async = [pool.apply_async(square, i) for i in \</p>
			<p class="source-code">      range(100))] </p>
			<p class="source-code">    results = [r.get() for r in results_async]</p>
			<p>To use the<a id="_idIndexMarker661"/> results computed<a id="_idIndexMarker662"/> and returned by these processes, we can simply<a id="_idIndexMarker663"/> access the data stored<a id="_idIndexMarker664"/> in <strong class="source-inline">results</strong>.</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor147"/>The Executor interface</h2>
			<p>From version 3.2 onward, it is possible to execute Python code in parallel using the <strong class="source-inline">Executor</strong> interface provided in the <strong class="source-inline">concurrent.futures</strong> module. We already saw the <strong class="source-inline">Executor</strong> interface in action in the previous<a id="_idIndexMarker665"/> chapter, when we used <strong class="source-inline">ThreadPoolExecutor</strong> to perform multiple tasks concurrently. In this<a id="_idIndexMarker666"/> subsection, we'll demonstrate the usage of the <strong class="source-inline">ProcessPoolExecutor</strong> class.</p>
			<p><strong class="source-inline">ProcessPoolExecutor</strong> exposes a very lean interface, at least when compared to the more featureful <strong class="source-inline">multiprocessing.Pool</strong>. A <strong class="source-inline">ProcessPoolExecutor</strong> class can be instantiated, similar to <strong class="source-inline">ThreadPoolExecutor</strong>, by passing a number of worker threads using the <strong class="source-inline">max_workers</strong> argument (by default, <strong class="source-inline">max_workers</strong> will be the number of CPU cores available). The main methods available to the <strong class="source-inline">ProcessPoolExecutor</strong> class are <strong class="source-inline">submit</strong> and <strong class="source-inline">map</strong>.</p>
			<p>The <strong class="source-inline">submit</strong> method will take a function and return a <strong class="source-inline">Future</strong> instance that will keep track of the execution of the submitted function. The <strong class="source-inline">map</strong> method works similarly to the <strong class="source-inline">pool.map</strong> function, except that it returns an iterator rather than a list. The code is illustrated in the following snippet:</p>
			<p class="source-code">    from concurrent.futures import ProcessPoolExecutor</p>
			<p class="source-code">    executor = ProcessPoolExecutor(max_workers=4)</p>
			<p class="source-code">    fut = executor.submit(square, 2)</p>
			<p class="source-code">    # Result:</p>
			<p class="source-code">    # &lt;Future at 0x7f5b5c030940 state=running&gt;</p>
			<p class="source-code">    result = executor.map(square, [0, 1, 2, 3, 4])</p>
			<p class="source-code">    list(result)</p>
			<p class="source-code">    # Result:</p>
			<p class="source-code">    # [0, 1, 4, 9, 16]</p>
			<p>To extract the result from one or more <strong class="source-inline">Future</strong> instances, you can use the <strong class="source-inline">concurrent.futures.wait</strong> and <strong class="source-inline">concurrent.futures.as_completed</strong> functions. The <strong class="source-inline">wait</strong> function accepts a list of <strong class="source-inline">future</strong> instances and will block the execution of the programs<a id="_idIndexMarker667"/> until all the futures have completed their<a id="_idIndexMarker668"/> execution. The result can then be extracted using the <strong class="source-inline">Future.result</strong> method. The <strong class="source-inline">as_completed</strong> function also accepts a function but will, instead, return an iterator over the results. The code is illustrated in the following snippet:</p>
			<p class="source-code">    from concurrent.futures import wait, as_completed</p>
			<p class="source-code">    fut1 = executor.submit(square, 2)</p>
			<p class="source-code">    fut2 = executor.submit(square, 3)</p>
			<p class="source-code">    wait([fut1, fut2])</p>
			<p class="source-code">    # Then you can extract the results using fut1.result() </p>
			<p class="source-code">      and fut2.result()</p>
			<p class="source-code">    results = as_completed([fut1, fut2])</p>
			<p class="source-code">    list(results)</p>
			<p class="source-code">    # Result:</p>
			<p class="source-code">    # [4, 9]</p>
			<p>Alternatively, you can generate futures using the <strong class="source-inline">asyncio.run_in_executor</strong> function and manipulate the results using all the tools and syntax provided by the <strong class="source-inline">asyncio</strong> libraries so that<a id="_idIndexMarker669"/> you can achieve concurrency and parallelism at the same time.  </p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor148"/>Monte Carlo approximation of pi</h2>
			<p>As an example, we will implement<a id="_idIndexMarker670"/> a canonical, embarrassingly parallel program—<strong class="bold">the Monte Carlo approximation of pi</strong>. Imagine that we have a square of size 2 units; its area will be 4 units. Now, we inscribe<a id="_idIndexMarker671"/> a circle of 1 unit radius in this square; the area of the circle will be <img src="image/Formula_8.1_B17499.png" alt="" width="143" height="48"/>. By substituting the value of <em class="italic">r</em> in the previous equation, we get that the numerical value for the area of the circle is <img src="image/Formula_8.2_B17499.png" alt="" width="177" height="46"/> <em class="italic">= pi</em>. You can refer to the following screenshot for a graphical representation of this:</p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/B17499_Figure_8.3.jpg" alt="Figure 8.3 – Illustration of our strategy of approximation of pi " width="811" height="324"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – Illustration of our strategy of approximation of pi</p>
			<p>If we shoot a lot of random points on this, some points will fall into the circle, which we'll call <em class="italic">hits</em>, while the remaining points, <em class="italic">misses</em>, will be outside the circle. The area of the circle will be proportional to the number of hits, while the area of the square will be proportional to the total number of shots. To get the value of pi, it is sufficient to divide the area of the circle (equal to <em class="italic">pi</em>) by the area of the square (equal to 4), as illustrated in the following code snippet:</p>
			<p class="source-code">    hits/total = area_circle/area_square = pi/4 </p>
			<p class="source-code">    pi = 4 * hits/total</p>
			<p>The strategy we will employ in our program will be as follows:</p>
			<ul>
				<li>Generate a lot of uniformly random (<em class="italic">x</em>, <em class="italic">y</em>) numbers in the range (-1, 1).</li>
				<li>Test whether those numbers lie inside the circle by checking whether <em class="italic">x**2 + y**2 &lt;= 1</em>.</li>
			</ul>
			<p>The first step when writing a parallel<a id="_idIndexMarker672"/> program is to write a serial version and verify that it works. In a real-world scenario, you also want to leave parallelization as the last step of your optimization process—first, because we need to identify the slow parts, and second, parallelization is time-consuming and <em class="italic">gives you at most a speedup equal to the number of processors</em>. The implementation of the serial program is shown here:</p>
			<p class="source-code">    import random </p>
			<p class="source-code">    samples = 1000000 </p>
			<p class="source-code">    hits = 0 </p>
			<p class="source-code">    for i in range(samples): </p>
			<p class="source-code">        x = random.uniform(-1.0, 1.0) </p>
			<p class="source-code">        y = random.uniform(-1.0, 1.0) </p>
			<p class="source-code">        if x**2 + y**2 &lt;= 1: </p>
			<p class="source-code">            hits += 1 </p>
			<p class="source-code">     </p>
			<p class="source-code">    pi = 4.0 * hits/samples</p>
			<p>The accuracy of our approximation will improve as we increase the number of samples. Note that each loop iteration is independent of the other—this problem is embarrassingly parallel.</p>
			<p>To parallelize this code, we can<a id="_idIndexMarker673"/> write a function, called <strong class="source-inline">sample</strong>, that corresponds<a id="_idIndexMarker674"/> to a single hit-miss check. If the sample hits the circle, the function will return <strong class="source-inline">1</strong>; otherwise, it will return <strong class="source-inline">0</strong>. By running <strong class="source-inline">sample</strong> multiple times and summing the results, we'll get the total number of hits. We can run <strong class="source-inline">sample</strong> over multiple processors with <strong class="source-inline">apply_async</strong> and get the results in the following way:</p>
			<p class="source-code">    def sample(): </p>
			<p class="source-code">        x = random.uniform(-1.0, 1.0) </p>
			<p class="source-code">        y = random.uniform(-1.0, 1.0) </p>
			<p class="source-code">        if x**2 + y**2 &lt;= 1: </p>
			<p class="source-code">            return 1 </p>
			<p class="source-code">        else: </p>
			<p class="source-code">            return 0 </p>
			<p class="source-code">    pool = multiprocessing.Pool() </p>
			<p class="source-code">    results_async = [pool.apply_async(sample) for i in \</p>
			<p class="source-code">      range(samples)] </p>
			<p class="source-code">    hits = sum(r.get() for r in results_async)</p>
			<p>We can wrap the two versions<a id="_idIndexMarker675"/> in the <strong class="source-inline">pi_serial</strong> and <strong class="source-inline">pi_apply_async</strong> functions (you can find their implementation in the <strong class="source-inline">pi.py</strong> file) and benchmark the<a id="_idIndexMarker676"/> execution speed, as follows:</p>
			<p class="source-code">$ time python -c 'import pi; pi.pi_serial()'</p>
			<p class="source-code">real    0m0.734s</p>
			<p class="source-code">user    0m0.731s</p>
			<p class="source-code">sys     0m0.004s</p>
			<p class="source-code">$ time python -c 'import pi; pi.pi_apply_async()'</p>
			<p class="source-code">real    1m36.989s</p>
			<p class="source-code">user    1m55.984s</p>
			<p class="source-code">sys     0m50.386</p>
			<p>As shown in the earlier benchmark, our first parallel version literally cripples our code, the reason being that the time spent doing the actual calculation is small compared to the overhead required to send and distribute the tasks to the workers.</p>
			<p>To solve the issue, we have to make the overhead negligible compared to the calculation time. For example, we can ask each worker to handle more than one sample at a time, thus reducing the task communication overhead. We can write a <strong class="source-inline">sample_multiple</strong> function that processes more than one hit and modifies our parallel version by dividing our problem<a id="_idIndexMarker677"/> by 10; more intensive tasks are shown<a id="_idIndexMarker678"/> in the following code snippet:</p>
			<p class="source-code">    def sample_multiple(samples_partial): </p>
			<p class="source-code">        return sum(sample() for i inrange(samples_partial)) </p>
			<p class="source-code">    n_tasks = 10 </p>
			<p class="source-code">    chunk_size = samples/n_tasks </p>
			<p class="source-code">    pool = multiprocessing.Pool() </p>
			<p class="source-code">    results_async = [pool.apply_async(sample_multiple, \</p>
			<p class="source-code">      chunk_size) for i in range(n_tasks)] </p>
			<p class="source-code">    hits = sum(r.get() for r in results_async)</p>
			<p>We can wrap this in a function called <strong class="source-inline">pi_apply_async_chunked</strong> and run it as follows:</p>
			<p class="source-code">$ time python -c 'import pi; pi.pi_apply_async_chunked()'</p>
			<p class="source-code">real    0m0.325s</p>
			<p class="source-code">user    0m0.816s</p>
			<p class="source-code">sys     0m0.008s</p>
			<p>The results are much better; we more than doubled the speed of our program. You can also notice that the <strong class="source-inline">user</strong> metric is larger than <strong class="source-inline">real</strong>; the total CPU time is larger than the total time because more than one CPU worked at the same time. If you increase the number of samples, you will note that the ratio of communication to calculation decreases, giving even better speedups.</p>
			<p>Everything is nice and simple<a id="_idIndexMarker679"/> when dealing with embarrassingly parallel<a id="_idIndexMarker680"/> problems. However, you sometimes have to share data between processes.</p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor149"/>Synchronization and locks</h2>
			<p>Even if <strong class="source-inline">multiprocessing</strong> uses processes (with their own independent memory), it lets you define certain variables<a id="_idIndexMarker681"/> and arrays as shared memory. You can define a shared<a id="_idIndexMarker682"/> variable using <strong class="source-inline">multiprocessing.Value</strong>, passing its data type as a string (<strong class="source-inline">i</strong> for integer, <strong class="source-inline">d</strong> for double, <strong class="source-inline">f</strong> for float, and so on). You can update<a id="_idIndexMarker683"/> the content of the variable through the <strong class="source-inline">value</strong> attribute, as shown<a id="_idIndexMarker684"/> in the following code snippet:</p>
			<p class="source-code">    shared_variable = multiprocessing.Value('f') </p>
			<p class="source-code">    shared_variable.value = 0</p>
			<p>When using shared memory, you should be aware of concurrent access. Imagine that you have a shared integer variable, and each process increments its value multiple times. You will define a <strong class="source-inline">Process</strong> class, as follows:</p>
			<p class="source-code">    class Process(multiprocessing.Process): </p>
			<p class="source-code">        def __init__(self, counter): </p>
			<p class="source-code">            super(Process, self).__init__() </p>
			<p class="source-code">            self.counter = counter </p>
			<p class="source-code">        def run(self): </p>
			<p class="source-code">            for i in range(1000): </p>
			<p class="source-code">                self.counter.value += 1</p>
			<p>You can initialize the shared variable in the main program and pass it to <strong class="source-inline">4</strong> processes, as shown in the following code snippet:</p>
			<p class="source-code">    def main(): </p>
			<p class="source-code">        counter = multiprocessing.Value('i', lock=True) </p>
			<p class="source-code">        counter.value = 0 </p>
			<p class="source-code">        processes = [Process(counter) for i in range(4)] </p>
			<p class="source-code">        [p.start() for p in processes] </p>
			<p class="source-code">        [p.join() for p in processes] # processes are done </p>
			<p class="source-code">        print(counter.value)</p>
			<p>If you run this program (<strong class="source-inline">shared.py</strong> in the code directory), you will note that the final value of <strong class="source-inline">counter</strong> is not <strong class="source-inline">4000</strong>, but it has random values (on my machine, they are between <strong class="source-inline">2000</strong> and <strong class="source-inline">2500</strong>). If we assume that the arithmetic is correct, we can conclude that there's a problem with the parallelization.</p>
			<p>What happens is that multiple<a id="_idIndexMarker685"/> processes are trying to access the same shared<a id="_idIndexMarker686"/> variable at the same time. The situation is best explained by looking<a id="_idIndexMarker687"/> at the following diagram. In a serial<a id="_idIndexMarker688"/> execution, the first process reads the number (<strong class="source-inline">0</strong>), increments it, and writes the new value (<strong class="source-inline">1</strong>); the second process reads the new value (<strong class="source-inline">1</strong>), increments it, and writes it again (<strong class="source-inline">2</strong>). </p>
			<p>In parallel execution, the two processes read the number (<strong class="source-inline">0</strong>), increment it, and write the value (<strong class="source-inline">1</strong>) at the same time, leading to a wrong answer:</p>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/B17499_Figure_8.4.jpg" alt="Figure 8.4 – Multiple processes accessing the same variable, leading to incorrect behavior " width="840" height="309"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.4 – Multiple processes accessing the same variable, leading to incorrect behavior</p>
			<p>To solve this problem, we need to synchronize the access to this variable so that only one process at a time can access, increment, and write the value on the shared variable. This feature is provided by the <strong class="source-inline">multiprocessing.Lock</strong> class. A lock can be acquired and released through the <strong class="source-inline">acquire</strong> and <strong class="source-inline">release</strong> methods respectively or by using the lock as a context manager. Since the lock can be acquired by only one process at a time, this method prevents multiple processes from executing the protected section of code at the same time.</p>
			<p>We can define a global<a id="_idIndexMarker689"/> lock and use<a id="_idIndexMarker690"/> it as a context manager to restrict<a id="_idIndexMarker691"/> access to the counter, as shown in the following<a id="_idIndexMarker692"/> code snippet:</p>
			<p class="source-code">    class Process(multiprocessing.Process): </p>
			<p class="source-code">        def __init__(self, counter): </p>
			<p class="source-code">            super(Process, self).__init__() </p>
			<p class="source-code">            self.counter = counter </p>
			<p class="source-code">        def run(self): </p>
			<p class="source-code">            for i in range(1000): </p>
			<p class="source-code">                with lock: # acquire the lock </p>
			<p class="source-code">                    self.counter.value += 1 </p>
			<p class="source-code">                # release the lock</p>
			<p>Synchronization primitives, such as locks, are essential to solving many problems, but they should be kept to a minimum to improve the performance of your program.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The <strong class="source-inline">multiprocessing</strong> module includes<a id="_idIndexMarker693"/> other communication and synchronization tools; you can refer to the official documentation at <a href="http://docs.python.org/3/library/multiprocessing.html">http://docs.python.org/3/library/multiprocessing.html</a> for a complete reference.</p>
			<p>In <a href="B17499_04_Final_SS_ePub.xhtml#_idTextAnchor068"><em class="italic">Chapter 4</em></a>, <em class="italic">C Performance with Cython</em>, we discussed Cython<a id="_idIndexMarker694"/> as a method of speeding up<a id="_idIndexMarker695"/> our programs. Cython itself also allows parallel<a id="_idIndexMarker696"/> processing via OpenMP, which<a id="_idIndexMarker697"/> we will examine next.</p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor150"/>Parallel Cython with OpenMP</h1>
			<p>Cython provides a convenient<a id="_idIndexMarker698"/> interface to perform shared-memory parallel processing through <em class="italic">OpenMP</em>. This lets you write extremely efficient parallel code directly<a id="_idIndexMarker699"/> in Cython without having to create a C wrapper.</p>
			<p>OpenMP is a specification<a id="_idIndexMarker700"/> and an API designed to write multithreaded, parallel programs. The OpenMP specification includes a series of C preprocessor directives to manage threads and provides communication patterns, load balancing, and other synchronization<a id="_idIndexMarker701"/> features. Several C/C++ and Fortran compilers (including the <strong class="bold">GNU Compiler Collection</strong> (<strong class="bold">GCC</strong>)) implement the OpenMP API.</p>
			<p>We can introduce the Cython parallel features with a small example. Cython provides a simple API based on OpenMP in the <strong class="source-inline">cython.parallel</strong> module. The simplest way to achieve parallelism is through <strong class="source-inline">prange</strong>, which is a construct that automatically distributes loop operations in multiple threads.</p>
			<p>First of all, we can write the serial version of a program that computes the square of each element of a NumPy array in the <strong class="source-inline">hello_parallel.pyx</strong> file. We define a function, <strong class="source-inline">square_serial</strong>, that takes a buffer as input and populates an output array with the squares of the input array elements; <strong class="source-inline">square_serial</strong> is shown in the following code snippet:</p>
			<p class="source-code">    import numpy as np </p>
			<p class="source-code">    def square_serial(double[:] inp): </p>
			<p class="source-code">        cdef int i, size </p>
			<p class="source-code">        cdef double[:] out </p>
			<p class="source-code">        size = inp.shape[0] </p>
			<p class="source-code">        out_np = np.empty(size, 'double') </p>
			<p class="source-code">        out = out_np </p>
			<p class="source-code">        for i in range(size): </p>
			<p class="source-code">            out[i] = inp[i]*inp[i] </p>
			<p class="source-code">        return out_np</p>
			<p>Implementing a parallel version of the loop over the array elements involves substituting the <strong class="source-inline">range</strong> call with <strong class="source-inline">prange</strong>. There's a caveat—to use <strong class="source-inline">prange</strong>, the body of the loop must be interpreter-free. As already explained, we need to release the GIL and, since interpreter calls generally<a id="_idIndexMarker702"/> acquire the GIL, they need to be<a id="_idIndexMarker703"/> avoided to make use of threads.</p>
			<p>In Cython, you can release the GIL using the <strong class="source-inline">nogil</strong> context, as follows:</p>
			<p class="source-code">    with nogil: </p>
			<p class="source-code">        for i in prange(size): </p>
			<p class="source-code">            out[i] = inp[i]*inp[i]</p>
			<p>Alternatively, you can use the <strong class="source-inline">nogil=True</strong> option of <strong class="source-inline">prange</strong> that will automatically wrap the loop body in a <strong class="source-inline">nogil</strong> block, as follows:</p>
			<p class="source-code">    for i in prange(size, nogil=True): </p>
			<p class="source-code">        out[i] = inp[i]*inp[i]</p>
			<p>Attempts to call Python code in a <strong class="source-inline">prange</strong> block will produce an error. Prohibited operations include function calls, object initialization, and so on. To enable such operations in a <strong class="source-inline">prange</strong> block (you may want to do so for debugging purposes), you have to re-enable the GIL using a <strong class="source-inline">with gil</strong> statement, as follows:</p>
			<p class="source-code">    for i in prange(size, nogil=True): </p>
			<p class="source-code">        out[i] = inp[i]*inp[i] </p>
			<p class="source-code">        with gil:   </p>
			<p class="source-code">            x = 0 # Python assignment</p>
			<p>We can now test our code by compiling it as a Python extension module. To enable OpenMP support, it is necessary<a id="_idIndexMarker704"/> to change the <strong class="source-inline">setup.py</strong> file so that<a id="_idIndexMarker705"/> it includes the <strong class="source-inline">-fopenmp</strong> compilation option. This can be achieved by using the <strong class="source-inline">distutils.extension.Extension</strong> class in <strong class="source-inline">distutils</strong> and passing it to <strong class="source-inline">cythonize</strong>. The complete <strong class="source-inline">setup.py</strong> file looks like this:</p>
			<p class="source-code">    from distutils.core import setup </p>
			<p class="source-code">    from distutils.extension import Extension </p>
			<p class="source-code">    from Cython.Build import cythonize </p>
			<p class="source-code">    hello_parallel = Extension(</p>
			<p class="source-code">        'hello_parallel', </p>
			<p class="source-code">        ['hello_parallel.pyx'], </p>
			<p class="source-code">        extra_compile_args=['-fopenmp'], </p>
			<p class="source-code">        extra_link_args=['-fopenmp']) </p>
			<p class="source-code">    setup( </p>
			<p class="source-code">       name='Hello', </p>
			<p class="source-code">       ext_modules = cythonize(['cevolve.pyx', </p>
			<p class="source-code">         hello_parallel]), </p>
			<p class="source-code">    )</p>
			<p>Using <strong class="source-inline">prange</strong>, we can easily parallelize the Cython version of our <strong class="source-inline">ParticleSimulator</strong> class. The following code snippet contains the <strong class="source-inline">c_evolve</strong> function of the <strong class="source-inline">cevolve.pyx</strong> Cython module that was written in <a href="B17499_04_Final_SS_ePub.xhtml#_idTextAnchor068"><em class="italic">Chapter 4</em></a>, <em class="italic">C Performance with Cython</em>:</p>
			<p class="source-code">    def c_evolve(double[:, :] r_i,double[:] ang_speed_i, \</p>
			<p class="source-code">                 double timestep,int nsteps): </p>
			<p class="source-code">        # cdef declarations </p>
			<p class="source-code">        for i in range(nsteps): </p>
			<p class="source-code">            for j in range(nparticles): </p>
			<p class="source-code">                # loop body</p>
			<p>First, we will invert the order of the loops so that the outermost loop will be executed in parallel (each iteration is independent of the other). Since the particles don't interact with each other, we can change<a id="_idIndexMarker706"/> the order of iteration<a id="_idIndexMarker707"/> safely, as shown in the following code snippet:</p>
			<p class="source-code">        for j in range(nparticles): </p>
			<p class="source-code">            for i in range(nsteps): </p>
			<p class="source-code">                # loop body</p>
			<p>Next, we will replace the <strong class="source-inline">range</strong> call of the outer loop with <strong class="source-inline">prange</strong> and remove calls that acquire the GIL. Since our code was already enhanced with static types, the <strong class="source-inline">nogil</strong> option can be applied safely, as follows:</p>
			<p class="source-code">    for j in prange(nparticles, nogil=True)</p>
			<p>We can now compare the functions by wrapping them in the <strong class="source-inline">benchmark</strong> function to assess any performance improvement, as follows:</p>
			<p class="source-code">    In [3]: %timeit benchmark(10000, 'openmp') # Running on </p>
			<p class="source-code">      4 processors</p>
			<p class="source-code">    1 loops, best of 3: 599 ms per loop </p>
			<p class="source-code">    In [4]: %timeit benchmark(10000, 'cython') </p>
			<p class="source-code">    1 loops, best of 3: 1.35 s per loop</p>
			<p>Interestingly, we achieved a two-times speedup by writing a parallel version using <strong class="source-inline">prange</strong>.</p>
			<p>As we mentioned earlier, normal Python programs have trouble achieving thread parallelism because of the GIL. So far, we worked around this problem using separate processes; starting a process, however, takes significantly more time and memory than starting a thread.</p>
			<p>We also saw that sidestepping<a id="_idIndexMarker708"/> the Python environment allowed us<a id="_idIndexMarker709"/> to achieve a two-times speedup on already fast Cython code. This strategy allowed us to achieve lightweight parallelism but required a separate compilation step. In the next section, we will further explore this strategy using special libraries that are capable of automatically translating our code into a parallel version for efficient execution.</p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor151"/>Automatic parallelism</h1>
			<p>Examples of packages that<a id="_idIndexMarker710"/> implement automatic parallelism are the (by now) familiar <strong class="bold">just-in-time</strong> (<strong class="bold">JIT</strong>) compilers <strong class="source-inline">numexpr</strong> and Numba. Other packages<a id="_idIndexMarker711"/> have been developed to automatically optimize and parallelize array and matrix-intensive expressions, which<a id="_idIndexMarker712"/> are crucial in specific numerical and <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) applications.</p>
			<p><strong class="bold">Theano</strong> is a project that allows you to define<a id="_idIndexMarker713"/> a mathematical expression on arrays (more generally, <em class="italic">tensors</em>), and compile them to a fast language, such as C or C++. Many of the operations that Theano implements are parallelizable and can run on both the CPU and GPU.</p>
			<p><strong class="bold">TensorFlow</strong> is another library that, similar to Theano, is targeted<a id="_idIndexMarker714"/> toward array-intensive mathematical expressions but, rather than translating the expressions to specialized C code, executes the operations on an efficient C++ engine.</p>
			<p>Both Theano and TensorFlow are ideal when the problem at hand can be expressed in a chain of matrix and element-wise operations (such as <em class="italic">neural networks</em>).</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor152"/>Getting started with Theano</h2>
			<p>Theano is somewhat similar<a id="_idIndexMarker715"/> to a compiler but with the added bonus of being able to express, manipulate, and optimize mathematical expressions as well as run code on the CPU and GPU. Since 2010, Theano has improved release after release and has been adopted by several other Python projects as a way to automatically generate efficient computational models on the fly.</p>
			<p>The package may be installed using the following command:</p>
			<p class="source-code">$pip install Theano</p>
			<p>In Theano, you first define the function you want to run by specifying variables and transformation using a pure Python API. This specification will then be compiled into machine code for execution.</p>
			<p>As a first example, let's examine how to implement a function that computes the square of a number. The input will be represented by a scalar variable, <strong class="source-inline">a</strong>, and then we will transform it to obtain its square, indicated by <strong class="source-inline">a_sq</strong>. In the following code snippet, we will use the <strong class="source-inline">T.scalar</strong> function to define a variable and use the normal <strong class="source-inline">**</strong> operator to obtain a new variable:</p>
			<p class="source-code">    import theano.tensor as T</p>
			<p class="source-code">    import theano as th</p>
			<p class="source-code">    a = T.scalar('a')</p>
			<p class="source-code">    a_sq = a ** 2</p>
			<p class="source-code">    print(a_sq)</p>
			<p class="source-code">    # Output:</p>
			<p class="source-code">    # Elemwise{pow,no_inplace}.0</p>
			<p>As you can see, no specific value is computed, and the transformation we apply is purely symbolic. In order to use this transformation, we need to generate a function. To compile a function, you can use the <strong class="source-inline">th.function</strong> utility that takes a list of the input variables as its first argument and the output transformation (in our case, <strong class="source-inline">a_sq</strong>) as its second argument, as illustrated in the following code snippet:</p>
			<p class="source-code">    compute_square = th.function([a], a_sq)</p>
			<p>Theano will take some time and translate the expression to efficient C code and compile it, all in the background! The return value of <strong class="source-inline">th.function</strong> will be a ready-to-use Python function, and its usage is demonstrated in the next line of code:</p>
			<p class="source-code">    compute_square(2)</p>
			<p class="source-code">    4.0</p>
			<p>Unsurprisingly, <strong class="source-inline">compute_square</strong> correctly returns the input value squared. Note, however, that the return type is not<a id="_idIndexMarker716"/> an integer (like the input type) but a floating-point number. This is because the Theano default variable type is <strong class="source-inline">float64</strong>. You can verify that by inspecting the <strong class="source-inline">dtype</strong> attribute of the <strong class="source-inline">a</strong> variable, as follows:</p>
			<p class="source-code">    a.dtype</p>
			<p class="source-code">    # Result: </p>
			<p class="source-code">    # float64</p>
			<p>The Theano behavior is very different compared to what we saw with Numba. Theano doesn't compile generic Python code and, also, doesn't do any type of inference; defining Theano functions requires a more precise specification of the types involved.</p>
			<p>The real power of Theano<a id="_idIndexMarker717"/> comes from its support for array expressions. Defining a <strong class="bold">one-dimensional</strong> (<strong class="bold">1D</strong>) vector can be done with the <strong class="source-inline">T.vector</strong> function; the returned variable supports broadcasting operations with the same semantics of NumPy arrays. For instance, we can take two vectors and compute the element-wise sum of their squares, as follows:</p>
			<p class="source-code">    a = T.vector('a')</p>
			<p class="source-code">    b = T.vector('b')</p>
			<p class="source-code">    ab_sq = a**2 + b**2</p>
			<p class="source-code">    compute_square = th.function([a, b], ab_sq)</p>
			<p class="source-code">    compute_square([0, 1, 2], [3, 4, 5])</p>
			<p class="source-code">    # Result:</p>
			<p class="source-code">    # array([  9.,  17.,  29.])</p>
			<p>The idea is, again, to use the Theano API as a mini-language to combine various NumPy array expressions that will be compiled as efficient machine code.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">One of the selling points of Theano is its ability to perform arithmetic simplifications and automatic gradient<a id="_idIndexMarker718"/> calculations. For more information, refer to the official documentation (<a href="https://theano-pymc.readthedocs.io/en/latest/">https://theano-pymc.readthedocs.io/en/latest/</a>).</p>
			<p>To demonstrate Theano functionality on a familiar use case, we can implement our parallel calculation of pi again. Our function will take a collection of two random coordinates as input and return the <strong class="source-inline">pi</strong> estimate. The input random numbers will be defined as vectors named <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong>, and we can test their position inside the circle using a standard element-wise operation that we will store in the <strong class="source-inline">hit_test</strong> variable, as illustrated in the following code snippet:</p>
			<p class="source-code">    x = T.vector('x')</p>
			<p class="source-code">    y = T.vector('y')</p>
			<p class="source-code">    hit_test = x ** 2 + y ** 2 &lt; 1</p>
			<p>At this point, we need to count<a id="_idIndexMarker719"/> the number of <strong class="source-inline">True</strong> elements in <strong class="source-inline">hit_test</strong>, which can be done by taking its sum (it will be implicitly cast to integer). To obtain the <strong class="source-inline">pi</strong> estimate, we finally need to calculate the ratio of hits versus the total number of trials. The calculation is illustrated in the following code snippet:</p>
			<p class="source-code">    hits = hit_test.sum()</p>
			<p class="source-code">    total = x.shape[0]</p>
			<p class="source-code">    pi_est = 4 * hits/total</p>
			<p>We can benchmark the execution of the Theano implementation using <strong class="source-inline">th.function</strong> and the <strong class="source-inline">timeit</strong> module. In our test, we will pass two arrays of size <strong class="source-inline">30000</strong> and use the <strong class="source-inline">timeit.timeit</strong> utility to execute the <strong class="source-inline">calculate_pi</strong> function multiple times, as illustrated in the following code snippet:</p>
			<p class="source-code">    calculate_pi = th.function([x, y], pi_est)</p>
			<p class="source-code">    x_val = np.random.uniform(-1, 1, 30000)</p>
			<p class="source-code">    y_val = np.random.uniform(-1, 1, 30000)</p>
			<p class="source-code">    import timeit</p>
			<p class="source-code">    res = timeit.timeit("calculate_pi(x_val, y_val)", \</p>
			<p class="source-code">    "from __main__ import x_val, y_val, calculate_pi", \</p>
			<p class="source-code">      number=100000)</p>
			<p class="source-code">    print(res)</p>
			<p class="source-code">    # Output:</p>
			<p class="source-code">    # 10.905971487998613</p>
			<p>The serial execution of this function takes about 10 seconds. Theano is capable of automatically parallelizing the code by implementing element-wise and matrix operations using<a id="_idIndexMarker720"/> specialized packages, such as OpenMP and the <strong class="bold">Basic Linear Algebra Subprograms</strong> (<strong class="bold">BLAS</strong>) linear algebra routines. Parallel execution can be enabled using configuration options.</p>
			<p>In Theano, you can set up configuration options by modifying variables in the <strong class="source-inline">theano.config</strong> object at import<a id="_idIndexMarker721"/> time. For example, you can issue the following commands to enable OpenMP support:</p>
			<p class="source-code">import theano</p>
			<p class="source-code">theano.config.openmp = True</p>
			<p class="source-code">theano.config.openmp_elemwise_minsize = 10</p>
			<p>The parameters<a id="_idIndexMarker722"/> relevant to OpenMP are outlined here:</p>
			<ul>
				<li><strong class="source-inline">openmp_elemwise_minsize</strong>: This is an integer number that represents the minimum size of the arrays where element-wise parallelization should be enabled (the overhead of the parallelization can harm performance for small arrays).</li>
				<li><strong class="source-inline">openmp</strong>: This is a Boolean flag that controls the activation of OpenMP compilation (it should be activated by default).</li>
			</ul>
			<p>Controlling the number of threads assigned for OpenMP execution can be done by setting the <strong class="source-inline">OMP_NUM_THREADS</strong> environmental variable before executing the code.</p>
			<p>We can now write a simple benchmark<a id="_idIndexMarker723"/> to demonstrate OpenMP usage in practice. In a <strong class="source-inline">test_theano.py</strong> file, we will put the complete code for the <strong class="source-inline">pi</strong> estimation example, as follows:</p>
			<p class="source-code">    # File: test_theano.py</p>
			<p class="source-code">    import numpy as np</p>
			<p class="source-code">    import theano.tensor as T</p>
			<p class="source-code">    import theano as th</p>
			<p class="source-code">    th.config.openmp_elemwise_minsize = 1000</p>
			<p class="source-code">    th.config.openmp = True</p>
			<p class="source-code">    x = T.vector('x')</p>
			<p class="source-code">    y = T.vector('y')</p>
			<p class="source-code">    hit_test = x ** 2 + y ** 2 &lt;= 1</p>
			<p class="source-code">    hits = hit_test.sum()</p>
			<p class="source-code">    misses = x.shape[0]</p>
			<p class="source-code">    pi_est = 4 * hits/misses</p>
			<p class="source-code">    calculate_pi = th.function([x, y], pi_est)</p>
			<p class="source-code">    x_val = np.random.uniform(-1, 1, 30000)</p>
			<p class="source-code">    y_val = np.random.uniform(-1, 1, 30000)</p>
			<p class="source-code">    import timeit</p>
			<p class="source-code">    res = timeit.timeit("calculate_pi(x_val, y_val)", </p>
			<p class="source-code">                        "from __main__ import x_val, y_val, </p>
			<p class="source-code">                        calculate_pi", number=100000)</p>
			<p class="source-code">    print(res)</p>
			<p>At this point, we can run the code from the command line and assess the scaling with an increasing number of threads<a id="_idIndexMarker724"/> by setting the <strong class="source-inline">OMP_NUM_THREADS</strong> environment variable, as follows:</p>
			<p class="source-code">    $ OMP_NUM_THREADS=1 python test_theano.py</p>
			<p class="source-code">    10.905971487998613</p>
			<p class="source-code">    $ OMP_NUM_THREADS=2 python test_theano.py</p>
			<p class="source-code">    7.538279129999864</p>
			<p class="source-code">    $ OMP_NUM_THREADS=3 python test_theano.py</p>
			<p class="source-code">    9.405846934998408</p>
			<p class="source-code">    $ OMP_NUM_THREADS=4 python test_theano.py</p>
			<p class="source-code">    14.634153957000308</p>
			<p>Interestingly, there is a small speedup when using two threads, but the performance degrades quickly as we increase their number. This means that for this input size, it is not advantageous to use more than two threads as the price you pay to start new threads and synchronize their shared data is higher than the speedup that you can obtain from the parallel execution. </p>
			<p>Achieving good parallel performance can be tricky as this will depend on the specific operations and how they access the underlying data. As a general rule, measuring the performance of a parallel program is crucial, and obtaining substantial speedups is a work of trial and error.</p>
			<p>As an example, we can see that the parallel performance quickly degrades using slightly different code. In our hit test, we used the <strong class="source-inline">sum</strong> method directly and relied on the explicit casting of the <strong class="source-inline">hit_tests</strong> Boolean array. If we make the cast explicit, Theano will generate slightly different code that benefits less from multiple threads. We can modify the <strong class="source-inline">test_theano.py</strong> file to verify this effect, as follows:</p>
			<p class="source-code">    # Older version</p>
			<p class="source-code">    # hits = hit_test.sum()</p>
			<p class="source-code">    hits = hit_test.astype('int32').sum()</p>
			<p>If we rerun our benchmark, we see that the number of threads does not affect the running time significantly, as illustrated<a id="_idIndexMarker725"/> here:</p>
			<p class="source-code">    $ OMP_NUM_THREADS=1 python test_theano.py</p>
			<p class="source-code">    5.822126664999814</p>
			<p class="source-code">    $ OMP_NUM_THREADS=2 python test_theano.py</p>
			<p class="source-code">    5.697357518001809</p>
			<p class="source-code">    $ OMP_NUM_THREADS=3 python test_theano.py </p>
			<p class="source-code">    5.636914656002773</p>
			<p class="source-code">    $ OMP_NUM_THREADS=4 python test_theano.py</p>
			<p class="source-code">    5.764030176000233</p>
			<p>Despite that, the timings improved considerably compared to the original version.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor153"/>Profiling Theano</h2>
			<p>Given the importance<a id="_idIndexMarker726"/> of measuring and analyzing performance, Theano provides powerful and informative profiling tools. To generate profiling data, the only modification needed is the addition of the <strong class="source-inline">profile=True</strong> option to <strong class="source-inline">th.function</strong>, as illustrated in the following code snippet:</p>
			<p class="source-code">    calculate_pi = th.function([x, y], pi_est, </p>
			<p class="source-code">      profile=True)</p>
			<p>The profiler will collect data as the function is being run (for example, through <strong class="source-inline">timeit</strong> or direct invocation). The profiling summary can be printed to the output by issuing the <strong class="source-inline">summary</strong> command, as follows:</p>
			<p class="source-code">    calculate_pi.profile.summary()</p>
			<p>To generate profiling data, we can rerun our script after adding the <strong class="source-inline">profile=True</strong> option (for this experiment, we will set the <strong class="source-inline">OMP_NUM_THREADS</strong> environmental variable to <strong class="source-inline">1</strong>). Also, we will revert our script to the version that performed the casting of <strong class="source-inline">hit_tests</strong> implicitly.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can also set up profiling globally using the <strong class="source-inline">config.profile</strong> option.</p>
			<p>The output printed by <strong class="source-inline">calculate_pi.profile.summary()</strong> is quite long and informative. A part of it is reported in the next block of code. The output is comprised of three sections that refer to timings sorted by <strong class="source-inline">Class</strong>, <strong class="source-inline">Ops</strong>, and <strong class="source-inline">Apply</strong>. In our example, we are concerned with <strong class="source-inline">Ops</strong>, which roughly maps to the functions used in the Theano compiled code. As you can see here, roughly 80% of the time is spent in taking the element-wise square and sum<a id="_idIndexMarker727"/> of the two numbers, while the rest of the time is spent calculating the sum:</p>
			<p class="source-code">Function profiling</p>
			<p class="source-code">==================</p>
			<p class="source-code">  Message: test_theano.py:15</p>
			<p class="source-code">... other output</p>
			<p class="source-code">   Time in 100000 calls to Function.__call__: 1.015549e+01s</p>
			<p class="source-code">... other output</p>
			<p class="source-code">Class</p>
			<p class="source-code">---</p>
			<p class="source-code">&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; </p>
			<p class="source-code">&lt;#call&gt; &lt;#apply&gt; &lt;Class name&gt;</p>
			<p class="source-code">.... timing info by class</p>
			<p class="source-code">Ops</p>
			<p class="source-code">---</p>
			<p class="source-code">&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; </p>
			<p class="source-code">&lt;#apply&gt; &lt;Op name&gt;</p>
			<p class="source-code">  80.0%    80.0%       6.722s       6.72e-</p>
			<p class="source-code">05s     C     100000        1   Elemwise{Composite{LT((sqr(</p>
			<p class="source-code">i0) + sqr(i1)), i2)}}</p>
			<p class="source-code">  19.4%    99.4%       1.634s       1.63e-</p>
			<p class="source-code">05s     C     100000        1   Sum{acc_dtype=int64}</p>
			<p class="source-code">   0.3%    99.8%       0.027s       2.66e-</p>
			<p class="source-code">07s     C     100000        1   Elemwise{Composite{((i0 * </p>
			<p class="source-code">i1) / i2)}}</p>
			<p class="source-code">   0.2%   100.0%       0.020s       2.03e-</p>
			<p class="source-code">07s     C     100000        1   Shape_i{0}</p>
			<p class="source-code">   ... (remaining 0 Ops account for   0.00%(0.00s) of the </p>
			<p class="source-code">     runtime)</p>
			<p class="source-code">Apply</p>
			<p class="source-code">------</p>
			<p class="source-code">&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;#call&gt; &lt;id&gt; </p>
			<p class="source-code">&lt;Apply name&gt;</p>
			<p class="source-code">... timing info by apply</p>
			<p>This information is consistent with what was found in our first benchmark. The code went from about 11 seconds to roughly 8 seconds when two threads were used. From these numbers, we can analyze how the time was spent.</p>
			<p>Out of these 11 seconds, 80% of the time (about 8.8 seconds) was spent doing element-wise operations. This means<a id="_idIndexMarker728"/> that, in perfectly parallel conditions, the increase in speed by adding two threads will be 4.4 seconds. In this scenario, the theoretical execution time would be 6.6 seconds. Considering that we obtained a timing of about 8 seconds, it looks like there is some extra overhead (1.4 seconds) for the thread usage.</p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor154"/>TensorFlow</h2>
			<p>TensorFlow is another library<a id="_idIndexMarker729"/> designed for fast numerical calculations and automatic parallelism. It was released as an open source project by Google in 2015. TensorFlow works by building mathematical expressions similar to Theano, except that the computation is not compiled as machine code but is executed on an external engine written in C++. TensorFlow supports the execution and deployment of parallel code on one or more CPUs and GPUs.</p>
			<p>We can install TensorFlow using the following command:</p>
			<p class="source-code">$pip install tensorflow</p>
			<p class="callout-heading">TensorFlow Version Compatibility</p>
			<p class="callout">Note that<a id="_idIndexMarker730"/> as the default option, TensorFlow 2.x will be installed without further specifications. However, since the number of users of TensorFlow 1.x is still considerable, the code we use next will follow the syntax of TensorFlow 1.x. You can either install version 1 by specifying <strong class="source-inline">pip install tensorflow==1.15</strong> or disable version 2's behavior using <strong class="source-inline">import tensorflow.compat.v1 as tf; tf.disable_v2_behavior()</strong> when importing the library, as shown next.</p>
			<p>The usage of TensorFlow is quite similar to that of Theano. To create a variable in TensorFlow, you can use the <strong class="source-inline">tf.placeholder</strong> function that takes a data type as input, as follows:</p>
			<p class="source-code">    import tensorflow.compat.v1 as tf</p>
			<p class="source-code">    tf.disable_v2_behavior()</p>
			<p class="source-code">    a = tf.placeholder('float64')</p>
			<p>TensorFlow mathematical expressions can be expressed quite similarly to Theano, except for a few different naming conventions as well as more restricted support for the NumPy semantics.</p>
			<p>TensorFlow doesn't compile functions to C and then machine code like Theano does, but serializes the defined mathematical functions (the data structure containing variables<a id="_idIndexMarker731"/> and transformations is called a <strong class="bold">computation graph</strong>) and executes them on specific devices. The configuration of devices and context can be done using the <strong class="source-inline">tf.Session</strong> object.</p>
			<p>Once the desired expression is defined, a <strong class="source-inline">tf.Session</strong> object needs to be initialized and can be used to execute computation graphs using the <strong class="source-inline">Session.run</strong> method. In the following example, we demonstrate<a id="_idIndexMarker732"/> the usage of the TensorFlow API to implement a simple element-wise sum of squares:</p>
			<p class="source-code">    a = tf.placeholder('float64')</p>
			<p class="source-code">    b = tf.placeholder('float64')</p>
			<p class="source-code">    ab_sq = a**2 + b**2</p>
			<p class="source-code">    with tf.Session() as session:</p>
			<p class="source-code">        result = session.run(ab_sq, feed_dict={a: [0, 1, \</p>
			<p class="source-code">          2], b: [3, 4, 5]})</p>
			<p class="source-code">        print(result)</p>
			<p class="source-code">    # Output:</p>
			<p class="source-code">    # array([  9.,  17.,  29.])</p>
			<p>Parallelism in TensorFlow is achieved automatically by its smart execution engine, and it generally works well without much fiddling. However, note that it is mostly suited for <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) workloads that involve the definition<a id="_idIndexMarker733"/> of complex functions that use a lot of matrix multiplications and calculate their gradient.</p>
			<p>We can now replicate the estimation of pi example using TensorFlow capabilities and benchmark its execution speed and parallelism against the Theano implementation. What we will do is this:</p>
			<ul>
				<li>Define our <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> variables and perform a hit test using broadcasted operations.</li>
				<li>Calculate the sum of <strong class="source-inline">hit_tests</strong> using the <strong class="source-inline">tf.reduce_sum</strong> function.</li>
				<li>Initialize a <strong class="source-inline">Session</strong> object with the <strong class="source-inline">inter_op_parallelism_threads</strong> and <strong class="source-inline">intra_op_parallelism_threads</strong> configuration options. These options control the number of threads used for different classes of parallel operations. Note that the first <strong class="source-inline">Session</strong> instance created with such options sets the number of threads for the whole script (even future <strong class="source-inline">Session</strong> instances).</li>
			</ul>
			<p>We can now write a script name, <strong class="source-inline">test_tensorflow.py</strong>, containing the following code. Note that the number of threads<a id="_idIndexMarker734"/> is passed as the first argument of the script (<strong class="source-inline">sys.argv[1]</strong>):</p>
			<p class="source-code">    import tensorflow.compat.v1 as tf</p>
			<p class="source-code">    tf.disable_v2_behavior()</p>
			<p class="source-code">    import numpy as np</p>
			<p class="source-code">    import time</p>
			<p class="source-code">    import sys</p>
			<p class="source-code">    NUM_THREADS = int(sys.argv[1])</p>
			<p class="source-code">    samples = 30000</p>
			<p class="source-code">    print('Num threads', NUM_THREADS)</p>
			<p class="source-code">    x_data = np.random.uniform(-1, 1, samples)</p>
			<p class="source-code">    y_data = np.random.uniform(-1, 1, samples)</p>
			<p class="source-code">    x = tf.placeholder('float64', name='x')</p>
			<p class="source-code">    y = tf.placeholder('float64', name='y')</p>
			<p class="source-code">    hit_tests = x ** 2 + y ** 2 &lt;= 1.0</p>
			<p class="source-code">    hits = tf.reduce_sum(tf.cast(hit_tests, 'int32'))</p>
			<p class="source-code">    with tf.Session</p>
			<p class="source-code">        (config=tf.ConfigProto</p>
			<p class="source-code">            (inter_op_parallelism_threads=NUM_THREADS,</p>
			<p class="source-code">             intra_op_parallelism_threads=NUM_THREADS)) as \</p>
			<p class="source-code">               sess:</p>
			<p class="source-code">        start = time.time()</p>
			<p class="source-code">        for i in range(10000):</p>
			<p class="source-code">            sess.run(hits, {x: x_data, y: y_data})</p>
			<p class="source-code">        print(time.time() - start)</p>
			<p>If we run the script multiple<a id="_idIndexMarker735"/> times with different values of <strong class="source-inline">NUM_THREADS</strong>, we see that the performance is quite similar to Theano and that the speedup increased by parallelization is quite modest, as illustrated here:</p>
			<p class="source-code">    $ python test_tensorflow.py 1</p>
			<p class="source-code">    13.059704780578613</p>
			<p class="source-code">    $ python test_tensorflow.py 2</p>
			<p class="source-code">    11.938535928726196</p>
			<p class="source-code">    $ python test_tensorflow.py 3</p>
			<p class="source-code">    12.783955574035645</p>
			<p class="source-code">    $ python test_tensorflow.py 4</p>
			<p class="source-code">    12.158143043518066</p>
			<p>The main advantage of using software<a id="_idIndexMarker736"/> packages such as TensorFlow and Theano is the support for parallel matrix operations that are commonly used in ML algorithms. This is very effective because those operations can achieve impressive performance gains on GPU hardware that is designed to perform these operations with high throughput. </p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor155"/>Running code on a GPU</h2>
			<p>In this subsection, we will demonstrate<a id="_idIndexMarker737"/> the usage of a GPU with Theano and TensorFlow. As an example, we will benchmark the execution of very simple matrix multiplication on the GPU and compare it to its running time on a CPU.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The code in this subsection requires the possession of a GPU. For learning<a id="_idIndexMarker738"/> purposes, it is possible to use the Amazon <strong class="bold">Elastic Compute Cloud</strong> (<strong class="bold">EC2</strong>) service (<a href="https://aws.amazon.com/ec2">https://aws.amazon.com/ec2</a>) to request a GPU-enabled instance.</p>
			<p>The following code performs a simple matrix<a id="_idIndexMarker739"/> multiplication using Theano. We use the <strong class="source-inline">T.matrix</strong> function to initialize a <strong class="bold">two-dimensional</strong> (<strong class="bold">2D</strong>) array, and then we use the <strong class="source-inline">T.dot</strong> method to perform the matrix multiplication:</p>
			<p class="source-code">    from theano import function, config</p>
			<p class="source-code">    import theano.tensor as T</p>
			<p class="source-code">    import numpy as np</p>
			<p class="source-code">    import time</p>
			<p class="source-code">    N = 5000</p>
			<p class="source-code">    A_data = np.random.rand(N, N).astype('float32')</p>
			<p class="source-code">    B_data = np.random.rand(N, N).astype('float32')</p>
			<p class="source-code">    A = T.matrix('A')</p>
			<p class="source-code">    B = T.matrix('B')</p>
			<p class="source-code">    f = function([A, B], T.dot(A, B))</p>
			<p class="source-code">    start = time.time()</p>
			<p class="source-code">    f(A_data, B_data)</p>
			<p class="source-code">    print("Matrix multiply ({}) took {} seconds".format(N, \</p>
			<p class="source-code">      time.time() - start))</p>
			<p class="source-code">    print('Device used:', config.device)</p>
			<p>It is possible to ask Theano to execute this code on a GPU by setting the <strong class="source-inline">config.device=gpu</strong> option. For added convenience, we can set up the configuration value from the command<a id="_idIndexMarker740"/> line using the <strong class="source-inline">THEANO_FLAGS</strong> environmental variable, shown as follows. After copying the previous code into the <strong class="source-inline">test_theano_matmul.py</strong> file, we can benchmark the execution time by issuing the following command:</p>
			<p class="source-code">    $ THEANO_FLAGS=device=gpu python test_theano_gpu.py </p>
			<p class="source-code">    Matrix multiply (5000) took 0.4182612895965576 seconds</p>
			<p class="source-code">    Device used: gpu</p>
			<p>We can analogously run the same code on the CPU using the <strong class="source-inline">device=cpu</strong> configuration option, as follows:</p>
			<p class="source-code">    $ THEANO_FLAGS=device=cpu python test_theano.py </p>
			<p class="source-code">    Matrix multiply (5000) took 2.9623231887817383 seconds</p>
			<p class="source-code">    Device used: cpu</p>
			<p>As you can see, the <em class="italic">GPU is 7.2 times faster than the CPU</em> version for this example!</p>
			<p>For comparison, we may benchmark equivalent code using TensorFlow. The implementation of a TensorFlow version is shown in the next code snippet. The main differences with the Theano version are outlined here:</p>
			<ul>
				<li>The usage of the <strong class="source-inline">tf.device</strong> config manager that serves to specify the target device (<strong class="source-inline">/cpu:0</strong> or <strong class="source-inline">/gpu:0</strong>).</li>
				<li>The matrix multiplication is performed using the <strong class="source-inline">tf.matmul</strong> operator.</li>
			</ul>
			<p>This is illustrated<a id="_idIndexMarker741"/> in the following code snippet:</p>
			<p class="source-code">    import tensorflow as tf</p>
			<p class="source-code">    import time</p>
			<p class="source-code">    import numpy as np</p>
			<p class="source-code">    N = 5000</p>
			<p class="source-code">    A_data = np.random.rand(N, N)</p>
			<p class="source-code">    B_data = np.random.rand(N, N)</p>
			<p class="source-code">    # Creates a graph.</p>
			<p class="source-code">    with tf.device('/gpu:0'):</p>
			<p class="source-code">        A = tf.placeholder('float32')</p>
			<p class="source-code">        B = tf.placeholder('float32')</p>
			<p class="source-code">        C = tf.matmul(A, B)</p>
			<p class="source-code">    with tf.Session() as sess:</p>
			<p class="source-code">        start = time.time()</p>
			<p class="source-code">        sess.run(C, {A: A_data, B: B_data})</p>
			<p class="source-code">        print('Matrix multiply ({}) took: {}'.format(N, \</p>
			<p class="source-code">          time.time() - start))</p>
			<p>If we run the <strong class="source-inline">test_tensorflow_matmul.py</strong> script with the appropriate <strong class="source-inline">tf.device</strong> option, we obtain the following timings:</p>
			<p class="source-code">    # Ran with tf.device('/gpu:0')</p>
			<p class="source-code">    Matrix multiply (5000) took: 1.417285680770874</p>
			<p class="source-code">    # Ran with tf.device('/cpu:0')</p>
			<p class="source-code">    Matrix multiply (5000) took: 2.9646761417388916 </p>
			<p>As you can see, the performance gain is substantial (but not as good as the Theano version) in this simple case.</p>
			<p>Another way to achieve automatic GPU computation<a id="_idIndexMarker742"/> is the now-familiar Numba. With Numba, it is possible<a id="_idIndexMarker743"/> to compile Python code to programs that can be run on a GPU. This flexibility allows for advanced GPU programming as well as more simplified interfaces. In particular, Numba makes extremely easy-to-write, GPU-ready, generalized universal functions.</p>
			<p>In the next example, we will demonstrate how to write a universal function that applies an exponential function on two numbers and sums the results. As we already saw in <a href="B17499_05_Final_SS_ePub.xhtml#_idTextAnchor085"><em class="italic">Chapter 5</em></a>, <em class="italic">Exploring Compilers</em>, this can be accomplished using the <strong class="source-inline">nb.vectorize</strong> function (we'll also specify the <strong class="source-inline">cpu</strong> target explicitly). The code is shown here:</p>
			<p class="source-code">    import numba as nb</p>
			<p class="source-code">    import math</p>
			<p class="source-code">    @nb.vectorize(target='cpu')</p>
			<p class="source-code">    def expon_cpu(x, y):</p>
			<p class="source-code">        return math.exp(x) + math.exp(y)</p>
			<p>The <strong class="source-inline">expon_cpu</strong> universal function can be compiled for the GPU device using the <strong class="source-inline">target='cuda'</strong> option. Also, note that it is necessary to specify the input types for CUDA universal functions. The implementation of <strong class="source-inline">expon_gpu</strong> is shown here:</p>
			<p class="source-code">    @nb.vectorize(['float32(float32, float32)'], </p>
			<p class="source-code">      target='cuda')</p>
			<p class="source-code">    def expon_gpu(x, y):</p>
			<p class="source-code">        return math.exp(x) + math.exp(y)</p>
			<p>We can now benchmark<a id="_idIndexMarker744"/> the execution of the two functions by applying the functions on two arrays of size <strong class="source-inline">1000000</strong>. Also, note in the following code snippet that we execute the function before measuring the timings to trigger the Numba JIT compilation:</p>
			<p class="source-code">    import numpy as np</p>
			<p class="source-code">    import time</p>
			<p class="source-code">    N = 1000000</p>
			<p class="source-code">    niter = 100</p>
			<p class="source-code">    a = np.random.rand(N).astype('float32')</p>
			<p class="source-code">    b = np.random.rand(N).astype('float32')</p>
			<p class="source-code">    # Trigger compilation</p>
			<p class="source-code">    expon_cpu(a, b)</p>
			<p class="source-code">    expon_gpu(a, b)</p>
			<p class="source-code">    # Timing</p>
			<p class="source-code">    start = time.time()</p>
			<p class="source-code">    for i in range(niter):</p>
			<p class="source-code">       expon_cpu(a, b)</p>
			<p class="source-code">    print("CPU:", time.time() - start)</p>
			<p class="source-code">    start = time.time()</p>
			<p class="source-code">    for i in range(niter): </p>
			<p class="source-code">        expon_gpu(a, b) </p>
			<p class="source-code">    print("GPU:", time.time() - start) </p>
			<p class="source-code">    # Output:</p>
			<p class="source-code">    # CPU: 2.4762887954711914</p>
			<p class="source-code">    # GPU: 0.8668839931488037</p>
			<p>Thanks to the GPU execution, we were able to achieve a three-times speedup over the CPU version. Note that transferring data on the GPU is quite expensive; therefore, GPU execution becomes advantageous<a id="_idIndexMarker745"/> only for very large arrays.</p>
			<p class="callout-heading">When To Use Which Package</p>
			<p class="callout">To close out this chapter, we will include a brief discussion regarding the parallel processing tools that we have examined thus far. First, we have seen how to use <strong class="source-inline">multiprocessing</strong> to manage multiple processes natively in Python. If you are using Cython, you may appeal to OpenMP to implement parallelism while being able to avoid working with C wrappers.</p>
			<p class="callout">Finally, we study Theano and TensorFlow as two packages that automatically compile array-centric code and parallelize the execution. While these two packages offer similar advantages when it comes to automatic parallelism, at the time of this writing, TensorFlow has gained significant popularity, especially within the DL community, where the parallelism of matrix multiplications is the norm.</p>
			<p class="callout">On the other hand, the active development of Theano stopped in 2018. While the package may still be utilized for automatic parallelism and DL uses, no new versions will be released. For this reason, TensorFlow is often preferred by Python programmers nowadays.</p>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor156"/>Summary</h1>
			<p>Parallel processing is an effective way to improve performance on large datasets. Embarrassingly parallel problems are excellent candidates for parallel execution that can be easily implemented to achieve good performance scaling.</p>
			<p>In this chapter, we illustrated the basics of parallel programming in Python. We learned how to circumvent Python threading limitations by spawning processes using the tools available in the Python standard library. We also explored how to implement a multithreaded program using Cython and OpenMP.</p>
			<p>For more complex problems, we learned how to use the Theano, TensorFlow, and Numba packages to automatically compile array-intensive expressions for parallel execution on CPU and GPU devices.</p>
			<p>In the next chapter, we will learn how to apply parallel programming techniques to build a hands-on application that makes and handles web requests concurrently.</p>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor157"/>Questions</h1>
			<ol>
				<li>Why doesn't running Python code across multiple threads offer any speedup? What is the alternative approach that we have discussed in this chapter?</li>
				<li>In the <strong class="source-inline">multiprocessing</strong> module, what is the difference between the <strong class="source-inline">Process</strong> and the <strong class="source-inline">Pool</strong> interface in terms of implementing multiprocessing?</li>
				<li>On a high level, how do libraries such as Theano and TensorFlow help in parallelizing Python code?</li>
			</ol>
		</div>
	</div>
</div>
</body></html>