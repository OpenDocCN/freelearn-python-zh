<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-194"><a id="_idTextAnchor195"/>8</h1>
<h1 id="_idParaDest-195"><a id="_idTextAnchor196"/>Scaling Your Code</h1>
<p>Now that we know how to interact with network devices, we should start thinking about building a solution that scales. But why do we need to scale our code? You might be thinking that the answer is obvious and it is just because it will allow your solution to grow easily as your network grows. But scaling is not just about going up but scaling down too. So, scaling your code means that you are going to build a solution that can follow demand easily, saving resources when not required and using more when required.</p>
<p>You should consider adding scaling capabilities adding scaling capabilities to your network automation solution before writing the code. It should be planned during design time and then executed during development time. The scaling capabilities have to be one of the requirements for building your solution. It also should be a clear milestone during implementation and testing.</p>
<p>In this chapter, we are going to check some techniques used today to scale your code up and down effectively. This will allow your solution to adapt easily to follow network growth and, if necessary, easily scale down to save resources.</p>
<p>We are going to cover the following topics in this chapter:</p>
<ul>
<li>Dealing with multitasking, threads, and coroutines</li>
<li>Adding schedulers and job dispatchers</li>
<li>Using microservices and containers</li>
</ul>
<p>By the end of this chapter, you should have enough information to choose the best scaling solution for your code.</p>
<h1 id="_idParaDest-196"><a id="_idTextAnchor197"/>Technical requirements</h1>
<p>The source code described in this chapter is stored in the GitHub repository at <a href="https://github.com/PacktPublishing/Network-Programming-and-Automation-Essentials/tree/main/Chapter08">https://github.com/PacktPublishing/Network-Programming-and-Automation-Essentials/tree/main/Chapter08</a>.</p>
<h1 id="_idParaDest-197"><a id="_idTextAnchor198"/>Dealing with multitasking, threads, and coroutines</h1>
<p>Multitasking, as the<a id="_idIndexMarker920"/> name suggests, is the capability of doing several tasks at the same time. In computers, a task is also known as a job or a process and there are different techniques for running tasks at the same time.</p>
<p>The capability to run code at the same time allows your system to scale up and down whenever necessary. If you have to communicate with more network devices, just run more code in parallel; if you need fewer devices, just run less code. That will enable your system to scale up and down.</p>
<p>But running code in parallel will have an impact on the available machine resources, and some of these resources will be limited by how your code is consuming them. For instance, if your code is using the network interface to download files, and running one single line of code is already consuming 50 Mbps of the network interface (which is 50% of the 100 Mbps interface), it is not advised to run multiple lines of code in parallel to increase the speed, as the limitation is on the network interface and not in the CPU.</p>
<p>Other factors are also important to be considered when running code in parallel, that is, the other shared resources besides the network, such as the CPU, disk, and memory. In some cases, bottlenecks in the disk might cause more limitations for code parallelism than the CPU, especially using disks mounted over the network. In other cases, a large program consuming lots of memory would block the execution of any other program running in parallel because of a lack of free memory. Therefore, the resources that your process will touch and how they interact will have an impact on how much parallelism is possible.</p>
<p>One thing we should clarify here is the term <strong class="bold">I/O</strong>, which is an acronym for computer <strong class="bold">input/output</strong>. I/O is<a id="_idIndexMarker921"/> used to designate any communication between the CPU of the machine and the external world, such as accessing the disk, writing to memory, or sending data to the network. If your code requires lots of external access and it is, most of the time, waiting to receive a response from external communication, we normally say the code<a id="_idIndexMarker922"/> is <strong class="bold">I/O bound</strong>. An example of slow I/O can be found when accessing remote networks and, in some cases, remote disks. On the other hand, if your code requires more CPU computation than I/O, we normally say the code <a id="_idIndexMarker923"/>is <strong class="bold">CPU bound</strong>. Most network automation systems will be I/O bound because of network device access.</p>
<p>Let’s now investigate a few techniques to run code at the same time in Go and Python.</p>
<h2 id="_idParaDest-198"><a id="_idTextAnchor199"/>Multiprocessing</h2>
<p>In computers, when a program <a id="_idIndexMarker924"/>is loaded in memory to run, it’s called a<a id="_idIndexMarker925"/> process. The program can be either a script or a binary file, but it is normally represented by one single file. This file will be loaded into memory and it is seen by the operating system as a process. The capability of running multiple processes at the same time is called multiprocessing, and it is normally managed by the operating system.</p>
<p>The number of CPUs of the hardware where the processes are running is irrelevant to the multiprocessing capability. The operating system is responsible for allocating the CPU time for all processes that are in memory and ready to run. However, as the number of CPUs, speed of the CPU, and memory are limited, the number of processes that can run at the same time will also be limited. Normally, it depends on the size of the process and how much CPU it consumes.</p>
<p>In most computer languages, multiprocessing is implemented using the <code>fork()</code> system call implemented by the operating system to create a complete copy of the currently running process.</p>
<p>Let’s investigate how we can use multiprocessing in Go and Python.</p>
<h3>Multiprocessing in Python</h3>
<p>In Python, multiprocessing is <a id="_idIndexMarker926"/>accomplished by the standard library called <code>multiprocessing</code>. Full <a id="_idIndexMarker927"/>documentation on <a id="_idIndexMarker928"/>Python <code>multiprocessing</code> can be found at <a href="http://docs.python.org/3/library/multiprocessing">docs.python.org/3/library/multiprocessing</a>.</p>
<p>In the first example, we are going to use the operating system program called <code>ping</code> to target one network node. Then, we are going to make it parallel for multiple targets.</p>
<p>The following is an example for one target network node:</p>
<pre class="source-code">
import subprocess
TARGET = "yahoo.com"
command = ["ping", "-c", "1", TARGET]
response = subprocess.call(
    command,
    stdout=subprocess.DEVNULL,
)
if response == 0:
    print(TARGET, "OK")
else:
    print(TARGET, "FAILED")</pre>
<p>It is important to note that calling <code>ping</code> from Python is not efficient. It will cause more overhead because Python will have to invoke an external program that resides in the filesystem. In order to make the example more efficient, we need to use the ICMP <code>echo request</code> and receive an ICMP <code>echo reply</code> from the Python network sockets, instead of invoking an external program such as <code>ping</code>. One solution is to use the Python third-party library <a id="_idIndexMarker929"/>called <code>pythonping</code> (<a href="https://pypi.org/project/pythonping/">https://pypi.org/project/pythonping/</a>). But there is one caveat: the <code>ping</code> program has <code>setuid</code> to allow ICMP packets to be sent by a non-privileged user. Thus, in order to run with <code>pythonping</code>, you need admin/root privileges (accomplished in Linux using <code>sudo</code>).</p>
<p>The following is the<a id="_idIndexMarker930"/> same example using <code>pythonping</code> for <a id="_idIndexMarker931"/>one target network node:</p>
<pre class="source-code">
import pythonping
TARGET = "yahoo.com"
response = pythonping.ping(TARGET, count=1)
if response.success:
    print(TARGET, "OK")
else:
    print(TARGET, "FAILED")</pre>
<p>Running this program should generate the following output:</p>
<pre class="console">
% sudo python3 single-pyping-example.py
yahoo.com OK</pre>
<p>If you want to <a id="_idIndexMarker932"/>send ICMP requests to multiple targets, you will have to<a id="_idIndexMarker933"/> send them sequentially one after the other. However, a better solution would be to run them in parallel using the <code>multiprocessing</code> Python library. The following is an example of four targets using <code>multiprocessing</code>:</p>
<pre class="source-code">
from pythonping import ping
from multiprocessing import Process
TARGETS = ["yahoo.com", "google.com", "cisco.com", "cern.ch"]
def myping(host):
    response = ping(host, count=1)
    if response.success:
        print("%s OK, latency is %.2fms" % (host, response.rtt_avg_ms))
    else:
        print(host, "FAILED")
def main():
    for host in TARGETS:
        Process(target=myping, args=(host,)).start()
if __name__ == "__main__":
    main()</pre>
<p>If you run the preceding program, you should get an output similar to the following:</p>
<pre class="console">
% sudo python3 multiple-pyping-example.py
google.com OK, latency is 45.31ms
yahoo.com OK, latency is 192.17ms
cisco.com OK, latency is 195.44ms
cern.ch OK, latency is 272.97ms</pre>
<p>Note that the response of each target does not depend on the response of others. Therefore, the output should always be in order from low latency to high latency. In the preceding example, <code>google.com</code> finished first, showing a latency of just 45.31 ms.</p>
<p class="callout-heading">Important note</p>
<p class="callout">It is important to call <code>multiprocessing</code> inside the <code>main()</code> function, or a function that is called from <code>main()</code>. Also, make sure <code>main()</code> can be safely imported by a Python interpreter (use <code>__name__</code> ). You can find more details on why at <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming">https://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming</a>.</p>
<p>Python has additional methods to <a id="_idIndexMarker934"/>invoke code parallelism besides the preceding<a id="_idIndexMarker935"/> example using <code>Process()</code>, called <code>multiprocessing.Pool()</code> and <code>multiprocessing.Queue()</code>. The <code>Pool()</code> class is used to instantiate a pool of workers that can do a job without the need to communicate with each other. The <code>Queue()</code> class is used when communication between processes is required. More on that can be found at <a href="https://docs.python.org/3/library/multiprocessing.html">https://docs.python.org/3/library/multiprocessing.html</a>.</p>
<p>Let’s see how we can use multiprocessing in Go.</p>
<h3>Multiprocessing in Go</h3>
<p>To create processes from a <a id="_idIndexMarker936"/>program, you need to create a copy of the data of the current <a id="_idIndexMarker937"/>running program to a new process. That is what Python’s <code>multiprocessing</code> does. However, Go implements parallelism very differently. Go was designed to work with routines similar to coroutines, they are called goroutines, which manage parallelism at runtime. As goroutines are much more efficient, there is no need to implement multiprocessing natively in Go.</p>
<p>Note that using the <code>exec</code> library, by calling <code>exec.Command()</code> and then <code>Cmd.Start()</code> and <code>Cmd.Wait()</code>, will allow you to create multiple processes at the same time, but it is a call to the operating system to execute an external program. Therefore, it is <a id="_idIndexMarker938"/>not <a id="_idIndexMarker939"/>considered native multiprocessing and is not efficient.</p>
<p>For these reasons, we don’t have an example of multi-processing in Go.</p>
<p>Let’s see now how we do multithreading.</p>
<h2 id="_idParaDest-199"><a id="_idTextAnchor200"/>Multithreading</h2>
<p>In computer languages, a thread is a smaller part of a process, which can have one or multiple threads. The memory is shared between the threads in the same process, in contrast with a process that does not share memory with another process. Therefore, a thread is known as a <a id="_idIndexMarker940"/>lightweight process because it requires less memory, and communication between threads within a process is faster. In consequence, spawning new threads is much faster in comparison with new processes.</p>
<p>A CPU with multithreading capability is a CPU that has the ability to run multiple threads in a single core by providing instruction-level parallelism or thread-level parallelism. This capability is also known <a id="_idIndexMarker941"/>as <strong class="bold">Simultaneous </strong><strong class="bold">Multithreading</strong> (<strong class="bold">SMT</strong>).</p>
<p>One example of SMT is the Intel CPU i9-10900K, which has 10 cores and the capability to run 2 threads at the same time per core, which allows up to 20 simultaneous threads. Intel has created a trademark name for SMT, which they<a id="_idIndexMarker942"/> call <strong class="bold">hyper-threading</strong>. Normally, AMD and Intel x86 CPU architectures can run up to two threads per core.</p>
<p>In contrast, the Oracle SPARC M8 processor has 32 cores that can run 8 threads each, allowing a staggering number of 256 simultaneous threads. More on this amazing CPU can be found at <a href="https://www.oracle.com/us/products/servers-storage/sparc-m8-processor-ds-3864282.pdf">https://www.oracle.com/us/products/servers-storage/sparc-m8-processor-ds-3864282.pdf</a>.</p>
<p>But for the CPU to perform its best using threads, two other requirements are necessary, an operating<a id="_idIndexMarker943"/> system that allows CPU-level multithreading and a computer language that allows the creation of simultaneous threads.</p>
<p>Let’s see how we can use multithreading in Python.</p>
<h3>Multithreading in Python</h3>
<p>Multithreading is the <a id="_idIndexMarker944"/>Achilles’ heel of Python. The main reason is that the Python<a id="_idIndexMarker945"/> interpreter called<a id="_idIndexMarker946"/> CPython (discussed in <a href="B18165_06.xhtml#_idTextAnchor166"><em class="italic">Chapter 6</em></a>) uses<a id="_idIndexMarker947"/> a <strong class="bold">Global Interpreter Lock</strong> (<strong class="bold">GIL</strong>) to make it thread-safe. This has a consequence of not allowing Python code to run multiple threads at the same time in a multithread CPU. GIL also adds overhead and using multithreading might cause the program to run slower in comparison with multiprocessing when more CPU work is required.</p>
<p>Therefore, in Python, multithreading is not recommended for programs that are CPU bound. For network and other I/O-bound programs, multithreading might be faster to spawn and easier to communicate with and save runtime memory. But it is important to note that only one thread will run at a time using the CPython interpreter, so if you require true parallelism, use the <code>multiprocessing</code> library instead.</p>
<p>In Python, the standard library offers multithreading by using the library called <code>threading</code>. So, let’s create one example using multithreading in Python by taking the same targets for ICMP tests used in the code example in the previous section. The following is the same example using ICMP but using threading:</p>
<pre class="source-code">
from pythonping import ping
import threading
TARGETS = ["yahoo.com", "google.com", "cisco.com", "cern.ch"]
class myPing(threading.Thread):
    def __init__(self, host):
        threading.Thread.__init__(self)
        self.host = host
    def run(self):
        response = ping(self.host)
        if response.success:
            print("%s OK, latency is %.2fms" % (self.host, response.rtt_avg_ms))
        else:
            print(self.host, "FAILED")
def main():
    for host in TARGETS:
        myPing(host).start()
if __name__ == "__main__":
    main()</pre>
<p>The <a id="_idIndexMarker948"/>output of running the preceding program will look as<a id="_idIndexMarker949"/> follows:</p>
<pre class="console">
% sudo python3 threads-pyping-example.py
google.com OK, latency is 36.21ms
yahoo.com OK, latency is 136.16ms
cisco.com OK, latency is 144.67ms
cern.ch OK, latency is 215.81ms</pre>
<p>As you can see, the output is quite similar using the <code>threading</code> and <code>multiprocessing</code> libraries, but which one runs faster?</p>
<p>Let’s now run a<a id="_idIndexMarker950"/> test program to compare the speed of using <code>threading</code> and <code>multiprocessing</code> for the ICMP tests. The source code of this program is<a id="_idIndexMarker951"/> included in the GitHub repository for this chapter. The name of the program is <code>performance-thread-process-example.py</code>.</p>
<p>Here is the output of this program running for 10, 20, 50, and 100 ICMP probes:</p>
<pre class="console">
% sudo python3 performance-thread-process-example.py 10
Multi-threading test --- duration 0.015 seconds
Multi-processing test--- duration 0.193 seconds
% sudo python3 performance-thread-process-example.py 20
Multi-threading test --- duration 0.030 seconds
Multi-processing test--- duration 0.315 seconds
% sudo python3 performance-thread-process-example.py 50
Multi-threading test --- duration 2.095 seconds
Multi-processing test--- duration 0.765 seconds
% sudo python3 performance-thread-process-example.py 100
Multi-threading test --- duration 2.273 seconds
Multi-processing test--- duration 1.507 seconds</pre>
<p>As shown in the preceding output, running multithreading in Python for a certain number of threads might be faster. However, as we get close to the number 50, it becomes less effective and runs much slower. It is important to notice that this will depend on where you are running your code. The Python interpreter running on Windows is different from in Linux or even in macOS, but the general idea is the same: more threads mean more overhead for the GIL.</p>
<p>The recommendation is not to use Python multithreading unless you are spawning a small number of threads and are not CPU bound.</p>
<p class="callout-heading">Important note</p>
<p class="callout">Because of the CPython GIL, it is not possible to run parallel threads in Python. Therefore, if your program is CPU bound and requires CPU parallelism, the way to go is to use the <code>multiprocessing</code> library instead of the <code>threading</code> library. More details can be found at <a href="http://docs.python.org/3/library/threading">docs.python.org/3/library/threading</a>.</p>
<p>But if you still<a id="_idIndexMarker952"/> want to use Python with multithreading, there are other <a id="_idIndexMarker953"/>Python interpreters that might provide some capability. One example <a id="_idIndexMarker954"/>is <code>threading</code> module. With PyPy-STM, it is possible for simultaneous threads to run, but you will have to use the <code>transaction</code> module, specifically the <code>TransactionQueue</code> class. More on multithreading using <a id="_idIndexMarker955"/>PyPy-STM can be found at <a href="http://doc.pypy.org/en/latest/stm.html#user-guide">doc.pypy.org/en/latest/stm.html#user-guide</a>.</p>
<p>Now, let’s see how we can do multithreading in Go.</p>
<h3>Multithreading in Go</h3>
<p>Writing code that<a id="_idIndexMarker956"/> scales in Go<a id="_idIndexMarker957"/> does not require the creation of threads or processes. Go implements parallelism very efficiently using goroutines, which are presented as threads to the operating system by the Go runtime. Goroutines will be explained in more detail in the following section, which talks about coroutines.</p>
<p>We will also see how we can run multiple lines of code at the same time using coroutines.</p>
<h2 id="_idParaDest-200"><a id="_idTextAnchor201"/>Coroutines</h2>
<p>The term <em class="italic">coroutine</em> was <a id="_idIndexMarker958"/>coined back in 1958 by Melvin Conway and Joel Erdwinn. Then, the idea was officially introduced in a paper published in the <em class="italic">ACM</em> magazine in 1963.</p>
<p>Despite being very old, the adoption of the term came later with some modern computer languages. Coroutines are essentially code that can be suspended. The concept is like a thread (in multithreading), because it is a small part of the code, and has local variables and its own stack. But the main difference between threads and coroutines in a multitasking system is threads can run in parallel and coroutines are collaborative. Some like to describe the difference as the same as between task concurrency and task parallelism.</p>
<p>Here is the definition taken from <em class="italic">Oracle Multithreaded </em><em class="italic">Programming Guide</em>:</p>
<p><em class="italic">In a multithreaded process on a single processor, the processor can switch execution resources between threads, resulting in concurrent execution. Concurrency indicates that more than one thread is making progress, but the threads are not actually running simultaneously. The switching between threads happens quickly enough that the threads might appear to run simultaneously. In the same multithreaded process in a shared-memory multiprocessor environment, each thread in the process can run concurrently on a separate processor, resulting in parallel execution, which is true </em><em class="italic">simultaneous execution.</em></p>
<p>The source can be found at <a href="https://docs.oracle.com/cd/E36784_01/html/E36868/mtintro-6.html">https://docs.oracle.com/cd/E36784_01/html/E36868/mtintro-6.html</a>.</p>
<p>So, let’s now check how we can use coroutines in Python and then in Go.</p>
<h3>Adding coroutines in Python</h3>
<p>Python has <a id="_idIndexMarker959"/>recently added coroutines to the standard library. They<a id="_idIndexMarker960"/> are part of the module called <code>asyncio</code>. Because of that, you won’t find this capability for older versions of Python; you need at least Python version 3.7.</p>
<p>But when do we use coroutines in Python? The best fit is when you require lots of parallel tasks that are I/O bound, such as a network. For CPU-bound applications, it is always recommended to use <code>multiprocessing</code> instead.</p>
<p>In comparison to <code>threading</code>, <code>asyncio</code> is more useful for our network automation work, because it is I/O bound and scales up more than using <code>threading</code>. In addition, it is even lighter than threads and processes.</p>
<p>Let’s then create the same ICMP probe test using coroutines in Python. The following is an example of the code for the same network targets used in previous examples (you can find this code in  <code>Chapter08/Python/asyncio-example.py</code> in the GitHub repo of the book):</p>
<pre class="source-code">
from <strong class="bold">pythonping</strong> import ping
import asyncio
TARGETS = ["yahoo.com", "google.com", "cisco.com", "cern.ch"]
<strong class="bold">async def myping(host)</strong>:
    response = ping(host)
    if response.success:
        print("%s OK, latency is %.3fms" % (host, response.rtt_avg_ms))
    else:
        print(host, "FAILED")
async def main():
    coroutines = []
    for target in TARGETS:
        coroutines.append(
            asyncio.ensure_future(myping(target)))
    for coroutine in coroutines:
        await coroutine
if __name__ == "__main__":
    asyncio.run(main())</pre>
<p>Running the <a id="_idIndexMarker961"/>preceding program example will generate the following<a id="_idIndexMarker962"/> output:</p>
<pre class="console">
% sudo python3 asyncio-example.py
yahoo.com OK, latency is 192.75ms
google.com OK, latency is 29.93ms
cisco.com OK, latency is 162.89ms
cern.ch OK, latency is 339.76ms</pre>
<p>Note that now, the <a id="_idIndexMarker963"/>first ping reply to be printed is not actually the one that <a id="_idIndexMarker964"/>has the least latency, which shows the program is running sequentially, following the order of the <code>TARGETS</code> variable in the loop. That means the <code>asyncio</code> coroutines are not being suspended to allow others to run when they are blocked. Therefore, this is not a good example of using coroutines if we want to scale up. This is because the library used in the example is <code>pythonping</code>, which is not <code>asyncio</code> compatible and is not suspending the coroutine when it is waiting for the network ICMP response.</p>
<p>We added this example to show how bad it is to use <code>asyncio</code> with coroutines that have code that is incompatible with <code>asyncio</code>. To fix this issue, let’s now use a third-party library for the ICMP probe that is compatible with <code>asyncio</code>, which is called <code>aioping</code>.</p>
<p>The following code only shows the change on the import to add <code>aioping</code> instead of <code>pythonping</code> and the change on the <code>myping()</code> function, where we added an <code>await</code> statement before the <code>ping()</code>function. The other difference is that <code>aioping</code> works with the exception called <code>TimeoutError</code> to detect a non-response of an ICMP request:</p>
<pre class="source-code">
from <strong class="bold">aioping</strong> import ping
async def myping(host):
    <strong class="bold">try:</strong>
        delay = <strong class="bold">await</strong> ping(host)
        print("%s OK, latency is %.3f ms" % (host, delay * 1000))
    <strong class="bold">except TimeoutError:</strong>
        print(host, "FAILED")</pre>
<p>The complete program with the fixes shown previously can be found in the GitHub repository of this book at <code>Chapter08/Python/asyncio-example-fixed.py</code>.</p>
<p>If you run this code now with the fix, it should show something like the following output:</p>
<pre class="console">
% sudo python3 asyncio-example-fixed.py
google.com OK, latency is 40.175 ms
cisco.com OK, latency is 170.222 ms
yahoo.com OK, latency is 181.696 ms
cern.ch OK, latency is 281.662 ms</pre>
<p>Note that, now, the output is based on how fast the targets answer the ICMP request and the output does not follow the <code>TARGETS</code> list order like in the previous example.</p>
<p>The important difference in the preceding code is the usage of <code>await</code> before <code>ping</code>, which indicates to the Python <code>asyncio</code> module that the coroutine may stop and allow another coroutine to run.</p>
<p>Now, you may be wondering whether you could, instead of using the new library, <code>aioping</code>, just add <code>await</code> to the previous example in front of the <code>ping</code> statement in the <code>pythonping</code> library. But that will not work and will generate the following exception:</p>
<pre class="console">
TypeError: object ResponseList can't be used in 'await' expression</pre>
<p>That is because the <code>pythonping</code> library is not compatible with the <code>asyncio</code> module.</p>
<p>Use <code>asyncio</code> whenever<a id="_idIndexMarker965"/> you need to have lots of tasks running because it is very <a id="_idIndexMarker966"/>cheap to use coroutines as a task, much faster and lighter than processes and threads. However, it requires that your application be I/O bound to take advantage of the concurrency of the coroutines. Access to network devices is a good example of a slow I/O-bound application and may be a perfect fit for our network automation cases.</p>
<p class="callout-heading">Important note</p>
<p class="callout">To allow efficient use of coroutines in Python, you have to make sure that the coroutine is suspending execution when there is a wait in I/O (such as a network) to allow other coroutines to run. This is normally indicated by the <code>asyncio</code> statement called <code>await</code>. Indeed, using the third-party library in your coroutine needs to be compatible with <code>asyncio</code>. As the <code>asyncio</code> module is quite new, there are not many third-party libraries that are compatible with <code>asyncio</code>. Without this compatibility, your code will run coroutines sequentially instead of concurrently, and using <code>asyncio</code> will not be a good idea.</p>
<p>Let’s see how coroutines can be used in Go.</p>
<h3>Coroutines in Go</h3>
<p>The Go language is special<a id="_idIndexMarker967"/> and shines best when it requires code to scale up with <a id="_idIndexMarker968"/>performance, and that is accomplished in Go using goroutines.</p>
<p>Goroutines are not the same as coroutines, because they can run like threads in parallel. But they are not like threads either, because they are much smaller (starting with only 8 KB for Go version 1.4) and use channels for communication. This may be confusing at first, but I promise you that goroutines are not difficult to understand and use. Indeed, they are easier to understand and use compared to coroutines in Python.</p>
<p>Since Go version 1.14, goroutines are implemented using asynchronously preemptible scheduling. That means the tasks are no longer in the control of the developer and are entirely managed by Go’s runtime (you can find details at <a href="https://go.dev/doc/go1.14#runtime">https://go.dev/doc/go1.14#runtime</a>). Go’s runtime is responsible for presenting to the operating system the threads that are going to run, which can run simultaneously in some cases.</p>
<p>Go’s runtime is responsible for creating and destroying the threads that correspond to a goroutine. These operations would be much heavier when implemented by the operating system using a native multithreading language, but in Go, they are light as Go’s runtime maintains a pool of threads for the goroutines. The fact that Go’s runtime controls the mapping between goroutines and threads makes the operating system completely unaware of goroutines.</p>
<p>In summary, Go doesn’t use coroutines, but instead uses goroutines, which are not the same and are more like a blend between coroutines and threads, with better performance than the two.</p>
<p>Let’s now go through a simple example of an ICMP probe using goroutines:</p>
<pre class="source-code">
import (
    "fmt"
    "time"
    "github.com/go-ping/ping"
)
func myPing(host string) {
    p, err := ping.NewPinger(host)
    if err != nil {
        panic(err)
    }
    p.Count = 1
    p.SetPrivileged(true)
    if err = p.Run(); err != nil {
        panic(err)
    }
    stats := p.Statistics()
    fmt.Println(host, "OK, latency is", stats.AvgRtt)
}
func main() {
    targets := []string{"yahoo.com", "google.com", "cisco.com", "cern.ch"}
    for _, target := range targets {
        <strong class="bold">go</strong> myPing(target)
    }
    time.Sleep(time.Second * 3) //Wait 3 seconds
}</pre>
<p>If you<a id="_idIndexMarker969"/> run this <a id="_idIndexMarker970"/>program, it should output something like the following:</p>
<pre class="console">
$ go run goroutine-icmp-probe.go
google.com OK, latency is 15.9587ms
cisco.com OK, latency is 163.6334ms
yahoo.com OK, latency is 136.3522ms
cern.ch OK, latency is 225.0571ms</pre>
<p>To use a goroutine, you just need to add <code>go</code> in front of the function you want to call as a goroutine. The <code>go</code> statement indicates that the function can be executed in the background with its own stack and variables. The program then executes the line after the <code>go</code> statement and continues the flow normally without waiting for anything to return from the goroutine. As the ICMP probe request takes a few milliseconds to receive an ICMP response, the program will exit before it can print anything by the goroutines. Therefore, we need to add a sleep time of 3 seconds before finishing the program to make sure all the goroutines that send ICMP requests have received and printed the results. Otherwise, you won’t be able to see any output, because the program will end before the goroutines finish printing the results.</p>
<p>If you want to wait until the goroutines end, Go has mechanisms to communicate and wait until they end. One simple one is using <code>sync.WaitGroup</code>. Let’s now rewrite our previous example, removing the sleep time and adding <code>WaitGroup</code> to wait for the goroutines to finish. The following is the same example that waits until all goroutines end:</p>
<pre class="source-code">
import (
    "fmt"
    "sync"
    "github.com/go-ping/ping"
)
func myping(host string, <strong class="bold">wg *sync.WaitGroup</strong>) {
    <strong class="bold">defer wg.Done()</strong>
    p, err := ping.NewPinger(host)
    if err != nil {
        panic(err)
    }
    p.Count = 1
    p.SetPrivileged(true)
    if err = p.Run(); err != nil {
        panic(err)
    }
    stats := p.Statistics()
    fmt.Println(host, "OK, latency is", stats.AvgRtt)
}
func main() {
    var targets = []string{"yahoo.com", "google.com", "cisco.com", "cern.ch"}
    <strong class="bold">var wg sync.WaitGroup</strong>
<strong class="bold">    wg.Add(len(targets))</strong>
    for _, target := range targets {
        go myping(target, <strong class="bold">&amp;wg</strong>)
    }
    <strong class="bold">wg.Wait(</strong>)
}</pre>
<p>If you run the <a id="_idIndexMarker971"/>preceding code, it should end faster than the previous one because it<a id="_idIndexMarker972"/> does not sleep for 3 seconds; it only waits until all goroutines end, which should be less than half a second.</p>
<p>To allow <code>sync.WaitGroup</code> to work, you have to set a value to it at the beginning using <code>Add()</code>. In the preceding example, it adds <code>4</code>, which is the number of goroutines that will run. Then, you pass the pointer of the variable to each goroutine function (<code>&amp;wg</code>), which will be marked as <code>Done()</code> as the function ends using <code>defer</code> (explained in <a href="B18165_07.xhtml#_idTextAnchor183"><em class="italic">Chapter 7</em></a>).</p>
<p>In the preceding example, we did not generate any communication between the goroutines, as they use the terminal to print. We only passed a pointer to the workgroup variable, called <code>wg</code>. If you want to communicate between goroutines, you can do that by using <code>channel</code>, which can be unidirectional or bidirectional.</p>
<p>More on goroutines<a id="_idIndexMarker973"/> can be found at the following links:</p>
<ul>
<li><em class="italic">Google I/O 2012 - Go Concurrency </em><em class="italic">Patterns</em>: <a href="https://www.youtube.com/watch?v=f6kdp27TYZs">https://www.youtube.com/watch?v=f6kdp27TYZs</a></li>
<li><em class="italic">Google I/O 2013 – Advanced Go Concurrency </em><em class="italic">Patterns</em>: <a href="https://www.youtube.com/watch?v=QDDwwePbDtw">https://www.youtube.com/watch?v=QDDwwePbDtw</a></li>
<li>More documentation on Goroutines can be found at <a href="http://go.dev/doc/effective_go#concurrency">go.dev/doc/effective_go#concurrency</a></li>
</ul>
<p>Before going to the next section, let’s summarize how we scale up in Python and Go. In Python, to make the right choice, use <em class="italic">Figure 8</em><em class="italic">.1</em>:</p>
<div><div><img alt="Figure 8.1 – Python decision-making for scaling your code" src="img/B18165_08_001.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Python decision-making for scaling your code</p>
<p>The diagram in <em class="italic">Figure 8</em><em class="italic">.1</em> shows which Python library to use when scaling your code. If CPU bound, use <code>multiprocessing</code>. If you have too many connections with slow I/O, use <code>asyncio</code>, and if the number of connections is small, use <code>threading</code>.</p>
<p>For Go, there is only one option, which is a goroutine. Easy answer!</p>
<p>Let’s now check how we can scale the system using schedulers and dispatchers.</p>
<h1 id="_idParaDest-201"><a id="_idTextAnchor202"/>Adding schedulers and job dispatchers</h1>
<p>A scheduler <a id="_idIndexMarker974"/>is a system that selects a job for <a id="_idIndexMarker975"/>running, where a <strong class="bold">job</strong> can be understood as a program or part of code that requires running. A dispatcher, on the other hand, is the system that takes the job and places it in the execution queue of a machine. They are complementary, and in some cases, they are treated as the same system. So, for the purpose of this section, we are going to talk about some systems that can do both scheduling and dispatching jobs.</p>
<p>The main objective of using systems that can schedule and dispatch jobs is to gain scale by adding machines that can run more jobs in parallel. It is kind of similar to a single program using multiprocessing but with the difference that the new processes are being executed on another machine.</p>
<p>You could do lots of work to improve the performance of your program, but in the end, you will be bound by the machine’s limitations, and if your application is CPU bound, it will be limited by the number of cores, the number of concurrent threads, and the speed of the CPU used. You could work hard to improve the performance of your code, but to grow more, the only solution is to add more CPU hardware, which can be accomplished by adding machines. The group of machines that are dedicated to running jobs for schedulers and dispatchers is normally<a id="_idIndexMarker976"/> called a <strong class="bold">cluster</strong>.</p>
<p>A cluster of machines that are ready to run jobs in parallel can be installed locally or can be installed in separate locations. The distance between the machines in a cluster adds latency to the communication between the machines and delays data synchronization. Quick synchronization between machines may or may not be relevant, depending on how fast the results are required and how they need to be combined if they depend on each other in time. Quicker results might require a local cluster. A more relaxed time frame for getting results would allow clusters to be located further apart.</p>
<p>Let’s now discuss how we can use a classical scheduler and dispatcher.</p>
<h2 id="_idParaDest-202"><a id="_idTextAnchor203"/>Using classical schedulers and dispatchers</h2>
<p>The classical scheduler<a id="_idIndexMarker977"/> and<a id="_idIndexMarker978"/> dispatcher would be any system that takes one job, deploys it to a machine in the cluster, and executes it. In a classical case, the job is just a program that is ready to run on a machine. The program can be written in any language; however, there are differences in how the installation would be compared between Python and Go.</p>
<p>Let’s investigate what the differences are between using a cluster ready to run Python scripts and ready to run Go-compiled code.</p>
<h3>Considerations for using Go and Python</h3>
<p>If the program was written in <a id="_idIndexMarker979"/>Python, it is required that all machines in the cluster have the Python interpreter version that is compatible with the Python code. For instance, if code was written for Python 3.10, the CPython interpreter to be installed must be at least version 3.10. Another important point here is that all third-party libraries used in the Python script will also have to be installed on all machines. The version of each third-party library must be compatible with the Python<a id="_idIndexMarker980"/> script, as newer versions of a particular third-party library might break the execution of your code. You might need to maintain a table somewhere containing the versions of each third-party library used to avoid wrong machine updates. In conclusion, using Python complicates your cluster installation, management, and updates a lot.</p>
<p>On the other hand, using the <a id="_idIndexMarker981"/>Go language is much simpler for deploying in a cluster. You just need to compile the Go program to the same CPU architecture that the code will run. All third-party libraries used will be added to the compiled code. The versions of each third-party library used in your program will be controlled automatically by your local development environment with the <code>go.sum</code> and <code>go.mod</code> files. In summary, you don’t need to install an interpreter on the machines and don’t need to worry about installing or updating any third-party libraries, which is much simpler.</p>
<p>Let’s now see a few examples of a scheduler and dispatcher for a cluster of machines.</p>
<h3>Using Nomad</h3>
<p>Nomad is an <a id="_idIndexMarker982"/>implementation for job scheduling that was built using Go and is supported by a company called HashiCorp (<a href="https://www.nomadproject.io/">https://www.nomadproject.io/</a>). Nomad<a id="_idIndexMarker983"/> is also very popular for scheduling and launching Docker containers in a cluster of machines, as we are going to see in the next section.</p>
<p>With Nomad, you are able to define a job by writing a configuration file that describes the job and how you want to run it. The job description can be written in any formatted file, such as YAML or TOML, but the default supported format is <strong class="bold">HCL</strong>. Once you have the job description completed, it is then translated to JSON, which will be used on the Nomad API (more details can be found at <a href="https://developer.hashicorp.com/nomad/docs/job-specification">https://developer.hashicorp.com/nomad/docs/job-specification</a>).</p>
<p>Nomad supports several task drivers, which allows you to schedule different kinds of programs. If you are using a Go-compiled program, you will have to use the <code>Fork/Exec</code> driver (further details are available at <a href="https://developer.hashicorp.com/nomad/docs/drivers/exec">https://developer.hashicorp.com/nomad/docs/drivers/exec</a>). Using the <code>Fork/Exec</code> driver, you can execute any program, including Python scripts, but with the caveat of having all third-party libraries and the Python interpreter previously installed on all machines of the cluster, which is not managed by Nomad and must be done on your own separately.</p>
<p>The following is an<a id="_idIndexMarker984"/> example of a job specification for an ICMP probe program:</p>
<pre class="source-code">
job "probe-icmp" {
  region = "us"
  datacenters = ["us-1", "us-12"]
  type = "service"
  update {
    stagger      = "60s"
    max_parallel = 4
  }
  task "probe" {
    driver = "exec"
    config {
      command = "/usr/local/bin/icmp-probe"
    }
    env {
      TARGETS = "cisco.com,yahoo.com,google.com"
    }
    resources {
      cpu    = 700 # approximated in MHz
      memory = 16 # in MBytes
    }
}</pre>
<p>Note that the preceding program example is called <code>icmp-probe</code> and would have to accept the operating system environment variable as input. In our example, the variable is called <code>TARGETS</code>.</p>
<p>Once you have defined your job, you can dispatch<a id="_idIndexMarker985"/> it by issuing the <code>nomad job dispatch &lt;job-description-file&gt;</code> command (more details can be found at <a href="http://developer.hashicorp.com/nomad/docs/commands/job/dispatch">developer.hashicorp.com/nomad/docs/commands/job/dispatch</a>).</p>
<p>Let’s now check how we could use another popular scheduler.</p>
<h3>Using Cronsun</h3>
<p>Cronsun is <a id="_idIndexMarker986"/>another scheduler and dispatcher that works in a similar way to the popular Unix cron but for multiple machines. The goal of Cronsun is to be easy and simple for managing jobs on lots of machines. It is developed in the Go language but can also launch jobs in any language by invoking a shell on the remote machine, such as in the Nomad <code>Fork/Exec</code> driver (more details can be found at <a href="https://github.com/shunfei/cronsun">https://github.com/shunfei/cronsun</a>). It also has a graphical interface that allows easy visualization of the running jobs. Cronsun was built and designed based on another Go third-party package, called <code>robfig/cron</code> (<a href="https://github.com/robfig/cron">https://github.com/robfig/cron</a>).</p>
<p>Using Cronsun, you will be able to launch several jobs on multiple machines, but there is no machine cluster management like in Nomad. Another important point is Cronsun does not work with Linux containers, so it purely focuses on executing Unix shell programs in the remote machine by doing process forking.</p>
<p>Let’s now look at a more complex scheduler.</p>
<h3>Using DolphinScheduler</h3>
<p>DolphinScheduler<a id="_idIndexMarker987"/> is a complete system for scheduling and dispatching jobs that is supported by the Apache Software Foundation. It has many more features compared to Nomad and Cronsun, with workflow capabilities that allow a job to wait for input from another job before executing. It also has a graphical interface that helps to visualize running jobs and dependencies (more <a id="_idIndexMarker988"/>details can be found at <a href="https://dolphinscheduler.apache.org/">https://dolphinscheduler.apache.org/</a>).</p>
<p>Although DolphinScheduler is primarily written in Java, it can dispatch jobs in Python and Go. It is much more complex and has many capabilities that might not be necessary for your requirements to scale up.</p>
<p>There are several other job schedulers and dispatchers that you could use, but some of them are used for specific languages, such as Quartz.NET, used for .NET applications (<a href="https://www.quartz-scheduler.net/">https://www.quartz-scheduler.net/</a>), and <a id="_idIndexMarker989"/>Bree, used for Node.js applications (<a href="https://github.com/breejs/bree">https://github.com/breejs/bree</a>).</p>
<p>Let’s see now how we can use big data schedulers and dispatchers for  carrying out computation at scale in network automation.</p>
<h2 id="_idParaDest-203"><a id="_idTextAnchor204"/>Working with big data</h2>
<p>There are specific <a id="_idIndexMarker990"/>applications that require lots of CPU for data processing. These applications require a system that allows running code with very specialized algorithms focused on data analysis. These are normally referred to as systems and applications for <strong class="bold">big data</strong>.</p>
<p>Big data is a collection of datasets that are too large to be analyzed on just one computer. It is a field that is dominated by data scientists, data engineers, and artificial intelligence engineers. The reason is that they normally analyze a lot of data to extract information, and their work requires a system that scales up a lot in terms of CPU processing. Such scale can only be achieved by using systems that can schedule and dispatch jobs over many computers in a cluster.</p>
<p>The algorithm model used for big data is<a id="_idIndexMarker991"/> called <strong class="bold">MapReduce</strong>. A MapReduce programming model is used to implement analysis on large datasets using an algorithm that runs on several machines in a cluster. Originally, the term MapReduce was related to a Google product, but now it is a term used for programs that deal with big data.</p>
<p>The original paper published by Jeffrey Dean and Sanjay Ghemawat called <em class="italic">MapReduce: Simplified Data Processing on Large Clusters</em> is a good reference and good reading to dive deeper into the subject. The paper is public and can be downloaded from the Google Research page at <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf">https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf</a>.</p>
<p>Let’s see how we can use big data in our network automation.</p>
<h3>Big data and network automation</h3>
<p>Big data is used in network <a id="_idIndexMarker992"/>automation to help with traffic engineering <a id="_idIndexMarker993"/>and optimization. MapReduce<a id="_idIndexMarker994"/> is used to calculate better traffic paths over a combination of traffic demands and routing paths. Traffic demands are collected and stored using the IP source and IP destination, then MapReduce is used to calculate a traffic demand matrix. For this work, routing and traffic information is collected from all network devices using BGP, SNMP, and a flow-based collection such as <code>sflow</code>, <code>ipfix</code>, or <code>netflow</code>. The data collected is normally big and real-time results are required to allow for proper network optimization and traffic engineering on time.</p>
<p>One example would be the collection of IP data flow from the transit routers and peering routers (discussed in <a href="B18165_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>). This flow information would then be analyzed in conjunction with the routing information obtained from the routers. Then, a better routing policy can be applied in real time to select better external paths or network interfaces that are less congested.</p>
<p>Let’s now investigate some popular systems that can be used for big data.</p>
<h3>Using systems for big data</h3>
<p>The two <a id="_idIndexMarker995"/>most <a id="_idIndexMarker996"/>popular open source systems <a id="_idIndexMarker997"/>for big <a id="_idIndexMarker998"/>data<a id="_idIndexMarker999"/> are <strong class="bold">Apache Hadoop</strong> (<a href="https://hadoop.apache.org/">https://hadoop.apache.org/</a>) and <strong class="bold">Apache Spark</strong> (<a href="https://spark.apache.org/">https://spark.apache.org/</a>). Both systems are supported and maintained by the Apache Software <a id="_idIndexMarker1000"/>Foundation (<a href="https://www.apache.org/">https://www.apache.org/</a>) and are used to build large cluster systems to run big data.</p>
<p>The difference between Hadoop and Spark is related to how they perform big data analysis. Hadoop is used for batch job scheduling without real-time requirements. It uses more disk capacity and the response time is more relaxed, so the cluster machines don’t need to be local, and the machines need to have large disks. On the other hand, Spark uses more memory and less disk space, the machines need to be located closer, and the response time is more predictable, therefore it is used for real-time applications.</p>
<p>For our network automation on traffic analysis, either option can be used, but Spark would be preferred for faster and more periodic results. Hadoop would be used to generate monthly and daily reports, but not to interact with real-time routing policies.</p>
<p>Let’s now look at a common problem with having your own cluster.</p>
<h4>Resource allocation and cloud services</h4>
<p>One of the problems with using Hadoop and Spark is that you will need to create your own cluster of machines. That means installing and maintaining the hardware and operating system software. But that is not the main problem. The main problem is that resource utilization will vary throughout the day and the year.</p>
<p>As an example, imagine you are using a big data system in your company to calculate the best path for a particular group of routers during the day. The problem is the collected data to be analyzed will change; in busy hours, you will need more CPU processing to calculate compared to non-busy hours. The difference can be hundreds of CPUs, which will lead to lots of idle CPU hours at the end of the month.</p>
<p>How do you <a id="_idIndexMarker1001"/>solve this issue? By using a cloud-based service provider to allocate machines for your cluster. With it, you can add and remove machines during the day and throughout the week, allowing growth when needed and releasing computing power when not needed. One example is to use AWS’ product called <strong class="bold">Elastic MapReduce</strong> (<strong class="bold">EMR</strong>), which can<a id="_idIndexMarker1002"/> be used with easy machine allocation for your cluster, scaling <a id="_idIndexMarker1003"/>up and down by software (more details can be found at <a href="https://aws.amazon.com/emr/">https://aws.amazon.com/emr/</a>). Similar services can be obtained from other cloud service providers, such as Google, Oracle, or Microsoft.</p>
<p>One important point to observe is that big data systems do not allow running any program or language, but only code that has the MapReduce concept capabilities. So, it is much more specific compared to Nomad or Cronsun, and focuses only on data analysis.</p>
<p>Let’s now check how we can scale using microservices and Linux containers.</p>
<h1 id="_idParaDest-204"><a id="_idTextAnchor205"/>Using microservices and containers</h1>
<p>When software is built based on a combination of small, independent services, we normally say the software was built using microservices architecture. Microservices architecture is a way<a id="_idIndexMarker1004"/> to develop applications by combining small services that might belong or not to the same software development team.</p>
<p>The success of this approach is due to the isolation between each service, which is accomplished by using Linux containers (described in <a href="B18165_02.xhtml#_idTextAnchor041"><em class="italic">Chapter 2</em></a>). Using Linux containers<a id="_idIndexMarker1005"/> is a good way to isolate memory, CPU, networks, and disks. Each Linux container can’t interact with other Linux containers in the same host unless a pre-defined communication channel is established. The communication channels of a service have to use well-documented APIs.</p>
<p>The machine that runs microservices is normally called<a id="_idIndexMarker1006"/> a <strong class="bold">container host</strong> or just a host. A host can have multiple microservices that may or may not communicate with each other. A combination of hosts is called a cluster of container hosts. Some orchestration software is able to spawn several copies of a service in one host or different hosts. Using microservices architecture is a good way to scale your system.</p>
<p>One very popular place to build and publish a microservice<a id="_idIndexMarker1007"/> is <strong class="bold">Docker</strong> (<a href="https://www.docker.com/">https://www.docker.com/</a>). A Docker container<a id="_idIndexMarker1008"/> is normally referred to as a service that is built using a Linux container. A Docker host<a id="_idIndexMarker1009"/> is where a Docker container can run, and in a similar way, a Docker cluster is a group of hosts that can run Docker containers.</p>
<p>Let’s see now how we can use Docker containers to scale our code.</p>
<h2 id="_idParaDest-205"><a id="_idTextAnchor206"/>Building a scalable solution by example</h2>
<p>Let’s build a <a id="_idIndexMarker1010"/>solution using microservices architecture by creating our own Docker container and then launching it multiple times. Our service has a few requirements, as follows:</p>
<ul>
<li>It needs to have an API to accept requests</li>
<li>The API needs to accept a list of targets</li>
<li>An ICMP probe will be sent to each target to verify latency concurrently</li>
<li>The API will respond using HTTP plain text</li>
<li>Each service can accept up to 1,000 targets</li>
<li>The timeout for each ICMP probe must be 2 seconds</li>
</ul>
<p>Based on these requirements, let’s write some code that will be used in our service.</p>
<h3>Writing the service code</h3>
<p>With the previous<a id="_idIndexMarker1011"/> requirements, let’s write some code in Go to build our service. We are going to use the Go third-party package for ICMP that we used before in this chapter called <code>go-ping/ping</code>, and <code>sync.WaitGroup</code> to wait for the goroutines to end.</p>
<p>Let’s break the code into two blocks. The second block of code is as follows, describing the <code>probeTargets()</code> and <code>main()</code> functions:</p>
<pre class="source-code">
func <strong class="bold">probeTargets</strong>(w http.ResponseWriter, r *http.Request) {
    httpTargets := r.URL.Query().Get("targets")
    targets := strings.Split(httpTargets, ",")
    if len(httpTargets) == 0 || len(targets) &gt; 1000{
        fmt.Fprintf(w, "error: 0 &lt; targets &lt; 1000\n")
        return
    }
    var wg sync.WaitGroup
    wg.Add(len(targets))
    for _, target := range targets {
        log.Println("requested ICMP probe for", target)
        go <strong class="bold">probe</strong>(target, w, &amp;wg)
    }
    wg.Wait()
}
func main() {
    http.HandleFunc("/latency", <strong class="bold">probeTargets</strong>)
    log.Fatal(http.ListenAndServe(":9900", nil))
}</pre>
<p>The preceding block represents the last two functions of our service. In the <code>main()</code> function, we just need to call <code>http.HandleFunc</code>, passing the API reference used for the <code>GET</code> method and the name of the function that will be invoked. Then, <code>http.ListenAndServe</code> is called using port <code>9900</code> to listen for API requests. Note that <code>log.Fatal</code> is used with <code>ListenAndServe</code> because it should never end unless it has a problem. The following is an API <code>GET</code> client request example:</p>
<pre class="source-code">
GET /latency?targets=google.com,cisco.com HTTP/1.0</pre>
<p>The preceding API request will call <code>probeTargets()</code>, which will run the loop invoking the goroutines (called <code>probe()</code>) two times, which will send ICMP requests to <code>google.com</code> and <code>cisco.com</code>.</p>
<p>Let’s now have a<a id="_idIndexMarker1012"/> look at the last block of code containing the <code>probe()</code> function:</p>
<pre class="source-code">
func probe(host string, w http.ResponseWriter, wg *sync.WaitGroup) {
    defer wg.Done()
    p, err := ping.NewPinger(host)
    if err != nil {
        fmt.Fprintf(w, "error ping creation: %v\n", err)
        return
    }
    p.Count = 1
    p.Timeout = time.Second * 2
    p.SetPrivileged(true)
    if err = p.Run(); err != nil {
        fmt.Fprintf(w, "error ping sent: %v\n", err)
        return
    }
    stats := p.Statistics()
    if stats.PacketLoss == 0 {
        fmt.Fprintf(w, "%s latency is %s\n", host, stats.AvgRtt)
    } else {
        fmt.Fprintf(w, "%s no response timeout\n", host)
    }
}</pre>
<p>Note that the <code>probe()</code> function does not return a value, a <code>log</code> message, or a print message. All messages, including errors, are returned to the HTTP client requesting the ICMP probes. To allow the return to the client, we have to use the <code>fmt.Fprintf()</code> function, passing the reference <code>w</code>, which points to an <code>http.ResponseWriter</code> type.</p>
<p>Before we<a id="_idIndexMarker1013"/> continue with our example, let’s make a modification to our <code>main()</code> function to allow reading the port number from the operating system environment variable. So, the service can be called with different port numbers when being invoked, just needing to change the operating system environment variable called <code>PORT</code>, as shown here:</p>
<pre class="source-code">
func main() {
    listen := ":9900"
    if port, ok := os.LookupEnv("PORT"); ok {
        listen = ":" + port
    http.HandleFunc("/latency", probeTargets)
    log.Fatal(http.ListenAndServe(listen, nil))
}</pre>
<p>Let’s now build our Docker container using a Dockerfile.</p>
<h3>Building our Docker container</h3>
<p>To build the Docker container, we are going<a id="_idIndexMarker1014"/> to use Dockerfile definitions. Then, we just need to run <code>docker build</code> to create our container. Before you install the Docker engine in your environment, check the documentation on how to install it at <a href="https://docs.docker.com/engine/install/">https://docs.docker.com/engine/install/</a>.</p>
<p>The following is the Dockerfile used in our example of an ICMP probe service:</p>
<pre class="source-code">
FROM golang:1.19-alpine
WORKDIR /usr/src/app
COPY go.mod go.sum ./
RUN go mod download &amp;&amp; go mod verify
COPY icmp-probe-service.go ./
RUN go build -v -o /usr/local/bin/probe-service
CMD ["/usr/local/bin/probe-service"]</pre>
<p>To build the Docker container, you just need to run <code>docker build . –t probe-service</code>. After running the build, you should be able to see the image by using the <code>docker image</code> command, as follows:</p>
<pre class="console">
% docker images
REPOSITORY      TAG   IMAGE ID CREATED              SIZE
probe-service   latest  e9c2   About a minute ago   438MB</pre>
<p>The Docker container name is <code>probe-service</code> and you can run the service by using the following command:</p>
<pre class="console">
docker run -p 9900:9900 probe-service</pre>
<p>To listen to a different port, you need to set the <code>PORT</code> environment variable. An example for port <code>7700</code> is as follows:</p>
<pre class="console">
docker run -e PORT=7700 -p 7700:7700 probe-service</pre>
<p>Note that you could map different host ports to port <code>9900</code> if you want to run multiple services in the same host without changing the port that the container listens to. You just need to specify a different port for the host when mapping, as in the following example running three services on the same machine:</p>
<pre class="console">
% docker run -d -p 9001:9900 probe-service
% docker run -d -p 9002:9900 probe-service
% docker run -d -p 9003:9900 probe-service</pre>
<p>Running the<a id="_idIndexMarker1015"/> preceding three commands will start three services on the host ports: <code>9001</code>, <code>9002</code>, and <code>9003</code>. The service inside the container still uses port <code>9900</code>. To check the services running in a host, use the <code>docker ps</code> command, as follows:</p>
<pre class="console">
% docker ps
CONTAINER ID   IMAGE           COMMAND                  CREATED         STATUS         PORTS                    NAMES
6266c895f11a   probe-service   "/usr/local/bin/prob…"   2 minutes ago   Up 2 minutes   0.0.0.0:<strong class="bold">9003</strong>-&gt;9900/tcp   gallant_heisenberg
270d73163d19   probe-service   "/usr/local/bin/prob…"   2 minutes ago   Up 2 minutes   0.0.0.0:<strong class="bold">9002</strong>-&gt;9900/tcp   intelligent_clarke
4acc6162e821   probe-service   "/usr/local/bin/prob…"   2 minutes ago   Up 2 minutes   0.0.0.0:<strong class="bold">9001</strong>-&gt;9900/tcp   hardcore_bhabha</pre>
<p>The preceding output shows that<a id="_idIndexMarker1016"/> there are three services running on the host, listening to ports <code>9001</code>, <code>9002</code>, and <code>9003</code>. You can access the APIs for each of them and probe up to 3,000 targets, 1,000 per service.</p>
<p>Let’s now see how we can automate launching multiple services using Docker Compose.</p>
<h3>Scaling up using Docker Compose</h3>
<p>Using Docker<a id="_idIndexMarker1017"/> Compose will help you to add services that will run at the same time without needing to invoke the <code>docker run</code> command. In our example, we are going to use Docker Compose to launch five ICMP probe services. The following is the Docker Compose file example in YAML format (described in <a href="B18165_04.xhtml#_idTextAnchor100"><em class="italic">Chapter 4</em></a>,):</p>
<pre class="source-code">
version: "1.0"
services:
  probe1:
    image: "probe-service:latest"
    ports: ["9001:9900"]
  probe2:
    image: "probe-service:latest"
    ports: ["9002:9900"]
  probe3:
    image: "probe-service:latest"
    ports: ["9003:9900"]
  probe4:
    image: "probe-service:latest"
    ports: ["9004:9900"]
  probe5:
    image: "probe-service:latest"
    ports: ["9005:9900"]</pre>
<p>To run the<a id="_idIndexMarker1018"/> services, just type <code>docker compose up –d</code>, and to stop them, just run <code>docker compose down</code>. The following is an example of the output of the command:</p>
<pre class="console">
% docker compose up –d
[+] Running 6/6
⠿ Network probe-service_default     Created   1.4s
⠿ Container probe-service-probe5-1  Started   1.1s
⠿ Container probe-service-probe4-1  Started   1.3s
⠿ Container probe-service-probe3-1  Started   1.5s
⠿ Container probe-service-probe1-1  Started   1.1s
⠿ Container probe-service-probe2-1  Started</pre>
<p>Now, let’s see how we can scale up using multiple machines with a Docker container.</p>
<h3>Scaling up with clusters</h3>
<p>To scale <a id="_idIndexMarker1019"/>even more, you could set up a cluster of Docker host containers. This will allow you to launch thousands of services, allowing our ICMP probe service to scale to millions of targets. You could build the cluster yourself by managing a group of machines and running the services, or you could use a system to do all that for you.</p>
<p>Let’s now investigate a few systems that are used to manage and launch services for a cluster of machines running container services.</p>
<h4>Using Docker Swarm</h4>
<p>With <strong class="bold">Docker Swarm</strong>, you are <a id="_idIndexMarker1020"/>able to launch containers on several machines. It is easy to use because it only requires installing Docker. Once you have installed it, it is easy to create a Docker Swarm cluster. You just have to run the following command:</p>
<pre class="console">
Host-1$ <strong class="bold">docker swarm init</strong>
Swarm initialized: current node (9f2777swvj1gmqegbxabahxm3) is now a manager.
To add a worker to this swarm, run the following command:
    docker swarm join --token <strong class="bold">SWMTKN-1-1gdb6i88ubq5drnigbwq2rh51fmyordkkpljjtwefwo2nk3ddx-6nwz531o6lqtkun4gagvrl7ws</strong> 192.168.86.158:2377
To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.</pre>
<p>Once you have <a id="_idIndexMarker1021"/>started the first Docker Swarm host, it will then take the lead place, and to add another host, you just need to use the <code>docker swarm join</code> command. To avoid any host joining the Docker Swarm cluster, a token is used. The preceding example starts with <code>SWMTKN-1</code>. Note that a host in a Docker Swarm cluster is also called <a id="_idIndexMarker1022"/>a <strong class="bold">node</strong>. So, let’s add more nodes to our cluster:</p>
<pre class="console">
host-2$ <strong class="bold">docker swarm join</strong> --token SWMTKN-1-1gdb6i88ubq5drnigbwq2rh51fmyordkkpljjtwefwo2nk3ddx-6nwz531o6lqtkun4gagvrl7ws 192.168.86.158:2377
host-3$ <strong class="bold">docker swarm join</strong> --token SWMTKN-1-1gdb6i88ubq5drnigbwq2rh51fmyordkkpljjtwefwo2nk3ddx-6nwz531o6lqtkun4gagvrl7ws 192.168.86.158:2377
host-4$ <strong class="bold">docker swarm join</strong> --token SWMTKN-1-1gdb6i88ubq5drnigbwq2rh51fmyordkkpljjtwefwo2nk3ddx-6nwz531o6lqtkun4gagvrl7ws 192.168.86.158:2377</pre>
<p>Now, we have four nodes in the cluster, with <code>host-1</code> as the leader. You can check the status of the cluster nodes by typing the following command:</p>
<pre class="console">
$ docker node ls
ID          HOSTNAME  STATUS  AVAILABILITY MANAGER
9f2777swvj* host-1    Ready   Active       Leader
a34f25affg* host-2    Ready   Active
7fdd77wvgf* host-4    Ready   Active
8ad531vabj* host-3    Ready   Active</pre>
<p>Once you have your cluster, you can launch a service by running the following command:</p>
<pre class="console">
$ <strong class="bold">docker service create</strong> --replicas 1 --name probe probe-service
7sv66ytzq0te92dkndz5pg5q2
overall progress: 1 out of 1 tasks
1/1: running
[==================================================&gt;]
verify: Service converged</pre>
<p>In the preceding<a id="_idIndexMarker1023"/> example, we just launched a Swarm service called <code>probe</code> using the <code>probe-service</code> image, the same image used in previous examples. Note that we’ve only launched one replica to later show how easy it is to scale up. Let’s check now how the service is installed by running the following command:</p>
<pre class="console">
$ <strong class="bold">docker service ls</strong>
ID          NAME  MODE       REPLICAS IMAGE
7sv66ytzq0  probe replicated 1/1      probe-service:latest</pre>
<p>Let’s now scale up for 10 probes by running the following command:</p>
<pre class="console">
$ <strong class="bold">docker service scale probe=10</strong>
probe scaled to 10
overall progress: 10 out of 10 tasks
1/10: running
[==================================================&gt;]
2/10: running
[==================================================&gt;]
3/10: running
[==================================================&gt;]
4/10: running
[==================================================&gt;]
5/10: running
[==================================================&gt;]
6/10: running
[==================================================&gt;]
7/10: running
[==================================================&gt;]
8/10: running
[==================================================&gt;]
9/10: running
[==================================================&gt;]
10/10: running
[==================================================&gt;]
verify: Service converged</pre>
<p>Now, if you check the service, it will show 10 replicas, as in the following command:</p>
<pre class="console">
$ <strong class="bold">docker service ls</strong>
ID          NAME  MODE       REPLICAS IMAGE
7sv66ytzq0  probe replicated <strong class="bold">10/10</strong>    probe-service:latest</pre>
<p>You can also <a id="_idIndexMarker1024"/>check where each replica is running by running the following command:</p>
<pre class="console">
$ <strong class="bold">docker service ps probe</strong>
ID      NAME    IMAGE                NODE  DESIRED STATE
y38830 probe.1 probe-service:latest host-1 Running Running v4493s probe.2 probe-service:latest host-2 Running Running
zhzbnj probe.3 probe-service:latest host-3 Running Running
i84s4g probe.4 probe-service:latest host-3 Running Running 3emx3f probe.5 probe-service:latest host-1 Running Running
rd1vp1 probe.6 probe-service:latest host-2 Running Running
p1oq0w probe.7 probe-service:latest host-3 Running Running ro0foo probe.8 probe-service:latest host-4 Running Running
l6prr4 probe.9 probe-service:latest host-4 Running Running
dwdr43 probe.10 probe-service:latest host-1 Running Running</pre>
<p>As you can see in the output of the preceding command, there are 10 probes running as replicas on nodes <code>host-1</code>, <code>host-2</code>, <code>host-3</code>, and <code>host-4</code>. You can also specify where you want the replica to run, among other parameters. In this example, we are able to scale up our ICMP probe service to 10,000 targets by using 4 hosts.</p>
<p>One important point we missed on these commands was allocating the ports to listen for our replicas. As replicas can run in the same host, they can’t use the same port. We then need to make sure each replica is assigned with a different port number to listen to. A client accessing our <code>probe-service</code> cluster needs to know the IP addresses of the hosts and the port numbers that are listening before connecting for requests.</p>
<p>A better and <a id="_idIndexMarker1025"/>more controlled way to deploy Docker Swarm is to use a YAML configuration file like we did when using Docker Compose. More details on the configuration file for Docker Swarm can be found at <a href="https://github.com/docker/labs/blob/master/beginner/chapters/votingapp.md">https://github.com/docker/labs/blob/master/beginner/chapters/votingapp.md</a>.</p>
<p>More documentation<a id="_idIndexMarker1026"/> on Docker Swarm can be found at <a href="https://docs.docker.com/engine/swarm/swarm-mode/">https://docs.docker.com/engine/swarm/swarm-mode/</a>.</p>
<p>Now let’s investigate how to use multiple hosts using Kubernetes.</p>
<h4>Using Kubernetes</h4>
<p>Kubernetes is <a id="_idIndexMarker1027"/>perhaps one of the most popular systems to manage the microservices architecture in a cluster. Its popularity is also due to it being backed by<a id="_idIndexMarker1028"/> the <strong class="bold">Cloud Native Computing Foundation</strong> (<a href="https://www.cncf.io/">https://www.cncf.io/</a>), which is part of<a id="_idIndexMarker1029"/> the <strong class="bold">Linux Foundation</strong> (<a href="https://www.linuxfoundation.org/">https://www.linuxfoundation.org/</a>). Large companies use Kubernetes, such as Amazon, Google, Apple, Cisco, and Huawei, among others.</p>
<p>Kubernetes provides <a id="_idIndexMarker1030"/>many more capabilities than Docker Swarm, such as service orchestration, load-balancing, service monitoring, self-healing, and auto-scaling by traffic, among other features. Despite the large community and vast capabilities, you might not want to use Kubernetes if your requirement is simple and needs to scale in large quantities. Kubernetes provides a lot of capabilities that might be an overhead to your development. For our <code>probe-service</code>, I would not recommend using Kubernetes, because it is too complex for our purposes of ICMP probing targets.</p>
<p>You can also use a <a id="_idIndexMarker1031"/>Docker Compose file to configure Kubernetes, which is done by using a service translator such as Kompose (<a href="https://kompose.io/">https://kompose.io/</a>). More details can <a id="_idIndexMarker1032"/>be found at <a href="https://kubernetes.io/docs/tasks/configure-pod-container/translate-compose-kubernetes/">https://kubernetes.io/docs/tasks/configure-pod-container/translate-compose-kubernetes/</a>.</p>
<p>If you want to start using Kubernetes, you can find plenty of examples and documentation on the internet. The best place to start is at <a href="https://kubernetes.io/docs/home/">https://kubernetes.io/docs/home/</a>.</p>
<p>Let’s now check how we can use another cluster based on Nomad.</p>
<h4>Using Nomad</h4>
<p>Nomad <a id="_idIndexMarker1033"/>is also used to implement Docker clustering (<a href="https://www.nomadproject.io/">https://www.nomadproject.io/</a>). Nomad<a id="_idIndexMarker1034"/> also has several capabilities that are comparable to Kubernetes, such as monitoring, self-healing, and auto-scaling. However, the features list is not as long and complete as Kubernetes.</p>
<p>So, why would we use Nomad instead of Kubernetes? There are three main reasons that you might want to use Nomad, as listed here:</p>
<ul>
<li>Simpler to deploy and easy to configure in comparison to Kubernetes.</li>
<li>Kubernetes can scale up to 5,000 nodes with 300,000 containers. Nomad, on the other hand, is able to scale to 10,000 nodes and more than 2 million containers (<a href="https://www.javelynn.com/cloud/the-two-million-container-challenge/">https://www.javelynn.com/cloud/the-two-million-container-challenge/</a>).</li>
<li>Can support other services besides Linux containers, such <a id="_idIndexMarker1035"/>as <strong class="bold">QEMU</strong> virtual machines, Java, Unix processes, and Windows containers.</li>
</ul>
<p>More documentation on Nomad can be found at <a href="https://developer.hashicorp.com/nomad/docs">https://developer.hashicorp.com/nomad/docs</a>.</p>
<p>Let’s now have a brief look at how to use microservice architectures provided by cloud service providers.</p>
<h4>Using cloud service providers</h4>
<p>There <a id="_idIndexMarker1036"/>are also proprietary<a id="_idIndexMarker1037"/> solutions that are provided by cloud<a id="_idIndexMarker1038"/> service providers, such as <strong class="bold">Azure Container Instances</strong>, <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>), and Amazon <strong class="bold">Elastic Container Service</strong> (<strong class="bold">ECS</strong>). The advantage of using a cloud service provider is you <a id="_idIndexMarker1039"/>don’t need physical machines in your infrastructure to create the cluster. There are also products where you don’t even need to care about the cluster and the nodes in it, such as a product from Amazon called AWS Fargate. With AWS Fargate, you just need the Docker container published in a Docker registry and a service specification without the need to specify the nodes or the cluster.</p>
<p>I hope this section has given you a good idea of how to scale your code by using Linux containers and host clustering. Microservice architecture is a hot topic that has gotten lots of attention from developers and cloud service providers in recent years. Several acronyms might be used to describe this technology, but we have covered the basics here. You now have enough knowledge to dive even deeper into this subject.</p>
<h1 id="_idParaDest-206"><a id="_idTextAnchor207"/>Summary</h1>
<p>This chapter has shown you a good summary of how you can improve and use systems to scale your code. We also demonstrated how we can use standard and third-party libraries to add capabilities to our code to scale.</p>
<p>Now, you are probably much more familiar with the technologies that you could use to interact with large networks. You are now in a better position to choose a language, a library, and a system that will support your network automation to scale to handle thousands or even millions of network devices.</p>
<p>In the next chapter, we are going to cover how to test your code and your system, which will allow you to build solutions for network automation that are less prone to failures.</p>
</div>


<div><h1 id="_idParaDest-207"><a id="_idTextAnchor208"/>Part 3: Testing, Hands-On, and Going Forward</h1>
<p>The third part of the book will discuss what has to be considered when building a framework for testing your code and how to do so, We will do some real hands-on testing and, finally, describe what to do to move forward in the network automation realm. We will provide the details on creating a testing framework and do hands-on work using an emulated network, which will help to put into practice all the information learned in previous parts.</p>
<p>This part has the following chapters:</p>
<ul>
<li><a href="B18165_09.xhtml#_idTextAnchor209"><em class="italic">Chapter 9</em></a>, <em class="italic">Network Code Testing Frameworks</em></li>
<li><a href="B18165_10.xhtml#_idTextAnchor227"><em class="italic">Chapter 10</em></a>, <em class="italic">Hands-On and Going Forward</em></li>
</ul>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
</body></html>