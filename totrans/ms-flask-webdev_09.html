<html><head></head><body>
        

                            
                    <h1 class="header-title">Creating Asynchronous Tasks with Celery</h1>
                
            
            
                
<p>While creating web apps, it is vital to keep the time taken to process a request below or around 50 ms. On web applications or web services that have a medium to high rate of requests per second, response time becomes even more paramount. Think of requests such as a flow of liquid that needs to be handled at least as quickly as its flow rate, or else it will overflow. Any extra processing on the server that can be avoided, should be avoided. However, it is quite common to have requirements to operations in a web app that take longer than a couple of seconds, especially when complex database operations or image processing are involved. </p>
<p>In building an application that is able to scale horizontally, it should be possible to decouple all the heavy processing procedures from the web server's layer, and couple them to a worker's layer that can independently scale itself.</p>
<p>To protect our user experience and site reliability, a task queue named Celery will be used to move these operations out of the Flask process.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Using Docker to run RabbitMQ and Redis</li>
<li>Celery and Flask integration</li>
<li>Learning to identify processes that should run outside the web server</li>
<li>Creating and calling several types of tasks from simple asynchronous to complex workflows</li>
<li>Using Celery as a scheduler with beats</li>
</ul>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">What is Celery?</h1>
                
            
            
                
<p><strong>Celery</strong> is an asynchronous task queue written in Python. Celery runs multiple tasks, which are user-defined functions, concurrently, through the Python multiprocessing library. Celery receives messages that tell it to start a task from a <strong>broker</strong>, which is usually called a message queue, as shown in the following diagram:</p>
<div><img src="img/bcdd964a-f516-4b59-b8cd-3e4392304658.png" style="width:40.58em;height:9.25em;"/></div>
<p>A <strong>message queue</strong> is a system specifically designed to send data between producer processes and consumer processes. <strong>Producer processes</strong> are any programs that create messages to be sent to the queue, and <strong>consumer processes</strong> are any programs that take the messages out of the queue. Messages sent from a producer are stored in a <strong>First In, First Out</strong> (<strong>FIFO</strong>) queue, where the oldest items are retrieved first. Messages are stored until a consumer receives the message, after which the message is deleted. Message queues provide real-time messaging without relying on polling, which means continuously checking the status of a process. As messages are sent from producers, consumers are listening on their connection to the message queue for new messages; the consumer is not constantly contacting the queue. This difference is like the difference between <strong>AJAX</strong> and <strong>WebSockets</strong>, in that AJAX requires constant contact with the server, while WebSockets are just a continuous bidirectional communication stream.</p>
<p>It is possible to replace the message queue with a traditional database. Celery even comes with built-in support for SQLAlchemy to allow this. However, using a database as a broker for Celery is highly discouraged. Using a database in place of a message queue requires the consumer to constantly poll the database for updates. Also, because Celery uses multiprocessing for concurrency, the number of connections making lots of reads goes up quickly. Under medium loads, using a database requires the producer to make lots of writes to the database at the same time as the consumer is reading.</p>
<p>It is also possible to use a message queue as a broker and a database to store the results of the tasks. In the preceding diagram, the message queue was used for sending task requests and task results. However, using a database to store the end result of the task allows the final product to be stored indefinitely, whereas the message queue will throw out the data as soon as the producer receives the data, as shown in the following diagram:</p>
<div><img class="aligncenter size-full wp-image-633 image-border" src="img/03a68834-f6ae-4b8f-aa49-36634765da94.png" style="width:42.50em;height:19.42em;"/></div>
<p>This database is often a key/value NoSQL store to help handle the load. This is useful if you plan on doing analytics on previously run tasks, but otherwise it's safer to just stick with the message queue.</p>
<p>There is even an option to drop the results of tasks entirely, and not have the results returned at all. This has the downside that the producer has no way of knowing if a task was successful or not, but often, this is permissible in smaller projects.</p>
<p>For our stack, we will use RabbitMQ as the message broker. RabbitMQ runs on all major operating systems and is very simple to be set up and run. Celery also supports RabbitMQ without any extra libraries, and is the recommended message queue in the Celery documentation.</p>
<p>At the time of writing, there is no way to use RabbitMQ with Celery in Python 3. You can use Redis, however, instead of RabbitMQ. The only difference will be the connection strings. For more information, see <a href="http://docs.celeryproject.org/en/latest/getting-started/brokers/redis.html">http://docs.celeryproject.org/en/latest/getting-started/brokers/redis.html</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Setting up Celery and RabbitMQ</h1>
                
            
            
                
<p>To install Celery on our <kbd>virtualenv</kbd>, we need to add it to our <kbd>requirements.txt</kbd> file:</p>
<pre>... <br/>Celery<strong><br/></strong>...</pre>
<p class="mce-root"/>
<p>As always, use the provided <kbd>init.sh</kbd> script, or use the procedure explained here to create and install all dependencies on a Python virtual environment.</p>
<p>We will also need a Flask extension to help handle the initialization of Celery:</p>
<pre><strong>$ pip install Flask-Celery-Helper</strong></pre>
<p>The Flask documentation states that Flask extensions for Celery are unnecessary. However, getting the Celery server to work with Flask's application context, when your app is organized with an application factory, is significant. So, we will use <kbd>Flask-Celery-Helper</kbd> to do the heavy lifting.</p>
<p>Next, RabbitMQ needs to be up and running. To do this easily, we will use a Docker container. Make sure you have Docker installed and properly set up; if not, then check out <a href="2d7573ed-1b2f-48df-9fac-9423d3f1cd51.xhtml">Chapter 1</a>, <em>Getting Started</em>, for instructions. First, we will need a very simple Dockerfile:</p>
<pre>FROM rabbitmq:3-management<br/><br/>ENV RABBITMQ_ERLANG_COOKIE "SWQOKODSQALRPCLNMEQG"<br/>ENV RABBITMQ_DEFAULT_USER "rabbitmq"<br/>ENV RABBITMQ_DEFAULT_PASS "rabbitmq"<br/>ENV RABBITMQ_DEFAULT_VHOST "/"</pre>
<p>This is all it takes to build and run a RabbitMQ Docker image with the management interface. We are using a Docker Hub image that is available for download at <a href="https://hub.docker.com/_/rabbitmq/">https://hub.docker.com/_/rabbitmq/</a>. Visit the Hub page for further configuration details.</p>
<p>Next, let's build our image issue the following command:</p>
<pre><strong>$ docker build -t blog-rmq .</strong></pre>
<p>The <kbd>-t</kbd> flag is used to tag our image with a friendly name; in this case, <kbd>blog-rmq</kbd>. Then run the newly created image in the background using the following command:</p>
<pre><strong>$ docker run -d -p 15672:15672 -p 5672:5672 blog-rmq</strong></pre>
<p>The <kbd>-d</kbd> flag is to run the container in the background (daemon). The <kbd>-p</kbd> flag is for port mapping between the container and our host/desktop.</p>
<p>Let's check if it's properly running:</p>
<pre><strong>$ </strong><strong>docker ps</strong><br/>CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES<br/>6eb2ab1da516 blog-rmq "docker-entrypoint.s…" 13 minutes ago Up 14 minutes 4369/tcp, 5671/tcp, 0.0.0.0:5672-&gt;5672/tcp, 15671/tcp, 25672/tcp, 0.0.0.0:15672-&gt;15672/tcp xenodochial_kepler</pre>
<p class="mce-root"/>
<p>Let's check out the RabbitMQ management interface. In your browser, navigate to <kbd>http://localhost:15672</kbd> and log in using the configured credentials set up on the Dockerfile. In this case, our username is <kbd>rabbitmq</kbd>, and our password is also <kbd>rabbitmq</kbd>.</p>
<p>If you need more information, RabbitMQ maintains a detailed list of installation and configuration instructions for each operating system at <a href="https://www.rabbitmq.com/download.html">https://www.rabbitmq.com/download.html</a>.</p>
<p>After RabbitMQ is installed, go to a Terminal window and run the following command:</p>
<pre><strong>$ rabbitmq-server</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating tasks in Celery</h1>
                
            
            
                
<p>As stated before, Celery tasks are just user-defined functions that perform some operations. But before any tasks can be written, our Celery object needs to be created. This is the object that the Celery server will import to handle running and scheduling all of the tasks.</p>
<p>At a bare minimum, Celery needs one configuration variable to run, and that is the connection to the message broker. The connection is defined the same as the SQLAlchemy connection; that is, as a URL. The backend, which stores our tasks' results, is also defined as a URL, as shown in the following code:</p>
<pre>class DevConfig(Config): 
    DEBUG = True 
    SQLALCHEMY_DATABASE_URI = 'sqlite:///../database.db' 
    CELERY_BROKER_URL = "amqp://rabbitmq:rabbitmq@localhost//" 
    CELERY_RESULT_BACKEND = "amqp://rabbitmq:rabitmq@localhost//" </pre>
<p>In the <kbd>__init__.py</kbd> file, the <kbd>Celery</kbd> class from <kbd>Flask-Celery-Helper</kbd> will be initialized:</p>
<pre>from flask_celery import Celery<br/>... 
celery = Celery()<br/>...<br/>def create_app(object_name):<br/>...<br/>    celery.init_app(app)<br/>...</pre>
<p class="mce-root"/>
<p>So, in order for our Celery process to work with the database and any other Flask extensions, it needs to work within our application context. In order to do so, Celery will need to create a new instance of our application for each process. Like most Celery apps, we need a Celery factory to create an application instance and register our Celery instance on it. In a new file, named <kbd>celery_runner.py</kbd>, in the top-level directory—the same location where <kbd>manage.py</kbd> resides—we have the following:</p>
<pre>import os<br/>from webapp import create_app<br/>from celery import Celery<br/><br/><br/>def make_celery(app):<br/>    celery = Celery(<br/>        app.import_name,<br/>        broker=app.config['CELERY_BROKER_URL'],<br/>        backend=app.config['CELERY_RESULT_BACKEND']<br/>    )<br/>    celery.conf.update(app.config)<br/>    TaskBase = celery.Task<br/><br/>    class ContextTask(TaskBase):<br/>        abstract = True<br/><br/>        def __call__(self, *args, **kwargs):<br/>            with app.app_context():<br/>                return TaskBase.__call__(self, *args, **kwargs)<br/><br/>    celery.Task = ContextTask<br/>    return celery<br/><br/>env = os.environ.get('WEBAPP_ENV', 'dev')<br/>flask_app = create_app('config.%sConfig' % env.capitalize())<br/><br/>celery = make_celery(flask_app)</pre>
<p>The <kbd>make_celery</kbd> function wraps every call to each Celery task in a Python <kbd>with</kbd> block. This makes sure that every call to any Flask extension will work as it is working with our app. Also, make sure not to name the Flask app instance <kbd>app</kbd>, as Celery tries to import any object named <kbd>app</kbd> or <kbd>celery</kbd> as the Celery application instance. So naming your Flask object <kbd>app</kbd> will cause Celery to try to use it as a Celery object.</p>
<p class="mce-root"/>
<p>Now we can write our first task. It will be a simple task to start with; one that just returns any string passed to it. We have a new file in the blog module directory, named <kbd>tasks.py</kbd>. In this file, find the following:</p>
<pre>from .. import celery<br/><br/>@celery.task() 
def log(msg): 
    return msg</pre>
<p>Now, the final piece of the puzzle is to run the Celery process, which is called a <strong>worker</strong>, in a new Terminal window. Again, this is the process that will be listening to our message broker for commands to start new tasks:</p>
<pre><strong>$ celery worker -A celery_runner --loglevel=info</strong></pre>
<p>The <kbd>loglevel</kbd> flag is there, so you will see the confirmation that a task was received, and its output was available, in the Terminal window.</p>
<p>Now, we can send commands to our Celery worker. Open a Flask shell session, as follows:</p>
<pre><strong>$ export FLASK_APP=main.py<br/></strong><strong>$ flask shell</strong>    <br/><strong>&gt;&gt;&gt; from webapp.blog.tasks import log</strong>
<strong>&gt;&gt;&gt; log("Message")</strong>
Message
<strong>&gt;&gt;&gt; result = log.delay("Message")</strong></pre>
<p>The function can be called as if it were any other function, and doing so will execute the function in the current process. However, calling the <kbd>delay</kbd> method on the task will send a message to the worker process to execute the function with the given arguments.</p>
<p>In the Terminal window that is running the Celery worker, you should see something like the following:</p>
<pre><strong>Task webapp.blog.tasks.log succeeded in 0.0005873600021s: 'Message'</strong></pre>
<p>As with any asynchronous task, the <kbd>ready</kbd> method can be used to tell if the task has successfully been completed. If <kbd>True</kbd>, the <kbd>get</kbd> method can be used to retrieve the result of the tasks as follows:</p>
<pre><strong>&gt;&gt;&gt; result.ready() <br/></strong>True<strong> <br/>&gt;&gt;&gt; result.get() <br/></strong>"Message"</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The <kbd>get</kbd> method causes the current process to wait until the <kbd>ready</kbd> function returns <kbd>True</kbd> to retrieve the result. So, calling <kbd>get</kbd> immediately after calling the task essentially makes the task synchronous. Because of this, it's rather rare for tasks to actually return a value to the producer. The vast majority of tasks perform some operation and then exit.</p>
<p>When a task is run on the Celery worker, the state of the task can be accessed via the <kbd>state</kbd> attribute. This allows for a more fine-grained understanding of what the task is currently doing in the worker process. The available states are as follows:</p>
<ul>
<li><kbd>FAILURE</kbd>: The task failed, and all of the retries failed as well.</li>
<li><kbd>PENDING</kbd>: The task has not yet been received by the worker.</li>
<li><kbd>RECEIVED</kbd>: The task has been received by the worker, but is not yet processing.</li>
<li><kbd>RETRY</kbd>: The task failed and is waiting to be retried.</li>
<li><kbd>REVOKED</kbd>: The task was stopped.</li>
<li><kbd>STARTED</kbd>: The worker has started processing the task.</li>
<li><kbd>SUCCESS</kbd>: The task was completed successfully.</li>
</ul>
<p>In Celery, if a task fails, then the task can recall itself with the <kbd>retry</kbd> method, as follows:</p>
<pre>@celery.task(bind=True) 
def task(self, param): 
  try: 
    some_code 
  except Exception, e: 
    self.retry(exc=e) </pre>
<p>The <kbd>bind</kbd> parameter in the decorator function tells Celery to pass a reference to the task object as the first parameter in the function. Using the <kbd>self</kbd> parameter, the <kbd>retry</kbd> method can be called, which will rerun the task with the same parameters. There are several other parameters that can be passed to the function decorator to change the behavior of the task:</p>
<ul>
<li><kbd>max_retries</kbd>: This is the maximum number of times the task can be retried before it is declared as failed.</li>
<li><kbd>default_retry_delay</kbd>: This is the time in seconds to wait before running the task again. It's a good idea to keep this at around a minute or so if you expect that the conditions that led to the task failing are transitory; for example, network errors.</li>
<li><kbd>rate_limit</kbd>: This specifies the total number of unique calls to this task that are allowed to run in a given interval. If the value is an integer, then it represents the total number of calls that this task that is allowed to run per second. The value can also be a string in the form of <em>x/m</em>,<em> </em>for <em>x</em> number of tasks per minute, or <em>x/h</em>, for <em>x</em> number of tasks per hour. For example, passing in <em>5/m</em> will only allow this task to be called five times a minute.</li>
<li><kbd>time_limit</kbd>: If this is specified, then the task will be killed if it runs longer than the specified number of seconds.</li>
<li><kbd>ignore_result</kbd>: If the task's return value isn't used, then don't send it back.</li>
</ul>
<p>It's a good idea to specify all of these for each task to avoid any chance that a task will not be run.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Running Celery tasks</h1>
                
            
            
                
<p>The <kbd>delay</kbd> method is a shorthand version of the <kbd>apply_async</kbd> method, which is called in this format:</p>
<pre>task.apply_async( 
  args=[1, 2], 
  kwargs={'kwarg1': '1', 'kwarg2': '2'} 
) </pre>
<p>However, the <kbd>args</kbd> keyword can be implicit, as shown here:</p>
<pre>apply_async([1, 2], kwargs={'kwarg1': '1', 'kwarg2': '2'}) </pre>
<p>Calling <kbd>apply_async</kbd> allows you to define some extra functionality in the task call that you cannot specify in the <kbd>delay</kbd> method. First, the <kbd>countdown</kbd> option specifies the amount of time in seconds that the worker, upon receiving the task, should wait before running it:</p>
<pre><strong>&gt;&gt;&gt; from webapp.blog.tasks import log
&gt;&gt;&gt; log.apply_async(["Message"], countdown=600)</strong></pre>
<p>The <kbd>countdown</kbd> is not a guarantee that the task will be run after <kbd>600</kbd> seconds. The <kbd>countdown</kbd> option only says that the task is up for processing after <em>x</em> number of seconds. If all of the worker processes are busy with the other tasks, then it will not be run immediately.</p>
<p>Another keyword argument that <kbd>apply_async</kbd> gives is the <kbd>eta</kbd> argument. <kbd>eta</kbd> is passed through a Python <kbd>datetime</kbd> object that specifies exactly when the task should be run. Again, <kbd>eta</kbd> is not reliable:</p>
<pre><strong>&gt;&gt;&gt; import datetime</strong>
<strong>&gt;&gt;&gt; from webapp.blog.tasks import log</strong>
<strong># Run the task one hour from now</strong>
<strong>&gt;&gt;&gt; eta = datetime.datetime.now() + datetime.timedelta(hours=1)</strong>
<strong>&gt;&gt;&gt; log.apply_async(["Message"], eta=eta)</strong>
  </pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Celery workflows</h1>
                
            
            
                
<p>Celery provides many ways to group multiple, dependent tasks together, or to execute many tasks in parallel. These methods take a large amount of influence from language features found in functional programming languages. However, to understand how this works, we first need to understand signatures. Consider the following task:</p>
<pre>@celery.task() 
def multiply(x, y): 
    return x * y </pre>
<p>Let's see a <strong>signature</strong> in action to understand it. Open up a Flask shell and enter the following:</p>
<pre><strong># Export FLASK_APP if you haven't already</strong><br/><strong>$ export FLASK_APP=main.py</strong><br/><strong>$ flask shell</strong><br/><strong>&gt;&gt;&gt; from celery import signature
&gt;&gt;&gt; from webapp.blog.tasks import multiply
# Takes the same keyword args as apply_async
&gt;&gt;&gt; signature('webapp.tasks.multiply', args=(4, 4), countdown=10)
</strong>webapp.tasks.multiply(4, 4)<strong>
# same as above
&gt;&gt;&gt; from webapp.blog.tasks import multiply
&gt;&gt;&gt; multiply.subtask((4, 4), countdown=10)
</strong>webapp.tasks.multiply(4, 4)<strong>
# shorthand for above, like delay in that it doesn't take
# apply_async's keyword args
&gt;&gt;&gt; multiply.s(4, 4)
</strong>webapp.blog.tasks.multiply(4, 4)<strong>
&gt;&gt;&gt; multiply.s(4, 4)()
</strong>16<strong>
&gt;&gt;&gt; multiply.s(4, 4).delay()
  </strong></pre>
<p class="mce-root"/>
<p>Calling the signature (sometimes referred to as a <strong>subtask</strong>) of a task creates a function that can be passed to the other functions to be executed. Executing the signature, like the third to last line in the preceding example, executes the function in the current process, and not in the worker.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Partials</h1>
                
            
            
                
<p>The first application of task signatures is functional programming style partials. <strong>Partials</strong> are functions, which originally take many arguments, but an operation is applied to the original function to return a new function, so the first <em>n</em> arguments are always the same. Consider the following example, where we have a <kbd>multiply</kbd> function that is not a task:</p>
<pre><strong>&gt;&gt;&gt; new_multiply = multiply(2)
&gt;&gt;&gt; new_multiply(5)
</strong>10<strong>
# The first function is unaffected
&gt;&gt;&gt; multiply(2, 2)
</strong>4
  </pre>
<p>This is a fictional API, but is very close to the Celery version:</p>
<pre><strong>&gt;&gt;&gt; partial = multiply.s(4)</strong>
<strong>&gt;&gt;&gt; partial.delay(4)</strong></pre>
<p>The output in the worker window should show <kbd>16</kbd>. Basically, we created a new function, saved to partial, that will always multiply its input by four.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Callbacks</h1>
                
            
            
                
<p>Once a task is completed, it is very common to run another task, based on the output of the previous task. To achieve this, the <kbd>apply_async</kbd> function has a <kbd>link</kbd> method, used as follows:</p>
<pre><strong>&gt;&gt;&gt; multiply.apply_async((4, 4), link=log.s())</strong></pre>
<p>The worker output should show that both the <kbd>multiply</kbd> task and the <kbd>log</kbd> task returned <kbd>16</kbd>.</p>
<p>If you have a function that does not take input, or your callback does not need the result of the original method, then the task signature must be marked as immutable with the <kbd>si</kbd> method:</p>
<pre><strong>&gt;&gt;&gt; multiply.apply_async((4, 4), link=log.si("Message"))</strong></pre>
<p><strong>Callbacks</strong> can be used to solve real-world problems. If we wanted to send a welcome email every time a task created a new user, then we could produce that effect with the following call:</p>
<pre><strong>&gt;&gt;&gt; create_user.apply_async(("John Doe", password), link=welcome.s())</strong></pre>
<p>Partials and callbacks can be combined to produce some powerful effects:</p>
<pre><strong>&gt;&gt;&gt; multiply.apply_async((4, 4), link=multiply.s(4))</strong></pre>
<p>It's important to note that, if this call were saved and the <kbd>get</kbd> method was called on it, the result would be <kbd>16</kbd>, rather than <kbd>64</kbd>. This is because the <kbd>get</kbd> method does not return the results for callback methods. This will be solved with later methods.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Group</h1>
                
            
            
                
<p>The <kbd>group</kbd> function takes a list of signatures and creates a callable function to execute all of the signatures in parallel, then returns a list of all of the results as follows:</p>
<pre><strong>&gt;&gt;&gt; from celery import group
&gt;&gt;&gt; sig = group(multiply.s(i, i+5) for i in range(10))
&gt;&gt;&gt; result = sig.delay()
&gt;&gt;&gt; result.get()
</strong>[0, 6, 14, 24, 36, 50, 66, 84, 104, 126]</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Chain</h1>
                
            
            
                
<p>The <kbd>chain</kbd> function takes task signatures and passes the value of each result to the next value in the chain, returning one result, as follows:</p>
<pre><strong>&gt;&gt;&gt; from celery import chain
&gt;&gt;&gt; sig = chain(multiply.s(10, 10), multiply.s(4), multiply.s(20))
# same as above
&gt;&gt;&gt; sig = (multiply.s(10, 10) | multiply.s(4) | multiply.s(20))
&gt;&gt;&gt; result = sig.delay()
&gt;&gt;&gt; result.get()
</strong>8000
  </pre>
<p>Chains and partials can be taken a bit further. Chains can be used to create new functions when using partials, and chains can be nested as follows:</p>
<pre><strong># combining partials in chains <br/>&gt;&gt;&gt; func = (multiply.s(10) | multiply.s(2)) <br/>&gt;&gt;&gt; result = func.delay(16) <br/>&gt;&gt;&gt; result.get() <br/></strong>320<strong><br/># chains can be nested <br/>&gt;&gt;&gt; func = ( multiply.s(10) | multiply.s(2) | (multiply.s(4) | multiply.s(5)) ) <br/>&gt;&gt;&gt; result = func.delay(16) <br/>&gt;&gt;&gt; result.get()<br/></strong>6400
  </pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Chord</h1>
                
            
            
                
<p>The <kbd>chord</kbd> function creates a signature that will execute a <kbd>group</kbd> of signatures and pass the final result to a callback:</p>
<pre><strong>&gt;&gt;&gt; from celery import chord
&gt;&gt;&gt; sig = chord(
        group(multiply.s(i, i+5) for i in range(10)),
        log.s()
)
&gt;&gt;&gt; result = sig.delay()
&gt;&gt;&gt; result.get()
</strong>[0, 6, 14, 24, 36, 50, 66, 84, 104, 126]
  </pre>
<p>Just like the link argument, the callback is not returned with the <kbd>get</kbd> method.</p>
<p>Using the <kbd>chain</kbd> syntax with a group and a callback automatically creates a chord signature:</p>
<pre><strong># same as above
&gt;&gt;&gt; sig = (group(multiply.s(i, i+5) for i in range(10)) | log.s())
&gt;&gt;&gt; result = sig.delay()
&gt;&gt;&gt; result.get()
</strong>[0, 6, 14, 24, 36, 50, 66, 84, 104, 126]
  </pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Running tasks periodically</h1>
                
            
            
                
<p>Celery also has the ability to call tasks periodically. For those familiar with <strong>*</strong><strong>nix</strong> operating systems, this system is a lot like the command-line utility <kbd>cron</kbd>, but it has the added benefit of being defined in our source code rather than on some system file. As such, it will be much easier to update our code when it is ready for publishing to production—a stage that we will reach in <a href="380101ac-fb85-4e2e-b664-8d6de77928f4.xhtml">Chapter 13</a>, <em>Deploying Flask Apps</em>. In addition, all of the tasks are run within the application context, whereas a Python script called by <kbd>cron</kbd> would not be.</p>
<p class="mce-root"/>
<p>To add periodic tasks, add the following to the <kbd>DevConfig</kbd> configuration object:</p>
<pre>import datetime 
... 
CELERYBEAT_SCHEDULE = { 
    'log-every-30-seconds': { 
        'task': 'webapp.blog.tasks.log', 
        'schedule': datetime.timedelta(seconds=30), 
        'args': ("Message",) 
    }, 
} </pre>
<p>This <kbd>configuration</kbd> variable defines that the <kbd>log</kbd> task should be run every 30 seconds, with the <kbd>args</kbd> tuple passed as the parameters. Any <kbd>timedelta</kbd> object can be used to define the interval to run the task on.</p>
<p>To run the periodic tasks, another specialised worker, named a <kbd>beat</kbd> worker, is needed. In another Terminal window, run the following command:</p>
<pre><strong>$ celery -A celery_runner beat</strong></pre>
<p>If you now watch the Terminal output for the main <kbd>Celery</kbd> worker, you should now see a log event every 30 seconds.</p>
<p>What if your task needs to run on much more specific intervals; say, for example, every Tuesday in June at 3 am and 5 pm? For very specific intervals, there is the Celery <kbd>crontab</kbd> object.</p>
<p>To illustrate how the <kbd>crontab</kbd> object represents intervals, consider the following examples:</p>
<pre><strong>&gt;&gt;&gt; from celery.schedules import crontab</strong>
<strong># Every midnight</strong>
<strong>&gt;&gt;&gt; crontab(minute=0, hour=0)</strong>
<strong># Once a 5AM, then 10AM, then 3PM, then 8PM</strong>
<strong>&gt;&gt;&gt; crontab(minute=0, hour=[5, 10, 15, 20])</strong>
<strong># Every half hour</strong>
<strong>&gt;&gt;&gt; crontab(minute='*/30')</strong>
<strong># Every Monday at even numbered hours and 1AM</strong>
<strong>&gt;&gt;&gt; crontab(day_of_week=1, hour ='*/2, 1')</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The object has the following arguments:</p>
<ul>
<li><kbd>minute</kbd></li>
<li><kbd>hour</kbd></li>
<li><kbd>day_of_week</kbd></li>
<li><kbd>day_of_month</kbd></li>
<li><kbd>month_of_year</kbd></li>
</ul>
<p>Each of these arguments can take various inputs. With plain integers, they operate much like the <kbd>timedelta</kbd> object, but can also take strings and lists. When passed a list, the task will execute on every moment that is in the list. When passed a string in the form of <em>*/x</em>, the task will execute every moment that the modulo operation returns zero. Also, the two forms can be combined to form a comma-separated string of integers and divisions.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Monitoring Celery</h1>
                
            
            
                
<p>When our code is pushed to the server, our <kbd>Celery</kbd> worker will not be run in the Terminal window—rather, it will be run as a background task. Because of this, Celery provides many command-line arguments to monitor the status of your <kbd>Celery</kbd> worker and tasks. These commands take the following form:</p>
<pre><strong>$ celery -A celery_runner &lt;command&gt;</strong></pre>
<p>The main tasks to view the status of your workers are as follows:</p>
<ul>
<li><kbd>status</kbd>: This prints the running workers and if they are up.</li>
<li><kbd>result</kbd>: When passed a task ID, this shows the return value and final status of the task.</li>
<li><kbd>purge</kbd>: Using this, all messages in the broker will be deleted.</li>
<li><kbd>inspect active</kbd>: This lists all active tasks.</li>
<li><kbd>inspect scheduled</kbd>: This lists all tasks that have been scheduled with the <kbd>eta</kbd> argument.</li>
<li><kbd>inspect registered</kbd>: This lists all of the tasks waiting to be processed.</li>
<li><kbd>inspect stats</kbd>: This returns a dictionary full of statics on the currently running workers and the broker.</li>
</ul>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Web-based monitoring with Flower</h1>
                
            
            
                
<p><strong>Flower</strong> is a web-based, real-time management tool for Celery. In Flower, all active, queued, and completed tasks can be monitored. Flower also provides graphs and statics on how long each task has been sitting in the queue versus how long its execution took, and the arguments to each of those tasks.</p>
<p>To install <kbd>flower</kbd>, use the <kbd>pip</kbd> command, as follows:</p>
<pre><strong>$ pip install flower</strong></pre>
<p>To run it, just treat <kbd>flower</kbd> as a Celery command, as follows:</p>
<pre><strong>$ celery flower -A celery_runner --loglevel=info</strong></pre>
<p>Now, open your browser to <kbd>http://localhost:5555</kbd>. It's best to familiarize yourself with the interface while tasks are running, so go to the command line and type the following:</p>
<pre><strong>&gt;&gt;&gt; export FLASK_APP=manage.py<br/>&gt;&gt;&gt; flask shell<br/>&gt;&gt;&gt; from webapp.blog.tasks import *<br/>&gt;&gt;&gt; from celery import chord, group<br/>&gt;&gt;&gt; </strong><strong>sig = chord(</strong> <strong> group(multiply.s(i, i+5) for i in xrange(10000)),</strong> <strong> log.s()</strong> <strong>)</strong>
<strong>&gt;&gt;&gt; sig.delay()</strong></pre>
<p>Your worker process will now start processing 10,000 tasks. Browse around the different pages while the tasks are running to see how <kbd>flower</kbd> interacts with your worker while it's really churning, as shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/dd0af794-6345-440f-a8b6-f02b99026662.png"/></p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a reminder app</h1>
                
            
            
                
<p>Let's get into some real-world example applications of Celery. Suppose another page on our site now requires a reminders feature. Users can create reminders that will send an email to a specified location at a specified time. We will need a model, a task, and a way to call our task automatically every time a model is created.</p>
<p>Let's start with the following basic SQLAlchemy model:</p>
<pre>class Reminder(db.Model): 
    id = db.Column(db.Integer(), primary_key=True) 
    date = db.Column(db.DateTime()) 
    email = db.Column(db.String()) 
    text = db.Column(db.Text()) 
  <br/>    def __repr__(self): 
        return "&lt;Reminder '{}'&gt;".format(self.text[:20]) </pre>
<p>Now, we need a task that will send an email to the location in the model. In our <kbd>blog/tasks.py</kbd> file, look up the following task:</p>
<pre>@celery.task(<br/>    bind=True,<br/>    ignore_result=True,<br/>    default_retry_delay=300,<br/>    max_retries=5<br/>)<br/>def remind(self, pk):<br/>    reminder = Reminder.query.get(pk)<br/>    msg = MIMEText(reminder.text)<br/><br/>    msg['Subject'] = "Your reminder"<br/>    msg['From'] = current_app.config['SMTP_FROM']<br/>    msg['To'] = reminder.email<br/>    try:<br/>        smtp_server = smtplib.SMTP(current_app.config['SMTP_SERVER'])<br/>        smtp_server.starttls()<br/>        smtp_server.login(current_app.config['SMTP_USER'], <br/>        current_app.config['SMTP_PASSWORD'])<br/>        smtp_server.sendmail("", [reminder.email], msg.as_string())<br/>        smtp_server.close()<br/>        return<br/>    except Exception as e:<br/>        self.retry(exc=e)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Note that our task takes a primary key, rather than a model. This is a hedge against a race condition, as a passed model could be stale by the time the worker finally gets around to processing it. You will also have to replace the placeholder emails and login details with your own login info.</p>
<p>How do we have our task called when the user creates a reminder model? We will use an SQLAlchemy feature, named <kbd>events</kbd>. SQLAlchemy allows us to register callbacks on our models that will be called when specific changes are made to our models. Our task will use the <kbd>after_insert</kbd> event, which is called after new data is entered into the database, whether the model is brand new or being updated.</p>
<p>We need a callback in <kbd>blog/tasks.py</kbd>:</p>
<pre>def on_reminder_save(mapper, connect, self): 
    remind.apply_async(args=(self.id,), eta=self.date) </pre>
<p>Now, in <kbd>blog/__init__.py</kbd>, we will register our callback on our model:</p>
<pre>from sqlalchemy import event<br/>from .models import db, Reminder<br/>from .tasks import on_reminder_save<br/><br/><br/>def create_module(app, **kwargs):<br/>    <strong>event.listen(Reminder, 'after_insert', on_reminder_save)</strong><br/>    from .controllers import blog_blueprint<br/>    app.register_blueprint(blog_blueprint)</pre>
<p>Now, every time a model is saved, a task is registered that will send an email to our user.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a weekly digest</h1>
                
            
            
                
<p>Say our blog has a lot of people who don't use RSS, and prefer mailing lists. We need some way to create a list of new posts at the end of every week to increase our site's traffic. To solve this problem, we will create a digest task that will be called by a beat worker at 10 am, every Saturday.</p>
<p>First, in <kbd>blog/tasks.py</kbd>, let's create our task as follows:</p>
<pre>@celery.task(<br/>    bind=True,<br/>    ignore_result=True,<br/>    default_retry_delay=300,<br/>    max_retries=5<br/>)<br/>def digest(self):<br/>    # find the start and end of this week<br/>    year, week = datetime.datetime.now().isocalendar()[0:2]<br/>    date = datetime.date(year, 1, 1)<br/>    if (date.weekday() &gt; 3):<br/>        date = date + datetime.timedelta(7 - date.weekday())<br/>    else:<br/>        date = date - datetime.timedelta(date.weekday())<br/>    delta = datetime.timedelta(days=(week - 1) * 7)<br/>    start, end = date + delta, date + delta + <br/>    datetime.timedelta(days=6)<br/><br/>    posts = Post.query.filter(<br/>        Post.publish_date &gt;= start,<br/>        Post.publish_date &lt;= end<br/>    ).all()<br/><br/>    if (len(posts) == 0):<br/>        return<br/><br/>    msg = MIMEText(render_template("digest.html", posts=posts), 'html')<br/><br/>    msg['Subject'] = "Weekly Digest"<br/>    msg['From'] = current_app.config['SMTP_FROM']<br/><br/>    try:<br/>        smtp_server = smtplib.SMTP(current_app.config['SMTP_SERVER'])<br/>        smtp_server.starttls()<br/>        smtp_server.login(current_app.config['SMTP_USER'], <br/>        current_app.config['SMTP_PASSWORD'])<br/>        smtp_server.sendmail("", [""], msg.as_string())<br/>        smtp_server.close()<br/><br/>        return<br/>    except Exception as e:<br/>        self.retry(exc=e)</pre>
<p>We will also need to add a periodic schedule to our configuration object in <kbd>config.py</kbd> to manage our task:</p>
<pre>from celery.schedules import crontab<br/>...<br/>CELERYBEAT_SCHEDULE = { 'weekly-digest': { 'task': 'blog.tasks.digest', 'schedule': crontab(day_of_week=6, hour='10') }, }</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We also need to configure our SMTP server so that we are able to send emails. This can be done using Gmail or your corporate email credentials. Add your chosen account information to the configuration object in <kbd>config.py</kbd> :</p>
<pre>...<br/>SMTP_SERVER = "smtp.gmail.com"<br/>SMTP_USER = "sometestemail@gmail.com"<br/>SMTP_PASSWORD = "password"<br/>SMTP_FROM = "from@flask.com"<br/>...</pre>
<p>Finally, we need our email template. Unfortunately, HTML in email clients is terribly outdated. Every single email client has different rendering bugs and quirks, and the only way to find them is to open your email in all the clients. Many email clients don't even support CSS, and those that do support a very small amount of selectors and attributes. In order to compensate, we have to use the web development methods of 10 years ago; that is, designing tables with inline styles. Here is our <kbd>digest.html</kbd> file:</p>
<pre>&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" <br/>   "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt; 
&lt;html &gt; 
  &lt;head&gt; 
    &lt;meta http-equiv="Content-Type" 
      content="text/html; charset=UTF-8" /&gt; 
    &lt;meta name="viewport" 
      content="width=device-width, initial-scale=1.0"/&gt; 
    &lt;title&gt;Weekly Digest&lt;/title&gt; 
  &lt;/head&gt; 
  &lt;body&gt; 
    &lt;table align="center" 
      border="0" 
      cellpadding="0" 
      cellspacing="0" 
      width="500px"&gt; 
      &lt;tr&gt; 
        &lt;td style="font-size: 32px; 
          font-family: Helvetica, sans-serif; 
          color: #444; 
          text-align: center; 
          line-height: 1.65"&gt; 
          Weekly Digest 
        &lt;/td&gt; 
      &lt;/tr&gt; 
      {% for post in posts %} 
      &lt;tr&gt; 
        &lt;td style="font-size: 24px; 
          font-family: sans-serif; 
          color: #444; 
          text-align: center; 
          line-height: 1.65"&gt; 
          {{ post.title }} 
        &lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
        &lt;td style="font-size: 14px; 
          font-family: serif; 
          color: #444; 
          line-height:1.65"&gt; 
          {{ post.text | truncate(500) | safe }} 
        &lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
        &lt;td style="font-size: 12px; 
          font-family: serif; 
          color: blue; 
          margin-bottom: 20px"&gt; 
          &lt;a href="{{ url_for('.post', post_id=post.id) }}"&gt;Read <br/>             More&lt;/a&gt; 
        &lt;/td&gt; 
      &lt;/tr&gt; 
      {% endfor %} 
    &lt;/table&gt; 
  &lt;/body&gt; 
&lt;/html&gt; </pre>
<p>Now, at the end of every week, our digest task will be called, and will send an email to all the users present in our mailing list.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>Celery is a very powerful task queue that allows programmers to defer the processing of slower tasks to another process. Now that you understand how to move complex tasks out of the Flask process, we will take a look at a collection of Flask extensions that simplify some common tasks seen in Flask apps.</p>
<p>In the next chapter, you will learn how to leverage some great community-built Flask extensions to improve performance, debug, and even quickly create an administration back office.</p>


            

            
        
    </body></html>