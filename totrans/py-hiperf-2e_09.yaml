- en: Designing for High Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the earlier chapters, we learned how to use the vast array of tools available
    in Python's standard library and third-party packages to assess and improve the
    performance of Python applications. In this chapter, we will provide some general
    guidelines on how to approach different kinds of applications as well as illustrate
    some good practices that are commonly adopted by several Python projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Picking the right performance technique for generic, number crunching, and big
    data applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structuring a Python project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isolating Python installations with virtual environments and containerization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up continuous integration with Travis CI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing a suitable strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many packages are available for improving the performance of programs, but how
    do we determine the best optimization strategy for our program? A variety of factors
    dictate the decision on which method to use. In this section, we will try to answer
    this question as comprehensively as possible, based on broad application categories.
  prefs: []
  type: TYPE_NORMAL
- en: The first aspect to take into consideration is the type of application. Python
    is a language that serves multiple and very diverse communities that span web
    services, system scripting, games, machine learning, and much more. Those different
    applications will require optimization efforts for different parts of the program.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a web service can be optimized to have a very short response time.
    Also, it has to be able to answer as many requests as possible using as little
    resources as possible (that is, it will try to achieve lower latency), while numerical
    code may require weeks to run. It's important to improve the amount of data the
    system may process, even if there's a significant start up overhead (in this case,
    we are interested in throughput).
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect is the platform and architecture we are developing for. While
    Python has support for a lot of platforms and architectures, many of the third-party
    libraries may have limited support for certain platforms, especially when dealing
    with packages that bind into C extensions. For this reason, it's necessary to
    check the availability of libraries for the target platforms and architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Also, some architectures, such as embedded systems and small devices, may have
    severe CPU and memory restrictions. This is an important factor to take into consideration
    as, for instance, some techniques (such as multiprocessing) may consume too much
    memory or require the execution of additional software.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the business requirements are equally important. Many times, software
    products require fast iterations and the ability to change the code quickly. Generally
    speaking, you want to keep your software stack as minimal as possible so that
    modification, testing, deployment, and introducing additional platform support
    becomes easy and feasible in a short period of time. This also applies to the
    team--installing the software stack and starting the development should be as
    smooth as possible. For this reason, one should generally prefer pure Python libraries
    over extensions, with the possible exception of solid, battle-tested libraries,
    such as NumPy. Additionally, various business aspects will help determine which
    operations need to be optimized first (always remember that *premature optimization
    is the root of all evil*).
  prefs: []
  type: TYPE_NORMAL
- en: Generic applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generic applications, such as web apps or mobile app backends, usually involve
    calls to remote services and databases. For such cases, it can be useful to take
    advantage of asynchronous frameworks, such as the ones presented in [Chapter 6](2b46e5c0-5308-4073-b1c6-4232a881b39f.xhtml),
    *Implementing Concurrency*; this will improve application logic, system design,
    responsiveness and, also, it will simplify the handling of network failures.
  prefs: []
  type: TYPE_NORMAL
- en: Use of asynchronous programming also makes it easier to implement and use microservices.
    A **microservice**, although there is no standard definition, can be thought of
    as a remote service that focuses on a specific aspect of the application (for
    example, authentication).
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind microservices is that you can build an application by composing
    different microservices that communicate through a simple protocol (such as gRPC,
    REST calls, or through a dedicated message queue). This architecture is in contrast
    with a monolithic application where all the services are handled by the same Python
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of microservices include strong decoupling of different parts of
    the application. Small, simple services can be implemented and maintained by different
    teams as well as be updated and deployed at different times. This also allows
    microservices to be easily replicated so that they can handle more users. Additionally,
    since the communication is done through a simple protocol, microservices can be
    implemented in a different language that can be more appropriate than Python for
    the specific application.
  prefs: []
  type: TYPE_NORMAL
- en: If the performance of a service is not satisfactory, the application can often
    be executed on a different Python interpreter, such as PyPy (provided that all
    the third-party extensions are compatible) to achieve sufficient speed gains.
    Otherwise, algorithmic strategies as well as porting bottlenecks to Cython is
    generally sufficient to achieve satisfactory performance.
  prefs: []
  type: TYPE_NORMAL
- en: Numerical code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If your goal is to write numerical code, an excellent strategy is to start directly
    with a NumPy implementation. Using NumPy is a safe bet because it is available
    and tested on many platforms and, as we have seen in the earlier chapters, many
    other packages treat NumPy arrays as first-class citizens.
  prefs: []
  type: TYPE_NORMAL
- en: When properly written (such as by taking advantage of broadcasting and other
    techniques we learned in [Chapter 2](68a7c14e-5270-49a5-862e-96cf59cddf60.xhtml),
    *Pure Python Optimizations*), NumPy performance is already quite close to the
    native performance achievable by C code, and won't require further optimization.
    That said, certain algorithms are hard to express efficiently using NumPy's data
    structures and methods. When this happens, two very good options can be, for example,
    Numba or Cython.
  prefs: []
  type: TYPE_NORMAL
- en: Cython is a very mature tool used intensely by many important projects, such
    as `scipy` and `scikit-learn`. Cython code, with its explicit, static type declarations,
    makes it very understandable, and most Python programmers will have no problem
    picking up its familiar syntax. Additionally, the absence of "magic" and good
    inspection tools make it easy for the programmer to predict its performance and
    have educated guesses as to what to change to achieve maximum performance.
  prefs: []
  type: TYPE_NORMAL
- en: Cython, however, has some drawbacks. Cython code needs to be compiled before
    it can be executed, thus breaking the convenience of the Python edit-run cycle.
    This also requires the availability of a compatible C compiler for the target
    platform.  This also complicates distribution and deployment, as multiple platforms,
    architectures, configurations, and compilers need to be tested for every target.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, Numba API requires only the definition of pure-Python functions,
    which get compiled on the fly, maintaining the fast Python edit-run cycle. In
    general, Numba requires a LLVM toolchain installation to be available on the target
    platform. Note that, as of version 0.30, there is some limited support for **Ahead-Of-Time**
    (**AOT**) compilation of Numba functions so that Numba-compiled functions can
    be packaged and deployed without requiring a Numba and LLVM installation.
  prefs: []
  type: TYPE_NORMAL
- en: Note that both Numba and Cython are usually available pre-packaged with all
    of their dependencies (including compilers) on the default channels of the conda
    package manager. Therefore, deployment of Cython can be greatly simplified on
    the platforms where the conda package manager is available.
  prefs: []
  type: TYPE_NORMAL
- en: What if Cython and Numba are still not enough? While this is generally not required,
    an additional strategy would be to implement a pure C module (which can be further
    optimized using compiler flags or hand-tuning) and use it from a Python module
    using either the `cffi` package ([https://cffi.readthedocs.io/en/latest/](https://cffi.readthedocs.io/en/latest/))
    or Cython.
  prefs: []
  type: TYPE_NORMAL
- en: Using NumPy, Numba, and Cython is a very effective strategy to obtain near-optimal
    performance on serial codes. For many applications, serial codes are certainly
    enough and, even if the ultimate plan is to have a parallel algorithm, it’s still
    very worthy working on a serial reference implementation for debugging purposes
    and because a serial implementation is likely to be faster on small datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel implementations vary considerably in complexity depending on the particular
    application. In many cases, the program can be easily expressed as a series of
    independent calculations followed by some sort of *aggregation* and is parallelizable
    using simple process-based interfaces, such as `multiprocessing.Pool` or `ProcessPoolExecutor`,
    which have the advantage of being able to execute generic Python code in parallel
    without much trouble.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid the time and memory overhead of starting multiple processes, one can
    use threads. NumPy functions typically release the GIL and are good candidates
    for thread-base parallelization. Additionally, Cython and Numba provide special `nogil` statements
    as well as automatic parallelization, which makes them suitable for simple, lightweight
    parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: For more complex use cases, you may have to change the algorithm significantly.
    In those cases, Dask arrays are a decent option, which provide an almost-drop-in
    replacement for standard NumPy. Dask has the further advantage of operating very
    transparently and is easy to tweak.
  prefs: []
  type: TYPE_NORMAL
- en: Specialized applications that make intensive use of linear algebra routines
    (such as deep learning and computer graphics) may benefit from packages such as
    Theano and Tensorflow, which are capable of highly performant and automatic parallelization
    with built-in GPU support.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, `mpi4py` usage can be used for deploying parallel python scripts on
    a MPI-based supercomputer (commonly available for researchers in universities).
  prefs: []
  type: TYPE_NORMAL
- en: Big data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large datasets (typically larger than 1 TB) are becoming increasingly common
    and a lot of resources have been invested in developing technologies capable of
    collecting, storing, and analyzing them. Typically, the choice of which framework
    to use is bound to how the data is stored in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: Many times, even if the complete dataset doesn't fit in a single machine, it
    is still possible to devise strategies to extract the answers without having to
    probe the whole dataset. For example, it is quite often possible to answer questions
    by extracting a small, interesting subset of data that can be easily loaded in
    memory and analyzed with highly convenient and performant libraries, such as Pandas.
    By filtering or randomly sampling data points, one can often find a good enough
    answer to a business question without having to resort to big data tools.
  prefs: []
  type: TYPE_NORMAL
- en: If the bulk of the company's software is written in Python, and you have the
    freedom to decide your software stack, it would make sense to use Dask distributed.
    The software package has a very simple setup and is tightly integrated with the
    Python ecosystem. Using something such as Dask `array` and `DataFrame`, it's very
    easy to scale your already-existing Python algorithms by adapting NumPy and Pandas
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Quite often, some companies may have already set up a Spark cluster. In this
    case, PySpark is the optimal choice, and the use of SparkSQL is encouraged for
    higher performance. One of the Spark advantages is that it allows the use of other
    languages, such as Scala and Java.
  prefs: []
  type: TYPE_NORMAL
- en: Organizing your source code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The repository structure of a typical Python project consists, at a minimum,
    of a directory containing a `README.md` file, a Python module or package containing
    the source code for the application or library, and a `setup.py` file. Projects
    may also adopt different conventions to comply with company policies or specific
    frameworks in use. In this section, we will illustrate some common practices that
    are commonly found in community-driven Python projects which can include some
    of the tools we illustrated in the earlier chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical directory structure for a Python project named  `myapp` can look
    like this. Now, we will elucidate the role of each file and directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`README.md` is a text file that contains general information about the software,
    such as project scope, installation, a quick start, and useful links. If the software
    is released to the public, a `LICENSE` file is used to specify terms and conditions
    for its usage.'
  prefs: []
  type: TYPE_NORMAL
- en: Python software is commonly packaged using the `setuptools` library in a `setup.py`
    file. As we have seen in the earlier chapters, `setup.py` is also an effective
    way to compile and distribute Cython code.
  prefs: []
  type: TYPE_NORMAL
- en: The `myapp` package contains the source code for the application, including
    Cython modules. Sometimes, it's convenient to maintain pure-Python implementations
    besides their Cython-optimized counterparts. Commonly, the Cython version of a
    module is named with a c prefix (such as `cmodule1.pyx` in the preceding example).
  prefs: []
  type: TYPE_NORMAL
- en: If the external `.c` and `.h` files are needed, those are usually stored under
    an additional `src/` directory placed in the top-level (`myapp`) project directory.
  prefs: []
  type: TYPE_NORMAL
- en: The `tests/` directory contains testing code for application (usually in the
    form of unit tests), which can be run using a test runner, such as `unittest`
    or `pytest`. However, some projects prefer to place the `tests/` directory inside
    the `myapp` package. Since high-performance code is tweaked and rewritten continuously,
    having a solid test suite is crucial to spot bugs as early as possible and to
    improve the developer experience by shortening the test-edit-run cycle.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarks can be placed in the `benchmarks` directory; the advantage of having
    benchmarks separated from tests is that benchmarks can potentially take more time
    to execute. Benchmarks can also be run on a build server (see the *Continuous
    integration* section) as a simple mean to compare the performance of versions.
    While benchmarks usually take longer to run than unit tests, it's best to keep
    their execution as short as possible to avoid waste of resources.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the `docs/` directory contains user and developer documentation and
    API references. This usually also includes configuration files for documentation
    tools, such as `sphinx`. Other tools and scripts can be placed in the `tools/`
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: Isolation, virtual environments, and containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The importance of having isolated environments for code testing and execution
    becomes quite apparent by noticing what happens when you ask a friend to run one
    of your Python scripts. What happens is that you provide instructions to install
    Python version X and dependent packages `Y`, `X`, and ask them to copy and execute
    the script on their machine.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, your friend will proceed and download Python for its platform
    as well as the dependent libraries and try to execute the script. However, it
    can happen (more often than not) that the script will fail because either their
    computer has a different operating system than yours, or the installed libraries
    are not the same version as the one you installed on your machine. At other times,
    there can be previous installations that are improperly removed and will cause
    hard-to-debug conflicts and a lot of frustration.
  prefs: []
  type: TYPE_NORMAL
- en: A very easy way to avoid this scenario is to use virtual environments. Virtual
    environments are used to create and manage several Python installations by isolating
    Python, related executables, and third-party packages. Since Python's 3.3 version,
    the standard library includes the `venv` module (previously known as **virtualenv**),
    which is a tool designed to create and manage simple isolated environments. Python
    packages in `venv`-based virtual environments can be installed using `setup.py`
    files or through `pip`.
  prefs: []
  type: TYPE_NORMAL
- en: Providing exact and specific library versions is crucial when dealing with high-performance
    code. Libraries evolve all the time between releases and changes in the algorithms
    may dramatically affect the performance. For instance, popular libraries, such
    as `scipy` and `scikit-learn`, often port some of their codes and data structures
    to Cython, so it's really important that the user installs the correct version
    for optimal performance.
  prefs: []
  type: TYPE_NORMAL
- en: Using conda environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the time, using `venv` is a fine choice. However, when writing high-performance
    code, it often happens that some high-performance libraries also require non-Python
    software to be installed. This typically involves additional setting up of compilers
    and high-performance native libraries (in C, C++, or Fortran) to which Python
    packages link. As `venv` and `pip` are designed to deal with Python packages only,
    this scenario is poorly supported by these tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `conda` package manager was created specifically to handle such cases.
    Creating a virtual environment with conda can be done using the `conda create`
    command. The command takes a `-n` argument (`-n` stands for `--name`) that specifies
    an identifier for the newly created environment and the packages we intend to
    install. If we wish to create an environment that uses python version `3.5` and
    the latest version of NumPy, we use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Conda will take care of fetching the relative packages from their repositories
    and placing them in an isolated Python installation. To enable the virtual environment,
    you can use the `source activate` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After executing this command, the default Python interpreter will be switched
    to the version we specified earlier. You can easily verify the location of your
    Python executable using the `which` command, which returns the full path of the
    executable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: At this point, you are free to add, remove, and modify packages in the virtual
    environment without affecting the global Python installation. Further packages
    can be installed using the `conda install <package name>` command or through `pip`.
  prefs: []
  type: TYPE_NORMAL
- en: The beauty of virtual environments is that you can install or compile any software
    you want in a well-isolated fashion. This means that if, for some reason, your
    environment gets corrupted, you can scratch it and start from zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'To remove the `myenv` environment, you first need to deactivate it, and then
    use the `conda env remove` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'What if a package is not available on the standard `conda` repositories? One
    option is to look whether it is available in the `conda-forge` community channel.
    To search for a package in `conda-forge`, you can add the `-c` option (which stands
    for `--channel`) to the `conda search` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The command will list a series of packages and versions available that match
    the `scipy` query string. Another option is to search for the package in the public
    channels hosted on **Anaconda Cloud**. The command-line client for Anaconda Cloud
    can be downloaded by installing the `anaconda-client` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the client is installed, you can use the `anaconda` command-line client
    to search for packages. In the following example, we demonstrate how to look for
    the `chemview` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Installation can then be easily performed by specifying the appropriate channel
    with the `-c` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Virtualization and Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Virtualization has been around for a long time as a way to run multiple operating
    systems on the same machine in order to better utilize physical resources.
  prefs: []
  type: TYPE_NORMAL
- en: One way to achieve virtualization is to employ a *virtual machine*. Virtual
    machines work by creating virtual hardware resources, such as CPU, memory, and
    devices, and use those to install and run multiple operating systems on the same
    machine. Virtualization can be accomplished by installing a hypervisor application
    on top of an operating system (called *host*). The hypervisor is capable of creating,
    managing, and monitoring virtual machines and their respective operating systems
    (called *guests*).
  prefs: []
  type: TYPE_NORMAL
- en: It's important to note that virtual environments, despite their name, have nothing
    to do with virtual machines. A virtual environment is Python-specific and works
    by setting up different Python interpreters through shell scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Containers are a way to isolate an application by creating an environment separated
    from the host operating system and contain only the necessary dependencies. Containers
    are an operating system feature that allows you to share the hardware resources
    (provided by the operating system kernel) for multiple instances. A container
    is different from a virtual machine because it does not abstract hardware resources,
    but merely shares the operating system's kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Containers are very efficient at utilizing hardware resources as those are accessed
    natively through the kernel. For this reason, they are an excellent solution for
    high-performance applications. They are also fast to create and destroy and can
    be used to quickly test an application in isolation. Containers are also used
    to simplify deployments (especially microservices) and to develop build servers,
    such as the ones we mentioned in the preceding section.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 8](95fc6212-c0ee-4dee-8d8a-56dc57fb6c97.xhtml), *Distributed Processing*,
    we used **docker** to easily set up a PySpark installation. Docker is one of the
    most popular containerization solutions available today. The best way to install
    docker is by following the instructions on the official website ([https://www.docker.com/](https://www.docker.com/)).
    After installation, it is possible to easily create and manage containers using
    the docker command-line interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can start a new container by using the `docker run` command. In the following
    example, we will demonstrate how to use `docker run` to execute a shell session
    in an Ubuntu 16.04 container. To do this, we will need to specify the following
    arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-i` specifies that we are trying to start an interactive session. It is also
    possible to execute individual docker commands without interactivity (for example,
    when starting a web server).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-t <image name>` specifies which system image to use. In the following example,
    we use the `ubuntu:16.04` image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '` /bin/bash`, which is the command to run inside the container, demonstrated
    as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This command will immediately take us into a separate, isolated shell where
    we can play around with the system and install software without touching the host
    operating system. Using a container is a very good way to test installations and
    deployments on different Linux flavors. After we are done with the interactive
    shell, we can type the `exit` command to return to the host system.
  prefs: []
  type: TYPE_NORMAL
- en: In the last chapter, we also made use of the port and detach options, `-p` and
    `-d`, to run the executable `pyspark`. The `-d` option simply asks Docker to run
    the command in the background. The `-p <host_port>:<guest_port>` option was, instead,
    necessary to map a network port of the host operating system to the guest system;
    without this option, the Jupyter Notebook would not have been reachable from a
    browser running in the host system.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can monitor the status of the containers with `docker ps`, as shown in the
    following snippet. The `-a` option (which stands for *all*) serves to output information
    about all the containers, whether they are currently running or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The information provided by `docker ps` includes a hexadecimal identifier, `585f53e77ce9`,
    as well as a human readable name, `pensive_hamilton`, both of which can be used
    to specify the container in other docker commands. It also includes additional
    information about the command executed, creation time, and the execution's current
    status.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can resume the execution of an exited container using the `docker start`
    command. To gain shell access to the container, you can use `docker attach`. Both
    these commands can be followed by either the container ID or its human readable
    name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'A container can be easily removed using the `docker run` command followed by
    a container identifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, you are free to execute commands, run, stop, and resume containers
    as needed, in less than a second. Using docker containers interactively is a great
    way to test things out and play with new packages without disturbing the host
    operating system. Since you can run many containers at the same time, docker can
    also be used to simulate a distributed system (for testing and learning purposes)
    without having to own an expensive computing cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Docker also allows you to create your own system images, which is useful for
    distribution, testing, deployment, and documentation purposes. This will be the
    topic of the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Creating docker images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker images are ready-to-use, pre-configured systems. The `docker run` command
    can be used to access and install the docker images available on the **DockerHub**
    ([https://hub.docker.com/](https://hub.docker.com/)), a web service where package
    maintainers upload ready-to-use images to test and deploy various applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to create a docker image is by using the `docker commit` command on
    an existing container. The docker commit command takes a container reference and
    the output image names as arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Using this method is useful to save snapshots of a certain container but, if
    the image is removed from the system, the steps to recreate the image are lost
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: A better way to create an image is to build it using a **Dockerfile**. A Dockerfile
    is a text file that provides instructions on how to build an image starting from
    another image. As an example, we will illustrate the contents of the Dockerfile
    we used in the last chapter to set up PySpark with Jupyter notebook support. The
    complete file is reported here.
  prefs: []
  type: TYPE_NORMAL
- en: Each Dockerfile needs a starting image, which can be declared with the `FROM`
    command. In our case, the starting image is `jupyter/scipy-notebook`, which is
    available through DockerHub ([https://hub.docker.com/r/jupyter/scipy-notebook/](https://hub.docker.com/r/jupyter/scipy-notebook/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have defined our starting image, we can start issuing shell commands
    to install packages and perform other configurations using a series of `RUN` and
    `ENV` commands. In the following example, you can recognize installation of Java
    Runtime Environment (`openjdk-7-jre-headless`) as well as downloading Spark and
    setting up relevant environment variables. The `USER` instructions can be used to
    specify which user executes the subsequent commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Dockerfiles can be used to create images using the following command from the
    directory where the Dockerfile is located. The `-t` option can be used to specify
    the tag that will be used to store the image. With the following line, we can
    create the image named `pyspark` from the preceding Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The command will automatically retrieve the starting image, `jupyter/scipy-notebook`,
    and produce a new image, named `pyspark`.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Continuous integration is a great way to ensure that the application stays bug-free
    at every development iteration. The main idea behind continuous integration is
    to run the test suite for the project very frequently, usually on a separate build
    server that pulls the code directly from the main project repository.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a build server can be accomplished by manually setting up software such
    as Jenkins ([https://jenkins.io/](https://jenkins.io/)), Buildbot ([http://buildbot.net/](http://buildbot.net/)),
    and Drone ([https://github.com/drone/drone](https://github.com/drone/drone)) on
    a machine. This a convenient and cheap solution, especially for small teams and
    private projects.
  prefs: []
  type: TYPE_NORMAL
- en: Most open source projects take advantage of Travis CI ([https://travis-ci.org/](https://travis-ci.org/)),
    a service capable of building and testing your code automatically from your repository
    because it's tightly integrated with GitHub. As of today, Travis CI provides a
    free plan for open source projects. Many open source Python projects take advantage
    of Travis CI to ensure that the programs run correctly on multiple Python versions
    and platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Travis CI can be set up easily from a GitHub repository by including a `.travis.yml`
    file containing the build instruction for the project and activating the builds
    on the Travis CI website ([https://travis-ci.org/](https://travis-ci.org/)) after
    registering an account.
  prefs: []
  type: TYPE_NORMAL
- en: An example `.travis.yml` for a high performance application is illustrated here.
    The file contains instructions to build and run the software that are specified
    using a few sections written in YAML syntax.
  prefs: []
  type: TYPE_NORMAL
- en: The `python` section specifies which Python versions to use. The `install` section
    will download and set up conda for testing, installing dependencies, and setting
    up the project. While this step is not necessary (one can use `pip` instead),
    conda is a great package manager for high-performance applications as it contains
    useful native packages.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `script` section contains the code needed to test the code. In this example,
    we limit ourselves to run our tests and benchmarks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Every time new code is pushed (as well as other configurable events) to the
    GitHub repository, Travis CI will spin up a container, install dependencies, and
    run the test suite. Using Travis CI in open source projects is a great practice
    as it is a form of constant feedback about the status of the project and also
    provides up-to-date installation instructions through a continuously tested `.travis.yml`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deciding on a strategy to optimize your software is a complex and delicate task that
    depends on the application type, target platforms, and business requirements.
    In this chapter, we provided some guidelines to help you think and choose an appropriate
    software stack for your own applications.
  prefs: []
  type: TYPE_NORMAL
- en: High-performance numerical applications sometimes require managing installation
    and deployment of third-party packages that may require handling of external tools
    and native extensions. In this chapter, we saw how to structure your Python project,
    including tests, benchmarks, documentation, Cython modules, and C extensions.
    Also, we introduced the continuous integration service Travis CI, which can be
    used to enable continuous testing for your projects hosted on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we also learned about virtual environments and docker containers that
    can be used to test applications in isolation and to greatly simplify deployments
    and ensure that multiple developers have access to the same platform.
  prefs: []
  type: TYPE_NORMAL
