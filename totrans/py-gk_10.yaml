- en: '*Chapter 7*: Multiprocessing, Multithreading, and Asynchronous Programming'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can write efficient and optimized code for faster execution time, but there
    is always a limit to the amount of resources available for the processes running
    our programs. However, we can still improve application execution time by executing
    certain tasks in parallel on the same machine or across different machines. This
    chapter will cover parallel processing or concurrency in Python for the applications
    running on a single machine. We will cover parallel processing using multiple
    machines in the next chapter. In this chapter, we focus on the built-in support
    available in Python for the implementation of parallel processing. We will start
    with the multithreading in Python followed by discussing the multiprocessing.
    After that, we will discuss how we can design responsive systems using asynchronous
    programming. For each of the approaches, we will design and discuss a case study
    of implementing a concurrent application to download files from a Google Drive
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding multithreading in Python and its limitations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going beyond a single CPU – implementing multiprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using asynchronous programming for responsive systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After completing this chapter, you will be aware of the different options for
    building multithreaded or multiprocessing applications using built-in Python libraries.
    These skills will help you to build not only more efficient applications but also
    build applications for large-scale users.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the technical requirements for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Python 3 (3.7 or later)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Google Drive account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: API key enabled for your Google Drive account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sample code for this chapter can be found at [https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter07](https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter07).
  prefs: []
  type: TYPE_NORMAL
- en: We will start our discussion with multithreading concepts in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding multithreading in Python and its limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A thread is a basic unit of execution within an operating system process, and
    it consists of its own program counter, a stack, and a set of registers. An application
    process can be built using multiple threads that can run simultaneously and share
    the same memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'For multithreading in a program, all the threads of a process share common
    code and other resources, such as data and system files. For each thread, all
    its related information is stored as a data structure inside the operating system
    kernel, and this data structure is called the **Thread Control Block** (**TCB**).
    The TCB has the following main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Program Counter (PC)**: This is used to track the execution flow of the program.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**System Registers (REG)**: These registers are used to hold variable data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stack**: The stack is an array of registers that manages the execution history.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The anatomy of a thread is exhibited in *Figure 7.1*, with three threads. Each
    thread has its own PC, a stack, and REG, but shares code and other resources with
    other threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Multiple threads in a process'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_07_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.1 – Multiple threads in a process
  prefs: []
  type: TYPE_NORMAL
- en: The TCB also contains a thread identifier, the state of the thread (such as
    running, waiting, or stopped), and a pointer to the process it belongs to. Multithreading
    is an operating system concept. It is a feature offered through the system kernel.
    The operating system facilitates the execution of multiple threads concurrently
    in the same process context, allowing them to share the process memory. This means
    the operating system has full control of which thread will be activated, rather
    than the application. We need to underline this point for a later discussion comparing
    different concurrency options.
  prefs: []
  type: TYPE_NORMAL
- en: 'When threads are run on a single-CPU machine, the operating system actually
    switches the CPU from one thread to the other such that the threads appear to
    be running concurrently. Is there any advantage to running multiple threads on
    a single-CPU machine? The answer is yes and no, and it depends on the nature of
    the application. For applications running using only the local memory, there may
    not be any advantage; in fact, it is likely to exhibit lower performance due to
    the overhead of switching threads on a single CPU. But for applications that depend
    on other resources, the execution can be faster because of the better utilization
    of the CPU: when one thread is waiting for another resource, another thread can
    utilize the CPU.'
  prefs: []
  type: TYPE_NORMAL
- en: When executing multiple threads on multiprocessors or multiple CPU cores, it
    is possible to execute them concurrently. Next, we will discuss the limitations
    of multithreaded programming in Python.
  prefs: []
  type: TYPE_NORMAL
- en: What is a Python blind spot?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From a programming perspective, multithreading is an approach to running different
    parts of an application concurrently. Python uses multiple kernel threads that
    can run the Python user threads. But the Python implementation (*CPython*) allows
    threads to access the Python objects through one global lock, which is called
    the **Global Interpreter Lock (GIL)**. In simple words, the GIL is a mutex that
    allows only one thread to use the Python interpreter at a time and blocks all
    other threads. This is necessary to protect the reference count that is managed
    for each object in Python from garbage collection. Without such protection, the
    reference count can get corrupted if it's updated by multiple threads at the same
    time. The reason for this limitation is to protect the internal interpreter data
    structures and third-party *C* code that is not thread safe.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: This GIL limitation does not exist in Jython and IronPython, which are other
    implementations of Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'This Python limitation may give us the impression that there is no advantage
    to writing multithreaded programs in Python. This is not true. We still can write
    code in Python that runs concurrently or in parallel, and we will see it in our
    case study. Multithreading can be beneficial in the following cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**I/O bound tasks**: When working with multiple I/O operations, there is always
    room to improve performance by running tasks using more than one thread. When
    one thread is waiting for a response from an I/O resource, it will release the
    GIL and let the other threads work. The original thread will wake up as soon as
    the response arrives from the I/O resource.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Responsive GUI application**: For interactive GUI applications, it is necessary
    to have a design pattern to display the progress of tasks running in the background
    (for example, downloading a file) and also to allow a user to work on other GUI
    features while one or more tasks are running in the background. This is all possible
    by using separate threads for the actions initiated by a user through the GUI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiuser applications**: Threads are also a prerequisite for building multiuser
    applications. A web server and a file server are examples of such applications.
    As soon as a new request arrives in the main thread of such an application, a
    new thread is created to serve the request while the main thread at the back listens
    for a new request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before discussing a case study of a multithreaded application, it is important
    to introduce the key components of multithreaded programming in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Learning the key components of multithreaded programming in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multithreading in Python allows us to run different components of a program
    concurrently. To create multiple threads of an application, we will use the Python
    `threading` module, and the main components of this module are described next.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by discussing the `threading` module in Python first.
  prefs: []
  type: TYPE_NORMAL
- en: The threading module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `threading` module comes as a standard module and provides simple and easy-to-use
    methods for building multiple threads of a program. Under the hood, this module
    uses the lower level `_thread` module, which was a popular choice of multithreading
    in the early version of Python.
  prefs: []
  type: TYPE_NORMAL
- en: To create a new thread, we will create an object of the `Thread` class that
    can take a function (to be executed) name as the `target` attribute and arguments
    to be passed to the function as the `args` attribute. A thread can be given a
    name that can be set at the time it is created using the `name` argument with
    the constructor.
  prefs: []
  type: TYPE_NORMAL
- en: After creating an object of the `Thread` class, we need to start the thread
    by using the `start` method. To make the main program or thread wait until the
    newly created thread object(s) finishes, we need to use the `join` method. The
    `join` method makes sure that the main thread (a calling thread) waits until the
    thread on which the `join` method is called completes its execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'To explain the process of creating, starting, and waiting to finish the execution
    of a thread, we will create a simple program with three threads. A complete code
    example of such a program is shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In this program, we implemented the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We created two simple functions, `print_hello` and `print_message`, that are
    to be used by the threads. We used the `sleep` function from the `time` module
    in both functions to make sure that the two functions finish their execution time
    at different times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We created three `Thread` objects. Two of the three objects will execute one
    function (`print_hello`) to illustrate the code sharing by the threads, and the
    third thread object will use the second function (`print_message`), which takes
    one argument as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We started all three threads one by one using the `start` method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We waited for each thread to finish by using the `join` method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `Thread` objects can be stored in a list to simplify the `start` and `join`
    operations using a `for` loop. The console output of this program will look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Thread 1 and thread 2 have more sleep time than thread 3, so thread 3 will always
    finish first. Thread 1 and thread 2 can finish in any order depending on who gets
    hold of the processor first.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: By default, the `join` method blocks the caller thread indefinitely. But we
    can use a timeout (in seconds) as an argument to the `join` method. This will
    make the caller thread block only for the timeout period.
  prefs: []
  type: TYPE_NORMAL
- en: We will review a few more concepts before discussing a more complex case study.
  prefs: []
  type: TYPE_NORMAL
- en: Daemon threads
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a normal application, our main program implicitly waits until all other threads
    finish their execution. However, sometimes we need to run some threads in the
    background so that they run without blocking the main program from terminating
    itself. These threads are known as **daemon threads**. These threads stay active
    as long as the main program (with non-daemon threads) is running, and it is fine
    to terminate the daemon threads once the non-daemon threads exit. The use of daemon
    threads is popular in situations where it is not an issue if a thread dies in
    the middle of its execution without losing or corrupting any data.
  prefs: []
  type: TYPE_NORMAL
- en: 'A thread can be declared a daemon thread by using one of the following two
    approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Pass the `daemon` attribute set to `True` with the constructor (`daemon = True`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set the `daemon` attribute to `True` on the thread instance (`thread.daemon
    = True`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a thread is set as a daemon thread, we start the thread and forget about
    it. The thread will be automatically killed when the program that called it quits.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next code shows the use of both daemon and non-daemon threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code example, we created one daemon and one non-daemon thread. The
    daemon thread (`daeom_func`) is executing a function that has a sleep time of
    `3` seconds, whereas the non-daemon thread is executing a function (`nondaeom_func`)
    that has a sleep time of 1 second. The sleep time of the two functions is set
    to make sure the non-daemon thread finishes its execution first. The console output
    of this program is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we did not use a `join` method in any thread, the main thread exits first,
    and then the non-daemon thread finishes a bit later with a print message. But
    there is no print message from the daemon thread. This is because the daemon thread
    is terminated as soon as the non-daemon thread finishes its execution. If we change
    the sleep time in the `nondaeom_func` function to `5`, the console output will
    be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: By delaying the execution of the non-daemon thread, we make sure the daemon
    thread finished its execution and does not get terminated abruptly.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If we use a `join` on the daemon thread, the main thread will be forced to wait
    for the daemon thread to finish its execution.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will investigate how to synchronize the threads in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronizing threads
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Thread synchronization** is a mechanism to ensure that the two or more threads
    do not execute a shared block of code at the same time. The block of code that
    is typically accessing shared data or shared resources is also known as the **critical
    section**. This concept can be made clearer through the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Two threads accessing a critical section of a program'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_07_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.2 – Two threads accessing a critical section of a program
  prefs: []
  type: TYPE_NORMAL
- en: Multiple threads accessing the critical section at the same time may try to
    access or change the data at the same time, which may result in unpredictable
    results on the data. This situation is called a **race condition**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the concept of the race condition, we will implement a simple
    program with two threads, and each thread increments a shared variable 1 million
    times. We chose a high number for the increment to make sure that we can observe
    the outcome of the race condition. The race condition may also be observed by
    using a lower value for the increment cycle on a slower CPU. In this program,
    we will create two threads that are using the same function (`inc` in this case)
    as the target. The code for accessing the shared variable and incrementing it
    by 1 occurs in the critical section, and the two threads are accessing it without
    any protection. The complete code example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The expected value of `x` at the end of the execution is *2,000,000*, which
    will not be observed in the console output. Every time we execute this program,
    we will get a different value of `x` that's a lot lower than 2,000,000\. This
    is because of the race condition between the two threads. Let's look at a scenario
    where threads `Th 1` and `Th 2` are running the critical section (`x+=1`) at the
    same time. Both threads will ask for the current value of `x`. If we assume the
    current value of `x` is `100`, both threads will read it as `100` and increment
    it to a new value of `101`. The two threads will write back to the memory the
    new value of `101`. This is a one-time increment and, in reality, the two threads
    should increment the variable independently of each other and the final value
    of `x` should be `102`. How can we achieve this? This is where thread synchronization
    comes to the rescue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thread synchronization can be achieved by using a `Lock` class from the `threading`
    module. The lock is implemented using a `Lock` class provides two methods, `acquire`
    and `release`, which are described next:'
  prefs: []
  type: TYPE_NORMAL
- en: The `acquire` method is used to acquire a lock. A lock can be `unlocked`), then
    the lock is provided to the requesting thread to proceed. In the case of a non-blocking
    acquire request, the thread execution is not blocked. If the lock is available
    (`unlocked`), then the lock is provided (and `locked`) to the requesting thread
    to proceed, otherwise the requesting thread gets `False` as a response.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `release` method is used to release a lock, which means it resets the lock
    to an `unlocked` state. If there is any thread blocking and waiting for the lock,
    it will allow one of the threads to proceed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `thread3a.py` code example is revised with the use of a lock around the
    increment statement on the shared variable `x`. In this revised example, we created
    a lock at the main thread level and then passed it to the `inc` function to acquire
    and release a lock around the shared variable. The complete revised code example
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: After using the `Lock` object, the value of `x` is always `2000000`. The `Lock`
    object made sure that only one thread increments the shared variable at a time.
    The advantage of thread synchronization is that you can use system resources with
    enhanced performance and predictable results.
  prefs: []
  type: TYPE_NORMAL
- en: However, locks have to be used carefully because improper use of locks can result
    in a deadlock situation. Suppose a thread acquires a lock on resource A and is
    waiting to acquire a lock on resource B. But another thread already holds a lock
    on resource B and is looking to acquire a lock resource A. The two threads will
    wait for each other to release the locks, but it will never happen. To avoid deadlock
    situations, the multithreading and multiprocessing libraries come with mechanisms
    such as adding a timeout for a resource to hold a lock, or using a context manager
    to acquire locks.
  prefs: []
  type: TYPE_NORMAL
- en: Using a synchronized queue
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `Queue` module in Python implements multi-producer and multi-consumer queues.
    Queues are very useful in multithread applications when the information has to
    be exchanged between different threads safely. The beauty of the synchronized
    queue is that they come with all the required locking mechanisms, and there is
    no need to use additional locking semantics.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three types of queues in the `Queue` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '**FIFO**: In the FIFO queue, the task added first is retrieved first.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LIFO**: In the LIFO queue, the last task added is retrieved first.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Priority queue**: In this queue, the entries are sorted and the entry with
    the lowest value is retrieved first.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These queues use locks to protect access to the queue entries from competing
    threads. The use of a queue with a multithreaded program is best illustrated with
    a code example. In the next example, we will create a FIFO queue with dummy tasks
    in it. To process the tasks from the queue, we will implement a custom thread
    class by inheriting the `Thread` class. This is another way of implementing a
    thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement a custom thread class, we need to override the `init` and `run`
    methods. In the `init` method, it is required to call the `init` method of the
    superclass (the `Thread` class). The `run` method is the execution part of the
    thread class. The complete code example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this code example, we created five worker threads using the custom thread
    class (`MyThread`). These five worker threads access the queue to get the task
    item from it. After getting the task item, the threads sleep for 1 second and
    then print the thread name and the task name. For each `get` call for an item
    of a queue, a subsequent call of `task_done()` indicates that the processing of
    the task has been completed.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that we used the `join` method on the `myqueue` object
    and not on the threads. The `join` method on the queue blocks the main thread
    until all items in the queue have been processed and completed (`task_done` is
    called for them). This is a recommended way to block the main thread when a queue
    object is used to hold the tasks' data for threads.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will implement an application to download files from Google Drive using
    the `Thread` class, the `Queue` class, and a couple of third-party libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Case study – a multithreaded application to download files from Google Drive
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have discussed in the previous section that multithreaded applications in
    Python stand out well when different threads are working on input and output tasks.
    That is why we selected to implement an application that downloads files from
    a shared directory of Google Drive. To implement this application, we will need
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Drive**: A Google Drive account (a free basic account is fine) with
    one directory marked as shared.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**API key**: An API key to access Google APIs is required. The API key needs
    to be enabled to use the Google APIs for Google Drive. The API can be enabled
    by following the guidelines on the Google Developers site ([https://developers.google.com/drive/api/v3/enable-drive-api](https://developers.google.com/drive/api/v3/enable-drive-api)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pip` tool.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pip` tool as well. There are other libraries available that offer the same
    functionality. We selected the `gdown` library for its ease of use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To use the `getfilelistpy` module, we need to create a resource data structure.
    This data structure will include a folder identifier as `id` (this will be Google
    Drive folder ID in our case), the API security key (`api_key`) for accessing the
    Google Drive folder, and a list of file attributes (`fields`) to be fetched when
    we get a list of files. We build the resource data structure as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We limit the file attributes to the `file id`, `name`, and its `web link` (URL)
    only. Next, we need to add each file item into a queue as a task for threads.
    The queue will be used by multiple worker threads to download the files in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the application more flexible in terms of the number of workers we
    can use, we build a pool of worker threads. The size of the pool is controlled
    by a global variable that is set at the beginning of the program. We created worker
    threads as per the size of the thread pool. Each worker thread in the pool has
    access to the queue, which has a list of files. Like the previous code example,
    each worker thread will take one file item from the queue at a time, download
    the file, and mark the file item as complete using the `task_done` method. An
    example code for defining a resource data structure and for defining a class for
    the worker thread is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the files'' metadata from a Google Drive directory using the resource
    data structure as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `main` function, we create a `Queue` object to insert file metadata
    into the queue. The `Queue` object is handed over to a pool of worker threads
    for downloading the files. The worker threads will download the files, as discussed
    earlier. We use the `time` class to measure the time it takes to complete the
    download of all the files from the Google Drive directory. The code for the `main`
    function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: For this application, we have 10 files in the Google Drive directory, varying
    in size from 500 KB to 3 MB. We ran the application with 1, 5, and 10 worker threads.
    The total time taken to download the 10 files with 1 thread was approximately
    20 seconds. This is almost equivalent to writing a code without any threads. In
    fact, we have written a code to download the same files without any threads and
    made it available with this book's source code as an example. The time it took
    to download 10 files with a non-threaded application was approximately 19 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: When we changed the number of worker threads to 5, the time taken to download
    the 10 files reduced significantly to approximately 6 seconds on our MacBook machine
    (Intel Core i5 with 16 GB RAM). If you run the same program on your computer,
    the time may be different, but there will definitely be an improvement if we increase
    the number of worker threads. With 10 threads, we observed the execution time
    to be around 4 seconds. This observation shows that there is an improvement in
    the execution time for I/O bound tasks by using multithreading regardless of the
    GIL limitation it has.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion of how to implement threads in Python and how
    to benefit from different locking mechanisms using the `Lock` class and the `Queue`
    class. Next, we will discuss multiprocessing programming in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Going beyond a single CPU – implementing multiprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen the complexity of multithreaded programming and its limitations.
    The question is whether the complexity of multithreading is worth the effort.
    It may be worth it for I/O-related tasks but not for general application use cases,
    especially when an alternative approach exists. The alternative approach is to
    use multiprocessing because separate Python processes are not constrained by the
    GIL and execution can happen in parallel. This is especially beneficial when applications
    run on multicore processors and involve intensive CPU-demanding tasks. In reality,
    the use of multiprocessing is the only option in Python's built-in libraries to
    utilize multiple processor cores.
  prefs: []
  type: TYPE_NORMAL
- en: '**Graphics Processing Units** (**GPUs**) provide a greater number of cores
    than regular CPUs and are considered more suitable for data processing tasks,
    especially when executing them in parallel. The only caveat is that in order to
    execute a data processing program on a GPU, we have to transfer the data from
    the main memory to the GPU''s memory. This additional step of data transfer will
    be compensated when we are processing a large dataset. But there will be little
    or no benefit if our dataset is small. Using GPUs for big data processing, especially
    for training machine learning models, is becoming a popular option. NVIDIA has
    introduced a GPU for parallel processing called CUDA, which is well supported
    through external libraries in Python.'
  prefs: []
  type: TYPE_NORMAL
- en: Each process has a data structure called the **Process Control Block** (**PCB**)
    at the operating system level. Like the TCB, the PCB has a **Process ID** (**PID**)
    for process identification, stores the state of the process (such as running or
    waiting), and has a program counter, CPU registers, CPU scheduling information,
    and many more attributes.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of multiple processes for CPUs, there is no sharing of memory natively.
    This means there is a lower chance of data corruption. If the two processes have
    to share the data, they need to use some interprocess communication mechanism.
    Python supports interprocess communication through its primitives. In the next
    subsections, we will first discuss the fundamentals of creating processes in Python
    and then discuss how to achieve interprocess communication.
  prefs: []
  type: TYPE_NORMAL
- en: Creating multiple processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For multiprocessing programming, Python provides a `multiprocessing` package
    that is very similar to the multithreading package. The `multiprocessing` package
    includes two approaches to implement multiprocessing, which are using the `Process`
    object and the `Pool` object. We will discuss each of these approaches one by
    one.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Process object
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The processes can be spawned by creating a `Process` object and then using
    its `start` method similar to the `start` method for starting a `Thread` object.
    In fact, the `Process` object offers the same API as the `Thread` object. A simple
    code example for creating multiple child processes is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As already mentioned, the methods used for the `Process` object are pretty much
    the same as those used for the `Thread` object. The explanation of this example
    is the same as for the example code in the multithreading code examples.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Pool object
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `Pool` object offers a convenient way (using its `map` method) of creating
    processes, assigning functions to each new process, and distributing input parameters
    across the processes. We selected the code example with a pool size of `3` but
    provided input parameters for five processes. The reason for setting the pool
    size to `3` is to make sure a maximum of three child processes are active at a
    time, regardless of how many parameters we pass with the `map` method of the `Pool`
    object. The additional parameters will be handed over to the same child processes
    as soon they finish their current execution. Here is a code example with a pool
    size of `3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The magic of distributing input parameters to a function that is tied to a set
    of pool processes is done by the `map` method. The `map` method waits until all
    functions complete their execution, and that is why there is no need to use a
    `join` method if the processes are created using the `Pool` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few differences between using the `Process` object versus the `Pool` object
    are shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 7.1 – Comparison of using the Pool object and the Process object'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_07_Table_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.1 – Comparison of using the Pool object and the Process object
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss how to exchange data between processes.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing data between processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two approaches in the multiprocessing package to share data between
    processes. These are **shared memory** and **server process**. They are described
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Using shared ctype objects (shared memory)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this case, a *shared memory* block is created, and the processes have access
    to this shared memory block. The shared memory is created as soon we initiate
    one of the `ctype` datatypes available in the `multiprocessing` package. The datatypes
    are `Array` and `Value`. The `Array` datatype is a `ctype` array and the `Value`
    datatype is a generic `ctype` object, both of which are allocated from the shared
    memory. To create a `ctype` array, we will use a statement like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create an array of the `integer` datatype with a size of `5`. `i`
    is one of the typecodes, and it stands for integer. We can use the `d` typecode
    for float datatypes. We can also initialize the array by providing the sequence
    as a second argument (instead of the size) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a `Value` `ctype` object, we will use a statement similar to the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This will create an object of the `integer` datatype because the typecode is
    set to `i`. The value of this object can be accessed or set by using the `value`
    attribute.
  prefs: []
  type: TYPE_NORMAL
- en: Both these `ctype` objects have `Lock` as an optional argument, which is set
    to `True` by default. This argument when set to `True` is used to create a new
    recursive lock object that provides synchronized access to the value of the objects.
    If it is set to `False`, there will be no protection and it will not be a safe
    process. If your process is only accessing the shared memory for reading purposes,
    it is fine to set the `Lock` to `False`. We leave this `Lock` argument as the
    default (`True`) in our next code examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the use of these `ctype` objects from the shared memory, we will
    create a default list with three numeric values, a `ctype` array of size `3` to
    hold the incremented values of the original array, and a `ctype` object to hold
    the sum of the incremented array. These objects will be created by a parent process
    in shared memory and will be accessed and updated by a child process from the
    shared memory. This interaction of the parent and the child processes with the
    shared memory is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Use of shared memory by a parent and a child process'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_07_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.3 – Use of shared memory by a parent and a child process
  prefs: []
  type: TYPE_NORMAL
- en: 'A complete code example of using the shared memory is shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The shared datatypes (`inc_list` and `sum` in this case) are accessed by both
    the parent process and the child process. It is important to mention that using
    the shared memory is not a recommended option because it requires synchronization
    and locking mechanisms (similar to what we did for multithreading) when the same
    shared memory objects are accessed by multiple processes and the `Lock` argument
    is set to `False`.
  prefs: []
  type: TYPE_NORMAL
- en: The next approach of sharing data between processes is using the server process.
  prefs: []
  type: TYPE_NORMAL
- en: Using the server process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this case, a server process is started as soon as a Python program starts.
    This new process is used to create and manage the new child processes requested
    by a parent process. This server process can hold Python objects that other processes
    can access using proxies.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement the server process and share the objects between the processes,
    the `multiprocessing` package provides a `Manager` object. The `Manager` object
    supports different data types such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Lists
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dictionaries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Locks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rlocks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Queues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arrays
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code example we selected for illustrating the server process creates a
    `dictionary` object using the `Manager` object, then passes the dictionary object
    to different child processes to insert more data and to print out the dictionary
    contents. We will create three child processes for our example: two for inserting
    data into the dictionary object and one for getting the dictionary contents as
    the console output. The interaction between the parent process, the server process,
    and the three child processes is shown in *Figure 7.4*. The parent process creates
    the server process as soon as a new process request is executed using the *Manager*
    context. The child processes are created and managed by the server process. The
    shared data is available within the server process and is accessible by all processes,
    including the parent process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Use of server process for sharing data between processes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_07_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.4 – Use of server process for sharing data between processes
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete code example is shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The server process approach offers more flexibility than the shared memory approach
    because it supports a large variety of object types. However, this comes at the
    cost of slower performance compared to the shared memory approach.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore the options of direct communication between
    the processes.
  prefs: []
  type: TYPE_NORMAL
- en: Exchanging objects between processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we studied how to share data between the processes
    through an external memory block or a new process. In this section, we will investigate
    exchanging of data between processes using Python objects. The `multiprocessing`
    module provides two options for this purpose. These are using the `Queue` object
    and the `Pipe` object.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Queue object
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `Queue` object is available from the `multiprocessing` package and is nearly
    the same as the synchronized queue object (`queue.Queue`) that we used for multithreading.
    This `Queue` object is process-safe and does not require any additional protection.
    A code example to illustrate the use of the multiprocessing `Queue` object for
    data exchange is shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In this code example, we created a standard `list` object and a multiprocessing
    `Queue` object. The `list` and `Queue` objects are passed to a new process, which
    is attached to a function called `copy_data`. This function will copy the data
    from the `list` object to the `Queue` object. A new process is initiated to print
    the contents of the `Queue` object. Note that the data in the `Queue` object is
    set by the previous process and the data will be available to the new process.
    This is a convenient way to exchange data without adding the complexity of shared
    memory or the server process.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Pipe object
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `Pipe` object is like a pipe between two processes for exchanging data.
    This is why this object is especially useful when two-way communication is required.
    When we create a `Pipe` object, it provides two connection objects, which are
    the two ends of the `Pipe` object. Each connection object provides a `send` and
    a `recv` method to send and receive data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the concept and use of the `Pipe` object, we will create two
    functions that will be attached to two separate processes:'
  prefs: []
  type: TYPE_NORMAL
- en: The first function is for sending the message through a `Pipe` object connection.
    We will send a few data messages and finish the communication with a `BYE` message.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second function is to receive the message using the other connection object
    of the `Pipe` object. This function will run in an infinite loop until it receives
    a `BYE` message.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The two functions (or processes) are provided with the two connection objects
    of a pipe. The complete code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'It is important to mention that the data in a `Pipe` object can easily be corrupted
    if the two processes try to read from or write to it using the same connection
    object at the same time. That is why multiprocessing queues are the preferred
    option: because they provide proper synchronization between the processes.'
  prefs: []
  type: TYPE_NORMAL
- en: Synchronization between processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Synchronization between processes makes sure that two or more processes do not
    access the same resources or program code at the same time, which is also called
    the `Lock` object, similar to what we used in the case of multithreading.
  prefs: []
  type: TYPE_NORMAL
- en: 'We illustrated the use of `queues` and `ctype` datatypes with `Lock` set to
    `True`, which is process safe. In the next code example, we will illustrate the
    use of the `Lock` object to make sure one process gets access to the console output
    at a time. We created the processes using the `Pool` object and to pass the same
    `Lock` object to all processes, we used the `Lock` from the `Manager` object and
    not the one from the multiprocessing package. We also used the `partial` function
    to tie the `Lock` object to each process, along with a list to be distributed
    to each process function. Here is the complete code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: If we do not use the `Lock` object, the output from the different processes
    can be mixed up.
  prefs: []
  type: TYPE_NORMAL
- en: Case study – a multiprocessor application to download files from Google Drive
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will implement the same case study as we did in the *Case
    study – a multithreaded application to download files from Google Drive* section,
    but using processors instead. The prerequisites and goals are the same as described
    for the case study of the multithreaded application.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this application, we used the same code that we built for the multithreaded
    application except that we used processes instead of threads. Another difference
    is that we used the `JoinableQueue` object from the `multiprocessing` module to
    achieve the same functionality as we were getting from the regular `Queue` object.
    Code for defining a resource data structure and for a function to download files
    from Google Drive is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the files'' metadata, such as the name and HTTP link, from a Google
    Drive directory using the resource data structure as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In our `main` function, we create a `JoinableQueue` object and insert the files''
    metadata into the queue. The queue will be handed over to a pool of processes
    to download the files. The processes will download the files. We used the `time`
    class to measure the time it takes to download all the files from the Google Drive
    directory. The code for the `main` function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We ran this application by varying the different number of processes, such
    as `3`, `5`, `7`, and `10`. We found that the time it took to download the same
    files (as for the case study of multithreading) is slightly better than with the
    multithreaded application. The execution time will vary from machine to machine,
    but on our machine (MacBook Pro: Intel Core i5 with 16 GB RAM), it took around
    5 seconds with 5 processes and 3 seconds with 10 processes running in parallel.
    This improvement of 1 second over the multithreaded application is in line with
    the expected results as multiprocessing provides true concurrency.'
  prefs: []
  type: TYPE_NORMAL
- en: Using asynchronous programming for responsive systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With multiprocessing and multithreaded programming, we were mostly dealing
    with synchronous programming, where we request something and wait for the response
    to be received before we move to the next block of code. If any context switching
    is applied, it is provided by the operating system. Asynchronous programming in
    Python is different mainly in the following two aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: The tasks are to be created for asynchronous execution. This means the parent
    caller does not have to wait for the response from another process. The process
    will respond to the caller once it finishes the execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The operating system is no longer managing the context switching between the
    processes and the threads. The asynchronous program will be given only a single
    thread in a process, but we can do multiple things with it. In this style of execution,
    every process or task voluntarily releases control whenever it is idle or waiting
    for another resource to make sure that the other tasks get a turn. This concept
    is called **cooperative multitasking**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cooperative multitasking is an effective tool for achieving concurrency at the
    application level. In cooperative multitasking, we do not build processes or threads,
    but tasks, which comprises `yield`) while keeping the stack of objects under control
    before it is resumed.
  prefs: []
  type: TYPE_NORMAL
- en: For a system based on cooperative multitasking, there is always a question of
    when to release the control back to a scheduler or to an event loop. The most
    commonly used logic is to use the I/O operation as the event to release the control
    because there is always a waiting time involved whenever we are doing an I/O operation.
  prefs: []
  type: TYPE_NORMAL
- en: But hold on, is it not the same logic we used for multithreading? We found that
    multithreading improves application performance when dealing with I/O operations.
    But there is a difference here. In the case of multithreading, the operating system
    is managing the context switching between the threads, and it can preempt any
    running thread for any reason and give control to another thread. But in asynchronous
    programming or cooperative multitasking, the tasks or coroutines are not visible
    to the operating systems and cannot be preempted. The coroutines in fact cannot
    be preempted by the main event loop. But this does not mean that the operating
    system cannot preempt the whole Python process. The main Python process is still
    competing for resources with other applications and processes at the operating
    system level.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss a few building blocks of asynchronous programming
    in Python, which is provided by the `asyncio` module, and we will conclude with
    a comprehensive case study.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the asyncio module
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `asyncio` module is available in Python 3.5 or later to write concurrent
    programs using the `async/await` syntax. But it is recommended to use Python 3.7
    or later to build any serious `asyncio` application. The library is rich with
    features and supports creating and running Python coroutines, performing network
    I/O operations, distributing tasks to queues, and synchronizing concurrent code.
  prefs: []
  type: TYPE_NORMAL
- en: We will start with how to write and execute coroutines and tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Coroutines and tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Coroutines are the functions that are to be executed asynchronously. A simple
    example of sending a string to the console output using a coroutine is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code example, it is important to note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The coroutine takes `delay` and `msg` arguments. The `delay` argument is used
    to add a delay before sending the `msg` string to the console output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We used the `asyncio.sleep` function instead of the traditional `time.sleep`
    function. If the `time.sleep` function is used, control will not be given back
    to the event loop. That is why it is important to use the compatible `asyncio.sleep`
    function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The coroutine is executed twice with two different values of the `delay` argument
    by using the `run` method. The `run` method will not execute the coroutines concurrently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The console output of this program will be as follows. This shows that the
    coroutines are executed one after the other as the total delay added is 3 seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: To run the coroutines in parallel, we need to use the `create_task` function
    from the `asyncio` module. This function creates a task that can be used to schedule
    coroutines to run concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next code example is a revised version of `asyncio1.py`, in which we wrapped
    the coroutine (`say` in our case) into a task using the `create_task` function.
    In this revised version, we created two tasks that are wrapping the `say` coroutine.
    We waited for the two tasks to be completed using the `await` keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The console output of this program is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This console output shows that the two tasks were completed in 1 second, which
    is proof that the tasks are executed in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Using awaitable objects
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An object is awaitable if we can apply the `await` statement to it. The majority
    of `asyncio` functions and modules inside it are designed to work with awaitable
    objects. But most Python objects and third-party libraries are not built for asynchronous
    programming. It is important to select compatible libraries that provide awaitable
    objects to use when building asynchronous applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Awaitable objects are split mainly into three types: coroutines, tasks, and
    `Future` is a low-level object that is like a callback mechanism used to process
    the result coming from the `async`/`await`. The `Future` objects are typically
    not exposed for user-level programming.'
  prefs: []
  type: TYPE_NORMAL
- en: Running tasks concurrently
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we have to run multiple tasks in parallel, we can use the `await` keyword
    as we did in the previous example. But there is a better way of doing this by
    using the `gather` function. This function will run the awaitable objects in the
    sequence provided. If any of the awaitable objects is a coroutine, it will be
    scheduled as a task. We will see the use of the `gather` function in the next
    section with a code example.
  prefs: []
  type: TYPE_NORMAL
- en: Distributing tasks using queues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `Queue` object in the `asyncio` package is similar to the `Queue` module
    but it is not thread safe. The `aysncio` module provides a variety of queue implementations,
    such as FIFO queues, priority queues, and LIFO queues. The queues in the `asyncio`
    module can be used to distribute the workloads to the tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the use of a queue with tasks, we will write a small program
    that will simulate the execution time of a real function by sleeping for a random
    amount of time. The random sleeping time is calculated for 10 such executions
    and added to a `Queue` object as working items by the main process. The `Queue`
    object is passed to a pool of three tasks. Each task in the pool executes the
    assigned coroutine, which consumes the execution time as per the queue entry available
    to it. The complete code is shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We used the `put_no_wait` function of the `Queue` object because it is a non-blocking
    operation. The console output of this program is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This clearly shows that the tasks are executed in parallel, and the execution
    is three times better than if tasks are run sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have covered the fundamental concepts of the `asyncio` package in
    Python. Before concluding this topic, we will revisit the case study we did for
    the multithreading section by implementing it using the `asyncio` tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Case study – asyncio application to download files from Google Drive
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will implement the same case study as we did in the *Case study – a multithreaded
    application to download files from Google Drive* section, but using the `asyncio`
    module with `async`, `await`, and `async queue`. The prerequisites for this case
    study are the same except that we use the `aiohttp` and `aiofiles` library instead
    of the `gdown` library. The reason is simple: the `gdown` library is not built
    as an async module. There is no benefit of using it with async programming. This
    is an important point to consider whenever selecting libraries to be used with
    async applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this application, we built a coroutine, `mydownloader`, to download a file
    from Google Drive using the `aiohttp` and `aiofiles` modules. This is shown in
    the following code, and the code that is different from the previous case studies
    is highlighted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The process to get the list of files from a shared Google Drive folder is the
    same as we used in the previous case study for multithreading and multiprocessing.
    In this case study, we created a pool of tasks (configurable) based on the `mydownloader`
    coroutine. These tasks are then scheduled to run together, and our parent process
    waits for all tasks to complete their execution. A code to get a list of files
    from Google Drive and then download the files using `asyncio` tasks is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We ran this application by varying the number of tasks, such as 3, 5, 7, and
    10\. We found that the time it took to download the files with the `asyncio` tasks
    is lower than when we downloaded the same files using the multithreading approach
    or the multiprocessing approach. The exact details of the time taken with the
    multithreading approach and the multiprocessing approach are available in the
    *Case study – a multithreaded application to download files from Google Drive*
    and *Case study – a multiprocessor application to download files from Google Drive*
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'The execution time can vary from machine to machine, but on our machine (MacBook
    Pro: Intel Core i5 with 16 GB RAM), it took around 4 seconds with 5 tasks and
    2 seconds with 10 tasks running in parallel. This is a significant improvement
    compared to the numbers we observed for the multithreading and multiprocessing
    case studies. This is in line with expected results, as `asyncio` provides a better
    concurrency framework when it comes to I/O-related tasks, but it has to be implemented
    using the right set of programming objects.'
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion of asynchronous programming. This section provided
    all the core ingredients to build an asynchronous application using the `asyncio`
    package.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed different options of concurrent programming in
    Python using the standard libraries. We started with multithreading with an introduction
    to the core concepts of concurrent programming. We introduced the challenges with
    multithreading, such as the GIL, which allows only one thread at a time to access
    Python objects. The concepts of locking and synchronization were explored with
    practical examples of Python code. We also discussed the types of task that multithreaded
    programming is more effective for using a case study.
  prefs: []
  type: TYPE_NORMAL
- en: We studied how to achieve concurrency using multiple processes in Python. With
    multiprocessing programming, we learned how to share data between processes using
    shared memory and the server process, and also how to exchange objects safely
    between processes using the `Queue` object and the `Pipe` object. In the end,
    we built the same case study as we did for the multithreading example, but using
    processes instead. Then, we introduced a completely different approach to achieving
    concurrency by using asynchronous programming. This was a complete shift in concept,
    and we started it by looking at the high-level concepts of the `async` and `await`
    keywords and how to build tasks, or coroutines, using the `asyncio` package. We
    concluded the chapter with the same case study we examined for multiprocessing
    and multithreading but using asynchronous programming.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter has provided a lot of hands-on examples of how to implement concurrent
    applications in Python. This knowledge is important for anyone who wants to build
    multithreaded or asynchronous applications using the standard libraries available
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore using third-party libraries to build concurrent
    applications in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What coordinates Python threads? Is it a Python interpreter?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the GIL in Python?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When should you use daemon threads?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a system with limited memory, should we use a `Process` object or `Pool`
    object to create processes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are Futures in the `asyncio` package?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is an event loop in asynchronous programming?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you write an asynchronous coroutine or function in Python?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Learning Concurrency in Python* by Elliot Forbes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Expert Python Programming* by Michal Jaworski and Tarek Ziade'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Python 3 Object-Oriented Programming,* *Second Edition* by Dusty Phillips'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mastering Concurrency* *in Python* by Quan Nguyen'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Python Concurrency with asyncio* by Mathew Fowler'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The threads and processes are coordinated by the operating system kernel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Python's GIL is a locking mechanism used by Python to allow only one thread
    to execute at a time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Daemon threads are used when it is not an issue for a thread to be terminated
    once its main thread terminates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `Pool` object keeps only the active processes in memory, so it is a better
    choice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Futures are like a callback mechanism that is used to process the result coming
    from async/await calls.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An event loop object keeps track of tasks and handles the flow of control between
    them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can write an asynchronous coroutine by starting with `async def`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
