- en: Chapter 1. Profiling 101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just like any infant needs to learn how to crawl before running 100 mts with
    obstacles in under 12 seconds, programmers need to understand the basics of profiling
    before trying to master that art. So, before we start delving into the mysteries
    of performance optimization and profiling on Python programs, we need to have
    a clear understanding of the basics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you know the basics, you''ll be able to learn about the tools and techniques.
    So, to start us off, this chapter will cover everything you need to know about
    profiling but were too afraid to ask. In this chapter we will do the following
    things:'
  prefs: []
  type: TYPE_NORMAL
- en: We will provide a clear definition of what profiling is and the different profiling
    techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will explain the importance of profiling in the development cycle, because
    profiling is not something you do only once and then forget about it. Profiling
    should be an integral part of the development process, just like writing tests
    is.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will cover things we can profile. We'll go over the different types of resources
    we'll be able to measure and how they'll help us find our problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will discuss the risk of premature optimization, that is, why optimizing
    before profiling is generally a bad idea.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will learn about running time complexity. Understanding profiling techniques
    is one step into successful optimization, but we also need to understand how to
    measure the complexity of an algorithm in order to understand whether we need
    to improve it or not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also look at good practices. Finally, we'll go over some good practices
    to keep in mind when starting the profiling process of your project.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is profiling?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A program that hasn't been optimized will normally spend most of its CPU cycles
    in some particular subroutines. Profiling is the analysis of how the code behaves
    in relation to the resources it's using. For instance, profiling will tell you
    how much CPU time an instruction is using or how much memory the full program
    is consuming. It is achieved by modifying either the source code of the program
    or the binary executable form (when possible) to use something called as a profiler.
  prefs: []
  type: TYPE_NORMAL
- en: Normally, developers profile their programs when they need to either optimize
    their performance or when those programs are suffering from some kind of weird
    bug, which can normally be associated with memory leaks. In such cases, profiling
    can help them get an in-depth understanding of how their code is using the computer's
    resources (that is, how many times a certain function is being called).
  prefs: []
  type: TYPE_NORMAL
- en: A developer can use this information, along with a working knowledge of the
    source code, to find the program's bottlenecks and memory leaks. The developer
    can then fix whatever is wrong with the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main methodologies for profiling software: event-based profiling
    and statistical profiling. When using these types of software, you should keep
    in mind that they both have pros and cons.'
  prefs: []
  type: TYPE_NORMAL
- en: Event-based profiling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Not every programming language supports this type of profiling. Here are some
    programming languages that support event-based profiling:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Java**: The **JVMTI** (**JVM Tools Interface**) provides hooks for profilers
    to trap events such as calls, thread-related events, class loads and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**.NET**: Just like with Java, the runtime provides events ([http://en.wikibooks.org/wiki/Introduction_to_Software_Engineering/Testing/Profiling#Methods_of_data_gathering](http://en.wikibooks.org/wiki/Introduction_to_Software_Engineering/Testing/Profiling#Methods_of_data_gathering))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Python**: Using the `sys.setprofile` function, a developer can trap events
    such as `python_[call|return|exception]` or `c_[call|return|exception]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event-based profilers** (also known as **tracing profilers**) work by gathering
    data on specific events during the execution of our program. These profilers generate
    a large amount of data. Basically, the more events they listen to, the more data
    they will gather. This makes them somewhat impractical to use, and they are not
    the first choice when starting to profile a program. However, they are a good
    last resort when other profiling methods aren''t enough or just aren''t specific
    enough. Consider the case where you''d want to profile all the return statements.
    This type of profiler would give you the granularity you''d need for this task,
    while others would simply not allow you to execute this task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple example of an event-based profiler on Python could be the following
    code (we''ll understand this topic better once we reach the upcoming chapters):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code contributes to the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, `PROFILER` is called on every event. We can print/gather the
    information we deem relevant inside the `PROFILER` function. The last line on
    the sample code shows that the simple execution of `fib_seq(2)` generates a lot
    of output data. If we were dealing with a real-world program, this output would
    be several orders of magnitude bigger. This is why event-based profiling is normally
    the last option when it comes to profiling. There are other alternatives out there
    (as we'll see) that generate much less output, but, of course, have a lower accuracy
    rate.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical profiling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Statistical profilers work by sampling the program counter at regular intervals.
    This in turn allows the developer to get an idea of how much time the target program
    is spending on each function. Since it works by sampling the PC, the resulting
    numbers will be a statistical approximation of reality instead of exact numbers.
    Still, it should be enough to get a glimpse of what the profiled program is doing
    and where the bottlenecks are.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some advantages of this type of profiling are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Less data to analyze**: Since we''re only sampling the program''s execution
    instead of saving every little piece of data, the amount of information to analyze
    will be significantly smaller.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Smaller profiling footprint**: Due to the way the sampling is made (using
    OS interrupts), the target program suffers a smaller hit on its performance. Although
    the presence of the profiler is not 100 percent unnoticed, statistical profiling
    does less damage than the event-based one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example of the output of **OProfile** ([http://oprofile.sourceforge.net/news/](http://oprofile.sourceforge.net/news/)),
    a Linux statistical profiler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of profiling the same Fibonacci code from the preceding
    code using a statistical profiler for Python called statprof:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there is quite a difference between the output of both profilers
    for the same code.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of profiling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know what profiling means, it is also important to understand how
    important and relevant it is to actually do it during the development cycle of
    our applications.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling is not something everyone is used to do, especially with non-critical
    software (unlike peace maker embedded software or any other type of execution-critical
    example). Profiling takes time and is normally useful only after we've detected
    that something is wrong with our program. However, it could still be performed
    before that even happens to catch possible unseen bugs, which would, in turn,
    help chip away the time spent debugging the application at a later stage.
  prefs: []
  type: TYPE_NORMAL
- en: As hardware keeps advancing, getting faster and cheaper, it is increasingly
    hard to understand why we, as developers, should spend resources (mainly time)
    on profiling our creations. After all, we have practices such as test-driven development,
    code review, pair programming and others that assure us our code is solid and
    that it'll work as we want it. Right?
  prefs: []
  type: TYPE_NORMAL
- en: However, what we sometimes fail to realize is that the higher level our languages
    become (we've gone from assembler to JavaScript in just a few years), the less
    we think about CPU cycles, memory allocation, CPU registries, and so on. New generations
    of programmers learn their craft using higher level languages because they're
    easier to understand and provide more power out of the box. However, they also
    abstract the hardware and our interaction with it. As this tendency keeps growing,
    the chances that new developers will even consider profiling their software as
    another step on its development grows weaker by the second.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the following scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: As we know, profiling measures the resources our program uses. As I've stated
    earlier, they keep getting cheaper and cheaper. So, the cost of getting our software
    out and the cost of making it available to a higher number of users is also getting
    cheaper.
  prefs: []
  type: TYPE_NORMAL
- en: These days, it is increasingly easy to create and publish an application that
    will be reached by thousands of people. If they like it and spread the word through
    social media, that number can blow up exponentially. Once that happens, something
    that is very common is that the software will crash, or it'll become impossibly
    slow and the users will just go away.
  prefs: []
  type: TYPE_NORMAL
- en: A possible explanation for the preceding scenario is, of course, a badly thought
    and non-scalable architecture. After all, one single server with a limited amount
    of RAM and processing power will get you so far until it becomes your bottleneck.
    However, another possible explanation, one that proves to be true many times,
    is that we failed to stress test our application. We didn't think about resource
    consumption; we just made sure our tests passed, and we were happy with that.
    In other words, we failed to go that extra mile, and as a result, our project
    crashed and burned.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling can help avoid that crash and burn outcome, since it provides a fairly
    accurate view of what our program is doing, no matter the load. So, if we profile
    it with a very light load, and the result is that we're spending 80 percent of
    our time doing some kind of I/O operation, it might raise a flag for us. Even
    if, during our test, the application performed correctly, it might not do so under
    heavy stress. Think of a memory leak-type scenario. In those cases, small tests
    might not generate a big enough problem for us to detect it. However, a production
    deployment under heavy stress will. Profiling can provide enough evidence for
    us to detect this problem before it even turns into one.
  prefs: []
  type: TYPE_NORMAL
- en: What can we profile?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Going deeper into profiling, it is very important to understand what we can
    actually profile. Measuring is the core of profiling, so let's take a detailed
    look at the things we can measure during a program's execution.
  prefs: []
  type: TYPE_NORMAL
- en: Execution time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most basic of the numbers we can gather when profiling is the execution
    time. The execution time of the entire process or just of a particular portion
    of the code will shed some light on its own. If you have experience in the area
    your program is running (that is, you''re a web developer and you''re working
    on a web framework), you probably already know what it means for your system to
    take too much time. For instance, a simple web server might take up to 100 milliseconds
    when querying the database, rendering the response, and sending it back to the
    client. However, if the same piece of code starts to slow down and now it takes
    60 seconds to do the same task, then you should start thinking about profiling.
    You also have to consider that numbers here are relative. Let''s assume another
    process: a MapReduce job that is meant to process 2 TB of information stored on
    a set of text files takes 20 minutes. In this case, you might not consider it
    as a slow process, even when it takes considerably more time than the slow web
    server mentioned earlier.'
  prefs: []
  type: TYPE_NORMAL
- en: To get this type of information, you don't really need a lot of profiling experience
    or even complex tools to get the numbers. Just add the required lines into your
    code and run the program.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, the following code will calculate the Fibonnacci sequence for
    the number 30:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the code will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the last three lines, we see the obvious results: the most expensive
    part of the code is the actual calculation of the Fibonacci sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Downloading the example code**'
  prefs: []
  type: TYPE_NORMAL
- en: You can download the example code files from your account at [http://www.packtpub.com](http://www.packtpub.com)
    for all the Packt Publishing books you have purchased. If you purchased this book
    elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  prefs: []
  type: TYPE_NORMAL
- en: Where are the bottlenecks?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you''ve measured how much time your code needs to execute, you can profile
    it by paying special attention to the slow sections. These are the bottlenecks,
    and normally, they are related to one or a combination of the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Heavy I/O operations, such as reading and parsing big files, executing long-running
    database queries, calling external services (such as HTTP requests), and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unexpected memory leaks that start building up until there is no memory left
    for the rest of the program to execute properly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unoptimized code that gets executed frequently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intensive operations that are not cached when they could be
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I/O-bound code (file reads/write, database queries, and so on) is usually harder
    to optimize, because that would imply changing the way the program is dealing
    with that I/O (normally using core functions from the language). Instead, when
    optimizing compute-bound code (like a function that is using a badly implemented
    algorithm), getting a performance improvement is easier (although not necessarily
    easy). This is because it just implies rewriting it.
  prefs: []
  type: TYPE_NORMAL
- en: A general indicator that you're near the end of a performance optimization process
    is when most of the bottlenecks left are due to I/O-bound code.
  prefs: []
  type: TYPE_NORMAL
- en: Memory consumption and memory leaks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another very important resource to consider when developing software is memory.
    Regular software developers don't really care much about it, since the era of
    the 640 KB of RAM PC is long dead. However, a memory leak on a long-running program
    can turn any server into a 640 KB computer. Memory consumption is not just about
    having enough memory for your program to run; it's also about having control over
    the memory that your programs use.
  prefs: []
  type: TYPE_NORMAL
- en: There are some developments, such as embedded systems, that actually require
    developers to pay extra attention to the amount of memory they use, because it
    is a limited resource in those systems. However, an average developer can expect
    their target system to have the amount of RAM they require.
  prefs: []
  type: TYPE_NORMAL
- en: With RAM and higher level languages that come with automatic memory management
    (like garbage collection), the developer is less likely to pay much attention
    to memory utilization, trusting the platform to do it for them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keeping track of memory consumption is relatively straightforward. At least
    for a basic approach, just use your OS''s task manager. It''ll display, among
    other things, the amount of memory used or at least the percentage of total memory
    used by your program. The task manager is also a great tool to check your CPU
    time consumption. As you can see in the next screenshot, a simple Python program
    (the preceding one) is taking up almost the entire CPU power (99.8 percent), and
    barely 0.1 percent of the total memory that is available:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Memory consumption and memory leaks](img/B02088_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: With a tool like that (the `top` command line tool from Linux), spotting memory
    leaks can be easy, but that will depend on the type of software you're monitoring.
    If your program is constantly loading data, its memory consumption rate will be
    different from another program that doesn't have to deal much with external resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if we were to chart the memory consumption over time of a program
    dealing with lots of external data, it would look like the following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Memory consumption and memory leaks](img/B02088_01_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There will be peaks, when these resources get fully loaded into memory, but
    there will also be some drops, when those resources are released. Although the
    memory consumption numbers fluctuate quite a bit, it's still possible to estimate
    the average amount of memory that the program will use when no resources are loaded.
    Once you define that area (marked as a green box in the preceding chart), you
    can spot memory leaks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at how the same chart would look with bad resource handling (not
    fully releasing allocated memory):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Memory consumption and memory leaks](img/B02088_01_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding chart, you can clearly see that not all memory is released
    when a resource is no longer used, which is causing the line to move out of the
    green box. This means the program is consuming more and more memory every second,
    even when the resources loaded are released.
  prefs: []
  type: TYPE_NORMAL
- en: The same can be done with programs that aren't resource heavy, for instance,
    scripts that execute a particular processing task for a considerable period of
    time. In those cases, the memory consumption and the leaks should be easier to
    spot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Memory consumption and memory leaks](img/B02088_01_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: When the processing stage starts, the memory consumption should stabilize within
    a clearly defined range. If we spot numbers outside that range, especially if
    it goes out of it and never comes back, we're looking at another example of a
    memory leak.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example of such a case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Memory consumption and memory leaks](img/B02088_01_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The risk of premature optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Optimization is normally considered a good practice. However, this doesn't hold
    true when the act of optimization ends up driving the design decisions of the
    software solution.
  prefs: []
  type: TYPE_NORMAL
- en: A very common pitfall developers face while starting to code a new piece of
    software is premature optimization.
  prefs: []
  type: TYPE_NORMAL
- en: When this happens, the end result ends up being quite the opposite of the intended
    optimized code. It can contain an incomplete version of the required solution,
    or it can even contain errors derived from the optimization-driven design decisions.
  prefs: []
  type: TYPE_NORMAL
- en: As a normal rule of thumb, if you haven't measured (profiled) your code, optimizing
    it might not be the best idea. First, focus on readable code. Then, profile it
    and find out where the real bottlenecks are, and as a final step, perform the
    actual optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Running time complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When profiling and optimizing code, it's really important to understand what
    **Running time complexity** (**RTC**) is and how we can use that knowledge to
    properly optimize our code.
  prefs: []
  type: TYPE_NORMAL
- en: RTC helps quantify the execution time of a given algorithm. It does so by providing
    a mathematical approximation of the time a piece of code will take to execute
    for any given input. It is an approximation, because that way, we're able to group
    similar algorithms using that value.
  prefs: []
  type: TYPE_NORMAL
- en: RTC is expressed using something called **Big O notation**. In mathematics,
    Big O notation is used to express the limiting behavior of a given function when
    the terms tend to infinity. If I apply that concept in computer science, we can
    use Big O notation to express the limiting behavior of the function describing
    the execution time.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, this notation will give us a broad idea of how long our algorithm
    will take to process an arbitrarily large input. It will not, however, give us
    a precise number for the time of execution, which would require a more in-depth
    analysis of the source code.
  prefs: []
  type: TYPE_NORMAL
- en: 'As I''ve said earlier, we can use this tendency to group algorithms. Here are
    some of the most common groups:'
  prefs: []
  type: TYPE_NORMAL
- en: Constant time – O(1)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the simplest of them all. This notation basically means that the action
    we're measuring will always take a constant amount of time, and this time is not
    dependent on the size of the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of code that have *O(1)* execution time:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Determining whether a number is odd or even:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Printing a message into standard output:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Even something more conceptually complex, like finding the value of a key inside
    a dictionary (or hash table), if implemented correctly, can be done in constant
    time. Technically speaking, accessing an element on the hash takes *O(1)* amortized
    time, which roughly means that the average time each operation takes (without
    taking into account edge cases) is a constant *O(1)* time.
  prefs: []
  type: TYPE_NORMAL
- en: Linear time – O(n)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear time dictates that for a given input of arbitrary length *n*, the amount
    of time required for the execution of the algorithm is linearly proportional to
    *n*, for instance, *3n*, *4n + 5*, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear time – O(n)](img/B02088_01_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding chart clearly shows that both the blue (*3n*) line and the red
    one (*4n + 5*) have the same upper limit as the black line (*n*) when *x* tends
    to infinity. So, to simplify, we can just say that all three functions are *O(n)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of algorithms with this execution order are:'
  prefs: []
  type: TYPE_NORMAL
- en: Finding the smallest value in an unsorted list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing two strings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deleting the last item inside a linked list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logarithmic time – O(log n)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An algorithm with logarithmic execution time is one that will have a very determined
    upper limit time. A logarithmic function grows quickly at first, but it'll slow
    down as the input size gets bigger. It will never stop growing, but the amount
    it grows by will be so small that it will be irrelevant.
  prefs: []
  type: TYPE_NORMAL
- en: '![Logarithmic time – O(log n)](img/B02088_01_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding chart shows three different logarithmic functions. You can clearly
    see that they all possess a similar shape, including the upper limit *x*, which
    keeps increasing to infinity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples of algorithms that have logarithmic execution time are:'
  prefs: []
  type: TYPE_NORMAL
- en: Binary search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating Fibonacci numbers (using matrix multiplications)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linearithmic time – O(nlog n)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A particular combination of the previous two orders of execution is the linearithmic
    time. It grows quickly as soon as the value of *x* starts increasing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of algorithms that have this order of execution:'
  prefs: []
  type: TYPE_NORMAL
- en: Merge sort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heap sort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quick sort (at least its average time complexity)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s see a few examples of plotted linearithmic functions to understand them
    better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linearithmic time – O(nlog n)](img/B02088_01_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Factorial time – O(n!)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Factorial time is one of the worst execution times we might get out of an algorithm.
    It grows so quickly that it's hard to plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a rough approximation of how the execution time of our algorithm would
    look with factorial time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Factorial time – O(n!)](img/B02088_01_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: An example of an algorithm with factorial execution time is the solution for
    the traveling salesman using brute force search (basically checking every single
    possible solution).
  prefs: []
  type: TYPE_NORMAL
- en: Quadratic time – O(n^)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Quadratic execution time is another example of a fast growing algorithm. The
    bigger the input size, the longer it's going to take (this is true for most complexities,
    but then again, specially true for this one). Quadratic execution time is even
    less efficient that linearithmic time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples of algorithms having this order of execution are:'
  prefs: []
  type: TYPE_NORMAL
- en: Bubble sort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traversing a 2D array
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Insertion sort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some examples of plotted exponential functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Quadratic time – O(n^)](img/B02088_01_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, let''s look at all examples plotted together to get a clear idea of
    algorithm efficiency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Quadratic time – O(n^)](img/B02088_01_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Leaving aside constant execution time, which is clearly faster but most of
    the time impossible to achieve in complex algorithms, the order or preference
    should be:'
  prefs: []
  type: TYPE_NORMAL
- en: Logarithmic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linearithmic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quadratic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Factorial
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obviously, there are cases when you'll have no choice but to get a quadratic
    execution time as the best possible result. The idea is to always aim for the
    faster algorithms, but the limitations of your problems and technology will affect
    the actual result.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that between quadratic and factorial times, there are several other alternatives
    (cubic, *n ^ 4*, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important consideration is that most algorithms don''t have only a
    single order of execution time. They can have up to three orders of execution
    time: for the best case, normal case, and worst case scenarios. The scenario is
    determined by the properties of the input data. For instance, the insertion sort
    algorithm will run much faster if the input is already sorted (best case), and
    it will be worst (exponential order) for other types of input.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other interesting cases to look at are the data types used. They inherently
    come with execution time that is associated with actions you can perform on them
    (lookup, insert, search, and so on). Let''s look at some of the most common data
    types and their associated actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Data Structure | Time complexity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|   | **Average case** | **Worst case** |'
  prefs: []
  type: TYPE_TB
- en: '|   | **Indexing** | **Search** | **Insertion** | **Deletion** | **Indexing**
    | **Search** | **Insertion** | **Deletion** |'
  prefs: []
  type: TYPE_TB
- en: '| **List** | *O(1)* | *O(n)* | *-* | *-* | *O(1)* | *O(n)* | *-* | *-* |'
  prefs: []
  type: TYPE_TB
- en: '| **Linked list** | *O(n)* | *O(n)* | *O(1)* | *O(1)* | *O(n)* | *O(n)* | *O(1)*
    | *O(n)* |'
  prefs: []
  type: TYPE_TB
- en: '| **Doubly linked list** | *O(n)* | *O(n)* | *O(1)* | *O(1)* | *O(n)* | *O(n)*
    | *O(1)* | *O(1)* |'
  prefs: []
  type: TYPE_TB
- en: '| **Dictionary** | *-* | *O(1)* | *O(1)* | *O(1)* | *-* | *O(n)* | *O(n)* |
    *O(n)* |'
  prefs: []
  type: TYPE_TB
- en: '| **Binary search tree** | *O(log(n))* | *O(log(n))* | *O(log(n))* | *O(log(n))*
    | *O(n)* | *O(n)* | *O(n)* | *O(n)* |'
  prefs: []
  type: TYPE_TB
- en: Profiling best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Profiling is a repetitive task. You''ll do it several times inside the same
    project in order to get the best results, and you''ll do it again on the next
    project. Just like with any other repetitive task in software development, there
    is a set of best practices you can follow to ensure that you get the most out
    of the process. Let''s look at some of them:'
  prefs: []
  type: TYPE_NORMAL
- en: Build a regression-test suite
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before starting any kind of optimization process, you need to make sure that
    the changes you make to the code will not affect its functioning in a bad way.
    The best way to do this, especially when it's a big code base, is to create a
    test suite. Make sure that your code coverage is high enough to provide the confidence
    you need to make the changes. A test suite with 60 percent code coverage can lead
    to very bad results.
  prefs: []
  type: TYPE_NORMAL
- en: A regression-test suite will allow you to make as many optimization tries as
    you need to without fear of breaking the code.
  prefs: []
  type: TYPE_NORMAL
- en: Mind your code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Functional code tends to be easier to refactor, mainly because the functions
    structured that way tend to avoid side effects. This reduces any risk of affecting
    unwanted parts of your system. If your functions avoid a local mutable state,
    that's another winning point for you. This is because the code should be pretty
    straightforward for you to understand and change. Functions that don't follow
    the previously mentioned guidelines will require more work and care while refactoring.
  prefs: []
  type: TYPE_NORMAL
- en: Be patient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Profiling is not fast, not easy, and not an exact process. What this means is
    that you should not expect to just run the profiler and expect the data from it
    to point directly to your problem. That could happen, yes. However, most of the
    time, the problems you're trying to solve are the ones that simple debugging couldn't
    fix. This means you'll be browsing through data, plotting it to try to make sense
    of it, and narrowing down the source of your problem until you either need to
    start again, or you find it.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that the deeper you get into the profiled data, the deeper into
    the rabbit hole you get. Numbers will stop making sense right away, so make sure
    you know what you're doing and that you have the right tools for the job before
    you start. Otherwise, you'll waste your time and end up with nothing but frustration.
  prefs: []
  type: TYPE_NORMAL
- en: Gather as much data as you can
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Depending on the type and size of software you're dealing with, you might want
    to get as much data as you can before you start analyzing it. Profilers are a
    great source for this. However, there are other sources, such as server logs from
    web applications, custom logs, system resources snapshots (like from the OS task
    manager), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess your data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After you have all the information from your profilers, your logs, and other
    sources, you will probably need to preprocess the data before analyzing it. Don't
    shy away from unstructured data just because a profiler can't understand it. Your
    analysis of the data will benefit from the extra numbers.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, getting the web server logs is a great idea if you're profiling
    a web application, but those files are normally just text files with one line
    per request. By parsing it and getting the data into some kind of database system
    (like MongoDB, MySQL, or the like), you'll be able to give that data meaning (by
    parsing the dates, doing geolocation by source IP address, and so on) and query
    that information afterwards.
  prefs: []
  type: TYPE_NORMAL
- en: The formal name for the stage is ETL, which stands for *extracting the data
    from it's sources, transforming it into something with meaning, and loading it
    into another system that you can later query*.
  prefs: []
  type: TYPE_NORMAL
- en: Visualize your data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you don't know exactly what it is that you're looking for and you're just
    looking for ways to optimize your code before something goes wrong, a great idea
    to get some insight into the data you've already preprocessed is to visualize
    it. Computers are great with numbers, but humans, on the other hand, are great
    with images when we want to find patterns and understand what kind of insight
    we can gather from the information we have.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, to continue with the web server logs example, a simple plot (such
    as the ones you can do with MS Excel) for the requests by hour can provide some
    insight into the behavior of your users:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualize your data](img/B02088_01_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding chart clearly shows that the majority of requests are done during
    late afternoon and continue into the night. You can use this insight later on
    for further profiling. For instance, an optional improvement of your setup here
    would be to provide more resources for your infrastructure during that time (something
    that can be done with service providers such as Amazon Web Services).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example, using custom profiling data, could be the following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualize your data](img/B02088_01_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It uses data from the first code example of this chapter by counting the number
    of each event that triggers the `profile` function. We can then plot it and get
    an idea of the most common events. In our case, the `call` and `return` events
    are definitely taking up most of our program's time.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've covered the basics of profiling. You understood profiling
    and its importance. You also learned how we can leverage it in order to get the
    most out of our code.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll start getting our hands dirty by looking at some
    Python profilers and how we can use them on our applications.
  prefs: []
  type: TYPE_NORMAL
