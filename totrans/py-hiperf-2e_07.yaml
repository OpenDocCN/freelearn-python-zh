- en: Parallel Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行处理
- en: With parallel processing by using multiple cores, you can increase the amount
    of calculations your program can do in a given time frame without needing a faster
    processor. The main idea is to divide a problem into independent subunits and
    use multiple cores to solve those subunits in parallel.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用多个核心的并行处理，您可以在给定的时间框架内增加程序可以完成的计算量，而无需更快的处理器。主要思想是将问题划分为独立的子单元，并使用多个核心并行解决这些子单元。
- en: Parallel processing is necessary to tackle large-scale problems. Companies produce
    massive quantities of data every day that need to be stored in multiple computers
    and analyzed. Scientists and engineers run parallel code on supercomputers to
    simulate massive systems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 并行处理对于解决大规模问题是必要的。公司每天产生大量数据，需要存储在多台计算机上并进行分析。科学家和工程师在超级计算机上运行并行代码以模拟大型系统。
- en: 'Parallel processing allows you to take advantage of multicore CPUs as well
    as GPUs that work extremely well with highly parallel problems. In this chapter,
    we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 并行处理允许您利用多核 CPU 以及与高度并行问题配合得非常好的 GPU。在本章中，我们将涵盖以下主题：
- en: A brief introduction to the fundamentals of parallel processing
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行处理基础简介
- en: Illustrating how to parallelize simple problems with the `multiprocessing` Python
    library
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `multiprocessing` Python 库并行化简单问题的说明
- en: Using the simple `ProcessPoolExecutor` interface
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用简单的 `ProcessPoolExecutor` 接口
- en: Parallelizing our programs using multithreading with the help of Cython and
    OpenMP
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Cython 和 OpenMP 的帮助下使用多线程并行化我们的程序
- en: Achieving parallelism automatically with Theano and Tensorflow
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Theano 和 Tensorflow 自动实现并行性
- en: Executing code on a GPU with Theano, Tensorflow, and Numba
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Theano、Tensorflow 和 Numba 在 GPU 上执行代码
- en: Introduction to parallel programming
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行编程简介
- en: In order to parallelize a program, it is necessary to divide the problem into
    subunits that can run independently (or almost independently) from each other.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了并行化一个程序，有必要将问题划分为可以独立（或几乎独立）运行的子单元。
- en: A problem where the subunits are totally independent from each other is called **embarrassingly parallel**.
    An element-wise operation on an array is a typical example--the operation needs
    to only know the element it is handling at the moment. Another example is our
    particle simulator. Since there are no interactions, each particle can evolve
    independently from the others. Embarrassingly parallel problems are very easy
    to implement and perform very well on parallel architectures.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当子单元完全独立于彼此时，该问题被称为**令人尴尬的并行**。数组上的元素级操作是一个典型的例子——操作只需要知道它当前处理的元素。另一个例子是我们的粒子模拟器。由于没有相互作用，每个粒子可以独立于其他粒子进化。令人尴尬的并行问题很容易实现，并且在并行架构上表现良好。
- en: Other problems may be divided into subunits but have to share some data to perform
    their calculations. In those cases, the implementation is less straightforward
    and can lead to performance issues because of the communication costs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 其他问题可能被划分为子单元，但必须共享一些数据以执行它们的计算。在这些情况下，实现过程不太直接，并且由于通信成本可能导致性能问题。
- en: We will illustrate the concept with an example. Imagine that you have a particle
    simulator, but this time the particles attract other particles within a certain
    distance (as shown in the following figure). To parallelize this problem, we divide
    the simulation box into regions and assign each region to a different processor.
    If we evolve the system one step at a time, some particles will interact with
    particles in a neighboring region. To perform the next iteration, communication
    with the new particle positions of the neighboring region is required.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个例子来说明这个概念。想象一下，你有一个粒子模拟器，但这次粒子在一定的距离内吸引其他粒子（如图所示）。为了并行化这个问题，我们将模拟区域划分为区域，并将每个区域分配给不同的处理器。如果我们一次进化系统的一步，一些粒子将与相邻区域的粒子相互作用。为了执行下一次迭代，需要与相邻区域的新粒子位置进行通信。
- en: '![](img/B06440_07CHPNO_01.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06440_07CHPNO_01.png)'
- en: 'Communication between processes is costly and can seriously hinder the performance
    of parallel programs. There exist two main ways to handle data communication in
    parallel programs:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 进程之间的通信成本高昂，可能会严重阻碍并行程序的性能。在并行程序中处理数据通信存在两种主要方式：
- en: '**Shared memory**'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**共享内存**'
- en: '**Distributed memory**'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式内存**'
- en: In shared memory, the subunits have access to the same memory space. The advantage
    of this approach is that you don't have to explicitly handle the communication
    as it is sufficient to write or read from the shared memory. However, problems
    arise when multiple processes try to access and change the same memory location
    at the same time. Care should be taken to avoid such conflict using synchronization
    techniques.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在共享内存中，子单元可以访问相同的内存空间。这种方法的优点是，你不需要显式处理通信，因为从共享内存中写入或读取就足够了。然而，当多个进程同时尝试访问和更改相同的内存位置时，就会出现问题。应谨慎使用同步技术来避免此类冲突。
- en: In the distributed memory model, each process is completely separated from the
    others and possesses its own memory space. In this case, communication is handled
    explicitly between the processes. The communication overhead is typically costlier
    compared to shared memory as data can potentially travel through a network interface.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式内存模型中，每个进程与其他进程完全隔离，并拥有自己的内存空间。在这种情况下，进程间的通信是显式处理的。与共享内存相比，通信开销通常更昂贵，因为数据可能需要通过网络接口传输。
- en: One common way to achieve parallelism with the shared memory model is **threads**.
    Threads are independent subtasks that originate from a process and share resources,
    such as memory. This concept is further illustrated in the following figure. Threads
    produce multiple execution context and share the same memory space, while processes
    provide multiple execution context that possess their own memory space and communication
    has to be handled explicitly.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 实现共享内存模型中的并行性的一个常见方法是**线程**。线程是从进程派生出来的独立子任务，并共享资源，如内存。这一概念在以下图中进一步说明。线程产生多个执行上下文并共享相同的内存空间，而进程提供多个具有自己内存空间和需要显式处理通信的执行上下文。
- en: '![](img/B06440_07CHPNO_02.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06440_07CHPNO_02.png)'
- en: Python can spawn and handle threads, but they can't be used to increase performance;
    due to the Python interpreter design, only one Python instruction is allowed to
    run at a time--this mechanism is called **Global Interpreter Lock** (**GIL**).
    What happens is that each time a thread executes a Python statement, the thread
    acquires a lock and, when the execution is completed, the same lock is released.
    Since the lock can be acquired only by one thread at a time, other threads are
    prevented from executing Python statements while some other thread holds the lock.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Python可以创建和处理线程，但它们不能用来提高性能；由于Python解释器的设计，一次只能允许一个Python指令运行--这种机制被称为**全局解释器锁**（**GIL**）。发生的情况是，每次线程执行Python语句时，线程都会获取一个锁，当执行完成后，释放相同的锁。由于锁一次只能被一个线程获取，因此当某个线程持有锁时，其他线程将无法执行Python语句。
- en: Even though the GIL prevents parallel execution of Python instructions, threads
    can still be used to provide concurrency in situations where the lock can be released,
    such as in time-consuming I/O operations or in C extensions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GIL阻止了Python指令的并行执行，但在可以释放锁的情况（如耗时的I/O操作或C扩展）中，线程仍然可以用来提供并发性。
- en: Why not remove the GIL? In past years, many attempts have been made, including
    the most recent gilectomy experiment. First, removing the GIL is not an easy task
    and requires modification of most of the Python data structures. Additionally,
    such fine-grained locking can be costly and may introduce substantial performance
    loss in single-threaded programs. Despite this, some Python implementations (notable
    examples are Jython and IronPython) do not use the GIL.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不移除全局解释器锁（GIL）呢？在过去的几年里，已经尝试过多次移除GIL，包括最近的gilectomy实验。首先，移除GIL并不是一件容易的事情，它需要修改大多数Python数据结构。此外，这种细粒度的锁定可能会带来高昂的成本，并可能在单线程程序中引入显著的性能损失。尽管如此，一些Python实现（如Jython和IronPython）并不使用GIL。
- en: 'The GIL can be completely sidestepped using processes instead of threads. Processes
    don''t share the same memory area and are independent from each other--each process
    has its own interpreter. Processes have a few disadvantages: starting up a new
    process is generally slower than starting a new thread, they consume more memory,
    and inter-process communication can be slow. On the other hand, processes are
    still very flexible, and they scale better as they can be distributed on multiple
    machines.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过使用进程而不是线程来完全绕过GIL。进程不共享相同的内存区域，彼此独立--每个进程都有自己的解释器。进程有一些缺点：启动一个新的进程通常比启动一个新的线程慢，它们消耗更多的内存，并且进程间通信可能较慢。另一方面，进程仍然非常灵活，并且随着它们可以在多台机器上分布而具有更好的可扩展性。
- en: Graphic processing units
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图形处理单元
- en: Graphic processing units are special processors designed for computer graphics
    applications. Those applications usually require processing the geometry of a
    3D scene and output an array of pixel to the screen. The operations performed
    by GPUs involve array and matrix operations on floating point numbers.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图形处理单元是专为计算机图形应用设计的特殊处理器。这些应用通常需要处理3D场景的几何形状，并将像素数组输出到屏幕上。GPU执行的操作涉及浮点数的数组和矩阵运算。
- en: GPUs are designed to run this graphics-related operation very efficiently, and
    they achieve this by adopting a highly parallel architecture. Compared to a CPU,
    a GPU has many more (thousands) of small processing units. GPUs are intended to
    produce data at about 60 frames per second, which is much slower than the typical
    response time of a CPU, which possesses higher clock speeds.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: GPU被设计用来非常高效地运行与图形相关的操作，它们通过采用高度并行的架构来实现这一点。与CPU相比，GPU拥有更多的（数千个）小型处理单元。GPU旨在每秒产生大约60帧的数据，这比CPU的典型响应时间慢得多，而CPU具有更高的时钟速度。
- en: GPUs possess a very different architecture from a standard CPU and are specialized
    for computing floating point operations. Therefore, to compile programs for GPUs,
    it is necessary to utilize special programming platforms, such as CUDA and OpenCL.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: GPU具有与标准CPU非常不同的架构，并且专门用于计算浮点运算。因此，为了为GPU编译程序，有必要利用特殊的编程平台，如CUDA和OpenCL。
- en: '**Compute Unified Device Architecture** (**CUDA**) is a proprietary NVIDIA
    technology. It provides an API that can be accessed from other languages. CUDA
    provides the NVCC tool that can be used to compile GPU programs written in a language
    similar to C (CUDA C) as well as numerous libraries that implement highly optimized
    mathematical routines.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**统一计算设备架构**（**CUDA**）是NVIDIA的专有技术。它提供了一个可以从其他语言访问的API。CUDA提供了NVCC工具，可以用来编译用类似于C（CUDA
    C）的语言编写的GPU程序，以及实现高度优化的数学例程的众多库。'
- en: '**OpenCL** is an open technology with the ability of writing parallel programs
    that can be compiled for a variety of target devices (CPUs and GPUs of several
    vendors) and is a good option for non-NVIDIA devices.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**OpenCL**是一种开放技术，具有编写可编译为多种目标设备（多个厂商的CPU和GPU）的并行程序的能力，对于非NVIDIA设备来说是一个不错的选择。'
- en: GPU programming sounds wonderful on paper. However, don't throw away your CPU
    yet. GPU programming is tricky and only specific use cases benefit from the GPU
    architecture. Programmers need to be aware of the costs incurred in memory transfers
    to and from the main memory and how to implement algorithms to take advantage
    of the GPU architecture.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: GPU编程在纸面上听起来很美妙。然而，不要急着丢弃你的CPU。GPU编程很复杂，并且只有特定的用例能从GPU架构中受益。程序员需要意识到在主内存之间进行数据传输所产生的成本，以及如何实现算法以利用GPU架构。
- en: Generally, GPUs are great at increasing the amount of operations you can perform
    per unit of time (also called **throughput**); however, they require more time
    to prepare the data for processing. In contrast, CPUs are much faster at producing
    an individual result from scratch (also called **latency**).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，GPU在增加每单位时间内可以执行的操作数量（也称为**吞吐量**）方面表现很好；然而，它们需要更多的时间来准备数据以进行处理。相比之下，CPU在从头开始生成单个结果方面要快得多（也称为**延迟**）。
- en: For the right problem, GPUs provide extreme (10 to 100 times) speedup. For this
    reason, they often constitute a very inexpensive (the same speedup will require
    hundreds of CPUs) solution to improve the performance of numerically intensive applications.
    We will illustrate how to execute some algorithms on a GPU in the *Automatic Parallelism*
    section.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于正确的问题，GPU提供了极端（10到100倍）的加速。因此，它们通常构成了一个非常经济的（同样的加速将需要数百个CPU）解决方案，用于提高数值密集型应用的性能。我们将在*自动并行性*部分说明如何在GPU上执行一些算法。
- en: Using multiple processes
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用多个进程
- en: The standard `multiprocessing` module can be used to quickly parallelize simple
    tasks by spawning several processes, while avoiding the GIL problem. Its interface
    is easy to use and includes several utilities to handle task submission and synchronization.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的`multiprocessing`模块可以通过启动几个进程来快速并行化简单任务，同时避免GIL问题。它的接口易于使用，包括处理任务提交和同步的几个实用工具。
- en: The Process and Pool classes
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Process和Pool类
- en: 'You can create a process that runs independently by subclassing `multiprocessing.Process`.
    You can extend the `__init__` method to initialize resources, and you can write
    the portion of the code that will be executed in a subprocess by implementing
    the `Process.run` method. In the following code, we define a `Process` class that
    will wait for one second and print its assigned `id`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过继承 `multiprocessing.Process` 来创建一个独立运行的进程。您可以扩展 `__init__` 方法来初始化资源，并且可以通过实现
    `Process.run` 方法来编写将在子进程中执行的部分代码。在以下代码中，我们定义了一个 `Process` 类，它将等待一秒钟并打印其分配的 `id`：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To spawn the process, we have to instantiate the `Process` class and call the
    `Process.start` method. Note that you don''t directly call `Process.run`; the
    call to `Process.start` will create a new process that, in turn, will call the
    `Process.run` method. We can add the following lines at the end of the preceding snippet
    to create and start the new process:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动进程，我们必须实例化 `Process` 类并调用 `Process.start` 方法。请注意，您不能直接调用 `Process.run`；对
    `Process.start` 的调用将创建一个新的进程，然后该进程将调用 `Process.run` 方法。我们可以在前面的代码片段末尾添加以下行来创建并启动新进程：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The instructions after `Process.start` will be executed immediately without
    waiting for the `p` process to finish. To wait for the task completion, you can
    use the `Process.join` method, as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `Process.start` 之后的指令将立即执行，而不需要等待 `p` 进程完成。要等待任务完成，您可以使用 `Process.join` 方法，如下所示：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can launch four different processes that will run parallely in the same
    way. In a serial program, the total required time will be four seconds. Since
    the execution is concurrent, the resulting wallclock time will be of one second.
    In the following code, we create four processes that will execute concurrently:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以启动四个不同的进程，它们将以相同的方式并行运行。在串行程序中，所需的总时间将是四秒钟。由于执行是并发的，因此结果的时间将是秒。在以下代码中，我们创建了四个将并发执行的进程：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that the order of the execution for parallel processes is unpredictable
    and ultimately depends on how the OS schedules their execution. You can verify
    this behavior by executing the program multiple times; the order will likely be
    different between runs.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，并行进程的执行顺序是不可预测的，最终取决于操作系统如何调度它们的执行。您可以通过多次执行程序来验证此行为；运行之间的顺序可能会不同。
- en: The `multiprocessing` module exposes a convenient interface that makes it easy
    to assign and distribute tasks to a set of processes that reside in the `multiprocessing.Pool`
    class.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`multiprocessing` 模块提供了一个方便的接口，使得将任务分配和分配给 `multiprocessing.Pool` 类中驻留的一组进程变得容易。'
- en: The `multiprocessing.Pool` class spawns a set of processes--called **workers**--and
    lets us submit tasks through the `apply`/`apply_async` and `map`/`map_async` methods.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`multiprocessing.Pool` 类会启动一组进程——称为 **工作者**——并允许我们通过 `apply`/`apply_async`
    和 `map`/`map_async` 方法提交任务。'
- en: The `Pool.map` method applies a function to each element of a list and returns
    the list of results. Its usage is equivalent to the built-in (serial) `map`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pool.map` 方法将函数应用于列表中的每个元素，并返回结果列表。其用法与内置的（串行）`map` 相当。'
- en: 'To use a parallel map, you should first initialize a `multiprocessing.Pool`
    object. It takes the number of workers as its first argument; if not provided,
    that number will be equal to the number of cores in the system. You can initialize
    a `multiprocessing.Pool` object in the following way:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用并行映射，您首先需要初始化一个 `multiprocessing.Pool` 对象。它将工作者数量作为其第一个参数；如果没有提供，则该数字将与系统中的核心数相等。您可以通过以下方式初始化一个
    `multiprocessing.Pool` 对象：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s see `pool.map` in action. If you have a function that computes the square
    of a number, you can map the function to the list by calling `Pool.map` and passing
    the function and the list of inputs as arguments, as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 `pool.map` 的实际应用。如果您有一个计算数字平方的函数，您可以通过调用 `Pool.map` 并传递函数和输入列表作为参数来将该函数映射到列表上，如下所示：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `Pool.map_async` function is just like `Pool.map` but returns an `AsyncResult`
    object instead of the actual result. When we call  `Pool.map`, the execution of
    the main program is stopped until all the workers are finished processing the
    result. With `map_async`, the `AsyncResult` object is returned immediately without
    blocking the main program and the calculations are done in the background. We
    can then retrieve the result using the `AsyncResult.get` method at any time, as
    shown in the following lines:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pool.map_async` 函数与 `Pool.map` 类似，但返回的是 `AsyncResult` 对象而不是实际的结果。当我们调用 `Pool.map`
    时，主程序的执行会停止，直到所有工作进程完成结果的处理。使用 `map_async`，会立即返回 `AsyncResult` 对象而不阻塞主程序，计算在后台进行。然后我们可以使用
    `AsyncResult.get` 方法在任何时候检索结果，如下面的代码所示：'
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`Pool.apply_async` assigns a task consisting of a single function to one of
    the workers. It takes the function and its arguments and returns an `AsyncResult`
    object. We can obtain an effect similar to `map` using `apply_async`, as shown:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pool.apply_async` 将一个由单个函数组成的任务分配给一个工作进程。它接受函数及其参数，并返回一个 `AsyncResult` 对象。我们可以使用
    `apply_async` 获得类似于 `map` 的效果，如下所示：'
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The Executor interface
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行器接口
- en: From version 3.2 onward, it is possible to execute Python code in parallel using the
    `Executor` interface provided in the `concurrent.futures` module. We already saw
    the `Executor` interface in action in the previous chapter, when we used `ThreadPoolExecutor`
    to perform multiple tasks concurrently. In this subsection, we'll demonstrate
    the usage of the `ProcessPoolExecutor` class.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从版本 3.2 开始，可以使用 `concurrent.futures` 模块中提供的 `Executor` 接口并行执行 Python 代码。我们在上一章中已经看到了
    `Executor` 接口的使用，当时我们使用 `ThreadPoolExecutor` 来并发执行多个任务。在本小节中，我们将演示 `ProcessPoolExecutor`
    类的使用。
- en: '`ProcessPoolExecutor` exposes a very lean interface, at least when compared
    to the more featureful `multiprocessing.Pool`. A `ProcessPoolExecutor` can be
    instantiated, similar to `ThreadPoolExecutor`, by passing a number of worker threads
    using the `max_workers` argument (by default, `max_workers` will be the number
    of CPU cores available). The main methods available to the `ProcessPoolExecutor` are
    `submit` and `map`.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`ProcessPoolExecutor` 提供了一个非常简洁的接口，至少与功能更丰富的 `multiprocessing.Pool` 相比是这样的。可以通过传递
    `max_workers` 参数（默认情况下，`max_workers` 将是可用的 CPU 核心数）来实例化 `ProcessPoolExecutor`，类似于
    `ThreadPoolExecutor`。`ProcessPoolExecutor` 可用的主要方法有 `submit` 和 `map`。'
- en: 'The `submit` method will take a function and return a `Future` (see the last chapter)
    that will keep track of the execution of the submitted function. The method map
    works similarly to the `Pool.map` function, except that it returns an iterator
    rather than a list:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`submit` 方法将接受一个函数并返回一个 `Future`（见最后一章），该 `Future` 将跟踪提交函数的执行。方法 `map` 与 `Pool.map`
    函数类似，但返回的是一个迭代器而不是列表：'
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To extract the result from one or more `Future` instances, you can use the
    `concurrent.futures.wait` and `concurrent.futures.as_completed` functions. The
    `wait` function accepts a list of `future` and will block the execution of the
    programs until all the futures have completed their execution. The result can
    then be extracted using the `Future.result` method. The `as_completed` function
    also accepts a function but will, instead, return an iterator over the results:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要从一个或多个 `Future` 实例中提取结果，可以使用 `concurrent.futures.wait` 和 `concurrent.futures.as_completed`
    函数。`wait` 函数接受一个 `future` 列表，并将阻塞程序的执行，直到所有 futures 完成它们的执行。然后可以使用 `Future.result`
    方法提取结果。`as_completed` 函数也接受一个函数，但会返回一个结果迭代器：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Alternatively, you can generate futures using the `asyncio.run_in_executor`
    function and manipulate the results using all the tools and syntax provided by
    the `asyncio` libraries so that you can achieve concurrency and parallelism at
    the same time.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以使用 `asyncio.run_in_executor` 函数生成 futures，并使用 `asyncio` 库提供的所有工具和语法来操作结果，这样你就可以同时实现并发和并行。
- en: Monte Carlo approximation of pi
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: π 的蒙特卡洛近似
- en: As an example, we will implement a canonical, embarrassingly parallel program--the
    **Monte Carlo approximation of pi**. Imagine that we have a square of size 2 units;
    its area will be 4 units. Now, we inscribe a circle of 1 unit radius in this square;
    the area of the circle will be *pi * r^2*. By substituting the value of *r* in
    the previous equation, we get that the numerical value for the area of the circle
    is *pi * (1)^2 = pi*. You can refer to the following figure for a graphical representation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 作为例子，我们将实现一个典型的、显而易见的并行程序——**蒙特卡洛逼近π**。想象一下，我们有一个边长为2个单位的正方形；其面积为4个单位。现在，我们在正方形内画一个半径为1个单位的圆；圆的面积将是
    *π * r^2*。通过将 *r* 的值代入前面的方程，我们得到圆面积的数值为 *π * (1)^2 = π*。你可以参考以下图表来获取图形表示。
- en: 'If we shoot a lot of random points on this figure, some points will fall into
    the circle, which we''ll call **hits, **while the remaining points, **misses, **will
    be outside the circle. The area of the circle will be proportional to the number
    of hits, while the area of the square will be proportional to the total number
    of shots. To get the value of *pi*, it is sufficient to divide the area of the
    circle (equal to *pi*) by the area of the square (equal to 4):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在这个图形上随机射击很多点，一些点将落在圆内，我们将它们称为**击中**，而剩余的点，**未击中**，将位于圆外。圆的面积将与击中的数量成正比，而正方形的面积将与射击的总数成正比。为了得到
    *π* 的值，只需将圆的面积（等于 *π*）除以正方形的面积（等于 4）即可：
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](img/image_07_003.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_07_003.png)'
- en: 'The strategy we will employ in our program will be as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在项目中将采用以下策略：
- en: Generate a lot of uniformly random (*x*, *y*) numbers in the range (**-1**,
    **1**)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在范围（**-1**，**1**）内生成大量的均匀随机（*x*，*y*）数字
- en: Test whether those numbers lie inside the circle by checking whether *x**2 +
    y**2* <= *1*
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过检查是否 *x**2 + y**2* <= *1* 来测试这些数字是否位于圆内
- en: 'The first step when writing a parallel program is to write a serial version
    and verify that it works. In a real-world scenario, you also want to leave the
    parallelization as the last step of your optimization process. First, we need
    to identify the slow parts, and second, parallelization is time-consuming and
    gives you *at most* a speedup equal to the number of processors. The implementation
    of the serial program is as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 编写并行程序的第一步是编写一个串行版本并验证其是否工作。在实际场景中，你希望将并行化作为优化过程的最后一步。首先，我们需要识别出慢速部分，其次，并行化耗时且只能提供与处理器数量相等的最大加速。串行程序的实现如下：
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The accuracy of our approximation will improve as we increase the number of
    samples. You can note that each loop iteration is independent from the other--this
    problem is embarrassingly parallel.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 随着样本数量的增加，我们的近似精度将提高。你可以注意到每个循环迭代都是独立的——这个问题是显而易见的并行问题。
- en: 'To parallelize this code, we can write a function, called `sample`, that corresponds
    to a single hit-miss check. If the sample hits the circle, the function will return
    `1`; otherwise, it will return `0`. By running `sample` multiple times and summing
    the results, we''ll get the total number of hits. We can run `sample` over multiple
    processors with `apply_async` and get the results in the following way:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了并行化这段代码，我们可以编写一个名为 `sample` 的函数，它对应于一次击中-未击中的检查。如果样本击中圆，函数将返回 `1`；否则，它将返回
    `0`。通过多次运行 `sample` 并汇总结果，我们将得到总的击中次数。我们可以使用 `apply_async` 在多个处理器上运行 `sample`
    并以以下方式获取结果：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can wrap the two versions in the `pi_serial` and `pi_apply_async` functions
    (you can find their implementation in the `pi.py` file) and benchmark the execution
    speed, as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这两个版本包裹在 `pi_serial` 和 `pi_apply_async` 函数中（你可以在 `pi.py` 文件中找到它们的实现）并比较执行速度，如下所示：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As shown in the earlier benchmark, our first parallel version literally cripples
    our code. The reason is that the time spent doing the actual calculation is small
    compared to the overhead required to send and distribute the tasks to the workers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述的基准测试所示，我们的第一个并行版本实际上削弱了我们的代码。原因是实际计算所需的时间与发送和分配任务给工作者的开销相比很小。
- en: 'To solve the issue, we have to make the overhead negligible compared to the
    calculation time. For example, we can ask each worker to handle more than one
    sample at a time, thus reducing the task communication overhead. We can write
    a `sample_multiple` function that processes more than one hit and modifies our
    parallel version by dividing our problem by 10; more intensive tasks are shown
    in the following code:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们必须使开销与计算时间相比可以忽略不计。例如，我们可以要求每个工作进程一次处理多个样本，从而减少任务通信开销。我们可以编写一个`sample_multiple`函数，它处理多个命中并修改我们的并行版本，通过将问题分成10份；更密集的任务如下面的代码所示：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can wrap this in a function called `pi_apply_async_chunked` and run it as
    follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个功能封装在一个名为`pi_apply_async_chunked`的函数中，并按如下方式运行：
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The results are much better; we more than doubled the speed of our program.
    You can also notice that the `user` metric is larger than `real`; the total CPU
    time is larger than the total time because more than one CPU worked at the same
    time. If you increase the number of samples, you will note that the ratio of communication
    to calculation decreases, giving even better speedups.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 结果要好得多；我们的程序速度提高了不止一倍。你还可以注意到`user`指标大于`real`；总CPU时间大于总时间，因为同时有多个CPU在工作。如果你增加样本数量，你会注意到通信与计算的比率降低，从而提供更好的加速。
- en: Everything is nice and simple when dealing with embarrassingly parallel problems.
    However, sometimes you have to share data between processes.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理令人尴尬的并行问题时，一切都很简单。然而，有时你必须在进程之间共享数据。
- en: Synchronization and locks
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 同步和锁
- en: 'Even if `multiprocessing` uses processes (with their own independent memory),
    it lets you define certain variables and arrays as shared memory. You can define
    a shared variable using `multiprocessing.Value`, passing its data type as a string
    (`i` integer, `d` double, `f` float, and so on). You can update the content of
    the variable through the `value` attribute, as shown in the following code snippet:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 即使`multiprocessing`使用进程（它们有自己独立的内存），它也允许你定义某些变量和数组作为共享内存。你可以使用`multiprocessing.Value`定义一个共享变量，将数据类型作为字符串传递（`i`表示整数，`d`表示双精度，`f`表示浮点数等）。你可以通过`value`属性更新变量的内容，如下面的代码片段所示：
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'When using shared memory, you should be aware of concurrent accesses. Imagine
    that you have a shared integer variable and each process increments its value
    multiple times. You will define a process class as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用共享内存时，你应该意识到并发访问。想象一下，你有一个共享的整数变量，每个进程多次增加其值。你可以定义一个进程类如下：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You can initialize the shared variable in the main program and pass it to `4`
    processes, as shown in the following code:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在主程序中初始化共享变量，并将其传递给`4`个进程，如下面的代码所示：
- en: '[PRE18]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: If you run this program (`shared.py` in the code directory), you will note that
    the final value of `counter` is not 4000, but it has random values (on my machine,
    they are between 2000 and 2500). If we assume that the arithmetic is correct,
    we can conclude that there's a problem with the parallelization.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这个程序（代码目录中的`shared.py`），你会注意到`counter`的最终值不是4000，而是有随机值（在我的机器上，它们在2000到2500之间）。如果我们假设算术是正确的，我们可以得出结论，并行化存在问题。
- en: What happens is that multiple processes are trying to access the same shared
    variable at the same time. The situation is best explained by looking at the following
    figure. In a serial execution, the first process reads (the number `0`), increments
    it, and writes the new value (`1`); the second process reads the new value (`1`),
    increments it, and writes it again (`2`).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 发生的情况是多个进程同时尝试访问相同的共享变量。这种情况最好通过查看以下图表来解释。在串行执行中，第一个进程读取（数字`0`），增加它，并写入新值（`1`）；第二个进程读取新值（`1`），增加它，并再次写入（`2`）。
- en: 'In the parallel execution, the two processes read (`0`), increment it, and
    write the value (`1`) at the same time, leading to a wrong answer:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在并行执行中，两个进程同时读取（`0`），增加它，并写入值（`1`），导致错误答案：
- en: '![](img/B06440_07CHPNO_04.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06440_07CHPNO_04.png)'
- en: To solve this problem, we need to synchronize the access to this variable so
    that only one process at a time can access, increment, and write the value on
    the shared variable. This feature is provided by the `multiprocessing.Lock` class.
    A lock can be acquired and released through the `acquire` method and `release`,
    or using the lock as a context manager. Since the lock can be acquired by only
    one process at a time, this method prevents multiple processes from executing
    the protected section of code at the same time.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们需要同步对这个变量的访问，以确保一次只有一个进程可以访问、增加和写入共享变量的值。这个功能由 `multiprocessing.Lock`
    类提供。锁可以通过 `acquire` 方法或 `release` 以及将锁用作上下文管理器来获取和释放。由于锁一次只能被一个进程获取，这种方法防止了多个进程同时执行受保护的代码段。
- en: 'We can define a global lock and use it as a context manager to restrict the
    access to the counter, as shown in the following code snippet:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义一个全局锁，并使用它作为上下文管理器来限制对计数器的访问，如下代码片段所示：
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Synchronization primitives, such as locks, are essential to solve many problems,
    but they should be kept to a minimum to improve the performance of your program.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 同步原语，如锁，对于解决许多问题是必不可少的，但它们应该保持最小，以提高程序的性能。
- en: The `multiprocessing` module includes other communication and synchronization
    tools; you can refer to the official documentation at [http://docs.python.org/3/library/multiprocessing.html](http://docs.python.org/3/library/multiprocessing.html) for
    a complete reference.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`multiprocessing` 模块还包括其他通信和同步工具；您可以参考官方文档[http://docs.python.org/3/library/multiprocessing.html](http://docs.python.org/3/library/multiprocessing.html)以获取完整参考。'
- en: Parallel Cython with OpenMP
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Parallel Cython with OpenMP
- en: Cython provides a convenient interface to perform shared-memory parallel processing
    through **OpenMP**. This lets you write extremely efficient parallel code directly
    in Cython without having to create a C wrapper.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Cython 提供了一个方便的接口，通过 **OpenMP** 执行共享内存并行处理。这允许您直接在 Cython 中编写非常高效的并行代码，而无需创建
    C 包装器。
- en: OpenMP is a specification and an API designed to write multithreaded, parallel
    programs. The OpenMP specification includes a series of C preprocessor directives
    to manage threads and provides communication patterns, load balancing, and other
    synchronization features. Several C/C++ and Fortran compilers (including GCC)
    implement the OpenMP API.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: OpenMP 是一个规范和一个 API，旨在编写多线程、并行程序。OpenMP 规范包括一系列 C 预处理器指令，用于管理线程，并提供通信模式、负载均衡和其他同步功能。几个
    C/C++ 和 Fortran 编译器（包括 GCC）实现了 OpenMP API。
- en: We can introduce the Cython parallel features with a small example. Cython provides
    a simple API based on OpenMP in the `cython.parallel` module. The simplest way
    to achieve parallelism is through `prange`, which is a construct that automatically
    distributes loop operations in multiple threads.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个小示例引入 Cython 并行功能。Cython 在 `cython.parallel` 模块中提供了一个基于 OpenMP 的简单 API。实现并行化的最简单方法是使用
    `prange`，这是一个自动在多个线程中分配循环操作的构造。
- en: 'First of all, we can write the serial version of a program that computes the
    square of each element of a NumPy array in the `hello_parallel.pyx` file. We define
    a function, `square_serial`, that takes a buffer as input and populates an output
    array with the squares of the input array elements; `square_serial` is shown in
    the following code snippet:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以在 `hello_parallel.pyx` 文件中编写一个程序的串行版本，该程序计算 NumPy 数组中每个元素的平方。我们定义了一个函数
    `square_serial`，它接受一个缓冲区作为输入，并将输入数组元素的平方填充到输出数组中；`square_serial` 如下代码片段所示：
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Implementing a parallel version of the loop over the array elements involves
    substituting the `range` call with `prange`. There's a caveat--to use `prange`,
    it is necessary that the body of the loop is interpreter-free. As already explained,
    we need to release the GIL and, since interpreter calls generally acquire the
    GIL, they need to be avoided to make use of threads.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 实现对数组元素进行循环的并行版本涉及用 `prange` 替换 `range` 调用。有一个注意事项——要使用 `prange`，循环体必须是解释器无关的。如前所述，我们需要释放
    GIL，由于解释器调用通常获取 GIL，因此需要避免它们以利用线程。
- en: 'In Cython, you can release the GIL using the `nogil` context, as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Cython 中，您可以使用 `nogil` 上下文释放 GIL，如下所示：
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Alternatively, you can use the option `nogil=True` of `prange` that will automatically
    wrap the loop body in a `nogil` block:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用 `prange` 的 `nogil=True` 选项，这将自动将循环体包装在 `nogil` 块中：
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Attempts to call Python code in a `prange` block will produce an error. Prohibited
    operations include function calls, objects initialization, and so on. To enable such
    operations in a `prange` block (you may want to do so for debugging purposes),
    you have to re-enable the GIL using the `with gil` statement:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试在 `prange` 块中调用 Python 代码将产生错误。禁止的操作包括函数调用、对象初始化等。为了在 `prange` 块中启用此类操作（你可能想这样做以进行调试），你必须使用
    `with gil` 语句重新启用 GIL：
- en: '[PRE23]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can now test our code by compiling it as a Python extension module. To enable
    OpenMP support, it is necessary to change the `setup.py` file so that it includes
    the compilation option `-fopenmp` . This can be achieved by using the `distutils.extension.Extension`
    class in `distutils` and passing it to `cythonize`. The complete `setup.py` file
    is as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过将其编译为 Python 扩展模块来测试我们的代码。为了启用 OpenMP 支持，需要更改 `setup.py` 文件，使其包含编译选项
    `-fopenmp`。这可以通过在 `distutils` 中使用 `distutils.extension.Extension` 类并传递给 `cythonize`
    来实现。完整的 `setup.py` 文件如下：
- en: '[PRE24]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Using `prange`, we can easily parallelize the Cython version of our `ParticleSimulator`.
    The following code contains the `c_evolve` function of the `cevolve.pyx` Cython
    module that was written in [Chapter 4](ce893a62-a46c-4575-8163-01921cf8bb7b.xhtml), *C
    Performance with Cython*:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `prange`，我们可以轻松地将我们的 `ParticleSimulator` 的 Cython 版本并行化。以下代码包含 `cevolve.pyx`
    Cython 模块的 `c_evolve` 函数，该函数在 [第 4 章](ce893a62-a46c-4575-8163-01921cf8bb7b.xhtml)，*C
    Performance with Cython* 中编写：
- en: '[PRE25]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'First, we will invert the order of the loops so that the outermost loop will
    be executed in parallel (each iteration is independent from the other). Since
    the particles don''t interact with each other, we can change the order of iteration
    safely, as shown in the following snippet:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将反转循环的顺序，使得最外层的循环将并行执行（每个迭代都是独立的）。由于粒子之间没有相互作用，我们可以安全地改变迭代的顺序，如下面的代码片段所示：
- en: '[PRE26]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we will replace the `range` call of the outer loop with  `prange` and
    remove calls that acquire the GIL. Since our code was already enhanced with static
    types, the `nogil` option can be applied safely as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将用 `prange` 替换外层循环的 `range` 调用，并移除获取 GIL 的调用。由于我们的代码已经通过静态类型进行了增强，因此可以安全地应用
    `nogil` 选项，如下所示：
- en: '[PRE27]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can now compare the functions by wrapping them in the benchmark function
    to assess any performance improvement:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过将它们包装在基准函数中来比较这些函数，以评估任何性能改进：
- en: '[PRE28]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Interestingly, we achieved a 2x speedup by writing a parallel version using
    `prange`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，我们通过编写 `prange` 的并行版本实现了 2 倍的速度提升。
- en: Automatic parallelism
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动并行化
- en: As we mentioned earlier, normal Python programs have trouble achieving thread
    parallelism because of the GIL. So far, we worked around this problem using separate
    processes; starting a process, however, takes significantly more time and memory
    than starting a thread.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，由于 GIL，普通的 Python 程序在实现线程并行化方面有困难。到目前为止，我们通过使用单独的进程来解决这个问题；然而，启动进程比启动线程花费更多的时间和内存。
- en: We also saw that sidestepping the Python environment allowed us to achieve a
    2x speedup on an already fast Cython code. This strategy allowed us to achieve
    lightweight parallelism but required a separate compilation step. In this section,
    we will further explore this strategy using special libraries that are capable
    of automatically translating our code into a parallel version for efficient execution.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到，绕过 Python 环境使我们能够在已经很快的 Cython 代码上实现 2 倍的速度提升。这种策略使我们能够实现轻量级并行化，但需要单独的编译步骤。在本节中，我们将进一步探讨这种策略，使用能够自动将我们的代码转换为并行版本的专用库，以实现高效的执行。
- en: Examples of packages that implement automatic parallelism are the (by now) familiar
    JIT compilers  `numexpr` and Numba. Other packages have been developed to automatically
    optimize and parallelize array and matrix-intensive expressions, which are crucial
    in specific numerical and machine learning applications.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 实现自动并行化的包示例包括现在熟悉的即时编译器 `numexpr` 和 Numba。其他包已经开发出来，用于自动优化和并行化数组密集型表达式和矩阵密集型表达式，这在特定的数值和机器学习应用中至关重要。
- en: '**Theano** is a project that allows you to define a mathematical expression
    on arrays (more generally, *tensors*), and compile them to a fast language, such
    as C or C++. Many of the operations that Theano implements are parallelizable
    and can run on both CPU and GPU.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**Theano** 是一个项目，它允许你在数组（更一般地说，*张量*）上定义数学表达式，并将它们编译为快速语言，如 C 或 C++。Theano 实现的大多数操作都是可并行化的，并且可以在
    CPU 和 GPU 上运行。'
- en: '**Tensorflow** is another library that, similar to Theano, is targeted towards expression
    of array-intensive mathematical expression but, rather than translating the expressions
    to specialized C code, executes the operations on an efficient C++ engine.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**Tensorflow** 是另一个库，与 Theano 类似，旨在表达密集型数学表达式，但它不是将表达式转换为专门的 C 代码，而是在高效的 C++
    引擎上执行操作。'
- en: Both Theano and Tensorflow are ideal when the problem at hand can be expressed
    in a chain of matrix and element-wise operations (such as *neural networks*).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当手头的问题可以用矩阵和逐元素操作的链式表达来表示时（例如 *神经网络*），Theano 和 Tensorflow 都是非常理想的。
- en: Getting started with Theano
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用 Theano
- en: Theano is somewhat similar to a compiler but with the added bonuses of being
    able to express, manipulate, and optimize mathematical expressions as well as
    run code on CPU and GPU. Since 2010, Theano has improved release after release
    and has been adopted by several other Python projects as a way to automatically
    generate efficient computational models on the fly.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Theano 在某种程度上类似于编译器，但增加了能够表达、操作和优化数学表达式以及能够在 CPU 和 GPU 上运行代码的额外好处。自 2010 年以来，Theano
    在版本更新中不断改进，并被其他几个 Python 项目采用，作为在运行时自动生成高效计算模型的一种方式。
- en: In Theano, you first *define* the function you want to run by specifying variables
    and transformation using a pure Python API. This specification will then be compiled
    to machine code for execution.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Theano 中，你首先通过指定变量和转换来使用纯 Python API *定义* 你想要运行的函数。然后，这个规范将被编译成机器代码以执行。
- en: 'As a first example, let''s examine how to implement a function that computes
    the square of a number. The input will be represented by a scalar variable, `a`,
    and then we will transform it to obtain its square, indicated by `a_sq`. In the
    following code, we will use the `T.scalar` function to define the variable and
    use the normal `**` operator to obtain a new variable:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一个例子，让我们看看如何实现一个计算数字平方的函数。输入将由一个标量变量 `a` 表示，然后我们将对其进行转换以获得其平方，表示为 `a_sq`。在下面的代码中，我们将使用
    `T.scalar` 函数来定义变量，并使用正常的 `**` 运算符来获取一个新的变量：
- en: '[PRE29]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'As you can see, no specific value is computed and the transformation we apply
    is purely symbolic. In order to use this transformation, we need to generate a
    function. To compile a function, you can use the `th.function` utility that takes
    a list of the input variables as its first argument, and the output transformation
    (in our case `a_sq`) as its second argument:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，没有计算特定的值，我们应用的是纯符号转换。为了使用这个转换，我们需要生成一个函数。要编译一个函数，你可以使用 `th.function` 工具，它将输入变量的列表作为其第一个参数，输出转换（在我们的情况下是
    `a_sq`）作为其第二个参数：
- en: '[PRE30]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Theano will take some time and translate the expression to efficient C code
    and compile it, all in the background! The return value of `th.function` will
    be a ready-to-use Python function and its usage is demonstrated in the next line
    of code:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Theano 将花费一些时间将表达式转换为高效的 C 代码并编译它，所有这些都是在后台完成的！`th.function` 的返回值将是一个可用的 Python
    函数，其用法在下一行代码中演示：
- en: '[PRE31]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Unsurprisingly, `compute_square` correctly returns the input value squared.
    Note, however, that the return type is not an integer (like the input type) but
    a floating point number. This is because the Theano default variable type is `float64`.
    you can verify that by inspecting the `dtype` attribute of the `a` variable:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 令人意外的是，`compute_square` 函数正确地返回了输入值的平方。然而，请注意，返回类型不是整数（与输入类型相同），而是一个浮点数。这是因为
    Theano 的默认变量类型是 `float64`。你可以通过检查 `a` 变量的 `dtype` 属性来验证这一点：
- en: '[PRE32]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The Theano behavior is very different compared to what we saw with Numba. Theano
    doesn't compile generic Python code and, also, doesn't do any type inference;
    defining Theano functions requires a more precise specification of the types involved.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前看到的 Numba 相比，Theano 的行为非常不同。Theano 不编译通用的 Python 代码，也不进行任何类型推断；定义 Theano
    函数需要更精确地指定涉及的类型。
- en: 'The real power of Theano comes from its support for array expressions. Defining
    a one-dimensional vector can be done with the `T.vector` function; the returned
    variable supports broadcasting operations with the same semantics of NumPy arrays.
    For instance, we can take two vectors and compute the element-wise sum of their
    squares, as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Theano 的真正力量来自于其对数组表达式的支持。使用 `T.vector` 函数可以定义一维向量；返回的变量支持与 NumPy 数组相同的广播操作语义。例如，我们可以取两个向量并计算它们平方的逐元素和，如下所示：
- en: '[PRE33]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The idea is, again, to use the Theano API as a mini-language to combine various
    Numpy array expressions will be compiled to efficient machine code.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 想法再次是使用Theano API作为迷你语言来组合各种Numpy数组表达式，这些表达式将被编译成高效的机器代码。
- en: One of the selling points of Theano is its ability to perform arithmetic simplifications
    and automatic gradient calculations. For more information, refer to the official
    documentation ([http://deeplearning.net/software/theano/introduction.html](http://deeplearning.net/software/theano/introduction.html)).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Theano的一个卖点是其执行算术简化以及自动梯度计算的能力。有关更多信息，请参阅官方文档（[http://deeplearning.net/software/theano/introduction.html](http://deeplearning.net/software/theano/introduction.html)）。
- en: 'To demonstrate Theano functionality on a familiar use case, we can implement
    our parallel calculation of pi again. Our function will take a collection of two
    random coordinates as input and return the `pi` estimate. The input random numbers
    will be defined as vectors named `x` and `y`, and we can test their position inside
    the circle using standard element-wise operation that we will store in the `hit_test`
    variable:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示Theano在熟悉的使用场景中的功能，我们可以再次实现我们的π的并行计算。我们的函数将接受两个随机坐标的集合作为输入，并返回π的估计值。输入的随机数将被定义为名为`x`和`y`的向量，我们可以使用标准元素级操作来测试它们在圆内的位置，这些操作我们将存储在`hit_test`变量中：
- en: '[PRE34]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'At this point, we need to count the number of `True` elements in `hit_test`,
    which can be done taking its sum (it will be implicitly cast to integer).  To
    obtain the pi estimate, we finally need to calculate the ratio of hits versus
    the total number of trials. The calculation is illustrated in the following code
    block:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们需要计算`hit_test`中`True`元素的数量，这可以通过取其和来完成（它将被隐式转换为整数）。为了获得π的估计值，我们最终需要计算击中次数与总试验次数的比率。计算过程在下面的代码块中展示：
- en: '[PRE35]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can benchmark the execution of the Theano implementation using `th.function`
    and the `timeit` module. In our test, we will pass two arrays of size 30,000 and
    use the `timeit.timeit` utility to execute the `calculate_pi` function multiple
    times:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`th.function`和`timeit`模块来基准测试Theano实现的执行。在我们的测试中，我们将传递两个大小为30,000的数组，并使用`timeit.timeit`实用程序多次执行`calculate_pi`函数：
- en: '[PRE36]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The serial execution of this function takes about 10 seconds. Theano is capable
    of automatically parallelizing the code by implementing element-wise and matrix
    operations using specialized packages, such as OpenMP and the **Basic Linear Algebra
    Subprograms** (**BLAS**) linear algebra routines. Parallel execution can be enabled
    using configuration options.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数的串行执行大约需要10秒。Theano能够通过实现元素级和矩阵操作使用专门的包（如OpenMP和**基本线性代数子程序**（**BLAS**）线性代数例程）来自动并行化代码。可以通过配置选项启用并行执行。
- en: 'In Theano, you can set up configuration options by modifying variables in the
    `theano.config` object at import time. For example, you can issue the following
    commands to enable OpenMP support:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在Theano中，你可以在导入时通过修改`theano.config`对象中的变量来设置配置选项。例如，你可以发出以下命令来启用OpenMP支持：
- en: '[PRE37]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The parameters relevant to OpenMP are as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 与OpenMP相关的参数如下：
- en: '`openmp_elemwise_minsize`: This is an integer number that represents the minimum
    size of the arrays where element-wise parallelization should be enabled (the overhead
    of the parallelization can harm performance for small arrays)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`openmp_elemwise_minsize`：这是一个整数，表示应该启用元素级并行化的数组的最小大小（对于小数组，并行化的开销可能会损害性能）'
- en: '`openmp`: This is a Boolean flag that controls the activation of OpenMP compilation
    (it should be activated by default)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`openmp`：这是一个布尔标志，用于控制OpenMP编译的激活（默认情况下应该被激活）'
- en: Controlling the number of threads assigned for OpenMP execution can be done
    by setting the `OMP_NUM_THREADS` environmental variable before executing the code.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在执行代码之前设置`OMP_NUM_THREADS`环境变量，可以控制分配给OpenMP执行的线程数量。
- en: 'We can now write a simple benchmark to demonstrate the OpenMP usage in practice.
    In a file `test_theano.py`, we will put the complete code for the pi estimation example:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以编写一个简单的基准测试来演示实际中的OpenMP使用。在一个名为`test_theano.py`的文件中，我们将放置π估计示例的完整代码：
- en: '[PRE38]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'At this point, we can run the code from the command line and assess the scaling
    with an increasing number of threads by setting the `OMP_NUM_THREADS` environment
    variable:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们可以从命令行运行代码，并通过设置`OMP_NUM_THREADS`环境变量来评估线程数量增加时的扩展性：
- en: '[PRE39]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Interestingly, there is a small speedup when using two threads, but the performance
    degrades quickly as we increase their number. This means that for this input size,
    it is not advantageous to use more than two threads as the price you pay to start
    new threads and synchronize their shared data is higher than the speedup that
    you can obtain from the parallel execution.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，当使用两个线程时，会有轻微的加速，但随着线程数量的增加，性能会迅速下降。这意味着对于这个输入大小，使用超过两个线程并不有利，因为启动新线程和同步共享数据所付出的代价高于从并行执行中获得的加速。
- en: Achieving good parallel performance can be tricky as it will depend on the specific
    operations and how they access the underlying data. As a general rule, measuring
    the performance of a parallel program is crucial and obtaining substantial speedups is
    a work of trial and error.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 实现良好的并行性能可能相当棘手，因为它将取决于特定的操作以及它们如何访问底层数据。一般来说，衡量并行程序的性能至关重要，而获得显著的加速则是一项试错的工作。
- en: 'As an example, we can see that the parallel performance quickly degrades using
    a slightly different code. In our hit test, we used the `sum` method directly
    and relied on the explicit casting of the `hit_tests` Boolean array. If we make
    the cast explicit, Theano will generate a slightly different code that benefits
    less from multiple threads. We can modify the `test_theano.py` file to verify
    this effect:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以看到使用略微不同的代码，并行性能会迅速下降。在我们的击中测试中，我们直接使用了 `sum` 方法，并依赖于对 `hit_tests` 布尔数组的显式转换。如果我们进行显式转换，Theano
    将生成略微不同的代码，从多个线程中获得的益处较少。我们可以修改 `test_theano.py` 文件来验证这一效果：
- en: '[PRE40]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'If we rerun our benchmark, we see that the number of threads does not affect
    the running time significantly. Despite that, the timings improved considerably
    as compared to the original version:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们重新运行我们的基准测试，我们会看到线程数量对运行时间没有显著影响。尽管如此，与原始版本相比，时间有所显著改善：
- en: '[PRE41]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Profiling Theano
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析 Theano
- en: 'Given the importance of measuring and analyzing performance, Theano provides
    powerful and informative profiling tools. To generate profiling data, the only
    modification needed is the addition of the `profile=True` option to `th.function`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到衡量和分析性能的重要性，Theano 提供了强大且信息丰富的分析工具。要生成分析数据，只需在 `th.function` 中添加 `profile=True`
    选项即可：
- en: '[PRE42]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The profiler will collect data as the function is being run (for example, through
    `timeit` or direct invocation). The profiling summary can be printed to output
    by issuing the `summary` command, as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 分析器将在函数运行时收集数据（例如，通过 `timeit` 或直接调用）。可以通过发出 `summary` 命令将分析摘要打印到输出，如下所示：
- en: '[PRE43]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: To generate profiling data, we can rerun our script after adding the `profile=True`
    option (for this experiment, we will set the `OMP_NUM_THREADS` environmental variable
    to 1). Also, we will revert our script to the version that performed the casting
    of `hit_tests` implicitly.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成分析数据，我们可以在添加 `profile=True` 选项后重新运行我们的脚本（对于这个实验，我们将 `OMP_NUM_THREADS` 环境变量设置为
    1）。此外，我们将我们的脚本恢复到执行 `hit_tests` 隐式转换的版本。
- en: You can also set up profiling globally using the `config.profile` option.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以使用 `config.profile` 选项全局设置分析。
- en: 'The output printed by `calculate_pi.profile.summary()` is quite long and informative.
    A part of it is reported in the next block of text. The output is comprised of
    three sections that refer to timings sorted by `Class`, `Ops`, and `Apply`. In
    our example, we are concerned with `Ops`, which roughly maps to the functions
    used in the Theano compiled code. As you can see, roughly 80% of the time is spent
    in taking the element-wise square and sum of the two numbers, while the rest of
    the time is spent calculating the sum:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`calculate_pi.profile.summary()` 打印的输出相当长且信息丰富。其中一部分在下一块文本中报告。输出由三个部分组成，分别按
    `Class`、`Ops` 和 `Apply` 排序。在我们的例子中，我们关注的是 `Ops`，它大致对应于 Theano 编译代码中使用的函数。正如你所见，大约
    80% 的时间用于对两个数字进行逐元素平方和求和，其余时间用于计算总和：'
- en: '[PRE44]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: This information is consistent with what was found in our first benchmark. The
    code went from about 11 seconds to roughly 8 seconds when two threads were used.
    From these numbers, we can analyze how the time was spent.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息与我们第一次基准测试的结果一致。当使用两个线程时，代码从大约 11 秒减少到大约 8 秒。从这些数字中，我们可以分析时间是如何被花费的。
- en: Out of these 11 seconds, 80% of the time (about 8.8 seconds) was spent doing
    element-wise operations. This means that, in perfectly parallel conditions, the
    increase in speed by adding two threads will be 4.4 seconds. In this scenario,
    the theoretical execution time will be 6.6 seconds. Considering that we obtained
    a timing of about 8 seconds, it looks like there is some extra overhead (1.4 seconds)
    for the thread usage.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这 11 秒中，80% 的时间（大约 8.8 秒）用于执行元素级操作。这意味着，在完全并行的情况下，增加两个线程将使速度提高 4.4 秒。在这种情况下，理论上的执行时间将是
    6.6 秒。考虑到我们获得了大约 8 秒的计时，看起来线程使用有一些额外的开销（1.4 秒）。
- en: Tensorflow
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tensorflow
- en: Tensorflow is another library designed for fast numerical calculations and automatic parallelism.
    It was released as an open source project by Google in 2015\. Tensorflow works
    by building mathematical expressions similar to Theano, except that the computation
    is not compiled to machine code but is executed on an external engine written
    in C++. Tensorflow supports execution and deployment of parallel codes on one
    or more CPUs and GPUs.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Tensorflow 是另一个用于快速数值计算和自动并行化的库。它于 2015 年由 Google 以开源项目形式发布。Tensorflow 通过构建类似于
    Theano 的数学表达式来工作，但计算不是编译成机器代码，而是在用 C++ 编写的外部引擎上执行。Tensorflow 支持在一台或多台 CPU 和 GPU
    上执行和部署并行代码。
- en: 'The usage of Tensorflow is quite similar to that of Theano. To create a variable
    in Tensorflow, you can use the `tf.placeholder` function that takes a data type
    as input:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Tensorflow 的使用方式与 Theano 非常相似。要在 Tensorflow 中创建一个变量，你可以使用 `tf.placeholder` 函数，该函数接受一个数据类型作为输入：
- en: '[PRE45]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Tensorflow mathematical expressions can be expressed quite similarly to Theano,
    except for a few different naming conventions as well as a more restricted support
    for the NumPy semantics.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Tensorflow 的数学表达式可以非常类似于 Theano，除了命名约定略有不同以及对于 NumPy 语义的支持更加有限。
- en: Tensorflow doesn't compile functions to C and then machine code like Theano,
    but serializes the defined mathematical functions (the data structure containing
    variables and transformations is called **computation graph**) and executes them
    on specific devices. The configuration of devices and context can be done using
    the `tf.Session` object.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Tensorflow 并不像 Theano 那样将函数编译成 C 语言和机器代码，而是将定义的数学函数（包含变量和转换的数据结构称为 **计算图**）序列化，并在特定设备上执行。设备配置和上下文可以通过
    `tf.Session` 对象来完成。
- en: 'Once the desired expression is defined, a `tf.Session` needs to be initialized
    and can be used to execute computation graphs using the `Session.run` method.
    In the following example, we demonstrate the usage of the Tensorflow API to implement a
    simple element-wise sum of squares:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了所需的表达式，就需要初始化一个 `tf.Session`，并可以使用 `Session.run` 方法来执行计算图。在以下示例中，我们展示了如何使用
    Tensorflow API 实现一个简单的元素级平方和：
- en: '[PRE46]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Parallelism in Tensorflow is achieved automatically by its smart execution engine,
    and it generally works well without much fiddling. However, note that it is mostly
    suited for deep learning workloads that involve the definition of complex functions
    that use a lot of matrix multiplications and calculate their gradient.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Tensorflow 的并行性是通过其智能执行引擎自动实现的，通常无需过多调整就能很好地工作。然而，请注意，它主要适用于涉及定义复杂函数（使用大量矩阵乘法并计算其梯度）的深度学习工作负载。
- en: 'We can now replicate the estimation of the pi example using Tensorflow capabilities
    and benchmark its execution speed and parallelism against the Theano implementation.
    What we will do is this:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 Tensorflow 的功能来复制 pi 的估计示例，并对其执行速度和并行性进行基准测试，与 Theano 实现进行比较。我们将这样做：
- en: Define our `x` and `y` variables and perform a hit test using broadcasted operations.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义我们的 `x` 和 `y` 变量，并使用广播操作进行碰撞测试。
- en: Calculate the sum of `hit_tests` using the `tf.reduce_sum` function.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `tf.reduce_sum` 函数计算 `hit_tests` 的总和。
- en: Initialize a `Session` object with the `inter_op_parallelism_threads` and `intra_op_parallelism_threads` configuration
    options. These options control the number of threads used for different classes of
    parallel operations. Note that the first `Session` created with such options sets
    the number of threads for the whole script (even future `Session` instances).
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `inter_op_parallelism_threads` 和 `intra_op_parallelism_threads` 配置选项初始化一个
    `Session` 对象。这些选项控制不同类别的并行操作使用的线程数。请注意，使用这些选项创建的第一个 `Session` 将设置整个脚本（甚至未来的 `Session`
    实例）的线程数。
- en: 'We can now write a script name, `test_tensorflow.py`, containing the following
    code. Note that the number of threads is passed as the first argument of the script
    (`sys.argv[1]`):'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以编写一个名为`test_tensorflow.py`的脚本，其中包含以下代码。请注意，线程数作为脚本的第一个参数传递（`sys.argv[1]`）：
- en: '[PRE47]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'If we run the script multiple times with different values of `NUM_THREADS`,
    we see that the performance is quite similar to Theano and that the speedup increased
    by parallelization is quite modest:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们多次运行脚本，并使用不同的`NUM_THREADS`值，我们会看到性能与Theano相当，并且通过并行化提高的速度提升相当有限：
- en: '[PRE48]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The main advantage of using software packages such as Tensorflow and Theano
    is the support for parallel matrix operations that are commonly used in machine
    learning algorithms. This is very effective because those operations can achieve
    impressive performance gains on GPU hardware that is designed to perform these
    operations with high throughput.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Tensorflow和Theano等软件包的主要优势是支持在机器学习算法中常用到的并行矩阵运算。这非常有效，因为这些操作可以在专门为以高吞吐量执行这些操作而设计的GPU硬件上实现令人印象深刻的性能提升。
- en: Running code on a GPU
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在GPU上运行代码
- en: In this subsection, we will demonstrate the usage of a GPU with Theano and Tensorflow.
    As an example, we will benchmark the execution of a very simple matrix multiplication
    on the GPU and compare it to its running time on a CPU.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将演示使用Theano和Tensorflow的GPU使用方法。作为一个例子，我们将基准测试GPU上非常简单的矩阵乘法执行时间，并将其与CPU上的运行时间进行比较。
- en: The code in this subsection requires the possession of a GPU. For learning purposes,
    it is possible to use the Amazon EC2 service ([https://aws.amazon.com/ec2](https://aws.amazon.com/ec2))
    to request a GPU-enabled instance.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节中的代码需要具备GPU。为了学习目的，可以使用Amazon EC2服务([https://aws.amazon.com/ec2](https://aws.amazon.com/ec2))来请求一个启用GPU的实例。
- en: 'The following code performs a simple matrix multiplication using Theano. We
    use the `T.matrix` function to initialize a two-dimensional array, and then we
    use the `T.dot` method to perform the matrix multiplication:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码使用Theano执行简单的矩阵乘法。我们使用`T.matrix`函数初始化一个二维数组，然后使用`T.dot`方法执行矩阵乘法：
- en: '[PRE49]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'It is possible to ask Theano to execute this code on a GPU by setting the `config.device=gpu` option.
    For added convenience, we can set up the configuration value from the command
    line using the `THEANO_FLAGS` environmental variable, shown as follows. After
    copying the previous code in the `test_theano_matmul.py` file, we can benchmark
    the execution time by issuing the following command:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过设置`config.device=gpu`选项让Theano在GPU上执行此代码。为了方便起见，我们可以使用`THEANO_FLAGS`环境变量从命令行设置配置值，如下所示。在将前面的代码复制到`test_theano_matmul.py`文件后，我们可以通过以下命令来基准测试执行时间：
- en: '[PRE50]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We can analogously run the same code on the CPU using the `device=cpu` configuration
    option:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`device=cpu`配置选项在CPU上类似地运行相同的代码：
- en: '[PRE51]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: As you can see, the GPU is 7.2 times faster than the CPU version for this example!
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在这个例子中，GPU比CPU版本快7.2倍！
- en: 'For comparison, we may benchmark equivalent code using Tensorflow. The implementation
    of a Tensorflow version is reported in the next code snippet. The main differences
    with the Theano version are as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行比较，我们可以使用Tensorflow基准测试等效代码。Tensorflow版本的实现将在下一个代码片段中报告。与Theano版本的主要区别如下：
- en: The usage of the `tf.device` config manager that serves to specify the target
    device (`/cpu:0` or `/gpu:0`)
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.device`配置管理器的使用，用于指定目标设备（`/cpu:0`或`/gpu:0`）'
- en: 'The matrix multiplication is performed using the `tf.matmul` operator:'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵乘法是通过`tf.matmul`运算符来执行的：
- en: '[PRE52]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'If we run the `test_tensorflow_matmul.py` script with the appropriate `tf.device`
    option, we obtain the following timings:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用适当的`tf.device`选项运行`test_tensorflow_matmul.py`脚本，我们将获得以下计时结果：
- en: '[PRE53]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: As you can see, the performance gain is substantial (but not as good as the
    Theano version) in this simple case.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在这个简单案例中，性能提升相当显著（但不如Theano版本好）。
- en: Another way to achieve automatic GPU computation is the now familiar Numba.
    With Numba, it is possible to compile Python code to programs that can be run
    on a GPU. This flexibility allows for advanced GPU programming as well as more
    simplified interfaces. In particular, Numba makes extremely easy-to-write, GPU-ready,
    generalized universal functions.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 实现自动GPU计算的另一种方法是现在熟悉的Numba。使用Numba，可以将Python代码编译成可以在GPU上运行的程序。这种灵活性允许进行高级GPU编程以及更简化的接口。特别是，Numba使得编写GPU就绪的通用函数变得极其简单。
- en: 'In the next example, we will demonstrate how to write a universal function
    that applies an exponential function on two numbers and sums the results. As we
    already saw in [Chapter 5](3797e4bb-99b8-4ba2-bb17-f757078b1d2b.xhtml), *Exploring
    Compilers* this can be accomplished using the `nb.vectorize` function (we''ll
    also specify the `cpu` target explicitly):'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，我们将演示如何编写一个通用函数，该函数对两个数字应用指数函数并求和结果。正如我们已经在[第5章](3797e4bb-99b8-4ba2-bb17-f757078b1d2b.xhtml)中看到的，*探索编译器*，这可以通过使用`nb.vectorize`函数（我们还将明确指定`cpu`目标）来实现：
- en: '[PRE54]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The `expon_cpu` universal function can be compiled for the GPU device using
    the `target=''cuda''` option. Also, note that it is necessary to specify the input
    types for CUDA universal functions. The implementation of `expon_gpu` is as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`target='cuda'`选项可以将`expon_cpu`通用函数编译为GPU设备。此外，请注意，对于CUDA通用函数，必须指定输入类型。`expon_gpu`的实现如下：
- en: '[PRE55]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We can now benchmark the execution of the two functions by applying the functions
    on two arrays of size 1,000,000\. Also, note that we execute the function before
    measuring the timings to trigger the Numba just-in-time compilation:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过在两个大小为1,000,000的数组上应用这两个函数来基准测试这两个函数的执行。请注意，我们在测量时间之前执行函数以触发Numba即时编译：
- en: '[PRE56]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Thanks to the GPU execution, we were able to achieve a 3x speedup over the CPU
    version. Note that transferring data on the GPU is quite expensive; therefore,
    GPU execution becomes advantageous only for very large arrays.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了GPU执行，我们能够将CPU版本的速度提高3倍。请注意，在GPU上传输数据相当昂贵；因此，只有对于非常大的数组，GPU执行才具有优势。
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Parallel processing is an effective way to improve performance on large datasets.
    Embarrassingly parallel problems are excellent candidates for parallel execution
    that can be easily implemented to achieve good performance scaling.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 并行处理是提高大数据集性能的有效方法。令人尴尬的并行问题是非常好的并行执行候选者，可以轻松实现以实现良好的性能扩展。
- en: In this chapter, we illustrated the basics of parallel programming in Python.
    We learned how to circumvent Python threading limitation by spawning processes
    using the tools available in the Python standard library. We also explored how
    to implement a multithreaded program using Cython and OpenMP.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了Python并行编程的基础。我们学习了如何通过使用Python标准库中的工具来生成进程来规避Python线程限制。我们还探讨了如何使用Cython和OpenMP实现多线程程序。
- en: For more complex problems, we learned how to use the Theano, Tensorflow, and
    Numba packages to automatically compile array-intensive expressions for parallel
    execution on CPU and GPU devices.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的问题，我们学习了如何使用Theano、Tensorflow和Numba包自动编译针对CPU和GPU设备并行执行的密集数组表达式。
- en: In the next chapter, we will learn how to write and execute parallel programs
    on multiple processors and machines using libraries such as dask and PySpark.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何使用dask和PySpark等库在多个处理器和机器上编写和执行并行程序。
