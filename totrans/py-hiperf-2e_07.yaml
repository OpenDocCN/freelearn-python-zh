- en: Parallel Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With parallel processing by using multiple cores, you can increase the amount
    of calculations your program can do in a given time frame without needing a faster
    processor. The main idea is to divide a problem into independent subunits and
    use multiple cores to solve those subunits in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel processing is necessary to tackle large-scale problems. Companies produce
    massive quantities of data every day that need to be stored in multiple computers
    and analyzed. Scientists and engineers run parallel code on supercomputers to
    simulate massive systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parallel processing allows you to take advantage of multicore CPUs as well
    as GPUs that work extremely well with highly parallel problems. In this chapter,
    we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: A brief introduction to the fundamentals of parallel processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Illustrating how to parallelize simple problems with the `multiprocessing` Python
    library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the simple `ProcessPoolExecutor` interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelizing our programs using multithreading with the help of Cython and
    OpenMP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achieving parallelism automatically with Theano and Tensorflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing code on a GPU with Theano, Tensorflow, and Numba
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to parallel programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to parallelize a program, it is necessary to divide the problem into
    subunits that can run independently (or almost independently) from each other.
  prefs: []
  type: TYPE_NORMAL
- en: A problem where the subunits are totally independent from each other is called **embarrassingly parallel**.
    An element-wise operation on an array is a typical example--the operation needs
    to only know the element it is handling at the moment. Another example is our
    particle simulator. Since there are no interactions, each particle can evolve
    independently from the others. Embarrassingly parallel problems are very easy
    to implement and perform very well on parallel architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Other problems may be divided into subunits but have to share some data to perform
    their calculations. In those cases, the implementation is less straightforward
    and can lead to performance issues because of the communication costs.
  prefs: []
  type: TYPE_NORMAL
- en: We will illustrate the concept with an example. Imagine that you have a particle
    simulator, but this time the particles attract other particles within a certain
    distance (as shown in the following figure). To parallelize this problem, we divide
    the simulation box into regions and assign each region to a different processor.
    If we evolve the system one step at a time, some particles will interact with
    particles in a neighboring region. To perform the next iteration, communication
    with the new particle positions of the neighboring region is required.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06440_07CHPNO_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Communication between processes is costly and can seriously hinder the performance
    of parallel programs. There exist two main ways to handle data communication in
    parallel programs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Shared memory**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed memory**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In shared memory, the subunits have access to the same memory space. The advantage
    of this approach is that you don't have to explicitly handle the communication
    as it is sufficient to write or read from the shared memory. However, problems
    arise when multiple processes try to access and change the same memory location
    at the same time. Care should be taken to avoid such conflict using synchronization
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In the distributed memory model, each process is completely separated from the
    others and possesses its own memory space. In this case, communication is handled
    explicitly between the processes. The communication overhead is typically costlier
    compared to shared memory as data can potentially travel through a network interface.
  prefs: []
  type: TYPE_NORMAL
- en: One common way to achieve parallelism with the shared memory model is **threads**.
    Threads are independent subtasks that originate from a process and share resources,
    such as memory. This concept is further illustrated in the following figure. Threads
    produce multiple execution context and share the same memory space, while processes
    provide multiple execution context that possess their own memory space and communication
    has to be handled explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06440_07CHPNO_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Python can spawn and handle threads, but they can't be used to increase performance;
    due to the Python interpreter design, only one Python instruction is allowed to
    run at a time--this mechanism is called **Global Interpreter Lock** (**GIL**).
    What happens is that each time a thread executes a Python statement, the thread
    acquires a lock and, when the execution is completed, the same lock is released.
    Since the lock can be acquired only by one thread at a time, other threads are
    prevented from executing Python statements while some other thread holds the lock.
  prefs: []
  type: TYPE_NORMAL
- en: Even though the GIL prevents parallel execution of Python instructions, threads
    can still be used to provide concurrency in situations where the lock can be released,
    such as in time-consuming I/O operations or in C extensions.
  prefs: []
  type: TYPE_NORMAL
- en: Why not remove the GIL? In past years, many attempts have been made, including
    the most recent gilectomy experiment. First, removing the GIL is not an easy task
    and requires modification of most of the Python data structures. Additionally,
    such fine-grained locking can be costly and may introduce substantial performance
    loss in single-threaded programs. Despite this, some Python implementations (notable
    examples are Jython and IronPython) do not use the GIL.
  prefs: []
  type: TYPE_NORMAL
- en: 'The GIL can be completely sidestepped using processes instead of threads. Processes
    don''t share the same memory area and are independent from each other--each process
    has its own interpreter. Processes have a few disadvantages: starting up a new
    process is generally slower than starting a new thread, they consume more memory,
    and inter-process communication can be slow. On the other hand, processes are
    still very flexible, and they scale better as they can be distributed on multiple
    machines.'
  prefs: []
  type: TYPE_NORMAL
- en: Graphic processing units
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Graphic processing units are special processors designed for computer graphics
    applications. Those applications usually require processing the geometry of a
    3D scene and output an array of pixel to the screen. The operations performed
    by GPUs involve array and matrix operations on floating point numbers.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs are designed to run this graphics-related operation very efficiently, and
    they achieve this by adopting a highly parallel architecture. Compared to a CPU,
    a GPU has many more (thousands) of small processing units. GPUs are intended to
    produce data at about 60 frames per second, which is much slower than the typical
    response time of a CPU, which possesses higher clock speeds.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs possess a very different architecture from a standard CPU and are specialized
    for computing floating point operations. Therefore, to compile programs for GPUs,
    it is necessary to utilize special programming platforms, such as CUDA and OpenCL.
  prefs: []
  type: TYPE_NORMAL
- en: '**Compute Unified Device Architecture** (**CUDA**) is a proprietary NVIDIA
    technology. It provides an API that can be accessed from other languages. CUDA
    provides the NVCC tool that can be used to compile GPU programs written in a language
    similar to C (CUDA C) as well as numerous libraries that implement highly optimized
    mathematical routines.'
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenCL** is an open technology with the ability of writing parallel programs
    that can be compiled for a variety of target devices (CPUs and GPUs of several
    vendors) and is a good option for non-NVIDIA devices.'
  prefs: []
  type: TYPE_NORMAL
- en: GPU programming sounds wonderful on paper. However, don't throw away your CPU
    yet. GPU programming is tricky and only specific use cases benefit from the GPU
    architecture. Programmers need to be aware of the costs incurred in memory transfers
    to and from the main memory and how to implement algorithms to take advantage
    of the GPU architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, GPUs are great at increasing the amount of operations you can perform
    per unit of time (also called **throughput**); however, they require more time
    to prepare the data for processing. In contrast, CPUs are much faster at producing
    an individual result from scratch (also called **latency**).
  prefs: []
  type: TYPE_NORMAL
- en: For the right problem, GPUs provide extreme (10 to 100 times) speedup. For this
    reason, they often constitute a very inexpensive (the same speedup will require
    hundreds of CPUs) solution to improve the performance of numerically intensive applications.
    We will illustrate how to execute some algorithms on a GPU in the *Automatic Parallelism*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Using multiple processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The standard `multiprocessing` module can be used to quickly parallelize simple
    tasks by spawning several processes, while avoiding the GIL problem. Its interface
    is easy to use and includes several utilities to handle task submission and synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: The Process and Pool classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can create a process that runs independently by subclassing `multiprocessing.Process`.
    You can extend the `__init__` method to initialize resources, and you can write
    the portion of the code that will be executed in a subprocess by implementing
    the `Process.run` method. In the following code, we define a `Process` class that
    will wait for one second and print its assigned `id`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To spawn the process, we have to instantiate the `Process` class and call the
    `Process.start` method. Note that you don''t directly call `Process.run`; the
    call to `Process.start` will create a new process that, in turn, will call the
    `Process.run` method. We can add the following lines at the end of the preceding snippet
    to create and start the new process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The instructions after `Process.start` will be executed immediately without
    waiting for the `p` process to finish. To wait for the task completion, you can
    use the `Process.join` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can launch four different processes that will run parallely in the same
    way. In a serial program, the total required time will be four seconds. Since
    the execution is concurrent, the resulting wallclock time will be of one second.
    In the following code, we create four processes that will execute concurrently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that the order of the execution for parallel processes is unpredictable
    and ultimately depends on how the OS schedules their execution. You can verify
    this behavior by executing the program multiple times; the order will likely be
    different between runs.
  prefs: []
  type: TYPE_NORMAL
- en: The `multiprocessing` module exposes a convenient interface that makes it easy
    to assign and distribute tasks to a set of processes that reside in the `multiprocessing.Pool`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: The `multiprocessing.Pool` class spawns a set of processes--called **workers**--and
    lets us submit tasks through the `apply`/`apply_async` and `map`/`map_async` methods.
  prefs: []
  type: TYPE_NORMAL
- en: The `Pool.map` method applies a function to each element of a list and returns
    the list of results. Its usage is equivalent to the built-in (serial) `map`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use a parallel map, you should first initialize a `multiprocessing.Pool`
    object. It takes the number of workers as its first argument; if not provided,
    that number will be equal to the number of cores in the system. You can initialize
    a `multiprocessing.Pool` object in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see `pool.map` in action. If you have a function that computes the square
    of a number, you can map the function to the list by calling `Pool.map` and passing
    the function and the list of inputs as arguments, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Pool.map_async` function is just like `Pool.map` but returns an `AsyncResult`
    object instead of the actual result. When we call  `Pool.map`, the execution of
    the main program is stopped until all the workers are finished processing the
    result. With `map_async`, the `AsyncResult` object is returned immediately without
    blocking the main program and the calculations are done in the background. We
    can then retrieve the result using the `AsyncResult.get` method at any time, as
    shown in the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`Pool.apply_async` assigns a task consisting of a single function to one of
    the workers. It takes the function and its arguments and returns an `AsyncResult`
    object. We can obtain an effect similar to `map` using `apply_async`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The Executor interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From version 3.2 onward, it is possible to execute Python code in parallel using the
    `Executor` interface provided in the `concurrent.futures` module. We already saw
    the `Executor` interface in action in the previous chapter, when we used `ThreadPoolExecutor`
    to perform multiple tasks concurrently. In this subsection, we'll demonstrate
    the usage of the `ProcessPoolExecutor` class.
  prefs: []
  type: TYPE_NORMAL
- en: '`ProcessPoolExecutor` exposes a very lean interface, at least when compared
    to the more featureful `multiprocessing.Pool`. A `ProcessPoolExecutor` can be
    instantiated, similar to `ThreadPoolExecutor`, by passing a number of worker threads
    using the `max_workers` argument (by default, `max_workers` will be the number
    of CPU cores available). The main methods available to the `ProcessPoolExecutor` are
    `submit` and `map`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `submit` method will take a function and return a `Future` (see the last chapter)
    that will keep track of the execution of the submitted function. The method map
    works similarly to the `Pool.map` function, except that it returns an iterator
    rather than a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To extract the result from one or more `Future` instances, you can use the
    `concurrent.futures.wait` and `concurrent.futures.as_completed` functions. The
    `wait` function accepts a list of `future` and will block the execution of the
    programs until all the futures have completed their execution. The result can
    then be extracted using the `Future.result` method. The `as_completed` function
    also accepts a function but will, instead, return an iterator over the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, you can generate futures using the `asyncio.run_in_executor`
    function and manipulate the results using all the tools and syntax provided by
    the `asyncio` libraries so that you can achieve concurrency and parallelism at
    the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo approximation of pi
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As an example, we will implement a canonical, embarrassingly parallel program--the
    **Monte Carlo approximation of pi**. Imagine that we have a square of size 2 units;
    its area will be 4 units. Now, we inscribe a circle of 1 unit radius in this square;
    the area of the circle will be *pi * r^2*. By substituting the value of *r* in
    the previous equation, we get that the numerical value for the area of the circle
    is *pi * (1)^2 = pi*. You can refer to the following figure for a graphical representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we shoot a lot of random points on this figure, some points will fall into
    the circle, which we''ll call **hits, **while the remaining points, **misses, **will
    be outside the circle. The area of the circle will be proportional to the number
    of hits, while the area of the square will be proportional to the total number
    of shots. To get the value of *pi*, it is sufficient to divide the area of the
    circle (equal to *pi*) by the area of the square (equal to 4):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/image_07_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The strategy we will employ in our program will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate a lot of uniformly random (*x*, *y*) numbers in the range (**-1**,
    **1**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test whether those numbers lie inside the circle by checking whether *x**2 +
    y**2* <= *1*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first step when writing a parallel program is to write a serial version
    and verify that it works. In a real-world scenario, you also want to leave the
    parallelization as the last step of your optimization process. First, we need
    to identify the slow parts, and second, parallelization is time-consuming and
    gives you *at most* a speedup equal to the number of processors. The implementation
    of the serial program is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The accuracy of our approximation will improve as we increase the number of
    samples. You can note that each loop iteration is independent from the other--this
    problem is embarrassingly parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'To parallelize this code, we can write a function, called `sample`, that corresponds
    to a single hit-miss check. If the sample hits the circle, the function will return
    `1`; otherwise, it will return `0`. By running `sample` multiple times and summing
    the results, we''ll get the total number of hits. We can run `sample` over multiple
    processors with `apply_async` and get the results in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can wrap the two versions in the `pi_serial` and `pi_apply_async` functions
    (you can find their implementation in the `pi.py` file) and benchmark the execution
    speed, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the earlier benchmark, our first parallel version literally cripples
    our code. The reason is that the time spent doing the actual calculation is small
    compared to the overhead required to send and distribute the tasks to the workers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve the issue, we have to make the overhead negligible compared to the
    calculation time. For example, we can ask each worker to handle more than one
    sample at a time, thus reducing the task communication overhead. We can write
    a `sample_multiple` function that processes more than one hit and modifies our
    parallel version by dividing our problem by 10; more intensive tasks are shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can wrap this in a function called `pi_apply_async_chunked` and run it as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The results are much better; we more than doubled the speed of our program.
    You can also notice that the `user` metric is larger than `real`; the total CPU
    time is larger than the total time because more than one CPU worked at the same
    time. If you increase the number of samples, you will note that the ratio of communication
    to calculation decreases, giving even better speedups.
  prefs: []
  type: TYPE_NORMAL
- en: Everything is nice and simple when dealing with embarrassingly parallel problems.
    However, sometimes you have to share data between processes.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronization and locks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even if `multiprocessing` uses processes (with their own independent memory),
    it lets you define certain variables and arrays as shared memory. You can define
    a shared variable using `multiprocessing.Value`, passing its data type as a string
    (`i` integer, `d` double, `f` float, and so on). You can update the content of
    the variable through the `value` attribute, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'When using shared memory, you should be aware of concurrent accesses. Imagine
    that you have a shared integer variable and each process increments its value
    multiple times. You will define a process class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You can initialize the shared variable in the main program and pass it to `4`
    processes, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: If you run this program (`shared.py` in the code directory), you will note that
    the final value of `counter` is not 4000, but it has random values (on my machine,
    they are between 2000 and 2500). If we assume that the arithmetic is correct,
    we can conclude that there's a problem with the parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: What happens is that multiple processes are trying to access the same shared
    variable at the same time. The situation is best explained by looking at the following
    figure. In a serial execution, the first process reads (the number `0`), increments
    it, and writes the new value (`1`); the second process reads the new value (`1`),
    increments it, and writes it again (`2`).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the parallel execution, the two processes read (`0`), increment it, and
    write the value (`1`) at the same time, leading to a wrong answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06440_07CHPNO_04.png)'
  prefs: []
  type: TYPE_IMG
- en: To solve this problem, we need to synchronize the access to this variable so
    that only one process at a time can access, increment, and write the value on
    the shared variable. This feature is provided by the `multiprocessing.Lock` class.
    A lock can be acquired and released through the `acquire` method and `release`,
    or using the lock as a context manager. Since the lock can be acquired by only
    one process at a time, this method prevents multiple processes from executing
    the protected section of code at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define a global lock and use it as a context manager to restrict the
    access to the counter, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Synchronization primitives, such as locks, are essential to solve many problems,
    but they should be kept to a minimum to improve the performance of your program.
  prefs: []
  type: TYPE_NORMAL
- en: The `multiprocessing` module includes other communication and synchronization
    tools; you can refer to the official documentation at [http://docs.python.org/3/library/multiprocessing.html](http://docs.python.org/3/library/multiprocessing.html) for
    a complete reference.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel Cython with OpenMP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cython provides a convenient interface to perform shared-memory parallel processing
    through **OpenMP**. This lets you write extremely efficient parallel code directly
    in Cython without having to create a C wrapper.
  prefs: []
  type: TYPE_NORMAL
- en: OpenMP is a specification and an API designed to write multithreaded, parallel
    programs. The OpenMP specification includes a series of C preprocessor directives
    to manage threads and provides communication patterns, load balancing, and other
    synchronization features. Several C/C++ and Fortran compilers (including GCC)
    implement the OpenMP API.
  prefs: []
  type: TYPE_NORMAL
- en: We can introduce the Cython parallel features with a small example. Cython provides
    a simple API based on OpenMP in the `cython.parallel` module. The simplest way
    to achieve parallelism is through `prange`, which is a construct that automatically
    distributes loop operations in multiple threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, we can write the serial version of a program that computes the
    square of each element of a NumPy array in the `hello_parallel.pyx` file. We define
    a function, `square_serial`, that takes a buffer as input and populates an output
    array with the squares of the input array elements; `square_serial` is shown in
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Implementing a parallel version of the loop over the array elements involves
    substituting the `range` call with `prange`. There's a caveat--to use `prange`,
    it is necessary that the body of the loop is interpreter-free. As already explained,
    we need to release the GIL and, since interpreter calls generally acquire the
    GIL, they need to be avoided to make use of threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Cython, you can release the GIL using the `nogil` context, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can use the option `nogil=True` of `prange` that will automatically
    wrap the loop body in a `nogil` block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Attempts to call Python code in a `prange` block will produce an error. Prohibited
    operations include function calls, objects initialization, and so on. To enable such
    operations in a `prange` block (you may want to do so for debugging purposes),
    you have to re-enable the GIL using the `with gil` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now test our code by compiling it as a Python extension module. To enable
    OpenMP support, it is necessary to change the `setup.py` file so that it includes
    the compilation option `-fopenmp` . This can be achieved by using the `distutils.extension.Extension`
    class in `distutils` and passing it to `cythonize`. The complete `setup.py` file
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `prange`, we can easily parallelize the Cython version of our `ParticleSimulator`.
    The following code contains the `c_evolve` function of the `cevolve.pyx` Cython
    module that was written in [Chapter 4](ce893a62-a46c-4575-8163-01921cf8bb7b.xhtml), *C
    Performance with Cython*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we will invert the order of the loops so that the outermost loop will
    be executed in parallel (each iteration is independent from the other). Since
    the particles don''t interact with each other, we can change the order of iteration
    safely, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will replace the `range` call of the outer loop with  `prange` and
    remove calls that acquire the GIL. Since our code was already enhanced with static
    types, the `nogil` option can be applied safely as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now compare the functions by wrapping them in the benchmark function
    to assess any performance improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, we achieved a 2x speedup by writing a parallel version using
    `prange`.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned earlier, normal Python programs have trouble achieving thread
    parallelism because of the GIL. So far, we worked around this problem using separate
    processes; starting a process, however, takes significantly more time and memory
    than starting a thread.
  prefs: []
  type: TYPE_NORMAL
- en: We also saw that sidestepping the Python environment allowed us to achieve a
    2x speedup on an already fast Cython code. This strategy allowed us to achieve
    lightweight parallelism but required a separate compilation step. In this section,
    we will further explore this strategy using special libraries that are capable
    of automatically translating our code into a parallel version for efficient execution.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of packages that implement automatic parallelism are the (by now) familiar
    JIT compilers  `numexpr` and Numba. Other packages have been developed to automatically
    optimize and parallelize array and matrix-intensive expressions, which are crucial
    in specific numerical and machine learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: '**Theano** is a project that allows you to define a mathematical expression
    on arrays (more generally, *tensors*), and compile them to a fast language, such
    as C or C++. Many of the operations that Theano implements are parallelizable
    and can run on both CPU and GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tensorflow** is another library that, similar to Theano, is targeted towards expression
    of array-intensive mathematical expression but, rather than translating the expressions
    to specialized C code, executes the operations on an efficient C++ engine.'
  prefs: []
  type: TYPE_NORMAL
- en: Both Theano and Tensorflow are ideal when the problem at hand can be expressed
    in a chain of matrix and element-wise operations (such as *neural networks*).
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Theano
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Theano is somewhat similar to a compiler but with the added bonuses of being
    able to express, manipulate, and optimize mathematical expressions as well as
    run code on CPU and GPU. Since 2010, Theano has improved release after release
    and has been adopted by several other Python projects as a way to automatically
    generate efficient computational models on the fly.
  prefs: []
  type: TYPE_NORMAL
- en: In Theano, you first *define* the function you want to run by specifying variables
    and transformation using a pure Python API. This specification will then be compiled
    to machine code for execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first example, let''s examine how to implement a function that computes
    the square of a number. The input will be represented by a scalar variable, `a`,
    and then we will transform it to obtain its square, indicated by `a_sq`. In the
    following code, we will use the `T.scalar` function to define the variable and
    use the normal `**` operator to obtain a new variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, no specific value is computed and the transformation we apply
    is purely symbolic. In order to use this transformation, we need to generate a
    function. To compile a function, you can use the `th.function` utility that takes
    a list of the input variables as its first argument, and the output transformation
    (in our case `a_sq`) as its second argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Theano will take some time and translate the expression to efficient C code
    and compile it, all in the background! The return value of `th.function` will
    be a ready-to-use Python function and its usage is demonstrated in the next line
    of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Unsurprisingly, `compute_square` correctly returns the input value squared.
    Note, however, that the return type is not an integer (like the input type) but
    a floating point number. This is because the Theano default variable type is `float64`.
    you can verify that by inspecting the `dtype` attribute of the `a` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The Theano behavior is very different compared to what we saw with Numba. Theano
    doesn't compile generic Python code and, also, doesn't do any type inference;
    defining Theano functions requires a more precise specification of the types involved.
  prefs: []
  type: TYPE_NORMAL
- en: 'The real power of Theano comes from its support for array expressions. Defining
    a one-dimensional vector can be done with the `T.vector` function; the returned
    variable supports broadcasting operations with the same semantics of NumPy arrays.
    For instance, we can take two vectors and compute the element-wise sum of their
    squares, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The idea is, again, to use the Theano API as a mini-language to combine various
    Numpy array expressions will be compiled to efficient machine code.
  prefs: []
  type: TYPE_NORMAL
- en: One of the selling points of Theano is its ability to perform arithmetic simplifications
    and automatic gradient calculations. For more information, refer to the official
    documentation ([http://deeplearning.net/software/theano/introduction.html](http://deeplearning.net/software/theano/introduction.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate Theano functionality on a familiar use case, we can implement
    our parallel calculation of pi again. Our function will take a collection of two
    random coordinates as input and return the `pi` estimate. The input random numbers
    will be defined as vectors named `x` and `y`, and we can test their position inside
    the circle using standard element-wise operation that we will store in the `hit_test`
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we need to count the number of `True` elements in `hit_test`,
    which can be done taking its sum (it will be implicitly cast to integer).  To
    obtain the pi estimate, we finally need to calculate the ratio of hits versus
    the total number of trials. The calculation is illustrated in the following code
    block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can benchmark the execution of the Theano implementation using `th.function`
    and the `timeit` module. In our test, we will pass two arrays of size 30,000 and
    use the `timeit.timeit` utility to execute the `calculate_pi` function multiple
    times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The serial execution of this function takes about 10 seconds. Theano is capable
    of automatically parallelizing the code by implementing element-wise and matrix
    operations using specialized packages, such as OpenMP and the **Basic Linear Algebra
    Subprograms** (**BLAS**) linear algebra routines. Parallel execution can be enabled
    using configuration options.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Theano, you can set up configuration options by modifying variables in the
    `theano.config` object at import time. For example, you can issue the following
    commands to enable OpenMP support:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters relevant to OpenMP are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`openmp_elemwise_minsize`: This is an integer number that represents the minimum
    size of the arrays where element-wise parallelization should be enabled (the overhead
    of the parallelization can harm performance for small arrays)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`openmp`: This is a Boolean flag that controls the activation of OpenMP compilation
    (it should be activated by default)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controlling the number of threads assigned for OpenMP execution can be done
    by setting the `OMP_NUM_THREADS` environmental variable before executing the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now write a simple benchmark to demonstrate the OpenMP usage in practice.
    In a file `test_theano.py`, we will put the complete code for the pi estimation example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can run the code from the command line and assess the scaling
    with an increasing number of threads by setting the `OMP_NUM_THREADS` environment
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, there is a small speedup when using two threads, but the performance
    degrades quickly as we increase their number. This means that for this input size,
    it is not advantageous to use more than two threads as the price you pay to start
    new threads and synchronize their shared data is higher than the speedup that
    you can obtain from the parallel execution.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving good parallel performance can be tricky as it will depend on the specific
    operations and how they access the underlying data. As a general rule, measuring
    the performance of a parallel program is crucial and obtaining substantial speedups is
    a work of trial and error.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, we can see that the parallel performance quickly degrades using
    a slightly different code. In our hit test, we used the `sum` method directly
    and relied on the explicit casting of the `hit_tests` Boolean array. If we make
    the cast explicit, Theano will generate a slightly different code that benefits
    less from multiple threads. We can modify the `test_theano.py` file to verify
    this effect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'If we rerun our benchmark, we see that the number of threads does not affect
    the running time significantly. Despite that, the timings improved considerably
    as compared to the original version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Profiling Theano
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given the importance of measuring and analyzing performance, Theano provides
    powerful and informative profiling tools. To generate profiling data, the only
    modification needed is the addition of the `profile=True` option to `th.function`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The profiler will collect data as the function is being run (for example, through
    `timeit` or direct invocation). The profiling summary can be printed to output
    by issuing the `summary` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: To generate profiling data, we can rerun our script after adding the `profile=True`
    option (for this experiment, we will set the `OMP_NUM_THREADS` environmental variable
    to 1). Also, we will revert our script to the version that performed the casting
    of `hit_tests` implicitly.
  prefs: []
  type: TYPE_NORMAL
- en: You can also set up profiling globally using the `config.profile` option.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output printed by `calculate_pi.profile.summary()` is quite long and informative.
    A part of it is reported in the next block of text. The output is comprised of
    three sections that refer to timings sorted by `Class`, `Ops`, and `Apply`. In
    our example, we are concerned with `Ops`, which roughly maps to the functions
    used in the Theano compiled code. As you can see, roughly 80% of the time is spent
    in taking the element-wise square and sum of the two numbers, while the rest of
    the time is spent calculating the sum:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: This information is consistent with what was found in our first benchmark. The
    code went from about 11 seconds to roughly 8 seconds when two threads were used.
    From these numbers, we can analyze how the time was spent.
  prefs: []
  type: TYPE_NORMAL
- en: Out of these 11 seconds, 80% of the time (about 8.8 seconds) was spent doing
    element-wise operations. This means that, in perfectly parallel conditions, the
    increase in speed by adding two threads will be 4.4 seconds. In this scenario,
    the theoretical execution time will be 6.6 seconds. Considering that we obtained
    a timing of about 8 seconds, it looks like there is some extra overhead (1.4 seconds)
    for the thread usage.
  prefs: []
  type: TYPE_NORMAL
- en: Tensorflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tensorflow is another library designed for fast numerical calculations and automatic parallelism.
    It was released as an open source project by Google in 2015\. Tensorflow works
    by building mathematical expressions similar to Theano, except that the computation
    is not compiled to machine code but is executed on an external engine written
    in C++. Tensorflow supports execution and deployment of parallel codes on one
    or more CPUs and GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The usage of Tensorflow is quite similar to that of Theano. To create a variable
    in Tensorflow, you can use the `tf.placeholder` function that takes a data type
    as input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Tensorflow mathematical expressions can be expressed quite similarly to Theano,
    except for a few different naming conventions as well as a more restricted support
    for the NumPy semantics.
  prefs: []
  type: TYPE_NORMAL
- en: Tensorflow doesn't compile functions to C and then machine code like Theano,
    but serializes the defined mathematical functions (the data structure containing
    variables and transformations is called **computation graph**) and executes them
    on specific devices. The configuration of devices and context can be done using
    the `tf.Session` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the desired expression is defined, a `tf.Session` needs to be initialized
    and can be used to execute computation graphs using the `Session.run` method.
    In the following example, we demonstrate the usage of the Tensorflow API to implement a
    simple element-wise sum of squares:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Parallelism in Tensorflow is achieved automatically by its smart execution engine,
    and it generally works well without much fiddling. However, note that it is mostly
    suited for deep learning workloads that involve the definition of complex functions
    that use a lot of matrix multiplications and calculate their gradient.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now replicate the estimation of the pi example using Tensorflow capabilities
    and benchmark its execution speed and parallelism against the Theano implementation.
    What we will do is this:'
  prefs: []
  type: TYPE_NORMAL
- en: Define our `x` and `y` variables and perform a hit test using broadcasted operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the sum of `hit_tests` using the `tf.reduce_sum` function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initialize a `Session` object with the `inter_op_parallelism_threads` and `intra_op_parallelism_threads` configuration
    options. These options control the number of threads used for different classes of
    parallel operations. Note that the first `Session` created with such options sets
    the number of threads for the whole script (even future `Session` instances).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can now write a script name, `test_tensorflow.py`, containing the following
    code. Note that the number of threads is passed as the first argument of the script
    (`sys.argv[1]`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run the script multiple times with different values of `NUM_THREADS`,
    we see that the performance is quite similar to Theano and that the speedup increased
    by parallelization is quite modest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The main advantage of using software packages such as Tensorflow and Theano
    is the support for parallel matrix operations that are commonly used in machine
    learning algorithms. This is very effective because those operations can achieve
    impressive performance gains on GPU hardware that is designed to perform these
    operations with high throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Running code on a GPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this subsection, we will demonstrate the usage of a GPU with Theano and Tensorflow.
    As an example, we will benchmark the execution of a very simple matrix multiplication
    on the GPU and compare it to its running time on a CPU.
  prefs: []
  type: TYPE_NORMAL
- en: The code in this subsection requires the possession of a GPU. For learning purposes,
    it is possible to use the Amazon EC2 service ([https://aws.amazon.com/ec2](https://aws.amazon.com/ec2))
    to request a GPU-enabled instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code performs a simple matrix multiplication using Theano. We
    use the `T.matrix` function to initialize a two-dimensional array, and then we
    use the `T.dot` method to perform the matrix multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'It is possible to ask Theano to execute this code on a GPU by setting the `config.device=gpu` option.
    For added convenience, we can set up the configuration value from the command
    line using the `THEANO_FLAGS` environmental variable, shown as follows. After
    copying the previous code in the `test_theano_matmul.py` file, we can benchmark
    the execution time by issuing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We can analogously run the same code on the CPU using the `device=cpu` configuration
    option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the GPU is 7.2 times faster than the CPU version for this example!
  prefs: []
  type: TYPE_NORMAL
- en: 'For comparison, we may benchmark equivalent code using Tensorflow. The implementation
    of a Tensorflow version is reported in the next code snippet. The main differences
    with the Theano version are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The usage of the `tf.device` config manager that serves to specify the target
    device (`/cpu:0` or `/gpu:0`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The matrix multiplication is performed using the `tf.matmul` operator:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run the `test_tensorflow_matmul.py` script with the appropriate `tf.device`
    option, we obtain the following timings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the performance gain is substantial (but not as good as the
    Theano version) in this simple case.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to achieve automatic GPU computation is the now familiar Numba.
    With Numba, it is possible to compile Python code to programs that can be run
    on a GPU. This flexibility allows for advanced GPU programming as well as more
    simplified interfaces. In particular, Numba makes extremely easy-to-write, GPU-ready,
    generalized universal functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next example, we will demonstrate how to write a universal function
    that applies an exponential function on two numbers and sums the results. As we
    already saw in [Chapter 5](3797e4bb-99b8-4ba2-bb17-f757078b1d2b.xhtml), *Exploring
    Compilers* this can be accomplished using the `nb.vectorize` function (we''ll
    also specify the `cpu` target explicitly):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The `expon_cpu` universal function can be compiled for the GPU device using
    the `target=''cuda''` option. Also, note that it is necessary to specify the input
    types for CUDA universal functions. The implementation of `expon_gpu` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now benchmark the execution of the two functions by applying the functions
    on two arrays of size 1,000,000\. Also, note that we execute the function before
    measuring the timings to trigger the Numba just-in-time compilation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Thanks to the GPU execution, we were able to achieve a 3x speedup over the CPU
    version. Note that transferring data on the GPU is quite expensive; therefore,
    GPU execution becomes advantageous only for very large arrays.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parallel processing is an effective way to improve performance on large datasets.
    Embarrassingly parallel problems are excellent candidates for parallel execution
    that can be easily implemented to achieve good performance scaling.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we illustrated the basics of parallel programming in Python.
    We learned how to circumvent Python threading limitation by spawning processes
    using the tools available in the Python standard library. We also explored how
    to implement a multithreaded program using Cython and OpenMP.
  prefs: []
  type: TYPE_NORMAL
- en: For more complex problems, we learned how to use the Theano, Tensorflow, and
    Numba packages to automatically compile array-intensive expressions for parallel
    execution on CPU and GPU devices.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to write and execute parallel programs
    on multiple processors and machines using libraries such as dask and PySpark.
  prefs: []
  type: TYPE_NORMAL
