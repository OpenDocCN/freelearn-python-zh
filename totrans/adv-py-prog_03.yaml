- en: '*Chapter 2*: Pure Python Optimizations'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in the previous chapter, one of the most effective ways of improving
    the performance of applications is through the use of better algorithms and data
    structures. The Python standard library provides a large variety of ready-to-use
    algorithms and data structures that can be directly incorporated into your applications.
    With the tools learned from this chapter, you will be able to use the right algorithm
    for the task and achieve massive speed gains.
  prefs: []
  type: TYPE_NORMAL
- en: Even though many algorithms have been around for quite a while, they are especially
    relevant in today's world as we continuously produce, consume, and analyze ever-increasing
    amounts of data. Buying a larger server or micro-optimizing can work for some
    time, but achieving better scaling through algorithmic improvement can solve the
    problem once and for all.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to achieve better scaling using standard
    algorithms and data structures. More advanced use cases where we take advantage
    of third-party libraries will also be covered. We will also learn about tools
    to implement caching, a technique used to achieve faster response times by sacrificing
    some space in memory or on disk.
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of topics to be covered in this chapter is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the right algorithms and data structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improved efficiency with caching and memoization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficient iteration with comprehensions and generators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will find the code files for this chapter here: [https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter02](https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter02).'
  prefs: []
  type: TYPE_NORMAL
- en: Using the right algorithms and data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Algorithmic improvements are especially effective in increasing performance
    because they typically allow the application to scale better with increasingly
    large inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm running times can be classified according to their computational complexity,
    a characterization of the resources required to perform a task.
  prefs: []
  type: TYPE_NORMAL
- en: Such classification is expressed through the *Big O notation*, an upper bound
    on the operations required to execute the task, which usually depends on the input
    size. Specifically, Big O notation describes how the runtime or memory requirement
    of an algorithm grows in terms of the input size. For this reason, a lower Big
    O denotes a more efficient algorithm, which is what we aim for.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, incrementing each element of a list can be implemented using a
    `for` loop, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If the operation does not depend on the size of the input (for example, accessing
    the first element of a list), the algorithm is said to take constant, or *O*(1),
    time. This means that, no matter how much data we have, the time to run the algorithm
    will always be the same.
  prefs: []
  type: TYPE_NORMAL
- en: In this simple algorithm, the `input[i] += 1` operation will be repeated `10`
    times, which is the size of the input. If we double the size of the input array,
    the number of operations will increase proportionally. Since the number of operations
    is proportional to the input size, this algorithm is said to take *O*(*N*) time,
    where *N* is the size of the input array.
  prefs: []
  type: TYPE_NORMAL
- en: In some instances, the running time may depend on the structure of the input
    (for example, if the collection is sorted or contains many duplicates). In these
    cases, an algorithm may have different best-case, average-case, and worst-case
    running times. Unless stated otherwise, the running times presented in this chapter
    are considered to be average running times.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will examine the running times of algorithms and data structures
    that are implemented in the Python standard library and understand how improving
    running times results in massive gains and allows us to solve large-scale problems
    with elegance.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code used to run the benchmarks in this chapter in the `Algorithms.ipynb`
    notebook, which can be opened using *Jupyter*.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will examine **lists** and **deques**.
  prefs: []
  type: TYPE_NORMAL
- en: Lists and deques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python lists are ordered collections of elements and, in Python, are implemented
    as resizable arrays. An **array** is a basic data structure that consists of a
    series of contiguous memory locations, and each location contains a reference
    to a Python object.
  prefs: []
  type: TYPE_NORMAL
- en: Lists shine in accessing, modifying, and appending elements. Accessing or modifying
    an element involves fetching the object reference from the appropriate position
    of the underlying array and has *O*(1) complexity. Appending an element is also
    very fast. When an empty list is created, an array of fixed size is allocated
    and, as we insert elements, the slots in the array are gradually filled up. Once
    all the slots are occupied, the list needs to increase the size of its underlying
    array, thus triggering a memory reallocation that can take *O*(*N*) time. Nevertheless,
    those memory allocations are infrequent, and the time complexity for the append
    operation is referred to as amortized *O*(1) time.
  prefs: []
  type: TYPE_NORMAL
- en: The list operations that may have efficiency problems are those that add or
    remove elements at the beginning (or somewhere in the middle) of the list. When
    an item is inserted or removed from the beginning of a list, all the subsequent
    elements of the array need to be shifted by a position, thus taking *O*(*N*) time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, the timings for different operations on a list of 10,000
    size are shown; you can see how insertion and removal performances vary quite
    dramatically if performed at the beginning or the end of the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 1.1 – The speed of different list operations ](img/B17499_Table_2.1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 1.1 – The speed of different list operations
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, it is necessary to efficiently perform the insertion or removal
    of elements both at the beginning and the end of the collection. Python provides
    a data structure with those properties in the `collections.deque` class. The word
    *deque* stands for **double-ended queue** because this data structure is designed
    to efficiently put and remove elements at the beginning and the end of the collection,
    as it is in the case of queues. In Python, deques are implemented as doubly linked
    lists.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deques, in addition to `pop` and `append`, expose the `popleft` and `appendleft`
    methods that have *O*(1) running time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 1.2 – The speed of different deque operations ](img/B17499_Table_2.2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 1.2 – The speed of different deque operations
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite these advantages, deques should not be used to replace regular lists
    in most cases. The efficiency gained by the `appendleft` and `popleft` operations
    comes at a cost – accessing an element in the middle of a deque is an *O*(N) operation,
    as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 1.3 – The inefficiency of deques in accessing the middle element ](img/B17499_Table_2.3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 1.3 – The inefficiency of deques in accessing the middle element
  prefs: []
  type: TYPE_NORMAL
- en: Searching for an item in a list is generally an *O*(*N*) operation and is performed
    using the `list.index` method. A simple way to speed up searches in lists is to
    keep the array sorted and perform a binary search using the `bisect` module.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `bisect` module allows fast searches on sorted arrays. The `bisect.bisect`
    function can be used on a sorted list to find the index to place an element while
    maintaining the array in sorted order. In the following example, we can see that
    if we want to insert the `3` element in the array while keeping `collection` in
    sorted order, we should put `3` in the third position (which corresponds to index
    `2`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This function uses the binary search algorithm that has *O*(*log*(*N*)) running
    time. Such a running time is exceptionally fast and, basically, means that your
    running time will increase by a constant amount every time you *double* your input
    size. This means that if, for example, your program takes `1` second to run on
    an input of `1000` size, it will take `2` seconds to process an input of `2000`
    size, `3` seconds to process an input of `4000` size, and so on. If you had `100`
    seconds, you could theoretically process an input of `10^33` size, which is larger
    than the number of atoms in your body!
  prefs: []
  type: TYPE_NORMAL
- en: 'If the value we are trying to insert is already present in the list, the `bisect.bisect`
    function will return the location *after* the already present value. Therefore,
    we can use the `bisect.bisect_left` variant, which returns the correct index in
    the following way (taken from the module documentation at [https://docs.python.org/3.5/library/bisect.html](https://docs.python.org/3.5/library/bisect.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following table, you can see how the running time of the `bisect` solution
    is barely affected by these input sizes, making it a suitable solution when searching
    through very large collections:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17499_Table_2.4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 1.4 – The efficiency of the bisect function
  prefs: []
  type: TYPE_NORMAL
- en: Dictionaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Dictionaries** are extremely versatile and extensively used in the Python
    language, for example, in package-, module-, and class-level namespaces, as well
    as object and class annotations. Dictionaries are implemented as hash maps and
    are very good at element insertion, deletion, and access; all these operations
    have an average *O*(1) time complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: In Python versions up to 3.5, dictionaries are *unordered collections*. Since
    Python 3.6, dictionaries are capable of maintaining their elements by order of
    insertion.
  prefs: []
  type: TYPE_NORMAL
- en: Hash map
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A `hash` function; Python implements `hash` functions for several data types.
    As a demonstration, the generic function to obtain hash codes is `hash`. In the
    following example, we show you how to obtain the hash code when given the `"hello"`
    string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Hash maps can be tricky to implement because they need to handle collisions
    that happen when two different objects have the same hash code. However, all the
    complexity is elegantly hidden behind the implementation and the default collision
    resolution works well in most real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: The access to, insertion, and removal of an item in a dictionary scales as *O*(1)
    with the size of the dictionary. However, note that the computation of the `hash`
    function still needs to happen and, for strings, the computation scales with the
    length of the string. As string keys are usually relatively small, this doesn't
    constitute a problem in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'A dictionary can be used to efficiently count unique elements in a list. In
    this example, we define the `counter_dict` function that takes a list and returns
    a dictionary containing the number of occurrences of each value in the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The code can be somewhat simplified using `collections.defaultdict`, which
    can be used to produce dictionaries where each new key is automatically assigned
    a default value. In the following code, the `defaultdict(int)` call produces a
    dictionary where every new element is automatically assigned a zero value and
    can be used to streamline the counting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `collections` module also includes a `Counter` class that can be used for
    the same purpose with a single line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Speed-wise, all these ways of counting have the same time complexity, but the
    `Counter` implementation is the most efficient, as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 1.5 – The different methods of computing counters ](img/B17499_Table_2.5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 1.5 – The different methods of computing counters
  prefs: []
  type: TYPE_NORMAL
- en: Building an in-memory search index using a hash map
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Dictionaries can be used to quickly search for a word in a list of documents,
    similar to a search engine. In this subsection, we will learn how to build an
    inverted index based on a dictionary of lists. Let''s say we have a collection
    of four documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'A simple way to retrieve all the documents that match a query is to scan each
    document and test for the presence of a word. For example, if we want to look
    up the documents where the word `table` appears, we can employ the following filtering
    operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This approach is simple and works well when we have one-off queries; however,
    if we need to query the collection very often, it can be beneficial to optimize
    querying time. Since the per-query cost of the linear scan is *O*(*N*), you can
    imagine that better scaling will allow us to handle much larger document collections.
  prefs: []
  type: TYPE_NORMAL
- en: A better strategy is to spend some time preprocessing the documents so that
    they are easier to find at query time. We can build a structure, called the `"table"`
    will be associated with the `"the cat is under the table"` and `"the dog is under
    the table"` documents; they correspond to `0` and `1` indices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Such a mapping can be implemented by going over our collection of documents
    and storing in a dictionary the index of the documents where that term appears.
    The implementation is similar to the `counter_dict` function, except that, instead
    of incrementing a counter, we are growing the list of documents that match the
    current term:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have built our index, doing a query involves a simple dictionary lookup.
    For example, if we want to return all the documents containing the `table` term,
    we can simply query the index and retrieve the corresponding documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Since all it takes to query our collection is dictionary access, the index can
    handle queries with *O*(1) time complexity! Thanks to the inverted index, we are
    now able to query any number of documents (as long as they fit in memory) in constant
    time. Needless to say, indexing is a technique widely used to quickly retrieve
    data, not only in search engines but also in databases and any system that requires
    fast searches.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Building an inverted index is an expensive operation and requires you to encode
    every possible query. This is a substantial drawback, but the benefits are great,
    and it may be worthwhile to pay the price in terms of decreased flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: Sets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Sets** are unordered collections of elements, with the additional restriction
    that the elements must be unique. The main use cases where sets are a good choice
    are membership tests (testing whether an element is present in the collection)
    and, unsurprisingly, set operations such as union, difference, and intersection.'
  prefs: []
  type: TYPE_NORMAL
- en: In Python, sets are implemented using a hash-based algorithm, just like dictionaries;
    therefore, the time complexities for addition, deletion, and testing for membership
    scale as *O*(1) with the size of the collection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sets contain only *unique elements*. An immediate use case of sets is the removal
    of duplicates from a collection, which can be accomplished by simply passing the
    collection through the `set` constructor, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The time complexity for removing duplicates is *O*(N), as it requires reading
    the input and putting each element in the set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sets offer a number of operations, such as union, intersection, and difference.
    The union of two sets is a new set containing all the elements of both the sets;
    the intersection is a new set that contains only the elements in common between
    the two sets, and the difference is a new set containing the elements of the first
    set that are not contained in the second set. The time complexities for these
    operations are shown in the following table. Note that since we have two different
    input sizes, we will use the letter `S` to indicate the size of the first set
    (called `s`), and `T` to indicate the size of the second set (called `t`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 1.6 – The running time of set operations ](img/B17499_Table_2.6.png)'
  prefs: []
  type: TYPE_IMG
- en: Table 1.6 – The running time of set operations
  prefs: []
  type: TYPE_NORMAL
- en: An application of set operations is, for example, Boolean queries. Going back
    to the inverted index example of the previous subsection, we may want to support
    queries that include multiple terms. For example, we may want to search for all
    the documents that contain the words `cat` and `table`. This kind of query can
    be efficiently computed by taking the intersection between the set of documents
    containing `cat` and the set of documents containing `table`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to efficiently support those operations, we can change our indexing
    code so that each term is associated with a set of documents (rather than a list).
    After applying this change, calculating more advanced queries is a matter of applying
    the right set operation. In the following code, we show the inverted index based
    on sets and the query using set operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Heaps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Heaps** are data structures designed to quickly find and extract the maximum
    (or minimum) value in a collection. A typical use case for heaps is to process
    a series of incoming tasks in order of maximum priority.'
  prefs: []
  type: TYPE_NORMAL
- en: You can theoretically use a sorted list using the tools in the `bisect` module;
    however, while extracting the maximum value will take *O*(1) time (using `list.pop`),
    insertion will still take *O*(N) time (remember that, even if finding the insertion
    point takes *O*(*log*(*N*)) time, inserting an element in the middle of a list
    is still an *O*(*N*) operation). A heap is a more efficient data structure that
    allows for the insertion and extraction of maximum values with *O*(*log*(*N*))
    time complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, heaps are built using the procedures contained in the `heapq` module
    on an underlying list. For example, if we have a list of 10 elements, we can reorganize
    it into a heap with the `heapq.heapify` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'To perform the insertion and extraction operations on the heap, we can use
    the `heapq.heappush` and `heapq.heappop` functions. The `heapq.heappop` function
    will extract the minimum value in the collection in *O*(*log*(*N*)) time and can
    be used in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, you can push the `1` integer with the `heapq.heappush` function,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Another easy-to-use option is the `queue.PriorityQueue` class, which as a bonus,
    is thread- and process-safe. The `PriorityQueue` class can be filled up with elements
    using the `PriorityQueue.put` method, while `PriorityQueue.get` can be used to
    extract the minimum value in the collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'If the maximum element is required, a simple trick is to multiply each element
    of the list by `-1`. In this way, the order of the elements will be inverted.
    Also, if you want to associate an object (for example, a task to run) with each
    number (which can represent the priority), you can insert tuples of the `(number,
    object)` form; the comparison operator for the tuple will be ordered with respect
    to its first element, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Tries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A perhaps less popular data structure, but very useful in practice, is the **trie**
    (sometimes called **prefix tree**). Tries are extremely fast at matching a list
    of strings against a prefix. This is especially useful when implementing features
    such as *search as you type* and *autocompletion*, where the list of available
    completions is very large and short response times are required.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, Python does not include a trie implementation in its standard
    library; however, many efficient implementations are readily available through
    *PyPI*. The one we will use in this subsection is `patricia-trie`, a single-file,
    pure Python implementation of trie. As an example, we will use `patricia-trie`
    to perform the task of finding the longest prefix in a set of strings (just like
    autocompletion).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can demonstrate how fast a trie is able to search through a list of
    strings. In order to generate a large amount of unique random strings, we can
    define a function, `random_string`. The `random_string` function will return a
    string composed of random uppercase characters and, while there is a chance to
    get duplicates, we can greatly reduce the probability of duplicates to the point
    of being negligible if we make the string long enough. The implementation of the
    `random_string` function is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can build a list of random strings and time how fast it searches for a prefix
    (in our case, the `"AA"` string) using the `str.startswith` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'List comprehension and `str.startwith` are already very optimized operations
    and, on this small dataset, the search takes only a millisecond or so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s try using a trie for the same operation. In this example, we will
    use the `patricia-trie` library that is installable through `pip`. The `patricia.trie`
    class implements a variant of the trie data structure with an interface similar
    to a dictionary. We can initialize our trie by creating a dictionary from our
    list of strings, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'To query `patricia-trie` for a matching prefix, we can use the `trie.iter`
    method, which returns an iterator over the matching strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we know how to initialize and query a trie, we can time the operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The timing for this input size is **60.1 µs**, which is about 30 times faster
    (1.76 ms = 1760 µs) than linear search! This impressive speedup is because of
    the better computational complexity of the trie prefix search. Querying a trie
    has an *O*(*S*) time complexity, where *S* is the length of the longest string
    in the collection, while the time complexity of a simple linear scan is *O*(*N*),
    where *N* is the size of the collection.
  prefs: []
  type: TYPE_NORMAL
- en: Note that if we want to return all the prefixes that match, the running time
    will be proportional to the number of results that match the prefix. Therefore,
    when designing timing benchmarks, care must be taken to ensure that we are always
    returning the same number of results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scaling properties of a trie versus a linear scan for datasets of different
    sizes that contains 10 prefix matches are shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 1.7 – The running time of a trie versus a linear scan ](img/B17499_Table_2.7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 1.7 – The running time of a trie versus a linear scan
  prefs: []
  type: TYPE_NORMAL
- en: An interesting fact is that the implementation of `patricia-trie` is actually
    a single Python file; this clearly shows how simple and powerful a clever algorithm
    can be. For extra features and performance, other C-optimized trie libraries are
    also available, such as `datrie` and `marisa-trie`.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have considered most of the important data structures that
    are native to Python. Appropriate use of these data structures will speed up your
    application by a significant degree. In addition to data structures, there are
    other computing techniques and concepts that we could utilize in order to make
    our programs even faster. In the next section, we will take a look at caching
    and memoization, which are common practices when you expect the same computations
    to be repeated multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: Improved efficiency with caching and memoization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Caching** is a great technique used to improve the performance of a wide
    range of applications. The idea behind caching is to store expensive results in
    a temporary location, called a **cache**, that can be located in memory, on disk,
    or in a remote location.'
  prefs: []
  type: TYPE_NORMAL
- en: Web applications make extensive use of caching. In a web application, it is
    often the case that multiple users request a certain page at the same time. In
    this case, instead of recomputing the page for each user, the web application
    can compute it once and serve the user the already rendered page. Ideally, caching
    also needs a mechanism for invalidation so that if the page needs to be updated,
    we can recompute it before serving it again. Intelligent caching allows web applications
    to handle the increasing number of users with fewer resources. Caching can also
    be done preemptively, such as the later sections of a video getting buffered when
    watching a video online.
  prefs: []
  type: TYPE_NORMAL
- en: Caching is also used to improve the performance of certain algorithms. A great
    example is computing the Fibonacci sequence. Since computing the next number in
    the Fibonacci sequence requires the previous number in the sequence, you can store
    and reuse previous results, dramatically improving the running time. Storing and
    reusing the results of the previous function calls in an application is usually
    termed **memoization** and is one of the forms of caching. Several other algorithms
    can take advantage of memoization to gain impressive performance improvements,
    and this programming technique is commonly referred to as **dynamic programming**,
    where you aim to solve a large problem by breaking it into smaller ones.
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of caching, however, do not come for free. What we are actually
    doing is sacrificing some space to improve the speed of the application. Additionally,
    if the cache is stored in a location on the network, we may incur transfer costs
    and the general time needed for communication. You should evaluate when it is
    convenient to use a cache and how much space you are willing to trade for an increase
    in speed.
  prefs: []
  type: TYPE_NORMAL
- en: Given the usefulness of this technique, the Python standard library includes
    a simple in-memory cache out of the box in the `functools` module. The `functools.lru_cache`
    decorator can be used to easily cache the results of a function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we create a function, `sum2`, that prints a statement
    and returns the sum of two numbers. By running the function twice, you can see
    that the first time the `sum2` function is executed, the `"Calculating ..."` string
    is produced, while the second time, the result is returned without running the
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `lru_cache` decorator also provides other basic features. To restrict the
    size of the cache, you can set the number of elements that we intend to maintain
    through the `max_size` argument. If we want our cache size to be unbounded, we
    can specify a value of `None`. An example of `max_size` usage is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In this way, as we execute `sum2` with different arguments, the cache will reach
    a maximum size of `16` and, as we keep requesting more calculations, new values
    will replace older values in the cache. The `lru` prefix originates from this
    strategy, which means *least recently used*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `lru_cache` decorator also adds extra functionalities to the decorated
    function. For example, it is possible to examine the cache performance using the
    `cache_info` method, and it is possible to reset the cache using the `cache_clear`
    method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As an example, we can see how a problem, such as computing the Fibonacci series,
    may benefit from caching. We can define a `fibonacci` function and time its execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The execution takes 5.57 milliseconds, which is very long. The scaling of the
    function written in this way has poor performance; the previously computed Fibonacci
    sequences are not reused, causing this algorithm to have an exponential scaling
    of roughly *O*(*2N*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Caching can improve this algorithm by storing and reusing the already-computed
    Fibonacci numbers. To implement the cached version, it is sufficient to apply
    the `lru_cache` decorator to the original `fibonacci` function. Also, to design
    a proper benchmark, we need to ensure that a new cache is instantiated for every
    run; to do this, we can use the `timeit.repeat` function, as shown in the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Even though we changed the algorithm by adding a simple decorator, the running
    time now is much less than a microsecond. The reason is, thanks to caching, we
    now have a linear time algorithm instead of an exponential one.
  prefs: []
  type: TYPE_NORMAL
- en: The `lru_cache` decorator can be used to implement simple in-memory caching
    in your application. For more advanced use cases, third-party modules can be used
    for more powerful implementation and on-disk caching.
  prefs: []
  type: TYPE_NORMAL
- en: Joblib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A simple library that, among other things, provides a simple on-disk cache is
    `joblib`. The package can be used in a similar way as `lru_cache`, except that
    the results will be stored on disk and will persist between runs.
  prefs: []
  type: TYPE_NORMAL
- en: The `joblib` module can be installed from PyPI using the `pip install joblib`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `joblib` module provides the `Memory` class, which can be used to memoize
    functions using the `Memory.cache` decorator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The function will behave similarly to `lru_cache`, with the exception that the
    results will be stored on disk in the directory specified by the `cachedir` argument
    during `Memory` initialization. Additionally, the cached results will persist
    over subsequent runs!
  prefs: []
  type: TYPE_NORMAL
- en: The `Memory.cache` method also allows limiting recomputation to only when certain
    arguments change, and the resulting decorated function supports basic functionalities
    to clear and analyze the cache.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the best `joblib` feature is, thanks to intelligent hashing algorithms,
    providing efficient memoization of functions that operate on `numpy` arrays, which
    is particularly useful in scientific and engineering applications.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have seen that by using caching and memoization, our program
    can reuse computations in an efficient way. Another common strategy to improve
    running time is to utilize specifically designed techniques as appropriate. In
    the next section, we will see how we can take advantage of comprehensions and
    generators when working with Python loops.
  prefs: []
  type: TYPE_NORMAL
- en: Efficient iteration with comprehensions and generators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore a few simple strategies to speed up Python
    loops using `for` loops, as they are designed to avoid many unnecessary computational
    overheads during their construction. Another reason to use this construct is readability;
    even if the speedup over a standard loop is modest, the comprehension and generator
    syntax is more compact and (most of the time) more intuitive.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we can see that both the list comprehension and generator
    expressions are faster than an explicit loop when combined with the `sum` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like lists, it is possible to use `dict` comprehension to build dictionaries
    slightly more efficiently and compactly, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Efficient looping (especially in terms of memory) can be implemented using
    iterators and functions such as `filter` and `map`. As an example, consider the
    problem of applying a series of operations to a list using list comprehension
    and then taking the maximum value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The problem with this approach is that for every list comprehension, we are
    allocating a new list, increasing memory usage. Instead of using list comprehension,
    we can employ generators. Generators are objects that, when iterated upon, compute
    a value on the fly and return the result.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the `map` function takes two arguments – a function and an iterator
    – and returns a generator that applies the function to every element of the collection.
    The important point is that the operation happens only *while we are iterating*,
    and not when `map` is invoked!
  prefs: []
  type: TYPE_NORMAL
- en: 'We can rewrite the previous function using `map` and by creating intermediate
    generators, rather than lists, thus saving memory by computing the values on the
    fly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We can profile the memory of the two solutions using the `memory_profiler`
    extension from an IPython session. The extension provides a small utility, `%memit`,
    that will help us evaluate the memory usage of a Python statement in a way similar
    to `%timeit`, as illustrated in the following details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the memory used by the first version is `102.54 MiB`, while
    the second version consumes `0.00 MiB`! For those who are interested, more functions
    that return generators can be found in the `itertools` module, which provides
    a set of utilities designed to handle common iteration patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Algorithmic optimization can improve how your application scales as we process
    increasingly large data. In this chapter, we demonstrated use cases and running
    times of the most common data structures available in Python, such as lists, deques,
    dictionaries, heaps, and tries. We also covered caching, a technique that can
    be used to trade some space, in memory or on disk, in exchange for the increased
    responsiveness of an application. We also demonstrated how to get modest speed
    gains by replacing `for` loops with fast constructs, such as list comprehensions
    and generator expressions.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, we have seen that by utilizing a specifically designed data structure
    or technique that is appropriate in certain situations, the efficiency of our
    program can be greatly improved. The topics covered in this chapter offer us the
    ability to do just that across a wide range of use cases.
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent chapters, we will learn how to improve performance further
    using numerical libraries such as `numpy` and how to write extension modules in
    a lower-level language with the help of *Cython*.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Identify the best/most appropriate from the data structures covered in this
    chapter concerning the following use cases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mapping items to another *set* of items (*set* being used in the most general
    sense)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Accessing, modifying, and appending elements
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Maintaining a collection of unique elements
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Keeping track of the minimum/maximum of a *set* (in the most general sense)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Appending and removing elements at the endpoints of a *sequence* (in the most
    general sense).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Fast searching according to some similarity criterion (for example, being used
    by autocompletion engines).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between caching and memoization?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why are comprehensions and generators (in most situations) more preferred than
    explicit `for` loops?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Consider the problem of representing a pairwise association between a set of
    letters and a set of numbers (for example, a  2, b  1, c  3, and so on), where
    we need to look at what number a given letter is associated with in our application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is a list an appropriate data structure for this task, and if not, what is?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What if each number represented the number of instances of a given letter in
    a text document? What would the best data structure for this task be?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Lazy evaluation in Python: [https://towardsdatascience.com/what-is-lazy-evaluation-in-python-9efb1d3bfed0](https://towardsdatascience.com/what-is-lazy-evaluation-in-python-9efb1d3bfed0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `yield` statement in Python: [https://realpython.com/introduction-to-python-generators/](https://realpython.com/introduction-to-python-generators/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
