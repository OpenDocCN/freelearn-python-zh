["```py\nGIVEN some precondition(s) for a scenario\nWHEN we exercise some method of a class\nTHEN some state change(s) or side effect(s) will occur that we can confirm \n```", "```py\ndef average(data: list[Optional[int]]) -> float:\n    \"\"\"\n    GIVEN a list, data = [1, 2, None, 3, 4]\n    WHEN we compute m = average(data)\n    THEN the result, m, is 2.5\n    \"\"\"\n    pass \n```", "```py\nimport unittest\nclass CheckNumbers(unittest.TestCase):\n    def test_int_float(self) -> None:\n        self.assertEqual(1, 1.0)\nif __name__ == \"__main__\":\n    unittest.main() \n```", "```py\n.\n--------------------------------------------------------------\nRan 1 test in 0.000s\nOK \n```", "```py\n def test_str_float(self) -> None: \n        self.assertEqual(1, \"1\") \n```", "```py\n.F\n============================================================\nFAIL: test_str_float (__main__.CheckNumbers)\n--------------------------------------------------------------\nTraceback (most recent call last):\n  File \"first_unittest.py\", line 9, in test_str_float\n    self.assertEqual(1, \"1\")\nAssertionError: 1 != '1'\n--------------------------------------------------------------\nRan 2 tests in 0.001s\nFAILED (failures=1) \n```", "```py\n% python -m  pip install pytest \n```", "```py\ndef test_int_float() -> None: \n    assert 1 == 1.0 \n```", "```py\nclass TestNumbers:\n    def test_int_float(self) -> None:\n        assert 1 == 1.0\n    def test_int_str(self) -> None:\n        assert 1 == \"1\" \n```", "```py\n% python -m pytest tests/test_with_pytest.py\n======================== test session starts ========================\nplatform darwin -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1\nrootdir: /path/to/ch_13\ncollected 2 items                                                   \ntests/test_with_pytest.py .F                                  [100%]\n============================= FAILURES ==============================\n_____________________ TestNumbers.test_int_str ______________________\nself = <test_with_pytest.TestNumbers object at 0x7fb557f1a370>\n    def test_int_str(self) -> None:\n>       assert 1 == \"1\"\nE       AssertionError: assert 1 == \"1\"\ntests/test_with_pytest.py:15: AssertionError\n====================== short test summary info ======================\nFAILED tests/test_with_pytest.py::TestNumbers::test_int_str - Asse...\n==================== 1 failed, 1 passed in 0.07s ==================== \n```", "```py\nfrom __future__ import annotations\nfrom typing import Any, Callable\ndef setup_module(module: Any) -> None:\n    print(f\"setting up MODULE {module.__name__}\")\ndef teardown_module(module: Any) -> None:\n    print(f\"tearing down MODULE {module.__name__}\")\ndef test_a_function() -> None:\n    print(\"RUNNING TEST FUNCTION\")\nclass BaseTest:\n    @classmethod\n    def setup_class(cls: type[\"BaseTest\"]) -> None:\n        print(f\"setting up CLASS {cls.__name__}\")\n    @classmethod\n    def teardown_class(cls: type[\"BaseTest\"]) -> None:\n        print(f\"tearing down CLASS {cls.__name__}\\n\")\n    def setup_method(self, method: Callable[[], None]) -> None:\n        print(f\"setting up METHOD {method.__name__}\")\n    def teardown_method(self, method: Callable[[], None]) -> None:\n        print(f\"tearing down METHOD {method.__name__}\")\nclass TestClass1(BaseTest):\n    def test_method_1(self) -> None:\n        print(\"RUNNING METHOD 1-1\")\n    def test_method_2(self) -> None:\n        print(\"RUNNING METHOD 1-2\")\nclass TestClass2(BaseTest):\n    def test_method_1(self) -> None:\n        print(\"RUNNING METHOD 2-1\")\n    def test_method_2(self) -> None:\n        print(\"RUNNING METHOD 2-2\") \n```", "```py\n% python -m pytest --capture=no tests/test_setup_teardown.py\n========================= test session starts ==========================\nplatform darwin -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1\nrootdir: /…/ch_13\ncollected 5 items                                                      \ntests/test_setup_teardown.py setting up MODULE test_setup_teardown\nRUNNING TEST FUNCTION\n.setting up CLASS TestClass1\nsetting up METHOD test_method_1\nRUNNING METHOD 1-1\n.tearing down METHOD test_method_1\nsetting up METHOD test_method_2\nRUNNING METHOD 1-2\n.tearing down METHOD test_method_2\ntearing down CLASS TestClass1\nsetting up CLASS TestClass2\nsetting up METHOD test_method_1\nRUNNING METHOD 2-1\n.tearing down METHOD test_method_1\nsetting up METHOD test_method_2\nRUNNING METHOD 2-2\n.tearing down METHOD test_method_2\ntearing down CLASS TestClass2\ntearing down MODULE test_setup_teardown\n========================== 5 passed in 0.01s =========================== \n```", "```py\nfrom typing import List, Optional\nclass StatsList(List[Optional[float]]):\n    \"\"\"Stats with None objects rejected\"\"\"\n    def mean(self) -> float:\n        clean = list(filter(None, self))\n        return sum(clean) / len(clean)\n    def median(self) -> float:\n        clean = list(filter(None, self))\n        if len(clean) % 2:\n            return clean[len(clean) // 2]\n        else:\n            idx = len(clean) // 2\n            return (clean[idx] + clean[idx - 1]) / 2\n    def mode(self) -> list[float]:\n        freqs: DefaultDict[float, int] = collections.defaultdict(int)\n        for item in filter(None, self):\n            freqs[item] += 1\n        mode_freq = max(freqs.values())\n        modes = [item \n            for item, value in freqs.items() \n            if value == mode_freq]\n        return modes \n```", "```py\nimport pytest\nfrom stats import StatsList\n@pytest.fixture\ndef valid_stats() -> StatsList:\n    return StatsList([1, 2, 2, 3, 3, 4])\ndef test_mean(valid_stats: StatsList) -> None:\n    assert valid_stats.mean() == 2.5\ndef test_median(valid_stats: StatsList) -> None:\n    assert valid_stats.median() == 2.5\n    valid_stats.append(4)\n    assert valid_stats.median() == 3\ndef test_mode(valid_stats: StatsList) -> None:\n    assert valid_stats.mode() == [2, 3]\n    valid_stats.remove(2)\n    assert valid_stats.mode() == [3] \n```", "```py\nimport tarfile\nfrom pathlib import Path\nimport hashlib\ndef checksum(source: Path, checksum_path: Path) -> None:\n    if checksum_path.exists():\n        backup = checksum_path.with_stem(f\"(old) {checksum_path.stem}\")\n        backup.write_text(checksum_path.read_text())\n    checksum = hashlib.sha256(source.read_bytes())\n    checksum_path.write_text(f\"{source.name} {checksum.hexdigest()}\\n\") \n```", "```py\nfrom __future__ import annotations\nimport checksum_writer\nimport pytest\nfrom pathlib import Path\nfrom typing import Iterator\nimport sys\n@pytest.fixture\ndef working_directory(tmp_path: Path) -> Iterator[tuple[Path, Path]]:\n    working = tmp_path / \"some_directory\"\n    working.mkdir()\n    source = working / \"data.txt\"\n    source.write_bytes(b\"Hello, world!\\n\")\n    checksum = working / \"checksum.txt\"\n    checksum.write_text(\"data.txt Old_Checksum\")\n    **yield source, checksum**\n    checksum.unlink()\n    source.unlink() \n```", "```py\n@pytest.mark.skipif(\n    sys.version_info < (3, 9), reason=\"requires python3.9 feature\")\ndef test_checksum(working_directory: tuple[Path, Path]) -> None:\n    source_path, old_checksum_path = working_directory\n    checksum_writer.checksum(source_path, old_checksum_path)\n    backup = old_checksum_path.with_stem(\n        f\"(old) {old_checksum_path.stem}\")\n    assert backup.exists()\n    assert old_checksum_path.exists()\n    name, checksum = old_checksum_path.read_text().rstrip().split()\n    assert name == source_path.name\n    assert (\n        checksum == \"d9014c4624844aa5bac314773d6b689a\"\n        \"d467fa4e1d1a50a1b8a99d5a95f72ff5\"\n    ) \n```", "```py\nb'\\x00\\x00\\x02d' b'}q\\x00(X\\x04\\x00\\x00\\x00nameq\\x01X\\x03\\x00\\x00\\x00appq\\x02X\\x03\\x00\\x00\\x00msgq\\x03X\\x0b\\x00\\x00\\x00Factorial \n…\n\\x19X\\n\\x00\\x00\\x00MainThreadq\\x1aX\\x0b\\x00\\x00\\x00processNameq\\x1bX\\x0b\\x00\\x00\\x00MainProcessq\\x1cX\\x07\\x00\\x00\\x00processq\\x1dMcQu.' \n```", "```py\nfrom __future__ import annotations\nimport json\nfrom pathlib import Path\nimport socketserver\nfrom typing import TextIO\nimport pickle\nimport struct\nclass LogDataCatcher(socketserver.BaseRequestHandler):\n    log_file: TextIO\n    count: int = 0\n    size_format = \">L\"\n    size_bytes = struct.calcsize(size_format)\n    def handle(self) -> None:\n        size_header_bytes = self.request.recv(LogDataCatcher.size_bytes)\n        while size_header_bytes:\n            payload_size = struct.unpack(\n                LogDataCatcher.size_format, size_header_bytes)\n            payload_bytes = self.request.recv(payload_size[0])\n            payload = pickle.loads(payload_bytes)\n            LogDataCatcher.count += 1\n            self.log_file.write(json.dumps(payload) + \"\\n\")\n            try:\n                size_header = self.request.recv(\n                    LogDataCatcher.size_bytes)\n            except (ConnectionResetError, BrokenPipeError):\n                break\ndef main(host: str, port: int, target: Path) -> None:\n    with target.open(\"w\") as unified_log:\n        LogDataCatcher.log_file = unified_log\n        with socketserver.TCPServer(\n                (host, port), LogDataCatcher) as server:\n            server.serve_forever() \n```", "```py\nif __name__ == \"__main__\":\n    HOST, PORT = \"localhost\", 18842\n    main(HOST, PORT, Path(\"one.log\")) \n```", "```py\nfrom __future__ import annotations\nimport logging\nimport logging.handlers\nimport time\nimport sys\nfrom math import factorial\nlogger = logging.getLogger(\"app\")\ndef work(i: int) -> int:\n    logger.info(\"Factorial %d\", i)\n    f = factorial(i)\n    logger.info(\"Factorial(%d) = %d\", i, f)\n    return f\nif __name__ == \"__main__\":\n    HOST, PORT = \"localhost\", 18842\n    socket_handler = logging.handlers.SocketHandler(HOST, PORT)\n    stream_handler = logging.StreamHandler(sys.stderr)\n    logging.basicConfig(\n        handlers=[socket_handler, stream_handler], \n        level=logging.INFO)\n    for i in range(10):\n        work(i)\n    logging.shutdown() \n```", "```py\nfrom __future__ import annotations\nimport subprocess\nimport signal\nimport time\nimport pytest\nimport logging\nimport sys\nimport remote_logging_app\nfrom typing import Iterator, Any\n@pytest.fixture(scope=\"session\")\ndef log_catcher() -> Iterator[None]:\n    print(\"loading server\")\n    p = subprocess.Popen(\n        [\"python3\", \"src/log_catcher.py\"],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        text=True,\n    )\n    time.sleep(0.25)\n    **yield**\n    p.terminate()\n    p.wait()\n    if p.stdout:\n        print(p.stdout.read())\n    assert (\n        p.returncode == -signal.SIGTERM.value\n    ), f\"Error in watcher, returncode={p.returncode}\"\n@pytest.fixture\ndef logging_config() -> Iterator[None]:\n    HOST, PORT = \"localhost\", 18842\n    socket_handler = logging.handlers.SocketHandler(HOST, PORT)\n    remote_logging_app.logger.addHandler(socket_handler)\n    yield\n    socket_handler.close()\n    remote_logging_app.logger.removeHandler(socket_handler) \n```", "```py\ndef test_1(log_catcher: None, logging_config: None) -> None:\n    for i in range(10):\n        r = remote_logging_app.work(i)\ndef test_2(log_catcher: None, logging_config: None) -> None:\n    for i in range(1, 10):\n        r = remote_logging_app.work(52 * i) \n```", "```py\nimport sys\nimport pytest\ndef test_simple_skip() -> None:\n    if sys.platform != \"ios\":\n        pytest.skip(\"Test works only on Pythonista for ios\")\n    import location  # type: ignore [import]\n    img = location.render_map_snapshot(36.8508, -76.2859)\n    assert img is not None \n```", "```py\nimport pytest\nimport sys\n@pytest.mark.skipif(\n    sys.version_info < (3, 9), \n    reason=\"requires 3.9, Path.removeprefix()\"\n)\ndef test_feature_python39() -> None:\n    file_name = \"(old) myfile.dat\"\n    assert file_name.removeprefix(\"(old) \") == \"myfile.dat\" \n```", "```py\n% python -m pip install redis\nCollecting redis\n  Downloading redis-3.5.3-py2.py3-none-any.whl (72 kB)\n     |████████████████████████████████| 72 kB 1.1 MB/s \nInstalling collected packages: redis\nSuccessfully installed redis-3.5.3 \n```", "```py\nfrom __future__ import annotations\nimport datetime\nfrom enum import Enum\nimport redis\nclass Status(str, Enum):\n    CANCELLED = \"CANCELLED\"\n    DELAYED = \"DELAYED\"\n    ON_TIME = \"ON TIME\"\nclass FlightStatusTracker:\n    def __init__(self) -> None:\n        self.redis = redis.Redis(host=\"127.0.0.1\", port=6379, db=0)\n    def change_status(self, flight: str, status: Status) -> None:\n        if not isinstance(status, Status):\n            raise ValueError(f\"{status!r} is not a valid Status\")\n        key = f\"flightno:{flight}\"\n        now = datetime.datetime.now(tz=datetime.timezone.utc)\n        value = f\"{now.isoformat()}|{status.value}\"\n        self.redis.set(key, value)\n    def get_status(self, flight: str) -> tuple[datetime.datetime, Status]:\n        key = f\"flightno:{flight}\"\n        value = self.redis.get(key).decode(\"utf-8\")\n        text_timestamp, text_status = value.split(\"|\")\n        timestamp = datetime.datetime.fromisoformat(text_timestamp)\n        status = Status(text_status)\n        return timestamp, status \n```", "```py\nimport datetime\nimport flight_status_redis\nfrom unittest.mock import Mock, patch, call\nimport pytest\n@pytest.fixture\ndef mock_redis() -> Mock:\n    mock_redis_instance = Mock(set=Mock(return_value=True))\n    return mock_redis_instance\n@pytest.fixture\ndef tracker(\n    monkeypatch: pytest.MonkeyPatch, mock_redis: Mock\n) -> flight_status_redis.FlightStatusTracker:\n    fst = flight_status_redis.FlightStatusTracker()\n    monkeypatch.setattr(fst, \"redis\", mock_redis)\n    return fst\ndef test_monkeypatch_class(\n    tracker: flight_status_redis.FlightStatusTracker, mock_redis: Mock\n) -> None:\n    with pytest.raises(ValueError) as ex:\n        tracker.change_status(\"AC101\", \"lost\")\n    assert ex.value.args[0] == \"'lost' is not a valid Status\"\n    assert mock_redis.set.call_count == 0 \n```", "```py\ndef test_patch_class(\n    tracker: flight_status_redis.FlightStatusTracker, mock_redis: Mock\n) -> None:\n    fake_now = datetime.datetime(2020, 10, 26, 23, 24, 25)\n    utc = datetime.timezone.utc\n    with patch(\"flight_status_redis.datetime\") as mock_datetime:\n        mock_datetime.datetime = Mock(now=Mock(return_value=fake_now))\n        mock_datetime.timezone = Mock(utc=utc)\n        tracker.change_status(\n        \"AC101\", flight_status_redis.Status.ON_TIME)\n    mock_datetime.datetime.now.assert_called_once_with(tz=utc)\n    expected = f\"2020-10-26T23:24:25|ON TIME\"\n    mock_redis.set.assert_called_once_with(\"flightno:AC101\", expected) \n```", "```py\ndef __init__(\n        self, \n        redis_instance: Optional[redis.Connection] = None\n) -> None:\n    self.redis = (\n        redis_instance\n        if redis_instance\n        else redis.Redis(host=\"127.0.0.1\", port=6379, db=0)\n    ) \n```", "```py\nclass FileChecksum:\n    def __init__(self, source: Path) -> None:\n        self.source = source\n        self.checksum = hashlib.sha256(source.read_bytes()) \n```", "```py\nfrom unittest.mock import Mock, sentinel\n@pytest.fixture\ndef mock_hashlib(monkeypatch) -> Mock:\n    mocked_hashlib = Mock(sha256=Mock(return_value=sentinel.checksum))\n    monkeypatch.setattr(checksum_writer, \"hashlib\", mocked_hashlib)\n    return mocked_hashlib\ndef test_file_checksum(mock_hashlib, tmp_path) -> None:\n    source_file = tmp_path / \"some_file\"\n    source_file.write_text(\"\")\n    cw = checksum_writer.FileChecksum(source_file)\n    assert cw.source == source_file\n    assert cw.checksum == sentinel.checksum \n```", "```py\n% export PYTHONPATH=$(pwd)/src:$PYTHONPATH\n% coverage run -m pytest tests/test_coverage.py \n```", "```py\n> $ENV:PYTHONPATH = \"$pwd\\src\" + \";\" + $PYTHONPATH\n> coverage run -m pytest tests/test_coverage.py \n```", "```py\n% coverage report \n```", "```py\nName                     Stmts   Miss  Cover\n--------------------------------------------\nsrc/stats.py                19     11    42%\ntests/test_coverage.py       7      0   100%\n--------------------------------------------\nTOTAL                       26     11    58% \n```", "```py\nName                     Stmts   Miss  Cover   Missing\n------------------------------------------------------\nsrc/stats.py                19     11    42%   18-23, 26-31\ntests/test_coverage.py       7      0   100%\n------------------------------------------------------\nTOTAL                       26     11    58% \n```", "```py\nimport pytest\nfrom stats import StatsList\n@pytest.fixture\ndef valid_stats() -> StatsList:\n    return StatsList([1, 2, 2, 3, 3, 4])\ndef test_mean(valid_stats: StatsList) -> None:\n    assert valid_stats.mean() == 2.5 \n```", "```py\n>>> from sympy import *\n>>> ED, k_sl, k_pl, k_sw, k_pw, u_sl, u_pl, u_sw, u_pw = symbols(\n...     \"ED, k_sl, k_pl, k_sw, k_pw, u_sl, u_pl, u_sw, u_pw\")\n>>> ED = sqrt( (k_sl-u_sl)**2 + (k_pl-u_pl)**2 + (k_sw-u_sw)**2 + (k_pw-u_pw)**2 )\n>>> ED\nsqrt((k_pl - u_pl)**2 + (k_pw - u_pw)**2 + (k_sl - u_sl)**2 + (k_sw - u_sw)**2)\n>>> print(pretty(ED, use_unicode=False))\n   ___________________________________________________________________\n  /              2                2                2                2 \n\\/  (k_pl - u_pl)  + (k_pw - u_pw)  + (k_sl - u_sl)  + (k_sw - u_sw) \n```", "```py\n>>> e = ED.subs(dict(\n...     k_sl=5.1, k_sw=3.5, k_pl=1.4, k_pw=0.2,\n...     u_sl=7.9, u_sw=3.2, u_pl=4.7, u_pw=1.4,\n... ))\n>>> e.evalf(9)\n4.50111097 \n```", "```py\nclass ED(Distance):\n    def distance(self, s1: Sample, s2: Sample) -> float:\n        return hypot(\n            s1.sepal_length - s2.sepal_length,\n            s1.sepal_width - s2.sepal_width,\n            s1.petal_length - s2.petal_length,\n            s1.petal_width - s2.petal_width,\n        ) \n```", "```py\nScenario: Euclidean Distance Computation\n  Given an unknown sample, U, and a known sample, K\n   When we compute the Euclidean Distance between them\n   Then we get the distance, ED. \n```", "```py\n@pytest.fixture\ndef known_unknown_example_15() -> Known_Unknown:\n    known_row: Row = {\n        \"species\": \"Iris-setosa\",\n        \"sepal_length\": 5.1,\n        \"sepal_width\": 3.5,\n        \"petal_length\": 1.4,\n        \"petal_width\": 0.2,\n    }\n    k = TrainingKnownSample(**known_row)\n    unknown_row = {\n        \"sepal_length\": 7.9,\n        \"sepal_width\": 3.2,\n        \"petal_length\": 4.7,\n        \"petal_width\": 1.4,\n    }\n    u = UnknownSample(**unknown_row)\n    return k, u \n```", "```py\nFrom __future__ import annotations\nimport pytest\nfrom model import TrainingKnownSample, UnknownSample\nfrom model import CD, ED, MD, SD\nfrom typing import Tuple, TypedDict\nKnown_Unknown = Tuple[TrainingKnownSample, UnknownSample]\nclass Row(TypedDict):\n    species: str\n    sepal_length: float\n    sepal_width: float\n    petal_length: float\n    petal_width: float \n```", "```py\ndef test_ed(known_unknown_example_15: Known_Unknown) -> None:\n    k, u = known_unknown_example_15\n    assert ED().distance(k, u) == pytest.approx(4.50111097) \n```", "```py\ndef test_cd(known_unknown_example_15: Known_Unknown) -> None:\n    k, u = known_unknown_example_15\n    assert CD().distance(k, u) == pytest.approx(3.3)\ndef test_md(known_unknown_example_15: Known_Unknown) -> None:\n    k, u = known_unknown_example_15\n    assert MD().distance(k, u) == pytest.approx(7.6) \n```", "```py\n>>> SD = sum(\n...     [abs(k_sl - u_sl), abs(k_sw - u_sw), abs(k_pl - u_pl), abs(k_pw - u_pw)]\n...  ) / sum( \n...     [k_sl + u_sl, k_sw + u_sw, k_pl + u_pl, k_pw + u_pw])\n>>> print(pretty(SD, use_unicode=False))\n|k_pl - u_pl| + |k_pw - u_pw| + |k_sl - u_sl| + |k_sw - u_sw|\n-------------------------------------------------------------\n    k_pl + k_pw + k_sl + k_sw + u_pl + u_pw + u_sl + u_sw \n```", "```py\n>>> e = SD.subs(dict(\n...     k_sl=5.1, k_sw=3.5, k_pl=1.4, k_pw=0.2,\n...     u_sl=7.9, u_sw=3.2, u_pl=4.7, u_pw=1.4,\n... ))\n>>> e.evalf(9)\n0.277372263 \n```", "```py\ndef test_sd(known_unknown_example_15: Known_Unknown) -> None:\n    k, u = known_unknown_example_15\n    assert SD().distance(k, u) == pytest.approx(0.277372263) \n```", "```py\nclass Hyperparameter:\n    def __init__(\n            self, \n            k: int, \n            algorithm: \"Distance\", \n            training: \"TrainingData\"\n    ) -> None:\n        self.k = k\n        self.algorithm = algorithm\n        self.data: weakref.ReferenceType[\"TrainingData\"] = \\\n            weakref.ref(training)\n        self.quality: float\n    def classify(\n            self, \n            sample: Union[UnknownSample, TestingKnownSample]) -> str:\n        \"\"\"The k-NN algorithm\"\"\"\n        training_data = self.data()\n        if not training_data:\n            raise RuntimeError(\"No TrainingData object\")\n        distances: list[tuple[float, TrainingKnownSample]] = sorted(\n            (self.algorithm.distance(sample, known), known)\n            for known in training_data.training\n        )\n        k_nearest = (known.species for d, known in distances[: self.k])\n        frequency: Counter[str] = collections.Counter(k_nearest)\n        best_fit, *others = frequency.most_common()\n        species, votes = best_fit\n        return species \n```", "```py\nfrom __future__ import annotations\nfrom model import Hyperparameter\nfrom unittest.mock import Mock, sentinel, call\n@pytest.fixture\ndef sample_data() -> list[Mock]:\n    return [\n        Mock(name=\"Sample1\", species=sentinel.Species3),\n        Mock(name=\"Sample2\", species=sentinel.Species1),\n        Mock(name=\"Sample3\", species=sentinel.Species1),\n        Mock(name=\"Sample4\", species=sentinel.Species1),\n        Mock(name=\"Sample5\", species=sentinel.Species3),\n    ] \n```", "```py\n@pytest.fixture\ndef hyperparameter(sample_data: list[Mock]) -> Hyperparameter:\n    mocked_distance = Mock(distance=Mock(side_effect=[11, 1, 2, 3, 13]))\n    mocked_training_data = Mock(training=sample_data)\n    mocked_weakref = Mock(\n        return_value=mocked_training_data)\n    fixture = Hyperparameter(\n        k=3, algorithm=mocked_distance, training=sentinel.Unused)\n    fixture.data = mocked_weakref\n    return fixture \n```", "```py\ndef test_hyperparameter(sample_data: list[Mock], hyperparameter: Mock) -> None:\n    s = hyperparameter.classify(sentinel.Unknown)\n    assert s == sentinel.Species1\n    assert hyperparameter.algorithm.distance.mock_calls == [\n        call(sentinel.Unknown, sample_data[0]),\n        call(sentinel.Unknown, sample_data[1]),\n        call(sentinel.Unknown, sample_data[2]),\n        call(sentinel.Unknown, sample_data[3]),\n        call(sentinel.Unknown, sample_data[4]),\n    ] \n```"]