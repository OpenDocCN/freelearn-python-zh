["```py\nclass Chef(Thread):\n    def __init__(self, name: str) -> None:\n        super().__init__(name=name)\n        self.total = 0\n    def get_order(self) -> None:\n        self.order = THE_ORDERS.pop(0)\n    def prepare(self) -> None:\n        \"\"\"Simulate doing a lot of work with a BIG computation\"\"\"\n        start = time.monotonic()\n        target = start + 1 + random.random()\n        for i in range(1_000_000_000):\n            self.total += math.factorial(i)\n            if time.monotonic() >= target:\n                break\n        print(\n            f\"{time.monotonic():.3f} {self.name} made {self.order}\")\n    def run(self) -> None:\n        while True:\n            try:\n                self.get_order()\n                self.prepare()\n            except IndexError:\n                break  # No more orders \n```", "```py\nimport math\nimport random\nfrom threading import Thread, Lock\nimport time\nTHE_ORDERS = [\n    \"Reuben\",\n    \"Ham and Cheese\",\n    \"Monte Cristo\",\n    \"Tuna Melt\",\n    \"Cuban\",\n    \"Grilled Cheese\",\n    \"French Dip\",\n    \"BLT\",\n] \n```", "```py\nMo = Chef(\"Michael\")\nConstantine = Chef(\"Constantine\")\nif __name__ == \"__main__\":\n    random.seed(42)\n    Mo.start()\n    Constantine.start() \n```", "```py\n1.076 Constantine made Ham and Cheese\n1.676 Michael made Reuben\n2.351 Constantine made Monte Cristo\n2.899 Michael made Tuna Melt\n4.094 Constantine made Cuban\n4.576 Michael made Grilled Cheese\n5.664 Michael made BLT\n5.987 Constantine made French Dip \n```", "```py\nfrom multiprocessing import Process, cpu_count\nimport time\nimport os\nclass MuchCPU(Process):\n    def run(self) -> None:\n        print(f\"OS PID {os.getpid()}\")\n        s = sum(\n            2*i+1 for i in range(100_000_000)\n        )\nif __name__ == \"__main__\":\n    workers = [MuchCPU() for f in range(cpu_count())]\n    t = time.perf_counter()\n    for p in workers:\n        p.start()\n    for p in workers:\n        p.join()\n    print(f\"work took {time.perf_counter() - t:.3f} seconds\") \n```", "```py\n% python src/processes_1.py\nOS PID 15492\nOS PID 15493\nOS PID 15494\nOS PID 15495\nOS PID 15497\nOS PID 15496\nOS PID 15498\nOS PID 15499\nwork took 20.711 seconds \n```", "```py\n% python src/processes_1.py\nOS PID 15772\nOS PID 15772\nOS PID 15772\nOS PID 15772\nOS PID 15772\nOS PID 15772\nOS PID 15772\nOS PID 15772\nwork took 69.316 seconds \n```", "```py\nfrom __future__ import annotations\nfrom math import sqrt, ceil\nimport random\nfrom multiprocessing.pool import Pool\ndef prime_factors(value: int) -> list[int]:\n    if value in {2, 3}:\n        return [value]\n    factors: list[int] = []\n    for divisor in range(2, ceil(sqrt(value)) + 1):\n        quotient, remainder = divmod(value, divisor)\n        if not remainder:\n            factors.extend(prime_factors(divisor))\n            factors.extend(prime_factors(quotient))\n            break\n    else:\n        factors = [value]\n    return factors\nif __name__ == \"__main__\":\n    to_factor = [\n        random.randint(100_000_000, 1_000_000_000)\n        for i in range(40_960)\n    ]\n    with Pool() as pool:\n        results = pool.map(prime_factors, to_factor)\n    primes = [\n        value\n        for value, factor_list in zip(to_factor, results)\n            if len(factor_list) == 1\n    ]\n    print(f\"9-digit primes {primes}\") \n```", "```py\nfrom __future__ import annotations\nfrom pathlib import Path\nfrom typing import List, Iterator, Optional, Union, TYPE_CHECKING\nif TYPE_CHECKING:\n    Query_Q = Queue[Union[str, None]]\n    Result_Q = Queue[List[str]]\ndef search(\n        paths: list[Path], \n        query_q: Query_Q, \n        results_q: Result_Q\n) -> None:\n    print(f\"PID: {os.getpid()}, paths {len(paths)}\")\n    lines: List[str] = []\n    for path in paths:\n        lines.extend(\n            l.rstrip() for l in path.read_text().splitlines())\n    while True:\n        if (query_text := query_q.get()) is None:\n            break\n        results = [l for l in lines if query_text in l]\n        results_q.put(results) \n```", "```py\nfrom __future__ import annotations\nfrom fnmatch import fnmatch\nimport os\nclass DirectorySearch:\n    def __init__(self) -> None:\n        self.query_queues: List[Query_Q]\n        self.results_queue: Result_Q\n        self.search_workers: List[Process]\n    def setup_search(\n        self, paths: List[Path], cpus: Optional[int] = None) -> None:\n        if cpus is None:\n            cpus = cpu_count()\n        worker_paths = [paths[i::cpus] for i in range(cpus)]\n        self.query_queues = [Queue() for p in range(cpus)]\n        self.results_queue = Queue()\n        self.search_workers = [\n            Process(\n                target=search, args=(paths, q, self.results_queue))\n            for paths, q in zip(worker_paths, self.query_queues)\n        ]\n        for proc in self.search_workers:\n            proc.start()\n    def teardown_search(self) -> None:\n        # Signal process termination\n        for q in self.query_queues:\n            q.put(None)\n        for proc in self.search_workers:\n            proc.join()\n    def search(self, target: str) -> Iterator[str]:\n        for q in self.query_queues:\n            q.put(target)\n        for i in range(len(self.query_queues)):\n            for match in self.results_queue.get():\n                yield match \n```", "```py\ndef all_source(path: Path, pattern: str) -> Iterator[Path]:\n    for root, dirs, files in os.walk(path):\n        for skip in {\".tox\", \".mypy_cache\", \"__pycache__\", \".idea\"}:\n            if skip in dirs:\n                dirs.remove(skip)\n        yield from (\n            Path(root) / f for f in files if fnmatch(f, pattern)) \n```", "```py\nif __name__ == \"__main__\":\n    ds = DirectorySearch()\n    base = Path.cwd().parent\n    all_paths = list(all_source(base, \"*.py\"))\n    ds.setup_search(all_paths)\n    for target in (\"import\", \"class\", \"def\"):\n        start = time.perf_counter()\n        count = 0\n        for line in ds.search(target):\n            **# print(line)**\n            count += 1\n        milliseconds = 1000*(time.perf_counter()-start)\n        print(\n            f\"Found {count} {target!r} in {len(all_paths)} files \"\n            f\"in {milliseconds:.3f}ms\"\n        )\n    ds.teardown_search() \n```", "```py\npython src/directory_search.py\nPID: 36566, paths 17\nPID: 36567, paths 17\nPID: 36570, paths 17\nPID: 36571, paths 17\nPID: 36569, paths 17\nPID: 36568, paths 17\nPID: 36572, paths 16\nPID: 36573, paths 16\nFound 579 'import' in 134 files in 111.561ms\nFound 838 'class' in 134 files in 1.010ms\nFound 1138 'def' in 134 files in 1.224ms \n```", "```py\nclass ImportResult(NamedTuple):\n    path: Path\n    imports: Set[str]\n    @property\n    def focus(self) -> bool:\n        return \"typing\" in self.imports\nclass ImportVisitor(ast.NodeVisitor):\n    def __init__(self) -> None:\n        self.imports: Set[str] = set()\n    def visit_Import(self, node: ast.Import) -> None:\n        for alias in node.names:\n            self.imports.add(alias.name)\n    def visit_ImportFrom(self, node: ast.ImportFrom) -> None:\n        if node.module:\n            self.imports.add(node.module)\ndef find_imports(path: Path) -> ImportResult:\n    tree = ast.parse(path.read_text())\n    iv = ImportVisitor()\n    iv.visit(tree)\n    return ImportResult(path, iv.imports) \n```", "```py\ndef main() -> None:\n    start = time.perf_counter()\n    base = Path.cwd().parent\n    with futures.ThreadPoolExecutor(24) as pool:\n        analyzers = [\n           pool.submit(find_imports, path) \n           for path in all_source(base, \"*.py\")\n        ]\n        analyzed = (\n            worker.result() \n            for worker in futures.as_completed(analyzers)\n        )\n    for example in sorted(analyzed):\n        print(\n            f\"{'->' if example.focus else '':2s} \" \n            f\"{example.path.relative_to(base)} {example.imports}\"\n        )\n    end = time.perf_counter()\n    rate = 1000 * (end - start) / len(analyzers)\n    print(f\"Searched {len(analyzers)} files at {rate:.3f}ms/file\") \n```", "```py\nimport asyncio\nimport random\nasync def random_sleep(counter: float) -> None:\n    delay = random.random() * 5\n    print(f\"{counter} sleeps for {delay:.2f} seconds\")\n    await asyncio.sleep(delay)\n    print(f\"{counter} awakens, refreshed\")\nasync def sleepers(how_many: int = 5) -> None:\n    print(f\"Creating {how_many} tasks\")\n    tasks = [\n        asyncio.create_task(random_sleep(i)) \n        for i in range(how_many)]\n    print(f\"Waiting for {how_many} tasks\")\n    await asyncio.gather(*tasks)\nif __name__ == \"__main__\":\n    asyncio.run(sleepers(5))\n    print(\"Done with the sleepers\") \n```", "```py\npython src/async_1.py \nCreating 5 tasks\nWaiting for 5 tasks\n0 sleeps for 4.69 seconds\n1 sleeps for 1.59 seconds\n2 sleeps for 4.57 seconds\n3 sleeps for 3.45 seconds\n4 sleeps for 0.77 seconds\n4 awakens, refreshed\n1 awakens, refreshed\n3 awakens, refreshed\n2 awakens, refreshed\n0 awakens, refreshed\nDone with the sleepers \n```", "```py\nSIZE_FORMAT = \">L\"\nSIZE_BYTES = struct.calcsize(SIZE_FORMAT)\nasync def log_catcher(\n    reader: asyncio.StreamReader, writer: asyncio.StreamWriter\n) -> None:\n    count = 0\n    client_socket = writer.get_extra_info(\"socket\")\n    size_header = await reader.read(SIZE_BYTES)\n    while size_header:\n        payload_size = struct.unpack(SIZE_FORMAT, size_header)\n        bytes_payload = await reader.read(payload_size[0])\n        await log_writer(bytes_payload)\n        count += 1\n        size_header = await reader.read(SIZE_BYTES)\n    print(f\"From {client_socket.getpeername()}: {count} lines\") \n```", "```py\nTARGET: TextIO\nLINE_COUNT = 0\ndef serialize(bytes_payload: bytes) -> str:\n    object_payload = pickle.loads(bytes_payload)\n    text_message = json.dumps(object_payload)\n    TARGET.write(text_message)\n    TARGET.write(\"\\n\")\n    return text_message\nasync def log_writer(bytes_payload: bytes) -> None:\n    global LINE_COUNT\n    LINE_COUNT += 1\n    text_message = await asyncio.to_thread(serialize, bytes_payload) \n```", "```py\nfrom __future__ import annotations\nimport asyncio\nimport asyncio.exceptions\nimport json\nfrom pathlib import Path\nfrom typing import TextIO\nimport pickle\nimport signal\nimport struct\nimport sys \n```", "```py\nserver: asyncio.AbstractServer\nasync def main(host: str, port: int) -> None:\n    global server\n    server = await asyncio.start_server(\n        log_catcher,\n        host=host,\n        port=port,\n    )\n    if sys.platform != \"win32\":\n        loop = asyncio.get_running_loop()\n        loop.add_signal_handler(signal.SIGTERM, server.close)\n    if server.sockets:\n        addr = server.sockets[0].getsockname()\n        print(f\"Serving on {addr}\")\n    else:\n        raise ValueError(\"Failed to create server\")\n    async with server:\n        await server.serve_forever() \n```", "```py\nif sys.platform == \"win32\":\n    from types import FrameType\n    def close_server(signum: int, frame: FrameType) -> None:\n        # print(f\"Signal {signum}\")\n        server.close()\n    signal.signal(signal.SIGINT, close_server)\n    signal.signal(signal.SIGTERM, close_server)\n    signal.signal(signal.SIGABRT, close_server)\n    signal.signal(signal.SIGBREAK, close_server) \n```", "```py\nif __name__ == \"__main__\":\n    # These often have command-line or environment overrides\n    HOST, PORT = \"localhost\", 18842\n    with Path(\"one.log\").open(\"w\") as TARGET:\n        try:\n            if sys.platform == \"win32\":\n                # https://github.com/encode/httpx/issues/914\n                loop = asyncio.get_event_loop()\n                loop.run_until_complete(main(HOST, PORT))\n                loop.run_until_complete(asyncio.sleep(1))\n                loop.close()\n            else:\n                asyncio.run(main(HOST, PORT))\n        except (\n                asyncio.exceptions.CancelledError, \n                KeyboardInterrupt):\n            ending = {\"lines_collected\": LINE_COUNT}\n            print(ending)\n            TARGET.write(json.dumps(ending) + \"\\n\") \n```", "```py\nfrom __future__ import annotations\nimport abc\nfrom itertools import permutations\nimport logging\nimport logging.handlers\nimport os\nimport random\nimport time\nimport sys\nfrom typing import Iterable\nlogger = logging.getLogger(f\"app_{os.getpid()}\")\nclass Sorter(abc.ABC):\n    def __init__(self) -> None:\n        id = os.getpid()\n        self.logger = logging.getLogger(            f\"app_{id}.{self.__class__.__name__}\")\n    @abc.abstractmethod\n    def sort(self, data: list[float]) -> list[float]:\n        ... \n```", "```py\nclass BogoSort(Sorter):\n    @staticmethod\n    def is_ordered(data: tuple[float, ...]) -> bool:\n        pairs: Iterable[Tuple[float, float]] = zip(data, data[1:])\n        return all(a <= b for a, b in pairs)\n    def sort(self, data: list[float]) -> list[float]:\n        self.logger.info(\"Sorting %d\", len(data))\n        start = time.perf_counter()\n        ordering: Tuple[float, ...] = tuple(data[:])\n        permute_iter = permutations(data)\n        steps = 0\n        while not BogoSort.is_ordered(ordering):\n            ordering = next(permute_iter)\n            steps += 1\n        duration = 1000 * (time.perf_counter() - start)\n        self.logger.info(\n            \"Sorted %d items in %d steps, %.3f ms\", \n            len(data), steps, duration)\n        return list(ordering) \n```", "```py\ndef main(workload: int, sorter: Sorter = BogoSort()) -> int:\n    total = 0\n    for i in range(workload):\n        samples = random.randint(3, 10)\n        data = [random.random() for _ in range(samples)]\n        ordered = sorter.sort(data)\n        total += samples\n    return total\nif __name__ == \"__main__\":\n    LOG_HOST, LOG_PORT = \"localhost\", 18842\n    socket_handler = logging.handlers.SocketHandler(\n        LOG_HOST, LOG_PORT)\n    stream_handler = logging.StreamHandler(sys.stderr)\n    logging.basicConfig(\n        handlers=[socket_handler, stream_handler], \n        level=logging.INFO)\n    start = time.perf_counter()\n    workload = random.randint(10, 20)\n    logger.info(\"sorting %d collections\", workload)\n    samples = main(workload, BogoSort())\n    end = time.perf_counter()\n    logger.info(\n        \"sorted %d collections, taking %f s\", workload, end - start)\n    logging.shutdown() \n```", "```py\nimport asyncio\nimport httpx\nimport re\nimport time\nfrom urllib.request import urlopen\nfrom typing import Optional, NamedTuple\nclass Zone(NamedTuple):\n    zone_name: str\n    zone_code: str\n    same_code: str  # Special Area Messaging Encoder\n    @property\n    def forecast_url(self) -> str:\n        return (\n            f\"https://tgftp.nws.noaa.gov/data/forecasts\"\n            f\"/marine/coastal/an/{self.zone_code.lower()}.txt\"\n        ) \n```", "```py\nZONES = [\n    Zone(\"Chesapeake Bay from Pooles Island to Sandy Point, MD\", \n        \"ANZ531\", \"073531\"),\n    Zone(\"Chesapeake Bay from Sandy Point to North Beach, MD\",      \n       \"ANZ532\", \"073532\"),\n. . . \n] \n```", "```py\nclass MarineWX:\n    advisory_pat = re.compile(r\"\\n\\.\\.\\.(.*?)\\.\\.\\.\\n\", re.M | re.S)\n    def __init__(self, zone: Zone) -> None:\n        super().__init__()\n        self.zone = zone\n        self.doc = \"\"\n    async def run(self) -> None:\n        async with httpx.AsyncClient() as client:\n            response = await client.get(self.zone.forecast_url)\n        self.doc = response.text\n    @property\n    def advisory(self) -> str:\n        if (match := self.advisory_pat.search(self.doc)):\n            return match.group(1).replace(\"\\n\", \" \")\n        return \"\"\n    def __repr__(self) -> str:\n        return f\"{self.zone.zone_name} {self.advisory}\" \n```", "```py\nasync def task_main() -> None:\n    start = time.perf_counter()\n    forecasts = [MarineWX(z) for z in ZONES]\n    await asyncio.gather(\n        *(asyncio.create_task(f.run()) for f in forecasts))\n    for f in forecasts:\n        print(f)\n    print(\n        f\"Got {len(forecasts)} forecasts \"\n        f\"in {time.perf_counter() - start:.3f} seconds\"\n    )\nif __name__ == \"__main__\":\n    asyncio.run(main()) \n```", "```py\nFORKS: List[asyncio.Lock]\nasync def philosopher(\n        id: int,\n        footman: asyncio.Semaphore\n) -> tuple[int, float, float]:\n    async with footman:\n        async with FORKS[id], FORKS[(id + 1) % len(FORKS)]:\n            eat_time = 1 + random.random()\n            print(f\"{id} eating\")\n            await asyncio.sleep(eat_time)\n        think_time = 1 + random.random()\n        print(f\"{id} philosophizing\")\n        await asyncio.sleep(think_time)\n    return id, eat_time, think_time \n```", "```py\nfrom __future__ import annotations\nimport asyncio\nimport collections\nimport random\nfrom typing import List, Tuple, DefaultDict, Iterator \n```", "```py\nasync def main(faculty: int = 5, servings: int = 5) -> None:\n    global FORKS\n    FORKS = [asyncio.Lock() for i in range(faculty)]\n    footman = asyncio.BoundedSemaphore(faculty - 1)\n    for serving in range(servings):\n        department = (\n            philosopher(p, footman) for p in range(faculty))\n        results = await asyncio.gather(*department)\n        print(results)\nif __name__ == \"__main__\":\n    asyncio.run(main()) \n```", "```py\nfor k in range(1, 41, 2):\n    for algo in ED(), MD(), CD(), SD():\n        h = Hyperparameter(k, algo, td)\n        print(h.test()) \n```", "```py\ndef test(self) -> \"Hyperparameter\":\n    \"\"\"Run the entire test suite.\"\"\"\n    pass_count, fail_count = 0, 0\n    for sample in self.data.testing:\n        sample.classification = self.classify(sample)\n        if sample.matches():\n            pass_count += 1\n        else:\n            fail_count += 1\n    self.quality = pass_count / (pass_count + fail_count)\n    return self \n```", "```py\ndef grid_search_1() -> None:\n    td = TrainingData(\"Iris\")\n    source_path = Path.cwd().parent / \"bezdekiris.data\"\n    reader = CSVIrisReader(source_path)\n    td.load(reader.data_iter())\n    tuning_results: List[Hyperparameter] = []\n    with futures.ProcessPoolExecutor(8) as workers:\n        test_runs: List[futures.Future[Hyperparameter]] = []\n        for k in range(1, 41, 2):\n            for algo in ED(), MD(), CD(), SD():\n                h = Hyperparameter(k, algo, td)\n                test_runs.append(workers.submit(h.test))\n        for f in futures.as_completed(test_runs):\n            tuning_results.append(f.result())\n    for result in tuning_results:\n        print(\n            f\"{result.k:2d} {result.algorithm.__class__.__name__:2s}\"\n            f\" {result.quality:.3f}\"\n        ) \n```", "```py\n 5 ED 0.967\n 5 MD 0.967\n 5 CD 0.967\n 5 SD 0.967\n 7 ED 0.967\n 7 MD 0.967\n 7 CD 1.000\n 7 SD 0.967\n 9 ED 0.967\n 9 MD 0.967\n 9 CD 1.000\n 9 SD 0.967 \n```", "```py\nSummary Statistics:\n                 Min  Max   Mean    SD    Class Correlation\n   sepal length: 4.3  7.9   5.84  0.83    0.7826   \n    sepal width: 2.0  4.4   3.05  0.43   -0.4194\n   petal length: 1.0  6.9   3.76  1.76    0.9490  (high!)\n    petal width: 0.1  2.5   1.20  0.76    0.9565  (high!) \n```"]