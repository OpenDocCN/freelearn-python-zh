- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ”Just as the wise accepts gold after testing it by heating, cutting, and rubbing
    it, so are my words to be accepted after examining them, but not out of respect
    for me.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – Buddha
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We love this quote by the Buddha. Within the software world, it translates perfectly
    into the healthy habit of never trusting code just because someone smart wrote
    it or because it has been working fine for a long time. If it has not been tested,
    the code is not to be trusted.
  prefs: []
  type: TYPE_NORMAL
- en: Why are tests so important? Well, for one, they give you predictability. Or,
    at least, they help you achieve high predictability. Unfortunately, there is always
    some bug that sneaks into the code. But we want our code to be as predictable
    as possible. What we do not want is to have a surprise; in other words, our code
    behaving in an unpredictable way. Unpredictability in software that checks the
    sensors of a plane, a train, or a nuclear power plant can lead to disastrous situations.
  prefs: []
  type: TYPE_NORMAL
- en: We need to test our code; we need to check that its behavior is correct, that
    it works as expected when it deals with edge cases, that it does not hang when
    the components it is talking to are broken or unreachable, that the performance
    is well within the acceptable range, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is all about that—making sure that your code is prepared to face
    the scary outside world, that it is fast enough, and that it can deal with unexpected
    or exceptional conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to explore the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: General testing guidelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief mention of **test-driven development** ( **TDD** )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let us start by understanding what testing is.
  prefs: []
  type: TYPE_NORMAL
- en: Testing your application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many kinds of tests; so many, in fact, that companies often have a
    dedicated department, called **quality assurance** ( **QA** ), made up of individuals
    whose job is to test the software the company developers produce.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start making an initial classification, we can divide tests into two broad
    categories: **white-box** and **black-box** tests.'
  prefs: []
  type: TYPE_NORMAL
- en: White-box tests are those that exercise the internals of the code; they inspect
    it down to a fine level of detail. On the other hand, black-box tests are those
    that consider the software under test as if within a box, the internals of which
    are ignored. Even the technology, or the language used inside the box, is not
    important for black-box tests. What they do is plug some input into one end of
    the box and verify the output at the other end—that’s it.
  prefs: []
  type: TYPE_NORMAL
- en: There is also an in-between category called **gray-box** testing, which involves
    testing a system in the same way we do with the black-box approach, but having
    some knowledge about the algorithms and data structures used to write the software
    and only partial access to its source code.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many kinds of tests in these categories, each of which serves a different
    purpose. To give you an idea, here are a few:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Frontend tests** : They make sure that the client side of your application
    is exposing the information that it should, all the links, the buttons, the advertising,
    and everything that needs to be shown to the client. They may also verify that
    it is possible to walk a certain path through the **user interface** ( **UI**
    ).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scenario tests** : They make use of stories (or scenarios) that help the
    tester work through a complex problem or test a part of the system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration tests** : They verify the behavior of the various components
    of your application when they are working together and sending messages through
    interfaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Smoke tests** : Particularly useful when you deploy a new update on your
    application, they check whether the most essential, vital parts of your application
    are still working as they should and that they are not *on fire* . This term comes
    from when engineers tested circuits by making sure nothing was smoking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Acceptance tests** , or **user acceptance testing** ( **UAT** ): What a developer
    does with a product owner (for example, in a SCRUM environment) to determine whether
    the work that was commissioned was carried out correctly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Functional tests** : They verify the features or functionalities of your
    software.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Destructive tests** : They take down parts of your system, simulating a failure,
    to establish how well the remaining parts of the system perform. This kind of
    test is performed extensively by companies that need to provide a highly reliable
    service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance tests** : They aim to verify how well the system performs under
    a specific load of data or traffic so that, for example, engineers can get a better
    understanding of the bottlenecks in the system that could bring it to its knees
    in a heavy-load situation, or those that prevent scalability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usability tests** , and the closely related **user experience** ( **UX**
    ) **tests** : They aim to check whether the UI is simple and easy to understand
    and use. They also aim to provide input to the designers so that the UX is improved.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and penetration tests** : They aim to verify how well the system
    is protected against attacks and intrusions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unit tests** : They help the developer write the code in a robust and consistent
    way, providing the first line of feedback and defense against coding mistakes,
    refactoring mistakes, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression tests** : They provide the developer with useful information about
    a feature being compromised in the system after an update. Some of the causes
    for a system being said to have a regression are an old bug resurfacing, an existing
    feature being compromised, or a new issue being introduced.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many books and articles have been written about testing, and we have to point
    you to those resources if you are interested in finding out more about all the
    different kinds of tests. In this chapter, we will concentrate on unit tests,
    since they are the backbone of software crafting and form the vast majority of
    tests that are written by a developer.
  prefs: []
  type: TYPE_NORMAL
- en: Testing is an *art* , an art that you do not learn from books, unfortunately.
    You can learn all the definitions (and you should) and try to collect as much
    knowledge about testing as you can, but you will likely be able to test your software
    properly only when you have accumulated enough experience.
  prefs: []
  type: TYPE_NORMAL
- en: When you are having trouble refactoring a bit of code because every little thing
    you touch makes a test blow up, you learn how to write less rigid and limiting
    tests that still verify the correctness of your code but, at the same time, allow
    you the freedom and joy to play with it, to shape it as you want.
  prefs: []
  type: TYPE_NORMAL
- en: When you are being called too often to fix unexpected bugs in your code, you
    learn how to write tests more thoroughly, how to come up with a more comprehensive
    list of edge cases, and strategies to cope with them before they turn into bugs.
  prefs: []
  type: TYPE_NORMAL
- en: When you are spending too much time reading tests and trying to refactor them
    to change a small feature in the code, you learn to write simpler, shorter, and
    better-focused tests.
  prefs: []
  type: TYPE_NORMAL
- en: We could go on with this *when you... you learn...* , but we guess you get the
    picture. You need to apply yourself and build experience. Our suggestion? Study
    the theory as much as you can, and then experiment using different approaches.
    Also, try to learn from experienced coders; it is very effective.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, the more experienced you become, the more you should feel that source
    code and unit tests are not two separate things. Tests are not optional. They
    are intimately connected to the code. Source code and unit tests mutually influence
    each other.
  prefs: []
  type: TYPE_NORMAL
- en: The anatomy of a test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we concentrate on unit tests, let us see what a test is, and what its
    purpose is.
  prefs: []
  type: TYPE_NORMAL
- en: A **test** is a piece of code whose purpose is to verify something in our system.
    It may be that we are calling a function passing two integers, that an object
    has a property called `donald_duck` , or that when you place an order on some
    **application programming interface** ( **API** ), after a minute you can see
    it dissected into its basic elements in the database.
  prefs: []
  type: TYPE_NORMAL
- en: 'A test is typically composed of three sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preparation** : This is where we set up the scene. We prepare all the data,
    the objects, and the services we need in the places we need them so that they
    are ready to be used.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Execution** : This is where we execute the bit of logic that is under testing.
    We perform an action using the data and the interfaces we set up in the preparation
    phase.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Verification** : This is where we verify the results and make sure they are
    according to our expectations. We check the returned value of a function, or that
    some data is in the database, some is not, some has changed, an HTTP request has
    been made, something has happened, a method has been called, and so on.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While tests usually follow this structure, in a test suite, you will typically
    find some other constructs that take part in the testing process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Setup** : This is something quite commonly found in several tests. It is
    logic that can be customized to run for every test, class, module, or even for
    a whole session. In this phase, developers usually set up connections to databases,
    populate them with data that will be needed there for the test to make sense,
    and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Teardown** : This is the opposite of the setup; the teardown phase takes
    place after the tests have run. Like the setup, it can be customized to run for
    every test, class, module, or session. Typically, in this phase, we destroy any
    artifacts that were created for the test suite and clean up after ourselves. This
    is important because we do not want to have any lingering objects around and because
    it helps to make sure that each test starts from a clean slate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fixtures** : These are pieces of data used in the tests. By using a specific
    set of fixtures, outcomes are predictable and therefore tests can perform verifications
    against them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will use the `pytest` Python library. It is a powerful tool
    that makes testing easier than it would be if we only used standard library tools.
    `pytest` provides plenty of helpers so that the test logic can focus more on the
    actual testing than the wiring and boilerplate around it. You will see, when we
    get to the code, that one of the characteristics of `pytest` is that fixtures,
    setup, and teardown often blend into one.
  prefs: []
  type: TYPE_NORMAL
- en: Testing guidelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like software, tests can be good or bad, with a whole range of shades in the
    middle. To write good tests, here are some guidelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Keep them as simple as possible** : It is okay to violate some good coding
    rules, such as hardcoding values or duplicating code. Tests need, first and foremost,
    to be as readable as possible and easy to understand. When tests are hard to read
    or understand, we can never be confident they are actually making sure our code
    is performing correctly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tests should verify one thing and one thing only** : It is important that
    we keep them short and contained. It is perfectly fine to write multiple tests
    to exercise a single object or function. We just need to make sure that each test
    has one and only one purpose.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tests should not make any unnecessary assumptions** : This may be tricky
    to understand at first, but it is important. Verifying that the result of a function
    call is `[1, 2, 3]` is not the same as saying the output is a list that contains
    the numbers 1, 2, and 3. In the former, we are also assuming the ordering; in
    the latter, we are only assuming which items are in the list. The differences
    sometimes are quite subtle, but they are still important.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tests should exercise the “what,” rather than the “how”** : Tests should
    focus on checking *what* a function is supposed to do, rather than *how* it is
    doing it. For example, focus on the fact that a function is calculating the square
    root of a number (the *what* ), instead of the fact that it is calling `math.sqrt()`
    to do it (the *how* ). Unless we are writing performance tests or we have a particular
    need to verify how a certain action is performed, we ought to try to avoid this
    type of testing and focus on the *what* . Testing the *how* leads to restrictive
    tests and makes refactoring hard. Moreover, the type of test we have to write
    when we concentrate on the *how* is more likely to degrade the quality of our
    testing codebase when we amend the software frequently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tests should use the minimal set of fixtures needed to do the job** : This
    is another crucial point. Fixtures tend to grow over time. They also tend to change
    every now and then. If we use many fixtures and ignore redundancies in the tests,
    refactoring will take longer. Spotting bugs will be harder. We ought to try to
    use a set of fixtures that is big enough for the test to perform correctly, but
    not any bigger.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tests should use as few resources as possible** : The reason for this is
    that every developer who checks out our code should be able to run the tests,
    no matter how powerful their machine is. It could be a skinny virtual machine
    or a CircleCI setup; tests should run without chewing up too many resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tests should run as fast as possible** : A good test codebase could end up
    being much longer than the code being tested itself. It varies according to the
    situation and the developer, but, whatever the length, we will end up having hundreds,
    if not thousands, of tests to run, which means the faster they run, the faster
    we can get back to writing code. When using **TDD** , for example, we run tests
    very often, so speed is essential.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CircleCI** is one of the largest **continuous integration/continuous delivery**
    ( **CI/CD)** platforms available today. It is easy to integrate with services
    like GitHub, for example. You just need to add some configuration (typically in
    the form of a file) in the source code, and CircleCI will run tests when the new
    code is prepared to be merged into the current codebase.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Unit testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have an idea about what testing is and why we need it, let us introduce
    the developer’s best friend: the **unit test** .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we proceed with the examples, allow us to share some words of caution:
    we will try to give you the fundamentals about unit testing, but we do not follow
    any particular school of thought or methodology to the letter. Over the years,
    we have tried many different testing approaches, eventually coming up with our
    own way of doing things, which is constantly evolving. To put it as Bruce Lee
    would have:'
  prefs: []
  type: TYPE_NORMAL
- en: Absorb what is useful, discard what is useless, and add what is specifically
    your own.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a unit test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unit tests take their name from the fact that they are used to test small units
    of code. To explain how to write a unit test, let us look at a simple snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `get_clean_data()` function is responsible for getting data from `source`
    , cleaning it, and returning it to the caller. How do we test this function?
  prefs: []
  type: TYPE_NORMAL
- en: One way of doing this is to call it and then make sure that `load_data()` was
    called once with `source` as its only argument. Then, we need to verify that `clean_data()`
    was called once, with the return value of `load_data()` . Finally, we would need
    to make sure that the return value of `clean_data()` is what is returned by the
    `get_clean_data()` function as well.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, we need to set up the source and run this code, and this may be
    a problem. One of the golden rules of unit testing is that *anything that crosses
    the boundaries of your application needs to be simulated* . We do not want to
    talk to a real data source, and we do not want to actually run real functions
    if they are communicating with anything that is not contained in our application.
    A few examples would be a database, a search service, an external API, or the
    filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: We need these restrictions to act as a shield so that we can always run our
    tests safely without the fear of destroying something in a real data source.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason is that it may be quite difficult for a developer to reproduce
    the whole architecture on their machine. It may require the setting up of databases,
    APIs, services, files and folders, and so on, and this can be difficult, time-consuming,
    or sometimes not even possible.
  prefs: []
  type: TYPE_NORMAL
- en: Very simply put, an **API** is a set of tools for building software applications.
    An API expresses a software component in terms of its operations, input and output,
    and underlying types. For example, if you create software that needs to interface
    with a data provider service, it is likely that you will have to go through their
    API in order to gain access to the data.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in our unit tests, we need to simulate all those things in some way.
    Unit tests need to be run by any developer without the need for the entire system
    to be set up on their machine.
  prefs: []
  type: TYPE_NORMAL
- en: A different approach, which we favor when it is possible to do so, is to simulate
    entities not by using fake objects but by using special-purpose test objects instead.
    For example, if our code talks to a database, instead of faking all the functions
    and methods that talk to the database and programming the fake objects so that
    they return what the real ones would, we would rather spawn a test database, set
    up the tables and data we need, and then patch the connection settings so that
    our tests are running real code against the test database. This is advantageous
    because if the underlying libraries change in a way that introduces an issue in
    our code, this setup will catch this issue. A test will break. A test with mocks,
    on the other hand, will blissfully continue to run successfully, because the mocked
    interface would have no idea about the change in the underlying library. In-memory
    databases are excellent options for these cases.
  prefs: []
  type: TYPE_NORMAL
- en: One of the applications that allows you to spawn a database for testing is Django.
    Within the `django.test` package, you can find several tools that help you write
    tests so that you won’t have to simulate the dialog with a database. By writing
    tests this way, you will also be able to check on transactions, encodings, and
    all other database-related aspects of programming. Another advantage of this approach
    consists of the ability to check against details that can change from one database
    to another.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, though, it is still not possible. For example, when the software
    interfaces with an API, and there is no test version of that API, we would need
    to simulate that API using fakes. In reality, most of the time we end up having
    to use a hybrid approach, where we use a test version of those technologies that
    allow this approach, and we use fakes for everything else. Let us now talk about
    fakes.
  prefs: []
  type: TYPE_NORMAL
- en: Mock objects and patching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First of all, in Python, these fake objects are called **mocks** . Up to version
    3.3, the `mock` library was a third-party library that basically every project
    would install via pip but, from version 3.3, it has been included in the standard
    library under the `unittest` module, and rightfully so, given its importance and
    how widespread it is.
  prefs: []
  type: TYPE_NORMAL
- en: The act of replacing a real object or function (or in general, any piece of
    data structure) with a `mock` is called **patching** . The `mock` library provides
    the `patch` tool, which can act as a function or class decorator, and even as
    a context manager that you can use to `mock` things out.
  prefs: []
  type: TYPE_NORMAL
- en: Assertions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The verification phase is done through the use of assertions. In most cases,
    an **assertion** is a function or method that you can use to verify equality between
    objects, as well as other conditions. When a condition is not met, the assertion
    will raise an exception that will cause the test to fail. You can find a list
    of assertions in the `unittest` module documentation; however, when using `pytest`
    , you will typically use the generic `assert` statement, which makes things even
    simpler.
  prefs: []
  type: TYPE_NORMAL
- en: Testing a CSV generator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us now adopt a practical approach. We will show you how to test a small
    piece of code, and we will touch on the rest of the important concepts around
    unit testing within the context of this example.
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to write an `export()` function that does the following: it takes a
    list of dictionaries, each of which represents a user. It creates a **comma-separated
    values** ( **CSV** ) file, puts a header in it, and then proceeds to add all the
    users who are deemed valid according to some rules. The function will take three
    parameters: the list of user dictionaries, the name of the CSV file to create,
    and an indication of whether an existing file with the same name should be overwritten.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To be considered valid, and added to the output file, a user dictionary must
    satisfy the following requirements: each user must have at least an email, a name,
    and an age. There can also be a fourth field representing the role, but it is
    optional. The user’s email address needs to be valid, the name needs to be non-empty,
    and the age must be an integer between 18 and 65.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is our task; so, now we are going to show you the code, and then we are
    going to analyze the tests we wrote for it. But, first things first, in the following
    code snippets, we will be using two third-party libraries: `marshmallow` and `pytest`
    . They are both in the requirements of the chapter’s source code, so please make
    sure you have installed them with pip.'
  prefs: []
  type: TYPE_NORMAL
- en: '`marshmallow` ( [https://marshmallow.readthedocs.io/](https://marshmallow.readthedocs.io/)
    ) is a library that provides us with the ability to serialize (or *dump* , in
    `marshmallow` terminology) and deserialize (or *load* , in `marshmallow` terminology)
    objects and, most importantly, gives us the ability to define a schema that we
    can use to validate a user dictionary. We will see another library that is used
    to create schemas, `pydantic` , in *Chapter 14* , *Introduction to API Development*
    .'
  prefs: []
  type: TYPE_NORMAL
- en: '`pytest` ( [https://docs.pytest.org/](https://docs.pytest.org/) ) is one of
    the best pieces of software we have ever seen. It is used almost everywhere and
    has replaced other libraries, such as *nose* . It provides us with useful tools
    to write tests quite efficiently.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us get to the code. We called it `api.py` just because it exposes a function
    that we can use to export the CSV. We will show it to you in chunks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This first part is where we import all that we need ( `Path` , `csv` , `deepcopy`
    , and some tools from `marshmallow)` , and then we define the schema for the users.
    As you can see, we inherit from `marshmallow.Schema` , and then we set four fields.
    Notice we are using two string fields ( `Str` ), an `Email` , and an integer (
    `Int` ). These will already provide us with some validation from `marshmallow`
    . Notice there is no `required=True` in the `role` field.
  prefs: []
  type: TYPE_NORMAL
- en: We need to add a couple of custom bits of code, though. We need to add validation
    on `age` to make sure the value is within the range we want. `marshmallow` will
    raise `ValidationError` if it is not. It will also take care of raising an error
    should we pass anything but an integer.
  prefs: []
  type: TYPE_NORMAL
- en: We also add validation on `name` , because the fact that there is a `name` key
    in a dictionary does not guarantee that the value of that name is non-empty. We
    validate that the length of the field’s value is at least one. Notice we do not
    need to add anything for the `email` field. This is because `marshmallow` will
    validate it for us.
  prefs: []
  type: TYPE_NORMAL
- en: After the field declarations, we write another method, `strip_name()` , which
    is decorated with the `pre_load()` `marshmallow` helper. This method will run
    before `marshmallow` deserializes (loads) the data. As you can see, we make a
    copy of `data` first, as in this context it is not a good idea to work directly
    on a mutable object, and then make sure we strip leading and trailing spaces away
    from `data['name']` . That key represents the name field we just declared above.
    We make sure we do this within a `try` / `except` block, so deserialization can
    run smoothly even in case of errors. The method returns the modified copy of `data`
    , and `marshmallow` does the rest.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then instantiate `schema` , so that we can use it to validate data. So,
    let us write the `export` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As you see, its logic is straightforward. If `overwrite` is `False` and the
    file already exists, we raise `IOError` with a message saying the file already
    exists. Otherwise, if we can proceed, we simply get the list of valid users and
    feed it to `write_csv()` , which is responsible for actually doing the job. Let
    us see how all these functions are defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We coded `get_valid_users()` as a generator, as there is no need to make a potentially
    big list before writing to a file. We can validate and save them one by one. The
    `is_valid()` function simply delegates to `marshmallow` 's `schema.validate()`
    to validate the user. This method returns a dictionary, which is empty if the
    data is valid according to the schema or else it will contain error information.
    We do not need to collect the error information for this task, so we simply ignore
    it, and the `is_valid()` function simply returns `True` if the return value from
    `schema.validate()` is empty, or `False` otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final piece of code in this module is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Again, the logic is straightforward. We define the header in `fieldnames` ,
    then we open `filename` for writing, and we specify `newline=""` , which is recommended
    in the documentation for CSV files. When the file has been created, we get a `writer`
    object by using the `csv.DictWriter` class. This tool maps the user dictionaries
    to the field names, so we do not need to take care of the ordering.
  prefs: []
  type: TYPE_NORMAL
- en: We write the header first, and then we loop over the users and add them one
    by one. Notice that this function assumes it is fed a list of valid users, and
    it may break if that assumption is false (with the default values, it would break
    if any user dictionary had extra fields).
  prefs: []
  type: TYPE_NORMAL
- en: That’s the code you should try and keep in mind. We suggest you spend a moment
    going through it again. There is no need to memorize it, and the fact that we
    have used small helper functions with meaningful names will enable you to follow
    the testing more easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us now get to the interesting part: testing the `export()` function. Once
    again, we will show you the code in chunks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us start with the imports: first, we import the `re` module from the standard
    library, as it’s needed in one of the tests. Then, we bring in some tools from
    `unittest.mock` , then `pytest` , and, finally, we fetch the three functions that
    we want to actually test: `is_valid()` , `export()` , and `write_csv()` .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can write tests, though, we need to make a few fixtures. As you will
    see, a **fixture** in `pytest` is a function decorated with the `pytest.fixture`
    decorator. Fixtures are run before each test to which they are applied. In most
    cases, we expect a fixture to return something so that we can use it in a test.
    We have some requirements for a user dictionary, so let us write a couple of users:
    one with minimal requirements, and one with full requirements. Both need to be
    valid. Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the only difference between the users is the presence of the
    `role` key, but it should be enough to show you the point.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that instead of simply declaring dictionaries at a module level, we have
    actually written two functions that return a dictionary, and we have decorated
    them with the `@pytest.fixture` decorator. This is because when you declare a
    dictionary that is supposed to be used in your tests at the module level, you
    need to make sure you copy it at the beginning of every test. If you do not, and
    any of the tests (or the code being tested) modify it, all the following tests
    might be compromised, as the dictionary would not be in its original form. By
    using these fixtures, `pytest` will give us a new dictionary for every test, so
    we do not need to go through that copy procedure. This helps to respect the principle
    of independence, which says that each test should be self-contained and independent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fixtures are also *composable* , which means they can be used in one another,
    which is a useful feature of `pytest` . To show you this, let us write a fixture
    for a list of users, in which we put the two we already have, plus one that would
    fail validation because it has no age. Let us take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We now have two users that we can use individually, and we also have a list
    of three users.
  prefs: []
  type: TYPE_NORMAL
- en: The first few tests will test how we validate a user. We will group all the
    tests for this task within a class. This helps to give related tests a namespace,
    a place to be. As we will see later, it also allows us to declare class-level
    fixtures, which are defined just for the tests belonging to the class. One of
    the benefits of declaring a fixture at a class level is that you can easily override
    one with the same name that lives outside the scope of the class.
  prefs: []
  type: TYPE_NORMAL
- en: Although, in this case, we found it convenient to organize our tests in classes,
    you can also just have tests defined at the module level. `pytest` allows for
    great flexibility in the way tests are structured.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, you will notice, as we walk you through the examples, that the name
    of each test function starts with `test_` and that of each test class starts with
    `Test` . This is to allow `pytest` to discover these functions and classes and
    consider them as tests. Please refer to the pytest documentation to learn the
    full specifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us come back to our code now. Take a look at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We start very simply by making sure our fixtures actually pass validation. This
    helps ensure that our code will correctly validate users that we know to be valid,
    with minimal as well as full data. Notice that we gave each test function a parameter
    matching the name of a fixture. This has the effect of activating the fixture
    for that test. When `pytest` runs the tests, it will inspect the parameters of
    each test and pass the return values of the corresponding fixture functions as
    arguments to the test.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we proceed, it would be good to run these two tests, just to make sure
    everything is wired up correctly. To run the tests, we invoke the `pytest` command
    in the shell, from the `ch10` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We have instructed the command to search for tests in the `tests` folder. Moreover,
    to show you the full details, we have invoked it with the verbose flag ( `-vv`
    ).
  prefs: []
  type: TYPE_NORMAL
- en: After a bit of boilerplate, we find two lines that we highlighted. They represent
    the full path to each of the tests that ran. First, the name of the module where
    the tests live, then in this case, the name of the class in which they are defined,
    and finally their names.
  prefs: []
  type: TYPE_NORMAL
- en: On the right, you can see the progression, indicated as a percentage. In this
    case, we only have two tests for now, so after running the first one, we have
    completed 50% of the test suite, and 100% after the second one. They both passed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Should any of the tests fail, `pytest` would print an error and some debug
    information, so we can inspect what is wrong and fix it. Let us simulate a failure
    by removing the `name` key from the `min_user` fixture and running the tests again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the highlighted sections, `pytest` reports which tests failed,
    and a snippet of the code where the failure happened, so we can inspect it and
    discover what the problem is. On the left-hand side of the snippet, there is a
    `>` sign, which indicates the line that threw the error, and underneath, two lines
    representing the error itself, which in this case is that `{''age'': 18, ''email'':
    ''minimal@example.com''}` is not a valid user.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to run tests, please feel free to run them any time you
    want. A good practice when we run tests is to make sure that they would fail if
    something was wrong, so feel free to play around with the fixtures and the assertions.
  prefs: []
  type: TYPE_NORMAL
- en: Let us go back to the test suite now. The next task is to test the age. To do
    that, we are going to use parametrization.
  prefs: []
  type: TYPE_NORMAL
- en: '**Parametrization** is a technique that enables us to run the same test multiple
    times but feed different data to it. It is quite useful as it allows us to write
    the test only once with no repetition, and the result will be intelligently handled
    by `pytest` , which will run all those tests as if they were actually separate,
    thus providing us with clear error messages when they fail. Another solution would
    be to write one test with a `for` loop inside that runs through all the pieces
    of data we want to test against. The latter solution is of much lower quality
    though, as the framework won’t be able to give you specific information as if
    you were running separate tests. Moreover, should any of the `for` loop iterations
    fail, there would be no information about what would have happened after that,
    as subsequent iterations will not happen. Finally, the body of the test would
    get more difficult to understand, due to the `for` loop extra logic. Therefore,
    parametrization is a far superior choice for this use case.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It also spares us from having to write a bunch of almost identical tests to
    exhaust all possible scenarios. Let us see how we test the age (we are repeating
    the class signature for you, but omitting the tests that have already been presented):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We start by writing a test to check that validation fails when the user is too
    young. According to our rule, a user is too young when they are younger than 18.
    We check for every age between 0 and 17 by using `range()` .
  prefs: []
  type: TYPE_NORMAL
- en: If you look at how the parametrization works, you see that we declare the name
    of an object and `age` and then we specify which values this object will take.
    The test will be run once for each of the specified values. In the case of this
    first test, the values are all those returned by `range(18)` , which means all
    integer numbers from 0 to 17 are included. Note that we also add an `age` parameter
    to the test. The values specified in the parameterization will be passed as arguments
    to the test through this parameter.
  prefs: []
  type: TYPE_NORMAL
- en: We also use the `min_user()` fixture in this test. In this case, we change the
    `age` within the `min_user()` dictionary, and then we verify that the result of
    `is_valid(min_user)` is `False` . We do this by asserting the fact that `not False`
    is `True` . In `pytest` , this is how you check for something. You simply assert
    that something is truthy. If that is the case, the test has succeeded. Should
    it instead be the opposite, the test will fail.
  prefs: []
  type: TYPE_NORMAL
- en: Note that `pytest` will re-evaluate the fixture function for each test run that
    uses it, so we are free to modify the fixture data within the test without affecting
    any other tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us proceed and add all the tests needed to make validation fail on the
    age:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Another two tests. One takes care of the other end of the spectrum, from 66
    years of age to 99. The second one instead makes sure that age is invalid when
    it is not an integer number, so we pass some values, such as a string, a float,
    and `None` , just to make sure. Notice how the structure of these tests is all
    the same, but, thanks to the parametrization, we feed different input arguments
    to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the age-failing logic sorted out, let us add a test that checks
    when age is within the valid range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: It is as easy as that. We pass the correct range, from 18 to 65, and remove
    the `not` in the assertion.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can consider the age as being taken care of. Let us move on to write tests
    on mandatory fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'These three tests still belong to the same class. The first one tests whether
    a user is invalid when one of the mandatory fields is missing. Remember that at
    every test run, the `min_user` fixture is restored, so we only have one missing
    field per test run, which is the appropriate way to check for mandatory fields.
    We simply remove that one key from the dictionary. This time, the parametrization
    object takes the name `field` , and, by looking at the first test, you see all
    the mandatory fields in the parametrization decorator: `email` , `name` , and
    `age` .'
  prefs: []
  type: TYPE_NORMAL
- en: In the second one, things are a little different. Instead of removing keys,
    we simply set them (one at a time) to the empty string. Finally, in the third
    one, we check for the name to be made of whitespace only.
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous tests take care of mandatory fields being there and being non-empty,
    and of the formatting around the `name` key of a user. Let us now write the last
    two tests for this class. We want to check that email is valid, and in the second
    one, the type for email, name, and role:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This time, the parametrization is slightly more complex. We define two objects
    ( `email` and `outcome` ) and then we pass a list of tuples, instead of a simple
    list, to the decorator. Each time the test is run, one of those tuples will be
    unpacked to fill the values of `email` and `outcome` , respectively. This allows
    us to write one test for both valid and invalid email addresses, instead of two
    separate ones. We define an email address, and we specify the outcome we expect
    from validation. The first four are invalid email addresses, and the last three
    are valid. We have used a couple of examples with non-ASCII characters, just to
    make sure we are not forgetting to include our friends from all over the world
    in the validation.
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the validation is done, asserting that the result of the call needs
    to match the outcome we have set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us now write a simple test to make sure validation fails when we feed the
    wrong type to the fields (again, the age has been taken care of separately before):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As we did before, we pass three different values, none of which is actually
    a string. This test could be expanded to include more values, but, honestly, we
    shouldn’t need to write tests such as this one. We have included it here just
    to show you what’s possible, but normally you would focus on making sure the code
    considers valid types, those that have to be considered valid, and that should
    be enough.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move to the next test class, let us take a moment to talk a bit more
    about something we briefly touched on when testing the age.
  prefs: []
  type: TYPE_NORMAL
- en: Boundaries and granularity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While checking for the age, we wrote three tests to cover the three ranges:
    0-17 (fail), 18-65 (success), and 66-99 (fail). Why did we do this? The answer
    lies in the fact that we are dealing with two boundaries: 18 and 65. So, our testing
    needs to focus on the three regions those two boundaries define: before 18, within
    18 and 65, and after 65. How you do it is not important, as long as you make sure
    you test the boundaries correctly. This means if someone changes the validation
    in the schema from `18 <= value <= 65` to `18 <= value < 65` (notice the second
    `<=` is now `<` ), there must be a test that fails on 65.'
  prefs: []
  type: TYPE_NORMAL
- en: This concept is known as a **boundary** , and it is crucial that you recognize
    them in your code so that you can test against them.
  prefs: []
  type: TYPE_NORMAL
- en: Another important thing is to understand how close to the boundaries to get.
    In other words, which unit should I use to approach them?
  prefs: []
  type: TYPE_NORMAL
- en: In the case of age, we are dealing with integers, so a unit of 1 will be the
    perfect choice (which is why we used 16, 17, 18, 19, 20, ...). But what if you
    were testing for a timestamp? Well, in that case, the correct granularity will
    likely be different. If the code has to act differently according to your timestamp
    and that timestamp represents seconds, then the granularity of your tests should
    zoom down to seconds. If the timestamp represents years, then years should be
    the unit you use. We hope you get the picture. This concept is known as **granularity**
    and needs to be combined with that of boundaries so that by going around the boundaries
    with the correct granularity, you can make sure your tests are not leaving anything
    to chance.
  prefs: []
  type: TYPE_NORMAL
- en: Let us now continue with our example and test the `export` function.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the export function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the same test module, we defined another class that represents a test suite
    for the `export()` function. Here it is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Let us start by analyzing the fixtures. We have defined them at the class level
    this time, which means they will be available for the tests in the same class.
    We do not need these fixtures outside of this class, so it does not make sense
    to declare them at a module level as we did with the user ones.
  prefs: []
  type: TYPE_NORMAL
- en: We need two files. If you recall what we wrote at the beginning of this chapter,
    when it comes to interaction with databases, disks, networks, and so on, we should
    `mock` everything out. However, when possible, we prefer to use a different technique.
    In this case, we will employ temporary folders, which will be created and deleted
    within the fixture. We are much happier if we can avoid mocking. To create temporary
    folders, we employ the `tmp_path` fixture, from `pytest` , which is a `pathlib.Path`
    object.
  prefs: []
  type: TYPE_NORMAL
- en: The first fixture, `csv_file()` , provides a reference to a temporary folder.
    We can consider the logic up to and including the `yield` as the setup phase.
    The fixture itself, in terms of data, is represented by the temporary filename.
    The file itself does not exist yet. When a test runs, the fixture is created,
    and at the end of the test, the rest of the fixture code (the part after `yield`
    , if any) is executed.
  prefs: []
  type: TYPE_NORMAL
- en: That part can be considered the teardown phase. In the case of the `csv_file()`
    fixture, it consists of calling `csv_path.unlink()` to delete the `.csv` file
    (if it exists). You can put much more in each phase of any fixture, and with experience,
    you will master the art of doing setup and teardown this way. It comes naturally
    quite quickly.
  prefs: []
  type: TYPE_NORMAL
- en: It is not strictly necessary to delete the `.csv` file after each test. The
    `tmp_path` fixture will create a new temporary directory for each test, so there
    is no risk of files created within this directory interfering with other tests.
    We chose to delete the file in this fixture only to demonstrate the use of `yield`
    in fixtures.
  prefs: []
  type: TYPE_NORMAL
- en: The second fixture, `existing_file()` , is quite similar to the first one, but
    we will use it to test that we can prevent overwriting when we call `export()`
    with `overwrite=False` . So, we create a file in the temporary folder, and we
    put some content into it, just to have the means to verify it hasn’t been touched.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us now see the tests (as we did before, we include the class declaration
    but omit tests which we already presented):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This test employs the `users()` and `csv_file()` fixtures, and immediately calls
    `export()` with them. We expect that a file has been created, and populated with
    the two valid users we have (remember the list contains three users, but one is
    invalid).
  prefs: []
  type: TYPE_NORMAL
- en: To verify that, we open the temporary file and collect all its text into a string.
    We then compare the content of the file with what we expect to be in it. Notice
    we only put the header, and the two valid users, in the correct order.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need another test to make sure that if there is a comma in one of the
    values, our CSV is still generated correctly. Being a **CSV** file, we need to
    make sure that a comma in the data does not break things up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This time, we do not need the whole users list; we just need one, as we are
    testing a specific thing and we have the previous test to make sure we are generating
    the file correctly with all the users. Remember, always try to minimize the work
    you do within a test.
  prefs: []
  type: TYPE_NORMAL
- en: So, we use `min_user()` and put a comma in its name. We then repeat the procedure,
    which is similar to that of the previous test, and finally, we make sure that
    the name is put in the CSV file surrounded by double quotes. This is enough for
    any good CSV parser to understand that they should not break the comma inside
    the double quotes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we want one more test, to check that when the file already exists and
    we do not want to override it, our code won’t do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This is an interesting test because it allows us to show you how you can tell
    `pytest` that you expect a function call to raise an exception. We do it in the
    context manager given to us by `pytest.raises()` , to which we feed the exception
    we expect from the call we make inside the body of that context manager. If the
    exception is not raised, the test will fail.
  prefs: []
  type: TYPE_NORMAL
- en: We like to be thorough in our tests, so we do not want to stop there. We also
    assert on the message, by using the convenient `err.match()` helper. Notice that
    we do not need to use an `assert` statement when calling `err.match()` . If the
    argument does not match, the call will raise an `AssertionError` , causing the
    test to fail. We also need to escape the string version of `existing_file` because
    on Windows, paths have backslashes, which would confuse the regular expression
    we feed to `err.match()` .
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we make sure that the file still contains its original content (which
    is why we created the `existing_file()` fixture) by reading it and comparing its
    content to the string we originally wrote to the file.
  prefs: []
  type: TYPE_NORMAL
- en: Final considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we move on to the next topic, let us wrap up with some considerations.
  prefs: []
  type: TYPE_NORMAL
- en: First, we hope you have noticed that we haven’t tested all the functions we
    wrote. Specifically, we didn’t test `get_valid_users()` , `validate()` , and `write_csv()`
    . The reason is that these functions are already implicitly tested by our test
    suite. We have tested `is_valid()` and `export()` , which is more than enough
    to make sure the schema is validating users correctly, and that the `export()`
    function is filtering out invalid users, respecting existing files when needed,
    and writing a proper CSV. The functions we haven’t tested are the internals; they
    provide logic that participates in doing something that we have thoroughly tested
    anyway.
  prefs: []
  type: TYPE_NORMAL
- en: Would adding extra tests for those functions be good or bad? The answer is actually
    difficult.
  prefs: []
  type: TYPE_NORMAL
- en: The more we test, the less easily we can refactor that code. As it is now, we
    could easily decide to rename `validate()` , and we wouldn’t have to change any
    of the tests we wrote. If you think about it, it makes sense, because as long
    as `validate()` provides correct validation to the `get_valid_users()` function,
    we do not really need to know about it.
  prefs: []
  type: TYPE_NORMAL
- en: If, instead, we had written tests for the `validate()` function, then we would
    have to change them, had we decided to rename it (or to change its signature,
    for example).
  prefs: []
  type: TYPE_NORMAL
- en: So, what is the right thing to do? Tests or no tests? It will be up to you.
    You have to find the right balance. Our personal take on this matter is that everything
    needs to be thoroughly tested, either directly or indirectly. We try to write
    the smallest possible test suite that guarantees that. This way, we will have
    a complete test suite in terms of coverage, but not any bigger than necessary.
    We need to maintain those tests.
  prefs: []
  type: TYPE_NORMAL
- en: We hope this example made sense to you; we think it has allowed us to touch
    on the important topics.
  prefs: []
  type: TYPE_NORMAL
- en: If you check out the source code for the book, in the `test_api.py` module,
    you will find a couple of extra test classes that will show you how different
    testing would have been had we decided to go all the way with the mocks. Make
    sure you read that code and understand it well. It is quite straightforward and
    will offer you a good comparison with the approach we have shown you here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us now run the full test suite:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned before, make sure you run `$ pytest test` from within the `ch10`
    folder (add the `-vv` flag for a verbose output that will show you how parametrization
    modifies the names of your tests). `pytest` scans your files and folders, searching
    for modules that start or end with `test_` , like `test_*.py` or `*_test.py` .
    Within those modules, it grabs `test` -prefixed functions or `test` -prefixed
    methods inside `Test` -prefixed classes (you can read the full specification in
    the `pytest` documentation). As you can see, 132 tests were run in 140 milliseconds,
    and they all succeeded. We strongly suggest you check out this code and experiment
    with it. Change something in the code and see whether any test fails. Understand
    why it fails (or does not).
  prefs: []
  type: TYPE_NORMAL
- en: Did the tests pass even though the code is no longer correct? Are the tests
    too rigid and failing even when you make a change that does not affect the correctness
    of the output? Thinking about these questions will help you gain a deeper insight
    into the art of testing.
  prefs: []
  type: TYPE_NORMAL
- en: We also suggest you study the `unittest` module, and the `pytest` library too.
    These are tools you will use all the time, so you need to be familiar with them.
  prefs: []
  type: TYPE_NORMAL
- en: Let us now discuss TDD.
  prefs: []
  type: TYPE_NORMAL
- en: Test-driven development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let us talk briefly about **TDD** . It is a methodology that was rediscovered
    by Kent Beck, who wrote *Test-Driven Development by Example, Addison Wesley, 2002*
    , which we encourage you to read if you want to learn about the fundamentals of
    this subject.
  prefs: []
  type: TYPE_NORMAL
- en: TDD is a software development methodology that is based on the continuous repetition
    of a very short development cycle.
  prefs: []
  type: TYPE_NORMAL
- en: First, the developer writes a test and makes it run. The test is supposed to
    check a feature that is not yet part of the code. Maybe it is a new feature to
    be added or something to be removed or amended. Running the test will make it
    fail and, because of this, this phase is called **Red** .
  prefs: []
  type: TYPE_NORMAL
- en: The developer then writes the minimal amount of code to make the test pass.
    When the test run succeeds, we have the so-called **Green** phase. In this phase,
    it is okay to write code that cheats, just to make the test pass. This technique
    is called *fake it ‘til you make it* . In a second iteration of the TDD cycle,
    tests are enriched with different edge cases, and if there is any cheating code,
    it will not be able to satisfy all the tests simultaneously, therefore the developer
    will have to write the actual logic that satisfies the tests. Adding other test
    cases is sometimes called **triangulation** .
  prefs: []
  type: TYPE_NORMAL
- en: The last piece of the cycle is where the developer takes care of refactoring
    code and tests until they are in the desired state. This last phase is called
    **Refactor** .
  prefs: []
  type: TYPE_NORMAL
- en: The TDD mantra therefore is **Red-Green-Refactor** .
  prefs: []
  type: TYPE_NORMAL
- en: At first, it might feel weird to write tests before the code, and we must confess
    it took us a while to get used to it. If you stick to it, though, and force yourself
    to learn this slightly counterintuitive method, at some point, something almost
    magical happens, and you will see the quality of your code increase in a way that
    would not have been possible otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: When we write our code before the tests, we must take care of *what* the code
    has to do and *how* it has to do it, both at the same time. On the other hand,
    when we write tests before the code, we can concentrate on the *what* part almost
    exclusively.
  prefs: []
  type: TYPE_NORMAL
- en: Afterward, when we write the code, we will mostly have to take care of *how*
    the code has to implement *what* is required by the tests. This shift in focus
    allows our minds to concentrate on the *what* and *how* parts separately, yielding
    a brainpower boost that can feel quite surprising.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several other benefits that come from the adoption of this technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved code quality** : Writing tests first ensures that the codebase is
    thoroughly tested and can lead to fewer bugs and errors in the production code.
    It encourages developers to write only the code necessary to pass tests, which
    can result in cleaner, simpler code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Better design decisions** : TDD encourages developers to think about the
    design and structure of the code from the beginning. This early consideration
    can lead to better software design and architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facilitates refactoring** : With a comprehensive suite of tests in place,
    developers can confidently refactor and improve the code, knowing that the tests
    will catch any regressions or issues introduced by changes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Documentation** : The tests themselves serve as documentation for the codebase.
    They describe what the code is supposed to do, which can be helpful for new team
    members or when revisiting old code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduces time spent on debugging** : By catching errors early in the development
    process, TDD can reduce the amount of time developers spend debugging code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Better understanding of business requirements** : Having a suite of tests
    that pass gives developers confidence that their code meets the required specifications
    and behaves as expected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the other hand, there are some shortcomings of this technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initial slowdown** : Writing tests before writing functional code can slow
    down the initial development process. This can be particularly challenging in
    fast-paced development environments or for tight deadlines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning curve** : TDD requires a different mindset and approach to coding
    than what many developers are accustomed to. There can be a significant learning
    curve, and developers may initially find it difficult to write effective tests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overhead for simple changes** : For very simple changes or fixes, the overhead
    of writing a test first can seem unnecessary and time-consuming.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Difficulty with complex UI or external systems** : Testing can become challenging
    when dealing with complex UIs or interactions with external systems, databases,
    or APIs. Mocking and stubbing can help, but they also add complexity and maintenance
    overhead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are quite passionate about TDD. However, through years of application, we
    have encountered scenarios where TDD proves to be less feasible. A prime example
    is when faced with test suites comprising hundreds or even thousands of tests.
    In such instances, predetermining the specific tests to alter for a desired change
    in the source code becomes an almost insurmountable task. It may, at times, be
    more pragmatic to directly modify the code and observe which tests fail as a result.
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, we maintain a firm belief in the value of mastering TDD. While
    the most significant advantages of TDD may lie in its educational merits rather
    than its practical application, the knowledge and mindset it instills are invaluable.
    Mastering TDD to the extent that it can be applied efficiently leaves an indelible
    mark on our coding practices, influencing our approach even in projects where
    TDD is not utilized.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is essential, therefore, to bear in mind the following principle: always
    rigorously test your code. This practice is fundamental to ensuring the reliability
    and integrity of software, regardless of the development methodology employed.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the world of testing.
  prefs: []
  type: TYPE_NORMAL
- en: We tried to give you a fairly comprehensive overview of testing, especially
    unit testing, which is the most common type of testing a developer does. We hope
    we have succeeded in conveying the message that testing is not something that
    is perfectly defined and that you can learn from a book. You need to experiment
    with it for a significant amount of time before you get comfortable. Of all the
    efforts a coder must make in terms of study and experimentation, we would say
    testing is amongst the most important.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to explore debugging and profiling, which
    are techniques that go hand in hand with testing. You should make sure you learn
    them well.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://discord.com/invite/uaKmaz7FEC](Chapter_10.xhtml)'
  prefs: []
  type: TYPE_NORMAL
- en: '![img](img/QR_Code119001106417026468.png)'
  prefs: []
  type: TYPE_IMG
