- en: Interacting with Forms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In earlier chapters, we downloaded static web pages that return the same content.
    In this chapter, we will interact with web pages which depend on user input and
    state to return relevant content. This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Sending a `POST` request to submit a form
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using cookies and sessions to log in to a website
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Selenium for form submissions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To interact with these forms, you'll need a user account to log in to the website.
    You can register an account manually at [http://example.webscraping.com/user/register](http://example.webscraping.com/user/register).
    Unfortunately, we can't yet automate the registration form until the next chapter,
    which deals with `CAPTCHA` images.
  prefs: []
  type: TYPE_NORMAL
- en: Form methods
  prefs: []
  type: TYPE_NORMAL
- en: HTML forms define two methods for submitting data to the server-`GET` and `POST`.
    With the `GET` method, data such as `?name1=value1&name2=value2` is appended to
    the URL, which is known as a "query string". The browser sets a limit on the URL
    length, so this is only useful for small amounts of data. Additionally, this method
    is generally intended to only retrieve data from the server and not make changes
    to it, but sometimes this intention is ignored. With `POST` requests, the data
    is sent in the request body, not the URL. Sensitive data should only be sent in
    a `POST` request to avoid exposing it in the URL. How the `POST` data is represented
    in the body depends on the encoding type.
  prefs: []
  type: TYPE_NORMAL
- en: Servers can also support other HTTP methods, such as `PUT` and `DELETE`, however,
    these are not supported in standard HTML forms.
  prefs: []
  type: TYPE_NORMAL
- en: The Login form
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first form we''ll automate is the Login form, which is available at [http://example.webscraping.com/user/login](http://example.webscraping.com/user/login).
    To understand the form, we can use our browser development tools. With the full
    version of Firebug or Chrome Developer Tools, it is possible to simply submit
    the form and check what data was transmitted in the Network tab (similar to how
    we did in [Chapter 5](py-web-scrp-2e_ch05.html), *Dynamic Content*). However,
    we can also see information about the form if we use "Inspect Element" features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/post_image.png)'
  prefs: []
  type: TYPE_IMG
- en: The important parts regarding how to send the form are the `action`, `enctype`,
    and `method` attributes of the `form` tag, and the two `input` fields (in the
    above image we have expanded the "password" field). The `action` attribute sets
    the HTTP location where the form data will be submitted, in this case, `#`, which
    represents the current URL. The `enctype` attribute (or encoding type) sets the
    encoding used for the submitted data, in this case, `application/x-www-form-urlencoded`.
    The `method` attribute is set to `post` to submit form data with a `POST` method in
    the message body to the server. For each `input` tags, the important attribute
    is `name`, which sets the name of the field when the `POST` data is submitted
    to the server.
  prefs: []
  type: TYPE_NORMAL
- en: Form encoding
  prefs: []
  type: TYPE_NORMAL
- en: When a form uses the `POST` method, there are two useful choices for how the
    form data is encoded before being submitted to the server. The default is `application/x-www-form-urlencoded`,
    which specifies all non-alphanumeric characters must be converted to ASCII Hex
    values. However, this is inefficient for forms which contain a large amount of
    non-alphanumeric data, such as a binary file upload, so `multipart/form-data`
    encoding was defined. Here, the input is not encoded but sent as multiple parts
    using the MIME protocol, which is the same standard used for e-mail.
  prefs: []
  type: TYPE_NORMAL
- en: The official details of this standard can be viewed at [http://www.w3.org/TR/html5/forms.html](http://www.w3.org/TR/html5/forms.html)#selecting-a-form-submission-encoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'When regular users open this web page in their browser, they will enter their
    e-mail and password, and click on the Login button to submit their details to
    the server. Then, if the login process on the server is successful, they will
    be redirected to the home page; otherwise, they will return to the Login page
    to try again. Here is an initial attempt to automate this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This example sets the e-mail and password fields, encodes them with `urlencode`,
    and submits them to the server. When the final print statement is executed, it
    will output the URL of the Login page, which means the login process has failed.
    You will notice we must also encode the already encoded data as bytes so `urllib`
    will accept it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write the same process using `requests` in fewer lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `requests` library allows us to explicitly post data, and will do the encoding
    internally. Unfortunately, this code still fails to log in.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Login form is particularly strict and requires some additional fields to
    be submitted along with the e-mail and password. These additional fields can be
    found at the bottom of the previous screenshot, but are set to `hidden` and so
    they aren''t displayed in the browser. To access these hidden fields, here is
    a function using the `lxml` library covered in Chapter 2, *Scraping the Data*,
    to extract all the `input` tag details in a form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The function in the preceding code uses `lxml` CSS selectors to iterate over
    all `input` tags in a form and return their `name` and `value` attributes in a
    dictionary. Here is the result when the code is run on the Login page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `_formkey` attribute is the crucial piece; it contains a unique ID used
    by the server to prevent multiple form submissions. Each time the web page is
    loaded, a different ID is used, and the server can tell whether a form with a
    given ID has already been submitted. Here is an updated version of the login process which submits
    `_formkey` and other hidden values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, this version doesn''t work either, because the login URL was
    again returned. We are missing another essential component--browser cookies. When
    a regular user loads the Login form, this `_formkey` value is stored in a cookie,
    which is compared to the `_formkey` value in the submitted Login form data. We
    can take a look at the cookies and their values via our `response` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also see via your Python interpreter that the `response.cookies` is
    a special object type, called a cookie jar. This object can also be passed to
    new requests. Let''s retry our submission with cookies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: What are cookies?
  prefs: []
  type: TYPE_NORMAL
- en: 'Cookies are small amounts of data sent by a website in the HTTP `response`
    headers, which look like this: `Set-Cookie: session_id=example`;. The web browser
    will store them, and then include them in the headers of subsequent requests to
    that website. This allows a website to identify and track users.'
  prefs: []
  type: TYPE_NORMAL
- en: Success! The submitted form values have been accepted and the `response` URL
    is the home page. Note that we needed to use the cookies which properly align
    with our form data from our initial request (which we have stored in the `html`
    variable). This snippet and the other login examples in this chapter are available
    for download at [https://github.com/kjam/wswp/tree/master/code/chp6](https://github.com/kjam/wswp/tree/master/code/chp6).
  prefs: []
  type: TYPE_NORMAL
- en: Loading cookies from the web browser
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working out how to submit the login details expected by a server can be quite
    complex, as demonstrated by the previous example. Fortunately, there's a workaround
    for difficult websites--we can log in to the website manually using a web browser,
    and have our Python script load and reuse the cookies to be automatically logged
    in.
  prefs: []
  type: TYPE_NORMAL
- en: Some web browsers store their cookies in different formats, but Firefox and
    Chrome use an easy-to-access format we can parse with Python: a `sqlite` database.
  prefs: []
  type: TYPE_NORMAL
- en: '[SQLite](https://www.sqlite.org/) is a very popular open-source SQL database.
    It can be easily installed on many platforms and comes pre-installed on Mac OSX.
    To download and install it on your operating system, check [the Download page](https://www.sqlite.org/download.html)
    or simply search for your operating system instructions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To take a look at your cookies, you can (if installed) run the `sqlite3` command
    and then the path to your cookie file (shown below is an example for Chrome):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You will need to first find the path to your browser's configuration files which
    can either be done by searching your filesystem or simply searching the web for
    your browser and operating system. To see table schema in SQLite, you can use
    `.schema` and select syntax functions similarly to other SQL databases.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to storing cookies in a `sqlite` database, some browsers (such
    as Firefox) store sessions directly in a JSON file, which can be easily parsed
    using Python. There are also numerous browser extensions, such as SessionBuddy
    which can export your sessions into JSON files. For the login, we only need to
    find the proper sessions, which are stored in this structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a function that can be used to parse Firefox sessions into a Python
    dictionary, which we can then feed to the `requests` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'One complexity is that the location of the Firefox sessions file will vary,
    depending on the operating system. On Linux, it should be located at this path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In OS X, it should be located at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, for Windows Vista and above, it should be located at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a helper function to return the path to the session file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the `glob` module used here will return all matching files for the
    given path. Now here is an updated snippet using the browser cookies to log in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To check whether the session was loaded successfully, we cannot rely on the
    login redirect this time. Instead, we will scrape the resulting HTML to check
    whether the logged in user label exists. If the result here is `Login`, the sessions
    have failed to load correctly. If this is the case, make sure you are already
    logged in to the example website using your Firefox browser. We can inspect the `User`
    label for the site using our browser tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/user_nav.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The browser tools show this label is located within a `<ul>` tag of ID "navbar",
    which can easily be extracted with the `lxml` library used in [Chapter 2](py-web-scrp-2e_ch02.html),
    *Scraping the Data*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The code in this section was quite complex and only supports loading sessions
    from the Firefox browser. There are numerous browser add-ons and extensions that
    support saving your sessions in JSON files, so you can explore these as an option
    if you need session data for login.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will take a look at the `requests` library advanced
    usage for sessions [http://docs.python-requests.org/en/master/user/advanced/#session-objects](http://docs.python-requests.org/en/master/user/advanced/#session-objects),
    which allows you utilize browser sessions easily when scraping with Python.
  prefs: []
  type: TYPE_NORMAL
- en: Extending the login script to update content
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we can login via a script, we can extend this script by adding code
    to update the website country data. The code used in this section is available
    at [https://github.com/kjam/wswp/blob/master/code/chp6/edit.py](https://github.com/kjam/wswp/blob/master/code/chp6/edit.py)
    and [https://github.com/kjam/wswp/blob/master/code/chp6/login.py](https://github.com/kjam/wswp/blob/master/code/chp6/login.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'You may have already noticed an Edit link at the bottom of each country:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4364OS_06_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When logged in, clicking this link leads to another page where each property
    of a country can be edited:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4364OS_06_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will make a script to increase the population of a country by one person
    every time it''s run. The first step is to rewrite our `login` function to utilize
    `Session` objects. This will make our code cleaner and allow us to remain logged
    into our current session. The new code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now our login form can work with or without sessions. By default, it doesn''t
    use sessions and expects the user to utilize the cookies to stay logged in. This
    can be problematic for some forms, however, so adding the session functionality
    is useful when extending our login function. Next, we need to extract the current
    values of the country by reusing the `parse_form()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can increase the population by one and submit the updated version to
    the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'When we return to the country page, we can verify that the population has increased
    to 62,348,449:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/population_increase.png)'
  prefs: []
  type: TYPE_IMG
- en: Feel free to test and modify the other fields as well--the database is restored
    to the original country data each hour to keep the data sane.  There is code for
    modifying the currency field in [the edit script](https://github.com/kjam/wswp/blob/master/code/chp6/edit.py)
    to use as another example. You can also play around with modifying other countries.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the example covered here is not strictly web scraping, but falls under
    the wider scope of online bots. The form techniques we used can also be applied
    to interacting with complex forms to access data you want to scrape. Make sure
    you use your new automated form powers for good and not for spam or malicious
    content bots!
  prefs: []
  type: TYPE_NORMAL
- en: Automating forms with Selenium
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The examples built so far work, but each form requires a fair amount of work
    and testing. This effort can be minimized by using Selenium as we did in [Chapter
    5](py-web-scrp-2e_ch05.html), *Dynamic Content*. Because it is a browser-based
    solution, Selenium can mock many user interactions including clicks, scrolling
    and typing. If you are using it with a headless browser like PhantomJS, you will
    also be able to parallelize and scale your processes because it has less overhead
    than running a full browser.
  prefs: []
  type: TYPE_NORMAL
- en: Using a complete browser can also be a good solution for "humanizing" your interactions,
    particularly if you are using a well-known browser or other browser-like headers
    which can set you apart from other more robot-like identifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Rewriting our login and editing scripts to use Selenium is fairly straightforward,
    but we must first investigate the page to pick out the CSS or XPath identifiers
    to use. Doing so with our browser tools, we notice the login form has easy-to-identify
    CSS IDs for the login form and the country edit form. Now we can rewrite the login
    and edit using Selenium.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s write a few methods for getting a driver and logging in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Here the `get_driver` function first attemps to get a PhantomJS driver, since
    it is faster and easier to install on servers. If that fails, we use Firefox.
    The `login` function uses a `driver` object passed as the argument, and uses the
    browser driver to login by first loading the page, then using the driver's `send_keys`
    method to type into the identified input elements. The `Keys.RETURN` sends the
    signal for a Return key, which on many forms will be mapped to submit the form.
  prefs: []
  type: TYPE_NORMAL
- en: We are also utilizing the Selenium explicit waits (`WebDriverWait` and `EC`
    for ExpectedConditions), which allow us to tell the browser to wait until a particular
    element or condition is met. In this case, we know that the homepage when logged
    in shows an element with the CSS ID `"results"`. The `WebDriverWait` object will
    wait 10 seconds for the element to load before raising an Exception. We can easily
    toggle this wait, or use other expected conditions to match how the page we are
    currently loading behaves.
  prefs: []
  type: TYPE_NORMAL
- en: To read more about Selenium explicit waits, I recommend looking at the Python
    bindings documentation: [http://selenium-python.readthedocs.io/waits.html](http://selenium-python.readthedocs.io/waits.html).
    Explicit waits are preferred to implicit waits as you are telling Selenium exactly
    what to wait for and can ensure the part of the page you want to interact with
    has been loaded.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we can get a webdriver and login to the site, we want to interact
    with the form and change the population:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The only new Selenium feature used is the `clear` method to clear the input
    value for the form (rather than appending it to the end of the field). We also
    use the element's `get_attribute` method to retrieve particular attributes from
    a HTML elements on the page. Because we are dealing with HTML `input` elements,
    we need to grab the `value` attribute, rather than checking the text attribute.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have all of our methods for using Selenium to add one to the population,
    so we can run this script like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Since our `assert` statement passed, we know we have updated the population
    using this simple script.
  prefs: []
  type: TYPE_NORMAL
- en: There are many more ways to use Selenium to interact with forms, and I encourage
    you to experiment further by reading the documentation. Selenium can be especially
    helpful for debugging problematic websites because of the ability to use `save_screenshot`
    to see what the browser has loaded.
  prefs: []
  type: TYPE_NORMAL
- en: '"Humanizing" methods for Web Scraping'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are sites which detect web scrapers via particular behaviors. In [Chapter
    5](py-web-scrp-2e_ch05.html), *Dynamic Content*, we covered how to avoid honeypots
    by avoiding clicking on hidden links. Here are a few other tips for appearing
    more like a human while scraping content online.
  prefs: []
  type: TYPE_NORMAL
- en: '**Utilize Headers**: Most of the scraping libraries we have covered can alter
    the headers of your requests, allowing you to modify things like `User-Agent`, `Referrer`, `Host`,
    and `Connection`. Also, when utilizing browser-based scrapers like Selenium, your
    scraper will look like a normal browser with normal headers. You can always take
    a look at what headers your browser is using by opening your browser tools and
    viewing one of the recent requests in the Network tab. This might give you a good
    idea of what headers the site is expecting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Add Delays:** Some scraper detection techniques use timing to determine if
    a form is filled out too quickly or links are clicked too soon after page load.
    To appear more "human-like", add reasonable delays when interacting with forms
    or use `sleep` to add delays between requests. This is also the polite way to
    scrape a site so as to not overload the server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use Sessions and Cookies:** As we have covered in this chapter, using sessions
    and cookies will help your scraper navigate the site easier and allow you to appear
    more like a normal browser. By saving sessions and cookies locally, you can pick
    up sessions where you left off and resume scraping with saved data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Interacting with forms is a necessary skill when scraping web pages. This chapter
    covered two approaches: first, analyzing the form to generate the expected `POST`
    request manually and utilizing browser sessions and cookies to stay logged in.
    Then, we were able to replicate those interactions using Selenium. We also covered
    some tips to follow when "humanizing" your scrapers.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will expand our form skillset and learn how to
    submit forms that require passing `CAPTCHA` image solving.
  prefs: []
  type: TYPE_NORMAL
