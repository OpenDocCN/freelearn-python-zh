- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Interacting with Other Services
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与其他服务交互
- en: In the previous chapter, our monolithic application was split up into several
    microservices, and consequently, more network interactions between the different
    parts were included.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们的单体应用被拆分为几个微服务，因此，不同部分之间的网络交互也相应增加。
- en: More interactions with other components can lead to complications of their own,
    however, such as a high volume of messages or large data sizes delaying responses,
    or long-running tasks taking up valuable resources. Since many of our useful tasks
    involve interacting with third-party services, the techniques to manage these
    changes are useful both inside our application and for communicating outside of
    it. Having the ability to loosely couple different parts of the system using some
    asynchronous messages is useful to prevent blockages and unwanted dependency entanglements.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他组件的更多交互可能导致其自身的复杂性，例如，大量消息或大数据量延迟响应，或者长时间运行的任务占用宝贵资源。由于我们许多有用的任务涉及与第三方服务的交互，因此管理这些变化的技术对我们应用程序内部和外部通信都很有用。能够使用一些异步消息松散耦合系统的不同部分，有助于防止阻塞和不受欢迎的依赖纠缠。
- en: In any case, the bottom line is that we need to interact with other services
    through the network, both synchronously and asynchronously. These interactions
    need to be efficient, and when something goes wrong, we need to have a plan.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，底线是我们需要通过网络与其他服务进行交互，无论是同步还是异步。这些交互需要高效，当出现问题时，我们需要有一个计划。
- en: 'The other problem introduced by adding more network connections is **testing**:
    how do we test a microservice in isolation that also needs to call other microservices
    to function? In this chapter, we will explore this in detail:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 通过增加更多的网络连接引入的另一个问题是**测试**：如何测试一个需要调用其他微服务才能正常工作的独立微服务？在本章中，我们将详细探讨这个问题：
- en: How one service can call another using synchronous and asynchronous libraries,
    and how to make these calls more efficient
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用同步和异步库调用另一个服务，以及如何使这些调用更加高效
- en: How a service can use messages to make asynchronous calls and communicate with
    other services via events
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务如何使用消息进行异步调用，并通过事件与其他服务进行通信
- en: We will also see some techniques to test services that have network dependencies
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还将看到一些测试具有网络依赖服务的技巧
- en: Calling other web resources
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调用其他网络资源
- en: As we have seen in the previous chapters, synchronous interactions between microservices
    can be done via HTTP APIs using JSON payloads. This is by far the pattern most
    often used, because both HTTP and JSON are common standards. If your web service
    implements an HTTP API that accepts JSON, any developer using any programming
    language will be able to use it. Most of these interfaces are also RESTful, meaning
    that they follow the **Representational State Transfer** (**REST**) architecture
    principles of being stateless—with each interaction containing all the information
    needed instead of relying on previous exchanges—as well as cacheable and having
    a well-defined interface.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前几章所看到的，微服务之间的同步交互可以通过使用JSON有效载荷的HTTP API来实现。这无疑是使用最频繁的模式，因为HTTP和JSON都是常见的标准。如果你的网络服务实现了接受JSON的HTTP
    API，任何使用任何编程语言的开发者都将能够使用它。大多数这些接口也是RESTful的，这意味着它们遵循**表示状态转移**（**REST**）架构原则，即无状态——每个交互都包含所需的所有信息，而不是依赖于之前的交换——以及可缓存和具有良好定义的接口。
- en: Following a RESTful scheme is not a requirement, however, and some projects
    implement **Remote Procedure Call** (**RPC**) APIs, which focus on the action
    being performed and abstract away the network requests from the code that handles
    the messages. In REST, the focus is on the resource, and actions are defined by
    HTTP methods. Some projects are a mix of both and don't strictly follow a given
    standard. The most important thing is that your service behavior should be consistent
    and well-documented. This book leans on REST rather than RPC, but is not strict
    about it, and recognizes that different situations have different solutions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然遵循RESTful方案不是强制性的，但一些项目实现了**远程过程调用**（**RPC**）API，这些API专注于正在执行的操作，并从处理消息的代码中抽象出网络请求。在REST中，重点是资源，操作由HTTP方法定义。一些项目是两者的混合，并不严格遵循某个特定标准。最重要的是，你的服务行为应该是连贯且文档齐全的。本书倾向于使用REST而不是RPC，但并不严格，并认识到不同情况有不同的解决方案。
- en: Sending and receiving JSON payloads is the simplest way for a microservice to
    interact with others, and only requires microservices to know the entry points
    and parameters to pass using HTTP requests.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 发送和接收 JSON 有效负载是微服务与其他服务交互的最简单方式，只需要微服务知道入口点和通过 HTTP 请求传递的参数。
- en: 'To do this, you just need to use an HTTP client. Python has one as part of
    the `http.client` module, and in a synchronous Python environment, the `Requests`
    library is rightfully popular: [https://docs.python-requests.org](https://docs.python-requests.org).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，你只需要使用一个 HTTP 客户端。Python 在 `http.client` 模块中提供了一个，在同步 Python 环境中，`Requests`
    库非常受欢迎：[https://docs.python-requests.org](https://docs.python-requests.org)。
- en: 'As we are in an asynchronous environment, we will use `aiohttp`, which has
    a clean way of creating asynchronous web requests and offers built-in features
    that make it easier to perform multiple simultaneous asynchronous requests: [https://docs.aiohttp.org/en/stable/](https://docs.aiohttp.org/en/stable/).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们处于异步环境中，我们将使用 `aiohttp`，它有一个创建异步 Web 请求的清晰方式，并提供了内置功能，使得执行多个同时进行的异步请求变得更容易：[https://docs.aiohttp.org/en/stable/](https://docs.aiohttp.org/en/stable/)。
- en: HTTP requests in the `aiohttp` library are built around the concept of a session,
    and the best way to use it is to call `CreateSession`, creating a `Session` object
    that can be reused every time you interact with any service.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`aiohttp` 库中的 HTTP 请求是围绕会话的概念构建的，最佳的使用方式是调用 `CreateSession`，创建一个每次与任何服务交互时都可以重用的
    `Session` 对象。'
- en: 'A `Session` object can hold authentication information and some default headers
    you may want to set for all requests that your application will make. It can also
    control default error handling behavior, storing cookies, and what timeouts to
    use. In the following example, the call to `ClientSession` will create an object
    with the right `Content-Type` headers:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`Session` 对象可以保存认证信息和一些你可能想要为所有请求设置的默认头信息。它还可以控制默认的错误处理行为，存储 cookies，以及使用哪些超时。在下面的示例中，对
    `ClientSession` 的调用将创建一个具有正确 `Content-Type` 头部的对象：'
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If we should limit how many concurrent requests are being made to an external
    endpoint, there are two main approaches. `aiohttp` has a concept of connectors,
    and we can set options to control how many outgoing TCP connections a `session`
    can operate at once, as well as limiting those numbers for a single destination:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们应该限制对外部端点发出的并发请求数量，有两种主要方法。`aiohttp` 有一个连接器的概念，我们可以设置选项来控制一个 `session` 一次可以操作多少个出站
    TCP 连接，以及限制单个目标的数量：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This might be enough for our needs; however, if we make several outgoing connections
    to complete one request, we could end up in a situation where each piece of work
    is continuously blocking after each one as we reach the limit. Ideally, we would
    like a discrete chunk of work to continue until it's done, and for that we can
    use a semaphore. A semaphore is a simple token that gives code permission to perform
    a task. If we were to add a semaphore with three slots, then the first three tasks
    that try to access the semaphore will take a slot each and carry on. Any other
    task that requests the semaphore will have to wait until one of the slots is free.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能已经足够满足我们的需求；然而，如果我们为了完成一个请求而建立多个出站连接，我们可能会陷入一种情况，即每完成一项工作后，由于达到限制，下一项工作会持续阻塞。理想情况下，我们希望一个独立的工作块能够持续进行，直到完成，为此我们可以使用信号量。信号量是一个简单的令牌，它允许代码执行任务。如果我们添加一个有三个槽位的信号量，那么前三个尝试访问信号量的任务将各自占用一个槽位并继续执行。任何其他请求信号量的任务都必须等待直到其中一个槽位空闲。
- en: 'Since the most common way to request a semaphore is inside a `with` block,
    this means that as soon as the context of the `with` block is over, the semaphore
    is released—inside the semaphore object''s `__exit__` function:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于请求信号量最常见的方式是在 `with` 块内部，这意味着一旦 `with` 块的上下文结束，信号量就会被释放——在信号量对象的 `__exit__`
    函数内部：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let us now see how we can generalize this pattern in a Quart app that needs
    to interact with other services.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何在需要与其他服务交互的 Quart 应用程序中泛化这种模式。
- en: This naive implementation is based on the hypothesis that everything will go
    smoothly, but real life is rarely so easy. We can set up different error handling
    options in a `ClientSession`, such as retries and timeouts, and we only need to
    set them up in that one place.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简单的实现基于一切都会顺利进行的假设，但现实生活很少如此简单。我们可以在 `ClientSession` 中设置不同的错误处理选项，如重试和超时，我们只需要在那个地方设置即可。
- en: Finding out where to go
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找去往何方
- en: When we make a web request to a service, we need to know which **Uniform Resource
    Locator** (**URL**) to use. Most of the examples in this book use hardcoded URLs—that
    is, they are written into the source code. This is nice and easy to read for an
    example, but can be a problem when maintaining software. What happens when a service
    gets a new URI, and its hostname or IP address changes? It might move between
    AWS regions due to a failure or be migrated from Google Cloud Platform to Microsoft
    Azure. An API update can make the path to a resource change, even if the hostname
    or IP address has not updated.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们向一个服务发出Web请求时，我们需要知道要使用哪个**统一资源定位符**（**URL**）。本书中的大多数示例都使用硬编码的URL——也就是说，它们被写入源代码。这对于示例来说很方便，易于阅读，但在维护软件时可能会出现问题。当服务获得新的URI，其主机名或IP地址发生变化时会发生什么？它可能会因为故障而在AWS区域之间移动，或者从Google
    Cloud Platform迁移到Microsoft Azure。即使主机名或IP地址没有更新，API更新也可能使资源路径发生变化。
- en: We want to pass in data about which URLs to use as configuration to our application.
    There are several options to manage more configuration options without adding
    them directly to the code, such as environment variables and service discovery.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将有关要使用的URL作为配置的数据传递给我们的应用程序。有几种选项可以管理更多的配置选项，而无需直接将它们添加到代码中，例如环境变量和服务发现。
- en: Environment variables
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境变量
- en: 'Container-based environments are common these days, and we will discuss them
    in more detail in *Chapter 10*, *Deploying on AWS*. The most common approach to
    get configuration options into a container is to pass the container some environment
    variables. This has the advantage of being straightforward, since the code just
    needs to examine the environment when processing its configuration:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 基于容器的环境现在很常见，我们将在第10章“在AWS上部署”中更详细地讨论它们。将配置选项传递到容器中最常见的方法是向容器传递一些环境变量。这有一个优点，即简单直接，因为代码在处理其配置时只需要检查环境：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The downside to this approach is that if the URL changes, then we need to restart
    the application—and sometimes redeploy it—with the new environment. If you don't
    expect the configuration to change very often, environment variables are still
    a good idea due to their simplicity, although we must be careful to not record
    any secrets that are in environment variables when we log messages.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是，如果URL发生变化，那么我们需要重新启动应用程序，有时甚至需要使用新环境重新部署它。如果你不期望配置经常改变，由于它们的简单性，环境变量仍然是一个好主意，尽管我们必须小心不要在记录消息时记录任何包含在环境变量中的秘密。
- en: Service discovery
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务发现
- en: 'But what if we did not need to tell our service about all its options when
    we deploy it? Service discovery is an approach that involves configuring an application
    with just a few pieces of information: where to ask for configuration and how
    to identify the right questions to ask.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们部署服务时不需要告诉它所有选项怎么办？服务发现是一种涉及仅用少量信息配置应用程序的方法：在哪里请求配置以及如何识别正确的提问方式。
- en: 'Services such as `etcd` ([https://etcd.io/](https://etcd.io/)) provide a reliable
    key-value store in which to keep this configuration data. For example, let''s
    use `etcd` to store the URL of the production and development RabbitMQ instances:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`etcd`（[https://etcd.io/](https://etcd.io/)）等服务提供了一个可靠的关键值存储，用于保存这些配置数据。例如，让我们使用`etcd`来存储生产环境和开发环境RabbitMQ实例的URL：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: When an application starts up, it can check to see whether it is running in
    production or in a local development environment and ask `etcd` for the right
    value—either `myservice/production/rabbitmq/url` or `myservice/development/rabbitmq/url`.
    With a single option in a deployment, it is possible to change a whole number
    of configuration options, use different external URLs, bind to different ports,
    or any other piece of configuration you might think of.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用程序启动时，它可以检查它是否在生产环境中运行或在本地开发环境中运行，并请求`etcd`的正确值——无论是`myservice/production/rabbitmq/url`还是`myservice/development/rabbitmq/url`。在部署中有一个单一选项，可以更改大量配置选项，使用不同的外部URL，绑定到不同的端口，或你可能想到的任何其他配置。
- en: It's also possible to update the values in `etcd`, and when your application
    next checks for a new value, it will update and use that instead. Deploying a
    new version of `RabbitMQ` can now be done alongside the old version, and the swap
    will be a value change in `etcd`—or a change back if it goes wrong.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以更新`etcd`中的值，当你的应用程序下次检查新值时，它将更新并使用该值。现在可以在旧版本旁边部署`RabbitMQ`的新版本，交换将是`etcd`中的值变化——或者如果出错，将是一个回退变化。
- en: This approach does add complexity, both as an extra service to run and in terms
    of updating these values within your application, but it can be a valuable approach
    in more dynamic environments. We will discuss service discovery more in *Chapter
    10*, *Deploying on AWS*, when we cover deploying an application on containers
    and in the cloud.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法确实增加了复杂性，既作为额外服务运行，也涉及到在您的应用程序中更新这些值，但在更动态的环境中，这可以是一种有价值的方法。我们将在第10章*部署在AWS*中更详细地讨论服务发现，当我们介绍在容器和云中部署应用程序时。
- en: Transferring data
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据传输
- en: JSON is a human-readable data format. There is a long history of human-readable
    data transfer on the internet—a good example would be email, as you can quite
    happily type out the protocol needed to send an email as a human author. This
    readability is useful for determining exactly what is happening in your code and
    its connections, especially as JSON maps directly onto Python data structures.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: JSON是一种可读的数据格式。互联网上有着悠久的人可读数据传输历史——一个很好的例子是电子邮件，因为你可以很愉快地以人类作者的身份键入发送电子邮件所需的协议。这种可读性对于确定代码及其连接中正在发生的事情非常有用，尤其是因为JSON直接映射到Python数据结构。
- en: The downside to this readability is the size of the data. Sending HTTP requests
    and responses with JSON payloads can add some bandwidth overhead in the long run,
    and serializing and deserializing data from Python objects to JSON structures
    also adds a bit of CPU overhead.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这种可读性的缺点是数据的大小。长期来看，发送带有JSON有效负载的HTTP请求和响应可能会增加一些带宽开销，而且将Python对象序列化为JSON结构以及反序列化也会增加一些CPU开销。
- en: There are other ways to transfer data that involve caching, compression, binary
    payloads, or RPC, however.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有其他涉及缓存、压缩、二进制有效负载或RPC的数据传输方式。
- en: HTTP cache headers
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HTTP缓存头部
- en: In the HTTP protocol, there are a few cache mechanisms that can be used to indicate
    to a client that a page that it's trying to fetch has not changed since its last
    visit. Caching is something we can do in our microservices on all the read-only
    API endpoints, such as `GETs` and `HEADs`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在HTTP协议中，有一些缓存机制可以用来向客户端指示它试图获取的页面自上次访问以来没有变化。缓存是我们可以在我们的微服务中的所有只读API端点上执行的操作，例如`GETs`和`HEADs`。
- en: The simplest way to implement it is to return, along with a result, an ETag
    header in the response. An `ETag` value is a string that can be considered as
    a version for the resource the client is trying to get. It can be a timestamp,
    an incremental version, or a hash. It's up to the server to decide what to put
    in it, but the idea is that it should be unique to the value of the response.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 实现它的最简单方法是在响应中返回结果的同时，返回一个ETag头部。`ETag`值是一个字符串，可以被认为是客户端试图获取的资源的一个版本。它可以是时间戳、增量版本或哈希。由服务器决定在其中放置什么，但理念是它应该对响应值是唯一的。
- en: Like web browsers, when the client fetches a response that contains such a header,
    it can build a local dictionary cache that stores the response bodies and `ETags`
    as its values, and the URLs as its keys.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 与网络浏览器类似，当客户端获取包含此类头部的响应时，它可以构建一个本地字典缓存，将响应体和`ETags`作为其值存储，将URL作为其键。
- en: When making a new request, the client can look in its local cache and pass along
    a stored `ETag` value in the `If-Modified-Since` header. If the server sends back
    a `304` status code, it means that the response has not changed, and the client
    can use the previously stored one.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当发起一个新的请求时，客户端可以查看其本地缓存，并在`If-Modified-Since`头部中传递一个存储的`ETag`值。如果服务器返回`304`状态码，这意味着响应没有变化，客户端可以使用之前存储的那个。
- en: This mechanism can greatly reduce the response times from the server, since
    it can immediately return an empty `304` response when the content has not changed.
    If it has changed, the client gets the full message in the usual way.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这种机制可以大大减少服务器的响应时间，因为它可以在内容没有变化时立即返回一个空的`304`响应。如果内容已更改，客户端将以通常的方式收到完整消息。
- en: Of course, this means the services that you are calling should implement this
    caching behavior by adding the proper `ETag` support. It's not possible to implement
    a generic solution for this because the cache logic depends on the nature of the
    data your service is managing. The rule of thumb is to version each resource and
    change that version every time the data changes. In the following example, the
    Quart app uses the current server time to create `ETag` values associated with
    users' entries. The `ETag` value is the current time since the epoch, in milliseconds,
    and is stored in the modified field.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这意味着你调用的服务应该通过添加适当的`ETag`支持来实现这种缓存行为。由于缓存逻辑取决于你服务管理的数据的性质，因此不可能实现一个通用的解决方案。一般规则是，为每个资源进行版本控制，并在数据更改时更改该版本。在下面的示例中，Quart应用使用当前服务器时间来创建与用户条目关联的`ETag`值。`ETag`值是自纪元以来的当前时间，以毫秒为单位，并存储在修改字段中。
- en: 'The `get_user()` method returns a user entry from `_USERS` and sets the `ETag`
    value with `response.set_etag`. When the view gets some calls, it also looks for
    the `If-None-Match` header to compare it to the user''s modified field, and returns
    a `304` response if it matches:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_user()`方法从`_USERS`返回一个用户条目，并使用`response.set_etag`设置`ETag`值。当视图接收到一些调用时，它也会查找`If-None-Match`头，将其与用户的修改字段进行比较，如果匹配则返回`304`响应：'
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `change_user()` view sets a new modified value when the client modifies
    a user. In the following client session, we''re changing the user, while also
    making sure that we get a `304` response when providing the new `ETag` value:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`change_user()`视图在客户端修改用户时设置一个新的修改值。在以下客户端会话中，我们正在更改用户，同时确保在提供新的`ETag`值时获得`304`响应：'
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This demonstration is a toy implementation that might not work well in production;
    relying on a server clock to store `ETag` values means you are sure that the clock
    is never set back in time and that if you have several servers, their clocks are
    all synchronized with a service, such as ntpdate.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个演示是一个玩具实现，可能在生产环境中工作得不好；依赖于服务器时钟来存储`ETag`值意味着你确信时钟永远不会倒退，并且如果你有多个服务器，它们的时钟都通过一个服务（如ntpdate）与该服务同步。
- en: There is also the problem of race conditions if two requests change the same
    entry within the same millisecond. Depending on your app, it may not be an issue,
    but then again if it is, then it may be a big one. A cleaner option is to have
    the modified field handled by your database system directly, and make sure its
    changes are done in serialized transactions. Sending the `ETag` with a `POST`
    request is also a good precaution against a race between concurrent updates—the
    server can use the `ETag` to verify what version of the data the client wants
    to update from, and if that version doesn't match, it is probably unsafe to update
    the data, as someone else has changed it first.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个请求在相同毫秒内更改相同的条目，也存在竞争条件的问题。根据你的应用，这可能不是问题，但如果它是，那么它可能是一个大问题。一个更干净的选择是让数据库系统直接处理修改字段，并确保其更改是在序列化事务中完成的。使用`POST`请求发送`ETag`也是防止并发更新之间竞争的好预防措施——服务器可以使用`ETag`来验证客户端想要更新的数据版本，如果该版本不匹配，那么更新数据可能是不安全的，因为其他人可能已经先更改了它。
- en: Some developers use hash functions for their `ETag` value because it's easy
    to compute in a distributed architecture, and it doesn't introduce any of the
    problems timestamps have. But calculating a hash has a CPU cost, and it means
    you need to pull the whole entry to do it—so it might be as slow as if you were
    sending back the actual data. That said, with a dedicated table in your database
    for all your hashes, you can probably come up with a solution that makes your
    `304` response fast in its return.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一些开发者使用哈希函数来计算他们的`ETag`值，因为在分布式架构中计算简单，而且不会引入时间戳可能带来的任何问题。但是计算哈希值需要CPU成本，这意味着你需要拉取整个条目来执行它——所以它可能和发送实际数据一样慢。话虽如此，如果你在数据库中有一个用于所有哈希值的专用表，你可能会想出一个解决方案，使得你的`304`响应在返回时更快。
- en: As we said earlier, there is no generic solution to implement an efficient HTTP
    cache logic—but it's worth implementing one if your client is doing a lot of reads
    on your service. When you have no choice but to send some data back, there are
    several ways to make it as efficient as possible, as we will see in the next section.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所说的，没有通用的解决方案来实现高效的HTTP缓存逻辑——但如果你的客户端在你的服务上做了很多读取操作，那么实现一个缓存机制是值得的。当你别无选择，只能发送一些数据时，有几种方法可以使它尽可能高效，我们将在下一节中看到。
- en: GZIP compression
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GZIP压缩
- en: Compression is an overarching term for reducing the size of data in such a way
    that the original data can be recovered. There are many different compression
    algorithms—some of them are general-purpose algorithms that can be used on any
    sort of data, while some of them are specialized to particular data formats and
    achieve very good results due to them making assumptions about how the data is
    structured.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩是一个总称，指的是以这种方式减小数据的大小，以便可以恢复原始数据。有许多不同的压缩算法——其中一些是通用算法，可以在任何类型的数据上使用，而另一些则是针对特定数据格式进行优化的，由于它们对数据的结构做出了假设，因此可以实现非常好的结果。
- en: There are trade-offs to make between the size of the compressed data, the speed
    of compression and decompression, and how widely implemented the compression algorithm
    is. It might be acceptable to spend a few minutes compressing a large data file
    if it spends most of its time being stored, as the space savings outweigh the
    access time taken, but for data that is short-lived or regularly accessed, then
    the overhead of compression and decompression is more important. For our purposes,
    we need a compression algorithm that is widely understood by different environments,
    even if it doesn't always achieve the smallest end result.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在压缩数据的大小、压缩和解压缩的速度以及压缩算法的普及程度之间需要做出权衡。如果大部分时间数据被存储，那么花几分钟压缩一个大的数据文件可能是可以接受的，因为节省的空间超过了访问时间所付出的代价，但对于短暂存在或经常访问的数据，压缩和解压缩的开销则更为重要。就我们的目的而言，我们需要一个在不同环境中被广泛理解的压缩算法，即使它并不总是实现最小的最终结果。
- en: GZIP compression is available on almost every single system, and web servers
    such as Apache or nginx provide native support to compress responses that pass
    through them—which is far better than implementing your own ad hoc compression
    at the level of Python. It's important to remember that while this will save network
    bandwidth, it will use more CPU, and so experimenting with metrics collection
    activated will let us see the results—and decide whether this option is a good
    idea.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: GZIP压缩几乎在所有系统中都可用，并且像Apache或nginx这样的Web服务器为通过它们的响应提供了原生支持，这比在Python级别实现自己的临时压缩要好得多。重要的是要记住，虽然这会节省网络带宽，但它会使用更多的CPU，因此通过激活指标收集进行实验将让我们看到结果——并决定这个选项是否是一个好主意。
- en: 'For example, this nginx configuration will enable GZIP compression for any
    response produced by the Quart app on port `5000`, with an `application/json`
    content type:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这个nginx配置将启用端口`5000`上Quart应用程序产生的任何响应的GZIP压缩，内容类型为`application/json`：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'From the client side, making an HTTP request to the nginx server at `localhost:8080`,
    proxying for the application at `localhost:5000` with an `Accept-Encoding: gzip`
    header, will trigger the compression:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '从客户端来看，向`localhost:8080`上的nginx服务器发送HTTP请求，通过带有`Accept-Encoding: gzip`头的代理为`localhost:5000`上的应用程序触发压缩：'
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In Python, requests made using the `aiohttp` and `requests` libraries will automatically
    decompress responses that are GZIP-encoded, so you don't have to worry about doing
    this when your service is calling another service.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，使用`aiohttp`和`requests`库发出的请求将自动解压缩GZIP编码的响应，因此当你的服务调用另一个服务时，你不必担心这一点。
- en: 'Decompressing the data adds some processing, but Python''s GZIP module relies
    on `zlib` (`http://www.zlib.net/`), which is very fast. To accept compressed responses to
    HTTP queries, we just need to add a header indicating we can deal with a GZIP-encoded
    response:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 解压缩数据会增加一些处理，但Python的GZIP模块依赖于`zlib`（`http://www.zlib.net/`），它非常快。为了接受压缩的HTTP查询响应，我们只需要添加一个头信息，表明我们可以处理GZIP编码的响应：
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To compress the data that you are sending to the server, you can use the `gzip`
    module and specify a `Content-Encoding` header:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要压缩发送到服务器的数据，你可以使用`gzip`模块并指定一个`Content-Encoding`头信息：
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In that case, however, you will get the zipped content in your Quart application,
    and you will need to decompress it in your Python code, or if you are using an
    nginx proxy that handles incoming web connections, nginx can decompress the requests
    for you. We discuss nginx in more detail in *Chapter 10*, *Deploying on AWS*.
    To summarize, setting up GZIP compression for all your service responses is a
    low-effort change with nginx, and your Python client can benefit from it by setting
    the right header. Sending compressed data is a little more complicated however,
    because the work isn't done for you—but it may still have benefits for large data
    transfers.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这种情况下，你将在Quart应用程序中获得压缩内容，你需要在Python代码中对其进行解压缩，或者如果你使用的是处理传入Web连接的nginx代理，nginx可以为你解压缩请求。我们将在第10章“在AWS上部署”中更详细地讨论nginx。总结来说，使用nginx为所有服务响应设置GZIP压缩是一个低成本的更改，你的Python客户端可以通过设置正确的头信息从中受益。然而，发送压缩数据要复杂一些，因为这项工作并不是为你完成的——但它可能对大量数据传输仍然有益。
- en: If you want to further reduce the size of HTTP request/response payloads, another
    option is to switch from JSON to binary payloads. That way, you do not have to
    deal with compression, and processing the data may be faster, but the message
    size reduction is not as good.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要进一步减小HTTP请求/响应负载的大小，另一个选项是将从JSON切换到二进制负载。这样，你就不必处理压缩，处理数据可能更快，但消息大小的减少并不那么显著。
- en: Protocol Buffers
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 协议缓冲区
- en: While it is usually not relevant, if your microservice deals with a lot of data,
    using an alternative format can be an attractive option to increase performance,
    and decrease the required network bandwidth without having to use extra processing
    power and time compressing and decompressing the data. Two widely used binary
    formats are **Protocol Buffers** (**protobuf**) ([https://developers.google.com/protocol-buffers](https://developers.google.com/protocol-buffers))
    and **MessagePack**.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常情况下并不相关，但如果你的微服务处理大量数据，使用替代格式可以是一个吸引人的选项，以提高性能，并减少所需的网络带宽，而无需使用额外的处理能力和时间来压缩和解压缩数据。两种广泛使用的二进制格式是**协议缓冲区**（**protobuf**）([https://developers.google.com/protocol-buffers](https://developers.google.com/protocol-buffers))和**MessagePack**。
- en: Protocol Buffers requires you to describe data that's being exchanged into some
    schema that will be used to index the binary content. The schemas add some work
    because all data that is transferred will need to be described in a schema, and
    you will need to learn a new **Domain-Specific Language** (**DSL**). In a typed
    language, such as Rust, C++, or Go, defining these structures is something that
    already has to be done, so the overhead is far less.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 协议缓冲区要求你描述正在交换的数据，以便将其索引到某个将用于索引二进制内容的模式中。这些模式增加了一些工作量，因为所有传输的数据都需要在模式中描述，你将需要学习一种新的**领域特定语言**（**DSL**）。在像Rust、C++或Go这样的类型语言中，定义这些结构已经是必须完成的任务，因此开销要小得多。
- en: 'However, the advantages are that the messages are well defined and can be easily
    validated before either end of the network conversation attempts to use the information.
    It is also possible to generate code for various languages—including Python—that
    let you construct the data in a way that is more suitable for the language being
    used. The following example is taken from the protobuf documentation:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，其优势在于消息定义良好，在网络对话的任一端尝试使用信息之前，可以轻松验证。还可能为各种语言生成代码，包括Python，让你以更适合所用语言的方式构造数据。以下示例取自protobuf文档：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The schema is not very Pythonic, as it is intended to support multiple languages
    and environments. If you interact with statically typed languages or would like
    a feature to do basic syntax checking on data for you, then a definition like
    this may be helpful.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 该模式并不非常符合Python风格，因为它旨在支持多种语言和环境。如果你与静态类型语言交互或希望有一个功能为你对数据进行基本语法检查，那么这样的定义可能很有帮助。
- en: Using Protocol Buffers with a framework such as gRPC ([https://grpc.io/](https://grpc.io/))
    can abstract away the network interaction from your application, and instead provide
    a client with a function call in Python and little need to consider how it generates
    its return value.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用gRPC框架（[https://grpc.io/](https://grpc.io/))与协议缓冲区结合可以抽象出你的应用程序的网络交互，并为客户端提供一个Python中的函数调用，几乎不需要考虑它如何生成返回值。
- en: MessagePack
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MessagePack
- en: 'Unlike Protocol Buffers, MessagePack ([http://msgpack.org/](http://msgpack.org/))
    is schemaless, and can serialize your data by just calling a function. It''s a
    simple alternative to JSON, and has implementations in most languages. The `msgpack`
    Python library (installed using the `pip install` `msgpack-python` command) offers
    the same level of integration as JSON:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 与Protocol Buffers不同，MessagePack ([http://msgpack.org/](http://msgpack.org/))是无模式的，只需调用一个函数就可以序列化你的数据。它是JSON的简单替代品，并在大多数语言中有实现。`msgpack`
    Python库（使用`pip install` `msgpack-python`命令安装）提供了与JSON相同级别的集成：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Using MessagePack is simple compared to protobuf, but which one is faster and
    provides the best compression ratio depends a lot on your data. In some rare cases,
    plain JSON might be even quicker to serialize than a binary format.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 与protobuf相比，使用MessagePack很简单，但哪个更快，提供最佳的压缩比率，很大程度上取决于你的数据。在少数情况下，纯JSON可能比二进制格式序列化得更快。
- en: In terms of compression, you can expect 10% to 20% compression with MessagePack,
    but if your JSON contains a lot of strings—which is often the case in microservices—GZIP
    will perform much better.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在压缩方面，你可以期望使用MessagePack有10%到20%的压缩率，但如果你的JSON包含大量字符串——这在微服务中很常见——GZIP将表现得更好。
- en: 'In the following example, a huge JSON payload of 48 KB that contains a lot
    of strings is converted using MessagePack and JSON and then GZIPped in both cases:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，一个包含大量字符串的48 KB的巨大JSON有效负载被使用MessagePack和JSON进行转换，然后在两种情况下都进行了GZIP压缩：
- en: '[PRE13]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Using MessagePack reduces the size of the payload by approximately 14%, but
    GZIP is making it 11 times smaller with both JSON and MessagePack payloads!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MessagePack可以将有效负载的大小减少大约14%，但GZIP将JSON和MessagePack有效负载的大小减少到原来的1/11！
- en: It's clear that whatever format you are using, the best way to reduce the payload
    sizes is to use GZIP—and if your web server does not deal with decompression,
    it's straightforward in Python thanks to `gzip.uncompress()`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，无论你使用什么格式，最佳方式是使用GZIP来减少有效负载大小——如果你的Web服务器不处理解压缩，那么在Python中通过`gzip.uncompress()`进行解压缩是直接的。
- en: Message serialization often only supports basic data types, as they must remain
    unaware of what environment is running in both the source and destination. This
    means that they cannot encode data that might be commonly used in Python, such
    as `datetime` objects to represent time. While other languages have date and time
    representation, it is not done in the same way, and so data like this and other
    Python objects need to be converted into a serializable form that other platforms
    can understand. For date and time, common options include an integer representing
    epoch time (the number of seconds since 1^(st) January 1970) or a string in ISO8601
    format, such as 2021-03-01T13:31:03+00:00.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 消息序列化通常只支持基本数据类型，因为它们必须对源和目标环境中的环境保持无知。这意味着它们不能编码在Python中可能常用的数据，例如使用`datetime`对象表示时间。虽然其他语言有日期和时间表示，但它们的方式并不相同，因此像这样的数据和其他Python对象需要转换为其他平台可以理解的可序列化形式。对于日期和时间，常见的选项包括表示纪元时间的整数（自1970年1月1日起的秒数）或ISO8601格式的字符串，例如2021-03-01T13:31:03+00:00。
- en: In any case, in a world of microservices where JSON is the most accepted standard,
    taking care of dates is a minor annoyance to stick with a universally adopted
    standard.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，在一个以JSON为最接受标准的微服务世界中，处理日期只是坚持一个普遍采用的标准的小烦恼。
- en: Unless all your services are in Python with well-defined structures, and you
    need to speed up the serialization steps as much as possible, it is probably simpler
    to stick with JSON.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 除非所有你的服务都在Python中并且具有明确的结构，并且你需要尽可能快地加快序列化步骤，否则坚持使用JSON可能更简单。
- en: Putting it together
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 整合起来
- en: 'Before moving on, we will quickly recall what we have covered so far:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们将快速回顾一下到目前为止我们已经覆盖了什么：
- en: Implementing HTTP cache headers is a great way to speed up repeated requests
    for data
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现HTTP缓存头是一个加快对数据重复请求的好方法。
- en: GZIP compression is an efficient way to lessen the size of requests and responses
    and is easy to set up
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GZIP压缩是一种有效的方法来减少请求和响应的大小，并且很容易设置
- en: Binary protocols are an attractive alternative to plain JSON, but it does depend
    on the situation
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二进制协议是纯JSON的有吸引力的替代品，但这取决于具体情况
- en: The next section will focus on asynchronous calls; everything your microservice
    can do that goes beyond the request/response pattern.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将重点介绍异步调用；你的微服务可以做的所有超出请求/响应模式的事情。
- en: Asynchronous messages
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异步消息
- en: In microservice architecture, asynchronous calls play a fundamental role when
    a process that is used to be performed in a single application now implicates
    several microservices. We touched briefly on this in the previous chapter with
    our change to the Jeeves application, which now communicates with its workers
    using an asynchronous message queue. To make the best use of these, we will investigate
    these tools in more depth.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在微服务架构中，当原本在一个单一应用程序中执行的过程现在涉及到多个微服务时，异步调用扮演着基本角色。我们在上一章中简要提到了这一点，通过我们对 Jeeves
    应用程序的更改，现在它通过异步消息队列与其工作进程进行通信。为了充分利用这些工具，我们将更深入地研究这些工具。
- en: Asynchronous calls can be as simple as a separate thread or process within a
    microservice app that is receiving some work to be done, and performs it without
    interfering with the HTTP request/response round trips that are happening at the
    same time.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 异步调用可以像微服务应用程序中的一个单独的线程或进程那样简单，它接收一些要执行的工作，并在不干扰同时发生的 HTTP 请求/响应往返过程中执行它。
- en: But doing everything directly from the same Python process is not very robust.
    What happens if the process crashes and gets restarted? How do we scale background
    tasks if they are built like that?
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 但直接从同一个 Python 进程中做所有事情并不是非常健壮。如果进程崩溃并重新启动会发生什么？如果它们是这样构建的，我们如何扩展后台任务？
- en: It's much more reliable to send a message that gets picked by another program,
    and let the microservice focus on its primary goal, which is to serve responses
    to clients. If a web request does not need an immediate answer, an endpoint in
    our service can then become code that accepts an HTTP request, processes it, and
    passes it on, and its response to the client is now whether or not our service
    has successfully received the request rather than whether the request has been
    processed.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 发送一条被另一个程序接收的消息要可靠得多，让微服务专注于其核心目标，即向客户端提供服务。如果一个网络请求不需要立即回答，那么我们服务中的一个端点可以成为接受
    HTTP 请求、处理它并将其传递出去的代码，而其对客户端的响应现在是我们的服务是否已成功接收请求，而不是请求是否已被处理。
- en: In the previous chapter, we looked at how Celery could be used to build a microservice
    that gets some work from a message broker like RabbitMQ. In that design, the Celery
    worker blocks—that is, it halts operation while it is waiting—until a new message
    is added to the RabbitMQ queue.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了如何使用 Celery 来构建一个从类似 RabbitMQ 的消息代理那里获取一些工作的微服务。在那个设计中，Celery 工作进程会阻塞——也就是说，它在等待新消息添加到
    RabbitMQ 队列时会停止操作。
- en: Message queue reliability
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消息队列可靠性
- en: As with any distributed system, there are considerations with regard to reliability
    and consistency. Ideally, we would like to add a message to the queue and have
    it delivered—and acted upon—exactly once. In practice this is almost impossible
    to achieve in a distributed system, as components fail, experiencing high latency
    or packet loss, while all sorts of complex interactions occur.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何分布式系统一样，在可靠性和一致性方面都需要考虑。理想情况下，我们希望将一条消息添加到队列中，并确保它被准确无误地投递并执行——恰好一次。在实践中，在分布式系统中几乎不可能实现这一点，因为组件可能会失败，经历高延迟或数据包丢失，同时发生各种复杂的交互。
- en: 'We have two real choices, encoded in RabbitMQ''s delivery strategies: "at-most-once"
    and "at-least-once."'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个实际的选择，这些选择编码在 RabbitMQ 的投递策略中：“最多一次”和“至少一次”。
- en: 'A strategy to deliver a message at most once will not account for any unreliability
    in the message delivery system or failures in a worker. Once a worker has accepted
    the message, that is it: the message queue forgets about it. If the worker then
    suffers a failure and does not complete the chunk of work it has been given, that
    is something the wider system needs to cope with.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一种最多一次投递消息的策略不会考虑消息投递系统中的任何不可靠性或工作进程中的失败。一旦工作进程接受了一条消息，那就结束了：消息队列会忘记它。如果工作进程随后发生故障并且没有完成分配给它的任务部分，这是整个系统需要应对的问题。
- en: With a promise to deliver a message at least once, in the case of any failures
    the deliveries will be attempted again until a worker both accepts the message
    and acknowledges that it has acted upon it. This ensures that no data is lost,
    but it does mean that there are situations where the message can be delivered
    to more than one worker, and so some sort of **universally unique identifier**
    (**UUID**) is a good idea, so that while some work may be duplicated, it can be
    deduplicated when it is written to any database or storage. A wider discussion
    of distributed system reliability and consensus protocols like PAXOS would require
    a book of its own.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个承诺至少发送一次消息，在出现任何失败的情况下，交付将再次尝试，直到工作者接受消息并确认它已采取行动。这确保了不会丢失任何数据，但这确实意味着在某些情况下，消息可以发送给多个工作者，因此某种**全局唯一标识符**（**UUID**）是一个好主意，这样虽然一些工作可能会重复，但在写入任何数据库或存储时可以进行去重。关于分布式系统可靠性和像PAXOS这样的共识协议的更广泛讨论将需要一本自己的书。
- en: Basic queues
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本队列
- en: The pattern used by Celery workers is a push-pull tasks queue. One service pushes
    messages into a specific queue, and some workers pick them up from the other end
    and perform an action on them. Each task goes to a single worker. Consider the
    following diagram, shown in *Figure 6.1*.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Celery工作者使用的模式是推拉任务队列。一个服务将消息推入特定的队列，一些工作者从另一端取走它们并对其执行操作。每个任务都只去一个工作者。考虑以下图示，如*图6.1*所示。
- en: '![image2.jpg](img/B17108_06_01.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![image2.jpg](img/B17108_06_01.png)'
- en: 'Figure 6.1: Tasks passing through a message queue'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：任务通过消息队列传递
- en: There is no bidirectional communication—the sender merely deposits a message
    in the queue and leaves. The next available worker gets the next message. This
    blind, unidirectional message passing is perfect when you want to perform some
    asynchronous parallel tasks, which makes it easy to scale.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 没有双向通信——发送者只是在队列中存入一条消息然后离开。下一个可用的工作者获取下一条消息。当你想要执行一些异步并行任务时，这种盲目单向的消息传递是完美的，这使得它很容易进行扩展。
- en: In addition, once the sender has confirmed that the message was added to the
    broker, we can have message brokers—such as RabbitMQ—offer some message persistence.
    In other words, if all workers go offline, we don't lose the messages that are
    in the queue.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一旦发送者确认消息已添加到代理，我们就可以让消息代理，如RabbitMQ，提供一些消息持久化。换句话说，如果所有工作者都离线，我们不会丢失队列中的消息。
- en: Topic exchanges and queues
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主题交换机和队列
- en: Topics are a way of filtering and classifying messages that travel through the
    queue. When using topics, each message is sent with an extra label that helps
    to identify what sort of message it is, and our workers can subscribe to specific
    topics, or patterns that match several topics.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 主题是一种过滤和分类通过队列传输的消息的方式。当使用主题时，每条消息都会附带一个额外的标签，有助于识别其类型，我们的工作者可以订阅特定的主题或匹配多个主题的模式。
- en: Let's imagine a scenario where we are releasing a mobile app to the Android
    Play Store and the Apple App Store. When our automation tasks finish building
    the Android app, we can send a message with a routing key of `publish.playstore`,
    so that RabbitMQ can route this message to the right topics. The reason that there
    is a difference between a routing key and a topic is that a topic can match a
    pattern. The worker that is capable of publishing files to the Play Store can
    subscribe to the topic `publish.playstore` and get its workload from those messages,
    but we could also have a queue for messages matching `publish.*` and a worker
    that sends notifications whenever something is about to be uploaded to the Play
    Store, the App Store, or any other place you might publish software.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设想一个场景，我们正在将移动应用到Android Play商店和Apple App商店发布。当我们的自动化任务完成Android应用的构建后，我们可以发送一个带有路由键`publish.playstore`的消息，这样RabbitMQ就可以将这条消息路由到正确的主题。路由键和主题之间有区别的原因是主题可以匹配模式。能够将文件发布到Play
    Store的工作者可以订阅`publish.playstore`主题，并从这些消息中获取其工作量，但我们也可以有一个匹配`publish.*`的消息队列和一个工作者，每当有内容即将上传到Play
    Store、App Store或其他可能发布软件的地方时，它会发送通知。
- en: In our microservices, this means we can have specialized workers that all register
    to the same messaging broker and get a subset of the messages that are added to
    it.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的微服务中，这意味着我们可以拥有专门的工作者，它们都注册到同一个消息代理，并获取添加到其中的消息的子集。
- en: '![image1.jpg](img/B17108_06_02.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![image1.jpg](img/B17108_06_02.png)'
- en: 'Figure 6.2: Tasks of different types passing through a message queue'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：不同类型的任务通过消息队列传递
- en: This sort of behavior exists in most message queue services, in slightly different
    forms. Let's look at how to set this up in RabbitMQ.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这种行为在大多数消息队列服务中都有，形式略有不同。让我们看看如何在RabbitMQ中设置这个。
- en: To install a **RabbitMQ** broker, you can look at the download page at [http://www.rabbitmq.com/download.html](http://www.rabbitmq.com/download.html).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装**RabbitMQ**代理，你可以查看[http://www.rabbitmq.com/download.html](http://www.rabbitmq.com/download.html)的下载页面。
- en: Running the container should be enough for any local experiments. RabbitMQ implements
    the **Advanced Message Queuing Protocol** (**AMQP**). This protocol, described
    at [http://www.amqp.org/](http://www.amqp.org/), is a complete standard that has
    been developed for years by a group of companies working together.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 运行容器应该足以进行任何本地实验。RabbitMQ实现了**高级消息队列协议**（**AMQP**）。该协议由一个合作工作的公司团体开发多年，描述在[http://www.amqp.org/](http://www.amqp.org/)。
- en: 'AMQP is organized into three concepts: queues, exchanges, and bindings:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: AMQP组织成三个概念：队列、交换机和绑定：
- en: A queue is a recipient that holds messages and waits for consumers to pick them
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 队列是一个持有消息并等待消费者取走的接收者
- en: An exchange is an entry point for publishers to add new messages to the system
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交换机是发布者向系统中添加新消息的入口点
- en: A binding defines how messages are routed from exchanges to queues
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绑定定义了消息如何从交换机路由到队列
- en: For our topic queue, we need to set one exchange, so RabbitMQ accepts new messages,
    and all the queues we want for workers to pick messages. Between those two ends,
    we want to route the messages to the different queues, depending on the topics,
    using a binding.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的主题队列，我们需要设置一个交换机，这样RabbitMQ才能接受新消息，以及所有我们希望工作者从中选择消息的队列。在这两个端点之间，我们希望根据主题使用绑定将消息路由到不同的队列。
- en: 'Let''s look at how we would set up our app publishing example from earlier.
    We will assume we have two workers: one that publishes Android applications, and
    the other that sends notifications, such as updating a website or sending an email.
    Using the `rabbitmqadmin` command line that gets installed with RabbitMQ, we can
    create all the necessary parts. If the admin command does not come installed,
    you can find instructions on installing it at [https://www.rabbitmq.com/management-cli.html](https://www.rabbitmq.com/management-cli.html):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何设置之前提到的应用发布示例。我们将假设我们有两个工作者：一个发布Android应用，另一个发送通知，例如更新网站或发送电子邮件。使用与RabbitMQ一起安装的`rabbitmqadmin`命令行，我们可以创建所有必要的部分。如果管理命令没有安装，你可以在[https://www.rabbitmq.com/management-cli.html](https://www.rabbitmq.com/management-cli.html)找到安装说明：
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this setup, whenever a message is sent to RabbitMQ—and if the topic starts
    with `publish`—it will be sent to the notifications queue; and if it is `publish.playstore`,
    then it will end up in both the notifications and playstore queues. Any other
    topics will cause the message to be discarded.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配置中，每当有消息发送到RabbitMQ——如果主题以`publish`开头——它将被发送到通知队列；如果是`publish.playstore`，那么它将同时进入通知和playstore队列。任何其他主题都将导致消息被丢弃。
- en: 'To interact with RabbitMQ in the code, we can use **Pika**. This is a Python
    RPC client that implements all the RPC endpoints that a Rabbit service publishes:
    [https://pika.readthedocs.io](https://pika.readthedocs.io).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要在代码中与RabbitMQ交互，我们可以使用**Pika**。这是一个Python RPC客户端，它实现了Rabbit服务发布的所有RPC端点：[https://pika.readthedocs.io](https://pika.readthedocs.io)。
- en: Everything we do with Pika can be done on the command line using `rabbitmqadmin`.
    You can directly get the status of all parts of the system, send and receive messages,
    and check what's in a queue. It is an excellent way to experiment with your messaging
    setup.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Pika做的所有事情都可以使用`rabbitmqadmin`在命令行上完成。你可以直接获取系统所有部分的状态，发送和接收消息，并检查队列中的内容。这是实验你的消息设置的一个极好方式。
- en: 'The following script shows how to publish two messages in RabbitMQ in the incoming
    exchange. One concerns a new app being published, and the other is about a newsletter:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本展示了如何在RabbitMQ的入站交换机中发布两条消息。一条是关于新应用发布的，另一条是关于通讯稿的：
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'These RPC calls will each add one message to the incoming topic exchange. For
    the first message, the exchange will then add one message to the `playstore` queue,
    and for the second, two messages will be added—one to each queue. A worker script
    that waits for work that needs to be published to the Play Store would look like
    this:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这些RPC调用将为每个入站主题交换机添加一条消息。对于第一条消息，交换机将为`playstore`队列添加一条消息，对于第二条，将添加两条消息——每条消息到一个队列。一个等待需要发布到Play
    Store的工作的工作者脚本可能看起来像这样：
- en: '[PRE16]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Notice that Pika sends back an ACK to RabbitMQ about the message, so it can
    be safely removed from the queue once the worker has succeeded. This is the at-least-once
    strategy approach to message delivery. The `notifications` receiver can be identical
    apart from the queue it subscribes to and what it does with the message body:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Pika会将一个ACK发送回RabbitMQ关于该消息，因此一旦工人成功处理，就可以安全地从队列中移除。这是至少一次消息传递策略。`notifications`接收器除了它订阅的队列和它对消息体的处理外，可以相同：
- en: '[PRE17]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'AMQP offers many patterns that you can investigate to exchange messages. The
    tutorial page has many examples, and they are all implemented using Python and
    Pika: [http://www.rabbitmq.com/getstarted.html](http://www.rabbitmq.com/getstarted.html).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: AMQP提供了许多可以调查的消息交换模式。教程页面有许多示例，它们都是使用Python和Pika实现的：[http://www.rabbitmq.com/getstarted.html](http://www.rabbitmq.com/getstarted.html)。
- en: To integrate these examples in our microservices, the publisher phase is straightforward.
    Your Quart application can create a connection to RabbitMQ using `pika.BlockingConnection`
    and send messages through it. Projects such as pika-pool ([https://github.com/bninja/pika-pool](https://github.com/bninja/pika-pool))
    implement simple connection pools so you can manage RabbitMQ channels without
    having to connect/disconnect every time you are sending something through RPC.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要将以下示例集成到我们的微服务中，发布阶段是直接的。您的Quart应用程序可以使用`pika.BlockingConnection`创建到RabbitMQ的连接并通过它发送消息。例如，pika-pool
    ([https://github.com/bninja/pika-pool](https://github.com/bninja/pika-pool))实现了简单的连接池，这样您就可以在发送RPC时无需每次都连接/断开RabbitMQ通道。
- en: The consumers, on the other hand, are trickier to integrate into microservices.
    Pika can be embedded into an event loop running in the same process as the Quart
    application, and trigger a function when a message is received. It will simply
    be another entry point into the same code, and could be run alongside a RESTful
    API if that's also required.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，消费者更难集成到微服务中。Pika可以嵌入到与Quart应用程序在同一进程中运行的事件循环中，并在接收到消息时触发一个函数。它将仅仅是进入相同代码的另一个入口点，如果需要，也可以与RESTful
    API并行运行。
- en: Publish/subscribe
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发布/订阅
- en: The previous pattern has workers that handle the specific topics of messages,
    and the messages consumed by a worker are completely gone from the queue. We even
    added code to acknowledge that the message was consumed.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的模式有处理特定消息主题的工人，工人消费的消息将完全从队列中消失。我们甚至添加了代码来确认消息已被消费。
- en: When you want a message to be published to several workers, however, the **Publish/Subscribe**
    (**pubsub**) pattern needs to be used.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当你想要将消息发布到多个工人时，必须使用**发布/订阅**（**pubsub**）模式。
- en: This pattern is the basis for building a general event system and is implemented
    exactly like the previous one, in which there is one exchange and several queues.
    The difference is that the exchange part has a fanout type.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式是构建通用事件系统的基础，其实现方式与之前完全相同，其中有一个交换机和几个队列。区别在于交换部分具有扇出类型。
- en: In that setup, every queue that you bind to a fanout exchange will receive the
    same message. With pubsub in place, you can broadcast messages to all your microservices
    if necessary.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置中，每个绑定到扇出交换机的队列都将接收到相同的信息。如果有必要，通过pubsub可以广播消息到所有的微服务。
- en: Putting it together
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 整合
- en: 'In this section, we have covered the following about asynchronous messaging:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了以下关于异步消息传递的内容：
- en: Non-blocking calls should be used every time a microservice can execute some
    work out of band. There's no good reason to block a request if what you are doing
    is not utilized in the response.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每当微服务可以执行一些非阻塞工作的时候，都应该使用非阻塞调用。如果你所做的工作在响应中没有被利用，就没有理由阻塞请求。
- en: Service-to-service communication is not always limited to task queues.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务到服务的通信并不总是限于任务队列。
- en: Sending events through a message queue is a good way to prevent tightly coupled
    components.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过消息队列发送事件是防止组件紧密耦合的好方法。
- en: We can build a full event system around a broker—such as RabbitMQ—to make our
    microservices interact with each other via messages.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以在一个代理（如RabbitMQ）周围构建一个完整的事件系统，使我们的微服务通过消息相互交互。
- en: RabbitMQ can be used to coordinate all the message passing, with messages sent
    using Pika.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用RabbitMQ来协调所有消息传递，使用Pika发送消息。
- en: Testing
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试
- en: 'As we learned in *Chapter 3*, *Coding, Testing, and Documentation: the Virtuous
    Cycle*, the biggest challenge when writing functional tests for a service that
    calls other services is to isolate all network calls. In this section, we''ll
    see how we can mock asynchronous calls made using `aiohttp`.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在*第3章*中学习的，*编码、测试和文档：良性循环*，为调用其他服务的服务编写功能测试时最大的挑战是隔离所有网络调用。在本节中，我们将看到如何模拟使用`aiohttp`进行的异步调用。
- en: 'Testing `aiohttp` and its outgoing web requests involves a different approach
    to traditional synchronous tests. The `aioresponses` project ([https://github.com/pnuckowski/aioresponses](https://github.com/pnuckowski/aioresponses))
    allows you to easily create mocked responses to web requests made using an `aiohttp`
    `ClientSession`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 测试`aiohttp`及其出站Web请求需要与传统同步测试不同的方法。`aioresponses`项目 ([https://github.com/pnuckowski/aioresponses](https://github.com/pnuckowski/aioresponses))允许您轻松创建使用`aiohttp`
    `ClientSession`进行的Web请求的模拟响应：
- en: '[PRE18]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this example, we tell `aioresponses` that any GET request made to `http://test.example.com`
    should return the data we specify. This way we can easily provide mocked responses
    for several URLs, and even the same URL by invoking `mocked.get` more than once
    to create multiple responses for the same endpoint.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们告诉`aioresponses`，对`http://test.example.com`发出的任何GET请求都应该返回我们指定的数据。这样我们就可以轻松地为多个URL提供模拟响应，甚至可以通过多次调用`mocked.get`为同一端点创建多个响应。
- en: If you are using Requests to perform all the calls—or you are using a library
    that is based on Requests that does not customize it too much—this isolation work
    is also easy to do thanks to the `requests-mock` project ([https://requests-mock.readthedocs.io](https://requests-mock.readthedocs.io)),
    which implements mocked calls in a similar way, and likely inspired `aioresponses`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用Requests执行所有调用——或者您使用的是基于Requests的库，并且没有对其进行太多定制——由于`requests-mock`项目 ([https://requests-mock.readthedocs.io](https://requests-mock.readthedocs.io))，这项隔离工作也变得容易进行，该项目以类似的方式实现了模拟调用，并可能启发了`aioresponses`。
- en: That said, mocking responses from other services is still a fair amount of work,
    and can be difficult to maintain. It means that an eye needs to be kept on how
    the other services are evolving over time, so your tests are not based on a mock
    that's no longer a reflection of the real API.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，模拟其他服务的响应仍然是一项相当多的工作，并且可能难以维护。这意味着需要关注其他服务随时间的发展，以确保您的测试不是基于不再反映真实API的模拟。
- en: Using mocks is encouraged to build good functional tests coverage, but make
    sure you are doing integration tests as well, where the service is tested in a
    deployment where it calls other services for real.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励使用模拟来构建良好的功能测试覆盖率，但请确保您也在进行集成测试，在该测试中，服务在一个部署环境中被测试，它调用其他服务进行真实操作。
- en: Using OpenAPI
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OpenAPI
- en: The OpenAPI Specification ([https://www.openapis.org/](https://www.openapis.org/)),
    previously known as Swagger, is a standard way of describing a set of HTTP endpoints,
    how they are used, and the structure of the data that is sent and received. By
    describing an API using a JSON or YAML file, it allows the intent to become machine-readable—this
    means that with an OpenAPI Specification, you can use a code generator to produce
    a client library in a language of your choosing, or to automatically validate
    data as it enters or leaves the system.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAPI规范 ([https://www.openapis.org/](https://www.openapis.org/))，之前被称为Swagger，是描述一组HTTP端点、它们的使用方式以及发送和接收的数据结构的标准方式。通过使用JSON或YAML文件描述API，它使得意图变得机器可读——这意味着有了OpenAPI规范，您可以使用代码生成器以您选择的语言生成客户端库，或者自动验证数据在进入或离开系统时的有效性。
- en: OpenAPI has the same goal that WSDL ([https://www.w3.org/TR/2001/NOTE-wsdl-20010315](https://www.w3.org/TR/2001/NOTE-wsdl-20010315))
    had back in the XML web services era, but it's much lighter and straight to the
    point.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAPI具有与WSDL ([https://www.w3.org/TR/2001/NOTE-wsdl-20010315](https://www.w3.org/TR/2001/NOTE-wsdl-20010315))在XML网络服务时代相同的目标，但它更轻量级，更直接。
- en: 'The following example is a minimal OpenAPI description file that defines one
    single `/apis/users_ids` endpoint and supports the `GET` method to retrieve the
    list of user IDs:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个最小的OpenAPI描述文件示例，它定义了一个单一的`/apis/users_ids`端点，并支持`GET`方法来检索用户ID列表：
- en: '[PRE19]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The full OpenAPI Specification can be found on GitHub; it is very detailed
    and will let you describe metadata about the API, its endpoints, and the data
    types it uses: [https://github.com/OAI/OpenAPI-Specification](https://github.com/OAI/OpenAPI-Specification).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的 OpenAPI 规范可以在 GitHub 上找到；它非常详细，并允许您描述有关 API、其端点和它使用的数据类型元数据：[https://github.com/OAI/OpenAPI-Specification](https://github.com/OAI/OpenAPI-Specification)。
- en: The data types described in the schema sections are following the JSON Schema
    specification ([http://json-schema.org/latest/json-schema-core.html](http://json-schema.org/latest/json-schema-core.html)).
    Here, we are describing that the `/get_ids` endpoint returns an array of integers.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 模式部分中描述的数据类型遵循 JSON Schema 规范([http://json-schema.org/latest/json-schema-core.html](http://json-schema.org/latest/json-schema-core.html))。在这里，我们描述了
    `/get_ids` 端点返回一个整数数组。
- en: You can provide a lot of detail about your API in that specification—things
    such as what headers should be present in your requests, or what will be the content
    type of some responses and can be added to it.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在该规范中提供有关您的 API 的许多详细信息——例如，您的请求中应该包含哪些标题，或者某些响应的内容类型是什么，以及可以添加到其中的内容。
- en: 'Describing your HTTP endpoints with OpenAPI offers some excellent possibilities:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 OpenAPI 描述您的 HTTP 端点提供了许多优秀的机会：
- en: There are a plethora of OpenAPI clients that can consume your description and
    do something useful with it, such as building functional tests against your service,
    or validating data that is sent to it.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有许多 OpenAPI 客户端可以消费您的描述并对其进行有用的操作，例如针对您的服务构建功能测试或验证发送给它的数据。
- en: It provides a standard, language-agnostic documentation for your API
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它为您的 API 提供了标准、语言无关的文档
- en: The server can check that the requests and responses follow the spec
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器可以检查请求和响应是否符合规范
- en: Some web frameworks even use the specification to create all the routing and
    I/O data checks for your microservices; for instance, Connexion ([https://github.com/zalando/connexion](https://github.com/zalando/connexion))
    does this for Flask. Support for this within Quart is limited at the time of writing,
    but the situation is always improving. For this reason, we won't be using OpenAPI
    a great deal in the examples presented here.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 一些 Web 框架甚至使用规范来创建所有路由和 I/O 数据检查，用于您的微服务；例如，Connexion ([https://github.com/zalando/connexion](https://github.com/zalando/connexion))
    为 Flask 做了这件事。在撰写本文时，Quart 对此的支持有限，但情况总是在不断改善。因此，我们在这里的示例中不会大量使用 OpenAPI。
- en: 'There are two schools of thought when people are building HTTP APIs with OpenAPI:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们使用 OpenAPI 构建HTTP API时，有两种不同的观点：
- en: Specification-first, where you create a Swagger specification file and then
    create your app on top of it, using all the information provided in that specification.
    That's the principle behind Connexion.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规范优先，即您首先创建 Swagger 规范文件，然后在它之上创建您的应用程序，使用该规范中提供的信息。这就是 Connexion 的原理。
- en: Specification-extracted, where it is your code that generates the Swagger specification
    file. Some toolkits out there will do this by reading your view docstrings, for
    instance.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规范提取，即您的代码生成 Swagger 规范文件。一些工具包会通过读取您的视图文档字符串来完成此操作，例如。
- en: Summary
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we've looked at how a service can interact with other services
    synchronously, by using a Requests session, and asynchronously, by using Celery
    workers or more advanced messaging patterns based on RabbitMQ.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了服务如何通过使用请求会话同步地与其他服务交互，以及通过使用 Celery 工作进程或基于 RabbitMQ 的更高级的消息模式异步交互。
- en: We've also looked at some ways to test a service in isolation by mocking other
    services, but without mocking the message brokers themselves.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还研究了通过模拟其他服务来单独测试服务的一些方法，但不需要模拟消息代理本身。
- en: Testing each service in isolation is useful, but when something goes wrong,
    it's hard to know what happened, particularly if the bug happens in a series of
    asynchronous calls.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 单独测试每个服务是有用的，但当出现问题时，很难知道发生了什么，尤其是如果错误发生在一系列异步调用中。
- en: In that case, tracking what's going on with a centralized logging system helps
    a lot. The next chapter will explain how we can tool our microservices to follow
    their activities.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在那种情况下，使用集中式日志系统跟踪发生的事情非常有帮助。下一章将解释我们如何配置我们的微服务以跟踪其活动。
