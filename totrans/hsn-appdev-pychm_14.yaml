- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building a Data Pipeline in PyCharm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term *data pipeline* generally denotes a step-wise procedure that entails
    collecting, processing, and analyzing data. This term is widely used in the industry
    to express the need for a reliable workflow that takes raw data and converts it
    into actionable insights. Some data pipelines work at massive scales, such as
    a **marketing technology** (**MarTech**) company ingesting millions of data points
    from Kafka streams, storing them in large data stores such as **Hadoop** or **Clickhouse**,
    and then cleansing, enriching, and visualizing that data. Other times, the data
    is smaller but far more impactful, such as the project we’ll be working on in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to work with and maintain datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to clean and preprocess data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to visualize data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to utilize **machine** **learning** (**ML**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughout this chapter, you will be able to apply what you have learned about
    the topic of scientific computing so far to a real project with PyCharm. This
    serves as a hands-on discussion to conclude this topic of working with scientific
    computing and data science projects.
  prefs: []
  type: TYPE_NORMAL
- en: I want to specifically point out that I am heavily leveraging the text, code,
    and data from the first edition, which was written by a different author, Quan
    Nguyen. In this second edition, my main job was to update the existing content.
    Quan’s treatment in this chapter was excellent, so most of what I did to update
    this chapter was use the newer version of PyCharm, update the libraries used to
    the latest versions, and then re-write this chapter in my own words so that the
    writing style matches the rest of this book. There is no way I could have pulled
    this off without Quan’s original work and I wanted to tip my hat to the original
    Python data science kung fu master.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To proceed through this chapter, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Anaconda, which is a Python distribution tailored to data science workloads.
    You can find it, along with installation instructions for your OS, at [https://anaconda.com](https://anaconda.com).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Likewise, instead of the usual `pip`, I’ll be leveraging `conda`, which is Anaconda’s
    package manager. It is installed alongside Anaconda.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An installed and working copy of PyCharm. Its installation was covered in [*Chapter
    2*](B19644_02.xhtml#_idTextAnchor028), *Installation and Configuration*, in case
    you are jumping into the middle of this book.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This book’s sample source code from GitHub. We covered cloning the code in [*Chapter
    2*](B19644_02.xhtml#_idTextAnchor028), *Installation and Configuration*. You’ll
    find this chapter’s code at [https://github.com/PacktPublishing/Hands-On-Application-Development-with-PyCharm---Second-Edition/tree/main/chapter-14](https://github.com/PacktPublishing/Hands-On-Application-Development-with-PyCharm---Second-Edition/tree/main/chapter-14).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Datasets are the backbone of any data science project. With a good, well-structured
    dataset, we have the opportunity to explore, ideate, and discover important insights
    from the data. The terms *good* and *well-structured* are key. In the real world,
    this rarely happens by accident. I am the lead developer on a project that does
    data science every day. We ingest diagnostic, utilization, and performance data
    from various hardware platforms such as storage arrays, switches, virtualization
    nodes (such as VMware), backup devices, and more. We collect it for the entire
    enterprise; every device in every data center. Our software then turns that raw
    data into visualizations that provide insights, allowing organizations to effectively
    manage their IT estate through consolidating health monitoring, utilization and
    performance reporting, and capacity planning.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve been at it for 10 years now and we’re always looking to support new devices
    and systems. Our challenge, though, is getting the data we need. When I started
    10 years ago, getting data out of a NetApp storage array was very hard because
    its diagnostic data is dumped as unstructured text. Contrast that with more modern
    arrays, which dump data in XML or JSON, or even better, have their own SDKs for
    interfacing with hardware and extracting the data we need.
  prefs: []
  type: TYPE_NORMAL
- en: A great deal of effort goes into taking data from various sources and working
    to mold the raw data into something useful. Sometimes it’s easy, and sometimes
    it is very difficult. Poorly formatted data can lead to erroneous conclusions
    and false insights.
  prefs: []
  type: TYPE_NORMAL
- en: A great cautionary tale comes from a large shoe manufacturer. About 20 years
    ago, I worked for a company that sold software designed to manage factory production.
    We consulted with the shoe company and told them exactly how to model their data
    for the best results. They ignored us and went a different way. We told them it
    wouldn’t work. They thanked us for our input. Their projections were galactically
    wrong, so they did what any big company with boards and shareholders would do
    – they blamed the software. Our CEO did the circuit on the business shows, but
    the damage was done. Our company stock tanked and a lot of people lost their jobs
    that year, including me. To this day, I won’t wear their shoes. Bad data can cost
    livelihoods, reputations, and, beyond the context of shoes, even lives. We must
    have tools and processes at our disposal that help us get things right.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go over a few steps of that process.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with a question
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Everything in science starts with a question. For our purposes, we’ll consider
    two possible scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: We have a specific question in mind and we need to collect and analyze appropriate
    data to answer that question
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We already have data, and during exploration, a question has arisen
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our case, we’re going to recreate the data analysis phase of a potentially
    important breakthrough in the field of medical diagnosis. I’ll be presenting an
    example from Kaggle taken from a paper titled *High-accuracy detection of early
    Parkinson’s Disease using multiple characteristics of finger movement while typing,*
    which was conducted by Warwick Adams in 2017\. You’ll find the full study paper
    and the dataset links in the *Further reading* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Kaggle is an online data community designed for data scientists and ML engineers.
    The site provides competitions, datasets, playgrounds, and other educational activities
    to promote the growth of data science, both in academia and the industry. More
    information about the website can be found on its home page: [https://www.kaggle.com/](https://www.kaggle.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parkinson’s Disease** (**PD**) is a condition that affects the brain and
    causes problems with movement. It’s a progressive disease, which means it gets
    worse over time. More than 6 million people around the world have this disease.
    In PD, a specific type of brain cell that produces a chemical called *dopamine*
    starts to die off. This leads to a variety of symptoms, including difficulty with
    movement and other non-movement-related issues.'
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, doctors don’t have a definite test to diagnose PD, especially
    in the early stages when the symptoms might not be very obvious. This results
    in mistakes in diagnosing the disease, with up to 25% of cases being misdiagnosed
    by doctors who aren’t specialists in PD. Some people can have PD for many years
    before they are correctly diagnosed.
  prefs: []
  type: TYPE_NORMAL
- en: This leads us to a question…
  prefs: []
  type: TYPE_NORMAL
- en: How can we effectively and accurately diagnose PD using some test, metric, or
    diagnostic data point without specialized clinical training?
  prefs: []
  type: TYPE_NORMAL
- en: Adams suggested a test that uses computer typing data collected over time. Since
    typing involves fine motor movement, and since this fine motor movement is the
    first thing to go during the early onset of PD, Adams hoped it would be possible
    to use the mundane task of typing as a diagnostic tool. The researchers tested
    this method on 103 people; 32 of them had mild PD and the rest, the control group,
    didn’t have PD. The computer analysis of their typing patterns was able to tell
    the difference between the people with early-stage PD and those without it. This
    method correctly identified PD with 96% accuracy in detecting those who had it,
    and 97% accuracy in correctly identifying those who didn’t. This suggests that
    this method might be good at distinguishing between the two groups. Let’s see
    whether we can draw the same conclusion given their study’s data.
  prefs: []
  type: TYPE_NORMAL
- en: Archived user data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Within this chapter’s source code, you’ll find a data science project called
    `pipeline`. The project contains a data folder containing our datasets in two
    folders: `Archived users` and `Tappy Data`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data within the `Archived users` folder is in text file format and appears
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For the sake of immersion, let’s demystify this a little bit. These are the
    fields we have in each record:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Birth Year: 1952`: This person was born in 1952.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Gender: Female`: This person identifies as female.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Parkinsons: True`: The person has been diagnosed with PD.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Tremors: True`: Tremors, which are involuntary shaking movements, are present
    in this person. Tremors are a common symptom of PD.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DiagnosisYear: 2000`: The person was diagnosed with PD in 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Sided: Left`: The term *sided* in this context likely refers to the side of
    the body where the symptoms are more pronounced. In this case, the symptoms are
    more noticeable on the left-hand side of the body.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`UPDRS: Don''t know`: The **Unified Parkinson’s Disease Rating Scale** (**UPDRS**)
    is a tool that’s used to assess the severity of PD. In this case, it’s not known
    what the specific UPDRS score is for this individual.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Impact: Severe`: The impact of PD on this person’s life is considered severe,
    indicating that the symptoms have a significant effect on their daily activities
    and quality of life.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Levodopa: True`: Levodopa is a common medication used to manage the symptoms
    of PD. This person is taking Levodopa as part of their treatment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DA: True`: **Dopamine agonists** (**DAs**) are another type of medication
    used to manage Parkinson’s symptoms. This person is taking dopamine agonists as
    part of their treatment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MAOB: False`: **Monoamine oxidase B inhibitors** (**MOABs**) are medications
    that can help manage Parkinson’s symptoms by increasing dopamine levels in the
    brain. In this case, the person is not taking MAOBs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Other: False`: If I were recreating this study for real, I would likely contact
    the original researcher if this data point wasn’t explained directly in the publication.
    Since I’m not, I’ll guess that it won’t affect our project. This likely refers
    to other specific medications or treatments for PD, indicating that the person
    is not undergoing any other specialized treatments beyond Levodopa and DAs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, this individual was a 65-year-old woman at the time of the study
    who was diagnosed with PD in 2000\. She experiences tremors, particularly on the
    left-hand side of her body. The impact of the disease on her life is severe. She
    is undergoing treatment with Levodopa and DAs to manage her symptoms, but she
    is not using MAOBs or any other specialized treatments. The specific severity
    of her symptoms, as measured by the UPDRS, is not provided in the given information.
  prefs: []
  type: TYPE_NORMAL
- en: The filenames in the folder are important. It isn’t ethical to publish `User_0EA27ICBLF.txt`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Tappy data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The study methodology uses an application called Tappy, which runs on Windows
    and records each subject’s keypress timing, along with positional data about each
    key. If you remember from our earlier discussion of the user data, the sidedness
    is a factor. The motor cortex is the region of the brain that is responsible for
    planning, controlling, and executing voluntary movements. It’s located in the
    cerebral cortex, which is the outermost layer of the brain.
  prefs: []
  type: TYPE_NORMAL
- en: 'The motor cortex, along with most of the rest of the brain, is divided into
    two hemispheres: the left hemisphere and the right hemisphere. Each hemisphere
    controls the voluntary movements of the opposite side of the body. In other words,
    the left hemisphere of the motor cortex controls movements on the right-hand side
    of the body, and the right hemisphere controls movements on the left-hand side
    of the body. Since this is true, knowing which side of the keyboard the keypress
    data is coming from is potentially of diagnostic importance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s open a Tappy dataset and see what’s inside:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1: I’ve opened the first file in the Tappy data folder and I can
    see it is tab-separated data](img/B19644_14_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.1: I’ve opened the first file in the Tappy data folder and I can
    see it is tab-separated data'
  prefs: []
  type: TYPE_NORMAL
- en: I can see a warning at the top stating the file is large, by code editor standards,
    and that code insight is not available. This is spurious since the data folder
    in a scientific project in PyCharm is excluded from indexing and code insights
    anyway. You can safely ignore the warning.
  prefs: []
  type: TYPE_NORMAL
- en: I can also see that the file is tab-delimited, which will play nicely in a data
    pipeline. It is always encouraging to see your data come to you in an easily parsable
    format. This is effectively structured data that would be suitable for import
    into a spreadsheet or database table. That isn’t necessarily what we will do with
    this data, but if we can do those kinds of imports with a given data file, we
    can pretty much do anything with the data.
  prefs: []
  type: TYPE_NORMAL
- en: As before, the filenames are significant. The first part of the file, delineated
    by an underscore, is the ID of the subject from the `Archived users` folder. We
    will be able to relate each subject’s performance data found in the `Tappy Data`
    folder with their demographical data found in the `Archived` `users` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fields from the Tappy data file are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Patient ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The date of data collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The timestamp of each keystroke
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which hand performed the keystroke (*L* for left and *R* for right)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hold time (time between press and release, in milliseconds)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The transition from the last keystroke
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latency time (time from pressing the previous key, in milliseconds)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flight time (time from releasing the previous key, in milliseconds)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have established that we have raw data in a workable format. Honestly, I’d
    call this a good day. It isn’t completely perfect; we’ll still need to do some
    munging, but it’s a very good starting point.
  prefs: []
  type: TYPE_NORMAL
- en: Jargon alert – munging
  prefs: []
  type: TYPE_NORMAL
- en: '**Munging** is a colloquial term used in computer programming and data processing
    to describe the process of manipulating, cleaning, or transforming data from one
    format into another. It often involves altering the structure or content of data
    to make it more suitable for a particular purpose, such as analysis, storage,
    or presentation. Munging can include activities such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '- **Data cleaning**: Removing errors, inconsistencies, or irrelevant information
    from datasets'
  prefs: []
  type: TYPE_NORMAL
- en: '- **Data transformation**: Changing the format, structure, or representation
    of data to fit a specific requirement'
  prefs: []
  type: TYPE_NORMAL
- en: '- **Data parsing**: Extracting specific pieces of information from a larger
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '- **Data aggregation**: Combining multiple sets of data into a single dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '- **Data filtering**: Selecting or excluding data based on certain criteria'
  prefs: []
  type: TYPE_NORMAL
- en: '- **Data formatting**: Changing the way data is presented or encoded for compatibility
    with a certain system or software'
  prefs: []
  type: TYPE_NORMAL
- en: The term *munging* is informal and comes from a blend of *mangle* and *modify*.
    It’s often used in a context where data needs to be prepared or adjusted for analysis,
    integration, or some other data-related task.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have a good start for our project, but we have a question: can we detect
    early-onset PD using a typing test? We have raw data from a study that implemented
    such a typing test. We’re ready to roll up our sleeves and get into it!'
  prefs: []
  type: TYPE_NORMAL
- en: Data collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’re lucky. I’ve already found our data and included it for your consideration.
    In the real world, we would have needed to have performed the normal step of data
    collection. While there are entire tomes on this topic – most 4-year scientific
    university degree programs focus heavily on this topic – I don’t plan on doing
    a deep dive here. However, I will at least give you an overview should you be
    new to what we’re trying to accomplish.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading from an external source
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the case for our example dataset since I downloaded it from Kaggle.
    When using a dataset downloaded from the internet, we should always make sure
    to check its copyright license. Most of the time, if it is in the public domain,
    we can freely use and distribute it without any worry. The example dataset we
    are using is an instance of this. On the other hand, if the dataset is copyrighted,
    we might still be able to use it by asking for permission from the author/owner
    of the dataset. I have found that, after reaching out to them via email and explaining
    how their datasets will be used in detail, dataset owners are often willing to
    share their data with others.
  prefs: []
  type: TYPE_NORMAL
- en: Manually collecting/web scraping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If the data we want is available online but not formatted in tables or CSV files,
    most of the time, we need to collect it and manually put it in a dataset ourselves.
    At most, we can write a web scraper that can send requests to the websites containing
    the target data and parse the returned HTML text. When you have to collect your
    data this way, it is also important to ensure that you are not doing it illegally.
    For example, it is against the law to have a program scrape data off some websites;
    sometimes, you might need to design the scraper so that only a certain number
    of requests are made at a given point. An example of this was when LinkedIn filed
    a lawsuit against many people who anonymously scraped their data in 2016\. For
    this reason, it is always a good practice to find the terms of use for the data
    you are trying to collect this way.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting data via third parties
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Students and researchers who find that the data they are looking for in their
    study cannot be collected online often rely on third-party services to collect
    that data for them (for example, via crowd-sourcing). Amazon **Mechanical Turk**
    (**MTurk**) is one such service – you can enter any type of question to make a
    survey and MTurk will introduce that survey to its users. Participants receive
    money for taking the survey, which is paid by the owner of the survey. This option
    is, again, specifically applicable when you want a representative dataset that
    is not available online anywhere.
  prefs: []
  type: TYPE_NORMAL
- en: Database exports
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is most likely the case if you are working with data from your company
    or organization. Luckily, PyCharm offers many useful features in terms of working
    with databases and their data sources. This process was discussed in [*Chapter
    11*](B19644_11.xhtml#_idTextAnchor266), and I highly recommend you check it out
    if you haven’t already.
  prefs: []
  type: TYPE_NORMAL
- en: Version control for datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we took a quick little side journey to discuss data collection, I hope
    you’ll indulge me once more while we talk about using data in a version control
    system such as Git. A little earlier, we opened a data file and PyCharm immediately
    complained about the size of the file. By modern standards, an 8 MB file isn’t
    very big. However, consider that most code files, PyCharm’s raison d’être, are
    on average well under 100K in size. If your files are very large, that’s a code
    smell and you should figure out what you’re doing wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we’re presenting PyCharm with a file that is about 8,000% bigger than
    what it is used to. Git is also primarily used to deal with small files coming
    out of an IDE. I’m bringing this up because there is somewhat of a crisis of reproducibility
    in the data science and scientific computing community. This is when one data
    team can extract a specific insight from a dataset but others cannot, even when
    using the same methods. Many instances of this are because the data used across
    these different teams is not compatible with each other. Some might be using the
    same but outdated dataset, while other datasets might have been collected from
    a different source.
  prefs: []
  type: TYPE_NORMAL
- en: Version control for datasets is an important topic to consider. Git normally
    has a hard limit of 100 MB for any file, and I can tell you from experience there
    is an upper limit to the total size of your projects in total on GitHub. The same
    limitations exist in other version control systems. I used to teach game development
    with a tool called Unity 3D, and we were always struggling with these limitations
    since video games typically have very large assets in the projects that aren’t
    necessarily code, but that could benefit from revision control.
  prefs: []
  type: TYPE_NORMAL
- en: Using Git Large File Support
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the problem is endemic, Git (and others) have added the ability to track
    larger assets through **Git Large File Support** (**Git LFS**). When we add a
    file using Git LFS, the system will replace that file with a pointer that simply
    references it. When the file is placed under version control, Git will only have
    a reference to the actual file, which is now stored in an external filesystem,
    possibly on another server. Git LFS allows us to apply version control to large
    files (in this case, datasets) with Git, without actually storing the files in
    Git.
  prefs: []
  type: TYPE_NORMAL
- en: 'This feature is normally installed with modern Git installers. *Figure 14**.2*
    shows me installing Git for Windows, where LFS is part of the default installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2: LFS is installed by default in Windows](img/B19644_14_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.2: LFS is installed by default in Windows'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check your installation, regardless of which OS you use, using the
    command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'My result from running this command in GitBash in Windows 11 is shown in *Figure
    14**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.3: If LFS is installed, it should tell you the version number](img/B19644_14_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.3: If LFS is installed, it should tell you the version number'
  prefs: []
  type: TYPE_NORMAL
- en: 'The only reason I have Windows (besides Ghost Recon and Steam in general) is
    so I can use Microsoft Word to write this book. This wasn’t my idea. I was going
    to write the whole thing in raw LaTeX using vi. Not vim. Not neovim. Original
    gangsta vi, which I naturally would be compiling from source. My editor said no.
    She’s so super polite! If our roles were reversed, who knows what would have been
    said? Anyway, the rest of my real work is done on **Pop_OS**, which is a variant
    of Ubuntu Linux. When I throw the command into that environment, I get a less
    hospitable answer, as shown in *Figure 14**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.4: My installer is not modern enough to have LFS pre-installed](img/B19644_14_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.4: My installer is not modern enough to have LFS pre-installed'
  prefs: []
  type: TYPE_NORMAL
- en: 'I don’t have it! I have to install it using these commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'With that done, I can test again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.5: Success! If you use some other Linux distribution, check your
    package management system for the git-lfs package if your installation lacks it](img/B19644_14_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.5: Success! If you use some other Linux distribution, check your
    package management system for the git-lfs package if your installation lacks it'
  prefs: []
  type: TYPE_NORMAL
- en: The `git-lfs` package specific to your Linux distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Using Git LFS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’re getting a little ahead of ourselves. If you’re going to follow along
    in this little sidetrack exercise, it would be best if you made a new folder somewhere
    outside of this book’s code repository. Let’s assume you’ve something like this
    in your OS’s terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This series of commands will work in any of the popular OSs (Windows, macOS,
    or Linux). If you are using Windows, this series of commands can be run in PowerShell
    and assumes you have the Git client for Windows installed. The installer is available
    at [https://git-scm.com/downloads](https://git-scm.com/downloads).
  prefs: []
  type: TYPE_NORMAL
- en: The first command takes you to your `home` folder. The second creates a new
    folder called `git-lfs-test`. Next, we change the directory to the `git-lfs-test`
    folder we just made and we initialize a new repository. Now, we are ready to set
    up support for Git LFS.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t forget the chapter files are already in a Git repo
  prefs: []
  type: TYPE_NORMAL
- en: If you’re following along with this chapter’s source, don’t forget that the
    files are already in a Git repo. Creating a second repo within the existing repo
    won’t work. If you want to practice, make a completely separate folder outside
    of this book’s repo, and copy the project files into your folder. When you copy,
    you specifically want to avoid copying the `.git` folder into your target.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our project, we’re going to use Git LFS to track files of a given extension,
    specifically text files with the `.txt` extension. Given these files are naturally
    plain text, you could get creative with the extension without affecting how they
    are used, but we’ll stick to just `.txt`. I’ll run this command in my terminal
    window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see my test run in *Figure 14**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.6: Git LFS is now tracking all files with the .txt extension](img/B19644_14_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.6: Git LFS is now tracking all files with the .txt extension'
  prefs: []
  type: TYPE_NORMAL
- en: 'To complete my LFS test, I’ll copy the file we examined earlier, `0EA27ICBLF_1607.txt`,
    from the `Tappy Data` folder into the `git-lfs-test` folder we’re using for the
    experiment. Just to be clear, *Figure 14**.7* shows my folder. We’re not doing
    this within any sub-folder within this book’s code repository since creating a
    repository inside another repository is a big no-no:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.7: I’ve copied 0EA27ICBLF_1607.txt into the git-lfs-test folder](img/B19644_14_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.7: I’ve copied 0EA27ICBLF_1607.txt into the git-lfs-test folder'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s add the newly copied text file to the repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We covered the first two Git commands extensively in [*Chapter 5*](B19644_05.xhtml#_idTextAnchor112).
    The last command will list all files being tracked by LFS in this repository.
    You can see my output in *Figure 14**.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.8: I can see that my text file is being tracked by LFS](img/B19644_14_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.8: I can see that my text file is being tracked by LFS'
  prefs: []
  type: TYPE_NORMAL
- en: 'You now understand how to use Git LFS to track large files. If this were a
    real repository we were interested in keeping, there’s one last thing we’d need
    to do. When we commanded Git to track our text files, a special file called `.gitattributes`
    was created on our behalf. We should add and commit that file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: You’re all set! Let’s move on to our next formal step in the process of data
    analysis, which entails data cleansing and preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Data cleansing and preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As I mentioned earlier, we’ve been pretty lucky. Some of the data my team works
    with can be downright filthy. When we use terms such as “dirty," “filthy,” and
    “cleansing” concerning data, what we’re talking about is addressing the format
    of the data, as well as the fitness of the data for processing. Data is only useful
    if it’s in a format we can work with. Structured data is what we always prefer.
  prefs: []
  type: TYPE_NORMAL
- en: Structured data refers to data that is split into identifiable fields. We’ve
    seen comma-separated and tab-separated text. Other examples of structured data
    include formats such as XML, JSON, Parquet, and HDF5\. The first two, XML and
    JSON, are very common and have the advantage of being text formats. The latter
    two, Parquet and HDF5, are binary files and are specialized for storing larger
    datasets than would be comfortable when working with text. As we’ve seen, most
    tools, including PyCharm, buckle when they try to read very large text files.
    You need tools specialized for working with large files if you want to peruse
    or edit them in place.
  prefs: []
  type: TYPE_NORMAL
- en: 'When I talk about dirty versus clean data, I’m looking for things such as missing
    or invalid field data. Recall our earlier data sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `UPDRS` field is marked as unknown. This isn’t ideal. If the field is included,
    I’d like to see a value there. In this case, there’s no way to backfill it, but
    in a perfect world, that might be a candidate for an exercise in data cleansing.
  prefs: []
  type: TYPE_NORMAL
- en: A toxic data example peripherally involving ninjas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most relatable example I’ve ever encountered with dirty – or in this case,
    toxic – data came from the corporate world rather than from a data science experiment.
    My company was consulting for a large aviation company, which is also a contractor
    for the US Department of Defense. I won’t be naming real names here because I
    am generally averse to government ninjas kicking in my door at 2 A.M., or worse,
    being flagged for a tax audit for what I’ve written here. So, we’ll keep this
    more or less theoretical.
  prefs: []
  type: TYPE_NORMAL
- en: The aviation company did business with lots of vendors, and when you do business
    with vendors at scale, it isn’t uncommon to see discounts applied to whatever
    you might be buying based on volume. If you or I go to Hammers R Us and buy a
    hammer, we might pay $12.95 for a hammer. But if the aviation company buys 5,000
    hammers across many orders in a single quarter, they might get a discount of up
    to 60%. It is the aviation company’s job to track what they buy and from whom
    so that they can cash in on whatever bulk purchasing deals their company has negotiated
    with their suppliers.
  prefs: []
  type: TYPE_NORMAL
- en: When it’s time to run the discount reports, an accounting analyst might query
    a database filled with data entered by hundreds or even thousands of people working
    in the field on behalf of the aviation company. Since these operatives are human,
    their ability to enter clean, standardized data into a poorly designed system
    without any kind of validation is virtually nil. In this case, the software used
    for order entry allowed users to type the name of the company into a text field,
    which was never validated against any sort of approved vendors list.
  prefs: []
  type: TYPE_NORMAL
- en: One guy enters a purchase with the vendor listed as “Hammers R Us.” Another
    enters it as "HRUS" (naturally that’s the stock symbol), and another as "H.R.U.S."
    Someone else misspells it as “Hammers Are Us” and yet another as “Hammers-R-Us.”
    Now, we have five different references to the same company, which dilutes our
    ability to figure out how much of a discount we can ask for. If there are 5 spellings
    and the purchase quantities are even across 5 orders, each order will only be
    for 1,000 hammers and our discount is only 20% instead of 60%. Our toxic data
    problem is costing the company serious money!
  prefs: []
  type: TYPE_NORMAL
- en: The aviation company hired my company to do **data cleansing**. It was our job
    to clean all the data up and standardize all the references to Hammers R Us. The
    project was successful for us because all we had to do was charge the client a
    few dollars less than what they were losing, which was substantial. Then, we helped
    them fix their software to make it impossible to enter toxic data after that.
    It was a win for everyone! I even got a free hammer from Hammers R Us, at least
    in my version of the story that entails me not getting audited or visited by ninjas.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory analysis in PyCharm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While data cleansing in a data science project isn’t usually financially profitable,
    it is a very necessary step. As you begin to examine your data for the first time,
    you will often hear this process referred to as **exploratory data analysis**,
    where we are exploring and analyzing the data at the same time. What we’re doing
    though is taking stock to see what we can do with our data. It would be very difficult
    to perform a tabulation, such as computing sums, means, and standard deviations,
    without first making sure all our necessary data is both there and in a usable
    numerical format. We might also look for outliers. Maybe a hammer order was misentered
    and we have an order for a million hammers that was canceled via a separate transaction.
    These kinds of outliers would likely need to be removed before we begin our analysis
    in earnest.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of our data, a few things are bothering me:'
  prefs: []
  type: TYPE_NORMAL
- en: The study says it examined 103 subjects; however, there are 277 user files in
    the `Archived users` folder. I suspect that not every user has matching collected
    data. We’ll need a way to check that each user in the `Archived users` folder
    has a related dataset in the `Tappy` `Data` folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our raw data is purely textual, which means when we import it into Python by
    reading the files, the data will be expressed as strings. This is not ideal for
    data analysis. I’d like numbers to be converted into number types, dates into
    date types, Booleans into Boolean, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Impact` column should be fully standardized to account for missing values
    in the data. Naturally, this applies to any other column where I can see or suspect
    the data might contain missing values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can convert some of the fields in the user datasets into a binary format
    to make analysis easier. Specific examples include `Parkinsons`, `Tremors`, `Levadopa`,
    `DA`, `MAOB`, and `Other`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use a process called one-hot encoding to more easily process the fields
    labeled `Sided`, `UPDRS`, and `Impact`. I’ll go into detail on one-hot encoding
    once we’re ready to perform this process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is just what I see at first glance. There may be other opportunities for
    cleansing that present themselves once we get underway.
  prefs: []
  type: TYPE_NORMAL
- en: Reading the data from text files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s look at what we need to do with preprocessing our data. If you open the
    `data_clean.py` file, you’ll see our clean-up script, which uses the cell mode
    discussed in [*Chapter 12*](B19644_12.xhtml#_idTextAnchor298). Our first cell
    handles our imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If you’re following along with this chapter’s code, don’t forget to create a
    virtual environment using the `requirements.txt` file. Here we’re importing a
    few old friends. `numpy` and `pandas` are standard analysis libraries. The `os`
    package will be needed for working with the file directories, and the `gc` package
    allows us to control the **garbage collection** (**GC**) process. If you’ve never
    heard of this before, it is because most programming languages, including Python,
    handle GC automatically. One common occurrence of GC happens when a variable,
    which will have memory allocated to store its value, goes out of scope and is
    no longer needed. In the C programming language, you would need to allocate that
    memory yourself before you could use the variable. When you were finished with
    the variable, you’d need to deallocate that memory “by hand.” If you didn’t, you’d
    be using more memory than you needed, and that’s the kind of thing that gets you
    uninvited to the Pi Day pizza party.
  prefs: []
  type: TYPE_NORMAL
- en: Most modern languages handle this allocation and deallocation automatically
    in a process called GC. However, there are times, especially when you are loading
    and manipulating large amounts of data, that it makes sense to take a more active
    role when the garbage gets taken out, which frees up memory for further exploits.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our imports out of the way, let’s read some data with the following cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `os.listdir` method takes our `data/Archived users/` folder and gives us
    an iterable list of files from that folder. This is important because we need
    a list of the IDs for each user, which is contained in the filename.
  prefs: []
  type: TYPE_NORMAL
- en: We create a variable called `user_set_v1`, and we instantiate a set. In Python,
    a `set` is a built-in data type that represents an unordered collection of unique
    elements. This means that a set cannot contain duplicate values, and the order
    in which elements are stored is not guaranteed to be the same as the order in
    which they were added.
  prefs: []
  type: TYPE_NORMAL
- en: 'We fill this `set` with data using a `map` statement, which iterates over our
    list of files in the `Archived users` folder. For each iteration of the map, we
    use a lambda function to extract a portion of each filename in `user_file_list`.
    Specifically, it takes a substring from the 5th to the 15th character of each
    filename. This is intended to extract user IDs from the filenames. Next, we’ll
    need to do roughly the same thing to the `Tappy` `Data` files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have two sets, one from the user files and one from the Tappy data files.
    We need to find the intersection between the sets.
  prefs: []
  type: TYPE_NORMAL
- en: In **set theory**, the term *intersection* refers to an operation that combines
    two sets to create a new set containing only the elements that are common to both
    of the original sets. The intersection of two sets, often denoted by the ∩ symbol,
    represents the overlap or shared elements between the sets.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, if you have two sets, A and B, the intersection of A and B is
    a new set that contains all the elements that are both in set A and set B.
  prefs: []
  type: TYPE_NORMAL
- en: I know all you math geeks out there love your symbols, and I also know that
    your brains are wired to scan for patterns rather than word-for-word reading,
    so I’ll help you out. Symbolically, it is represented as A ∩ B = {x ∣ x ∈ A and
    x ∈ B}.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of programming in Python, the `intersection()` method of `set`
    performs this mathematical operation. Given two sets, it returns a new set containing
    only the elements that exist in both sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s say you have the following two sets:'
  prefs: []
  type: TYPE_NORMAL
- en: Set A = {1,2,3,4}
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set B = {3,4,5,6}
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The intersection of A and B would be A ∩ B = {3, 4} since 3 and 4 are in both
    sets. In our case, it is important to get the intersection because the study text
    stated it examined 103 subjects, yet there are 227 subjects listed in the `Archived
    users` folder. I could make a list, and then go through and visually compare the
    contents of the `Tappy Data` folder to make sure everyone is accounted for, but
    that would be boring, time-consuming, and error-prone. I’ll just have Python do
    it for me:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Don’t you just love Python’s one-liners? Sure, there was some setup (hee hee,
    `intersection` method and we have our new set, which is all funk and no junk!
    Let’s see what we’ve got by printing out the length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'I’m going to run the first two cells using the green arrows indicated in *Figure
    14**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.9: I’m running the first two cells we’ve covered so far using the
    green arrows at the top of each cell](img/B19644_14_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.9: I’m running the first two cells we’ve covered so far using the
    green arrows at the top of each cell'
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of the run is shown in *Figure 14**.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.10: I have a relatively clean list of users after our first steps
    of cleaning the data](img/B19644_14_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.10: I have a relatively clean list of users after our first steps
    of cleaning the data'
  prefs: []
  type: TYPE_NORMAL
- en: We got 217 users with correlated data between the two sets, so we’ve managed
    to eliminate 60 user files we aren’t going to use. The number doesn’t match the
    103 subjects reported in the test, but that’s OK – the day is still young, and
    we might eliminate more later. Even if we don’t, there might be other reasons
    to eliminate properly matched data later on. Our new set can be used to iterate
    over data in either data folder since the filenames in both use the ID as a major
    part of the filename. This will be very useful in the next step in our data preparation
    process.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 14**.10*, I’m clicking on the view link to see my list with the **SciView**
    panel. It isn’t particularly exciting since it’s just a list of IDs, but the ability
    to easily inspect as we work without performing additional prints is very useful.
  prefs: []
  type: TYPE_NORMAL
- en: Getting our data into a pandas DataFrame
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our next cell contains code designed to take our loaded dataset and pull that
    data into a pandas DataFrame. pandas is a library that allows for easy analysis
    of tabular data and even provides a lot of very useful methods for loading data
    directly into a DataFrame, which is a tabular structure within pandas. A DataFrame
    object is a lot like an in-memory spreadsheet without the editor. You can perform
    all kinds of calculations with minimal effort.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine the code from the next cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Don’t forget that `#%%` is a special formatting comment in PyCharm. It isn’t
    part of Python. We covered this back in [*Chapter 13*](B19644_13.xhtml#_idTextAnchor318).
    These characters are used to split cells in our code, which allows us to use one
    script but operate step-wise from one cell to the next. At the end of the day,
    it is still a comment, so we should include some documentation to explain what
    is happening in the cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll create a function that reads the data from the files in the `Archived`
    `users` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The function simply takes a filename as an argument and opens the file. It
    then reads the file line by line. For each line, we’re using the `split` string
    function to split the line into chunks as a list. This allows us to grab only
    the parts we need. As you may recall, a few lines of data for these files look
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The separation between the field name and the data is a colon and a space (`:`
    ). We’re using that as our splitter, so if you split `"BirthYear: 1952".split('':
    '')`, you’ll get back a list: `["BirthYear", "1952"]`. We don’t care about the
    field name right now, we care about the value. To get that, we grab `[1]`, which
    gives us `"1952"`, which is the value, but there is a newline character at the
    end of each line, and that was included in our split. The last thing we do, then,
    before moving on with the next iteration, is clear off the newline character with
    the Python split operator, `[:-1]`, which effectively says “go to the end of the
    string,” as evidenced by the fact that the number is after the colon, “and slice
    off one character from the end,” as denoted by the negative number. Rather than
    using a loop, we’ve used list comprehension, which is an alternative way to iterate
    a list. These are generally more performant than a normal `for` loop. The result
    of the list comprehension is a new list that contains only the data we want.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next few lines are setting us up for filling in a pandas DataFrame. First,
    we get a list of files in the `Archived` `users` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a list of fields. We’ve already set up a function to rip the
    data out of the files without the field name. Ripping the names at the same time
    might add a lot of time since it is the same thing over and over; this is simply
    more efficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we make an empty DataFrame as a starting point using our `columns` list.
    Think of this like making a new spreadsheet, and filling in the first row of your
    sheet with your column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s loop through `user_set`, which we created in the previous cell.
    Remember, this is the list of user IDs that have data in the `Tappy Data` folder.
    Recall that the structure of the filename for this file is the word `User` followed
    by an underscore followed by the user ID and appended with the `.txt` file extension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we make sure that the file is there. It should be since we did our `set`
    operation earlier, but it is a good idea to check. If the file isn’t there, our
    analysis set will crash. This isn’t a big deal for a few hundred files, but it
    can be heartbreaking if you’re going through tens of thousands. Assuming the file
    is there, we read it into a variable called `temp_data` using the function we
    created earlier. Remember, that function returns a list of data values that look
    just like the cells in a row of a spreadsheet. Then, we insert that data into
    the DataFrame using the user ID as the index for the row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Naturally, we want to check, but we don’t want every row – we just want the
    first few to make sure they are formatted as we expect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'When I run this cell, I get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.11: My run of our latest cell shows we have a populated pandas
    DataFrame](img/B19644_14_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.11: My run of our latest cell shows we have a populated pandas DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember, you can view the DataFrame in **SciView** by clicking the **View
    as DataFrame** button indicated by the arrow in *Figure 14**.11*. Mine is shown
    in *Figure 14**.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.12: Viewing the DataFrame I created in the previous step is easy
    and colorful in PyCharm](img/B19644_14_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.12: Viewing the DataFrame I created in the previous step is easy
    and colorful in PyCharm'
  prefs: []
  type: TYPE_NORMAL
- en: I can see from this that I still have work to do. The diagnosis year is messy,
    as are a few of the other fields. Let’s keep chipping away at it.
  prefs: []
  type: TYPE_NORMAL
- en: Data cleansing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we can see our data in a tabular format, there are some ways we can
    improve the format of this data with the express purpose of performing numerical
    analysis across any dimensions we might choose.
  prefs: []
  type: TYPE_NORMAL
- en: Changing numeric data into actual numbers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our next cell contains a few lines of code designed to convert numeric values
    into numeric types. Remember, everything is coming in as text and is treated like
    a string until you tell pandas otherwise. Here’s the code for the cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: An application programmer would be tempted to process the data line by line
    and handle type conversions field by field. The neat thing about pandas is that
    once you have your data in a DataFrame, you can operate on entire rows and columns.
  prefs: []
  type: TYPE_NORMAL
- en: In this code, we’re doing just that. `BirthYear` and `DiagnosisYear` are being
    converted into numbers using the `pd.to_numeric` method. The second argument,
    `errors='coerce'`, will attempt to force a data conversion to a numeric type.
    If this is impossible, such as with a value of “`-------`” (a bunch of dashes),
    which we saw in the `NaN`, or “not a number.” While `NaN` isn’t computationally
    valuable, it does at least standardize all non-numeric values to just this one,
    which will make these rows easier to ignore should we choose.
  prefs: []
  type: TYPE_NORMAL
- en: The mention of `NaN` also indicates it’s time to bake some delicious bread in
    your mom’s tandoori oven. Some authors do Patreon, and I do bread, but it has
    to be your mom’s recipe. That means you have to call her and tell her you love
    her. Do it now, even if she doesn’t have a tandoori oven and can’t bake bread!
    I’ll wait.
  prefs: []
  type: TYPE_NORMAL
- en: 'While you were on the phone, I ran the cell; my result is shown in *Figure
    14**.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.13: The year fields are not actual numbers. Wherever there was
    invalid data, we now see a standardized value of nan](img/B19644_14_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.13: The year fields are not actual numbers. Wherever there was invalid
    data, we now see a standardized value of nan'
  prefs: []
  type: TYPE_NORMAL
- en: Binarizing data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Any place where we can convert data that is essentially binary, we should.
    Within our data, gender is reported with two possible values, male and female,
    representing a possibility of representing it in a binary format. Likewise, many
    of the fields are presented as binaries, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In these cases, we just need to standardize the values as actual binaries,
    which may result in renaming or expanding our list of field names. Let’s look
    at the cell code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we renamed the column in our DataFrame from `Gender`
    to `Female`. The second line changes the value in each row for the newly renamed
    column to the result of an expression comparing the current value versus the word
    `Female`. It either is or isn’t `Female`, so we get back a `True` or `False` value.
    The third line converts the Boolean type into an integer, making it more amenable
    to analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll turn our attention to the previously listed columns and do the
    same conversion. This time, we’re checking for the word “True” in our expression.
    The value is either `True` or it isn’t, which results in a Boolean value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code yields changes to our DataFrame, as shown in *Figure 14**.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.14: We’ve successfully binarized our fields](img/B19644_14_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.14: We’ve successfully binarized our fields'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that our fields are now binary numbers! This is going to make things
    easier later!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s jump into the next cell since the first part of the code is doing some
    more cleanup, similar to what we have done so far. In the first part of the cell,
    we are cleaning up the `Impact` field. We’re standardizing any value that isn’t
    `Mild`, `Medium`, or `Severe` as `None`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Next, while staying in the same cell, we’re going to explore a powerful and
    popular technique that is used by an ML algorithm when preparing data for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For some reason, when I first heard the term **one-hot encoding**, I immediately
    thought of hot dogs and how I would love to encode one with mustard and sweet
    relish on a nice steamed bun, or maybe the NaN y’all are doing to send me. For
    the record, I know that’s not how the bread is spelled, and I don’t care. The
    joke only works if I spell it incorrectly. I don’t know why I’m telling you that,
    but here we are.
  prefs: []
  type: TYPE_NORMAL
- en: 'One-hot coding is a technique that allows you to take data that isn’t inherently
    Boolean, and make it so. When I was in the market for a new Jeep Wrangler, there
    were only a few colors I considered:'
  prefs: []
  type: TYPE_NORMAL
- en: Firecracker Red
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ocean Blue Metallic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mojito!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hellayella
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are more colors than that, but they are all boring variants of black,
    white, or gray. I can’t get an orange Jeep because people will think I went to
    Oklahoma State University, and we can’t have that. I can ignore those colors,
    leaving me with a list that will fit on the page. Now, let’s one-hot encode that
    list:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Color_Firecracker_Red** | **Color_Ocean_Blue_Metallic** | **Color_Mojito**
    | **Color_Hellayella** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: You can easily see how one-hot encoding works – it pivots the fields and then
    makes them binary. If you’re a relational database guru, you have probably just
    lost your lunch. Data scientists do things a little differently. In the one-hot
    encoded representation, each observation gets a “1” in the column corresponding
    to its category and a “0” in all other columns. This encoding ensures that the
    categorical information is preserved in a way that ML algorithms can understand
    and use effectively. For the record, I went with *Hellayella* based on the idea
    that if I got my Jeep stuck somewhere inaccessible, such as the deserts of Big
    Bend National Park, or deep in the Piney Woods region of east Texas, the rescue
    helicopters would easily find my corpse.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding is commonly used for features such as categorical variables,
    which can’t be directly used as numerical inputs in many ML algorithms. It’s an
    important step in data preprocessing to convert such variables into a suitable
    format for training models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go back to our code for the current cell. We’ve explained the first few
    lines, so let’s move on to setting up for one-hot encoding on several fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re going to encode these three columns. One of the columns under consideration
    is the `Impact` column, which we just standardized as a lead-in for this step.
    We’ll perform the one-hot encoding for all three columns here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Within the loop, the code performs the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '`user_df.iloc[:, : user_df.columns.get_loc(column)]`: Selects the columns to
    the left of the current column being processed. This preserves the columns before
    the one being one-hot encoded.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`pd.get_dummies(user_df[column], prefix=str(column))`: Applies one-hot encoding
    to the current column using the `pd.get_dummies()` method. It creates a DataFrame
    with binary columns representing the different categories in the column. The `prefix`
    parameter adds a prefix to the column names to indicate which original column
    they were derived from.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`user_df.iloc[:, user_df.columns.get_loc(column) + 1 :]`: Selects the columns
    to the right of the current column being processed. This preserves the columns
    after the one being one-hot encoded.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When fed into the `pd.concat` method, these steps effectively replace each
    of the three categorical columns with one-hot encoded binary columns while keeping
    the rest of the DataFrame intact. When you run the cell, you should see results
    like mine, as shown in *Figure 14**.15*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.15: I’ve scrolled to the right so that you can see the newly added
    one-hot encoded columns that were added](img/B19644_14_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.15: I’ve scrolled to the right so that you can see the newly added
    one-hot encoded columns that were added'
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding will have added many new columns to the DataFrame, so you might
    need to scroll to the right to see them all.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the second dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With our user data fairly well-cleaned up and sitting in a pandas DataFrame,
    we are now ready to tackle the Tappy data. To keep things relatable, I’m going
    to arbitrarily pick one file from the `Tappy Data` set. Let’s look at the code
    in our next cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'As I said, I picked one arbitrary file to examine. We opened one of these files
    earlier and noted they were all in tab-separated format. pandas has a method that
    will easily read this file directly into a DataFrame. Despite the method being
    called `read_csv`, you get to specify a delimiter, which doesn’t have to be a
    comma. The method will read any kind of delimited file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'For our purposes, we don’t need the `UserKey` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this cell, we create a new DataFrame called `df`. Be sure to pick
    it from the console variables panel shown in *Figure 14**.16*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.16: Our new DataFrame can be viewed by clicking the View as DataFrame
    button](img/B19644_14_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.16: Our new DataFrame can be viewed by clicking the View as DataFrame
    button'
  prefs: []
  type: TYPE_NORMAL
- en: Formatting datetime data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The next cell fixes our `datetime` data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This first line tries to force the values in the `Date` column to be dates.
    If the coercion doesn’t work, we’ll see `NaT` (not a time), which is disappointing
    since there’s no food joke to be made. Next, we’ll do some more coercion on the
    `Hold time`, `Latency time`, and `Flight` `time` fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Any observations lacking time data should be dropped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s print the result for inspection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s run it! My cell run results are shown in *Figure 14**.17*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.17: Our datetime data is now numeric and any observation with missing
    time data, being useless, has been dropped](img/B19644_14_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.17: Our datetime data is now numeric and any observation with missing
    time data, being useless, has been dropped'
  prefs: []
  type: TYPE_NORMAL
- en: Washing hands and fixing direction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The next cell cleans up the hand and direction columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This code uses a logical `OR` to filter out anything that doesn’t have a value
    of `L`, `R`, or `S`. Since it is presented as an `OR`, anything outside the three
    desirable possibilities will return as `false`, and be excluded.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do the same thing with direction, which has more possibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, we’ll print the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Go ahead and run the cell. All rows containing invalid data have been removed.
    This result isn’t as visual as most have been, so I don’t think we need a screenshot
    for this one.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our next cell provides an example of how to summarize our data, which we have
    been working so hard to set up for analysis. We’re ready! Let’s try something
    simple. As usual, the first line of code in the cell just marks the beginning
    of the cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Recall that the data we have been working with so far is typing speed data for
    a specific subject at a given time. A subject (`User`) is simply a single data
    point within our first dataset, and we would like to combine the two datasets
    somehow, so we need a way to aggregate our current data into a single data point.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are working with numerical data (typing time), we can take the average
    (mean) of the time data across different columns as a way to summarize the data
    of a given user. We can achieve this with the `groupby()` function from pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, we should print it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the run is shown in *Figure 14**.18*. The code puts the results
    in a new DataFrame called `direction_group_df`, so be sure you select it as shown
    in the figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.18: Hooray! We have our first calculated insight!](img/B19644_14_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.18: Hooray! We have our first calculated insight!'
  prefs: []
  type: TYPE_NORMAL
- en: This is exciting! We have the mechanics working, but now, we need to concentrate
    on making this work with many data files instead of just one.
  prefs: []
  type: TYPE_NORMAL
- en: Refactoring for scale
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our exploration of the Tappy data has focused on one file to establish in an
    easily verifiable way that our code is working. We’ve determined that it is, so
    now, we should refactor our code so that we can process thousands of files. To
    do this, we should consolidate some of our cells into a function. The code in
    the next cell is long but familiar since it is just all the code we’ve written
    so far combined into one function. If you’re an application developer, and you
    understand the design principle known as the **single responsibility principle**
    (**SRP**), you know this is an antipattern. Remember, though, this isn’t application
    code. Nobody will run this beyond performing the analysis, so the rigors of SOLID
    principles that normally apply to software development are not observed in data
    science work.
  prefs: []
  type: TYPE_NORMAL
- en: Processing the Tappy data with one function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here’s the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we’re reading in the CSV filename passed as an argument to our function.
    We enrich the data with hardcoded field names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We drop the unneeded column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We fix the dates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Always wash your hands by getting rid of invalid values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Do the same with direction data values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re doing our math! This is where the manual GC process comes in. It’s a
    good thing we washed our hands, right? In the following code, we’re doing our
    calculations. The results are being returned as a new DataFrame, so to save memory,
    we’re deleting the old DataFrames as we go. This frees up memory since this kind
    of work is memory intensive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'With our new result, we re-index and then sort:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This line returns the flattened NumPy array, which contains the mean values
    of the grouped data. The `.values.flatten()` method converts the DataFrame into
    a two-dimensional NumPy array and then flattens it into a one-dimensional array
    for ease of use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Processing the users with a function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Within the same cell is a second function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'This line initializes an empty NumPy array named `running_user_data`. This
    array will be used to accumulate data as the function iterates through filenames,
    which is what the following block does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: This loop iterates through the list of filenames. If the provided user ID is
    found in the filename, it calls the `read_tappy()` function (which returns a flattened
    NumPy array of mean values) and appends its contents to the `running_user_data`
    array.
  prefs: []
  type: TYPE_NORMAL
- en: 'After iterating through the filenames and appending the data, the following
    line reshapes the `running_user_data` array into a two-dimensional array, with
    each row containing 27 columns. This flattening of time data allows for further
    analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The last line calculates the mean values along the rows (`axis=0`) of the `running_user_data`
    array using `np.nanmean()`. The `np.nanmean()` function ignores `NaN` values while
    calculating the mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: To summarize, the `process_user` function processes data for a specific user
    by iterating through relevant filenames, aggregating the data using the `read_tappy`
    function, reshaping the data, and calculating the mean values while ignoring `NaN`
    values. The final result is an array of mean values for each column of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Processing all the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This one’s for all the marbles! The following cell processes the data for all
    available users by aggregating and calculating mean values based on the Tappy
    data. First, there’s a little housekeeping. We’re going to ignore any warnings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll make one more trip through the `Tappy` `Data` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll make some column names for the final DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s loop through the user indexes and use our `process_user` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'These next few lines do a little interim cleaning by ensuring any NaN values
    are substituted with zeros, and any negative numeric data is also normalized to
    zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'And then, we print like we’ve never printed before! OK, that’s not true – we’ve
    done this a lot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Saving the processed data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The last code cell likely doesn’t need much explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we concatenate the two DataFrames together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we save it to a CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: This is generally a good practice in a given data pipeline. Saving the processed,
    cleaned version of a dataset can save data engineers a lot of effort if something
    goes wrong along the way. It also offers flexibility, if and when we want to change
    or extend our pipeline further.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ll open the CSV file in PyCharm for one last look before we start doing the
    real analysis work. You can see mine in *Figure 14**.19*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.19: Our hard work has paid off! Our data is ready for analysis](img/B19644_14_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.19: Our hard work has paid off! Our data is ready for analysis'
  prefs: []
  type: TYPE_NORMAL
- en: With that, we are ready to start exploring our dataset and searching for insights.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis and insights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Remember what we said about the importance of having a question in mind when
    starting to work on a data science project? This is especially true during this
    phase, where we explore our dataset and extract insights, which should revolve
    around our initial question – the connection between typing speed and whether
    a patient has PD or not.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this section, we will be working with the `EDA.ipynb` file, located
    in the `notebooks` folder of our current project. In the following subsections,
    we will be looking at the code included in this `notebooks` folder. Go ahead and
    open this Jupyter notebook in your PyCharm editor, or, if you are following our
    discussions and entering your own code, create a new Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Starting the notebook and reading in our processed data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Remember that when you open a Jupyter notebook in Python, you can see the code,
    but Jupyter won’t run unless you click the **Run** button. You can see PyCharm
    ready for this in *Figure 14**.20*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.20: The notebook is open, I’ve clicked in the first cell (In 1),
    and I’ll now click the Run button indicated by the arrow](img/B19644_14_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.20: The notebook is open, I’ve clicked in the first cell (In 1),
    and I’ll now click the Run button indicated by the arrow'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you click the **Run** button, a Jupyter server will start and run the
    first cell in the notebook, which handles our imports and reads in our cleaned
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the last line has us printing the first five lines of our output, you’ll
    see them appear below the code and next to a marker that says **Out 2**, as shown
    in *Figure 14**.21*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.21: The output from the head statement in In 2 is shown in Out
    2 and is horizontally scrollable](img/B19644_14_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.21: The output from the head statement in In 2 is shown in Out 2
    and is horizontally scrollable'
  prefs: []
  type: TYPE_NORMAL
- en: Now that our cleaned data has been loaded up, we can move on to analysis techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Using charts and graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Visualization is normally the end goal for most of my work, so for me, this
    is a natural next step. I’m going to start by creating a bar graph that will show
    me the distribution of the counts of unique values within the data. I think this
    might give us some insight into which factor would affect the dependent variable
    in this study, which is whether a subject has early-onset PD. However, there’s
    still a problem. As shown in *Figure 14**.21*, there are still some holes in the
    data I will need to account for before I begin analysis in earnest.
  prefs: []
  type: TYPE_NORMAL
- en: 'What I’m going to do first is create a bar chart to visualize our missing data.
    The following code cell handles this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code produces the visualization shown in *Figure 14**.22*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.22: The missing data is visualized in the bar chart](img/B19644_14_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.22: The missing data is visualized in the bar chart'
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, our chart is very sparse. There is only a small amount of data that
    is missing, or incomplete. There are some missing values for `BirthYear` and `DiagnosisYear`.
    You can even see one in the preview shown in *Figure 14**.21*. Analyzing missing
    values is important, and we will come back to the process of filling in these
    values later on. But for now, let’s continue with the visualization process.
  prefs: []
  type: TYPE_NORMAL
- en: 'A great feature in Matplotlib is subplots, which allow us to generate multiple
    visualizations side by side. In the following code cell, we are creating multiple
    visualizations with this feature to highlight potential differences between patients
    with and without Parkinson’s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this code cell, a visualization will be generated, as shown in
    *Figure 14**.23*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.23: Four plots drawn together from the previous cell](img/B19644_14_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.23: Four plots drawn together from the previous cell'
  prefs: []
  type: TYPE_NORMAL
- en: The top two visualizations represent the distribution in the year of birth of
    people with (top right) and without (top left) Parkinson’s. We can see that these
    distributions roughly follow the normal bell curve. If you were to encounter a
    distribution that is skewed or in a strange shape, it might be worth digging into
    that data further. Note that we can also apply the same visualization for the
    `DiagnosisYear` column.
  prefs: []
  type: TYPE_NORMAL
- en: In the bottom-left visualization, we have a bar chart representing the count
    of male patients (two bars on the left) and female patients (two bars on the right).
    Patients with Parkinson’s are counted with the orange bars, and patients without
    are counted with the blue bars. In this visualization, we can see that while there
    are more patients with the disease than the ones without, the breakdown across
    the two genders is roughly the same.
  prefs: []
  type: TYPE_NORMAL
- en: The bottom-right visualization, on the other hand, illustrates the breakdown
    between patients with tremors (two bars on the right) and those without tremors
    (two bars on the left). From this visualization, we can see that tremors are significantly
    more common in patients with Parkinson’s, which is quite intuitive and can serve
    as a sanity check for our analyses so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will move on to box plots. Specifically, we will use box plots to
    visualize the distributions of different time data (`Hold time`, `Latency time`,
    and `Flight time`) among patients with and without Parkinson’s. Once again, we
    will use the subplots feature to generate multiple visualizations at the same
    time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code cell, each subplot will visualize data of a specific direction
    type (`LL`, `LR`, `LS`, and so on) and will contain different splits denoting
    patients with and without the disease. You should obtain the visualization shown
    in *Figure 14**.24*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.24: The plots from the previous run cell](img/B19644_14_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.24: The plots from the previous run cell'
  prefs: []
  type: TYPE_NORMAL
- en: What we can gather from this visualization is that, surprisingly, the distribution
    of typing speed among patients without Parkinson’s can span across higher values
    and have more variance than that among patients with Parkinson’s, which might
    contradict the intuition some might have that patients with Parkinson’s take more
    time to press keystrokes.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, bar charts, distribution plots, and box plots are some of the most
    common visualization techniques in data science tasks, mostly because they are
    both simple to understand and powerful enough to highlight important patterns
    in our datasets. In the next and final subsection on the topic of data analysis,
    we will consider more advanced techniques – namely, the correlation matrix between
    attributes and leveraging ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning-based insights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike the previous analysis methods, the methods discussed in this subsection
    and other similar ones are based on more complex mathematical models and ML algorithms.
    Given the scope of this book, we will not be going into the specific theoretical
    details for these models, but it’s still worth seeing some of them in action by
    applying them to our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s consider the feature correlation matrix for our dataset. As the
    name suggests, this model is a matrix (a 2D table) that contains the correlation
    between each pair of numerical attributes (or features) within our dataset. A
    correlation between two features is a real number between -1 and 1, indicating
    the magnitude and direction of the correlation. The higher the value, the more
    correlated the two features are.
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain the feature correlation matrix from a pandas DataFrame, we must call
    the `corr()` method, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'We usually visualize a correlation matrix using a heat map, as implemented
    in the same code cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'This code will produce the visualization shown in *Figure 14**.25*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.25: A heatmap is ideal for visualizing correlation matrices](img/B19644_14_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.25: A heatmap is ideal for visualizing correlation matrices'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will try applying an ML model to our dataset. Contrary to popular belief,
    in many data science projects, we don’t take advantage of ML models for predictive
    tasks, where we train our models to be able to predict future data. Instead, we
    feed our dataset to a specific model so that we can extract more insights from
    that current dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we are using the linear **support vector classifier** (**SVC**) model
    from scikit-learn to analyze the data we have and return the feature importance
    list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Note that before we feed the data we have to the ML model, we need to fill in
    the missing values we have in the two columns we identified earlier – `BirthYear`
    and `DiagnosisYear`. Most ML models cannot handle missing values very well, and
    it is up to the data engineers to choose how these values should be filled.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are using the `coef_` attribute of the model afterward.
  prefs: []
  type: TYPE_NORMAL
- en: 'This attribute contains the feature importance list, which is visualized by
    the last section of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code produces the visualization shown in *Figure 14**.26*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.26: A graph of the feature important list identifies features used
    extensively while training an ML model](img/B19644_14_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.26: A graph of the feature important list identifies features used
    extensively while training an ML model'
  prefs: []
  type: TYPE_NORMAL
- en: From the feature importance list, we can identify any features that were used
    extensively by the ML model while training. A feature with a very high importance
    value could be correlated with the target attribute (whether someone has Parkinson’s
    or not) in some interesting way. For example, we can see that `Tremors` (which
    we know is quite correlated to our target attribute) is the third most important
    feature of our current ML model.
  prefs: []
  type: TYPE_NORMAL
- en: That’s our last discussion point regarding analyzing our dataset. In the last
    section of this chapter, we will have a brief discussion on deciding how to write
    a script for a Python data science project.
  prefs: []
  type: TYPE_NORMAL
- en: Scripts versus notebooks in data science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the preceding data science pipeline, there are two main sections: data cleaning,
    where we remove inconsistent data, fill in missing data, and appropriately encode
    the attributes, and data analysis, where we generate visualizations and insights
    from our cleaned dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: The data cleaning process was implemented by a Python script while the data
    analysis process was done with a Jupyter notebook. In general, deciding whether
    a Python program should be done in a script or a notebook is quite an important,
    yet often overlooked, aspect while working on a data science project.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in the previous chapter, Jupyter notebooks are perfect for iterative
    development processes, where we can transform and manipulate our data as we go.
    A Python script, on the other hand, offers no such dynamism. We need to enter
    all of the code necessary in the script and run it as a complete program.
  prefs: []
  type: TYPE_NORMAL
- en: However, as illustrated in the *Data cleansing and preprocessing* section, PyCharm
    allows us to divide a traditional Python script into separate code cells and inspect
    the data we have as we go using the **SciView** panel. The dynamism offered by
    Jupyter notebooks can also be found within PyCharm.
  prefs: []
  type: TYPE_NORMAL
- en: Now, another core difference between regular Python scripts and Jupyter notebooks
    is the fact that printed output and visualizations are included inside a notebook,
    together with the code cells that generated them. While looking at this from the
    perspective of data scientists, we can see that this feature is considerably useful
    when making reports and presentations.
  prefs: []
  type: TYPE_NORMAL
- en: Say you are tasked with finding actionable insights from a dataset in a company
    project, and you need to present your final findings, as well as how you came
    across them with your team. A Jupyter notebook can effectively serve as the main
    platform for your presentation. Not only will people be able to see which specific
    commands were used to process and manipulate the original data but you will also
    be able to include Markdown texts to further explain any subtle discussion points.
  prefs: []
  type: TYPE_NORMAL
- en: Regular Python scripts can simply be used for low-level tasks where the general
    workflow has already been agreed upon, and you will not need to present it to
    anyone else. In our current example, I chose to clean the dataset using a Python
    script as most of the cleaning and formatting changes we applied to the dataset
    don’t generate any actionable insights that can address our initial question.
    I only used a notebook for data analysis tasks, where there were many visualizations
    and insights worthy of further discussion.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the decision to use either a traditional Python script or a Jupyter
    notebook solely depends on your tasks and purposes. We simply need to remember
    that, for whichever tool we would like to use, PyCharm offers incredible support
    that can streamline our workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we walked through the hands-on process of working on a data
    science pipeline. First, we discussed the importance of having version control
    for not just our code and project-related files but also our datasets; we then
    learned how to use Git LFS to apply version control to large files and datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we looked at various data cleaning and preprocessing techniques that are
    specific to the example dataset. Using the **SciView** panel in PyCharm, we can
    dynamically inspect the current state of our data and variables and see how they
    change after each command.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we considered several techniques to generate visualizations and extract
    insights from our dataset. Using the Jupyter editor in PyCharm, we were able to
    avoid working with a Jupyter server and work on our notebook entirely within PyCharm.
    Having walked through this process, you are now ready to tackle real-life data
    science problems and projects using the same tools and functionalities that we
    have discussed so far.
  prefs: []
  type: TYPE_NORMAL
- en: So, we have finished our discussion on using PyCharm in the context of scientific
    computing and data science. In the next chapter, we will finally consider a topic
    that we have mentioned multiple times through our previous chapters – PyCharm
    plugins.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Answer the following questions to test your knowledge of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What are some of the main ways of collecting datasets for a data science project?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can Git LFS be used with Git? If so, what is the overall process?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which type of attribute can have its missing values filled out with the mean?
    What about the mode?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What problem does one-hot encoding address? What problem can arise from using
    one-hot encoding?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which type of attribute can benefit from bar charts? What about distribution
    plots?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is it important to consider the feature correlation matrix for a dataset?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Aside from predictive tasks, what can we use ML models for (like we did in this
    chapter)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Be sure to check out the companion website for this book at [https://www.pycharm-book.com](https://www.pycharm-book.com).
  prefs: []
  type: TYPE_NORMAL
- en: 'More information can be found in the following articles and reading materials:'
  prefs: []
  type: TYPE_NORMAL
- en: Adams, W. R. (2017). *High-accuracy detection of early Parkinson’s Disease using
    multiple characteristics of finger movement while typing*. PloS one, *12*(11),
    e0188226.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *Tappy Keystroke Data with Parkinson’s Patients* data, uploaded by Patrick
    DeKelly: [https://www.kaggle.com/valkling/tappy-keystroke-data-with-parkinsons-patients](https://www.kaggle.com/valkling/tappy-keystroke-data-with-parkinsons-patients).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Building a Data Pipeline from Scratch*, by Alan Marazzi: [https://medium.com/the-data-experience/building-a-data-pipeline-from-scratch-32b712cfb1db](https://medium.com/the-data-experience/building-a-data-pipeline-from-scratch-32b712cfb1db).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Business Perspective to Designing an Enterprise-Level Data Science Pipeline*,
    by Vikram Reddy: [https://www.datascience.com/blog/designing-an-enterprise-level-data-science-pipeline](https://www.datascience.com/blog/designing-an-enterprise-level-data-science-pipeline).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data Science for Startups: Data Pipelines*, by Ben Weber: [https://towardsdatascience.com/data-science-for-startups-data-pipelines-786f6746a59a](https://towardsdatascience.com/data-science-for-startups-data-pipelines-786f6746a59a).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Documentation for the pandas library: [https://pandas.pydata.org/pandas-docs/stable/](https://pandas.pydata.org/pandas-docs/stable/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 5: Plugins and Conclusion'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part will introduce readers to the concept of PyCharm plugins and walk
    through the process of downloading plugins and adding them to their PyCharm environment.
    It will also go into details regarding the most popular plugins and how they can
    optimize a programmer’s productivity even further. We’ll also gloss over important
    topics discussed in previous chapters of the book and offers a comprehensive view
    on PyCharm’s most popular features.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 15*](B19644_15.xhtml#_idTextAnchor379), *More Possibilities with
    PyCharm Plugins*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 16*](B19644_16.xhtml#_idTextAnchor401), *Future Developments*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
