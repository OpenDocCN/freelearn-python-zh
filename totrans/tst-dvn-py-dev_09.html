<html><head></head><body>
  <div><div><div><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Unit Testing Patterns</h1></div></div></div><p>Throughout this book, we have looked at various patterns and anti-patterns in TDD. In this chapter, you are going to take a look at some additional patterns that we haven't discussed before in this book. In the process of doing so, you will also take a look at some more advanced features provided by the Python <code class="literal">unittest</code> module, such as test loaders, test runners, and skipping tests.</p><div><div><div><div><h1 class="title"><a id="ch09lvl1sec61"/>Pattern – fast tests</h1></div></div></div><p>One of the key goals of TDD is to write tests that execute quickly. We will be running the tests often when doing TDD— possibly even every few minutes. The TDD habit is to run the tests multiple times <a id="id407" class="indexterm"/>when developing code, refactoring, before checkins, and before deployments. If tests run any longer, we will be reluctant to run them often, which defeats the purpose of the tests.</p><p>With that in mind, some techniques for keeping tests fast are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Disable unwanted external services</strong>: Some services are not central to the purpose <a id="id408" class="indexterm"/>of the application and can be disabled. For instance, perhaps we use a service to collect analytics on how users use our application. Our application might be making a call to this service on every action. Such services can be disabled, enabling tests to run faster.</li><li class="listitem" style="list-style-type: disc"><strong>Mock out external services</strong>: Other external services such as servers, databases, caches, and so on might be central to the functioning of the application. External services take time to start up, shut down, and communicate with. We want to mock these out and have our tests run against the mock.</li><li class="listitem" style="list-style-type: disc"><strong>Use fast variants of services</strong>: If we must use a service, then make sure it is fast. For example, replace a database with an in-memory database, which is much faster and takes little time to start and shut down. Similarly, we can replace a call to an e-mail server with a fake in-memory e-mail server that just records the e-mails to be sent, without actually sending the e-mail.</li><li class="listitem" style="list-style-type: disc"><strong>Externalize configuration</strong>: What does configuration have to do with unit testing? Simple: if we need to enable or disable services, or replace services with fake<a id="id409" class="indexterm"/> services, then we need to have different configurations for the regular application and for when running unit tests. This requires us to design the application in a way that allows us to easily switch between multiple configurations.</li><li class="listitem" style="list-style-type: disc"><strong>Run tests for the current module only</strong>: Both the <code class="literal">unittest</code> test runner and third-party<a id="id410" class="indexterm"/> runners allow us to run a subset of tests—tests for a specific module, class, or even a single test. This is a great feature for large tests suites with thousands of tests, as it allows us to run just the tests for the module we are working on.</li></ul></div></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec62"/>Pattern – running a subset of tests</h1></div></div></div><p>We already<a id="id411" class="indexterm"/> saw a simple way of running a subset of tests by simply specifying the module or test class on the command line, as shown in the following:</p><div><pre class="programlisting"><strong>python -m unittest stock_alerter.tests.test_stock</strong>
<strong>python -m unittest stock_alerter.tests.test_stock.StockTest</strong>
</pre></div><p>This works<a id="id412" class="indexterm"/> for the common case of when we want to run a subset based on the module. What if we want to run tests based on some other parameter? Maybe we want to run a set of basic smoke tests, or we want to run only integration tests, or we want to skip tests when running on a specific platform or Python version.</p><p>The <code class="literal">unittest</code> module <a id="id413" class="indexterm"/>allows us to create test suites. A <strong>test suite</strong> is a collection of test classes that are run. By default, <code class="literal">unittest</code> performs an autodiscovery for tests and internally creates a test suite with all the tests that match the discovery pattern. However, we can also manually create different test suites and run them.</p><p>Test suites are created using the <code class="literal">unittest.TestSuite</code> class. The <code class="literal">TestSuite</code> class has the following two methods of interest:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">addTest</code>: This method takes a <code class="literal">TestCase</code> or another <code class="literal">TestSuite</code> and adds it to the suite</li><li class="listitem" style="list-style-type: disc"><code class="literal">addTests</code>: Similar to <code class="literal">addTest</code>, this method takes a list of <code class="literal">TestCase</code> or <code class="literal">TestSuite</code> and adds it to the suite</li></ul></div><p>So, how do we use this function?</p><p>First, we write a function that makes the suite and returns it, as shown in the following:</p><div><pre class="programlisting">def suite():
    test_suite = unittest.TestSuite()
    test_suite.addTest(StockTest("test_stock_update"))
    return test_suite</pre></div><p>We can choose the specific tests that we want in the suite. We've added a single test to the suite over here.</p><p>Next, we<a id="id414" class="indexterm"/> need to write a script to run this suite, as shown in the following:</p><div><pre class="programlisting">import unittest

from stock_alerter.tests import test_stock

if __name__ == "__main__":
    runner = unittest.TextTestRunner()
    runner.run(test_stock.suite())</pre></div><p>Here, we create a <code class="literal">TextTestRunner</code> that will run the tests and pass it the suite or tests. <code class="literal">unittest.TextTestRunner</code> is a test runner that accepts a suite of tests and runs the suite, showing the results of the test, run on the console.</p><div><div><h3 class="title"><a id="note25"/>Note</h3><p>
<code class="literal">unittest.TextTestRunner</code> is the default test runner that we have been using so far. It is possible to write our own test runners. For example, we might write a custom test runner to implement a GUI interface, or one that writes test output into an XML file.</p></div></div><p>When we run this script, we get the following output:</p><div><pre class="programlisting"><strong>.</strong>
<strong>------------------------</strong>
<strong>Ran 1 test in 0.000s</strong>

<strong>OK</strong>
</pre></div><p>Similarly, we can create different suites for different subsets of tests—for example, a separate suite containing just integration tests—and run only specific suites as per our needs.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec67"/>Test loaders</h2></div></div></div><p>One of the <a id="id415" class="indexterm"/>problems with the suite function is that we have to add <a id="id416" class="indexterm"/>each test individually into the suite. This is a cumbersome process if we have a lot of tests. Fortunately, we can simplify the process by using a <code class="literal">unittest.TestLoader</code> object to load a bunch of tests for us, as shown in the following:</p><div><pre class="programlisting">def suite():
    loader = unittest.TestLoader()
    test_suite = unittest.TestSuite()
    test_suite.addTest(StockTest("test_stock_update"))
    test_suite.addTest(
        loader.loadTestsFromTestCase(StockCrossOverSignalTest))
    return test_suite</pre></div><p>Here, the <a id="id417" class="indexterm"/>loader extracts all the tests from the <code class="literal">StockCrossOverSignalTest</code> class and creates a suite out of it. We can return the suite directly if <a id="id418" class="indexterm"/>that is all we want, or we can create a new suite with additional tests. In the example above, we create a suite containing a single test from the <code class="literal">StockTest</code> class and all the tests from the <code class="literal">StockCrossOverSignalTest</code> class.</p><p>
<code class="literal">unittest.TestLoader</code> also contains some other methods for loading tests:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">loadTestsFromModule</code>: This method takes a module and returns a suite of all the tests in that module.</li><li class="listitem" style="list-style-type: disc"><code class="literal">loadTestsFromName</code>: This method takes a string reference to a module, class, or function and extracts the tests from there. If it is a function, the function is called and the test suite returned by the function is returned. The string reference is in dotted format, meaning we can pass in something like <code class="literal">stock_alerter.tests.test_stock</code> or <code class="literal">stock_alerter.tests.test_stock.StockTest</code>, or even <code class="literal">stock_alerter.tests.test_stock.suite</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">discover</code>: This method executes the default autodiscovery process and returns the collected tests as a suite. The method takes three parameters: the start directory, the pattern to find <code class="literal">test</code> module (default <code class="literal">test*.py</code>), and the top-level directory.</li></ul></div><p>Using these methods, we can create test suites with just the tests that we want. We can create different suites for different purposes and execute them from the test script.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec68"/>Using the load_tests protocol</h2></div></div></div><p>A simpler <a id="id419" class="indexterm"/>way to create test suites is with the <code class="literal">load_tests</code> function. As we saw in <a class="link" href="ch07.html" title="Chapter 7. Executable Documentation with doctest">Chapter 7</a>, <em>Executable Documentation with doctest</em>, the <code class="literal">unittest</code> framework calls the <code class="literal">load_tests</code> function if it is present in the test module. The<a id="id420" class="indexterm"/> function should return a <code class="literal">TestSuite</code> object containing the tests to be run. <code class="literal">load_tests</code> is a better solution when we want to just slightly modify the default autodiscovery process.</p><p>
<code class="literal">load_tests</code> passes three parameters: the loader being used to load the tests, a suite of tests that are going to be loaded by default, and the test pattern that has been specified for the search.</p><p>Suppose we do not want to run the <code class="literal">StockCrossOverSignalTest</code> tests if the current platform is Windows. We can write a <code class="literal">load_tests</code> function like the following:</p><div><pre class="programlisting">def load_tests(loader, tests, pattern):
    suite = unittest.TestSuite()
    suite.addTest(loader.loadTestsFromTestCase(StockTest))
    if not sys.platform.startswith("win"):
        suite.addTest(
            loader.loadTestsFromTestCase(StockCrossOverSignalTest))
    return suite</pre></div><p>Now the <code class="literal">StockCrossOverSignalTest</code> tests will be run only on non-windows platforms. When <a id="id421" class="indexterm"/>using the <code class="literal">load_tests</code> method, we<a id="id422" class="indexterm"/> don't need to write a separate script to run tests or create a test runner. It hooks into the autodiscovery process and is therefore much simpler to use.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec69"/>Skipping tests</h2></div></div></div><p>In the <a id="id423" class="indexterm"/>previous section, we used the <code class="literal">load_tests</code> mechanism to<a id="id424" class="indexterm"/> skip some tests if the platform was Windows. The <code class="literal">unittest</code> module gives a simpler way to do the same using the <code class="literal">skip</code> decorator. Simply decorate a class or method with the decorator and the test will be skipped, as shown in the following:</p><div><pre class="programlisting">@unittest.skip("skip this test for now")
def test_stock_update(self):
    self.goog.update(datetime(2014, 2, 12), price=10)
    assert_that(self.goog.price, equal_to(10))</pre></div><p>The decorator takes a parameter where we specify the reason that the test is being skipped. When we run all the tests, we get an output like the following:</p><div><pre class="programlisting"><strong>........................................................s..</strong>
<strong>-------------------------------------------------------------</strong>
<strong>Ran 59 tests in 0.094s</strong>

<strong>OK (skipped=1)</strong>
</pre></div><p>And when the tests are run in verbose mode, we get an output like the following:</p><div><pre class="programlisting"><strong>test_stock_update (stock_alerter.tests.test_stock.StockTest) ... skipped 'skip this test for now'</strong>
</pre></div><p>The <code class="literal">skip</code> decorator skips a test unconditionally, but <code class="literal">unittest</code> provides two more decorators, <code class="literal">skipIf</code> and <code class="literal">skipUnless</code>, which allow us to specify a condition to skip the tests. These decorators take a <code class="literal">Boolean</code> value as the first parameter and a message as the second parameter. <code class="literal">skipIf</code> will skip the test if the <code class="literal">Boolean</code> is <code class="literal">True</code>, while <code class="literal">skipUnless</code> will skip the test if the <code class="literal">Boolean</code> is <code class="literal">False</code>.</p><p>The following<a id="id425" class="indexterm"/> test will run on all platforms except windows:</p><div><pre class="programlisting">@unittest.skipIf(sys.platform.startswith("win"), "skip on windows")
def test_stock_price_should_give_the_latest_price(self):
    self.goog.update(datetime(2014, 2, 12), price=10)
    self.goog.update(datetime(2014, 2, 13), price=8.4)
    self.assertAlmostEqual(8.4, self.goog.price, delta=0.0001)</pre></div><p>While<a id="id426" class="indexterm"/> the following test will run only on windows:</p><div><pre class="programlisting">@unittest.skipUnless(sys.platform.startswith("win"), "only run on windows")
def test_price_is_the_latest_even_if_updates_are_made_out_of_order(self):
    self.goog.update(datetime(2014, 2, 13), price=8)
    self.goog.update(datetime(2014, 2, 12), price=10)
    self.assertEqual(8, self.goog.price)</pre></div><p>The <code class="literal">skip</code>, <code class="literal">skipIf</code>, and <code class="literal">skipUnless</code> decorators can be used on test methods as well as test classes. When applied to classes, all the tests in the class are skipped.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec63"/>Pattern – using attributes</h1></div></div></div><p>The <code class="literal">nose2</code> test<a id="id427" class="indexterm"/> runner has a useful <code class="literal">attrib</code> plugin that allows us to set attributes on test cases and select tests that match particular attributes.</p><p>For example, the following test has three attributes set:</p><div><pre class="programlisting">def test_stock_update(self):
    self.goog.update(datetime(2014, 2, 12), price=10)
    self.assertEqual(10, self.goog.price)

test_stock_update.slow = True
test_stock_update.integration = True
test_stock_update.python = ["2.6", "3.4"]</pre></div><p>When nose2 is run via the following command, then the plugin is enabled, and only the tests that have the integration attribute set to <code class="literal">True</code> are executed:</p><div><pre class="programlisting"><strong>nose2 --plugin=nose2.plugins.attrib -A "integration"</strong>
</pre></div><p>The plugin can also run all tests that have a specific value in a list. Take the following command:</p><div><pre class="programlisting"><strong>nose2 --plugin=nose2.plugins.attrib -A "python=2.6"</strong>
</pre></div><p>The preceding command will run all tests that have the <code class="literal">python</code> attribute set to <code class="literal">2.6</code> or containing the <a id="id428" class="indexterm"/>value <code class="literal">2.6</code> in a list. It will select and run the <code class="literal">test_stock_update</code> test, shown previously.</p><p>The plugin can also run all tests that <em>do not</em> have an attribute set. Take the following command:</p><div><pre class="programlisting"><strong>nose2 --plugin=nose2.plugins.attrib -A "!slow"</strong>
</pre></div><p>The preceding command will run all tests that are not marked as slow.</p><p>The plugin can also take complex conditions, so we can give the following command:</p><div><pre class="programlisting"><strong>nose2 --plugin=nose2.plugins.attrib -E "integration and '2.6' in python"</strong>
</pre></div><p>This test runs all the tests that have the <code class="literal">integration</code> attribute, as well as <code class="literal">2.6</code> in the <code class="literal">python</code> attribute list. Note that we used the <code class="literal">-E</code> switch to specify that we are giving a <code class="literal">python</code> condition expression.</p><p>The attribute plugin is a great way to run specific subsets of tests without having to manually make test suites out of each and every combination that we might want to run.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec70"/>Attributes with vanilla unittests</h2></div></div></div><p>The <code class="literal">attrib</code> plugin requires nose2 to work. What if we are using the regular <code class="literal">unittest</code> module? The<a id="id429" class="indexterm"/> design of the <code class="literal">unittest</code> module <a id="id430" class="indexterm"/>allows us to easily write a simplified version in just a few lines of code, as shown in the following:</p><div><pre class="programlisting">import unittest

class AttribLoader(unittest.TestLoader):
    def __init__(self, attrib):
        self.attrib = attrib

    def loadTestsFromModule(self, module, use_load_tests=False):
        return super().loadTestsFromModule(module, use_load_tests=False)

    def getTestCaseNames(self, testCaseClass):
        test_names = super().getTestCaseNames(testCaseClass)
        filtered_test_names = [test
                               for test in test_names
                               if hasattr(getattr(testCaseClass, test), self.attrib)]
        return filtered_test_names

if __name__ == "__main__":
    loader = AttribLoader("slow")
    test_suite = loader.discover(".")
    runner = unittest.TextTestRunner()
    runner.run(test_suite)</pre></div><p>This little <a id="id431" class="indexterm"/>piece of code will only run those tests that have the <code class="literal">integration</code> attribute set on the test function. Let us look a little deeper <a id="id432" class="indexterm"/>into the code.</p><p>First, we subclass the default <code class="literal">unittest.TestLoader</code> class and create our own loader called <code class="literal">AttribLoader</code>. Remember, the <strong>loader</strong> is the class responsible for loading the tests from a class or module.</p><p>Next, we override the <code class="literal">getTestCaseNames</code> method. This method returns a list of test case names from a class. Here, we call the parent method to get the default list of tests, and then select those test function that have the required attribute. This filtered list is returned, and it is only these tests that will be executed.</p><p>So why have we overridden the <code class="literal">loadTestsFromModule</code> method as well? Well, simple: the default behavior for loading tests is to match by the <code class="literal">test</code> prefix on the method, but if the <code class="literal">load_tests</code> function is present, then everything is delegated to the <code class="literal">load_tests</code> function. Therefore, all modules that have the <code class="literal">load_tests</code> function defined will take priority over our attribute filtering scheme.</p><p>When using our loader, we call the default implementation, but set the <code class="literal">use_load_tests</code> parameter to <code class="literal">False</code>. This means that none of the <code class="literal">load_tests</code> functions will be executed, and the tests to be loaded will be determined only by the filtered list that we return. If we would like to give priority to <code class="literal">load_tests</code> (as is the default behavior), then we just need to remove this method from <code class="literal">AttribLoader</code>.</p><p>Okay, now that the loader is ready, we then modify our test running script to use this loader, instead of the default loader. We get the loaded test suite by calling the <code class="literal">discover</code> method, which, in turn, calls our overridden <code class="literal">getTestCaseNames</code>. We pass this suite to the runner and run the tests.</p><p>The loader can be easily modified to support selecting tests that <em>don't</em> have a given attribute or to support more complex conditionals. We can then add support to the script to accept the attribute on the command line and pass it on to the loader.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec64"/>Pattern – expected failures</h1></div></div></div><p>Sometimes, we have tests that are failing, but, for whatever reason, we don't want to fix it yet. It could be<a id="id433" class="indexterm"/> that we found a bug and wrote a failing test that demonstrates the bug (a very good practice), but we have decided to fix the bug later. Now, the whole test suite is failing.</p><p>On one hand, we don't want the suite to fail because we know this bug and want to fix it later. On the other hand, we don't want to remove the test from the suite because it reminds us that we need to fix the bug. What do we do?</p><p>Python's <code class="literal">unittest</code> module provides a solution: marking tests as expected failures. We can do this <a id="id434" class="indexterm"/>by applying the <code class="literal">unittest.expectedFailure</code> decorator to the test. The following is an example of it in action:</p><div><pre class="programlisting">class AlertTest(unittest.TestCase):
    @unittest.expectedFailure
    def test_action_is_executed_when_rule_matches(self):
        goog = mock.MagicMock(spec=Stock)
        goog.updated = Event()
        goog.update.side_effect = \
            lambda date, value: goog.updated.fire(self)
        exchange = {"GOOG": goog}
        rule = mock.MagicMock(spec=PriceRule)
        rule.matches.return_value = True
        rule.depends_on.return_value = {"GOOG"}
        action = mock.MagicMock()
        alert = Alert("sample alert", rule, action)
        alert.connect(exchange)
        exchange["GOOG"].update(datetime(2014, 2, 10), 11)
        action.execute.assert_called_with("sample alerts")</pre></div><p>We get the following output when the tests are executed:</p><div><pre class="programlisting"><strong>......x....................................................</strong>
<strong>------------------------------------------------------------</strong>
<strong>Ran 59 tests in 0.188s</strong>

<strong>OK (expected failures=1)</strong>
</pre></div><p>And the following is the verbose output:</p><div><pre class="programlisting"><strong>test_action_is_executed_when_rule_matches (stock_alerter.tests.test_alert.AlertTest) ... expected failure</strong>
</pre></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec65"/>Pattern – data-driven tests</h1></div></div></div><p>We briefly <a id="id435" class="indexterm"/>explored data-driven tests earlier. Data-driven tests reduce the amount of boilerplate test code by allowing us to write a single test execution flow and run it with different combinations of data.</p><p>The following is an example using the nose2 parameterization plugin that we looked at earlier in this book:</p><div><pre class="programlisting">from nose2.tools.params import params

def given_a_series_of_prices(stock, prices):
    timestamps = [datetime(2014, 2, 10), datetime(2014, 2, 11),
                  datetime(2014, 2, 12), datetime(2014, 2, 13)]
    for timestamp, price in zip(timestamps, prices):
        stock.update(timestamp, price)

@params(
    ([8, 10, 12], True),
    ([8, 12, 10], False),
    ([8, 10, 10], False)
)
def test_stock_trends(prices, expected_output):
    goog = Stock("GOOG")
    given_a_series_of_prices(goog, prices)
    assert goog.is_increasing_trend() == expected_output</pre></div><p>Running<a id="id436" class="indexterm"/> tests like this requires the use of nose2. Is there a way to do something similar using the regular <code class="literal">unittest</code> module? For a long time there was no way to do this without resorting to metaclasses, but a new feature added with Python 3.4 has made this possible.</p><p>This new feature is the <code class="literal">unittest.subTest</code> context manager. All the code within the context manager block will be treated as a separate test, and any failures are reported independently. The following is an example:</p><div><pre class="programlisting">class StockTrendTest(unittest.TestCase):
    def given_a_series_of_prices(self, stock, prices):
        timestamps = [datetime(2014, 2, 10), datetime(2014, 2, 11),
                      datetime(2014, 2, 12), datetime(2014, 2, 13)]
        for timestamp, price in zip(timestamps, prices):
            stock.update(timestamp, price)

    def test_stock_trends(self):
        dataset = [
            ([8, 10, 12], True),
            ([8, 12, 10], False),
            ([8, 10, 10], False)
        ]
        for data in dataset:
            prices, output = data
            with self.subTest(prices=prices, output=output):
                goog = Stock("GOOG")
                self.given_a_series_of_prices(goog, prices)
                self.assertEqual(output, goog.is_increasing_trend())</pre></div><p>In this example, the test loops through the different scenarios and asserts each one. The whole Arrange-Act-Assert pattern occurs inside the <code class="literal">subTest</code> context manager. The context <a id="id437" class="indexterm"/>manager takes any keyword arguments as parameters and these are used in displaying error messages.</p><p>When we run the test, we get an output like the following:</p><div><pre class="programlisting"><strong>.</strong>
<strong>------------------------</strong>
<strong>Ran 1 test in 0.000s</strong>

<strong>OK</strong>
</pre></div><p>As we can see, the whole test is considered a single test and it shows that the test passed.</p><p>Suppose we change the test to make it fail in two of the three cases, as shown in the following:</p><div><pre class="programlisting">class StockTrendTest(unittest.TestCase):
    def given_a_series_of_prices(self, stock, prices):
        timestamps = [datetime(2014, 2, 10), datetime(2014, 2, 11),
                      datetime(2014, 2, 12), datetime(2014, 2, 13)]
        for timestamp, price in zip(timestamps, prices):
            stock.update(timestamp, price)

    def test_stock_trends(self):
        dataset = [
            ([8, 10, 12], True),
            ([8, 12, 10], True),
            ([8, 10, 10], True)
        ]
        for data in dataset:
            prices, output = data
            with self.subTest(prices=prices, output=output):
                goog = Stock("GOOG")
                self.given_a_series_of_prices(goog, prices)
                self.assertEqual(output, goog.is_increasing_trend())</pre></div><p>Then, the output becomes as follows:</p><div><pre class="programlisting"><strong>======================================================================</strong>
<strong>FAIL: test_stock_trends (stock_alerter.tests.test_stock.StockTrendTest) (output=True, prices=[8, 12, 10])</strong>
<strong>----------------------------------------------------------------------</strong>
<strong>Traceback (most recent call last):</strong>
<strong>  File "c:\Projects\tdd_with_python\src\stock_alerter\tests\test_stock.py", line 78, in test_stock_trends</strong>
<strong>    self.assertEqual(output, goog.is_increasing_trend())</strong>
<strong>AssertionError: True != False</strong>

<strong>======================================================================</strong>
<strong>FAIL: test_stock_trends (stock_alerter.tests.test_stock.StockTrendTest) (output=True, prices=[8, 10, 10])</strong>
<strong>----------------------------------------------------------------------</strong>
<strong>Traceback (most recent call last):</strong>
<strong>  File "c:\Projects\tdd_with_python\src\stock_alerter\tests\test_stock.py", line 78, in test_stock_trends</strong>
<strong>    self.assertEqual(output, goog.is_increasing_trend())</strong>
<strong>AssertionError: True != False</strong>

<strong>----------------------------------------------------------------------</strong>
<strong>Ran 1 test in 0.000s</strong>

<strong>FAILED (failures=2)</strong>
</pre></div><p>As we can <a id="id438" class="indexterm"/>see in the preceding output, it shows a single test was run, but each failure is reported individually. In addition, the values that were used when the test failed are appended to the end of the test name, making it easy to see exactly which condition failed. The values displayed here are the parameters that were passed into the <code class="literal">subTest</code> context manager.</p></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec66"/>Pattern – integration and system tests</h1></div></div></div><p>Throughout <a id="id439" class="indexterm"/>this book, we've stressed the fact that unit tests are not integration tests. They have a different purpose to validating that the system works when<a id="id440" class="indexterm"/> integrated. Having said that, integration tests are also important and shouldn't be ignored. Integration tests can be written using the same <code class="literal">unittest</code> framework that we use for writing unit tests. The key points to keep in mind when writing<a id="id441" class="indexterm"/> integration tests are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Still disable non-core services</strong>: Keep non-core services such as analytics or logging disabled. These do not affect the functionality of the application.</li><li class="listitem" style="list-style-type: disc"><strong>Enable all core services</strong>: Every other service should be live. We don't want to mock or fake these because this defeats the whole purpose of an integration test.</li><li class="listitem" style="list-style-type: disc"><strong>Use attributes to tag integration tests</strong>: By doing this, we can easily select only the unit tests to run during development, while enabling integration tests to be run during continuous integration or before deployment.</li><li class="listitem" style="list-style-type: disc"><strong>Try to minimize setup and teardown time</strong>: For example, don't start and stop a server for each and every test. Instead, use module or package level fixtures to start and stop a service once for the entire set of tests. When doing this, we<a id="id442" class="indexterm"/> have to be careful that our tests do <a id="id443" class="indexterm"/>not mess up the state of the service in-between tests. In particular, a failing test or a test error should not leave the service in an inconsistent state.</li></ul></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec67"/>Pattern – spies</h1></div></div></div><p>Mocks allow us to replace an object or class with a dummy mock object. We've seen how we can then make the mock return predefined values, so that the class under test doesn't even know that it <a id="id444" class="indexterm"/>has made a call to a mock object. However, sometimes we might want to just record that the call was made to an object, but allow the execution flow to continue to the real object and return. Such an object is known as a <strong>spy</strong>. A spy retains the functionality of recording calls and being able to assert on the calls afterwards, but it does not replace a real object like a regular mock does.</p><p>The <code class="literal">wraps</code> parameter when creating a <code class="literal">mock.Mock</code> object allows us to create spy behavior in our code. It takes an object as a value, and all calls to the mock are forwarded to the object we pass, and the return value is sent back to the caller. The following is an example:</p><div><pre class="programlisting">def test_action_doesnt_fire_if_rule_doesnt_match(self):
    goog = Stock("GOOG")
    exchange = {"GOOG": goog}
    rule = PriceRule("GOOG", lambda stock: stock.price &gt; 10)
    rule_spy = mock.MagicMock(wraps=rule)
    action = mock.MagicMock()
    alert = Alert("sample alert", rule_spy, action)
    alert.connect(exchange)
    alert.check_rule(goog)
    rule_spy.matches.assert_called_with(exchange)
    self.assertFalse(action.execute.called)</pre></div><p>In the above example, we are creating a spy for the <code class="literal">rule</code> object. The spy is nothing but a regular mock object that wraps the real object, as specified in the <code class="literal">wraps</code> parameter. We then pass the spy to the alert. When <code class="literal">alert.check_rule</code> is executed, the method called the <code class="literal">matches</code> method on the spy. The spy records the call details, and then forwards the call to the real rule object and returns the value from the real object. We can then assert on the spy to validate the call.</p><p>Spies are typically used when we would like to avoid over-mocking and use a real object, but we also would like to assert on specific calls. They are also used when it is difficult to calculate mock return <a id="id445" class="indexterm"/>values by hand, and it is better to just do the real calculation and return the value.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec71"/>Pattern – asserting a sequence of calls</h2></div></div></div><p>Sometimes, we <a id="id446" class="indexterm"/>want to assert that a particular sequence of <a id="id447" class="indexterm"/>calls occurred across multiple objects. Consider the following test case:</p><div><pre class="programlisting">def test_action_fires_when_rule_matches(self):
    goog = Stock("GOOG")
    exchange = {"GOOG": goog}
    rule = mock.MagicMock()
    rule.matches.return_value = True
    rule.depends_on.return_value = {"GOOG"}
    action = mock.MagicMock()
    alert = Alert("sample alert", rule, action)
    alert.connect(exchange)
    goog.update(datetime(2014, 5, 14), 11)
    rule.matches.assert_called_with(exchange)
    self.assertTrue(action.execute.called)</pre></div><p>In this test, we are asserting that a call was made to the <code class="literal">rule.matches</code> method as well as a call being made to the <code class="literal">action.execute</code> method. The way we have written the assertions does not check the order of these two calls. This test will still pass even if the <code class="literal">matches</code> method is called after the <code class="literal">execute</code> method. What if we want to specifically check that the call to the <code class="literal">matches</code> method happened before the call to the <code class="literal">execute</code> method?</p><p>Before answering this question, let us take a look at this interactive Python session. First, we create a mock object, as follows:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; from unittest import mock</strong>
<strong>&gt;&gt;&gt; obj = mock.Mock()</strong>
</pre></div><p>Then, we get two child objects that are attributes of the mock, as follows:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; child_obj1 = obj.child1</strong>
<strong>&gt;&gt;&gt; child_obj2 = obj.child2</strong>
</pre></div><p>Mock objects by default return new mocks whenever an attribute is accessed that doesn't have a <code class="literal">return_value</code> configured. So <code class="literal">child_obj1</code> and <code class="literal">child_obj2</code> will also be mock objects.</p><p>Next, we call some methods on our mock objects, as follows:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; child_obj1.method1()</strong>
<strong>&lt;Mock name='mock.child1.method1()' id='56161448'&gt;</strong>
<strong>&gt;&gt;&gt; child_obj2.method1()</strong>
<strong>&lt;Mock name='mock.child2.method1()' id='56161672'&gt;</strong>
<strong>&gt;&gt;&gt; child_obj2.method2()</strong>
<strong>&lt;Mock name='mock.child2.method2()' id='56162008'&gt;</strong>
<strong>&gt;&gt;&gt; obj.method()</strong>
<strong>&lt;Mock name='mock.method()' id='56162232'&gt;</strong>
</pre></div><p>Again, no <code class="literal">return_value</code> is configured, so the default behavior for the method call is to return new mock objects. We can ignore those for this example.</p><p>Now, let <a id="id448" class="indexterm"/>us look at the <code class="literal">mock_calls</code> attribute for the child <a id="id449" class="indexterm"/>objects. This attribute contains a list of all the recorded calls on the mock object, as shown in the following:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; child_obj1.mock_calls</strong>
<strong>[call.method1()]</strong>
<strong>&gt;&gt;&gt; child_obj2.mock_calls</strong>
<strong>[call.method1(), call.method2()]</strong>
</pre></div><p>The mock objects have the appropriate method calls recorded, as expected. Let us now take a look at the attribute on the main <code class="literal">obj</code> mock object, as follows:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; obj.mock_calls</strong>
<strong>[call.child1.method1(),</strong>
<strong> call.child2.method1(),</strong>
<strong> call.child2.method2(),</strong>
<strong> call.method()]</strong>
</pre></div><p>Now, this is surprising! The main mock object seems to not only have details of its own calls, but also all the calls made by the child mocks!</p><p>So, how can we use this feature in our test to assert the order of the calls made across different mocks?</p><p>Well, what if we wrote the above test like the following:</p><div><pre class="programlisting">def test_action_fires_when_rule_matches(self):
    goog = Stock("GOOG")
    exchange = {"GOOG": goog}
    main_mock = mock.MagicMock()
    rule = main_mock.rule
    rule.matches.return_value = True
    rule.depends_on.return_value = {"GOOG"}
    action = main_mock.action
    alert = Alert("sample alert", rule, action)
    alert.connect(exchange)
    goog.update(datetime(2014, 5, 14), 11)
    main_mock.assert_has_calls(
        [mock.call.rule.matches(exchange),
         mock.call.action.execute("sample alert")])</pre></div><p>Here, we create a main mock object called <code class="literal">main_mock</code>, and the <code class="literal">rule</code> and <code class="literal">action</code> mocks are child mocks of this. We then use the mocks as usual. The difference is that we use <code class="literal">main_mock</code> in the assert section. Since <code class="literal">main_mock</code> has a record of the order in which calls are made to the child mocks, this assertion can check the order of calls to the <code class="literal">rule</code> and <code class="literal">action</code> mocks.</p><p>Let us go <a id="id450" class="indexterm"/>a step further. The <code class="literal">assert_has_calls</code> method only asserts that the calls were made and that they were in that particular <a id="id451" class="indexterm"/>order. The method <em>does not</em> guarantee that these were the <em>only</em> calls made. There could have been other calls before the first call or after the last call, or even in-between the two calls. The assertion will pass as long as the calls we are asserting were made, and that between them the order was maintained.</p><p>To strictly match the calls, we can simply do an <code class="literal">assertEqual</code> on the <code class="literal">mock_calls</code> attribute like the following:</p><div><pre class="programlisting">def test_action_fires_when_rule_matches(self):
    goog = Stock("GOOG")
    exchange = {"GOOG": goog}
    main_mock = mock.MagicMock()
    rule = main_mock.rule
    rule.matches.return_value = True
    rule.depends_on.return_value = {"GOOG"}
    action = main_mock.action
    alert = Alert("sample alert", rule, action)
    alert.connect(exchange)
    goog.update(datetime(2014, 5, 14), 11)
    self.assertEqual([mock.call.rule.depends_on(),
                      mock.call.rule.matches(exchange),
                      mock.call.action.execute("sample alert")],
                     main_mock.mock_calls)</pre></div><p>In the above, we assert the <code class="literal">mock_calls</code> with a list of expected calls. The list must match exactly—no missing calls, no extra calls, nothing different. The thing to be careful about is that we must list out <em>every</em> call. There is a call to <code class="literal">rule.depends_on</code>, which is done in the <code class="literal">alert.connect</code> method. We have to specify that call, even though it is not related to the functionality we are trying to test.</p><p>Usually, matching every call will lead to verbose tests as all calls that are tangential to the functionality being tested also need to be put in the expected output. It also leads to brittle tests as even slight change in calls elsewhere, which might not lead to change in behavior in this <a id="id452" class="indexterm"/>particular test, will still cause the test to fail. This is<a id="id453" class="indexterm"/> why the default behavior for <code class="literal">assert_has_calls</code> is to only determine whether the expected calls are present, instead of checking for an exact match of calls. In the rare cases where an exact match is required, we can always assert on the <code class="literal">mock_calls</code> attribute directly.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec72"/>Pattern – patching the open function</h2></div></div></div><p>One of the most common cases of mocking is to mock out file access. This is actually a little <a id="id454" class="indexterm"/>cumbersome because the <code class="literal">open</code> function can be used in a<a id="id455" class="indexterm"/> number of different ways. It can be used as a regular function or as a context manager. The data can be read using many methods such as <code class="literal">read</code>, <code class="literal">readlines</code>, and so on. In turn, some of these functions, return iterators that can be iterated upon. It is a pain to sit and mock all these out in order to be able to use them in tests.</p><p>Fortunately, the mocking library provides an extremely helpful <code class="literal">mock_open</code> function, which returns a mock that handles all these situations. Let us see how we can use this function.</p><p>The following is the code for a <code class="literal">FileReader</code>:</p><div><pre class="programlisting">class FileReader:
    """Reads a series of stock updates from a file"""
    def __init__(self, filename):
        self.filename = filename

    def get_updates(self):
        """Returns the next update everytime the method is called"""

        with open(self.filename, "r") as fp:
            for line in fp:
                symbol, time, price = line.split(",")
                yield (symbol, datetime.strptime(time, "%Y-%m-%dT%H:%M:%S.%f"), int(price))</pre></div><p>This class reads stock updates from a file and returns each update, one by one. The method is a generator, and uses the <code class="literal">yield</code> keyword to return updates, one at a time.</p><div><div><h3 class="title"><a id="note26"/>Note</h3><p>
<strong>A quick primer on generators</strong>
</p><p>
<strong>Generators</strong> are functions that use a <code class="literal">yield</code> statement instead of a <code class="literal">return</code> statement<a id="id456" class="indexterm"/> to return values. Each time the generator is executed, the execution does not start at the beginning of the function, but instead continues running from the previous <code class="literal">yield</code> statement. In the example above, when the generator is executed, it parses the first line of the file, then yields the value. The next time it is executed, it continues once more through the loop and returns the second value, then the third value, and so on until the loop is over. Each execution of the generator returns one stock update. For more on generators, check out the Python documentation or online articles. One such article<a id="id457" class="indexterm"/> can be found at <a class="ulink" href="http://www.jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/">http://www.jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/</a>.</p></div></div><p>In order to test the <code class="literal">get_update</code> method, we will need to create different kinds of file data and verify that the method reads them properly and returns values as expected. In order to do this, we will mock out the open function. The following is one such test:</p><div><pre class="programlisting">class FileReaderTest(unittest.TestCase):
    @mock.patch("builtins.open",
                mock.mock_open(read_data="""\
                GOOG,2014-02-11T14:10:22.13,10"""))
    def test_FileReader_returns_the_file_contents(self):
        reader = FileReader("stocks.txt")
        updater = reader.get_updates()
        update = next(updater)
        self.assertEqual(("GOOG",
                          datetime(2014, 2, 11, 14, 10, 22, 130000),
                          10), update)</pre></div><p>In the <a id="id458" class="indexterm"/>above test, we are starting with patching the <code class="literal">builtins.open</code> function. The <code class="literal">patch</code> decorator can take a second parameter, in which we can specify <a id="id459" class="indexterm"/>the mock object to be used after patching. We call the <code class="literal">mock.mock_open</code> function to create an appropriate mock object, which we pass to the <code class="literal">patch</code> decorator.</p><p>The <code class="literal">mock_open</code> function takes a <code class="literal">read_data</code> parameter, in which we can specify what data should be returned when the mocked file is read. We use this parameter to specify the file data we want to test against.</p><p>The rest of the test is fairly simple. The only thing to note is in the following line:</p><div><pre class="programlisting">updater = reader.get_updates()</pre></div><p>Since <code class="literal">get_updates</code> is a generator function, a call to the <code class="literal">get_updates</code> method does not actually return a stock update, but instead returns the generator object. This generator object is stored in the <code class="literal">updater</code> variable. We use the built-in <code class="literal">next</code> function to get the stock update from the generator and assert that it is as expected.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec73"/>Pattern – mocking with mutable args</h2></div></div></div><p>One gotcha<a id="id460" class="indexterm"/> that can bite us is when arguments to mocked<a id="id461" class="indexterm"/> out objects are mutable. Take a look at the following example:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; from unittest import mock</strong>
<strong>&gt;&gt;&gt; param = ["abc"]</strong>
<strong>&gt;&gt;&gt; obj = mock.Mock()</strong>
<strong>&gt;&gt;&gt; _ = obj(param)</strong>
<strong>&gt;&gt;&gt; param[0] = "123"</strong>

<strong>&gt;&gt;&gt; obj.assert_called_with(["abc"])</strong>
<strong>Traceback (most recent call last):</strong>
<strong>  File "&lt;stdin&gt;", line 1, in &lt;module&gt;</strong>
<strong>  File "C:\Python34\lib\unittest\mock.py", line 760, in assert_called_with</strong>
<strong>    raise AssertionError(_error_message()) from cause</strong>
<strong>AssertionError: Expected call: mock(['abc'])</strong>
<strong>Actual call: mock(['123'])</strong>
</pre></div><p>Whoa! What happened there? The error says the following:</p><div><pre class="programlisting"><strong>AssertionError: Expected call: mock(['abc'])</strong>
<strong>Actual call: mock(['123'])</strong>
</pre></div><p>Actual call was <code class="literal">mock(['123'])</code>? But we called the mock as follows:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; param = ["abc"]</strong>
<strong>&gt;&gt;&gt; obj = mock.Mock()</strong>
<strong>&gt;&gt;&gt; _ = obj(param)</strong>
</pre></div><p>It's pretty clear that we called it with <code class="literal">["abc"]</code>. So why is this failing?</p><p>The answer is that the mock object only stores a reference to the call arguments. So, when the line <code class="literal">param[0] = "123"</code> was executed, it affected the value that was saved as the call argument in the mock. In the assertion, it looks at the saved call argument, and sees that the call was made with the data <code class="literal">["123"]</code>, so the assertion fails.</p><p>The obvious question is: why is it that the mock stores a reference to the parameters? Why doesn't it make a copy of the arguments so that the stored copy doesn't get changed if the object passed as a parameter is changed later on? The answer is that making a copy creates a new object, so all assertions where object identity is compared in the argument list will fail.</p><p>So what do we do now? How do we make this test work?</p><p>Simple: we just inherit from <code class="literal">Mock</code> or <code class="literal">MagicMock</code> and change the behavior to make a copy of the arguments, as shown in the following:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; from copy import deepcopy</strong>
<strong>&gt;&gt;&gt;</strong>
<strong>&gt;&gt;&gt; class CopyingMock(mock.MagicMock):</strong>
<strong>...     def __call__(self, *args, **kwargs):</strong>
<strong>...         args = deepcopy(args)</strong>
<strong>...         kwargs = deepcopy(kwargs)</strong>
<strong>...         return super().__call__(*args, **kwargs)</strong>
</pre></div><p>This mock just makes a copy of the arguments and then invokes the default behavior passing in the copy.</p><p>The assertion now passes, as shown in the following:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; param = ["abc"]</strong>
<strong>&gt;&gt;&gt; obj = CopyingMock()</strong>
<strong>&gt;&gt;&gt; _ = obj(param)</strong>
<strong>&gt;&gt;&gt; param[0] = "123"</strong>
<strong>&gt;&gt;&gt; obj.assert_called_with(["abc"])</strong>
</pre></div><p>Keep in <a id="id462" class="indexterm"/>mind that when we use <code class="literal">CopyingMock</code>, we<a id="id463" class="indexterm"/> cannot use any object identity comparisons with the arguments as they will now fail, as shown in the following:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; class MyObj:</strong>
<strong>...     pass</strong>
<strong>...</strong>
<strong>&gt;&gt;&gt; param = MyObj()</strong>
<strong>&gt;&gt;&gt; obj = CopyingMock()</strong>
<strong>&gt;&gt;&gt; _ = obj(param)</strong>

<strong>&gt;&gt;&gt; obj.assert_called_with(param)</strong>
<strong>Traceback (most recent call last):</strong>
<strong>  File "&lt;stdin&gt;", line 1, in &lt;module&gt;</strong>
<strong>  File "C:\Python34\lib\unittest\mock.py", line 760, in assert_called_with</strong>
<strong>    raise AssertionError(_error_message()) from cause</strong>
<strong>AssertionError: Expected call: mock(&lt;__main__.MyObj object at 0x00000000026BAB70&gt;)</strong>
<strong>Actual call: mock(&lt;__main__.MyObj object at 0x00000000026A8E10&gt;)</strong>
</pre></div></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec68"/>Summary</h1></div></div></div><p>In this chapter, you looked at some other patterns for unit testing. You looked at how to speed up tests and how you can run specific subsets of tests. You looked at various patterns for running subset of tests, including creating your own test suites and using the <code class="literal">load_tests</code> protocol. You saw how to use the nose2 attrib plugin to run a subset of tests based on test attributes and how to implement that functionality with the default unit test runner. We then examined features for skipping tests and marking tests as expected failures. You finally looked at how we could write data-driven tests.</p><p>Next, we moved on to some mocking patterns, starting with how to implement spy functionality. You also looked at the problem of validating a sequence of mock calls across multiple mocks. You then looked at the <code class="literal">mock_open</code> function to help us easily mock filesystem access, and in the process you took a peek at how to work with generator functions. Finally, you looked at the problem of using mocks when the arguments are mutable.</p><p>The next chapter is the final chapter in this book, where you will look at other tools that we can use in our TDD practice.</p></div></div>
</body></html>