<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Achieve a Goal through Reinforcement Learning</h1>
                </header>
            
            <article>
                
<p>After the background on reinforcement learning that we provided in the previous chapter, we will go one step forward with GoPiGo3, making it not only perform perception tasks, but also trigger chained actions in sequence to achieve a pre-defined goal. That it is to say, it will have to decide what action to execute at every step of the simulation to achieve the goal. <span>At the end of the execution of every action, it will be provided with a reward, which will show how good the decision was by the amount of reward given. After some training, this reinforcement will </span><span>naturally drive its next decisions, improving the performance of the task.</span></p>
<p>For example, let's say that we set a target location and instruct the robot that it has to carry an object <span>there</span>. The way in which GoPiGo3 will be told that it is performing well is by giving it rewards. This way of providing feedback encourages it to pursue the goal. In specific terms, the robot has to select from a set of possible actions (move forward, backward, left, or right), and select the most effective action in each step, since the optimum action will depend on the robot's physical location in the environment.</p>
<p><span>The field of machine learning that deals with this kind of problem is known as</span><span> <strong>reinforcement learning</strong>, and it is a very active topic of research. It has surpassed human performance in some scenarios, as in the recent case of <strong>Alpha Go</strong>, <a href="https://deepmind.com/blog/article/alphago-zero-starting-scratch">https://deepmind.com/blog/article/alphago-zero-starting-scratch</a>.</span></p>
<p><span>The following topics will be covered in the chapter:</span></p>
<ul>
<li style="font-weight: 400">Preparing the environment with TensorFlow, Keras, and Anaconda</li>
<li style="font-weight: 400">Installing the ROS machine learning packages</li>
<li style="font-weight: 400">Setting the training task parameters</li>
<li style="font-weight: 400">Training GoPiGo3 to reach a target location while avoiding obstacles</li>
</ul>
<p>You will find in the practice case how GoPiGo3 learns by trying different actions, being encouraged to select the most effective action for every location. You may have guessed that this is a very costly computational task, and you will get an idea of the challenge robotics engineers are facing nowadays to make robots smarter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will make use of the code located in the <kbd>Chapter12_Reinforcement_Learning</kbd><span> folder:</span><span> <a href="https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter12_Reinforcement_Learning">https://github.com/PacktPublishing/Hands-On-ROS-for-Robotics-Programming/tree/master/Chapter12_Reinforcement_Learning</a>.</span></p>
<p><span>Copy the files of this chapter to the ROS workspace, putting them inside the <kbd>src</kbd> folder</span>:</p>
<pre><strong>$ cp -R ~/Hands-On-ROS-for-Robotics-Programming/Chapter12_Reinforcement_Learning  ~/catkin_ws/src/</strong></pre>
<p><span>As usual, you need to rebuild the workspace in the laptop:</span></p>
<div>
<pre><strong>$ cd ~/catkin_ws</strong><br/><strong>$ catkin_make</strong></pre></div>
<p>Once you have the code for the chapter, we dedicate the next section to describing and installing the software stack for the practical project we will develop.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the environment with TensorFlow, Keras, and Anaconda</h1>
                </header>
            
            <article>
                
<p>Together with <strong>Anaconda</strong><span>, which you were instructed to install in the previous chapter, </span>you will <span>now </span>install the machine learning tools <strong>TensorFlow</strong><span> </span>and<span> </span><strong>Keras</strong>. You will need them to make the neural networks that are required to solve the reinforcement learning tasks:</p>
<ul>
<li><strong>TensorFlow</strong><span> </span>is the low-level layer of your machine learning environment. It deals with the mathematical operations involved in the creation of neural networks. Since they are mathematically resolved as matrix operations, you need a framework that is effective at solving this algebra, and TensorFlow is one of the most efficient frameworks for that. The name of the library comes from the mathematical concept of a <em>tensor</em>, which can be understood as a matrix with more than two dimensions.</li>
</ul>
<ul>
<li><strong>Keras</strong> <span>is the high-level layer of your machine learning environment. This library lets you easily define the structure of the neural network in a declarative way: you just have to define the structure of the nodes and edges, and TensorFlow (the low-level layer) will take care of all the mathematical operations to create the network.</span></li>
</ul>
<div class="packt_tip">Here, we will make use of the isolation feature that Anaconda <span>provides</span>. Remember that in the previous chapter you created a <strong>Conda</strong> environment called <strong>gym</strong>, inside which you installed <strong>OpenAI Gym</strong> together with TensorFlow and Keras. Now you will be instructed to work in a different Conda environment, where you will install only the modules you will need for<span> this chapter. This way, you keep the code for each chapter isolated, as they apply to different projects. In fact, you will install specific versions of TensorFlow and Keras that may be different than the latest versions that were used in the previous chapter. This is a common way to proceed in Python projects.</span></div>
<p class="mce-root">Once we have clarified what each component provides, let's install each of them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow backend</h1>
                </header>
            
            <article>
                
<div>
<p>First, create a dedicated<span> </span><kbd>conda</kbd><span> </span>environment called<span> </span><kbd>tensorflow</kbd>. It consists of a virtual space that lets users isolate the set of Python packages you will use for a specific project. This has the advantage of making it straightforward to replicate the environment in another machine with minimal effort:</p>
<ol>
<li>Let's run the following commands:</li>
</ol>
<div class="language-bash highlighter-rouge">
<div class="highlight">
<pre class="highlight" style="padding-left: 60px"><strong><span class="nv">$ </span>conda create <span class="nt">-n</span> tensorflow pip <span class="nv">python</span><span class="o">=</span>2.7</strong><br/><strong>$ conda activate tensorflow</strong></pre></div>
</div>
<p style="padding-left: 60px">The second line produces the activation and binds the next installations to this <kbd>tensorflow</kbd> <span>environment.</span></p>
</div>
<div class="packt_infobox">Conda enviroment are isolated buckets that contain the python modules you need for a specific <strong>project</strong>. For example, for <strong>tensorflow</strong> environment, every Python module you install with either <kbd>conda install</kbd> or <kbd>pip install</kbd> will be placed at <kbd>~/anaconda2/envs/tensorflow/bin</kbd>. The <strong>activation</strong> means that whenever a Python scripts needs to import some module, it will look for it at such <strong>project</strong> path.</div>
<div>
<ol start="2">
<li>Now you can pr<span>oceed to install TensorFlow: </span></li>
</ol>
<div class="language-bash highlighter-rouge">
<div class="highlight">
<pre class="highlight" style="padding-left: 60px"><strong><span class="nv">(</span>tensorflow<span class="nv">) $ </span>pip <span class="nb">install</span> <span class="nt">--ignore-installed</span> <span class="nt">--upgrade</span> https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.8.0-cp27-none-linux_x86_64.whl</strong><br/><br/></pre></div>
</div>
</div>
<ol start="3">
<li>Additionally, you should also install<span> </span><kbd>matplotlib</kbd> and<span> </span><kbd>pyqtgraph</kbd> in order to draw graphs of the results:</li>
</ol>
<pre class="highlight" style="padding-left: 60px"><strong><span class="nv">(tensorflow) </span><span>$ conda install matplotlib</span><span> </span></strong><span><strong>pyqtgraph</strong><br/></span></pre>
<ol start="4">
<li>Then check for the versions:</li>
</ol>
<pre class="highlight" style="padding-left: 60px"><strong><span class="nv">(tensorflow) </span><span>$ conda list | grep matplotlib</span><span><br/></span><span class="nv">(tensorflow) </span><span>$ conda list | </span><span>grep </span><span>pyqtgraph</span></strong></pre>
<p>These last two commands have been added to give you practical examples of common<span> </span><kbd>conda</kbd><span> </span>commands.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning with Keras</h1>
                </header>
            
            <article>
                
<div>
<p>Keras<span> </span>is a high-level neural network API, written in Python and capable of running on top of TensorFlow. You can easily install a specific version with this command:</p>
<div class="language-bash highlighter-rouge">
<div class="highlight">
<pre class="highlight"><strong><span class="nv">(tensorflow) $ </span>pip <span class="nb">install </span><span class="nv">keras</span><span class="o">==</span>2.1.5</strong></pre></div>
</div>
</div>
<p><span>We have specified version</span><span> </span>2.1.5<em> </em><span>to make sure that you run the code in exactly the same environment we tested it in.</span><span> The l</span>atest version at the time of writing is 2.3.1.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ROS dependency packages</h1>
                </header>
            
            <article>
                
<div>
<p>To use<span> </span><strong>ROS</strong><span> </span>and<span> </span><strong>Anaconda</strong><span> </span>together, you must additionally install the ROS dependency packages:</p>
</div>
<pre class="highlight"><strong><span class="nv">(tensorflow) </span><span class="nv">$ </span>pip <span class="nb">install</span> <span class="nt">-U</span> rosinstall msgpack empy defusedxml netifaces</strong></pre>
<p>You can check the version of any of them by using<span> </span><kbd>pip show</kbd>:</p>
<pre class="highlight"><strong><span class="nv">(tensorflow) </span><span class="nv">$ </span>pip <span class="nb">show</span> rosinstall</strong></pre>
<p>In the next section, we will describe the machine learning package of the code for this chapter, which you should have already cloned to your ROS workspace, as per the <em>Technical requirements</em> section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the ROS Machine Learning packages</h1>
                </header>
            
            <article>
                
<p>The code for this chapter implements the classical reinforcement learning methodology of training a neural network. This neural network is mathematically similar to the one we introduced in <a href="3bf944de-e0f8-4e78-a38b-47796c91185b.xhtml">Chapter 10</a>, <em>Applying Machine Learning in Robotics</em>, stacking layers of <span>(hidden) </span><span>nodes to establish a relationship between the states (the input layer) and the actions (the output layer).</span></p>
<p>The algorithm we will use for reinforcement learning is called <span><strong>Deep Q-Network</strong> (</span><strong>DQN</strong><span>) and was introduced in <a href="c5f1bcd8-306d-4ef8-b0ad-982c8bbb2b73.xhtml">Chapter 11</a>, <em>Machine Learning with OpenAI Gym</em> in the <em>Running an environment </em></span><span>section</span><span>. In the next section, </span><em>Setting the training task parameters</em><span>, you will be given the operational description of states, actions, and rewards that characterize the reinforcement learning problem that we are going to solve with ROS.</span></p>
<p>Next, we will present the training scenarios, and then we will explain how the files inside the <span>ROS</span><span> </span><span>packages are chained to launch a training task.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training scenarios</h1>
                </header>
            
            <article>
                
<p>This section is devoted to explaining how the reinforcement learning <span>package –</span> <span>the content of the code sample provided with the book repository –</span> is organized.</p>
<p>First, let's take into account that we are going to train the robot for two scenarios:</p>
<ul>
<li>Scenario 1: <strong>Travel to a target location</strong>. This scene is shown in the following image and consists of a square limited by four walls. There are no obstacles in the environment. The target location can be any point within the limits of the walls:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="assets/709dcd93-2212-43d4-a12b-7853f71f5567.png" style="width:25.50em;height:18.50em;"/></p>
<ul>
<li>Scenario 2: <strong>Travel to a target location avoiding obstacles</strong>. <span>This scene consists of the same square plus four static cylindrical obstacles. The target location can be any point within the limits of the walls, except for the locations of the obstacles:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0dd52063-189c-403b-8275-99abfa6fca1c.png" style="width:25.75em;height:16.42em;"/></p>
<p>Now that we have presented the training scenarios, let's describe the ROS package that we will use for reinforcement learning for GoPiGo3.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ROS package structure for running a reinforcement learning task</h1>
                </header>
            
            <article>
                
<p>The stack of code for this chapter is composed of the following packages:</p>
<ul>
<li><strong><kbd>gopigo3_model</kbd></strong> is the ROS compilation that allows us to visualize GoPiGo3 in RViz using the launch file <kbd>gopigo3_rviz.launch</kbd>. The robot configuration is the familiar 3D solid model that we have used in the previous chapters. More precisely, it corresponds to the GoPiGo3 complete version, which includes the distance sensor, the Pi camera, and the laser distance sensor, that is, the model that was built in <a href="25ac032c-5bfe-47ff-aa5a-f178dbff7c57.xhtml">Chapter 8</a>,  <em>Virtual SLAM and Navigation Using Gazebo</em>.</li>
<li><strong><kbd>gopigo3_gazebo</kbd></strong> <span>is built on top of the previous package, allowing us to simulate GoPiGo3 in Gazebo using the <kbd>gopigo3_world.launch</kbd> file. The URDF model that this file loads is the same as the one in the visualization in the <kbd>gopigo3_model</kbd> package.</span></li>
<li><strong><kbd>gopigo3_dqn</kbd></strong> is the specific package for carrying out the reinforcement learning task with GoPiGo3. As it is ROS, it is modular, and the decoupling we provide by separating model, simulation, and reinforcement learning makes it straightforward to use this same package to train other robots.</li>
</ul>
<p>In this ROS package, we use the <strong>DQN</strong> algorithm to train the robot. Remember that DQN was introduced in the previous chapter, in the <em>Running an environment </em><span>section</span>. In brief, what DQN will do is establish a relation between the <em>states of the robot</em> and the <em>actions to execute</em> using a neural network, where the <em>states</em> are <span>the <strong>input layer</strong> and the <em>actions</em> are the <strong>output layer</strong></span>.</p>
<p>The mathematics behind reinforcement learning theory is complex, and it is not absolutely necessary to learn how to use this technique to train simple robots as GoPiGo3. So, let's focus on the configuration of the training tasks, abstracting the specific implementation in Python of the example that supports the practical elements of this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting the training task parameters</h1>
                </header>
            
            <article>
                
<p>At this point, we are briefly going to introduce the three essential concepts in reinforcement learning: <strong>states</strong>, <strong>actions</strong>, and <strong>rewards</strong>. In this section, we will give you minimal information so that you can understand the practical exercise in this chapter. In this case, w<span>e are applying </span>the strategy of <em>focus on the practice to really understand the theory</em>.</p>
<div class="packt_infobox">This method of <span><em>focus on the practice to really understand the theor</em>y</span><span> is especially required for complex topics that are better understood if you follow an empirical approach with easy-to-run examples. This preliminary <em>practical success</em> should provide you with enough motivation to get deeper into the topic, a task that in any case will be hard both in the algorithms and in the mathematics behind them.</span></div>
<p>So, let's proceed to define these core concepts involved in the learning task of the robot:</p>
<ul>
<li>A<span> </span><strong>state</strong><span> </span>is an<span> </span><em>observation of the environment</em>. Thanks to the data streaming from the LDS, the state is characterized by the range and the angle to the goal location. For example, if you get LDS measurements with a one-degree resolution, each state will be a set of 360 points, each value corresponding to every angle in the full circumference of 360º. As the robot moves, the state changes, and this is reflected in the new set of 360 range values provided by the LDS. Remember that each range value corresponds to the distance to the nearest obstacle in that specific direction.</li>
<li>The<span> </span><strong>action</strong><span> </span>is what the robot can do using its motors to move in the environment, that is, translate and/or rotate to approach the target. By executing an action, the robot moves and changes its <strong>state</strong> – defined by the new set of range values coming from the LDS. </li>
<li>The <strong>reward</strong> is the <em>prize</em> you give to the robot every time it executes an action. The prize you give for each action in every possible state is called the <strong>reward policy</strong>, and it is an essential part of the success of a reinforcement learning task. Hence, it has to be defined by the user (the <em>trainer</em>). In plain words, you will give a greater reward to an action the robot performs that is more effective in achieving the goal.</li>
</ul>
<p>For the practical case, we are going to run the<span> </span>reward policy<span> </span>as follows:</p>
<ul>
<li>If the obstacle is in the <span>forward half space of the robot (180° angle covering left to right in the forward motion direction),</span> it obtains a<span> </span>positive<span> </span>angle-based reward<span> </span>ranging from 0 to 5. The maximum value (5) corresponds to the case in which the robot is oriented in the target direction. We specify this half-space by the relative angle being between -90° and +90° (in this angle the reward is the minimum, that is 0).  The r<span>eference direction is the line crossing the target location and the robot.</span></li>
<li>If it is in the <span>back half space of the robot (180° angle covering left to right opposite to forward half space),</span> the obtained reward is<span> </span>negative<em>,</em> <span>ranging f</span><span>rom 0 to -5 (linear dependence with respect to the angle: 0 at 90° and -90°, and -5 at -180°).</span></li>
<li><span>If the current distance from the goal is above a preset threshold, the agent obtains a</span><em> </em>distance-based reward <span>&gt;2. </span>If it is below this threshold, the reward is lower than 2, approaching the minimum value of 1 as the robot gets closer. Then, the<span> </span><strong>approaching reward</strong><span> </span><span>is the dot product of the </span>angle <span>and</span> distance-based rewards<strong><em> </em></strong><em>=</em> <strong>[a]</strong> <em>*</em> <strong>[b]</strong>.</li>
<li><span>If the robot achieves the goal, a <strong>success reward</strong> of 200 is given.</span></li>
<li><span>If the robot crashes into an obstacle, a <strong>penalty</strong> of 150 is given, that is, a negative reward of -150.</span></li>
</ul>
<p>The reward is cumulative, and we could add any of these terms to the reward at every step of the simulation:</p>
<ul>
<li>Approaching reward</li>
<li>Success reward</li>
<li>Obstacle penalty</li>
</ul>
<div class="packt_infobox">A <strong>step</strong> in the simulation is <em>what happens to the robot between two consecutive states</em>. And what happens is, that the robot executes an action, and – as a consequence – it moves, changing its state and obtaining a reward based on the policy defined in this section.</div>
<p>After this task configuration is understood, you are ready to run the training of GoPiGo3 in the two scenarios that have been defined.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training GoPiGo3 to reach a target location while avoiding obstacles</h1>
                </header>
            
            <article>
                
<p>Prior to running training in the scenario, we should note the adjustment of a parameter that <span>dramatically </span>affects the computational cost. This is the horizontal sampling of the LDS, since the <strong>state</strong> of the robot is characterized by the set of range values in a given step of the simulation. In previous chapters, when we performed navigation in Gazebo, we used a sampling rate of 720 for LDS. This means that we have circumferential range measurements at 1º resolution.</p>
<p>For this example of reinforcement learning, we are reducing the sampling to 24, which means a range resolution of 15º. The positive aspect of this decision is that you reduce the <strong>state</strong> vector from 360 items to 24, which is a factor of 15. You may have guessed that this will make the simulation more computationally efficient. In contrast, you will find that the drawback is that GoPiGo3 loses its perception capability, since it will only be able to detect objects whose angle coverage with respect to the point of view of the robot is larger than 15º. At a <span>distance of 1 </span>meter, this is equivalent to a minimum obstacle width of 27 cm.</p>
<div class="packt_tip packt_infobox">On a positive note, as the robot gets closer to an obstacle, its discrimination capability improves. For example, at a <span>distance of </span>10 cm, an arc of 15º means it can detect <span>obstacles with </span>a minimum width of 5.4 cm.</div>
<p>The horizontal sampling is set in the URDF model, in the part of the file that describes the LDS, located at <kbd>./gopigo3_model/urdf/gopigo3.gazebo</kbd>. The number to specify for obtaining rays with 15º spacing is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f7961a60-89f9-457c-93c9-1ac292cc2cb1.png" style="width:11.42em;height:1.42em;"/></p>
<div class="packt_infobox">Since LDS covers from 0º to 360º, to get 24 equally spaced rays, you have add one more sample, making it 25, since 0º and 360º are actually the same angle.</div>
<p>Then, the LDS part of the URDF file has to be modified as follows:</p>
<div>
<pre>&lt;gazebo reference="base_scan"&gt;<br/>    &lt;material&gt;Gazebo/FlatBlack&lt;/material&gt;<br/>    &lt;sensor type="ray" name="lds_lfcd_sensor"&gt;<br/>    &lt;pose&gt;0 0 0 0 0 0&lt;/pose&gt;<br/>    &lt;visualize&gt;<strong>true</strong>&lt;/visualize&gt;<br/>    &lt;update_rate&gt;5&lt;/update_rate&gt;<br/>    &lt;ray&gt;<br/>    &lt;scan&gt;<br/>         &lt;horizontal&gt;<br/>             &lt;samples&gt;<strong>25</strong>&lt;/samples&gt;<br/>            &lt;resolution&gt;1&lt;/resolution&gt;<br/>            &lt;min_angle&gt;<strong>0</strong>&lt;/min_angle&gt;<br/>            &lt;max_angle&gt;<strong>6.28319</strong>&lt;/max_angle&gt;<br/>...</pre></div>
<p>By setting the <kbd>&lt;visualize&gt;</kbd> <span>tag </span>to <kbd>true</kbd>, the ray tracing is shown in Gazebo.</p>
<p>Is this sampling enough to ensure that the robot can be effectively trained? Let's answer the question comparing the number of rays for every case. This first figure shows the 0.5° actual resolution of the physical LDS. Rays are so close to each other that you cannot almost see the resolution. It provides very faithful sensing of the environment:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7e6ea91e-bbd6-41fc-8e7e-d2e1cff32850.png" style="width:33.67em;height:32.08em;"/></p>
<p>This second image shows <span>the case of 24 samples and 15º resolution:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3f61be4c-4697-4f54-b419-8a8fab1d0d75.png" style="width:33.67em;height:33.33em;"/></p>
<p>In this picture, the ray tracing shows that, even with so few rays, the obstacles are detected, since only one ray is needed to identify an obstacle. This fact helps to mitigate the loss of perception resolution. However, bear in mind that the robot will not be able to know the obstacle's width, only that it will be less than 30º. Why? Because you need three rays to detect an obstacle of <span>finite width</span>, the central ray detecting it and the extreme ones not interfering with it. Hence, this upper limit for the obstacle width is equal to twice the angular distance between adjacent rays, that is, <em><span>2 x 15º = 30º</span></em>. In some cases, this may be too imprecise, but, for the simple scenario we are using in this example, it should be precise enough.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to run the simulations</h1>
                </header>
            
            <article>
                
<p>Before beginning the training process for each scenario, let's recap what we learned in the previous chapter about visualization (RViz) and simulation (Gazebo) to establish a link with the learning process, which will make use of these tools and related scripts:</p>
<ol>
<li>To launch the visualization in RViz,<span> </span>you have to simply execute the following command:</li>
</ol>
<div>
<pre style="padding-left: 60px"><strong><span>T1 $ roslaunch gopigo3_model gopigo3_rviz.launch</span></strong></pre></div>
<ol start="2">
<li>To launch the simulation in Gazebo, you may proceed in a similar manner using this single command:</li>
</ol>
<div>
<pre style="padding-left: 60px"><strong><span>T1 $ roslaunch gopigo3_gazebo gopigo3_world.launch</span></strong></pre></div>
<ol start="3">
<li>Finally, to run the reinforcement learning task you <span>first </span>have to launch Gazebo – as explained in step 2 – but with the selected training environment, instead of the general <kbd>gopigo3_world.launch</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>T1 $ roslaunch gopigo3_gazebo gopigo3_stage_1.launch<br/></span>T2 $ roslaunch gopigo3_dqn gopigo3_dqn_stage_1.launch</strong></pre>
<p style="padding-left: 60px">These two commands run the task in <strong>scenario 1</strong><span> </span>described earlier. To perform the training for <strong>scenario 2</strong><span>, </span>you only need to execute the corresponding launch files:</p>
<pre style="padding-left: 60px"><strong><span>T1 $ roslaunch gopigo3_gazebo gopigo3_stage_2.launch<br/></span>T2 $ roslaunch gopigo3_dqn gopigo3_dqn_stage_2.launch</strong></pre>
<p style="padding-left: 60px">The first line loads the scenario 2 environment, and the second launches the training task for it.</p>
<p>The following two sub-sections show ROS in action by executing the commands in step 3.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scenario 1 – travel to a target location</h1>
                </header>
            
            <article>
                
<p>Follow the next procedure to make sure that the training process happens as expected:</p>
<ol>
<li>First, launch the virtual robot model in Gazebo:</li>
</ol>
<pre style="padding-left: 60px"><strong>T1 $ roslaunch <span>gopigo3_gazebo gopigo3_stage_1.launch</span></strong><span><br/></span></pre>
<ol start="2">
<li>Then, you can start the training process. But first, you have to be in the <kbd>tensorflow</kbd> virtual environment:</li>
</ol>
<pre style="padding-left: 60px"><strong>T2 $ conda activate tensorflow</strong></pre>
<ol start="3">
<li>Now, start the training:</li>
</ol>
<pre style="padding-left: 60px"><strong>T2 (tensorflow) $ roslaunch gopigo3_dqn gopigo3_dqn_stage_1.launch</strong></pre>
<p><span><span>You might </span></span>receive an error like this one: </p>
<pre><strong>inotify_add_watch("/home/user/.config/ibus/bus/59ba2b2ca56a4b45be932f4cbc9c914d-unix-0") failed: "No space left on device"</strong></pre>
<p>Don't worry, you can solve it by executing the following command:</p>
<pre><strong>T2 $ echo fs.inotify.max_user_watches=65536 | sudo tee -a /etc/sysctl.conf &amp;&amp; sudo sysctl -p</strong></pre>
<p>Hence, if you receive the error above, solve it as suggested and launch the training script <span>again</span>. Then subscribe to the relevant topics:</p>
<pre><strong>T3 $ rostopic echo <em>get_action</em></strong><br/><strong>T4 $ rostopic echo <em>result</em></strong></pre>
<div>
<div><kbd>get_action</kbd> is a <span><kbd>Float32MultiArray</kbd> message type whose data definition is as follows:</span></div>
<pre><strong><span>get_action.data </span><span>=</span><span> [action, score, reward]<br/></span><span>pub_get_action.publish(get_action)</span></strong></pre></div>
<p>Let's see each component:</p>
<ul>
<li><span> GoPiGo3 always has a linear velocity of 0.15 m/s. The <kbd>action</kbd> item changes the angular velocity from -1.5 to 1.5 rad/s in steps of 0.75, to cover the integer range from 0 to 4.</span></li>
<li>The obtained <kbd>reward</kbd> in each step is as was described in the <em>Setting the training task parameters</em> section.</li>
<li><kbd>score</kbd> is the cumulative reward that the robot obtains in each episode.</li>
</ul>
<p>The corresponding ROS graph can be seen in the following figure:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1422 image-border" src="assets/2304b8fd-3115-40c7-8100-ce730b6c0962.png" style="width:35.00em;height:13.08em;"/></p>
<p>The key node in this graph is <kbd>gopigo3_dqn_stage_1</kbd>, which takes the robot state from the Gazebo simulation and performs the training task by issuing <kbd>cmd_vel</kbd> messages (remember that <span>the velocity commands that drive the robot are published </span>in this topic) and getting the reward for every new state GoPiGo3 achieves.</p>
<p>The episode record can be followed in the console log of terminal <strong>T2</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/dbd99d98-dc54-42d8-aeba-dcaa644b9dc1.png" style="width:51.50em;height:19.50em;"/></p>
<div class="packt_infobox">The red square is the target location and the blue lines are the LDS rays as described at the beginning of the section.</div>
<p>This first scenario aims to get you familiar with the training process in ROS. Let's move on to scenario 2, in which we will give quantitative information about how the training process improves through the episodes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scenario 2 – travel to a target location avoiding the obstacles</h1>
                </header>
            
            <article>
                
<p>The procedure to start the training is similar to scenario 1:</p>
<ol>
<li>We only need to change the name of the files and use the relevant ones:</li>
</ol>
<pre style="padding-left: 60px"><strong>T1 $ roslaunch <span>gopigo3_gazebo gopigo3_stage_2.launch<br/>T2 $ conda activate tensorflow<br/></span>T2 (tensorflow) $ roslaunch gopigo3_dqn gopigo3_dqn_stage_2.launch</strong></pre>
<ol start="2">
<li>If you want to see graphically how the GoPiGo3 learning process is performing, execute the <kbd>result_graph.launch</kbd><span> </span>file:</li>
</ol>
<pre style="padding-left: 60px"><strong>T3 $ roslaunch gopigo3_dqn result_graph.launch</strong></pre>
<p>The following screenshot shows all the content you should see on the screen of your laptop:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1423 image-border" src="assets/c69b591e-269a-44e6-8acc-eddd92fdc9a1.png" style="width:124.08em;height:68.75em;"/></p>
<p>The red graph shows the <span class="packt_screen"><strong>Total rewa</strong><strong>rd</strong></span> obtained in each episode. Remember that an episode is defined as the sequence of steps that comes to an end when a criterion is met. In this problem, an episode finishes if the goal (the red square) is reached or if there is a collision with an obstacle. These graphs show the evolution in the first 300 episodes.</p>
<p>The green graph represents the average Q value of the trained model. Remember that this is the action-value function <em>Q(s,a)</em> that tells you how good it is to execute an action <em>a</em> in a given state <em>s</em>. This concept was explained in the previous chapter in the basic example of the self-driving cab in the <em>Q-learning explained</em> <span>section.</span></p>
<p>You can see how, on average, GoPiGo3 is performing better as it accumulates experience. But how is it operationally using that experience? The answer comes from the reinforcement learning algorithm that has been applied, that is, by associating effective actions to every state given the rewards the robot has been receiving in that state during the training process.</p>
<p>Finally, we should note that in an execution environment like this, the ROS graph alternates between two states. One is when the robot executes an action by publishing a <kbd>cmd_vel</kbd> message that moves the robot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1424 image-border" src="assets/f812a21e-7455-4e92-b73b-80c61c91bf5a.png" style="width:45.50em;height:15.58em;"/></p>
<div class="packt_infobox"><span>For clarity, in this graph we are excluding the nodes of the launch file issued in terminal </span><kbd>T3</kbd><strong>.</strong></div>
<p>The other ROS graph corresponds to the instants in which the agent computes the next state using the set of 24 values coming from the LDS in the <kbd>/scan</kbd> topic:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1425 image-border" src="assets/bb27cf2e-180a-47e0-8940-abb25374aced.png" style="width:46.00em;height:14.50em;"/></p>
<p>At this point, you have covered the end-to-end training process of GoPiGo3. The next challenge is to test the trained model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing the trained model</h1>
                </header>
            
            <article>
                
<p>Taking into account that we are using the <strong><span>Deep Q-Network</span></strong> <span>(</span><strong><span>DQN</span></strong><span>)</span><strong><span> </span></strong><span>algorithm</span>, what we have to save from the training process is the structure of the neural network and the<span> weights of the edges</span><span>. This is what the <kbd>gopigo3_dqn_stage_2</kbd> node performs every 10 episodes. Hence, you can find the saved models inside the <kbd>./gopigo3_dqn/save_model</kbd> folder, and the weights of the network are stored in t</span>he <kbd>h5</kbd> f<span>ile type. Every file contains in its name the scenario</span> (<kbd>stage_1</kbd> or <kbd>stage_2</kbd>) an<span>d the episode number. Follow these steps to evaluate the trained model:</span></p>
<ol>
<li><span>Select the file with the highest episode number, that is, <kbd>stage_2_1020.h5</kbd></span>.</li>
</ol>
<div class="packt_tip packt_infobox">Every <kbd>h5</kbd> file contains the weights of the DQN network as it appears at the end of the episode referenced by the filename. For example, <kbd>stage_2_1020.h5</kbd> refers to the network of scenario 2 at the end of episode 1020.</div>
<p style="padding-left: 60px">In order to use these weights, you basically have to use the same Python script of the training model (<kbd>./nodes/gopigo3_dqn_stage_2</kbd>), but initialize with different values the parameters marked in bold letters in the snippet below that reproduces the first lines of the <kbd>class ReinforceAgent()</kbd> definition:</p>
<pre style="padding-left: 60px">class ReinforceAgent():<br/>    def __init__(self, state_size, action_size):<br/>        self.pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)<br/>        self.dirPath = os.path.dirname(os.path.realpath(__file__))<br/>        self.dirPath = self.dirPath.replace('gopigo3_dqn/nodes', 'gopigo3_dqn/save_model/stage_2_')<br/>        self.result = Float32MultiArray()<br/><br/>        # Load model from last EPISODE<br/>        self.<strong>load_model</strong> = True<br/>        self.<strong>load_episode</strong> = 1020<br/>...</pre>
<p style="padding-left: 60px">Then, what each parameter provides is as follows:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li> <span><kbd>self.load_model = True</kbd> tells the script to load the weights of a pretrained model.</span></li>
<li><span><kbd>self.load_episode = 1020</kbd>  sets</span> the number of the episode from which you want to load the DQN network weights, being the corresponding file <span><kbd>stage_2_1020.h5</kbd>. </span></li>
</ul>
</li>
</ul>
<p class="mce-root"/>
<ol start="2">
<li>Then rename the Python script as <kbd><span>gopigo3_dqn_stage_2-test</span></kbd> and generate the new launch file <kbd>gopigo3_dqn_stage_2-test.launch</kbd>, which will call the created test script:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>&lt;</span><span>launch</span><span>&gt;<br/>    </span><span>&lt;</span><span>node</span><span> </span><span>pkg</span><span>=</span><span>"gopigo3_dqn"</span><span> </span><span>type</span><span>=</span><span>"gopigo3_dqn_stage_2-test"</span><span> </span><span>name</span><span>=</span><span>"gopigo3_dqn_stage_2-test"</span><span> </span><span>output</span><span>=</span><span>"screen"</span><span> </span><span>/&gt;<br/></span><span>&lt;/</span><span>launch</span><span>&gt;</span></pre></div>
<ol start="3">
<li>To launch the test process, follow the same steps as for running the training scenario, but using the test version of the launch file:</li>
</ol>
<pre style="padding-left: 60px"><strong>T1 $ roslaunch <span>gopigo3_gazebo gopigo3_stage_2.launch<br/></span>T2 (tensorflow) $ roslaunch gopigo3_dqn gopigo3_dqn_stage_2-test.launch</strong><br/><strong>T3 $ roslaunch gopigo3_dqn result_graph.launch</strong></pre>
<ol start="4">
<li>Remember that for <kbd>T2</kbd> you have to activate the TensorFlow environment <span>with the </span><kbd>$ <span>conda activate tensorflow</span></kbd> command. When it starts, you will see in <kbd>T2</kbd>, a message telling you that a model from episode 1380 will be used:</li>
</ol>
<pre style="padding-left: 60px"><strong>[INFO] [1579452338.257175, 196.685000]: +++++++++++++++++++++++++++++++++++++++++++++++++++++</strong><br/><strong>[INFO] [1579452338.258111, 196.686000]: STARTING TRAINING MODEL FROM self.load_episode = 1380</strong><br/><strong>[INFO] [1579452338.258537, 196.686000]: =====================================================</strong><br/><strong>[INFO] [1579452339.585559, 1.276000]: Goal position : 0.6, 0.0</strong></pre>
<ol start="5">
<li>If you plot the graphs <span>for the first episodes</span> (as per the command in terminal <kbd>T3</kbd>), you can confirm that the values are pretty good, that is, above 2,000 for the <em><span class="packt_screen">Total reward</span></em><span> </span> and above 100 for the <em><span class="packt_screen">Average max Q-value</span></em><span>:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ce631c69-8dce-4f2b-a122-9c9a83514da0.png" style="width:35.75em;height:36.50em;"/></p>
<p>Although you are testing the model, every ten episodes the network's weights are saved to an <kbd>h5</kbd> file referencing the current episode number.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter has been a quick and practical introduction to how you can apply reinforcement learning so that a robot can perform useful tasks such as transporting materials to a target location. You should be aware that this kind of machine learning technique is at the very beginning of its maturity, and there are as yet few practical solutions working in the real world. The reason is that the process of training is very expensive in terms of time and cost, since you have to perform thousands of episodes to get a well-trained model, and later replay the process with the physical robot to address <span>behavioral</span><span> </span>differences between <span>the real world and the simulated environment</span>.</p>
<p>Be aware that the training process in Gazebo is not a substitute for training in the real world: a simulation necessarily implies a simplification of the reality, and every difference between the training environment (Gazebo) and the physical world introduces new states that can be missing in the training set, and hence the trained neural network will not be able to perform well in such situations. The solution? More training to cover more states, something that also means higher cost.</p>
<p><span>In the last part of the chapter, we have also covered the testing of a model using the same scenario in which the robot was trained. </span><span>A more formal testing approach requires that you check how the trained model generalizes to different conditions in the scenario, such as having more obstacles or moving their positions. </span><span>This is a complex topic, since reinforcement learning algorithms currently struggle to achieve generalization. Why? Because, when you introduce changes in the scenario, you are generating new states in the models. Since they are new to the robot, it will not know the most effective action to execute. Hence, new training is required to explore these new states.</span></p>
<p>Reinforcement learning is currently a very active field of research, and we should expect great advances in the years to come. What we should see is reinforcement learning being applied to real robots at a reasonable cost (that is, training a robot at a pace that doesn't require thousands of episodes) and providing techniques for the generalization of the models to environments other than those that were used for training.</p>
<p>This chapter closes the introduction to the application of machine learning in robotics. In this book, we have only scratched the surface of its potential, and, if you have followed the explanations you should have checked by yourself that this is a complex field where at some point you will have to master statistics, data analytics, and neural networks.</p>
<p>At the same time, it is a field where the focus is on experimentation. Instead of trying to model the reality with analytical formulas or computer-aided simulation, you observe the real world, get data from sensors, and try to infer patterns of behavior from them. Hence, the ability to successfully apply machine learning to robotics relies on being capable of streaming data continuously so that the robot can make smart decisions in real time. And the first step is to produce well-trained models. For this reason, a robot will be able to develop smart behavior in the medium and long term, as it accumulates experience that can be made available in structured trained models.</p>
<p>This is a challenging goal, both for the data scientist and the software engineer. They should work together to create mature robot frameworks that benefit from machine learning as much as common web applications and digital businesses are today.</p>
<p>Finally, many thanks for reading the book. At this point, you are challenged to explore advanced <span>ROS </span>topics, and we hope you can also become an active contributor to the ROS open source community.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What are the essential concepts of reinforcement learning?</li>
</ol>
<p style="padding-left: 60px">A) Robot actions and penalties<br/>
<span>B) Neural networks and deep learning<br/></span>C) S<span>tates, actions, and rewards</span></p>
<ol start="2">
<li>Why do you need to use neural networks in reinforcement learning?</li>
</ol>
<p style="padding-left: 60px">A) Because the robot needs to use deep learning to recognize objects and obstacles.<br/>
B) B<span>ecause the robot has to learn to associate states with the most effective actions.<br/></span><span>C) We do not need </span><span>neural networks in reinforcement learning; we apply different algorithms.</span></p>
<ol start="3">
<li>How do you encourage the robot to achieve the goal of the task?</li>
</ol>
<p style="padding-left: 60px">A) By giving it rewards when it performs <em>good</em> actions.<br/>
<span>B) </span><span>By giving it penalties when it performs </span><em>bad</em><span> actions.<br/></span>C)<span> </span><span>By giving it rewards when it performs </span><em>good</em><span> actions, and penalties in the case of <em>bad</em> actions.</span></p>
<ol start="4">
<li>Can you apply the reinforcement learning ROS package from this chapter to other robots?</li>
</ol>
<p style="padding-left: 60px">A) Y<span>es, because we have separated the robot model, the scenario and the training algorithm into different packages.<br/></span><span>B) No, because you have to rewrite the ROS package for every scenario.<br/></span><span>C) </span><span>No: it is specific for training GoPiGo3.</span></p>
<ol start="5">
<li>Do you need to use the full data feed coming from a real LDS to train a robot?</li>
</ol>
<p style="padding-left: 60px">A) Yes: if you want to obtain accurate results; you have to use all the data.<br/>
B) No<span>: you have to decide the ray tracing density as a function of the typical size of the obstacles in the scenario.<br/></span><span>C) </span><span>No: it depends on how much accuracy you require.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>To delve deeper into the concepts explained in this chapter you can follow the following references:</p>
<ul>
<li><em>Practical Reinforcement Learning</em> from <strong>Coursera</strong>: <a href="https://www.coursera.org/learn/practical-rl">https://www.coursera.org/learn/practical-rl</a></li>
<li>Welcome to Deep Reinforcement Learning Part 1: DQN <a href="https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-dqn-c3cab4d41b6b">https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-dqn-c3cab4d41b6b</a></li>
<li>Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0">https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0</a></li>
<li>Simple Reinforcement Learning with Tensorflow Part 4: Deep Q-Networks and Beyond <a href="https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df">https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df</a></li>
</ul>


            </article>

            
        </section>
    </body></html>