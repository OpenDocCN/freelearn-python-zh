["```py\n            pip install -U requests\n\n    ```", "```py\n            import requests r =\n            requests.get('http://ip.jsontest.com/')\n            print(\"Response object:\", r)\n            print(\"Response Text:\", r.text)\n    ```", "```py\n            payload = {'q': 'chetan'} r =\n            requests.get('https://github.com/search', params=payload)\n            print(\"Request URL:\", r.url)\n    ```", "```py\n            payload = {'key1': 'value1'} r = \n            requests.post(\"http://httpbin.org/post\", data=payload)\n            print(\"Response text:\", r.json())\n    ```", "```py\n            try:\n                r = requests.get(\"http://www.google.com/\")\n            except requests.exceptions.RequestException as e:\n                print(\"Error Response:\", e.message)\n    ```", "```py\n     pip install lxml \n     pip install requests\n\n    ```", "```py\n            from lxml import html \n            import requests \n\n            page = requests.get('https://github.com/pricing/') \n            tree = html.fromstring(page.content) \n            print(\"Page Object:\", tree) \n            plans = tree.xpath('//h2[@class=\"pricing-card-name \n            alt-h3\"]/text()') \n            pricing = tree.xpath('//span[@class=\"default-\n            currency\"]/text()') \n            print(\"Plans:\", plans, \"\\nPricing:\", pricing) \n\n    ```", "```py\n     pip install beautifulsoup4\n\n    ```", "```py\n            <html >\n            <head>\n                <title>Enjoy Facebook!</title> \n            </head>\n            <body>\n                <p>\n                  <span>You know it's easy to get intouch with\n                  your <strong>Friends</strong> on web!<br></span>\n                  Click here <a href=\"https://facebook.com\">here</a>\n                  to sign up and enjoy<br>\n                </p>\n                <p class=\"wow\"> Your gateway to social web! </p>\n                <div id=\"inventor\">Mark Zuckerberg</div>\n                Facebook, a webapp used by millions\n            </body>\n            </html>\n    ```", "```py\n            import bs4\n            myfile = open('python.html')\n            soup = bs4.BeautifulSoup(myfile, \"lxml\")\n            #Making the soup\n            print \"BeautifulSoup Object:\", type(soup)\n    ```", "```py\n            #Find Elements By tags\n            print soup.find_all('a')\n            print soup.find_all('strong')\n            #Find Elements By id\n            print soup.find('div', {\"id\":\"inventor\"})\n            print soup.select('#inventor')\n            #Find Elements by css print\n            soup.select('.wow')\n    ```", "```py\n        print \"Facebook URL:\", soup.find_all('a')[0]['href']\n        print \"Inventor:\", soup.find('div', {\"id\":\"inventor\"}).text \n        print \"Span content:\", soup.select('span')[0].getText()\n```", "```py\n            from bs4 import BeautifulSoup\n            import re\n            import urllib2\n            import os \n            ## Download paramters\n            image_type = \"Project\"\n            movie = \"Avatar\"\n            url = \"https://www.google.com/search?q=\"+movie+\"&source=lnms&tbm=isch\"\n    ```", "```py\n            header = {'User-Agent': 'Mozilla/5.0'}\n            soup = BeautifulSoup(urllib2.urlopen\n            (urllib2.Request(url,headers=header)))\n    ```", "```py\n            images = [a['src'] for a in soup.find_all(\"img\", {\"src\":\n            re.compile(\"gstatic.com\")})][:5]\n            for img in images:\n            print \"Image Source:\", img\n    ```", "```py\n            for img in images:\n              raw_img = urllib2.urlopen(img).read()\n              cntr = len([i for i in os.listdir(\".\") if image_type in i]) + 1\n            f = open(image_type + \"_\"+ str(cntr)+\".jpg\", 'wb') \n            f.write(raw_img)\n            f.close()\n    ```", "```py\n            import requests\n            import json\n            BASE_URL = 'https://api.github.com'\n            Link_URL = 'https://gist.github.com'\n            username = '<username>' ## Fill in your github username\n            api_token = '<api_token>'  ## Fill in your token\n            header = {  'X-Github-Username': '%s' % username,\n                        'Content-Type': 'application/json',\n                        'Authorization': 'token %s' % api_token,\n            }\n            url = \"/gists\" \n            data ={\n              \"description\": \"the description for this gist\",\n              \"public\": True,\n              \"files\": { \n                \"file1.txt\": { \n                  \"content\": \"String file contents\" \n                } \n              }\n            }\n            r = requests.post('%s%s' % (BASE_URL, url), \n                headers=header, \n               data=json.dumps(data))\n           print r.json()['url']\n    ```", "```py\n            import requests\n            import json\n            BASE_URL = 'https://api.github.com'\n            Link_URL =\n            'https://gist.github.com'\n\n            username = '<username>'\n            api_token = '<api_token>'\n            gist_id = '<gist id>' \n\n            header = { 'X-Github-Username': '%s' % username,\n                       'Content-Type': 'application/json',\n                       'Authorization': 'token %s' % api_token,\n            }\n            url = \"/gists/%s\" % gist_id\n            r = requests.get('%s%s' % (BASE_URL, url),\n                              headers=header)\n            print r.json()\n    ```", "```py\n            import requests\n            import json\n\n            BASE_URL = 'https://api.github.com'\n            Link_URL = 'https://gist.github.com'\n\n            username = '<username>'\n            api_token = '<api_token>'\n            gist_id = '<gist_id>'\n\n            header = { 'X-Github-Username': '%s' % username,\n                       'Content-Type': 'application/json',\n                       'Authorization': 'token %s' % api_token,\n            }\n            data = {   \"description\": \"Updating the description\n                       for this gist\",\n                       \"files\": {\n                         \"file1.txt\": {\n                           \"content\": \"Updating file contents..\"\n                         }\n                       } \n            }\n            url = \"/gists/%s\" % gist_id\n            r = requests.patch('%s%s' %(BASE_URL, url), \n                               headers=header,\n                               data=json.dumps(data))\n            print r.json()\n    ```", "```py\n            import requests\n            import json\n            BASE_URL = 'https://api.github.com'\n            Link_URL = 'https://gist.github.com'\n            username = '<username>'\n            api_token = '<api_token>'\n            gist_id = '<gist_id>'\n\n            header = {  'X-Github-Username': '%s' % username,\n                        'Content-Type': 'application/json', \n                        'Authorization': 'token %s' % api_token,\n            }\n            url = \"/gists/%s\" % gist_id \n            r = requests.delete('%s%s' %(BASE_URL, url),\n                                headers=header, )\n    ```", "```py\n        import requests\n\n        BASE_URL = 'https://api.github.com'\n        Link_URL = 'https://gist.github.com'\n\n        username = '<username>'      ## Fill in your github username \n        api_token = '<api_token>'  ## Fill in your token\n\n        header = {  'X-Github-Username': '%s' % username, \n                    'Content-Type': 'application/json',\n                    'Authorization': 'token %s' % api_token,\n        }\n        url = \"/users/%s/gists\" % username\n        r = requests.get('%s%s' % (BASE_URL, url),\n                          headers=header)\n        gists = r.json()\n        for gist in gists:\n            data = gist['files'].values()[0]\n            print data['filename'],\n            data['raw_url'], data['language']\n```", "```py\n pip install -U tornado\n\n```", "```py\n            import tornado.ioloop\n            import tornado.web\n            import httplib2\n\n            class AsyncHandler(tornado.web.RequestHandler):\n                @tornado.web.asynchronous\n                def get(self):\n                  http = httplib2.Http()\n                  self.response, self.content = \n                    http.request(\"http://ip.jsontest.com/\", \"GET\")\n                  self._async_callback(self.response, self.content)\n\n                def _async_callback(self, response, content): \n                print \"Content:\", content\n                print \"Response:\\nStatusCode: %s Location: %s\"\n                  %(response['status'], response['content-location']) \n                self.finish()\n                tornado.ioloop.IOLoop.instance().stop()\n            application = tornado.web.Application([\n                  (r\"/\", AsyncHandler)], debug=True)\n           if __name__ == \"__main__\":\n             application.listen(8888)\n             tornado.ioloop.IOLoop.instance().start()\n    ```", "```py\n     python tornado_async.py\n\n    ```", "```py\n     pip install selenium\n\n    ```", "```py\n            from selenium import webdriver browser =\n            webdriver.Firefox()\n            print \"WebDriver Object\", browser\n    ```", "```py\n            browser.maximize_window()\n            browser.get('https://facebook.com')\n    ```", "```py\n            email = browser.find_element_by_name('email')\n            password = browser.find_element_by_name('pass')\n            print \"Html elements:\"\n            print \"Email:\", email, \"\\nPassword:\", password\n    ```", "```py\n            email.send_keys('abc@gmail.com') *#Enter correct email\n            address*password.send_keys('pass123') *#Enter correct password*\n\n    ```", "```py\n            browser.find_element_by_id('loginbutton').click()\n    ```", "```py\n            from bs4 import BeautifulSoup \n            from threading import Thread \n            import urllib \n\n            #Location of restaurants \n            home_url = \"https://www.yelp.com\" \n            find_what = \"Restaurants\" \n            location = \"London\" \n\n            #Get all restaurants that match the search criteria \n            search_url = \"https://www.yelp.com/search?find_desc=\" +\n            find_what + \"&find_loc=\" + location \n            s_html = urllib.urlopen(search_url).read() \n            soup_s = BeautifulSoup(s_html, \"lxml\") \n\n            #Get URLs of top 10 Restaurants in London \n            s_urls = soup_s.select('.biz-name')[:10] \n            url = [] \n            for u in range(len(s_urls)): \n            url.append(home_url + s_urls[u]['href']) \n\n            #Function that will do actual scraping job \n            def scrape(ur): \n                    html = urllib.urlopen(ur).read() \n                    soup = BeautifulSoup(html, \"lxml\") \n\n                    title = soup.select('.biz-page-title') \n                    saddress = soup.select('.street-address') \n                    phone = soup.select('.biz-phone') \n\n                    if title: \n                         print \"Title: \", title[0].getText().strip() \n                    if saddress: \n                         print \"Street Address: \",\n            saddress[0].getText().strip() \n                    if phone: \n                         print \"Phone Number: \", phone[0].getText().strip() \n                    print \"-------------------\" \n\n            threadlist = [] \n            i=0 \n            #Making threads to perform scraping \n            while i<len(url): \n                      t = Thread(target=scrape,args=(url[i],)) \n                      t.start() \n                      threadlist.append(t) \n                      i=i+1 \n\n            for t in threadlist: \n                      t.join() \n\n    ```"]