- en: Chapter 8. Putting It All into Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to the last chapter of the book. If you've made it this far, you've
    gone over several optimization techniques, both specific to the Python programming
    language and generic ones applicable to other similar technologies.
  prefs: []
  type: TYPE_NORMAL
- en: You've also read about tools for profiling and visualizing those results. We
    also delved into one specific use case for Python, which is number crunching for
    scientific purposes. You learned about the tools that allow you to optimize the
    performance of your code.
  prefs: []
  type: TYPE_NORMAL
- en: In this final chapter, we'll go over one practical use case that covers all
    the technologies we covered in the earlier chapters (remember that some of the
    tools we've seen are alternatives, so using all of them is not really a good plan).
    We will write an initial version of the code, measure its performance, and then
    go through the optimization process to finally rewrite the code and measure the
    performance again.
  prefs: []
  type: TYPE_NORMAL
- en: The problem to solve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we even start thinking about writing the initial version of our code,
    we need to understand the problem we're trying to solve.
  prefs: []
  type: TYPE_NORMAL
- en: Given the scope of the book, a full-blown application might be too big an undertaking,
    so we'll focus on a small task. It'll give us better control over what we want
    to do, and we won't run the risk of having too many things to optimize at the
    same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'To keep things interesting, we''ll split the problem into the following two
    parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Part 1**: This will take care of finding the data we want to process. It
    won''t just be a dataset we download from some given URL. Instead, we''ll scrape
    it from the Web.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Part 2**: This will focus on processing the data obtained after solving the
    first part of the problem. In this step, we may perform the most CPU-intensive
    computations and calculate some statistics from the data gathered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In both cases, we'll create an initial version of the code that solves the problem
    without taking performance into account. Afterwards, we'll analyze each solution
    individually and try to improve them as much as we can.
  prefs: []
  type: TYPE_NORMAL
- en: Getting data from the Web
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The site we'll scrape is **Science Fiction & Fantasy** ([http://scifi.stackexchange.com/](http://scifi.stackexchange.com/)).
    The site is dedicated to answering questions about sci-fi and fantasy topics.
    It is much like StackOverflow but meant for sci-fi and fantasy geeks.
  prefs: []
  type: TYPE_NORMAL
- en: To be more specific, we'll want to scrape the list of latest questions. For
    each question, we'll get the page with the question's text and all the available
    answers. After all the scraping and parsing is done, we'll save the relevant information
    in the JSON format for easier postprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that we''ll deal with HTML pages. However, we don''t want that. We
    want to strip away all HTML code and save only the following items:'
  prefs: []
  type: TYPE_NORMAL
- en: The question's title
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The question's author
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The question's body (the actual text of the question)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The body of the answers (if there are any)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer's author
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this information, we'll be able to do some interesting postprocessing and
    get some relevant statistics (more on that in a minute).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a quick example of how the output of this script should look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This script will take care of saving all the information into one single JSON
    file, which will be predefined inside its code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll try to keep the initial version of both scripts simple. This means using
    the least amount of modules. In this case, the main list of modules will be as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Beautiful Soup** ([http://www.crummy.com/software/BeautifulSoup/](http://www.crummy.com/software/BeautifulSoup/)):
    This is used to parse the HTML files, mainly because it provides a full parsing
    API, automatic encoding detection (which, if you''ve being in this business long
    enough, you''ve probably come to hate) and the ability to use selectors to traverse
    the parsed tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Requests** ([http://docs.python-requests.org/en/latest/](http://docs.python-requests.org/en/latest/)):
    This is used to make HTTP requests. Although Python already provides the required
    modules for this, this module simplifies the API and provides a more Pythonic
    way of handling this task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can install both modules using the `pip` command-line tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows an example of the pages we''ll be scraping and
    parsing in order to get the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting data from the Web](img/B02088_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Postprocessing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The second script will take care of reading the JSON-encoded file and getting
    some stats out of it. Since we want to make it interesting, we won''t limit ourselves
    to just counting the number of questions per user (although we will get this stat
    as well). We''ll also calculate the following elements:'
  prefs: []
  type: TYPE_NORMAL
- en: Top ten users with most questions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Top ten users with most answers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most common topics asked about
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The shortest answer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Top ten most common phrases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Top ten most answered questions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since this book's main topic is performance and not **Natural Language Processing**
    (**NLP**), we will not delve into the details of the small amount of NLP that
    this script will have. Instead, we'll just limit ourselves to improving the performance
    based on what we've seen so far about Python.
  prefs: []
  type: TYPE_NORMAL
- en: The only non-built-in module we'll use in the first version of this script is
    **NLTK** ([http://www.nltk.org](http://www.nltk.org)) to handle all the NLP functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: The initial code base
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now list all of the code that we'll optimize in future, based on the earlier
    description.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first of the following points is quite simple: a single file script that
    takes care of scraping and saving in JSON format like we discussed earlier. The
    flow is simple, and the order is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It will query the list of questions page by page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each page, it will gather the question's links.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, for each link, it will gather the information listed from the previous
    points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It will move on to the next page and start over again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It will finally save all of the data into a JSON file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: By looking at the preceding code, you'll notice that we kept our promise. Right
    now, we're only using the proposed external modules, plus the JSON module, which
    comes built-in with Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second script, on the other hand, is split into two, mainly for organizational
    purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`analyzer.py`: This file contains the main code. It takes care of loading the
    JSON file into a `dict` structure and performs a series of calculations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`visualizer.py`: This file simply contains a set of functions used to visualize
    the different results from the analyzer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s now take a look at the code in both these files. The first set of functions
    will be the utility functions used to sanitize the data, load it into memory,
    and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following set of functions are the ones that actually performs the *counting*
    of data and gets the statistics we want by analyzing the JSON in different ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shows how to use the functions declared earlier and display
    their results. It all boils down to three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: It loads the JSON into memory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It processes the data and saves the results into a dictionary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It goes over that dictionary to display the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The preceding steps are performed in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The code in the following file is merely used to format the output in a human-friendly
    way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Analyzing the code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Analyzing the code will be done in two steps, just like we've being doing so
    far. For each project, we'll profile the code, get the numbers, consider our optimization
    alternatives, and then refactor and measure the code's performance again.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the process described earlier can lead to several iterations of profiling—refactoring—profiling
    again, we'll limit the steps to the final results. However, keep in mind that
    this process is long and takes time.
  prefs: []
  type: TYPE_NORMAL
- en: Scraper
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To start off the optimization process, let's first get some measurements so
    that we can compare our changes with them.
  prefs: []
  type: TYPE_NORMAL
- en: An easy-to-get number is the total time spent during the program's execution
    (in our example, and to keep things simple, we're limiting the total number of
    pages to query to 20).
  prefs: []
  type: TYPE_NORMAL
- en: 'Simply using the `time` command-line tool, we can get that number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows that we have 7 minutes and 30 seconds to scrape
    and parse the 20 pages of questions, which translate into a 3 MB JSON file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scraper](img/B02088_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The scraper script is essentially an IO-bound loop that pulls data from the
    Internet with a minimum amount of processing. So, the first and most logical optimization
    we can spot here is the lack of parallelization of the requests. Since our code
    is not really CPU-bound, we can safely use the multithreading module (refer to
    [Chapter 5](ch05.html "Chapter 5. Multithreading versus Multiprocessing"), *Multithreading
    versus Multiprocessing*) and get an interesting speed boost with minimum effort.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just to clarify what we''re going to be doing, the following diagram shows
    the current status of the scraper script:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scraper](img/B02088_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We're spending most of our running time on I/O operations, more specifically
    on the HTTP requests we're doing to get the list of questions and each question's
    page.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we''ve seen earlier, I/O operations can be parallelized easily using the
    multithreading module. So, we will transform our script so it resembles as shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scraper](img/B02088_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s look at the actual optimized code. We''ll first look at the `ThreadManager`
    class, which will take care of centralizing the configuration of the threads as
    well as the status of the entire parallel process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following functions take care of scraping the information from a page using
    `BeatifulSoup`, either by getting the lists of pages or getting the actual information
    for each question:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The highlighted code in the preceding snippet shows the main change done to
    the initial script. Instead of starting at page 1 and moving forward one by one,
    we're starting a preconfigured number of threads (using the `threading.Thread`
    class directly) that will call our `get_question_page` function in parallel. All
    we had to do was pass in that function as the target of each new thread.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we also needed a way to centralize the configuration parameters
    and the temporary results from each thread. For that, we created the `ThreadManager`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this change, we go from the 7 minutes mark all the way down to 2 minutes
    13 seconds, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scraper](img/B02088_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tweaking the number of threads, for instance, might lead to even better numbers,
    but the main improvement is already there.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code for the analyzer script is different compared to the scraper. Instead
    of having a heavy I/O-bound script, we have the opposite: a CPU-bound one. It
    does very little I/O, mainly to read the input file and output the results. So,
    we will focus on measuring in more detail.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first get some basic measurements so that we know where we stand:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzer](img/B02088_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot shows the output of the `time` command-line utility.
    So now that we have a base number to work with, we know we need to get the execution
    time lower than 3.5 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first approach would be to use `cProfile` and start getting some numbers
    from the inside of our code. This should help us get a general overview of our
    program to start understanding where our pain points are. The output looks like
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzer](img/B02088_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'There are two areas of interest in the preceding screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: On the left-hand side, we can see the functions and how much time they consume.
    Pay attention to how most of the list is composed of external functions, mainly
    from the `nltk` module (the first two are just consumers of the others below,
    so they don't really matter).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the right-hand side, the **Callee Map** looks way too complex to interpret
    it (quite apart from the fact that again, most of the functions listed there aren't
    from our code, but from the libraries we're using).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With that being said, it looks like improving our code directly is not going
    to be a simple task. Instead, we might want to go on another route: since we''re
    doing a lot of counting, we might benefit from typed code. So, let''s try our
    hand at using Cython.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An initial analysis using the Cython command-line utility shows that most of
    our code can''t directly be translated into C, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzer](img/B02088_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot shows a portion of the analysis of our code. We can
    clearly see the darker lines filling most of the screen, showing that most of
    our code can't be directly translated into C. Sadly, this is because we're dealing
    with a complex object in most of our functions, so there isn't much we can do
    about it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Still, simply by compiling our code with Cython, we get much better results.
    So, let''s take a look at how we need to modify the source so that we can compile
    it with Cython. The first file is basically the same as the original analyzer
    with the changes highlighted in the code and minus the actual function calls,
    as we''re now turning it into an external library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following file is the one that takes care of setting everything up for
    Cython to compile our code, we''ve seen this code before (refer to [Chapter 6](ch06.html
    "Chapter 6. Generic Optimization Options"), *Generic Optimization Options*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The last file is the one that uses our new external library by importing the
    compiled module. The file calls on the `load_json_data` and `analyze_data` methods
    and, finally, uses the visualizer module to format the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code can be compiled using the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, by running the `analyzer-use-cython.py` script, we will get the following
    execution time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzer](img/B02088_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The time went down from 3.5 to 1.3 seconds. This is quite an improvement from
    simply reorganizing of our code and compiling it using Cython, like we saw in
    [Chapter 6](ch06.html "Chapter 6. Generic Optimization Options"), *Generic Optimization
    Options*. This simple compilation can produce great results.
  prefs: []
  type: TYPE_NORMAL
- en: The code can be further broken down and rewritten to remove most of the need
    for complex structures, thus allowing us to declare the primitive types for all
    variables. We could even try to remove `nltk` and use some NLP library written
    in C, such as OpenNLP ([http://opennlp.sourceforge.net/projects.html](http://opennlp.sourceforge.net/projects.html)).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You've reached the end of the chapter and, with it, the end of this book. The
    examples provided in this last chapter are meant to show how a random piece of
    code can be analyzed and improved using the techniques shown in the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: As not all techniques are compatible with each other, not all of them were applicable
    here. However, we were able to see how some of them work, more specifically, multithreading,
    profiling with `cProfile` and `kcachegrind`, and finally, compilation with Cython.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading and, hopefully, enjoying the book!
  prefs: []
  type: TYPE_NORMAL
