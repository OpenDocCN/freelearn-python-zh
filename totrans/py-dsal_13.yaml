- en: Implementations, Applications, and Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning about algorithms without any real-life application remains a purely
    academic pursuit. In this chapter, we will explore data structures and algorithms
    that are shaping our world.
  prefs: []
  type: TYPE_NORMAL
- en: One of the golden nuggets of this age is the abundance of data. E-mails, phone
    numbers, text, and image documents contain large amounts of data. In this data
    is found valuable information that makes the data become more important. But to
    extract this information from the raw data, we will have to use data structures,
    processes, and algorithms specialized for this task.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning employs a significant number of algorithms to analyze and predict
    the occurrence of certain variables. Analyzing data on a purely numerical basis
    still leaves much of the latent information buried in the raw data. Presenting
    data visually thus enables one to understand and gain valuable insights too.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you should be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Prune and present data accurately
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use both supervised and unsupervised learning algorithms for the purposes of
    prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visually represent data in order to gain more insight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools of the trade
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to proceed with this chapter, you will need to install the following
    packages. These packages will be used to preprocess and visually represent the
    data being processed. Some of the packages also contain well-written and perfected
    algorithms that will operate on our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Preferably, these modules should be installed within a virtual environment
    such as `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'These packages may require other platform-specific modules to be installed
    first. Take note and install all dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Numpy**: A library with functions to operate on n-dimensional arrays and
    matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scikit-learn**: A highly advanced module for machine learning. It contains
    a good number of algorithms for classification, regression, and clustering, among
    others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Matplotlib**: This is a plotting library that makes use of NumPy to graph
    a good variety of charts, including line plots, histograms, scatter plots, and
    even 3D graphs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pandas**: This library deals with data manipulation and analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Collection of data from the real world is fraught with massive challenges. The
    raw data collected is plagued with a lot of issues, so much so that we need to
    adopt ways to sanitize the data to make it suitable for use in further studies.
  prefs: []
  type: TYPE_NORMAL
- en: Why process raw data?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Raw data as collected from the field is rigged with human error. Data entry
    is a major source of error when collecting data. Even technological methods of
    collecting data are not spared. Inaccurate reading of devices, faulty gadgetry,
    and changes in environmental factors can introduce significant margins of errors
    as data is collected.
  prefs: []
  type: TYPE_NORMAL
- en: The data collected may also be inconsistent with other records collected over
    time. The existence of duplicate entries and incomplete records warrant that we
    treat the data in such a way as to bring out hidden and buried treasure. The raw
    data may also be shrouded in a sea of irrelevant data.
  prefs: []
  type: TYPE_NORMAL
- en: To clean the data up, we can totally discard irrelevant data, better known as
    noise. Data with missing parts or attributes can be replaced with sensible estimates.
    Also, where the raw data suffers from inconsistency, detecting and correcting
    them becomes necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Let us explore how we can use NumPy and pandas for data preprocessing techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Missing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data collection is tedious and, as such, once data is collected, it should not
    be easily discarded. Just because a dataset has missing fields or attributes does
    not mean it is not useful. Several methods can be used to fill up the nonexistent
    parts. One of these methods is by either using a global constant, using the mean
    value in the dataset, or supplying the data manually. The choice is based on the
    context and sensitivity of what the data is going to be used for.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take, for instance, the following data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the data elements `data[1][0]` and `data[1][1]` have values being
    `np.NAN`, representing the fact that they have no value. If the `np.NAN` values
    are undesired in a given dataset, they can be set to some constant figure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s set data elements with the value `np.NAN` to 0.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The new state of the data becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To apply the mean values instead, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The mean value for each column is calculated and inserted in those data areas
    with the `np.NAN` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: For the first column, column `0`, the mean value was obtained by `(4 + 94)/2`.
    The resulting `49.0` is then stored at `data[1][0]`. A similar operation is carried
    out for columns `1` and `2`.
  prefs: []
  type: TYPE_NORMAL
- en: Feature scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The columns in a data frame are known as its features. The rows are known as
    records or observations. Now examine the following data matrix. This data will
    be referenced in subsections so please do take note:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Feature 1, with data `58`, `10`, `20`, has its values lying between `10` and
    `58`. For feature 2, the data lies between `1` and `200`. Inconsistent results
    will be produced if we supply this data to any machine learning algorithm. Ideally,
    we will need to scale the data to a certain range in order to get consistent results.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, a closer inspection reveals that each feature (or column) lies around
    different mean values. Therefore, what we would want to do is to align the features
    around similar means.
  prefs: []
  type: TYPE_NORMAL
- en: One benefit of feature scaling is that it boosts the learning parts of machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: The `scikit` module has a considerable number of scaling algorithms that we
    shall apply to our data.
  prefs: []
  type: TYPE_NORMAL
- en: Min-max scalar
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The min-max scalar form of normalization uses the mean and standard deviation
    to box all the data into a range lying between a certain min and max value. For
    most purposes, the range is set between 0 and 1\. At other times, other ranges
    may be applied but the 0 to 1 range remains the default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'An instance of the `MinMaxScaler` class is created with the range `(0,1)` and
    passed to the variable `scaled_values`. The `fit` function is called to make the
    necessary calculations that will be used internally to change the dataset. The
    `transform` function effects the actual operation on the dataset, returning the
    value to `results`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We can see from the preceding output that all the data is normalized and lies
    between 0 and 1\. This kind of output can now be supplied to a machine learning
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Standard scalar
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The mean values for the respective features in our initial dataset or table
    are 29.3, 92, and 38\. To make all the data have a similar mean, that is, a zero
    mean and a unit variance across the data, we shall apply the standard scalar algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`data` is passed to the `fit` method of the object returned from instantiating
    the `StandardScaler` class. The `transform` method acts on the data elements in
    the data and returns the output to the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Examining the results, we observe that all our features are now evenly distributed.
  prefs: []
  type: TYPE_NORMAL
- en: Binarizing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To binarize a given feature set, we make use of a threshold. If any value within
    a given dataset is greater than the threshold, the value is replaced by 1\. If
    the value is less than the threshold 1, we will replace it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'An instance of `Binarizer` is created with the argument 50.0\. 50.0 is the
    threshold that will be used in the binarizing algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: All values in the data that are less than 50 will have 0 in their stead. The
    opposite also holds true.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is a subfield of artificial intelligence. We know that we can
    never truly create machines that actually "think" but we can supply machines with
    enough data and models by which sound judgment can be reached. Machine learning
    focuses on creating autonomous systems that can continue the process of decision
    making, with little or no human intervention.
  prefs: []
  type: TYPE_NORMAL
- en: In order to teach the machine, we need data drawn from the real world. For instance,
    to shift through which e-mails constitute spam and which ones don't, we need to
    feed the machine with samples of each. After obtaining this data, we have to run
    the data through models (algorithms) that will use probability and statistics
    to unearth patterns and structure from the data. If this is properly done, the
    algorithm by itself will be able to analyze e-mails and properly categorize them.
    Sorting e-mails is just one example of what machines can do if they are "trained".
  prefs: []
  type: TYPE_NORMAL
- en: Types of machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three broad categories of machine learning, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning**: Here, an algorithm is fed a set of inputs and their
    corresponding outputs. The algorithm then has to figure out what the output will
    be for an unfamiliar input. Examples of such algorithms include naive Bayes, linear
    regression, and decision tree algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised learning**: Without using the relationship that exists between
    a set of input and output variables, the unsupervised learning algorithm uses
    only the inputs to unearth groups, patterns, and clusters within the data. Examples
    of such algorithms include hierarchical clustering and k-means clustering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reinforcement learning**: The computer in this kind of learning dynamically
    interacts with its environment in such a way as to improve its performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hello classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To invoke the blessing of the programming gods in our quest to understand machine
    learning, we begin with an hello world example of a text classifier. This is meant
    to be a gentle introduction to machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: This example will predict whether a given text carries a negative or positive
    connotation. Before this can be done, we need to train our algorithm (model) with
    some data.
  prefs: []
  type: TYPE_NORMAL
- en: The naive Bayes model is suited for text classification purposes. Algorithms
    based on naive Bayes models are generally fast and produce accurate results. The
    whole model is based on the assumption that features are independent from each
    other. To accurately predict the occurrence of rainfall, three conditions need
    to be considered. These are wind speed, temperature, and the amount of humidity
    in the air. In reality, these factors do have an influence on each other to tell
    the likelihood of rainfall. But the abstraction in naive Bayes is to assume that
    these features are unrelated in any way and thus independently contribute to chances
    of rainfall. Naive Bayes is useful in predicting the class of an unknown dataset,
    as we will see soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now back to our hello classifier. After we have trained our mode, its prediction
    will fall into either the positive or negative category:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: First, we will import the `NaiveBayesClassifier` class from the `textblob` package.
    This classifier is very easy to work with and is based on the Bayes theorem.
  prefs: []
  type: TYPE_NORMAL
- en: The `train` variable consists of tuples that each holds the actual training
    data. Each tuple contains the sentence and the group it is associated with.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to train our model, we will instantiate a `NaiveBayesClassifier` object
    by passing the train to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The updated naive Bayesian model `cl` will predict the category that an unknown
    sentence belongs to. Up to this point, our model knows of only two categories
    that a phrase can belong to, `neg` and `pos`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code runs the following tests using our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of our test is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the algorithm has had some degree of success in classifying
    the input phrases into their categories well.
  prefs: []
  type: TYPE_NORMAL
- en: This contrived example is overly simplistic but it does show promise that if
    given the right amounts of data and a suitable algorithm or model, it is possible
    for a machine to carry out tasks without any human help.
  prefs: []
  type: TYPE_NORMAL
- en: The specialized class `NaiveBayesClassifier` also did some heavy lifting for
    us in the background so we could not appreciate the innards by which the algorithm
    arrived at the various predictions. Our next example will use the `scikit` module
    to predict the category that a phrase may belong to.
  prefs: []
  type: TYPE_NORMAL
- en: A supervised learning example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Assume that we have a set of posts to categorize. As with supervised learning,
    we need to first train the model in order for it to accurately predict the category
    of an unknown post.
  prefs: []
  type: TYPE_NORMAL
- en: Gathering data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `scikit` module comes with a number of sample data we will use for training
    our model. In this case, we will use the newsgroups posts. To load the posts,
    we will use the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'After we have trained our model, the results of a prediction must belong to
    one of the following categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The number of records we are going to use as training data is obtained by the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Machine learning algorithms do not mix well with textual attributes so the
    categories that each post belongs to are presented as numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The categories have integer values that we can map back to the categories themselves
    with `print(training_data.target_names[0])`.
  prefs: []
  type: TYPE_NORMAL
- en: Here, 0 is a numerical random index picked from `set(training_data.target)`.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the training data has been obtained, we must feed the data to a machine
    learning algorithm. The bag of words model will break down the training data in
    order to make it ready for the learning algorithm or model.
  prefs: []
  type: TYPE_NORMAL
- en: Bag of words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The bag of words is a model that is used for representing text data in such
    a way that it does not take into consideration the order of words but rather uses
    word counts to segment words into regions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the following sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The bag of words enables us to decompose text into numerical feature vectors
    represented by a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reduce our two sentences into the bag of words model, we need to obtain
    a unique list of all the words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This set will become our columns in the matrix. The rows in the matrix will
    represent the documents that are being used in training. The intersection of a
    row and column will store the number of times that word occurs in the document.
    Using our two sentences as examples, we obtain the following matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **As** | **Fit** | **A** | **Fiddle** | **You** | **Like** | **it** |'
  prefs: []
  type: TYPE_TB
- en: '| **Sentence 1** | 2 | 1 | 1 | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Sentence 2** | 1 | 0 | 0 | 0 | 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: The preceding data alone will not enable us to predict accurately the category
    that new documents or articles will belong to. The table has some inherent flaws.
    There may be situations where longer documents or words that occur in many of
    the posts reduce the precision of the algorithm. Stop words can be removed to
    make sure only relevant data is analyzed. Stop words include is, are, was, and
    so on. Since the bag of words model does not factor grammar into its analysis,
    the stop words can safely be dropped. It is also possible to add to the list of
    stop words that one feels should be exempted from final analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate the values that go into the columns of our matrix, we have to tokenize
    our training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The `training_matrix` has a dimension of (2257, 35788). This means that 2257
    corresponds to the dataset while 35788 corresponds to the number of columns that
    make up the unique set of words in all posts.
  prefs: []
  type: TYPE_NORMAL
- en: We instantiate the `CountVectorizer` class and pass the `training_data.data`
    to the `fit_transform` method of the `count_vect` object. The result is stored
    in `training_matrix`. The `training_matrix` holds all the unique words and their
    respective frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'To mitigate the problem of basing prediction on frequency count alone, we will
    import the `TfidfTransformer` that helps to smooth out the inaccuracies in our
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`tfidf_data[1:4].todense()` only shows a truncated list of a three rows by
    35,788 columns matrix. The values seen are the term frequency--inverse document
    frequency that reduce the inaccuracy resulting from using a frequency count:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`MultinomialNB` is a variant of the naive Bayes model. We pass the rationalized
    data matrix, `tfidf_data` and categories, `training_data.target`, to its `fit`
    method.'
  prefs: []
  type: TYPE_NORMAL
- en: Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To test whether our model has learned enough to predict the category that an
    unknown post is likely to belong to, we have the following sample data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The list `test_data` is passed to the `count_vect.transform` function to obtain
    the vectorized form of the test data. To obtain the term frequency--inverse document
    frequency representation of the test dataset, we call the `transform` method of
    the `matrix_transformer` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'To predict which category the docs may belong to, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The loop is used to iterate over the prediction, showing the categories they
    are predicted to belong to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'When the loop has run to completion, the phrase, together with the category
    that it may belong to, is displayed. A sample output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: All that we have seen up to this point is a prime example of supervised learning.
    We started by loading posts whose categories are already known. These posts were
    then fed into the machine learning algorithm most suited for text processing based
    on the naive Bayes theorem. A set of test post fragments were supplied to the
    model and the category was predicted.
  prefs: []
  type: TYPE_NORMAL
- en: To explore an example of an unsupervised learning algorithm, we shall study
    the k-means algorithm for clustering some data.
  prefs: []
  type: TYPE_NORMAL
- en: An unsupervised learning example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A category of learning algorithms is able to discover inherent groups that may
    exist in a set of data. An example of these algorithms is the k-means algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: K-means algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The k-means algorithm uses the mean points in a given dataset to cluster and
    discover groups within the dataset. K is the number of clusters that we want and
    are hoping to discover. After the k-means algorithm has generated the groupings,
    we can pass it additional but unknown data for it to predict to which group it
    will belong.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in this kind of algorithm, only the raw uncategorized data is fed
    to the algorithm. It is up to the algorithm to find out if the data has inherent
    groups within it.
  prefs: []
  type: TYPE_NORMAL
- en: To understand how this algorithm works, we will examine 100 data points consisting
    of x and y values. We will feed these values to the learning algorithm and expect
    that the algorithm will cluster the data into two sets. We will color the two
    sets so that the clusters are visible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a sample data of 100 records of *x* and *y* pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: First, we create 100 records with `-2 * np.random.rand(100, 2)`. In each of
    the records, we will use the data in it to represent x and y values that will
    eventually be plotted.
  prefs: []
  type: TYPE_NORMAL
- en: The last 50 numbers in `original_set` will be replaced by `1 + 2 * np.random.rand(50,
    2)`. In effect, what we have done is to create two subsets of data, where one
    set has numbers in the negative while the other set has numbers in the positive.
    It is now the responsibility of the algorithm to discover these segments appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: 'We instantiate the `KMeans` algorithm class and pass it `n_clusters=2`. That
    makes the algorithm cluster all its data under only two groups. It is through
    a series of trial and error that this figure, `2`, is obtained. But for academic
    purposes, we already know this number. It is not at all obvious when working with
    unfamiliar datasets from the real world:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The dataset is passed to the `fit` function of `kmean`, `kmean.fit(original_set)`.
    The clusters generated by the algorithm will revolve around a certain mean point.
    The points that define these two mean points are obtained by `kmean.cluster_centers_`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mean points when printed appear as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Each data point in `original_set` will belong to a cluster after our k-means
    algorithm has finished its training. The k-mean algorithm represents the two clusters
    it discovers as 1s and 0s. If we had asked the algorithm to cluster the data into
    four, the internal representation of these clusters would have been 0, 1, 2, and
    3\. To print out the various clusters that each dataset belongs to, we do the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'There are 100 1s and 0s. Each shows the cluster that each data point falls
    under. By using `matplotlib.pyplot`, we can chart the points of each group and
    color it appropriately to show the clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '`index = kmean.labels_ == i` is a nifty way by which we select all points that
    correspond to the group `i`. When `i=0`, all points belonging to the group 0 are
    returned to index. It''s the same for `index =1, 2` ... , and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: '`plt.plot(original_set[index,0], original_set[index,1], ''o'')` then plots
    these data points using o as the character for drawing each point.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will plot the centroids or mean values around which the clusters have
    formed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we show the whole graph with the two means illustrated by a star:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/image_12_001.png)'
  prefs: []
  type: TYPE_IMG
- en: The algorithm discovers two distinct clusters in our sample data. The two mean
    points of the two clusters are denoted with the red star symbol.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the two clusters that we have obtained, we can predict the group that a
    new set of data might belong to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s predict which group the points `[[-1.4, -1.4]]` and `[[2.5, 2.5]]` will
    belong to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is seen as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: At the barest minimum, we can expect the two test datasets to belong to different
    clusters. Our expectation is proved right when the `print` statement prints 1
    and 0, thus confirming that our test data does indeed fall under two different
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Data visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Numerical analysis does not sometimes lend itself to easy understanding. Indeed,
    a single image is worth 1,000 words and in this section, an image would be worth
    1,000 tables comprised of numbers only. Images present a quick way to analyze
    data. Differences in size and lengths are quick markers in an image upon which
    conclusions can be drawn. In this section, we will take a tour of the different
    ways to represent data. Besides the graphs listed here, there is more that can
    be achieved when chatting data.
  prefs: []
  type: TYPE_NORMAL
- en: Bar chart
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To chart the values 25, 5, 150, and 100 into a bar graph, we will store the
    values in an array and pass it to the `bar` function. The bars in the graph represent
    the magnitude along the *y*-axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '`x_values` stores an array of values generated by `range(len(data))`. Also,
    `x_values` will determine the points on the *x*-axis where the bars will be drawn.
    The first bar will be drawn on the *x*-axis where x is 0\. The second bar with
    data 5 will be drawn on the *x*-axis where x is 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The width of each bar can be changed by modifying the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_003.png)'
  prefs: []
  type: TYPE_IMG
- en: However, this is not visually appealing because there is no space anymore between
    the bars, which makes it look clumsy. Each bar now occupies one unit on the *x*-axis.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple bar charts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In trying to visualize data, stacking a number of bars enables one to further
    understand how one piece of data or variable varies with another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The y values for the first batch of data are `[8., 57., 22., 10.]`. The second
    batch is `[16., 7., 32., 40.]`. When the bars are plotted, 8 and 16 will occupy
    the same x position, side by side.
  prefs: []
  type: TYPE_NORMAL
- en: '`x_values = np.arange(4)` generates the array with values `[0, 1, 2, 3]`. The
    first set of bars are drawn first at position `x_values + 0.30`. Thus, the first
    x values will be plotted at `0.00, 1.00, 2.00 and 3.00`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second batch of `x_values` will be plotted at `0.30, 1.30, 2.30 and 3.30`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_004.png)'
  prefs: []
  type: TYPE_IMG
- en: Box plot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The box plot is used to visualize the median value and low and high ranges of
    a distribution. It is also referred to as a box and whisker plot.
  prefs: []
  type: TYPE_NORMAL
- en: Let's chart a simple box plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by generating 50 numbers from a normal distribution. These are then
    passed to `plt.boxplot(data)` to be charted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure is what is produced:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A few comments on the preceding figure: the features of the box plot include
    a box spanning the interquartile range, which measures the dispersion; the outer
    fringes of the data are denoted by the whiskers attached to the central box; the
    red line represents the median.'
  prefs: []
  type: TYPE_NORMAL
- en: The box plot is useful to easily identify the outliers in a dataset as well
    as determining in which direction a dataset may be skewed.
  prefs: []
  type: TYPE_NORMAL
- en: Pie chart
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The pie chart interprets and visually presents data as if to fit into a circle.
    The individual data points are expressed as sectors of a circle that add up to
    360 degrees. This chart is good for displaying categorical data and summaries
    too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The sectors in the graph are labeled with the strings in the labels array:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_006.png)'
  prefs: []
  type: TYPE_IMG
- en: Bubble chart
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another variant of the scatter plot is the bubble chart. In a scatter plot,
    we only plot the x, y points of the data. Bubble charts add another dimension
    by illustrating the size of the points. This third dimension may represent sizes
    of markets or even profits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: With the variable `n`, we specify the number of randomly generated x and y values.
    This same number is used to determine the random colors for our x and y coordinates.
    Random bubble sizes are determined by `area = np.pi * (60 * np.random.rand(n))**2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows this bubble chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05630_12_07.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have explored how data and algorithms come together to aid
    machine learning. Making sense of huge amounts of data is made possible by first
    pruning our data through normalization processes. Feeding this data to specialized
    algorithms, we are able to predict the categories and sets that our data will
    fall into.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, charting and plotting the condensed data helps to better understand
    and make insightful discoveries.
  prefs: []
  type: TYPE_NORMAL
