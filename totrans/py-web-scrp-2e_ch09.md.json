["```py\n>>> from lxml.html import fromstring\n>>> import requests \n>>> html = requests.get('https://www.google.com/search?q=test') \n>>> tree = fromstring(html.content) \n>>> results = tree.cssselect('h3.r a') \n>>> results \n[<Element a at 0x7f3d9affeaf8>, \n <Element a at 0x7f3d9affe890>, \n <Element a at 0x7f3d9affe8e8>, \n <Element a at 0x7f3d9affeaa0>, \n <Element a at 0x7f3d9b1a9e68>, \n <Element a at 0x7f3d9b1a9c58>, \n <Element a at 0x7f3d9b1a9ec0>, \n <Element a at 0x7f3d9b1a9f18>, \n <Element a at 0x7f3d9b1a9f70>, \n <Element a at 0x7f3d9b1a9fc8>] \n\n```", "```py\n>>> link = results[0].get('href') \n>>> link \n'/url?q=http://www.speedtest.net/&sa=U&ved=0ahUKEwiCqMHNuvbSAhXD6gTMAA&usg=AFQjCNGXsvN-v4izEgZFzfkIvg' \n\n```", "```py\n>>> from urllib.parse import parse_qs, urlparse \n>>> qs = urlparse(link).query \n>>> parsed_qs = parse_qs(qs)\n>>> parsed_qs\n{'q': ['http://www.speedtest.net/'], \n 'sa': ['U'], \n 'ved': ['0ahUKEwiCqMHNuvbSAhXD6gTMAA'], \n 'usg': ['AFQjCNGXsvN-v4izEgZFzfkIvg']}\n>>> parsed_qs.get('q', []) \n['http://www.speedtest.net/']\n\n```", "```py\n>>> links = [] \n>>> for result in results: \n...     link = result.get('href') \n...     qs = urlparse(link).query \n...     links.extend(parse_qs(qs).get('q', [])) \n... \n>>> links \n['http://www.speedtest.net/', \n'test', \n'https://www.test.com/', \n'https://ro.wikipedia.org/wiki/Test', \n'https://en.wikipedia.org/wiki/Test', \n'https://www.sri.ro/verificati-va-aptitudinile-1', \n'https://www.sie.ro/AgentiaDeSpionaj/test-inteligenta.html', 'http://www.hindustantimes.com/cricket/india-vs-australia-live-cricket-score-4th-test-dharamsala-day-3/story-8K124GMEBoiKOgiAaaB5bN.html', \n'https://sports.ndtv.com/india-vs-australia-2017/live-cricket-score-india-vs-australia-4th-test-day-3-dharamsala-1673771', \n'http://pearsonpte.com/test-format/'] \n\n```", "```py\nfrom selenium import webdriver\n\ndef get_driver():\n    try:\n        return webdriver.PhantomJS()\n    except:\n        return webdriver.Firefox()\n\ndef facebook(username, password, url):\n    driver = get_driver()\n    driver.get('https://facebook.com')\n    driver.find_element_by_id('email').send_keys(username)\n    driver.find_element_by_id('pass').send_keys(password)\n    driver.find_element_by_id('loginbutton').submit()\n    driver.implicitly_wait(30)\n    # wait until the search box is available,\n    # which means it has successfully logged in\n    search = driver.find_element_by_name('q')\n    # now logged in so can go to the page of interest\n    driver.get(url)\n    # add code to scrape data of interest here ...\n\n```", "```py\npip install facebook-sdk\n\n```", "```py\nIn [1]: from facebook import GraphAPI\n\nIn [2]: access_token = '....'  # insert your actual token here\n\nIn [3]: graph = GraphAPI(access_token=access_token, version='2.7')\n\nIn [4]: graph.get_object('PacktPub')\nOut[4]: {'id': '204603129458', 'name': 'Packt'}\n\n```", "```py\nIn [5]: graph.get_object('PacktPub', fields='about,events,feed,picture')\nOut[5]: \n{'about': 'Packt provides software learning resources, from eBooks to video courses, to everyone from web developers to data scientists.',\n 'feed': {'data': [{'created_time': '2017-03-27T10:30:00+0000',\n 'id': '204603129458_10155195603119459',\n 'message': \"We've teamed up with CBR Online to give you a chance to win 5 tech eBooks - enter by March 31! http://bit.ly/2mTvmeA\"},\n...\n 'id': '204603129458',\n 'picture': {'data': {'is_silhouette': False,\n 'url': 'https://scontent.xx.fbcdn.net/v/t1.0-1/p50x50/14681705_10154660327349459_72357248532027065_n.png?oh=d0a26e6c8a00cf7e6ce957ed2065e430&oe=59660265'}}}\n\n```", "```py\nSitemap: http://www.gap.com/products/sitemap_index.xml \n\n```", "```py\n<?xml version=\"1.0\" encoding=\"UTF-8\"?> \n<sitemapindex xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"> \n    <sitemap> \n        <loc>http://www.gap.com/products/sitemap_1.xml</loc> \n        <lastmod>2017-03-24</lastmod> \n    </sitemap> \n    <sitemap> \n        <loc>http://www.gap.com/products/sitemap_2.xml</loc> \n        <lastmod>2017-03-24</lastmod> \n    </sitemap> \n</sitemapindex> \n\n```", "```py\nfrom lxml import etree \nfrom threaded_crawler import threaded_crawler \n\ndef scrape_callback(url, html): \n    if url.endswith('.xml'): \n        # Parse the sitemap XML file \n        tree = etree.fromstring(html) \n        links = [e[0].text for e in tree] \n        return links \n    else: \n        # Add scraping code here \n        pass \n\n```", "```py\nIn [1]: from chp9.gap_scraper_callback import scrape_callback\n\nIn [2]: from chp4.threaded_crawler import threaded_crawler\n\nIn [3]: sitemap = 'http://www.gap.com/products/sitemap_index.xml'\n\nIn [4]: threaded_crawler(sitemap, '[gap.com]*', scraper_callback=scrape_callback)\n10\n[<Thread(Thread-517, started daemon 140145732585216)>]\nException in thread Thread-517:\nTraceback (most recent call last):\n...\n File \"src/lxml/parser.pxi\", line 1843, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:118282)\nValueError: Unicode strings with encoding declaration are not supported. Please use bytes input or XML fragments without declaration.\n\n```", "```py\nimport requests\n\ndef scrape_callback(url, html): \n    if url.endswith('.xml'): \n        # Parse the sitemap XML file \n        resp = requests.get(url)\n        tree = etree.fromstring(resp.content) \n        links = [e[0].text for e in tree] \n        return links \n    else: \n        # Add scraping code here \n        pass\n\n```", "```py\nIn [4]: threaded_crawler(sitemap, '[gap.com]*', scraper_callback=scrape_callback)\n10\n[<Thread(Thread-51, started daemon 139775751223040)>]\nDownloading: http://www.gap.com/products/sitemap_index.xml \nDownloading: http://www.gap.com/products/sitemap_2.xml \nDownloading: http://www.gap.com/products/gap-canada-français-index.jsp \nDownloading: http://www.gap.co.uk/products/index.jsp \nSkipping http://www.gap.co.uk/products/low-impact-sport-bras-women-C1077315.jsp due to depth Skipping http://www.gap.co.uk/products/sport-bras-women-C1077300.jsp due to depth \nSkipping http://www.gap.co.uk/products/long-sleeved-tees-tanks-women-C1077314.jsp due to depth Skipping http://www.gap.co.uk/products/short-sleeved-tees-tanks-women-C1077312.jsp due to depth ...\n\n```", "```py\nhttps://c2b-services.bmw.com/c2b-localsearch/services/api/v3/ \n    clients/BMWDIGITAL_DLO/DE/ \n        pois?country=DE&category=BM&maxResults=99&language=en& \n            lat=52.507537768880056&lng=13.425269635701511 \n\n```", "```py\n>>> import requests\n>>> url = 'https://c2b-services.bmw.com/c2b-localsearch/services/api/v3/clients/BMWDIGITAL_DLO/DE/pois?country=DE&category=BM&maxResults=%d&language=en&         lat=52.507537768880056&lng=13.425269635701511' \n>>> jsonp = requests.get(url % 1000) \n>>> jsonp.content \n'callback({\"status\":{ \n... \n})' \n\n```", "```py\n>>> import json \n>>> pure_json = jsonp.text[jsonp.text.index('(') + 1 : jsonp.text.rindex(')')] \n>>> dealers = json.loads(pure_json) \n>>> dealers.keys() \ndict_keys(['status', 'translation', 'metadata', 'data', 'count'])\n>>> dealers['count'] \n715 \n\n```", "```py\n>>> dealers['data']['pois'][0] \n{'attributes': {'businessTypeCodes': ['NO', 'PR'],\n 'distributionBranches': ['T', 'F', 'G'],\n 'distributionCode': 'NL',\n 'distributionPartnerId': '00081',\n 'facebookPlace': '',\n 'fax': '+49 (30) 200992110',\n 'homepage': 'http://bmw-partner.bmw.de/niederlassung-berlin-weissensee',\n 'mail': 'nl.berlin@bmw.de',\n 'outletId': '3',\n 'outletTypes': ['FU'],\n 'phone': '+49 (30) 200990',\n 'requestServices': ['RFO', 'RID', 'TDA'],\n 'services': ['EB', 'PHEV']},\n 'category': 'BMW',\n 'city': 'Berlin',\n 'country': 'Germany',\n 'countryCode': 'DE',\n 'dist': 6.662869863289401,\n 'key': '00081_3',\n 'lat': 52.562568863415,\n 'lng': 13.463589476607,\n 'name': 'BMW AG Niederlassung Berlin Filiale Weißensee',\n 'oh': None,\n 'postalCode': '13088',\n 'postbox': None,\n 'state': None,\n 'street': 'Gehringstr. 20'}\n\n```", "```py\nwith open('../../data/bmw.csv', 'w') as fp: \n    writer = csv.writer(fp) \n    writer.writerow(['Name', 'Latitude', 'Longitude']) \n    for dealer in dealers['data']['pois']: \n        name = dealer['name'] \n        lat, lng = dealer['lat'], dealer['lng'] \n        writer.writerow([name, lat, lng]) \n\n```", "```py\nName,Latitude,Longitude \nBMW AG Niederlassung Berlin Filiale Weissensee,52.562568863415,13.463589476607 \nAutohaus Graubaum GmbH,52.4528925,13.521265 \nAutohaus Reier GmbH & Co. KG,52.56473,13.32521 \n... \n\n```"]