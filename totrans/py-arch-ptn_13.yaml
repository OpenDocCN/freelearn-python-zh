- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Testing and TDD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No matter how good a developer is, they'll write code that doesn't always perform
    correctly. This is unavoidable, as no developer is perfect. But it's also because
    the expected results are sometimes not the ones that one would think of while
    immersed in coding.
  prefs: []
  type: TYPE_NORMAL
- en: Designs rarely go as expected and there's always a discussion going back and
    forth while they are being implemented, until refining them and getting them correct.
  prefs: []
  type: TYPE_NORMAL
- en: Everyone has a plan until they get punched in the mouth. – Mike Tyson
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Writing software is notoriously difficult because of its extreme plasticity,
    but at the same time, we can use software to double-check that the code is doing
    what it is supposed to do.
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that, as with any other code, tests can have bugs as well.
  prefs: []
  type: TYPE_NORMAL
- en: Writing tests allows you to detect problems while the code is fresh and with
    some sane skepticism to verify that the expected results are the actual results.
    We will see during the chapter how to write tests easily, as well as different
    strategies to write different tests for capturing different kinds of problems.
  prefs: []
  type: TYPE_NORMAL
- en: We will describe how to work under TDD, a methodology that works by defining
    the tests first, to ensure that the validation is as independent of the actual
    code implementation as possible.
  prefs: []
  type: TYPE_NORMAL
- en: We will also show how to create tests in Python using common unit test frameworks,
    the standard `unittest` module, and the more advanced and powerful `pytest`.
  prefs: []
  type: TYPE_NORMAL
- en: Note this chapter is a bit longer than others, mostly due to the need to present
    example code.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Testing the code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different levels of testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing philosophy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test-Driven Development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to unit testing in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing external dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced pytest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with some basic concepts about testing.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first question when discussing testing the code is a simple one: What exactly
    do we mean by testing the code?'
  prefs: []
  type: TYPE_NORMAL
- en: While there are multiple answers to that, in the broadest sense, the answer
    could be "*any procedure that probes the application to check that it works correctly
    before it reaches the final customers.*" In this sense, any formal or informal
    testing procedure will fulfil the definition.
  prefs: []
  type: TYPE_NORMAL
- en: The most relaxed approach, which is sometimes seen in small applications with
    one or two developers, is to not create specific tests but to do informal "full
    application runs" checking that a newly implemented feature works as expected.
  prefs: []
  type: TYPE_NORMAL
- en: This approach may work for small, simple applications, but the main problem
    is ensuring that older features remain stable.
  prefs: []
  type: TYPE_NORMAL
- en: 'But, for high-quality software that is big and complex enough, we need to be
    a bit more careful about the testing. So, let''s try to come up with a more precise
    definition of testing: *Testing is any documented procedure, preferably automated,
    that, from a known setup, checks the different elements of the application work
    correctly before it reaches the final customers.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we check the differences with the previous definition, there are several
    key words. Let''s check each of them to see the different details:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Documented**: Compared with the previous version, the aim should be that
    the tests are documented. This allows you to reproduce them precisely if necessary
    and allows you to compare them to discover blind spots.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are multiple ways that a test can be documented, either by specifying
    a list of steps to run and expected results or by creating code that runs the
    test. The main idea is that a test can be analyzed, be run several times by different
    people, be changed if necessary, and have a clear design and result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Preferably automated**:Testsshould be able to be run automatically, with
    as little human intervention as possible. This allows you to trigger Continuous
    Integration techniques to run many tests over and over, creating a "safety net"
    that is able to catch unexpected errors as early as possible. We say "preferably"
    because perhaps some tests are impossible or very costly to totally automate.
    In any case, the objective should be to have the vast majority of tests automated,
    to allow computers to do the heavy lifting and save precious human time. There
    are also multiple software tools that allow you to run tests, which can help.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**From a known setup**: To be able to run tests in isolation, we need to know
    what the status of the system should be before running the test. That ensures
    that the result of a test will not create a certain state that could interfere
    with the next test. Before and after a test, certain cleanup may be required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can make running tests in batches slower, compared with not worrying about
    the initial or end status, but it will create a solid foundation to avoid problems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As a general rule, and especially in automated tests, the order in which the
    tests are executed should be irrelevant, to avoid cross-contamination. This is
    easier said than done, and in some cases, the order of tests can create problems.
    For example, test A creates an entry that test B reads. If test B is run in isolation,
    it will fail as it expects the entry created by A. These cases should be fixed,
    as they can greatly complicate debugging. Also, being able to run tests independently
    allows them to be parallelized.
  prefs: []
  type: TYPE_NORMAL
- en: '**Different elements of the application**: Most tests should not address the
    whole application, but smaller parts of it. We will talk more later about the
    different levels of testing, but tests should be specific about what are they
    testing and cover different elements, as tests covering more ground will be costlier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A key element of testing is to have a good return on investment. Designing and
    running tests takes time, and that time needs to be well spent. Any test needs
    to be maintained, which should be worth it. Over the whole chapter, we will be
    commenting on this important aspect of testing.
  prefs: []
  type: TYPE_NORMAL
- en: There's an important kind of testing that we are not covering with this definition,
    which is called *exploratory testing*. These tests are typically run by QA engineers,
    who use the final application without a clear preconceived idea but try to pre-emptively
    find problems. If the application has a customer-facing UI, this style of testing
    can be invaluable in detecting inconsistencies and problems that are not detected
    in the design phase.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a good QA engineer will be able to say that the color of a button
    on page X is not the same as the button on page Y, or that the button is not evident
    enough to perform an action, or that to perform a certain action there's a prerequisite
    that's not evident or possible with the new interface. Any **user experience**
    (**UX**) check will probably fall into this category.
  prefs: []
  type: TYPE_NORMAL
- en: By its nature, this kind of testing cannot be "designed" or "documented," as
    it ultimately comes down to interpretation and a good eye to understand whether
    the application *feels correct*. Once a problem is detected, then it can be documented
    to be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: While this is certainly useful and recommended, this style of testing is more
    an art than an engineering practice and we won't be discussing it in detail.
  prefs: []
  type: TYPE_NORMAL
- en: This general definition helps to start the discussion, but we can be more concrete
    about the different tests defined by how much of the system is under test, during
    each test.
  prefs: []
  type: TYPE_NORMAL
- en: Different levels of testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we described before, tests should cover different elements of the system.
    This means that a test can address a small or big part of the system (or the whole
    system), trying to reduce its range of action.
  prefs: []
  type: TYPE_NORMAL
- en: When testing a small part of the system, we reduce the complexity of the test
    and scope. We need to call only that small part of the system, and the setup is
    easier to start with. In general, the smaller the element to test, the faster
    and easier it is to test it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will define three different levels or kinds of tests, from small to big
    scopes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unit tests**, for tests that check only part of a service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration tests**, for tests that check a single service as a whole'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**System tests**, for tests that check multiple services working together'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Names can actually vary quite a lot. In this book, we won't be very strict with
    definitions, instead defining soft limits and suggesting finding a balance that
    works for your specific project. Don't be shy to take decisions on the proper
    level for each test and define your own nomenclature, and always keep in mind
    how much effort it takes to create tests to be sure that they are always worth
    it.
  prefs: []
  type: TYPE_NORMAL
- en: The definition of the levels can be a little blurred. For example, integration
    and unit tests can be defined side by side, and the difference between them could
    be more academic in that case.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start describing each of the levels in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The smallest kind of test is also the one where most effort is typically invested,
    the *unit test*. This kind of test checks the behavior of a small unit of code,
    not the whole system. This unit of code could be as small as a single function
    or test a single API endpoint, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: As we said above, there's a lot of debate on how big a unit test should actually
    be, based on what the "unit" is and whether it is actually a unit. For example,
    in some cases, people will only call a test a unit test if it involves a single
    function or class.
  prefs: []
  type: TYPE_NORMAL
- en: Because a unit test checks a small part of the functionality, it can be very
    easy to set up and quick to run. Therefore, making new unit tests is quick and
    can thoroughly test the system, checking that the small individual pieces that
    make the whole system work as expected.
  prefs: []
  type: TYPE_NORMAL
- en: The objective of unit tests is to check in depth the behavior of a defined feature
    of a service. Any external requests or elements should be simulated, meaning that
    they are defined as part of the test. We will cover unit tests in more detail
    later in the chapter, as they are the key elements of the TDD approach.
  prefs: []
  type: TYPE_NORMAL
- en: Integration tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next level is the integration test. This is checking the whole behavior
    of a service or a couple of services.
  prefs: []
  type: TYPE_NORMAL
- en: The main goal of integration testing is to be sure that the different services
    or different modules inside the same service can work with each other. While in
    unit tests, external requests are simulated, integration tests use the real service.
  prefs: []
  type: TYPE_NORMAL
- en: The simulation of external APIs may still be required. For example, simulating
    an external payment provider for the tests. But, in general, as many real services
    should be used for integration tests as possible, as the point of the test is
    to test that the different services work together.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to note that, commonly, different services will be developed
    by different developers or even different teams, and they can diverge in their
    understanding of how a particular API is implemented, even in the event of a well-defined
    spec.
  prefs: []
  type: TYPE_NORMAL
- en: The setup in integration tests is more complex than in unit tests, as more elements
    need to be properly set up. This makes integration tests slower and more expensive
    than unit tests.
  prefs: []
  type: TYPE_NORMAL
- en: Integration tests are great to check that different services work in unison,
    but there are some limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Integration tests are normally not as thorough as unit tests, focusing on checking
    basic functionality and following a *happy path*. A happy path is a concept in
    testing meaning that the test case should produce no errors or exceptions.
  prefs: []
  type: TYPE_NORMAL
- en: Expected errors and exceptions are normally tested in unit tests, since they
    are also elements that can fail. That doesn't mean that every single integration
    test should follow a happy path; some integration errors may be worth checking,
    but in general, a happy path tests the expected general behavior of the feature.
    They will compose the bulk of the integration tests.
  prefs: []
  type: TYPE_NORMAL
- en: System tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final level is the system level. System tests check that all the different
    services work correctly together.
  prefs: []
  type: TYPE_NORMAL
- en: A requirement for this kind of test is that there are actually multiple services
    in the system. If not, they are not different from tests at the lower levels.
    The main objective of these tests is to check that the different services can
    cooperate, and the configuration is correct.
  prefs: []
  type: TYPE_NORMAL
- en: System tests are slow and difficult to implement. They require the whole system
    to be set up, with all the different services properly configured. Creating that
    environment can be complicated. Sometimes, it's so difficult that the only way
    of actually performing any system tests is to run them in the live environment.
  prefs: []
  type: TYPE_NORMAL
- en: The environment configuration is an important part of what these tests check.
    That may make them important to run on each environment that is under test, including
    the live environment.
  prefs: []
  type: TYPE_NORMAL
- en: While this is not ideal, sometimes it is unavoidable and can help to improve
    confidence after deployments, to ensure that the new code is working correctly.
    In that case, given the constraints, only a minimum amount of tests should be
    run, as the live environment is critical. The tests to run should also exercise
    the maximum amount of common functionality and services to detect any critical
    problem as fast as possible. This set of tests is sometimes called *acceptance
    tests* or *smoke tests*. They may be run manually, as a way of ensuring that everything
    looks correct.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, smoke tests can be run not only on the live environment and can work
    as a way to ensure that other environments are working correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Smoke tests should be very clear, well documented, and designed carefully to
    cover the most critical parts of the whole system. Ideally, they should also be
    read-only, so they don't leave useless data after their execution.
  prefs: []
  type: TYPE_NORMAL
- en: Testing philosophy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A key element of everything involved with testing is another question: *Why
    test?* What are we trying to achieve with it?'
  prefs: []
  type: TYPE_NORMAL
- en: As we've seen, testing is a way of ensuring that the behavior of the code is
    the expected one. The objective of testing is to detect possible problems (sometimes
    called *defects*) before the code is published and used by real users.
  prefs: []
  type: TYPE_NORMAL
- en: There's a subtle difference between *defects* and *bugs*. Bugs are a kind of
    defect where the software behaves in a way that it's not expected to. For example,
    certain input produces an unexpected error. Defects are more general. A defect
    could be that a button is not visible enough, or that the logo on a page is not
    the correct one. In general, tests are way better at detecting bugs than other
    defects, but remember what we said about exploratory testing.
  prefs: []
  type: TYPE_NORMAL
- en: A defect that goes undetected and gets deployed into a live system is pretty
    expensive to repair. First of all, it needs to be detected. In a live application
    with a lot of activity, detecting a problem can be difficult (though we will talk
    about it in *Chapter 16*, *Ongoing Architecture*), but even worse, it will normally
    be detected by a user of the system using the application. It's possible that
    the user won't properly communicate the problem back, so the problem is still
    present, creating problems or limiting activity. The detecting user might abandon
    the system, or at the very least their confidence in the system will decrease.
  prefs: []
  type: TYPE_NORMAL
- en: Any reputational cost will be bad, but it can also be difficult to extract enough
    information from the user to know exactly what happened and how to fix it. This
    makes the cycle between detecting the problem and fixing it long.
  prefs: []
  type: TYPE_NORMAL
- en: Any testing system will improve the ability to fix defects earlier. Not only
    can we create a specific test that simulates exactly the same problem, but we
    can also create a framework that executes tests regularly to have a clear approach
    to how to detect and fix problems.
  prefs: []
  type: TYPE_NORMAL
- en: Different testing levels have different effects on this cost. In general, any
    problem that can be detected at the unit test level is going to be cheaper to
    fix there, and the cost increases from there. Designing and running a unit test
    is easier and faster than doing the same with an integration test, and an integration
    test is cheaper than a system test.
  prefs: []
  type: TYPE_NORMAL
- en: The different test levels could be understood as different layers capturing
    possible problems. Each layer will capture different problems if they appear.
    The closer to the start of the process (design and unit tests while coding), the
    cheaper it is to create a dense net that will detect and alert for problems. The
    cost of fixing a problem increases the farther away it is from the controlled
    environment at the start of the process.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17580_10_01.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.1: The cost of fixing defects increases the later they get detected'
  prefs: []
  type: TYPE_NORMAL
- en: Some defects are impossible to detect at the unit test level, like the integration
    of different parts. That's where the next level comes into play. As we've seen,
    the worst scenario is not detecting a problem and it affecting real users on the
    live system.
  prefs: []
  type: TYPE_NORMAL
- en: But having tests is not only a good way of capturing problems once. Because
    a test can still remain, and be run on new code changes, it also creates a safety
    net while developing to be sure that creating new code or modifying the code does
    not affect the old functionality.
  prefs: []
  type: TYPE_NORMAL
- en: This is one of the best arguments for running tests automatically and constantly,
    as per Continuous Integration practices. The developer can focus on the feature
    being developed, while the Continuous Integration tool will run every test, alerting
    early if there's a problem with some test. A problem with previously introduced
    functionality that is failing is called a *regression*.
  prefs: []
  type: TYPE_NORMAL
- en: Regression problems are quite common, so having good test coverage is great
    to prevent them going undetected. Specific tests covering previous functionality
    to ensure that it keeps running as expected can be introduced. These are regression
    tests, and sometimes they are added after we have detected a regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit of having good tests that check the behavior of the system is
    that the code itself can be changed heavily, knowing that the behavior will remain
    the same. These changes can be made to restructure the code, clean it, and in
    general improve it. These changes are called *refactoring* the code, changing
    how the code is written without changing the expected behavior of it.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we should answer the question "what is a good test?" As we discussed, writing
    a test is not free, there's an effort involved, and we need to be sure that it's
    worth it. How can we create good ones?
  prefs: []
  type: TYPE_NORMAL
- en: How to design a great test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Designing good tests requires a certain mindset. The objective while designing
    the code that covers certain functionality is to make the code fulfill that functionality
    while at the same time being efficient, writing clear code that could even be
    described as elegant.
  prefs: []
  type: TYPE_NORMAL
- en: The objective of the test is to be sure that the functionality sticks to the
    expected behavior, and that all the different problems that can arise produce
    results that make sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to be able to really put the functionality to the test, the mindset should
    be to stress the code as much as possible. For example, let''s imagine a function
    `divide(A, B)`, that divides two integers between -100 and 100: `A` between `B`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While approaching the test, we need to check what the limits are of this, trying
    to check that the function is performing properly with the expected behavior.
    For example, the following tests could be created:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Action | Expected behavior | Comments |'
  prefs: []
  type: TYPE_TB
- en: '| `divide(10, 2)` | `return 5` | Basic case |'
  prefs: []
  type: TYPE_TB
- en: '| `divide(-20, 4)` | `return -5` | Divide one negative and one positive integer
    |'
  prefs: []
  type: TYPE_TB
- en: '| `divide(-10, -5)` | `return 2` | Divide two negative integers |'
  prefs: []
  type: TYPE_TB
- en: '| `divide(12, 2)` | `return 5` | Not exact division |'
  prefs: []
  type: TYPE_TB
- en: '| `divide(100, 50)` | `return 2` | Maximum value of A |'
  prefs: []
  type: TYPE_TB
- en: '| `divide(101, 50)` | `Produce an input error` | Value of A exceeding the maximum
    |'
  prefs: []
  type: TYPE_TB
- en: '| `divide(50, 100)` | `return 0` | Maximum value of B |'
  prefs: []
  type: TYPE_TB
- en: '| `divide(50, 101)` | `Produce an input error` | Value of B exceeding the maximum
    |'
  prefs: []
  type: TYPE_TB
- en: '| `divide(10, 0)` | `Produce an exception` | Divide by zero |'
  prefs: []
  type: TYPE_TB
- en: '| `divide(''10'', 2)` | `Produce an input error` | Invalid format for parameter
    A |'
  prefs: []
  type: TYPE_TB
- en: '| `divide(10, ''2'')` | `Produce an input error` | Invalid format for parameter
    B |'
  prefs: []
  type: TYPE_TB
- en: 'Note how we are testing different possibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: The usual behavior of all the parameters is correct, and the division works
    correctly. This includes both positive and negative numbers, exact division, and
    inexact division.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Values within the maximum and minimum values: We check that the maximum values
    are hit and correct, and the next value is properly detected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Division by zero: A known limitation on functionality that should produce a
    predetermined response (exception).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wrong input format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can really create a lot of test cases for simple functionality! Note that
    all these cases can be expanded. For example, we can add `divide(-100, 50)` and
    `divide(100, -50)` cases. In those cases, the question is the same: are those
    tests adding better detection of problems?'
  prefs: []
  type: TYPE_NORMAL
- en: The best test is the test that really stresses the code and ensures that it's
    working as expected, trying very hard to cover the most difficult use cases. Making
    the tests ask difficult questions of the code under test is the best way of preparing
    your code for the real action. A system under load will see all kinds of combinations,
    so the best preparation for that is to create tests that try as hard as possible
    to find problems, to be able to solve them before moving to the next phase.
  prefs: []
  type: TYPE_NORMAL
- en: This is analogous to football training, where a series of very demanding exercises
    are presented to be sure that the trainee will be able to perform later, during
    the match. Be sure that your training regime is hard enough to properly prepare
    for demanding matches!
  prefs: []
  type: TYPE_NORMAL
- en: The proper balance between the number of tests and not having tests that cover
    functionality already checked by an existing test (for example, creating a big
    table dividing numbers with a lot of divisions) may depend greatly on the code
    under test and practices in your organization. Some critical areas may require
    more thorough testing as a failure there could be more important.
  prefs: []
  type: TYPE_NORMAL
- en: For example, any external API should test any input with care and be really
    defensive about that, as external users may abuse external APIs. For example,
    testing what happens when strings are input in integer fields, infinity or `NaN`
    (Not a Number) values are added, payload limits are exceeded, the maximum size
    of a list or page is exceeded, etc.
  prefs: []
  type: TYPE_NORMAL
- en: By comparison, interfaces that are mostly internal will require less testing,
    as the internal code is less likely to abuse the API. For example, if the `divide`
    function is only internal, it might not be required to test that the input format
    is incorrect, just to check that the limits are respected.
  prefs: []
  type: TYPE_NORMAL
- en: Note that tests are done independently from the implementation of the code.
    A test definition is done purely from an external view of the function to test,
    without requiring knowing what's inside. This is called *black-box testing*. A
    heathy test suite always starts with this approach.
  prefs: []
  type: TYPE_NORMAL
- en: A critical ability to develop as a developer writing tests is to detach from
    the knowledge of the code itself and approach tests independently.
  prefs: []
  type: TYPE_NORMAL
- en: Testing can be so detached that it may use independent people just to create
    the tests, like a QA team performing tests. Unfortunately, this is not a possible
    approach for unit tests, which will likely be created by the same developers that
    write the code itself.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, this external approach won't be enough. If the developer knows
    that there's some specific area where there could be problems, it may be good
    to complement it with tests that check functionality that is not apparent from
    an external point of view.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a function that calculates a result based on some input may have
    an internal point where the algorithm changes to calculate it using different
    models. This information doesn't need to be known by the external user, but it
    will be good to add a couple of checks that the transition works correctly.
  prefs: []
  type: TYPE_NORMAL
- en: This kind of testing is called *white-box testing*, in comparison to the black-box
    approach discussed early.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to remember that, in a test suite, white-box tests should always
    be secondary to black-box tests. The main objective is to test the functionality
    from an external perspective. White-box testing may be a good addition, especially
    in some aspects, but it should have a lower priority.
  prefs: []
  type: TYPE_NORMAL
- en: Developing the ability to be able to create good black-box tests is important
    and should be transmitted to the team.
  prefs: []
  type: TYPE_NORMAL
- en: Black-box testing tries to avoid a common problem where the same developer writes
    both the code and the test and then checks that the interpretation of the feature
    implemented in the code works as expected, instead of checking that it works as
    it should when looking from an external endpoint. We will take a look later at
    TDD, which tries to ensure tests are created without the implementation in mind
    by writing the tests before writing the code.
  prefs: []
  type: TYPE_NORMAL
- en: Structuring tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In terms of structure, especially for unit tests, a nice way to structure tests
    is using the **Arrange Act Assert** (**AAA**) pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'This pattern means the test is in three different phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Arrange**: Prepare the environment for the tests. This includes all the setup
    to get the system right at the point before performing the next step, at a stable
    moment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Act**: Perform the action that is the objective of the test.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Assert**: Check that the result of the action is the expected one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The test gets structured as a sentence like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GIVEN**(Arrange)an environment known, the**ACTION**(Act)produces the specified
    **RESULT** (Assert)'
  prefs: []
  type: TYPE_NORMAL
- en: This pattern is also sometimes called *GIVEN*, *WHEN*, *THEN* as each step can
    be described in those terms.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this structure aims for all the tests to be independent, and for each
    to test a single thing.
  prefs: []
  type: TYPE_NORMAL
- en: A common different pattern is to group act steps in tests, testing multiple
    functionalities in a single test. For example, test that writing a value is correct
    and then check that the search for the value returns the proper value. This won't
    follow the AAA pattern. Instead, to follow the AAA pattern, two tests should be
    created, the first one to validate that the write works correctly and the second
    where the value is created as part of the setup in the Arrange step before doing
    the search.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this structure can be used whether the tests are executed through
    code or run manually, though they'll be used more for automated tests. When running
    them manually, the Arrange stage can take a long time to produce for each test,
    leading to a lot of time spent on that. Instead, manual tests are normally grouped
    together in the pattern that we describe above, executing a series of Act and
    Assert and using the input in the previous stage as setup for the next. This creates
    a dependency in requiring to run tests in a specific sequence, which is not great
    for unit test suites, but it can be better for smoke tests or other environments
    where the Arrange step is very expensive.
  prefs: []
  type: TYPE_NORMAL
- en: In the same way, if the code to test is purely functional (meaning that only
    the input parameters are the ones that determine its state, like the `divide`
    example above), the Arrange step is not required.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see an example of code created with this structure. Imagine that we have
    a method that we want to test, called `method_to_test`. The method is part of
    a class called `ClassToTest`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Each of the steps is very clearly defined. The first one prepares, in this case,
    an object in the class that we want to test. Note that we may need to add some
    parameters or some preparation so the object is in a known starting point so the
    next steps work as expected.
  prefs: []
  type: TYPE_NORMAL
- en: The Act step just generates the action that is under test. In this case, call
    the `method_to_test` method for the prepared object with the proper parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the Assert step is very straightforward and just checks the response
    is the expected one.
  prefs: []
  type: TYPE_NORMAL
- en: In general, both the Act and Assert steps are simple to define and write. The
    Arrange step is where most of the effort of the test will normally be.
  prefs: []
  type: TYPE_NORMAL
- en: Another common pattern that appears using the AAA pattern for tests is to create
    common functions for testing in Arrange steps. For example, creating a basic environment,
    which could require a complex setup, and then having multiple copies where the
    Act and Assert steps are different. This reduces the repetition of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We will see later how we can structure multiple tests that are very similar
    to avoid repetition, which is a problem when having big test suites. Having big
    test suites is important to create good test coverage, as we saw above.
  prefs: []
  type: TYPE_NORMAL
- en: Repetition in tests is, up to a certain point, unavoidable and even healthy
    to a certain degree. When changing the behavior of some part of the code because
    there are changes, the tests need to be changed accordingly to accommodate the
    changes. This change helps to weigh the size of the changes and avoid making big
    changes lightly, as the tests will work as a reminder of the affected functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, mindless repetition is not great, and we will see later some options
    to reduce the amount of code to be repeated.
  prefs: []
  type: TYPE_NORMAL
- en: Test-Driven Development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A very popular technique to approach programming is **Test-Driven Development**
    or **TDD**. TDD consists of putting tests at the center of the developing experience.
  prefs: []
  type: TYPE_NORMAL
- en: This builds on some of the ideas that we exposed earlier in the chapter, though
    working on them with a more consistent view.
  prefs: []
  type: TYPE_NORMAL
- en: 'The TDD flow to develop software works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: New functionality is decided on to be added to the code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A new test is written to define the new functionality. Note that this is done
    *before* the code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The test suite is run to show that it's failing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The new functionality is then added to the main code, focusing on simplicity.
    Only the required feature, without extra details, should be added.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The test suite is run to show that the new test is working. This may need to
    be done several times until the code is ready.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The new functionality is ready! Now the code can be refactored to improve it,
    avoiding duplication, rearranging elements, grouping it with previously existing
    code, etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cycle can start again for any new functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, TDD is based on three main ideas:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Write the tests before writing the code**: This prevents the problem of creating
    a test that is too tightly coupled with the current implementation, forcing the
    developer to think about the test and the feature before jumping into writing
    it. It also forces the developer to check that the test actually fails before
    the feature is written, being sure that a problem later on will be detected. This
    is similar to the black box testing approach that we described earlier in the
    *How to design a great test* section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Run the tests constantly**: A critical part of the process is running the
    whole test suite to check that all the functionality in the system is correct.
    This is done over and over, every time that a new test is created, but also while
    the functionality is being written. Running the tests is an essential part of
    developing in TDD. This ensures that all functionality is always checked and that
    the code works as expected at all times so any bug or discrepancy can be solved
    quickly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Work in very small increments**: Focus on the task at hand, so each step
    builds and grows a test suite that is big and covers the whole functionality of
    the code in depth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This big test suite creates a safety net that allows you to perform refactors
    of the code often, big and small, therefore improving the code constantly. Small
    increments mean small tests that are specific and need to be thought about before
    adding the code.
  prefs: []
  type: TYPE_NORMAL
- en: An extension of this idea is a focus on writing only the code that's required
    for the task at hand and not more. This is sometimes referred to as the **YAGNI**
    principle (**You Ain't Gonna Need It**). The intention of this principle is to
    prevent overdesigning or creating code for "foreseeable requests in the future,"
    which, in practice, have a high probability of never materializing and, even worse,
    makes the code more difficult to change in other directions. Given that software
    development is notoriously difficult to plan in advance, the emphasis should be
    on keeping things small and not getting too far ahead of yourself.
  prefs: []
  type: TYPE_NORMAL
- en: These three ideas interact constantly during the development cycle, and it keeps
    the tests at the center of the development process, hence the name of the practice.
  prefs: []
  type: TYPE_NORMAL
- en: Another important advantage of TDD is that putting the focus so heavily on the
    tests means that how the code is going to be tested is thought about from the
    start, which helps in designing code that's easily testable. Also, reducing the
    amount of code to write, focusing on it being strictly required to pass the test
    reduces the probability of overdesign. The requirement to create small tests and
    work in increments also tends to generate modular code, in small units that are
    combined together but are able to be tested independently.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general flow is to be constantly working with new failing tests, making
    them pass and then refactoring, sometimes called the "*red/green/refactor*" pattern:
    red when the test is failing and green when all tests are passing.'
  prefs: []
  type: TYPE_NORMAL
- en: Refactoring is a critical aspect of the TDD process. It is strongly encouraged,
    to constantly improve the quality of the existing code. One of the best outcomes
    of this way of working is the generation of very extensive test suites that cover
    each detail of the code functionality, meaning that refactoring code can be done
    knowing that there's a solid ground that is going to capture any problems introduced
    by changing the code and adding bugs.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the code's readability, usability, and so on, by refactoring is known
    to have a good impact in terms of improving the morale of developers and increasing
    the pace at which changes can be introduced, as the code is kept in good shape.
  prefs: []
  type: TYPE_NORMAL
- en: In general, and not only in TDD, allowing time to clean up old code and improve
    it is critical to maintain a good pace for changes. Old code that is stale tends
    to be more and more difficult to work with, and over time it will require way
    more effort to change it to make more changes. Encouraging healthy habits to care
    about the current state of the code and allowing time to perform maintenance improvements
    is critical for the long-term sustainability of any software system.
  prefs: []
  type: TYPE_NORMAL
- en: Another important aspect of TDD is the requirement of speedy tests. As tests
    are always running following TDD practices, the total execution time is quite
    important. The time that it takes for each test should be considered carefully,
    as the growing size of the test suite will make it take longer to run.
  prefs: []
  type: TYPE_NORMAL
- en: There's a general threshold where focus gets lost, so running tests taking longer
    than around 10 seconds will make them not "part of the same operation," risking
    the developer thinking about other stuff.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, running the whole test suite in under 10 seconds will be extremely
    difficult, especially as the number of tests grows. A full unit test suite for
    a complex application can consist of 10,000 tests or more! In real life, there
    are multiple strategies that can help alleviate this fact.
  prefs: []
  type: TYPE_NORMAL
- en: The whole test suite doesn't need to be run all the time. Instead, any test
    runner should allow you to select a range of tests to run, allowing you to reduce
    the number of tests to run on each run while the feature is in development. This
    means running only the tests that are relevant for the same module, for example.
    It can even mean running a single test, in certain cases, to speed up the result.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, at some point, the whole test suite should be run. TDD is actually
    aligned with Continuous Integration, as it is also based on running tests, this
    time automatically once the code is checked out into a repo. The combination of
    being able to run a few tests locally to ensure that things are working correctly
    while developing with the whole test suite running in the background once the
    code is committed to the repo is great.
  prefs: []
  type: TYPE_NORMAL
- en: Anyway, as the time taken to run tests is important in TDD, observing the duration
    of tests is important, and generating tests that can run quickly is key to being
    able to work in the TDD way. This is mainly achieved by creating tests that cover
    small portions of the code, and therefore the time to set up can be kept under
    control.
  prefs: []
  type: TYPE_NORMAL
- en: TDD practices work best with unit tests. Integration and system tests may require
    a big setup that is not compatible with the speed and tight feedback loop required
    for TDD to work.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, as we saw before, unit testing is where the bulk of testing is
    typically focused on most projects.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing TDD into new teams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Introducing TDD practices in an organization can be tricky, as they change the
    way to perform actions that are quite basic, and go a bit against the usual way
    of working (writing tests after writing the code).
  prefs: []
  type: TYPE_NORMAL
- en: When considering introducing TDD into a team, it's good to have an advocate
    that can act as a point of contact for the rest of the team and solve the questions
    and problems that may arise through creating tests.
  prefs: []
  type: TYPE_NORMAL
- en: TDD is very popular in environments where pair programming is also common, so
    it's another possibility to have someone drive a session while training the other
    developers and introducing the practice.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, the key element of TDD is the mindset of forcing the developer to
    think first about how a particular feature is going to be tested before starting
    to think about the implementation. This mindset doesn't come naturally and needs
    to be trained and practiced.
  prefs: []
  type: TYPE_NORMAL
- en: It may be challenging to apply TDD techniques with already existing code, as
    pre-existing code can be difficult to test in this configuration, especially if
    the developers are new to the practice. TDD works great for new projects, though,
    as a test suite for new code will be created at the same time as the code. A mixed
    approach of starting a new module inside an existing project, so most code is
    new and can be designed using TDD techniques, reduces the problem of dealing with
    legacy code.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to see if TDD can be effective for new code, try to start small,
    using some small project with a small team to be sure that it's not too disruptive
    and that the principles can be properly digested and applied. There are some developers
    that really love to use TDD principles, as it fits their personality and how they
    approach the process of developing. Remember that this is not necessarily how
    everyone will feel and that starting with these practices requires time, and perhaps
    it won't be possible to apply them 100% as the previous code might limit it.
  prefs: []
  type: TYPE_NORMAL
- en: Problems and limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TDD practices are very popular and widely followed in the industry, though they
    have their limits. One is the problem of big tests that take too long to run.
    These tests may be unavoidable in certain situations.
  prefs: []
  type: TYPE_NORMAL
- en: Another is the difficulty of fully taking this approach if it is not done from
    the beginning, as parts of the code will already be written, and perhaps new tests
    should be added, violating the rule of creating the tests before the code.
  prefs: []
  type: TYPE_NORMAL
- en: Another problem is designing new code while the features to be implemented are
    fluid and not fully defined. This requires experimentation, for example, to design
    a function to return a color that contrasts with an input color, for example,
    to present a contrast color based on a theme selectable by the user. This function
    may require inspection to see if it "looks right," which can require tweaking
    that's difficult to achieve with a preconfigured unit test.
  prefs: []
  type: TYPE_NORMAL
- en: Not a problem specifically with TDD, but something to be careful about is to
    remember to avoid dependencies between tests. This can happen with any test suite,
    but given the focus on creating new tests, it's a likely problem if the team is
    starting with TDD practices. Dependencies can be introduced by requiring tests
    to run in a particular order, as the tests can contaminate the environment. This
    is normally not done on purpose, but it's done inadvertently while writing multiple
    tests.
  prefs: []
  type: TYPE_NORMAL
- en: A typical effect on that will be that some tests fail if run independently,
    as their dependencies are not run in that case.
  prefs: []
  type: TYPE_NORMAL
- en: In any case, remember that TDD is not necessarily something that it's all or
    nothing, but a set of ideas and practices that can help you design code that's
    well tested and high quality. Not every single test in the system needs to be
    designed using TDD, but a lot of them can be.
  prefs: []
  type: TYPE_NORMAL
- en: Example of the TDD process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s imagine that we need to create a function that:'
  prefs: []
  type: TYPE_NORMAL
- en: For values lower than 0, returns zero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For values greater than 10, returns 100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For values between, it returns the power of two of the value. Note that for
    the edges, it returns the power of two of the input (0 for 0 and 100 for 10)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To write the code in full TDD fashion, we start with the smallest possible test.
    Let's create the smallest skeleton and the first test.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We run the test, and get an error with the test failing. Right now, we will
    use pure Python code, but later in the chapter, we'll see how to run tests more
    efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The implementation of the use case is quite straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Yes, we are actually returning a hardcoded value, but that's really all that
    is required to pass the first tests. Let's run the tests now and you'll see no
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: But now we add tests for the lower edge. While these are two lines, they can
    be considered the same test, as they're checking that the edge is correct.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Let's run the tests again.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We need to add code to handle the lower edge.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: When running the test, we see that it's running the tests correctly. Let's add
    parameters now to handle the upper edge.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This triggers the corresponding error.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Let's add the higher edge.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This runs correctly. We are not confident that all the code is fine, and we
    really want to be sure that the intermediate section is correct, so we add another
    test.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Aha! Now it shows an error, due to the initial hardcoding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: So let's fix it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This runs all the tests correctly. Now, with the safety net of the tests, we
    think we can refactor the code a little bit to clean it up.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We can run the tests all through the process and be sure that the code is correct.
    The final result may be different based on what the team considers good code or
    what is more explicit, but we have our test suite that will ensure that the tests
    are consistent, and the behavior is correct.
  prefs: []
  type: TYPE_NORMAL
- en: The function here is quite small, but this shows what the flow is when writing
    code in the TDD style.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to unit testing in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are multiple ways to run tests in Python. One, as we have seen above,
    a bit crude, is to execute code with multiple asserts. A common one is the standard
    library `unittest`.
  prefs: []
  type: TYPE_NORMAL
- en: Python unittest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`unittest` is a module included in the Python standard library. It is based
    on the concept of creating a testing class that groups several testing methods.
    Let''s write a new file with the tests written in the proper format, called `test_unittest_example.py`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Let's analyze the different elements. The first ones are the imports on top.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We import the `unittest` module and the function to test. The most important
    part comes next, which defines the tests.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The class `TestTDDExample` groups the different tests. Notice that it's inheriting
    from `unittest.TestCase`. Then, methods that start with `test_` will produce the
    independent tests. Here, we will show one. Internally, it calls the function and
    compares the result with 0, using the `self.assertEqual` function.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that `test_seven` is defined incorrectly. We do this to produce an error
    when running it.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we add this code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This runs the tests automatically if we run the file. So, let''s run the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, it has run all six tests, and shows any errors. Here, we can
    clearly see the problem. If we need more detail, we can run with `-v showing`
    showing each of the tests that are being run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: You can also run a single test or combination of them using the `-k` option,
    which searches for matching tests.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '`unittest` is extremely popular and can accept a lot of options, and it''s
    compatible with virtually every framework in Python. It''s also very flexible
    in terms of ways of testing. For example, there are multiple methods to compare
    values, like `assertNotEqual` and `assertGreater`.'
  prefs: []
  type: TYPE_NORMAL
- en: There's a specific assert function that works differently, which is `assertRaises`,
    used to detect when the code generates an exception. We will take a look at it
    later when testing mocking external calls.
  prefs: []
  type: TYPE_NORMAL
- en: It also has `setUp` and `tearDown` methods to execute code before and after
    the execution of each test in the class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Be sure to take a look at the official documentation: [https://docs.python.org/3/library/unittest.html](https://docs.python.org/3/library/unittest.html).'
  prefs: []
  type: TYPE_NORMAL
- en: While `unittest` is probably the most popular test framework, it's not the most
    powerful one. Let's take a look at it.
  prefs: []
  type: TYPE_NORMAL
- en: Pytest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pytest simplifies writing tests even further. One common complaint about `unittest`
    is that it forces you to set a lot of `assertCompare` calls that are not obvious.
    It also needs to structure the tests, adding a bit of boilerplate code, like the
    `test` class. Other problems are not as obvious, but when creating big test suites,
    the setup of different tests can start to get complicated.
  prefs: []
  type: TYPE_NORMAL
- en: A common pattern is to create classes that inherit from other test classes.
    Over time, that can grow legs of its own.
  prefs: []
  type: TYPE_NORMAL
- en: Pytest instead simplifies the running and defining of tests, and captures all
    the relevant information using standard `assert` statements that are easier to
    read and recognize.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will use `pytest` in the simplest way. Later in the chapter,
    we will cover more interesting cases.
  prefs: []
  type: TYPE_NORMAL
- en: Be sure to install `pytest` through pip in your environment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Let's see how to run the tests defined in the `unittest`, in the file `test_pytest_example.py`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: If you compare it with the equivalent code in `test_unittest_example.py`, the
    code is significantly leaner. When running it with `pytest`, it also shows more
    detailed, colored information.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As with `unittest`, we can see more information with `-v` and run a selection
    of tests with `-k`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: And it's totally compatible with `unittest` defined tests, which allows you
    to combine both styles or migrate them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Another great feature of `pytest` is easy autodiscovery to find files that start
    with `test_` and run inside all the tests. If we try it, pointing at the current
    directory, we can see it runs both `test_unittest_example.py` and `test_pytest_example.py`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We will continue talking about more features of `pytest` during the chapter,
    but first, we need to go back to how to define tests when the code has dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Testing external dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When building unit tests, we talked about how it's based around the concept
    of isolating a unit in the code to test it independently.
  prefs: []
  type: TYPE_NORMAL
- en: This isolation concept is key, as we want to focus on small sections of the
    code to create small, clear tests. Creating small tests also helps in keeping
    the tests fast.
  prefs: []
  type: TYPE_NORMAL
- en: In our example above, we tested a purely functional function, `parameter_tdd`,
    that had no dependencies. It was not using any external library or any other function.
    But inevitably, at some point, you'll need to test something that depends on something
    else.
  prefs: []
  type: TYPE_NORMAL
- en: The question in this case is *should the other component be part of the test
    or not?*
  prefs: []
  type: TYPE_NORMAL
- en: This is not an easy question to answer. Some developers think that all unit
    tests should be purely about a single function or method, and therefore, any dependency
    should not be part of the test. But, on a more practical level, there are sometimes
    pieces of code that form a unit that it's easier to test in conjunction than separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, think about a function that:'
  prefs: []
  type: TYPE_NORMAL
- en: For values lower than 0, returns zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For values greater than 100, returns 10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For values between, it returns the square root of the value. Note that for the
    edges, it returns the square root of them (0 for 0 and 10 for 100).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is very similar to the previous function, `parameter_tdd,` but this time
    we need the help of an external library to produce the square root of a number.
    Let's take a look at the code.
  prefs: []
  type: TYPE_NORMAL
- en: It's divided into two files. `dependent.py` contains the definition of the function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The code is pretty similar to the code in the `parameter_tdd` example. The module
    `math.sqrt` returns the square root of a number.
  prefs: []
  type: TYPE_NORMAL
- en: And the tests are in `test_dependent.py`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we are completely using the external library and testing it at
    the same time that we are testing our code. For this simple example, this is a
    perfectly valid option, though that may not be the case for other cases.
  prefs: []
  type: TYPE_NORMAL
- en: The code is available in GitHub at [https://github.com/PacktPublishing/Python-Architecture-Patterns/tree/main/chapter_10_testing_and_tdd](https://github.com/PacktPublishing/Python-Architecture-Patterns/tree/main/chapter_10_testing_and_tdd).
  prefs: []
  type: TYPE_NORMAL
- en: For example, the external dependency could be making external HTTP calls that
    need to be captured to prevent making them while running tests and to have control
    over the returned values, or other big pieces of functionality that should be
    tested in isolation.
  prefs: []
  type: TYPE_NORMAL
- en: To detach a function from its dependencies, there are two different approaches.
    We will show them using `parameter_dependent` as a baseline.
  prefs: []
  type: TYPE_NORMAL
- en: Again, in this case, the tests work perfectly fine with the dependency included,
    as it's simple and doesn't produce side effects like external calls, etc.
  prefs: []
  type: TYPE_NORMAL
- en: We will see next how to mock the external calls.
  prefs: []
  type: TYPE_NORMAL
- en: Mocking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mocking is a practice that internally replaces the dependencies, replacing them
    with fake calls, under the control of the test itself. This way, we can introduce
    a known response for any external dependency, and not call the actual code.
  prefs: []
  type: TYPE_NORMAL
- en: Internally, mocking is implemented using what is known as *monkey-patching*,
    which is the dynamic replacement of existing libraries with alternatives. While
    this can be achieved in different ways in different programming languages, it's
    especially popular in dynamic languages like Python or Ruby. Monkey-patching can
    be used for other purposes than testing, though it should be used with care, as
    it can change the behavior of libraries and can be quite disconcerting for debugging.
  prefs: []
  type: TYPE_NORMAL
- en: To be able to mock the code, in our test code, we need to prepare the mock as
    part of the Arrange step. There are different libraries to mock calls, but the
    easiest is to use the `unittest.mock` library included as part of the standard
    library.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest usage of `mock` is to patch an external library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The `patch` decorator intercepts the calls to the defined library, `math.sqrt`,
    and replaces it with a `mock` object that passes to the function, here called
    `mock_sqrt`.
  prefs: []
  type: TYPE_NORMAL
- en: This object is a bit special. It basically allows any calls, accesses almost
    any method or attributes (except predefined ones), and keeps returning a mock
    object. This makes the mock object something really flexible that will adapt to
    whatever code surrounds it. When necessary, the returning value can be set calling
    `.return_value`, as we show in the first line.
  prefs: []
  type: TYPE_NORMAL
- en: We are, in essence, saying that calls to `mock_sqrt` will return the value 5\.
    So, we are preparing the output of the external call, so we can control it.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we check that we called the mock `mock_sqrt` once, with the input (`25`)
    using the method `assert_called_once_with`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In essence, we are:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the mock so it replaces `math.sqrt`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting the value that it will return when called
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking that the call works as expected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Double-checking that the mock was called with the right value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For other tests, for example, we can check that the mock was not called, indicating
    that the external dependence wasn't called.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'There are multiple `assert` functions that allow you to detect how the mock
    has been used. Some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `called` attribute returning `True` or `False` based on whether the mock
    has been called or not, allowing you to write:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `call_count` attribute returning the number of times a mock has been called.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `assert_called_with()` method to check the number of times that it has been
    called. It will raise an exception if the last call is not produced in the specified
    way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `assert_any_call()` method to check whether any of the calls have been produced
    in the specified way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With that information, the full file for testing, `test_dependent_mocked_test.py`,
    will be like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: If the mock needs to return different values, you can define the `side_effect`
    attribute of the mock as a list or tuple. `side_effect` is similar to `return_value`,
    but it has a few differences, as we'll see.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '`side_effect` can also be used to produce an exception, if needed.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The `with` section asserts that the expected `Exception` is raised in the block.
    If not, it shows an error.
  prefs: []
  type: TYPE_NORMAL
- en: In `unittest`, checking a raised exception can be done with a similar `with`
    block.
  prefs: []
  type: TYPE_NORMAL
- en: '`with self.assertRaises(ValueError):`'
  prefs: []
  type: TYPE_NORMAL
- en: '`parameter_dependent(25)`'
  prefs: []
  type: TYPE_NORMAL
- en: Mocking is not the only way to handle dependencies for tests. We will see a
    different approach next.
  prefs: []
  type: TYPE_NORMAL
- en: Dependency injection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While mocking replaces the dependency without the original code noticing, by
    patching it externally, dependency injection is a technique to make that dependency
    explicit when calling the function under test, so it can be replaced with a testing
    substitute.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, it's a way of designing the code that makes dependencies explicit
    by requiring them as input parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Dependency injection, while useful for testing, is not only aimed at that. By
    adding the dependencies explicitly, it also reduces the need for a function to
    know how to initialize a particular dependency, instead relying on the interface
    of the dependency. It creates a separation between "initializing" a dependency
    (which should be taken care of externally) and "using" it (which is the only part
    that the dependent code will do). This differentiation will become clearer later
    when we see an OOP example.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how this changes the code under test.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Notice how now the `sqrt` function is an input parameter.
  prefs: []
  type: TYPE_NORMAL
- en: If we want to use the `parameter_dependent` function in a normal scenario, we
    will have to produce the dependency, for example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'And if we want to perform tests, we can do it by replacing the `math.sqrt`
    function with a specific function, and then using it. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We can also provoke an error if calling the dependency to ensure that in some
    tests the dependency is not used, for example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Note how this approach is more explicit than mocking. The code to test becomes,
    in essence, totally functional as it doesn't have external dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Dependency injection in OOP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dependency injection can also be used with OOP. In this case, we can start with
    code that is like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the `settings` class stores different elements that are required
    on where the data will be stored. The model receives some data and then saves
    it. The code in operation will require minimal initialization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The model receives some data and then saves it. The code in operation requires
    minimal initialization, but at the same time, it's not explicit.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use dependency injection principles, the code will need to be written in
    this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: In this case, every value that is a dependency is provided explicitly. In the
    definition of the code, the `settings` module is not present anywhere, but instead,
    that will be specified when the class is instantiated. The code will now need
    to define the configuration directly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We can compare how to test both cases, as seen in the file `test_dependency_injection_test.py`.
    The first test is mocking, as we saw before, the `write` method of the `Writer`
    class to assert that it has been called correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Compared to that, the dependency injection example doesn't require a mock through
    monkey-patching. It just creates its own `Writer` that simulates the interface.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'This second style is more verbose, but it shows some of the differences when
    writing code in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: No monkey-patching mock is required. Monkey-patching can be quite fragile, as
    it's meddling with internal code that's not supposed to be exposed. While in testing
    this interference is not the same as doing it for regular code running, it's still
    something that can be messy and have unintended effects, especially if the internal
    code changes in some unforeseen way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep in mind that mocks will likely involve, at some point, relating to second-level
    dependencies, which can start having strange or complicated effects requiring
    you to spend time handling that extra complexity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The way of writing the code is different in itself. Code produced with dependency
    injection is, as we've seen, more modular and composed of smaller elements. This
    tends to create smaller and more combinable modules that play along together,
    with fewer unknown dependencies, as they are always explicit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be careful, though, as this requires a certain amount of discipline and mental
    framing to produce truly loosely coupled modules. If this is not considered when
    designing the interfaces, the resulting code will instead be artificially divided,
    resulting in tightly coupled code across different modules. Developing this discipline
    requires certain training; do not expect it to come naturally to all developers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code can sometimes be more difficult to debug, as the configuration will
    be separated from the rest of the code, sometimes making it difficult to understand
    the flow of the code. The complexity can be produced at the interaction of classes,
    which may be more difficult to understand and test. Typically, the upfront effort
    to develop code in this style is a bit greater as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dependency injection is a very popular technique in certain software circles
    and programming languages. Mocking is more difficult in less dynamic languages
    than Python, and also different programming languages have their own sets of ideas
    on how to structure code. For example, dependency injection is very popular in
    Java, where there are specific tools to work in this style.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced pytest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While we've described the basic functionalities for `pytest`, we barely scratched
    the surface in terms of the number of possibilities that it presents to help generate
    testing code.
  prefs: []
  type: TYPE_NORMAL
- en: Pytest is a big and comprehensive tool. It is worth learning how to use it.
    Here, we will only scratch the surface. Be sure to check the official documentation
    at [https://docs.pytest.org/](https://docs.pytest.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Without being exhaustive, we will see some useful possibilities of the tool.
  prefs: []
  type: TYPE_NORMAL
- en: Grouping tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes it is useful to group tests together so they are related to specific
    things, like modules, or to run them in unison. The simplest way of grouping tests
    together is to join them into a single class.
  prefs: []
  type: TYPE_NORMAL
- en: For example, going back to the test examples before, we could structure tests
    into two classes, as we see in `test_group_classes.py`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'This is an easy way to divide tests and allows you to run them independently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Another possibility is to use markers. Markers are indicators that can be added
    through a decorator in the tests, for example, in `test_markers.py`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: See that we are defining a decorator, `@pytest.mark.edge`, on all the tests
    that checks the edge of the values.
  prefs: []
  type: TYPE_NORMAL
- en: If we execute the tests, we can use the parameter `-m` to run only the ones
    with a certain tag.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The warning `PytestUnknownMarkWarning: Unknown pytest.mark.edge` is produced
    if the marker `edge` is not registered.'
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that the GitHub code includes the `pytest.ini` code. You won't see
    the warning if the `pytest.ini` file is present, for example, if you clone the
    whole repo.
  prefs: []
  type: TYPE_NORMAL
- en: This is very useful for finding typos, like accidentally writing `egde` or similar.
    To avoid this warning, you'll need to add a `pytest.ini` config file with the
    definition of the markers, like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Now, running the tests shows no warning.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Note that markers can be used across the full test suite, including multiple
    files. That allows for making markers to identify common patterns across the tests,
    for example, creating a quick test suite with the most important tests to run
    with the marker `basic`.
  prefs: []
  type: TYPE_NORMAL
- en: There are also some predefined markers with some built-in features. The most
    common ones are `skip` (which will skip the test) and `xfail` (which will reverse
    the test, meaning that it expects it to fail).
  prefs: []
  type: TYPE_NORMAL
- en: Using fixtures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The use of fixtures is the preferred way to set up tests in `pytest`. A fixture,
    in essence, is a context created to set up a test.
  prefs: []
  type: TYPE_NORMAL
- en: Fixtures are used as input for the test functions, so they can be set up and
    create specific environments for the test to be created.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let's take a look at a simple function that counts the number of
    occurrences of a character in a string.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: That's a pretty simple loop that iterates through the string and counts the
    matching characters.
  prefs: []
  type: TYPE_NORMAL
- en: This is equivalent to using the function `.count()` for a string, but this is
    included to present a working function. It could be refactored afterward!
  prefs: []
  type: TYPE_NORMAL
- en: A regular test to cover the functionalities could be as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Pretty straightforward. Now let's see how we can define a fixture to define
    a setup, in case we want to replicate it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'First of all, the fixture is decorated with `pytest.fixture` to mark it as
    such. A fixture is divided into three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Setup**: Here, we simply defined a string, but this will probably be the
    biggest part, where the values are prepared.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Return the value**: If we use the `yield` functionality, we will be able
    to go to the next step; if not, the fixture will finish here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Teardown and clean up values**: Here, we simply delete the variable as an
    example, though this will happen automatically later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Later, we will see a more complex fixture. Here, we are just presenting the
    concept.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Defining the fixture this way will allow us to reuse it easily in different
    test functions, just using the name as the input parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Note how the `prepare_string` parameter is automatically providing the value
    that we defined with `yield`. If we run the tests, we can see the effect. Even
    more, we can use the parameter `--setup-show` to see the setup and tear down all
    of the fixtures.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: This fixture was very simple and did not do anything that couldn't be done defining
    the string, but fixtures can be used to connect to a database or prepare files,
    taking into account that they can clean them up at the end.
  prefs: []
  type: TYPE_NORMAL
- en: For example, complicating the same example a bit, instead of counting from a
    string, it should count from a file, so the function needs to open a file, read
    it, and count the characters. The function will be like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The fixture should then create a file, return it, and then remove it as part
    of the teardown. Let's take a look at it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Note that in the filename, we define the name adding the timestamp when it's
    generated. This means that each of the files that will be generated by this fixture
    will be unique.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: The file then gets created and the data is written.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: The name of the file, which, as we've seen, is unique, gets yielded. Finally,
    the file is deleted in the teardown.
  prefs: []
  type: TYPE_NORMAL
- en: The tests are similar to the previous ones, as most of the complexity is stored
    in the fixture.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: When running it, we see it works as expected, and we can check that the teardown
    step deletes the testing files after each test.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Fixtures don't need to be defined in the same file. They can also be stored
    in a special file called `conftest.py`, which will automatically be shared by
    `pytest` across all the tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fixtures can also be combined, they can be set to be used automatically, and
    there are already built-in fixtures to work with temporal data and directories
    or capture output. There are also a lot of plugins for useful fixtures in PyPI,
    installable as third-party modules, covering functionality like connecting to
    databases or interacting with other external resources. Be sure to check the Pytest
    documentation and to search before implementing your own fixture to see if you
    can leverage an already existing module: [https://docs.pytest.org/en/latest/explanation/fixtures.html#about-fixtures](https://docs.pytest.org/en/latest/explanation/fixtures.html#about-fixtures).'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we only scratched the surface in terms of the possibilities
    of `pytest`. It is a fantastic tool and one that I encourage you to learn about.
    It will pay off greatly to efficiently run tests and design them in the best possible
    way. Testing is a critical part of a project and it's one of the development stages
    where developers spend most of their time.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through the whys and hows of tests to describe how
    a good testing strategy is required to produce high-quality software and prevent
    problems once the code is in use by customers.
  prefs: []
  type: TYPE_NORMAL
- en: We started by describing the general principles behind testing, how to make
    tests that provide more value than their cost, and the different levels of testing
    to ensure this. We saw the three main levels of tests, which we called unit tests
    (parts of a single component), system tests (the whole system), and integration
    tests in the middle (a whole component or several components, but not all).
  prefs: []
  type: TYPE_NORMAL
- en: We continued by describing different strategies to ensure that our tests are
    great ones, and how to structure them using the Arrange-Act-Assert pattern, for
    ease of writing and understanding them after they are written.
  prefs: []
  type: TYPE_NORMAL
- en: Later, we described in detail the principles behind Test-Driven Development,
    a technique that puts tests at the center of development, which mandates writing
    the tests before the code, working in small increments, and running the tests
    over and over to create a good test suite that protects against unexpected behavior.
    We also analyzed the limits and caveats of working in a TDD fashion and provided
    an example of what the flow looks like.
  prefs: []
  type: TYPE_NORMAL
- en: We continued by presenting ways of creating unit tests in Python, both using
    the standard `unittest` module and by introducing the more powerful `pytest`.
    We also presented a section with advanced usage of `pytest` to show a bit of what
    this great third-party module is capable of.
  prefs: []
  type: TYPE_NORMAL
- en: We described how to test external dependencies, something that is critically
    important when writing unit tests to isolate functionality. We also described
    how to mock dependencies and how to work under the dependency injection principles.
  prefs: []
  type: TYPE_NORMAL
