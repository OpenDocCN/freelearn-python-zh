<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<meta charset="utf-8"/>
<meta content="pandoc" name="generator"/>
<title>ch020.xhtml</title>

<!-- kobo-style -->
<style id="koboSpanStyle" type="text/css" xmlns="http://www.w3.org/1999/xhtml">.koboSpan { -webkit-text-combine: inherit; }</style>
</head>
<body epub:type="bodymatter">

<h1 data-number="20">Chapter 16<br/>
Project 5.2: Simple Multivariate Statistics</h1>
<p>Are variables related? If so what’s the relationship? An analyst tries to answer these two questions. A negative answer — the null hypothesis — doesn’t require too many supporting details. A positive answer, on the other hand, suggests that a model can be defined to describe the relationship. In this chapter, we’ll look at simple correlation and linear regression as two elements of modeling a relationship between variables.</p>
<p>In this chapter, we’ll expand on some skills of data analysis:</p>
<ul>
<li><p>Use of the built-in <code>statistics</code> library to compute correlation measures and linear regression coefficients.</p></li>
<li><p>Use of the <strong>matplotlib </strong>library to create images. This means creating plot images outside a Jupyter Lab environment.</p></li>
<li><p>Expanding on the base modeling application to add features.</p></li>
</ul>
<p>This chapter’s project will expand on earlier projects. Look back at <a href="ch017.xhtml#x1-29700013"><em>Chapter</em><em> 13</em></a>, <a href="ch017.xhtml#x1-29700013"><em>Project 4.1: Visual Analysis Techniques</em></a> for some of the graphical techniques used in a Jupyter Lab context. These need to be more fully automated. The project will add multivariate statistics and graphs to illustrate relationships among variables. </p>

<h2 data-number="20.1">16.1  Description</h2>
<p>In <a href="ch019.xhtml#x1-32500015"><em>Chapter</em><em> 15</em></a>, <a href="ch019.xhtml#x1-32500015"><em>Project 5.1: Modeling Base Application</em></a> we created an application to create a summary document with some core statistics. In that application, we looked at univariate statistics to characterize the data distributions. These statistics included measurements of the location, spread, and shape of a distribution. Functions like mean, median, mode, variance, and standard deviation were emphasized as ways to understand location and spread. The characterization of shape via skewness and kurtosis was left as an extra exercise for you.</p>
<p>The base application from the previous chapter needs to be expanded to include the multivariate statistics and diagrams that are essential for clarifying the relationships among variables. There are a vast number of possible functions to describe the relationships among two variables. See <a class="url" href="https://www.itl.nist.gov/div898/handbook/pmd/section8/pmd8.htm">https://www.itl.nist.gov/div898/handbook/pmd/section8/pmd8.htm</a> for some insight into the number of choices available.</p>
<p>We’ll limit ourselves to linear functions. In the simplest cases, there are two steps to creating a linear model: identifying a correlation and creating the coefficients for a line that fits the data. We’ll look at each of these steps in the next two sections. </p>

<h3 data-number="20.1.1">16.1.1  Correlation coefficient</h3>
<p>The coefficient of correlation measures how well the values of two variables correlate with each other. A value of 1 indicates perfect correlation; a value of zero indicates no discernable correlation. A value of -1 indicates an “anti-correlation”: when one variable is at its maximum value, the other variable is at its minimum.</p>
<p>See <a href="#16.1"><em>Figure 16.1</em></a> to see how the correlation coefficient describes the distribution of the two variables.</p>
<figure class="IMG---Figure">
<img alt="Figure 16.1: Correlation Coefficients " src="img/file64.jpg"/>
<figcaption class="IMG---Caption">Figure 16.1: Correlation Coefficients </figcaption>
</figure>
<p>The computation of the coefficient compares individual values of variables, <em>X</em><sub>i</sub> and <em>Y</em> <sub>i</sub>, to the mean values for those variables, <em>X</em> and <em>Ȳ</em>. Here’s a formula:</p>
<div><img alt=" ∑ r = ∘-∑---(Xi-−-¯X∘)(Y∑i-−-¯Y)----- (Xi − ¯X )2 (Yi − ¯Y )2 " class="math-display" src="img/file65.jpg"/>
</div>
<p>The computations of the mean values, <em>X</em> and <em>Ȳ</em>, can be factored into this, creating a somewhat more complicated version that’s often used to create the coefficient in a single pass through the data.</p>
<p>This function is available as <code>statistics.correlation()</code> in the <code>standard</code> library.</p>
<p>If two variables correlate with each other, then a linear function will map one variable to a value near the other variable. If the correlation is 1.0 or -1.0, the mapping will be exact. For other correlation values, the mapping will be close, but not exact. In the next section, we’ll show how to transform the correlation coefficient into the parameters for a line. </p>


<h3 data-number="20.1.2">16.1.2  Linear regression</h3>
<p>One equation for a line is <em>y </em>= <em>mx </em>+ <em>b</em>. The values of <em>m </em>and <em>b </em>are parameters that describe the specific linear relationship between the <em>x </em>and <em>y </em>variables.</p>
<p>When fitting a line to data, we’re estimating the parameters for a line. The goal is to minimize the error between the line and the actual data. The “least squares” technique is often used.</p>
<p>The two coefficients, <em>b </em>and <em>m</em>, can be computed as follows:</p>
<div><img alt=" ∑ ∑ ∑ ∑ X2i Y 2i − Xi XiYi b = ----n-∑--X2-−-(∑-X--)2---- i i " class="math-display" src="img/file66.jpg"/>
</div>
<div><img alt=" ∑ ∑ ∑ m = n---X∑iYi-−--∑-Xi---Yi n X2i − ( Xi)2 " class="math-display" src="img/file67.jpg"/>
</div>
<p>This is available as <code>statistics.linear_regression()</code> in the standard library. This saves us from having to write these two functions.</p>
<p>The various sums and sums of squares are not terribly difficult values to compute. The built-in <code>sum()</code> function is the basis for most of this. We can use <code>sum(map(lambda</code><code> x:</code><code> x^2,</code><code> x_values))</code> to compute ∑ <em>X</em><sub>i</sub><sup>2</sup>.</p>
<p>To clarify these multivariate relationships, diagrams can be very helpful. In the next sections, we’ll look at the most important type of diagram that needs to be part of the overall application. </p>


<h3 data-number="20.1.3">16.1.3  Diagrams</h3>
<p>One essential diagram for showing multivariate data is the X-Y “scatter” plot. In <a href="ch017.xhtml#x1-29700013"><em>Chapter</em><em> 13</em></a>, <a href="ch017.xhtml#x1-29700013"><em>Project 4.1: Visual Analysis Techniques</em></a> we looked at ways to create these. In that chapter, we relied on Jupyter Lab to present the diagram as part of the overall web page. For this application, we’ll need to embed the diagram into a document.</p>
<p>This generally means there will be a markup document that includes a reference to a diagram file. The format of the diagram file can be SVG, PNG, or even JPEG. For technical graphics, the SVG files are often the smallest and scale extremely well.</p>
<p>Each markup language, including Markdown, RST, HTML, and LaTeX, have unique ways to identify the place where an image needs to be inserted. In the case of Markdown, it’s often necessary to use HTML syntax to properly include frames and captions.</p>
<p>Now that we’ve seen what the application needs to do, we can look at an approach to create the software. </p>



<h2 data-number="20.2">16.2  Approach</h2>
<p>As with the previous project, this application works in these two distinct parts:</p>
<ol>
<li><div><p>Compute the statistics and create the diagram files.</p>
</div></li>
<li><div><p>Create a report file in a simplified markup language from a template with the details interpolated. A tool like <strong>Jinja </strong>is very helpful for this.</p>
</div></li>
</ol>
<p>Once the report file in a markup language — like Markdown or RST — is available, then a tool like <strong>Pandoc </strong>can be used to create an HTML page or a PDF document from the markup file. Using a tool like <strong>Pandoc </strong>permits quite a bit of flexibility in choosing the final format. It also allows the insertion of style sheets and page templates in a tidy, uniform way.</p>
<div><div><p>The LaTeX language as markup provides the most comprehensive capabilities. It is challenging to work with, however. Languages like Markdown and RST are designed to offer fewer, easier-to-use capabilities.</p>
<p>This book is written with LaTeX.</p>
</div>
</div>
<p>We’ll look at three aspects of this application: the statistical computations, creating the diagrams, and finally, creating the final markup file to include the diagrams. We’ll start with a quick review of the statistical computations. </p>

<h3 data-number="20.2.1">16.2.1  Statistical computations</h3>
<p>The statistical summary output file, in TOML notation, has a section for each variable and the univariate statistics about those variables.</p>
<p>This section of the file looked like the following snippet of TOML:</p>
<div><div><pre class="source-code">[x.location]
    mean = 9.0
[x.spread]
    variance = 11.0
[y.location]
    mean = 7.5
[y.spread]
    variance = 4.125</pre>
</div>
</div>
<p>When parsed, the TOML syntax of <code>x.location</code> and <code>x.spread</code> creates a dictionary that looks like the following fragment of a Python object:</p>
<div><div><pre class="source-code">{
    some metadata here...

    ’x’: {
        ’location’: {
            ’mean’: 9.0
        },
        ’spread’: {
            ’variance’: 11.0
        }
    },
    ’y’: {
        etc.
    }
}</pre>
</div>
</div>
<p>This structure can be expanded to include additional locations and spread statistical measures. It can also be expanded to include multivariate statistics. The <code>statistics</code> module has <code>correlation()</code> and <code>covariance()</code> functions, making it easy to include these measures.</p>
<p>For datasets with few variables, it’s common to consider a matrix that includes all the combinations of covariance between variables. This leads to two alternative representations of these additional statistics:</p>
<ul>
<li><p>A separate section for a covariance matrix. A section label of <code>[covariance]</code> can be followed by nested dictionaries that include all combinations of variables. Since the covariance matrix is symmetric, all <em>n</em><sup>2</sup> combinations aren’t needed; only <em>n</em>× (<em>n</em>− 1) values are unique.</p></li>
<li><p>Multivariate sub-sections within each variable’s section. This means we’d have <code>x.location</code>, <code>x.spread</code>, <code>x.covariance.y</code>, and <code>x.correlation.y</code> sub-sections for the <code>x</code> variable.</p></li>
</ul>
<p>For a dataset with fewer variables, it seems sensible to bundle covariance and correlation into the details for a given variable. In the case of Anscombe’s Quartet, with only two variables, the covariance and correlation seem like they belong with the other statistics.</p>
<p>For a dataset with a larger number of variables, the covariance among all the variables can become bewildering. In these cases, a technique like finding principal components might be needed to reduce the number of variables to a more manageable population. In this case, separate sections with covariance and auto-correlation might be more useful.</p>
<p>The resulting model is often the result of some careful thought, based on the covariance matrix. For this reason, a separate <code>[model]</code> section should be provided with some details about the model’s structure and the coefficients. In the case of a linear model, there are two coefficients, sometimes called <em>β</em><sub>0</sub> and <em>β</em><sub>1</sub>. We’ve called them <em>b </em>and <em>m</em>.</p>
<p>For Python 3.11, the included <code>tomllib</code> module doesn’t create TOML-format files. It’s, therefore, necessary to properly format a text file that can be parsed by the <code>tomllib</code> module. It’s helpful to use a Jinja template for this. </p>


<h3 data-number="20.2.2">16.2.2  Analysis diagrams</h3>
<p>Diagrams must first be created. Once created, they can then be included in a document. The process of creating a diagram is nearly identical to the approach used in Jupyter Lab. A few extra steps need to be taken to export the diagram to a file that can be imported into a document.</p>
<p>When working in Jupyter Lab, some cells to load the data are required to create two variables, <code>x</code> and <code>y</code>, with the values to be plotted. After these cells, a cell like the following example will create and display a scatter plot:</p>
<div><div><pre class="source-code">import matplotlib.pyplot as plt

fig, ax = plt.subplots()

# Labels and Title
ax.set_xlabel(’X’)
ax.set_ylabel(’Y’)
ax.set_title(’Series I’)

# Draw Scatter
_ = ax.scatter(x, y)</pre>
</div>
</div>
<p>This presumes previous cells have loaded clean data and extracted two list objects, <code>x</code> and <code>y</code>, with the values to be plotted.</p>
<p>The above code sample doesn’t save the resulting PNG or SVG file, however. To save the figure, we need to perform two more steps. Here are the lines of code required to create a file from the plot:</p>
<div><div><pre class="source-code">    plt.savefig(’scatter_x_y.png’)
    plt.close(fig)</pre>
</div>
</div>
<p>It helps to transform this cell’s code into a function. This function has proper type annotations so that a tool like <strong>mypy </strong>can confirm the types are used properly. It can also have unit test cases to be sure it really works.</p>
<p>The <code>savefig()</code> function will write a new file in PNG format with the image. If the file path suffix is <code>’.jpg’</code> then an SVG format file will be created.</p>
<p>The size of the figure is defined by the <code>figure()</code> function. There are often design and page layout considerations that suggest an appropriate size for a figure. This decision can be deferred, and the size can be provided by the markup used to create a final PDF file or HTML page. It’s often best, however, to create the figure in the required size and resolution to avoid any unexpected alterations as part of the final publication.</p>
<p>Once the diagram has been created, the Markdown needs to refer to the diagram’s PNG or SVG file so it can be included in a document. We’ll look at some examples of this in the next section. </p>


<h3 data-number="20.2.3">16.2.3  Including diagrams in the final document</h3>
<p>Diagrams are included in the final document by using markup commands to show where the diagram should be placed, and providing other information about captions and sizing.</p>
<p>The Markdown language has a tidy format for the simplest case of including an image in a document:</p>
<div><div><pre class="source-code">![Alt text to include!](path/to/file.png "Figure caption")</pre>
</div>
</div>
<p>Depending on the style sheet, this may be perfectly acceptable. In some cases, the image is the wrong size for its role in the document. Markdown permits using HTML directly instead of the <code>![image!](path)</code> construct. Including a diagram often looks like this:</p>
<div><div><pre class="source-code">&lt;figure&gt;
    &lt;img src="img/file.png"
         alt="Alt text to include"
         height="8cm"&gt;
    &lt;figcaption&gt;Figure caption&lt;/figcaption&gt;
&lt;/figure&gt;</pre>
</div>
</div>
<p>Using HTML permits more control over image size and placement via references to CSS.</p>
<p>When using RST, the syntax offers more options without switching to HTML. Including a diagram would be like this:</p>
<div><div><pre class="source-code">..  figure:: path/to/file.png
   :height: 8cm
   :alt: Alt text to include

   Figure caption</pre>
</div>
</div>
<p>Using this kind of markup technique creates considerable freedom. The report’s author can include content from a variety of sources. This can include boilerplate text that doesn’t change, the results of computations, some text based on the computations, and important diagrams.</p>
<p>The formatting of the markup has little impact on the final document. The way a browser renders HTML depends on the markup and the style sheets, not the formatting of the source file. Similarly, when creating a PDF document, this is often done by LaTeX tools, which create the final document based on LaTeX settings in the document’s preamble.</p>
<p>Now that we have an approach, we can look at the deliverable files that must be built. </p>



<h2 data-number="20.3">16.3  Deliverables</h2>
<p>This project has the following deliverables:</p>
<ul>
<li><p>Documentation in the <code>docs</code> folder.</p></li>
<li><p>Acceptance tests in the <code>tests/features</code> and <code>tests/steps</code> folders.</p></li>
<li><p>Unit tests for model module classes in the <code>tests</code> folder.</p></li>
<li><p>Mock objects for the <code>csv_extract</code> module tests that will be part of the unit tests.</p></li>
<li><p>Unit tests for the <code>csv_extract</code> module components that are in the <code>tests</code> folder.</p></li>
<li><p>An application to extend the summary written to a TOML file, including figures with diagrams.</p></li>
<li><p>An application secondary feature to transform the TOML file to an HTML page or PDF file with the summary.</p></li>
</ul>
<p>We’ll look at a few of these deliverables in a little more detail. We’ll start with some suggestions for creating the acceptance tests. </p>

<h3 data-number="20.3.1">16.3.1  Acceptance tests</h3>
<p>As we noted in the previous chapter’s section on acceptance testing, <a href="ch019.xhtml#x1-3340001"><em>Acceptance</em> <em>testing</em></a>, the output TOML document can be parsed and examined by the <code>Then</code> steps of a scenario. Because we’re looking at Anscombe’s Quartet data in the examples in this book, a subset of data for testing doesn’t really make much sense. For any other dataset, a subset should be extracted and used for acceptance testing.</p>
<div><div><p>It is often helpful to extract a small subset that’s used for acceptance testing. Instead of processing millions of rows, a few dozen rows are adequate to confirm the application read and summarized data. The data should be representative of the entire set of samples under consideration.</p>
<p>This subset is part of the testing suite; as such, it rarely changes. This makes the results predictable.</p>
</div>
</div>
<p>The secondary feature of this application — expanding on the TOML output to add extensive Markdown — also works with text files. This makes it relatively easy to create scenarios to confirm the correct behavior by reading and writing text files. In many cases, the <code>Then</code> steps will look for a few key features of the resulting document. They may check for specific section titles or a few important keywords included in boilerplate text. Of course, the test scenario can check for substitution values that are computed and are part of the TOML summary.</p>
<p>The automated testing can’t easily confirm that the document makes sense to prospective readers. It can’t be sure the colors chosen for the figures make the relationships clear. For this kind of usability test, a good copy editor or trusted associate is essential. </p>


<h3 data-number="20.3.2">16.3.2  Unit tests</h3>
<p>A unit test for a function to create a figure can’t do very much. It’s limited to confirming that a PNG or SVG file was created. It’s difficult for an automated test to “look” at the image to be sure it has a title, labels for the axes, and sensible colors.</p>
<p>It is important not to overlook the unit test cases that confirm output files are created. A figure that looks great in a Jupyter notebook will not get written to a file unless the CLI application saves the figure to a file.</p>
<p>For some applications, it makes sense to mock the <code>plt</code> package functions to be sure the application calls the right functions with the expected argument values. Note that a mocked version of <code>plt.subplots()</code> may need to return a tuple with several <code>Mock</code> objects.</p>
<p>We’ll need to define a complex collection of mock objects to form the fixture for testing. The fixture creation can look like the following example:</p>
<div><div><pre class="source-code">@fixture
def mocked_plt_module(monkeypatch):
    fig_mock = Mock()
    ax_mock = Mock(
        set_xlabel=Mock(),
        set_ylabel=Mock(),
        set_tiutle=Mock(),
        scatter=Mock(),
    )
    plt_mock = Mock(
        subplots=Mock(
            return_value=(fig_mock, ax_mock)
        ),
        savefig=Mock(),
        close=Mock()
    )
    monkeypatch.setattr(summary_app, ’plt’, plt_mock)
    return plt_mock, fig_mock, ax_mock</pre>
</div>
</div>
<p>This fixture creates three mock objects. The <code>plt_mock</code> is a mock of the overall <code>plt</code> module; it defines three mock functions that will be used by the application. The <code>fig_mock</code> is a mock of the figure object returned by the <code>subplots()</code> function. The <code>ax_mock</code> is a mock of the axes object, which is also returned by the <code>subplots()</code> function. This mocked axes object is used to provide axis labels, and the title, and perform the scatter plot request.</p>
<p>This three-tuple of mock objects is then used by a test as follows:</p>
<div><div><pre class="source-code">def test_scatter(mocked_plt_module):
    plt_mock, fig_mock, ax_mock = mocked_plt_module
    summary_app.scatter_figure([sentinel.X], [sentinel.Y])

    assert plt_mock.subplots.mock_calls == [call()]
    assert plt_mock.savefig.mock_calls == [call(’scatter_x_y.png’)]
    assert plt_mock.close.mock_calls == [call(fig_mock)]</pre>
</div>
</div>
<p>This test function evaluates the application’s <code>scatter_figure()</code> function. The test function then confirms that the various functions from the <code>plt</code> module are called with the expected argument values.</p>
<p>The test can continue by looking at the calls to the <code>ax_mock</code> object to see if the labels and title requests were made as expected. This level of detail — looking at the calls to the axes object — may be a bit too fine-grained. These tests become very brittle as we explore changing titles or colors to help make a point more clearly.</p>
<p>The overall use of mock objects, however, helps make sure the application will create the needed file with an image. </p>



<h2 data-number="20.4">16.4  Summary</h2>
<p>In this chapter, we’ve extended the automated analysis and reporting to include more use of the built-in <code>statistics</code> library to compute correlation and linear regression coefficients. We’ve also made use of the <strong>matplotlib </strong>library to create images that reveal relationships among variables.</p>
<p>The objective of automated reporting is designed to reduce the number of manual steps and avoid places where omissions or errors can lead to unreliable data analysis. Few things are more embarrassing than a presentation that reuses a diagram from the previous period’s data. It’s far too easy to fail to rebuild one important notebook in a series of analysis products.</p>
<p>The level of automation needs to be treated with a great deal of respect. Once a reporting application is built and deployed, it must be actively monitored to be sure it’s working and producing useful, informative results. The analysis job shifts from developing an understanding to monitoring and maintaining the tools that confirm — or reject — that understanding.</p>
<p>In the next chapter, we’ll review the journey from raw data to a polished suite of applications that acquires, cleans, and summarizes the data. </p>


<h2 data-number="20.5">16.5  Extras</h2>
<p>Here are some ideas for you to add to this project. </p>

<h3 data-number="20.5.1">16.5.1  Use pandas to compute basic statistics</h3>
<p>The <strong>pandas </strong>package offers a robust set of tools for doing data analysis. The core concept is to create a <code>DataFrame</code> that contains the relevant samples. The <code>pandas</code> package needs to be installed and added to the <code>requirements.txt</code> file.</p>
<p>There are methods for transforming a sequence of <code>SeriesSample</code> objects into a <code>DataFrame</code>. The best approach is often to convert each of the <strong>pydantic </strong>objects into a dictionary, and build the dataframe from the list of dictionaries.</p>
<p>The idea is something like the following:</p>
<div><div><pre class="source-code">import pandas as pd

df = pd.DataFrame([dict(s) for s in series_data])</pre>
</div>
</div>
<p>In this example, the value of <code>series_data</code> is a sequence of <code>SeriesSample</code> instances.</p>
<p>Each column in the resulting dataframe will be one of the variables of the sample. Given this object, methods of the <code>DataFrame</code> object produce useful statistics.</p>
<p>The <code>corr()</code> function, for example, computes the correlation values among all of the columns in the dataframe.</p>
<p>The <code>cov()</code> function computes the pairwise covariance among the columns in the dataframe.</p>
<p>Pandas doesn’t compute the linear regression parameters, but it can create a wide variety of descriptive statistics.</p>
<p>See <a class="url" href="https://pandas.pydata.org">https://pandas.pydata.org</a> for more information on Pandas.</p>
<p>In addition to a variety of statistics computations, this package is designed for interactive use. It works particularly well with Juypyter Lab. The interested reader may want to revisit <a href="ch017.xhtml#x1-29700013"><em>Chapter</em><em> 13</em></a>, <a href="ch017.xhtml#x1-29700013"><em>Project 4.1: Visual Analysis Techniques</em></a> using Pandas instead of native Python. </p>


<h3 data-number="20.5.2">16.5.2  Use the dask version of pandas</h3>
<p>The <strong>pandas </strong>package offers a robust set of tools for doing data analysis. When the volume of data is vast, it helps to process parts of the dataset concurrently. The <strong>Dask </strong>project has an implementation of the <strong>pandas </strong>package that maximizes opportunities for concurrent processing.</p>
<p>The <code>dask</code> package needs to be installed and added to the <code>requirements.txt</code> file. This will include a <code>pandas</code> package that can be used to improve overall application performance. </p>


<h3 data-number="20.5.3">16.5.3  Use numpy for statistics</h3>
<p>The <strong>numpy </strong>package offers a collection of tools for doing high-performance processing on large arrays of data. These basic tools are enhanced with libraries for statistics and linear algebra among many, many other features. This package needs to be installed and added to the <code>requirements.txt</code> file.</p>
<p>The <strong>numpy </strong>package works with its own internal array type. This means the <code>SeriesSample</code> objects aren’t used directly. Instead, a <code>numpy.array</code> object can be created for each of the variables in the source series.</p>
<p>The conversion might look like the following:</p>
<p>-</p>
<div><div><pre class="source-code">import numpy as np

x = np.array(s.x for s in series_data)
y = np.array(s.y for s in series_data)</pre>
</div>
</div>
<p>In this example, the value of <code>series_data</code> is a sequence of <code>SeriesSample</code> instances.</p>
<p>It’s also sensible to create a single multi-dimensional array. In this case, axis 0 (i.e. rows) will be the individual samples, and axis 1 (i.e. columns) will be the values for each variable of the sample.</p>
<p>An array has methods like <code>mean()</code> to return the mean of the values. When using a multi-dimensional array, it’s essential to provide the <code>axis=0</code> parameter to ensure that the results come from processing the collection of rows:</p>
<div><div><pre class="source-code">import numpy as np

a = np.array([[s.x, s.y] for s in series_data])
print(f"means = {a.mean(axis=0)}")</pre>
</div>
</div>
<p>See <a class="url" href="https://numpy.org/doc/stable/reference/routines.statistics.html#">https://numpy.org/doc/stable/reference/routines.statistics.html#</a></p>
<p>Using the least squares technique to compute the coefficients for a line can be confusing. The least squares solver in <strong>numpy.linalg </strong>is a very general algorithm, which can be applied to creating a linear model. The <code>numpy.linalg.lstsq()</code> function expects a small matrix that contains the ”x” values. The result will be a vector with the same length as each of the ”x” matrices. The ”y” values will also be a vector.</p>
<p>The processing winds up looking something like the following:</p>
<div><div><pre class="source-code">import numpy as np

A = np.array([[s.x, 1] for s in series_data])
y = np.array([s.y for s in series_data])
m, b = np.linalg.lstsq(A, y, rcond=None)[0]
print(f"y = {m:.1f}x + {b:.1f}")</pre>
</div>
</div>
<p>The value of <code>A</code> is a small matrix based on the x values. The value of <code>y</code> is a simple array of the y values. The least-squares algorithm returns a four-tuple with the coefficients, residuals, the rank of the source matrix, and any singular values. In the above example, we only wanted the vector of the coefficients, so we used <code>[0]</code> to extract the coefficient values from the four-tuple with the results.</p>
<p>This is further decomposed to extract the two coefficients for the line that best fits this set of points. See: <a class="url" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html">https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html</a>.</p>
<p>This approach has a distinct advantage when working with very large sets of data. The <strong>numpy </strong>libraries are very fast and designed to scale to extremely large data volumes. </p>


<h3 data-number="20.5.4">16.5.4  Use scikit-learn for modeling</h3>
<p>The <strong>scikit-learn </strong>library has a vast number of tools focused on modeling and machine learning. This library is built on the foundation of <strong>numpy</strong>, so both packages need to be installed.</p>
<p>The data needs to be converted into <strong>numpy </strong>arrays. Because the modeling approach is very generalized, the assumption is there may be multiple independent variables that predict the value of a dependent variable.</p>
<p>The conversion might look like the following:</p>
<div><div><pre class="source-code">import numpy as np

x = np.array([[s.x] for s in series_data])
y = np.array([s.y for s in series_data])</pre>
</div>
</div>
<p>In this example, the value of <code>series_data</code> is a sequence of <code>SeriesSample</code> instances. The <code>x</code> array uses a very short vector for each sample; in this case, there’s only a single value. It needs to be a vector to fit the generalized least-squares regression that <strong>scikit-learn </strong>is capable of solving.</p>
<p>The scikit-learn library is designed to create models in a very generalized way. The model isn’t always a simple line with a coefficient and an intercept that define the relationship. Because of this very general approach to modeling, we’ll create an instance of the <code>linear_model.LinearRegression</code> class. This object has methods to create coefficients that fit a given set of data points. We can then examine the coefficients, or use them to interpolate new values.</p>
<p>The code might look like the following:</p>
<div><div><pre class="source-code">from sklearn import linear_model

reg = linear_model.LinearRegression()
reg.fit(x, y)
print(f"y = {reg.coef_[0]:.1f}x + {reg.intercept_:.1f}")</pre>
</div>
</div>
<p>The linear model’s <code>coef_</code> attribute is a vector of coefficients, the same length as each row of the <code>x</code> independent variable values. Even when the row length is 1, the result is a vector with a length of 1.</p>
<p>Because this works with <strong>numpy </strong>it can work with very large sets of data. Further, the scikit-learn approach to creating models to fit data generalizes to a number of machine-learning approaches. This is often the next step in creating richer and more useful models. </p>


<h3 data-number="20.5.5">16.5.5  Compute the correlation and regression using functional programming</h3>
<p>The computations for correlation and the coefficients for a line can be summarized as follows. First, we’ll define a function <em>M</em>(<em>a</em>;<em>f</em>()) that computes the mean of a transformed sequence of values. The <em>f</em>() function transforms each value, <em>a</em><sub>i</sub>. An identity function, <em>ϕ</em>(<em>a</em><sub>i</sub>) = <em>a</em><sub>i</sub>, does no transformation:</p>
<div><img alt="M (a;f()) = 1-∑ f(a) N i " class="math-display" src="img/file68.jpg"/>
</div>
<p>We’ll also need a function to compute the standard deviation for a variable, <em>a</em>.</p>
<div><img alt=" ∘ -∑--------- --(ai −-¯a)2 S (a) = N " class="math-display" src="img/file69.jpg"/>
</div>
<p>This lets us define a number of related values as mean values after some transformation.</p>
<div><img alt="¯x = M (x;f(ai) = ai) " class="math-display" src="img/file70.jpg"/>
</div>
<div><img alt="¯y = M (y;f(ai) = ai) " class="math-display" src="img/file71.jpg"/>
</div>
<div><img alt="--- x2 = M (x;f(ai) = a2i) " class="math-display" src="img/file72.jpg"/>
</div>
<div><img alt="--- y2 = M (y;f(ai) = a2i) " class="math-display" src="img/file73.jpg"/>
</div>
<div><img alt="--- xy = M (x,y;f(ai,bi) = ai × bi) " class="math-display" src="img/file74.jpg"/>
</div>
<p>From these individual values, we can compute the correlation coefficient, <em>r</em><sub>xy</sub>.</p>
<div><img alt=" --- -----xy-−-¯xy¯------ rxy = ∘ --2---2---2----2- (x − ¯x )(y − y¯) " class="math-display" src="img/file75.jpg"/>
</div>
<p>In addition to the above values, we need two more values for the standard deviations of the two variables.</p>
<div><img alt="sx = S(x)sy = S(y) " class="math-display" src="img/file76.jpg"/>
</div>
<p>From the correlation coefficient, and the two standard deviations, we can compute the coefficient of the line, <em>m</em>, and the intercept value, <em>b</em>.</p>
<div><img alt="m = rxysy sx " class="math-display" src="img/file77.jpg"/>
</div>
<div><img alt="b = ¯y − m ¯x " class="math-display" src="img/file78.jpg"/>
</div>
<p>This yields the coefficient, <em>m</em>, and intercept, <em>b</em>, for the equation <em>y </em>= <em>mx </em>+ <em>b</em>, which minimizes the error between the given samples and the line. This is computed using one higher-order function, <em>M</em>(<em>a</em>;<em>f</em>()), and one ordinary function, <em>S</em>(<em>a</em>). This doesn’t seem to be a significant improvement over other methods. Because it’s built using standard library functions and functional programming techniques, it can be applied to any Python data structure. This can save the step of transforming data into <strong>numpy </strong>array objects. </p>



</body>
</html>
