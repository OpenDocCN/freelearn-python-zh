<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">A More In-depth View of the RESTful Services Paradigm</h1>
                </header>
            
            <article>
                
<p>With the grand arrival of a bevy of futuristic and flexible information technologies, automated tools, optimized infrastructures, integrated platforms, and multifaceted devices, the world is bound to experiment and experience hitherto unknown software applications and services toward empowering businesses and people. With the faster proliferation of slim and sleek, handy and trendy smartphones, we are destined to have a dazzling array of easy-to-use and eye-catching mobile applications and services. With wearables, portables, fixed devices, handhelds, and wireless gadgets joining in mainstream computing, the application scope, size, speed, and structure are bound to escalate appreciably. To fit applications into every segment of devices, software developers across the world are expected to bring forth customizable, composable, and configurable applications by leveraging the potential and promising programming languages, platforms, toolkits, software-development methodologies, design patterns, and best practices. The software landscape is, therefore, expanding continuously with the utmost confidence and clarity. Hence, the rollout and the impact of pioneering software packages, homegrown applications, turnkey solutions, insightful platforms, and process-aware composite software are running on expected lines. That is, we are tending toward the software-defined world—software is the vital cog in humanity's bright future.</p>
<p>This book, especially this chapter, is dedicated to explaining the emerging techniques and tips to produce RESTful services and their corresponding APIs quickly and easily. This chapter has the following objectives:</p>
<ul>
<li>Software-defined and driven world</li>
<li>Describing the emerging application types</li>
<li>The REST paradigm for application modernization and integration</li>
<li>The RESTful services for digital transformation and intelligence</li>
<li>Best practices for REST-based microservices</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>There are no special technical requirements for this chapter as it doesn't talk about designing, developing, or deploying a software package. Other chapters describe how RESTful APIs for microservices are being designed and used. Further on, a few chapters explain the nitty-gritty of producing standalone and utility-like RESTful services for application and data integration. API design techniques and best practices are also covered in detail in this book. This chapter throws more light on how the enigmatic RESTful paradigm is going to be a trendsetter for the forthcoming era of a software-defined world. The RESTful techniques, tools, and tips presented in the other chapters come good for this chapter. Precisely speaking, the role and responsibility of the path-breaking RESTful idea is bound to escalate, with the world tending toward the one fully enriched, enabled, and empowered by adaptive and adroit software.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tending toward the software-defined and software-driven world</h1>
                </header>
            
            <article>
                
<p>Every common and casual thing in our mid gets digitized in a systematic manner to be extremely computational, communicative, responsive, and active. With a variety of network options, every kind of embedded system (small and large) becomes interlinked with one another in order to be purposefully discoverable, accessible, assessable, interoperable, and collaborative. That is, all kinds of heterogeneities and complexities that constitute a significant barrier in terms of enabling digitized objects and connected devices to talk to one another in a sensible fashion are being immediately decimated through software-enabled abstraction techniques and tips. It is a well-known and indisputable truth that software eats the world. Every device, consumer electronics, industry machine, flying drone, humanoid robot, medical instrument, home utensil and ware, enabling toolkit, handheld, networkable wearable, portable, fixed system, network solution, automobile, and engine in our everyday environment is getting empowered by embedded software adapters, connectors, and drivers. That is, software enablement is the most vital process and target for everyday objects to be empowered accordingly to decisively join in the mainstream IT. Anything stuffed with some relevant software snippets can become digitally important. That is, ordinary things are getting readied to be extraordinary for the ensuing era of the digital economy. Advanced cars and vehicles are being stuffed with a lot of software modules in order to establish and ensure high-level automation.</p>
<p>Similarly, home, building, and industrial automation is accomplished through the embedding of software libraries. Flights, drones, robots, SCADA systems, sensors, actuators, industrial machinery, boilers, and oil rigs, are being enriched with software agents in order to be software-enabled. Thus, software enablement has become a mandatory assignment for <strong>original equipment manufacturers</strong> (<strong>OEMs</strong>). The next question is how to enable software-defined and -driven systems to find one another in order to initiate fruitful collaborations. APIs have emerged as the mechanism to assist in facilitating seamless and spontaneous device integration.</p>
<p>RESTful services and APIs are the prominent and dominant method for enabling <strong>device-to-device</strong> (<strong>D2D</strong>) and <strong>device-to-cloud</strong> (<strong>D2C</strong>) integration capabilities. The grandiose vision of <strong>cyber-physical systems</strong> (<strong>CPS</strong>) is to get nourished and then flourish with the smart application of RESTful methods.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Software-enabled clouds for the digital intelligence era</h1>
                </header>
            
            <article>
                
<p>As outlined previously, the role and responsibility of software is on the climb. The software is participative, pervasive, and persuasive, too. All business establishments embrace software engineering technologies and tools to bring in deeper and more decisive automation. Industry verticals are keen on strategizing and strengthening their software portfolio in order to get ahead of their competitors. Software developers across the globe build, curate, refine, and deposit their software applications in publicly available software registries/repositories to facilitate overwhelming usage. Therefore, with the faster proliferation of software modules, the application of software packages, products, and programs is growing steadily. The application landscape is expanding its horizon constantly. The application types are also expanding correspondingly to tackle more complex requirements, since there is a widespread realization that software decides the automation journey. In the following sections, we'll discuss the principal application types.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The IoT applications and services</h1>
                </header>
            
            <article>
                
<p>With the faster maturity and stability of scores of connectivity and integration technologies, the mesmerizing IoT era has started to unfold and supply its unique contributions for society as a whole. Without an ounce of doubt, the IoT paradigm will bring forth a variety of innovations, disruptions, and transformations for all kinds of business enterprises and organizations. If leveraged appropriately and aggressively, every business vertical is bound to enjoy the distinct benefits of the flourishing IoT model for a long time. Not only businesses, but also every individual, institution, and innovator will benefit immensely from the growing IoT power. The short-term and long-term implications of the IoT domain are undoubtedly tremendous and trend setting as per the reports of worldwide market research and analyst groups. The IT discipline, which is being widely recognized as the greatest business enabler, is receiving a massive boost with the arrival and articulation of the IoT concept. The IoT idea is permeating quickly and becoming pervasive and persuasive too.</p>
<p>With the continued spread and adoption of the IoT paradigm, the business and IT worlds are going to be bombarded with a number of premium IoT applications and services. The various industry domains, including manufacturing, retail, energy, healthcare, smart cities, government, defense, utility, and logistics, are meticulously exploring, and experimenting with, various IoT technologies and tools in order to get ahead of their competitors. A dazzling array of pioneering IoT use cases are being illustrated by the various industry segments in order to be correct and relevant to their loyal consumers.</p>
<p>The first and foremost implication of the IoT concept is the grandiose and voluminous realization of digitized entities and elements (alternatively touted as smart objects). With the systematic leveraging of edge technologies, all kinds of everyday things are being transitioned to be self-, surrounding-, and situation-aware. The second noteworthy output arising from the advancements in the IoT space is the faster proliferation of connected devices. All kinds of embedded systems are being networked and made to join in the mainstream computing. The third and final result is the growing array of device services, which are typically microservices. With the speedier adoption of the <strong>microservices architecture</strong> (<strong>MSA</strong>), we will be bombarded with a large number of microservices to produce next-generation enterprise, cloud, web, and mobile applications. It's anticipated that by the year 2020, we will have the following:</p>
<ul>
<li>Millions of microservices</li>
<li>Billions of connected devices</li>
<li>Trillions of digitized entities</li>
</ul>
<p>We have massive cloud centers for large-scale data storage and processing. We have both shared and dedicated network infrastructures in order to quickly carry data to faraway cloud environments. There are persistent and transient storage options. The costs of cloud storage capacities are coming down steadily. The faster proliferation of cloud centers across the world takes away the worry of setting up and sustaining massive IT infrastructures locally. There are insightful and integrated big, fast, streaming and IoT data analytics platforms. There are many enabling frameworks, automated tools, and powerful engines that are emerging and evolving to accelerate the process of transitioning data to information to knowledge, and then to wisdom. Thus, the emergence and convergence of several proven and potential technologies and tools simplifies and speeds up data analytics and mining for the purpose of extracting actionable insights. In short, the communication and collaboration of IoT devices results in a huge amount of poly-structured data, which has to be consciously captured, cleansed, and crunched in order to unearth hidden patterns, useful associations, bigger possibilities, newer opportunities, avoidable risks, and real-world intelligence. The knowledge discovered gets disseminated to the correct systems, devices, applications, data sources, and storages in order to empower them to determine their next course of action.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cloud-enabled applications</h1>
                </header>
            
            <article>
                
<p>As articulated in the preceding section, we will look at a variety of IoT applications and services in the days ahead. Besides, we will also see enterprise-scale, distributed applications running on cloud servers. With the overwhelming success of MSA, all kinds of legacy (monolithic) applications are methodically being enabled to be cloud-ready and microservices-centric applications, which are famous for scalability, availability, and flexibility. That is, massive applications are being partitioned into a number of fine-grained, publicly-discoverable, network-accessible, interoperable, API-enabled, composable, portable, horizontally-scalable, and independently-deployable microservices. There are application-modernization and -migration toolkits to move refactored and remedied applications to cloud environments in order to reap all the originally envisaged benefits of the cloud conundrum. There are best practices, integrated platforms, and patterns (architecture, design, integration, orchestration, security, and deployment) aplenty to quickly undertake legacy modernization to produce cloud-enabled applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cloud-native applications</h1>
                </header>
            
            <article>
                
<p>Newer applications are being designed, developed, debugged, and deployed directly in cloud environments. There are production, execution, and orchestration platforms made available on cloud environments (private, public, and hybrid). These applications are intelligent enough to accrue all the benefits of cloud computing. Microservices are the basic and optimal building block for building enterprise-scale and business-critical applications to be delivered through cloud environments. Microservices are formally containerized and deployed multiple times per day in order to meet the varying expectations of businesses and customers. There are a number of automated tools that eliminate all sorts of friction between development and operation teams so as to ensure the faster delivery of updated and upgraded applications to their users and subscribers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mobile, handheld, and wearable applications</h1>
                </header>
            
            <article>
                
<p>Today, there are billions of smartphones acting as the main device to connect, access, and assess software applications any time, anywhere, and on any network. That is, all kinds of cloud, web, and enterprise applications are being stuffed with mobile interfaces. This mobility enablement has become the norm for application developers and providers. Not only applications, but also software and hardware infrastructures, are being fitted with mobile interfaces to enable remote monitoring, management, diagnosing, and repairs. In our everyday life, we encounter millions of mobile apps being developed and stocked in mobile stores. Smartphone users can download and install them with ease. Thus, these days, mobile, handheld, and wearable applications are receiving a lot of attention and affection from users and developers alike. We need competent mechanisms to attach APIs to these applications so that other applications/services can easily find and bind to come out with business-centric applications. There are several easy-to-understand and -use methods and mechanisms for producing technology-agnostic RESTful services and their APIs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transactional, operational, and analytical applications</h1>
                </header>
            
            <article>
                
<p>Enterprise applications vary in their capabilities. They not only have to fulfill their functional requirements well, but should also fulfil a number of <strong>non-functional requirements</strong> (<strong>NFRs</strong>), which are known as the <strong>quality of service</strong> (<strong>QoS</strong>) and <strong>quality of<span> </span>experience</strong> (<strong>QoE</strong>) attributes. Performance, scalability, sustainability, modifiability, extensibility, availability, resiliency, security, reliability, and adaptability requirements are being categorized as the important NFRs. As for the public, open and cheap internet emerges as the prime communication infrastructure, since most of the applications are web-enabled. That is, web interfaces are the widely-used mechanism for accessing web-scale applications.</p>
<p><strong>Business-to-business</strong> (<strong>B2B</strong>) and <strong>business-to-consumer</strong> (<strong>B2C</strong>) applications are typically transactional in nature. There are viable techniques for ensuring complex transactions. With the spread of geographically distributed systems, the need for distributed transactions becomes more important. As we all know, the web is the dominant and decisive way of accessing information, content, applications, services, and data sources. Therefore, a number of <strong>online transaction processing</strong> (<strong>OLTP</strong>) systems have arisen in order to natively support different transaction requirements.</p>
<p>Like transactions, data analytics occupies the central part of next-generation, enterprise-scale systems. Data gets carefully collected, cleansed, and crunched in order to extract actionable insights from growing data heaps. Extracted insights are used to make intelligent and real-time decisions. Precisely speaking, these are going to be data-driven insights and insight-driven decisions. There is no place for intuition-based decisions any more. There are analytical platforms and data warehouses/marts/cubes that facilitate data mining, analytics, and investigations. Analytics is the core of any business application, IT platform, and infrastructure to deliver their unique services. All of the next-generation applications are innately empowered to be analytic. Existing applications are being enabled to be analytic by incorporating analytical applications, such as <strong>online analytical processing</strong> (<strong>OLAP</strong>) applications. Thus, analytical applications are going to be the prime component of mainstream IT.</p>
<p>Finally, with the seamless linkage of operational technology and information technology environments, software applications hosted in enterprise and cloud servers have to be able to receive real-time operational data and event messages to be right and relevant for their consumers. A variety of operational applications and databases are emerging to meet the evolving needs of operational environments. Ground-level operational systems emit a lot of time-series data and event data, and they get streamed to enterprise systems (transactional and analytical data stores). There are data analytics platforms that make sense of the data streaming into them. Smarter environments, such as smarter hotels, homes, hospitals, manufacturing floors, and <strong>cloud-enabled data centers</strong> (<strong>CeDCs</strong>), are going to be empowered when they get seamlessly integrated with nearby or faraway transactional and analytical systems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Knowledge visualization applications</h1>
                </header>
            
            <article>
                
<p>The ensuing era is definitely knowledge-centric. Knowledge discovery and dissemination are going to be the prime activity for knowledge workers. Software services, personal as well as professional devices, IT systems, and business applications have to be supplied with real-time and real-world knowledge in order to be adaptive terms of in their actions and reactions. There are 360-degree dashboards, visualization platforms, and report generation tools available in order to graphically and illustratively display and convey the results. Let's look at these applications in more detail.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Social applications </h1>
                </header>
            
            <article>
                
<p>This set of applications is currently very popular among young people. Typically, web 1.0 applications are simple and one-way, whereas web 2.0 applications are social and facilitate two-way communication. That is, users not only read, but also write back. To facilitate outside-in thinking, social applications are the way forward. There are several social and professional applications that empower society as a whole. Digital communities are being formulated and employed to equip people with their skills and knowledge sharing. Social applications, because of their large number of subscribers and followers, produce a lot of useful data. When social- and people-related data gets consistently collected and subjected to a variety of investigations, individuals and institutions are bound to find a lot of actionable insights. There are new types of analytical capabilities, such as social network analytics, behavioral analytics, and sales promotion and marketing campaign analytics, that have emerged due to the widespread proliferation of social applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scientific and technical applications </h1>
                </header>
            
            <article>
                
<p><span>Software plays a vital role in shaping various scientific and technical applications. Scientific experiments generate a lot of data, which can be captured and crunched to generate usable results. Similarly, there are technical applications that leverage the software capabilities. These two disciplines require the unique contributions of software platforms, products, patterns, and processes in order to be highly relevant to their users. There are mathematics-specific software packages aplenty to help mathematicians with their research activities; other science, technology, engineering, and art disciplines are benefiting immensely and immeasurably from the advancements and developments in the software space. There are innumerable innovations in the <strong>information and communication technology</strong> (<strong>ICT</strong>) landscape toward software deployment and execution.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Centralized and distributed applications</h1>
                </header>
            
            <article>
                
<p>There has been a swing between centralized and distributed computing models. Clouds typically are centralized, consolidated, and converged environments that host applications. Recently, clouds have been federated in order to host and run distributed applications. Considering the exponential growth of data and the complexity of applications, the onset of distributed computing can't be stopped. With the cloud paradigm, leveraging commodity servers for large-scale application and data processing is gaining momentum. Horizontal scalability is preferred over vertical scalability and, hence, distributed systems are enjoying a surge in popularity. Applications are also being methodically partitioned to be distributed. Data gets distributed across thousands of worker/slave nodes. The computation moves to the place where data resides. There are a number of paradigm shifts with the explosion in distributed computing. The issue of network latency comes into the picture. Server virtualization has also spread to network and storage virtualization with the greater acceptance and adoption of distributed computing. The arrival and articulation of MSA lead to the realization of distributed applications. In short: distributed computing and applications are inevitable. They have to be welcome in order to meet fast-evolving business and IT requirements.</p>
<p>There are certain limitations associated with centralized environments and applications. The overarching views are that distributed systems ensure high availability, affordability, and scalability. That is, the motto of future IT is distributed deployment and centralized monitoring, measurement, and management.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decentralized and intelligent applications with blockchain technology </h1>
                </header>
            
            <article>
                
<p>As noted in the preceding section, centralized applications are well-suited to certain situations. However, in the recent past, there has been an increased market for decentralized applications, with the faster adoption and adaptation of the blockchain technology. The blockchain paradigm promises a bevy of disruptions and transformations when realizing and running decentralized applications across multiple industry verticals. The issues that face centralized systems and applications are nullified in decentralized services and solutions. Typically, decentralized systems are owned and operated by different organizations and, hence, the security of decentralized software is technologically tightened. The much-touted unbreakable and impenetrable security of software systems is guaranteed through the decentralized approach. The <strong>peer-to-peer</strong> (<strong>P2P</strong>) interactions facilitated by the decentralized approach is turning out to be a silver bullet for many recent use cases. The faster maturity and stability of blockchain technology is clearly driving IT professionals and organizations toward the production of decentralized systems. The blockchain paradigm also resulted in the new concept of smart contracts, which leads to the realization of adaptive applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Composite and multi-container applications </h1>
                </header>
            
            <article>
                
<p>Decomposition and composition techniques have been extensively used to achieve breakthroughs in software engineering. Monolithic applications are being dismantled through decomposition tips, whereas decomposed application modules are being combined with one another in a sequenced manner in order to create smarter and more sophisticated applications. With containers emerging as the most appropriate runtime environment for microservices, we need to produce enterprise-scale, mission-critical, and adaptive applications out of containerized microservices. There are composition (orchestration and choreography) platforms and engines to simplify and speed up the act of building process-aware and people-centric applications. There are process-enabling languages that help us to develop process-optimized and -integrated applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Event-driven applications </h1>
                </header>
            
            <article>
                
<p>We still have scores of monolithic and mainframe applications, especially in the financial sector. Today, most of the applications are following the client/server style. There are cloud (online, on demand, and off-premise) applications. With a variety of additional and third-party systems joining, the application architecture moves to <em>n</em>-tier distributed computing. With blockchain, the P2P architectural pattern is getting a lot of attention from various stakeholders. There are other variants, such as <strong>service-oriented architecture</strong> (<strong>SOA</strong>) and <strong>resource-oriented architecture</strong> (<strong>ROA</strong>), that inherently support request/response, and fire and forget. And also, there are ways to achieve light and loose coupling between participating components.</p>
<p>But the future undoubtedly belongs to <strong>event-driven architecture</strong> (<strong>EDA</strong>), which is the way forward to realize sensitive and responsive (S and R) applications. With the unprecedented explosion of independent, yet connected, devices, the EDA style facilitates decoupled applications. Any noteworthy event or state change triggers other devices and applications to jump into action. There are event stores and processing platforms (open source as well as commercial-grade solutions). Simple/atomic events get accumulated and aggregated to form complex events. Event messages are streamed to be subjected to a variety of investigations, which are greatly simplified through leveraging streaming analytics solutions and solutions. There are enabling frameworks for event and stream processing. Applications are being innately empowered to be adaptive, based on any insights extracted from event messages. Applications are going to be people-centric and proactive, while delivering their unique services. EDA is one of the prescribed ways to build intelligent systems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">High-quality applications</h1>
                </header>
            
            <article>
                
<p>As discussed, there are I/O device-specific, server-centric, language-oriented, architecture-inspired, and technology-agnostic applications aplenty. Today, most applications are being coded to fulfil the functional requirements identified. With IT being pronounced as the greater business-enabler, evolving business requirements are insisting that IT professionals and professors devise workable ways of incorporating NFRs into the source code. The prominent NFRs include performance, scalability, availability, resiliency, reliability, security, extensibility, accessibility, and modifiability. These QoS and QoE attributes are being mandated to be elegantly embedded in our everyday software applications. There is a new discipline that is emerging and grasping the attention and affection of software engineers and architects—<strong><span>site reliability engineering</span></strong><span> </span><span>(</span><strong><span>SRE</span></strong><span>)</span><span>; that is, not only business applications and IT systems produced in an agile fashion, but also they have to be designed, developed, debugged, delivered, and deployed in a highly reliable manner. The written goal is to ensure application resiliency and reliable IT infrastructures. Future challenges for IT experts are many and diverse. Building high-quality applications is beset with innumerable difficulties. Scholars and scientists are working overtime to bring forth best practices, knowledge guides, optimized processes, architectural and design patterns, integrated platforms, competent infrastructures, and easy-to-understand and -use procedures to simplify and streamline the production of high-quality software systems.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Resilient applications </h1>
                </header>
            
            <article>
                
<p><span>As mentioned elsewhere, microservices is positioned as the preferred element for building and deploying next-generation applications. Different and distributed microservices, when composed, form flexible applications. With the widespread insistence of reliable IT systems and business applications, there are viable methods emerging and evolving fast for embedding precisely the much-needed reliability competency into software systems. Generally speaking, system reliability is the application's resiliency and the system's elasticity. That is, when faced with any internal or external attack, the application has to survive in order to continuously deliver its obligations. Applications have to be innately empowered to proactively detect any issues, and then contain them to stop them from cascading into other system components. Thus, systems have to be technologically fault-tolerant in order to be highly available. In short, the resiliency capability is to identify and avoid issues without bringing down the whole system. The deployment of additional instances for each service comes in handy when fulfilling user requests. The second aspect is the elasticity feature; that is, when systems are under heavy load, they have to scale up or out accordingly to tackle the extra rush of users and data messages. Thus, besides ensuring the utmost security for the application and data, guaranteeing reliability is gaining importance.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The REST paradigm for application modernization and integration</h1>
                </header>
            
            <article>
                
<p>Monolithic and massive applications are being modernized and migrated to cloud environments in order to reap all the originally envisaged benefits of the cloud computing model. Microservices are emerging as the most optimized building block to produce enterprise-scale applications. Not only for development but also for application modernization, microservices are being touted as the most suitable approach. That is, legacy applications are being systematically partitioned into multiple interoperable, portable, publicly discoverable, network accessible, reusable, composable, fine grained, technology-agnostic, containerized, horizontally scalable, and independently deployable microservices. The point here is that every microservice exposes one or more interfaces. RESTful interfaces are the most popular ones for microservices to connect and compose bigger and better services. Microservices are therefore typically RESTful services. Thus, application refactoring and remediation are being sped up and streamlined through RESTful services and their APIs. Service, application and data integration, and orchestration happen through RESTful APIs.</p>
<p>In short, new technologies and toolkits, programming and script languages, architecture and design patterns, integrated platforms, pioneering algorithms, enabling frameworks, composable and clustered infrastructures, optimized processes, fresh building blocks, data formats, and protocols are constantly emerging and impacting the discipline of software engineering. Agile software development methodologies are getting the importance to build applications quickly. Besides, the role and responsibility of microservices is increasingly felt in realizing enterprise-scale applications. That is, besides agile techniques, microservices contribute immensely to the rapid development of applications. In other words, applications are being readied instantaneously by compositing multiple microservices.</p>
<p>Applications are mainly interdependent. They can't work in isolation. They have to be integrated dynamically to offer users an integrated experience. Applications also have to be linked up with other applications, data sources and stores, data processing and data analytics platforms, and messaging and middleware systems. Thus, the inescapable integration has to happen via well-intended and designed APIs. Finally, legacy applications have to be dismantled into easily manageable and loosely coupled modules. These modular components, in conjunction with management solutions, will significantly enhance their utilization, efficiency, visibility, and controllability. Further on, on a per-need basis, several modules can be picked up and combined to create bigger and better applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Application programming interfaces</h1>
                </header>
            
            <article>
                
<p>We are heading toward everything as a service. <strong>Application programming interfaces</strong> (<strong>APIs</strong>) have become the technology of choice for enterprises to express and expose their capabilities as services. Every service has to have one or more interfaces and backend implementations. Companies around the world are plunging into the usage of APIs. A few unique usage models have emerged to assist business enterprises in meeting changing business needs with all alacrity and adaptivity. Enterprises that are strategizing for digital transformation are expediting the task of leveraging multiple channels and RESTful APIs to get ahead of their competitors. APIs have become the strategic asset for organizations to be easily connected with their business partners, suppliers, retailers, distributors, warehouse providers, logistics and supply chain experts, and consumers. It's simply the API economy. They are enabling design patterns and usage models as ambitious businesses across the world embrace the concept of APIs.</p>
<p>This section looks at the following four APIs usage models that can address business needs with all the requisite agility and efficiency:</p>
<ul>
<li>Public APIs</li>
<li>Internal and private APIs</li>
<li>APIs for IoT sensors and actuators</li>
<li>APIs for integration</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Public APIs for external integration and innovation</h1>
                </header>
            
            <article>
                
<p>We're being bombarded with a number of next-generation and novelty-attached I/O devices. With miniaturization technologies flourishing consistently, we're being supplied with a slew of slim and sleek, multifaceted, and powerful smartphones, tablets, wearables, portables, and other IoT devices. That is, the digital device ecosystem is expanding continuously. On the other side, the rapidly accumulating digital content, information, and services are being made available to be found, accessed, and consumed anywhere, at any time, on any network, and on any device. That is, there are fresh channels to connect with enterprise and cloud servers to acquire and aggregate the various services, such as information, commercial, transaction, analytical, and other online services. In order to standardize service discovery, matching, and leveraging, the widely recommended approach is to leverage APIs. APIs are emerging as the next-generation channel for enterprises to share their services and information in a controlled manner. With the steadily growing digital ecosystem, APIs have emerged as the way forward for multinational corporations to deliver their offerings to a wider market. With the explosion in digital assets, APIs are being established as the prime method for accessing, assessing, and using digital assets. There is a rush of digital service providers. To create enhanced business value, geographically distributed service providers need to be identified and integrated with one another on-demand through APIs. Every service is being blessed with one or more APIs. There are API management and gateway solutions from the open source community as commercial-grade solution providers to reduce the API development, operational, and management complexities.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Private APIs for internal purposes </h1>
                </header>
            
            <article>
                
<p>Internal APIs improve utilization and enhance efficiency within and across an enterprise. Internal APIs makes it easy for internal developers to discover and consume internal services in a free-flowing fashion. The emerging trend is that every worthwhile application meticulous being service and API-enabled. Typically, every enterprise is blessed with a variety of backend systems, such as database management systems, <strong>message-oriented middleware</strong> (<strong>MoM</strong>) solutions, message brokers and queues, data processing and data analytics systems, and knowledge visualization tools, which are service enabled to expose their own interfaces (APIs) to facilitate the goals of service connectivity, integration, and orchestration. When an organization wants to create fresh APIs for internal use, it has to add them on top of the service APIs of existing systems.</p>
<p>There are a few public APIs, but there are many APIs for internal use because there may be hundreds of internal services that leverage multiple data formats and transmission protocols. To strongly foster reuse, speed, efficiency, and agile application development, enterprises should publish their internal APIs in a searchable catalogue.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">APIs for IoT devices</h1>
                </header>
            
            <article>
                
<p>We discussed IoT devices and their services to humankind at the beginning of this chapter. It's anticipated that there will be 50,000,000,000 connected devices in the years ahead. Disposable, yet indispensable, devices are being produced in large quantities to automate everyday activities. Embedded devices are networked to team up with one another in a collaborative manner in order to develop and deliver situation-aware services to people. There are novel use cases being unearthed and rolled out to enhance the proliferation and penetration of the IoT paradigm. Due to the multiplicity and heterogeneity of IoT devices, the operational, management, and security complexities of IoT devices are being deployed in homes, hotels, hospitals, retail stores, manufacturing floors, railway stations, restaurants, and self-driving cars. Nowadays, sensors and actuators have become the eyes and ears of every digital application these days. Every device is becoming computational, communicative, perceptive, and active.</p>
<p>There are edge and fog devices with sufficient power to form ad hoc clouds to capture, store, process, and analyze real-time, time-series, and streaming data to extract actionable insights that can be looped back to devices and people to make intelligent decisions in time and to engage in correct and relevant activities with all the clarity and confidence. Futuristic IoT applications and services need to be exposed via RESTful APIs in order to perform device integration and service orchestration.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">APIs for application integration</h1>
                </header>
            
            <article>
                
<p>APIs are bound to play a vital role in fulfilling the complicated tasks of process, data, and application integration. SOA has laid down a stimulating platform for service-oriented applications and integration. The faster maturity and stability of <strong>e</strong><strong>nterprise service bus</strong> (<strong>ES</strong><strong>B</strong>) products has expedited the setting up and sustenance of service-oriented, integrated, and insight-driven enterprises. The API-driven integration of disengaged services for producing integrated systems is attracting a lot of attention.</p>
<p>In short, worldwide enterprises are embracing APIs as a strategic path to achieve the much-touted digital disruption, innovation, and transformation. External APIs opens the way for clients' developers to implement and involve the exposed APIs to connect and collaborate with the software application. The remote monitoring, measuring, and managing of applications is being made possible by means of externally exposed APIs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Describing the RESTful services paradigm</h1>
                </header>
            
            <article>
                
<p>The developing trend in the software engineering space is that applications are now being made out of multiple services through integration, orchestration, and choreography. Workflows are the prominent unifying factor. That is, they help to identify the relevant services, and the chosen services get integrated into proper sequencing to arrive at competent composite services and applications. The challenge is to service discovery, access, assess, and use. There are communication and data transmission protocols to facilitate service-to-service interactions and collaborations. The REST approach is becoming easy to understand and use. This section will describe how REST is streamlined and how interactive services lead to powerful applications.</p>
<p>A resource, which is the primary architectural component of the ROA, is similar to an object in the popular <strong>object-oriented programming</strong> (<strong>OOP</strong>) concept. As objects, there are methods that work on the resource directly. The key difference between OOP and ROA is that only a few standard methods are defined for the resource. However, for an object, there can be one or more methods. Further on, resources in ROA can be consolidated into<span> </span>collections of resources. The condition is that each such collection created has to be homogeneous. <span>Resources can be blessed with data. The richness of data that gets  associated with a resource comes handy in leveraging the resource in an efficient and effective manner.  JSON is the most appropriate and popular data model to appropriately enrich resources. </span>Resources are increasingly represented using JSON objects. The special <kbd>_type</kbd> key-value pair is used overwhelmingly to store the type of any resource. The JSON data types such as string, number, Boolean, null, or arrays are the values of any key-value pairs. </p>
<p><span>The REST paradigm comprises three prominent classes of architectural elements: c</span>onnectors, c<span>omponents, and data elements.</span></p>
<p>A <strong>connector</strong> is a thing that connects the point of your reference to a destination system. Connectors are responsible for identifying and accessing various web resources and changing the current representations of those identified resources. Roles are the interface for various components that get implemented by different programming languages: </p>
<ul>
<li><strong>Client connector</strong>: REST is for the request-and-response paradigm. Clients send requests and get appropriate responses from resources.</li>
<li><strong>Server connector</strong>: RESTful servers continuously listen for requests from clients. Servers deliver responses for requests.</li>
<li><strong>Cacheable responses</strong>: Can be stocked in clients or servers in order to speed up the response for subsequent requests. Several clients can concurrently get cached information.</li>
<li><strong>Resolver</strong>: Translates resource identifiers into network addresses.</li>
<li><strong>Tunnel</strong>: Relays service requests. Sometimes, components deviate from their primary obligations to do tunneling.</li>
</ul>
<p><span>In the REST paradigm, the various pieces of software that interact with one another are called <strong>components</strong>: </span></p>
<ul>
<li><strong>Server software</strong>: This software solution uses a server connector to receive the request from the client software. The server is the source for resources and their representations. </li>
<li><strong>Client software</strong>: Clients use a client connector to formulate and convey a request to the server and it receives the response. </li>
<li><strong>Gateway software</strong>: This is a kind of middleware solution that intelligently translates the requests and responses that happen between clients and servers. </li>
</ul>
<p class="mce-root"><strong>Data elements</strong> are<span> an important ingredient of the powerful RESTful service paradigm. The previously indicated REST components communicate the state representations of data elements. There are six data elements in the REST paradigm: </span></p>
<ul>
<li class="mce-root"><strong>Resource</strong>: As per the REST specification, the ROA is the core architectural style and pattern. Resources are the main element of the RESTful paradigm. A resource is a conceptual mapping to a variety of physical or logical/digital/cyber/virtual entities. The mappings change over time. Every RESTful resource is uniquely represented and identified through an appropriate address. Resources include images from Instagram, movie titles, and so on. </li>
<li class="mce-root"><strong>Resource identifier</strong>: Resources need to be uniquely identified in order to be found, accessed, and used. The <strong>uniform resource identifier</strong> (<strong>URI</strong>) identifies every resource. URI is the way forward for clients and servers to communicate in an unambiguous manner. A resource can have multiple URIs. This becomes necessary for indicating the varying location details of the resource. URIs are used to exchange resource representations. </li>
<li class="mce-root"><strong>Resource metadata</strong>: Metadata is important for resource utilization and management; it provides additional details regarding the resource. The added information, such as location information and alternative resource identifiers for resources, enables resource manipulation and management. <span>That is, resources also include the RESTful API-specific information, such as URLs and relationships.</span></li>
<li class="mce-root"><strong>Representation</strong>: <span>We have talked about resources and indicated that JSON is the preferred data model for defining the data associated with resources. However, for resources to communicate to a client over an HTTP connection, their representation have to be converted into a textual representation. This representation has to be formally embedded as an entity in an HTTP message body. Precisely speaking, the</span> resource representation is the state of the resource at that point in time, and bound to change. The state value is transmitted between clients and servers. A representation typically captures and conveys the current or desired state of a resource. A particular resource can have multiple representations.</li>
<li class="mce-root"><strong>Representation metadata</strong>: This metadata gives extra details regarding the representation in order to simplify representation.</li>
<li class="mce-root"><strong>Control data</strong><span>: This defines the action being requested on resources.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">REST architectural constraints</h1>
                </header>
            
            <article>
                
<p>The REST architectural constraints are primarily the design rules that clearly convey the distinct characteristics of the REST paradigm. These constraints aren't there to dictate what kind of technologies and tools to use; they just indicate how data gets transferred between the various components, such as clients and servers: </p>
<ul>
<li><strong>Separation of concerns </strong><strong>between clients and servers</strong>: There is a clear disconnect between clients and servers. This unique separation enables the development and deployment of client-server applications independently. Any advancement and alteration of one thing doesn't affect the functioning of another. The decoupled nature guarantees the elimination of dependency-induced issues.</li>
<li><strong>Stateless communication</strong>: Server machines don't need to store any session information about the contexts of clients' calls. This means that any new requests can be handled by any server instance. However, the authentication details of clients in a session can be stocked so that new requests within the session don't need to be authenticated by the identification and access management system. </li>
<li><strong>Clients to cache responses</strong>:<strong> </strong>Clients can cache server responses, since every server response comes with the decision enabling, cache-related details. </li>
<li><strong>Connections may be direct or indirect</strong>:<strong> </strong>Clients can talk to servers directly or through intermediaries, which can be a proxy or other brokers. This separation increases system flexibility. The need for scalability is fulfilled easily with this intermediary.</li>
<li><strong>Uniform interface</strong>: The interactions between the various web application components (clients, servers, and intermediaries) get simplified due to the uniformity of their interfaces. If any of the components deviate from the established standards, there is a possibility that the web applications will break down. <span>The four basic HTTP operations,</span><span> </span><kbd>GET</kbd><span>,</span><span> </span><kbd>POST</kbd><span>,</span><span> </span><kbd>PUT</kbd><span>, and</span><span> </span><kbd>DELETE</kbd><span>, provide the much-needed uniformity for all the contributing components to find, interact with, and accomplish tasks with clarity and confidence. The other operations,</span><span> </span><kbd>HEAD</kbd><span> </span><span>and</span><span> </span><kbd>OPTIONS</kbd><span>, primarily deal with metadata management.</span></li>
<li><strong>Layered system</strong>: Layered and tiered systems mitigate development and operational complexities. Increasingly, there are many layers between the client and the server. These layers act as intermediaries, such as gateways and proxies, to automate some of the aspects of interactions. A proxy typically is an intermediary that's been selected by a client. The proxy provides interfaces to services, such as data translation, performance enhancement, and security protection. On the other hand, the gateway is another intermediary imposed by the network or server to provide an interface for specific services. </li>
</ul>
<p><span>Leveraging intermediary components leads to a substantial reduction in the interaction latency, security enforcement, and encapsulation of legacy systems. Thus, for the faster development of web applications and to make legacy applications more modern, modular, and web-enabled, the RESTful services paradigm contributes immensely. </span></p>
<p>The <strong>simple object-access protocol (SOAP)</strong> mechanism is the original approach to access and use services. It's an XML-based messaging protocol that exchanges information among computers. There are a few perpetual issues with SOAP, and hence the origin and the dissemination of the REST paradigm, which is a comparatively lightweight and service-oriented application protocol, is widely appreciated. Due to the explosion in web applications and services, the creation and the sustenance of competent web architectures has become an important task.</p>
<p>The REST method is a famous architectural style that is closely aligned with the concepts used in the ubiquitous HTTP protocol. REST doesn't prescribe the details of component implementation and protocol syntax. However, it includes the fundamental constraints upon connectors, components, and data. This outlines the basis of the web architecture and, so the essence of its behavior as a network-based application. The REST paradigm illustrates a simple web application design and development through a set of architectural constraints. These architectural considerations ensure the scalability of component interactions, the standard interface, and the independent deployment of components. </p>
<p>Precisely speaking, REST<span> </span>itself doesn't define where or how the state of various resources should be stored. REST specifies how the state can be retrieved using the ubiquitous <kbd>GET</kbd> operation. The state can also be provided through the <kbd>PUT</kbd> and <kbd>POST</kbd> operations. It's also possible to have read-only resources that just support the <kbd>GET</kbd> method. As discussed elsewhere, the resource state could be provided by any means, such as a filesystem, a dynamic combination of other resources, or physical sensors. The REST paradigm is an approach or design pattern for designing resource-oriented application services. That is, REST is an architectural style for networked applications. </p>
<p>REST improves the performance, scalability, simplicity, and visibility of network-based applications. REST naturally encourages correct and efficient developer-computer and developer-developer communication. So, REST can be described as a way of building services and applications (we described a variety of software applications in the beginning of this chapter) by following a set of specific constraints between consumers and providers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The advantages of REST</h1>
                </header>
            
            <article>
                
<p>Typically, there could be many interactions between requesting and serving services to fulfill a business process and, hence, the communication among different and distributed microservices has to happen fast with less overhead. REST is mandated to have less latency networks for speedier service requests and fulfillment, which makes REST APIs a good fit. REST APIs not only support XML and JSON, but also support more optimized binary representation formats, such as a protocol buffer or Avro. Further on, it can upgrade to HTTP/2.0. Billions of people across the globe use the web for a variety of purposes. There are millions of developers creating a variety of web applications. Thus, a competent web architecture is needed to fulfill the varying requirements of businesses and people. In short, the powerful REST paradigm helps to build software architectures and applications that implicitly inherit all the praiseworthy qualities of the web. RESTful services bring forth a few important capabilities, such as greater scalability, efficient network use, and the independent functioning of clients and servers.</p>
<p>The REST paradigm inherently supports the ROA. Everything, as per the REST specifications, is a resource. Software architectures are typically a combination of several configurable architectural elements. In the case of REST, the principal architectural elements are components, connectors, resources, representations, and a few data elements. Resources are the building blocks of RESTful APIs. A resource can be anything that an application wants to expose on the network for other applications to find and operate through various HTTP methods. It can be text content, images, video and audio, a white paper, or a bank account. These resources are linked together by embedding the respective hyperlinks in HTML documents. Note that the resources can be retrieved, updated, and deleted by both humans and software programs. The following diagram illustrates this and shows the potential relationships between distributed and different resources via hyperlinks:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fd4c0fd4-ffa3-4798-b964-7aceff3a60c7.png" style="width:29.67em;height:28.08em;"/></p>
<p>To unambiguously find and use them, each resource has to be given a unique name, which is called a URI. A sample URI is <a href="http://www.paris.fr/weather">http://www.paris.fr/weather</a>. As indicated in the preceding diagram, a resource can expose its state through the concept of representation. A representation typically contains both metadata (such as resource size, media type, or character set) and content (binary image or text document). The representation varies significantly. For example, the representation of a confirmation of purchase on an e-commerce site could be an HTML document. For a wedding picture, the representation could be a JPEG image streaming. For a contact in an address book web service, it could be an XML file. Thus, it starts with the resource identification and there's a corresponding representation for each resource identified.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Self-descriptive messages</h1>
                </header>
            
            <article>
                
<p>Clients should express their preferred state through a host of request messages. A resource's<span> </span>current<span> </span>state gets communicated by the server to any client through a response message. For example, a wiki page editor can send a message to the server requesting to transfer a representation. The representation change may suggest a page update, which is the new state for the server-hosted and -managed web page. However, the server takes the call to accept or deny the client's request. As indicated in the previous paragraph, the self-descriptive messages may include<span> </span>metadata<span> </span>to carry and convey the additional details regarding the resource state, the representation format and size, and the message itself:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ba9d4bed-7f99-4cbc-9e2c-31f45669d000.png"/></p>
<p>The core operations (<kbd>GET</kbd>, <kbd>PUT</kbd>, <kbd>DELETE</kbd>, and <kbd>POST</kbd>) performed on the current state of the resource are pictorially indicated in the following diagram. The state gets modified and updated by those operations. The diagram also indicates that the resource is delimited from its external environment, which interacts with it. That is, multiple parties can interact with the resource without any problem:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c00b0a95-ec2d-4adf-afdb-bf050227fbee.png"/></p>
<p>The resource is initiated with a state, which is in the middle of the previous diagram. The state is managed in any way that makes sense. Writing or reading from a database or file is one thing, but without any backend database, the business logic can't do any dynamic computation. The result is sent back to the requesting client. The previously mentioned core operations define its uniform interface. </p>
<p><span>Establishing and providing a REST API isn't an easy thing to do. An application, as per the REST paradigm, comprises multiple and distributed resources that provide useful capabilities to the application's consumers. API developers have to understand the problem domain, analyze the business, technology, and user requirements, and accordingly have to design the various participating and contributing resources. The resource selection finally leads to the formation of the APIs, which need to be found and used for business automation. The following diagram illustrates how we define a REST API as a set of hyperlinked resources. The resources are being exposed by a web service, websites, and microservices:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b39aa910-64f4-4773-81fd-8bffad59b7da.png" style="width:42.00em;height:37.50em;"/></p>
<p>We are heading toward the software-defined world, and every tangible thing gets enabled through appropriate APIs. Thus, the much-touted API economy is beginning to shine. Corporations are keen to embrace this strategically sound transition. APIs have become the interesting and inspiring component for our business and IT systems: </p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3a3345dd-7433-4d8a-882a-5fe85b732098.png" style="width:40.33em;height:36.67em;"/></p>
<p>Precisely speaking, the ROA facilitated by the REST paradigm is being pronounced as the major contributor for enacting the envisioned digital transformation, not only for business enterprises, government organizations, and IT companies, but also for humanity. With technologies permeating everyday life, people are going to be the smartest in decision making and in deeds. SOAP is the first industry-wide approach for the goal of service-enablement. Due to various complexity factors, REST is gaining market and mind shares, as illustrated in the following section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">SOAP versus REST</h1>
                </header>
            
            <article>
                
<p>SOAP is a matured and stabilized protocol with a number of standard specifications, and these specifications are simplifying and streamlining the development, deployment, management, governance, and composition of services. There are standardized markup languages to represent the interfaces of services. In order to be unambiguously understood, SOAP predominantly uses XML rather than HTTP to define message content. <strong>Web services description language</strong> (<strong>WSDL</strong>) can enforce the use of formal contracts between the service API and consumers. SOAP has a built-in WS-reliable messaging standard to increase service security during asynchronous execution and processing. SOAP has a built-in stateful operation capability for conversational state management.</p>
<p>As indicated in the preceding section, REST is easy to understand as it uses HTTP as the data transmission protocol and the basic CRUD operations. This ease of use simplifies work for software developers. REST also consumes less network bandwidth as it isn't as verbose/bulky as SOAP. Unlike SOAP, REST is designed to be stateless, and REST responses can be cached at clients to guarantee better performance and scalability. REST intrinsically supports many data formats. The overwhelmingly used data format is JSON, which is capable of providing better support for web browsers and mini-clients. JSON's association with JavaScript simplifies the consumption of API payloads. RESTful services are becoming pervasive due to their lightweight nature. All kinds of cloud, mobile, embedded, and IoT applications are leveraging the REST paradigm, which is increasingly paramount for application, data, and UI integration requirements. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">When to use REST versus SOAP</h1>
                </header>
            
            <article>
                
<p>REST is superior to SOAP in many respects, including the following:</p>
<ul>
<li><strong>Developing a public API</strong>:<strong> </strong>REST, which is famous for ROA, invariably focuses on resource-based or data-based operations. Its core operations (<kbd>GET</kbd>, <kbd>PUT</kbd>, <kbd>POST</kbd>, and <kbd>DELETE</kbd>) are inherited from the ubiquitous HTTP. This minimalist approach makes the REST paradigm easy to understand and use across application and industry domains. Application and service developers find it easy to play around with the RESTful approach. The response gets easily consumed by web browsers. Precisely speaking, the simplicity innately provided by REST is one of the key reasons for its unprecedented support and success. Realizing its potential, there is a consistent transition from SOAP to REST. </li>
<li><strong>Performance, flexibility, and scalability</strong>: APIs are the entry point for applications to be identified, integrated, and used. There are event messages and procedure/method calls between applications' APIs. A widely circulated tip is that applications that require a lot of back-and-forth messaging have to choose the REST way in order to be successful. If there's a networking issue, the RESTful service approach allows the application/process to retry once the connection is re-established. REST makes it easy to do so without any major interruption. With SOAP stateful operations, the retry aspect seems to be a difficult affair as it involves more initialization at multiple levels, including the state code. Since REST is stateless, the session information isn't stored on the server machine, and this enables REST services to be independently retried and horizontally scalable. </li>
</ul>
<p>The RESTful service paradigm enables us to perform easy and fast calls to a URL and get an immediate response. SOAP services require the keeping of a stateful connection with a complex client. REST, on the other hand, bats for stateless connection. The cache isn't stored in server applications. Therefore, testing RESTful applications is quite easy compared to SOAP applications. </p>
<p>SOAP provides ways of remotely accessing and manipulating objects (nouns) through procedure/method requests; REST focuses on the operations (verbs) that can be executed on resources. REST, therefore, has been widely adopted by public API practitioners. REST is always better than SOAP in situations that don't mandate for the full map for a set of objects to the client. Transmitting object details back and forth will waste a lot of expensive network bandwidth. Also, network latency comes into the picture. Especially in bandwidth-starved environments, this multiple-call requirement can be a huge barrier. APIs consumed mostly by mobile applications relate to those scenarios where we don't need to leverage SOAP at all costs. Public APIs frequently change because the expectations of consumers and businesses vary. In this world of start-ups and APIs without specific contractual agreements, REST is a natural and universal choice.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Best practices for REST-based microservices</h1>
                </header>
            
            <article>
                
<p>In this section, we'll discuss a few best practices that make your MSA developer-friendly, so they can manage and track errors easily:</p>
<ul>
<li><strong>Meaningful names</strong>:<strong> </strong>It's always important to provide a meaningful name in the request header, so if any problem, such as performance degradation, memory wastage, or a spike in user load, occurs, developers and performance engineers can easily understand from which microservice this request was originated and cascaded. It's therefore a best practice to provide the logical <kbd>name/{service id}</kbd> in the <kbd>User-Agent</kbd> property in the request header, for example, <kbd>User-Agent:EmployeeSearchService</kbd>. </li>
<li><strong>API management</strong>:<strong> </strong><span>In the REST-based microservice architecture, one microservice accesses another microservice via an API. The API acts as a facade to other microservices. Therefore, it's mandatory to build APIs carefully, and changing APIs entails additional problems. That is, APIs have to be designed with future demands in mind. Any changes in the API method signature aren't good because many microservices depend on the APIs to access and assess the microservice. Therefore, the tasks, such as API usage, versioning, and management, acquire special significance in our increasingly API-centric world.</span></li>
<li><strong>Correlate ID</strong>:<strong> </strong>Microservices, for the sake of guaranteeing high availability, are typically spread across multiple servers. That is, there can be multiple instances of the same microservice. With containers emerging as the most optimized runtime for microservices, running multiple instances of a microservice has become the new normal. To fulfill one client request, the control has to go through multiple microservices and instances. If one service isn't doing OK in the pipeline due to a problem, we need to understand the real situation of the service to determine our course of action. The aspects of service tracking and distributed tracing gain importance for the microservices architecture to be successful and smart in the connected and cloud era. The<span><span> widely-recommended mechanism is always to generate a random UUID for every client request and pass that UUID to every internal service request. Then, by capturing the log files, it becomes easy for service operators to pinpoint the problematic service.</span></span></li>
<li><strong>ELK implementation</strong>:<strong> </strong><span>Microservices are small and simple. In any IT environment, there can be hundreds of microservices, and each microservice has multiple redundant instances in order to ensure the much-wanted fault tolerance. Each instance generates a log file, and administrators find that visiting each log file to locate something useful is not an easy affair. So, capturing and stocking log files, implementing a powerful search engine on the log file store, and applying appropriate <strong>machine learning</strong> (<strong>ML</strong>) algorithms to that log data in order to extract and emit any useful patterns, noteworthy information, or beneficial associations are vital in order to make sense of the log data. ELK, which is an open source software, fulfills these differing requirements in a tightly-integrated manner. E stands for Elasticsearch, L for Logstash, and K for Kibana. Elasticsearch just dumps the logs and provides a fuzzy search capability, Logstash is used to collect logs from different sources and transform them, and Kibana is a <strong>graphical user interface</strong> (<strong>GUI</strong>) that helps data scientists, testers, developers, and even businesspeople to insightfully search the logs as per their evolving requirements. Considering the significance of log analytics, there are open source as well as commercial-grade solutions to extract log, operational, performance, scalability, and security insights from microservice interaction log data.</span></li>
<li><strong>Resiliency implementation</strong>:<strong> </strong>There are frameworks and solutions that guarantee reliability (resiliency + elasticity) when services interact with one another.</li>
</ul>
<p>REST-based microservices are popular not only due to their extreme simplicity, but also due to the fact that services communicate directly (synchronously) with each other over HTTP. This direct communication means that there's no need for any kind of intermediary, such as a hub, bus, broker, or gateway. For example, consider a B2C e-commerce system that instantly notifies customers when a particular product is back in stock. This notification could be implemented via RESTful microservices:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ba450c8b-8d8f-4a35-a568-a3676ea0dd8b.png" style="width:42.17em;height:16.25em;"/></p>
<p>It should be noted that the communication is point-to-point. Still, hardcoding services' addresses isn't a good thing to do. Therefore, the prominent workaround is to leverage a service discovery mechanism, such as Eureka or Consul. These are highly available centralized servers where services register their API addresses. The availability status of services for instantaneous serving is registered with the centralized servers. Client services can request a specific API address from this centralized server in order to identify and leverage the appropriate services. Still, there are several shortcomings, which are listed as follows:</p>
<ul>
<li><strong>Blocking</strong>:<strong> </strong>Due to the synchronous nature of the REST approach, the update stock operation won't do anything until the notification service completes its task of notifying all relevant customers. If there are thousands of customers wishing to be notified of the additional stock, the system's performance is bound to degrade sharply. This performance issue happens due to the tight-coupling approach. One way to overcome these issues is to embrace the pipeline pattern. The architecture diagram then gets modified as follows:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7cdc592a-a337-4cc2-8438-bc901681bfde.png" style="width:42.58em;height:22.50em;"/></div>
<p style="padding-left: 60px">Here, the communication is still REST-based, but the real shift is that the point-to-point communication is eliminated forever. The <strong>Pipeline</strong> entity is entirely responsible for orchestrating control and data flows. The services are totally decoupled, and this decoupling makes microservices autonomous. However, with this approach, the services must rely on the pipeline orchestration in order to contribute to the cause and, hence, services are self-defined, yet not self-sufficient.</p>
<ul>
<li><strong>Asynchronous messaging</strong>:<strong> </strong>Consider a typical messaging-based system. Here, both the services—input and output—can be defined as commands or events. Each of these subscribes to the events that it's interested in consuming. Further, these events are received reliably through a mechanism, such as a messaging queue/broker, when the events are placed on the queue by other services. With this approach, the stock notification subsystem could now be remodeled as follows:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e464d20f-c08c-4cf4-9134-35097b18433c.png"/></div>
<p>This refurbished architecture brings forth a number of crucial advantages, such as enhanced flexibility, service isolation, and autonomy. This shift eases the addition, removal, or modification of services without affecting the operation or code of other services. Any kind of service failure can be gracefully handled. These need to be carefully considered when designing and developing microservices-based enterprise applications.</p>
<p>As technologies become increasingly complex, best practices and procedures sourced through various experimentation come in handy for architects and developers to create strategically sound software systems. As microservices emerge as the most optimal building block for production-grade and extensible business and IT systems, our focus gets turned toward the ways of leveraging the matured and stabilized REST paradigm to create and sustain business-critical and microservices-centric software applications. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The API-first approach</h1>
                </header>
            
            <article>
                
<p>We are going to have a dazzling array of IoT devices, data, services, and applications. Data analytics shall assume a greater proportion due to the uninhibited explosion of IoT sensors and actuators, which make any physical, mechanical, and electrical system to be digitized and connected. Also, web, enterprise, and cloud applications are mobile-enabled. Thus, there are multiple channels for application, device, and information access and usage. The domain of enterprise mobility is very popular. There are performance-hungry legacy applications. Application dismantling takes a lot of time and talent. Therefore, the popular approach is to attach extra APIs to existing applications in order to make them available to the external world. However, this isn't a strategically sound approach. On the other side, considering the optimal and organized nature of cloud infrastructures, new-generation applications are being directly designed, developed, and deployed on cloud servers.</p>
<p>Therefore, the prescribed approach is to build the API first and then to have cloud applications on top of that API. The idea behind the widely circulated API-first development (<a href="https://www.restcase.com/">https://www.restcase.com/</a>) strategy is to arrive at a futuristic API. This approach enables software developers to accomplish their work with clarity and confidence. The implementations can be highly advanced and efficient. This separation between interface and implementation facilitates the incorporation of modified code at a later point in time without affecting access to the application and service.</p>
<p>With clouds positioned as the one-stop IT solution for all kinds of software systems, monolithic and massive applications are being cloud-enabled to become open for innovation, disruption, and transformation. Greenfield applications are predominantly initiated and implemented as cloud-native applications. There are code repositories (public and private). Cloud-native code, once finished, gets checked into a repository. Thus, code is built and integrated with all the necessary code segments to create an integrated application. Vital tests are performed on the application to check whether its code passes through all the gates successfully. Then, there are continuous delivery and deployment tools to deliver the curated and refined code to provisioned IT resources to start the deployment process. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Developing API-first</h1>
                </header>
            
            <article>
                
<p>This is an interesting strategy. Development works are happening in a sequence. The requirements elicitation, service design, development, debugging, delivery, deployment, and decommissioning typically happen in a synchronous manner. The quality control and assurance team is waiting for the code to be fully developed and integrated. Once the prototype is ready, the documentation team starts preparing the APIs. If any improvement, correction, or addition requirement arises, the design and development teams start working on them. This is a waste of talent and time. If the API for the proposed service is finalized in consultation with the project sponsor and the end users, the software construction and deployment time is reduced sharply. This is depicted in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/bcf3d28c-0e43-4e3d-bbfe-e13e8d1c0e19.png" style="width:43.58em;height:23.42em;"/></div>
<p>Therefore the API-first development process is being recommended because it facilitates a kind of parallel development by all teams. The software can be released independently. The dependency on other teams is substantially less in this case: </p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/77123d9b-b782-454b-91d8-81b691af4492.png"/></p>
<p>Here, the APIs are first created and mocked. Then backend, frontend, and test teams start to work with the mocked APIs. Once the <strong>API</strong> is finalized, all teams can easily switch to the production or staging API. This procedure saves a lot of development time.</p>
<div class="mce-root packt_infobox"><strong>RestCase</strong> is a cloud-based API development platform. The platform allows developers to collaboratively create REST APIs using an intuitive browser-based interface, which automatically generates documentation, tests, and mocks. RestCase further enables rapid iteration and testing by creating a mock of the API that developers can make calls against immediately, without waiting for the actual development and deployment of the API, thereby eliminating impediments from various development teams.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building services API-first</h1>
                </header>
            
            <article>
                
<p>This is a best practice that's wisely and widely recommended by experts. By defining and designing the APIs first, a variety of things can be done properly. Project sponsors get an overall view of the system under development. Incorporating changes is quite easy in this case. The software developers can proceed with their development and testing tasks with confidence. Salespeople can explain the nitty-gritty of the software system to any prospective customers and consumers. API testing can be done first. API management is made easy in the increasingly API world. Everything is being fitted with one or more APIs in order to be found, bound, and used according to evolving needs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed the salient features of the RESTful services and APIs in making the road toward digitally transformed businesses and societies easier to navigate. The simplicity and modularity of the REST paradigm leads to the production of highly flexible and futuristic software applications. As the web and cloud enablement of digital assets assume a more prominent role, the role and responsibility of the REST idea are bound to increase in the days to come. We tend toward deeply connecting our everyday environments. The dream of having many cognitive environments is consistently on the rise. Hence, it's clear that the unique and innate capabilities of the mesmerizing RESTful services should be reaped in order to achieve the things described in this chapter. This chapter has given you a lot of useful information on the REST concept and how it's going to be a trendsetter in the IT world. This book will cover the various aspects of the RESTful paradigm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><a href="https://www.packtpub.com/application-development/restful-java-web-services-third-edition">https://www.packtpub.com/application-development/restful-java-web-services-third-edition</a></li>
<li><a href="https://www.packtpub.com/application-development/building-restful-web-services-go">https://www.packtpub.com/application-development/building-restful-web-services-go</a></li>
<li><a href="https://www.packtpub.com/application-development/building-restful-web-services-net-core">https://www.packtpub.com/application-development/building-restful-web-services-net-core</a></li>
</ul>


            </article>

            
        </section>
    </body></html>