["```py\npip install piccolo\n```", "```py\npip install piccolo-admin\n```", "```py\nAPP_CONFIG = AppConfig(\n    app_name=\"survey\",\n    migrations_folder_path=os.path.join(\n        CURRENT_DIRECTORY, \"piccolo_migrations\"\n    ),\n    table_classes=[Answers, Education, Question, Choices, \n       Profile, Login, Location, Occupation, Respondent],\n    migration_dependencies=[],\n    commands=[],\n)\n```", "```py\nfrom piccolo.engine.postgres import PostgresEngine\nfrom piccolo.conf.apps import AppRegistry\nDB = PostgresEngine(\n    config={\n        \"database\": \"pccs\",\n        \"user\": \"postgres\",\n        \"password\": \"admin2255\",\n        \"host\": \"localhost\",\n        \"port\": 5433,\n    }\n)\nAPP_REGISTRY = AppRegistry(\n    apps=[\"survey.piccolo_app\", \n          \"piccolo_admin.piccolo_app\"]\n)\n```", "```py\nfrom piccolo.columns import ForeignKey, Integer, Varchar,\n       Text, Date, Boolean, Float\nfrom piccolo.table import Table\nclass Login(Table):\n    username = Varchar(unique=True)\n    password = Varchar()\nclass Education(Table):\n    name = Varchar()\nclass Profile(Table):\n    fname = Varchar()\n    lname = Varchar()\n    age = Integer()\n    position = Varchar()\n    login_id = ForeignKey(Login, unique=True)\n    official_id = Integer()\n    date_employed = Date()\n```", "```py\nfrom survey.tables import Respondent\nfrom typing import Dict, List, Any\nclass RespondentRepository:\n    async def insert_respondent(self, \n             details:Dict[str, Any]) -> bool: \n        try:\n            respondent = Respondent(**details)\n            await respondent.save()\n        except Exception as e: \n            return False \n        return True\n```", "```py\n    async def get_all_respondent(self):\n        return await Respondent.select()\n                  .order_by(Respondent.id)\n```", "```py\npip install sympy\n```", "```py\nfrom sympy import symbols, sympify\n@router.post(\"/sym/equation\")\nasync def substitute_bivar_eqn(eqn: str, xval:int, \n               yval:int):\n    try:\n        x, y = symbols('x, y')\n        expr = sympify(eqn)\n        return str(expr.subs({x: xval, y: yval}))\n    except:\n        return JSONResponse(content={\"message\": \n            \"invalid equations\"}, status_code=500)\n```", "```py\nfrom sympy import Eq, symbols, Poly, solve, sympify\n@router.get(\"/sym/linear\")\nasync def solve_linear_bivar_eqns(eqn1:str, \n            sol1: int, eqn2:str, sol2: int):\n    x, y = symbols('x, y')\n\n    expr1 = parse_expr(eqn1, locals())\n    expr2 = parse_expr(eqn2, locals())\n\n    if Poly(expr1, x).is_linear and \n                 Poly(expr1, x).is_linear:\n        eq1 = Eq(expr1, sol1)\n        eq2 = Eq(expr2, sol2)\n        sol = solve([eq1, eq2], [x, y])\n        return str(sol)\n    else:\n        return None\n```", "```py\n@router.get(\"/sym/nonlinear\")\nasync def solve_nonlinear_bivar_eqns(eqn1:str, sol1: int, \n           eqn2:str, sol2: int):\n    … … … … … …\n    … … … … … …    \n    if not Poly(expr1, x, y).is_linear or \n              not Poly(expr1, x, y).is_linear:\n    … … … … … …\n    … … … … … …\n        return str(sol)\n    else:\n        return None\n```", "```py\n@router.get(\"/sym/inequality\")\nasync def solve_univar_inequality(eqn:str, sol:int):\n    x= symbols('x')\n    expr1 = Ge(parse_expr(eqn, locals()), sol)\n    sol = solve([expr1], [x])\n    return str(sol)\n```", "```py\npip install numpy\n```", "```py\nfrom survey.repository.answers import AnswerRepository\nfrom survey.repository.location import LocationRepository\nimport ujson\nimport numpy as np\n@router.get(\"/answer/respondent\")\nasync def get_respondent_answers(qid:int):\n    repo_loc = LocationRepository()\n    repo_answers = AnswerRepository()\n    locations = await repo_loc.get_all_location()\n    data = []\n    for loc in locations:\n        loc_q = await repo_answers\n            .get_answers_per_q(loc[\"id\"], qid)\n        if not len(loc_q) == 0:\n            loc_data = [ weights[qid-1]\n              [str(item[\"answer_choice\"])] \n                for item in loc_q]\n            data.append(loc_data)\n    arr = np.array(data)\n    return ujson.loads(ujson.dumps(arr.tolist())) \n```", "```py\npip install pandas matplotlib openpxyl\n```", "```py\n@router.get(\"/answer/increase/{gradient}\")\nasync def answers_weight_multiply(gradient:int, qid:int):\n    repo_loc = LocationRepository()\n    repo_answers = AnswerRepository()\n    locations = await repo_loc.get_all_location()\n    data = []\n    for loc in locations:\n        loc_q = await repo_answers\n            .get_answers_per_q(loc[\"id\"], qid)\n        if not len(loc_q) == 0:\n            loc_data = [ weights[qid-1]\n             [str(item[\"answer_choice\"])] \n                 for item in loc_q]\n            data.append(loc_data)\n    arr = np.array(list(itertools.chain(*data)))\n    arr = arr * gradient\n    return ujson.loads(ujson.dumps(arr.tolist()))\n```", "```py\nimport ujson\nimport numpy as np\nimport pandas as pd\n@router.get(\"/answer/all\")\nasync def get_all_answers():\n    repo_loc = LocationRepository()\n    repo_answers = AnswerRepository()\n    locations = await repo_loc.get_all_location()\n    temp = []\n    data = []\n    for loc in locations:\n        for qid in range(1, 13):\n            loc_q1 = await repo_answers\n               .get_answers_per_q(loc[\"id\"], qid)\n            if not len(loc_q1) == 0:\n                loc_data = [ weights[qid-1]\n                   [str(item[\"answer_choice\"])] \n                      for item in loc_q1]\n                temp.append(loc_data)\n        temp = list(itertools.chain(*temp))\n        if not len(temp) == 0:\n            data.append(temp)\n        temp = list()\n    arr = np.array(data)\n    return ujson.loads(pd.DataFrame(arr)\n           .to_json(orient='split'))\n```", "```py\npip install scipy\n```", "```py\nfrom scipy import stats\ndef ConvertPythonInt(o):\n    if isinstance(o, np.int32): return int(o)  \n    raise TypeError\n@router.get(\"/answer/stats\")\nasync def get_respondent_answers_stats(qid:int):\n    repo_loc = LocationRepository()\n    repo_answers = AnswerRepository()\n    locations = await repo_loc.get_all_location()\n    data = []\n    for loc in locations:\n        loc_q = await repo_answers\n           .get_answers_per_q(loc[\"id\"], qid)\n             if not len(loc_q) == 0:\n                 loc_data = [ weights[qid-1]\n                   [str(item[\"answer_choice\"])] \n                       for item in loc_q]\n            data.append(loc_data)\n    result = stats.describe(list(itertools.chain(*data)))\n    return json.dumps(result._asdict(), \n                  default=ConvertPythonInt)\n```", "```py\nfrom fastapi.responses import StreamingResponse\nimport pandas as pd\nfrom io import StringIO\nfrom survey.repository.respondent import \n        RespondentRepository\n@router.get(\"/respondents/csv\", response_description='csv')\nasync def create_respondent_report_csv():\n    repo = RespondentRepository()\n    result = await repo.get_all_respondent()\n\n    ids = [ item[\"id\"] for item in result ]\n    fnames = [ f'{item[\"fname\"]}' for item in result ]\n    lnames = [ f'{item[\"lname\"]}' for item in result ]\n    ages = [ item[\"age\"] for item in result ]\n    genders = [ f'{item[\"gender\"]}' for item in result ]\n    maritals = [ f'{item[\"marital\"]}' for item in result ]\n\n    dict = {'Id': ids, 'First Name': fnames, \n            'Last Name': lnames, 'Age': ages, \n            'Gender': genders, 'Married?': maritals} \n\n    df = pd.DataFrame(dict)\n    outFileAsStr = StringIO()\n    df.to_csv(outFileAsStr, index = False)\n    return StreamingResponse(\n        iter([outFileAsStr.getvalue()]),\n        media_type='text/csv',\n        headers={\n            'Content-Disposition': \n              'attachment;filename=list_respondents.csv',\n            'Access-Control-Expose-Headers': \n               'Content-Disposition'\n        }\n    )\n```", "```py\nimport xlsxwriter\nfrom io import BytesIO\n@router.get(\"/respondents/xlsx\", \n          response_description='xlsx')\nasync def create_respondent_report_xlsx():\n    repo = RespondentRepository()\n    result = await repo.get_all_respondent()\n    output = BytesIO()\n    workbook = xlsxwriter.Workbook(output)\n    worksheet = workbook.add_worksheet()\n    worksheet.write(0, 0, 'ID')\n    worksheet.write(0, 1, 'First Name')\n    worksheet.write(0, 2, 'Last Name')\n    worksheet.write(0, 3, 'Age')\n    worksheet.write(0, 4, 'Gender')\n    worksheet.write(0, 5, 'Married?')\n    row = 1\n    for respondent in result:\n        worksheet.write(row, 0, respondent[\"id\"])\n        … … … … … …\n        worksheet.write(row, 5, respondent[\"marital\"])\n        row += 1\n    workbook.close()\n    output.seek(0)\n    headers = {\n        'Content-Disposition': 'attachment; \n             filename=\"list_respondents.xlsx\"'\n    }\n    return StreamingResponse(output, headers=headers)\n```", "```py\n@router.post(\"/upload/csv\")\nasync def upload_csv(file: UploadFile = File(...)):\n    df = pd.read_csv(StringIO(str(file.file.read(), \n            'utf-8')), encoding='utf-16')\n    return orjson.loads(df.to_json(orient='split'))\n```", "```py\n@router.get(\"/upload/survey/form\", \n          response_class = HTMLResponse)\ndef upload_survey_form(request:Request):\n    return templates.TemplateResponse(\"upload_survey.html\",\n             {\"request\": request})\n@router.post(\"/upload/survey/form\")\nasync def submit_survey_form(request: Request, \n              file: UploadFile = File(...)):\n    df = pd.read_csv(StringIO(str(file.file.read(), \n               'utf-8')), encoding='utf-8')\n    return templates.TemplateResponse('render_survey.html', \n         {'request': request, 'data': df.to_html()})\n```", "```py\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom survey.repository.answers import AnswerRepository\nfrom survey.repository.location import LocationRepository\n@router.get(\"/answers/line\")\nasync def plot_answers_mean():\n    x = [1, 2, 3, 4, 5, 6, 7]\n    repo_loc = LocationRepository()\n    repo_answers = AnswerRepository()\n    locations = await repo_loc.get_all_location()\n    temp = []\n    data = []\n    for loc in locations:\n        for qid in range(1, 13):\n            loc_q1 = await repo_answers\n               .get_answers_per_q(loc[\"id\"], qid)\n            if not len(loc_q1) == 0:\n                loc_data = [ weights[qid-1]\n                  [str(item[\"answer_choice\"])] \n                     for item in loc_q1]\n                temp.append(loc_data)\n        temp = list(itertools.chain(*temp))\n        if not len(temp) == 0:\n            data.append(temp)\n        temp = list()\n    y = list(map(np.mean, data))\n    filtered_image = BytesIO()\n    plt.figure()\n\n    plt.plot(x, y)\n\n    plt.xlabel('Question Mean Score')\n    plt.ylabel('State/Province')\n    plt.title('Linear Plot of Poverty Status')\n\n    plt.savefig(filtered_image, format='png')\n    filtered_image.seek(0)\n\n    return StreamingResponse(filtered_image, \n                media_type=\"image/png\")\n```", "```py\n@router.get(\"/sparse/bar\")\nasync def plot_sparse_data():\n   df = pd.DataFrame(np.random.randint(10, size=(10, 4)),\n      columns=[\"Area 1\", \"Area 2\", \"Area 3\", \"Area 4\"])\n   filtered_image = BytesIO()\n   plt.figure()\n   df.sum().plot(kind='barh', color=['red', 'green', \n          'blue', 'indigo', 'violet'])\n   plt.title(\"Respondents in Survey Areas\")\n   plt.xlabel(\"Sample Size\")\n   plt.ylabel(\"State\")\n   plt.savefig(filtered_image, format='png')\n\n   filtered_image.seek(0)\n   return StreamingResponse(filtered_image, \n           media_type=\"image/jpeg\")\n```", "```py\n@router.get(\"/respondents/gender\")\nasync def plot_pie_gender():\n    repo = RespondentRepository()\n    count_male = await repo.list_gender('M')\n    count_female = await repo.list_gender('F')\n    gender = [len(count_male), len(count_female)]\n    filtered_image = BytesIO()\n    my_labels = 'Male','Female'\n    plt.pie(gender,labels=my_labels,autopct='%1.1f%%')\n    plt.title('Gender of Respondents')\n    plt.axis('equal')\n    plt.savefig(filtered_image, format='png')\n    filtered_image.seek(0)\n\n    return StreamingResponse(filtered_image, \n               media_type=\"image/png\")\n```", "```py\n@router.post(\"/survey/compute/avg\")\nasync def chained_workflow(surveydata: SurveyDataResult):\n    survey_dict = surveydata.dict(exclude_unset=True)\n    result = chain(compute_sum_results\n        .s(survey_dict['results']).set(queue='default'), \n            compute_avg_results.s(len(survey_dict))\n             .set(queue='default'), derive_percentile.s()\n             .set(queue='default')).apply_async()\n    return {'message' : result.get(timeout = 10) }\n```", "```py\n@celery.task(bind=True)\ndef compute_sum_results(self, results:Dict[str, int]):\n    scores = []\n    for key, val in results.items():\n        scores.append(val)\n    return sum(scores)\n```", "```py\n@celery.task(bind=True)\ndef compute_avg_results(self, value, len):\n    return (value/len)\n```", "```py\n@celery.task(bind=True)\ndef derive_percentile(self, avg):\n    percentage = f\"{avg:.0%}\"\n    return percentage\n```", "```py\n@router.post(\"/survey/save\")\nasync def grouped_workflow(surveydata: SurveyDataResult):\n    survey_dict = surveydata.dict(exclude_unset=True)\n    result = group([save_result_xlsx\n       .s(survey_dict['results']).set(queue='default'), \n         save_result_csv.s(len(survey_dict))\n          .set(queue='default')]).apply_async()\n    return {'message' : result.get(timeout = 10) } \n```", "```py\n@router.post(\"/process/surveys\")\nasync def process_surveys(surveys: List[SurveyDataResult]):\n    surveys_dict = [s.dict(exclude_unset=True) \n         for s in surveys]\n    result = group([chain(compute_sum_results\n       .s(survey['results']).set(queue='default'), \n         compute_avg_results.s(len(survey['results']))\n         .set(queue='default'), derive_percentile.s()\n         .set(queue='default')) for survey in \n                surveys_dict]).apply_async()\n    return {'message': result.get(timeout = 10) }\n```", "```py\npip install graphene\n```", "```py\nfrom models.data.pccs_graphql import LoginData\nfrom graphene import String, Int, Mutation, Field\nfrom repository.login import LoginRepository\nclass CreateLoginData(Mutation):\n    class Arguments:\n      id = Int(required=True)\n      username = String(required=True)\n      password = String(required=True)\n    ok = Boolean()\n    loginData = Field(lambda: LoginData)\n    async def mutate(root, info, id, username, password):\n        login_dict = {\"id\": id, \"username\": username, \n                   \"password\": password}\n        login_json = dumps(login_dict, default=json_serial)\n        repo = LoginRepository()\n        result = await repo.add_login(loads(login_json))\n        if not result == None:\n          ok = True\n        else: \n          ok = False\n        return CreateLoginData(loginData=result, ok=ok)\n```", "```py\nclass ChangeLoginPassword(Mutation):\n    class Arguments:\n      username = String(required=True)\n      password = String(required=True)\n    ok = Boolean()\n    loginData = Field(lambda: LoginData)\n    async def mutate(root, info, username, password):       \n        repo = LoginRepository()\n        result = await repo.change_password(username, \n                  password)\n\n        if not result == None:\n          ok = True\n        else: \n          ok = False\n        return CreateLoginData(loginData=result, ok=ok)\n```", "```py\nclass DeleteLoginData(Mutation):\n    class Arguments:\n      id = Int(required=True)\n\n    ok = Boolean()\n    loginData = Field(lambda: LoginData)\n    async def mutate(root, info, id):       \n        repo = LoginRepository()\n        result = await repo.delete_login(id)\n        if not result == None:\n          ok = True\n        else: \n          ok = False\n        return DeleteLoginData(loginData=result, ok=ok)\n```", "```py\nclass LoginMutations(ObjectType):\n    create_login = CreateLoginData.Field()\n    edit_login = ChangeLoginPassword.Field()\n    delete_login = DeleteLoginData.Field()\n```", "```py\nclass LoginQuery(ObjectType):\n    login_list = None\n    get_login = Field(List(LoginData))\n\n    async def resolve_get_login(self, info):\n      repo = LoginRepository()\n      login_list = await repo.get_all_login()\n      return login_list\n```", "```py\nfrom graphene import Schema \nschema = Schema(query=LoginQuery, mutation=LoginMutations,\n    auto_camelcase=False)\n```", "```py\nfrom starlette_graphene3 import GraphQLApp, \n          make_playground_handler\napp = FastAPI()\napp.mount(\"/ch10/graphql/login\", \n       GraphQLApp(survey_graphene_login.schema, \n          on_get=make_playground_handler()) )\napp.mount(\"/ch10/graphql/profile\", \n       GraphQLApp(survey_graphene_profile.schema, \n          on_get=make_playground_handler()) )\n```", "```py\npip install neo4j\n```", "```py\nfrom neo4j import GraphDatabase\nuri = \"bolt://127.0.0.1:7687\"\ndriver = GraphDatabase.driver(uri, auth=(\"neo4j\", \n      \"admin2255\"))\n```", "```py\n@router.post(\"/neo4j/location/add\")\ndef create_survey_loc(node_name: str, \n        node_req_atts: LocationReq):\n    node_attributes_dict = \n          node_req_atts.dict(exclude_unset=True)\n    node_attributes = '{' + ', '.join(f'{key}:\\'{value}\\''\n        for (key, value) in node_attributes_dict.items()) \n              + '}'\n    query = f\"CREATE ({node_name}:Location  \n         {node_attributes})\"\n    try:\n        with driver.session() as session:\n            session.run(query=query)\n        return JSONResponse(content={\"message\":\n         \"add node location successful\"}, status_code=201)\n    except Exception as e:\n        print(e)\n        return JSONResponse(content={\"message\": \"add node \n            location unsuccessful\"}, status_code=500)\n```", "```py\n@router.patch(\"/neo4j/update/location/{id}\")\nasync def update_node_loc(id:int, \n           node_req_atts: LocationReq):\n    node_attributes_dict = \n         node_req_atts.dict(exclude_unset=True)\n    node_attributes = '{' + ', '.join(f'{key}:\\'{value}\\'' \n       for (key, value) in \n            node_attributes_dict.items()) + '}'\n    query = f\"\"\"\n        MATCH (location:Location)\n        WHERE ID(location) = {id}\n        SET location += {node_attributes}\"\"\"\n    try:\n        with driver.session() as session:\n            session.run(query=query)\n        return JSONResponse(content={\"message\": \n          \"update location successful\"}, status_code=201)\n    except Exception as e:\n        print(e)\n        return JSONResponse(content={\"message\": \"update \n           location  unsuccessful\"}, status_code=500)\n```", "```py\n@router.delete(\"/neo4j/delete/location/{node}\")\ndef delete_location_node(node:str):\n    node_attributes = '{' + f\"name:'{node}'\" + '}'\n    query = f\"\"\"\n        MATCH (n:Location {node_attributes})\n        DETACH DELETE n\n    \"\"\"\n    try:\n        with driver.session() as session:\n            session.run(query=query)\n        return JSONResponse(content={\"message\": \n          \"delete location node successful\"}, \n             status_code=201)\n    except:\n        return JSONResponse(content={\"message\": \n           \"delete location node unsuccessful\"}, \n               status_code=500)\n```", "```py\n@router.get(\"/neo4j/nodes/all\")\nasync def list_all_nodes():\n    query = f\"\"\"\n        MATCH (node)\n        RETURN node\"\"\"\n    try:\n        with driver.session() as session:\n            result = session.run(query=query)\n            nodes = result.data()\n        return nodes\n    except Exception as e:\n        return JSONResponse(content={\"message\": \"listing\n            all nodes unsuccessful\"}, status_code=500)\n```", "```py\n@router.get(\"/neo4j/location/{id}\")\nasync def get_location(id:int):\n    query = f\"\"\"\n        MATCH (node:Location)\n        WHERE ID(node) = {id}\n        RETURN node\"\"\"\n    try:\n        with driver.session() as session:\n            result = session.run(query=query)\n            nodes = result.data()\n        return nodes\n    except Exception as e:\n        return JSONResponse(content={\"message\": \"get \n          location node unsuccessful\"}, status_code=500)\n```", "```py\n@router.post(\"/neo4j/link/respondent/loc\")\ndef link_respondent_loc(respondent_node: str, \n    loc_node: str, node_req_atts:LinkRespondentLoc):\n    node_attributes_dict = \n         node_req_atts.dict(exclude_unset=True)\n\n    node_attributes = '{' + ', '.join(f'{key}:\\'{value}\\'' \n       for (key, value) in \n          node_attributes_dict.items()) + '}'\n\n    query = f\"\"\"\n        MATCH (respondent:Respondent), (loc:Location)\n        WHERE respondent.name = '{respondent_node}' AND \n            loc.name = '{loc_node}'\n        CREATE (respondent) -[relationship:LIVES_IN \n              {node_attributes}]->(loc)\"\"\"\n    try:\n        with driver.session() as session:\n            session.run(query=query)\n        return JSONResponse(content={\"message\": \"add … \n            relationship successful\"}, status_code=201)\n    except:\n        return JSONResponse(content={\"message\": \"add \n          respondent-loc relationship unsuccessful\"}, \n                 status_code=500)\n```"]