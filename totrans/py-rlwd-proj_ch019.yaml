- en: Chapter 15
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第15章
- en: 'Project 5.1: Modeling Base Application'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 项目5.1：建模基础应用
- en: The next step in the pipeline from acquisition to clean-and-convert is the analysis
    and some preliminary modeling of the data. This may lead us to use the data for
    a more complex model or perhaps machine learning. This chapter will guide you
    through creating another application in the three-stage pipeline to acquire, clean,
    and model a collection of data. This first project will create the application
    with placeholders for more detailed and application-specific modeling components.
    This makes it easier to insert small statistical models that can be replaced with
    more elaborate processing if needed.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在从获取到清洗和转换的管道中的下一步是数据分析和一些初步建模。这可能会引导我们使用数据构建更复杂的模型或机器学习。本章将指导您在三个阶段的管道中创建另一个应用程序，以获取、清洗和建模数据集合。这个第一个项目将创建一个具有更多详细和特定应用建模组件占位符的应用程序。这使得插入可以替换为更复杂处理的简单统计模型变得更加容易。
- en: 'In this chapter, we’ll look at two parts of data analysis:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨数据分析的两个部分：
- en: CLI architecture and how to design a more complex pipeline of processes for
    gathering and analyzing data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLI架构以及如何设计更复杂的流程来收集和分析数据
- en: The core concepts of creating a statistical model of the data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据构建统计模型的核心概念
- en: Viewed from a distance, all analytical work can be considered to be creating
    a simplified model of important features of some complicated processes. Even something
    as simple-sounding as computing an average suggests a simple model of the central
    tendency of a variable. Adding a standard deviation suggests an expected range
    for the variable’s values and – further – assigns a probability to values outside
    the range.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 从远处看，所有分析工作都可以被认为是创建某些复杂过程重要特征的简化模型。即使是像计算平均值这样简单的事情，也暗示了一个变量集中趋势的简单模型。添加标准差暗示了变量值的预期范围，并且——进一步——为范围之外的值分配了概率。
- en: Models, can, of course, be considerably more detailed. Our purpose is to start
    down the path of modeling in a way that builds flexible, extensible application
    software. Each application will have unique modeling requirements, depending on
    the nature of the data, and the nature of the questions being asked about the
    data. For some processes, means and standard deviations are adequate for spotting
    outliers. For other processes, a richer and more detailed simulation may be required
    to estimate the expected distribution of data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 模型当然可以更加详细。我们的目的是以构建灵活、可扩展的应用软件的方式开始建模之路。每个应用程序都将具有独特的建模需求，这取决于数据的性质以及关于数据的提问性质。对于某些过程，均值和标准差足以发现异常值。对于其他过程，可能需要更丰富和更详细的模拟来估计数据的预期分布。
- en: We’ll start the modeling by looking at variables in isolation, sometimes called
    *univariate* statistics. This will examine a variety of commonly recognized distributions
    of data. These distributions generally have a few parameters that can be discovered
    from the given data. In this chapter, we’ll also look at measures like mean, median,
    standard deviation, variance, and standard deviation. These can be used to describe
    data that has a normal or Gaussian distribution. The objective is to create create
    a CLI application separate from an analytic notebook used to present results.
    This creates a higher degree of automation for modeling. The results may then
    be presented in an analytical notebook.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过单独观察变量来开始建模，有时这被称为*单变量*统计学。这将考察数据的一些常见分布。这些分布通常有几个可以从给定数据中发现的参数。在本章中，我们还将探讨均值、中位数、标准差、方差和标准差等度量。这些可以用来描述具有正态或高斯分布的数据。目标是创建一个独立的CLI应用程序，用于展示分析结果，从而为建模提供更高的自动化程度。结果可以随后在分析笔记本中展示。
- en: There is a longer-term aspect to having automated model creation. Once a data
    model has been created, an analyst can also look at changes to a model and what
    implications the changes should have on the way an enterprise operates. For example,
    an application may perform a monthly test to be sure new data matches the established
    mean, median, and standard deviation reflecting the expected normal distribution
    of data. In the event that a batch of data doesn’t fit the established model,
    further investigation is required to uncover the root cause of this change.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化模型创建有一个长期的影响。一旦创建了数据模型，分析师还可以查看模型的变化以及这些变化对企业运营方式的影响。例如，一个应用程序可能每月进行一次测试，以确保新数据与已建立的均值、中位数和标准差相匹配，这些均值、中位数和标准差反映了数据的预期正态分布。如果一批数据不符合已建立的模型，则需要进一步调查以找出这种变化的原因。
- en: 15.1 Description
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.1 描述
- en: This application will create a report on a dataset presenting a number of statistics.
    This automates the ongoing monitoring aspect of an Analysis Notebook, reducing
    the manual steps and creating reproducible results. The automated computations
    stem from having a statistical model for the data, often created in an analysis
    notebook, where alternative models are explored. This reflects variables with
    values in an expected range.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 此应用程序将创建一个关于数据集的报告，展示一系列统计数据。这自动化了分析笔记本的持续监控方面，减少了手动步骤并创建了可重复的结果。自动计算源于对数据的统计模型，通常在分析笔记本中创建，其中探索了不同的模型。这反映了具有预期范围内值的变量。
- en: For industrial monitoring, this is part of an activity called **Gage repeatability**
    **and** **r****eproducibility**. The activity seeks to confirm that measurements
    are repeatable and reproducible. This is described as looking at a “measurement
    instrument.” While we often think of an instrument as being a machine or a device,
    the definition is actually very broad. A survey or questionnaire is a measurement
    instrument focused on people’s responses to questions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于工业监控，这是称为**量具重复性**和**再现性**的活动的一部分。这项活动旨在确认测量是可重复和可再现的。这被描述为查看“测量仪器”。虽然我们经常认为仪器是机器或设备，但实际上定义非常广泛。调查或问卷是关注人们对问题回答的测量仪器。
- en: When these computed statistics deviate from expectations, it suggests something
    has changed, and the analyst can use these unexpected values to investigate the
    root cause of the deviation. Perhaps some enterprise process has changed, leading
    to shifts in some metrics. Or, perhaps some enterprise software has been upgraded,
    leading to changes to the source data or encodings used to create the clean data.
    More complex still, it may be that the instrument doesn’t actually measure what
    we thought it measured; this new discrepancy may expose a gap in our understanding.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当这些计算出的统计数据与预期不符时，这表明某些东西已经改变，分析师可以使用这些意外的值来调查偏差的根本原因。可能是一些企业流程发生了变化，导致某些指标发生了变化。或者，可能是一些企业软件进行了升级，导致用于创建干净数据的源数据或编码发生了变化。更复杂的是，可能仪器实际上并没有测量我们所认为的量；这种新的差异可能暴露了我们理解上的差距。
- en: The repeatability of the model’s measurements is central to the usability of
    the measurements. Consider a ruler that’s so worn down over years of use that
    it is no longer square or accurate. This single instrument will produce different
    results depending on what part of the worn end is used to make the measurement.
    This kind of measurement variability may obscure the variability in manufacturing
    a part. Understanding the causes of changes is challenging and can require thinking
    “outside the box” — challenging assumptions about the real-world process, the
    measurements of the process, and the model of those measurements.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 模型测量的可重复性是测量可用性的关键。考虑一把经过多年使用而磨损严重的尺子，它已经不再方或不准确。这个单一的工具将根据磨损端被用来进行测量的部分产生不同的结果。这种测量变异性可能会掩盖制造部件的变异性。理解变化的原因具有挑战性，可能需要“跳出思维定式”——挑战关于现实世界过程、过程测量以及这些测量的模型的假设。
- en: Exploratory data analysis can be challenging and exhilarating precisely because
    there aren’t obvious, simple answers to explain why a measurement has changed.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 探索性数据分析可能具有挑战性和令人兴奋，正是因为没有明显的简单答案来解释为什么测量发生了变化。
- en: The implementation of this preliminary model is through an application, separate
    from the previous stages in the pipeline to acquire and clean the data. With some
    careful design, this stage can be combined with those previous stages, creating
    a combined sequence of operations to acquire, clean, and create the summary statistics.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这个初步模型的实现是通过一个应用程序来完成的，它与数据获取和清理的前一阶段是分开的。通过一些精心设计，这个阶段可以与前述阶段合并，创建一个获取、清理和创建总结统计的联合操作序列。
- en: This application will overlap with the analysis notebook and the initial inspection
    notebook. Some of the observations made during those earlier ad-hoc analysis stages
    will be turned into fixed, automated processing.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这个应用程序将与分析笔记本和初始检查笔记本重叠。在那些早期的临时分析阶段中做出的某些观察将被转化为固定、自动化的处理。
- en: This is the beginning of creating a more complicated machine-learning model
    of the data. In some cases, a statistical model using linear or logistic regression
    is adequate, and a more complex artificial intelligence model isn’t needed. In
    other cases, the inability to create a simple statistical model can point toward
    a need to create and tune the hyperparameters of a more complicated model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是创建更复杂机器学习数据模型的开端。在某些情况下，使用线性或逻辑回归的统计模型是足够的，不需要更复杂的人工智能模型。在其他情况下，无法创建简单的统计模型可能表明需要创建和调整更复杂模型的超参数。
- en: The objective of this application is to save a statistical summary report that
    can be aggregated with and compared to other summary reports. The ideal structure
    will be a document in an easy-to-parse notation. JSON is suggested, but other
    easier-to-read formats like TOML are also sensible.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本应用的目标是保存一个可以与其他总结报告汇总和比较的统计摘要报告。理想的结构将是一个易于解析的文档。建议使用JSON，但其他易于阅读的格式，如TOML，也是合理的。
- en: 'There are three key questions about data distribution:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据分布有三个关键问题：
- en: What is the **location** or expected value for the output being measured?
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 被测量的输出**位置**或预期值是什么？
- en: What is the **spread** or expected variation for this variable?
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个变量的**分布**或预期变化是什么？
- en: What is the general **shape**, e.g., is it symmetric or skewed in some way?
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一般的**形状**是什么，例如，它是否是对称的或者以某种方式偏斜？
- en: For more background on these questions, see [https://www.itl.nist.gov/div898/handbook/ppc/section1/ppc131.htm](https://www.itl.nist.gov/div898/handbook/ppc/section1/ppc131.htm)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些问题的更多背景信息，请参阅[https://www.itl.nist.gov/div898/handbook/ppc/section1/ppc131.htm](https://www.itl.nist.gov/div898/handbook/ppc/section1/ppc131.htm)
- en: 'This summary processing will become part of an automated acquire, clean, and
    summarize operation. The **User Experience** (**UX**) will be a command-line application.
    Our expected command line should look something like the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这项总结处理将成为自动化获取、清理和总结操作的一部分。**用户体验**（**UX**）将是一个命令行应用程序。我们预期的命令行可能看起来像以下这样：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `-o` option specifies the path to an output sub-directory. The output filename
    added to this path will be derived from the source file name. The source file
    name often encodes information on the applicable date range for the extracted
    data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`-o`选项指定输出子目录的路径。添加到该路径的输出文件名将来自源文件名。源文件名通常包含有关提取数据适用日期范围的信息。'
- en: The Anscombe’s Quartet data doesn’t change and wouldn’t really have an “applicable
    date” value.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 安斯康姆四重奏数据不会改变，并且实际上不会有“适用日期”的值。
- en: We’ve introduced the **idea** of periodic enterprise extractions. None of the
    projects actually specify a data source subject to periodic change.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了周期性企业提取的**想法**。实际上，没有任何项目指定一个周期性变化的数据源。
- en: Some web services like [http://www.yelp.com](http://www.yelp.com) have health-code
    data for food-service businesses; this is subject to periodic change and serves
    as a good source of analytic data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一些网络服务，如[http://www.yelp.com](http://www.yelp.com)，为餐饮业提供健康代码数据；这些数据是周期性变化的，并且是分析数据的好来源。
- en: Now that we’ve seen the expectations, we can turn to an approach to the implementation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了期望，我们可以转向实现的方法。
- en: 15.2 Approach
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.2 方法
- en: 'We’ll take some guidance from the C4 model ( [https://c4model.com](https://c4model.com))
    when looking at our approach:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在审视我们的方法时，我们将借鉴C4模型（[https://c4model.com](https://c4model.com)）的一些指导：
- en: '**Context**: For this project, a context diagram would show a user creating
    analytical reports. You may find it helpful to draw this diagram.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文**：对于这个项目，一个上下文图将显示用户创建分析报告。你可能觉得绘制这个图会有所帮助。'
- en: '**Containers**: There only seems to be one container: the user’s personal computer.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Components**: We’ll address the components below.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code**: We’ll touch on this to provide some suggested directions.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The heart of this application is a module to summarize data in a way that lets
    us test whether it fits the expectations of a model. The statistical model is
    a simplified reflection of the underlying real-world processes that created the
    source data. The model’s simplifications include assumptions about events, measurements,
    internal state changes, and other details of the processing being observed.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: For very simple cases — like Anscombe’s Quartet data — there are only two variables,
    which leaves a single relationship in the model. Each of the four sample collections
    in the quartet has a distinct relationship. Many of the summary statistics, however,
    are the same, making the relationship often surprising.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: For other datasets, with more variables and more relationships, there are numerous
    choices available to the analyst. The *NIST* *Engineering Statistics Handbook*
    has an approach to modeling. See [https://www.itl.nist.gov/div898/handbook/index.htm](https://www.itl.nist.gov/div898/handbook/index.htm)
    for the design of a model and analysis of the results of the model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'As part of the preliminary work, we will distinguish between two very broad
    categories of statistical summaries:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '**Univariate statistics**: These are variables viewed in isolation.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multivariate statistics**: These are variables in pairs (or higher-order
    groupings) with an emphasis on the relationship between the variable’s values.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For univariate statistics, we need to understand the distribution of the data.
    This means measuring the location (the center or expected values), the spread
    (or scale), and the shape of the distribution. Each of these measurement areas
    has several well-known statistical functions that can be part of the summary application.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: We’ll look at the multivariate statistics in the next chapter. We’ll start the
    univariate processing by looking at the application in a general way, and then
    focus on the statistical measures, the inputs, and finally, the outputs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 15.2.1 Designing a summary app
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This application has a command-line interface to create a summary from the cleaned
    data. The input file(s) are the samples to be summarized. The summary must be
    in a form that’s easy to process by subsequent software. This can be a JSON- or
    a TOML-formatted file with the summary data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: The summaries will be ”measures of location,” sometimes called a ”central tendency.”
    See [https://www.itl.nist.gov/div898/handbook/eda/section3/eda351.htm](https://www.itl.nist.gov/div898/handbook/eda/section3/eda351.htm).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: The output must include enough context to understand the data source, and the
    variable being measured. The output also includes the measured values to a sensible
    number of decimal places. It’s important to avoid introducing additional digits
    into floating-point values when those digits are little more than noise.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 输出必须包含足够的信息以理解数据源和被测量的变量。输出还包括以合理的小数位数测量的值。当这些数字只是噪声时，避免在浮点值中引入额外的数字是很重要的。
- en: 'A secondary feature of this application is to create an easy-to-read presentation
    of the summary. This can be done by using tools like **Docutils** to transform
    a reStructuredText report into HTML or a PDF. A tool like **Pandoc** could also
    be used to convert a source report into something that isn’t simply text. The
    technique explored in [*Chapter** 14*](ch018.xhtml#x1-31300014), [*Project 4.2:
    Creating Reports*](ch018.xhtml#x1-31300014) is to use Jupyter{Book} to create
    a document suitable for publication.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此应用程序的次要功能是创建一个易于阅读的摘要展示。这可以通过使用像**Docutils**这样的工具将reStructuredText报告转换为HTML或PDF来实现。也可以使用**Pandoc**这样的工具将源报告转换为不仅仅是文本的内容。在第[*第14章*](ch018.xhtml#x1-31300014)、[*项目4.2：创建报告*](ch018.xhtml#x1-31300014)中探讨的技术是使用Jupyter{Book}创建适合出版的文档。
- en: We’ll start by looking at some of the measures of location that need to be computed.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先查看需要计算的一些位置度量标准。
- en: 15.2.2 Describing the distribution
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.2 描述分布
- en: As noted above, there are three aspects of the distribution of a variable. The
    data tends to scatter around a central tendency value; we’ll call this the location.
    There will be an expected limit on the scattering; we’ll call this the spread.
    There may be a shape that’s symmetric or skewed in some way. The reasons for scattering
    may include measurement variability, as well as variability in the process being
    measured.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，变量的分布有三个方面。数据倾向于围绕一个中心趋势值分散；我们将称之为位置。将有一个预期的分散极限；我们将称之为范围。可能有一个对称或以某种方式偏斜的形状。分散的原因可能包括测量变异性，以及被测量的过程中的变异性。
- en: 'The NIST Handbook defines three commonly-used measures of location:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: NIST手册定义了三个常用的位置度量标准：
- en: '**mean**: The sum of the variable’s values divided by the count of values:
    *X* = ![∑ --XNi](img/file60.jpg).'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均值**：变量的值的总和除以值的计数：*X* = ![∑ --XNi](img/file60.jpg)。'
- en: '**median**: The value of a value that is in the center of the distribution.
    Half the values are less than or equal to this value, and half the values are
    greater than or equal to this value. First, sort the values into ascending order.
    If there’s an odd number, it’s the value in the center. For an even number of
    values, split the difference between the two center-most values.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中位数**：位于分布中心位置的值。一半的值小于或等于这个值，另一半的值大于或等于这个值。首先，将值按升序排序。如果数量是奇数，那么就是中间的值。对于偶数个值，取两个最中心值之间的平均值。'
- en: '**mode**: The most common value. For some of the Anscombe Quartet data series,
    this isn’t informative because all of the values are unique.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**众数**：最常见值。对于Anscombe四重奏数据系列中的一些数据，这并不具有信息性，因为所有值都是唯一的。'
- en: These functions are first-class parts of the built-in `statistics` module, making
    them relatively easy to compute.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数是内置`statistics`模块的一级部分，使得它们相对容易计算。
- en: There are some alternatives that may be needed when the data is polluted by
    outliers. There are techniques like *Mid-Mean* and *Trimmed Mean* to discard data
    outside some range of percentiles.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据受到异常值污染时，可能需要一些替代方案。有一些技术，如*中值均值*和*截断均值*，可以丢弃某些百分位数范围之外的数据。
- en: The question of an ”outlier” is a sensitive topic. An outlier may reflect a
    measurement problem. An outlier may also hint that the processing being measured
    is quite a bit more complicated than is revealed in a set of samples. Another,
    separate set of samples may reveal a different mean or a larger standard deviation.
    The presence of outliers may suggest more study is needed to understand the nature
    of these values.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: “异常值”的问题是一个敏感的话题。异常值可能反映了一个测量问题。异常值也可能暗示被测量的处理过程比样本集中揭示的要复杂得多。另一个独立的样本集可能揭示不同的均值或更大的标准差。异常值的存在可能表明需要更多的研究来了解这些值的本质。
- en: 'There are three commonly-used measures for the scale or spread of the data:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据的规模或分布，有三个常用的度量标准：
- en: '**Variance** and standard deviation. The variance is — essentially — the average
    of the squared distance of each sample from the mean: *s*² = ![∑ Xi−X¯ -(N−1)-](img/file61.jpg).
    The standard deviation is the square root of the variance.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方差**和标准差。方差本质上是从平均值到每个样本平方距离的平均值：*s*² = ![∑ Xi−X¯ -(N−1)-](img/file61.jpg)。标准差是方差的平方根。'
- en: '**Range** is the difference between the largest and smallest values.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**范围**是最大值和最小值之间的差异。'
- en: '**Median absolute deviation** is the median of the distance of each sample
    from the mean: MAD[Y] = median(|*Y* [i] −*Ỹ*|). See [*Chapter** 7*](ch011.xhtml#x1-1610007),
    [*Data Inspection Features*](ch011.xhtml#x1-1610007).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中值绝对偏差**是每个样本与平均值距离的中值：MAD[Y] = median(|*Y* [i] −*Ỹ*|)。参见[*第7章*](ch011.xhtml#x1-1610007)，[*数据检查功能*](ch011.xhtml#x1-1610007)。'
- en: The variance and standard deviation functions are first-class parts of the built-in
    `statistics` module. The range can be computed using the built-in `min()` and
    `max()` functions. A median absolute deviation function can be built using functions
    in the `statistics` module.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 方差和标准差函数是内置`statistics`模块的一级部分。范围可以使用内置的`min()`和`max()`函数来计算。可以使用`statistics`模块中的函数构建一个中值绝对偏差函数。
- en: There are also measures for skewness and kurtosis of a distribution. We’ll leave
    these as extras to add to the application once the base statistical measures are
    in place.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分布的偏度和峰度也有相应的度量。我们将把这些作为额外的功能添加到应用中，一旦基础统计度量已经到位。
- en: 15.2.3 Use cleaned data model
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.3 使用清洗数据模型
- en: It’s essential to use the cleaned, normalized data for this summary processing.
    There is some overlap between an inspection notebook and this more detailed analysis.
    An initial inspection may also look at some measures of location and range to
    determine if the data can be used or contains errors or problems. During the inspection
    activities, it’s common to start creating an intuitive model of the data. This
    leads to formulating hypotheses about the data and considering experiments to
    confirm or reject those hypotheses.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个总结处理，使用清洗和归一化的数据是至关重要的。检查笔记本和这个更详细的分析之间存在一些重叠。初步检查也可能查看一些位置和范围的度量，以确定数据是否可以使用或包含错误或问题。在检查活动中，通常开始创建数据的直观模型。这导致了对数据的假设形成，并考虑实验来证实或拒绝这些假设。
- en: This application formalizes hypothesis testing. Some functions from an initial
    data inspection notebook may be refactored into a form where those functions can
    be used on the cleaned data. The essential algorithm may be similar to the raw
    data version of the function. The data being used, however, will be the cleaned
    data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这个应用形式化了假设检验。一些从初始数据检查笔记本中的函数可能被重构为可以使用清洗数据的形式。基本算法可能与函数的原始数据版本相似。然而，所使用的数据将是清洗后的数据。
- en: This leads to a sidebar design decision. When we look back at the data inspection
    notebook, we’ll see some overlaps.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了一个侧边栏设计决策。当我们回顾数据检查笔记本时，我们会看到一些重叠。
- en: 15.2.4 Rethink the data inspection functions
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.4 重新思考数据检查函数
- en: Because Python programming can be generic — independent of any specific data
    type — it’s tempting to try to unify the raw data processing and the cleaned data
    processing. The desire manifests as an attempt to write exactly one version of
    some algorithm, like the Median Absolute Deviation function that’s usable for
    *both* raw and cleaned data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Python编程可以是一般的——独立于任何特定的数据类型——因此尝试统一原始数据处理和清洗数据处理是有诱惑力的。这种愿望表现为尝试编写一个算法的精确版本，例如可用于*原始*和清洗数据的**中值绝对偏差**函数。
- en: This is not always an achievable goal. In some situations, it may not even be
    desirable.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不总是可以达到的目标。在某些情况下，甚至可能并不希望这样做。
- en: A function to process raw data must often do some needed cleaning and filtering.
    These overheads are later refactored and implemented in the pipeline to create
    cleaned data. To be very specific, the `if` conditions used to exclude bad data
    can be helpful during the inspection. These conditions will become part of the
    clean-and-convert applications. Once this is done, they are no longer relevant
    for working with the cleaned data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 处理原始数据的功能通常需要进行一些必要的清理和过滤。这些开销随后被重构并实现到管道中，以创建清洗数据。具体来说，用于排除不良数据的`if`条件在检查期间可能是有帮助的。这些条件将成为清洁和转换应用的一部分。一旦完成，它们就不再与处理清洗数据相关了。
- en: Because the extra data cleanups are required for inspecting raw data, but not
    required for analyzing cleaned data, it can be difficult to create a single process
    that covers both cases. The complications required to implement this don’t seem
    to be worth the effort.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于额外的数据清理对于检查原始数据是必需的，但对于分析清理后的数据则不是必需的，因此创建一个涵盖这两种情况的单个过程可能很困难。实现这些复杂性的努力似乎不值得。
- en: There are some additional considerations. One of these is the general design
    pattern followed by Python’s `statistics` module. This module works with sequences
    of atomic values. Our applications will read (and write) complicated `Sample`
    objects that are not atomic Python integer or float values. This means our applications
    will extract sequences of atomic values from sequences of complicated `Sample`
    objects.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些额外的考虑。其中之一是Python的`statistics`模块遵循的一般设计模式。此模块与原子值的序列一起工作。我们的应用程序将读取（并写入）复杂的`Sample`对象，这些对象不是原子的Python整数或浮点值。这意味着我们的应用程序将从复杂的`Sample`对象序列中提取原子值序列。
- en: The raw data, on the other hand, may not have a very sophisticated class definition.
    This means the decomposition of complicated objects isn’t part of the raw data
    processing.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，原始数据可能没有非常复杂的类定义。这意味着复杂对象的分解不是原始数据处理的一部分。
- en: For some very, very large datasets the decomposition of complicated multivariate
    objects to individual values may happen as the data is being read. Rather than
    ingest millions of objects, the application may extract a single attribute for
    processing.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些非常大型的数据集，复杂的多变量对象的分解可能会在读取数据时发生。而不是摄入数百万个对象，应用程序可能只提取一个属性进行处理。
- en: 'This might lead to input processing that has the following pattern:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会导致以下模式的输入处理：
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This example defines a generic function, `attr_iter()`, to read an ND JSON file
    to build instances of some class, `Sample`. (The details of the `Sample` class
    are omitted.)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例定义了一个通用函数，`attr_iter()`，用于读取ND JSON文件以构建某些类，`Sample`的实例。（`Sample`类的详细信息被省略。）
- en: The `x_values()` function uses the generic `attr_iter()` function with a concrete
    lambda object to extract a specific variable’s value, and create a list object.
    This list object can then be used with various statistical functions.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`x_values()`函数使用通用的`attr_iter()`函数和一个具体的lambda对象来提取特定变量的值，并创建一个列表对象。这个列表对象可以与各种统计函数一起使用。'
- en: While a number of individual `Sample` objects are created, they aren’t retained.
    Only the values of the `x` attribute are saved, reducing the amount of memory
    used to create summary statistics from a large collection of complicated values.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然创建了多个`Sample`对象，但它们不会被保留。只有`x`属性的值被保存，从而减少了从大量复杂值创建汇总统计所使用的内存量。
- en: 15.2.5 Create new results model
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.5 创建新的结果模型
- en: 'The statistical summary contains three broad kinds of data:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 统计摘要包含三种类型的数据：
- en: Metadata to specify what source data is used to create the summary.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元数据用于指定用于创建汇总的源数据。
- en: Metadata to specify what measures are being used.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元数据用于指定正在使用的度量标准。
- en: The computed values for location, shape, and spread.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置、形状和分布的计算值。
- en: In some enterprise applications, source data is described by a range of dates
    defining the earliest and latest samples. In some cases, more details are required
    to describe the complete context. For example, the software to acquire raw data
    may have been upgraded in the past. This means older data may be incomplete. This
    means the context for processing data may require some additional details on software
    versions or releases in addition to the range of dates and data sources.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些企业应用中，源数据由一系列日期定义，这些日期定义了最早和最晚的样本。在某些情况下，需要更多细节来描述完整的上下文。例如，过去可能已经升级了获取原始数据的软件。这意味着旧数据可能不完整。这意味着数据处理的环境可能需要一些额外的细节，比如软件版本或发布信息，除了日期范围和数据源。
- en: Similarly, the measures being used may shift over time. The computation of skewness,
    for example, may switch from the **Fisher-Pearson** formula to the **adjusted
    Fisher-Pearson** formula. This suggests the version information for the summary
    program should also be recorded along with the results computed.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，所使用的度量标准可能会随时间而变化。例如，偏度的计算可能会从**Fisher-Pearson**公式切换到**调整后的Fisher-Pearson**公式。这表明摘要程序的版本信息也应与计算出的结果一起记录。
- en: Each of these metadata values provides necessary context and background information
    on the data source, the method of collection, and any computations of derived
    data. This context may be helpful in uncovering the root cause of changes. In
    some cases, the context is a way to catalog underlying assumptions about a process
    or a measurement instrument; seeing this context may allow an analyst to challenge
    assumptions and locate the root cause of a problem.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 每个这些元数据值都提供了关于数据源、收集方法和任何派生数据计算所必需的上下文和背景信息。这种上下文可能有助于揭示变化的原因。在某些情况下，上下文是记录关于过程或测量仪器的潜在假设的一种方式；看到这种上下文可能允许分析师质疑假设并定位问题的根本原因。
- en: 'The application must create a result document that looks something like the
    following example:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序必须创建一个看起来像以下示例的结果文档：
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This file can be parsed by the `toml` or `tomllib` module to create a nested
    collection of dictionaries. The secondary feature of the summary application is
    to read this file and write a report, perhaps using Markdown or ReStructuredText
    that provides the data in a readable format suitable for publication.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件可以被`toml`或`tomllib`模块解析，以创建嵌套的字典集合。总结应用程序的次要功能是读取此文件并生成报告，可能使用Markdown或ReStructuredText，以提供适合发布的可读格式数据。
- en: For Python 3.11 or newer, the `tomllib` module is built in. For older Python
    installations, the `toml` module needs to be installed.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Python 3.11或更新的版本，`tomllib`模块是内置的。对于较旧的Python安装，需要安装`toml`模块。
- en: Now that we’ve seen the overall approach, we can look at the specific deliverable
    files.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了整体方法，我们可以看看具体的可交付成果文件。
- en: 15.3 Deliverables
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.3 可交付成果
- en: 'This project has the following deliverables:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目有以下可交付成果：
- en: Documentation in the `docs` folder.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docs`文件夹中的文档。'
- en: Acceptance tests in the `tests/features` and `tests/steps` folders.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tests/features`和`tests/steps`文件夹中的接受测试。'
- en: Unit tests for model module classes in the `tests` folder.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tests`文件夹中模型模块类的单元测试。'
- en: Mock objects for the `csv_extract` module tests will be part of the unit tests.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`csv_extract`模块测试的模拟对象将是单元测试的一部分。'
- en: Unit tests for the `csv_extract` module components that are in the `tests` folder.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tests`文件夹中`csv_extract`模块组件的单元测试。'
- en: An application to summarize the cleaned data in a TOML file.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于将清洗后的数据总结到TOML文件中的应用程序。
- en: An application secondary feature to transform the TOML file to an HTML page
    or PDF file with the summary.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将TOML文件转换为包含总结的HTML页面或PDF文件的应用程序次要功能。
- en: In some cases, especially for particularly complicated applications, the summary
    statistics may be best implemented as a separate module. This module can then
    be expanded and modified without making significant changes to the overall application.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，特别是对于特别复杂的应用程序，总结统计可能最好作为一个单独的模块实现。然后，可以扩展和修改此模块，而无需对整体应用程序进行重大更改。
- en: 'The idea is to distinguish between these aspects of this application:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 理念是要区分这个应用程序的以下方面：
- en: The CLI, which includes argument parsing and sensible handling of input and
    output paths.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命令行界面（CLI），包括参数解析和输入输出路径的合理处理。
- en: The statistical model, which evolves as our understanding of the problem domain
    and the data evolve.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计模型，随着我们对问题域和数据理解的发展而发展。
- en: The data classes, which describe the structure of the samples, independent of
    any specific purpose.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述样本结构的数据类，独立于任何特定目的。
- en: For some applications, these aspects do not involve a large number of classes
    or functions. In a case where the definitions are small, a single Python module
    will do nicely. For other applications, particularly those where initial assumptions
    turned out to be invalid and significant changes were made, having separate modules
    can permit more flexibility, and more agility with respect to future changes.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些应用程序，这些方面不涉及大量的类或函数。在定义较小的情况下，一个单独的Python模块就足够好了。对于其他应用程序，尤其是那些初始假设最终被证明无效且进行了重大更改的情况，拥有独立的模块可以提供更多的灵活性，以及在未来变化方面的更多敏捷性。
- en: We’ll look at a few of these deliverables in a little more detail. We’ll start
    with some suggestions for creating the acceptance tests.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将更详细地查看一些这些可交付成果。我们将从创建接受测试的建议开始。
- en: 15.3.1 Acceptance testing
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.1 接受测试
- en: The acceptance tests need to describe the overall application’s behavior from
    the user’s point of view. The scenarios will follow the UX concept of a command-line
    application to acquire data and write output files. Because the input data has
    been cleaned and converted, there are few failure modes for this application;
    extensive testing of potential problems isn’t as important as it was in earlier
    data-cleaning projects.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'For relatively simple datasets, the results of the statistical summaries are
    known in advance. This leads to features that might look like the following example:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We could continue the scenario with a number of additional `Then` steps to validate
    each of the locations and the spread and shape the statistical summaries.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The step definitions will be similar to step definitions for a number of previous
    projects. Specifically, the `When` step will use the `subprocess.run()` function
    to execute the given application with the required command-line arguments.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: The first of the `Then` steps will need to read — and parse — the TOML file.
    The resulting summary object can be placed in the `context` object. Subsequent
    `Then` steps can examine the structure to locate the individual values, and confirm
    the values match the acceptance test expectations.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: It is often helpful to extract a small subset of data to use for acceptance
    testing. Instead of processing millions of rows, a few dozen rows are adequate
    to confirm the application has read and summarized data. The data only needs to
    be representative of the larger set of samples under consideration.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Because the chosen subset is part of the testing suite; it rarely changes. This
    makes the results predictable.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: As the data collection process evolves, it’s common to have changes to the data
    sources. This will lead to changes in the data cleaning. This may, in turn, lead
    to changes in the summary application as new codes or new outliers must be handled
    properly. The evolution of the data sources implies that the test data suite will
    also need to evolve to expose any of the special, edge, or corner cases.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, the test data suite is a mixture of ordinary — no surprises — data,
    mixed with representative examples of each of the special, atypical cases. As
    this test data suite evolves, the acceptance test scenario will also evolve.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: The TOML file is relatively easy to parse and verify. The secondary feature
    of this application — expanding on the TOML output to add extensive Markdown —
    also works with text files. This makes it relatively easy to confirm with test
    scenarios that read and write text files.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: The final publication, whether done by Pandoc or a combination of Pandoc and
    a LaTeX toolchain, isn’t the best subject for automated testing. A good copy editor
    or trusted associate needs to make sure the final document meets the stakeholder’s
    expectations.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 15.3.2 Unit testing
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s important to have unit testing for the various components that are unique
    to this application. The clean data class definition, for example, is created
    by another application, with its own test suite. The unit tests for this application
    don’t need to repeat those tests. Similarly, the `statistics` module has extensive
    unit tests; this application’s unit tests do not need to replicate any of that
    testing.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: This further suggests that the `statistics` module should be replaced with `Mock`
    objects. Those mock objects can — generally — return `sentinel` objects that will
    appear in the resulting TOML-format summary document.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'This suggests test cases structured like the following example:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The two test fixtures provide mock results, using `sentinel` objects. Using
    `sentinel` objects allows easy comparison to be sure the results of the mocked
    functions were not manipulated unexpectedly by the application.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: The test case, `test_var_summary()`, provides a mocked source of data in the
    form of another `sentinel` object. The results have the expected structure and
    the expected `sentinel` objects.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: The final part of the test confirms the sample data — untouched — was provided
    to the mocked statistical functions. This confirms the application doesn’t filter
    or transform the data in any way. The results are the expected `sentinel` objects;
    this confirms the module didn’t adulterate the results of the `statistics` module.
    And the final check confirms that the mocked functions were called exactly once
    with the expected parameters.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: This kind of unit test, with numerous mocks, is essential for focusing the testing
    on the new application code, and avoiding tests of other modules or packages.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 15.3.3 Application secondary feature
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A secondary feature of this application transforms the TOML summary into a more
    readable HTML or PDF file. This feature is a variation of the kinds of reporting
    done with Jupyter Lab (and associated tools like **Jupyter** {**Book**}).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s an important distinction between these two classes of reports:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: The Jupyter Lab reports involve discovery. The report content is always new.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The summary application’s reports involve confirmation of expectations. The
    report content should not be new or surprising.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In some cases, the report will be used to confirm (or deny) an expected trend
    is continuing. The application applies the trend model to the data. If the results
    don’t match expectations, this suggests follow-up action is required. Ideally,
    it means the model is incorrect, and the trend is changing. The less-than-ideal
    case is the observation of an unexpected change in the applications providing
    the source data.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'This application decomposes report writing into three distinct steps:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '**Content**: This is the TOML file with the essential statistical measures.'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Structure**: The secondary feature creates an intermediate markup file in
    Markdown or the RST format. This has an informative structure around the essential
    content.'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Presentation**: The final publication document is created from the structured
    markup plus any templates or style sheets that are required.'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final presentation is kept separate from the document’s content and structure.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: An HTML document’s final presentation is created by a browser. Using a tool
    like **Pandoc** to create HTML from Markdown is — properly — replacing one markup
    language with another markup language.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Creating a PDF file is a bit more complicated. We’ll leave this in the extras
    section at the end of this chapter.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: The first step toward creating a nicely formatted document is to create the
    initial Markdown or ReStructuredText document from the summary. In many cases,
    this is easiest done with the **Jinja** package. See [https://jinja.palletsprojects.com/en/3.1.x/](https://jinja.palletsprojects.com/en/3.1.x/)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'One common approach is the following sequence of steps:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Write a version of the report using Markdown (or RST).
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Locate a template and style sheets that produce the desired HTML page when converted
    by the **Pandoc** or **Docutils** applications.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Refactor the source file to replace the content with **Jinja** placeholders.
    This becomes the template report.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write an application to parse the TOML, then apply the TOML details to the template
    file.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When using **Jinja** to enable filling in the template, it must be added to
    the `requirements.txt` file. If **ReStructuredText** (**RST**) is used, then the
    **docutils** project is also useful and should be added to the `requirements.txt`
    file.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: If Markdown is used to create the report, then **Pandoc** is one way to handle
    the conversion from Markdown to HTML. Because **Pandoc** also converts RST to
    HTML, the **docutils** project is not required.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the parsed TOML is a dictionary, fields can be extracted by the Jinja
    template. We might have a Markdown template file with a structure like the following:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `{{`` some-expression`` }}` constructs are placeholders. This is where Jinja
    will evaluate the Python expression and replace the placeholders with the resulting
    value. Because of Jinja’s clever implementation, a name like `summary[’x’][’location’][’mean’]`
    can be written as `summary.x.location.mean`, also.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: The lines with `#` and `##` are the way Markdown specifies the section headings.
    For more information on Markdown, see [https://daringfireball.net/projects/markdown/](https://daringfireball.net/projects/markdown/).
    Note that there are a large number of Markdown extensions, and it’s important
    to be sure the rendering engine (like Pandoc) supports the extensions you’d like
    to use.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: The Jinja template language has numerous options for conditional and repeating
    document sections. This includes `{%`` for`` name`` in`` sequence`` %}` and `{%`` if`` condition`` %}`
    constructs to create extremely sophisticated templates. With these constructs,
    a single template can be used for a number of closely related situations with
    optional sections to cover special situations.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The application program to inject values from the `summary` object into the
    template shouldn’t be much more complicated than the examples shown on the Jinja
    basics page. See [https://jinja.palletsprojects.com/en/3.1.x/api/#basics](https://jinja.palletsprojects.com/en/3.1.x/api/#basics)
    for some applications that load a template and inject values.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: This program’s output is a file with a name like `summary_report.md`. This file
    would be ready for conversion to any of a large number of other formats.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of converting a Markdown file to HTML is handled by the **Pandoc**
    application. See [https://pandoc.org/demos.html](https://pandoc.org/demos.html).
    The command might be as complicated as the following:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `pandoc.css` file can provide the CSS styles to create a body that’s narrow
    enough to be printed on an ordinary US letter or A4 paper.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: The application that creates the `summary_report.md` file can use `subprocess.run()`
    to execute the **Pandoc** application and create the desired HTML file. This provides
    a command-line UX that results in a readable document, ready to be distributed.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 15.4 Summary
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter we have created a foundation for building and using a statistical
    model of source data. We’ve looked at the following topics:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Designing and building a more complex pipeline of processes for gathering and
    analyzing data.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the core concepts behind creating a statistical model of some data.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use of the built-in `statistics` library.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publishing the results of the statistical measures.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This application tends to be relatively small. The actual computations of the
    various statistical values leverage the built-in `statistics` library and tend
    to be very small. It often seems like there’s far more programming involved in
    parsing the CLI argument values, and creating the required output file, than doing
    the “real work” of this application.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a consequence of the way we’ve been separating the various concerns
    in data acquisition, cleaning, and analysis. We’ve partitioned the work into several,
    isolated stages along a pipeline:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Acquiring raw data, generally in text form. This can involve database access
    or RESTful API access, or complicated file parsing problems.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cleaning and converting the raw data to a more useful, native Python form. This
    can involve complications of parsing text and rejecting outlier values.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summarizing and analyzing the cleaned data. This can focus on the data model
    and reporting conclusions about the data.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The idea here is the final application can grow and adapt as our understanding
    of the data matures. In the next chapter, we’ll add features to the summary program
    to create deeper insights into the available data.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 15.5 Extras
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here are some ideas for you to add to this project.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 15.5.1 Measures of shape
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The measurements of shape often involve two computations for skewness and kurtosis.
    These functions are not part of Python’s built-in `statistics` library.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that there are a very large number of distinct, well-understood
    distributions of data. The normal distribution is one of many different ways data
    can be distributed.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: See [https://www.itl.nist.gov/div898/handbook/eda/section3/eda366.htm](https://www.itl.nist.gov/div898/handbook/eda/section3/eda366.htm).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'One measure of skewness is the following:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑(Y− ¯Y)3 ----iN---- g1 = s3 ](img/file62.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: Where *Ȳ* is the mean, and *s* is the standard deviation.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: A symmetric distribution will have a skewness, *g*[1], near zero. Larger numbers
    indicate a ”long tail” opposite a large concentration of data around the mean.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'One measure of kurtosis is the following:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![ ∑ (Y −Y¯)4 ---iN----- kurtosis = s4 ](img/file63.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: The kurtosis for the standard normal distribution is 3\. A value larger than
    3 suggests more data is in the tails; it’s ”flatter” or ”wider” than the standard
    normal distribution. A value less than three is ”taller” or ”narrower” than the
    standard.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: These metrics can be added to the application to compute some additional univariate
    descriptive statistics.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 15.5.2 Creating PDF reports
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the [*Application secondary feature*](#x1-3360003) section we looked at creating
    a Markdown or RST document with the essential content, some additional information,
    and an organizational structure. The intent was to use a tool like **Pandoc**
    to convert the Markdown to HTML. The HTML can be rendered by a browser to present
    an easy-to-read summary report.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'Publishing this document as a PDF requires a tool that can create the necessary
    output file. There are two common choices:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the **ReportLab** tool: [https://www.reportlab.com/dev/docs/](https://www.reportlab.com/dev/docs/).
    This is a commercial product with some open-source components.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the **Pandoc** tool coupled with a LaTeX processing tool.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See [*Preparing a report*](ch018.xhtml#x1-3190002) of [*Chapter** 14*](ch018.xhtml#x1-31300014),
    [*Project 4.2: Creating Reports*](ch018.xhtml#x1-31300014) for some additional
    thoughts on using LaTeX to create PDF files. While this involves a large number
    of separate components, it has the advantage of having the most capabilities.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: It’s often best to learn the LaTeX tools separately. The TeXLive project maintains
    a number of tools useful for rendering LaTeX. For macOS users, the MacTex project
    offers the required binaries. An online tool like Overleaf is also useful for
    handling LaTeX. Sort out any problems by creating small `hello_world.tex` example
    documents to see how the LaTeX tools work.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Once the basics of the LaTeX tools are working, it makes sense to add the **Pandoc**
    tool to the environment.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Neither of these tools are Python-based and don’t use **conda** or **pip** installers.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'As noted in [*Chapter** 14*](ch018.xhtml#x1-31300014), [*Project 4.2: Creating
    Reports*](ch018.xhtml#x1-31300014), there are a lot of components to this tool
    chain. This is a large number of separate installs that need to be managed. The
    results, however, can be very nice when a final PDF is created from a few CLI
    interactions.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 15.5.3 Serving the HTML report from the data API
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In [*Chapter** 12*](ch016.xhtml#x1-27600012), [*Project 3.8: Integrated Data
    Acquisition Web Service*](ch016.xhtml#x1-27600012) we created a RESTful API service
    to provide cleaned data.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: This service can be expanded to provide several other things. The most notable
    addition is the HTML summary report.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of creating a summary report will look like this:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: A user makes a request for a summary report for a given time period.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The RESTful API creates a “task” to be performed in the background and responds
    with the status showing the task has been created.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The user checks back periodically to see if the processing has finished. Some
    clever JavaScript programming can display an animation while an application program
    checks to see if the work is completed.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the processing is complete, the user can download the final report.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This means two new resources paths will need to be added to the OpenAPI specification.
    These two new resources are:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Requests to create a new summary. A POST request creates the task to build a
    summary and a GET request shows the status. A `2023.03/summarize` path will parallel
    the `2023.02/creation` path used to create the series.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requests for a summary report. A GET request will download a given statistical
    summary report. Perhaps a `2023.03/report` path would be appropriate.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we add features to the RESTful API, we need to consider the resource names
    more and more carefully. The first wave of ideas sometimes fails to reflect the
    growing understanding of the user’s needs.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'In retrospect, the `2023.02/create` path, defined in [*Chapter** 12*](ch016.xhtml#x1-27600012),
    [*Project 3.8: Integrated Data Acquisition Web Service*](ch016.xhtml#x1-27600012),
    may not have been the best name.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s an interesting tension between requests to create a resource and the
    resulting resource. The request to create a series is clearly distinct from the
    resulting series. Yet, they can both be meaningfully thought of as instances of
    “series.” The creation request is a kind of *future*: an expectation that will
    be fulfilled later.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: An alternative naming scheme is to use `2023.02/creation` for series, and use
    `2023.03/create/series` and `2023.03/create/summary` as distinct paths to manage
    the long-running background that does the work.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'The task being performed in the background will execute a number of steps:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Determine if the request requires new data or existing data. If new data is
    needed, it is acquired, and cleaned. This is the existing process to acquire the
    series of data points.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine if the requested summary does not already exist. (For new data, of
    course, it will not exist.) If a summary is needed, it is created.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定请求的总结是否已经存在。（对于新数据，当然不会存在。）如果需要总结，则创建它。
- en: Once the processing is complete, the raw data, cleaned data, and summary can
    all be available as resources on the API server. The user can request to download
    any of these resources.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦处理完成，原始数据、清洗后的数据和总结都可以作为资源在API服务器上提供。用户可以请求下载这些资源中的任何一项。
- en: It’s essential to be sure each of the components for the task work in isolation
    before attempting to integrate them as part of a web service. It’s far easier
    to diagnose and debug problems with summary reporting outside the complicated
    world of web services.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试将任务组件作为网络服务的一部分进行集成之前，确保每个组件都能独立工作是非常重要的。在复杂的网络服务世界之外，通过总结报告来诊断和调试问题要容易得多。
