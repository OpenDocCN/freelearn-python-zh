- en: Chapter 6. Audio Controls and Effects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, the focus was on learning fundamentals of audio processing.
    It introduced us to the GStreamer multimedia framework. We applied this knowledge
    to develop some frequently needed audio processing tools. In this chapter, we
    will go one step further by developing tools for adding audio effects, mixing
    audio tracks, creating custom music tracks, and so on.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In this chapter, we shall:'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to control a streaming audio.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spice up the audio by adding effects such as fading, echo, and panorama.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Work on a project where a custom music track will be created by combining different
    audio clips.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add visualization effect to a streaming audio.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mix two audio streams into a single track. For example, mix an audio containing
    only a *vocal track* with an audio containing only *background music track*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So let's get on with it.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling playback
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In an audio player, various options such as Play, Pause, Stop, and so on, provide
    a way to control the streaming audio. Such playback controls also find use in
    other audio processing techniques. We have already used some of the playback controls
    in [Chapter 5](ch05.html "Chapter 5. Working with Audios"), *Working with Audios*.
    In this chapter, we will study some more controlling options.
  prefs: []
  type: TYPE_NORMAL
- en: Play
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous chapter, we developed a preliminary command-line audio player
    using GStreamer. The audio streaming can be started by instructing the GStreamer
    pipeline to begin the flow of audio data. This was achieved by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: With the above instruction, the audio will be streamed until the end of the
    stream is reached. Refer to the code in the *Playing Audio* section of [Chapter
    5](ch05.html "Chapter 5. Working with Audios"), *Working with Audios* to see what
    the surrounding code looks like. If you develop a user interface for a simple
    audio player, the "Play" button can be connected to a method that will set the
    state of pipeline to `gst.STATE_PLAYING`.
  prefs: []
  type: TYPE_NORMAL
- en: Pause/resume
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The streaming audio can be paused temporarily by setting the GStreamer pipeline
    state to `gst.STATE_PAUSED`. Pausing music in an audio player is another commonly
    performed operation. But this also finds use while doing some special audio processing.
  prefs: []
  type: TYPE_NORMAL
- en: Time for action - pause and resume a playing audio stream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now review a very simple example demonstrating various playback control
    techniques. The same example will be used in the next few sections. This exercise
    will be an ideal preparation while working on the project 'Extract Audio Using
    Playback Controls'. So let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Download the file `PlaybackControlExamples.py` from the Packt website. This
    file has all the necessary code that illustrates various playback controls. The
    overall class and its methods are illustrated below for reference. See the source
    file to know more about each of these methods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The overall code layout is very similar to the code developed in the Playing
    audio section of Chapter 5, Working with Audios. Thus, we will just review some
    of the newly added methods relevant to this section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here is the code for `self.play` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Inside the while loop, on line 9, the current position of the streaming audio
    is queried using the query_position call. This is an API method of GStreamer Pipeline
    object. When the pipeline approaches the end of the stream, it may throw an error
    while querying the current position. Therefore, we catch the exception gst.QueryError,
    in the try-except block. The time.sleep call is important before entering the
    try-except block. It ensures that the position is queried every 0.5 seconds. If
    you remove this call, the next code will be executed for each incremental tiny
    step. From a performance standpoint this is unnecessary. The current position
    thus obtained is expressed in nanoseconds, Thus, if the time is say 0.1 seconds,
    it is obtained as 100 000 000 nanoseconds. To convert it into seconds, it is divided
    by a GStreamer constant gst.SECOND. On line 30, the main method that runs various
    audio control examples is called.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's see the code in `self.runExamples` method now.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The method self.okToRunExamples does some preliminary error checking and ensures
    that the total streaming duration is greater than 20 seconds. This method will
    not be discussed here. When the current track position reaches 5 seconds, one
    of the examples is run. Which example to run is determined by the corresponding
    boolean flag. For instance, if self.pause_example flag is set to True, it will
    run the code that will 'pause' the audio stream. Likewise for the other examples.
    These three flags are initialized to False in the __init__ method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The last method we will review is `self.runPauseExample`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The streaming audio is paused by the call on line 4\. The time.sleep call will
    keep the audio paused for 5 seconds and then the audio playback is resumed by
    the call on line 7.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make sure to set the flag `self.pause_example` to True in the `__init__` method
    and specify the proper audio file path for the variable for `self.inFileLocation`.
    Then run this example from the command prompt as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The audio will be played for the first 5 seconds. It will be then paused for
    another 5 seconds and finally the playback will be resumed.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What just happened?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the help of a simple example, we learned how to pause a streaming audio.
    We also saw how the current position of the streaming audio is queried. This knowledge
    will be used in a project later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Stop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Setting the state of the GStreamer pipeline to `gst.STATE_NULL` stops the audio
    streaming. Recall the `message_handler` method explained in the Playing Audio
    section of the previous chapter. We made use of this state when the end of stream
    message was put on the `bus`. In the file `PlaybackControlExamples.py`, the following
    code stops the streaming of the audio.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this file, set the flag `self.stop_example` to `True` and then run the program
    from the command line to see this illustration.
  prefs: []
  type: TYPE_NORMAL
- en: Fast-forward/rewind
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fast-forwarding or rewinding a track simply means that the current position
    on the audio track being played is shifted to some other position. This is also
    called seeking a position on a track. The `pipeline` element of GStreamer defines
    an API method, `seek_simple`, that facilitates jumping to a specified position
    on the track in a streaming audio. In the file `PlabackControlExamples.py`, this
    is illustrated by the following method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: When this method is called, the current audio position is shifted to a position
    corresponding to 15 seconds duration on the audio track. The highlighted lines
    of code are the key. The `seek_simple` method takes three arguments. The first
    argument, `gst.FORMAT_TIME`, represents the time on the track. The second argument,gst.SEEK_GLAG_FLUSH,
    is a 'seek flag'. It tells the pipeline to clear the currently playing audio data.
    In other words it instructs to flush the pipeline. This makes the seek operation
    faster according to the documentation. There are several other seek flags. Refer
    to the GStreamer documentation to know more about these flags. The third argument
    specifies the time on the track that will be the new 'current position' of the
    streaming audio. This time I specified in nanoseconds and so, it is multiplied
    by a constant `gst.SECOND`. Note that pipeline should be in playing state, before
    calling `seek_simple` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Project: extract audio using playback controls'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the last chapter, we learned how to use `gnonlin` plugin to extract a piece
    of audio. `Gnonlin` made our job very easy. In this project, we will see another
    way of extracting the audio files, by applying basic audio processing techniques
    using GStreamer. We will use some of the audio playback controls just learned.
    This project will serve as a refresher on various fundamental components of GStreamer
    API.
  prefs: []
  type: TYPE_NORMAL
- en: Time for action - MP3 cutter from basic principles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's create an MP3 cutter from 'basic principles'. That is we won't be using
    `gnonlin` to do this. In this project, we will apply knowledge about seeking a
    track playing, pausing the pipeline along with the basic audio processing operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'This utility can be run from the command line as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Where, the `[options]` are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--input_file:` The input audio file in MP3 format from which a piece of audio
    needs to be cut.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--output_file:` The output file path where the extracted audio will be saved.
    This needs to be in MP3 format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--start_time:` The position in seconds on the original track. This will be
    the starting position of the audio to be extracted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--end_time:` The position in seconds on the original track. This will be the
    end position of the extracted audio.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--verbose_mode:` Prints useful information such as current position of the
    track (in seconds) while extracting the audio. By default, this flag is set to
    `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download the file `AudioCutter_Method2.py` from the Packt website. We will discuss
    only the most important methods here. You can refer to the source code in this
    file for developing the rest of the code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will start as usual, by defining a class with empty methods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, the overall structure and the method names are very much consistent
    with the MP3 cutter example in earlier chapters. Instead of method `gnonlin_pad_added`
    we have `decodebin_pad_added` which indicates we are going to capture the `pad_added`
    signal for the `decodebin`. Also, there are new methods `run` and `extractAudio`.
    We will discuss these in detail.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now let's review the constructor of the class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `__init__` method calls methods to process user input and then constructs
    the GStreamer pipeline by calling the `constructPipeline()` method. This is similar
    to what we have seen in several examples earlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Think about this. To extract an audio, what elements do you need? We need all
    the elements used in audio conversion utility developed in last chapter. Note
    that in this example we are saving the output in the same audio format as the
    input. Let's try to construct an initial pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We are already familiar with most of the elements included in this pipeline.
    The pipeline looks identical to the one in audio conversion utility except for
    the sink element. Notice that the `filesink` element is created on line 18\. But
    it is not added to the pipeline! Instead we have added a `fakesink` element. Can
    you guess why? This is an extraction utility. We just need to save a portion of
    an input audio file. The start position of the extracted portion may not be the
    start position of the original track. Thus, at this time, we will not add the
    `filesink` to the pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next write the `AudioCutter.run` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: On line 4, we apply one of the playback control commands to instruct the pipeline
    to 'begin'. The state of the input audio is set to `STATE_PLAYING`. As seen earlier,
    the flag `self.is_playing` is changed in the `message_handler` method. In the
    `while` loop, the workhorse method `self.extractAudio()` is called. The rest of
    the code is self-explanatory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we will review the method that does the job of cutting the piece of input
    audio. Let us first see the important things considered in `extractAudio()` method.
    Then it will be very easy to understand the code. This following illustration
    lists these important things.![Time for action - MP3 cutter from basic principles](img/0165_06_01.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important steps considered in AudioCutter.extractAudio() method appear in the
    preceding image.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To extract a piece of audio from the input, the flow of data through the pipeline
    needs to be 'started'. Then, we need to jump to a position in the input audio
    that corresponds to the start position of the audio file to be extracted. Once
    the start position is identified, the GStreamer pipeline needs to be tweaked so
    that there is a `filesink` element. The `filesink` will specify the output audio
    file. After setting the pipeline, we need to begin the flow of data. When the
    user-specified end position is reached, the program execution should stop. Now
    let's write the code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The code block between lines 3 to 25 is executed only once, when the program
    enters this method for the first time. The flag `self.seek_done` ensures it is
    executed only once. This is an important piece of code that does the steps 2 to
    5 represented by rectangular blocks in the above illustration. Let's review this
    code in detail now.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On line 3, we ask the program to wait for 0.1 seconds by `time.sleep` call.
    This is necessary for the next line of code that queries the total duration of
    the playback. The API method query duration returns the total duration of the
    playback. The argument `gst.FORMAT_TIME` ensures that the return value is in time
    format (nanoseconds). To get it in seconds, we divide it by `gst.SECOND`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, on lines 15-17, we jump to the position on the input audio track pertaining
    to the user-supplied argument `self.start_time`. Note that the time argument in
    the method `seek_simple` needs to be in nanoseconds. So it is multiplied by `gst.SECOND`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On line 19, the `gst.STATE_PAUSED` call pauses the flow of data in the pipeline.
    The `fakesink` element is removed from the pipeline with `self.pipline.remove`
    call. This also unlinks it from the pipeline. Then the `self.filesink` element
    is added and linked in the pipeline on lines 23 and 24\. With this, we are all
    set to start playing the audio file again. Here onwards, the audio data will be
    saved to the audio file indicated by the `filesink` element.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On line 27, the current position being played is queried. Note that this is
    done in a try-except block to avoid any possible error while querying the position
    when the audio is very near to the end of the file. When `self.position` reaches
    the specified `self.end_time`, the data flow through the pipeline is stopped by
    the `gst.STATE_NULL` call.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write other methods such as `decodebin_pad_added, connectSignals`. The source
    code can be found in the file `AudioCutter_Method2.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are now all set to run the program. Run it from the command line by specifying
    the appropriate arguments mentioned at the beginning of this section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What just happened?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By applying fundamental audio processing techniques, we developed an MP3 cutter
    utility. This is just another way of extracting audio. We accomplished this task
    by making use of various playback controls learned in earlier sections.
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting volume
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most common audio operations we perform is to adjust the volume level
    of a playing audio. Suppose you have a collection of your favourite songs on your
    computer. You have been adding songs to this collection from various sources over
    the years and have created a 'playlist' so that you can listen to them one after
    the other. But some of the songs start much louder than the others. Of course
    you can adjust the volume every time such songs start playing but that's not what
    you would like to do is it?? You want to fix this, but how? Let's learn how!
  prefs: []
  type: TYPE_NORMAL
- en: The `volume` element in GStreamer can be used to control the volume of the streaming
    audio. It is classified as a type of audio filter. Run `gst-inspect-0.10` command
    on `volume` to know more details about its properties.
  prefs: []
  type: TYPE_NORMAL
- en: How will you adjust volume using the command-line version of GStreamer? Here
    is the command on Windows XP that accomplishes this. You should use forward slashes
    as the backward slashes are not parsed properly by the 'location' property.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This pipeline is very similar to the audio playing example. All we did was to
    add a `volume` element after `audioconvert`.
  prefs: []
  type: TYPE_NORMAL
- en: Time for action - adjusting volume
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's develop a Python example of modifying volume of an audio file. We
    will write a utility that can take an input audio file and write the output file
    with increased or decreased level of the default volume. The utility will support
    writing audio files with MP3 format. If you need some other formats, you can extend
    this application. Refer to the Audio Converter project we did in the previous
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Download the file `AudioEffects.py` from Packt website. It has the source code
    for this example as well as for the *Fading effect*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the constructor of the class `AudioEffects`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The flag `self.fade_example` should be set to `False` in this example. You can
    ignore it for now. It will be used in the *Fading effects* section. Specify appropriate
    input and output audio file paths on lines 6 and 8.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will review the `self.constructPipeline()` method next.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Various GStreamer elements are created the usual way. On line 17, the volume
    element is created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `volume` element has a "volume" property. This determines the level of volume
    in the streaming audio. By default, this has a value of 1.0 which indicates 100%
    of the current default volume of the audio. A value of 0.0 indicates no volume.
    A value greater than 1.0 will make the audio louder than the original level. Let's
    set this level as 2.0, which means the resultant volume will be louder than the
    original. The rest of the code in this method adds and links elements in the GStreamer
    pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the rest of the code from the file mentioned earlier. It is self- explanatory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the program on the command prompt as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Play the resultant audio and compare its default sound level with the original
    audio.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: What just happened?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With a very simple illustration, we learned how to change the default sound
    level of an audio file. What if you want to have varying sound levels at certain
    points in the audio? We will discuss that very soon, in the *Fading effects* section.
  prefs: []
  type: TYPE_NORMAL
- en: Audio effects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One adds spices for improved taste to food, similarly, to enhance the music
    or any sound we add audio effects. There is a wide range of audio effect plugins
    available in GStreamer. We will discuss some of the commonly used audio effects
    in the coming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Fading effects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fading is a gradual increase or decrease in the volume level of an audio. Fading-out
    means gradually decreasing the volume of the audio file as it approaches the end.
    Typically, at the end, the volume level is set as 0\. On similar lines, fade-in
    effect gradually increases the volume level from the beginning of an audio. In
    this chapter, we will learn how to add fade-out effect to an audio. Once we learn
    that, it is trivial to implement fade-in effects.
  prefs: []
  type: TYPE_NORMAL
- en: Time for action - fading effects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's add fade-out effect to an input audio. We will use the same source file
    as used in the *Adjusting volume* section.
  prefs: []
  type: TYPE_NORMAL
- en: If you haven't already, download the file `AudioEffects.py` that has the source
    code for this example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `__init__` method of this class, you will need to do one small change.
    Set the flag `self.fade_example` to `True` so that it now runs the code that adds
    fade-out effect.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We already reviewed the `self.constructPipeline()` method in *Adjusting volume*
    section. It calls the method `self.setupVolumeControl()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The GStreamer `Controller` object is created on line 2\. It is a light-weight
    object that provides a way to control various properties of GStreamer objects.
    In this case, it will be used to adjust the 'volume' property of `self.volume`.
    The set method of the `Controller` takes three arguments, namely, the property
    that needs to be controlled ("volume"), the time on the audio track at which it
    needs to be changed, and the new value of that property (self.volumeLevel). Here,
    the volume level at the beginning of the audio is set `self.volumeLevel`. Next,
    the interpolation mode is set for the `volume` property being adjusted by the
    `Controller` object. Here, we ask the `self.volumeControl` to linearly change
    the volume from its earlier value to the new value as the audio track progresses.
    For example, if the sound level at the beginning is set as 1.0 and at 30 seconds
    it is set as 0.5, the volume levels between 0 to 30 seconds on the track will
    be linearly interpolated. In this case it will linearly decrease from level 1.0
    at 0 seconds to level 0.5 at 30 seconds.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: The GStreamer documentation suggests that `Controller.set_interpolation_mode`
    is deprecated (but is still backward compatible in the version 0.10.5 which is
    used in this book). See a 'TODO' comment in file `AudioEffects.py`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In order to add a fade-out effect towards the end, first we need to get the
    total duration of the audio being played. We can query the duration only after
    the audio has been set for playing (example, when it is in `gst.STATE_PLAYING`
    mode). This is done in `self.play()` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once the pipeline's state is set to `gst.STATE_PLAYING`, the `self.addFadingEffects()`
    method will be called as shown by the highlighted line of code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will review this method now.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: First we ensure that duration of the audio being played can be computed without
    any errors. This is done by the code block 2-24\. Next, the `fade_start` time
    is defined. At this control point the fade-out effect will begin. The fade-out
    will start 4 seconds before the end of the audio. The volume will linearly decrease
    from `fade_start` time to `fade_end` time. The fade_volume is the reference volume
    level when the fade-out begins. On lines 30 and 34 we actually set these fade
    timing and volume parameters for `self.volumeController` , the `Controller` object
    that adjusts the volume. The gradual decrease in the volume level is achieved
    by the `gst.INTERPOLATE_LINEAR`, discussed in an earlier step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Develop or review the remaining code using the reference file `AudioEffects.py`.
    Make sure to specify appropriate input and output audio paths for variables `self.inFileLocation`
    and `self.outFileLocation` respectively. Then run the program from the command
    line as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This should create the output audio file, with a fade-out effect that begins
    4 seconds before the end of the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: What just happened?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We learned how to add a fading effect to an audio file using GStreamer multimedia
    framework. We used the same GStreamer pipeline as the one used in the *Adjusting
    volume* section, but this time, the volume level was controlled using the `Controller`
    object in GStreamer. The technique we just learned will come handy while working
    on project 'Combining Audio Clips ' later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Have a go hero add fade-in effect
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is going to be straightforward. We added a fade-out effect earlier. Now
    extend this utility by adding a fade-in effect to the input audio. Use a total
    fade duration of 4 seconds. The `fade_start` time in this case will be 0 seconds.
    Try the interpolation mode as `gst.INTERPOLATE_CUBIC`.
  prefs: []
  type: TYPE_NORMAL
- en: Echo echo echo...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Echo is a reflection of a sound heard a short time period after the original
    sound. In audio processing, to achieve this effect the input audio signal is recorded
    and then played back after the specified ''delay time'' with a specified intensity.
    An echo effect can be added using the `audioecho` plugin in GStreamer. The audio
    echo plugin should be available by default in your GStreamer installation. Check
    this by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: If it is not available, you will need to install it separately. Refer to the
    GStreamer website for installation instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Time for action - adding echo effect
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's write code to add an echo effect to an input audio. The code is very similar
    to the one in the `AudioEffects.py` file discussed in earlier section. Just to
    simplify the matter, we will use the code in file `EchoEffect.py` file for easier
    understanding. Later, you can easily integrate this with the code in `AudioEffects.py`.
  prefs: []
  type: TYPE_NORMAL
- en: Download the file `EchoEffect.py` that has the source code to add audio echo
    effect. The file contains class `AudioEffects` whose constructor has the following
    code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It is similar to the __init__ method discussed in the Fading Effects section.
    One difference here is the flag self.use_echo_controller. If it is set to True,
    the GStreamer Controller object will be used to adjust certain echo properties
    while the audio is being streamed. We will first see how a simple echo effect
    can be implemented and then discuss the echo control details. Specify the appropriate
    values for audio file path variables self.inFileLocation and self.outFileLocation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's build the GStreamer pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The audioecho element is created on line 21\. The property delay specifies the
    duration after which the echo sound will be played. We specify it as 1 second,
    and you can increase or decrease this value further. The echo feedback value is
    set as 0.3\. On line 28, the intensity property is set to 0.5\. It can be set
    in a range 0.0 to 1.0 and determines the sound intensity of the echo. Thus, if
    you set it to 0.0, the echo won't be heard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Notice that there are two `audioconvert` elements. The first `audioconvert`
    converts the decoded audio stream into a playable format input to the `self.echo`
    element. Similarly on the other end of the echo element, we need `audioconvert`
    element to process the audio format after the echo effect has been added. This
    audio is then encoded in MP3 format and saved to the location specified by `self.filesink`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the program from the command line as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you play the output file, the echo sound will be audible throughout the playback
    duration.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we will add a feature that will allow us to add echo effect only for a certain
    duration of the audio track. In the `__init__` method, set the flag `self.use_echo_controller`
    to `True`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will now review the method `self.setupEchoControl()` which is called in `self.constructPipeline()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Setting up `gst.Controller` object is very similar to the one developed in the
    *Fading effects* section. Here, we ask the `Controller` object, `self.echoControl`,
    to control the property 'intensity' of the `audioecho` element, `self.echo`. At
    the beginning of the playback (0 seconds), we set the echo intensity as `0.5`.
    We add another control point at 4 seconds during the playback and set the `intensity`
    level as `0.0`. What this effectively means is that we don't want to hear any
    echo after the first 4 seconds of the audio playback! .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the program again from the command line as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the only change done here is the value of flag self.use_echo_controller
    is set to True. Play the output file; the echo sound will be audible only for
    the first 4 seconds during the playback.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What just happened?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We learned how to add echo to an audio clip. To accomplish this, the `audioecho`
    element was added and linked in the GStreamer pipeline. We also learned how to
    selectively add echo effect to the audio using GStreamer `Controller` object.
  prefs: []
  type: TYPE_NORMAL
- en: Have a go hero add Reverberation Effect
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose you are in a theater. When an actor at the center stage talks, the sound
    waves are reflected from the surfaces of the theater before reaching your ears.
    Thus what you hear is a bunch of these reflected sounds. This is known as reverberation
    effect. According to the `audioecho` plugin documentation, if you set the `delay`
    property to a value of less than 0.2 seconds in `audioecho` element, it produces
    a reverberation effect. Try setting different values for `delay`, less than 0.2
    seconds and see how it affects the output audio. Note, this argument is taken
    as an integer. Therefore, specify this value in nanoseconds. For example specify
    0.05 seconds as `50000000` instead of `0.05*gst.SECOND`. This is illustrated below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Panning/panorama
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The stereo panorama effect can be added to a sound by using `audiopanorama`
    plugin (part of `audiofx` plugin). This plugin should be available by default
    in your GStreamer installation. Use `gst-inspect-0.10` to verify it is there and
    also to know more about its properties. Download the file `PanoramaEffect.py`
    from the Packt website. This file is more or less identical to `AudioEffects.py`
    or `EchoEffect.py`. The following is a code snippet from the `self.contructPipeline`
    method in file `PanoramaEffect.py`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We have discussed the following many times. Let's go over the code once again
    as a refresher… just in case you missed it earlier. The code block 6-11 adds all
    the elements to the GStreamer pipeline. Notice that we call `gst.element_link_many`
    twice. Do you recall why? The first call on line 14 makes a connection between
    `self.filesrc` and `self.decodebin`. There is one important point to note when
    we make a second call to `gst.element_link_many`. Notice that we have not linked
    `self.decodebin` with `self.audioconvert`. This is because `self.decodebin` implements
    dynamic pads. So we connect it at the runtime, using the callback method, `decodebin_pad_added`.
  prefs: []
  type: TYPE_NORMAL
- en: You can review the rest of the code from this file. The `audiopanorama` element
    is created on line 2 in the code snippet. The `panorama` property can have a value
    in the range `-1.0` to `1.0`. If you have stereo speakers connects, the sound
    will entirely come from the left speaker if a value of `-1.0` is specified. Likewise,
    a value of `1.0` will make the sound come from right speaker only. In the above
    code snippet, we instruct the program to exclusively use the right speaker for
    audio streaming. The audio will be streamed from both speakers if the value is
    in-between these two limits. Each speaker's contribution will be determined by
    actual value.
  prefs: []
  type: TYPE_NORMAL
- en: Have a go hero control panorama effect and more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '''Move'' the sound around! Add a GStreamer `Controller` object to adjust the
    `panorama` property of the `self.panorama` element. This is similar to what we
    did in `EchoEffect.py`. Add some control points in the audio stream as done earlier,
    and specify different values for the `panorama` property.'
  prefs: []
  type: TYPE_NORMAL
- en: Integrate this feature with the code in `AudioEffects.py` discussed earlier
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Project: combining audio clips'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is time for a project! In this project, we will create a single audio file,
    which has custom audio clips appended one after the other. Here, we will apply
    several of the things learned in earlier section, and also in the previous chapter
    on audio processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a new audio file, which is a combination of several audio tracks of
    your choice involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First thing we need are the audio files that need to be included. Depending
    upon our requirement, we may need only a small portion of an audio track. So we
    will develop a general application considering this possibility. This is illustrated
    in the time-line illustrated earlier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we need to make sure that these audio pieces are played in a specified
    order.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There should be a 'blank' or a 'silent' audio in-between the two audio pieces.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will also implement audio fade-out effect for each of the pieces in
    the track. This will ensure that the audio doesn't end abruptly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Media 'timeline' explained
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we begin this project, it is important to understand the concept of a
    timeline. A timeline can be viewed as the overall representation of a path where
    you can control the time for which an individual audio clip is played.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this project, since we are saving the resultant audio, it is the same as
    the total playback time of the resultant audio. In this timeline, we can specify
    ''when'' an audio needs to be played and how long it needs to be played. This
    is better explained with the illustration below. Consider a timeline with a total
    duration of 250 seconds. This is represented by the central thick line with circles
    at the end. Suppose there are three audio clips, namely, `Media #1, Media #2`
    and `Media #3` as indicated in the illustration. We wish to include a portion
    of each of these audio clips in the main timeline (the audio file to be saved).
    In the main media timeline, the audio between 0 seconds to 80 second represents
    a portion from `Media #1`. It corresponds to the audio between 30 seconds to 110
    seconds in `Media #1`. Likewise, audio between 90 to 200 seconds on main media
    timeline represents a chunk from `Media #2` and so on. Thus, we can tweak the
    priority and position of the individual audio clips on the main media timeline
    to create the desired audio output.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Media ''timeline'' explained](img/0165_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Main media timeline is represented with multiple media tracks in the preceding
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Time for action - creating custom audio by combining clips
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's develop an application where we will combine multiple audio clips into
    a single audio file.
  prefs: []
  type: TYPE_NORMAL
- en: Download the file `CombiningAudio.py`. This file contains all the code necessary
    to run this application. As done earlier, we will discuss only the most important
    methods in this class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the following code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The overall structure of the code is identical to several other examples in
    this book. We will expand some of the class methods such as addFadingEffect, setupFadeBin
    in the next steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's review the `constructPipeline` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We used functionality such as gnlcomposition, gnlcontroller, and so on while
    implementing audio fading effects in an earlier section. These modules will be
    used in this project as well. On line 7, all the audio clips we wish to include
    are added to the timeline or gnlcomposition. We will review this method later.
    Note that the gnlcomposition uses dynamic pads. The pad-added signal is connected
    in self.connectSignals. On line 17, a fading effect is set up for the audio clips.
    This ensures smooth termination of individual audio clips in the timeline. Finally,
    the code block between lines 19 to 26 constructs the pipeline and links various
    GStreamer elements in the pipeline. Let's review other important methods in this
    class one by one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The method `self.addGnlFileSources` does multiple things. It adds the audio
    clips to the main timeline in a desired order. This method also ensures that there
    is some 'breathing space' or a blank audio of a short duration in between any
    two audio clips. Write the following method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: First we declare various parameters needed to put the audio clips in the main
    timeline. Here, the audio clips are mostly the gnlfilesource elements whereas
    the timeline is the total length of the output audio track. This parameter setting
    is done by the code between lines 3 to 13\. In this example, we are combining
    only two audio clips. Replace the audio file paths on lines 7 and 13 with the
    appropriate file paths on your machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Important note for Windows users: Make sure to specify the file path with forward
    slashes ''/'' as shown on line 13 of the code snippet. If the path is specified
    as, for instance, `C:\AudioFiles\audio2.mp3`, the ''\a'' is treated differently
    by GStreamer! A workaround could be to normalize the path or to always use forward
    slashes while specifying the path. In this case `C:/AudioFiles/audio2.mp3`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The first media file will be placed for 20 seconds on the main timeline. The
    total duration of the audio is specified by the parameter media_duration_1\. The
    parameter media_start_1 specifies the actual time of the first audio file which
    will be the start_time_1 on the main timeline. The basic concept behind timeline
    is explained earlier in this section. Try tweaking a few parameters to get a good
    grasp of how the timeline works. For the second audio, notice how the start_time_2
    is specified. It is equal to duration_1 + 3\. A time of 3 seconds is added so
    that there is a 'sound of silence' between two tracks. You can change this to
    a silent duration of your choice.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, the parameters necessary for the blank audio are defined. In general,
    the `gnlcomposition` will 'play' the blank audio when nothing else is being played
    (this is with the assumption that a proper `priority` is set). We define the total
    duration of this silent track sufficiently long enough, longer than the combined
    duration of all the audio clips, so that this track is 'available to play' when
    the time comes. Note that `gnlcomposition` won't play the silent track for its
    complete duration! It is just so that we have a long enough track that can be
    played at various points. In this project, we are only using two audio files.
    So, it is not really necessary to set blank duration parameter as greater than
    or equal to the total timeline duration. It is okay if we just have it for 3 seconds.
    But imagine that you have more than 2 audio clips. The silent audio will be played
    between tracks 1 and 2 but then it won't be available for tracks between 2 and
    3! If we were to have 3 audio tracks, then the blank audio duration can be set
    as illustrated in the following code snippet and by adding another `gnlfilesource`
    to the `self.composition`. You can also test the resultant audio file by specifying
    `blank_duration = 3`. In that case, there won't be a silent track between audio
    clips 2 and 3!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The code between lines 19 to 24 sets up some instance variables needed to add
    fade-out effect to the individual audio clips in the `gnlcomposition`. These will
    be used in the `self.addFadingEffect` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code blocks 26-36 and 50-61 define the `gnlfilesource` elements to be added
    to the `self.composition` along with their properties. We have already learned
    about `gnlfilesource`, so these code blocks should be self-explanatory. However,
    see the code on lines 36 ad 61? Here we set the priority of the audio clips in
    the main timeline. It is important step. If you don't define the priority, by
    default, each `gnlsource` will have highest priority indicated by value '0'. This
    is a little bit tricky. It is best explained by tweaking certain values and actually
    playing the output audio! Let's keep it simple for now. See the next 'Have a go
    Hero' section that asks you to experiment a few things related to the `priority`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's review the code block 40-44\. Here, a `gnlsource` (and not `gnlfilesource)`
    is created on line 40\. We call it `gnlBlankAudio`. Line 41 is very important.
    It tells the program to consider this element last. That is, `gnlBlankAudio` is
    set with the least possible priority among the elements added to the `gnlcomposition`.
    This ensures that the blank piece of audio is played only between the tracks and
    not as an audio clip of its own. Whenever the start point of the next audio in
    the `gnlcomposition` approaches, it will push the `gnlBlankAudio` to a backseat
    and start playing this new audio clip instead. This is because the other audio
    clips are set at a higher `priority` than the `gnlBlankAudio`. You might be wondering
    what the value `4294967295` for `priority` signifies. If you run `gst-inspect-0.10`
    command on `gnlsource` you will notice that the `priority` has a range from `0`
    to4294967295\. Thus the least possible priority level is `4294967295`. In this
    example, we can get away with the priority level of `3` because we have specified
    the `blank_duration` parameter appropriately. But, suppose you don't know beforehand
    what `blank_duration` should be and you set it to a large number. In this case,
    if you have set the priority of `gnlBlankAudio` as `3`, at the end of the output
    audio it will play the remaining portion of the `gnlBlankAudio`. Thus, the total
    track duration will be unnecessarily increased. Instead, if you use priority as
    `4294967295`, it won't play the surplus portion of the blank audio. If you have
    multiple of audio tracks and if their number is not known to begin with, the least
    priority level we are using is the safest value for the blank audio clip. As mentioned
    earlier, the following priority for `gnlBlankAudio` should work as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: On line 46, an `audiotestsrc` element is created. This plugin should be available
    in your installation of GStreamer. This plugin can be used to generate several
    elementary audio signals such as a sine waveform, a silent wave form, and so on.
    Run `gst-inspect-0.10` on `audiotestsrc` to see what types of audio signals it
    can generate. The type of audio signal we need can be specified by the 'wave'
    property of `audiotestsrc` . The value of 4 for `wave` property corresponds to
    a silence waveform. A value of 3 generates triangle wave forms and so on. On line
    48, the `audiotestsrc` element is added to the `gnlsource` element (gnlBlankAudio).
    This simply means that when we start playing the `gnlcomposition`, the silent
    audio pertaining `gnlsource` element is generated using `audiotestsrc` element
    within it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the code between lines 63-65 adds the `gnlfilesource` and `gnlsource`
    elements to the `self.composition`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we will quickly review the method `self.addFadingEffect()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In *Fading effects* section, we added fade-out effect to an audio file. In that
    section individual elements such as audio convert and volume were added and linked
    in the main pipeline. Here, we will follow a different way, so as to learn a few
    more things in GStreamer. We will create a GStreamer `bin` element to add the
    fade-out effect to the audio clips. You can choose to do it the old way, but creating
    a `bin` provides a certain level of abstraction. The `bin` element is created
    by the highlighted line of code. We will review that method next. The rest of
    the code in this method is very similar to the one developed earlier. The `self.volumeControl`
    is a GStreamer `Controller` element. We specify volume at appropriate time intervals
    in the timeline to implement fade-out effect for the individual audio clips. It
    is important to adjust the level of volume back to the original one after each
    `fade_end` time. This ensures that the next clip starts with an appropriate level
    of volume. This is achieved by code between lines 22-24.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now let's see how to construct a GStreamer bin element for the fading effect.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: On lines 2-6, we define the elements necessary to change volume of an audio
    in a GStreamer pipeline. This is nothing new. On line 8, we create `self.fadeBin`,
    a GStreamer bin element. A `bin` is a container that manages the element objects
    added to it. The essential elements are added to this bin on line 10\. The elements
    are then linked the same way we link elements in a GStreamer pipeline. The bin
    itself is pretty much set up. But there is one more important thing. We need to
    ensure that this bin can be linked with other elements in a GStreamer pipeline.
    For that we need to create ghost pads.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recall what a `ghost pad` is from the last chapter. A `bin` element is an 'abstract
    element'. It doesn't have `pads` of its own. But in order to work like an element,
    it needs `pads` to connect to the other elements within the pipeline. So the `bin`
    uses a `pad` of an element within it as if it was its own `pad.` This is called
    a ghost pad. Thus the `ghost pads` are used to connect an appropriate element
    inside a `bin`. It enables using a `bin` object as an abstract element in a GStreamer
    pipeline. We create two `ghost pads`. One as `src pad` and one as `sink pad`.
    It is done by the code on lines 19-22\. Note that we use `sink pad` of `self.audioconvert`
    as the `sink ghost pad` of the bin and `src pad` of `self.audioconvert2` as `src
    ghost pad`. Which pad to use as src or sink is decided by how we link elements
    within the bin. Looking at the code between lines 14 to 17 will make it clear.
    Finally, the `ghost pads` are added to the `self.fadeBin` on lines 24 and 25.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The method `self.gnonlin_pad_added()` gets called whenever the `pad-added` signal
    is emitted for `self.composition`. Notice that `compatible_pad` in this method
    is obtained from `self.fadeBin`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Develop the rest of the methods by reviewing the code in file `CombiningAudio.py`.
    Be sure to specify appropriate input and output audio file locations. Once all
    the pieces are in place, run the program as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should create the output audio file containing audio clips combined together!
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What just happened?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this project we developed a cool application that can combine two or more
    audio clips into a single audio file. To accomplish this, we used many audio processing
    techniques learned in earlier sections and the previous chapter on audio processing.
    We made use of various elements from `gnonlin` plugin such as `gnlcomposition,
    gnlfilesource`, and `gnlsource` . We learned how to create and link a GStreamer
    `bin` container to represent the fade-out effect as an abstract element in the
    pipeline. Among other things, we learned how to insert a blank audio in-between
    audio clips.
  prefs: []
  type: TYPE_NORMAL
- en: Have a go hero change various properties of 'gnlfilesource'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the earlier *Time for action* section, we set priority property for the two
    `gnlfilesource` elements added to the `gnlcomposition`. Tweak the `start` and
    the `priority` properties of the two `gnlfilesource` elements to see what happens
    to the output audio. For example, swap the priority of two `gnlfilesource` elements
    and change the `start_time_2` to `duration_1`, and see what happens. Notice how
    it affects the playback of the first audio clip!
  prefs: []
  type: TYPE_NORMAL
- en: Audio mixing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine that you have some instrumental music files in your collection. You
    have a hidden desire to become a playback singer and you wish to sing these songs
    with the background music. What will you do? Well, the simplest thing to do is
    to put on headphones and play any instrumental music. Then sing along and record
    your vocal. OK, what's next? You need to mix the instrumental music and your own
    vocal together to get what you want!
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how to mix two audio tracks together. The `interleave` is a GStreamer
    plugin that facilitates mixing of two audio tracks. It merges multiple mono channel
    input audios into a single audio stream in a non-contiguous fashion. This plugin
    should be available in your default GStreamer installation.
  prefs: []
  type: TYPE_NORMAL
- en: Time for action - mixing audio tracks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's write a utility that can mix two audio streams together.
  prefs: []
  type: TYPE_NORMAL
- en: Download the file `AudioMixer.py` which contains the source code for this utility.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we will review the `constructPipeline` method. The API method `gst.parse_launch()`
    explained in the previous chapter will be used here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `audio1_str` and `audio2_str` are the portions of the main pipeline strings.
    Each of these contain `filesrc` , `decodebin`, and `audioconvert` elements. The
    `filesrc` provides the location of respective input audio files. By now, we very
    well know what this portion of a GStreamer pipeline does.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On lines 10-12, the `interleave_str` defines another portion of the main pipeline
    string. The data output from the `interleave` element needs to be converted into
    a format expected by the encoder element. The encoder is then connected to the
    `filesink` element where the output audio will be stored.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As mentioned earlier, the `interleave` merges multiple audio channels into a
    single audio stream. In this case, the `interleave` element reads in data from
    two different audio streams via queue elements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The sink pad of the queue element is linked with the audioconvert element. The
    queue element is a buffer to which the audio data from the audioconvert is written.
    Then this data is further read by the interleave element. This linkage within
    the GStreamer pipeline can be represented by the following string "audioconvert
    ! queue ! mix.". Note that the dot '.' after 'mix' is important. It is a part
    of the syntax when gst.parse_launch is used.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To summarize, the data streamed from the portions of the pipeline, `audio1_str`
    and `audio2_str`, will be ultimately read by the `interleave` via 'queue' elements
    and then it will follow the rest of the pipeline represented by `interleave_str`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On line 20, the pipeline string is fed to gst.parse_launch to create a GStreamer
    pipeline instance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Review the rest of the code from the source file `AudioMixer.py`. Change the
    input and output audio file path strings represented by `self.inFileLocation_1,
    self.inFileLocation_2`, and `self.outFileLocation`. Then run the code as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should create the interleaved audio output. If you play this audio file,
    you will hear both the audio clips playing at once. Try selecting only a single
    audio channel, such as "Left" channel or "Right" channel. In this case, you will
    notice that each of these audio clips is sent stored on a separate channel. For
    example, if you play only the left channel, only one of these audio clips will
    be heard, so would be the case for the other channel.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What just happened?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using `interleave` element, we merged two audio tracks to create an interleaved
    audio. This can be used as an audio mixing utility. We learned how to use `queue`
    element as an audio data buffer which is then read by the interleave element.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing an audio track
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the popular audio players provide a feature to 'visualize' the audio
    being played. This visualization effect is typically generated on the fly and
    is synchronized with the audio signal. Typically, the visualizer responds to changes
    in audio frequency and volume level among other properties. These changes are
    then shown by use of animated graphics. GStreamer provides certain plugins to
    visualize a track. The 'monoscope' visualization plugin is generally available
    in the default GStreamer installation. It displays a highly stabilized waveform
    of the streaming audio. Make sure that the GStreamer installation has the `monoscope`
    plugin by running the `gst-inspect-0.10` command. There are several other popular
    plugins such as `goom` and `libvisual`. But these are not available by default
    in the GStreamer binary installed on Windows XP. You can install these plugins
    and try using these to add visualization effects.
  prefs: []
  type: TYPE_NORMAL
- en: Time for action - audio visualizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The visualization effect can be added to the streaming audio using different
    techniques. We will use the simplest approach of all to develop a Music Visualizer
    utility.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will be using the `playbin` plugin of GStreamer. Recall that the `playbin`
    was first used in the *Playing an audio from a Website* section of the Working
    with Audios chapter. This plugin provides a higher level audio /video player and
    it should be available in the default GStreamer installation.
  prefs: []
  type: TYPE_NORMAL
- en: Download the file `MusicVisualizer.py` from the Packt website. This is a small
    program. The class methods are represented below. Look at the code from this file
    for more details.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Most of the code is identical to the one illustrated in the Playing audio from
    a website section of the previous chapter. The only difference here is the constructor
    of the class where various properties of the playbin element are defined.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now let's review the constructor of the class AudioPlayer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Modify the `inFileLocation` on line 3 to match an audio file path on your computer.
    On line 6 and 8, the `playbin` and `monoscope` elements are created. The latter
    is a plugin that enables audio visualization. On line 12, we set the value for
    property `vis-plugin` as the `monoscope` element created earlier. The `vis-plugin`
    stands for 'visualization plugin' that the `playbin` element should use to visualize
    the music.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'That''s all! You can review the rest of the code from the file `MusicVisualizer.py`.
    Now run the program from the command line as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should start playing the input audio file and at the same time, it should
    also pop up a small window where you can 'visualize' the streaming audio.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Note: The overall performance of this application may depend on the number
    of processes running at the time this program is run. It may also depend on the
    specifications of your computer such as processor speed.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, the stable audio waveform will be shown as the music plays. The following
    shows a snapshot of this visualization window at two different timeframes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Time for action - audio visualizer](img/0165_6_3.jpg)![Time for action -
    audio visualizer](img/0165_6_4.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Snapshots at some random timeframes using Music Visualizer using 'monoscope'
    are depicted here.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What just happened?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We used the GStreamer plugins `playbin` and `monoscope` to develop an audio
    visualization utility for a streaming audio. The `monoscope` element provided
    a way to visualize highly stable audio waveforms.
  prefs: []
  type: TYPE_NORMAL
- en: Have a go hero use other visualization plugins
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To illustrate visualization effects for an audio, the `monoscope` plugin was
    used. If you have some other visualization plugins available in the GStreamer
    installation, use those to create different visualization effects. The following
    are some of the plugins that can be used for this purpose: `goom, goom2k1, libvisual`,
    and `synaesthesia`. The audio visualization accomplished by `synaesthesia` plugin
    is shown in the next illustration.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Have a go hero use other visualization plugins](img/0165_6_5.jpg)![Have a
    go hero use other visualization plugins](img/0165_6_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Music Visualizer using ''synaesthesia'': Snapshots at some random timeframes
    is depicted here.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We learned a lot in this chapter about various audio enhancement and control
    techniques. The GStreamer multimedia framework was used to accomplish this. We
    specifically covered:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Audio controls: How to control the streaming of an audio data. With the help
    of coding illustrations, we learned about playback controls such as play, pause,
    seek, and stop. These controls were then used in a project where a portion of
    an audio was extracted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adding effects: Enhancing the audio by adding audio effects such as fade-in,
    echo/reverberation, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Non-linear audio editing: How to combine two or more audio streams into a single
    track. This was done in one of the projects we undertook.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audio mixing technique to merge multiple mono channel audio streams into a single
    interleaved audio.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, we also learned techniques such as visualizing an audio. This
    concludes our discussion on audio processing in Python using GStreamer framework.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to process videos using Python.
  prefs: []
  type: TYPE_NORMAL
