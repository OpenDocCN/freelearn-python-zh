- en: Testing Object-Oriented Programs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Skilled Python programmers agree that testing is one of the most important
    aspects of software development. Even though this chapter is placed near the end
    of the book, it is not an afterthought; everything we have studied so far will
    help us when writing tests. In this chapter, we''ll look at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The importance of unit testing and test-driven development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standard `unittest` module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pytest` automated testing suite
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `mock` module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-platform testing with `tox`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why test?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many programmers already know how important it is to test their code. If you're
    among them, feel free to skim this section. You'll find the next section–where
    we actually see how to create tests in Python–much more scintillating. If you're
    not convinced of the importance of testing, I promise that your code is broken,
    you just don't know it. Read on!
  prefs: []
  type: TYPE_NORMAL
- en: Some people argue that testing is more important in Python code because of its
    dynamic nature; compiled languages such as Java and C++ are occasionally thought
    to be somehow *safer* because they enforce type checking at compile time. However,
    Python tests rarely check types. They check values. They make sure that the right
    attributes have been set at the right time or that the sequence has the right
    length, order, and values. These higher-level concepts need to be tested in any
    language. The real reason Python programmers test more than programmers of other
    languages is that it is so easy to test in Python!
  prefs: []
  type: TYPE_NORMAL
- en: But why test? Do we really need to test? What if we didn't test? To answer those
    questions, write a tic-tac-toe game from scratch without any testing at all. Don't
    run it until it is completely written, start to finish. Tic-tac-toe is fairly
    simple to implement if you make both players human players (no artificial intelligence).
    You don't even have to try to calculate who the winner is. Now run your program.
    And fix all the errors. How many were there? I recorded eight in my tic-tac-toe
    implementation, and I'm not sure I caught them all. Did you?
  prefs: []
  type: TYPE_NORMAL
- en: We need to test our code to make sure it works. Running the program, as we just
    did, and fixing the errors is one crude form of testing. Python's interactive
    interpreter and near-zero compile times makes it easy to write a few lines of
    code and run the program to make sure those lines are doing what is expected.
    But changing a few lines of code can affect parts of the program that we haven't
    realized will be influenced by the changes, and therefore neglect to test those
    parts. Furthermore, as a program grows, the number of paths that the interpreter
    can take through that code also grow, and it quickly becomes impossible to manually
    test all of them.
  prefs: []
  type: TYPE_NORMAL
- en: To handle this, we write automated tests. These are programs that automatically
    run certain inputs through other programs or parts of programs. We can run these
    test programs in seconds and cover far more potential input situations than one
    programmer would think to test every time they change something.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four main reasons to write tests:'
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that code is working the way the developer thinks it should
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To ensure that code continues working when we make changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To ensure that the developer understood the requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To ensure that the code we are writing has a maintainable interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first point really doesn't justify the time it takes to write a test; we
    can test the code directly in the interactive interpreter in the same time or
    less. But when we have to perform the same sequence of test actions multiple times,
    it takes less time to automate those steps once and then run them whenever necessary.
    It is a good idea to run tests every time we change code, whether it is during
    initial development or maintenance releases. When we have a comprehensive set
    of automated tests, we can run them after code changes and know that we didn't
    inadvertently break anything that was tested.
  prefs: []
  type: TYPE_NORMAL
- en: The last two of the preceding points are more interesting. When we write tests
    for code, it helps us design the API, interface, or pattern that code takes. Thus,
    if we misunderstood the requirements, writing a test can help highlight that misunderstanding.
    From the other side, if we're not certain how we want to design a class, we can
    write a test that interacts with that class so we have an idea of the most natural
    way to interface with it. In fact, it is often beneficial to write the tests before
    we write the code we are testing.
  prefs: []
  type: TYPE_NORMAL
- en: Test-driven development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Write tests first* is the mantra of test-driven development. Test-driven development
    takes the *untested code is broken code* concept one step further and suggests
    that only unwritten code should be untested. We don''t write any code until we
    have written the tests that will prove it works. The first time we run a test
    it should fail, since the code hasn''t been written. Then, we write the code that
    ensures the test passes, then write another test for the next segment of code.'
  prefs: []
  type: TYPE_NORMAL
- en: Test-driven development is fun; it allows us to build little puzzles to solve.
    Then, we implement the code to solve those puzzles. Then, we make a more complicated
    puzzle, and we write code that solves the new puzzle without unsolving the previous
    one.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two goals to the test-driven methodology. The first is to ensure
    that tests really get written. It''s so very easy, after we have written code,
    to say:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Hmm, it seems to work. I don''t have to write any tests for this. It was just
    a small change; nothing could have broken."'
  prefs: []
  type: TYPE_NORMAL
- en: If the test is already written before we write the code, we will know exactly
    when it works (because the test will pass), and we'll know in the future if it
    is ever broken by a change we or someone else has made.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, writing tests first forces us to consider exactly how the code will
    be used. It tells us what methods objects need to have and how attributes will
    be accessed. It helps us break up the initial problem into smaller, testable problems,
    and then to recombine the tested solutions into larger, also tested, solutions.
    Writing tests can thus become a part of the design process. Often, when we're
    writing a test for a new object, we discover anomalies in the design that force
    us to consider new aspects of the software.
  prefs: []
  type: TYPE_NORMAL
- en: As a concrete example, imagine writing code that uses an object-relational mapper
    to store object properties in a database. It is common to use an automatically
    assigned database ID in such objects. Our code might use this ID for various purposes.
    If we are writing a test for such code, before we write it, we may realize that
    our design is faulty because objects do not have IDs assigned until they have
    been saved to the database. If we want to manipulate an object without saving
    it in our test, it will highlight this problem before we have written code based
    on the faulty premise.
  prefs: []
  type: TYPE_NORMAL
- en: Testing makes software better. Writing tests before we release the software
    makes it better before the end user sees or purchases the buggy version (I have
    worked for companies that thrive on the *users can test it* philosophy; it's not
    a healthy business model). Writing tests before we write software makes it better
    the first time it is written.
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start our exploration with Python's built-in test library. This library
    provides a common object-oriented interface for **unit tests**. Unit tests focus
    on testing the least amount of code possible in any one test. Each one tests a
    single unit of the total amount of available code.
  prefs: []
  type: TYPE_NORMAL
- en: The Python library for this is called, unsurprisingly, `unittest`. It provides
    several tools for creating and running unit tests, the most important being the
    `TestCase` class. This class provides a set of methods that allow us to compare
    values, set up tests, and clean up when they have finished.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we want to write a set of unit tests for a specific task, we create a
    subclass of `TestCase` and write individual methods to do the actual testing.
    These methods must all start with the name `test`. When this convention is followed,
    the tests automatically run as part of the test process. Normally, the tests set
    some values on an object and then run a method, and use the built-in comparison
    methods to ensure that the right results were calculated. Here''s a very simple
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This code simply subclasses the `TestCase` class and adds a method that calls
    the `TestCase.assertEqual` method. This method will either succeed or raise an
    exception, depending on whether the two parameters are equal. If we run this code,
    the `main` function from `unittest` will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Did you know that floats and integers can be compared as equal? Let''s add
    a failing test, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The output of this code is more sinister, as integers and strings are not
  prefs: []
  type: TYPE_NORMAL
- en: 'considered equal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The dot on the first line indicates that the first test (the one we wrote before)
    passed successfully; the letter `F` after it shows that the second test failed.
    Then, at the end, it gives us some informative output telling us how and where
    the test failed, along with a summary of the number of failures.
  prefs: []
  type: TYPE_NORMAL
- en: We can have as many test methods on one `TestCase` class as we like. As long
    as the method name begins with `test`, the test runner will execute each one as
    a separate, isolated test. Each test should be completely independent of other
    tests. Results or calculations from a previous test should have no impact on the
    current test. The key to writing good unit tests is keeping each test method as
    short as possible, testing a small unit of code with each test case. If our code
    does not seem to naturally break up into such testable units, it's probably a
    sign that the code needs to be redesigned.
  prefs: []
  type: TYPE_NORMAL
- en: Assertion methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The general layout of a test case is to set certain variables to known values,
    run one or more functions, methods, or processes, and then *prove* that correct
    expected results were returned or calculated by using `TestCase` assertion methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few different assertion methods available to confirm that specific
    results have been achieved. We just saw `assertEqual`, which will cause a test
    failure if the two parameters do not pass an equality check. The inverse, `assertNotEqual`,
    will fail if the two parameters do compare as equal. The `assertTrue` and `assertFalse`
    methods each accept a single expression, and fail if the expression does not pass
    an `if` test. These tests do not check for the Boolean values `True` or `False`.
    Rather, they test the same condition as though an `if` statement were used: `False`,
    `None`, `0`, or an empty list, dictionary, string, set, or tuple would pass a
    call to the `assertFalse` method. Nonzero numbers, containers with values in them,
    or the value `True` would succeed when calling the `assertTrue` method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an `assertRaises` method that can be used to ensure that a specific
    function call raises a specific exception or, optionally, it can be used as a
    context manager to wrap inline code. The test passes if the code inside the `with`
    statement raises the proper exception; otherwise, it fails. The following code
    snippet is an example of both versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The context manager allows us to write the code the way we would normally write
    it (by calling functions or executing code directly), rather than having to wrap
    the function call in another function call.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also several other assertion methods, summarized in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Methods** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `assertGreater``assertGreaterEqual``assertLess``assertLessEqual` | Accept
    two comparable objects and ensure the named inequality holds. |'
  prefs: []
  type: TYPE_TB
- en: '| `assertIn``assertNotIn` | Ensure an element is (or is not) an element in
    a container object. |'
  prefs: []
  type: TYPE_TB
- en: '| `assertIsNone``assertIsNotNone` | Ensure an element is (or is not) the exact
    `None` value (but not another falsey value). |'
  prefs: []
  type: TYPE_TB
- en: '| `assertSameElements` | Ensure two container objects have the same elements,
    ignoring the order. |'
  prefs: []
  type: TYPE_TB
- en: '| `assertSequenceEqualassertDictEqual``assertSetEqual``assertListEqual``assertTupleEqual`
    | Ensure two containers have the same elements in the same order. If there''s
    a failure, show a code difference comparing the two lists to see where they differ.
    The last four methods also test the type of the list. |'
  prefs: []
  type: TYPE_TB
- en: Each of the assertion methods accepts an optional argument named `msg.` If supplied,
    it is included in the error message if the assertion fails. This can be useful
    for clarifying what was expected or explaining where a bug may have occurred to
    cause the assertion to fail. I rarely use this syntax, however, preferring to
    use descriptive names for the test method instead.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing boilerplate and cleaning up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After writing a few small tests, we often find that we have to write the same
    setup code for several related tests. For example, the following `list` subclass
    has three methods for statistical calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Clearly, we''re going to want to test situations with each of these three methods
    that have very similar inputs. We''ll want to see what happens with empty lists,
    with lists containing non-numeric values, or with lists containing a normal dataset,
    for example. We can use the `setUp` method on the `TestCase` class to perform
    initialization for each test. This method accepts no arguments, and allows us
    to do arbitrary setup before each test is run. For example, we can test all three
    methods on identical lists of integers as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If we run this example, it indicates that all tests pass. Notice first that
    the `setUp` method is never explicitly called inside the three `test_*` methods.
    The test suite does this on our behalf. More importantly, notice how `test_median`
    alters the list, by adding an additional `4` to it, yet when the subsequent `test_mode`
    is called, the list has returned to the values specified in `setUp`. If it had
    not, there would be two fours in the list, and the `mode` method would have returned
    three values. This demonstrates that `setUp` is called individually before each
    test, ensuring the test class starts with a clean slate. Tests can be executed
    in any order, and the results of one test must never depend on any other tests.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the `setUp` method, `TestCase` offers a no-argument `tearDown`
    method, which can be used for cleaning up after each and every test on the class
    has run. This method is useful if cleanup requires anything other than letting
    an object be garbage collected.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we are testing code that does file I/O, our tests may create
    new files as a side effect of testing. The `tearDown` method can remove these
    files and ensure the system is in the same state it was before the tests ran.
    Test cases should never have side effects. In general, we group test methods into
    separate `TestCase` subclasses depending on what setup code they have in common.
    Several tests that require the same or similar setup will be placed in one class,
    while tests that require unrelated setup go in another class.
  prefs: []
  type: TYPE_NORMAL
- en: Organizing and running tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It doesn''t take long for a collection of unit tests to grow very large and
    unwieldy. It can quickly become complicated to load and run all the tests at once.
    This is a primary goal of unit testing: trivially run all tests on our program
    and get a quick *yes or no* answer to the question, *did my recent changes break
    anything?*.'
  prefs: []
  type: TYPE_NORMAL
- en: As with normal program code, we should divide our test classes into modules
    and packages that keep them organized. If you name each test module starting with
    the four characters *test*, there's an easy way to find and run them all. Python's
    `discover` module looks for any modules in the current folder or subfolders with
    names that start with `test`. If it finds any `TestCase` objects in these modules,
    the tests are executed. It's a painless way to ensure we don't miss running any
    tests. To use it, ensure your test modules are named `test_<something>.py` and
    then run the `python3``-m``unittest``discover `command.
  prefs: []
  type: TYPE_NORMAL
- en: Most Python programmers choose to put their tests in a separate package (usually
    named `tests/` alongside their source directory). This is not required, however.
    Sometimes it makes sense to put the test modules for different packages in a subpackage
    next to that package, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring broken tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, a test is known to fail, but we don't want the test suite to report
    the failure. This may be because a broken or unfinished feature has tests written,
    but we aren't currently focusing on improving it. More often, it happens because
    a feature is only available on a certain platform, Python version, or for advanced
    versions of a specific library. Python provides us with a few decorators to mark
    tests as expected to fail or to be skipped under known conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'These decorators are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`expectedFailure()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skip(reason)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skipIf(condition, reason)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skipUnless(condition, reason)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are applied using the Python decorator syntax. The first one accepts
    no arguments, and simply tells the test runner not to record the test as a failure
    when it fails. The `skip` method goes one step further and doesn''t even bother
    to run the test. It expects a single string argument describing why the test was
    skipped. The other two decorators accept two arguments, one a Boolean expression
    that indicates whether or not the test should be run, and a similar description.
    In use, these three decorators might be applied as they are in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The first test fails, but it is reported as an expected failure; the second
    test is never run. The other two tests may or may not be run depending on the
    current Python version and operating system. On my Linux system, running Python
    3.7, the output looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `x` on the first line indicates an expected failure; the two `s` characters
    represent skipped tests, and the `F` indicates a real failure, since the conditional
    to `skipUnless` was `True` on my system.
  prefs: []
  type: TYPE_NORMAL
- en: Testing with pytest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Python `unittest` module requires a lot of boilerplate code to set up and
    initialize tests. It is based on the very popular JUnit testing framework for
    Java. It even uses the same method names (you may have noticed they don't conform
    to the PEP-8 naming standard, which suggests snake_case rather than CamelCase
    to indicate a method name) and test layout. While this is effective for testing
    in Java, it's not necessarily the best design for Python testing. I actually find
    the `unittest` framework to be an excellent example of overusing object-oriented
    principles.
  prefs: []
  type: TYPE_NORMAL
- en: Because Python programmers like their code to be elegant and simple, other test
    frameworks have been developed, outside the standard library. Two of the more
    popular ones are `pytest` and `nose`. The former is more robust and has had Python
    3 support for much longer, so we'll discuss it here.
  prefs: []
  type: TYPE_NORMAL
- en: Since `pytest` is not part of the standard library, you'll need to download
    and install it yourself. You can get it from the `pytest` home page at [http://pytest.org/](http://pytest.org/).
    The website has comprehensive installation instructions for a variety of interpreters
    and platforms, but you can usually get away with the more common Python package
    installer, pip. Just type `pip install pytest` on your command line and you'll
    be good to go.
  prefs: []
  type: TYPE_NORMAL
- en: '`pytest` has a substantially different layout from the `unittest` module. It
    doesn''t require test cases to be classes. Instead, it takes advantage of the
    fact that Python functions are objects, and allows any properly named function
    to behave like a test. Rather than providing a bunch of custom methods for asserting
    equality, it uses the `assert` statement to verify results. This makes tests more
    readable and maintainable.'
  prefs: []
  type: TYPE_NORMAL
- en: When we run `pytest`, it starts in the current folder and searches for any modules
    or subpackages with names beginning with the characters `test_`. If any functions
    in this module also start with `test`, they will be executed as individual tests.
    Furthermore, if there are any classes in the module whose name starts with `Test`,
    any methods on that class that start with `test_` will also be executed in the
    test environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the following code, let''s port the simplest possible `unittest` example
    we wrote earlier to `pytest`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: For the exact same test, we've written two lines of more readable code, in comparison
    to the six lines required in our first `unittest` example.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we are not forbidden from writing class-based tests. Classes can be
    useful for grouping related tests together or for tests that need to access related
    attributes or methods on the class. The following example shows an extended class
    with a passing and a failing test; we''ll see that the error output is more comprehensive
    than that provided by the `unittest` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the class doesn''t have to extend any special objects to be picked
    up as a test (although `pytest` will run standard `unittest TestCases` just fine).
    If we run `pytest <filename>`, the output looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The output starts with some useful information about the platform and interpreter.
    This can be useful for sharing or discussing bugs across disparate systems. The
    third line tells us the name of the file being tested (if there are multiple test
    modules picked up, they will all be displayed), followed by the familiar `.F`
    we saw in the `unittest` module; the `.` character indicates a passing test, while
    the letter `F` demonstrates a failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'After all tests have run, the error output for each of them is displayed. It
    presents a summary of local variables (there is only one in this example: the
    `self` parameter passed into the function), the source code where the error occurred,
    and a summary of the error message. In addition, if an exception other than an
    `AssertionError` is raised, `pytest` will present us with a complete traceback,
    including source code references.'
  prefs: []
  type: TYPE_NORMAL
- en: By default, `pytest` suppresses output from `print` statements if the test is
    successful. This is useful for test debugging; when a test is failing, we can
    add `print` statements to the test to check the values of specific variables and
    attributes as the test runs. If the test fails, these values are output to help
    with diagnosis. However, once the test is successful, the `print` statement output
    is not displayed, and they are easily ignored. We don't have to *clean up* output
    by removing `print` statements. If the tests ever fail again, due to future changes,
    the debugging output will be immediately available.
  prefs: []
  type: TYPE_NORMAL
- en: One way to do setup and cleanup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`pytest` supports setup and teardown methods similar to those used in `unittest`,
    but it provides even more flexibility. We''ll discuss these briefly, since they
    are familiar, but they are not used as extensively as in the `unittest` module,
    as `pytest` provides us with a powerful fixtures facility, which we''ll discuss
    in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are writing class-based tests, we can use two methods called `setup_method`
    and `teardown_method` in the same way that `setUp` and `tearDown` are called in
    `unittest`. They are called before and after each test method in the class to
    perform setup and cleanup duties. There is one difference from the `unittest`
    methods though. Both methods accept an argument: the function object representing
    the method being called.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, `pytest` provides other setup and teardown functions to give us
    more control over when setup and cleanup code is executed. The `setup_class` and
    `teardown_class` methods are expected to be class methods; they accept a single
    argument (there is no `self` argument) representing the class in question. These
    methods are only run when the class is initiated rather than on each test run.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have the `setup_module` and `teardown_module` functions, which are
    run immediately before and after all tests (in functions or classes) in that module.
    These can be useful for *one time* setup, such as creating a socket or database
    connection that will be used by all tests in the module. Be careful with this
    one, as it can accidentally introduce dependencies between tests if the object
    stores state that isn't correctly cleaned up between tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'That short description doesn''t do a great job of explaining exactly when these
    methods are called, so let''s look at an example that illustrates exactly when
    it happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The sole purpose of the `BaseTest` class is to extract four methods that are
    otherwise identical to the test classes, and use inheritance to reduce the amount
    of duplicate code. So, from the point of view of `pytest`, the two subclasses
    have not only two test methods each, but also two setup and two teardown methods
    (one at the class level, one at the method level).
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run these tests using `pytest` with the `print` function output suppression
    disabled (by passing the `-s` or `--capture=no` flag), they show us when the various
    functions are called in relation to the tests themselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The setup and teardown methods for the module are executed at the beginning
    and end of the session. Then the lone module-level test function is run. Next,
    the setup method for the first class is executed, followed by the two tests for
    that class. These tests are each individually wrapped in separate `setup_method`
    and `teardown_method` calls. After the tests have executed, the teardown method
    on the class is called. The same sequence happens for the second class, before
    the `teardown_module` method is finally called, exactly once.
  prefs: []
  type: TYPE_NORMAL
- en: A completely different way to set up variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most common uses for the various setup and teardown functions is
    to ensure certain class or module variables are available with a known value before
    each test method is run.
  prefs: []
  type: TYPE_NORMAL
- en: '`pytest` offers a completely different way of doing this, using what are known
    as **fixtures**. Fixtures are basically named variables that are predefined in
    a test configuration file. This allows us to separate configuration from the execution
    of tests, and allows fixtures to be used across multiple classes and modules.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To use them, we add parameters to our test function. The names of the parameters
    are used to look up specific arguments in specially named functions. For example,
    if we wanted to test the `StatsList` class we used while demonstrating `unittest`,
    we would again want to repeatedly test a list of valid integers. But we can write
    our tests as follows instead of using a setup method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Each of the three test methods accepts a parameter named `valid_stats`; this
    parameter is created by calling the `valid_stats` function, which was decorated
    with `@pytest.fixture`.
  prefs: []
  type: TYPE_NORMAL
- en: Fixtures can do a lot more than return basic variables. A `request` object can
    be passed into the fixture factory to provide extremely useful methods and attributes
    to modify the funcarg's behavior. The `module`, `cls`, and `function` attributes
    allow us to see exactly which test is requesting the fixture. The `config` attribute
    allows us to check command-line arguments and a great deal of other configuration
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we implement the fixture as a generator, we can run cleanup code after each
    test is run. This provides the equivalent of a teardown method, except on a per-fixture
    basis. We can use it to clean up files, close connections, empty lists, or reset
    queues. For example, the following code tests the `os.mkdir` functionality by
    creating a temporary directory fixture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The fixture creates a new empty temporary directory for files to be created
    in. It yields this for use in the test, but removes that directory (using `shutil.rmtree`,
    which recursively removes a directory and anything inside it) after the test has
    completed. The filesystem is then left in the same state in which it started.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can pass a `scope` parameter to create a fixture that lasts longer than
    one test. This is useful when setting up an expensive operation that can be reused
    by multiple tests, as long as the resource reuse doesn''t break the atomic or
    unit nature of the tests (so that one test does not rely on, and is not impacted
    by, a previous one). For example, if we were to test the following echo server,
    we may want to run only one instance of the server in a separate process, and
    then have multiple tests connect to that instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'All this code does is listen on a specific port and wait for input from a client
    socket. When it receives input, it sends the same value back. To test this, we
    can start the server in a separate process and cache the result for use in multiple
    tests. Here''s how the test code might look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We've created two fixtures here. The first runs the echo server in a separate
    process, and yields the process object, cleaning it up when it's finished. The
    second instantiates a new socket object for each test, and closes the socket when
    the test has completed.
  prefs: []
  type: TYPE_NORMAL
- en: The first fixture is the one we're currently interested in. From the `scope="session"`
    keyword argument passed into the decorator's constructor, `pytest` knows that
    we only want this fixture to be initialized and terminated once for the duration
    of the unit test session.
  prefs: []
  type: TYPE_NORMAL
- en: The scope can be one of the strings `class`, `module`, `package`,  or `session`.
    It determines just how long the argument will be cached. We set it to `session` in
    this example, so it is cached for the duration of the entire `pytest` run. The
    process will not be terminated or restarted until all tests have run. The `module` scope,
    of course, caches it only for tests in that module, and the `class` scope treats
    the object more like a normal class setup and teardown.
  prefs: []
  type: TYPE_NORMAL
- en: At the time the third edition of this book went to print, the `package` scope
    was labeled experimental in `pytest`. Be careful with it, and they request that
    you supply bug reports.
  prefs: []
  type: TYPE_NORMAL
- en: Skipping tests with pytest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As with the `unittest` module, it is frequently necessary to skip tests in
    `pytest`, for a similar variety of reasons: the code being tested hasn''t been
    written yet, the test only runs on certain interpreters or operating systems,
    or the test is time-consuming and should only be run under certain circumstances.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can skip tests at any point in our code, using the `pytest.skip` function.
    It accepts a single argument: a string describing why it has been skipped. This
    function can be called anywhere. If we call it inside a test function, the test
    will be skipped. If we call it at the module level, all the tests in that module
    will be skipped. If we call it inside a fixture, all tests that call that funcarg
    will be skipped.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, in all these locations, it is often desirable to skip tests only
    if certain conditions are or are not met. Since we can execute the `skip` function
    at any place in Python code, we can execute it inside an `if` statement. So we
    may write a test that looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: That's some pretty silly code, really. There is no Python platform named `fakeos`,
    so this test will skip on all operating systems. It shows how we can skip conditionally,
    and since the `if` statement can check any valid conditional, we have a lot of
    power over when tests are skipped. Often, we check `sys.version_info` to check
    the Python interpreter version, `sys.platform` to check the operating system,
    or `some_library.__version__` to check whether we have a recent enough version
    of a given API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since skipping an individual test method or function based on a certain conditional
    is one of the most common uses of test skipping, `pytest` provides a convenience
    decorator that allows us to do this in one line. The decorator accepts a single
    string, which can contain any executable Python code that evaluates to a Boolean
    value. For example, the following test will only run on Python 3 or higher:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `pytest.mark.xfail` decorator behaves similarly, except that it marks a
    test as expected to fail, similar to `unittest.expectedFailure()`. If the test
    is successful, it will be recorded as a failure. If it fails, it will be reported
    as expected behavior. In the case of `xfail`, the conditional argument is optional.
    If it is not supplied, the test will be marked as expected to fail under all conditions.
  prefs: []
  type: TYPE_NORMAL
- en: The `pytest` has a ton of other features besides those described here and the
    developers are constantly adding innovative new ways to make your testing experience
    more enjoyable. They have thorough documentation on their website at [https://docs.pytest.org/](https://docs.pytest.org/).
  prefs: []
  type: TYPE_NORMAL
- en: The `pytest` can find and run tests defined using the standard `unittest` library
    in addition to its own testing infrastructure. This means that if you want to
    migrate from `unittest` to `pytest`, you don't have to rewrite all your old tests.
  prefs: []
  type: TYPE_NORMAL
- en: Imitating expensive objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, we want to test code that requires an object be supplied that is
    either expensive or difficult to construct. In some cases, this may mean your
    API needs rethinking to have a more testable interface (which typically means
    a more usable interface). But we sometimes find ourselves writing test code that
    has a ton of boilerplate to set up objects that are only incidentally related
    to the code under test.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, imagine we have some code that keeps track of flight statuses
    in an external key-value store (such as `redis` or `memcache`), such that we can
    store the timestamp and the most recent status. A basic version of such code might
    look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: There are a lot of things we ought to test for that `change_status` method.
    We should check that it raises the appropriate error if a bad status is passed
    in. We need to ensure that it converts statuses to uppercase. We can see that
    the key and value have the correct formatting when the `set()` method is called
    on the `redis` object.
  prefs: []
  type: TYPE_NORMAL
- en: One thing we don't have to check in our unit tests, however, is that the `redis`
    object is properly storing the data. This is something that absolutely should
    be tested in integration or application testing, but at the unit test level, we
    can assume that the py-redis developers have tested their code and that this method
    does what we want it to. As a rule, unit tests should be self-contained and shouldn't
    rely on the existence of outside resources, such as a running Redis instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, we only need to test that the `set()` method was called the appropriate
    number of times and with the appropriate arguments. We can use `Mock()` objects
    in our tests to replace the troublesome method with an object we can introspect.
    The following example illustrates the use of `Mock`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This test, written using `pytest` syntax, asserts that the correct exception
    is raised when an inappropriate argument is passed in. In addition, it creates
    a `Mock` object for the `set` method and makes sure that it is never called. If
    it was, it would mean there was a bug in our exception handling code.
  prefs: []
  type: TYPE_NORMAL
- en: Simply replacing the method worked fine in this case, since the object being
    replaced was destroyed in the end. However, we often want to replace a function
    or method only for the duration of a test. For example, if we want to test the
    timestamp formatting in the `Mock` method, we need to know exactly what `datetime.datetime.now()`
    is going to return. However, this value changes from run to run. We need some
    way to pin it to a specific value so we can test it deterministically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Temporarily setting a library function to a specific value is one of the few
    valid use cases for monkey-patching. The mock library provides a patch context
    manager that allows us to replace attributes on existing libraries with mock objects.
    When the context manager exits, the original attribute is automatically restored
    so as not to impact other test cases. Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we first construct a value called `fake_now`, which
    we will set as the return value of the `datetime.datetime.now` function. We have
    to construct this object before we patch `datetime.datetime`, because otherwise
    we'd be calling the patched `now` function before we constructed it.
  prefs: []
  type: TYPE_NORMAL
- en: The `with` statement invites the patch to replace the `datetime.datetime` module
    with a mock object, which is returned as the `dt `value. The neat thing about
    mock objects is that any time you access an attribute or method on that object,
    it returns another mock object. Thus, when we access `dt.now`, it gives us a new
    mock object. We set the `return_value` of that object to our `fake_now` object.
    Now, whenever the `datetime.datetime.now` function is called, it will return our
    object instead of a new mock object. But when the interpreter exits the context
    manager, the original `datetime.datetime.now()` functionality is restored.
  prefs: []
  type: TYPE_NORMAL
- en: After calling our `change_status` method with known values, we use the `assert_called_once_with`
    function of the `Mock` class to ensure that the `now` function was indeed called
    exactly once with no arguments. We then call it a second time to prove that the
    `redis.set` method was called with arguments that were formatted as we expected
    them to be.
  prefs: []
  type: TYPE_NORMAL
- en: Mocking dates so you can have deterministic test results is a common patching
    scenario. If you are in a situation where you are doing a lot of this, you might
    appreciate the `freezegun` and `pytest-freezegun` projects available in the Python
    Package Index.
  prefs: []
  type: TYPE_NORMAL
- en: The previous example is a good indication of how writing tests can guide our
    API design. The `FlightStatusTracker` object looks sensible at first glance; we
    construct a `redis` connection when the object is constructed, and we call into
    it when we need it. When we write tests for this code, however, we discover that
    even if we mock out that `self.redis` variable on a `FlightStatusTracker`, the
    `redis` connection still has to be constructed. This call actually fails if there
    is no Redis server running, and our tests also fail.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could solve this problem by mocking out the `redis.StrictRedis` class to
    return a mock in a `setUp` method. A better idea, however, might be to rethink
    our implementation. Instead of constructing the `redis` instance inside`__init__`,
    perhaps we should allow the user to pass one in, as in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This allows us to pass a mock in when we are testing, so the `StrictRedis`
    method never gets constructed. Additionally, it allows any client code that talks
    to `FlightStatusTracker` to pass in their own `redis` instance. There are a variety
    of reasons they might want to do this: they may have already constructed one for
    other parts of their code; they may have created an optimized implementation of
    the `redis` API; perhaps they have one that logs metrics to their internal monitoring
    systems. By writing a unit test, we''ve uncovered a use case that makes our API
    more flexible from the start, rather than waiting for clients to demand we support
    their exotic needs.'
  prefs: []
  type: TYPE_NORMAL
- en: This has been a brief introduction to the wonders of mocking code. Mocks are
    part of the standard `unittest` library since Python 3.3, but as you see from
    these examples, they can also be used with `pytest` and other libraries. Mocks
    have other more advanced features that you may need to take advantage of as your
    code gets more complicated. For example, you can use the `spec` argument to invite
    a mock to imitate an existing class so that it raises an error if code tries to
    access an attribute that does not exist on the imitated class. You can also construct
    mock methods that return different arguments each time they are called by passing
    a list as the `side_effect` argument. The `side_effect` parameter is quite versatile;
    you can also use it to execute arbitrary functions when the mock is called or
    to raise an exception.
  prefs: []
  type: TYPE_NORMAL
- en: In general, we should be quite stingy with mocks. If we find ourselves mocking
    out multiple elements in a given unit test, we may end up testing the mock framework
    rather than our real code. This serves no useful purpose whatsoever; after all,
    mocks are well-tested already! If our code is doing a lot of this, it's probably
    another sign that the API we are testing is poorly designed. Mocks should exist
    at the boundaries between the code under test and the libraries they interface
    with. If this isn't happening, we may need to change the API so that the boundaries
    are redrawn in a different place.
  prefs: []
  type: TYPE_NORMAL
- en: How much testing is enough?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've already established that untested code is broken code. But how can we
    tell how well our code is tested? How do we know how much of our code is actually
    being tested and how much is broken? The first question is the more important
    one, but it's hard to answer. Even if we know we have tested every line of code
    in our application, we do not know that we have tested it properly. For example,
    if we write a stats test that only checks what happens when we provide a list
    of integers, it may still fail spectacularly if used on a list of floats, strings,
    or self-made objects. The onus of designing complete test suites still lies with
    the programmer.
  prefs: []
  type: TYPE_NORMAL
- en: The second question–how much of our code is actually being tested–is easy to
    verify. **Code coverage** is an estimate of the number of lines of code that are
    executed by a program. If we know that number and the number of lines that are
    in the program, we can get an estimate of what percentage of the code was really
    tested, or covered. If we additionally have an indicator as to which lines were
    not tested, we can more easily write new tests to ensure those lines are less
    broken.
  prefs: []
  type: TYPE_NORMAL
- en: The most popular tool for testing code coverage is called, memorably enough,
    `coverage.py`. It can be installed like most other third-party libraries, using
    the `pip install coverage `command.
  prefs: []
  type: TYPE_NORMAL
- en: 'We don''t have space to cover all the details of the coverage API, so we''ll
    just look at a few typical examples. If we have a Python script that runs all
    our unit tests for us (for example, using `unittest.main`,  `discover`, `pytest`,
    or a custom test runner), we can use the following command to perform a coverage
    analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will exit normally, but it creates a file named `.coverage`, which
    holds the data from the run. We can now use the `coverage``report` command to
    get an analysis of the code coverage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting output should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This basic report lists the files that were executed (our unit test and a module
    it imported). The number of lines of code in each file, and the number that were
    executed by the test are also listed. The two numbers are then combined to estimate
    the amount of code coverage. If we pass the `-m` option to the `report` command,
    it will additionally add a column that looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The ranges of lines listed here identify lines in the `stats` module that were
    not executed during the test run.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example we just ran the code coverage tool on uses the same stats module
    we created earlier in the chapter. However, it deliberately uses a single test
    that fails to test a lot of code in the file. Here''s the test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This code doesn't test the median or mode functions, which correspond to the
    line numbers that the coverage output told us were missing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The textual report provides sufficient information, but if we use the `coverage
    html` command, we can get an even more useful interactive HTML report, which we
    can view in a web browser. The web page even highlights which lines in the source
    code were and were not tested. Here''s how it looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/27269204-aae4-40d5-9455-96a15d846345.png)'
  prefs: []
  type: TYPE_IMG
- en: We can use the `coverage.py` module with `pytest` as well. We'll need to install
    the `pytest` plugin for code coverage, using `pip install pytest-coverage`. The
    plugin adds several command-line options to `pytest`, the most useful being `--cover-report`,
    which can be set to `html`, `report`, or `annotate` (the latter actually modifies
    the original source code to highlight any lines that were not covered).
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, if we could somehow run a coverage report on this section of
    the chapter, we'd find that we have not covered most of what there is to know
    about code coverage! It is possible to use the coverage API to manage code coverage
    from within our own programs (or test suites), and `coverage.py` accepts numerous
    configuration options that we haven't touched on. We also haven't discussed the
    difference between statement coverage and branch coverage (the latter is much
    more useful, and the default in recent versions of `coverage.py`), or other styles
    of code coverage.
  prefs: []
  type: TYPE_NORMAL
- en: Bear in mind that while 100 percent code coverage is a lofty goal that we should
    all strive for, 100 percent coverage is not enough! Just because a statement was
    tested does not mean that it was tested properly for all possible inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's walk through test-driven development by writing a small, tested, cryptography
    application. Don't worry–you won't need to understand the mathematics behind complicated
    modern encryption algorithms such as AES or RSA. Instead, we'll be implementing
    a sixteenth-century algorithm known as the Vigenère cipher. The application simply
    needs to be able to encode and decode a message, given an encoding keyword, using
    this cipher.
  prefs: []
  type: TYPE_NORMAL
- en: If you want a deep dive into how the RSA algorithm works, I wrote one on my
    blog at [https://dusty.phillips.codes/](https://dusty.phillips.codes/).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to understand how the cipher works if we apply it manually (without
    a computer). We start with a table like the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Given a keyword, TRAIN, we can encode the message ENCODED IN PYTHON as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Repeat the keyword and message together, such that it is easy to map letters
    from one to the other:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: For each letter in the plaintext, find the row that begins with that letter
    in the table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the column with the letter associated with the keyword letter for the chosen
    plaintext letter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The encoded character is at the intersection of this row and column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For example, the row starting with E intersects the column starting with T at
    character X. So, the first letter in the ciphertext is X. The row starting with
    N intersects the column starting with R at character E, leading to the ciphertext
    XE. C intersects A at C, and O intersects I at W. D and N map to Q, while E and
    T map to X. The full encoded message is XECWQXUIVCRKHWA.
  prefs: []
  type: TYPE_NORMAL
- en: Decoding follows the opposite procedure. First, find the row with the character
    for the shared keyword (the T row), then find the location in that row where the
    encoded character (the X) is located. The plaintext character is at the top of
    the column for that row (the E).
  prefs: []
  type: TYPE_NORMAL
- en: Implementing it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our program will need an `encode` method that takes a keyword and plaintext
    and returns the ciphertext, and a `decode` method that accepts a keyword and ciphertext
    and returns the original message.
  prefs: []
  type: TYPE_NORMAL
- en: 'But rather than just writing those methods, let''s follow a test-driven development
    strategy. We''ll be using `pytest` for our unit testing. We need an `encode` method,
    and we know what it has to do; let''s write a test for that method first, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This test fails, naturally, because we aren't importing a `VigenereCipher` class
    anywhere. Let's create a new module to hold that class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the following `VigenereCipher` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: If we add a `from``vigenere_cipher``import``VigenereCipher` line to the top
    of our test class and run `pytest`, the preceding test will pass! We've finished
    our first test-driven development cycle.
  prefs: []
  type: TYPE_NORMAL
- en: 'This may seem like a ridiculously silly thing to test, but it''s actually verifying
    a lot. The first time I implemented it, I mispelled cipher as *cypher* in the
    class name. Even my basic unit test helped catch a bug. Even so, returning a hardcoded
    string is obviously not the most sensible implementation of a cipher class, so
    let''s add a second test, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Ah, now that test will fail. It looks like we''re going to have to work harder.
    But I just thought of something: what if someone tries to encode a string with
    spaces or lowercase characters? Before we start implementing the encoding, let''s
    add some tests for these cases, so we don''t forget them. The expected behavior
    will be to remove spaces, and to convert lowercase letters to capitals, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: If we run the new test suite, we find that the new tests pass (they expect the
    same hardcoded string). But they ought to fail later if we forget to account for
    these cases.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have some test cases, let's think about how to implement our encoding
    algorithm. Writing code to use a table like we used in the earlier manual algorithm
    is possible, but seems complicated, considering that each row is just an alphabet
    rotated by an offset number of characters. It turns out (I asked Wikipedia) that
    we can use modular arithmetic to combine the characters instead of doing a table
    lookup.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given plaintext and keyword characters, if we convert the two letters to their
    numerical values (according to their position in the alphabet, with A being 0
    and Z being 25), add them together, and take the remainder mod 26, we get the
    ciphertext character! This is a straightforward calculation, but since it happens
    on a character-by-character basis, we should probably put it in its own function.
    Before we do that, then, we should write a test for the new function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can write the code to make this function work. In all honesty, I had
    to run the test several times before I got this function completely correct. First,
    I accidentally returned an integer, and then I forgot to shift the character back
    up to the normal ASCII scale from the zero-based scale. Having the test available
    made it easy to test and debug these errors. This is another bonus of test-driven
    development. The final, working version of the code looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that `combine_characters` is tested, I thought we''d be ready to implement
    our `encode` function. However, the first thing we want inside that function is
    a repeating version of the keyword string that is as long as the plaintext. Let''s
    implement a function for that first. Oops, I mean let''s implement the test first,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Before writing this test, I expected to write `extend_keyword` as a standalone
    function that accepted a keyword and an integer. But as I started drafting the
    test, I realized it made more sense to use it as a helper method on the `VigenereCipher`
    class so it could access the `self.keyword` attribute. This shows how test-driven
    development can help design more sensible APIs. The following is the method implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Once again, this took a few runs of the test to get right. I ended up adding
    an amended copy of the test, one with fifteen and one with sixteen letters, to
    make sure it works if the integer division has an even number.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we''re finally ready to write our `encode` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: That looks correct. Our test suite should pass now, right?
  prefs: []
  type: TYPE_NORMAL
- en: 'Actually, if we run it, we''ll find that two tests are still failing. The previously
    failing encode test is actually passing, but we totally forgot about the spaces
    and lowercase characters! It is a good thing we wrote those tests to remind us.
    We''ll have to add the following line at the beginning of the method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: If we have an idea about a corner case in the middle of implementing something,
    we can create a test describing that idea. We don't even have to implement the
    test; we can just run `assert False` to remind us to implement it later. The failing
    test will never let us forget the corner case and it can't be ignored as easily
    as a ticket in an issue tracker. If it takes a while to get around to fixing the
    implementation, we can mark the test as an expected failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now all the tests pass successfully. This chapter is pretty long, so we''ll
    condense the examples for decoding. The following are a couple of tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'And the following is the `separate_character` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can add the `decode` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'These methods have a lot of similarity to those used for encoding. The great
    thing about having all these tests written and passing is that we can now go back
    and modify our code, knowing it is still safely passing the tests. For example,
    if we replace our existing `encode` and `decode` methods with the following refactored
    methods, our tests still pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the final benefit of test-driven development, and the most important.
    Once the tests are written, we can improve our code as much as we like and be
    confident that our changes didn''t break anything we have been testing for. Furthermore,
    we know exactly when our refactor is finished: when the tests all pass.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, our tests may not comprehensively test everything we need them to;
    maintenance or code refactoring can still cause undiagnosed bugs that don''t show
    up in testing. Automated tests are not foolproof. If bugs do occur, however, it
    is still possible to follow a test-driven plan, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Write a test (or multiple tests) that duplicates or *proves* that the bug in
    question is occurring. This will, of course, fail.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then write the code to make the tests stop failing. If the tests were comprehensive,
    the bug will be fixed, and we will know if it ever happens again, as soon as we
    run the test suite.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we can try to determine how well our tests operate on this code. With
    the `pytest` coverage plugin installed, `pytest -coverage-report=report` tells
    us that our test suite has 100 percent code coverage. This is a great statistic,
    but we shouldn't get too cocky about it. Our code hasn't been tested when encoding
    messages that have numbers, and its behavior with such inputs is thus undefined.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Practice test-driven development. That is your first exercise. It's easier to
    do this if you're starting a new project, but if you have existing code you need
    to work on, you can start by writing tests for each new feature you implement.
    This can become frustrating as you become more enamored with automated tests.
    The old, untested code will start to feel rigid and tightly coupled, and will
    become uncomfortable to maintain; you'll start feeling like changes you make are
    breaking the code and you have no way of knowing, for lack of tests. But if you
    start small, adding tests to the code base improves it over time.
  prefs: []
  type: TYPE_NORMAL
- en: So, to get your feet wet with test-driven development, start a fresh project.
    Once you've started to appreciate the benefits (you will) and realize that the
    time spent writing tests is quickly regained in terms of more maintainable code,
    you'll want to start writing tests for existing code. This is when you should
    start doing it, not before. Writing tests for code that we *know* works is boring.
    It is hard to get interested in the project until you realize just how broken
    the code we thought was working really is.
  prefs: []
  type: TYPE_NORMAL
- en: Try writing the same set of tests using both the built-in `unittest` module
    and `pytest`. Which do you prefer? `unittest` is more similar to test frameworks
    in other languages, while `pytest` is arguably more Pythonic. Both allow us to
    write object-oriented tests and to test object-oriented programs with ease.
  prefs: []
  type: TYPE_NORMAL
- en: We used `pytest` in our case study, but we didn't touch on any features that
    wouldn't have been easily testable using `unittest`. Try adapting the tests to
    use test skipping or fixtures (an instance of `VignereCipher` would be helpful).
    Try the various setup and teardown methods, and compare their use to funcargs.
    Which feels more natural to you?
  prefs: []
  type: TYPE_NORMAL
- en: Try running a coverage report on the tests you've written. Did you miss testing
    any lines of code? Even if you have 100 percent coverage, have you tested all
    the possible inputs? If you're doing test-driven development, 100 percent coverage
    should follow quite naturally, as you will write a test before the code that satisfies
    that test. However, if writing tests for existing code, it is more likely that
    there will be edge conditions that go untested.
  prefs: []
  type: TYPE_NORMAL
- en: 'Think carefully about the values that are somehow different, such as the following,
    for example:'
  prefs: []
  type: TYPE_NORMAL
- en: Empty lists when you expect full ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Negative numbers, zero, one, or infinity compared to positive integers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Floats that don't round to an exact decimal place
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strings when you expected numerals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unicode strings when you expected ASCII
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ubiquitous `None` value when you expected something meaningful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your tests cover such edge cases, your code will be in good shape.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have finally covered the most important topic in Python programming: automated
    testing. Test-driven development is considered a best practice. The standard library
    `unittest` module provides a great out-of-the-box solution for testing, while
    the `pytest` framework has some more Pythonic syntaxes. Mocks can be used to emulate
    complex classes in our tests. Code coverage gives us an estimate of how much of
    our code is being run by our tests, but it does not tell us that we have tested
    the right things.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we''ll jump into a completely different topic: concurrency.'
  prefs: []
  type: TYPE_NORMAL
