<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Testing, Profiling, and Dealing with Exceptions</h1></div></div></div><div><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td valign="top"> </td><td valign="top"><p><em>"Code without tests is broken by design."</em></p></td><td valign="top"> </td></tr><tr><td valign="top"> </td><td colspan="2" align="right" valign="top" style="text-align: center">--<em>Jacob Kaplan-Moss</em></td></tr></table></div><p>Jacob Kaplan-Moss is one of the core developers of the Django web framework. We're going to explore it in the next chapters. I strongly agree with this quote of his. I believe code without tests shouldn't be deployed to production.</p><p>Why are tests so important? Well, for one, they give you predictability. Or, at least, they help you achieve high predictability. Unfortunately, there is always some bug that sneaks into our code. But we definitely want our code to be as predictable as possible. What we don't want is to have a surprise, our code behaving in an unpredictable way. Would you be happy to know that the software that checks on the sensors of the plane that is taking you on holidays sometimes goes crazy? No, probably not.</p><p>Therefore we need to test our code, we need to check that its behavior is correct, that it works as expected when it deals with edge cases, that it doesn't hang when the components it's talking to are down, that the performances are well within the acceptable range, and so on.</p><p>This chapter is all about this topic, making sure that your code is prepared to face the scary outside world, that is fast enough and that it can deal with unexpected or exceptional conditions.</p><p>We're <a id="id461" class="indexterm"/>going to explore testing, including a brief introduction to <strong>test-driven development</strong> (<strong>TDD</strong>), which is one of my favorite working methodologies. Then, we're going to explore the world of exceptions, and finally we're going to talk a little bit about performances and profiling. Deep breath, and here we go...</p><div><div><div><div><h1 class="title"><a id="ch07lvl1sec63"/>Testing your application</h1></div></div></div><p>There<a id="id462" class="indexterm"/> are many different kinds of tests, so many in fact that companies often have a dedicated department, called <strong>quality assurance</strong> (<strong>QA</strong>), made up of <a id="id463" class="indexterm"/>individuals that spend their day testing the software the company developers produce.</p><p>To start making an initial classification, we can divide tests into two broad categories: white-box and black-box tests.</p><p>
<strong>White-box tests</strong> are <a id="id464" class="indexterm"/>those which exercise the internals of the code, they inspect it down to a very fine level of granularity. On the other hand, <strong>black-box tests</strong> are <a id="id465" class="indexterm"/>those which consider the software under testing as if being within a box, the internals of which are ignored. Even the technology, or the language used inside the box is not important for black-box tests. What they do is to plug input to one end of the box and verify the output at the other end, and that's it.</p><div><div><h3 class="title"><a id="note44"/>Note</h3><p>There<a id="id466" class="indexterm"/> is also an in-between category, called <strong>gray-box</strong> testing, that involves testing a system in the same way we do with the black-box approach, but having some knowledge about the algorithms and data structures used to write the software and only partial access to its source code.</p></div></div><p>There are many different kinds of tests in these categories, each of which serves a different purpose. Just to give you an idea, here's a few:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Front-end tests</strong> make sure that the client side of your application is exposing the<a id="id467" class="indexterm"/> information that it should, all the links, the buttons, the advertising, everything that needs to be shown to the client. It may also verify that it is possible to walk a certain path through the user interface.</li><li class="listitem" style="list-style-type: disc"><strong>Scenario tests</strong> make use of stories (or scenarios) that help the tester work through<a id="id468" class="indexterm"/> a complex problem or test a part of the system.</li><li class="listitem" style="list-style-type: disc"><strong>Integration tests</strong> verify the behavior of the various components of your application <a id="id469" class="indexterm"/>when they are working together sending messages through interfaces.</li><li class="listitem" style="list-style-type: disc"><strong>Smoke tests</strong> are <a id="id470" class="indexterm"/>particularly useful when you deploy a new update on your application. They check whether the most essential, vital parts of your application are still working as they should and that they are not <em>on fire</em>. This term comes from when engineers tested circuits by making sure nothing was smoking.</li><li class="listitem" style="list-style-type: disc"><strong>Acceptance tests</strong>, or <strong>user acceptance testing</strong> (<strong>UAT</strong>) is what a developer does <a id="id471" class="indexterm"/>with a product owner (for example, in a SCRUM<a id="id472" class="indexterm"/> environment) to determine if the work that was commissioned was carried out correctly.</li><li class="listitem" style="list-style-type: disc"><strong>Functional tests</strong> verify<a id="id473" class="indexterm"/> the features or functionalities of your software.</li><li class="listitem" style="list-style-type: disc"><strong>Destructive tests</strong> take down parts of your system, simulating a failure, in order <a id="id474" class="indexterm"/>to establish how well the remaining parts of the system perform. These kinds of tests are performed extensively by companies that need to provide an extremely reliable service, such as Amazon, for example.</li><li class="listitem" style="list-style-type: disc"><strong>Performance tests</strong> aim to verify how well the system performs under a specific <a id="id475" class="indexterm"/>load of data or traffic so that, for example, engineers can get a better understanding of which are the bottlenecks in the system that could bring it down to its knees in a heavy load situation, or those which prevent scalability.</li><li class="listitem" style="list-style-type: disc"><strong>Usability tests</strong>, and the closely related <strong>user experience</strong> (<strong>UX</strong>) tests, aim to check<a id="id476" class="indexterm"/> if the user interface is simple and easy to understand <a id="id477" class="indexterm"/>and use. They aim to provide input to the<a id="id478" class="indexterm"/> designers so that the user experience is improved.</li><li class="listitem" style="list-style-type: disc"><strong>Security and penetration tests</strong> aim<a id="id479" class="indexterm"/> to verify how well the system is protected against attacks and intrusions.</li><li class="listitem" style="list-style-type: disc"><strong>Unit tests</strong> help the developer to write the code in a robust and consistent way, providing <a id="id480" class="indexterm"/>the first line of feedback and defense against coding mistakes, refactoring mistakes, and so on.</li><li class="listitem" style="list-style-type: disc"><strong>Regression tests</strong> provide the developer with useful information about a feature<a id="id481" class="indexterm"/> being compromised in the system after an update. Some of the causes for a system being said to have a regression are an old bug coming back to life, an existing feature being compromised, or a new issue being introduced.</li></ul></div><p>Many books and articles have been written about testing, and I have to point you to those resources if you're interested in finding out more about all the different kinds of tests. In this chapter, we will concentrate on unit tests, since they are the backbone of software crafting and form the vast majority of tests that are written by a developer.</p><p>Testing is an <em>art</em>, an art that you don't learn from books, I'm afraid. You can learn all the definitions (and you should), and try and collect as much knowledge about testing as you can but I promise you, you will be able to test your software properly only when you have done it for long enough in the field.</p><p>When you are having trouble refactoring a bit of code, because every little thing you touch makes a test blow up, you learn how to write less rigid and limiting tests, which still verify the correctness of your code but, at the same time, allow you the freedom and joy to play with it, to shape it as you want.</p><p>When you are being called too often to fix unexpected bugs in your code, you learn how to write tests more thoroughly, how to come up with a more comprehensive list of edge cases, and strategies to cope with them before they turn into bugs.</p><p>When you are spending too much time reading tests and trying to refactor them in order to change a small feature in the code, you learn to write simpler, shorter, and better focused tests.</p><p>I could<a id="id482" class="indexterm"/> go on with this <em>when you... you learn...</em>, but I guess you get the picture. You need to get your hands dirty and build experience. My suggestion? Study the theory as much as you can, and then experiment using different approaches. Also, try to learn from experienced coders; it's very effective.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec93"/>The anatomy of a test</h2></div></div></div><p>Before we <a id="id483" class="indexterm"/>concentrate on unit tests, let's see what a test is, and what its purpose is.</p><p>A <strong>test</strong> is a piece of code whose purpose is to verify something in our system. It may be that we're calling a function passing two integers, that an object has a property called <code class="literal">donald_duck</code>, or that when you place an order on some API, after a minute you can see it dissected into its basic elements, in the database.</p><p>A test is typically comprised of three sections:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Preparation</strong>: This<a id="id484" class="indexterm"/> is where you set up the scene. You prepare all the data, the objects, the services you need in the places you need them so that they are ready to be used.</li><li class="listitem" style="list-style-type: disc"><strong>Execution</strong>: This <a id="id485" class="indexterm"/>is where you execute the bit of logic that you're checking against. You perform an action using the data and the interfaces you have set up in the preparation phase.</li><li class="listitem" style="list-style-type: disc"><strong>Verification</strong>: This is<a id="id486" class="indexterm"/> where you verify the results and make sure they are according to your expectations. You check the returned value of a function, or that some data is in the database, some is not, some has changed, a request has been made, something has happened, a method has been called, and so on.</li></ul></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec94"/>Testing guidelines</h2></div></div></div><p>Like<a id="id487" class="indexterm"/> software, tests can be good or bad, with the whole range of shades in the middle. In order to write good tests, here are some guidelines:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Keep them as simple as possible</strong>: It's okay to violate some good coding rules, such as hardcoding values or duplicating code. Tests need first and foremost to be as readable as possible and easy to understand. When tests are hard to read or understand, you can never be sure if they are actually making sure your code is performing correctly.</li><li class="listitem" style="list-style-type: disc"><strong>Tests should verify one thing and one thing only</strong>: It's very important that you keep them short and contained. It's perfectly fine to write multiple tests to exercise a single object or function. Just make sure that each test has one and only one purpose.</li><li class="listitem" style="list-style-type: disc"><strong>Tests should not make any unnecessary assumption when verifying data</strong>: This is tricky to understand at first, but say you are testing the return value of a function and it is an unordered list of numbers (like <code class="literal">[2, 3, 1]</code>). If the order in that list is random, in the test you may be tempted to sort it and compare it with <code class="literal">[1, 2, 3]</code>. If you do, you will introduce an extra assumption on the ordering of the result of your function call, and <em>this is bad practice</em>. You should always find a way to verify things without introducing any assumptions or any feature that doesn't belong in the use case you're describing with your test.</li><li class="listitem" style="list-style-type: disc"><strong>Tests should exercise the what, rather than the how</strong>: Tests should focus on checking <em>what</em> a function is supposed to do, rather than <em>how</em> it is doing it. For example, focus on the fact that it's calculating the square root of a number (the <em>what</em>), instead of on the fact that it is calling <code class="literal">math.sqrt</code> to do it (the <em>how</em>). Unless you're writing performance tests or you have a particular need to verify how a certain action is performed, try to avoid this type of testing and focus on the <em>what</em>. Testing the <em>how</em> leads to restrictive tests and makes refactoring hard. Moreover, the type of test you have to write when you concentrate on the <em>how</em> is more likely to degrade the quality of your testing code base when you amend your software frequently (more on this later).</li><li class="listitem" style="list-style-type: disc"><strong>Tests should assume the least possible in the preparation phase</strong>: Say you have 10 tests that are checking how a data structure is manipulated by a function. And let's say this data structure is a dict with five key/value pairs. If you put the complete dict in each test, the moment you have to change something in that dict, you also have to amend all ten tests. On the other hand, if you strip down the test data as much as you can, you will find that, most of the time, it's possible to have the majority of tests checking only a partial version of the data, and only a few running with a full version of it. This means that when you need to change your data, you will have to amend only those tests that are actually exercising it.</li><li class="listitem" style="list-style-type: disc"><strong>Test should run as fast as possible</strong>: A good test codebase could end up being much longer than the code being tested itself. It varies according to the situation and the developer but whatever the length, you'll end up having hundreds, if not thousands, of tests to run, which means the faster they run, the faster you can get back to writing code. When using TDD, for example, you run tests very often, so speed is essential.</li><li class="listitem" style="list-style-type: disc"><strong>Tests should use up the least possible amount of resources</strong>: The reason for this is that every developer who checks out your code should be able to run your tests, no <a id="id488" class="indexterm"/>matter how powerful their box is. It could be a skinny virtual machine or a neglected Jenkins box, your tests should run without chewing up too many resources.<div><div><h3 class="title"><a id="note45"/>Note</h3><p>A <strong>Jenkins</strong> box is a machine that runs Jenkins, software that is capable of, amongst many other things, running your tests automatically. Jenkins is frequently used in companies where developers use practices like continuous integration, extreme programming, and so on.</p></div></div></li></ul></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec95"/>Unit testing</h2></div></div></div><p>Now that <a id="id489" class="indexterm"/>you have an idea about what testing is and why we need it, let's <a id="id490" class="indexterm"/>finally introduce the developer's best friend: the <strong>unit test</strong>.</p><p>Before we proceed with the examples, allow me to spend some words of caution: I'll try to give you the fundamentals about unit testing, but I don't follow any particular school of thought or methodology to the letter. Over the years, I have tried many different testing approaches, eventually coming up with my own way of doing things, which is constantly evolving. To put it as Bruce Lee would have:</p><div><blockquote class="blockquote"><p><em>"Absorb what is useful, discard what is useless and add what is specifically your own".</em></p></blockquote></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec16"/>Writing a unit test</h3></div></div></div><p>In order<a id="id491" class="indexterm"/> to explain how to write a unit test, let's help ourselves with a simple snippet:</p><p>
<code class="literal">data.py</code>
</p><div><pre class="programlisting">def get_clean_data(source):
    data = <strong>load_data</strong>(source)
    cleaned_data = <strong>clean_data</strong>(data)
    return cleaned_data</pre></div><p>The function <code class="literal">get_clean_data</code> is responsible for getting data from <code class="literal">source</code>, cleaning it, and returning it to the caller. How do we test this function?</p><p>One way of doing this is to call it and then make sure that <code class="literal">load_data</code> was called once with <code class="literal">source</code> as its only argument. Then we have to verify that <code class="literal">clean_data</code> was called once, with the return value of <code class="literal">load_data</code>. And, finally, we would need to make sure that the return value of <code class="literal">clean_data</code> is what is returned by the <code class="literal">get_clean_data</code> function as well.</p><p>In order to do this, we need to set up the source and run this code, and this may be a problem. One of the golden rules of unit testing is that <em>anything that crosses the boundaries of your application needs to be simulated</em>. We don't want to talk to a real data source, and <a id="id492" class="indexterm"/>we don't want to actually run real functions if they are communicating with anything that is not contained in our application. A few examples would be a database, a search service, an external API, a file in the filesystem, and so on.</p><p>We need these restrictions to act as a shield, so that we can always run our tests safely without the fear of destroying something in a real data source.</p><p>Another reason is that it may be quite difficult for a single developer to reproduce the whole architecture on their box. It may require the setting up of databases, APIs, services, files and folders, and so on and so forth, and this can be difficult, time consuming, or sometimes not even possible.</p><div><div><h3 class="title"><a id="note46"/>Note</h3><p>Very<a id="id493" class="indexterm"/> simply put, an <strong>application programming interface</strong> (<strong>API</strong>) is a set of tools for building software applications. An API expresses a software component in terms of its operations, inputs and outputs, and underlying types. For example, if you create a software that needs to interface with a data provider service, it's very likely that you will have to go through their API in order to gain access to the data.</p></div></div><p>Therefore, in our unit tests, we need to simulate all those things in some way. Unit tests need to be run by any developer without the need for the whole system to be set up on their box.</p><p>A different approach, which I always favor when it's possible to do so, is to simulate entities without using fake objects, but using special purpose test objects instead. For example, if your code talks to a database, instead of faking all the functions and methods that talk to the database and programming the fake objects so that they return what the real ones would, I'd much rather prefer to spawn a test database, set up the tables and data I need, and then patch the connection settings so that my tests are running real code, against the test database, thereby doing no harm at all. In-memory databases are excellent options for these cases.</p><div><div><h3 class="title"><a id="note47"/>Note</h3><p>One of the applications that allow you to spawn a database for testing, is Django. Within the <code class="literal">django.test</code> package you can find several tools that help you write your tests so that you won't have to simulate the dialog with a database. By writing tests this way, you will also be able to check on transactions, encodings, and all other database related aspects of programming. Another advantage of this approach consists in the ability of checking against things that can change from one database to another.</p></div></div><p>Sometimes, though, it's still not possible, and we need to use fakes, therefore let's talk about them.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec17"/>Mock objects and patching</h3></div></div></div><p>First <a id="id494" class="indexterm"/>of all, in Python, these fake objects are called <strong>mocks</strong>. Up to version 3.3, the <code class="literal">mock</code> library was a third-party library that basically every project would<a id="id495" class="indexterm"/> install via <code class="literal">pip</code> but, from version 3.3, it has been included in the standard library under the <code class="literal">unittest</code> module, and rightfully so, given its importance and how widespread it is.</p><p>The act of replacing a real object or function (or in general, any piece of data structure) with a mock, is <a id="id496" class="indexterm"/>called <strong>patching</strong>. The <code class="literal">mock</code> library provides the <code class="literal">patch</code> tool, which<a id="id497" class="indexterm"/> can act as a function or class decorator, and even as a context manager (more on this in <a class="link" href="ch08.html" title="Chapter 8. The Edges – GUIs and Scripts">Chapter 8</a>, <em>The Edges – GUIs and Scripts</em>), that you can use to mock things out. Once you have replaced everything you need not to run, with suitable mocks, you can pass to the second phase of the test and run the code you are exercising. After the execution, you will be able to check those mocks to verify that your code has worked correctly.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec18"/>Assertions</h3></div></div></div><p>The <a id="id498" class="indexterm"/>verification phase is done through the use of assertions. An <strong>assertion</strong> is a function (or method) that you can use to verify equality between objects, as well as other conditions. When a condition is not met, the assertion will raise an exception<a id="id499" class="indexterm"/> that will make your test fail. You can find a list of assertions in the <code class="literal">unittest</code> module documentation, and their corresponding Pythonic version in the nose third-party library, which provides a few advantages over the sheer <code class="literal">unittest</code> module, starting from an improved test discovery strategy (which is the way a test runner detects and discovers the tests in your application).</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec19"/>A classic unit test example</h3></div></div></div><p>Mocks, patches, and <a id="id500" class="indexterm"/>assertions are the basic tools we'll be using to write tests. So, finally, let's see an example. I'm going to write a function that takes a list of integers and filters out all those which aren't positive.</p><p>
<code class="literal">filter_funcs.py</code>
</p><div><pre class="programlisting">def filter_ints(v):
    return [num for num in v if is_positive(num)]

def is_positive(n):
    return n &gt; 0</pre></div><p>In the preceding example, we define the <code class="literal">filter_ints</code> function, which basically uses a list comprehension to retain all the numbers in <code class="literal">v</code> that are positive, discarding zeros and negative ones. I hope, by now, any further explanation of the code would be insulting.</p><p>What is interesting, though, is to start thinking about how we can test it. Well, how about we call <code class="literal">filter_ints</code> with a list of numbers and we make sure that <code class="literal">is_positive</code> is called for each of them? Of course, we would have to test <code class="literal">is_positive</code> as well, but I will show <a id="id501" class="indexterm"/>you later on how to do that. Let's write a simple test for <code class="literal">filter_ints</code> now.</p><div><div><h3 class="title"><a id="note48"/>Note</h3><p>Just to be sure we're on the same page, I am putting the code for this chapter in a folder called <code class="literal">ch7</code>, which lies within the root of our project. At the same level of <code class="literal">ch7</code>, I have created a folder called <code class="literal">tests</code>, in which I have placed a folder called <code class="literal">test_ch7</code>. In this folder I have one test file, called <code class="literal">test_filter_func.py</code>.</p><p>Basically, within the <code class="literal">tests</code> folder, I will recreate the tree structure of the code I'm testing, prepending everything with <code class="literal">test_</code>. This way, finding tests is really easy, as well as is keeping them tidy.</p></div></div><p>
<code class="literal">tests/test_ch7/test_filter_funcs.py</code>
</p><div><pre class="programlisting">from unittest import <strong>TestCase</strong>  # 1
from <strong>unittest.mock</strong> import <strong>patch</strong>, <strong>call</strong>  # 2
from nose.tools import <strong>assert_equal</strong>  # 3
from ch7.filter_funcs import filter_ints  # 4

class FilterIntsTestCase(TestCase):  # 5

    <strong>@patch</strong>('ch7.filter_funcs.<strong>is_positive</strong>')  # 6
    def test_filter_ints(self, <strong>is_positive_mock</strong>):  # 7
        # preparation
        v = [3, -4, 0, 5, 8]

        # execution
        filter_ints(v)  # 8

        # verification
        assert_equal(
            [call(3), call(-4), call(0), call(5), call(8)],
            <strong>is_positive_mock.call_args_list</strong>
        )  # 9</pre></div><p>My, oh my, so little code, and yet so much to say. First of all: <code class="literal">#1</code>. The <code class="literal">TestCase</code> class is the base class that we use to have a contained entity in which to run our tests. It's not just a bare container; it provides you with methods to write tests more easily.</p><p>On <code class="literal">#2</code>, we import <code class="literal">patch</code> and <code class="literal">call</code> from the <code class="literal">unittest.mock</code> module. <code class="literal">patch</code> is responsible for substituting an object with a <code class="literal">Mock</code> instance, thereby giving us the ability to check on it after the execution phase has been completed. <code class="literal">call</code> provides us with a nice way of expressing a (for example, function) call.</p><p>On <code class="literal">#3</code>, you <a id="id502" class="indexterm"/>can see that I prefer to use assertions from <code class="literal">nose</code>, rather than the ones that come with the <code class="literal">unittest</code> module. To give you an example, <code class="literal">assert_equal(...)</code> would become <code class="literal">self.assertEqual(...)</code> if I didn't use <code class="literal">nose</code>. I don't enjoy typing <code class="literal">self.</code> for any assertion, if there is a way to avoid it, and I don't particularly enjoy <strong>camel case</strong>, therefore I always prefer to use <code class="literal">nose</code> to make my assertions.</p><p>
<code class="literal">assert_equal</code> is a function that takes two parameters (and an optional third one that acts as a message) and verifies that they are the same. If they are equal, nothing happens, but if they differ, then an <code class="literal">AssertionError</code> exception is raised, telling us something is wrong. When I write my tests, I always put the expected value as the first argument, and the real one as the second. This convention saves me time when I'm reading tests.</p><p>On <code class="literal">#4</code>, we import the function we want to test, and then (<code class="literal">#5</code>) we proceed to create the class where our tests will live. Each method of this class starting with <code class="literal">test_</code>, will be interpreted as a test. As you can see, we need to decorate <code class="literal">test_filter_ints</code> with <code class="literal">patch</code> (<code class="literal">#6</code>). Understanding this part is crucial, we need to patch the object where it is actually used. In this case, the path is very simple: <code class="literal">ch7.filter_func.is_positive</code>.</p><div><div><h3 class="title"><a id="tip15"/>Tip</h3><p>Patching can <a id="id503" class="indexterm"/>be very tricky, so I urge you to read the <em>Where to patch</em> section in the mock documentation: <a class="ulink" href="https://docs.python.org/3/library/unittest.mock.html#where-to-patch">https://docs.python.org/3/library/unittest.mock.html#where-to-patch</a>.</p></div></div><p>When we decorate a function using <code class="literal">patch</code>, like in our example, we get an extra argument in the test signature (<code class="literal">#7</code>), which I like to call as the patched function name, plus a <code class="literal">_mock</code> suffix, just to make it clear that the object has been patched (or mocked out).).</p><p>Finally, we get to the body of the test, and we have a very simple preparation phase in which we set up a list with at least one representative of all the integer number categories (negative, zero, and positive).</p><p>Then, in <code class="literal">#8</code>, we perform the execution phase, which runs the <code class="literal">filter_ints</code> function, without collecting its results. If all has gone as expected, the fake <code class="literal">is_positive</code> function must have been called with each of the integers in <code class="literal">v</code>.</p><p>We can verify this by comparing a list of call objects to the <code class="literal">call_args_list</code> attribute on the mock (<code class="literal">#9</code>). This attribute is the list of all the calls performed on the object since its creation.</p><p>Let's run this test. First of all, make sure that you install <code class="literal">nose</code> (<code class="literal">$ pip freeze</code> will tell you if you have it already):</p><div><pre class="programlisting">
<strong>$ pip install nose</strong>
</pre></div><p>Then, change into the root of the project (mine is called <code class="literal">learning.python</code>), and run the tests like this:</p><div><pre class="programlisting">
<strong>$ nosetests tests/test_ch7/</strong>
<strong>.</strong>
<strong>------------------------------------------------------------</strong>
<strong>Ran 1 test in 0.006s</strong>
<strong>OK</strong>
</pre></div><p>The<a id="id504" class="indexterm"/> output shows one dot (each dot is a test), a separation line, and the time taken to run the whole test suite. It also says <code class="literal">OK</code> at the end, which means that our tests were all successful.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec20"/>Making a test fail</h3></div></div></div><p>Good, so just<a id="id505" class="indexterm"/> for fun let's make one fail. In the test file, change the last call from <code class="literal">call(8)</code> to <code class="literal">call(9)</code>, and run the tests again:</p><div><pre class="programlisting">
<strong>$ nosetests tests/test_ch7/</strong>
<strong>F</strong>
<strong>============================================================</strong>
<strong>FAIL: test_filter_ints (test_filter_funcs.FilterIntsTestCase)</strong>
<strong>------------------------------------------------------------</strong>
<strong>Traceback (most recent call last):</strong>
<strong>  File "/usr/lib/python3.4/unittest/mock.py", line 1125, in patched</strong>
<strong>    return func(*args, **keywargs)</strong>
<strong>  File "/home/fab/srv/learning.python/tests/test_ch7/test_filter_funcs.py", line 21, in test_filter_ints</strong>
<strong>    is_positive_mock.call_args_list</strong>
<strong>AssertionError: [call(3), call(-4), call(0), call(5), call(9)] != [call(3), call(-4), call(0), call(5), call(8)]</strong>
<strong>------------------------------------------------------------</strong>
<strong>Ran 1 test in 0.008s</strong>
<strong>FAILED (failures=1)</strong>
</pre></div><p>Wow, we made the beast angry! So much wonderful information, though. This tells you that the test <code class="literal">test_filter_ints</code> (with the path to it), was run and that it failed (the big <code class="literal">F</code> at the top, where the dot was before). It gives you a <code class="literal">Traceback</code>, that tells you that in the <code class="literal">test_filter_funcs.py</code> module, at line 21, when asserting on <code class="literal">is_positive_mock.call_args_list</code>, we have a discrepancy. The test expects the list of calls to end with a <code class="literal">call(9)</code> instance, but the real list ends with a <code class="literal">call(8)</code>. This is nothing less than wonderful.</p><p>If you<a id="id506" class="indexterm"/> have a test like this, can you imagine what would happen if you refactored and introduced a bug into your function by mistake? Well, your tests will break! They will tell you that <em>you have screwed something up, and here's the details</em>. So, you go and check out what you broke.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec21"/>Interface testing</h3></div></div></div><p>Let's add <a id="id507" class="indexterm"/>another test that checks on the returned value. It's another method in the class, so I won't reproduce the whole code again:</p><p>
<code class="literal">tests/test_ch7/test_filter_funcs.py</code>
</p><div><pre class="programlisting">def test_filter_ints_return_value(self):
    v = [3, -4, 0, -2, 5, 0, 8, -1]

    result = filter_ints(v)

    assert_list_equal([3, 5, 8], result)</pre></div><p>This test is a bit different from the previous one. Firstly, we cannot mock the <code class="literal">is_positive</code> function, otherwise we wouldn't be able to check on the result. Secondly, we don't check on calls, but only on the result of the function when input is given.</p><p>I like this test much more than the previous one. This type of test is called an <strong>interface test</strong> because it checks on the interface (the set of inputs and outputs) of the function we're testing. It doesn't use any mocks, which is why I use this technique much more than the previous one. Let's run the new test suite and then let's see why I like interface testing more than those with mocks.</p><div><pre class="programlisting">
<strong>$ nosetests tests/test_ch7/</strong>
<strong>..</strong>
<strong>------------------------------------------------------------</strong>
<strong>Ran 2 tests in 0.006s</strong>
<strong>OK</strong>
</pre></div><p>Two tests ran, all good (I changed that <code class="literal">9</code> back to an <code class="literal">8</code> in the first test, of course).</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec22"/>Comparing tests with and without mocks</h3></div></div></div><p>Now, let's <a id="id508" class="indexterm"/>see why I don't really like mocks and use them only<a id="id509" class="indexterm"/> when I have no choice. Let's refactor the code in this way:</p><p>
<code class="literal">filter_funcs_refactored.py</code>
</p><div><pre class="programlisting">def filter_ints(v):
    v = [num for num in v if num != 0]  # 1
    return [num for num in v if is_positive(num)]</pre></div><p>The code for <code class="literal">is_positive</code> is the same as before. But the logic in <code class="literal">filter_ints</code> has now changed in a way that <code class="literal">is_positive</code> will never be called with a <code class="literal">0</code>, since they are all filtered out in <code class="literal">#1</code>. This leads to an interesting result, so let's run the tests again:</p><div><pre class="programlisting">
<strong>$ nosetests tests/test_ch7/test_filter_funcs_refactored.py </strong>
<strong>F.</strong>
<strong>============================================================</strong>
<strong>FAIL: test_filter_ints (test_filter_funcs_refactored.FilterIntsTestCase)</strong>
<strong>------------------------------------------------------------</strong>
<strong>... omit ...</strong>
<strong>AssertionError: [call(3), call(-4), call(0), call(5), call(8)] != [call(3), call(-4), call(5), call(8)]</strong>
<strong>------------------------------------------------------------</strong>
<strong>Ran 2 tests in 0.002s</strong>
<strong>FAILED (failures=1)</strong>
</pre></div><p>One test<a id="id510" class="indexterm"/> succeeded but the other one, the one with the mocked <code class="literal">is_positive</code> function, failed. The <code class="literal">AssertionError</code> message shows us that we now need<a id="id511" class="indexterm"/> to amend the list of expected calls, removing <code class="literal">call(0)</code>, because it is no longer performed.</p><p>This is not good. We have changed neither the interface of the function nor its behavior. The function is still keeping to its <em>original contract</em>. What we've done by testing it with a mocked object is limit ourselves. In fact, we now have to amend the test in order to use the new logic.</p><p>This is just a simple example but it shows one important flaw in the whole mock mechanism. <em>You must keep your mocks up-to-date and in sync with the code they are replacing</em>, otherwise you risk having issues like the preceding one, or even worse. Your tests may not fail because they are using mocked objects that perform fine, but because the real ones, now not in sync any more, are actually failing.</p><p>So <em>use mocks only when necessary</em>, only when there is no other way of testing your functions. When you cross the boundaries of your application in a test, try to use a replacement, like a test database, or a fake API, and only when it's not possible, resort to mocks. They are very powerful, but also very dangerous when not handled properly.</p><p>So, let's remove that first test and keep only the second one, so that I can show you another issue you could run into when writing tests. The whole test module now looks like this:</p><p>
<code class="literal">tests/test_ch7/test_filter_funcs_final.py</code>
</p><div><pre class="programlisting">from unittest import TestCase
from nose.tools import assert_list_equal
from ch7.filter_funcs import filter_ints

class FilterIntsTestCase(TestCase):
    def test_filter_ints_return_value(self):
        v = [3, -4, 0, -2, 5, 0, 8, -1]
        result = filter_ints(v)
        assert_list_equal([3, 5, 8], result)</pre></div><p>If we run it, it will pass.</p><p>A brief chat about triangulation. Now let me ask you: what happens if I change my <code class="literal">filter_ints</code> function to this?</p><p>
<code class="literal">filter_funcs_triangulation.py</code>
</p><div><pre class="programlisting">def filter_ints(v):
    return [3, 5, 8]</pre></div><p>If you run<a id="id512" class="indexterm"/> the test suite, the test we have will still pass! You<a id="id513" class="indexterm"/> may think I'm crazy but I'm showing you this because I <a id="id514" class="indexterm"/>want to talk about a concept called <strong>triangulation</strong>, which is very important when doing interface testing with TDD.</p><p>The whole idea is to remove cheating code, or badly performing code, by pinpointing it from different angles (like going to one vertex of a triangle from the other two) in a way that makes it impossible for our code to cheat, and the bug is exposed. We can simply modify the test like this:</p><p>
<code class="literal">tests/test_ch7/test_filter_funcs_final_triangulation.py</code>
</p><div><pre class="programlisting">def test_filter_ints_return_value(self):
    v1 = [3, -4, 0, -2, 5, 0, 8, -1]
    v2 = [7, -3, 0, 0, 9, 1]

    assert_list_equal([3, 5, 8], filter_ints(v1))
    assert_list_equal([7, 9, 1], filter_ints(v2))</pre></div><p>I have moved the execution section in the assertions directly, and you can see that we're now pinpointing our function from two different angles, thereby requiring that the real code be in it. It's no longer possible for our function to cheat.</p><p>Triangulation is a very powerful technique that teaches us to always try to exercise our code from many different angles, to cover all possible edge cases to expose any problems.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec23"/>Boundaries and granularity</h3></div></div></div><p>Let's<a id="id515" class="indexterm"/> now <a id="id516" class="indexterm"/>add a test for the <code class="literal">is_positive</code> function. I know it's a one-liner, but it presents us with opportunity to discuss two very important concepts: <strong>boundaries</strong> and <strong>granularity</strong>.</p><p>That <code class="literal">0</code> in the body of the function is a <strong>boundary</strong>, the <code class="literal">&gt;</code> in the inequality is how we behave with <a id="id517" class="indexterm"/>regards to this boundary. Typically, when you set a boundary, you divide the space into three areas: what lies before the boundary, after the boundary, and on the boundary itself. In the example, before the boundary we find the negative numbers, the boundary is the element <code class="literal">0</code> and, after the boundary, we find the positive numbers. We need to test each of these areas to be sure we're testing the function correctly. So, let's see one possible solution (I will add the test to the class, but I won't show the repeated code):</p><p>
<code class="literal">tests/test_ch7/test_filter_funcs_is_positive_loose.py</code>
</p><div><pre class="programlisting">def test_is_positive(self):
    assert_equal(False, is_positive(-2))  # before boundary
    assert_equal(False, is_positive(0))  # on the boundary
    assert_equal(True, is_positive(2))  # after the boundary</pre></div><p>You can see that we are exercising one number for each different area around the boundary. Do you think this test is good? Think about it for a minute before reading on.</p><p>The answer is no, this test is not good. Not good enough, anyway. If I change the body of the <code class="literal">is_positive</code> function to read <code class="literal">return n &gt; 1</code>, I would expect my test to fail, but it won't. <code class="literal">-2</code> is still <code class="literal">False</code>, as well as <code class="literal">0</code>, and <code class="literal">2</code> is still <code class="literal">True</code>. Why does that happen? It is because we haven't taken granularity properly into account. We're dealing with integers, so what is the minimum granularity when we move from one integer to the next one? It's 1. Therefore, when we surround the boundary, taking all three areas into account is not enough. We need to do it with the minimum possible granularity. Let's change the test:</p><p>
<code class="literal">tests/test_ch7/test_filter_funcs_is_positive_correct.py</code>
</p><div><pre class="programlisting">def test_is_positive(self):
    assert_equal(False, is_positive(-1))
    assert_equal(False, is_positive(0))
    assert_equal(True, is_positive(1))</pre></div><p>Ah, now it's better. Now if we change the body of <code class="literal">is_positive</code> to read <code class="literal">return n &gt; 1</code>, the third assertion will fail, which is what we want. Can you think of a better test?</p><p>
<code class="literal">tests/test_ch7/test_filter_funcs_is_positive_better.py</code>
</p><div><pre class="programlisting">def test_is_positive(self):
    assert_equal(False, is_positive(0))
    for n in range(1, 10 ** 4):
        assert_equal(False, is_positive(-n))
        assert_equal(True, is_positive(n))</pre></div><p>This test is even better. We test the first ten thousand integers (both positive and negative) and 0. It basically provides us with a better coverage than just the one across the boundary. So, keep<a id="id518" class="indexterm"/> this in mind. Zoom closely around each boundary <a id="id519" class="indexterm"/>with minimal granularity, but try to expand as well, finding a good compromise between optimal coverage and execution speed. We would love to check the first billion integers, but we can't wait days for our tests to run.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec24"/>A more interesting example</h3></div></div></div><p>Okay, this <a id="id520" class="indexterm"/>was as gentle an introduction as I could give you, so let's move on to something more interesting. Let's write and test a function that flattens a nested dictionary structure. For a couple of years, I have worked very closely with Twitter and Facebook APIs. Handling such humongous data structures is not easy, especially since they're often deeply nested. It turns out that it's much easier to flatten them in a way that you can work on them without losing the original structural information, and then recreate the nested structure from the flat one. To give you an example, we want something like this:</p><p>
<code class="literal">data_flatten.py</code>
</p><div><pre class="programlisting">
<strong>nested</strong> = {
    'fullname': 'Alessandra',
    'age': 41,
    'phone-numbers': ['+447421234567', '+447423456789'],
    'residence': {
        'address': {
            'first-line': 'Alexandra Rd',
            'second-line': '',
        },
        'zip': 'N8 0PP',
        'city': 'London',
        'country': 'UK',
    },
}

<strong>flat</strong> = {
    'fullname': 'Alessandra',
    'age': 41,
    'phone-numbers': ['+447421234567', '+447423456789'],
    'residence.address.first-line': 'Alexandra Rd',
    'residence.address.second-line': '',
    'residence.zip': 'N8 0PP',
    'residence.city': 'London',
    'residence.country': 'UK',
}</pre></div><p>A structure like <code class="literal">flat</code> is much simpler to manipulate. Before writing the flattener, let's make some assumptions: the keys are strings, we leave every data structure as it is unless it's a dictionary, in which case we flatten it, we use the dot as separator, but we want to be able to pass a different one to our function. Here's the code:</p><p>
<code class="literal">data_flatten.py</code>
</p><div><pre class="programlisting">def flatten(data, prefix='', separator='.'):
    """Flattens a nested dict structure. """
    if not isinstance(data, dict):
        return {prefix: data} if prefix else data

    result = {}
    for (key, value) in data.items():
        result.update(
            flatten(
                value,
                _get_new_prefix(prefix, key, separator),
                separator=separator))
    return result

def _get_new_prefix(prefix, key, separator):
    return (separator.join(<strong>(</strong>prefix, str(key)<strong>)</strong>)
            if prefix else str(key))</pre></div><p>The<a id="id521" class="indexterm"/> preceding example is not difficult, but also not trivial so let's go through it. At first, we check if <code class="literal">data</code> is a dictionary. If it's not a dictionary, then it's data that doesn't need to be flattened; therefore, we simply return either <code class="literal">data</code> or, if <code class="literal">prefix</code> is not an empty string, a dictionary with one key/value pair: <code class="literal">prefix</code>/<code class="literal">data</code>.</p><p>If instead <code class="literal">data</code> is a dict, we prepare an empty <code class="literal">result</code> dict to return, then we parse the list of <code class="literal">data</code>'s items, which, at I'm sure you will remember, are 2-tuples <em>(key, value)</em>. For each <em>(key, value)</em> pair, we recursively call <code class="literal">flatten</code> on them, and we update the <code class="literal">result</code> dict with what's returned by that call. Recursion is excellent when running through nested structures.</p><p>At a glance, can you understand what the <code class="literal">_get_new_prefix</code> function does? Let's use the inside-out technique once again. I see a ternary operator that returns the stringified <code class="literal">key</code> when <code class="literal">prefix</code> is an empty string. On the other hand, when <code class="literal">prefix</code> is a non-empty string, we use the <code class="literal">separator</code> to <code class="literal">join</code> the <code class="literal">prefix</code> with the stringified version of <code class="literal">key</code>. Notice that the braces inside the call to <code class="literal">join</code> aren't redundant, we need them. Can you figure out why?</p><p>Let's write a couple of tests for this function:</p><p>
<code class="literal">tests/test_ch7/test_data_flatten.py</code>
</p><div><pre class="programlisting"># ... imports omitted ...
class FlattenTestCase(TestCase):

    def test_flatten(self):
        test_cases = [
            (<strong>{'A': {'B': 'C', 'D': [1, 2, 3], 'E': {'F': 'G'}},</strong>
<strong>              'H': 3.14,</strong>
<strong>              'J': ['K', 'L'],</strong>
<strong>              'M': 'N'}</strong>,
             {'A.B': 'C',
              'A.D': [1, 2, 3],
              'A.E.F': 'G',
              'H': 3.14,
              'J': ['K', 'L'],
              'M': 'N'}),
            (<strong>0</strong>, 0),
            (<strong>'Hello'</strong>, 'Hello'),
            (<strong>{'A': None}</strong>, {'A': None}),
        ]
        for (<strong>nested</strong>, flat) in test_cases:
            <strong>assert_equal(flat, flatten(nested))</strong>

    def test_flatten_custom_separator(self):
        nested = {'A': {'B': {'C': 'D'}}}
        assert_equal(
            {'A#B#C': 'D'}, flatten(nested, separator='#'))</pre></div><p>Let's start<a id="id522" class="indexterm"/> from <code class="literal">test_flatten</code>. I defined a list of 2-tuples <code class="literal">(nested, flat)</code>, each of which represents a test case (I highlighted <code class="literal">nested</code> to ease reading). I have one big dict with three levels of nesting, and then some smaller data structures that won't change when passed to the <code class="literal">flatten</code> function. These test cases are probably not enough to cover all edge cases, but they should give you a good idea of how you could structure a test like this. With a simple <code class="literal">for</code> loop, I cycle through each test case and assert that the result of <code class="literal">flatten(nested)</code> is equal to <code class="literal">flat</code>.</p><div><div><h3 class="title"><a id="tip16"/>Tip</h3><p>One thing to say about this example is that, when you run it, it will show you that two tests have been run. This is actually not correct because even if technically there were only two tests running, in one of them we have multiple test cases. It would be nicer to have them run in a way that they were recognized as separate. This<a id="id523" class="indexterm"/> is possible through the use of libraries such as <code class="literal">nose-parameterized</code>, which I encourage you to check out. It's on <a class="ulink" href="https://pypi.python.org/pypi/nose-parameterized">https://pypi.python.org/pypi/nose-parameterized</a>.</p></div></div><p>I also provided a second test to make sure the custom separator feature worked. As you can see, I used only one data structure, which is much smaller. We don't need to go big again, nor to test other edge cases. Remember, tests should make sure of one thing and one thing only, and <code class="literal">test_flatten_custom_separator</code> just takes care of verifying whether or not we can feed the <code class="literal">flatten</code> function a different <code class="literal">separator</code>.</p><p>I could keep blathering on about tests for about another book if only I had the space, but unfortunately, we need to stop here. I haven't told you about <strong>doctests</strong> (tests written in the documentation using a Python interactive shell style), and about another half a million things that could be said about this subject. You'll have to discover that for yourself.</p><p>Take a look at the documentation for the <code class="literal">unittest</code> module, the <code class="literal">nose</code> and <code class="literal">nose-parameterized</code> libraries, and <code class="literal">pytest</code> (<a class="ulink" href="http://pytest.org/">http://pytest.org/</a>), and you will be fine. In my experience, mocking and patching seem to be quite hard to get a good grasp of for developers <a id="id524" class="indexterm"/>who<a id="id525" class="indexterm"/> are new to them, so allow yourself a little time to digest these techniques. Try and learn them gradually.</p></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec64"/>Test-driven development</h1></div></div></div><p>Let's talk<a id="id526" class="indexterm"/> briefly about <strong>test-driven development</strong> or <strong>TDD</strong>. It is a methodology that was rediscovered by Kent Beck, who wrote <em>Test Driven Development by Example</em>, <em>Addison Wesley – 2002</em>, which I encourage you to check out if you want to learn about the fundamentals of this subject, which I'm quite obsessed with.</p><div><blockquote class="blockquote"><p><em>TDD is a software development methodology that is based on the continuous repetition of a very short development cycle.</em></p></blockquote></div><p>At first, the developer writes a test, and makes it run. The test is supposed to check a feature that is not yet part of the code. Maybe is a new feature to be added, or something to be removed or amended. Running<a id="id527" class="indexterm"/> the test will make it fail and, because of this, this phase is called <strong>Red</strong>.</p><p>When the test has failed, the developer writes the minimal amount of code to make it pass. When running the test succeeds, we have the so-called <strong>Green</strong> phase. In this phase, it is okay to<a id="id528" class="indexterm"/> write code that cheats, just to make the test pass (that's why you would then use triangulation). This technique is called, <em>fake it 'til you make it</em>.</p><p>The last piece of this cycle is where the developer takes care of both the code and the tests (in separate times) and refactors them until they are in the desired state. This last phase is <a id="id529" class="indexterm"/>called <strong>Refactor</strong>.</p><p>The <strong>TDD mantra</strong> therefore recites, <strong>Red-Green-Refactor</strong>.</p><p>At first, it feels really weird to write tests before the code, and I must confess it took me a while to get used to it. If you stick to it, though, and force yourself to learn this slightly counter-intuitive way of working, at some point something almost magical happens, and you will see the quality of your code increase in a way that wouldn't be possible otherwise.</p><p>When you write your code before the tests, you have to take care of <em>what</em> the code has to do and <em>how</em> it has to do it, both at the same time. On the other hand, when you write tests before the code, you can concentrate on the <em>what</em> part alone, while you write them. When you write the code afterwards, you will mostly have to take care of <em>how</em> the code has to implement <em>what</em> is required by the tests. This shift in focus allows your mind to concentrate on the <em>what</em> and <em>how</em> parts in separate moments, yielding a brain power boost that will <a id="id530" class="indexterm"/>surprise you.</p><p>There <a id="id531" class="indexterm"/>are several other benefits that come from the adoption of this technique:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>You will refactor with much more confidence</strong>: Because when you touch your code you know that if you screw things up, you will break at least one test. Moreover, you will be able to take care of the architectural design in the refactor phase, where having tests that act as guardians will allow you to enjoy massaging the code until it reaches a state that satisfies you.</li><li class="listitem" style="list-style-type: disc"><strong>The code will be more readable</strong>: This is crucial in our time, when coding is a social activity and every professional developer spends much more time reading code than writing it.</li><li class="listitem" style="list-style-type: disc"><strong>The code will be more loose-coupled and easier to test and maintain</strong>: This is simply because writing the tests first forces you to think more deeply about its structure.</li><li class="listitem" style="list-style-type: disc"><strong>Writing tests first requires you to have a better understanding of the business requirements</strong>: This is fundamental in delivering what was actually asked for. If your understanding of the requirements is lacking information, you'll find writing a test extremely challenging and this situation acts as a sentinel for you.</li><li class="listitem" style="list-style-type: disc"><strong>Having everything unit tested means the code will be easier to debug</strong>: Moreover, small tests are perfect for providing alternative documentation. English can be misleading, but five lines of Python in a simple test are very hard to be misunderstood.</li><li class="listitem" style="list-style-type: disc"><strong>Higher speed</strong>: It's faster to write tests and code than it is to write the code first and then lose time debugging it. If you don't write tests, you will probably deliver the code sooner, but then you will have to track the bugs down and solve them (and, rest assured, there will be bugs). The combined time taken to write the code and then debug it is usually longer than the time taken to develop the code with TDD, where having tests running before the code is written, ensuring that the amount of bugs in it will be much lower than in the other case.</li></ul></div><p>On the <a id="id532" class="indexterm"/>other hand, the main shortcomings of this technique are:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>The whole company needs to believe in it</strong>: Otherwise you will have to constantly argue with your boss, who will not understand why it takes you so long to deliver. The truth is, it may take you a bit longer to deliver in the short term, but in the long term you gain a lot with TDD. However, it is quite hard to see the long term because it's not under our noses like the short term is. I have fought battles with stubborn bosses in my career, to be able to code using TDD. Sometimes it has been painful, but always well worth it, and I have never regretted it because, in the end, the quality of the result has always been appreciated.</li><li class="listitem" style="list-style-type: disc"><strong>If you fail to understand the business requirements, this will reflect in the tests you write, and therefore it will reflect in the code too</strong>: This kind of problem is quite hard to spot until you do UAT, but one thing that you can do to reduce the likelihood of it happening is to pair with another developer. Pairing will inevitably require discussions about the business requirements, and this will help having a better idea about them before the tests are written.</li><li class="listitem" style="list-style-type: disc"><strong>Badly written tests are hard to maintain</strong>: This is a fact. Tests with too many mocks or with extra assumptions or badly structured data will soon become a burden. Don't let this discourage you; just keep experimenting and change the way you write them until you find a way that doesn't require you a huge amount of work every time you touch your code.</li></ul></div><p>I'm so passionate about TDD that when I interview for a job, I always ask if the company I'm about to join adopts it. If the answer is no, it's kind of a deal-breaker for me. I encourage you to check it out and use it. Use it until you feel something clicking in your mind. You won't regret it, I promise.</p></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec65"/>Exceptions</h1></div></div></div><p>Even<a id="id533" class="indexterm"/> though I haven't formally introduced them to you, by now I expect you to at least have a vague idea of what an <strong>exception</strong> is. In the previous chapters, we've seen that when an iterator is exhausted, calling <code class="literal">next</code> on it raises a <code class="literal">StopIteration</code> exception. We've met <code class="literal">IndexError</code> when we tried accessing a list at a position that was outside the valid range. We've also met <code class="literal">AttributeError</code> when we tried accessing an attribute on an object that didn't have it, and <code class="literal">KeyError</code> when we did the same with a key and a dictionary. We've also just met <code class="literal">AssertionError</code> when running tests.</p><p>Now, the time has come for us to talk about exceptions.</p><p>Sometimes, even though an operation or a piece of code is correct, there are conditions in which something may go wrong. For example, if we're converting user input from <code class="literal">string</code> to <code class="literal">int</code>, the user could accidentally type a letter in place of a digit, making it impossible for us to convert that value into a number. When dividing numbers, we may not know in advance if we're attempting a division by zero. When opening a file, it could be missing or corrupted.</p><p>When an error is detected during execution, it is called an <strong>exception</strong>. Exceptions are not necessarily lethal; in fact, we've seen that <code class="literal">StopIteration</code> is deeply integrated in Python generator and iterator mechanisms. Normally, though, if you don't take the necessary precautions, an exception will cause your application to break. Sometimes, this is the desired behavior but in other cases, we want to prevent and control problems such as these. For <a id="id534" class="indexterm"/>example, we may alert the user that the file they're trying to open is corrupted or that it is missing so that they can either fix it or provide another file, without the need for the application to die because of this issue. Let's see an example of a few exceptions:</p><p>
<code class="literal">exceptions/first.example.py</code>
</p><div><pre class="programlisting">&gt;&gt;&gt; gen = (n for n in range(2))
&gt;&gt;&gt; next(gen)
0
&gt;&gt;&gt; next(gen)
1
&gt;&gt;&gt; next(gen)
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
<strong>StopIteration</strong>
&gt;&gt;&gt; print(undefined_var)
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
<strong>NameError</strong>: name 'undefined_var' is not defined
&gt;&gt;&gt; mylist = [1, 2, 3]
&gt;&gt;&gt; mylist[5]
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
<strong>IndexError</strong>: list index out of range
&gt;&gt;&gt; mydict = {'a': 'A', 'b': 'B'}
&gt;&gt;&gt; mydict['c']
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
<strong>KeyError</strong>: 'c'
&gt;&gt;&gt; 1 / 0
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
<strong>ZeroDivisionError</strong>: division by zero</pre></div><p>As you can see, the Python shell is quite forgiving. We can see the <code class="literal">Traceback</code>, so that we have information about the error, but the program doesn't die. This is a special behavior, a regular program or a script would normally die if nothing were done to handle exceptions.</p><p>To handle an exception, Python gives you the <code class="literal">try</code> statement. What happens when you enter the <code class="literal">try</code> clause is that Python will watch out for one or more different types of exceptions (according to how you instruct it), and if they are raised, it will allow you to react. The <code class="literal">try</code> statement is comprised of the <code class="literal">try</code> clause, which opens the statement; one or more <code class="literal">except</code> clauses (all optional) that define what to do when an exception is caught; an <code class="literal">else</code> clause (optional), which is executed when the <code class="literal">try</code> clause is exited without any exception<a id="id535" class="indexterm"/> raised; and a <code class="literal">finally</code> clause (optional), whose code is executed regardless of whatever happened in the other clauses. The <code class="literal">finally</code> clause is typically used to clean up resources. Mind the order, it's important. Also, <code class="literal">try</code> must be followed by at least one <code class="literal">except</code> clause or a <code class="literal">finally</code> clause. Let's see an example:</p><p>
<code class="literal">exceptions/try.syntax.py</code>
</p><div><pre class="programlisting">def try_syntax(numerator, denominator):
    <strong>try</strong>:
        print('In the try block: {}/{}'
              .format(numerator, denominator))
        result = numerator / denominator
    <strong>except</strong> ZeroDivisionError as zde:
        print(zde)
    <strong>else</strong>:
        print('The result is:', result)
        return result
    <strong>finally</strong>:
        print('Exiting')

print(try_syntax(12, 4))
print(try_syntax(11, 0))</pre></div><p>The preceding example defines a simple <code class="literal">try_syntax</code> function. We perform the division of two numbers. We are prepared to catch a <code class="literal">ZeroDivisionError</code> exception if we call the function with <code class="literal">denominator = 0</code>. Initially, the code enters the <code class="literal">try</code> block. If <code class="literal">denominator</code> is not <code class="literal">0</code>, <code class="literal">result</code> is calculated and the execution, after leaving the <code class="literal">try</code> block, resumes in the <code class="literal">else</code> block. We print <code class="literal">result</code> and return it. Take a look at the output and you'll notice that just before returning <code class="literal">result</code>, which is the exit point of the function, Python executes the <code class="literal">finally</code> clause.</p><p>When <code class="literal">denominator</code> is <code class="literal">0</code>, things change. We enter the <code class="literal">except</code> block and print <code class="literal">zde</code>. The <code class="literal">else</code> block isn't executed because an exception was raised in the <code class="literal">try</code> block. Before (implicitly) returning <code class="literal">None</code>, we still execute the <code class="literal">finally</code> block. Take a look at the output and see if it makes sense to you:</p><div><pre class="programlisting">
<strong>$ python exceptions/try.syntax.py </strong>
<strong>In the try block: 12/4</strong>
<strong>The result is: 3.0</strong>
<strong>Exiting</strong>
<strong>3.0</strong>
<strong>In the try block: 11/0</strong>
<strong>division by zero</strong>
<strong>Exiting</strong>
<strong>None</strong>
</pre></div><p>When you<a id="id536" class="indexterm"/> execute a <code class="literal">try</code> block, you may want to catch more than one exception. For example, when trying to decode a JSON object, you may incur into <code class="literal">ValueError</code> for malformed JSON, or <code class="literal">TypeError</code> if the type of the data you're feeding to <code class="literal">json.loads()</code> is not a string. In this case, you may structure your code like this:</p><p>
<code class="literal">exceptions/json.example.py</code>
</p><div><pre class="programlisting">import json
json_data = '{}'
try:
    data = json.loads(json_data)
except (ValueError, TypeError) as e:
    print(type(e), e)</pre></div><p>This code will catch both <code class="literal">ValueError</code> and <code class="literal">TypeError</code>. Try changing <code class="literal">json_data = '{}'</code> to <code class="literal">json_data = 2</code> or <code class="literal">json_data = '{{'</code>, and you'll see the different output.</p><div><div><h3 class="title"><a id="note49"/>Note</h3><p>
<strong>JSON</strong> stands for <strong>JavaScript Object Notation</strong> and it's an open standard format that <a id="id537" class="indexterm"/>uses human-readable text to transmit data objects consisting of key/value pairs. It's an exchange format widely used when moving data across applications, especially when data needs to be treated in a language or platform-agnostic way.</p></div></div><p>If you want to handle multiple exceptions differently, you can just add more <code class="literal">except</code> clauses, like this:</p><p>
<code class="literal">exceptions/multiple.except.py</code>
</p><div><pre class="programlisting">try:
    # some code
except Exception1:
    # react to Exception1
except (Exception2, Exception3):
    # react to Exception2 and Exception3
except Exception3:
    # react to Exception3
...</pre></div><p>Keep in mind that an exception is handled in the first block that defines that exception class or any of its bases. Therefore, when you stack multiple <code class="literal">except</code> clauses like we've just done, make sure that you put specific exceptions at the top and generic ones at the bottom. In OOP terms, children on top, grandparents at the bottom. Moreover, remember that only one <code class="literal">except</code> handler is executed when an exception is raised.</p><p>You can also write <strong>custom exceptions</strong>. In order to do that, you just have to inherit from any other exception class. Python built-in exceptions are too many to be listed here, so I have to <a id="id538" class="indexterm"/>point <a id="id539" class="indexterm"/>you towards the official documentation. One important thing to know is that every Python exception derives from <code class="literal">BaseException</code>, but your custom exceptions should never inherit directly from that one. The reason for it is that handling such an exception will trap also <strong>system-exiting exceptions</strong> such <a id="id540" class="indexterm"/>as <code class="literal">SystemExit</code> and <code class="literal">KeyboardInterrupt</code>, which derive from <code class="literal">BaseException</code>, and this could lead to severe issues. In case of disaster, you want to be able to <em>Ctrl</em> + <em>C</em> your way out of an application.</p><p>You can easily solve the problem by inheriting from <code class="literal">Exception</code>, which inherits from <code class="literal">BaseException</code>, but doesn't include any system-exiting exception in its children because they are siblings in<a id="id541" class="indexterm"/> the built-in exceptions hierarchy (see <a class="ulink" href="https://docs.python.org/3/library/exceptions.html#exception-hierarchy">https://docs.python.org/3/library/exceptions.html#exception-hierarchy</a>).</p><p>Programming with exceptions can be very tricky. You could inadvertently silence out errors, or trap exceptions that aren't meant to be handled. Play it safe by keeping in mind a few guidelines: always put in the <code class="literal">try</code> clause only the code that may cause the exception(s) that you want to handle. When you write <code class="literal">except</code> clauses, be as specific as you can, don't just resort to <code class="literal">except Exception</code> because it's easy. Use tests to make sure your code handles edge cases in a way that requires the least possible amount of exception handling. Writing an <code class="literal">except</code> statement without specifying any exception would catch any exception, therefore exposing your code to the same risks you incur when you derive your custom exceptions from <code class="literal">BaseException</code>.</p><p>You will find information about exceptions almost everywhere on the web. Some coders use them abundantly, others sparingly (I belong to the latter category). Find your own way of dealing with them by taking examples from other people's source code. There's plenty<a id="id542" class="indexterm"/> of interesting projects whose sources are open, and you can find them <a id="id543" class="indexterm"/>on either GitHub (<a class="ulink" href="https://github.com">https://github.com</a>) or Bitbucket (<a class="ulink" href="https://bitbucket.org/">https://bitbucket.org/</a>).</p><p>Before we talk <a id="id544" class="indexterm"/>about <strong>profiling</strong>, let me show you an unconventional use of exceptions, just to give you something to help you expand your views on them. They are not just simply errors.</p><p>
<code class="literal">exceptions/for.loop.py</code>
</p><div><pre class="programlisting">n = 100
<strong>found = False</strong>
for a in range(n):
    <strong>if found: break</strong>
    for b in range(n):
        <strong>if found: break</strong>
        for c in range(n):
            if 42 * a + 17 * b + c == 5096:
                <strong>found = True</strong>
                print(a, b, c)  # 79 99 95</pre></div><p>The preceding code is quite a common idiom if you deal with numbers. You have to iterate over a few nested ranges and look for a particular combination of <code class="literal">a</code>, <code class="literal">b</code>, and <code class="literal">c </code>that satisfies a condition. In the example, condition is a trivial linear equation, but imagine something<a id="id545" class="indexterm"/> much cooler than that. What bugs me is having to check if the solution has been found at the beginning of each loop, in order to break out of them as fast as we can when it is. The break out logic interferes with the rest of the code and I don't like it, so I came up with a different solution for this. Take a look at it, and see if you can adapt it to other cases too.</p><p>
<code class="literal">e</code>
<code class="literal">xceptions/for.loop.py</code>
</p><div><pre class="programlisting">
<strong>class</strong> <strong>ExitLoopException(Exception):</strong>
<strong>    pass</strong>

<strong>try</strong>:
    n = 100
    for a in range(n):
        for b in range(n):
            for c in range(n):
                if 42 * a + 17 * b + c == 5096:
                    <strong>raise ExitLoopException(a, b, c)</strong>
<strong>except ExitLoopException as ele:</strong>
    print(ele)  # (79, 99, 95)</pre></div><p>Can you see how much more elegant it is? Now the breakout logic is entirely handled with a simple exception whose name even hints at its purpose. As soon as the result is found, we raise it, and immediately the control is given to the except clause which handles it. This is food for thought. This example indirectly shows you how to raise your own exceptions. Read up on the official documentation to dive into the beautiful details of this subject.</p></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec66"/>Profiling Python</h1></div></div></div><p>There<a id="id546" class="indexterm"/> are a few different ways to profile a Python application. Profiling means having the application run while keeping track of several different parameters, like the number of times a function is called, the amount of time spent inside it, and so on. Profiling can help us find the bottlenecks in our application, so that we can improve only what is really slowing us down.</p><p>If you take a look at the profiling section in the standard library official documentation, you will see that there are a couple of different implementations of the same profiling interface: <code class="literal">profile</code> and <code class="literal">cProfile</code>.</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">cProfile</code> is recommended for most users, it's a C extension with reasonable overhead that makes it suitable for profiling long-running programs</li><li class="listitem" style="list-style-type: disc"><code class="literal">profile</code> is a pure Python module whose interface is imitated by <code class="literal">cProfile</code>, but which adds significant overhead to profiled programs</li></ul></div><p>This<a id="id547" class="indexterm"/> interface does <strong>determinist profiling</strong>, which means that all function calls, function<a id="id548" class="indexterm"/> returns and exception events are monitored, and precise timings are made for the intervals between these events. Another<a id="id549" class="indexterm"/> approach, called <strong>statistical profiling</strong>, randomly samples the effective instruction pointer, and deduces where time is being spent.</p><p>The latter usually involves less overhead, but provides only approximate results. Moreover, because of the way the Python interpreter runs the code, deterministic profiling doesn't add that as much overhead as one would think, so I'll show you a simple example using <code class="literal">cProfile</code> from the command line.</p><p>We're going to calculate Pythagorean triples (I know, you've missed them...) using the following code:</p><p>
<code class="literal">profiling/triples.py</code>
</p><div><pre class="programlisting">def calc_triples(mx):
    triples = []
    for a in range(1, mx + 1):
        for b in range(a, mx + 1):
            hypotenuse = <strong>calc_hypotenuse</strong>(a, b)
            if <strong>is_int</strong>(hypotenuse):
                <strong>triples.append</strong>((a, b, int(hypotenuse)))
    return triples

def calc_hypotenuse(a, b):
    return (a**2 + b**2) ** .5

def is_int(n):  # n is expected to be a float
    return n.is_integer()

triples = calc_triples(1000)</pre></div><p>The script is extremely simple; we iterate over the interval [1, <em>mx</em>] with <code class="literal">a</code> and <code class="literal">b</code> (avoiding repetition of pairs by setting <code class="literal">b &gt;= a</code>) and we check if they belong to a right triangle. We use <code class="literal">calc_hypotenuse</code> to get <code class="literal">hypotenuse</code> for <code class="literal">a</code> and <code class="literal">b</code>, and then, with <code class="literal">is_int</code>, we check if it is an integer, which means (<em>a</em>, <em>b</em>, <em>c</em>) is a Pythagorean triple. When we profile this script, we get information in tabular form. The columns are <code class="literal">ncalls</code>, <code class="literal">tottime</code>, <code class="literal">percall</code>, <code class="literal">cumtime</code>, <code class="literal">percall</code>, and <code class="literal">filename:lineno(function)</code>. They represent the amount of calls we made to a function, how much time we spent in it, and so on. I'll trim a couple of columns<a id="id550" class="indexterm"/> to save space, so if you run the profiling yourself, don't worry if you get a different result.</p><div><pre class="programlisting">
<strong>$ python -m cProfile profiling/triples.py</strong>
<strong>1502538 function calls in 0.750 seconds</strong>
<strong>Ordered by: standard name</strong>
<strong>ncalls  tottime  percall filename:lineno(function)</strong>
<strong>500500    0.469    0.000 triples.py:14(calc_hypotenuse)</strong>
<strong>500500    0.087    0.000 triples.py:18(is_int)</strong>
<strong>     1    0.000    0.000 triples.py:4(&lt;module&gt;)</strong>
<strong>     1    0.163    0.163 triples.py:4(calc_triples)</strong>
<strong>     1    0.000    0.000 {built-in method exec}</strong>
<strong>  1034    0.000    0.000 {method 'append' of 'list' objects}</strong>
<strong>     1    0.000    0.000 {method 'disable' of '_lsprof.Profil...</strong>
<strong>500500    0.032    0.000 {method 'is_integer' of 'float' objects}</strong>
</pre></div><p>Even with this limited amount of data, we can still infer some useful information about this code. Firstly, we can see that the time complexity of the algorithm we have chosen grows with the square of the input size. The amount of times we get inside the inner loop body is exactly <em>mx (mx + 1) / 2</em>. We run the script with <code class="literal">mx = 1000</code>, which means we get <code class="literal">500500</code> times inside the inner <code class="literal">for</code> loop. Three main things happen inside that loop, we call <code class="literal">calc_hypotenuse</code>, we call <code class="literal">is_int</code> and, if the condition is met, we append to the <code class="literal">triples</code> list.</p><p>Taking a look at the profiling report, we notice that the algorithm has spent <code class="literal">0.469</code> seconds inside <code class="literal">calc_hypotenuse</code>, which is way more than the <code class="literal">0.087</code> seconds spent inside <code class="literal">is_int</code>, given that they were called the same number of times, so let's see if we can boost <code class="literal">calc_hypotenuse</code> a little.</p><p>As it turns out, we can. As I mentioned earlier on in the book, the power operator <code class="literal">**</code> is quite expensive, and in <code class="literal">calc_hypotenuse,</code> we're using it three times. Fortunately, we can easily transform two of those into simple multiplications, like this:</p><p>
<code class="literal">profiling/triples.py</code>
</p><div><pre class="programlisting">def calc_hypotenuse(a, b):
    return (<strong>a*a</strong> + <strong>b*b</strong>) ** .5</pre></div><p>This simple change should improve things. If we run the profiling again, we see that now the <code class="literal">0.469</code> is now down to <code class="literal">0.177</code>. Not bad! This means now we're spending only about 37% of the time inside <code class="literal">calc_hypotenuse</code> as we were before.</p><p>Let's see if we can improve <code class="literal">is_int</code> as well, by changing it like this:</p><p>
<code class="literal">profiling/triples.py</code>
</p><div><pre class="programlisting">def is_int(n):
    return n == int(n)</pre></div><p>This implementation is different and the advantage is that it also works when <code class="literal">n</code> is an integer. Alas, when we run the profiling against it, we see that the time taken inside the <code class="literal">is_int</code> function has gone up to <code class="literal">0.141</code> seconds. This means that it has roughly doubled, compared to what it was before. In this case, we need to revert to the previous implementation.</p><p>This <a id="id551" class="indexterm"/>example was trivial, of course, but enough to show you how one could profile an application. Having the amount of calls that are performed against a function helps us understand better the time complexity of our algorithms. For example, you wouldn't believe how many coders fail to see that those two <code class="literal">for</code> loops run proportionally to the square of the input size.</p><p>One thing to mention: depending on what system you're using, results may be different. Therefore, it's quite important to be able to profile software on a system that is as close as possible to the one the software is deployed on, if not actually on that one.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec96"/>When to profile?</h2></div></div></div><p>Profiling <a id="id552" class="indexterm"/>is super cool, but we need to know when it is appropriate to do it, and in what measure we need to address the results we get from it.</p><p>Donald Knuth once said that <em>premature optimization is the root of all evil</em> and, although I wouldn't have put it down so drastically, I do agree with him. After all, who am I to disagree with the man that gave us <em>The Art of Computer Programming</em>, <em>TeX</em>, and some of the coolest algorithms I have ever studied when I was a university student?</p><p>So, first and foremost: <em>correctness</em>. You want you code to deliver the result correctly, therefore write tests, find edge cases, and stress your code in every way you think makes sense. Don't be protective, don't put things in the back of your brain for later because you think they're not likely to happen. Be thorough.</p><p>Secondly, take care of coding <em>best practices</em>. Remember readability, extensibility, loose coupling, modularity, and design. Apply OOP principles: encapsulation, abstraction, single responsibility, open/closed, and so on. Read up on these concepts. They will open horizons for you, and they will expand the way you think about code.</p><p>Thirdly, <em>refactor like a beast!</em> The Boy Scouts Rule says to <em>Always leave the campground cleaner than you found it</em>. Apply this rule to your code.</p><p>And, finally, when all of the above has been taken care of, then and only then, you take care of profiling.</p><p>Run your profiler and identify bottlenecks. When you have an idea of the bottlenecks you need to address, start with the worst one first. Sometimes, fixing a bottleneck causes a ripple effect that will expand and change the way the rest of the code works. Sometimes this is only a little, sometimes a bit more, according to how your code was designed and implemented. Therefore, start with the biggest issue first.</p><p>One of the reasons Python is so popular is that it is possible to implement it in many different ways. So, if you find yourself having troubles boosting up some part of your code using <a id="id553" class="indexterm"/>sheer Python, nothing prevents you from rolling up your sleeves, buying a couple of hundred liters of coffee, and rewriting the slow piece of code in C. Guaranteed to be fun!</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec67"/>Summary</h1></div></div></div><p>In this chapter, we explored the world of testing, exceptions, and profiling.</p><p>I tried to give you a fairly comprehensive overview of testing, especially unit testing, which is the kind of testing that a developer mostly does. I hope I have succeeded in channeling the message that testing is not something that is perfectly defined and that you can learn from a book. You need to experiment with it a lot before you get comfortable. Of all the efforts a coder must make in terms of study and experimentation, I'd say testing is one of those that are most worth it.</p><p>We've briefly seen how we can prevent our program from dying because of errors, called exceptions, that happen at runtime. And, to steer away from the usual ground, I have given you an example of a somewhat unconventional use of exceptions to break out of nested <code class="literal">for</code> loops. That's not the only case, and I'm sure you'll discover others as you grow as a coder.</p><p>In the end, we very briefly touched base on profiling, with a simple example and a few guidelines. I wanted to talk about profiling for the sake of completeness, so at least you can play around with it.</p><p>We're now about to enter <a class="link" href="ch08.html" title="Chapter 8. The Edges – GUIs and Scripts">Chapter 8</a>, <em>The Edges – GUIs and Scripts</em>, where we're going to get our hands dirty with scripts and GUIs and, hopefully, come up with something interesting.</p><div><div><h3 class="title"><a id="note50"/>Note</h3><p>I am aware that I gave you a lot of pointers in this chapter, with no links or directions. I'm afraid this is by choice. As a coder, there won't be a single day at work when you won't have to look something up in a documentation page, in a manual, on a website, and so on. I think it's vital for a coder to be able to search effectively for the information they need, so I hope you'll forgive me for this extra training. After all, it's all for your benefit.</p></div></div></div></body></html>