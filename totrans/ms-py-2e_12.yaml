- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance – Tracking and Reducing Your Memory and CPU Usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we talk about performance, there is a quote by *Donald Knuth* you need
    to consider first:'
  prefs: []
  type: TYPE_NORMAL
- en: “The real problem is that programmers have spent far too much time worrying
    about efficiency in the wrong places and at the wrong times; premature optimization
    is the root of all evil (or at least most of it) in programming”.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Donald Knuth is often called the father of algorithm analysis. His book series,
    *The Art of Computer Programming*, can be considered the Bible of all fundamental
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: As long as you pick the correct data structures with the right algorithms, performance
    should not be something to worry about. That does not mean you should ignore performance
    entirely, but just make sure you pick the right battles and optimize only when
    it is actually needed. Micro/premature optimizations can definitely be fun, but
    are only very rarely useful.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen the performance characteristics of many data structures in *Chapter
    2*, *Pythonic Syntax and Common Pitfalls*, already, so we won’t discuss that,
    but we will show you how performance can be measured and how problems can be detected.
    There are cases where micro optimizations make a difference, but you won’t know
    until you measure the performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Profiling CPU usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling memory usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to correctly compare performance metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding and fixing memory leaks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Globally, the chapter is split between CPU usage and/or CPU time, and memory
    usage. The first half of the chapter mainly concerns CPU/time; the second half
    covers memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: What is performance?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance is a very broad term. It has many different meanings and, in many
    cases, it is defined incorrectly. Within this chapter, we will attempt to measure
    and improve performance in terms of CPU usage/time and memory usage. Many of the
    examples here are a trade-off between execution time and memory usage. Note that
    a fast algorithm that can only use a single CPU core can be outperformed in terms
    of execution time by a slower algorithm that is easily parallelizable given enough
    CPU cores.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to incorrect statements about performance, you have probably
    heard statements similar to “Language X is faster than Python.” That statement
    is inherently wrong. Python is neither fast nor slow; Python is a programming
    language, and a language has no performance metrics whatsoever. If you were to
    say that the CPython interpreter is faster or slower than interpreter Y for language
    X, that would be possible. The performance characteristics of code can vary greatly
    between different interpreters. Just take a look at this small test (which uses
    ZSH shell script):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Five different Python interpreters, each with a different performance! All are
    Python, but the interpreters obviously vary.
  prefs: []
  type: TYPE_NORMAL
- en: You might not have heard of the PyPy3 and Pyston interpreters yet.
  prefs: []
  type: TYPE_NORMAL
- en: The PyPy3 interpreter is an alternative Python interpreter that uses JIT (Just-In-Time)
    compiling to perform much better than CPython in many, but certainly not all,
    cases. The big caveat of PyPy3 is that code that has speedups in C and depends
    on CPython extensions (which is a large portion of performance-critical libraries)
    either does not support PyPy3 or suffers a performance hit.
  prefs: []
  type: TYPE_NORMAL
- en: Pyston attempts to be a drop-in replacement for CPython with JIT compiling added
    to it. While JIT compiling might be added to CPython pretty soon, as of Python
    3.10, that is not the case yet. This is why Pyston can offer a great performance
    benefit over CPython. The downside is that it is currently only supported on Unix/Linux
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at this benchmark, you might be tempted to drop the CPython interpreter
    completely and only use PyPy3\. The danger with benchmarks such as these is that
    they rarely offer any meaningful results. For this limited example, the Pypy interpreter
    was about 200 times faster than the CPython3.10 interpreter, but that has very
    little relevance for the general case. The only conclusion that can safely be
    drawn here is that this specific version of the PyPy3 interpreter is much faster
    than this specific version of CPython3 **for this exact test**. For any other
    test and interpreter version, the results could be vastly different.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring CPU performance and execution time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When talking about performance you can measure a great number of things. When
    it comes to CPU performance, we can measure:'
  prefs: []
  type: TYPE_NORMAL
- en: The “wall time” (the absolute time on the clock).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relative time (when comparing multiple runs or multiple functions)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Used CPU time. Due to multithreading, multiprocessing, or asynchronous processing,
    this can be vastly different from the wall time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When inspecting really low-level performance, measuring the number of CPU cycles
    and loop counts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to all these different measurement options, you should also consider
    the observer effect. Simply put, measuring takes time, and depending on how you
    are measuring the performance, the impact can be huge.
  prefs: []
  type: TYPE_NORMAL
- en: Within this section, we will be exploring several methods to inspect the CPU
    performance and execution time of your code. Tricks to improve your performance
    after measuring will come later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Timeit – comparing code snippet performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we can start improving execution/CPU times, we need a reliable method
    to measure them. Python has a really nice module (`timeit`) with the specific
    purpose of measuring the execution times of bits of code. It executes a bit of
    code many times to make sure there is as little variation as possible and to make
    the measurement fairly clean. It’s very useful if you want to compare a few code
    snippets. Following are some example executions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: These few examples demonstrate the performance difference between `list.insert`,
    `list.append`, a list comprehension, and the `list` function. As we have seen
    in *Chapter 4*, doing `list.insert` is very inefficient and that quickly shows
    here, in this case being 30 times slower than `list.append`.
  prefs: []
  type: TYPE_NORMAL
- en: 'More importantly, however, the code demonstrates how we can use the `timeit`
    module and how it works. As you can see in the output, the `list.append` variant
    was executed only `10` times, whereas the `list` call was executed `10000` times.
    That is one of the most convenient features of the `timeit` module: it automatically
    figures out some useful parameters for you, and it shows the “best of 3” to try
    and reduce the amount of variance in your tests.'
  prefs: []
  type: TYPE_NORMAL
- en: The `timeit` module is great at comparing the performance of similar bits of
    code within a code base. Comparing the execution time between different Python
    interpreters using `timeit` is generally useless because it is rarely representative
    of the performance of your whole application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Naturally, the command can be used with regular scripts as well, but that won’t
    automatically determine the number of repetitions like the command-line interface
    does. So we will have to do that ourselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'When executing this, you will get something along the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As you may have noticed, this script is still a bit basic. While the command-line
    version of `timeit` keeps trying until it reaches 0.2 seconds or more, this script
    just has a fixed number of executions. Since Python 3.6, we do have the option
    of using `timeit.Timer.autorange` to replicate this behavior, but it is a bit
    less convenient to use and would produce a lot more output in our current case.
    Depending on your use case, however, it could be useful to try this benchmark
    code instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to use `timeit` interactively, I would recommend using IPython,
    since it has a magic `%timeit` command that shows even more useful output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this case, IPython automatically takes care of the string wrapping and passing
    of `globals()`. Still, this is all very limited and useful only for comparing
    multiple methods of doing the same thing. When it comes to full Python applications,
    there are more methods available, as we will see later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: To view the source of both IPython functions and regular modules, entering `object??`
    in the IPython shell returns the source. In this case, just enter `timeit??` to
    view the `timeit` IPython function definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way you can implement a function similar to the `%timeit` function
    is to call `timeit.main`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This effectively does the same as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The internals of the `timeit` module are nothing too special, but take care
    to minimize a few sources of inaccuracy, such as the setup and the teardown code.
    Additionally, the module reports the fastest run because other processes on your
    system can interfere with the measurement.
  prefs: []
  type: TYPE_NORMAL
- en: 'A basic version can be implemented with a few calls to `time.perf_counter`
    (the highest resolution timer available in Python), which is also used by `timeit`
    internally. The `timeit.default_timer` function is simply a reference to `time.perf_counter`.
    This basic implementation of the `timeit` function is comparable to the internals
    of the `timeit` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The actual `timeit` code is a bit more advanced in terms of checking the input,
    but this example roughly shows how the `timeit.timeit` function can be implemented,
    including several of the features added for more precision:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we can see that the code has a `number` parameter that defaults to 1
    million. This has been done to reduce the result variance a little, as we will
    see when running the code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, the code disables the Python garbage collector so we don’t get any slowdowns
    from Python deciding to clean up its memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When we actually call this code, we will see why a high value for `number`
    can be important:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Even though we called the exact same code each time, the single repetition took
    more than two times as long in the first run and more than 10 times as long in
    the second run compared to the 1 million repetitions version. To make your results
    more consistent and reliable between runs, it is always good to repeat your tests
    several times and `timeit` can certainly help with that.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `timeit.repeat` function simply calls the `timeit.timeit` function several
    times and can be emulated using a list comprehension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now that we know how to test simple code statements, let’s look at how to find
    slow statements in our code.
  prefs: []
  type: TYPE_NORMAL
- en: cProfile – Finding the slowest components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `profile` and `cProfile` modules offer the exact same interface, but the
    latter is written in C and is much faster. I would recommend using `cProfile`
    if it is available on your system. If not, you can safely replace any occurrence
    of `cProfile` with `profile` in the following examples.
  prefs: []
  type: TYPE_NORMAL
- en: First profiling run
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s profile our Fibonacci function from *Chapter 6*, *Decorators – Enabling
    Code Reuse by Decorating*, both with and without the cache function. First, the
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: For the sake of readability, all `cProfile` statistics will be stripped of the
    `percall` columns in all `cProfile` outputs. These columns contain the duration
    per function call, which is irrelevant for these examples since they will be either
    0 or identical to the `cumtime` (cumulative time) column in nearly all cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll execute the function without cache:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We see `2692557` calls in total, which is quite a lot of calls. We called the
    `test_fibonacci` function nearly 3 million times. That is where the profiling
    modules provide a lot of insight. Let’s analyze the metrics a bit further, in
    the order they appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ncalls`: The number of calls that were made to the function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tottime`: The total time spent in this function, **excluding** the sub-functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`percall`: The time per call without sub-functions: `tottime / ncalls`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cumtime`: The total time spent in this function, **including** sub-functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`percall`: The time per call including sub-functions: `cumtime / ncalls`. This
    is distinct from the `percall` metric above, despite having the same name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Which is the most useful depends on your use case. It’s quite simple to change
    the sort order using the `-s` parameter within the default output. But now let’s
    see what the result is with the cached version. Once again, with stripped output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This time, we see a `tottime` of `0.000` because it’s just too fast to measure.
    But also, while the `fibonacci_cached` function is still the most executed function,
    it’s only being executed 31 times instead of 3 million times.
  prefs: []
  type: TYPE_NORMAL
- en: Calibrating your profiler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To illustrate the difference between `profile` and `cProfile`, let’s try the
    uncached run again with the `profile` module instead. Just a heads up: this is
    much slower, so don’t be surprised if it stalls a little:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The code now runs nearly 10 times more slowly, and the only difference is using
    the pure Python `profile` module instead of the `cProfile` module. This does indicate
    a big problem with the `profile` module. The overhead from the module itself is
    great enough to skew the results, which means we should account for that offset.
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s what the `Profile.calibrate()` function takes care of, as it calculates
    the performance bias incurred by the profile module. To calculate the bias, we
    can use the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The numbers will vary slightly, but you should be able to get a fair estimate
    of the performance bias that the `profile` module introduces to your code. It
    effectively runs a bit of code both with and without profiling enabled and calculates
    a multiplier to apply to all results so they are closer to the actual duration.
  prefs: []
  type: TYPE_NORMAL
- en: If the numbers still vary a lot, you can increase the trials from `100000` to
    something even larger.
  prefs: []
  type: TYPE_NORMAL
- en: Note that with many modern processors, the burst CPU performance (the first
    few seconds) can vary greatly from the sustained CPU performance (2 minutes or
    more).
  prefs: []
  type: TYPE_NORMAL
- en: The CPU performance is also highly temperature-dependent, so if your system
    has a large CPU cooler or is water-cooled, it can take up to 20 minutes at 100%
    CPU load before the CPU performance becomes consistent. The bias after that 20
    minutes would be completely unusable as a bias for a cold CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'This type of calibration only works for the `profile` module and should help
    a lot in achieving more accurate results. The bias can be set globally for all
    newly created profilers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Or for a specific `Profile` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that in general, a smaller bias is better to use than a large one because
    a large bias could cause very strange results. If the bias is large enough, you
    will even get negative timings. Let’s give it a try for our Fibonacci code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'While running it, it indeed appears that we’ve used a bias that’s too large:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Still, it shows how the code can be used properly. You can even incorporate
    the bias calculation within the script using a snippet like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Selective profiling using decorators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Calculating simple timings is easy enough using decorators, but profiling can
    show a lot more and can also be applied selectively using decorators or context
    wrappers. Let’s look at a `timer` and a `profiler` decorator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have created the decorators, we can profile and time our functions
    with them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The code is simple enough: just a basic `timer` and `profiler` decorator printing
    some default statistics. Which functions best for you depends on your use case,
    of course. The `timer()` decorator is very useful for quick performance tracking
    and/or a sanity check while developing. The `profiler()` decorator is great while
    you are actively working on the performance characteristics of a function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The added advantage of this selective profiling is that the output is more
    limited, which helps with readability, albeit still much more verbose than the
    `timer()` decorator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the profiler still makes the code about twice as slow, but it’s
    definitely usable.
  prefs: []
  type: TYPE_NORMAL
- en: Using profile statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To get slightly more interesting profiling results, we will profile using the
    `pyperformance.benchmarks.bm_float` script.
  prefs: []
  type: TYPE_NORMAL
- en: The `pyperformance` library is the official Python benchmarks library optimized
    for the CPython interpreter. It contains a large (ever-growing) list of benchmarks
    to monitor the performance of the CPython interpreter under many scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be installed through `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let’s create the statistics using this script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'When executing the script, you should get something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: After running the script, you should have a `bm_float.profile` file containing
    the profiling results. As we can see in the script, these statistics can be viewed
    through the `pstats` module.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, it can be interesting to combine the results from multiple measurements.
    That is possible by specifying multiple files or by using `stats.add(*filenames)`.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of saving these profile results to files is that several
    applications support this output and can visualize it in a clearer way. One option
    is SnakeViz, which uses your web browser to render the profile results interactively.
    Also, we have QCacheGrind, a very nice visualizer for profile statistics, but
    which requires some manual compiling to get running or some searching for binaries
    of course.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the output from QCacheGrind. In the case of Windows, the QCacheGrindWin
    package provides a binary, whereas within Linux it is most likely available through
    your package manager, and with OS X you can try `brew install qcachegrind`.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there is one more package you will require: the `pyprof2calltree`
    package. It transforms the `profile` output into a format that QCacheGrind understands.
    So, after a simple `pip install pyprof2calltree`, we can now convert the `profile`
    file into a `callgrind` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the running of the `QCacheGrind` application. After switching
    to the appropriate tabs, you should see something like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15882_12_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.1: QCacheGrind'
  prefs: []
  type: TYPE_NORMAL
- en: For a simple script such as this, pretty much all output works. However, with
    full applications, a tool such as QCacheGrind is invaluable. Looking at the output
    generated by QCacheGrind, it is immediately obvious which process took the most
    time. The structure at the top right shows bigger rectangles if the amount of
    time taken was greater, which is a very useful visualization of the chunks of
    CPU time that were used. The list at the left is very similar to `cProfile` and
    therefore nothing new. The tree at the bottom right can be very valuable or very
    useless, as it is in this case. It shows you the percentage of CPU time taken
    in a function and, more importantly, the relationship of that function with the
    other functions.
  prefs: []
  type: TYPE_NORMAL
- en: Because these tools scale depending on the input, the results are useful for
    just about any application. Whether a function takes 100 milliseconds or 100 minutes
    makes no difference – the output will show a clear overview of the slow parts,
    which is what we will try to fix.
  prefs: []
  type: TYPE_NORMAL
- en: Line profiler – Tracking performance per line
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`line_profiler` is actually not a package that’s bundled with Python, but it’s
    far too useful to ignore. While the regular `profile` module profiles all (sub)functions
    within a certain block, `line_profiler` allows for profiling line *per line* within
    a function. The Fibonacci function is not best suited here, but we can use a prime
    number generator instead. But first, install `line_profiler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have installed the `line_profiler` module (and with that the `kernprof`
    command), let’s test `line_profiler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'You might be wondering where the `profile` decorator is coming from. It originates
    from the `line_profiler` module, which is why we have to run the script with the
    `kernprof` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'As the command says, the results have been written to the `T_08_line_profiler.py.lprof`
    file, so we can now look at the output of that file. For readability, we’ve skipped
    the `Line #` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Wonderful output, isn’t it? It makes it trivial to find the slow part within
    a bit of code. Within this code, the slowness is obviously originating from the
    loop, but within other code it might not be that clear.
  prefs: []
  type: TYPE_NORMAL
- en: This module can be added as an IPython extension as well, which enables the
    `%lprun` command within IPython. To load the extension, the `load_ext` command
    can be used from the IPython shell, `%load_ext line_profiler`.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen several methods of measuring CPU performance and execution time.
    Now it’s time to look at how to improve performance. Since this largely applies
    to CPU performance and not memory performance, we will cover that first. Later
    in this chapter, we will take a look at memory usage and leaks.
  prefs: []
  type: TYPE_NORMAL
- en: Improving execution time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Much can be said about performance optimization, but truthfully, if you have
    read the entire book up to this point, you know most of the Python-specific techniques
    for writing fast code. The most important factor in overall application performance
    will always be the choice of algorithms and, by extension, the data structures.
    Searching for an item within a `list (O(n))` is almost always a worse idea than
    searching for an item in a `dict` or `set (O(1))`, as we have seen in *Chapter
    4*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Naturally, there are more factors and tricks that can help make your application
    faster. The extremely abbreviated version of all performance tips is quite simple,
    however: do as little as possible. No matter how fast you make your calculations
    and operations, doing nothing at all will always be faster. The following sections
    cover some of the most common performance bottlenecks in Python and test a few
    common assumptions about performance, such as the performance of `try`/`except`
    blocks versus `if` statements, which can have a huge impact in many languages.'
  prefs: []
  type: TYPE_NORMAL
- en: Some of the tricks in this section will be a trade-off between memory and execution
    time; others will trade readability with performance. When in doubt, go for readability
    by default and only improve performance if you have to.
  prefs: []
  type: TYPE_NORMAL
- en: Using the right algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Within any application, the right choice of algorithm is by far the most important
    performance characteristic, which is why I am repeating it to illustrate the results
    of a bad choice. Consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Checking whether an item is within a `list` is an `O(n)` operation, and checking
    whether an item is within a `dict` is an `O(1)` operation. This makes a huge difference
    when `n=1000000`; in this simple test, we can see that for 1 million items, it’s
    300,000 times faster.
  prefs: []
  type: TYPE_NORMAL
- en: The big-O notation ( `O(...)`) is covered in more detail in *Chapter 4*, but
    we can provide a quick recap.
  prefs: []
  type: TYPE_NORMAL
- en: '`O(n)` means that for a `list` with `len(some_list) = n`, it will take `n`
    steps to perform the operation. Consequently, `O(1)` means that it takes a constant
    amount of time regardless of the size of the collection.'
  prefs: []
  type: TYPE_NORMAL
- en: All other performance tips combined might make your code twice as fast, but
    using the right algorithm for the job can cause a much greater improvement. Using
    an algorithm that takes `O(n)` time instead of `O(n`²`)` time will make your code
    `1000` times faster for `n=1000`, and with a larger `n`, the difference only grows
    further.
  prefs: []
  type: TYPE_NORMAL
- en: Global interpreter lock
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most obscure components of the CPython interpreter is the **global
    interpreter lock** (**GIL**), a **mutual exclusion lock** (**mutex**) required
    to prevent memory corruption. The Python memory manager is not thread-safe, which
    is why the GIL is needed. Without the GIL, multiple threads might alter memory
    at the same time, causing all sorts of unexpected and potentially dangerous results.
    The GIL is covered in much more detail in *Chapter 14*.
  prefs: []
  type: TYPE_NORMAL
- en: What is the impact of the GIL in a real-life application? Within single-threaded
    applications, it makes no difference whatsoever and is actually an extremely fast
    method for memory consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Within multithreaded applications, however, it can slow your application down
    a bit because only a single thread can access the GIL at a time. If your code
    has to access the GIL a lot, it might benefit from some restructuring.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, Python offers a few other options for parallel processing. The `asyncio`
    module, which we will see in *Chapter 13*, can help a lot by switching tasks whenever
    you are waiting for a slow operation. In *Chapter 14*, we will see the `multiprocessing`
    library, which allows us to use multiple processors simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: try versus if
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In many languages, a `try/except` type of block incurs quite a performance hit,
    but within Python, this is *not* the case as long as you don’t hit the `except`
    block. If you do hit an `except`, it will be slightly heavier than an `if` statement,
    but not enough to be noticeable in most cases.
  prefs: []
  type: TYPE_NORMAL
- en: It’s not that an `if` statement is heavy, but if you expect your `try/except`
    to succeed most of the time and only fail in rare cases, it is definitely a valid
    alternative. As always though, focus on readability and conveying the purpose
    of the code. If the intention of the code is clearer using an `if` statement,
    use the `if` statement. If `try`/`except` conveys the intention in a better way,
    use that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most programming languages depend on the use of the **Look Before You Leap**
    (**LBYL**) ideology. This means that you always check before you try, so if you
    are getting `some_key` from a `dict`, you use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Because you are always doing the `if`, it hints that `some_key` is usually not
    part of `some_dict`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within Python, it is common to use the **Easier to Ask for Forgiveness than
    Permission** (**EAFP**) ideology when applicable. This means that the code assumes
    everything will work, but still catches errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: These two examples function mostly the same, but the latter gives the idea that
    you expect the key to be available and will catch errors if needed. This is one
    of the cases where the Zen of Python (explicit is better than implicit) applies.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only caveat of the code above is that you might accidentally catch a `KeyError`
    from `process_value()`, so if you want to avoid that you should use the following
    code instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Which one you use comes mostly down to personal preference, but the takeaway
    should be that, with Python, both options are perfectly valid and will perform
    similarly.
  prefs: []
  type: TYPE_NORMAL
- en: Lists versus generators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluating code lazily using generators is almost always a better idea than
    calculating the entire dataset. The most important rule of performance optimization
    is probably that you shouldn’t calculate anything you’re not going to use. If
    you’re not sure that you are going to need it, don’t calculate it.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t forget that you can easily chain multiple generators, so everything is
    calculated only when it’s actually needed. Do be careful that this won’t result
    in recalculation though; `itertools.tee()` is generally a better idea than recalculating
    your results completely.
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap `itertools.tee()` from *Chapter 7*, a regular generator can only be
    consumed once, so if you need to process the results two or more times, you can
    use `itertools.tee()` to store the intermediate results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, if you forget to use `itertools.tee()` here, you would only
    process the results once, and both would process different values. The alternative
    fix is to use `list()` and store the intermediate results, but this can cost much
    more memory, and you are required to pre-calculate all items without knowing whether
    you actually need them all.
  prefs: []
  type: TYPE_NORMAL
- en: String concatenation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You might have seen benchmarks saying that using `+=` is much slower than joining
    strings because the `str` object (as is the case with `bytes`) is immutable. The
    result is that every time you do `+=` on a string, it will have to create a new
    object. At one point, this made quite a lot of difference indeed. With Python
    3, however, most of the differences have vanished:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: There are still some differences, of course, but they are so small that I recommend
    you simply ignore them and choose the most readable option instead.
  prefs: []
  type: TYPE_NORMAL
- en: Addition versus generators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As is the case with string concatenation, addition from a loop was significantly
    slower with older Python versions, but the difference is now too small to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: What does help, though, is letting Python handle everything internally using
    native functions, as can be seen in the last example.
  prefs: []
  type: TYPE_NORMAL
- en: Map versus generators and list comprehensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once again, readability generally counts more than performance, so only rewrite
    for performance if it really makes a difference. There are a few cases where `map()`
    is faster than list comprehensions and generators, but only if the `map()` function
    can use a predefined function. As soon as you need to whip out `lambda`, it’s
    actually slower. Not that it matters much, since readability should be key anyhow.
    If `map()` makes your code more readable than a generator or list comprehension,
    feel free to use it. Otherwise, I would not recommend it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the list comprehension is quite a bit faster than the generator.
    In many cases, I would still recommend the generator over the list comprehension,
    though, if only because of the memory usage and the potential laziness.
  prefs: []
  type: TYPE_NORMAL
- en: If, for some reason, you are only going to use the first 10 items when generating
    1,000 items, you’re still wasting a lot of resources by calculating the full list
    of items.
  prefs: []
  type: TYPE_NORMAL
- en: Caching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have already covered the `functools.lru_cache` decorator in *Chapter 6*,
    *Decorators – Enabling Code Reuse by Decorating*, but its importance should not
    be underestimated. Regardless of how fast and smart your code is, not having to
    calculate results is always better and that’s what caching does. Depending on
    your use case, there are many options available. Within a simple script, `functools.lru_cache`
    is a very good contender, but between multiple executions of an application, the
    `cPickle` module can be a lifesaver as well.
  prefs: []
  type: TYPE_NORMAL
- en: We have already seen the effects of this with the `fibonacci_cached` function
    in the `cProfile` section of this chapter, which uses `functools.lru_cache()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several scenarios where you need a more powerful solution, however:'
  prefs: []
  type: TYPE_NORMAL
- en: If you need caching between multiple executions of a script
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you need caching shared across multiple processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you need caching shared across multiple servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At least for the first two scenarios, you could write the cache to a local pickle/CSV/JSON/YAML/DBM/etc.
    file. This is a perfectly valid solution that I use often.
  prefs: []
  type: TYPE_NORMAL
- en: If you need a more powerful solution, however, I can highly recommend taking
    a look at **Redis**. The Redis server is a fully in-memory server that is extremely
    fast and has many useful data structures available. If you see articles or tutorials
    about improving performance using Memcached, simply replace Memcached with Redis
    everywhere. Redis is superior to Memcached in every way and, in its most basic
    form, the API is compatible.
  prefs: []
  type: TYPE_NORMAL
- en: Lazy imports
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common problem in application load times is that everything is loaded immediately
    at the start of the program while, with many applications, this is actually not
    needed and certain parts of the application only require loading when they are
    actually used. To facilitate this, you can occasionally move the imports inside
    of functions so they can be loaded on demand.
  prefs: []
  type: TYPE_NORMAL
- en: 'While it’s a valid strategy in some cases, I don’t generally recommend it for
    two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: It makes your code less clear; having all imports in the same style at the top
    of the file improves readability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It doesn’t make the code faster as it just moves the load time to a different
    part.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using slots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `__slots__` feature was written by Guido van Rossum to enhance Python performance.
    Effectively what the `__slots__` feature does is specify a fixed list of attributes
    for a class. When `__slots__` are used, several changes are made to a class and
    several (side-)effects must be considered:'
  prefs: []
  type: TYPE_NORMAL
- en: All attributes must be explicitly named in the `__slots__`. It is not possible
    to do `some_instance.some_variable = 123` if `some_variable` is not in `__slots__`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because the list of attributes is fixed in `__slots__`, there is no longer any
    need for a `__dict__` attribute, which saves memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attribute access is faster because there is no intermediate lookup through `__dict__`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is not possible to use multiple inheritance if both parents have defined
    `__slots__`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, how much performance benefit can `__slots__` give us? Well, let’s give
    it a test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'When we actually run this code, we can definitely see some improvements from
    using `__slots__`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: In most cases, I would argue that the 5-15% difference in performance isn’t
    going to help you that much. However, if it’s applied to a bit of code that is
    near the core of your application and executed very often, it can help.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t expect miracles from this method, but use it when you need it.
  prefs: []
  type: TYPE_NORMAL
- en: Using optimized libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is actually a very broad tip, but useful nonetheless. If there’s a highly
    optimized library that suits your purpose, you most likely won’t be able to beat
    its performance without a significant amount of effort. Libraries such as `numpy`,
    `pandas`, `scipy`, and `sklearn` are highly optimized for performance and their
    native operations can be incredibly fast. If they suit your purpose, be sure to
    give them a try.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you can use `numpy`, you need to install it: `pip3 install numpy.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just to illustrate how fast `numpy` can be compared to plain Python, refer
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The `numpy` code does exactly the same as the Python code, except that it uses
    `numpy` arrays instead of Python lists. This little difference has made the code
    more than 25 times faster.
  prefs: []
  type: TYPE_NORMAL
- en: Just-in-time compiling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Just-in-time** (**JIT**) compiling is a method of dynamically compiling (parts
    of) an application during runtime. Because there is much more information available
    at runtime, this can have a huge effect and make your application much faster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to JIT compiling, you currently have three options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pyston**: An alternative, currently Linux only, CPython-compatible Python
    interpreter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pypy**: A really fast alternative Python interpreter without full CPython
    compatibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Numba**: A package that allows for JIT compiling per function and execution
    on either the CPU or the GPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CPython 3.12 and 3.13**? At the time of writing, there is little concrete
    data about the upcoming Python releases, but there are plans to greatly increase
    the CPython interpreter performance. How much will be achieved and how well it
    will work is currently unknown, but the ambitious plan is to make CPython 5x faster
    over the next 5 releases (with 3.10 being the first in the series). The expectation
    is to add JIT compiling in CPython 3.12 and extend that further in 3.13\.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are looking for global JIT compiling in existing projects, I can currently
    recommend trying Pyston. It is a CPython fork that promises about a 30% performance
    increase without having to change any code. In addition, because it is CPython-compatible,
    you can still use regular CPython modules.
  prefs: []
  type: TYPE_NORMAL
- en: The downside is that it currently only supports Linux systems and, as will always
    be the case with forks, it’s behind the current Python version. At the time of
    writing, CPython is at Python 3.10.1, whereas Pyston is at Python 3.8.
  prefs: []
  type: TYPE_NORMAL
- en: If compatibility with all CPython modules is not a requirement for you and you
    don’t require Python features that are too recent, PyPy3 can also offer amazing
    performance in many cases. They are up to Python 3.7, whereas the main Python
    release is at 3.10.1 at the time of writing. That makes PyPy roughly 2-3 years
    behind CPython in terms of features, but I doubt this is a big issue. The differences
    between Python 3.7, 3.8, 3.9, and 3.10 are largely incremental and Python 3.7
    is already a very well-rounded Python version.
  prefs: []
  type: TYPE_NORMAL
- en: The `numba` package provides selective JIT compiling for you, allowing you to
    mark the functions that are JIT compiler-compatible. Essentially, if your functions
    follow the functional programming paradigm of basing the calculations only on
    the input, then it will most likely work with the JIT compiler.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a basic example of how the `numba` JIT compiler can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: If you are using `numpy` or `pandas`, you will most likely benefit from looking
    at `numba`.
  prefs: []
  type: TYPE_NORMAL
- en: Another very interesting fact to note is that `numba` supports not only CPU-optimized
    execution, but GPU as well. This means that for certain operations you can use
    the fast processor in your video card to process the results.
  prefs: []
  type: TYPE_NORMAL
- en: Converting parts of your code to C
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will see more about this in *Chapter 17*, *Extensions in C/C++, System Calls,
    and C/C++ Libraries*, but if high performance is really required, then a native
    C function can help quite a lot. This doesn’t even have to be that difficult;
    the Cython module makes it trivial to write parts of your code with performance
    very close to native C code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example from the Cython manual to approximate the value
    of pi:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: While there are some small differences, such as `cdef` instead of `def` and
    type definitions such as `int i` instead of just `i` for the values and parameters,
    the code is largely the same as regular Python would be, but certainly much faster.
  prefs: []
  type: TYPE_NORMAL
- en: Memory usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have simply looked at the execution times and largely ignored the
    memory usage of the scripts. In many cases, the execution times are the most important,
    but memory usage should not be ignored. In almost all cases, CPU and memory are
    traded; an algorithm either uses a lot of CPU time or a lot of memory, which means
    that both do matter a lot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within this section, we are going to look at:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing memory usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When Python leaks memory and how to avoid these scenarios
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to reduce memory usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tracemalloc
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Monitoring memory usage used to be something that was only possible through
    external Python modules such as **Dowser** or **Heapy**. While those modules still
    work, they are partially obsolete now because of the `tracemalloc` module. Let’s
    give the `tracemalloc` module a try to see how easy memory usage monitoring is
    nowadays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: You can easily see how every part of the code allocated memory and where it
    might be wasted. While it might still be unclear which part was actually causing
    the memory usage, there are options for that as well, as we will see in the following
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Profiler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `memory_profiler` module is very similar to `line_profiler` discussed earlier,
    but for memory usage instead. Installing it is as easy as `pip install memory_profiler`,
    but the optional `pip install psutil` is also highly recommended (and required
    in the case of Windows) as it increases your performance by a large amount. To
    test `memory_profiler`, we will use the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we actually import `memory_profiler` here although that is not strictly
    required. It can also be executed through `python3 -m memory_profiler your_scripts.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Even though everything runs as expected, you might be wondering about the varying
    amounts of memory used by the lines of code here.
  prefs: []
  type: TYPE_NORMAL
- en: Why does `e` take `9.8 MiB` and `f` `5.0 MiB`? This is caused by the Python
    memory allocation code; it reserves memory in larger blocks, which is subdivided
    and reused internally. Another problem is that `memory_profiler` takes snapshots
    internally, which results in memory being attributed to the wrong variables in
    some cases. The variations should be small enough not to make a large difference
    in the end, but some changes are to be expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'This module can be added as an IPython extension as well, which enables the
    `%mprun` command within IPython. To load the extension, the `load_ext` command
    can be used from the IPython shell: `%load_ext memory_profiler`. Another very
    useful command is `%memit`, which is the memory equivalent of the `%timeit` command.'
  prefs: []
  type: TYPE_NORMAL
- en: Memory leaks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The usage of these modules will generally be limited to the search for memory
    leaks. In particular, the `tracemalloc` module has a few features to make that
    fairly easy. The Python memory management system is fairly straightforward; it
    has a simple reference counter to see whether an object is (still) used. While
    this works great in most cases, it can easily introduce memory leaks when circular
    references are involved. The basic premise of a memory leak with leak detection
    code looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The line numbers in the code above are provided as a reference for the `tracemalloc`
    output and are not functionally part of the code.
  prefs: []
  type: TYPE_NORMAL
- en: The big problem in this code is that we have two objects that are referencing
    each other. As we can see, `a.b` is referencing `b`, and `b.a` is referencing
    `a`. This loop makes it so that Python doesn’t immediately understand that the
    objects can be safely deleted from memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how badly this code is actually leaking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: This example shows a leak of 22.1 megabytes due to the nearly 200,000 instances
    of `SomeClass`. Python correctly lets us know that this memory was allocated at
    lines 24 and 25, which can really help when trying to ascertain what is causing
    the memory usage in your application.
  prefs: []
  type: TYPE_NORMAL
- en: The Python garbage collector (`gc`) is smart enough to clean circular references
    like these eventually, but it won’t clean them until a certain limit is reached.
    More about that soon.
  prefs: []
  type: TYPE_NORMAL
- en: Circular references
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Whenever you want to have a circular reference that does not cause memory leaks,
    the `weakref` module is available. It creates references that don’t count toward
    the object reference count. Before we look at the `weakref` module, let’s take
    a look at the object references themselves through the eyes of the Python garbage
    collector (`gc`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: First, we create two instances of `SomeClass` and add some circular references
    between them. Once that is done, we delete them from memory, except that they
    are not actually deleted until the garbage collector runs.
  prefs: []
  type: TYPE_NORMAL
- en: To verify this, we inspect the objects in memory through `gc.get_objects()`,
    and until we tell the garbage collector to manually collect, they stay in memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we do run `gc.collect()` to manually call the garbage collector, the objects
    are gone from memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Now, you might wonder, are you always required to manually call `gc.collect()`
    to remove these references? No, that is not needed, as the Python garbage collector
    will automatically collect once thresholds have been reached.
  prefs: []
  type: TYPE_NORMAL
- en: By default, the thresholds for the Python garbage collector are set to `700,
    10, 10` for the three generations of collected objects. The collector keeps track
    of all the memory allocations and deallocations in Python, and as soon as the
    number of allocations minus the number of deallocations reaches `700`, the object
    is either removed if it’s not referenced anymore, or it is moved to the next generation
    if it still has a reference. The same is repeated for generations 2 and 3, albeit
    with the lower thresholds of 10.
  prefs: []
  type: TYPE_NORMAL
- en: 'This begs the question: where and when is it useful to manually call the garbage
    collector? Since the Python memory allocator reuses blocks of memory and only
    rarely releases it, for long-running scripts the garbage collector can be very
    useful. That’s exactly where I recommend its usage: long-running scripts in memory-strapped
    environments and, specifically, right before you **allocate** a large amount of
    memory. If you call the garbage collector before doing a memory-intensive operation,
    you can maximize the amount of reuse of the memory that Python has previously
    reserved.'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing memory usage using the garbage collector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `gc` module can help you a lot when looking for memory leaks as well. The
    `tracemalloc` module can show you the parts that take the most memory in bytes,
    but the `gc` module can help you find the most commonly occurring object types
    (for example, `SomeClass`, `int`, and `list`). Just be careful when setting the
    garbage collector debug settings such as `gc.set_debug(gc.DEBUG_LEAK)`; this returns
    a large amount of output even if you don’t reserve any memory yourself. Let’s
    see the output for one of the most basic scripts you can get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when we run the code, you can see a bit of what has been added to our
    memory with such a simple script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there are actually 42 different types of objects that should
    have been shown here, but even without that, the number of different objects in
    memory is impressive, if you ask me. With just a little bit of extra code, the
    output can quickly explode and become unusable without significant filtering.
  prefs: []
  type: TYPE_NORMAL
- en: Weak references
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An easy method to make the work easier for the garbage collector is to use **weak
    references**. These are references to variables that are not included when counting
    the references to a variable. Since the garbage collector removes an object from
    memory when its reference count gets to zero, this can help a lot with memory
    leaks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the earlier example, we saw that the objects weren’t removed until we manually
    called `gc.collect()`. Now we will see what happens if we use the `weakref` module
    instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s see what remained this time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Perfect – no instances of `SomeClass` exist in memory after `del`, which is
    exactly what we had hoped for.
  prefs: []
  type: TYPE_NORMAL
- en: Weakref limitations and pitfalls
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You might be wondering what happens when you still try to reference a since-removed
    `weakref`. As you would expect, the object is gone now, so you can no longer use
    it. What is more, not all objects can be used through weak references directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use `weakref` for custom classes though, so we can subclass the types
    before we create the `weakref`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: For `dict` and `set` instances, the `weakref` library also has the `weakref.WeakKeyDictionary`,
    `weakref.WeakValueDictionary`, and `weakref.WeakSet` classes. These behave similarly
    to the regular instances of `dict` and `set`, but remove the values based on the
    key or value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to be careful when using a `weakref`, of course. As soon as all regular
    references are deleted, the object will be inaccessible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: After deleting `a`, which is the only real reference to the `SomeClass` instance,
    we cannot use the instance anymore. While this is to be expected, you should be
    wary of this problem if your main reference has a chance to disappear.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever you are working with large self-referencing data structures, it can
    be a good idea to use the `weakref` module. However, don’t forget to check if
    your instance still exists before using it.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing memory usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, memory usage probably won’t be your biggest problem in Python, but
    it can still be useful to know what you can do to reduce it. When trying to reduce
    memory usage, it’s important to understand how Python allocates memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four concepts that you need to know about within the Python memory
    manager:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we have the **heap**. The heap is the collection of all Python-managed
    memory. Note that this is separate from the regular heap and mixing the two could
    result in corrupt memory and crashes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second are the **arenas**. These are the chunks that Python requests from the
    system. These chunks have a fixed size of 256 KiB each and they are the objects
    that make up the heap.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third we have the **pools**. These are the chunks of memory that make up the
    arenas. These chunks are 4 KiB each. Since the pools and arenas have fixed sizes,
    they are simple arrays.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fourth and last, we have the **blocks**. The Python objects get stored within
    these and every block has a specific format depending on the data type. Since
    an integer takes up more space than a character, for efficiency, a different block
    size is used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know how the memory is allocated, we can also understand how it
    can be returned to the operating system and why this is often very hard to do.
  prefs: []
  type: TYPE_NORMAL
- en: 'Releasing a block back to the pool is easy enough: a simple `del some_variable`
    followed by a `gc.collect()` should do the trick. The problem is that this is
    no guarantee that the memory will be released back to the operating system yet.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate what needs to happen in order for the memory to release to the
    operating system:'
  prefs: []
  type: TYPE_NORMAL
- en: All blocks in a pool need to be released before the pool can be released
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All pools in an arena need to be released before the arena can be released
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the arena has been released to the heap, memory *might* be released to
    the operating system, but that depends on the C runtime and/or operating system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That is why I would always recommend running `gc.collect()` in long-running
    scripts right before you start allocating large blocks of memory.
  prefs: []
  type: TYPE_NORMAL
- en: It is a common and incorrect misconception that Python never releases any memory
    to the system. Before Python 2.5, this was indeed the case because arenas were
    never freed to the heap.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s illustrate the effects of allocating and releasing memory by allocating
    and releasing twice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'You might expect that the memory usage after the second block has been released
    will be near identical to after the first block has been released, or even back
    to the original state. Let’s see what actually happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: That’s odd, isn’t it? The memory usage has grown between the two allocations.
    The truth is that I cherry-picked the result somewhat and that the output changes
    between each run, because releasing memory back to the operating system is not
    a guarantee that the operating system will immediately handle it. In some other
    cases, the memory had properly returned to 17 MiB.
  prefs: []
  type: TYPE_NORMAL
- en: The astute among you might wonder if the results are skewed because I forgot
    the `gc.collect()`. In this case, the answer is no because the memory allocation
    is large enough to immediately trigger the garbage collector by itself and the
    difference is negligible.
  prefs: []
  type: TYPE_NORMAL
- en: This is roughly the best case, however – just a few contiguous blocks of memory.
    The real challenge is when you have many variables so only parts of the pools/arenas
    are used. Python uses some heuristics to find space in an empty arena so it doesn’t
    have to allocate new arenas when you are storing new variables, but that does
    not always succeed, of course. This is a case where running `gc.collect()` before
    allocation can help because it can tell Python which pools are now free.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the regular heap and Python heap are maintained
    separately, as mixing them can result in corruption and/or the crashing of applications.
    Unless you write your own Python extensions in C/C++, you will probably never
    have to worry about manual memory allocation though.
  prefs: []
  type: TYPE_NORMAL
- en: Generators versus lists
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most important tip is to use generators whenever possible. Python 3 has
    come a long way in replacing lists with generators already, but it really pays
    to keep that in mind as it saves not only memory, but CPU as well, when not all
    of that memory needs to be kept at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: The `range()` generator takes such little memory that it doesn’t even register,
    whereas the list of numbers takes `38.6 MiB`.
  prefs: []
  type: TYPE_NORMAL
- en: Recreating collections versus removing items
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One very important detail about collections in Python is that many of them
    can only grow; they won’t just shrink by themselves. To illustrate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Even after removing all items from the `dict`, the memory usage remains the
    same. This is one of the most common memory usage mistakes made with lists and
    dictionaries. The only way to reclaim the memory is by recreating the object.
    Or, never allocate the memory at all by using generators.
  prefs: []
  type: TYPE_NORMAL
- en: Using slots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to the performance benefits of using `__slots__` that we saw earlier
    in this chapter, `__slots__` can also help to reduce memory usage. As a recap,
    `__slots__` allows you to specify which fields you want to store in a class and
    it skips all the others by not implementing `instance.__dict__`.
  prefs: []
  type: TYPE_NORMAL
- en: While this method does save a little bit of memory in your class definitions,
    the effect is often limited. For a nearly empty class with just a single/tiny
    attribute such as a `bool` or `byte`, this can make quite a bit of difference.
    For classes that actually store a bit of data, the effect can diminish quickly.
  prefs: []
  type: TYPE_NORMAL
- en: The biggest caveat of `__slots__` is that multiple inheritance is impossible
    if both parent classes have `__slots__` defined. Beyond that, it can be used in
    nearly all cases.
  prefs: []
  type: TYPE_NORMAL
- en: You might wonder if `__slots__` will limit dynamic attribute assignments, effectively
    blocking you from doing `Spam.eggs = 123` if `eggs` was not part of `__slots__`.
    And you are right – partially, at least. With a standard fixed list of attributes
    in `__slots__`, you cannot dynamically add new attributes – but you can if you
    add `__dict__` to `__slots__`.
  prefs: []
  type: TYPE_NORMAL
- en: I’m embarrassed to say that it took me about 15 years before I found out about
    this feature, but knowing about this feature makes `__slots__` so much more versatile
    that I really feel like I should mention it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now illustrate the difference in memory usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'And the memory usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: You might argue that this is not a fair comparison since they both store a lot
    of data, which skews the results. And you would indeed be right because the “bare”
    comparison, storing only `index` and nothing else, gives `2 MiB` versus `4.5 MiB`.
    But, let’s be honest, if you’re not going to store data, then what’s the point
    in creating class instances? I’m not saying that `__slots__` have no purpose,
    but don’t go overboard because the advantages are generally limited.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one more structure that’s even more memory-efficient: the `array`
    module. It stores the data in pretty much the same way a bare memory array in
    C would do. Note that this is generally slower than lists and much less convenient
    to use. If you need to store large amounts of numbers, I would suggest looking
    at `numpy.array` or `scipy.sparse` instead.'
  prefs: []
  type: TYPE_NORMAL
- en: Performance monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen how to measure and improve both CPU and memory performance,
    but there is one part we have completely skipped over. Performance changes due
    to external factors such as growing amounts of data are very hard to predict.
    In real-life applications, bottlenecks aren’t constant. They change all the time
    and code that was once extremely fast might bog down as soon as more load is applied.
  prefs: []
  type: TYPE_NORMAL
- en: Because of that, I recommend implementing a monitoring solution that tracks
    the performance of anything and everything over time. The big problem with performance
    monitoring is that you can’t know what will slow down in the future and what the
    cause is going to be. I’ve even had websites slow down because of Memcached and
    Redis calls. These are memory-only caching servers that respond well within a
    millisecond, which makes slowdowns highly unlikely, until you do over 100 cache
    calls and the latency toward the cache server increases from 0.1 milliseconds
    to 2 milliseconds, and all of a sudden those 100 calls take 200 milliseconds instead
    of 10 milliseconds. Even though 200 milliseconds still sounds like very little,
    if your total page load time is generally below 100 milliseconds, that is, all
    of a sudden, an enormous increase and definitely noticeable.
  prefs: []
  type: TYPE_NORMAL
- en: 'To monitor performance and to be able to track changes over time and find the
    responsible components, I can personally recommend several systems for monitoring
    performance:'
  prefs: []
  type: TYPE_NORMAL
- en: For simple short-term (up to a few weeks) application performance tracking,
    the **Prometheus** monitoring system is very easy to set up and when paired with
    **Grafana**, you can create the prettiest charts to monitor your performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you want a more long-term performance tracking solution that scales well
    to large numbers of variables, you might be interested in **InfluxDB** instead.
    It can also be paired with Grafana for really useful interactive charting:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15882_12_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.2: Grafana heatmap of response times'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15882_12_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.3: Grafana chart of request latency'
  prefs: []
  type: TYPE_NORMAL
- en: To enter data into systems like these, you have several options. You can use
    the native APIs, but you can also use an intermediate system such as **StatsD**.
    The StatsD system doesn’t store data itself, but it makes it really easy to fire
    and forget performance metrics from your system without having to worry whether
    the monitoring system is still up and running. Because the system commonly uses
    UDP to send the information, even if the monitoring server is completely down
    and unreachable, your application won’t notice the difference.
  prefs: []
  type: TYPE_NORMAL
- en: To be able to use these, you will have to send the metrics from your application
    to the StatsD server. To do just that, I have written the Python-StatsD ([https://pypi.org/project/python-statsd/](Chapter_12.xhtml))
    and Django-StatsD ([https://pypi.org/project/django-statsd/](Chapter_12.xhtml))
    packages. These packages allow you to monitor your application from beginning
    to end and, in the case of Django, you will be able to monitor your performance
    per application or view, and within those see all of the components, such as the
    database, template, and caching layers. This way, you know exactly what is causing
    the slowdowns in your website (or application). And best of all, it is in (near)
    real time.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you have learned about many of the available tools for performance
    measuring and optimization, try and create a few useful decorators or context
    wrappers that will help you prevent issues:'
  prefs: []
  type: TYPE_NORMAL
- en: Try to create a decorator that monitors each run of a function and warns you
    if the memory usage grows each run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to create a decorator that monitors the runtime of a function and warns
    you if it deviates too much from the previous run. Optionally, you could make
    the function generate a (running) average runtime as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to create a memory manager for your classes that warns you when more than
    a configured number of instances remain in memory. If you never expect more than
    5 instances of a certain class, you can warn the user when that number is exceeded.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example answers for these exercises can be found on GitHub: [https://github.com/mastering-python/exercises](Chapter_12.xhtml).
    You are encouraged to submit your own solutions and learn about alternative solutions
    from others.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to performance, there is no holy grail, no single thing you can
    do to ensure peak performance in all cases. This shouldn’t worry you, however,
    as in most cases, you will never need to tune the performance and, if you do,
    a single tweak could probably fix your problem. You should be able to find performance
    problems and memory leaks in your code now, which is what matters most, so just
    try to contain yourself and only tweak when it’s actually needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a quick recap of the tools in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Measuring CPU performance: `timeit`, `profile`/`cProfile`, and `line_profiler`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Analyzing profiling results: SnakeViz, `pyprof2calltree`, and QCacheGrind'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Measuring memory usage: `tracemalloc`, `memory_profiler`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reducing memory usage and leaks: `weakref` and `gc` (garbage collector)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you know how to use these tools, you should be able to track down and fix
    most performance issues in your code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important takeaways from this chapter are:'
  prefs: []
  type: TYPE_NORMAL
- en: Test before you invest any effort. Making some functions faster seems like a
    great achievement, but it is only rarely needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the correct data structure/algorithm is much more effective than any
    other performance optimization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Circular references drain the memory until the garbage collector starts cleaning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slots come with several caveats, so I would recommend limited usage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next chapter will properly introduce us to working asynchronously using
    the `asyncio` module. This module makes it possible to “background” the waiting
    for external I/O. Instead of keeping your foreground thread busy, it can switch
    to a different thread when your code is waiting for endpoints such as TCP, UDP,
    files, and processes.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers: [https://discord.gg/QMzJenHuJf](Chapter_12.xhtml)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code156081100001293319171.png)'
  prefs: []
  type: TYPE_IMG
