- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Performance – Tracking and Reducing Your Memory and CPU Usage
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能 - 跟踪和减少你的内存和CPU使用
- en: 'Before we talk about performance, there is a quote by *Donald Knuth* you need
    to consider first:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论性能之前，首先需要考虑的是Donald Knuth的一句话：
- en: “The real problem is that programmers have spent far too much time worrying
    about efficiency in the wrong places and at the wrong times; premature optimization
    is the root of all evil (or at least most of it) in programming”.
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “真正的问题在于程序员在错误的地方和错误的时间花费了太多的时间去担心效率；过早的优化是编程中所有邪恶（至少是大部分邪恶）的根源。”
- en: Donald Knuth is often called the father of algorithm analysis. His book series,
    *The Art of Computer Programming*, can be considered the Bible of all fundamental
    algorithms.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Donald Knuth 通常被称为算法分析的鼻祖。他的书系《计算机程序设计艺术》可以被认为是所有基本算法的圣经。
- en: As long as you pick the correct data structures with the right algorithms, performance
    should not be something to worry about. That does not mean you should ignore performance
    entirely, but just make sure you pick the right battles and optimize only when
    it is actually needed. Micro/premature optimizations can definitely be fun, but
    are only very rarely useful.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 只要你选择正确的数据结构和合适的算法，性能就不应该是你担心的事情。这并不意味着你应该完全忽略性能，但只是确保你选择了正确的战斗，并且只有在真正需要的时候才进行优化。微/过早的优化确实很有趣，但只有非常少的情况下才有用。
- en: We have seen the performance characteristics of many data structures in *Chapter
    2*, *Pythonic Syntax and Common Pitfalls*, already, so we won’t discuss that,
    but we will show you how performance can be measured and how problems can be detected.
    There are cases where micro optimizations make a difference, but you won’t know
    until you measure the performance.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在第二章《Pythonic语法和常见陷阱》中看到了许多数据结构的性能特征，所以我们将不会讨论这一点，但我们会向你展示如何测量性能以及如何检测问题。在某些情况下，微优化会有所帮助，但你不知道直到你测量了性能。
- en: 'Within this chapter, we will cover:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: Profiling CPU usage
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析CPU使用情况
- en: Profiling memory usage
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析内存使用情况
- en: Learning how to correctly compare performance metrics
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何正确比较性能指标
- en: Optimizing performance
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化性能
- en: Finding and fixing memory leaks
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找和修复内存泄漏
- en: Globally, the chapter is split between CPU usage and/or CPU time, and memory
    usage. The first half of the chapter mainly concerns CPU/time; the second half
    covers memory usage.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从全局来看，本章分为CPU使用和/或CPU时间以及内存使用。本章的前半部分主要关注CPU/时间；后半部分涵盖内存使用。
- en: What is performance?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是性能？
- en: Performance is a very broad term. It has many different meanings and, in many
    cases, it is defined incorrectly. Within this chapter, we will attempt to measure
    and improve performance in terms of CPU usage/time and memory usage. Many of the
    examples here are a trade-off between execution time and memory usage. Note that
    a fast algorithm that can only use a single CPU core can be outperformed in terms
    of execution time by a slower algorithm that is easily parallelizable given enough
    CPU cores.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 性能是一个非常广泛的概念。它有许多不同的含义，在许多情况下，它被错误地定义。在本章中，我们将尝试从CPU使用/时间和内存使用方面来测量和改进性能。这里的大多数例子都是执行时间和内存使用之间的权衡。请注意，一个只能使用单个CPU核心的快速算法，在执行时间上可能会被一个足够多的CPU核心就能轻松并行化的较慢算法所超越。
- en: 'When it comes to incorrect statements about performance, you have probably
    heard statements similar to “Language X is faster than Python.” That statement
    is inherently wrong. Python is neither fast nor slow; Python is a programming
    language, and a language has no performance metrics whatsoever. If you were to
    say that the CPython interpreter is faster or slower than interpreter Y for language
    X, that would be possible. The performance characteristics of code can vary greatly
    between different interpreters. Just take a look at this small test (which uses
    ZSH shell script):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到关于性能的错误陈述时，你可能已经听到了类似“语言X比Python快”这样的说法。这个说法本身是错误的。Python既不快也不慢；Python是一种编程语言，而一种语言根本没有任何性能指标。如果你要说CPython解释器比语言X的解释器Y快或慢，那是可能的。代码的性能特征在不同解释器之间可能会有很大的差异。只需看看这个小的测试（它使用ZSH
    shell脚本）：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Five different Python interpreters, each with a different performance! All are
    Python, but the interpreters obviously vary.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 五种不同的Python解释器，每个都有不同的性能！它们都是Python，但解释器显然各不相同。
- en: You might not have heard of the PyPy3 and Pyston interpreters yet.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还没有听说过PyPy3和Pyston解释器。
- en: The PyPy3 interpreter is an alternative Python interpreter that uses JIT (Just-In-Time)
    compiling to perform much better than CPython in many, but certainly not all,
    cases. The big caveat of PyPy3 is that code that has speedups in C and depends
    on CPython extensions (which is a large portion of performance-critical libraries)
    either does not support PyPy3 or suffers a performance hit.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: PyPy3解释器是一个替代Python解释器，它使用JIT（即时）编译，在许多情况下比CPython表现更好，但当然并非所有情况。PyPy3的一个大问题是，那些在C中有速度提升且依赖于CPython扩展（这是大量性能关键库的一部分）的代码要么不支持PyPy3，要么会遭受性能损失。
- en: Pyston attempts to be a drop-in replacement for CPython with JIT compiling added
    to it. While JIT compiling might be added to CPython pretty soon, as of Python
    3.10, that is not the case yet. This is why Pyston can offer a great performance
    benefit over CPython. The downside is that it is currently only supported on Unix/Linux
    systems.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Pyston试图成为CPython的替代品，并添加了JIT编译。虽然JIT编译可能很快就会添加到CPython中，但截至Python 3.10，这还不是事实。这就是为什么Pyston可以提供比CPython更大的性能优势。缺点是它目前仅支持Unix/Linux系统。
- en: Looking at this benchmark, you might be tempted to drop the CPython interpreter
    completely and only use PyPy3\. The danger with benchmarks such as these is that
    they rarely offer any meaningful results. For this limited example, the Pypy interpreter
    was about 200 times faster than the CPython3.10 interpreter, but that has very
    little relevance for the general case. The only conclusion that can safely be
    drawn here is that this specific version of the PyPy3 interpreter is much faster
    than this specific version of CPython3 **for this exact test**. For any other
    test and interpreter version, the results could be vastly different.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 看到这个基准测试，你可能想完全放弃CPython解释器，只使用PyPy3。这样的基准测试的危险在于，它们很少提供任何有意义的成果。在这个有限的例子中，Pypy解释器比CPython3.10解释器快了大约200倍，但这对于一般情况几乎没有相关性。可以安全得出的唯一结论是，这个特定版本的PyPy3解释器在这个特定测试中比这个特定版本的CPython3快得多。对于任何其他测试和解释器版本，结果可能会有很大不同。
- en: Measuring CPU performance and execution time
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量CPU性能和执行时间
- en: 'When talking about performance you can measure a great number of things. When
    it comes to CPU performance, we can measure:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈论性能时，你可以测量许多事物。当涉及到CPU性能时，我们可以测量：
- en: The “wall time” (the absolute time on the clock).
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “墙上的时间”（时钟上的绝对时间）。
- en: Relative time (when comparing multiple runs or multiple functions)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相对时间（当比较多次运行或多个函数时）
- en: Used CPU time. Due to multithreading, multiprocessing, or asynchronous processing,
    this can be vastly different from the wall time.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CPU时间。由于多线程、多进程或异步处理，这可能与墙上的时间有很大差异。
- en: When inspecting really low-level performance, measuring the number of CPU cycles
    and loop counts.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当检查真正低级别的性能时，需要测量CPU周期数和循环计数。
- en: In addition to all these different measurement options, you should also consider
    the observer effect. Simply put, measuring takes time, and depending on how you
    are measuring the performance, the impact can be huge.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 除了所有这些不同的测量选项之外，你还应该考虑观察者效应。简单来说，测量需要时间，并且根据你如何测量性能，影响可能很大。
- en: Within this section, we will be exploring several methods to inspect the CPU
    performance and execution time of your code. Tricks to improve your performance
    after measuring will come later in the chapter.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨几种检查代码CPU性能和执行时间的方法。在测量后提高性能的技巧将在本章后面介绍。
- en: Timeit – comparing code snippet performance
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Timeit – 比较代码片段性能
- en: 'Before we can start improving execution/CPU times, we need a reliable method
    to measure them. Python has a really nice module (`timeit`) with the specific
    purpose of measuring the execution times of bits of code. It executes a bit of
    code many times to make sure there is as little variation as possible and to make
    the measurement fairly clean. It’s very useful if you want to compare a few code
    snippets. Following are some example executions:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始提高执行/CPU时间之前，我们需要一个可靠的方法来测量它们。Python有一个非常棒的模块（`timeit`），其特定目的是测量代码片段的执行时间。它多次执行一小段代码，以确保尽可能少的变异性，并使测量尽可能干净。如果你想要比较几个代码片段，这非常有用。以下是一些示例执行：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: These few examples demonstrate the performance difference between `list.insert`,
    `list.append`, a list comprehension, and the `list` function. As we have seen
    in *Chapter 4*, doing `list.insert` is very inefficient and that quickly shows
    here, in this case being 30 times slower than `list.append`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'More importantly, however, the code demonstrates how we can use the `timeit`
    module and how it works. As you can see in the output, the `list.append` variant
    was executed only `10` times, whereas the `list` call was executed `10000` times.
    That is one of the most convenient features of the `timeit` module: it automatically
    figures out some useful parameters for you, and it shows the “best of 3” to try
    and reduce the amount of variance in your tests.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The `timeit` module is great at comparing the performance of similar bits of
    code within a code base. Comparing the execution time between different Python
    interpreters using `timeit` is generally useless because it is rarely representative
    of the performance of your whole application.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'Naturally, the command can be used with regular scripts as well, but that won’t
    automatically determine the number of repetitions like the command-line interface
    does. So we will have to do that ourselves:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When executing this, you will get something along the following lines:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As you may have noticed, this script is still a bit basic. While the command-line
    version of `timeit` keeps trying until it reaches 0.2 seconds or more, this script
    just has a fixed number of executions. Since Python 3.6, we do have the option
    of using `timeit.Timer.autorange` to replicate this behavior, but it is a bit
    less convenient to use and would produce a lot more output in our current case.
    Depending on your use case, however, it could be useful to try this benchmark
    code instead:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'If you want to use `timeit` interactively, I would recommend using IPython,
    since it has a magic `%timeit` command that shows even more useful output:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this case, IPython automatically takes care of the string wrapping and passing
    of `globals()`. Still, this is all very limited and useful only for comparing
    multiple methods of doing the same thing. When it comes to full Python applications,
    there are more methods available, as we will see later in this chapter.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: To view the source of both IPython functions and regular modules, entering `object??`
    in the IPython shell returns the source. In this case, just enter `timeit??` to
    view the `timeit` IPython function definition.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way you can implement a function similar to the `%timeit` function
    is to call `timeit.main`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This effectively does the same as:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The internals of the `timeit` module are nothing too special, but take care
    to minimize a few sources of inaccuracy, such as the setup and the teardown code.
    Additionally, the module reports the fastest run because other processes on your
    system can interfere with the measurement.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'A basic version can be implemented with a few calls to `time.perf_counter`
    (the highest resolution timer available in Python), which is also used by `timeit`
    internally. The `timeit.default_timer` function is simply a reference to `time.perf_counter`.
    This basic implementation of the `timeit` function is comparable to the internals
    of the `timeit` module:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 基本版本可以通过调用几次`time.perf_counter`（Python中可用的最高分辨率计时器）来实现，该计时器也被`timeit`内部使用。`timeit.default_timer`函数仅仅是`time.perf_counter`的一个引用。`timeit`函数的基本实现与`timeit`模块的内部实现相当：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The actual `timeit` code is a bit more advanced in terms of checking the input,
    but this example roughly shows how the `timeit.timeit` function can be implemented,
    including several of the features added for more precision:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的`timeit`代码在检查输入方面要复杂一些，但这个例子大致展示了如何实现`timeit.timeit`函数，包括为提高精度而添加的几个特性：
- en: First, we can see that the code has a `number` parameter that defaults to 1
    million. This has been done to reduce the result variance a little, as we will
    see when running the code.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们可以看到代码有一个默认值为1百万的`number`参数。这样做是为了稍微减少结果的变化性，正如我们在运行代码时将看到的。
- en: Second, the code disables the Python garbage collector so we don’t get any slowdowns
    from Python deciding to clean up its memory.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，代码禁用了Python垃圾回收器，这样我们就不会因为Python决定清理其内存而出现任何减速。
- en: 'When we actually call this code, we will see why a high value for `number`
    can be important:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们实际调用这段代码时，我们将看到为什么`number`的高值可能很重要：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Even though we called the exact same code each time, the single repetition took
    more than two times as long in the first run and more than 10 times as long in
    the second run compared to the 1 million repetitions version. To make your results
    more consistent and reliable between runs, it is always good to repeat your tests
    several times and `timeit` can certainly help with that.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们每次都调用了完全相同的代码，但第一次运行的单次重复时间比1百万次重复版本多两倍以上，第二次运行则比1百万次重复版本多10倍以上。为了使你的结果在运行之间更加一致和可靠，总是重复测试几次是个好主意，而`timeit`可以在这方面提供帮助。
- en: 'The `timeit.repeat` function simply calls the `timeit.timeit` function several
    times and can be emulated using a list comprehension:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`timeit.repeat`函数简单地多次调用`timeit.timeit`函数，可以使用列表推导来模拟：'
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that we know how to test simple code statements, let’s look at how to find
    slow statements in our code.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经知道了如何测试简单的代码语句，那么让我们看看如何找到代码中的慢速语句。
- en: cProfile – Finding the slowest components
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: cProfile – 寻找最慢的组件
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `profile` and `cProfile` modules offer the exact same interface, but the
    latter is written in C and is much faster. I would recommend using `cProfile`
    if it is available on your system. If not, you can safely replace any occurrence
    of `cProfile` with `profile` in the following examples.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`profile`和`cProfile`模块提供了完全相同的接口，但后者是用C编写的，速度要快得多。如果系统上可用，我建议使用`cProfile`。如果不可用，你可以在以下示例中安全地将任何`cProfile`的出现替换为`profile`。'
- en: First profiling run
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 首次分析运行
- en: 'Let’s profile our Fibonacci function from *Chapter 6*, *Decorators – Enabling
    Code Reuse by Decorating*, both with and without the cache function. First, the
    code:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析第6章中的斐波那契函数，*装饰器 – 通过装饰实现代码重用*，既有缓存函数也有无缓存函数的情况。首先，代码：
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: For the sake of readability, all `cProfile` statistics will be stripped of the
    `percall` columns in all `cProfile` outputs. These columns contain the duration
    per function call, which is irrelevant for these examples since they will be either
    0 or identical to the `cumtime` (cumulative time) column in nearly all cases.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高可读性，所有`cProfile`统计信息都将从所有`cProfile`输出中的`percall`列中去除。这些列包含每次函数调用的持续时间，在这些示例中，这些值几乎总是为0或与`cumtime`（累积时间）列相同，因此对于这些示例来说是不相关的。
- en: 'First, we’ll execute the function without cache:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将不使用缓存执行该函数：
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We see `2692557` calls in total, which is quite a lot of calls. We called the
    `test_fibonacci` function nearly 3 million times. That is where the profiling
    modules provide a lot of insight. Let’s analyze the metrics a bit further, in
    the order they appear:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到总共有`2692557`次调用，这相当多。我们几乎调用了300万次`test_fibonacci`函数。这就是分析模块提供大量见解的地方。让我们进一步分析这些指标，按照它们出现的顺序：
- en: '`ncalls`: The number of calls that were made to the function.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ncalls`: 调用该函数的次数。'
- en: '`tottime`: The total time spent in this function, **excluding** the sub-functions.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tottime`: 该函数中花费的总时间，**不包括**子函数。'
- en: '`percall`: The time per call without sub-functions: `tottime / ncalls`.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`percall`: 每次调用（不包括子函数）的时间：`tottime / ncalls`。'
- en: '`cumtime`: The total time spent in this function, **including** sub-functions.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cumtime`：在这个函数中花费的总时间，**包括**子函数。'
- en: '`percall`: The time per call including sub-functions: `cumtime / ncalls`. This
    is distinct from the `percall` metric above, despite having the same name.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`percall`：包括子函数的每次调用时间：`cumtime / ncalls`。这个指标与上面的`percall`指标名称相同，但含义不同。'
- en: 'Which is the most useful depends on your use case. It’s quite simple to change
    the sort order using the `-s` parameter within the default output. But now let’s
    see what the result is with the cached version. Once again, with stripped output:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个最有用取决于你的用例。使用默认输出中的`-s`参数更改排序顺序非常简单。但现在让我们看看使用缓存版本的结果。再次，使用去除输出的方式：
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This time, we see a `tottime` of `0.000` because it’s just too fast to measure.
    But also, while the `fibonacci_cached` function is still the most executed function,
    it’s only being executed 31 times instead of 3 million times.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们看到一个`tottime`为`0.000`，因为它太快而无法测量。此外，尽管`fibonacci_cached`函数仍然是执行次数最多的函数，但它只执行了31次，而不是300万次。
- en: Calibrating your profiler
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 校准剖析器
- en: 'To illustrate the difference between `profile` and `cProfile`, let’s try the
    uncached run again with the `profile` module instead. Just a heads up: this is
    much slower, so don’t be surprised if it stalls a little:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明`profile`和`cProfile`之间的区别，让我们再次尝试使用`profile`模块而不是缓存运行。提醒一下：这会慢得多，所以如果你发现它稍微卡顿，不要感到惊讶：
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The code now runs nearly 10 times more slowly, and the only difference is using
    the pure Python `profile` module instead of the `cProfile` module. This does indicate
    a big problem with the `profile` module. The overhead from the module itself is
    great enough to skew the results, which means we should account for that offset.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 代码现在运行速度慢了近10倍，唯一的区别是使用纯Python的`profile`模块而不是`cProfile`模块。这确实表明`profile`模块存在一个大问题。模块本身的开销足够大，足以扭曲结果，这意味着我们应该考虑到这个偏差。
- en: 'That’s what the `Profile.calibrate()` function takes care of, as it calculates
    the performance bias incurred by the profile module. To calculate the bias, we
    can use the following script:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是`Profile.calibrate()`函数负责的，因为它计算了剖析模块引起的性能偏差。为了计算偏差，我们可以使用以下脚本：
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The numbers will vary slightly, but you should be able to get a fair estimate
    of the performance bias that the `profile` module introduces to your code. It
    effectively runs a bit of code both with and without profiling enabled and calculates
    a multiplier to apply to all results so they are closer to the actual duration.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 数字会有轻微的变化，但你应该能够得到一个关于`profile`模块引入到你的代码中的性能偏差的合理估计。它实际上在启用和禁用剖析的情况下运行了一小段代码，并计算一个乘数，将其应用于所有结果，使它们更接近实际持续时间。
- en: If the numbers still vary a lot, you can increase the trials from `100000` to
    something even larger.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数字仍然变化很大，你可以将试验次数从`100000`增加到更大。
- en: Note that with many modern processors, the burst CPU performance (the first
    few seconds) can vary greatly from the sustained CPU performance (2 minutes or
    more).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在许多现代处理器中，CPU的突发性能（前几秒）与持续性能（2分钟或更长时间）可能会有很大的差异。
- en: The CPU performance is also highly temperature-dependent, so if your system
    has a large CPU cooler or is water-cooled, it can take up to 20 minutes at 100%
    CPU load before the CPU performance becomes consistent. The bias after that 20
    minutes would be completely unusable as a bias for a cold CPU.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: CPU性能也高度依赖于温度，所以如果你的系统有一个大型的CPU散热器或者水冷，在100% CPU负载下，它可能需要20分钟才能使CPU性能变得一致。那20分钟之后的偏差将完全无法作为冷CPU的偏差使用。
- en: 'This type of calibration only works for the `profile` module and should help
    a lot in achieving more accurate results. The bias can be set globally for all
    newly created profilers:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这种校准类型仅适用于`profile`模块，并且应该有助于实现更准确的结果。对于所有新创建的剖析器，偏差可以全局设置：
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Or for a specific `Profile` instance:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，对于特定的`Profile`实例：
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Note that in general, a smaller bias is better to use than a large one because
    a large bias could cause very strange results. If the bias is large enough, you
    will even get negative timings. Let’s give it a try for our Fibonacci code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，一般来说，使用较小的偏差比使用较大的偏差更好，因为大的偏差可能会导致非常奇怪的结果。如果偏差足够大，你甚至可能会得到负的时间值。让我们在我们的斐波那契代码上试一试：
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'While running it, it indeed appears that we’ve used a bias that’s too large:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行时，确实看起来我们使用了一个太大的偏差：
- en: '[PRE20]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Still, it shows how the code can be used properly. You can even incorporate
    the bias calculation within the script using a snippet like this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，它仍然显示了代码的正确使用方法。您甚至可以在脚本中使用类似这样的片段将偏差计算包含在内：
- en: '[PRE21]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Selective profiling using decorators
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用装饰器的选择性分析
- en: 'Calculating simple timings is easy enough using decorators, but profiling can
    show a lot more and can also be applied selectively using decorators or context
    wrappers. Let’s look at a `timer` and a `profiler` decorator:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用装饰器计算简单的计时很容易，但分析可以显示更多内容，并且也可以通过装饰器或上下文包装器有选择性地应用。让我们看看 `timer` 和 `profiler`
    装饰器：
- en: '[PRE23]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now that we have created the decorators, we can profile and time our functions
    with them:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了装饰器，我们可以使用它们来分析和计时我们的函数：
- en: '[PRE24]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The code is simple enough: just a basic `timer` and `profiler` decorator printing
    some default statistics. Which functions best for you depends on your use case,
    of course. The `timer()` decorator is very useful for quick performance tracking
    and/or a sanity check while developing. The `profiler()` decorator is great while
    you are actively working on the performance characteristics of a function.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 代码很简单：只是一个基本的 `timer` 和 `profiler` 装饰器打印一些默认统计信息。哪个最适合您取决于您的用例，当然。`timer()`
    装饰器在开发过程中用于快速性能跟踪和/或合理性检查非常有用。`profiler()` 装饰器在您积极工作于函数的性能特征时非常出色。
- en: 'The added advantage of this selective profiling is that the output is more
    limited, which helps with readability, albeit still much more verbose than the
    `timer()` decorator:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这种选择性分析的附加优势是输出更有限，这有助于可读性，尽管仍然比 `timer()` 装饰器冗长得多：
- en: '[PRE25]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As you can see, the profiler still makes the code about twice as slow, but it’s
    definitely usable.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，分析器仍然使代码大约慢了两倍，但绝对是可用的。
- en: Using profile statistics
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用配置文件统计信息
- en: To get slightly more interesting profiling results, we will profile using the
    `pyperformance.benchmarks.bm_float` script.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更有趣的分析结果，我们将使用 `pyperformance.benchmarks.bm_float` 脚本进行分析。
- en: The `pyperformance` library is the official Python benchmarks library optimized
    for the CPython interpreter. It contains a large (ever-growing) list of benchmarks
    to monitor the performance of the CPython interpreter under many scenarios.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`pyperformance` 库是针对 CPython 解释器优化的官方 Python 基准测试库。它包含大量（持续增长）的基准测试，以监控 CPython
    解释器在许多场景下的性能。'
- en: 'It can be installed through `pip`:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过 `pip` 进行安装：
- en: '[PRE26]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'First, let’s create the statistics using this script:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用此脚本创建统计信息：
- en: '[PRE27]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'When executing the script, you should get something like this:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行脚本时，您应该得到类似以下内容：
- en: '[PRE28]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: After running the script, you should have a `bm_float.profile` file containing
    the profiling results. As we can see in the script, these statistics can be viewed
    through the `pstats` module.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本后，您应该有一个包含分析结果的 `bm_float.profile` 文件。正如我们在脚本中所见，这些统计信息可以通过 `pstats` 模块查看。
- en: In some cases, it can be interesting to combine the results from multiple measurements.
    That is possible by specifying multiple files or by using `stats.add(*filenames)`.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，结合多个测量的结果可能很有趣。这可以通过指定多个文件或使用 `stats.add(*filenames)` 实现。
- en: The main advantage of saving these profile results to files is that several
    applications support this output and can visualize it in a clearer way. One option
    is SnakeViz, which uses your web browser to render the profile results interactively.
    Also, we have QCacheGrind, a very nice visualizer for profile statistics, but
    which requires some manual compiling to get running or some searching for binaries
    of course.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些配置文件结果保存到文件中的主要优势是，多个应用程序支持这种输出，并且可以以更清晰的方式可视化它。一个选项是 SnakeViz，它使用您的网络浏览器以交互方式渲染配置文件结果。此外，我们还有
    QCacheGrind，这是一个非常好的配置文件统计信息可视化器，但需要一些手动编译才能运行，或者当然需要寻找二进制文件。
- en: Let’s look at the output from QCacheGrind. In the case of Windows, the QCacheGrindWin
    package provides a binary, whereas within Linux it is most likely available through
    your package manager, and with OS X you can try `brew install qcachegrind`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 QCacheGrind 的输出。在 Windows 的情况下，QCacheGrindWin 软件包提供了一个二进制文件，而在 Linux 中，它很可能通过您的软件包管理器提供，而在
    OS X 中，您可以尝试 `brew install qcachegrind`。
- en: 'However, there is one more package you will require: the `pyprof2calltree`
    package. It transforms the `profile` output into a format that QCacheGrind understands.
    So, after a simple `pip install pyprof2calltree`, we can now convert the `profile`
    file into a `callgrind` file:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，您还需要一个额外的软件包：`pyprof2calltree` 软件包。它将 `profile` 输出转换为 QCacheGrind 可以理解的格式。因此，在简单的
    `pip install pyprof2calltree` 之后，我们现在可以将 `profile` 文件转换为 `callgrind` 文件：
- en: '[PRE29]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This results in the running of the `QCacheGrind` application. After switching
    to the appropriate tabs, you should see something like the following screenshot:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这会导致`QCacheGrind`应用程序的运行。切换到适当的标签后，你应该能看到以下截图类似的内容：
- en: '![](img/B15882_12_01.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B15882_12_01.png)'
- en: 'Figure 12.1: QCacheGrind'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1：QCacheGrind
- en: For a simple script such as this, pretty much all output works. However, with
    full applications, a tool such as QCacheGrind is invaluable. Looking at the output
    generated by QCacheGrind, it is immediately obvious which process took the most
    time. The structure at the top right shows bigger rectangles if the amount of
    time taken was greater, which is a very useful visualization of the chunks of
    CPU time that were used. The list at the left is very similar to `cProfile` and
    therefore nothing new. The tree at the bottom right can be very valuable or very
    useless, as it is in this case. It shows you the percentage of CPU time taken
    in a function and, more importantly, the relationship of that function with the
    other functions.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这样一个简单的脚本，几乎所有的输出都是有效的。然而，对于完整的应用程序，像QCacheGrind这样的工具是无价的。查看QCacheGrind生成的输出，可以立即看出哪个进程花费了最多时间。右上角的布局显示，如果花费的时间更多，则更大的矩形，这是对使用的CPU时间块非常有用的可视化。左边的列表与`cProfile`非常相似，因此没有什么新内容。右下角的树可能非常有价值，也可能毫无价值，就像在这个例子中一样。它显示了函数中占用的CPU时间百分比，更重要的是，该函数与其他函数的关系。
- en: Because these tools scale depending on the input, the results are useful for
    just about any application. Whether a function takes 100 milliseconds or 100 minutes
    makes no difference – the output will show a clear overview of the slow parts,
    which is what we will try to fix.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些工具根据输入进行扩展，因此结果对几乎所有应用程序都很有用。无论函数需要100毫秒还是100分钟，都没有区别——输出将清楚地显示慢的部分，这正是我们试图修复的部分。
- en: Line profiler – Tracking performance per line
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 行性能分析器 – 按行跟踪性能
- en: '`line_profiler` is actually not a package that’s bundled with Python, but it’s
    far too useful to ignore. While the regular `profile` module profiles all (sub)functions
    within a certain block, `line_profiler` allows for profiling line *per line* within
    a function. The Fibonacci function is not best suited here, but we can use a prime
    number generator instead. But first, install `line_profiler`:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`line_profiler`实际上不是一个与Python捆绑的包，但它非常实用，不能忽视。虽然常规的`profile`模块在某个块内对所有的（子）函数进行性能分析，但`line_profiler`允许对函数中的每一行进行逐行性能分析。斐波那契函数在这里并不适用，但我们可以使用素数生成器。但首先，安装`line_profiler`：'
- en: '[PRE30]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now that we have installed the `line_profiler` module (and with that the `kernprof`
    command), let’s test `line_profiler`:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了`line_profiler`模块（以及`kernprof`命令），让我们测试`line_profiler`：
- en: '[PRE31]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'You might be wondering where the `profile` decorator is coming from. It originates
    from the `line_profiler` module, which is why we have to run the script with the
    `kernprof` command:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道`profile`装饰器是从哪里来的。它起源于`line_profiler`模块，这就是为什么我们必须使用`kernprof`命令运行脚本的原因：
- en: '[PRE32]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As the command says, the results have been written to the `T_08_line_profiler.py.lprof`
    file, so we can now look at the output of that file. For readability, we’ve skipped
    the `Line #` column:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '正如命令所说，结果已经写入`T_08_line_profiler.py.lprof`文件，因此我们现在可以查看该文件的输出。为了便于阅读，我们已跳过`Line
    #`列：'
- en: '[PRE33]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Wonderful output, isn’t it? It makes it trivial to find the slow part within
    a bit of code. Within this code, the slowness is obviously originating from the
    loop, but within other code it might not be that clear.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 多么棒的输出，不是吗？它使得在一段代码中找到慢的部分变得非常简单。在这段代码中，缓慢的原因显然来自循环，但在其他代码中可能并不那么明显。
- en: This module can be added as an IPython extension as well, which enables the
    `%lprun` command within IPython. To load the extension, the `load_ext` command
    can be used from the IPython shell, `%load_ext line_profiler`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 此模块也可以作为IPython扩展添加，这将在IPython中启用`%lprun`命令。要从IPython shell中加载扩展，可以使用`load_ext`命令，`%load_ext
    line_profiler`。
- en: We have seen several methods of measuring CPU performance and execution time.
    Now it’s time to look at how to improve performance. Since this largely applies
    to CPU performance and not memory performance, we will cover that first. Later
    in this chapter, we will take a look at memory usage and leaks.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了几种测量CPU性能和执行时间的方法。现在是时候看看如何提高性能了。由于这主要适用于CPU性能而不是内存性能，我们将首先介绍这一点。在本章的后面部分，我们将探讨内存使用和泄漏。
- en: Improving execution time
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高执行时间
- en: Much can be said about performance optimization, but truthfully, if you have
    read the entire book up to this point, you know most of the Python-specific techniques
    for writing fast code. The most important factor in overall application performance
    will always be the choice of algorithms and, by extension, the data structures.
    Searching for an item within a `list (O(n))` is almost always a worse idea than
    searching for an item in a `dict` or `set (O(1))`, as we have seen in *Chapter
    4*.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 关于性能优化可以有很多说法，但说实话，如果你已经阅读了这本书到目前为止的所有内容，你就知道大多数Python特定的快速代码编写技术。整体应用程序性能最重要的因素始终是算法的选择，以及由此扩展的数据结构。在`list`（`O(n)`）中搜索一个项目几乎总是比在`dict`或`set`（`O(1)`）中搜索一个项目更糟糕，正如我们在*第4章*中看到的。
- en: 'Naturally, there are more factors and tricks that can help make your application
    faster. The extremely abbreviated version of all performance tips is quite simple,
    however: do as little as possible. No matter how fast you make your calculations
    and operations, doing nothing at all will always be faster. The following sections
    cover some of the most common performance bottlenecks in Python and test a few
    common assumptions about performance, such as the performance of `try`/`except`
    blocks versus `if` statements, which can have a huge impact in many languages.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 自然，还有更多因素和技巧可以帮助使你的应用程序更快。然而，所有性能提示的极度简略版本非常简单：尽可能少做。无论你使计算和操作多快，什么都不做总是最快的。以下章节将涵盖Python中最常见的性能瓶颈，并测试一些关于性能的常见假设，例如`try`/`except`块与`if`语句的性能，这在许多语言中可能产生巨大的影响。
- en: Some of the tricks in this section will be a trade-off between memory and execution
    time; others will trade readability with performance. When in doubt, go for readability
    by default and only improve performance if you have to.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的一些技巧将在内存和执行时间之间进行权衡；其他技巧则会在可读性和性能之间进行权衡。当不确定时，默认选择可读性，并且只有在必要时才提高性能。
- en: Using the right algorithm
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用正确的算法
- en: 'Within any application, the right choice of algorithm is by far the most important
    performance characteristic, which is why I am repeating it to illustrate the results
    of a bad choice. Consider the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何应用程序中，选择正确的算法无疑是最重要的性能特征，这就是为什么我要重复强调这一点，以说明错误选择的结果。考虑以下情况：
- en: '[PRE34]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Checking whether an item is within a `list` is an `O(n)` operation, and checking
    whether an item is within a `dict` is an `O(1)` operation. This makes a huge difference
    when `n=1000000`; in this simple test, we can see that for 1 million items, it’s
    300,000 times faster.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 检查一个项目是否在`list`中是一个`O(n)`操作，而检查一个项目是否在`dict`中是一个`O(1)`操作。当`n=1000000`时，这会带来巨大的差异；在这个简单的测试中，我们可以看到对于一百万个项目，它快了300,000倍。
- en: The big-O notation ( `O(...)`) is covered in more detail in *Chapter 4*, but
    we can provide a quick recap.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 大O符号（`O(...)`）在*第4章*中有更详细的介绍，但我们可以提供一个快速回顾。
- en: '`O(n)` means that for a `list` with `len(some_list) = n`, it will take `n`
    steps to perform the operation. Consequently, `O(1)` means that it takes a constant
    amount of time regardless of the size of the collection.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`O(n)`表示对于`len(some_list) = n`的`list`，执行操作需要`n`步。因此，`O(1)`表示无论集合的大小如何，它都需要恒定的时间。'
- en: All other performance tips combined might make your code twice as fast, but
    using the right algorithm for the job can cause a much greater improvement. Using
    an algorithm that takes `O(n)` time instead of `O(n`²`)` time will make your code
    `1000` times faster for `n=1000`, and with a larger `n`, the difference only grows
    further.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 所有其他性能提示加在一起可能使你的代码速度提高一倍，但使用适合工作的正确算法可以带来更大的改进。使用一个需要`O(n)`时间而不是`O(n²)`时间的算法，当`n=1000`时，将使你的代码快`1000`倍，而对于更大的`n`，差异只会进一步扩大。
- en: Global interpreter lock
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全局解释器锁
- en: One of the most obscure components of the CPython interpreter is the **global
    interpreter lock** (**GIL**), a **mutual exclusion lock** (**mutex**) required
    to prevent memory corruption. The Python memory manager is not thread-safe, which
    is why the GIL is needed. Without the GIL, multiple threads might alter memory
    at the same time, causing all sorts of unexpected and potentially dangerous results.
    The GIL is covered in much more detail in *Chapter 14*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: CPython解释器中最神秘的部分之一是**全局解释器锁**（**GIL**），这是一个**互斥锁**（**mutex**），用于防止内存损坏。Python内存管理器不是线程安全的，这就是为什么需要GIL。没有GIL，多个线程可能会同时更改内存，导致各种意外和可能危险的结果。GIL在*第14章*中有更详细的介绍。
- en: What is the impact of the GIL in a real-life application? Within single-threaded
    applications, it makes no difference whatsoever and is actually an extremely fast
    method for memory consistency.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Within multithreaded applications, however, it can slow your application down
    a bit because only a single thread can access the GIL at a time. If your code
    has to access the GIL a lot, it might benefit from some restructuring.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, Python offers a few other options for parallel processing. The `asyncio`
    module, which we will see in *Chapter 13*, can help a lot by switching tasks whenever
    you are waiting for a slow operation. In *Chapter 14*, we will see the `multiprocessing`
    library, which allows us to use multiple processors simultaneously.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: try versus if
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In many languages, a `try/except` type of block incurs quite a performance hit,
    but within Python, this is *not* the case as long as you don’t hit the `except`
    block. If you do hit an `except`, it will be slightly heavier than an `if` statement,
    but not enough to be noticeable in most cases.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: It’s not that an `if` statement is heavy, but if you expect your `try/except`
    to succeed most of the time and only fail in rare cases, it is definitely a valid
    alternative. As always though, focus on readability and conveying the purpose
    of the code. If the intention of the code is clearer using an `if` statement,
    use the `if` statement. If `try`/`except` conveys the intention in a better way,
    use that.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'Most programming languages depend on the use of the **Look Before You Leap**
    (**LBYL**) ideology. This means that you always check before you try, so if you
    are getting `some_key` from a `dict`, you use:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Because you are always doing the `if`, it hints that `some_key` is usually not
    part of `some_dict`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'Within Python, it is common to use the **Easier to Ask for Forgiveness than
    Permission** (**EAFP**) ideology when applicable. This means that the code assumes
    everything will work, but still catches errors:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: These two examples function mostly the same, but the latter gives the idea that
    you expect the key to be available and will catch errors if needed. This is one
    of the cases where the Zen of Python (explicit is better than implicit) applies.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'The only caveat of the code above is that you might accidentally catch a `KeyError`
    from `process_value()`, so if you want to avoid that you should use the following
    code instead:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Which one you use comes mostly down to personal preference, but the takeaway
    should be that, with Python, both options are perfectly valid and will perform
    similarly.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Lists versus generators
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluating code lazily using generators is almost always a better idea than
    calculating the entire dataset. The most important rule of performance optimization
    is probably that you shouldn’t calculate anything you’re not going to use. If
    you’re not sure that you are going to need it, don’t calculate it.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Don’t forget that you can easily chain multiple generators, so everything is
    calculated only when it’s actually needed. Do be careful that this won’t result
    in recalculation though; `itertools.tee()` is generally a better idea than recalculating
    your results completely.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap `itertools.tee()` from *Chapter 7*, a regular generator can only be
    consumed once, so if you need to process the results two or more times, you can
    use `itertools.tee()` to store the intermediate results:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: As you can see, if you forget to use `itertools.tee()` here, you would only
    process the results once, and both would process different values. The alternative
    fix is to use `list()` and store the intermediate results, but this can cost much
    more memory, and you are required to pre-calculate all items without knowing whether
    you actually need them all.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: String concatenation
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You might have seen benchmarks saying that using `+=` is much slower than joining
    strings because the `str` object (as is the case with `bytes`) is immutable. The
    result is that every time you do `+=` on a string, it will have to create a new
    object. At one point, this made quite a lot of difference indeed. With Python
    3, however, most of the differences have vanished:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: There are still some differences, of course, but they are so small that I recommend
    you simply ignore them and choose the most readable option instead.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Addition versus generators
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As is the case with string concatenation, addition from a loop was significantly
    slower with older Python versions, but the difference is now too small to consider:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: What does help, though, is letting Python handle everything internally using
    native functions, as can be seen in the last example.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Map versus generators and list comprehensions
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once again, readability generally counts more than performance, so only rewrite
    for performance if it really makes a difference. There are a few cases where `map()`
    is faster than list comprehensions and generators, but only if the `map()` function
    can use a predefined function. As soon as you need to whip out `lambda`, it’s
    actually slower. Not that it matters much, since readability should be key anyhow.
    If `map()` makes your code more readable than a generator or list comprehension,
    feel free to use it. Otherwise, I would not recommend it:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As you can see, the list comprehension is quite a bit faster than the generator.
    In many cases, I would still recommend the generator over the list comprehension,
    though, if only because of the memory usage and the potential laziness.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: If, for some reason, you are only going to use the first 10 items when generating
    1,000 items, you’re still wasting a lot of resources by calculating the full list
    of items.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Caching
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have already covered the `functools.lru_cache` decorator in *Chapter 6*,
    *Decorators – Enabling Code Reuse by Decorating*, but its importance should not
    be underestimated. Regardless of how fast and smart your code is, not having to
    calculate results is always better and that’s what caching does. Depending on
    your use case, there are many options available. Within a simple script, `functools.lru_cache`
    is a very good contender, but between multiple executions of an application, the
    `cPickle` module can be a lifesaver as well.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: We have already seen the effects of this with the `fibonacci_cached` function
    in the `cProfile` section of this chapter, which uses `functools.lru_cache()`.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several scenarios where you need a more powerful solution, however:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: If you need caching between multiple executions of a script
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you need caching shared across multiple processes
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you need caching shared across multiple servers
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At least for the first two scenarios, you could write the cache to a local pickle/CSV/JSON/YAML/DBM/etc.
    file. This is a perfectly valid solution that I use often.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: If you need a more powerful solution, however, I can highly recommend taking
    a look at **Redis**. The Redis server is a fully in-memory server that is extremely
    fast and has many useful data structures available. If you see articles or tutorials
    about improving performance using Memcached, simply replace Memcached with Redis
    everywhere. Redis is superior to Memcached in every way and, in its most basic
    form, the API is compatible.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Lazy imports
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common problem in application load times is that everything is loaded immediately
    at the start of the program while, with many applications, this is actually not
    needed and certain parts of the application only require loading when they are
    actually used. To facilitate this, you can occasionally move the imports inside
    of functions so they can be loaded on demand.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'While it’s a valid strategy in some cases, I don’t generally recommend it for
    two reasons:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: It makes your code less clear; having all imports in the same style at the top
    of the file improves readability.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It doesn’t make the code faster as it just moves the load time to a different
    part.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using slots
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `__slots__` feature was written by Guido van Rossum to enhance Python performance.
    Effectively what the `__slots__` feature does is specify a fixed list of attributes
    for a class. When `__slots__` are used, several changes are made to a class and
    several (side-)effects must be considered:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: All attributes must be explicitly named in the `__slots__`. It is not possible
    to do `some_instance.some_variable = 123` if `some_variable` is not in `__slots__`.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because the list of attributes is fixed in `__slots__`, there is no longer any
    need for a `__dict__` attribute, which saves memory.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attribute access is faster because there is no intermediate lookup through `__dict__`.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is not possible to use multiple inheritance if both parents have defined
    `__slots__`.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, how much performance benefit can `__slots__` give us? Well, let’s give
    it a test:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'When we actually run this code, we can definitely see some improvements from
    using `__slots__`:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: In most cases, I would argue that the 5-15% difference in performance isn’t
    going to help you that much. However, if it’s applied to a bit of code that is
    near the core of your application and executed very often, it can help.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Don’t expect miracles from this method, but use it when you need it.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Using optimized libraries
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is actually a very broad tip, but useful nonetheless. If there’s a highly
    optimized library that suits your purpose, you most likely won’t be able to beat
    its performance without a significant amount of effort. Libraries such as `numpy`,
    `pandas`, `scipy`, and `sklearn` are highly optimized for performance and their
    native operations can be incredibly fast. If they suit your purpose, be sure to
    give them a try.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you can use `numpy`, you need to install it: `pip3 install numpy.`'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'Just to illustrate how fast `numpy` can be compared to plain Python, refer
    to the following:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The `numpy` code does exactly the same as the Python code, except that it uses
    `numpy` arrays instead of Python lists. This little difference has made the code
    more than 25 times faster.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Just-in-time compiling
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Just-in-time** (**JIT**) compiling is a method of dynamically compiling (parts
    of) an application during runtime. Because there is much more information available
    at runtime, this can have a huge effect and make your application much faster.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to JIT compiling, you currently have three options:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '**Pyston**: An alternative, currently Linux only, CPython-compatible Python
    interpreter.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pypy**: A really fast alternative Python interpreter without full CPython
    compatibility.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Numba**: A package that allows for JIT compiling per function and execution
    on either the CPU or the GPU.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CPython 3.12 and 3.13**? At the time of writing, there is little concrete
    data about the upcoming Python releases, but there are plans to greatly increase
    the CPython interpreter performance. How much will be achieved and how well it
    will work is currently unknown, but the ambitious plan is to make CPython 5x faster
    over the next 5 releases (with 3.10 being the first in the series). The expectation
    is to add JIT compiling in CPython 3.12 and extend that further in 3.13\.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are looking for global JIT compiling in existing projects, I can currently
    recommend trying Pyston. It is a CPython fork that promises about a 30% performance
    increase without having to change any code. In addition, because it is CPython-compatible,
    you can still use regular CPython modules.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: The downside is that it currently only supports Linux systems and, as will always
    be the case with forks, it’s behind the current Python version. At the time of
    writing, CPython is at Python 3.10.1, whereas Pyston is at Python 3.8.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: If compatibility with all CPython modules is not a requirement for you and you
    don’t require Python features that are too recent, PyPy3 can also offer amazing
    performance in many cases. They are up to Python 3.7, whereas the main Python
    release is at 3.10.1 at the time of writing. That makes PyPy roughly 2-3 years
    behind CPython in terms of features, but I doubt this is a big issue. The differences
    between Python 3.7, 3.8, 3.9, and 3.10 are largely incremental and Python 3.7
    is already a very well-rounded Python version.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: The `numba` package provides selective JIT compiling for you, allowing you to
    mark the functions that are JIT compiler-compatible. Essentially, if your functions
    follow the functional programming paradigm of basing the calculations only on
    the input, then it will most likely work with the JIT compiler.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a basic example of how the `numba` JIT compiler can be used:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: If you are using `numpy` or `pandas`, you will most likely benefit from looking
    at `numba`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Another very interesting fact to note is that `numba` supports not only CPU-optimized
    execution, but GPU as well. This means that for certain operations you can use
    the fast processor in your video card to process the results.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Converting parts of your code to C
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will see more about this in *Chapter 17*, *Extensions in C/C++, System Calls,
    and C/C++ Libraries*, but if high performance is really required, then a native
    C function can help quite a lot. This doesn’t even have to be that difficult;
    the Cython module makes it trivial to write parts of your code with performance
    very close to native C code.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example from the Cython manual to approximate the value
    of pi:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: While there are some small differences, such as `cdef` instead of `def` and
    type definitions such as `int i` instead of just `i` for the values and parameters,
    the code is largely the same as regular Python would be, but certainly much faster.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Memory usage
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have simply looked at the execution times and largely ignored the
    memory usage of the scripts. In many cases, the execution times are the most important,
    but memory usage should not be ignored. In almost all cases, CPU and memory are
    traded; an algorithm either uses a lot of CPU time or a lot of memory, which means
    that both do matter a lot.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Within this section, we are going to look at:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing memory usage
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When Python leaks memory and how to avoid these scenarios
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to reduce memory usage
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tracemalloc
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Monitoring memory usage used to be something that was only possible through
    external Python modules such as **Dowser** or **Heapy**. While those modules still
    work, they are partially obsolete now because of the `tracemalloc` module. Let’s
    give the `tracemalloc` module a try to see how easy memory usage monitoring is
    nowadays:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This results in:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: You can easily see how every part of the code allocated memory and where it
    might be wasted. While it might still be unclear which part was actually causing
    the memory usage, there are options for that as well, as we will see in the following
    sections.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Memory Profiler
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `memory_profiler` module is very similar to `line_profiler` discussed earlier,
    but for memory usage instead. Installing it is as easy as `pip install memory_profiler`,
    but the optional `pip install psutil` is also highly recommended (and required
    in the case of Windows) as it increases your performance by a large amount. To
    test `memory_profiler`, we will use the following script:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Note that we actually import `memory_profiler` here although that is not strictly
    required. It can also be executed through `python3 -m memory_profiler your_scripts.py`:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Even though everything runs as expected, you might be wondering about the varying
    amounts of memory used by the lines of code here.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Why does `e` take `9.8 MiB` and `f` `5.0 MiB`? This is caused by the Python
    memory allocation code; it reserves memory in larger blocks, which is subdivided
    and reused internally. Another problem is that `memory_profiler` takes snapshots
    internally, which results in memory being attributed to the wrong variables in
    some cases. The variations should be small enough not to make a large difference
    in the end, but some changes are to be expected.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'This module can be added as an IPython extension as well, which enables the
    `%mprun` command within IPython. To load the extension, the `load_ext` command
    can be used from the IPython shell: `%load_ext memory_profiler`. Another very
    useful command is `%memit`, which is the memory equivalent of the `%timeit` command.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Memory leaks
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The usage of these modules will generally be limited to the search for memory
    leaks. In particular, the `tracemalloc` module has a few features to make that
    fairly easy. The Python memory management system is fairly straightforward; it
    has a simple reference counter to see whether an object is (still) used. While
    this works great in most cases, it can easily introduce memory leaks when circular
    references are involved. The basic premise of a memory leak with leak detection
    code looks like this:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The line numbers in the code above are provided as a reference for the `tracemalloc`
    output and are not functionally part of the code.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: The big problem in this code is that we have two objects that are referencing
    each other. As we can see, `a.b` is referencing `b`, and `b.a` is referencing
    `a`. This loop makes it so that Python doesn’t immediately understand that the
    objects can be safely deleted from memory.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how badly this code is actually leaking:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: This example shows a leak of 22.1 megabytes due to the nearly 200,000 instances
    of `SomeClass`. Python correctly lets us know that this memory was allocated at
    lines 24 and 25, which can really help when trying to ascertain what is causing
    the memory usage in your application.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: The Python garbage collector (`gc`) is smart enough to clean circular references
    like these eventually, but it won’t clean them until a certain limit is reached.
    More about that soon.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Circular references
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Whenever you want to have a circular reference that does not cause memory leaks,
    the `weakref` module is available. It creates references that don’t count toward
    the object reference count. Before we look at the `weakref` module, let’s take
    a look at the object references themselves through the eyes of the Python garbage
    collector (`gc`):'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: First, we create two instances of `SomeClass` and add some circular references
    between them. Once that is done, we delete them from memory, except that they
    are not actually deleted until the garbage collector runs.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: To verify this, we inspect the objects in memory through `gc.get_objects()`,
    and until we tell the garbage collector to manually collect, they stay in memory.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we do run `gc.collect()` to manually call the garbage collector, the objects
    are gone from memory:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Now, you might wonder, are you always required to manually call `gc.collect()`
    to remove these references? No, that is not needed, as the Python garbage collector
    will automatically collect once thresholds have been reached.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: By default, the thresholds for the Python garbage collector are set to `700,
    10, 10` for the three generations of collected objects. The collector keeps track
    of all the memory allocations and deallocations in Python, and as soon as the
    number of allocations minus the number of deallocations reaches `700`, the object
    is either removed if it’s not referenced anymore, or it is moved to the next generation
    if it still has a reference. The same is repeated for generations 2 and 3, albeit
    with the lower thresholds of 10.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'This begs the question: where and when is it useful to manually call the garbage
    collector? Since the Python memory allocator reuses blocks of memory and only
    rarely releases it, for long-running scripts the garbage collector can be very
    useful. That’s exactly where I recommend its usage: long-running scripts in memory-strapped
    environments and, specifically, right before you **allocate** a large amount of
    memory. If you call the garbage collector before doing a memory-intensive operation,
    you can maximize the amount of reuse of the memory that Python has previously
    reserved.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing memory usage using the garbage collector
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `gc` module can help you a lot when looking for memory leaks as well. The
    `tracemalloc` module can show you the parts that take the most memory in bytes,
    but the `gc` module can help you find the most commonly occurring object types
    (for example, `SomeClass`, `int`, and `list`). Just be careful when setting the
    garbage collector debug settings such as `gc.set_debug(gc.DEBUG_LEAK)`; this returns
    a large amount of output even if you don’t reserve any memory yourself. Let’s
    see the output for one of the most basic scripts you can get:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Now, when we run the code, you can see a bit of what has been added to our
    memory with such a simple script:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: As you can see, there are actually 42 different types of objects that should
    have been shown here, but even without that, the number of different objects in
    memory is impressive, if you ask me. With just a little bit of extra code, the
    output can quickly explode and become unusable without significant filtering.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Weak references
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An easy method to make the work easier for the garbage collector is to use **weak
    references**. These are references to variables that are not included when counting
    the references to a variable. Since the garbage collector removes an object from
    memory when its reference count gets to zero, this can help a lot with memory
    leaks.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'In the earlier example, we saw that the objects weren’t removed until we manually
    called `gc.collect()`. Now we will see what happens if we use the `weakref` module
    instead:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now let’s see what remained this time:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Perfect – no instances of `SomeClass` exist in memory after `del`, which is
    exactly what we had hoped for.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Weakref limitations and pitfalls
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You might be wondering what happens when you still try to reference a since-removed
    `weakref`. As you would expect, the object is gone now, so you can no longer use
    it. What is more, not all objects can be used through weak references directly:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We can use `weakref` for custom classes though, so we can subclass the types
    before we create the `weakref`:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: For `dict` and `set` instances, the `weakref` library also has the `weakref.WeakKeyDictionary`,
    `weakref.WeakValueDictionary`, and `weakref.WeakSet` classes. These behave similarly
    to the regular instances of `dict` and `set`, but remove the values based on the
    key or value.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to be careful when using a `weakref`, of course. As soon as all regular
    references are deleted, the object will be inaccessible:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: After deleting `a`, which is the only real reference to the `SomeClass` instance,
    we cannot use the instance anymore. While this is to be expected, you should be
    wary of this problem if your main reference has a chance to disappear.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Whenever you are working with large self-referencing data structures, it can
    be a good idea to use the `weakref` module. However, don’t forget to check if
    your instance still exists before using it.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Reducing memory usage
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, memory usage probably won’t be your biggest problem in Python, but
    it can still be useful to know what you can do to reduce it. When trying to reduce
    memory usage, it’s important to understand how Python allocates memory.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four concepts that you need to know about within the Python memory
    manager:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: First, we have the **heap**. The heap is the collection of all Python-managed
    memory. Note that this is separate from the regular heap and mixing the two could
    result in corrupt memory and crashes.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second are the **arenas**. These are the chunks that Python requests from the
    system. These chunks have a fixed size of 256 KiB each and they are the objects
    that make up the heap.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third we have the **pools**. These are the chunks of memory that make up the
    arenas. These chunks are 4 KiB each. Since the pools and arenas have fixed sizes,
    they are simple arrays.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fourth and last, we have the **blocks**. The Python objects get stored within
    these and every block has a specific format depending on the data type. Since
    an integer takes up more space than a character, for efficiency, a different block
    size is used.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know how the memory is allocated, we can also understand how it
    can be returned to the operating system and why this is often very hard to do.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'Releasing a block back to the pool is easy enough: a simple `del some_variable`
    followed by a `gc.collect()` should do the trick. The problem is that this is
    no guarantee that the memory will be released back to the operating system yet.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate what needs to happen in order for the memory to release to the
    operating system:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: All blocks in a pool need to be released before the pool can be released
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All pools in an arena need to be released before the arena can be released
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the arena has been released to the heap, memory *might* be released to
    the operating system, but that depends on the C runtime and/or operating system
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That is why I would always recommend running `gc.collect()` in long-running
    scripts right before you start allocating large blocks of memory.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: It is a common and incorrect misconception that Python never releases any memory
    to the system. Before Python 2.5, this was indeed the case because arenas were
    never freed to the heap.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s illustrate the effects of allocating and releasing memory by allocating
    and releasing twice:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'You might expect that the memory usage after the second block has been released
    will be near identical to after the first block has been released, or even back
    to the original state. Let’s see what actually happens:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: That’s odd, isn’t it? The memory usage has grown between the two allocations.
    The truth is that I cherry-picked the result somewhat and that the output changes
    between each run, because releasing memory back to the operating system is not
    a guarantee that the operating system will immediately handle it. In some other
    cases, the memory had properly returned to 17 MiB.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: The astute among you might wonder if the results are skewed because I forgot
    the `gc.collect()`. In this case, the answer is no because the memory allocation
    is large enough to immediately trigger the garbage collector by itself and the
    difference is negligible.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: This is roughly the best case, however – just a few contiguous blocks of memory.
    The real challenge is when you have many variables so only parts of the pools/arenas
    are used. Python uses some heuristics to find space in an empty arena so it doesn’t
    have to allocate new arenas when you are storing new variables, but that does
    not always succeed, of course. This is a case where running `gc.collect()` before
    allocation can help because it can tell Python which pools are now free.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the regular heap and Python heap are maintained
    separately, as mixing them can result in corruption and/or the crashing of applications.
    Unless you write your own Python extensions in C/C++, you will probably never
    have to worry about manual memory allocation though.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Generators versus lists
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most important tip is to use generators whenever possible. Python 3 has
    come a long way in replacing lists with generators already, but it really pays
    to keep that in mind as it saves not only memory, but CPU as well, when not all
    of that memory needs to be kept at the same time.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the difference:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The `range()` generator takes such little memory that it doesn’t even register,
    whereas the list of numbers takes `38.6 MiB`.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Recreating collections versus removing items
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One very important detail about collections in Python is that many of them
    can only grow; they won’t just shrink by themselves. To illustrate:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Even after removing all items from the `dict`, the memory usage remains the
    same. This is one of the most common memory usage mistakes made with lists and
    dictionaries. The only way to reclaim the memory is by recreating the object.
    Or, never allocate the memory at all by using generators.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Using slots
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to the performance benefits of using `__slots__` that we saw earlier
    in this chapter, `__slots__` can also help to reduce memory usage. As a recap,
    `__slots__` allows you to specify which fields you want to store in a class and
    it skips all the others by not implementing `instance.__dict__`.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: While this method does save a little bit of memory in your class definitions,
    the effect is often limited. For a nearly empty class with just a single/tiny
    attribute such as a `bool` or `byte`, this can make quite a bit of difference.
    For classes that actually store a bit of data, the effect can diminish quickly.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: The biggest caveat of `__slots__` is that multiple inheritance is impossible
    if both parent classes have `__slots__` defined. Beyond that, it can be used in
    nearly all cases.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: You might wonder if `__slots__` will limit dynamic attribute assignments, effectively
    blocking you from doing `Spam.eggs = 123` if `eggs` was not part of `__slots__`.
    And you are right – partially, at least. With a standard fixed list of attributes
    in `__slots__`, you cannot dynamically add new attributes – but you can if you
    add `__dict__` to `__slots__`.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: I’m embarrassed to say that it took me about 15 years before I found out about
    this feature, but knowing about this feature makes `__slots__` so much more versatile
    that I really feel like I should mention it.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now illustrate the difference in memory usage:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'And the memory usage:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: You might argue that this is not a fair comparison since they both store a lot
    of data, which skews the results. And you would indeed be right because the “bare”
    comparison, storing only `index` and nothing else, gives `2 MiB` versus `4.5 MiB`.
    But, let’s be honest, if you’re not going to store data, then what’s the point
    in creating class instances? I’m not saying that `__slots__` have no purpose,
    but don’t go overboard because the advantages are generally limited.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one more structure that’s even more memory-efficient: the `array`
    module. It stores the data in pretty much the same way a bare memory array in
    C would do. Note that this is generally slower than lists and much less convenient
    to use. If you need to store large amounts of numbers, I would suggest looking
    at `numpy.array` or `scipy.sparse` instead.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Performance monitoring
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen how to measure and improve both CPU and memory performance,
    but there is one part we have completely skipped over. Performance changes due
    to external factors such as growing amounts of data are very hard to predict.
    In real-life applications, bottlenecks aren’t constant. They change all the time
    and code that was once extremely fast might bog down as soon as more load is applied.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Because of that, I recommend implementing a monitoring solution that tracks
    the performance of anything and everything over time. The big problem with performance
    monitoring is that you can’t know what will slow down in the future and what the
    cause is going to be. I’ve even had websites slow down because of Memcached and
    Redis calls. These are memory-only caching servers that respond well within a
    millisecond, which makes slowdowns highly unlikely, until you do over 100 cache
    calls and the latency toward the cache server increases from 0.1 milliseconds
    to 2 milliseconds, and all of a sudden those 100 calls take 200 milliseconds instead
    of 10 milliseconds. Even though 200 milliseconds still sounds like very little,
    if your total page load time is generally below 100 milliseconds, that is, all
    of a sudden, an enormous increase and definitely noticeable.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 'To monitor performance and to be able to track changes over time and find the
    responsible components, I can personally recommend several systems for monitoring
    performance:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: For simple short-term (up to a few weeks) application performance tracking,
    the **Prometheus** monitoring system is very easy to set up and when paired with
    **Grafana**, you can create the prettiest charts to monitor your performance.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you want a more long-term performance tracking solution that scales well
    to large numbers of variables, you might be interested in **InfluxDB** instead.
    It can also be paired with Grafana for really useful interactive charting:'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15882_12_02.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.2: Grafana heatmap of response times'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15882_12_03.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.3: Grafana chart of request latency'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: To enter data into systems like these, you have several options. You can use
    the native APIs, but you can also use an intermediate system such as **StatsD**.
    The StatsD system doesn’t store data itself, but it makes it really easy to fire
    and forget performance metrics from your system without having to worry whether
    the monitoring system is still up and running. Because the system commonly uses
    UDP to send the information, even if the monitoring server is completely down
    and unreachable, your application won’t notice the difference.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: To be able to use these, you will have to send the metrics from your application
    to the StatsD server. To do just that, I have written the Python-StatsD ([https://pypi.org/project/python-statsd/](Chapter_12.xhtml))
    and Django-StatsD ([https://pypi.org/project/django-statsd/](Chapter_12.xhtml))
    packages. These packages allow you to monitor your application from beginning
    to end and, in the case of Django, you will be able to monitor your performance
    per application or view, and within those see all of the components, such as the
    database, template, and caching layers. This way, you know exactly what is causing
    the slowdowns in your website (or application). And best of all, it is in (near)
    real time.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you have learned about many of the available tools for performance
    measuring and optimization, try and create a few useful decorators or context
    wrappers that will help you prevent issues:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: Try to create a decorator that monitors each run of a function and warns you
    if the memory usage grows each run.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to create a decorator that monitors the runtime of a function and warns
    you if it deviates too much from the previous run. Optionally, you could make
    the function generate a (running) average runtime as well.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to create a memory manager for your classes that warns you when more than
    a configured number of instances remain in memory. If you never expect more than
    5 instances of a certain class, you can warn the user when that number is exceeded.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example answers for these exercises can be found on GitHub: [https://github.com/mastering-python/exercises](Chapter_12.xhtml).
    You are encouraged to submit your own solutions and learn about alternative solutions
    from others.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-366
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to performance, there is no holy grail, no single thing you can
    do to ensure peak performance in all cases. This shouldn’t worry you, however,
    as in most cases, you will never need to tune the performance and, if you do,
    a single tweak could probably fix your problem. You should be able to find performance
    problems and memory leaks in your code now, which is what matters most, so just
    try to contain yourself and only tweak when it’s actually needed.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a quick recap of the tools in this chapter:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: 'Measuring CPU performance: `timeit`, `profile`/`cProfile`, and `line_profiler`'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Analyzing profiling results: SnakeViz, `pyprof2calltree`, and QCacheGrind'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Measuring memory usage: `tracemalloc`, `memory_profiler`'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reducing memory usage and leaks: `weakref` and `gc` (garbage collector)'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you know how to use these tools, you should be able to track down and fix
    most performance issues in your code.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important takeaways from this chapter are:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: Test before you invest any effort. Making some functions faster seems like a
    great achievement, but it is only rarely needed.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the correct data structure/algorithm is much more effective than any
    other performance optimization.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Circular references drain the memory until the garbage collector starts cleaning.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slots come with several caveats, so I would recommend limited usage.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next chapter will properly introduce us to working asynchronously using
    the `asyncio` module. This module makes it possible to “background” the waiting
    for external I/O. Instead of keeping your foreground thread busy, it can switch
    to a different thread when your code is waiting for endpoints such as TCP, UDP,
    files, and processes.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  id: totrans-380
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers: [https://discord.gg/QMzJenHuJf](Chapter_12.xhtml)'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code156081100001293319171.png)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
