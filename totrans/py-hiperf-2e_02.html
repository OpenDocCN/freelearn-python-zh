<html><head></head><body>
        

            
                <h1 class="header-title">Pure Python Optimizations</h1>
            

            
                
<p>As mentioned in the last chapter, one of the most effective ways of improving the performance of applications is through the use of better algorithms and data structures. The Python standard library provides a large variety of ready-to-use algorithms and data structures that can be directly incorporated in your applications. With the tools learned from this chapter, you will be able to use the right algorithm for the task and achieve massive speed gains.</p>
<p>Even though many algorithms have been around for quite a while, they are especially relevant in today's world as we continuously produce, consume, and analyze ever increasing amounts of data. Buying a larger server or microoptimizing can work for some time, but achieving better scaling through algorithmic improvement can solve the problem once and for all.</p>
<p>In this chapter, we will understand how to achieve better scaling using standard algorithms and data structures. More advanced use cases will also be covered by taking advantage of third-party libraries. We will also learn about tools to implement caching, a technique used to achieve faster response times by sacrificing some space on memory or on disk.</p>
<p>The list of topics to be covered in this chapter is as follows:</p>
<ul>
<li>Introduction to computational complexity</li>
<li>Lists and deques</li>
<li>Dictionaries</li>
<li>How to build an inverted index using a dictionary</li>
<li>Sets</li>
<li>Heaps and priority queues</li>
<li>Implementing autocompletion using tries</li>
<li>Introduction to caching</li>
<li>In-memory caching with the <kbd>functools.lru_cache</kbd> decorator</li>
<li>On-disk cache with <kbd>joblib.Memory</kbd></li>
<li>Fast and memory-efficient loops with comprehensions and generators</li>
</ul>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Useful algorithms and data structures</h1>
            

            
                
<p>Algorithmic improvements are especially effective in increasing performance because they typically allow the application to scale better with increasingly large inputs.</p>
<p>Algorithm running times can be classified according to their computational complexity, a characterization of the resources required to perform a task. Such classification is expressed through the Big-O notation, an upper bound on the operations required to execute the task, which usually depends on the input size.</p>
<p>For example, incrementing each element of a list can be implemented using a <kbd>for</kbd> loop, as follows:</p>
<pre>
    input = list(range(10))<br/>    for i, _ in enumerate(input):<br/>        input[i] += 1 
</pre>
<p>If the operation does not depend on the size of the input (for example, accessing the first element of a list), the algorithm is said to take constant, or <em>O</em>(1), time. This means that, no matter how much data we have, the time to run the algorithm will always be the same.</p>
<p>In this simple algorithm, the <kbd>input[i] += 1</kbd> operation will be repeated 10 times, which is the size of the input. If we double the size of the input array, the number of operations will increase proportionally. Since the number of operations is proportional to the input size, this algorithm is said to take <em>O</em>(<em>N</em>) time, where <em>N</em> is the size of the input array.</p>
<p>In some instances, the running time may depend on the structure of the input (for example, if the collection is sorted or contains many duplicates). In these cases, an algorithm may have different best-case, average-case, and worst-case running times. Unless stated otherwise, the running times presented in this chapter are considered to be average running times.</p>
<p>In this section, we will examine the running times of the main algorithms and data structures that are implemented in the Python standard library, and understand how improving running times results in massive gains and allows us to solve large-scale problems with elegance.</p>
<p>You can find the code used to run the benchmarks in this chapter in the <kbd>Algorithms.ipynb</kbd> notebook, which can be opened using Jupyter.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Lists and deques</h1>
            

            
                
<p>Python lists are ordered collections of elements and, in Python, are implemented as resizable arrays. An array is a basic data structure that consists of a series of contiguous memory locations, and each location contains a reference to a Python object.</p>
<p>Lists shine in accessing, modifying, and appending elements. Accessing or modifying an element involves fetching the object reference from the appropriate position of the underlying array and has complexity <em>O</em>(1). Appending an element is also very fast. When an empty list is created, an array of fixed size is allocated and, as we insert elements, the slots in the array are gradually filled up. Once all the slots are occupied, the list needs to increase the size of its underlying array, thus triggering a memory reallocation that can take <em>O</em>(<em>N</em>) time. Nevertheless, those memory allocations are infrequent, and the time complexity for the append operation is referred to as amortized O(1) time.</p>
<p>The list operations that may have efficiency problems are those that add or remove elements at the beginning (or somewhere in the middle) of the list. When an item is inserted, or removed, from the beginning of a list, all the subsequent elements of the array need to be shifted by a position, thus taking <em>O</em>(<em>N</em>) time.</p>
<p>In the following table, the timings for different operations on a list of size 10,000 are shown; you can see how insertion and removal performances vary quite dramatically if performed at the beginning or at the end of the list:</p>
<table>
<tbody>
<tr>
<td><strong>Code</strong></td>
<td><strong><strong>N=10000 (</strong></strong><strong>µs)</strong></td>
<td><strong><strong>N=20000 (</strong></strong><strong>µs)</strong></td>
<td><strong><strong>N=30000 (</strong></strong><strong>µs)</strong></td>
<td><strong><strong>Time</strong></strong></td>
</tr>
<tr>
<td><kbd>list.pop()</kbd></td>
<td>0.50</td>
<td>0.59</td>
<td>0.58</td>
<td><em>O</em>(1)</td>
</tr>
<tr>
<td><kbd>list.pop(0)</kbd></td>
<td>4.20</td>
<td>8.36</td>
<td>12.09</td>
<td><em>O</em>(<em>N</em>)</td>
</tr>
<tr>
<td><kbd>list.append(1)</kbd></td>
<td>0.43</td>
<td>0.45</td>
<td>0.46</td>
<td><em>O</em>(1)</td>
</tr>
<tr>
<td><kbd>list.insert(0, 1)</kbd></td>
<td>6.20</td>
<td>11.97</td>
<td>17.41</td>
<td><em>O</em>(<em>N</em>)</td>
</tr>
</tbody>
</table>
<p>In some cases, it is necessary to efficiently perform insertion or removal of elements both at the beginning and at the end of the collection. Python provides a data structure with those properties in the <kbd>collections.deque</kbd> class. The word <strong>deque</strong> stands for double-ended queue because this data structure is designed to efficiently put and remove elements at the beginning and at the end of the collection, as it is in the case of queues. In Python, deques are implemented as doubly-linked lists.</p>
<p>Deques, in addition to <kbd>pop</kbd> and <kbd>append</kbd>, expose the <kbd>popleft</kbd> and <kbd>appendleft</kbd> methods that have <em>O</em>(1) running time:</p>
<table>
<tbody>
<tr>
<td><strong>Code</strong></td>
<td><strong><strong>N=10000 (</strong></strong><strong>µs)</strong></td>
<td><strong><strong>N=20000 (</strong></strong><strong>µs)</strong></td>
<td><strong><strong>N=30000 (</strong></strong><strong>µs)</strong></td>
<td><strong><strong>Time</strong></strong></td>
</tr>
<tr>
<td><kbd>deque.pop()</kbd></td>
<td>0.41</td>
<td>0.47</td>
<td>0.51</td>
<td><em>O</em>(1)</td>
</tr>
<tr>
<td><kbd>deque.popleft()</kbd></td>
<td>0.39</td>
<td>0.51</td>
<td>0.47</td>
<td><em>O</em>(1)</td>
</tr>
<tr>
<td><kbd>deque.append(1)</kbd></td>
<td>0.42</td>
<td>0.48</td>
<td>0.50</td>
<td><em>O</em>(1)</td>
</tr>
<tr>
<td><kbd>deque.appendleft(1)</kbd></td>
<td>0.38</td>
<td>0.47</td>
<td>0.51</td>
<td><em>O</em>(1)</td>
</tr>
</tbody>
</table>
<p>Despite these advantages, deques should not be used to replace regular lists in most cases. The efficiency gained by the <kbd>appendleft</kbd> and <kbd>popleft</kbd> operations comes at a cost: accessing an element in the middle of a deque is a O(N) operation, as shown in the following table:</p>
<table>
<tbody>
<tr>
<td><strong>Code</strong></td>
<td><strong><strong>N=10000 (</strong></strong><strong>µs)</strong></td>
<td><strong><strong>N=20000 (</strong></strong><strong>µs)</strong></td>
<td><strong><strong>N=30000 (</strong></strong><strong>µs)</strong></td>
<td><strong><strong>Time</strong></strong></td>
</tr>
<tr>
<td><kbd>deque[0]</kbd></td>
<td>0.37</td>
<td>0.41</td>
<td>0.45</td>
<td><em>O</em>(1)</td>
</tr>
<tr>
<td><kbd>deque[N -  1]</kbd></td>
<td>0.37</td>
<td>0.42</td>
<td>0.43</td>
<td><em>O</em>(1)</td>
</tr>
<tr>
<td><kbd>deque[int(N / 2)]</kbd></td>
<td>1.14</td>
<td>1.71</td>
<td>2.48</td>
<td><em>O</em>(N)</td>
</tr>
</tbody>
</table>
<p>Searching for an item in a list is generally a <em>O</em>(<em>N</em>) operation and is performed using the <kbd>list.index</kbd> method. A simple way to speed up searches in lists is to keep the array sorted and perform a binary search using the <kbd>bisect</kbd> module.</p>
<p>The <kbd>bisect</kbd> module allows fast searches on sorted arrays. The <kbd>bisect.bisect</kbd> function can be used on a sorted list to find the index to place an element while maintaining the array in sorted order. In the following example, we can see that if we want to insert the <kbd>3</kbd> element in the array while keeping <kbd>collection</kbd> in sorted order, we should put <kbd>3</kbd> in the third position (which corresponds to index 2):</p>
<pre>
    insert bisect<br/>    collection = [1, 2, 4, 5, 6]<br/>    bisect.bisect(collection, 3)<br/>    # Result: 2
</pre>
<p>This function uses the binary search algorithm that has <em>O</em>(<em>log</em>(<em>N</em>)) running time. Such a running time is exceptionally fast, and basically means that your running time will increase by a constant amount every time you <em>double</em> your input size. This means that if, for example, your program takes <kbd>1</kbd> second to run on an input of size <kbd>1000</kbd>, it will take <kbd>2</kbd> seconds to process an input of size <kbd>2000</kbd>, <kbd>3</kbd> seconds to process an input of size <kbd>4000</kbd>, and so on. If you had <kbd>100</kbd> seconds, you could theoretically process an input of size <kbd>10<sup>33</sup></kbd>, which is larger than the number of atoms in your body!</p>
<p>If the value we are trying to insert is already present in the list, the <kbd>bisect.bisect</kbd> function will return the location <em>after</em> the already present value.  Therefore, we can use the <kbd>bisect.bisect_left</kbd> variant, which returns the correct index in the following way (taken from the module documentation at <a href="https://docs.python.org/3.5/library/bisect.html">https://docs.python.org/3.5/library/bisect.html</a>):</p>
<pre>
    def index_bisect(a, x):<br/>      'Locate the leftmost value exactly equal to x'<br/>      i = bisect.bisect_left(a, x)<br/>      if i != len(a) and a[i] == x:<br/>      return i<br/>      raise ValueError
</pre>
<p>In the following table, you can see how the running time of the <kbd>bisect</kbd> solution is barely affected at these input sizes, making it a suitable solution when searching through very large collections:</p>
<table>
<tbody>
<tr>
<td><strong>Code</strong></td>
<td><strong>N=10000 (</strong><strong>µs)</strong></td>
<td><strong>N=20000 (</strong><strong>µs)</strong></td>
<td><strong>N=30000 (</strong><strong>µs)</strong></td>
<td><strong>Time</strong></td>
</tr>
<tr>
<td><kbd>list.index(a)</kbd></td>
<td>87.55</td>
<td>171.06</td>
<td>263.17</td>
<td><em>O</em>(N)</td>
</tr>
<tr>
<td><kbd>index_bisect(list, a)</kbd></td>
<td>3.16</td>
<td>3.20</td>
<td>4.71</td>
<td><em>O</em>(log(N))</td>
</tr>
</tbody>
</table>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Dictionaries</h1>
            

            
                
<p>Dictionaries are extremely versatile and extensively used in the Python language. Dictionaries are implemented as hash maps and are very good at element insertion, deletion, and access; all these operations have an average <em>O</em>(1) time complexity. </p>
<p>In Python versions up to 3.5, dictionaries are unordered collections. Since Python 3.6, dictionaries are capable of maintaining their elements by order of insertion.</p>
<p>A hash map is a data structure that associates a set of key-value pairs. The principle behind hash maps is to assign a specific index to each key so that its associated value can be stored in an array. The index can be obtained through the use of a <kbd>hash</kbd> function; Python implements hash functions for several data types. As a demonstration, the generic function to obtain hash codes is <kbd>hash</kbd>. In the following example, we show you how to obtain the hash code given the <kbd>"hello"</kbd> string:</p>
<pre>
    hash("hello")<br/>    # Result: -1182655621190490452<br/><br/>    # To restrict the number to be a certain range you can use<br/>    # the modulo (%) operator<br/>    hash("hello") % 10<br/>    # Result: 8
</pre>
<p>Hash maps can be tricky to implement because they need to handle collisions that happen when two different objects have the same hash code. However, all the complexity is elegantly hidden behind the implementation and the default collision resolution works well in most real-world scenarios.</p>
<p>Access, insertion, and removal of an item in a dictionary scales as <em>O</em>(1) with the size of the dictionary. However, note that the computation of the hash function still needs to happen and, for strings, the computation scales with the length of the string. As string keys are usually relatively small, this doesn't constitute a problem in practice.</p>
<p>A dictionary can be used to efficiently count unique elements in a list. In this example, we define the <kbd>counter_dict</kbd> function that takes a list and returns a dictionary containing the number of occurrences of each value in the list:</p>
<pre>
    def counter_dict(items): <br/>        counter = {} <br/>        for item in items: <br/>            if item not in counter: <br/>                counter[item] = 0 <br/>            else: <br/>                counter[item] += 1 <br/>        return counter
</pre>
<p>The code can be somewhat simplified using <kbd>collections.defaultdict</kbd>, which can be used to produce dictionaries where each new key is automatically assigned a default value. In the following code, the <kbd>defaultdict(int)</kbd> call produces a dictionary where every new element is automatically assigned a zero value, and can be used to streamline the counting:</p>
<pre>
    from collections import defaultdict<br/>    def counter_defaultdict(items):<br/>        counter = defaultdict(int)<br/>        for item in items:<br/>            counter[item] += 1<br/>        return counter
</pre>
<p>The <kbd>collections</kbd> module also includes a <kbd>Counter</kbd> class that can be used for the same purpose with a single line of code:</p>
<pre>
    from collections import Counter<br/>    counter = Counter(items)
</pre>
<p>Speed-wise, all these ways of counting have the same time complexity, but the <kbd>Counter</kbd> implementation is the most efficient, as shown in the following table:</p>
<table style="width: 628px;height: 191px">
<tbody>
<tr>
<td><strong>Code</strong></td>
<td><strong>N=1000 (</strong><strong>µs)</strong></td>
<td><strong>N=2000 (</strong><strong>µs)</strong></td>
<td><strong>N=3000 (</strong><strong>µs)</strong></td>
<td><strong>Time</strong></td>
</tr>
<tr>
<td><kbd>Counter(items)</kbd></td>
<td>51.48</td>
<td>96.63</td>
<td>140.26</td>
<td><em>O</em>(N)</td>
</tr>
<tr>
<td><kbd>counter_dict(items)</kbd></td>
<td>111.96</td>
<td>197.13</td>
<td>282.79</td>
<td><em>O</em>(N)</td>
</tr>
<tr>
<td><kbd>counter_defaultdict(items)</kbd></td>
<td>120.90</td>
<td>238.27</td>
<td>359.60</td>
<td><em>O</em>(N)</td>
</tr>
</tbody>
</table>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Building an in-memory search index using a hash map</h1>
            

            
                
<p>Dictionaries can be used to quickly search for a word in a list of documents, similar to a search engine. In this subsection, we will learn how to build an inverted index based on a dictionary of lists. Let's say we have a collection of four documents:</p>
<pre>
    docs = ["the cat is under the table",<br/>            "the dog is under the table",<br/>            "cats and dogs smell roses",<br/>            "Carla eats an apple"]
</pre>
<p>A simple way to retrieve all the documents that match a query is to scan each document and test for the presence of a word. For example, if we want to look up the documents where the word <kbd>table</kbd> appears, we can employ the following filtering operation:</p>
<pre>
    matches = [doc for doc in docs if "table" in doc]
</pre>
<p>This approach is simple and works well when we have one-off queries; however, if we need to query the collection very often, it can be beneficial to optimize querying time. Since the per-query cost of the linear scan is <em>O</em>(<em>N</em>), you can imagine that a better scaling will allow us to handle much larger document collections.</p>
<p>A better strategy is to spend some time preprocessing the documents so that they are easier to find at query time. We can build a structure, called the <strong>inverted index</strong>, that associates each word in our collection with the list of documents where that word is present. In our earlier example, the word <kbd>"table"</kbd> will be associated to the <kbd>"the cat is under the table"</kbd> and <kbd>"the dog is under the table"</kbd> documents; they correspond to indices <kbd>0</kbd> and <kbd>1</kbd>.</p>
<p>Such a mapping can be implemented by going over our collection of documents and storing in a dictionary the index of the documents where that term appears. The implementation is similar to the <kbd>counter_dict</kbd> function, except that, instead of accumulating a counter, we are growing the list of documents that match the current term:</p>
<pre>
    # Building an index<br/>    index = {}<br/>    for i, doc in enumerate(docs):<br/>        # We iterate over each term in the document<br/>        for word in doc.split():<br/>            # We build a list containing the indices <br/>            # where the term appears<br/>            if word not in index:<br/>                index[word] = [i]<br/>            else:<br/>                index[word].append(i)
</pre>
<p>Once we have built our index, doing a query involves a simple dictionary lookup. For example, if we want to return all the documents containing the term table, we can simply query the index, and retrieve the corresponding documents:</p>
<pre>
    results = index["table"]<br/>    result_documents = [docs[i] for i in results]
</pre>
<p>Since all it takes to query our collection is a dictionary access, the index can handle queries with time complexity <em>O</em>(1)! Thanks to the inverted index, we are now able to query any number of documents (as long as they fit in memory) in constant time. Needless to say, indexing is a technique widely used to quickly retrieve data not only in search engines, but also in databases and any system that requires fast searches.</p>
<p>Note that building an inverted index is an expensive operation and requires you to encode every possible query. This is a substantial drawback, but the benefits are great and it may be worthwhile to pay the price in terms of decreased flexibility.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Sets</h1>
            

            
                
<p>Sets are unordered collections of elements, with the additional restriction that the elements must be unique.  The main use-cases where sets are a good choice are membership tests (testing if an element is present in the collection) and, unsurprisingly, set operations such as union, difference, and intersection.</p>
<p>In Python, sets are implemented using a hash-based algorithm just like dictionaries; therefore, the time complexities for addition, deletion, and test for membership scale as <em>O</em>(1) with the size of the collection.</p>
<p>Sets contain only unique elements. An immediate use case of sets is the removal of duplicates from a collection, which can be accomplished by simply passing the collection through the <kbd>set</kbd> constructor, as follows:</p>
<pre>
    # create a list that contains duplicates<br/>    x = list(range(1000)) + list(range(500))<br/>    # the set *x_unique* will contain only <br/>    # the unique elements in x<br/>    x_unique = set(x)
</pre>
<p>The time complexity for removing duplicates is <em>O</em>(<em>N</em>), as it requires to read the input and put each element in the set.</p>
<p>Sets expose a number of operations like union, intersection, and difference. The union of two sets is a new set containing all the elements of both the sets; the intersection is a new set that contains only the elements in common between the two sets, and the difference is a new set containing the element of the first set that are not contained in the second set. The time complexities for these operations are shown in the following table. Note that since we have two different input sizes, we will use the letter S to indicate the size of the first set (called <kbd>s</kbd>), and T to indicate the size of the second set (called <kbd>t</kbd>):</p>
<table>
<tbody>
<tr>
<td><strong>Code</strong></td>
<td><strong>Time</strong></td>
</tr>
<tr>
<td><kbd>s.union(t)</kbd></td>
<td><em>O</em>(<em>S</em> + <em>T</em>)</td>
</tr>
<tr>
<td><kbd>s.intersection(t)</kbd></td>
<td><em>O</em>(<em>min</em>(<em>S</em>, <em>T</em>))</td>
</tr>
<tr>
<td><kbd>s.difference(t)</kbd></td>
<td><em>O</em>(<em>S</em>)</td>
</tr>
</tbody>
</table>
<p>An application of set operations are, for example, Boolean queries. Going back to the inverted index example of the previous subsection, we may want to support queries that include multiple terms. For example, we may want to search for all the documents that contain the words <kbd>cat</kbd> and <kbd>table</kbd>. This kind of a query can be efficiently computed by taking the intersection between the set of documents containing <kbd>cat</kbd> and the set of documents containing <kbd>table</kbd>.</p>
<p>In order to efficiently support those operations, we can change our indexing code so that each term is associated to a set of documents (rather than a list). After applying this change, calculating more advanced queries is a matter of applying the right set operation. In the following code, we show the inverted index based on sets and the query using set operations:</p>
<pre>
    # Building an index using sets<br/>    index = {}<br/>    for i, doc in enumerate(docs):<br/>        # We iterate over each term in the document<br/>        for word in doc.split():<br/>            # We build a set containing the indices <br/>            # where the term appears<br/>            if word not in index:<br/>                index[word] = {i}<br/>            else:<br/>                index[word].add(i)<br/>    <br/>    # Querying the documents containing both "cat" and "table"<br/>    index['cat'].intersection(index['table'])
</pre>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Heaps</h1>
            

            
                
<p>Heaps are data structures designed to quickly find and extract the maximum (or minimum) value in a collection. A typical use-case for heaps is to process a series of incoming tasks in order of maximum priority.</p>
<p>One can theoretically use a sorted list using the tools in the <kbd>bisect</kbd> module; however, while extracting the maximum value will take <em>O</em>(1) time (using <kbd>list.pop</kbd>), insertion will still take <em>O</em>(N) time (remember that, even if finding the insertion point takes <em>O</em>(<em>log</em>(<em>N</em>)) time, inserting an element in the middle of a list is still a <em>O</em>(<em>N</em>) operation). A heap is a more efficient data structure that allows for insertion and extraction of maximum values with <em>O</em>(<em>log</em>(<em>N</em>)) time complexity.</p>
<p>In Python, heaps are built using the procedures contained in the <kbd>heapq</kbd> module on an underlying list. For example, if we have a list of 10 elements, we can reorganize it into a heap with the <kbd>heapq.heapify</kbd> function:</p>
<pre>
    import heapq<br/><br/>    collection = [10, 3, 3, 4, 5, 6]<br/>    heapq.heapify(collection)
</pre>
<p>To perform the insertion and extraction operations on the heap, we can use the <kbd>heapq.heappush</kbd> and <kbd>heapq.heappop</kbd> functions. The <kbd>heapq.heappop</kbd> function will extract the minimum value in the collection in <em>O</em>(<em>log</em>(<em>N</em>)) time and can be used in the following way:</p>
<pre>
    heapq.heappop(collection)<br/>    # Returns: 3
</pre>
<p>Similarly, you can push the integer <kbd>1</kbd>, with the <kbd>heapq.heappush</kbd> function, as follows:</p>
<pre>
    heapq.heappush(collection, 1)
</pre>
<p>Another easy-to-use option is the <kbd>queue.PriorityQueue</kbd> class that, as a bonus, is thread and process-safe. The <kbd>PriorityQueue</kbd> class can be filled up with elements using the <kbd>PriorityQueue.put</kbd> method, while <kbd>PriorityQueue.get</kbd> can be used to extract the minimum value in the collection:</p>
<pre>
    from queue import PriorityQueue<br/><br/>    queue = PriorityQueue()<br/>    for element in collection:<br/>        queue.put(element)<br/><br/>    queue.get()<br/>    # Returns: 3
</pre>
<p>If the maximum element is required, a simple trick is to multiply each element of the list by <kbd>-1</kbd>. In this way, the order of the elements will be inverted. Also, if you want to associate an object (for example, a task to run) to each number (which can represent the priority), one can insert tuples of the <kbd>(number, object)</kbd> form; the comparison operator for the tuple will be ordered with respect to its first element, as shown in the following example:</p>
<pre>
    queue = PriorityQueue()<br/>    queue.put((3, "priority 3"))<br/>    queue.put((2, "priority 2"))<br/>    queue.put((1, "priority 1"))<br/><br/>    queue.get()<br/>    # Returns: (1, "priority 1")
</pre>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Tries</h1>
            

            
                
<p>A perhaps less popular data structure, very useful in practice, is the trie (sometimes called prefix tree). Tries are extremely fast at matching a list of strings against a prefix. This is especially useful when implementing features such as search-as-you type and autocompletion, where the list of available completions is very large and short response times are required.</p>
<p>Unfortunately, Python does not include a trie implementation in its standard library; however, many efficient implementations are readily available through PyPI. The one we will use in this subsection is <kbd>patricia-trie</kbd>, a single-file, pure Python implementation of trie. As an example, we will use <kbd>patricia-trie</kbd> to perform the task of finding the longest prefix in a set of strings (just like autocompletion).</p>
<p>As an example, we can demonstrate how fast a trie is able to search through a list of strings. In order to generate a large amount of unique random strings, we can define a function, <kbd>random_string</kbd>. The <kbd>random_string</kbd> function will return a string composed of random uppercase characters and, while there is a chance to get duplicates, we can greatly reduce the probability of duplicates to the point of being negligible if we make the string long enough. The implementation of the <kbd>random_string</kbd> function is shown as follows:</p>
<pre>
    from random import choice<br/>    from string import ascii_uppercase<br/><br/>    def random_string(length):<br/>     """Produce a random string made of *length* uppercase ascii <br/>     characters"""<br/>     return ''.join(choice(ascii_uppercase) for i in range(length))
</pre>
<p>We can build a list of random strings and time how fast it searches for a prefix (in our case, the <kbd>"AA"</kbd> string) using the <kbd>str.startswith</kbd> function:</p>
<pre>
    strings = [random_string(32) for i in range(10000)]<br/>    matches = [s for s in strings if s.startswith('AA')]
</pre>
<p>List comprehension and <kbd>str.startwith</kbd> are already very optimized operations and, on this small dataset, the search takes only a millisecond or so:</p>
<pre>
    %timeit [s for s in strings if s.startswith('AA')]<br/><br/>    1000 loops, best of 3: 1.76 ms per loop
</pre>
<p>Now, let's try using a trie for the same operation. In this example, we will use the <kbd>patricia-trie</kbd> library that is installable through <kbd>pip</kbd>. The <kbd>patricia.trie</kbd> class implements a variant of the trie data structure with an interface similar to a dictionary. We can initialize our trie by creating a dictionary from our list of strings, as follows:</p>
<pre>
    from patricia import trie<br/>    strings_dict = {s:0 for s in strings} <br/>    # A dictionary where all values are 0<br/>    strings_trie = trie(**strings_dict)
</pre>
<p>To query <kbd>patricia-trie</kbd> for a matching prefix, we can use the <kbd>trie.iter</kbd> method, which returns an iterator over the matching strings:</p>
<pre>
    matches = list(strings_trie.iter('AA'))
</pre>
<p>Now that we know how to initialize and query a trie, we can time the operation:</p>
<pre>
    %timeit list(strings_trie.iter('AA'))<br/>    10000 loops, best of 3: 60.1 µs per loop
</pre>
<div><div><p class="prompt input_prompt">If you look closely, the timing for this input size is <strong>60.1 µs</strong>, which is about 30 times faster (1.76 ms = 1760 µs) than linear search! The speed up is so impressive because of the better computational complexity of the trie prefix search. Querying a trie has a time complexity <em>O</em>(<em>S</em>), where S is the length of the longest string in the collection, while the time complexity of a simple linear scan is <em>O</em>(<em>N</em>), where <em>N</em> is the size of the collection. </p>
<p class="prompt input_prompt">Note that if we want to return all the prefixes that match, the running time will be proportional to the number of results that match the prefix. Therefore, when designing timing benchmarks, care must be taken to ensure that we are always returning the same number of results.</p>
<p class="prompt input_prompt">The scaling properties of a trie versus a linear scan for datasets of different sizes that contains ten prefix matches are shown in the following table:</p>
</div>
</div>
<table>
<tbody>
<tr>
<td><strong>Algorithm</strong></td>
<td><strong><strong>N=10000 (</strong></strong><strong>µs)</strong></td>
<td><strong><strong>N=20000 (</strong></strong><strong>µs)</strong></td>
<td><strong><strong>N=30000 (</strong></strong><strong>µs)</strong></td>
<td><strong><strong>Time</strong></strong></td>
</tr>
<tr>
<td>Trie</td>
<td>17.12</td>
<td>17.27</td>
<td>17.47</td>
<td><em>O</em>(<em>S</em>)</td>
</tr>
<tr>
<td>Linear scan</td>
<td>1978.44</td>
<td>4075.72</td>
<td>6398.06</td>
<td><em>O</em>(<em>N</em>)</td>
</tr>
</tbody>
</table>
<p>An interesting fact is that the implementation of <kbd>patricia-trie</kbd> is actually a single Python file; this clearly shows how simple and powerful a clever algorithm can be. For extra features and performance, other C-optimized trie libraries are also available, such as <kbd>datrie</kbd> and <kbd>marisa-trie</kbd>.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Caching and memoization</h1>
            

            
                
<p>Caching is a great technique used to improve the performance of a wide range of applications. The idea behind caching is to store expensive results in a temporary location, called cache, that can be located in memory, on-disk, or in a remote location.</p>
<p>Web applications make extensive use of caching. In a web application, it often happens that users request a certain page at the same time. In this case, instead of recomputing the page for each user, the web application can compute it once and serve the user the already rendered page. Ideally, caching also needs a mechanism for invalidation so that if the page needs to be updated, we can recompute it before serving it again. Intelligent caching allows web applications to handle increasing number of users with less resources. Caching can also be done preemptively, such as the later sections of the video get buffered when watching a video online.</p>
<p>Caching is also used to improve the performance of certain algorithms. A great example is computing the Fibonacci sequence. Since computing the next number in the Fibonacci sequence requires the previous number in the sequence, one can store and reuse previous results, dramatically improving the running time. Storing and reusing the results of the previous function calls in an application is usually termed as <strong>memoization</strong>, and is one of the forms of caching. Several other algorithms can take advantage of memoization to gain impressive performance improvements, and this programming technique is commonly referred to as <strong>dynamic programming</strong>.</p>
<p>The benefits of caching, however, do not come for free. What we are actually doing is sacrificing some space to improve the speed of the application. Additionally, if the cache is stored in a location on the network, we may incur transfer costs and general time needed for communication. One should evaluate when it is convenient to use a cache and how much space we are willing to trade for an increase in speed.</p>
<p>Given the usefulness of this technique, the Python standard library includes a simple in-memory cache out of the box in the <kbd>functools</kbd> module. The <kbd>functools.lru_cache</kbd> decorator can be used to easily cache the results of a function. In the following example, we create a function, <kbd>sum2</kbd>, that prints a statement and returns the sum of two numbers. By running the function twice, you can see that the first time the <kbd>sum2</kbd> function is executed the <kbd>"Calculating ..."</kbd> string is produced, while the second time the result is returned without running the function:</p>
<pre>
    from functools import lru_cache<br/><br/>    @lru_cache()<br/>    def sum2(a, b):<br/>        print("Calculating {} + {}".format(a, b))<br/>        return a + b<br/><br/>    print(sum2(1, 2))<br/>    # Output: <br/>    # Calculating 1 + 2<br/>    # 3<br/><br/>    print(sum2(1, 2))<br/>    # Output: <br/>    # 3
</pre>
<p>The <kbd>lru_cache</kbd> decorator also provides other basic features. To restrict the size of the cache, one can set the number of elements that we intend to maintain through the <kbd>max_size</kbd> argument. If we want our cache size to be unbounded, we can specify a value of <kbd>None</kbd>. An example usage of <kbd>max_size</kbd> is shown here:</p>
<pre>
    @lru_cache(max_size=16)<br/>    def sum2(a, b):<br/>        ...
</pre>
<p>In this way, as we execute <kbd>sum2</kbd> with different arguments, the cache will reach a maximum size of <kbd>16</kbd> and, as we keep requesting more calculations, new values will replace older values in the cache. The <kbd>lru</kbd> prefix originates from this strategy, which means least recently used.</p>
<p>The <kbd>lru_cache</kbd> decorator also adds extra functionalities to the decorated function. For example, it is possible to examine the cache performance using the <kbd>cache_info</kbd> method, and it is possible to reset the cache using the <kbd>cache_clear</kbd> method, as follows:</p>
<pre>
    sum2.cache_info()<br/>    # Output: CacheInfo(hits=0, misses=1, maxsize=128, currsize=1)<br/>    sum2.cache_clear()
</pre>
<p>As an example, we can see how a problem, such as computing the fibonacci series, may benefit from caching. We can define a <kbd>fibonacci</kbd> function and time its execution:</p>
<pre>
    def fibonacci(n):<br/>        if n &lt; 1:<br/>            return 1<br/>        else:<br/>            return fibonacci(n - 1) + fibonacci(n - 2)<br/><br/>    # Non-memoized version<br/>    %timeit fibonacci(20)<br/>    100 loops, best of 3: 5.57 ms per loop
</pre>
<p>The execution takes 5.57 ms, which is very high. The scaling of the function written in this way has poor performance; the previously computed fibonacci sequences are not reused, causing this algorithm to have an exponential scaling of roughly <em>O</em>(<em>2<sup>N</sup></em>).</p>
<p>Caching can improve this algorithm by storing and reusing the already-computed fibonacci numbers. To implement the cached version, it is sufficient to apply the <kbd>lru_cache</kbd> decorator to the original <kbd>fibonacci</kbd> function. Also, to design a proper benchmark, we need to ensure that a new cache is instantiated for every run; to do this, we can use the <kbd>timeit.repeat</kbd> function, as shown in the following example:</p>
<pre>
    import timeit<br/>    setup_code = '''<br/>    from functools import lru_cache<br/>    from __main__ import fibonacci<br/>    fibonacci_memoized = lru_cache(maxsize=None)(fibonacci)<br/>    '''<br/><br/>    results = timeit.repeat('fibonacci_memoized(20)',<br/>                            setup=setup_code,<br/>                            repeat=1000,<br/>                            number=1)<br/>    print("Fibonacci took {:.2f} us".format(min(results)))<br/>    # Output: Fibonacci took 0.01 us
</pre>
<p>Even though we changed the algorithm by adding a simple decorator, the running time now is much less than a microsecond. The reason is that, thanks to caching, we now have a linear time algorithm instead of an exponential one.</p>
<p>The <kbd>lru_cache</kbd> decorator can be used to implement simple in-memory caching in your application. For more advanced use cases, third-party modules can be used for more powerful implementation and on-disk caching.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Joblib</h1>
            

            
                
<p>A simple library that, among other things, provides a simple on-disk cache is <kbd>joblib</kbd>. The package can be used in a similar way as <kbd>lru_cache</kbd>, except that the results will be stored on disk and will persist between runs.</p>
<p>The <kbd>joblib</kbd> module can be installed from PyPI using the <kbd>pip install joblib</kbd> command.</p>
<p>The <kbd>joblib</kbd> module provides the <kbd>Memory</kbd> class that can be used to memoize functions using the <kbd>Memory.cache</kbd> decorator:</p>
<pre>
    from joblib import Memory<br/>    memory = Memory(cachedir='/path/to/cachedir')<br/><br/>    @memory.cache<br/>    def sum2(a, b):<br/>        return a + b
</pre>
<p>The function will behave similar to <kbd>lru_cache</kbd>, with the exception that the results will be stored on-disk in the directory specified by the <kbd>cachedir</kbd> argument during <kbd>Memory</kbd> initialization. Additionally, the cached results will persist over subsequent runs!</p>
<p>The <kbd>Memory.cache</kbd> method also allows to limit recomputation only when certain arguments change, and the resulting decorated function supports basic functionalities to clear and analyze the cache. </p>
<p>Perhaps the best <kbd>joblib</kbd> feature is that, thanks to intelligent hashing algorithms, it provides efficient memoization of functions that operate on <kbd>numpy</kbd> arrays, and is particularly useful in scientific and engineering applications.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Comprehensions and generators</h1>
            

            
                
<p>In this section, we will explore a few simple strategies to speed up Python loops using comprehension and generators. In Python, comprehension and generator expressions are fairly optimized operations and should be preferred in place of explicit for-loops. Another reason to use this construct is readability; even if the speedup over a standard loop is modest, the comprehension and generator syntax is more compact and (most of the times) more intuitive.</p>
<p>In the following example, we can see that both the list comprehension and generator expressions are faster than an explicit loop when combined with the <kbd>sum</kbd> function:</p>
<pre>
    def loop(): <br/>        res = [] <br/>        for i in range(100000): <br/>            res.append(i * i) <br/>        return sum(res) <br/><br/>    def comprehension(): <br/>        return sum([i * i for i in range(100000)]) <br/><br/>    def generator(): <br/>        return sum(i * i for i in range(100000)) <br/><br/>    %timeit loop() <br/>    100 loops, best of 3: 16.1 ms per loop <br/>    %timeit comprehension() <br/>    100 loops, best of 3: 10.1 ms per loop <br/>    %timeit generator() <br/>    100 loops, best of 3: 12.4 ms per loop 
</pre>
<p>Just like lists, it is possible to use <kbd>dict</kbd> comprehension to build dictionaries slightly more efficiently and compactly, as shown in the following code:</p>
<pre>
    def loop(): <br/>        res = {} <br/>        for i in range(100000): <br/>            res[i] = i<br/>        return res<br/><br/>    def comprehension(): <br/>        return {i: i for i in range(100000)}<br/>    %timeit loop() <br/>    100 loops, best of 3: 13.2 ms per loop <br/>    %timeit comprehension() <br/>    100 loops, best of 3: 12.8 ms per loop
</pre>
<p>Efficient looping (especially in terms of memory) can be implemented using iterators and functions such as <kbd>filter</kbd> and <kbd>map</kbd>. As an example, consider the problem of applying a series of operations to a list using list comprehension and then taking the maximum value:</p>
<pre>
    def map_comprehension(numbers):<br/>        a = [n * 2 for n in numbers]<br/>        b = [n ** 2 for n in a]<br/>        c = [n ** 0.33 for n in b]<br/>        return max(c)
</pre>
<p>The problem with this approach is that for every list comprehension, we are allocating a new list, increasing memory usage. Instead of using list comprehension, we can employ generators. Generators are objects that, when iterated upon, compute a value on the fly and return the result.</p>
<p>For example, the <kbd>map</kbd> function takes two arguments--a function and an iterator--and returns a generator that applies the function to every element of the collection. The important point is that the operation happens only <em>while we are iterating</em>, and not when <kbd>map</kbd> is invoked!</p>
<p>We can rewrite the previous function using <kbd>map</kbd> and by creating intermediate generators, rather than lists, thus saving memory by computing the values on the fly:</p>
<pre>
    def map_normal(numbers):<br/>        a = map(lambda n: n * 2, numbers)<br/>        b = map(lambda n: n ** 2, a)<br/>        c = map(lambda n: n ** 0.33, b)<br/>        return max(c)
</pre>
<p>We can profile the memory of the two solutions using the <kbd>memory_profiler</kbd> extension from an IPython session. The extension provides a small utility, <kbd>%memit</kbd>, that will help us evaluate the memory usage of a Python statement in a way similar to <kbd>%timeit</kbd>, as illustrated in the following snippet:</p>
<pre>
    %load_ext memory_profiler<br/>    numbers = range(1000000)<br/>    %memit map_comprehension(numbers)<br/>    peak memory: 166.33 MiB, increment: 102.54 MiB<br/>    %memit map_normal(numbers)<br/>    peak memory: 71.04 MiB, increment: 0.00 MiB
</pre>
<p>As you can see, the memory used by the first version is <kbd>102.54 MiB</kbd>, while the second version consumes <kbd>0.00 MiB</kbd>! For the interested reader, more functions that return generators can be found in the <kbd>itertools</kbd> module, which provides a set of utilities designed to handle common iteration patterns.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    

        

            
                <h1 class="header-title">Summary</h1>
            

            
                
<p>Algorithmic optimization can improve how your application scales as we process increasingly large data. In this chapter, we demonstrated use-cases and running times of the most common data structures available in Python, such as lists, deques, dictionaries, heaps, and tries. We also covered caching, a technique that can be used to trade some space, in memory or on-disk, in exchange for increased responsiveness of an application. We also demonstrated how to get modest speed gains by replacing for-loops with fast constructs, such as list comprehensions and generator expressions.</p>
<p>In the subsequent chapters, we will learn how to improve performance further using numerical libraries such as <kbd>numpy</kbd>, and how to write extension modules in a lower-level language with the help of <em>Cython</em>.</p>


            

            <footer style="margin-top: 5em;">
                
            </footer>

        
    </body></html>