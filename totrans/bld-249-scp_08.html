<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Rendering and Image Manipulation</h1></div></div></div><p>In the previous chapters, we looked mainly at the scripting aspects of the individual components that make up a Blender scene such as meshes, lamps, materials, and so on. In this chapter, we will turn to the rendering process as a whole. We will automate this rendering process, combine the resulting images in various ways, and even turn Blender into a specialized web server.</p><p>In this chapter, you will learn how to:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Automate the rendering process</li><li class="listitem" style="list-style-type: disc">Create multiple views for product presentations</li><li class="listitem" style="list-style-type: disc">Create billboards from complex objects</li><li class="listitem" style="list-style-type: disc">Manipulate images, including render results, by using the Python Imaging Library (PIL)</li><li class="listitem" style="list-style-type: disc">Create a server that creates on-demand images that may be used as CAPTCHA challenges</li><li class="listitem" style="list-style-type: disc">Create a contact sheet</li></ul></div><div><div><div><div><h1 class="title"><a id="ch08lvl1sec65"/>A different view—combining multiple camera angles </h1></div></div></div><a class="indexterm" id="id526"/><p>By now, you might expect that rendering can be automated as well, and you're quite right. The Blender Python API provides access to almost all parameters of the rendering process and lets you render individual frames as well as animations. This allows for automating many tasks that would be tedious to do by hand.</p><p>Say you have created an object and want to create a single image that shows it from different angles. You could render these out separately and combine them in an external application, but we will write a script that not only renders these views but also combines them in a single image by using Blender's image manipulation capabilities and an external module called PIL. The effect we try to achieve is shown in the illustration of Suzanne, showing her from all of her best sides.</p><div><img alt="A different view—combining multiple camera angles" src="img/0400-08-01.jpg"/></div><p>Blender is an excellent tool that provides you not only with modeling, animating, and rendering options but has compositing functionality as well. One area that it does not <a class="indexterm" id="id527"/>excel in is "image manipulation". It does have an UV-editor/Image window of course, but that is very specifically engineered to manipulate UV maps and to view images rather than to manipulate them. The Node editor is also capable of sophisticated image manipulation but it has no documented API so it can't be configured from a script.</p><p>Of course, Blender cannot do everything and surely it doesn't try to compete with packages such as <strong>GIMP</strong> (<a class="ulink" href="http://www.gimp.org">www.gimp.org</a>), but some built-in image manipulation functions would have been welcomed. (Each image can be manipulated on the pixel level but this would be fairly slow on large images and we still would have to implement high-level functionality, such as alpha blending or rotating images.)</p><a class="indexterm" id="id528"/><p>Fortunately, we can access any image generated by Blender from Python, and in Python it is fairly simple to add additional packages that do provide the extra functionality and use them from our scripts. The only drawback is that any script that uses these additional libraries is not automatically portable so users would have to check that the relevant libraries are available to them.</p><p>The <strong>Python Imaging Library</strong> (<strong>PIL</strong><a class="indexterm" id="id529"/>) that we will be using is freely available and easy to install. Therefore, it should pose no problem for the average user. However, as it is possible to implement the simple pasting functionality (we will see below) using just Blender's <code class="literal">Image</code> module, we do provide in the full code a minimalist module <code class="literal">pim</code> that implements just the bare minimum to be able to use our example without the need to install PIL. This independence comes at a price: our <code class="literal">paste()</code> function<a class="indexterm" id="id530"/> is almost 40 times slower than the one provided by PIL and the resulting image can be saved only in TARGA (<code class="literal">.tga</code>) format. But you probably won't notice that as Blender can display TARGA files<a class="indexterm" id="id531"/> just fine. The full code is equipped with some trickery to use PIL (if it's available) and our replacement module if it isn't. (This is not shown in the book.)</p><div><div><h3 class="title"><a id="note26"/>Note</h3><p>
<strong>The Python Imaging Library (PIL)</strong>
</p><p>PIL is an open source package available for free from <a class="ulink" href="http://www.pythonware.com/products/pil/index.htm">http://www.pythonware.com/products/pil/index.htm</a>. It consists of a number of Python modules and a core library that comes precompiled for Windows (and is easy enough to compile on Linux or might even be available in the distribution already). Just follow the instructions on the site to install it (just remember to use the correct python version to install PIL; if you have more than one version of Python installed, use the one Blender uses as well to install).</p></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec89"/>Code outline—combine.py</h2></div></div></div><a class="indexterm" id="id532"/><a class="indexterm" id="id533"/><a class="indexterm" id="id534"/><p>What steps do we have to take to create our combined image? We will have to:</p><div><ol class="orderedlist arabic"><li class="listitem">Create cameras if needed.</li><li class="listitem">Frame the cameras on the subject.</li><li class="listitem">Render views from all cameras.</li><li class="listitem">Combine the rendered images to a single image.</li></ol></div><p>The code starts off by importing all of the necessary modules. From the PIL package we need the <code class="literal">Image</code> module, but we import it under a different name (<code class="literal">pim</code>) to prevent name clashes with Blender's <code class="literal">Image</code> module, which we will be using as well:</p><div><pre class="programlisting">from PIL import Image as pim
import Blender
from Blender import Camera, Scene, Image, Object, Mathutils, Window
import bpy
import os</pre></div><a class="indexterm" id="id535"/><p>The first utility function that we encounter is <code class="literal">paste()</code>. This function will combine four images into one. The images are passed as filenames and the result is saved as <code class="literal">result.png</code> unless another output filename is specified. We assume all four images to have the same dimensions, which we determine by opening the first file as a PIL image and examining its <code class="literal">size</code> attribute (highlighted in the next code). The images will be separated and bordered by a small line with a solid color. The width and color are hardcoded as the <code class="literal">edge</code><a class="indexterm" id="id536"/>
<a class="indexterm" id="id537"/> and <code class="literal">edgecolor</code> variables, although you might consider passing them as arguments:</p><div><pre class="programlisting">def paste(top,right,front,free,output="result.png"):
   im = pim.open(top)
<strong>   w,h= im.size</strong>
   edge=4
   edgecolor=(0.0,0.0,0.0)</pre></div><p>Next, we create an empty image big enough to hold the four images with the appropriate borders. We will not be drawing any borders specifically, but just defining the new image with a solid color onto which the four images will be pasted at a fitting offset:</p><div><pre class="programlisting">   comp = pim.new(im.mode,(w*2+3*edge,h*2+3*edge),edgecolor)</pre></div><p>We already opened the top image so all we have to do is paste it in the upper-left quadrant of our combined image offset in both the horizontal and vertical directions by the border width:</p><div><pre class="programlisting">   comp.paste(im,(edge,edge))</pre></div><p>Pasting the three other images follows the same line: open the image and paste it at the correct position. Finally, the combined image is saved (highlighted). The file type of the saved image is determined by its extension (for example, <code class="literal">png</code>) but might have been overridden had we passed a format argument to the <code class="literal">save()</code> method<a class="indexterm" id="id538"/>. Note that there was no reason to specify a format for the input files as the image type is determined from its contents by the <code class="literal">open()</code> function<a class="indexterm" id="id539"/>.</p><div><pre class="programlisting">   im = pim.open(right)
   comp.paste(im,(w+2*edge,edge))
   im = pim.open(front)
   comp.paste(im,(edge,h+2*edge))
   im = pim.open(free)
   comp.paste(im,(w+2*edge,h+2*edge))
<strong>   comp.save(output)</strong>
</pre></div><a class="indexterm" id="id540"/><p>Our next function renders the view from a specific camera and saves the result to a file. The camera to render is passed as the name of the Blender Object (that is, not the name of the underlying <code class="literal">Camera</code> object). The first line retrieves the <code class="literal">Camera</code> object and the current scene and makes the camera current in the scene—the one that will be rendered (highlighted below). <code class="literal">setCurrentCamera()</code><a class="indexterm" id="id541"/> takes a Blender Object, not a <code class="literal">Camera</code> object, and that's the reason we passed the name of the object.</p><div><pre class="programlisting">def render(camera):
   cam = Object.Get(camera)
   scn = Scene.GetCurrent()
<strong>   scn.setCurrentCamera(cam)</strong>
   context = scn.getRenderingContext()</pre></div><p>As we might use this function in a <strong>background</strong> <strong>process</strong> we will be using the <code class="literal">renderAnim()</code> method<a class="indexterm" id="id542"/> of the rendering context rather than the <code class="literal">render()</code> method<a class="indexterm" id="id543"/>. This is because the <code class="literal">render()</code> method cannot be used in a background process. Therefore, we set the current frame and both the start and end frames to the same value to ensure that <code class="literal">renderAnim()</code> will render just a single frame. We also set <code class="literal">displayMode</code> to <code class="literal">0</code> to prevent an extra render window popping up (highlighted in the next code snippet):</p><div><pre class="programlisting">   frame = context.currentFrame()
   context.endFrame(frame)
   context.startFrame(frame)
<strong>   context.displayMode=0</strong>
   context.renderAnim()</pre></div><p>The <code class="literal">renderAnim()</code> method renders frames to files so our next task is to retrieve the filename of the frame that we just rendered. The exact format of the filename may be specified by the user in the <strong>User</strong> <strong>Preferences</strong> window, but by calling <code class="literal">getFrameFilename()</code> explicitly we ensure that we get the right one:</p><div><pre class="programlisting">   filename= context.getFrameFilename()</pre></div><p>As the frame number will be the same for each camera view that we render, we will have to rename this file otherwise it would be overwritten. Therefore, we create a suitable new name consisting of the path of the frame we just rendered and the name of the camera. We use portable path manipulation functions from Python's <code class="literal">os.path</code> module so everything will work just as well under Windows as on Linux, for example.</p><p>As our script may have been used already, we try to remove any existing file with the same name because renaming a file to an existing filename will fail under Windows. Of course, there might not be a file yet—a situation we guard against in the <code class="literal">try</code> block. Finally, our function returns the name of the newly created file:</p><div><pre class="programlisting">   camera = os.path.join(os.path.dirname(filename),camera)
   try:
      os.remove(camera)
   except:
      pass
   os.rename(filename,camera)
   return camera</pre></div><a class="indexterm" id="id544"/><p>The next important task is to frame the cameras, that is, to choose a suitable <strong>camera angle</strong> for all of the cameras in such a way that the subject fits the available area in the picture in an optimal way. We want the camera angle to be the same for all cameras to provide the viewer with a consistent perspective from all viewing angles. Of course, this could be done manually, but this is tedious so we define a function to do the work for us.</p><p>The way we do this is to take the <strong>bounding box</strong> of our subject and determine the viewing angle of the camera by assuming that this bounding box must just fill our view. Because we can calculate the distance of the camera to the center of the bounding box, the viewing angle must be the same as the acute angle of the triangle formed by the bounding box and the camera distance.</p><div><img alt="Code outline—combine.py" src="img/0400-08-02.jpg"/></div><p>We calculate this angle for all of the cameras and then set the camera angle for each camera to the widest angle calculated to prevent unwanted clipping of our subject. Note that this algorithm may fail if the cameras are too close to the subject (or equivalently, if the subject is too large), in which case some clipping may occur.</p><p>The code is pretty heavy on the math, so we start off by importing the necessary functions:</p><div><pre class="programlisting">from math import asin,tan,pi,radians</pre></div><a class="indexterm" id="id545"/><p>The function itself will take a list of names of Blender objects (the cameras) and a bounding box (a list of vectors, one for each corner of the bounding box). It starts off by determining the minimum and maximum extents of the bounding box for all three axes and the widths. We assume that our subject is centered on the origin. <code class="literal">maxw</code> will hold the largest width along any axis.</p><div><pre class="programlisting">def frame(cameras,bb):
   maxx = max(v.x for v in bb)
   maxy = max(v.y for v in bb)
   maxz = max(v.z for v in bb)
   minx = min(v.x for v in bb)
   miny = min(v.y for v in bb)
   minz = min(v.z for v in bb)
   wx=maxx-minx
   wy=maxy-miny
   wz=maxz-minz
   m=Mathutils.Vector((wx/2.0,wy/2.0,wz/2.0))
   maxw=max((wx,wy,wz))/2.0</pre></div><p>Next, we get the world space coordinates for each <code class="literal">Camera</code> object to calculate the distance <code class="literal">d</code> to the midpoint of the bounding box (highlighted in the next code). We store the quotient of maximum width and distance:</p><div><pre class="programlisting">   sins=[]
   for cam in cameras:
      p=Mathutils.Vector(Object.Get(cam).getLocation('worldspace'))
<strong>      d=(p-m).length</strong>
      sins.append(maxw/d)</pre></div><p>We take the largest quotient calculated (as this will amount to the widest angle) and determine the angle by calculating the arc sinus and finish by setting the <code class="literal">lens</code> attribute of the <code class="literal">Camera</code> object. The relation between camera's viewing angle and the value of the <code class="literal">lens</code> attribute in Blender is complex and scarcely documented (<code class="literal">lens</code> holds an approximation of the focal length of an ideal lens). The formula shown is the one taken from Blender's source code (highlighted).</p><div><pre class="programlisting">   maxsin=max(sins)
   angle=asin(maxsin)
   for cam in cameras:
      Object.Get(cam).getData().lens = <strong>16.0/tan(angle)</strong>
</pre></div><p>Another convenience function is the one that defines four cameras and puts them into the scene suitably arranged around the origin. The function is straightforward in principle but is a little bit complicated because it tries to reuse existing cameras with the same name to prevent unwanted proliferation of cameras if the script is run more than once. The <code class="literal">cameras</code> dictionary is indexed by name and holds a list of positions, rotations, and lens values:</p><div><pre class="programlisting">def createcams():
   cameras = {
              'Top'  : (( 0.0,  0.0,10.0),( 0.0,0.0, 0.0),35.0),'Right': ((10.0,  0.0, 0.0),(90.0,0.0,90.0),35.0),
              'Front': (( 0.0,-10.0, 0.0),(90.0,0.0, 0.0),35.0),'Free' : (( 5.8, -5.8, 5.8),(54.7,0.0,45.0),35.0)
             }</pre></div><p>For each camera in the <code class="literal">cameras</code> dictionary we check if it already exists as a Blender object. If it does, we check whether the Blender object has a <code class="literal">Camera</code> object associated with it. If the latter is not true we create a perspective camera with the same name as the top-level object (highlighted) and associate it with the top-level object by way of the <code class="literal">link()</code> method:</p><div><pre class="programlisting">   for cam in cameras:
      try:
         ob = Object.Get(cam)
         camob = ob.getData()
         if camob == None:
            <strong>camob = Camera.New('persp',cam)</strong>
            ob.link(camob)</pre></div><a class="indexterm" id="id546"/><p>If there wasn't a top-level object present already we create one and associate a new perspective <code class="literal">Camera</code> object with it:</p><div><pre class="programlisting">      except ValueError:
         ob = Object.New('Camera',cam)
         Scene.GetCurrent().link(ob)
         camob = Camera.New('persp',cam)
         ob.link(camob)</pre></div><p>We end by setting the <code class="literal">location</code><a class="indexterm" id="id547"/>, <code class="literal">rotation</code><a class="indexterm" id="id548"/>, and <code class="literal">lens</code> attributes<a class="indexterm" id="id549"/>. Note that the rotation angles are in radians so we convert them from the more intuitive degrees that we used in our table (highlighted). We end by calling <code class="literal">Redraw()</code> to make the changes show up in the user interface:<a class="indexterm" id="id550"/>
</p><div><pre class="programlisting">      ob.setLocation(cameras[cam][0])
      ob.setEuler([<strong>radians(a) for a in cameras[cam][1]</strong>])
      camob.lens=cameras[cam][2]
      Blender.Redraw()</pre></div><p>Finally, we define a <code class="literal">run()</code> method<a class="indexterm" id="id551"/> that strings all components together. It determines the active object and then cycles through a list of camera names to render each view and add the resulting filename to a list (highlighted):</p><div><pre class="programlisting">def run():
   ob = Scene.GetCurrent().objects.active
   cameras = ('Top','Right','Front','Free')
   frame(cameras,ob.getBoundBox())
   files = []
   for cam in cameras:
<strong>      files.append(render(cam))</strong>
</pre></div><a class="indexterm" id="id552"/><p>We will put the combined pictures in the same directory as the individual views and call it <code class="literal">result.png</code>:</p><div><pre class="programlisting">   outfile = os.path.join(os.path.dirname(files[0]),'result.png')</pre></div><p>We then call our <code class="literal">paste()</code> function<a class="indexterm" id="id553"/>, passing the list of component filenames expanded as individual arguments by the asterisk (<code class="literal">*</code>) operator and end with a finishing touch of loading the result file as a Blender image and showing it in the image editor window (highlighted below). The <code class="literal">reload</code> is necessary to ensure that a previous image of the same name is refreshed:</p><div><pre class="programlisting">   paste(*files,output=outfile)
   im=Image.Load(outfile)
<strong>   bpy.data.images.active = im</strong>
   im.reload()
   Window.RedrawAll()</pre></div><p>The <code class="literal">run()</code> function deliberately did not create any cameras because the user might want to do that himself. The final script itself does take care of creating the cameras, but this might be changed quite easily and is as usual quite simple. After the check to see if it runs standalone it just creates the cameras and calls the <code class="literal">run</code> method:</p><div><pre class="programlisting">if __name__ == "__main__":
   createcams()
   run()</pre></div><p>The full code is available as <code class="literal">combine.py</code> in <code class="literal">combine.blend</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec90"/>Workflow—how to showcase your model</h2></div></div></div><p>The script can be used in the following way:</p><div><ol class="orderedlist arabic"><li class="listitem">Put your subject at the origin (position (0, 0, 0)).</li><li class="listitem">Create suitable lighting conditions.</li><li class="listitem">Run <code class="literal">combine.py</code>.</li></ol></div><p>The script may be loaded into the text editor to run with <em>Alt + P</em> but you may also put the script in Blender's <code class="literal">scripts</code> directory to make it available from the <strong>Scripts | Render</strong> menu.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec66"/>Now, strip—creating a film strip from an animation</h1></div></div></div><a class="indexterm" id="id554"/><a class="indexterm" id="id555"/><p>Fitting multiple camera views to a single image is just one example where multiple images might be effectively combined to a single image. Another example is when we would like to show frames from an animation where we don't have access to facilities to replay the animation. In such situations we would like to show something resembling a film strip where we combine a small rendition of, for example, every tenth frame to a single sheet of images. An example is shown in the following illustration.</p><p>Although there are more images to combine than in the multiple camera view, the code to create such a film strip is fairly similar.</p><div><img alt="Now, strip—creating a film strip from an animation" src="img/0400-08-03.jpg"/></div><p>The first function that we develop is <code class="literal">strip()</code> that takes a list of filenames of images to combine and an optional <code class="literal">name</code> that will be given to the combined image. A third optional argument is <code class="literal">cols</code>, which is the number of columns in the combined image. The default is four, but for long sequences it might be more natural to print on landscape paper and use a higher value here. The function will return a Blender <code class="literal">Image</code> object containing the combined image.</p><a class="indexterm" id="id556"/><p>We again use the <code class="literal">pim</code> module, which is either an alias for the PIL module if it's available or will refer to our bare bones implementation if PIL is not available. The important difference with our previous image combination code is highlighted. The first highlighted part shows how to calculate the dimensions of the combined image based on the number of rows and columns plus the amount of pixels needed for the colored edges around images. The second highlighted line shows where we paste an image in the destination image:</p><div><pre class="programlisting">def strip(files,name='Strip',cols=4):
   rows = int(len(files)/cols)
   if len(files)%int(cols) : rows += 1
   
   im = pim.open(files.pop(0))
   w,h= im.size
   edge=2
   edgecolor=(0.0,0.0,0.0)
   
   comp =  pim.new(im.mode,<strong>(w*cols+(cols+1)*edge,h*rows+(rows+1)*edge)</strong>,edgecolor)
   
   for y in range(rows):
      for x in range(cols):
<strong>         comp.paste(im,(edge+x*(w+edge),edge+y*(h+edge)))</strong>
         if len(files)&gt;0:
            im = pim.open(files.pop(0))
         else:
            comp.save(name,format='png')
            return Image.Load(name)</pre></div><p>The <code class="literal">render()</code> function<a class="indexterm" id="id557"/> that we define here will take the number of frames to skip as an argument and will render any number of frames between the start and end frames. These start and end frames may be set by the user in the render buttons. The render buttons also contain a step value, but this value is not provided to the Python API. This means that our function is a little bit more verbose than we like as we have to create a loop that renders each frame ourselves (highlighted in the next code) instead of just calling <code class="literal">renderAnim()</code>. We therefore have to manipulate the <code class="literal">startFrame</code><a class="indexterm" id="id558"/> and <code class="literal">endFrame</code> attributes<a class="indexterm" id="id559"/> of the render context (as before) but we take care to restore those attributes before returning a list of filenames of the rendered images. If we did not need any programmatic control of setting the <code class="literal">skip</code> value, we could have simply replaced a call to <code class="literal">render()</code> by a call to <code class="literal">renderAnim()</code>:</p><div><pre class="programlisting">def render(skip=10):
   context = Scene.GetCurrent().getRenderingContext()
   filenames = []
   e = context.endFrame()
   s = context.startFrame()
   context.displayMode=0
<strong>   for frame in range(s,e+1,skip):</strong>
      context.currentFrame(frame)
      context.startFrame(frame)
      context.endFrame(frame)
      context.renderAnim()
      filenames.append(context.getFrameFilename())
   context.startFrame(s)
   context.endFrame(e)
   return filenames</pre></div><a class="indexterm" id="id560"/><p>With these functions defined the script itself now simply calls <code class="literal">render()</code> to create the images and <code class="literal">strip()</code> to combine them. The resulting Blender image is reloaded to force an update if an image with the same name was already present and all windows are prompted to redraw themselves (highlighted):</p><div><pre class="programlisting">def run():
   files = render()
   im=strip(files)
   bpy.data.images.active = im
   im.reload()
<strong>   Window.RedrawAll()</strong>

if __name__ == "__main__":
   run()</pre></div><p>The full code is available as <code class="literal">strip.py</code> in <code class="literal">combine.blend</code>.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec91"/>Workflow—using strip.py</h2></div></div></div><a class="indexterm" id="id561"/><a class="indexterm" id="id562"/><p>Creating a strip of animated frames can now be done as follows:</p><div><ol class="orderedlist arabic"><li class="listitem">Create your animation.</li><li class="listitem">Run <code class="literal">strip.py</code> from the text editor.</li><li class="listitem">The combined image will show up in the UV-editor/image window.</li><li class="listitem">Save the image with a name of your choice.</li></ol></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec67"/>Rendering billboards</h1></div></div></div><a class="indexterm" id="id563"/><p>Realism in scenes is often accomplished by providing lots of detail, especially in natural objects. However, this kind of realism comes with a price as detailed models often contain many faces and these faces consume memory and take time to render. A realistic tree model may contain as much as half a million faces so a forest of these would be almost impossible to render, even more so, if this forest is part of the scenery in a fast-paced game.</p><p>Blender comes with a number of tools to reduce the amount of memory needed when rendering many copies of an object; different <code class="literal">Mesh</code> objects may refer to the same mesh data as may <strong>DupliVerts.</strong><a class="indexterm" id="id564"/> (Child objects that are replicated at the position of each vertex of a parent object. See <a class="ulink" href="http://wiki.blender.org/index.php/Doc:Manual/Modeling/Objects/Duplication/DupliVerts">http://wiki.blender.org/index.php/Doc:Manual/Modeling/Objects/Duplication/DupliVerts</a> for more information.) Duplication of objects in particle systems also allows us to create many instances of the same object without actually duplicating all the data. These techniques may save huge amounts of memory but detailed objects still may take a lot of CPU power to render because the details are still there to be rendered.</p><a class="indexterm" id="id565"/><p>
<strong>Billboards</strong> are a technique used to apply a picture of a complex object to a simple object, such as a single square face, and replicate this simple object as many times as needed. The picture must have suitable transparency otherwise each object may occlude the others in unrealistic ways. Apart from that, this technique is quite simple and may save a lot of rendering time and it will give fairly realistic results for objects placed in the middle distance or farther away. Blender's particle systems may use billboards either as simple squares with images applied or by applying an image to a simple object ourselves and using that as a duplicating object. The latter also holds for dupliverted objects.</p><a class="indexterm" id="id566"/><p>The trick is to generate an image with suitable lighting to be used as an image that can be applied to a square. Actually, we want to create two images: one shot from the front, one from the right, and construct an object consisting of two square faces perpendicular to each other with the two images applied. Such an object will give us a limited amount of freedom later in the placement of the camera in our scene as they do not have to be seen from just one direction. This works well only for objects with a roughly cylindrical symmetry, such as trees or high-rises, but then it is quite effective.</p><p>The workflow for constructing such objects is complex enough to warrant automation:</p><div><ol class="orderedlist arabic"><li class="listitem">Position two cameras front and right of the detailed object.</li><li class="listitem">Frame both cameras to capture all of the object with the same angle.</li><li class="listitem">Render the transparent images with alpha premultiplied and without sky.</li><li class="listitem">Construct a simple object of two perpendicular squares.</li><li class="listitem">Apply each rendered image to a square.</li><li class="listitem">Hide the detailed object from rendering.</li><li class="listitem">Optionally, replicate the simple object in a particle system (the user may opt not to automate this part but place the simple objects manually).</li></ol></div><p>The "premultiplication" mentioned in the third step may need some clarification. Obviously, the rendered images of our complex object need not show any background sky as their replicated clones may be positioned anywhere and may show different parts of the sky through their transparent parts. As we will see, this is simple enough to accomplish but when we simply render a transparent image and overlay it later on some background the image may have unsightly glaring edges.</p><a class="indexterm" id="id567"/><p>The way to avoid this is to adjust the rendered colors by multiplying them with the alpha value and the render context has the necessary attributes to indicate this. We should not forget to mark the images produced as "premultiplied" when using them as textures, otherwise they will look too dark. The difference is illustrated in the following screenshot where we composited and enlarged a correctly premultiplied half on the left and a sky rendered half on the right. The trunk of the tree shows a light edge on the right. (Refer to Roger Wickes' excellent book "Foundation Blender Compositing" for more details.)</p><div><img alt="Rendering billboards" src="img/0400-08-04.jpg"/></div><a class="indexterm" id="id568"/><p>The beech tree (used in these and the following illustrations) is a highly-detailed model (over 30,000 faces) created by Yorik van Havre with the free plant-modeling package <strong>ngPlant</strong>.<a class="indexterm" id="id569"/> (See his website for more fine examples: <a class="ulink" href="http://yorik.uncreated.net/greenhouse.html">http://yorik.uncreated.net/greenhouse.html</a>) The following first set of images shows the beech tree from the front and the resulting front facing render of the two billboards on the left. (slightly darker because of the premultiplication). </p><div><img alt="Rendering billboards" src="img/0400-08-05_NEW.jpg"/></div><p>The next set of screenshots shows the same beech tree rendered from the right together with a right-facing render <a class="indexterm" id="id570"/>of the billboard on the left. As can be seen, the rendition is certainly not perfect from this angle and this closeup, but a reasonable three-dimensional aspect is retained.</p><div><img alt="Rendering billboards" src="img/0400-08-07_NEW.jpg"/></div><p>To give an impression of the construction of the billboards the next screenshot shows the two faces with the rendered images applied.<a class="indexterm" id="id571"/> The transparency is deliberately lessened to show the individual faces.</p><div><img alt="Rendering billboards" src="img/0400-08-09.jpg"/></div><p>Our first challenge is to reuse some of the functions that we wrote for the generation of our contact sheet. <a class="indexterm" id="id572"/>These functions are in a text buffer called <code class="literal">combine.py</code> and we did not save this to an external file. We will create our <code class="literal">cardboard.py</code> script as a new text buffer in the same <code class="literal">.blend</code> file as <code class="literal">combine.py</code> and would like to refer to the latter just like some external module. Blender will make this possible for us as it searches for a module in the current text buffers if it cannot find an external file.</p><p>Because internal text buffers have no information on when they were last modified, we have to make sure that the latest version is loaded. That is what the <code class="literal">reload()</code> function<a class="indexterm" id="id573"/> will take care of. If we didn't do this Blender would not detect if <code class="literal">combine.py</code> had changed, which could lead to us using an older compiled version of it:</p><div><pre class="programlisting">import combine
reload(combine)</pre></div><a class="indexterm" id="id574"/><p>We will not reuse the <code class="literal">render()</code> function from <code class="literal">combine.py</code>
<strong> </strong>because we have different requirements for the rendered images that we will apply to the billboards. As explained, we have to make sure that we won't get any bright edges at points where we have partial transparency so we have to premultiply the alpha channel in advance (highlighted). We reset the rendering context to 'rendering the sky' again just before we return from this function because it's easy to forget to turn this on again manually and you may waste time wondering where your sky has gone:</p><div><pre class="programlisting">def render(camera):
   cam = Object.Get(camera)
   scn = Scene.GetCurrent()
   scn.setCurrentCamera(cam)
   context = scn.getRenderingContext()
   frame = context.currentFrame()
   context.endFrame(frame)
   context.startFrame(frame)
   context.displayMode=0
<strong>   context.enablePremultiply()</strong>
   context.renderAnim()
   filename= context.getFrameFilename()
   camera = os.path.join(os.path.dirname(filename),camera)
   try:
      os.remove(camera) # remove otherwise rename fails on windows
   except:
      pass
   os.rename(filename,camera)
   
   context.enableSky()
   return camera</pre></div><p>Each rendered image will have to be converted to a suitable material to apply to a UV-mapped square. The function <code class="literal">imagemat()</code><a class="indexterm" id="id575"/> will do just that; it will take a Blender <code class="literal">Image</code> object as an argument and will return a <code class="literal">Material</code> object. This material will be made completely transparent (highlighted) but this transparency and the color will be modified by the texture we assign to the first texture channel (second highlighted line). The textures type is set to <code class="literal">Image</code> and because we rendered these images with a premultiplied alpha channel, we use the <code class="literal">setImageFlags()</code> method<a class="indexterm" id="id576"/> to indicate that we want to use this alpha channel and set the <code class="literal">premul</code> attribute<a class="indexterm" id="id577"/> of the image to <code class="literal">True</code>:</p><div><pre class="programlisting">def imagemat(image):
   mat = Material.New()
<strong>   mat.setAlpha(0.0)</strong>
   mat.setMode(mat.getMode()|Material.Modes.ZTRANSP)
   tex = Texture.New()
   tex.setType('Image')
   tex.image = image
   tex.setImageFlags('UseAlpha')
   image.premul=True
<strong>   mat.setTexture(0,tex,Texture.TexCo.UV,Texture.MapTo.COL|Texture.MapTo.ALPHA)</strong>
   return mat</pre></div><p>Each face that we will apply a material to will have to be UV-mapped. In this case, this will be the simplest mapping possible as the square face will be mapped to match a rectangular image exactly once. This is often called <strong>reset mapping</strong><a class="indexterm" id="id578"/> and therefore the function we define is called <code class="literal">reset()</code>. It will take a Blender <code class="literal">MFace</code> object that we assume to be a quad and set its <code class="literal">uv</code> attribute<a class="indexterm" id="id579"/> to a list of 2D vectors, one for each vertex. These vectors map each vertex to a corner of the image:</p><div><pre class="programlisting">def reset(face):
   face.uv=[vec(0.0,0.0),vec(1.0,0.0),vec(1.0,1.0),vec(0.0,1.0)]</pre></div><p>The <code class="literal">cardboard()</code> function<a class="indexterm" id="id580"/> takes care of constructing an actual <code class="literal">Mesh</code> object from the two <code class="literal">Image</code> objects passed as arguments. It starts off by constructing two square faces that cross each other along the z-axis. The next step is to add an UV-layer (highlighted) and make it the active one:</p><div><pre class="programlisting">def cardboard(left,right):
   mesh = Mesh.New('Cardboard')
   verts=[(0.0,0.0,0.0),(1.0,0.0,0.0),(1.0,0.0,1.0),(0.0,0.0,1.0),
          (0.5,-0.5,0.0),(0.5,0.5,0.0),(0.5,0.5,1.0),(0.5,-0.5,1.0)]
   faces=[(0,1,2,3),(4,5,6,7)]
   mesh.verts.extend(verts)
   mesh.faces.extend(faces)
   
<strong>   mesh.addUVLayer('Reset')</strong>
   mesh.activeUVLayer='Reset'</pre></div><p>Next, we construct suitable materials from both images and assign these materials to the <code class="literal">materials</code> attribute<a class="indexterm" id="id581"/> of the mesh. Then, we reset the UV coordinates of both faces and assign the materials to them (highlighted). We update the mesh to make the changes visible before we return it:</p><div><pre class="programlisting">   mesh.materials=[imagemat(left),imagemat(right)]
   
   reset(mesh.faces[0])
   reset(mesh.faces[1])
<strong>   mesh.faces[0].mat=0</strong>
<strong>   mesh.faces[1].mat=1</strong>
   
   mesh.update()
   return mesh</pre></div><p>To replace the mesh of the duplication object of a particle system we implement a utility function <code class="literal">setmesh()</code>.<a class="indexterm" id="id582"/> It takes the name of the object with an associated particle system and a <code class="literal">Mesh</code> object as arguments. It locates the Object by name and retrieves the first particle system (highlighted in the next code snippet). The duplication object is stored in the <code class="literal">duplicateObject</code> attribute<a class="indexterm" id="id583"/>. Note that this is a <em>read-only</em> attribute so currently there is no possibility of replacing the object from Python. But we can replace the <em>data</em> of the object and that is what we do by passing the <code class="literal">Mesh</code> object to the <code class="literal">link()</code> method<a class="indexterm" id="id584"/>. Both the emitter object and the particle system's duplication object are changed so we ensure that the changes are visible by calling the <code class="literal">makeDisplayList()</code> method<a class="indexterm" id="id585"/> on both of them before initiating a redraw of all Blender's windows:</p><div><pre class="programlisting">def setmesh(obname,mesh):
   ob = Object.Get(obname)
<strong>   ps = ob.getParticleSystems()[0]</strong>
   dup = ps.duplicateObject
   dup.link(mesh)
   ob.makeDisplayList()
   dup.makeDisplayList()
   Window.RedrawAll()</pre></div><a class="indexterm" id="id586"/><p>The <code class="literal">run()</code> function encapsulates all the work that needs to be done to convert the active object to a set of billboards and assign them to a particle system. First, we retrieve a reference to the active object and make sure that it will be visible when rendered:</p><div><pre class="programlisting">def run():
   act_ob = Scene.GetCurrent().objects.active
   act_ob.restrictRender = False</pre></div><p>The next step is to make the rest of the objects in the scene invisible before we render the billboards. Some object may have been made invisible by the user, therefore, we have to remember the states so that we can restore them later. Also,we do not alter the state of lamps or cameras as making these invisible would leave us with all black images (highlighted):</p><div><pre class="programlisting">   renderstate = {}
   for ob in Scene.GetCurrent().objects:
      renderstate[ob.getName()] = ob.restrictRender
<strong>      if not ob.getType() in ('Camera','Lamp' ):</strong>
         ob.restrictRender = True
   act_ob.restrictRender = False</pre></div><p>Once everything is set up to render just the active object, we render front and right images with suitably framed cameras, just like we did in the <code class="literal">combine.py</code>
<strong> </strong>script. In fact, here we reuse the <code class="literal">frame()</code> function<a class="indexterm" id="id587"/> (highlighted):</p><div><pre class="programlisting">   cameras = ('Front','Right')
<strong>   combine.frame(cameras,act_ob.getBoundBox())</strong>
   images={}
   for cam in cameras:
      im=Image.Load(render(cam))
      im.reload()
      images[cam]=im
   bpy.data.images.active = im
   Window.RedrawAll()</pre></div><a class="indexterm" id="id588"/><p>Then we restore the previous visibility of all the objects in the scene before we construct a new mesh from the two images. We finish by making the active object invisible for rendering and replacing the mesh of the duplication object in a designated particle system by our new mesh:</p><div><pre class="programlisting">   for ob in Scene.GetCurrent().objects:
      ob.restrictRender = renderstate[ob.getName()]
      
   mesh = cardboard(images['Front'],images['Right'])
   act_ob.restrictRender = True
   setmesh('CardboardP',mesh)</pre></div><p>The final lines of code create the cameras necessary to render the billboards (if those cameras are not already present) by calling the <code class="literal">createcams()</code> function<a class="indexterm" id="id589"/> from the <code class="literal">combine</code> module before calling <code class="literal">run()</code>:</p><div><pre class="programlisting">if __name__ == "__main__":
   combine.createcams()
   run()</pre></div><p>The full code is available as <code class="literal">cardboard.py</code> in <code class="literal">combine.blend</code>.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec92"/>Workflow—using cardboard.py</h2></div></div></div><a class="indexterm" id="id590"/><a class="indexterm" id="id591"/><p>Assuming that you have a high poly object that you would like to convert to a set of billboards, a possible work flow would look like this:</p><div><ol class="orderedlist arabic"><li class="listitem">Create an object called <code class="literal">CardboardP</code>.</li><li class="listitem">Assign a particle system to this object.</li><li class="listitem">Create a dummy cube.</li><li class="listitem">Assign the dummy cube as the duplicate object on the first particle system of the <code class="literal">CarboardP</code> object.</li><li class="listitem">Select (make active) the object to be rendered as a set of billboards.</li><li class="listitem">Run <code class="literal">cardboard.py</code>.</li><li class="listitem">Select the original camera and render the scene.</li></ol></div><p>Of course, the script might be changed to omit the automated replacement of the duplication objects mesh if that is more suitable. For example, if we would like to use dupliverted objects instead of particles we would simply generate the cardboard object and assign its mesh to the dupliverted object<a class="indexterm" id="id592"/>. If we do use a particle system we probably do not want all replicated objects to be oriented in exactly the same way. We might, therefore, randomize their rotation, an example setup to accomplish that is shown in the following screenshot:</p><div><img alt="Workflow—using cardboard.py" src="img/0400-08-10.jpg"/></div><p>The next screenshot illustrates the application of billboards created from a tree model and used in a particle system:</p><div><img alt="Workflow—using cardboard.py" src="img/0400-08-11.jpg"/></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec68"/>Generating CAPTCHA challenges</h1></div></div></div><a class="indexterm" id="id593"/><p>In many situations such as blogs, forums, and online polls (to name a few) website operators want to guard against automated postings by spambots without wanting to burden human visitors with registration and authentication. In such situations it has become common to provide the visitor with a so-called CAPTCHA challenge (<a class="ulink" href="http://en.wikipedia.org/wiki/Captcha">http://en.wikipedia.org/wiki/Captcha</a>). A <strong>CAPTCHA</strong> <strong>challenge</strong> (or just <strong>Captcha</strong>) in its simplest form is a picture that should be hard to recognize for a computer, yet simple to decipher by a human as it is, typically a distorted or blurred word or number.</p><p>Of course, no method is foolproof and certainly Captchas are neither without their flaws nor immune to the ever-growing computing power available, but they still remain quite effective. Although the current consensus is that simple blurring and coloring schemes are not up to the task, computers still have a hard time separating individual characters in words when they slightly overlap where humans have hardly any problem doing that.</p><p>Given these arguments, this might be an excellent application of 3D rendering of text as presumably three-dimensional renditions of words in suitable lighting conditions (that is, harsh shadows) are even harder to interpret than two-dimensional text. Our challenge then is to design a server that will respond to requests to render three-dimensional images of some text.</p><a class="indexterm" id="id594"/><p>We will design our server as a web server that will respond to requests addressed to it as URLs of the form <code class="literal">http:&lt;hostname&gt;:&lt;port&gt;/captcha?text=&lt;sometext&gt;</code> and that will return a PNG image—a 3D rendition of that text. In this way it will be easy to integrate this server into an architecture where some software, such as a blog, can easily incorporate this functionality by simply accessing our server through <code class="literal">HTTP</code>. An example of a generated challenge is shown in the illustration:</p><div><img alt="Generating CAPTCHA challenges" src="img/0400-08-12.jpg"/></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec93"/>Design of a CAPTCHA server</h2></div></div></div><a class="indexterm" id="id595"/><p>By making use of the modules available in a full Python distribution the task of implementing an <code class="literal">HTTP</code> server is not as daunting as is may seem. Our Captcha server will be based on the classes provided in Python's <code class="literal">BaseHTTPServer</code> module<a class="indexterm" id="id596"/> so we start by importing this module along with some additional utility modules:</p><div><pre class="programlisting">import BaseHTTPServer
import re
import os
import shutil</pre></div><p>The <code class="literal">BaseHTTPServer</code> module defines two classes that together comprise a complete <code class="literal">HTTP</code> server implementation. The <code class="literal">BaseHTTPServer</code> class implements the basic server that will listen to incoming <code class="literal">HTTP</code> requests on some network port and we will use this class as is.</p><p>Upon receiving a valid <code class="literal">HTTP</code> request <code class="literal">BaseHTTPServer</code> will dispatch this request to a request handler. Our implementation of such a request handler based on the <code class="literal">BaseHTTPRequestHandler</code> is pretty lean as all it is expected to do is to field <code class="literal">GET</code> and <code class="literal">HEAD</code> requests for URIs of the form <code class="literal">captcha?text=abcd</code>. Therefore, all we have to do is <a class="indexterm" id="id597"/>override the <code class="literal">do_GET()</code> and <code class="literal">do_HEAD()</code> methods<a class="indexterm" id="id598"/> of the base class.</p><p>A <code class="literal">HEAD</code> request is expected to return only the headers of a requested object, not its content, to save time when the content isn't changed since the last request (something that can be determined by checking the <code class="literal">Last-Modified</code> header). We ignore such niceties; we will return just the headers when we receive a <code class="literal">HEAD</code> request but we will generate a completely new image nonetheless. This is something of a waste but does keep the code simple. If performance is important, another implementation may be devised.</p><a class="indexterm" id="id599"/><p>Our implementation starts off by defining a <code class="literal">do_GET()</code> method that just calls the <code class="literal">do_HEAD()</code> method that will generate a Captcha challenge and return the headers to the client. <code class="literal">do_GET()</code> subsequently copies the contents of the file object returned by <code class="literal">do_HEAD()</code> to the output file, such as object of the request handler (highlighted), which will in turn return this content to the client (the browser for example):</p><div><pre class="programlisting">class CaptchaRequestHandler(BaseHTTPServer.BaseHTTPRequestHandler):

   def do_GET(self):
      f=self.do_HEAD()
<strong>      shutil.copyfileobj(f,self.wfile)</strong>
      f.close()</pre></div><p>The <code class="literal">do_HEAD()</code> method first determines whether we received a valid request (that is, a URI of the form <code class="literal">captcha?text=abcd</code>) by calling the <code class="literal">gettext()</code> method<a class="indexterm" id="id600"/> (highlighted, defined later in the code). If the URI is not valid, <code class="literal">gettext()</code> will return <code class="literal">None</code> and <code class="literal">do_HEAD()</code> will return a <strong>File not found</strong> error to the client by calling the <code class="literal">send_error()</code> method<a class="indexterm" id="id601"/> of the base class:</p><div><pre class="programlisting">   def do_HEAD(self):
<strong>      text=self.gettext()</strong>
      if text==None:
            self.send_error(404, "File not found")
            return None</pre></div><p>If a valid URI was requested, the actual image is generated by the <code class="literal">captcha()</code> method<a class="indexterm" id="id602"/> that will return the filename of the generated image. If this method fails for any reason an <strong>Internal server</strong> error is returned to the client:</p><div><pre class="programlisting">      try:
            filename = self.captcha(text)
      except:
            self.send_error(500, "Internal server error")
            return None</pre></div><p>If everything went well we open the image file, send a <strong>200</strong> response to the client (indicating a successful operation), and return a <code class="literal">Content-type</code> header stating that we will return a <code class="literal">png</code> image. Next, we use the <code class="literal">fstat()</code> function<a class="indexterm" id="id603"/> with the number of the open file handle as argument to retrieve the length of the generate image and return this as a <code class="literal">Content-Length</code> header (highlighted) followed by the modification time and an empty line signifying the end of the headers before returning the open file object <code class="literal">f</code>:</p><div><pre class="programlisting">      f = open(filename,'rb')
      self.send_response(200)
      self.send_header("Content-type", 'image/png')
<strong>      fs = os.fstat(f.fileno())</strong>
<strong>      self.send_header("Content-Length", str(fs[6]))</strong>
      self.send_header("Last-Modified",self.date_time_string(fs.st_mtime))
      self.end_headers()
      return f</pre></div><a class="indexterm" id="id604"/><p>The <code class="literal">gettext()</code> method verifies that the request passed to our request handler in the path variable is a valid URI by matching it against a regular expression. The <code class="literal">match()</code> function<a class="indexterm" id="id605"/> from Python's <code class="literal">re</code> module will return a <code class="literal">MatchObject</code> if the regular expression matches and <code class="literal">None</code> if it does not. If there actually is a match we return the contents of the first match group (the characters that match the expression between the parentheses in the regular expression, in our case the value of the <code class="literal">text</code> argument), otherwise we return <code class="literal">None</code>:</p><div><pre class="programlisting">   def gettext(self):
      match = re.match(r'^.*/captcha\?text=(.*)$',self.path)
      if match != None:
         return match.group(1)
      
      return None</pre></div><p>Now we come to the Blender-specific task of actually generating the rendered 3D text that will be returned as a <code class="literal">png</code> image. The <code class="literal">captcha()</code> method will take the text to render as an argument and will return the filename of the generated image. We will assume that the lights and camera in the <code class="literal">.blend</code> file we run <code class="literal">captcha.py</code> from are correctly set up to display our text in a readable way. Therefore, the <code class="literal">captcha()</code> method will just consider itself with configuring a suitable <code class="literal">Text3d</code> object and rendering it.</p><p>Its first task is to determine the current scene and check whether there is an Object called <code class="literal">Text</code> that can be reused (highlighted). Note that it is perfectly valid to have other objects in the scene to obfuscate the display even more:</p><div><pre class="programlisting">   def captcha(self,text):
      import Blender
      scn = Blender.Scene.GetCurrent()

      text_ob = None
      for ob in scn.objects:
<strong>         if ob.name == 'Text' :</strong>
            text_ob = ob.getData()
            break</pre></div><p>If there was no reusable <code class="literal">Text3d</code> object, a new one is created:</p><div><pre class="programlisting">      if text_ob == None:
         text_ob = Blender.Text3d.New('Text')
         ob=scn.objects.new(text_ob)
         ob.setName('Text')</pre></div><p>The next step is to set the text of the <code class="literal">Text3d</code> object to the argument passed to the <code class="literal">captcha()</code> method and make it 3D by setting its extrude depth. We also alter the width of the characters and shorten the spacing between them to deteriorate the separation. Adding a small bevel will soften the contours of the characters what may add to the difficulty for a robot to discern the characters if the lighting is subtle (highlighted). We could have chosen to use a different font for our text that is even harder to read for a bot and this would be the place to set this font (see the following information box).</p><div><div><h3 class="title"><a id="note27"/>Note</h3><p>
<strong>Something is missing</strong>
</p><p>Blender's API documentation has a small omission: there seems to be no way to configure a different font for a <code class="literal">Text3d</code> object. There is an undocumented <code class="literal">setFont()</code> method,<a class="indexterm" id="id606"/> however, that will take a <code class="literal">Font</code> object as argument. The code to accomplish the font change would look like this: </p><p>
<code class="literal">fancyfont=Text3d.Load( '/usr/share/fonts/ttf/myfont.ttf')</code>
<code class="literal">text_ob.setFont(fancyfont)</code>
</p><p>We have chosen not to include this code, however, partly because it is undocumented but mostly because the available fonts differ greatly from system to system. If you do have a suitable font available, by all means use it. Script type fonts which resemble handwriting for example may raise the bar even further for a computer.</p></div></div><a class="indexterm" id="id607"/><p>The final step is to update Blender's display list for this object so that our changes will be rendered:</p><div><pre class="programlisting">      text_ob.setText(text)
      text_ob.setExtrudeDepth(0.3)
      text_ob.setWidth(1.003)
      text_ob.setSpacing(0.8)
<strong>      text_ob.setExtrudeBevelDepth(0.01)</strong>
      ob.makeDisplayList()</pre></div><p>Once our <code class="literal">Text3d</code> object is in place our next task is to actually render an image to a file. First, we retrieve the rendering context from the current scene and set the <code class="literal">displayMode</code> to <code class="literal">0</code> to prevent an additional render window popping up:</p><div><pre class="programlisting">      context = scn.getRenderingContext()
      context.displayMode=0</pre></div><p>Next, we set the image size and indicate that we want a <code class="literal">png</code> image. By enabling RGBA and setting the alpha mode to <code class="literal">2</code> we ensure that there won't be any sky visible and that our image will have a nice transparent background:</p><div><pre class="programlisting">      context.imageSizeX(160)
      context.imageSizeY(120)
      context.setImageType(Blender.Scene.Render.PNG)
      context.enableRGBAColor()
      context.alphaMode=2</pre></div><p>Even though we will render just a still image, we will use the <code class="literal">renderAnim()</code> method of the rendering context because otherwise the results will not be rendered to a file but to a buffer. Therefore, we set the start and end frames of the animation to 1 (just like the current frame) to ensure that we generate just a single frame. We then use the <code class="literal">getFrameFilename()</code> method<a class="indexterm" id="id608"/> to return the filename (with the complete path) of the rendered frame (highlighted). We then both store this filename and return it as a result:</p><div><pre class="programlisting">      context.currentFrame(1)
      context.sFrame=1
      context.eFrame=1
      context.renderAnim()
<strong>      self.result=context.getFrameFilename()</strong>
      return self.result</pre></div><a class="indexterm" id="id609"/><p>The final part of the script defines a <code class="literal">run()</code> function<a class="indexterm" id="id610"/> to start the Captcha server and calls this function if the script is running standalone (that is, not included as a module). By defining a <code class="literal">run()</code> function this way we can encapsulate the often used server defaults, such as port number to listen on (highlighted), yet allow reuse of the module if a different setup is required: </p><div><pre class="programlisting">def run(HandlerClass = CaptchaRequestHandler,
       ServerClass = BaseHTTPServer.HTTPServer,
       protocol="HTTP/1.1"):

<strong>      port = 8080</strong>
      server_address = ('', port)

      HandlerClass.protocol_version = protocol

      httpd = ServerClass(server_address, HandlerClass)

      httpd.serve_forever()

if __name__ == '__main__':
      run()</pre></div><p>The full code is available as <code class="literal">captcha.py</code> in the file <code class="literal">captcha.blend</code> and the server may be started in a number of ways: from the text editor (with <em>Alt + P</em>) from the menu <strong>Scripts | render | captcha</strong> or by invoking Blender in <em>background</em> mode from the command line. To stop the server again it is necessary to terminate Blender. Typically, this can be done by pressing <strong>Ctrl + C</strong> in the console or DOSbox</p><div><div><h3 class="title"><a id="note28"/>Note</h3><p>
<strong>Warning</strong>
</p><p>Note that as this server responds to requests from anybody it is far from secure. As a minimum it should be run behind a firewall that restricts access to it to just the server that needs the Captcha challenges. Before running it in any location that might be accessible from the Internet you should think thoroughly about your network security!</p></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec69"/>Summary</h1></div></div></div><p>In this chapter, we automated the render process and learned how to perform a number of operations on images without the need for an external image editing program. We have learned:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How to automate the rendering process</li><li class="listitem" style="list-style-type: disc">How to create multiple views for product presentations</li><li class="listitem" style="list-style-type: disc">How to create billboards from complex objects</li><li class="listitem" style="list-style-type: disc">How to manipulate images, including render results by using the Python Imaging Library (PIL)</li><li class="listitem" style="list-style-type: disc">How to create a server that creates on demand images that may be used as CAPTCHA challenges</li></ul></div><p>In the final chapter, we will look at some housekeeping tasks.</p></div></body></html>