<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<meta charset="utf-8"/>
<meta content="pandoc" name="generator"/>
<title>ch007.xhtml</title>

<!-- kobo-style -->
<style id="koboSpanStyle" type="text/css" xmlns="http://www.w3.org/1999/xhtml">.koboSpan { -webkit-text-combine: inherit; }</style>
</head>
<body epub:type="bodymatter">

<h1 data-number="7">Chapter 3<br/>
Project 1.1: Data Acquisition Base Application</h1>
<p>The beginning of the data pipeline is acquiring the raw data from various sources. This chapter has a single project to create a <strong>command-line</strong> <strong>application </strong>(<strong>CLI</strong>) that extracts relevant data from files in CSV format. This initial application will restructure the raw data into a more useful form. Later projects (starting in <a href="ch013.xhtml#x1-2080009"><em>Chapter</em><em> 9</em></a>, <a href="ch013.xhtml#x1-2080009"><em>Project 3.1: Data Cleaning Base</em> <em>Application</em></a>) will add features for cleaning and validating the data.</p>
<p>This chapter’s project covers the following essential skills:</p>
<ul>
<li><p>Application design in general. This includes an object-oriented design and the SOLID design principles, as well as functional design.</p></li>
<li><p>A few CSV file processing techniques. This is a large subject area, and the project focuses on restructuring source data into a more usable form.</p></li>
<li><p>CLI application construction.</p></li>
<li><p>Creating acceptance tests using the Gherkin language and <strong>behave</strong> step definitions.</p></li>
<li><p>Creating unit tests with mock objects.</p></li>
</ul>
<p>We’ll start with a description of the application, and then move on to talk about the architecture and construction. This will be followed by a detailed list of deliverables. </p>

<h2 data-number="7.1">3.1  Description</h2>
<p>Analysts and decision-makers need to acquire data for further analysis. In many cases, the data is available in CSV-formatted files. These files may be extracts from databases or downloads from web services.</p>
<p>For testing purposes, it’s helpful to start with something relatively small. Some of the Kaggle data sets are very, very large, and require sophisticated application design. One of the most fun small data sets to work with is Anscombe’s Quartet. This can serve as a test case to understand the issues and concerns in acquiring raw data.</p>
<p>We’re interested in a few key features of an application to acquire data:</p>
<ul>
<li><p>When gathering data from multiple sources, it’s imperative to convert it to a common format. Data sources vary, and will often change with software upgrades. The acquisition process needs to be flexible with respect to data sources and avoid assumptions about formats.</p></li>
<li><p>A CLI application permits a variety of automation possibilities. For example, a CLI application can be ”wrapped” to create a web service. It can be run from the command line manually, and it can be automated through enterprise job scheduling applications.</p></li>
<li><p>The application must be extensible to reflect source changes. In many cases, enterprise changes are not communicated widely enough, and data analysis applications discover changes ”the hard way” — a source of data suddenly includes unexpected or seemingly invalid values.</p></li>
</ul>
<p></p>

<h3 data-number="7.1.1">3.1.1  User experience</h3>
<p>The <strong>User Experience </strong>(<strong>UX</strong>) will be a command-line application with options to fine-tune the data being gathered. This essential UX pattern will be used for many of this book’s projects. It’s flexible and can be made to run almost anywhere.</p>
<p>Our expected command line should look something like the following:</p>
<div><div><pre class="console">% python src/acquire.py -o quartet Anscombe_quartet_data.csv</pre>
</div>
</div>
<p>The <code>-o</code><code> quartet</code> argument specifies a directory into which the resulting extracts are written. The source file contains four separate series of data. Each of the series can be given an unimaginative name like <code>quartet/series_1.json</code>.</p>
<p>The positional argument, <code>Anscombe_quartet_data.csv</code>, is the name of the downloaded source file.</p>
<p>While there’s only one file – at the present time – a good design will work with multiple input files and multiple source file formats.</p>
<p>In some cases, a more sophisticated ”dashboard” or ”control panel” application might be desirable as a way to oversee the operation of the data acquisition process. The use of a web-based API can provide a very rich interactive experience. An alternative is to use tools like <strong>rich </strong>or <strong>Textual </strong>to build a small text-based display. Either of these choices should be built as a wrapper that executes the essential CLI application as a subprocess.</p>
<p>Now that we’ve seen an overview of the application’s purpose and UX, let’s take a look at the source data. </p>


<h3 data-number="7.1.2">3.1.2  About the source data</h3>
<p>Here’s the link to the dataset we’ll be using:</p>
<p><a class="url" href="https://www.kaggle.com/datasets/carlmcbrideellis/data-anscombes-quartet">https://www.kaggle.com/datasets/carlmcbrideellis/data-anscombes-quartet</a></p>
<p>You’ll need to register with Kaggle to download this data.</p>
<p>The Kaggle URL presents a page with information about the CSV-formatted file. Clicking the <strong>Download </strong>button will download the small file of data to your local computer.</p>
<p>The data is available in this book’s GitHub repository’s <code>data</code> folder, also.</p>
<p>Once the data is downloaded, you can open the <code>Anscombe_quartet_data.csv</code> file to inspect the raw data.</p>
<p>The file contains four series of (<em>x,y</em>) pairs in each row. We can imagine each row as having [(<em>x</em><sub>1</sub><em>,y</em><sub>1</sub>)<em>,</em>(<em>x</em><sub>2</sub><em>,y</em><sub>2</sub>)<em>,</em>(<em>x</em><sub>3</sub><em>,y</em><sub>3</sub>)<em>,</em>(<em>x</em><sub>4</sub><em>,y</em><sub>4</sub>)]. It is, however, compressed, as we’ll see below.</p>
<p>We might depict the idea behind this data with an entity-relationship diagram as shown in <a href="#3.1"><em>Figure 3.1</em></a>.</p>
<figure class="IMG---Figure">
<img alt="Figure 3.1: Notional entity-relationship diagram " src="img/file8.jpg"/>
<figcaption class="IMG---Caption">Figure 3.1: Notional entity-relationship diagram </figcaption>
</figure>
<p>Interestingly, the data is not organized as four separate (<em>x,y</em>) pairs. The downloaded file is organized as follows:</p>
<div><img alt="[x1,2,3,y1,y2,y3,x4,y4] " class="math-display" src="img/file9.jpg"/>
</div>
<p>We can depict the actual source entity type in an ERD, as shown in <a href="#3.2"><em>Figure 3.2</em></a>.</p>
<figure class="IMG---Figure">
<img alt="Figure 3.2: Source entity-relationship diagram " src="img/file10.jpg"/>
<figcaption class="IMG---Caption">Figure 3.2: Source entity-relationship diagram </figcaption>
</figure>
<p>One part of this application’s purpose is to disentangle the four series into separate files. This forces us to write some transformational processing to rearrange each row’s data elements into four separate data sets.</p>
<p>The separate series can then be saved into four separate files. We’ll look more deeply at the details of creating the separate files for a separate project in <a href="ch015.xhtml#x1-26400011"><em>Chapter</em><em> 11</em></a>, <a href="ch015.xhtml#x1-26400011"><em>Project 3.7: Interim Data Persistence</em></a>. For this project, any file format for the four output files will do nicely; ND JSON serialization is often ideal.</p>
<p>We encourage you to take a look at the file before moving on to consider how it needs to be transformed into distinct output files.</p>
<p>Given this compressed file of source data, the next section will look at the expanded output files. These will separate each series to make them easier to work with. </p>


<h3 data-number="7.1.3">3.1.3  About the output data</h3>
<p>The ND JSON file format is described in <a class="url" href="http://ndjson.org">http://ndjson.org</a> and <a class="url" href="https://jsonlines.org">https://jsonlines.org</a>. The idea is to put each individual entity into a JSON document written as a single physical line. This fits with the way the Python <code>json.dumps()</code> function works: if no value is provided for the <code>indent</code> parameter (or if the value is <code>indent=None</code>), the text will be as compact as possible.</p>
<p>The <code>series_1.json</code> output file should start like this:</p>
<pre class="source-code">{"x": "10.0", "y": "8.04"}
{"x": "8.0", "y": "6.95"}
{"x": "13.0", "y": "7.58"}
...</pre>
<p>Each row is a distinct, small JSON document. The row is built from a subset of fields in the input file. The values are strings: we won’t be attempting any conversions until the cleaning and validating projects in <a href="ch013.xhtml#x1-2080009"><em>Chapter</em><em> 9</em></a>, <a href="ch013.xhtml#x1-2080009"><em>Project 3.1:</em> <em>Data Cleaning Base Application</em></a>.</p>
<p>We’ll require the user who runs this application to create a directory for the output and provide the name of the directory on the command line. This means the application needs to present useful error messages if the directory doesn’t actually exist. The <code>pathlib.Path</code> class is very helpful for confirming a directory exists.</p>
<p>Further, the application should be cautious about overwriting any existing files. The <code>pathlib.Path</code> class is very helpful for confirming a file already exists.</p>
<p>This section has looked at the input, processing, and output of this application. In the next section, we’ll look at the overall architecture of the software. </p>



<h2 data-number="7.2">3.2  Architectural approach</h2>
<p>We’ll take some guidance from the C4 model ( <a class="url" href="https://c4model.com">https://c4model.com</a>) when looking at our approach.</p>
<ul>
<li><p><strong>Context</strong>: For this project, a context diagram would show a user extracting data from a source. You may find it helpful to draw this diagram.</p></li>
<li><p><strong>Containers</strong>: This project will run on the user’s personal computer. As with the context, the diagram is small, but some readers may find it helpful to take the time to draw it.</p></li>
<li><p><strong>Components</strong>: We’ll address these below.</p></li>
<li><p><strong>Code</strong>: We’ll touch on this to provide some suggested directions.</p></li>
</ul>
<p>We can decompose the software architecture into these two important components:</p>
<ul>
<li><p><code>model</code>: This module has definitions of target objects. In this project, there’s only a single class here.</p></li>
<li><p><code>extract</code>: This module will read the source document and creates model objects.</p></li>
</ul>
<p>Additionally, there will need to be these additional functions:</p>
<ul>
<li><p>A function for parsing the command-line options.</p></li>
<li><p>A <code>main()</code> function to parse options and do the file processing.</p></li>
</ul>
<p>As suggested in <a href="ch005.xhtml#x1-170001"><em>Chapter</em><em> 1</em></a>, <a href="ch005.xhtml#x1-170001"><em>Project Zero: A Template for Other Projects</em></a>, the initialization of logging will often look like the following example:</p>
<div><div><pre class="source-code">if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    main()</pre>
</div>
</div>
<p>The idea is to write the <code>main()</code> function in a way that maximizes reuse. Avoiding logging initialization means other applications can more easily import this application’s <code>main()</code> function to reuse the data acquisition features.</p>
<p>Initializing logging within the <code>main()</code> function can undo previous logging initialization. While there are ways to have a composite application tolerate each <code>main()</code> function doing yet another initialization of logging, it seems simpler to refactor this functionality outside the important processing.</p>
<p>For this project, we’ll look at two general design approaches for the model and extract components. We’ll utilize this opportunity to highlight the importance of adhering to SOLID design principles.</p>
<p>First, we’ll show an object-oriented design using class definitions. After that, we’ll show a functional design, using only functions and stateless objects. </p>

<h3 data-number="7.2.1">3.2.1  Class design</h3>
<p>One possible structure for the classes and functions of this application is shown in <a href="#3.3"><em>Figure 3.3</em></a>.</p>
<figure class="IMG---Figure">
<img alt="Figure 3.3: Acquisition Application Model " src="img/file11.jpg"/>
<figcaption class="IMG---Caption">Figure 3.3: Acquisition Application Model </figcaption>
</figure>
<p>The <code>model</code> module contains a single class definition for the raw <code>XYPair</code>. Later, this is likely to expand and change. For now, it can seem like over-engineering.</p>
<p>The <code>acquisition</code> module contains a number of classes that collaborate to build <code>XYPair</code> objects for any of the four series. The abstract <code>PairBuilder</code> class defines the general features of creating an <code>XYPair</code> object.</p>
<p>Each subclass of the <code>PairBuilder</code> class has a slightly different implementation. Specifically, the <code>Series1Pair</code> class has a <code>from_row()</code> method that assembles a pair from the <em>x</em><sub>1,2,3</sub> and <em>y</em><sub>1</sub> values. Not shown in the diagram are the three other subclasses that use distinct pairs of columns to create <code>XYPair</code> objects from the four series.</p>
<div><div><p>The diagram and most of the examples here use <code>list[str]</code> as the type for a row from a CSV reader.</p>
<p>If a <code>csv.DictReader</code> is used, the source changes from <code>list[str]</code> to</p>
<p><code>dict[str,</code><code> str]</code>. This small, but important, change will ripple throughout the examples.</p>
<p>In many cases, it seems like a <code>csv.DictReader</code> is a better choice. Column names can be provided if the CSV file does not have names in the first row.</p>
<p>We’ve left the revisions needed for this change as part of the design work for you.</p>
</div>
</div>
<p>The overall <code>Extract</code> class embodies the various algorithms for using an instance of the <code>PairBuilder</code> class and a row of source data to build <code>XYPair</code> instances. The <code>build_pair(list[str])</code><code> -&gt;</code><code> XYPair</code> method makes a single item from a row parsed from a CSV file.</p>
<p>The job of the <code>main()</code> function is to create instances of each of the four <code>PairBuilder</code> subclasses. These instances are then used to create four instances of the <code>Extract</code> class. These four <code>Extract</code> objects can then build four <code>XYPair</code> objects from each source row.</p>
<p>The <code>dataclass.asdict()</code> function can be used to convert an <code>XYPair</code> object into a <code>dict[str,</code><code> str]</code> object. This can be serialized by <code>json.dumps()</code> and written to an appropriate output file. This conversion operation seems like a good choice for a method in the abstract <code>PairBuilder</code> class. This can be used to write an <code>XYPair</code> object to an open file.</p>
<p>The top-level functions, <code>main()</code> and <code>get_options()</code>, can be placed in a separate module, named <code>acquisition</code>. This module will import the various class definitions from the <code>model</code> and <code>csv_extract</code> modules.</p>
<p>It’s often helpful to review the SOLID design principles. In particular, we’ll look closely at the <strong>Dependency Inversion principle</strong>. </p>


<h3 data-number="7.2.2">3.2.2  Design principles</h3>
<p>We can look at the SOLID design principles to be sure that the object-oriented design follows those principles.</p>
<ul>
<li><p><strong>Single Responsibility</strong>: Each of the classes seems to have a single responsibility.</p></li>
<li><p><strong>Open-Closed</strong>: Each class seems open to extension by adding subclasses.</p></li>
<li><p><strong>Liskov Substitution</strong>: The <code>PairBuilder</code> class hierarchy follows this principle since each subclass is identical to the parent class.</p></li>
<li><p><strong>Interface Segregation</strong>: The interfaces for each class are minimized.</p></li>
<li><p><strong>Dependency Inversion</strong>: There’s a subtle issue regarding dependencies among classes. We’ll look at this in some detail.</p></li>
</ul>
<p>One of the SOLID design principles suggests avoiding tight coupling between the <code>PairBuilder</code> subclasses and the <code>XYPair</code> class. The idea would be to provide a protocol (or interface) for the <code>XYPair</code> class. Using the protocol in type annotations would permit any type that implemented the protocol to be provided to the class. Using a protocol would break a direct dependency between the <code>PairBuilder</code> subclasses and the <code>XYPair</code> class.</p>
<p>This object-oriented design issue surfaces often, and generally leads to drawn-out, careful thinking about the relationships among classes and the SOLID design principles.</p>
<p>We have the following choices:</p>
<ul>
<li><p>Have a direct reference to the <code>XYPair</code> class inside the <code>PairBuilder</code> class. This would be <code>def</code><code> from_row(row:</code><code> list[str])</code><code> -&gt;</code><code> XYPair:</code>. This breaks the Dependency Inversion principle.</p></li>
<li><p>Use <code>Any</code> as the type annotation. This would be <code>def</code><code> from_row(row:</code><code> list[str])</code><code> -&gt;</code><code> Any:</code>. This makes the type hints less informative.</p></li>
<li><p>Attempt to create a protocol for the resulting type, and use this in the type hints.</p></li>
<li><p>Introduce a type alias that (for now) only has one value. In future expansions of the <code>model</code> module, additional types might be introduced.</p></li>
</ul>
<p>The fourth alternative gives us the flexibility we need for type annotation checking. The idea is to include a type alias like the following in the <code>model</code> module:</p>
<div><div><pre class="source-code">from dataclasses import dataclass
from typing import TypeAlias

@dataclass
class XYPair:
    # Definition goes here

RawData: TypeAlias = XYPair</pre>
</div>
</div>
<p>As alternative classes are introduced, the definition of <code>RawData</code> can be expanded to include the alternatives. This might evolve to look like the following:</p>
<div><div><pre class="source-code">from dataclasses import dataclass
from typing import TypeAlias

@dataclass
class XYPair:
    # Definition goes here
    pass

@dataclass
class SomeOtherStructure:
    # Some other definition, here
    pass

RawData: TypeAlias = XYPair | SomeOtherStructure</pre>
</div>
</div>
<p>This permits extension to the <code>PairBuilder</code> subclasses as the <code>model</code> module evolves. The <code>RawData</code> definition needs to be changed as new classes are introduced. Annotation-checking tools like <strong>mypy </strong>cannot spot the invalid use of any of the classes that comprise the alternative definitions of the <code>RawData</code> type alias.</p>
<p>Throughout the rest of the application, classes and functions can use <code>RawData</code> as an abstract class definition. This name represents a number of alternative definitions, any one of which might be used at run-time.</p>
<p>With this definition of <code>RawData</code>, the <code>PairBuilder</code> subclasses can use a definition of the following form:</p>
<div><div><pre class="source-code">from model import RawData, XYPair
from abc import ABC, abstractmethod

class PairBuilder(ABC):
    target_class: type[RawData]

    @abstractmethod
    def from_row(self, row: list[str]) -&gt; RawData:
        ...

class Series1Pair(PairBuilder):
    target_class = XYPair

    def from_row(self, row: list[str]) -&gt; RawData:
        cls = self.target_class
        # the rest of the implementation...
        # return cls(arguments based on the value of row)</pre>
</div>
</div>
<p>A similar analysis holds for the <code>main()</code> function. This can be directly tied to the <code>Extract</code> class and the various subclasses of the <code>PairBuilder</code> class. It’s very important for these classes to be injected at run time, generally based on command-line arguments.</p>
<p>For now, it’s easiest to provide the class names as default values. A function like the following might be used to get options and configuration parameters:</p>
<div><div><pre class="source-code">def get_options(argv: list[str]) -&gt; argparse.Namespace:
    defaults = argparse.Namespace(
        extract_class=Extract,
        series_classes=[Series1Pair, Series2Pair, Series3Pair, Series4Pair],
    )</pre>
</div>
</div>
<p>The <code>defaults</code> namespace is provided as an argument value to the <code>ArgumentParser.parse_args()</code> method. This set of defaults serves as a kind of dependency injection throughout the application. The <code>main</code> function can use these class names to build an instance of the given extract class, and then process the given source files.</p>
<p>A more advanced CLI could provide options and arguments to tailor the class names. For more complex applications, these class names would be read from a configuration file.</p>
<p>An alternative to the object-oriented design is a functional design. We’ll look at that alternative in the next section. </p>


<h3 data-number="7.2.3">3.2.3  Functional design</h3>
<p>The general module structure shown in <a href="#x1-620001"><em>Class design</em></a> applies to a functional design also. The <code>model</code> module with a single class definition is also a part of a functional design; this kind of module with a collection of dataclass definitions is often ideal.</p>
<p>As noted above in the <a href="#x1-630002"><em>Design principles</em></a> section, the <code>model</code> module is best served by using a type variable, <code>RawData</code>, as a placeholder for any additional types that may be developed.</p>
<p>The <code>csv_extract</code> module will use a collection of independent functions to build <code>XYPair</code> objects. Each function will be similar in design.</p>
<p>Here are some example functions with type annotations:</p>
<div><div><pre class="source-code">def series_1_pair(row: list[str]) -&gt; RawData:
    ...

def series_2_pair(row: list[str]) -&gt; RawData:
    ...

def series_3_pair(row: list[str]) -&gt; RawData:
    ...

def series_4_pair(row: list[str]) -&gt; RawData:
    ...</pre>
</div>
</div>
<p>These functions can then be used by an <code>extract()</code> function to create the <code>XYPair</code> objects for each of the four series represented by a single row of the source file.</p>
<p>One possibility is to use the following kind of definition:</p>
<div><div><pre class="source-code">SeriesBuilder: TypeVar = Callable[[list[str]], RawData]

def extract(row: list[str], builders: list[SeriesBuilder]) -&gt; list[RawData]:
    ...</pre>
</div>
</div>
<p>This <code>extract()</code> function can then apply all of the given builder functions (<code>series_1_pair()</code> to <code>series_4_pair()</code>) to the given row to create <code>XYPair</code> objects for each of the series.</p>
<p>This design will also require a function to apply <code>dataclass.asdict()</code> and <code>json.dumps()</code> to convert <code>XYPair</code> objects into strings that can be written to an NDJSON file.</p>
<p>Because the functions used are provided as argument values, there is little possibility of a dependency issue among the various functions that make up the application. The point throughout the design is to avoid binding specific functions in arbitrary places. The <code>main()</code> function should provide the row-building functions to the <code>extract</code> function. These functions can be provided via command-line arguments, a configuration file, or be default values if no overrides are given.</p>
<p>We’ve looked at the overall objective of the project, and two suggested architectural approaches. We can now turn to the concrete list of deliverables. </p>



<h2 data-number="7.3">3.3  Deliverables</h2>
<p>This project has the following deliverables:</p>
<ul>
<li><p>Documentation in the <code>docs</code> folder.</p></li>
<li><p>Acceptance tests in the <code>tests/features</code> and <code>tests/steps</code> folders.</p></li>
<li><p>Unit tests for model module classes in the <code>tests</code> folder.</p></li>
<li><p>Mock objects for the <code>csv_extract</code> module tests will be part of the unit tests.</p></li>
<li><p>Unit tests for the <code>csv_extract</code> module components in the <code>tests</code> folder.</p></li>
<li><p>Application to acquire data from a CSV file in the <code>src</code> folder.</p></li>
</ul>
<p>An easy way to start is by cloning the project zero directory to start this project. Be sure to update the <code>pyproject.toml</code> and <code>README.md</code> when cloning; the author has often been confused by out-of-date copies of old projects’ metadata.</p>
<p>We’ll look at a few of these deliverables in a little more detail. We’ll start with some suggestions for creating the acceptance tests. </p>

<h3 data-number="7.3.1">3.3.1  Acceptance tests</h3>
<p>The acceptance tests need to describe the overall application’s behavior from the user’s point of view. The scenarios will follow the UX concept of a command-line application that acquires data and writes output files. This includes success as well as useful output in the event of failure.</p>
<p>The features will look something like the following:</p>
<div><div><pre class="source-code">Feature: Extract four data series from a file with
the peculiar Anscombe Quartet format.

Scenario: When requested, the application extracts all four series.
  Given the "Anscombe_quartet_data.csv" source file exists
  And the "quartet" directory exists
  When we run
    command "python src/acquire.py -o quartet Anscombe_quartet_data.csv"
  Then the "quartet/series_1.json" file exists
  And the "quartet/series_2.json" file exists
  And the "quartet/series_3.json" file exists
  And the "quartet/series_3.json" file exists
  And the "quartet/series_1.json" file starts with
    ’{"x": "10.0", "y": "8.04"}’</pre>
</div>
</div>
<p>This more complex feature will require several step definitions. These include the following:</p>
<ul>
<li><p><code>@given(’The</code><code> "{name}"</code><code> source</code><code> file</code><code> exists’)</code>. This function should copy the example file from a source data directory to the temporary directory used to run the test.</p></li>
<li><p><code>@given(’the</code><code> "{name}"</code><code> directory</code><code> exists’)</code>. This function can create the named directory under the directory used to run the test.</p></li>
<li><p><code>@then(’the</code><code> "{name}"</code><code> file</code><code> exists’)</code>. This function can check for the presence of the named file in the output directory.</p></li>
<li><p><code>@then(’the</code><code> "quartet/series_1.json"</code><code> file</code><code> starts</code><code> with</code><code> ...’)</code>. This function will examine the first line of the output file. In the event the test fails, it will be helpful to display the contents of the file to help debug the problem. A simple <code>assert</code> statement might not be ideal; a more elaborate <code>if</code> statement is needed to write debugging output and raise an <code>AssertionError</code> exception.</p></li>
</ul>
<p>Because the application under test consumes and produces files, it is best to make use of the <strong>behave </strong>tool’s <code>environment.py</code> module to define two functions to create (and destroy) a temporary directory used when running the test. The following two functions are used by <strong>behave </strong>to do this:</p>
<ul>
<li><p><code>before\_scenario(context,</code><code> scenario)</code>: This function can create a directory. The <code>tempfile</code> module has a <code>mkdtemp()</code> function that handles this. The directory needs to be placed into the context so it can be removed.</p></li>
<li><p><code>after_scenario(context,</code><code> scenario)</code>: This function can remove the temporary directory.</p></li>
</ul>
<p>The format for one of the <code>Then</code> clauses has a tiny internal inconsistency. The following uses a mixture of <code>"</code> and <code>’</code> to make it clear where values are inserted into the text:</p>
<pre class="source-code">And the "quartet/series_1.json" file starts with’{"x": "10.0", "y": "8.04"}’</pre>
<p>Some people may be bothered by the inconsistency. One choice is to use <code>’</code> consistently. When there aren’t too many feature files, this pervasive change is easy to make. Throughout the book, we’ll be inconsistent, leaving the decision to make changes for consistency up to you.</p>
<p>Also, note the <code>When</code> clause command is rather long and complicated. The general advice when writing tests like this is to use a summary of the command and push the details into the step implementation function. We’ll address this in a later chapter when the command becomes even longer and more complicated.</p>
<p>In addition to the scenario where the application works correctly, we also need to consider how the application behaves when there are problems. In the next section, we’ll touch on the various ways things might go badly, and how the application should behave. </p>


<h3 data-number="7.3.2">3.3.2  Additional acceptance scenarios</h3>
<p>The suggested acceptance test covers only one scenario. This single scenario — where everything works — can be called the ”happy path”. It would be wise to include scenarios in which various kinds of errors occur, to be sure the application is reliable and robust in the face of problems. Here are some suggested error scenarios:</p>
<ul>
<li><p>Given the <code>Anscombe\_quartet\_data.csv</code> source file does not exist.</p></li>
<li><p>Given the <code>quartet</code> directory does not exist.</p></li>
<li><p>When we run the command <code>python</code><code> src/acquire.py</code><code> --unknown</code><code> option</code></p></li>
<li><p>Given an <code>Anscombe\_quartet\_data.csv</code> source file exists, and the file is in the wrong format. There are numerous kinds of formatting problems.</p>
<ul>
<li><p>The file is empty.</p></li>
<li><p>The file is not a proper CSV file, but is in some other format.</p></li>
<li><p>The file’s contents are in valid CSV format, but the column names do not match the expected column names.</p></li>
</ul></li>
</ul>
<p>Each of the unhappy paths will require examining the log file to be sure it has the expected error messages. The <strong>behave </strong>tool can capture logging information. The <code>context</code> available in each step function has attributes that include captured logging output. Specifically, <code>context.log_capture</code> contains a <code>LogCapture</code> object that can be searched for an error message.</p>
<p>See <a class="url" href="https://behave.readthedocs.io/en/stable/api.html#behave.runner.Context">https://behave.readthedocs.io/en/stable/api.html#behave.runner.Context</a> for the content of the context.</p>
<p>These unhappy path scenarios will be similar to the following:</p>
<div><div><pre class="source-code">Scenario: When the file does not exist, the log has the expected
error message.
  Given the "Anscombe_quartet_data.csv" source file does not exist
  And the "quartet" directory exists
  When we run command "python src/acquire.py -o quartet
  Anscombe_quartet_data.csv"
  Then the log contains "File not found: Anscombe_quartet_data.csv"</pre>
</div>
</div>
<p>This will also require some new step definitions to handle the new <code>Given</code> and <code>Then</code> steps.</p>
<div><div><p>When working with Gherkin, it’s helpful to establish clear language and consistent terminology. This can permit a few step definitions to work for a large number of scenarios. It’s a common experience to recognize similarities after writing several scenarios, and then choose to alter scenarios to simplify and normalize steps.</p>
<p>The <strong>behave </strong>tool will extract missing function definitions. The code snippets can be copied and pasted into a steps module.</p>
</div>
</div>
<p>Acceptance tests cover the application’s overall behavior. We also need to test the individual components as separate units of code. In the next section, we’ll look at unit tests and the mock objects required for those tests. </p>


<h3 data-number="7.3.3">3.3.3  Unit tests</h3>
<p>There are two suggested application architectures in <a href="#x1-610002"><em>Architectural approach</em></a>. Class-based design includes two functions and a number of classes. Each of these classes and functions should be tested in isolation.</p>
<p>Functional design includes a number of functions. These need to be tested in isolation. Some developers find it easier to isolate function definitions for unit testing. This often happens because class definitions may have explicit dependencies that are hard to break.</p>
<p>We’ll look at a number of the test modules in detail. We’ll start with tests for the <code>model</code> module.</p>

<h4 class="likesubsubsectionHead" data-number="7.3.3.1">Unit testing the model</h4>
<p>The <code>model</code> module only has one class, and that class doesn’t really do very much. This makes it relatively easy to test. A test function something like the following should be adequate:</p>
<div><div><pre class="source-code">from unittest.mock import sentinel
from dataclasses import asdict

def test_xypair():
    pair = XYPair(x=sentinel.X, y=sentinel.Y)
    assert pair.x == sentinel.X
    assert pair.y == sentinel.Y
    assert asdict(pair) == {"x": sentinel.X, "y": sentinel.Y}</pre>
</div>
</div>
<p>This test uses the <code>sentinel</code> object from the <code>unittest.mock</code> module. Each <code>sentinel</code> attribute — for example, <code>sentinel.X</code> — is a unique object. They’re easy to provide as argument values and easy to spot in results.</p>
<p>In addition to testing the <code>model</code> module, we also need to test the <code>csv_extract</code> module, and the overall <code>acquire</code> application. In the next section, we’ll look at the extract unit test cases.</p>


<h4 class="likesubsubsectionHead" data-number="7.3.3.2">Unit testing the PairBuilder class hierarchy</h4>
<p>When following an object-oriented design, the suggested approach is to create a <code>PairBuilder</code> class hierarchy. Each subclass will perform slightly different operations to build an instance of the <code>XYPair</code> class.</p>
<p>Ideally, the implementation of the <code>PairBuilder</code> subclasses is not tightly coupled to the <code>XYPair</code> class. There is some advice in the <a href="#x1-630002"><em>Design principles</em></a> section on how to support dependency injection via type annotations. Specifically, the <code>model</code> module is best served by using a type variable, <code>RawData</code>, as a placeholder for any additional types that may be developed.</p>
<p>When testing, we want to replace this class with a mock class to assure that the interface for the family of <code>RawData</code> classes — currently only a single class, <code>XYPair</code> — is honored.</p>
<p>A <code>Mock</code> object (built with the <code>unittest.mock</code> module) works out well as a replacement class. It can be used for the <code>XYPair</code> class in the subclasses of the <code>PairBuilder</code> class.</p>
<p>The tests will look like the following example:</p>
<div><div><pre class="source-code">from unittest.mock import Mock, sentinel, call

def test_series1pair():
    mock_raw_class = Mock()
    p1 = Series1Pair()
    p1.target_class = mock_raw_class
    xypair = p1.from_row([sentinel.X, sentinel.Y])
    assert mock_raw_class.mock_calls == [
        call(sentinel.X, sentinel.Y)
    ]</pre>
</div>
</div>
<p>The idea is to use a <code>Mock</code> object to replace the specific class defined in the <code>Series1Pair</code> class. After the <code>from_row()</code> method is evaluated, the test case confirms that the mock class was called exactly once with the expected two <code>sentinel</code> objects. A further check would confirm that the value of <code>xypair</code> was also a mock object.</p>
<p>This use of <code>Mock</code> objects guarantees that no additional, unexpected processing was done on the objects. The interface for creating a new <code>XYPair</code> was performed correctly by the <code>Series1Pair</code> class.</p>
<p>Similar tests are required for the other pair-building classes.</p>
<p>In addition to testing the <code>model</code> and <code>csv_extract</code> modules, we also need to test the overall <code>acquire</code> application. In the next section, we’ll look at the <code>acquire</code> application unit test cases.</p>


<h4 class="likesubsubsectionHead" data-number="7.3.3.3">Unit testing the remaining components</h4>
<p>The test cases for the overall <code>Extract</code> class will also need to use <code>Mock</code> objects to replace components like a <code>csv.reader</code> and instances of the <code>PairBuilder</code> subclasses.</p>
<p>As noted above in the <a href="#x1-640003"><em>Functional design</em></a> section, the <code>main()</code> function needs to avoid having explicitly named classes or functions. The names need to be provided via command-line arguments, a configuration file, or as default values.</p>
<p>The unit tests should exercise the <code>main()</code> function with <code>Mock</code> objects to be sure that it is defined with flexibility and extensions in mind. </p>




<h2 data-number="7.4">3.4  Summary</h2>
<p>This chapter introduced the first project, the Data Acquisition Base Application. This application extracts data from a CSV file with a complex structure, creating four separate series of data points from a single file.</p>
<p>To make the application complete, we included a command-line interface and logging. This will make sure the application behaves well in a controlled production environment.</p>
<p>An important part of the process is designing an application that can be extended to handle data from a variety of sources and in a variety of formats. The base application contains modules with very small implementations that serve as a foundation for making subsequent extensions.</p>
<p>Perhaps the most difficult part of this project is creating a suite of acceptance tests to describe the proper behavior. It’s common for developers to compare the volume of test code with the application code and claim testing is taking up ”too much” of their time.</p>
<p>Pragmatically, a program without automated tests cannot be trusted. The tests are every bit as important as the code they’re exercising.</p>
<p>The unit tests are — superficially — simpler. The use of mock objects makes sure each class is tested in isolation.</p>
<p>This base application acts as a foundation for the next few chapters. The next chapter will add RESTful API requests. After that, we’ll have database access to this foundation. </p>


<h2 data-number="7.5">3.5  Extras</h2>
<p>Here are some ideas for you to add to this project. </p>

<h3 data-number="7.5.1">3.5.1  Logging enhancements</h3>
<p>We skimmed over logging, suggesting only that it’s important and that the initialization for logging should be kept separate from the processing within the <code>main()</code> function.</p>
<p>The <code>logging</code> module has a great deal of sophistication, however, and it can help to explore this. We’ll start with logging ”levels”.</p>
<p>Many of our logging messages will be created with the <code>INFO</code> level of logging. For example:</p>
<pre class="source-code">logger.info("%d rows processed", input_count)</pre>
<p>This application has a number of possible error situations that are best reflected with <strong>error</strong>-level logging.</p>
<p>Additionally, there is a tree of named loggers. The root logger, named <code>""</code>, has settings that apply to all the lower-level loggers. This tree tends to parallel the way object inheritance is often used to create classes and subclasses. This can make it advantageous to create loggers for each class. This permits setting the logging level to <strong>debug </strong>for one of many classes, allowing for more focused messages.</p>
<p>This is often handled through a logging configuration file. This file provides the configuration for logging, and avoids the potential complications of setting logging features through command-line options.</p>
<p>There are three extras to add to this project:</p>
<ul>
<li><p>Create loggers for each individual class.</p></li>
<li><p>Add debug-level information. For example, the <code>from_row()</code> function is a place where debugging might be helpful for understanding why an output file is incorrect.</p></li>
<li><p>Get the logging configuration from an initialization file. Consider using a file in <strong>TOML </strong>format as an alternative to the <strong>INI </strong>format, which is a first-class part of the <code>logging</code> module.</p></li>
</ul>
<p></p>


<h3 data-number="7.5.2">3.5.2  Configuration extensions</h3>
<p>We’ve described a little of the CLI for this application. This chapter has provided a few examples of the expected behavior. In addition to command-line parameters, it can help to have a configuration file that provides the slowly changing details of how the application works.</p>
<p>In the discussion in the <a href="#x1-630002"><em>Design principles</em></a> section, we looked closely at dependency inversion. The intent is to avoid an explicit dependency among classes. We want to ”invert” the relationship, making it indirect. The idea is to inject the class name at run time, via parameters.</p>
<p>Initially, we can do something like the following:</p>
<div><div><pre class="source-code">EXTRACT_CLASS: type[Extract] = Extract
BUILDER_CLASSES: list[type[PairBuilder]] = [
    Series1Pair, Series2Pair, Series3Pair, Series4Pair]

def main(argv: list[str]) -&gt; None:
    builders = [cls() for vls in BUILDER_CLASSES]
    extractor = EXTRACT_CLASS(builders)
    # etc.</pre>
</div>
</div>
<p>This provides a base level of parameterization. Some global variables are used to ”inject” the run-time classes. These initializations can be moved to the <code>argparse.Namespace</code> initialization value for the <code>ArgumentParser.parse_args()</code> method.</p>
<p>The initial values for this <code>argparse.Namespace</code> object can be literal values, essentially the same as shown in the global variable parameterization shown in the previous example.</p>
<p>It is more flexible to have the initial values come from a parameter file that’s separate from the application code. This permits changing the configuration without touching the application and introducing bugs through inadvertent typing mistakes.</p>
<p>There are two popular alternatives for a configuration file that can be used to fine-tune the application. These are:</p>
<ul>
<li><p>A separate Python module that’s imported by the application. A module name like <code>config.py</code> is popular for this.</p></li>
<li><p>A non-Python text file that’s read by the application. The TOML file format, parsed by the <code>tomllib</code> module, is ideal.</p></li>
</ul>
<p>Starting with Python 3.11, the <code>tomllib</code> module is directly available as part of the standard library. Older versions of Python should be upgraded to 3.11 or later.</p>
<p>When working with a TOML file, the class name will be a string. The simple and reliable way to translate the class name from string to class object is to use the <code>eval()</code> function. An alternative is to provide a small dictionary with class name strings and class objects. Class names can be resolved through this mapping.</p>
<div><div><p>Some developers worry that the <code>eval()</code> function allows a class of Evil Super Geniuses to tweak the configuration file in a way that will crash the application.</p>
<p>What these developers fail to notice is that the entire Python application is plain text. The Evil Super Genius can more easily edit the application and doesn’t need to do complicated, nefarious things to the parameter file.</p>
<p>Further, ordinary OS-level ownership and permissions can restrict access to the parameter file to a few trustworthy individuals.</p>
</div>
</div>
<p>Don’t forget to include unit test cases for parsing the parameter file. Also, an acceptance test case with an invalid parameter file will be an important part of this project. </p>


<h3 data-number="7.5.3">3.5.3  Data subsets</h3>
<p>To work with large files it will be necessary to extract a subset of the data. This involves adding features like the following:</p>
<ul>
<li><p>Create a subclass of the <code>Extract</code> class that has an upper limit on the number of rows created. This involves a number of unit tests.</p></li>
<li><p>Update the CLI options to include an optional upper limit. This, too, will involve some additional unit test cases.</p></li>
<li><p>Update the acceptance test cases to show operation with the upper limit.</p></li>
</ul>
<p>Note that switching from the <code>Extract</code> class to the <code>SubsetExtract</code> class is something that should be based on an optional command-line parameter. If the <code>--limit</code> option is not given, then the <code>Extract</code> class is used. If the <code>--limit</code> option is given (and is a valid integer), then the <code>SubsetExtract</code> class is used. This will lead to an interesting set of unit test cases to make sure the command-line parsing works properly. </p>


<h3 data-number="7.5.4">3.5.4  Another example data source</h3>
<p>Perhaps the most important extra for this application is to locate another data source that’s of interest to you.</p>
<p>See the <strong>CO</strong><strong>2</strong> <strong>PPM — Trends in Atmospheric Carbon Dioxide </strong>data set, available at <a class="url" href="https://datahub.io/core/co2-ppm">https://datahub.io/core/co2-ppm</a>, for some data that’s somewhat larger. This has a number of odd special-case values that we’ll explore in <a href="ch010.xhtml#x1-1460006"><em>Chapter</em><em> 6</em></a>, <a href="ch010.xhtml#x1-1460006"><em>Project 2.1: Data Inspection Notebook</em></a>.</p>
<p>This project will require you to manually download and unzip the file. In later chapters, we’ll look at automating these two steps. See <a href="ch008.xhtml#x1-780004"><em>Chapter</em><em> 4</em></a>, <a href="ch008.xhtml#x1-780004"><em>Data</em> <em>Acquisition Features: Web APIs and Scraping</em></a> specifically, for projects that will expand on this base project to properly acquire the raw data from a CSV file.</p>
<p>What’s important is locating a source of data that’s in CSV format and small enough that it can be processed in a few seconds. For large files, it will be necessary to extract a subset of the data. See <a href="#x1-760003"><em>Data subsets</em></a> for advice on handling large sets of data.</p>
<p></p>



</body>
</html>
