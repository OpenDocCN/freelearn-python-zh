- en: Deploying Flask Apps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have reached the last chapter of the book, and have a fully functioning
    web app made in Flask, the final step in our development cycle is to make the
    app available for the world. There are many different approaches for hosting your
    Flask app, each of them with its own pros and cons. This chapter will cover the
    best solutions and guide you through situations in which you should choose one
    over the other.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A brief introduction to the most commonly used web servers and gateway interfaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to deploy on various cloud services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build Docker images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to describe services using Docker compose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to describe your infrastructure using AWS CloudFormation (IaC)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to set up and work with a CI/CD system to easily build, test, review, and
    deploy our application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web servers and gateway interfaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will make a quick introduction to the most commonly used
    web servers and **Web Server Gateway Interfaces** (**WSGI**), and their differences
    and configuration. A WSGI is an application-agnostic layer between the web server
    and the python application itself.
  prefs: []
  type: TYPE_NORMAL
- en: Gevent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The simplest option to get a web server up and running is to use a Python library,
    named `gevent`, to host your application. `Gevent` is a Python library that adds
    an alternative way of doing concurrent programming,called co-routines, outside
    of the Python threading library. Gevent has an interface to run WSGI applications
    that is both simple and has good performance. A simple gevent server can easily
    handle hundreds of concurrent users, which is 99% more than the users of websites
    on the internet will ever have. The downside to this option is that its simplicity
    means a lack of configuration options. There is no way, for example, to add rate
    limiting to the server, or to add HTTPS traffic. This deployment option is purely
    for sites that you don''t expect to receive a huge amount of traffic. Remember
    YAGNI: only upgrade to a different web server if you really need to.'
  prefs: []
  type: TYPE_NORMAL
- en: Co-routines are a bit outside of the scope of this book, but a good explanation
    can be found at [https://en.wikipedia.org/wiki/Coroutine](https://en.wikipedia.org/wiki/Coroutine).
  prefs: []
  type: TYPE_NORMAL
- en: 'To install `gevent`, we will use `pip` with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the root of the project directory, in a new file named `gserver.py`, add
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the server with supervisor, just change the command value to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now when you deploy, `gevent` will automatically be installed for you by running
    your `requirements.txt` on every deployment; that is, if you are properly pip
    freezing after every new dependency is added.
  prefs: []
  type: TYPE_NORMAL
- en: Tornado
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Tornado** is another very simple way to deploy WSGI apps purely with Python.
    Tornado is a web server that is designed to handle thousands of simultaneous connections.
    If your application needs real-time data, Tornado also supports WebSockets for
    continuous, long-lived connections to the server.'
  prefs: []
  type: TYPE_NORMAL
- en: Do not use Tornado in production on a Windows server. The Windows version of
    Tornado is not only slower—it is also considered beta-stage quality software.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use Tornado with our application, we will use Tornado''s `WSGIContainer`
    in order to wrap the application object to make it Tornado-compatible. Then, Tornado
    will start to listen on port *80* for requests until the process is terminated.
    In a new file, named `tserver.py`, add the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the Tornado with supervisor privileges, just change the command value
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Nginx and uWSGI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you need better performance or more options for customization, the most
    popular way to deploy a Python web application is to use a Nginx web server as
    a frontend for the WSGI-based uWSGI server by using a reverse proxy. A *reverse
    proxy* is a program in networks that retrieves contents for a client from a server,
    as if it returned from the proxy itself. This process is shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6547d721-3adc-45eb-baa1-850483f73ff3.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Nginx** and **uWSGI** are used like this, because this way, we get the power
    of the Nginx frontend, while having the customization of uWSGI.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nginx** is a very powerful web server that became popular by providing the
    best combination of speed and customization. Nginx is consistently faster than
    other web severs, such as Apache''s httpd, and has native support for WSGI applications.
    It achieves this speed thanks to the developers taking several good architecture
    decisions, as well as not going to try to cover a large amount of use cases, as
    Apache does. The latter point here was a decision taken early on in development
    of Nginx. Having a smaller feature set makes it much easier to maintain and optimize
    the code. From a programmer''s perspective, it is also much easier to configure
    Nginx, as there is no giant default configuration file (`httpd.conf`) that can
    be overridden with `.htaccess` files in each of your project directories.'
  prefs: []
  type: TYPE_NORMAL
- en: '**uWSGI** is a web server that supports several different types of server interfaces,
    including WSGI. uWSGI handles the severing of the application content, as well
    as things such as the load balancing of traffic across several different processes
    and threads.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install uWSGI, we will use a `pip` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In order to run our application, uWSGI needs a file with an accessible WSGI
    application. In a file named `wsgi.py` in the top level of the project directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test uWSGI, we can run it from the **command-line interface** (**CLI**)
    with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If you are running this on your server, you should be able to access port 8080
    and see your app (if you don't have a firewall, that is).
  prefs: []
  type: TYPE_NORMAL
- en: What this command does is load the app object from the `wsgi.py` file, and make
    it accessible from `localhost` on port *8080*. It also spawns four different processes
    with two threads each, which are automatically load balanced by a master process.
    This amount of processes is overkill for the vast majority of websites. To start
    off, use a single process with two threads and scale up from there.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of adding all of the configuration options on the CLI, we can create
    a text file to hold our configuration, which gives us the same benefits for configuration
    that were listed in the *Gevent *section, about supervisor. In the root of the
    project directory, create a file named `uwsgi.ini` and add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: uWSGI supports hundreds of configuration options, as well as several official
    and unofficial plugins. To leverage the full power of uWSGI, you can explore the
    documentation at [http://uwsgi-docs.readthedocs.org/](http://uwsgi-docs.readthedocs.org/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now run the server from supervisor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Because we are installing Nginx from the OS's package manager, the OS will handle
    the running of Nginx for us.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the Nginx version in the official Debian package manager
    is several years old. To install the most recent version, follow the instructions
    available at [http://wiki.nginx.org/Install](http://wiki.nginx.org/Install).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to create an Nginx configuration file, and then, when we push
    the code, we need to copy the configuration file to the `/etc/nginx/sites-available/`
    directory. In the root of the project directory, create a new file named `nginx.conf`,
    and add the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: What this configuration file does is tells Nginx to listen for incoming requests
    on port *80*, and forwards all requests to the WSGI application that is listening
    on port *8080*. Also, it makes an exception for any requests for static files,
    and instead sends those requests directly to the file system. Bypassing uWSGI
    for static files gives a great boost to performance, as Nginx is really good at
    serving static files quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Apache and uWSGI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using Apache httpd with uWSGI mostly requires the same setup. First off, we
    need an Apache configuration file, so let''s create a new file, named `apache.conf`,
    in the root of our project directory, and add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This file simply tells Apache to pass all requests on port *80* to the uWSGI
    web server listening on port *8080*. However, this functionality requires an extra
    Apache plugin from uWSGI, named `mod-proxy-uwsgi`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will cover several solutions for deploying our application on **Platform
    as a Service** (**PaaS**) and **Infrastructure as a Service** (**IaaS**) utilities.
    You will learn how to create several types of environments and make our example
    Blog application available to the world.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying on Heroku
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Heroku** is the first of the **Platform as a Service** (**PaaS**) providers
    that this chapter will cover. PaaS is a service given to web developers that allows
    them to host their websites on a platform that is controlled and maintained by
    someone else. At the cost of some freedom, you gain assurances that your website
    will automatically scale with the number of users your site has, with no extra
    work on your part. Using PaaS utilities may, however, tend to be more expensive
    than running your own servers.'
  prefs: []
  type: TYPE_NORMAL
- en: Heroku is a PaaS utility that aims to provide ease of use to web developers
    by hooking into already existing tools, and not requiring any large changes in
    the app. Heroku works by reading a file named `Procfile`, which contains commands
    that your Heroku dyno (basically a virtual machine sitting on a server) will run.
    Before we begin, you will need a Heroku account. If you wish to just experiment,
    there is a free account available.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the root of the directory, in a new file named `Procfile`, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This tells Heroku that we have a process named `web`, which will run the uWSGI
    command and pass the `uwsgi.ini` file. Heroku also needs a file named `runtime.txt`,
    which will tell Heroku what Python runtime you wish to use—at the time of writing,
    the latest Python release is 3.7.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Next, make sure that **uwsgi** is present in the `requirements.txt` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we need to make some modifications to the `uwsgi.ini` file that we
    made earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We set the port on which uWSGI listens to the environment variable port, because
    Heroku does not directly expose the dyno to the internet. Instead, it has a very
    complicated load balancer and reverse proxy system, so we need to have uWSGI listening
    on the port that Heroku needs us to listen on. Also, we set die-on-term to true,
    so that uWSGI listens for a signal termination event from the OS correctly.
  prefs: []
  type: TYPE_NORMAL
- en: To work with Heroku's command-line tools, we first need to install them, which
    can be done from [https://toolbelt.heroku.com](https://toolbelt.heroku.com).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you need to log in to your account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can test our setup to make sure that it will work on Heroku before we deploy
    it, by using the `foreman` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `foreman` command simulates the same production environment that Heroku
    uses to run our app. To create the dyno, which will run the application on Heroku''s
    servers, we will use the `create` command. Then, we can push Heroku to the remote
    branch on our Git repository to have Heroku servers automatically pull down our
    changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything went well, you should now have a working application on your
    new Heroku dyno. You can open a new tab to your new web application with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: To see the app in action in a Heroku deployment, visit [https://mastering-flask.herokuapp.com/](https://mastering-flask.herokuapp.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Using Heroku Postgres
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Maintaining a database properly is a full-time job. Thankfully, we can use
    one of Heroku''s built-in features in order to automate this process for us. Heroku
    Postgres offers a database that is maintained and hosted entirely by Heroku. Because
    we are using SQLAlchemy, using Heroku Postgres is trivial. In your dyno''s dashboard,
    there is a link to your Heroku Postgres information. By clicking on it, you will
    be taken to a page similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7d77ee8-61c8-46ff-9354-bea69d15beca.png)'
  prefs: []
  type: TYPE_IMG
- en: By clicking on the URL field, you will be given an SQLAlchemy URL, which you
    can copy directly to your production configuration object.
  prefs: []
  type: TYPE_NORMAL
- en: Using Celery on Heroku
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have our production web server and database set up, but we still need to
    set up Celery. Using one of Heroku''s many plugins, we can host a RabbitMQ instance
    in the cloud, while running the Celery worker on the dyno. The first step is to
    tell Heroku to run your Celery worker in `Procfile`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, to install the Heroku RabbitMQ plugin with the free plan (the `lemur`
    plan), use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: To get the full list of Heroku add-ons, go to [https://elements.heroku.com/addons](https://elements.heroku.com/addons).
  prefs: []
  type: TYPE_NORMAL
- en: 'At the same location on the dashboard where Heroku Postgres was listed, you
    will now find CloudAMQP:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0579d3f-1e34-4b2f-b01d-5b8ebfb9ac06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Clicking on CloudAMQP will also give you a screen with a URL, which you can
    copy and paste into your production configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed113cc9-8bb1-4a24-b6be-0dce512a0284.png)'
  prefs: []
  type: TYPE_IMG
- en: Deploying on Amazon Web Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Amazon Web Services** (**AWS**) is a collection of services maintained by
    Amazon, and built on top of the same infrastructure that runs Amazon.com. To deploy
    our Flask code, we will be using Amazon Elastic Beanstalk in this section, while
    the database will be hosted on Amazon''s **Relational Database Service** (**RDS**),
    and our messaging queue for Celery will be hosted on Amazon''s **Simple Queue
    Service** (**SQS**).'
  prefs: []
  type: TYPE_NORMAL
- en: Using Flask on Amazon Elastic Beanstalk
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elastic Beanstalk is a platform for web applications that offers many powerful
    features for developers, so they don't have to worry about maintaining servers.
    For example, your Elastic Beanstalk application will automatically scale by utilizing
    more and more servers as the number of people using your app at once grows. For
    Python apps, Elastic Beanstalk uses Apache, in combination with `mod_wsgi`, to
    connect to WSGI applications—if your deployment is simple with mid-to-low load,
    there is no extra configuration needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin, you will need an Amazon.com account to log in to the console.
    Next, you need to install **awscli** and configure it with your credentials—you
    must generate an AWS access key and secret: go to the AWS console, choose IAM
    service, choose your user, then choose the Security Credentials tab, and click
    on the Create access key. Next, we need to install awsebcli to manage Elastic
    Beanstalk from the CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, from the root directory of our project, we are going to configure the
    CLI and create a new Elastic Beanstalk application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Elastic Beanstalk looks for a file named `application.py` in your project directory,
    and it expects to find a WSGI application, named `application`, in that file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Next, we are going to create a development environment. Each Elastic Beanstalk
    application can contain one or many environments. But as things currently stand,
    our application will fail—we need to tell Elastic Beanstalk how to install Flask-YouTube
    on Python's virtual environment and initialize the database. To do this, we need
    to extend the default setup.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the root directory, we need a directory named `.ebextensions`. This is where
    we create a lot of extra configuration and setup scripts. In `.ebextensions`,
    we create two shell scripts that will run in the post-deploy phase. So, in the `.ebextensions/10_post_deploy.config` file,
    add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Using YAML notation here, we tell Elastic Beanstalk to create two shell scripts
    to install Flask-YouTube and create or migrate the database. The location of these
    files is special—`/opt/elasticbeanstalk/hooks/appdeploy/post` is where we can
    drop scripts to be executed after deploying. These scripts are executed in alphabetic
    order. Also, take note of the following locations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`/opt/python/current/app`: This is the deploy location of the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/opt/python/current/env`: This is a file containing defined environment variables
    on Elastic Beanstalk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/opt/python/run/venv`: This is python''s `virtualenv`, and is where Elastic
    Beanstalk installed all our defined dependencies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, for our environment creation, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, after the environment has finished provisioning the infrastructure
    and deployment, we can check out our application using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'To deploy new versions of our application, we just have to run this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Note that our development environment uses SQLite, so the database is on a file
    on the web server itself. On each deployment or instance recreation, this database
    is recreated.
  prefs: []
  type: TYPE_NORMAL
- en: Using Amazon RDS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Amazon RDS** is a database-hosting platform in the cloud that automatically
    manages several things, such as recovery on node failure, scheduled backups, and
    master/slave setups.'
  prefs: []
  type: TYPE_NORMAL
- en: To use RDS, go to the Services tab on the AWS console and click on Relational
    Database Service.
  prefs: []
  type: TYPE_NORMAL
- en: Now, create and configure a new database—make sure that on the Publicly accessible option,
    you choose No. Choose the same VPC as the instances, and register your admin credentials
    carefully. Now, wait a few minutes for the instance creation. After that, choose
    your instance, go to the details configuration, and find the field for the **endpoint**—it
    should look something like `myblog.c7pdwgffmbqdm.eu-central-1.rds.amazonaws.com`.
    Our production configuration uses system environment variables to set up the database
    URI, so we have to configure Elastic Beanstalk to set the `DB_URI` environment
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use these environment variables, we need to change our blog''s `config.py` file
    to use the actual OS environment variables, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Make sure your instances can connect to the database. If you chose the security
    group default options and RDS creation, then the wizard will have created a security
    group for you (the default name is '`rds-launch-wizard`'). On EC2, edit this security
    group and open port 3306 to your instances' VPC CIDR.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `.ebextensions`, take a look at the `01_env.config`—this is where we set
    our environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s create the production environment with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Using Celery with Amazon SQS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to use Celery on AWS, we need to have our Elastic Beanstalk instance
    run our Celery worker in the background, as well as set up an SQS messaging queue.
    For Celery to support SQS, it needs to install a helper library from `pip`. Once
    more, verify that our `requirements.txt` file contains the **boto3** package.
    Elastic Beanstalk will look at this file and create a virtual environment from
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting up a new messaging queue on SQS is very easy. Go to the Services tab
    and click on Simple Queue Service in the applications tab, then click on **Create
    New Queue**. After a very short configuration screen, you should see a screen
    much like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f22c7501-612e-4e62-b7cc-0f12df21d56a.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, we have to give our instances access to the newly created SQS. The easiest
    way to do this is editing the Elastic Beanstalk default instance profile (this
    is not recommended, however—you should create a separate instance profile and
    associate all your instances with it using `.ebextensions` option settings). The
    default IAM instance profile is named [aws-elasticbeanstalk-ec2-role](https://console.aws.amazon.com/iam/home#/roles/aws-elasticbeanstalk-ec2-role).
    Go to IAM service, then roles, then choose the [aws-elasticbeanstalk-ec2-role](https://console.aws.amazon.com/iam/home#/roles/aws-elasticbeanstalk-ec2-role) role.
    Next, click on Add inline policy and follow the wizard to give access to the newly
    created SQS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have to change our `CELERY_BROKER_URL` to the new URL, which takes the
    following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Change the `AWS_ACCOUNT_ID` value to your AWS account ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we need to tell Elastic Beanstalk to run a Celery worker in the background.
    Once more, we can do this in `.ebextensions`. Create a file named `11_celery_start.config`,
    and insert the following code into it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Note that this kind of Celery worker deployment lives on the web server (which
    is not recommended), and will also scale along with the web servers in line with
    demand. A better option would be to explore the worker feature from Elastic Beanstalk,
    but this would imply a complete rework of the feature, and we'd suffer from subsequent
    vendor lock-in.
  prefs: []
  type: TYPE_NORMAL
- en: Using Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker is a container-based technology created in 2013 by Docker, Inc. Container
    technology is not new, and has been around for some time on Unix OS, with chroot
    created in 1982, Solaris Zones in 2004, and WPAR available on AIX or OS400 systems
    (although WPAR is more of a virtualization technology than a container). Later,
    two important features were integrated on Linux: **namespaces**, which isolate
    OS function names, and **cgroups**, a collection of processes that are bound by
    configuration and resource limits. These new features gave birth to Linux containers,
    so why use Docker?
  prefs: []
  type: TYPE_NORMAL
- en: Mainly, because Docker made configuration definitions simple. Using a very easy-to-write
    Dockerfile, you can describe how to provision your container and create a new
    image with it. Each Dockerfile line will create a new FS layer using UnionFS,
    which makes changes very quick to apply, and it's equally easy to roll back and
    forward between changes. Also Docker, Inc. created an open image repository, where
    you can find quality images of almost any Linux software available . We have already
    used some of these for Redis and RabbitMQ in [Chapter 9](5672073f-7a18-4865-9800-a2124147042c.xhtml),
    *Creating Asynchronous Tasks with Celery*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker has gained enormous traction and hype. Some of its best features are
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Solving dependency issues from the OS: Since we are packing a thin OS with
    your container image, it is safe to assume that what runs on your laptop will
    run on production as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containers are very light, and users are able to run multiple containers on
    the same VM or hardware host, which can reduce operations costs and increase efficiency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containers bootstrap very quickly, enabling your infrastructure to scale equally
    quickly, if, for example, you needed to address an increase in workload.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developers can easily share their application with other developers using containers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker supports DevOps principles: developers and operations can and should
    work together on the image and architecture definition, using Dockerfile or Docker
    Compose.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we consider the differences in features on offer from Docker containers
    versus VMs, let''s remember that containers share the same kernel and normally
    run a single process, while VMs run a fully featured guest OS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ef2a593-2af8-4fb8-9282-79e7c50ac68a.png)'
  prefs: []
  type: TYPE_IMG
- en: This architecture makes containers very lightweight and quick to spawn.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Docker images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout the previous chapters, our Blog application has grown from a simple
    three-tier architecture to a multi-tier one. We now need to address a web server,
    database, cache system, and queue. We are going to define each of these layers
    as Docker containers.
  prefs: []
  type: TYPE_NORMAL
- en: First, let's begin with our web server and Flask application. For this, we will
    be using an Nginx frontend, and a WSGI, called uWSGI, for the backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Dockerfile is a text file that contains special instructions with which we
    use to specify our Docker image and how it should be run. The build process is
    going to execute the commands one by one, creating a new layer on each one. Some
    of the most used Dockerfile commands include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FROM`**: **Specifies the base image that our new image is based upon. We can
    start from a really thin OS, such as Alpine, or directly from an RabbitMQ image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EXPOSE`:Informs Docker that the container listens on a specified network port/protocol.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ENV`:Sets environment variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WORKDIR`: Establishes the base directory for the Dockerfile.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RUN`:Runs bash Linux commands on a new layer. This is normally used to install
    additional packages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`COPY`:Copies files or directories from local filesystem to the Docker image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CMD`:There can be only one instance of CMD. It specifies how the container
    should be run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ENTRYPOINT`: This has the same objective as CMD, but is a script in Docker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a full reference of Dockerfile commands, check out the documentation at [https://docs.docker.com/engine/reference/builder/#usage](https://docs.docker.com/engine/reference/builder/#usage).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our directory structure for Docker deploy is going to be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The images we are going to create will be used with Docker Compose (more on
    this later in this chapter), so they will not work on a standalone basis. If you
    don't want to use Docker Compose, very few modification are needed for the images
    to work—you just have to change the `prod.env` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create a Dockerfile for our web server. We will use a previous
    image that already contains NGINX and uWSGI, saving us the work to install and
    configure them. Our `Dockerfile_frontend` is the Dockerfile containing the definition
    for creating frontend images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: First, in the preceding snippet, we base our image on `uwsgi-nginx:python3.6`,
    which means we are going to use Python 3.6\. Next, we create and set the directory
    where our application will live—this will be in `/srv/app`. Then we copy all our
    local content (myblog code) to the image itself using the `COPY . .`. Next, we
    copy the configuration file for our WSGI, finally configuring the number of workers
    that NGINX will use. At the end, we inform Docker that this image will be listening
    on port 80, using `EXPOSE 80`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s take a look at our Celery worker Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, our base image is going to be Ubuntu (in particular, a really thin
    Ubuntu version for Docker). We are going to use the **supervisor** Python package
    to monitor and launch our Celery process, so if Celery crashes for some reason,
    supervisor will restart it. So, at the OS level, we are installing the supervisor,
    Python 3, and MySQL client packages. Take a look at the `worker_entrypoint.sh` shell
    script in the preceding code block, where we are doing some interesting things:'
  prefs: []
  type: TYPE_NORMAL
- en: We are waiting for MySQL to become available. When using Docker Compose, we
    can define the order that each task (that is, each Docker container) is launched,
    but we don't have a way to know if the service is already available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we use the Flask CLI and Alembic to create or migrate our database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we insert test data to our database (simply because it's nice to have
    for the readers), so that when you launch the app, it's in a workable state with
    some fake post data already present.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To build and create our images, execute the following Docker commands on the
    shell in the root directory of our project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This will create an image named **myblog **with the tag **latest**. As part
    of production best practices, you should tag your images with your project version,
    also using a **git **tag. This way, we can always be sure what code is in which
    images; for example, what changed between `myblog:1.0` and `myblog:1.1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, create the Celery worker image with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have our custom images created, we are ready to go to the next section,
    where we are going define our of all infrastructure and link the containers to
    each other.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Compose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Docker Compose** is a tool for defining our multi-layer application. This
    is where we define all the services needed to run our application, configure them,
    and link them together.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker Compose is based on YAML files, which is where all the definition happens,
    so let''s dive right into it and take a look at the `deploy/docker/docker-compose.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'In Docker Compose, we have defined the following services:'
  prefs: []
  type: TYPE_NORMAL
- en: '**mysql**: This is based on the Docker Hub community image for MySQL 5.7\.
    All the custom configuration happens with environment variables, as defined in
    the `prod.env` file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rmq**: Rabbit MQ is based on the Docker Hub community image, customized by
    us to create user credentials, cookies, and VHOST. This will install the management
    interface as well, which can be accessed on `http://localhost:15672`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**redis**: This is the Redis service for our cache.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**worker**: This uses our previously built `myblog_worker` Docker image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**frontend**: This uses our previously built `myblog_worker` Docker image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a very simple composer definition. Note `depends_on`, where we define
    which services depend on other services. So, for example, our frontend service
    is going to depend on the database and Rabbit MQ. The `ports` key is a list of
    exposed ports; in this case, the frontend port 80 is going to be exposed by the
    Docker host on port 80 also. This way, we can access our application on the Docker
    host IP port 80, or by using a load balancer in front of the Docker hosts. On
    your machine with Docker already installed, you can access the application on ` http://localhost`.
  prefs: []
  type: TYPE_NORMAL
- en: The use of the `prod.env` file is important, because this way, we can define
    different configurations for different environments and still use the same compose
    file. Using the same compose file across environments obeys another Twelve-Factor
    App rule about making the infrastructure components the same across all environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the `prod.env` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This file environment variables will set actual OS-level environment variables
    so that it's simple to use them on the configuration file for our application.
    This will comply with another of the Twelve-Factor App rules from `https://12factor.net/`.
  prefs: []
  type: TYPE_NORMAL
- en: At the top, we set our application environment for production configuration
    using `WEBAPP_ENV=Prod`.
  prefs: []
  type: TYPE_NORMAL
- en: The `MYSQL_*` variables is where we configure the MySQL 5.7 container. We set
    the root password and an initial database to create (if necessary) a user and
    password for this database.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to note that the `REDIS_HOST` , `DB_URI`, `CELERY_BROKER_URL` variables
    are using the actual host names that each container will use to communicate with
    the other containers. By default, these are the service names, which makes everything
    pretty simple. So, the frontend container accesses the database using the `db` network
    hostname.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s start our application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Wait for all the containers to start up, then open your browser and go to `http://localhost`.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Docker containers on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To deploy on AWS, we are going to use the **Amazon Elastic Container Service**
    (**ECS**). ECS is a service that provides a scalable cluster for Docker, without
    the need to install any software to orchestrate your containers. It's based on
    **AWS Auto Scaling Groups** (**ASG**), which scale instances up or down with Docker
    installed. This scaling is triggered by monitoring metrics, such as CPU usage
    or network load. ECS also migrates all containers from an instance that, for some
    reason, terminates, or gets its service impaired. ECS thus acts as a cluster.
    After this, the ASG will spawn a new instance to replace the faulty one.
  prefs: []
  type: TYPE_NORMAL
- en: CloudFormation Basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS provides many services, each of which has many configuration options. You
    also need to wire these services up. To effectively and reliably create, configure,
    update, or destroy these services, we are going to show you how to use an **IaC**
    (**Infrastructure as code**) technology from AWS, called CloudFormation. **CloudFormation**
    is not a complex technology, but follows the extension of all AWS services and
    configuration options. The details and operation of CloudFormation could be subject
    to a book on its own.
  prefs: []
  type: TYPE_NORMAL
- en: 'CloudFormation is an extended data structure that you write using JSON or YAML.
    I say extended, because it''s possible to use references, functions, and conditions.
    A CloudFormation file is composed of the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a quick look at the provided CloudFormation file in `./deploy/docker/cfn_myblog.yml`.
    We are going to follow all the CloudFormation sections, one be one. First, let''s
    examine the **Parameters** section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Without going into much detail, in this file, an input parameter is defined
    by a name, and may contain a description, a type, a default value, and rules for
    accepted values. All these values will be referenced later when configuring our
    infrastructure. These values are going to be filled when deploying or updating
    the CloudFormation stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, look at the **Mappings** section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This is simply a convenient data structure for mapping AWS regions into AMIs.
    An AMI is a base OS image that we are using for our Docker VMs. Each AMI has a
    different identification in each region, so we need to map them out to make our
    stack deployable on any AWS region. On our case, we will be using Amazon ECS-optimized
    Linux.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s consider the **Metadata** section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are declaring an `Interface` to group our parameters. This is just
    to make the parameters display in a nicer way to whomever is going to deploy the
    stack. Remember that the parameters section is a dictionary, and that dictionary
    keys have no order.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main, and more important section is **Resources**. We are not going to
    go into full detail on this, rather, we''ll just quickly highlight the main infrastructure
    resources we are going to create and how they are wired. First, for the database,
    we are going to use another AWS service, called **RDS**, and create a MySQL server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Each resource has a type. For RDS, this is `AWS::RDS:DBInstance`. Each type
    has its own specific set of properties. Also, notice how `!Ref` declares values
    that are references from other resources or parameters. `DBUsername` and `DBPassword` are
    parameters, but `DBSubnetGroup` and `DBSecurityGroup` are resources created by
    CloudFormation to set up the network ACL and subnet placement for our database.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ECS cluster resource declaration is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: All these definitions belong to the ECS cluster. This cluster can be used to
    provision many different applications, so it would make sense to declare these
    definitions on a separate CloudFormation file, or use nested stacks. To simplify
    the deployment, we will use a single file to create our application. First, we
    create the ECS cluster, and set its name to be a concatenation with the `Environment` and `ApplicationName` parameters. This
    is done using the `!Sub` CloudFormation function.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we declare the **Auto Scaling Group** (**ASG**) for our cluster, and set
    up the way AWS is going to provision each instance that belongs to this ASG. These
    are the `ECSAutoScalingGroup` and `ECSLaunchConfiguration` resources. Finally,
    `ECSRole`, `ECSInstanceProfile`, and `ECSServiceRole` are used to set up the security
    permissions needed for the ECS cluster to fetch Docker images, work with AWS load
    balancers (ELB), S3, and so on. These permissions are the standard used by AWS
    as an example, and can be most certainly be downgraded.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for our application, we are going to define ECS services and ECS task
    definitions. A task definition is where we define one or more container definitions
    that reference the Docker image to use, along with environment variables. Then,
    the ECS service references an ECS task definition, and may tie it up with a load
    balancer and set up deployment configuration options, such as performance limits
    and auto scaling options (yes, the ECS cluster can scale up or down on load shifts,
    but our containers may scale up or down independently as well):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the task definition for our frontend containers. You may notice that
    this is the CloudFormation version of the Docker Compose service that we''ve already
    seen. We declare a name for our container, `Name: "frontend"`, that will later
    be referenced in the load balancers. Next, the image: `!Ref DockerFrontEndImageArn` is
    a reference to an input parameter. This will allow us to easily deploy new versions
    of our blog application. The port mappings for Docker are declared in `PortMappings`.
    This is a list of key values, repeating the keys for `ContainerPort` and `HostPort`.
    The environment is, once again, a list of key values, and here we make the "wiring"
    for DB, RMQ, and Redis from other resources we are creating. For example, here
    is how we use `DB_URI`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: This `Value` is where we construct the URI for the database, using our already
    known `!Sub` function and a reference for `DBUsername` and `DBPassword`. The `DB.Endpoint.Address` is
    how we can reference the DNS name that AWS created for our newly created MySQL
    server.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the service definition, we tie our container to an AWS Elastic Load Balancer,
    and make some deployment configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we declare that this service will run on our newly created ECS cluster,
    using `Cluster: !Ref ECSCluster`. Then, using the `DeploymentConfiguration` and `DesiredCount`,
    we say that this service will start with two containers (for high availability)
    and allow it to scale up and down between 4 and 1\. This obeys the following formulas:'
  prefs: []
  type: TYPE_NORMAL
- en: The maximum number of containers = DesiredCount * (MaximumPercent / 100)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The minimum number of containers = DesiredCount * (MinimumPercent / 100)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, applying the formulas to our case gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 4 = 2 * (200/100)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 = 2 * (50/100)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With `TaskDefinition: !RefFrontEndTask`, we say that this service uses our previous
    frontend task definition. And finally, with the `LoadBalancers` key property,
    we tie our service with a load balancer. This means that our two newly created
    containers will evenly receive requests from the users, and new containers will
    automatically be registered on the load balancer as they are created, as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s look at the load balancer definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This is an AWS classic ELB definition, where we associate the ELB with a network
    security group, which serves more or less like a firewall. This is done with the `SecurityGroups` key
    property. Next, we define in which subnets the ELB is going to serve. Each subnet
    is created in a different AWS availability zone, each of which represent a data
    center in an AWS region (each region contains two or more data centers, or availability
    zones). Then, we define that this ELB is going to be exposed to the internet using `Scheme:
    internet-facing`. For `Listeners`, we say that port 80 of the ELB is mapped to
    port 80 of the Docker host. And finally, we define a health check for the service,
    and the period for which this will occur.'
  prefs: []
  type: TYPE_NORMAL
- en: Check out more details on ELB CloudFormation definitions at [https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-elb.html](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-elb.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'We further create the following resources in the `./deploy/docker/cfn_myblog.yml` YAML
    file provided by CloudFormation:'
  prefs: []
  type: TYPE_NORMAL
- en: Several security groups for ELBs and Docker hosts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task definition and the respective service for our myblog Celery workers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task definition and the respective service for our RabbitMQ container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task definition and the respective service for our Redis container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancer for the Redis container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancer for RabbitMQ
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a load balancer for RabbitMQ is a cheap way to get service discovery functionality—it's
    strange to balance load on a single instance, but if the Docker host, located
    where our RabbitMQ is, crashes for some reason, then the RabbitMQ container is
    going to be created on another Docker host, and the application needs to be able
    to find it dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: Create and update a CloudFormation stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can create and deploy our CloudFormation stack using the console or the
    CLI. To create it using the console, choose the AWS CloudFormation service, and
    then click on the Create Stack button. You will see the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/462ffb4f-b781-49f1-8851-98dfca713ea6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Choose the Upload a template to Amazon S3 option, then choose the `deploy/docker/cfn_myblog.yaml`
    file from the provided code, and click Next. Now, we need to fill the stack parameters
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stack Name: Provide a name to identify this stack; use whatever you want.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Environment: Choose the environment of this stack for production, staging,
    and development.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ApplicationName: Here, use whatever you want to identify the ECS cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'VPC: Choose an AWS VPC.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Subnets: From the drop-down menu, choose all the subnets that belong to the
    VPC (if you have public and private subnets, choose only public subnets, remember
    that the ELB''s are internet facing).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ClusterSize: This is the ECS cluster size; leave the default setting of `2`
    here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'InstanceType: This is the AWS instance type for the Docker hosts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'KeyName: This is the AWS key pair, and needs to be one that we created previously.
    We can use the private key to SSH to the Docker hosts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DockerFrontEndImageArn: This is the ARN of the ECR repository to which we uploaded
    our Docker image for the frontend.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DockerWorkerImageArn: This is the ARN of the ECR repository to which we uploaded
    our Docker image for the worker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DBUsername, DBPassword, RMQUsername, and RMQPassword: These are all the credentials
    for the database and RabbitMQ; choose whatever values you want.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After filing all the parameters, click Next. An Options form is presented—just
    click Next again. A review page is presented with our parameters and possible
    stack changes. Here, we need to check the **I acknowledge that AWS CloudFormation
    might create IAM resources with custom names.** option, and click Create. The
    creation of all the resources is going to take a few minutes—wait for the CREATE_COMPLETED state.
    To check out our application, just go to the Output tab and click on the URL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see how easily we can develop and deploy a code change. First,
    make a simple code change. For example, in the `webapp/templates/head.html` file,
    find the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, change the preceding line to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Then create a new Docker image, and tag it with `v2`, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, push this image to AWS ECR using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Then, go to AWS console and choose our previously created stack. On Actions,
    choose Update Stack. On the first form, choose Use current template. Then, in
    the input parameters, we need to change `DockerFrontEndImageArn`—update it with
    the new tag, and postfix it with `:v2`. The new ARN should look something like
    this: `XXXXXXXX.dkr.ecr.eu-central-1.amazonaws.com/myblog:v2`**. **Then, click
    Next, and on the Options forms click Next again. On the preview form, notice how,
    in the Preview your Changes section, the updater identifies exactly what needs
    to be updated. In this case, `FrontEndTask` and `MyBlogFrontendService` are selected
    for updates, so let's update them. While we wait for the UPDATE_COMPLETE state,
    just keep using the application—notice how no downtime occurs. After one to two
    minutes. notice how our Blog displays the main title as My Blog v2.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see how to integrate this approach with a modern
    CI/CD system to build, run tests, check code quality, and deploy on different
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: Building and deploying highly available applications readily
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whether our web app is on the cloud or in a data center, we should aim for reliability.
    Reliability can impact the user is various ways, either by downtime, data loss,
    application error, response time degradation, or even on user deploy delay. Next,
    we are going to cover some aspects to help you think about architecture and reliability,
    to help you plan ahead to handle issues, such as failures or increased load. First
    of all, we will cover the necessary steps for you to deploy rapidly and, of course,
    reliably.
  prefs: []
  type: TYPE_NORMAL
- en: Building and deploying reliably
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With today's demanding markets, we need to build and deploy easily and quickly.
    But the speed of our deployment must also deliver reliability. One of the steps
    needed to achieve this is to use automation via scripts, or with CI/CD tools.
  prefs: []
  type: TYPE_NORMAL
- en: To help us set up the entire process, we should use a CI/CD tool, such as Jenkins,
    Bamboo, TeamCity, or Travis. First, what exactly is CI/CD?
  prefs: []
  type: TYPE_NORMAL
- en: '**CI** stands for **Continuous Integration**, and is the process defined for
    integrating software changes, made by many developers, into a main repository—and,
    of course, doing so quickly and reliably. Let''s enumerate what we need, from
    bottom to top:'
  prefs: []
  type: TYPE_NORMAL
- en: First, it is imperative to use a source control and versioning system, such
    as Git, along with a well established and internally defined branching model,
    such as **GitFlow**. This will give us a clear view of code changes, along with
    the ability to accept and test them, at either feature or hotfix level. This will
    also make it easy to rollback to a previous version.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before approving any merges proposed by pull requests, make sure to set up automated
    triggering of tests and reviewing of code. Pull-request reviewers can then make
    more informed decisions before approving a merge. Failed tests are certainly a
    warning sign that we want to see before merging code that will end up on production.
    Fail fast, and don't be afraid to fail often.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As was said previously, we have several tools to automate this process. One
    easy way to do this is to use GitHub with Travis and landscape.io. You can freely
    create an account on all three of them and try them out. After this, just create
    the following two files on your repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `.travis.yml` file, which should contain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: This is all we need to have automated tests running on every commit. Also, our
    tests will run independently using Python versions 3.6, 3.3, and 2.7\. GitHub
    and Travis integration will also give us the result of these tests on every pull
    request.
  prefs: []
  type: TYPE_NORMAL
- en: For code quality control, landscape.io is very easy to use with GitHub (other
    tools include flake8, Sonarqube, and Codacy, for example).
  prefs: []
  type: TYPE_NORMAL
- en: 'To set up landscape.io, we just have to create the following `.landscape.yml` file
    at the root of our project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Further automation can be achieved by merging every branch automatically to
    the develop branch, for example, but we need a third tool to automate this process
    on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: '**CD** stands for** Continuous Delivery**,and is based on reduced cycles of
    development and the actual delivery of changes. This must be done quickly and
    reliably, and rollback should always be accounted for. To help us define and execute
    this process, we can use **Jenkins/Blue Ocean pipelines.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Jenkins pipelines, we can define the entire pipeline process, from build
    to deployment. This process is defined using a `Jenkinsfile` at the root of our
    project. First, let''s create and start our Jenkins CI server from the CLI, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'On start, the Docker output will show the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Copy the password from your output and open Jenkins in your browser by going
    to `http://localhost:8080`. On startup, Jenkins will ask for a one-time password—paste
    in the password provided by the Docker output. Next, Jenkins will ask you for
    some initial configuration. This consists of creating an Admin user, and installing
    plugins (for our example, you can simply accept the suggested plugins).
  prefs: []
  type: TYPE_NORMAL
- en: To set up an automated approach to build and deploy our Docker images to AWS
    ECR, we need an extra plugin called Amazon ECR. To install this plugin, go to Manage
    Jenkins, then choose Manage Plugins, and click on the Available Tab for a list
    of available and not-yet-installed plugins. From this list, choose the Amazon
    ECR plugin, and finally click on the Install without restart option.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we must configure a set of credentials, so that Jenkins can authenticate
    on AWS and push our newly built Docker images. For this, on the left-hand menu,
    choose Credentials, then choose Jenkins credential scope and Global credentials.
    Now, on the left-hand panel, choose Add credentials and fill the form with the
    following info:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kind: AWS Credentials'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scope: Global'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ID: ecr-credentials
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Description: ecr-credentials
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Access Key ID: Use the AWS Access Key ID that you already created in the previous
    section for pushing your Docker images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secret Access key:  Use the AWS Secret Access Key that you already created in
    the previous section for pushing your Docker images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For security reasons, it's better to choose the IAM role approach. However,
    for the sake of simplicity, we are using AWS keys here. If you still want to use
    AWS keys, remember to never use your personal keys on automation processes—instead,
    create a specific user for the process with contained and managed privileges.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are ready to create our first CI/CD pipeline. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: On the main page, choose the Create new Jobs link
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the input box for "nter an item name, write `myblog`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the Multibranch pipeline option. Then click Ok
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the Jobs configuration, you need to fill in the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Branch Sources: Create new Jenkins'' credentials for your GitHub account, or
    set up using your own credentials from your private Git repository. Then, choose
    the GitHub repository for this book, or use your private repository URL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, for now, remove all behaviors except "Discover branches", as shown here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/2fd29995-b9d8-4b05-af7d-6d2cdf3e6895.png)'
  prefs: []
  type: TYPE_IMG
- en: On the "Build Configuration" job section, change the "Script Path" to `Chapter-13/Jenkinsfile` if
    you're using this book's GitHub repository. This is required because the repository
    is organised by chapters, and the `Jenkinsfile` is not at the root of the repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is all it takes, because the heavy lifting is done using the `Jenkinsfile`
    pipeline definition. Let''s take a look at this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The Jenkins pipeline definition gives you a huge amount of configuration options.
    We can even use Groovy scripts embedded in it. Please take a look at the documentation
    for more details, available at [https://jenkins.io/doc/book/pipeline/jenkinsfile/](https://jenkins.io/doc/book/pipeline/jenkinsfile/).
  prefs: []
  type: TYPE_NORMAL
- en: On the `pipeline` main section, we have created a manual parameter for you to
    fill out the AWS ECR URL to which the images should be pushed. This section also
    configures some necessary environment variable to make our stages more dynamic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s take a look at the pipeline stages section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: The `stages` section will hold all the stages necessary to build, test, check,
    and deploy our application. The build declared with `stage('Build')` just executes
    a checkout of our repository using `checkout scm`.
  prefs: []
  type: TYPE_NORMAL
- en: In the *Style* stage, we will check the code style using **flake8**. We are
    assuming that a critical style problem is enough to make the pipeline fail, and
    never deploy the application. To run it, we tell Jenkins to run a Docker container
    with Python 3 by using the `docker 'python:3'` command, and inside, we install
    all the necessary dependencies and run **flake8 **against our code.
  prefs: []
  type: TYPE_NORMAL
- en: Next you will find a *Test* stage, which very similar to the St*y*le stage.
    Notice that we can easily define tests for Python 3 and 2.7 using specific Docker
    containers to run it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Docker build stage is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: In this stage, we use Groovy to build our images for the frontend and Celery
    workers. The images will be produced and tagged with the Jenkins build identification,
    which we can use as an `env.BUILD_ID` environment variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the final stage, we push the newly created images to the AWS ECR Docker
    image repository as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Finally, to run our job, choose the "myblog" job, then "master," and on the
    left panel, choose "Build with parameters." Fill in your AWS ECR URL (this URL
    takes the form `http://<ACCOUNT_NUMBER>.dkr.ecr.<REGION>.amazonaws.com`), and
    then click Build. After the build is done, we just have to update our CloudFormation
    with the newly created Docker images.
  prefs: []
  type: TYPE_NORMAL
- en: 'A great final stage would be to update the previously deployed CloudFormation,
    scripting the process with what we''ve already tested in this book, in the previous
    *Create and Update a CloudFormation Stack* section. For this, we could use the
    "pipeline: AWS steps" plugin.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating highly available applications that scale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**High availability** (**HA**) and scalability is an ever more important subject.
    It should be taken into consideration from the development phase, all the way
    up to the release stage. Monolithic architectures, where all the features and
    services that comprise your application can''t be separated or are installed on
    one single instance, will not resist failure, and won''t scale either. Vertical
    scaling will only go so far, and in case of failure, will increase recovery times,
    as well as the impact on the user. This is an important and complex subject and,
    as you may have guessed, there is no single solution to solve it.'
  prefs: []
  type: TYPE_NORMAL
- en: To think about HA, we have to be pessimistic. Remember—failure can't be eliminated,
    but failure points can be identified, and recovery plans should be put in place
    so that downtime takes seconds or minutes, instead of hours or even days.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s think about all the components that our Blog application has,
    and identify the stateless ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Frontend**: Webserver and uWSGI – stateless'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Celery workers**: Celery – stateless'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Message queue**: RabbitMQ or AWS SQS – state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cache**: Redis – state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Database**: SQL or NoSQL – state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our first goal is to identify all the **Single Points of Failure** (**SPOF**)
    in our application, and try to eliminate them. For this, we have to think about
    redundancy:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Frontend**: This is a stateless service that receives direct requests from
    the users. We can balance these requests using a load balancer, and by always
    having at least two instances. If one fails, the other immediately starts receiving
    all the load. Looks good? Maybe, but can a single instance support all the load?
    Huge response times are a failure too, so think about it—maybe you need at least
    three instances. Next, can your load balancer fail too? This is not a problem
    when using some sort of cloud-based load balancer, such as AWS ELB or ALB, but
    if you aren''t using these, then set up redundancy on this layer as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Celery workers**: Workers are stateless, and a complete failure does not
    have an immediate impact on users. You can have at least one instance, as long
    as recovery is done automatically, or failure can be easily identified and a failed
    instance can rapidly be replaced with a new one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Message queue**: If using AWS SQS or CloudMQ, failure is already accounted
    for. If not, a clustered RabbitMQ can be an option, or you can make sure that
    message loss is an option, and that RabbitMQ replacement is automatic, or can
    at least be rapidly executed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cache: **Make sure you have more then one memcached instance (using cluster
    key sharding), or your application can gracefully account for failure. Remember
    that a memcached replacement comes with a cold cache, which can have a huge impact
    on the database, depending on your load.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Database**: Make sure you have an SQL or NoSQL slave/cluster in place, ready
    to replace writes from the failed master.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layers that contain state are more problematic, and a small failure (seconds
    or milliseconds) may be inevitable. Hot standbys or cold standbys should be accounted
    for. It's very useful to test system failures of all your services while load
    testing. Redundancy is like a software feature—if not tested, it's probably broken.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling can be verified with load tests. It's a very good idea to include it
    somewhere along the way in your production pipeline release. **Locust** is an
    excellent Python tool to implement highly configurable load tests that can scale
    to any load level you want. These kinds of tests are a great opportunity to verify
    your high availability setup. Take down instances while simulating your expected
    load, and load test until you break your stack. This way you will know your limits—knowing
    what will break first *before* it breaks on production will help you test performance
    tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Locust python package documentation is available at [https://docs.locust.io/en/stable/](https://docs.locust.io/en/stable/).
  prefs: []
  type: TYPE_NORMAL
- en: Scaling using cloud infrastructure, such as AWS, Azure, and GCP, is all about
    automation. You need to set up your instances automatically, so that monitoring
    metrics can automatically trigger the creation of new VMs or Dockers containers.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, please make sure you backup your database periodically. The delta time
    between backups is a point of possible data loss, so identify it and report back.
    Also, it's very important to restore your production backups—again, if not tested,
    then they're probably broken.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and collecting logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitor all your systems and components, collect OS level metrics, and produce
    application metrics. You have great tools for doing this, including DataDog; NewRelic;
    a combination of StatsD, Graphana, InfluxDB, and Prometheus; and ELK.
  prefs: []
  type: TYPE_NORMAL
- en: Set up alarms on failures based on metric thresholds. It's very important not
    to go overboard on the amount of alarms you create—make sure that a critical alarm
    really implies that the system is down or severely impaired. Set up time charts
    so that you can identify issues or upscale necessities early.
  prefs: []
  type: TYPE_NORMAL
- en: Collect logs from OS, applications, and cloud services. Parsing, structuring,
    and adding metadata to your logs enriches your data, and enables proper log aggregation,
    filtering, and charting. Being able to easily filter all of your logs relative
    to a specific user, IP, or country is a step forward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Log collection has become more critical on the cloudc and even more so on containers,
    because they are short-lived and break your applications down into microservices,
    so that by the time something happens, your logs may no longer exist, or you may
    have to manually go through dozens, if not thousands, of log files to find out
    what was and is happening. This is increasingly becoming impossible to do. There
    are many good solutions out there, however: you can use ELK (ElasticSearch, logstash,
    and Kibana) or EFK (ElasticSearch, Fluentd, and Kibana) stacks, Sumo logic, or
    DataDog.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As this chapter explained, there are many different options for hosting your
    application, each with their own pros and cons. Deciding on one depends on the
    amount of time and money you are willing to spend, as well as the total number
    of users you expect.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have reached the conclusion of the book. I hope that this book was helpful
    in building your understanding of Flask, and how it can be used to create applications
    of any degree of complexity with both ease and simple maintainability.
  prefs: []
  type: TYPE_NORMAL
- en: Web application development is a fast paced area that touches different technologies
    and concepts. Don't stop here—keep improving your Python skills, read about UX
    design, improve your knowledge on CSS and HTML, master SQL and query performance,
    and develop a single page application using Flask and Javascript. Each chapter
    of this book is an invitation for further knowledge.
  prefs: []
  type: TYPE_NORMAL
