- en: Message Passing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will briefly cover the **Message Passing Interface** (**MPI**),
    which is a specification for message exchange. The primary goal of the MPI is
    to establish an efficient, flexible, and portable standard for message exchange
    communication.
  prefs: []
  type: TYPE_NORMAL
- en: Mainly, we will show the functions of the library that include synchronous and
    asynchronous communication primitives, such as (send/receive) and (broadcast/all-to-all),
    the operations of combining the partial results of the calculation (gather/reduce), and
    finally, the synchronization primitives between processes (barriers).
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the control functions of the communication network will be presented
    by defining the topologies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the `mpi4py` Python module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing point-to-point communication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding deadlock problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collective communication using a broadcast
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collective communication using the `scatter` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collective communication using the`gather` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collective communication using `Alltoall`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reduction operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing communication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need the `mpich` and `mpi4py` libraries for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The `mpich` library is a portable implementation of MPI. It is free software
    and is available for various versions of Unix (including Linux and macOS) and
    Microsoft Windows.
  prefs: []
  type: TYPE_NORMAL
- en: To install `mpich`, use the installer downloaded from the downloads page ([http://www.mpich.org/static/downloads/1.4.1p1/](http://www.mpich.org/static/downloads/1.4.1p1/)).
    Moreover, make sure to choose between the 32-bit or 64-bit versions to get the
    right one for your machine.
  prefs: []
  type: TYPE_NORMAL
- en: The `mpi4py` Python module provides Python bindings for the MPI ([https://www.mpi-forum.org](https://www.mpi-forum.org))
    standard. It is implemented on top of the MPI-1/2/3 specification and exposes
    an API that is based on the standard MPI-2 C++ bindings.
  prefs: []
  type: TYPE_NORMAL
- en: 'The installation procedure of `mpi4py` on a Windows machine is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Anaconda users must type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note that for all the examples in this chapter, we used `mpi4py` installed by
    using the `pip` installer
  prefs: []
  type: TYPE_NORMAL
- en: 'This implies that the notation used to run the `mpi4py` examples is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `mpiexec` command is the typical way to start parallel jobs: `x` is the
    total number of processes to use, while `mpi4py_script_name.py` is the name of
    the script to be executed.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the MPI structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The MPI standard defines the primitives for the management of virtual topologies,
    synchronization, and communication between processes. There are several MPI implementations
    that differ in the version and features of the standard supported.
  prefs: []
  type: TYPE_NORMAL
- en: We will introduce the MPI standard through the Python `mpi4py` library.
  prefs: []
  type: TYPE_NORMAL
- en: Before the 1990s, writing parallel applications for different architectures
    was a more difficult job than what it is today. Many libraries facilitated the
    process, but there was not a standard way to do it. At that time, most parallel
    applications were destined for scientific research environments.
  prefs: []
  type: TYPE_NORMAL
- en: The model that was most commonly adopted by the various libraries was the message-passing
    model, in which the communication between the processes takes place through the
    exchange of messages and without the use of shared resources. For example, the
    master process can assign a job to the slaves simply by sending a message that
    describes the work to be done. A second, very simple, example here is a parallel
    application that performs a merge sort. The data is sorted locally to the processes
    and the results are passed to other processes that will deal with the merge.
  prefs: []
  type: TYPE_NORMAL
- en: Since the libraries largely used the same model, albeit with minor differences
    from each other, the authors of the various libraries met in 1992 to define a
    standard interface for the exchange of messages, and, from here, MPI was born.
    This interface had to allow programmers to write portable parallel applications
    on most parallel architectures, using the same features and models they were already
    used to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Originally, MPI was designed for distributed memory architectures, which began
    to grow in popularity 20 years ago:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5eade1c-0ee1-4194-a00c-d8686149c550.png)'
  prefs: []
  type: TYPE_IMG
- en: The distributed memory architecture schema
  prefs: []
  type: TYPE_NORMAL
- en: 'Over time, distributed memory systems began to be combined with each other,
    creating hybrid systems with distributed/shared memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ac1b93c-4e24-4612-b1d7-1935c8f0a661.png)'
  prefs: []
  type: TYPE_IMG
- en: The hybrid system architecture schema
  prefs: []
  type: TYPE_NORMAL
- en: Today, MPI runs on distributed memory, shared memory, and hybrid systems. However,
    the programming model remains that of distributed memory, although the true architecture
    on which the calculation is performed may be different.
  prefs: []
  type: TYPE_NORMAL
- en: 'The strengths of MPI can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Standardization**: It is supported by all **High-Performance** **Computing**
    (**HPC**) platforms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Portability**: The changes applied to the source code are minimal, which
    is useful if you decide to use the application on a different platform that also
    supports the same standard.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance**: Manufacturers can create implementations optimized for a specific
    type of hardware and get better performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Functionality**: Over 440 routines are defined in MPI-3, but many parallel
    programs can be written using fewer than even 10 routines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following sections, we will examine the main Python library for message
    passing: the `mpi4py` library.'
  prefs: []
  type: TYPE_NORMAL
- en: Using the mpi4py Python module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Python programming language provides several MPI modules to write parallel
    programs. The most interesting of these is the `mpi4py` library. It is constructed
    on top of the MPI-1/2 specifications and provides an object-oriented interface,
    which closely follows the MPI-2 C++ bindings. A C MPI user could use this module
    without learning a new interface. Therefore, it is widely used as an almost-full
    package of an MPI library in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main applications of the module, which will be described in this chapter,
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Point-to-point communication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collective communication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Topologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start our journey to the MPI library by examining the classic code of
    a program that prints the phrase `Hello, world!` on each process that is instantiated:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `mpi4py` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In MPI, the processes involved in the execution of a parallel program are identified by
    a sequence of non-negative integers called **ranks**.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have a number (*p* of processes) that runs a program, then the processes
    will have a `rank` that goes from *0* to *p*-1\. In particular, in order to assess
    the rank of each process, we must use the `COMM_WORLD` MPI function in particular.
    This function is called a **communicator**, as it defines its own set of all processes
    that can communicate together:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the following `Get_rank()` function returns `rank` of the process
    calling it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once evaluated, `rank` is printed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: According to the MPI execution model, our application consists of *N* (5 in
    this example) autonomous processes, each with their own local memory able to communicate
    data through the exchange of messages.
  prefs: []
  type: TYPE_NORMAL
- en: The communicator defines a group of processes that can communicate with each
    other. The `MPI_COMM_WORLD` work used here is the default communicator and includes
    all processes.
  prefs: []
  type: TYPE_NORMAL
- en: The identification of a process is based on ranks. Each process is assigned
    a rank for each communicator to which it belongs. The rank is an integer that
    is assigned, which starts from zero and identifies each individual process in
    the context of a specific communicator. The common practice is to define the process
    with a global rank of *0* as the master process. Through the rank, the developer
    can specify what the sending process is and what the recipient processes are instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'It should be noted that, for illustration purposes only, the `stdout` output
    will not always be ordered, as multiple processes can apply at the same time by
    writing on the screen and the OS arbitrarily chooses the order. So, we are ready
    for a fundamental observation: every process involved in the execution of MPI
    runs the same compiled binary, so each process receives the same instructions
    to be executed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To execute the code, type the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the result that we will get after executing this code (notice how the
    order of execution of the processes *is not sequential*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: It should be noted that the number of processes to be used is strictly dependent
    on the characteristics of the machine on which the program must run.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MPI belongs to the **Single Program Multiple Data** (**SPMD**) programming technique.
  prefs: []
  type: TYPE_NORMAL
- en: SPMD is a programming technique in which all processes execute the same program,
    each on different data. The distinction in executions between different processes
    occurs by differentiating the flow of the program, based on the local rank of
    the process.
  prefs: []
  type: TYPE_NORMAL
- en: SPMD is a programming technique in which a single program is executed by several
    processes at the same time, but each process can operate on different data. At
    the same time, the processes can execute both the same instruction and different
    instructions. Obviously, the program will contain appropriate instructions that
    allow the execution of only parts of the code and/or to operate on a subset of
    the data. This can be implemented using different programming models, and all
    executables start at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The complete reference to the `mpi4py` library can be found at [https://mpi4py.readthedocs.io/en/stable/](https://mpi4py.readthedocs.io/en/stable/).
  prefs: []
  type: TYPE_NORMAL
- en: Implementing point-to-point communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Point-to-point operations consist of the exchange of messages between two processes.
    In a perfect world, every sending operation would be perfectly synchronized with
    the respective reception operation. Obviously, this is not the case, and the MPI
    implementation must be able to preserve the data sent when the sender and recipient
    processes are not synchronized. Typically, this occurs using a buffer, which is
    transparent to the developer and entirely managed by the `mpi4py` library.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `mpi4py` Python module enables point-to-point communication via two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Comm.Send(data, process_destination)`: This function sends data to the destination
    process identified by its rank in the communicator group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Comm.Recv(process_source)`: This function receives data from the sourcing process,
    which is also identified by its rank in the communicator group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Comm` parameter, which is short for *communicator*, defines the group of
    processes that may communicate through message passing using `comm = MPI.COMM_WORLD`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following example, we will utilize the `comm.send` and `comm.recv` directives
    to exchange messages between different processes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant `mpi4py` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define the communicator parameter, namely `comm`, through the `MPI.COMM_WORLD`
    statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `rank` parameter is used to identify the process itself:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'It is useful to print out the `rank` of a process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we start considering the rank of the process. In this case, for the process
    of `rank` equal to `0`, we set `destination_process` and `data` (in this case
    `data = 10000000`) to be sent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, by using the `comm.send` statement, the data that was previously set
    is sent to the destination process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'For the process of `rank` equal to `1`, the `destination_process` value is
    `8`, while the data to be sent is the `"hello"` string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The process of `rank` equal to `4` is a receiver process. Indeed, the source
    process (that is, the process of `rank` equal to `0`) is set as a parameter in
    the `comm.recv` statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, using the following code, the data received from the process of `0` must
    be displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The last process to be set is number `9`. Here, we define the source process
    of `rank` equal to `1` as a parameter in the `comm.recv` statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `data1` value is then printed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We ran the example with a total number of processes equal to `9`. So, in the `comm` communicator
    group, we have nine tasks that can communicate with each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, to identify a task or processes inside the group, we use their `rank` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We have two sender processes and two receiver processes. The process of `rank`
    equal to `0` sends numerical data to the receiver process of `rank` equal to `4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we must specify the receiver process of `rank` equal to `4`. We
    also note that the `comm.recv` statement must contain, as an argument, the rank
    of the sender process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: For the other sender and receiver processes (the process of `rank` equal to
    `1` and the process of `rank` equal to `8`, respectively), the situation is the
    same, the only difference being the type of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, for the sender process, we have a string that is to be sent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'For the receiver process of `rank` equal to `8`, the rank of the sender process
    is pointed out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram summarizes the point-to-point communication protocol
    in `mpi4py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c92bb67f-1f34-4624-9dd7-9907f38c32e1.png)'
  prefs: []
  type: TYPE_IMG
- en: The send/receive transmission protocol
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, it describes a two-step process, consisting of sending some **DATA**from
    one task (*sender*) and another task (*receiver*) receiving this data. The sending
    task must specify the data to be sent and its destination (the *receiver *process), while
    the receiving task has to specify the source of the message to be received.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the script, we shall use `9` processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output that you''ll get after you run the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `comm.send()` and `comm.recv()` functions are blocking functions, which
    means that they block the caller until the buffered data involved can be used safely.
    Also, in MPI, there are two management methods of sending and receiving messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Buffered mode**: The flow control returns to the program as soon as the data
    to be sent has been copied to a buffer. This does not mean that the message is
    sent or received.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Synchronous mode**: The function only gets terminated when the corresponding
    `receive` function begins receiving the message.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An interesting tutorial on this topic can be found at [https://github.com/antolonappan/MPI_tutorial](https://github.com/antolonappan/MPI_tutorial).
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding deadlock problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common problem we face is deadlock. This is a situation where two (or more)
    processes block each other and wait for the other to perform a certain action
    that serves another and vice versa. The `mpi4py` module doesn't provide any specific
    functionality to resolve the deadlock problem, but there are some measures that
    the developer must follow in order to avoid the problem of deadlock.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s first analyze the following Python code, which will introduce a typical
    deadlock problem. We have two processes—`rank` equal to `1` and `rank` equal to
    `5`—that communicate with each other and both have the data sender and data receiver
    functionalities:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `mpi4py` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the communicator as `comm` and the `rank` parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The process of `rank` equal to `1` sends and receives data from the process of
    `rank` equal to `5`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In the same way, here, we define the process of `rank` equal to `5`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The destination and sender processes are equal to `1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we try to run this program (it makes sense to execute it with only two processes),
    then we note that none of the two processes can proceed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Both the processes prepare to receive a message from the other and get stuck
    there. This happens because of the `comm.recv()` MPI function and the `comm.send()` MPI
    blocking them. This means that the calling process awaits their completion. As
    for the `comm.send()` MPI, the completion occurs when the data has been sent and
    may be overwritten without modifying the message.
  prefs: []
  type: TYPE_NORMAL
- en: 'The completion of the `comm.recv()` MPI instead occurs when the data has been
    received and can be used. To solve this problem, the first idea is to invert the
    `comm.recv()` MPI with the `comm.send()` MPI, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This solution, even if correct, does not guarantee that we will avoid deadlock.
    In fact, communication is performed through a buffer with the instruction of `comm.send()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'MPI copies the data to be sent. This mode works without problems, but only
    if the buffer is able to keep them all. If this does not happen, then there is
    a deadlock: the sender cannot finish sending the data because the buffer is busy,
    and the receiver cannot receive data because it is blocked by the `comm.send()` MPI
    call, which has not yet completed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, the solution that allows us to avoid deadlocks is used to swap
    the sending and receiving functions so as to make them asymmetrical:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we get the correct output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The solution proposed to the deadlock is not the only solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is, for example, a function that unifies the single call that sends a
    message to a given process and receives another message that comes from another
    process. This function is called `Sendrecv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the required parameters are the same as the `comm.send()`  and `comm.recv()` MPI
    (in this case, also the function blocks). However, `Sendrecv` offers the advantage
    of leaving the communication subsystem responsible for checking the dependencies
    between sending and receiving, thus avoiding the deadlock.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this way, the code of the previous example becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An interesting analysis of how parallel programming is difficult due to deadlock
    management can be found at [https://codewithoutrules.com/2017/08/16/concurrency-python/](https://codewithoutrules.com/2017/08/16/concurrency-python/).
  prefs: []
  type: TYPE_NORMAL
- en: Collective communication using a broadcast
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During the development of parallel code, we often find ourselves in a situation
    where we must share, between multiple processes, the value of a certain variable
    at runtime or certain operations on variables that each process provides (presumably
    with different values).
  prefs: []
  type: TYPE_NORMAL
- en: To resolve these types of situations, communication trees are used (for example,
    process 0 sends data to the processes 1 and 2, which will, respectively, take
    care of sending them to processes 3, 4, 5, 6, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, MPI libraries provide functions that are ideal for the exchange of
    information or the use of multiple processes that are clearly optimized for the
    machine in which they are performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48fa28e4-27d9-4ee6-981e-3c72d22b1c27.png)'
  prefs: []
  type: TYPE_IMG
- en: Broadcasting data from process 0 to processes 1, 2, 3, and 4
  prefs: []
  type: TYPE_NORMAL
- en: A communication method that involves all the processes that belong to a communicator
    is called a collective communication. Consequently, collective communication generally
    involves more than two processes. However, instead of this, we will call the collective
    communication broadcast, wherein a single process sends the same data to any other
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `mpi4py` broadcast functionalities are offered by the following method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This function sends the information contained in the message process root to
    every other process that belongs to the `comm` communicator.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now see an example in which we''ve used the `broadcast` function. We
    have a root process of `rank` equal to `0` that shares its own data, `variable_to_share`,
    with the other processes defined in the communicator group:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s import the `mpi4py` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s define the communicator and the `rank` parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'As far as the process of `rank` equal to `0` is concerned, we define the variable
    to be shared among the other processes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we define a broadcast, having the `rank` process equal to zero as
    its `root`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The root process of `rank` equal to `0` instantiates a variable, `variable_to_share`,
    which is equal to `100`. This variable will be shared with the other processes
    of the communication group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'To perform this, we also introduce the broadcast communication statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the parameters in the function are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The data to be shared (`variable_to_share`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The root process, that is, the process of rank equal to 0 (`root=0`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Running the code, we have a communication group of 10 processes, and `variable_to_share` is
    shared between the other processes in the group. Finally, the `print` statement
    visualizes the rank of the running process and the value of its variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'After setting `10` processes, the output obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Collective communication allows simultaneous data transmission between multiple
    processes in a group. The `mpi4py` library provides collective communications,
    but only in the blocking version (that is, it blocks the caller method until the
    buffered data involved can safely be used).
  prefs: []
  type: TYPE_NORMAL
- en: 'The most commonly used collective communication operations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Barrier synchronization across the group's processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Communication functions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Broadcasting data from one process to all processes in the group
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Gathering data from all processes to one process
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Scattering data from one process to all processes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduction operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to this link ([https://nyu-cds.github.io/python-mpi/](https://nyu-cds.github.io/python-mpi/))
    to find a complete introduction to Python and MPI.
  prefs: []
  type: TYPE_NORMAL
- en: Collective communication using the scatter function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The scatter functionality is very similar to a scatter broadcast, but with
    one major difference: while `comm.bcast` sends the same data to all listening
    processes, `comm.scatter` can send chunks of data in an array to different processes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the scatter functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a6e8b54-06df-43d5-ad30-aa1221ae3f85.png)'
  prefs: []
  type: TYPE_IMG
- en: Scattering data from process 0 to processes 1, 2, 3, and 4
  prefs: []
  type: TYPE_NORMAL
- en: 'The **`comm.scatter`** function takes the elements of the array and distributes
    them to the processes according to their rank, for which the first element will
    be sent to process 0, the second element to process 1, and so on. The function
    implemented in **`mpi4py`** is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following example, we''ll see how to distribute data to different processes
    using the `scatter` functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `mpi4py` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the `comm` and `rank` parameters in the usual way:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'For the process of `rank` equal to `0`, the following array will be scattered:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, `recvbuf` is set. The `root` process is the process of `rank` equal to `0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The process of `rank` equal to `0` distributes the `array_to_share` data structure
    to other processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The `recvbuf` parameter indicates the value of the *i^(th)* variable that will
    be sent to the process through the `comm.scatter` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We also remark that one of the restrictions to `comm.scatter` is that you can
    scatter as many elements as the processors you specify in the execution statement.
    In fact, if you attempt to scatter more elements than the processors specified
    (three, in this example), then you will get an error similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `mpi4py` library provides two other functions that are used to scatter
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '`comm.scatter(sendbuf, recvbuf, root=0)`: This function sends data from one
    process to all other processes in a communicator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`comm.scatterv(sendbuf, recvbuf, root=0)`: This function scatters data from
    one process to all other processes in a given group that provide a different amount
    of data and displacements at the sending side.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `sendbuf` and `recvbuf` arguments must be given in terms of a list (as
    in the `comm.send` point-to-point function):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Here, `data` must be a buffer-like object of the `data_size` size and of the `data_type` type.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An interesting tutorial on MPI broadcasting is presented at [https://pythonprogramming.net/mpi-broadcast-tutorial-mpi4py/](https://pythonprogramming.net/mpi-broadcast-tutorial-mpi4py/).
  prefs: []
  type: TYPE_NORMAL
- en: Collective communication using the gather function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `gather` function performs the inverse of the `scatter` function. In this
    case, all processes send data to a root process that collects the data received.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `gather` function, which is implemented in `mpi4py`, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `sendbuf` is the data that is sent, and `rank_of_root_process` represents
    the processing of the receiver of all the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3fc357c4-5541-4c15-94f4-2dd7bee64b9b.png)'
  prefs: []
  type: TYPE_IMG
- en: Gathering data from processes 1, 2, 3, and 4
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following example, we''ll represent the condition shown in the preceding
    diagram, in which each process builds its own data, which is to be sent to the
    root processes that are identified with the `rank` zero:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Type the necessary import:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the following three parameters. The `comm` parameter is the
    communicator, `rank` provides the rank of the process, and `size` is the total
    number of processes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we define the data to be gathered from the process of `rank` zero:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the gathering is provided through the `comm.gather` function. Also,
    note that the root process (the process that will gather the data from the other
    ones) is the zero rank process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'For the `rank` equal to the `0` process, the data gathered and the sending
    process are printed out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The root process of `0` receives data from the other four processes, as represented
    in the previous diagram.
  prefs: []
  type: TYPE_NORMAL
- en: 'We set *n (= 5)* processes sending their data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'If the `rank` of the process is `0`, then the data is collected in an array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The gathering of data is given, instead, by the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we run the code setting the group of processes equal to `5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To collect data, `mpi4py` provides the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: Gathering to one task*:* `comm.Gather`, `comm.Gatherv`, and `comm.gather`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gathering to all tasks: `comm.Allgather`, `comm.Allgatherv`, and `comm.allgather`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More information on `mpi4py` can be found at [http://www.ceci-hpc.be/assets/training/mpi4py.pdf](http://www.ceci-hpc.be/assets/training/mpi4py.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Collective communication using Alltoall
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `Alltoall` collective communication combines the `scatter` and `gather`
    functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following example, we''ll see an `mpi4py` implementation of `comm.Alltoall`.
    We''ll consider a communicator a group of processes, where each process sends
    and receives an array of numerical data from the other processes defined in the
    group:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, the relevant `mpi4py` and `numpy` libraries must be imported:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'As in the previous example, we need to set the same parameters, `comm`, `size`,
    and `rank`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Hence, we must define the data that each process will send (`senddata`) and,
    at the same time, receive (`recvdata`) from the other processes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the `Alltoall` function is executed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The data that is sent and received for each process is displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `comm.alltoall` method takes the *i^(th)* object from the `sendbuf` argument
    of task `j` and copies it into the *j^(th)* object of the `recvbuf` argument of
    task `i`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run the code with a communicator group of `5` processes, then our output
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'We could also figure out what happened by using the following schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae2c13f2-c674-4f5e-a05b-bf8982a010bd.png)'
  prefs: []
  type: TYPE_IMG
- en: The Alltoall collective communication
  prefs: []
  type: TYPE_NORMAL
- en: 'Our observations regarding the schema are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The *P0* process contains the [**0 1 2 3 4**]  data array, where it assigns
    0 to itself, 1 to the *P1 *process, 2 to the *P2 *process, 3 to the *P3 *process,
    and 4 to the *P4 *process;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *P1* process contains the [**0 2 4 6 8**] data array, where it assigns 0
    to the *P0 *process, 2 to itself, 4 to the *P2 *process, 6 to the *P3 *process,
    and 8 to the *P4 *process;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *P2* process contains the [**0 3 6 9 12**] data array, where it assigns
    0 to the *P0 *process, 3 to the *P1 *process, 6 to itself, 9 to the *P3 *process, and
    12 to the *P4 *process;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *P3* process contains the [**0 4 8 12 16**] data array, where it assigns
    0 to the *P0 *process, 4 to the *P1 *process, 8 to the *P2* process, 12 to itself,
    and 16 to the *P4 *process;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *P4* process contains the [**0 5 10 15 20**] data array, where it assigns
    0 to the *P0* process, 5 to the *P1 *process, 10 to the *P2 *process, 15 to the
    *P3 *process, and 20 to itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Alltoall` personalized communication is also known as a total exchange. This
    operation is used in a variety of parallel algorithms, such as the fast Fourier
    transform, matrix transpose, sample sort, and some parallel database join operations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In `mpi4py`, there are *three types* of `Alltoall` collective communication:'
  prefs: []
  type: TYPE_NORMAL
- en: '`comm.Alltoall(sendbuf, recvbuf)`: The `Alltoall` scatter/gather sends data
    from all-to-all processes in a group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`comm.Alltoallv(sendbuf, recvbuf)`: The `Alltoall` scatter/gather vector sends
    data from all-to-all processes in a group, providing a different amount of data
    and displacements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`comm.Alltoallw(sendbuf, recvbuf)`: Generalized `Alltoall` communication allows
    different counts, displacements, and datatypes for each partner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An interesting analysis of MPI Python modules can be downloaded from [https://www.duo.uio.no/bitstream/handle/10852/10848/WenjingLinThesis.pdf](https://www.duo.uio.no/bitstream/handle/10852/10848/WenjingLinThesis.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: The reduction operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to `comm.gather`, `comm.reduce` takes an array of input elements in
    each process and returns an array of output elements to the root process. The
    output elements contain the reduced result.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In `mpi4py`, we define the reduction operation through the following statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: We must note that the difference with the `comm.gather` statement resides in
    the `op` parameter, which is the operation that you wish to apply to your data,
    and the `mpi4py` module contains a set of reduction operations that can be used.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we'll see how to implement the sum of an array of elements with the `MPI.SUM` reduction
    operation by using the reduction functionality. Each process will manipulate an
    array of size 10.
  prefs: []
  type: TYPE_NORMAL
- en: 'For array manipulation, we use the functions provided by the `numpy` Python
    module:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the relevant libraries, `mpi4py` and `numpy`, are imported:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `comm`, `size`, and `rank` parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the size of the array (`array_size`) is set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'The data to be sent and received is defined:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The process sender and the sent data are printed out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the `Reduce` operation is executed. Note that the `root` process is set
    to `0` and the `op` parameter is set to `MPI.SUM`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the reduction operation is then shown, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To perform the reduction sum, we use the `comm.Reduce` statement. Also, we
    identify with `rank` zero, which is the `root` process that will contain `recvbuf`,
    which represents the final result of the computation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: It makes sense to run the code with a communicator group of `10` processes,
    as this is the size of the manipulated array.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Note that with the `op=MPI.SUM` option, we apply the sum operation to all the
    elements of the column array. To better understand how the reduction operates,
    let''s look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa28bcc1-08cf-4699-b559-9f4f1e6e948c.png)'
  prefs: []
  type: TYPE_IMG
- en: Reduction in collective communication
  prefs: []
  type: TYPE_NORMAL
- en: 'The sending operation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The **P0** process sends the [**0 1 2**] data array.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **P1** process sends the [**0 2 4**] data array.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **P2** process sends the [**0 3 6**] data array.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reduction operation sums the *i^(th)* elements of each task and then puts
    the result in the *i^(th)* element of the array in the **P0** root process. For
    the receiving operation, the **P0** process receives the [**0 6 12**] data array.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the reduction operations defined by MPI are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MPI.MAX`: This returns the maximum element.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MPI.MIN`: This returns the minimum element.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MPI.SUM`: This sums up the elements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MPI.PROD`: This multiplies all elements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MPI.LAND`: This performs the AND logical operation across the elements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MPI.MAXLOC`: This returns the maximum value and the rank of the process that
    owns it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MPI.MINLOC`: This returns the minimum value and the rank of the process that
    owns it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At [http://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/](http://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/),
    you can find a good tutorial on this topic and much more.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An interesting feature that is provided by MPI regards virtual topologies. As
    already noted, all the communication functions (point-to-point or collective)
    refer to a group of processes. We have always used the `MPI_COMM_WORLD` group
    that includes all processes. It assigns a rank of *0* to *n-1* for each process
    that belongs to a communicator of the size *n*.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, MPI allows us to assign a virtual topology to a communicator. It defines
    an assignment of labels to the different processes: by building a virtual topology,
    each node will communicate only with its virtual neighbor, improving performance
    because it reduces execution times.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the rank was randomly assigned, then a message could be forced
    to pass to many other nodes before it reaches the destination. Beyond the question
    of performance, a virtual topology makes sure that the code is clearer and more
    readable.
  prefs: []
  type: TYPE_NORMAL
- en: MPI provides two building topologies. The first construct creates Cartesian
    topologies, while the latter creates any kind of topologies. Specifically, in
    the second case, we must supply the adjacency matrix of the graph that you want
    to build. We will only deal with Cartesian topologies, through which it is possible
    to build several structures that are widely used, such as mesh, ring, and toroid.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `mpi4py` function used to create a Cartesian topology is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: Here, `number_of_rows` and `number_of_columns` specify the rows and columns
    of the grid that is to be made.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following example, we see how to implement a Cartesian topology of the
    size *M×N*. Also, we define a set of coordinates to understand how all the processes
    are disposed of:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the relevant libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the following parameter in order to move along the topology:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'For each process, the following array defines the neighbor processes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `main` program, the `comm.rank` and `size` parameters are then defined:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s build the topology:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'The following conditions ensure that the processes are always within the topology:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'The `rank` equal to `0` process starts the topology construction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For each process, the output should read as follows: if `neighbour_processes
    = -1`, then it has no topological proximity, otherwise, `neighbour_processes`
    shows the rank of the process closely.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting topology is a mesh of *2*×*2* (refer to the previous diagram
    for a mesh representation), the size of which is equal to the number of processes
    in the input; that is, four:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the Cartesian topology is built using the `comm.Create_cart` function
    (note also the parameter, `periods = (False,False)`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'To know the position of the process, we use the `Get_coords()` method in the
    following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'For the processes, in addition to getting their coordinates, we must calculate
    and find out which processes are topologically closer. For this purpose, we use
    the `comm.Shift (rank_source,rank_dest)` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'The topology obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f716b5dc-9c4c-4e31-9fc3-b128b439f010.png)'
  prefs: []
  type: TYPE_IMG
- en: The virtual mesh 2x2 topology
  prefs: []
  type: TYPE_NORMAL
- en: As the diagram shows, the *P0* process is chained to the **P1** `(RIGHT)` and
    **P2** `(DOWN)` processes. The **P1** process is chained to the **P3** `(DOWN)`
    and **P0** `(LEFT)` processes, the **P3** process is chained to the **P1** `(UP)`
    and **P2** `(LEFT)` processes, and the **P2** process is chained to the **P3**
    `(RIGHT)` and **P0** `(UP)` processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, by running the script, we obtain the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To obtain a toroidal topology of the size *M*×*N*, let''s use `comm.Create_cart`
    again, but, this time, let''s set the `periods` parameter to `periods=(True,True)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output is obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'The output covers the topology represented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0de01d9b-fe04-43f0-9700-68e7955534b8.png)'
  prefs: []
  type: TYPE_IMG
- en: The virtual toroidal 2x2 topology
  prefs: []
  type: TYPE_NORMAL
- en: The topology represented in the previous diagram indicates that the **P0** process
    is chained to the **P1** (`RIGHT` and `LEFT`) and **P2** (`UP` and `DOWN`) processes,
    the **P1** process is chained to the **P3** (`UP` and `DOWN`) and **P0** (`RIGHT`
    and `LEFT`) processes, the **P3** process is chained to the **P1** (`UP` and `DOWN`)
    and **P2** (`RIGHT` and `LEFT`) processes, and the **P2** process is chained to the
    **P3** (`LEFT` and `RIGHT`) and **P0** (`UP` and `DOWN`) processes.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More information on MPI can be found at [http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-topo.html](http://pages.tacc.utexas.edu/~eijkhout/pcse/html/mpi-topo.html).
  prefs: []
  type: TYPE_NORMAL
