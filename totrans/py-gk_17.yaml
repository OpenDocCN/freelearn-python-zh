- en: '*Chapter 13*: Python and Machine Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Machine learning** (**ML**) is a branch of **artificial intelligence** (**AI**)
    that is based on building models by learning patterns from data and then using
    those models to make predictions. It is one of the most popular AI techniques
    for helping humans as well as businesses in many ways. For example, it is being
    used for medical diagnosis, image processing, speech recognition, predicting threats,
    data mining, classification, and many more scenarios. We all understand the importance
    and usefulness of machine learning in our lives. Python, being a concise but powerful
    language, is being used extensively to implement machine learning models. Python''s
    ability to process and prepare data using libraries such as NumPy, pandas, and
    PySpark makes it a preferred choice for developers for building and training ML
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss using Python for machine learning tasks in
    an optimized way. This is especially important because training an ML model is
    a compute-intensive task and optimizing the code is fundamental when using Python
    for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Python for machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing and evaluating machine learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying machine learning models in the cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After completing this chapter, you will understand how to use Python to build,
    train, and evaluate machine learning models and how to deploy them in the cloud
    and use them to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the technical requirements for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: You need to have Python 3.7 or later installed on your computer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need to install additional libraries for machine learning such as SciPy,
    NumPy, pandas, and scikit-learn.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To deploy an ML model on GCP's AI Platform, you will need a GCP account (a free
    trial will work fine).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sample code for this chapter can be found at [https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter13](https://github.com/PacktPublishing/Python-for-Geeks/tree/master/Chapter13).
  prefs: []
  type: TYPE_NORMAL
- en: We will start our discussion with an introduction to machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In traditional programming, we provide data and some rules as input to our
    program to get the desired output. Machine learning is a fundamentally different
    programming approach, in which the data and the expected output are provided as
    input to produce a set of rules. This is called a **model** in machine learning
    nomenclature. This concept is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.1 – Traditional programming versus machine learning programming'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_13_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.1 – Traditional programming versus machine learning programming
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how machine learning works, we need to familiarize ourselves
    with its core components or elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset**: Without a good set of data, machine learning is nothing. Good
    data is the real power of machine learning. It has to be collected from different
    environments and cover various situations to represent a model close to a real-world
    process or system. Another requirement for data is that it has to be large, and
    by large we mean thousands of records. Moreover, the data should be as accurate
    as possible and have meaningful information in it. Data is used to train the system
    and also to evaluate its accuracy. We can collect data from many sources but most
    of the time, it is in a raw format. We can use data processing techniques by utilizing
    libraries such as pandas, as we discussed in the previous chapters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature extraction**: Before using any data to build a model, we need to
    understand what type of data we have and how it is structured. Once we have understood
    that, we can select what features of the data can be used by an ML algorithm to
    build a model. We can also compute additional features based on the original feature
    set. For example, if we have raw image data in the form of pixels, which itself
    may not be useful for training a model, we can use the length or breadth of the
    shape inside an image as features to build rules for our model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithm**: This is a program that is used to build an ML model from the
    available data. In mathematical terms, a machine learning algorithm tries to learn
    a target function *f(X)* that can map the input data, *X*, to an output, *y*,
    like so:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17189_13_001.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: There are several algorithms available for different types of problems and situations
    because there is not a single algorithm that can solve every problem. A few popular
    algorithms are **linear regression**, **classification and regression trees**,
    and **support vector classifier** (**SVC**). The mathematical details of how these
    algorithms work are beyond the scope of this book. We recommend checking the additional
    links provided in the *Further reading* section for details regarding these algorithms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Models**: Often, we hear the term model in machine learning. A model is a
    mathematical or computational representation of a process that is happening in
    our day-to-day life. From a machine learning perspective, it is the output of
    a machine learning algorithm when we apply it to our dataset. This output (model)
    can be a set of rules or some specific data structure that can be used to make
    predictions when used for any real-world data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training**: This is not a new component or step in machine learning. When
    we say training a model, this means applying an ML algorithm to a dataset to produce
    an ML model. The model we get as output is said to be trained on a certain dataset.
    There are three different ways to train a model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a) **Supervised learning**: This includes providing the desired output, along
    with our data records. The goal here is to learn how the input (X) can be mapped
    to the output (Y) using the available data. This approach of learning is used
    for classification and regression problems. Image classification and predicting
    house prices (regression) are a couple of real-world examples of supervised learning.
    In the case of image processing, we can train a model to identify the type of
    animal in an image, such as a cat or a dog, based on the shape, length, and breadth
    of the image. To train our image classification model, we will label each image
    in the training dataset with the animal''s name. To predict house pricing, we
    must provide data about the houses in the location we are looking at, such as
    the area they''re in, the number of rooms and bathrooms, and so on.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) **Unsupervised learning**: In this case, we train a model without knowing
    the desired output. Unsupervised learning is typically applied to clustering and
    association use cases. This type of learning is mainly based on observations and
    finding groups or clusters of data points so that the data points in a group or
    cluster have similar characteristics. This type of learning approach is extensively
    used by online retail stores such as Amazon to find different groups of customers
    (clustering) based on their shopping behavior and offer them items they''re interested
    in. Online stores also try to find an association between different purchases,
    such as how likely that a person buying item A will want to buy item B as well.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) **Reinforcement learning**: In the case of reinforcement learning, the model
    is rewarded for making an appropriate decision in a particular situation. In this
    case, no training data is available at all, but the model has to learn from experience.
    Autonomous cars are a popular example of reinforcement learning.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Testing**: We need to test our model on a dataset that is not used to train
    the model. A traditional approach is to train our model using two-thirds of the
    dataset and test the model using the remaining one-third.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to the three learning approaches we discussed, we also have deep
    learning. This is an advanced type of machine learning based on the approach of
    how the human brain achieves a certain type of knowledge using neural network
    algorithms. In this chapter, we will use supervised learning to build our sample
    models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore the options that are available in Python
    for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Using Python for machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python is a popular language in the data scientist community because of its
    simplicity, cross-platform compatibilities, and rich support for data analysis
    and data processing through its libraries. One of the key steps in machine learning
    is preparing data for building the ML models, and Python is a natural winner in
    doing this. The only challenge in using Python is that it is an interpreted language,
    so the speed of executing code is slow in comparison to languages such as C. But
    this is not a major issue as there are libraries available to maximize Python's
    speed by using multiple cores of **central processing units** (**CPUs**) or **graphics
    processing units** (**GPUs**) in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsection, we will introduce a few Python libraries for machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing machine learning libraries in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python comes with several machine learning libraries. We already mentioned supporting
    libraries such as NumPy, SciPy, and pandas, which are fundamental for data refinement,
    data analysis, and data manipulation. In this section, we will briefly discuss
    the most popular Python libraries for building machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This library is a popular choice because it has a large variety of built-in
    ML algorithms and tools to evaluate the performance of those ML algorithms. These
    algorithms include classification and regression algorithms for supervised learning
    and clustering and association algorithms for unsupervised learning. scikit-learn
    is mostly written in Python and relies on the NumPy library for many operations.
    For beginners, we recommend starting with the scikit-learn library and then moving
    to the next level of libraries, such as TensorFlow. We will use scikit-learn to
    illustrate the concepts of building, training, and evaluating the ML models.
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn also offers **gradient boost algorithms**. These algorithms are
    based on the mathematical concept of **Gradient**, which is a slope of a function.
    It measures the change in an error in the ML context. The idea of gradient-based
    algorithms is to fine-tune the parameters iteratively to find the local minimum
    of a function (minimizing errors for the ML models). Gradient boost algorithms
    use the same strategy to improve a model iteratively by taking into account the
    performance of the previous model, by fine-tuning the parameters for the new model,
    and by setting the target to accept the new model if it minimizes the errors more
    than the previous model.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: XGBoost, or **eXtreme Gradient Boosting**, is a library of algorithms that relies
    on gradient boosted decision trees. This library is popular as it is extremely
    fast and offers the best performance compared to other implementations of gradient
    boosting algorithms, as well as traditional machine learning algorithms. scikit-learn
    also offers gradient boost algorithms, which are fundamentally the same as XGBoost,
    though XGBoost is significantly fast. The main reason is the maximal utilization
    of parallelism across different cores of a single machine or in a distributed
    cluster of nodes. XGBoost can also regularize the decision trees to avoid overfitting
    the model to the data. XGBoost is not a full framework for machine learning but
    offers mainly algorithms (models). To use XGBoost, we must use scikit-learn for
    the rest of the utility functions and tools, such as data analysis and data preparation.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TensorFlow is another very popular open source library for machine learning,
    developed by the Google Brain team for high-performance computation. TensorFlow
    is particularly useful for training and running deep neural networks and is a
    popular choice in the area of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Keras
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is an open source API for deep learning for neural networks in Python.
    Keras is more of a high-level API on top of TensorFlow. For developers, using
    Keras is more convenient than using TensorFlow directly, so it is recommended
    to use Keras if you are starting to develop deep learning models with Python.
    Keras can work with both CPUs and GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PyTorch is another open source machine learning library that is a Python implementation
    of the popular **Torch** library in C.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will briefly discuss the best practices for using Python
    for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices of training data with Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have already highlighted how important the data is when training a machine
    learning model. In this section, we will highlight a few best practices and recommendations
    when preparing and using data to train your ML model. These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned previously, collecting a large set of data is of key importance
    (a few thousand data records or at least many hundreds). The bigger the size of
    the data, the more accurate the ML model will be.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clean and refine your data before starting any training. This means that there
    should not be any missing fields or misleading fields in the data. Python libraries
    such as pandas are very handy for such tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a dataset without compromising the privacy and security of data is important.
    You need to make sure you are not using the data of some other organization without
    the appropriate approval.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPUs work well with data-intensive applications. We encourage you to use GPUs
    to train your algorithms for faster results. Libraries such as XGBoost, TensorFlow,
    and Keras are well known for using GPUs for training purposes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When dealing with a large set of data for training, it is important to utilize
    the system memory efficiently. We should be loading the data in memory in chunks,
    or utilizing distributed clusters to process the data. We encourage you to use
    the generator function as much as you can.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also a good practice to watch your memory usage during data-intensive
    tasks (for example, while training a model) and free up memory periodically by
    forcing garbage collection to release unreferenced objects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we've covered the available Python libraries and the best practices
    of using Python for machine learning, it is time to start working with real code
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: Building and evaluating a machine learning model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start writing a Python program, we will evaluate the process of building
    a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about an ML model building process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We discussed the different components of machine learning in the *Introducing
    machine learning* section. The machine learning process uses those elements as
    input to train a model. This process follows a procedure with three main phases,
    and each phase has several steps in it. These phases are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.2 – Steps of building an ML model using a classic learning approach'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_13_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.2 – Steps of building an ML model using a classic learning approach
  prefs: []
  type: TYPE_NORMAL
- en: 'Each phase, along with detailed steps of it, is described here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data analysis**: In this phase, we collect raw data and transform it into
    a form that can be analyzed and then used to train and test a model. We may discard
    some data, such as records with empty values. Through data analysis, we try to
    select the features (attributes) that can be used to identify patterns in our
    data. Extracting features is a very important step, and a lot depends on these
    features when building a successful model. In many cases, we have to fine-tune
    features after the testing phase to make sure we have the right set of features
    for the data. Typically, we partition the data into two sets; one part is used
    to train the model in the modeling phase, while the other part is used to test
    the trained model for accuracy in the testing phase. We can skip the testing phase
    if we are evaluating the model using other approaches, such as **cross-validation**.
    We recommend having a testing phase in your ML building process and keeping some
    data (unseen to the model) aside for the testing phase, as shown in the preceding
    diagram.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modeling**: This phase is about training our model based on the training
    data and features we extracted in the previous phase. In a traditional ML approach,
    we can use the training data as-is to train our model. But to ensure our model
    has better accuracy, we can use the following additional techniques:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a) We can partition our training data into slices and use one slice for evaluating
    of our model and use the remaining slices for training the model. We repeat this
    for a different combination of training slices and the evaluation slice. This
    evaluation approach is called cross-validation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) ML algorithms come with several parameters that can be used to fine-tune
    the model to best fit the data. Fine-tuning these parameters, also known as **hyperparameters**,
    is typically done along with cross-validation during the modeling phase.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The feature values in data may use different scales of measurement, which makes
    it difficult to build rules with a combination of such features. In such cases,
    we can transform the data (feature values) into a common scale or into a normalized
    scale (say 0 to 1). This step is called scaling the data, or normalization. All
    these scaling and evaluation steps (or some of them) can be added to a pipeline
    (such as an Apache Beam pipeline) and can be executed together to evaluate different
    combinations for selecting the best model. The output of this phase is a candidate
    ML model, as shown in the preceding diagram.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Testing**: In the testing phase, we use the data we set aside to test the
    accuracy of the candidate ML model we built in the previous phase. The output
    of this phase can be used to add or remove some features and fine-tune the model
    until we get one with acceptable accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we are satisfied with the accuracy of our model, we can implement it to
    predict based on the data from the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Building a sample ML model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will build a sample ML model using Python, which will identify
    three types of Iris plants. To build this model, we will use a commonly available
    dataset containing four features (length and width of sepals and petals) and three
    types of Iris plants.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this code exercise, we will use the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: We will use the Iris dataset provided by *UC Irvine Machine Learning Repository*
    (http://archive.ics.uci.edu/ml/). This dataset contains 150 records and three
    expected patterns to identify. This is a refined dataset that comes with the necessary
    features already identified.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will use various Python libraries, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a) The pandas and the matplotlib libraries, for data analysis
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The scikit-learn library, for training and testing our ML model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: First, we will write a Python program for analyzing the Iris dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the Iris dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For ease of programming, we downloaded the two files for the Iris dataset (`iris.data`
    and `iris.names`) from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/).
  prefs: []
  type: TYPE_NORMAL
- en: We can directly access the data file from this repository through Python. But
    in our sample program, we will use a local copy of the files. The scikit-learn
    library also provides several datasets as part of the library and can be used
    directly for evaluation purposes. We decided to use the actual files as this will
    be close to real-world scenarios, where you collect data yourself and then use
    it in your program.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Iris data file contains 150 records that are sorted based on the expected
    output. In the data file, the values of four different features are provided.
    These four features are described in the `iris.names` file as `sepal-length`,
    `sepal-width`, `petal-length`, and `petal-width`. The expected output types of
    Iris plant, as per the data file, are `Iris-setosa`, `Iris-versicolor`, and `Iris-virginica`.
    We will load the data into a pandas DataFrame and then analyze it for different
    attributes of interest. Some sample code for analyzing the Iris data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first part of the data analysis, we checked a few metrics about the
    data using the pandas library functions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We used the `shape` method to get the dimension of the DataFrame. This should
    be [150, 5] for the Iris dataset as we have 150 records and five columns (four
    for features and one for the expected output). This step ensures that all the
    data is loaded into our DataFrame correctly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We checked the actual data using the `head` or `tail` method. This is only to
    see the data visually, especially if we have not seen what is inside the data
    file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `describe` method gave us the different statistical KPIs available for
    the data. The outcome of this method is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: These KPIs can help us select the right algorithm for the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `groupby` method was used to identify the number of records for each `class`
    (name of the column for the expected output). The output will indicate that there
    are 50 records for each type of Iris plant:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the second part of the analysis, we tried to use a `describe` method (minimum
    value, first quartile, second quartile (median), third quartile, and the maximum
    value). This plot will tell you if your data is symmetrically distributed or grouped
    in a certain range, or how much of your data is skewed toward one side of the
    distribution. For our Iris dataset, we will get the box plots for our four features
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.3 – Box and whisker plots of Iris dataset features'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_13_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.3 – Box and whisker plots of Iris dataset features
  prefs: []
  type: TYPE_NORMAL
- en: 'From these plots, we can see that the **petal-length** and the **petal-width**
    data has the most grouping between the first quartile and the third quartile.
    We can confirm this by analyzing the data distribution by using the histogram
    plots, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.4 – Histogram of Iris dataset features'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_13_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.4 – Histogram of Iris dataset features
  prefs: []
  type: TYPE_NORMAL
- en: After analyzing the data and selecting the right type of algorithm (model) to
    use, we will move on to the next step, which is training our model.
  prefs: []
  type: TYPE_NORMAL
- en: Training and testing a sample ML model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To train and test an ML algorithm (model), we must follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, we will split our original dataset into two groups: training
    data and testing data. This approach of splitting the data is called the `train_test_split`
    function to make this split convenient:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the next step, we will create a model and provide the training data (`X_train`
    and `y_train`) to train this model. The choice of ML algorithm is not that important
    for this exercise. For the Iris dataset, we will use the SVC algorithm with default
    parameters. Some sample Python code is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, the predictions will be evaluated with the expected results, as per
    the test data (`y_test`), using the `accuracy_score` and `classification_report`
    functions of the scikit-learn library, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The console output of this program is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The accuracy scope is very high (*0.966*), which indicates that the model can
    predict the Iris plant with nearly 96% accuracy for the testing data. The model
    is doing an excellent job for **Iris-setosa** and **Iris-versicolor** but only
    a decent job (86% precise) in the case of **Iris-virginica**. There are several
    ways to improve the performance of our model, all of which we will discuss in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a model using cross-validation and fine tuning hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the previous sample model, we kept the training process simple for the
    sake of learning the core steps of building an ML model. For production deployments,
    we cannot rely on a dataset that only contains 150 records. Additionally, we must
    evaluate the model for the best predictions using techniques such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**k-fold cross validation**: In the previous model, we shuffled the data before
    splitting it into training and testing datasets using the Holdout method. Due
    to this, the model can give us different results every time we train it, thus
    resulting in an unstable model. It is not trivial to select training data from
    a small dataset that contains 150 records since in our case, that can truly represent
    the data of a real-world system or environment. To make our previous model more
    stable with a small dataset, k-fold cross-validation is the recommended approach.
    This approach is based on dividing our dataset into *k* folds or slices. The idea
    is to use *k-1* slices for training and to use the *kth* slice for evaluating
    or testing. This process is repeated until we use every slice of data for testing
    purposes. This is equivalent to repeating the holdout method *k* times using the
    different slices of data for testing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To elaborate further, we must split our whole dataset or training data set
    into five slices, say *k=5*, for 5-fold cross-validation. In the first iteration,
    we can use the first slice (20%) for testing and the remaining four slices (80%)
    for training. In the second iteration, we can use the second slice for testing
    and the remaining four slices for training, and so on. We can evaluate the model
    for all five possible training datasets and select the best model in the end.
    The selection scheme of data for training and testing is shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.5 – Cross-validation scheme for five slices of data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17189_13_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.5 – Cross-validation scheme for five slices of data
  prefs: []
  type: TYPE_NORMAL
- en: The cross-validation accuracy is calculated by taking the average accuracy of
    each model we build in each iteration, as per the value of *k*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimizing hyperparameters**: In the previous code example, we used the machine
    learning algorithm with default parameters. Each machine learning algorithm comes
    with many hyperparameters that can be fine-tuned to customize the model, as per
    the dataset. It may be possible for statisticians to set a few parameters manually
    by analyzing the data distribution, but it is tedious to analyze the impact of
    a combination of these parameters. There is a need to evaluate our model by using
    different values of these hyperparameters, which can assist us in selecting the
    best hyperparameter combination in the end. This technique is called fine-tuning
    or optimizing the hyperparameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cross-validation and fine-tuning hyperparameters are tedious to implement,
    even through programming. The good news is that the scikit-learn library comes
    with tools to achieve these evaluations in a couple of lines of Python code. The
    scikit-learn library offers two types of tools for this evaluation: `GridSearchCV`
    and `RandomizedSearchCV`. We will discuss each of these tools next.'
  prefs: []
  type: TYPE_NORMAL
- en: GridSearchCV
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `GridSearchCV` tool evaluates any given model by using the cross-validation
    approach for all possible combinations of values provided for the hyperparameters.
    Each combination of values of hyperparameters will be evaluated by using cross-validation
    on dataset slices.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code example, we will use the `GridSearchCV` class from the
    scikit-learn library to evaluate the SVC model for a combination of `C` and `gamma`
    parameters. The `C` parameter is a regularization parameter that manages the tradeoff
    between having a low training error versus having a low testing error. A higher
    value of `C` means we can accept a higher number of errors. We will use 0.001,
    0.01, 1, 5, 10, and 100 as values for `C`. The `gamma` parameter is used to define
    the non-linear hyperplanes or non-linear lines for the classification. The higher
    the value of `gamma`, the model can try to fit more data by adding more curvature
    or curve to the hyperplane or the line. We will use values such as 0.001, 0.01,
    1, 5, 10, and 100 for `gamma` as well. The complete code for `GridSearchCV` is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code example, the following points need to be highlighted:'
  prefs: []
  type: TYPE_NORMAL
- en: We loaded the data directly from the scikit-learn library for illustration purposes.
    You can use the previous code to load the data from a local file as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to define the `params` dictionary for fine-tuning hyperparameters
    as a first step. We set the values for the `C` and `gamma` parameters in this
    dictionary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We set `cv=5`. This will evaluate each parameter combination by using cross-validation
    across the five slices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output of this program will give us the best combination of `C` and `gamma`
    and the accuracy of the model with cross-validation. The console output for the
    best parameters and the accuracy of the best model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: By evaluating different combinations of parameters and using cross-validation
    with `GridSearchCV`, the overall accuracy of the model is improved to 98% from
    96%, compared to the results we observed without cross-validation and hyperparameter
    fine-tuning. The classification report (not shown in the program output) shows
    that the precision for the three plant types is 100% for our test data. However,
    this tool is not feasible to use when we have a large number of parameter values
    with a large dataset.
  prefs: []
  type: TYPE_NORMAL
- en: RandomizedSearchCV
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the case of the `RandomizedSearchCV` tool, we only evaluate a model for randomly
    selected hyperparameter values instead of all the different combinations. We can
    provide the parameter values and the number of random iterations to perform as
    input. The `RandomizedSearchCV` tool will randomly select the parameter combination
    as per the number of iterations provided. This tool is useful when we are dealing
    with a large dataset and when many combinations of parameters/values are possible.
    Evaluating all the possible combinations for a large dataset can be a very long
    process that requires a lot of computing resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python code for using `RandomizedSearchCV` is the same as for the `GridSearchCV`
    tool, except for the following additional lines of codes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Since we defined `n_iter=5`, `RandomizedSearchCV` will select only five combinations
    of the `C` and `gamma` parameters and evaluate the model accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'When this part of the program is executed, we will get an output similar to
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that you may get a different output because this tool may select different
    parameter values for the evaluation. If we increase the number of iterations (`n_iter`)
    for the `RandomizedSearchCV` object, we will observe more accuracy in the output.
    If we do not set `n_iter`, we will run the evaluation for all combinations, which
    means we'll get the same output that `GridSearchCV` provides.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the best parameter combination that's selected by the `GridSearchCV`
    tool is different than the one selected by the `RandomizedSearchCV` tool. This
    is expected because we ran the two tools for a different number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion on building a sample ML model using the scikit-learn
    library. We covered the core steps and concepts that are required in building
    and evaluating such models. In practice, we also scale the data for normalization.
    This scaling can be achieved either by using built-in scaler classes in the scikit-learn
    library, such as `StandardScaler`, or by building our own scaler class. The scaling
    operation is a data transformation operation and can be combined with the model
    training task under a single pipeline. scikit-learn supports combining multiple
    operations or tasks as a pipeline using the `Pipeline` class. The `Pipeline` class
    can also be used directly with the `RandomizedSearchCV` or `GridSearchCV` tools.
    You can find out more about how to use scalers and pipelines with the scikit-learn
    library by reading the online documentation for the scikit-learn library ([https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html)).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss how to save a model to a file and restore
    a model from a file.
  prefs: []
  type: TYPE_NORMAL
- en: Saving an ML model to a file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we have evaluated a model and selected the best one as per our dataset,
    the next step is to implement this model for future predictions. This model can
    be implemented as part of any Python application, such as web applications, Flask,
    or Django, or it can be used as a microservice or even a cloud function. The real
    question is how to transfer the model object from one program to the other. There
    are a couple of libraries such as `pickle` and `joblib` that can be used to serialize
    a model into a file. The file can then be used in any application to load the
    model again in Python and make predictions using the `predict` method of the model
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this concept, we will save the ML model we created in one of
    the previous code examples (for example, the `model` object in the `iris_build_svm_model.py`
    program) to a file called `model.pkl`. In the next step, we will load the model
    from this file using the pickle library and make a prediction using new data to
    emulate the use of a model in any application. The complete sample code is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The use of the joblib library is simpler than the pickle library but it may
    require you to install this library if it has not been installed as a dependency
    of scikit-learn. The following sample code shows the use of the joblib library
    to save our best model, as per the evaluation of the `GridSearchCV` tool we did
    in the previous section, and then load the model from the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The code for the joblib library is concise and simple. The prediction part of
    the sample code is the same as in the previous code sample for the pickle library.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've learned how the model can be saved in a file, we can take the
    model to any application for deployment and even to a cloud platform, such as
    GCP AI Platform. We will discuss how to deploy our ML model on a GCP platform
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying and predicting an ML model on GCP Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Public cloud providers are offering several AI platforms for training built-in
    models, as well as your custom models, for deploying the models for predictions.
    Google offers the **Vertex AI** platform for ML use cases, whereas Amazon and
    Azure offer the **Amazon SageMaker** and **Azure ML** services, respectively.
    We selected Google because we assume you have set up an account with GCP and that
    you are already familiar with the core concepts of GCP. GCP offers its AI Platform,
    which is part of the Vertex AI Platform, for training and deploying your ML models
    at scale. The GCP AI Platform supports libraries such as scikit-learn, TensorFlow,
    and XGBoost. In this section, we will explore how to deploy our already trained
    model on GCP and then predict the outcome based on that model.
  prefs: []
  type: TYPE_NORMAL
- en: Google AI Platform offers its prediction server (compute node) either through
    a global endpoint (`ml.googleapis.com`) or through a regional endpoint (`<region>-ml.googleapis.com`).
    The global API endpoint is recommended for batch predictions, which are available
    for TensorFlow on Google AI Platform. The regional endpoint offers additional
    protection against outages in other regions. We will use a regional endpoint to
    deploy our sample ML model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start deploying a model in GCP, we will need to have a GCP project.
    We can create a new GCP project or use an existing GCP project that we''ve created
    for previous exercises. The steps of creating a GCP project and associating a
    billing account with it were discussed in [*Chapter 9*](B17189_09_Final_PG_ePub.xhtml#_idTextAnchor247),
    *Python Programming for the Cloud*. Once we have a GCP project ready, we can deploy
    the `model.joblib` model, which we created in the previous section. The steps
    for deploying our model are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, we will create a storage bucket where we will store our model
    file. We can use the following Cloud SDK command to create a new bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once our bucket is ready, we can upload our model file (`model.joblib`) to
    this storage bucket using the following Cloud SDK command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: gcloud ai-platform models create my_iris_model –  region=us-central1
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can create a version for our model by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using endpoint [https://us-central1-  ml.googleapis.com/]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Creating version (this might take a few   minutes)......done.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To check if our model and version have been deployed correctly, we can use
    the `describe` command under the `versions` context, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once our model has been deployed with its version, we can use new data to predict
    the outcome using the model we deployed on Google AI Platform. For testing, we
    added a couple of data records, different from the original dataset, in a JSON
    file (`input.json`), as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can use the following Cloud SDK command to predict the outcome based on
    the records inside the `input.json` file, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The console output will show the predicted class for each record, as well as
    the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To use the deployed model in our application (local or cloud), we can use *Cloud
    SDK* or *Cloud Shell*, but the recommended approach is to use the Google AI API
    to make any predictions.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have covered cloud deployment and the prediction options for our
    ML model using Google AI Platform. However, you can also take your ML model to
    other platforms, such as Amazon SageMaker and Azure ML for deployment and prediction.
    You can find more details about the Amazon SageMaker platform at https://docs.aws.amazon.com/sagemaker/
    and more details about Azure ML at [https://docs.microsoft.com/en-us/azure/machine-learning/](https://docs.microsoft.com/en-us/azure/machine-learning/).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced machine learning and its main components, such
    as datasets, algorithms, and models, as well as training and testing a model.
    This introduction was followed by a discussion of popular machine learning frameworks
    and libraries available for Python. These include scikit-learn, TensorFlow, PyTorch,
    and BGBoost. We also discussed the best practices of refining and managing the
    data for training ML models. To get familiar with the scikit-learn library, we
    built a sample ML model using the SVC algorithm. We trained the model and evaluated
    it using techniques such as k-fold cross-validation and fine-tuning hyperparameters.
    We also learned how to store a trained model in a file and then load that model
    into any program for prediction purposes. In the end, we demonstrated how we can
    deploy our ML model and predict results using the Google AI Platform with a few
    GCP Cloud SDK commands.
  prefs: []
  type: TYPE_NORMAL
- en: The concepts and the hands-on exercises included in this chapter are adequate
    to help build the foundation for using Python for machine learning projects. This
    theoretical and hands-on knowledge is beneficial for those who are looking to
    start using Python for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore how to use Python for network automation.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are supervised learning and unsupervised learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is k-fold cross-validation and how it is used to evaluate a model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is `RandomizedSearchCV` and how it is different from `GridSearchCV`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What libraries can we use to save a model in a file?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why are regional endpoints preferred option over global endpoints for Google
    AI Platform?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Machine Learning Algorithms*, by Giuseppe Bonaccorso'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*40 Algorithms Every Programmer Should Know*, by Imran Ahmad'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mastering Machine Learning with scikit-learn*, by Gavin Hackeling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn,
    and TensorFlow 2*, by Sebastian Raschka and Vahid Mirjalili'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*scikit-learn User Guide*, available at [https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Google AI Platform Guides* for training and deploying ML models are available
    at [https://cloud.google.com/ai-platform/docs](https://cloud.google.com/ai-platform/docs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In supervised learning, we provide the desired output with the training data.
    The desired output is not included as part of the training data for unsupervised
    learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cross-validation is a statistical technique that's used to measure the performance
    of an ML model. In k-fold cross-validation, we divide the data into *k* folds
    or slices. We train our model using the *k-1* slices of the dataset and test the
    accuracy of the model using the *kth* slice. We repeat this process until each
    *kth* slice is used as testing data. The cross-validation accuracy of the model
    is computed by taking the average of the accuracy of all the models we built through
    *k* iterations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`RandomizedSearchCV` is a tool that''s available with scikit-learn for applying
    cross-validation functionality to an ML model for randomly selected hyperparameters.
    `GridSearchCV` provides similar functionality to `RandomizedSearchCV`, except
    that it validates the model for all the combinations of hyperparameter values
    provided to it as an input.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pickle and Joblib.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Regional endpoints offer additional protection against any outages in other
    regions, and the availability of computing resources is more for regional endpoints
    than global endpoints.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
