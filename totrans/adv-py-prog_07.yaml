- en: '*Chapter 6*: Automatic Differentiation and Accelerated Linear Algebra for Machine
    Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the recent explosion of data and data generating systems, machine learning
    has grown to be an exciting field, both in research and industry. However, implementing
    a machine learning model might prove to be a difficult endeavor. Specifically,
    common tasks in machine learning, such as deriving the loss function and its derivative,
    using gradient descent to find the optimal combination of model parameters, or
    using the kernel method for nonlinear data, demand clever implementations to make
    predictive models efficient.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the JAX library, the premier high-performance
    machine learning tool in Python. We will explore some of its most powerful features,
    such as automatic differentiation, JIT compilation, and automatic vectorization.
    These features streamline the tasks that are central to machine learning mentioned
    previously, making training a predictive model as simple and accessible as possible.
    All these discussions will revolve around a hands-on example of a binary classification
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the chapter, you will be able to use JAX to power your machine
    learning applications and explore the more advanced features that it offers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of topics covered in this chapter is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A crash course in machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting JAX up and running
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic differentiation for loss minimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just-In-Time compilation for improved efficiency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic vectorization for efficient kernels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A crash course in machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To fully appreciate the functionalities that JAX offers, let's first talk about
    the principal components of a typical workflow of training a machine learning
    model. If you are already familiar with the basics, feel free to skip to the next
    section, where we begin discussing JAX.
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, we set out to solve the problem of predicting an unknown
    target value of interest of a data point by considering its observable features.
    The goal is to design a predictive model that processes the observable features
    and outputs an estimate of what the target value might be. For example, image
    recognition models analyze the pixel values of an image to predict which object
    the image depicts, while a model processing weather data could predict the probability
    of rainy weather for tomorrow by accounting for temperature, wind, and humidity.
  prefs: []
  type: TYPE_NORMAL
- en: In general, a machine learning model could be viewed as a general mathematical
    function that takes in the observable features, typically referred to as *X*,
    and produces a prediction about the target value, *Y*. Implicitly assumed by the
    model is the fact that there is a certain mathematical relationship between the
    features (input) and the target values (output). Machine learning models in turn
    aim to learn about this relationship by studying a large number of examples. As
    such, data is crucial in any machine learning workflow.
  prefs: []
  type: TYPE_NORMAL
- en: A **dataset** is a collection of *labeled* examples – data points whose observable
    features, *X*, and target values, *Y*, are both available. It is through these
    labeled points that a model will attempt to uncover the relationship between *X*
    and *Y*; this is known as *training* the model. After training, the learned model
    can then take in the features of *unlabeled* data points, whose target values
    are unknown to us, and output its predictions. These predictions are what the
    model believes the target values of the unlabeled data points to be, given the
    observable features that they have.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will talk about how training is facilitated for many common
    machine learning models. Specifically, it consists of three core components—the
    model's parameters, the model's loss function, and the minimization of that loss
    function—which we will consider one by one.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will ground our discussions with an explicit example problem. Assume that
    we have a number of points in a two-dimensional space (which is simply the *xy*
    plane), each of which belongs to either the positive class or the negative class.
    The following is an example where the yellow points are the positives, and the
    purple ones are the negatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – An example binary classification problem ](img/Figure_6.1_17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – An example binary classification problem
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the observable features are the *x* and *y* coordinates of the
    points, so *X* consists of two features, while the target value *y* is their class
    membership, a positive or a negative. Our goal is to train a machine learning
    model on a set of labeled data and make predictions on unlabeled points. For example,
    after training, our model should be able to predict whether point (2, 1) is positive
    or negative.
  prefs: []
  type: TYPE_NORMAL
- en: It is quite clear to us what the separation between the positives and the negatives
    is, where the lower-right corner corresponds to the positive class, and the upper-left
    corner corresponds to the negative class. Point (2, 1) specifically is likely
    a positive (a yellow point). But how does a machine learning model do this automatically,
    without humans having to hardcode such a classification rule? We will see this
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Model parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A predictive model starts out with a formula that attempts to explain the relationship
    between the observable features and the target value. We will consider the class
    of linear models (one of the most common and simplest types of machine learning
    models), which assume that the target value is a linear combination of the features
    (hence the model's name)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_6.1_B17499.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/Formula_6.2_B17499.png) and ![](img/Formula_6.3_B17499.png) are
    the *x* and *y* coordinates of the points, and the numbers ![](img/Formula_6.4_B17499.png)
    are the model's *parameters*.
  prefs: []
  type: TYPE_NORMAL
- en: We do not know the values of these parameters, and each combination of values
    for these parameters defines a unique relationship between *x* and *y*, a unique
    hypothesis. It is the values of these parameters that we need to identify during
    the training phase, such that the aforementioned relationship fits the labeled
    data that we have access to.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our classification model, we constrain the target values *y* to be either
    positive (+1) or negative (-1), but *y* as defined by the linear relationship
    discussed earlier can take on any real value. As such, it is customary to transform
    a real-valued *y* to be either +1 or -1 by simply taking the sign of the value.
    In other words, the actual model we are working with is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_6.5_B17499.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Performing predictions is at the heart of a common machine learning task. Given
    the linear relationship between features *X* and the target value *y*, predicting
    the target value ![](img/Formula_6.6_B17499.png) of a new data point with features
    ![](img/Formula_6.7_B17499.png) and ![](img/Formula_6.8_B17499.png) simply involves
    applying the mathematical model we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_6.9_B17499.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The only missing piece of our puzzle is how to identify the correct values
    for our parameters ![](img/Formula_6.10_B17499.png). Again, we need to determine
    the values for the model''s parameters to fit our training data. But how can we
    exactly quantify how well a specific value combination for the parameters explains
    a given labeled dataset? This question leads to the next component of a machine
    learning workflow: the model''s loss function.'
  prefs: []
  type: TYPE_NORMAL
- en: Loss function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A model's loss function quantifies a property about the model that we'd like
    to minimize. This could, in general, be the cost incurred when the model is trained
    a certain way. Within our context, the loss function is specifically the amount
    of predictive error that the model makes on the training dataset. This quantity
    approximates the error made on future unseen data, which is what we really care
    about. (In different settings, you might encounter **utility functions**, which,
    most of the time, are simply the negative of corresponding loss functions, which
    are to be maximized.) The lower the loss, the better our model is likely to perform
    (barring technical problems such as overfitting).
  prefs: []
  type: TYPE_NORMAL
- en: In a classification problem like ours, there are many ways to design an appropriate
    loss function. For example, the 0-1 loss simply counts how many instances there
    are in which the model makes an incorrect prediction, while the binary cross-entropy
    loss is a more popular choice as the function is smoother than 0-1 and thus easy
    to optimize (we will come back to this point later).
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example, we will use a version of the support-vector loss function,
    which is formalized as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_6.11_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *i*, indicating the index of unique examples in the training set, ranges
    from 1 to *n*, the training set size. ![](img/Formula_6.12_B17499.png) is the
    true label of the *i*-th example, while ![](img/Formula_6.13_B17499.png) is the
    corresponding label predicted by the learning model.
  prefs: []
  type: TYPE_NORMAL
- en: This non-negative function calculates the average of the ![](img/Formula_6.14_B17499.png)
    term across all training examples. It is easy to see that for each example *i*,
    if ![](img/Formula_6.15_B17499.png) and ![](img/Formula_6.16_B17499.png) are similar
    to each other, the max term will evaluate to a small value. On the other hand,
    if ![](img/Formula_6.17_B17499.png) and ![](img/Formula_6.18_B17499.png) have
    opposite signs (in other words, if the model misclassifies the example), the max
    term will increase. As such, this function is appropriate to quantify the error
    that our predictive model is making for a given parameter combination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given this loss function, the problem of training our linear model reduces
    to finding the optimal parameter ![](img/Formula_6.19_B17499.png) that minimizes
    the loss (preferably at 0). Of course, the search space of ![](img/Formula_6.20_B17499.png)
    is very large—each parameter could be any real-valued number—so exhaustively trying
    out all possible combinations of values for ![](img/Formula_6.21_B17499.png) is
    out of the question. This leads us to the final subsection: intelligently minimizing
    a loss function.'
  prefs: []
  type: TYPE_NORMAL
- en: Loss minimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Without going into too much technical detail, you can mathematically find the
    minimum of a given function by utilizing its *derivative* information. Derivates
    of a function denote the rate of change in the function value with respect to
    the rate of change of its input. By analyzing the value of the derivative, or
    more commonly referred to in machine learning as the *gradient*, we could algorithmically
    identify input locations at which the objective function is likely to be low-valued.
  prefs: []
  type: TYPE_NORMAL
- en: However, derivatives are only informative in helping us locate the function
    minimum if the function itself is *smooth*. Smoothness is a mathematical property
    that states that if the input value of a function only changes by a little, the
    function value (output) should not change by too much. Notable examples of non-smooth
    functions include the step function and the absolute value function, but most
    functions that you will encounter are likely to be smooth. The smoothness property
    explains our preference for the binary cross-entropy loss over the 0-1 loss, the
    latter of which is a non-smooth function. The support vector-style loss function
    we will be using is also smooth.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall takeaway is that if we have access to the derivate of a smooth
    function, finding its minimum will be easy to do. But how do we do this exactly?
    Many gradient-based optimization routines have been studied, but we will consider
    arguably the most common method: *gradient descent*. The high-level idea is to
    inspect the gradient of the loss function at a given location and move in the
    direction *opposite* to that gradient (in other words, descending the gradient,
    hence the name), illustrated next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Illustration of gradient descent ](img/Figure_6.2_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Illustration of gradient descent
  prefs: []
  type: TYPE_NORMAL
- en: Due to the definition of the gradient, the function value *f(x')* evaluated
    at the point *x'* that is immediately in the opposite direction of the gradient
    with respect to a given point *x* is lower than *f(x)*. As such, by using the
    gradient of the loss function as a guide and incrementally moving in the opposite
    direction, we can theoretically arrive at the function minimum. One important
    consideration is that, if we move our evaluation by a large amount, even in the
    correct direction, we could end up missing the true minimum. To address this,
    we typically adjust our step to be only a fraction of the loss gradient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Concretely, denote ![](img/Formula_6.22_B17499.png) as the gradient of the
    loss function *L* with respect to the *i*-th parameter ![](img/Formula_6.23_B17499.png),
    so the gradient descent *update rule* is to adjust the value of ![](img/Formula_6.24_B17499.png)
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_6.25_B17499.png)'
  prefs: []
  type: TYPE_IMG
- en: where *γ* is a small number (much lower than 1) that acts as a step size. By
    repeatedly applying this update rule, we could incrementally adjust the values
    of the model's parameters to lower the model's loss, thereby improving its predictive
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: And with that, we have finished sketching out the main components of a typical
    machine learning workflow. As a summary, we work with a predictive model (in our
    case, a linear one) that has a certain number of adjusting parameters that seek
    to explain the data we have. Each parameter combination leads to a different hypothesis
    whose validity is quantified by the loss function. Finally, by taking the derivative
    of the loss function and using gradient descent, we have a way to incrementally
    update the model's parameters to achieve better performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now begin the main topic of this chapter: the JAX library.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting JAX up and running
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As briefly mentioned, JAX is a combination of different tools for developing
    accelerated, high-performance computations with a focus on machine learning applications.
    Remember from the last chapter that the NumPy library offers optimized computation
    for numerical operations such as finding the min/max or taking the sum of the
    average along an axis. We can think of JAX as the NumPy equivalent for machine
    learning, where common tasks in machine learning could be done in highly optimized
    code. These, as we will see, include automatic differentiation, accelerated linear
    algebra using a Just-In-Time compiler, and efficient vectorization and parallelization
    of code, among other things.
  prefs: []
  type: TYPE_NORMAL
- en: JAX offers these functionalities through what's known as **functional transformations**.
    In the simplest sense, a functional transformation in JAX converts a function,
    typically one that we build ourselves, to an optimized version where different
    functionalities are facilitated. This is done via a simple API, as we will see
    later.
  prefs: []
  type: TYPE_NORMAL
- en: But first, we will talk about installing the library on a local system.
  prefs: []
  type: TYPE_NORMAL
- en: Installing JAX
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A barebones version of JAX could be simply installed using the regular Python
    package manager `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'These commands will install a CPU-only version of JAX, which is recommended
    if you are planning to try it out on a local system such as a laptop. The full
    version of JAX, with support for NVIDIA GPUs (commonly used by deep learning models),
    could also be installed via a more involved procedure, outlined in their documentation:
    [https://jax.readthedocs.io/en/latest/developer.html](https://jax.readthedocs.io/en/latest/developer.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, note that JAX can only be installed on Linux (Ubuntu 16.04 or above)
    or macOS (10.12 or above). To run JAX on a Windows machine, you would need the
    Windows Subsystem for Linux, which lets you run a GNU/Linux environment on Windows.
    If you typically run your machine learning jobs on a cluster, you should talk
    to your system administrator about finding the correct way to install JAX.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are an independent machine learning practitioner, you might find these
    technical requirements to install the full version of JAX intimidating. Luckily,
    there is a free platform that we could use to see JAX in action: *Google Colab*.'
  prefs: []
  type: TYPE_NORMAL
- en: Using Google Colab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generally, Google Colab is a free Jupyter notebook platform that is integrated
    with a high-performance backend and machine learning tools on which you could
    build prototypes of machine learning models with ease.
  prefs: []
  type: TYPE_NORMAL
- en: There are two main advantages to using Google Colab. First, the platform comes
    with most machine learning-related libraries (for example, TensorFlow, PyTorch,
    and scikit-learn) and tools preinstalled, including a **GPU** (**graphics processing
    unit**) and a **TPU** (**tensor processing unit**), free of charge! This allows
    independent machine learning researchers and students to try out expensive hardware
    on their machine learning pipelines at no cost. Second, Google Colab allows collaborative
    editing between different users, enabling streamlined group work when building
    models.
  prefs: []
  type: TYPE_NORMAL
- en: To utilize this platform, all you need is a Google/Gmail account when signing
    in at [https://colab.research.google.com/](https://colab.research.google.com/).
    Using this platform is generally simple and intuitive, but you can also refer
    to several readings included at the end of this chapter for further content relating
    to Google Colab. Note that the code presented in the remainder of this chapter
    is run on Google Colab, which means you could go to **[INSERT LINK]** and simply
    run the code yourself to follow along with our later discussions.
  prefs: []
  type: TYPE_NORMAL
- en: And with that, we are ready to begin talking about the functionalities that
    JAX offers.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic differentiation for loss minimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall from our previous discussion that to fit a predictive model to a training
    dataset, we first analyze an appropriate loss function, derive the gradient of
    this loss, and then adjust the parameters of the model in the opposite direction
    of the gradient to achieve a lower loss. This procedure is only possible if we
    have access to the derivative of the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier machine learning models were able to do this because researchers derived
    the derivatives of common loss functions by hand using calculus, which were then
    hardcoded into the training algorithm so that a loss function could be minimized.
    Unfortunately, taking the derivative of a function could be difficult to do at
    times, especially if the loss function being used is not well behaved. In the
    past, you would have to choose a different, more mathematically convenient loss
    function to make your model run even if the new function was less appropriate,
    potentially sacrificing some of the expressiveness of the original function.
  prefs: []
  type: TYPE_NORMAL
- en: Recent advances in computational algebra have resulted in the technique called
    **automatic differentiation**. Exactly as it sounds, this technique allows for
    an automated process to numerically evaluate the derivative of a given function,
    expressed in computer code, without relying on a human to provide the closed-form
    solution for that derivative. On the highest level, automatic differentiation
    traces the order in which the considered function is computed and which arithmetic
    operations (for example, addition, multiplication, and exponentiation) were involved,
    and then applies the chain rule to obtain the numerical value of the derivative
    in an automated manner. By intelligently taking advantage of the information present
    in the computational calculation of the function itself, automatic differentiation
    can compute its derivative efficiently and accurately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that in our running toy problem, we would like to minimize the loss
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_6.26_B17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/Formula_6.27_B17499.png) is the prediction made by our linear
    predictive model for the *i*-th data point. Of course, we could derive the gradient
    of this loss function when implementing gradient descent (which would require
    us to brush up on our calculus!), but with automatic differentiation, we will
    not have to.
  prefs: []
  type: TYPE_NORMAL
- en: The technique is widely used in deep learning tools such as TensorFlow and PyTorch,
    where computing the gradient of a loss function is a central task. We could use
    the API for automatic differentiation from these libraries to compute the loss
    gradient, but by using JAX's API, we will be able to utilize other powerful functionalities
    that we will get to later. For now, let's build our model and the gradient descent
    algorithm using JAX's automatic differentiation tool.
  prefs: []
  type: TYPE_NORMAL
- en: Making the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we need to generate the toy dataset shown in *Figure 6.1*. To do this,
    we will make use of the `make_blobs` function from the `datasets` module in scikit-learn.
    If you are running your code locally, you would need to install scikit-learn using
    the `pip` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Otherwise, if you are using Google Colab, scikit-learn, as well as all the other
    external libraries used in this chapter's code, already comes preinstalled, so
    you only need to import it into your program.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `make_blobs` function, as the name suggests, generates blobs (in other
    words, clusters) of data points. Its API is simple and readable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here we are creating a two-dimensional (specified by `n_features`) dataset of
    500 points (specified by `n_samples`). These data points should belong to one
    of two classes (specified by `centers`). The `cluster_std` argument controls how
    tightly the points belonging to the same class cluster together; in this case,
    we are setting it to `0.5`. At this point, we have a dataset of 500 points, each
    of which has two features and belongs to either the positive class or the negative
    class. The variable `X` is a NumPy array that contains the features, having a
    shape of `(500, 2)`, while `y` is a 500-long, one-dimensional, binary array containing
    the labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, the negative class has the label of `0` in `y`. As a data preprocessing
    step, we will convert all the instances of `0` in `y` to `-1` using NumPy''s convenient
    conditional indexing syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We are also adding to `X` a third feature column that only contains instances
    of `1`. This column is a constant and corresponds to the free coefficient ![](img/Formula_6.28.1_B17499.png)
    in our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_6.28_B17499.png)'
  prefs: []
  type: TYPE_IMG
- en: On the other hand, ![](img/Formula_6.29_B17499.png) and ![](img/Formula_6.30_B17499.png)
    are the coefficients for the first and second features in `X`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now generated our toy example. If you now call Matplotlib to generate
    a scatter plot on `X`, for example, via the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You will obtain the same plot as in *Figure 6.1*. And with that, we are ready
    to begin building our predictive model.
  prefs: []
  type: TYPE_NORMAL
- en: Building a linear model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Remember two core components of a machine learning pipeline: the model and
    the loss function. To build our model, we first need to write functions that correspond
    to these components. First, the model comes in the form of the `predict` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This function takes in the model's parameters `w`, implicitly assumed to be
    in the form of a NumPy array, and the features of the training data points `X`,
    a NumPy two-dimensional matrix. Here we are using the `dot` function from the
    NumPy module of JAX, aliased `jnp` in the code, to take the dot product of `w`
    and `X`. If you are unfamiliar with the dot product, it is simply an algebraic
    operation that is a shorthand for the linear combination ![](img/Formula_6.31_B17499.png)
    that corresponds to our model. Overall, this function encodes for our model that
    the prediction we make about the label of a data point is (the sign of) the dot
    product of the parameters and the data features.
  prefs: []
  type: TYPE_NORMAL
- en: 'While not a focus of our discussion, it is important at this point to note
    that the `numpy` module of JAX provides nearly identical APIs for mathematical
    operations as NumPy. The `dot` function is simply one example; others could be
    found on their documentation site: [https://jax.readthedocs.io/en/latest/jax.numpy.html](https://jax.readthedocs.io/en/latest/jax.numpy.html).
    This is to say that if you have accumulated a significant amount of code in NumPy
    but would like to convert it to JAX, replacing your import statement of `import
    numpy as np` with `import jax.numpy as np` would, in most cases, do the trick;
    no complicated, painstaking conversion is necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: So, we have implemented the prediction function of our model. Our next task
    is to write the loss function
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_6.32_B17499.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The loss function can be written with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Let's take a moment to break this code down. This `loss` function is a function
    of `w`, since, if `w` changes, the loss will change accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s focus on the term inside the summation symbol in the previous formula:
    it is the max between 0 and ![](img/Formula_6.33_B17499.png), where ![](img/Formula_6.34_B17499.png)
    is the true label of a data point and ![](img/Formula_6.35_B17499.png) is the
    corresponding prediction made by the model. As such, we compute this quantity
    with `jnp.clip(1 - jnp.multiply(y, preds), a_min=0)`, before which we call the
    `predict` function to obtain the values for ![](img/Formula_6.36_B17499.png),
    stored in the `preds` variable.'
  prefs: []
  type: TYPE_NORMAL
- en: If you are familiar with the NumPy equivalent, you might have noticed that these
    functions that we are using, `clip` and `multiply`, have an identical interface
    in NumPy; again, JAX tries to make the transition for NumPy users as seamless
    as possible. Finally, we compute the mean of this max term across all training
    examples with the `mean` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Believe it or not, we have successfully implemented our linear model! With
    a specific value for `w`, we will be able to predict what the label of a data
    point is using the `predict` function and compute our loss with the `loss` function.
    At this point, we can try out different values for `w`; as a common practice,
    we will initialize `w` to be random numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The last step of our training procedure is to find the best value for `w` that
    minimizes the loss, which we will implement next.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent with automatic differentiation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall that in the gradient descent algorithm, we compute the gradient of the
    loss function for the current value of `w` and then adjust `w` by subtracting
    a fraction of the gradient (effectively moving in the opposite direction of the
    gradient).
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, in the past, we would need to derive the gradient in closed form and
    implement it in our code. With automatic differentiation, we simply need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`loss` is the function we implemented in the last subsection. Here we are using
    the `grad` function from JAX to obtain the *gradient function* of `loss`. This
    is to say that if we were to call `loss_grad`, the gradient function, on a given
    value of `w` using `loss_grad(w)`, we would obtain the gradient of the `loss`
    function at `w`, in the same way `loss(w)` gives us the loss itself at `w`.'
  prefs: []
  type: TYPE_NORMAL
- en: This is a so-called *function transformation*, which takes in a function in
    Python (`loss` in our case) and returns a related function (`loss_grad`) that
    could then be called on actual inputs to compute quantities that we are interested
    in. Function transformations are a powerful interface that makes it easy to work
    with functional logic and are the main way in which you use JAX. We will see that
    all other functionalities offered by JAX discussed in this chapter are function
    transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our gradient function in hand, we now implement gradient descent using
    a simple `for` loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: If you are looking at the Google Colab notebook, you might see some other book-keeping
    code that is used to show a real-time progress bar, but the preceding code is
    the main logic of gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: 'At each iteration of the `for` loop, we compute the gradient of the loss function
    at the current value of `w`, and then use that gradient to adjust the value of
    `w`: `w = w - lr * grads`. How big of a step we will take in the opposite direction
    of the gradient is controlled by the `lr` variable, set to `0.01`. This variable
    is typically referred to as the **learning rate** in the language of machine learning.'
  prefs: []
  type: TYPE_NORMAL
- en: After the adjustment, we will check to see whether this new value of `w` does,
    in fact, minimize our loss at 0, in which case we simply exit the `for` loop with
    the `break` statement. If not, we repeat this process until some termination condition
    is satisfied; in our case, we simply say that we only do this for at most 200
    times (specified by the `n_iters` variable).
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run the code, we will observe that as the `for` loop progresses, the
    value of the loss decreases for each adjustment. At the end of the loop, our loss
    decreases to a small number: roughly 0.013\. Using Matplotlib, we could quickly
    sketch out the progression of this decrease with `plt.plot(losses)`, which produces
    the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – The decrease in loss of the linear model ](img/Figure_6.3_17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – The decrease in loss of the linear model
  prefs: []
  type: TYPE_NORMAL
- en: We see that with the random guess for the value of `w` at the beginning, we
    receive a loss of roughly 2.5, but using gradient descent, we have reduced it
    to almost zero. At this point, we can be confident that the value of `w` that
    we currently have after gradient descent fits our data well.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the training of our linear predictive model. A customary task
    that follows training a model is to examine the predictions made by the model
    across a whole feature space. This process helps ensure that our model is learning
    appropriately. When working in a low-dimensional space such as ours, we can even
    visualize this space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, our decision boundary is a straight line that (hopefully) separates
    the two classes that we''d like to perform classification: ![](img/Formula_6.37_B17499.png),
    where ![](img/Formula_6.38_B17499.png) and ![](img/Formula_6.39_B17499.png) are
    coordinates of points within our two-dimensional space. As such, we can draw this
    line by first generating a fine-grid array for the *x* coordinates ![](img/Formula_6.40_B17499.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can derive the *y* coordinates ![](img/Formula_6.41_B17499.png) using the
    equation for the line we had previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Plotting this line together with the scattered points corresponding to our
    training data, we obtain the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – The decision boundary of the learned model ](img/Figure_6.4_17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – The decision boundary of the learned model
  prefs: []
  type: TYPE_NORMAL
- en: We see that our decision boundary nicely separates the two classes that we have.
    Now, when we'd like to make a prediction regarding which class an unseen point
    belongs to, we simply put its features through our `predict` function and look
    at the sign of the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'And that is all there is to it! Of course, when we work with more complicated
    models, other more specialized considerations need to be made. However, the high-level
    process remains unchanged: designing a model and its loss function and then using
    gradient descent (or some other loss minimization strategy) to find the optimal
    combination of parameters. We have seen that using the automatic differentiation
    module in JAX, we were able to do this easily with minimal code while avoiding
    the hairy math that is usually involved with deriving the gradient of a loss function.'
  prefs: []
  type: TYPE_NORMAL
- en: With that said, the benefits that JAX offers don't stop there. In our next section,
    we will see how to make our current code more efficient by using JAX's internal
    **Just-In-Time** (**JIT**) **compiler**.
  prefs: []
  type: TYPE_NORMAL
- en: Just-In-Time compilation for improved efficiency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have learned from the last chapter, JIT compilation allows a piece of
    code that is expected to run many times to be executed more efficiently. This
    process is specifically useful in machine learning where functions such as the
    loss or the gradient of the loss of a model need to be computed many times during
    the loss minimization phase. We hence expect that by leveraging a JIT compiler,
    we can make our machine learning models train faster.
  prefs: []
  type: TYPE_NORMAL
- en: You might think that to do this, we would need to hook one of the JIT compilers
    we considered in the last chapter into JAX. However, JAX comes with its own JIT
    compiler, which requires minimal code to integrate in an existing program. We
    will see how to use it by modifying the training loop we made in the last section.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we reset the ![](img/Formula_6.10_B17499.png) parameters of our models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the way we will integrate the JIT compiler into our program is to point
    it to the gradient of our loss function. Remember that we can reap the benefits
    of JIT compilation by applying it to a piece of code that we expect to execute
    many times, which we have mentioned is exactly the case for the loss gradient.
    Recall that we were able to derive this gradient in the last section using the
    `grad` function transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'To obtain the JIT-compiled version of this function, we can follow a very similar
    process, in which we transform this loss gradient function using `jit`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Amazingly, that is all we need to do. Now, if we run the same training loop
    we currently have, with this one line changed, we will obtain the same result,
    including the learning curve and the decision boundary we plotted earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, upon inspecting the running time of the training loop, we will notice
    that the training loop is now much more efficient: within our Google Colab notebook,
    the speed goes from roughly 77 iterations to 175 iterations per second! Note that
    although these numbers might vary across different runs of the code, the improvement
    should stay apparent. While this speedup might not be meaningful to us and our
    simple linear model, it will prove useful for more heavyweight models such as
    neural networks, with millions to billions of parameters, which take weeks and
    months to train.'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, this speedup was achieved with a single line of code changed using
    the function transformation, `jit`. This means that however complicated the functions
    you are working with are, if they are JAX-compatible, you can simply pass them
    through `jit` to enjoy the benefits of JIT compilation.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we have explored two powerful techniques to accelerate our machine
    learning pipeline: automatic differentiation and JIT compilation. In the next
    section, we will look at a final valuable feature of JAX – automatic vectorization.'
  prefs: []
  type: TYPE_NORMAL
- en: Automatic vectorization for efficient kernels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might remember from our discussions on NumPy that the library is efficient
    at applying numerical operations to all elements in an array or the elements along
    specific axes. By exploiting the fact that the same operation is to be applied
    to multiple elements, the library optimizes low-level code that performs the operation,
    making the computation much more efficient than doing the same thing via an iterative
    loop. This process is called **vectorization**.
  prefs: []
  type: TYPE_NORMAL
- en: When working with machine learning models, we would like to go through a procedure
    of vectorizing a specific function, rather than looping through an array or a
    matrix, to gain performance speedup. Vectorization is typically not easy to do
    and might involve clever tricks to rewrite the function that we'd like to vectorize
    into another form that admits vectorization easily.
  prefs: []
  type: TYPE_NORMAL
- en: JAX addresses this concern by providing a function transformation that automatically
    vectorizes a given function, even if the function is only designed to take in
    single-valued variables (which means traditionally, an iterative loop would be
    necessary). In this section, we will see how to use this feature by going through
    the process of kernelizing our predictive model. First, we need to briefly discuss
    what kernels are as well as their place in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Data that is not linearly separable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember that the predictive model we currently have assumes that the target
    we'd like to predict for can be expressed as a linear combination of the data
    features; the model is thus a linear one. This assumption is quite restrictive
    in practice, as data can present highly nonlinear and still meaningful patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here we use the `make_moons` function from the same `datasets`
    module of scikit-learn to generate another toy dataset and visualize it, again
    using a scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – A linearly non-separable dataset ](img/Figure_6.5_17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – A linearly non-separable dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, this is a binary classification problem where we need to distinguish
    between the yellow and the dark blue points. As you can see, these data points
    are not linearly separable since, unlike our previous dataset, a straight line
    that perfectly separates the two classes here does not exist. In fact, if we were
    to try to fit a linear model to this data, we would obtain a model with a relatively
    high loss (roughly 0.7) and a decision boundary that is clearly unsuitable, as
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Fitting a linear model on nonlinear data ](img/Figure_6.6_17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Fitting a linear model on nonlinear data
  prefs: []
  type: TYPE_NORMAL
- en: How, then, could we modify our current model so that it can handle nonlinear
    data? A typical solution in these kinds of situations is the kernel method in
    machine learning, which we will briefly discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: The kernel method in machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Without going into much technical detail, the kernel method (sometimes referred
    to as the **kernel trick**) refers to the process of transforming a low-dimensional
    dataset, such as ours, that is nonlinear to higher dimensions, with the hope that
    in higher dimensions, linear hyperplanes that separate the data will exist.
  prefs: []
  type: TYPE_NORMAL
- en: To take a data point to higher dimensions, additional features are created from
    the features already included in the original data. In our running example, we
    are given the *x* and *y* coordinates of the data points; hence, our data is two-dimensional.
    A common way to compose more features in this case is to compute polynomials of
    these coordinates up to a degree, *d*. For instance, polynomials that are up to
    degree 2 for features *x* and *y* include *xy* and ![](img/Formula_6.42_B17499.png).
    The larger *d* is, the more expressiveness we will gain from these new features
    we are creating, but we will also incur a higher computational cost.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that these polynomial features are nonlinear, which motivates their use
    in helping us find a model that separates our nonlinear data. With these new,
    nonlinear features in hand, we will then fit our predictive model on this bigger,
    more expressive dataset we just engineered. Note that while the predictive model
    remains in the same form as the linear parameters ![](img/Formula_6.43_B17499.png),
    the data features that the model learns from are nonlinear, so the resulting decision
    boundary will also be nonlinear.
  prefs: []
  type: TYPE_NORMAL
- en: I mentioned earlier that the more expressive we want the features we are synthetically
    creating to be, the more computational costs we will incur. Here, kernels are
    a way to manipulate the data points so that we can interact with them in higher
    dimensions without having to explicitly compute the high-dimensional features.
    In essence, a kernel is simply a function that takes in a pair of data points
    and returns the inner product of some transformation of the feature vectors. This
    is just to say that it returns a matrix that represents the two data points in
    higher dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will use one of the most popular kernels in machine learning:
    the radial basis function, or RBF, kernel, which is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_6.44_B17499.png)'
  prefs: []
  type: TYPE_IMG
- en: where *x* and *x'* are the low-dimensional feature vectors of the two data points,
    and *l* is commonly referred to as the length scale, a tunable parameter of the
    kernel. There are many benefits to using the RBF kernel that we won't be going
    into in much detail here, but on the highest level, RBF can be regarded as an
    infinite-dimensional kernel, which means that it will provide us with a high level
    of expressiveness in the features we are computing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The new model using this kernel method, modified from our previous linear model,
    now makes the following assumption for each pair of feature vector *x* and label
    *y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_6.45_B17499.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where ![](img/Formula_6.46_B17499.png) is the *i*-th data point in the training
    set and ![](img/Formula_6.47_B17499.png) are the model parameters to be optimized.
    While this assumption is written to contrast the linear assumption that we had
    earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_6.48_B17499.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/Formula_6.49_B17499.png) now refers to the entire feature vector of
    a data point, not the *i*-th feature of a data point. Further, we now have *n*
    terms of ![](img/Formula_6.50_B17499.png), as opposed to 2 (or *d*) terms of ![](img/Formula_6.51_B17499.png)
    as before.'
  prefs: []
  type: TYPE_NORMAL
- en: We see the role of the kernel here is to transform the feature of *x* by computing
    the inner product of its features and those of each of the data points in the
    training set. The output of the kernel is then used as the new features to be
    multiplied by the model parameters ![](img/Formula_6.52_B17499.png), whose values
    will be optimized as part of the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now implement this kernel in our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we are setting the length scale at 0.3, but feel free to play around with
    this value and observe its effect on the learned model afterward. The `rbf_kernel`
    function takes in `x` and `z`, two given data points. The code inside the function
    is self-explanatory: we compute the squared norm of the difference between `x`
    and `z` (`linalg.norm` in JAX follows the same API as in NumPy), divide it by
    `lengthscale`, and use its negative as the input of the natural exponential function,
    `exp`.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we need to implement the new version of the `predict` function for our
    kernelized model, which we will see how to do in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic vectorization for kernelized models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You might remember that the job of the `prediction` function is to implement
    the assumption our model is making, which in this case is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_6.53_B17499.png)'
  prefs: []
  type: TYPE_IMG
- en: The challenge here is in the sum ![](img/Formula_6.54_B17499.png), where ![](img/Formula_6.55_B17499.png),
    ![](img/Formula_6.56_B17499.png), …, ![](img/Formula_6.57_B17499.png) are the
    individual data points in our training set. In a naïve implementation, we would
    iterate through each data point ![](img/Formula_6.58_B17499.png), compute ![](img/Formula_6.59_B17499.png),
    multiply it by ![](img/Formula_6.60_B17499.png), and finally, add these terms
    together. Computing the inner product of two vectors is generally a computationally
    expensive operation, so repeating this computation *n* times would be prohibitively
    costly.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make this kernelized model more practical, we''d like to *vectorize* this
    procedure. Normally, this would require reimplementing our kernel function using
    various tricks to enable vectorization. However, with JAX, we can effortlessly
    obtain the vectorized version of the `rbf_kernel` function using the `vmap` transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Don't let the last line of code intimidate you. Here we are simply getting the
    vectorized form of our kernel function along the first axis (remember the kernel
    has two inputs) with `vmap(kernel, (0, None)`, and then vectorizing that very
    vectorized-along-the-first-axis kernel along the second axis with `vmap(vmap(kernel,
    (0, None)), (None, 0))`. Finally, we derive the JIT-compiled version of the function
    with `jit`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Aside from being a concise one-liner, this code perfectly illustrates why function
    transformation (JAX''s design choice) is so useful: we can compose different function
    transformations in a nested way, repeatedly calling a transformation on the output
    of another transformation; in this case, we have the JIT-compiled function of
    a twice-vectorized function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this vectorized kernel function composed, we are now ready to implement
    the corresponding `predict` and `loss` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Of note here is the `predict` function, where we are calling `vec_kernel(X,
    X_test)` to compute the vector of ![](img/Formula_6.61_B17499.png) via vectorization.
    Remember that without `vmap`, we would need to either iterate through the individual
    data points (for example, using a `for` loop), which is specifically more inefficient,
    or rewrite our `rbf_kernel` function so that the function itself facilitates the
    vectorization. In the end, we simply compute the dot product between the returned
    output and the vector of the parameters we'd like to optimize, `alphas`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our loss function stays the same. We now can apply the same training procedure
    as we have before, this time on the `alphas` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also plot out the progression of the loss throughout this training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – The decrease in loss of the kernelized model ](img/Figure_6.7_17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – The decrease in loss of the kernelized model
  prefs: []
  type: TYPE_NORMAL
- en: We see a steady decrease in loss, indicating that the model is fitting its parameters
    to the data. In the end, our loss is well below `0.7`, which was the loss of the
    purely linear model we previously had.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we''d like to visualize what our model has learned. This is a bit
    more involved as we no longer have a nice linear equation that we can translate
    into a line as before. Instead, we visualize the predictions made by the model
    itself. In the following figure, we show these predictions made on a fine grid
    that spans across our training data points, where the colors show the predicted
    classes, and the hues show the confidence in the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Predictions made by the learned kernelized model ](img/Figure_6.8_17499.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Predictions made by the learned kernelized model
  prefs: []
  type: TYPE_NORMAL
- en: We see that the model was able to identify the nonlinear pattern in our data,
    indicated by the yellow and dark blue blobs lying right in the center of the moons.
    As such, we have successfully modified our predictive model to learn from nonlinear
    data, which was done by using a kernel to create nonlinear features and JAX's
    automatic vectorization transformation to accelerate this computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the speed we achieve is roughly 160 iterations per second. To once again
    see how big a speedup the JIT compiler offers our program, we remove the `jit()`
    function calls from the implementation of our model and rerun the code with the
    following changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, the speedup is even more impressive: without JIT, the speed drops
    to 47 iterations per second.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This concludes our discussion on JAX and some of its main features. However,
    there are other more advanced tools included in JAX that we didn''t cover but
    could prove useful in your machine learning applications such as asynchronous
    dispatch, parallelization, or computing convolutions. Information on these features
    may be found on the documentation page: [https://jax.readthedocs.io/en/latest/](https://jax.readthedocs.io/en/latest/).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JAX is a Python- and NumPy-friendly library that offers high-performance tools
    that are specific to machine learning tasks. JAX centers its API around function
    transformations, allowing users, in one line of code, to pass in generic Python
    functions and receive transformed versions of the functions that would otherwise
    either be expensive to compute or require more advanced implementations. The syntax
    of function transformations also enables flexible and complex compositions of
    functions, which are common in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we have seen how to utilize JAX to compute the gradient
    of machine learning loss functions using automatic differentiation, JIT-compile
    our code for further optimization, and vectorize kernel functions via a binary
    classification example. However, these tasks are present in most use cases, and
    you will be able to seamlessly apply what we have discussed here to your own machine
    learning needs.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have reached the end of the first part of this book, in which
    we discuss Python-native and various other techniques to accelerate our Python
    applications. In the second part of the book, we examine parallel and concurrent
    programming, the techniques of which allow us to distribute computational loads
    across multiple threads and processes, making our applications more efficient
    on a linear scale.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the main components of a machine learning pipeline?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is the loss function of a machine learning model typically minimized and
    how does JAX help with this process?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can the predictive model used in this chapter handle nonlinear data and
    how does JAX help with this process?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear models in machine learning: [https://blog.dataiku.com/top-machine-learning-algorithms-how-they-work-in-plain-english-1](https://blog.dataiku.com/top-machine-learning-algorithms-how-they-work-in-plain-english-1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The JAX ecosystem: [https://moocaholic.medium.com/jax-a13e83f49897](https://moocaholic.medium.com/jax-a13e83f49897)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A brief tutorial that further explores JAX: [https://colinraffel.com/blog/you-don-t-know-jax.html](https://colinraffel.com/blog/you-don-t-know-jax.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
