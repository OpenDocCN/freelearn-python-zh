<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-333"><a id="_idTextAnchor340"/>14</h1>
<h1 id="_idParaDest-334"><a id="_idTextAnchor341"/>Building a Data Pipeline in PyCharm</h1>
<p>The term <em class="italic">data pipeline</em> generally denotes a step-wise procedure that entails collecting, processing, and analyzing data. This term is widely used in the industry to express the need for a reliable workflow that takes raw data and converts it into actionable insights. Some data pipelines work at massive scales, such as a <strong class="bold">marketing technology</strong> (<strong class="bold">MarTech</strong>) company ingesting millions of data points from Kafka streams, storing them in large data stores such as <strong class="bold">Hadoop</strong> or <strong class="bold">Clickhouse</strong>, and then cleansing, enriching, and visualizing that data. Other times, the data is smaller but far more impactful, such as the project we’ll be working on in this chapter.</p>
<p>In this chapter, we will learn about the following topics:</p>
<ul>
<li>How to work with and maintain datasets</li>
<li>How to clean and preprocess data</li>
<li>How to visualize data</li>
<li>How to utilize <strong class="bold">machine </strong><strong class="bold">learning</strong> (<strong class="bold">ML</strong>)</li>
</ul>
<p>Throughout this chapter, you will be able to apply what you have learned about the topic of scientific computing so far to a real project with PyCharm. This serves as a hands-on discussion to conclude this topic of working with scientific computing and data science projects.</p>
<p><a id="_idTextAnchor342"/>I want to specifically point out that I am heavily leveraging the text, code, and data from the first edition, which was written by a different author, Quan Nguyen. In this second edition, my main job was to update the existing content. Quan’s treatment in this chapter was excellent, so most of what I did to update this chapter was use the newer version of PyCharm, update the libraries used to the latest versions, and then re-write this chapter in my own words so that the writing style matches the rest of this book. There is no way I could have pulled this off without Quan’s original work and I wanted to tip my hat to the original Python data science kung fu master.</p>
<h1 id="_idParaDest-335"><a id="_idTextAnchor343"/>Technical requirements</h1>
<p>To proceed through this chapter, you will need the following:</p>
<ul>
<li>Anaconda, which is a Python distribution tailored to data science workloads. You can find it, along with installation instructions for your OS, at <a href="https://anaconda.com">https://anaconda.com</a>.</li>
<li>Likewise, instead of the usual <code>pip</code>, I’ll be leveraging <code>conda</code>, which is Anaconda’s package manager. It is installed alongside Anaconda.</li>
<li>An installed and working copy of PyCharm. Its installation was covered in <a href="B19644_02.xhtml#_idTextAnchor028"><em class="italic">Chapter 2</em></a>, <em class="italic">Installation and Configuration</em>, in case you are jumping into the middle of this book.</li>
<li>This book’s sample source code from GitHub. We covered cloning the code in <a href="B19644_02.xhtml#_idTextAnchor028"><em class="italic">Chapter 2</em></a>, <em class="italic">Installation and Configuration</em>. You’ll find this chapter’s code at <a href="https://github.com/PacktPublishing/Hands-On-Application-Development-with-PyCharm---Second-Edition/tree/main/chapter-14">https://github.com/PacktPublishing/Hands-On-Application-Development-with-PyCharm---Second-Edition/tree/main/chapter-14</a>.</li>
</ul>
<h1 id="_idParaDest-336"><a id="_idTextAnchor344"/>Working with datasets</h1>
<p>Datasets are the<a id="_idIndexMarker1183"/> backbone of any data science project. With a good, well-structured dataset, we have the opportunity to explore, ideate, and discover important insights from the data. The terms <em class="italic">good</em> and <em class="italic">well-structured</em> are key. In the real world, this rarely happens by accident. I am the lead developer on a project that does data science every day. We ingest diagnostic, utilization, and performance data from various hardware platforms such as storage arrays, switches, virtualization nodes (such as VMware), backup devices, and more. We collect it for the entire enterprise; every device in every data center. Our software then turns that raw data into visualizations that provide insights, allowing organizations to effectively manage their IT estate through consolidating health monitoring, utilization and performance reporting, and capacity planning.</p>
<p>I’ve been at it for 10 years now and we’re always looking to support new devices and systems. Our challenge, though, is getting the data we need. When I started 10 years ago, getting data out of a NetApp storage array was very hard because its diagnostic data is dumped as unstructured text. Contrast that with more modern arrays, which dump data in XML or JSON, or even better, have their own SDKs for interfacing with hardware and extracting the data we need.</p>
<p>A great deal of effort goes into taking data from various sources and working to mold the raw data into something useful. Sometimes it’s easy, and sometimes it is very difficult. Poorly formatted data can lead to erroneous conclusions and false insights.</p>
<p>A great cautionary tale comes from a large shoe manufacturer. About 20 years ago, I worked for a company that sold software designed to manage factory production. We consulted with the shoe company and told them exactly how to model their data for the best results. They ignored us and <a id="_idIndexMarker1184"/>went a different way. We told them it wouldn’t work. They thanked us for our input. Their projections were galactically wrong, so they did what any big company with boards and shareholders would do – they blamed the software. Our CEO did the circuit on the business shows, but the damage was done. Our company stock tanked and a lot of people lost their jobs that year, including me. To this day, I won’t wear their shoes. Bad data can cost livelihoods, reputations, and, beyond the context of shoes, even lives. We must have tools and processes at our disposal that help us get things right.</p>
<p>Let’s go over a few steps of that process.</p>
<h1 id="_idParaDest-337"><a id="_idTextAnchor345"/>Starting with a question</h1>
<p>Everything in science starts with a <a id="_idIndexMarker1185"/>question. For our purposes, we’ll consider two possible scenarios:</p>
<ul>
<li>We have a specific question in mind and we need to collect and analyze appropriate data to answer that question</li>
<li>We already have data, and during exploration, a question has arisen</li>
</ul>
<p>In our case, we’re going to recreate the data analysis phase of a potentially important breakthrough in the field of medical diagnosis. I’ll be presenting an example from Kaggle taken from a paper titled <em class="italic">High-accuracy detection of early Parkinson’s Disease using multiple characteristics of finger movement while typing,</em> which was conducted by Warwick Adams in 2017. You’ll find the full study paper and the dataset links in the <em class="italic">Further reading</em> section of this chapter.</p>
<p class="callout-heading">Note</p>
<p class="callout">Kaggle is an online data community designed for data scientists and ML engineers. The site provides competitions, datasets, playgrounds, and other educational activities to promote the growth of data science, both in academia and the industry. More information about the website can be found on its home page: <a href="https://www.kaggle.com/">https://www.kaggle.com/</a>.</p>
<p><strong class="bold">Parkinson’s Disease</strong> (<strong class="bold">PD</strong>) is a condition that affects<a id="_idIndexMarker1186"/> the brain and causes problems with movement. It’s a progressive disease, which means it gets worse over time. More than 6 million people around the world have this disease. In PD, a specific type of brain cell that produces a chemical called <em class="italic">dopamine</em> starts to die<a id="_idIndexMarker1187"/> off. This leads to a variety of symptoms, including difficulty with movement and other non-movement-related issues.</p>
<p>At the time of writing, doctors don’t have a definite test to diagnose PD, especially in the early stages when the symptoms might not be very obvious. This results in mistakes in diagnosing the disease, with up to 25% of cases being misdiagnosed by doctors who aren’t specialists in PD. Some people can have PD for many <a id="_idIndexMarker1188"/>years before they are correctly diagnosed.</p>
<p class="callout-heading">This leads us to a question…</p>
<p class="callout">How can we effectively and accurately diagnose PD using some test, metric, or diagnostic data point without specialized clinical training?</p>
<p>Adams suggested a test that uses computer typing data collected over time. Since typing involves fine motor movement, and since this fine motor movement is the first thing to go during the early onset of PD, Adams hoped it would be possible to use the mundane task of typing as a diagnostic tool. The researchers tested this method on 103 people; 32 of them had mild PD and the rest, the control group, didn’t have PD. The computer analysis of their typing patterns was able to tell the difference between the people with early-stage PD and those without it. This method correctly identified PD with 96% accuracy in detecting those who had it, and 97% accuracy in correctly identifying those who didn’t. This suggests that this method might be good at distinguishing between the two groups. Let’s see whether we can draw the same conclusion given their study’s data.</p>
<h2 id="_idParaDest-338"><a id="_idTextAnchor346"/>Archived user data</h2>
<p>Within this chapter’s source code, you’ll find a data science project called <code>pipeline</code>. The project contains a data folder<a id="_idIndexMarker1189"/> containing our datasets in two folders: <code>Archived users</code> and <code>Tappy Data</code>.</p>
<p>The data within the <code>Archived users</code> folder is in text file format and appears like this:</p>
<pre class="source-code">
BirthYear: 1952
Gender: Female
Parkinsons: True
Tremors: True
DiagnosisYear: 2000
Sided: Left
UPDRS: Don't know
Impact: Severe
Levodopa: True
DA: True
MAOB: False
Other: False</pre> <p>For the sake of immersion, let’s demystify this a little bit. These are the fields we have in each record:</p>
<ul>
<li><code>Birth Year: 1952</code>: This person was born in 1952.</li>
<li><code>Gender: Female</code>: This person identifies as female.</li>
<li><code>Parkinsons: True</code>: The person has been diagnosed with PD.</li>
<li><code>Tremors: True</code>: Tremors, which are involuntary shaking movements, are present in this person. Tremors are a common symptom of PD.</li>
<li><code>DiagnosisYear: 2000</code>: The person was diagnosed with PD in 2000.</li>
<li><code>Sided: Left</code>: The term <em class="italic">sided</em> in this context likely refers to the side of the body where the symptoms are more pronounced. In this case, the symptoms are more noticeable on the left-hand side of the body.</li>
<li><code>UPDRS: Don't know</code>: The <strong class="bold">Unified Parkinson’s Disease Rating Scale</strong> (<strong class="bold">UPDRS</strong>) is a tool that’s used to<a id="_idIndexMarker1190"/> assess the severity of PD. In this case, it’s not known what the specific UPDRS score is for<a id="_idIndexMarker1191"/> this individual.</li>
<li><code>Impact: Severe</code>: The impact of PD on this person’s life is considered severe, indicating that the symptoms have a significant effect on their daily activities and quality of life.</li>
<li><code>Levodopa: True</code>: Levodopa is a common medication used to manage the symptoms of PD. This person is taking Levodopa as part of their treatment.</li>
<li><code>DA: True</code>: <strong class="bold">Dopamine agonists</strong> (<strong class="bold">DAs</strong>) are another<a id="_idIndexMarker1192"/> type of medication used to manage Parkinson’s symptoms. This person is taking dopamine agonists as part of their treatment.</li>
<li><code>MAOB: False</code>: <strong class="bold">Monoamine oxidase B inhibitors</strong> (<strong class="bold">MOABs</strong>) are medications that can help manage Parkinson’s symptoms<a id="_idIndexMarker1193"/> by increasing dopamine levels in the brain. In this case, the person is not taking MAOBs.</li>
<li><code>Other: False</code>: If I were recreating this study for real, I would likely contact the original researcher if this data point wasn’t explained directly in the publication. Since I’m not, I’ll guess that it won’t affect our project. This likely refers to other specific medications or treatments for PD, indicating that the person is not undergoing any other specialized treatments beyond Levodopa and DAs.</li>
</ul>
<p>In summary, this individual was a 65-year-old woman at the time of the study who was diagnosed with PD in 2000. She experiences tremors, particularly on the left-hand side of her body. The impact of the disease on her life is severe. She is undergoing treatment with Levodopa and DAs to manage her symptoms, but she is not using MAOBs or any other specialized treatments. The specific severity of her symptoms, as measured by the UPDRS, is not provided in the given information.</p>
<p>The filenames in the folder are important. It isn’t ethical to publish <code>User_0EA27ICBLF.txt</code> file.</p>
<h2 id="_idParaDest-339"><a id="_idTextAnchor347"/>Tappy data</h2>
<p>The study methodology uses an <a id="_idIndexMarker1196"/>application called Tappy, which runs on Windows and records each subject’s keypress timing, along with positional data about each key. If you remember from our earlier discussion of the user data, the sidedness is a factor. The motor cortex is the region of the brain that is responsible for planning, controlling, and executing voluntary movements. It’s located in the cerebral cortex, which is the outermost layer of the brain.</p>
<p>The motor cortex, along with most of the rest of the brain, is divided into two hemispheres: the left hemisphere and the right hemisphere. Each hemisphere controls the voluntary movements of the opposite side of the body. In other words, the left hemisphere of the motor cortex controls movements on the right-hand side of the body, and the right hemisphere controls movements on the left-hand side of the body. Since this is true, knowing which side of the keyboard the keypress data is coming from is potentially of diagnostic importance.</p>
<p>Let’s open a Tappy dataset and see what’s inside:</p>
<div><div><img alt="Figure 14.1: I’ve opened the first file in the Tappy data folder and I can see it is tab-separated data" src="img/B19644_14_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.1: I’ve opened the first file in the Tappy data folder and I can see it is tab-separated data</p>
<p>I can see a warning at the top stating the file is large, by code editor standards, and that code insight is not available. This is spurious since the data folder in a scientific project in PyCharm is excluded from indexing and code insights anyway. You can safely ignore the warning.</p>
<p>I can also see that the file is <a id="_idIndexMarker1197"/>tab-delimited, which will play nicely in a data pipeline. It is always encouraging to see your data come to you in an easily parsable format. This is effectively structured data that would be suitable for import into a spreadsheet or database table. That isn’t necessarily what we will do with this data, but if we can do those kinds of imports with a given data file, we can pretty much do anything with the data.</p>
<p>As before, the filenames are significant. The first part of the file, delineated by an underscore, is the ID of the subject from the <code>Archived users</code> folder. We will be able to relate each subject’s performance data found in the <code>Tappy Data</code> folder with their demographical data found in the <code>Archived </code><code>users</code> folder.</p>
<p>The fields from the Tappy data file are as follows:</p>
<ul>
<li>Patient ID</li>
<li>The date of data collection</li>
<li>The timestamp of each keystroke</li>
<li>Which hand performed the keystroke (<em class="italic">L</em> for left and <em class="italic">R</em> for right)</li>
<li>Hold time (time between press and release, in milliseconds)</li>
<li>The transition from the last keystroke</li>
<li>Latency time (time from pressing the previous key, in milliseconds)</li>
<li>Flight time (time from releasing the previous key, in milliseconds)</li>
</ul>
<p>We have established that we have raw data in a workable format. Honestly, I’d call this a good day. It isn’t completely perfect; we’ll still <a id="_idIndexMarker1198"/>need to do some munging, but it’s a very good starting point.</p>
<p class="callout-heading">Jargon alert – munging</p>
<p class="callout"><strong class="bold">Munging</strong> is a colloquial term used in computer programming and data processing to describe the process of manipulating, cleaning, or transforming data from one format into another. It often involves <a id="_idIndexMarker1199"/>altering the structure or content of data to make it more suitable for a particular purpose, such as analysis, storage, or presentation. Munging can include activities such as the following:</p>
<p class="callout">- <strong class="bold">Data cleaning</strong>: Removing errors, inconsistencies, or irrelevant<a id="_idIndexMarker1200"/> information from datasets</p>
<p class="callout">- <strong class="bold">Data transformation</strong>: Changing the format, structure, or <a id="_idIndexMarker1201"/>representation of data to fit a specific requirement</p>
<p class="callout">- <strong class="bold">Data parsing</strong>: Extracting specific pieces <a id="_idIndexMarker1202"/>of information from a larger dataset</p>
<p class="callout">- <strong class="bold">Data aggregation</strong>: Combining multiple <a id="_idIndexMarker1203"/>sets of data into a single dataset</p>
<p class="callout">- <strong class="bold">Data filtering</strong>: Selecting or<a id="_idIndexMarker1204"/> excluding data based on certain criteria</p>
<p class="callout">- <strong class="bold">Data formatting</strong>: Changing the way data is<a id="_idIndexMarker1205"/> presented or encoded for compatibility with a certain system or software</p>
<p class="callout">The term <em class="italic">munging</em> is informal and comes from a blend of <em class="italic">mangle</em> and <em class="italic">modify</em>. It’s often used in a context where data needs to be <a id="_idIndexMarker1206"/>prepared or adjusted for analysis, integration, or some other data-related task.</p>
<p>We have a good start for our project, but <a id="_idIndexMarker1207"/>we have a question: can we detect early-onset PD using a typing test? We have raw data from a study that implemented such a typing test. We’re ready to roll up our sleeves and get into it!</p>
<h1 id="_idParaDest-340"><a id="_idTextAnchor348"/>Data collection</h1>
<p>We’re lucky. I’ve already found our data and included it for your consideration. In the real world, we would have needed to<a id="_idIndexMarker1208"/> have performed the normal step of data collection. While there are entire tomes on this topic – most 4-year scientific university degree programs focus heavily on this topic – I don’t plan on doing a deep dive here. However, I will at least give you an overview should you be new to what we’re trying to accomplish.</p>
<h2 id="_idParaDest-341"><a id="_idTextAnchor349"/>Downloading from an external source</h2>
<p>This is the case for our example dataset<a id="_idIndexMarker1209"/> since I downloaded it from Kaggle. When using a dataset downloaded from the internet, we should always make sure to check its copyright license. Most of the time, if it is in the public domain, we can freely use and distribute it without any worry. The example dataset we are using is an instance of this. On the other hand, if the dataset is copyrighted, we might still be able to use it by asking for permission from the author/owner of the dataset. I have found that, after reaching out to them via email and explaining how their datasets will be used in detail, dataset owners are often willing to share their data with others.</p>
<h2 id="_idParaDest-342"><a id="_idTextAnchor350"/>Manually collecting/web scraping</h2>
<p>If the data we want is available online but not formatted in tables or CSV files, most of the time, we need to collect it and<a id="_idIndexMarker1210"/> manually put it in a dataset ourselves. At most, we can write a web scraper that can send requests to the websites containing the target data and parse the returned HTML text. When you have to collect your data this way, it is also important to ensure that you are not doing it illegally. For example, it is against the law to have a program scrape data off some websites; sometimes, you might need to design the scraper so that only a certain number of requests are made at a given point. An example of this was when LinkedIn filed a lawsuit against many people who anonymously scraped their data in 2016. For this reason, it is always a good practice to find the terms of use for the data you are trying to collect this way.</p>
<h2 id="_idParaDest-343"><a id="_idTextAnchor351"/>Collecting data via third parties</h2>
<p>Students and researchers who <a id="_idIndexMarker1211"/>find that the data they are looking for in their study cannot be collected online often rely on third-party services to collect that data for them (for example, via crowd-sourcing). Amazon <strong class="bold">Mechanical Turk</strong> (<strong class="bold">MTurk</strong>) is one such service – you can enter any type of question to make a survey and MTurk will introduce that<a id="_idIndexMarker1212"/> survey to its users. Participants receive money for taking the survey, which is paid by the owner of the survey. This option is, again, specifically applicable when you want a representative dataset that is not available online anywhere.</p>
<h2 id="_idParaDest-344"><a id="_idTextAnchor352"/>Database exports</h2>
<p>This is most likely the case if you are <a id="_idIndexMarker1213"/>working with data from your company or organization. Luckily, PyCharm offers many useful features in terms of working with databases and their data sources. This process was discussed in <a href="B19644_11.xhtml#_idTextAnchor266"><em class="italic">Chapter 11</em></a>, and I highly recommend you check it out if you haven’t already.</p>
<h1 id="_idParaDest-345"><a id="_idTextAnchor353"/>Version control for datasets</h1>
<p>Since we took a quick little side<a id="_idIndexMarker1214"/> journey to discuss data collection, I hope you’ll indulge me once more while we talk about using data in a version control system such as Git. A little earlier, we opened a data file and PyCharm immediately complained about the size of the file. By modern standards, an 8 MB file isn’t very big. However, consider that most code files, PyCharm’s raison d’être, are on average well under 100K in size. If your files are very large, that’s a code smell and you should figure out what you’re doing wrong.</p>
<p>Here, we’re presenting PyCharm with a file that is about 8,000% bigger than what it is used to. Git is also primarily used to deal with small files coming out of an IDE. I’m bringing this up because there is somewhat of a crisis of reproducibility in the data science and scientific computing community. This is when one data team can extract a specific insight from a dataset but others cannot, even when using the same methods. Many instances of this are because the data used across these different teams is not compatible with each other. Some might be using the same but outdated dataset, while other datasets might have been collected from a different source.</p>
<p>Version control for datasets is an<a id="_idIndexMarker1215"/> important topic to consider. Git normally has a hard limit of 100 MB for any file, and I can tell you from experience there is an upper limit to the total size of your projects in total on GitHub. The same limitations exist in other version control systems. I used to teach game development with a tool called Unity 3D, and we were always struggling with these limitations since video games typically have very large assets in the projects that aren’t necessarily code, but that could benefit from revision control.</p>
<h2 id="_idParaDest-346"><a id="_idTextAnchor354"/>Using Git Large File Support</h2>
<p>Since the problem is endemic, Git (and others) have added the ability to track larger assets through <strong class="bold">Git Large File Support</strong> (<strong class="bold">Git LFS</strong>). When we add a file using Git LFS, the system will replace that file<a id="_idIndexMarker1216"/> with a pointer that simply references it. When the file is placed under version control, Git will only have a reference to the actual file, which is now stored in an external filesystem, possibly on another server. Git LFS allows us to apply version control to large files (in this case, datasets) with Git, without actually storing the files in Git.</p>
<p>This feature is normally installed with modern Git installers. <em class="italic">Figure 14</em><em class="italic">.2</em> shows me installing Git for Windows, where LFS is part of the default installation:</p>
<div><div><img alt="Figure 14.2: LFS is installed by default in Windows" src="img/B19644_14_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.2: LFS is installed by default in Windows</p>
<p>You can check your installation, regardless<a id="_idIndexMarker1217"/> of which OS you use, using the command line:</p>
<pre class="source-code">
git lfs version</pre> <p>My result from running this command in GitBash in Windows 11 is shown in <em class="italic">Figure 14</em><em class="italic">.3</em>:</p>
<div><div><img alt="Figure 14.3: If LFS is installed, it should tell you the version number" src="img/B19644_14_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.3: If LFS is installed, it should tell you the version number</p>
<p>The only reason I have Windows (besides Ghost Recon and Steam in general) is so I can use Microsoft Word to write this book. This wasn’t my idea. I was going to write the whole thing in raw LaTeX using vi. Not vim. Not neovim. Original gangsta vi, which I naturally would be compiling from source. My editor said no. She’s so super polite! If our roles were reversed, who knows<a id="_idIndexMarker1218"/> what would have been said? Anyway, the rest of my real work is<a id="_idIndexMarker1219"/> done on <strong class="bold">Pop_OS</strong>, which is a variant of Ubuntu Linux. When I throw the command into that environment, I get a less hospitable answer, as shown in <em class="italic">Figure 14</em><em class="italic">.4</em>:</p>
<div><div><img alt="Figure 14.4: My installer is not modern enough to have LFS pre-installed" src="img/B19644_14_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.4: My installer is not modern enough to have LFS pre-installed</p>
<p>I don’t have it! I have to install it using these commands:</p>
<pre class="source-code">
sudo apt update
sudo apt install git-lfs</pre> <p>With that done, I can test again:</p>
<div><div><img alt="Figure 14.5: Success! If you use some other Linux distribution, check your package management system for the git-lfs package if your installation lacks it" src="img/B19644_14_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.5: Success! If you use some other Linux distribution, check your package management system for the git-lfs package if your installation lacks it</p>
<p>The <code>git-lfs</code> package specific to your Linux distribution.</p>
<h3>Using Git LFS</h3>
<p>We’re getting a little ahead of <a id="_idIndexMarker1220"/>ourselves. If you’re going to follow along in this little sidetrack exercise, it would be best if you made a new folder somewhere outside of this book’s code repository. Let’s assume you’ve something like this in your OS’s terminal:</p>
<pre class="source-code">
cd ~/
mkdir git-lfs-test
cd git-lfs-test
git init</pre> <p>This series of commands will work in any of the popular OSs (Windows, macOS, or Linux). If you are using Windows, this series of commands can be run in PowerShell and assumes you have the Git client for Windows installed. The installer is available at <a href="https://git-scm.com/downloads">https://git-scm.com/downloads</a>.</p>
<p>The first command takes you to your <code>home</code> folder. The second creates a new folder called <code>git-lfs-test</code>. Next, we change the directory to the <code>git-lfs-test</code> folder we just made and we initialize a new repository. Now, we are ready to set up support for Git LFS.</p>
<p class="callout-heading">Don’t forget the chapter files are already in a Git repo</p>
<p class="callout">If you’re following along with this chapter’s source, don’t forget that the files are already in a Git repo. Creating a second repo within the existing repo won’t work. If you want to practice, make a completely separate folder outside of this book’s repo, and copy the project files into your folder. When you copy, you specifically want to avoid copying the <code>.git</code> folder into your target.</p>
<p>In our project, we’re going to use Git LFS to track files of a given extension, specifically text files with the <code>.txt</code> extension. Given these files are naturally plain text, you could get creative with the extension without affecting how they are used, but we’ll stick to just <code>.txt</code>. I’ll run this command in my terminal window:</p>
<pre class="source-code">
git lfs track "*.txt"</pre> <p>You can see my test run in <em class="italic">Figure 14</em><em class="italic">.6</em>:</p>
<div><div><img alt="Figure 14.6: Git LFS is now tracking all files with the .txt extension" src="img/B19644_14_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.6: Git LFS is now tracking all files with the .txt extension</p>
<p>To complete my LFS test, I’ll copy the file we examined earlier, <code>0EA27ICBLF_1607.txt</code>, from the <code>Tappy Data</code> folder into the <code>git-lfs-test</code> folder we’re using for the experiment. Just to be clear, <em class="italic">Figure 14</em><em class="italic">.7</em> shows my folder. We’re not doing this within any <a id="_idIndexMarker1221"/>sub-folder within this book’s code repository since creating a repository inside another repository is a big no<a id="_idTextAnchor355"/>-no:</p>
<div><div><img alt="Figure 14.7: I’ve copied 0EA27ICBLF_1607.txt into the git-lfs-test folder" src="img/B19644_14_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.7: I’ve copied 0EA27ICBLF_1607.txt into the git-lfs-test folder</p>
<p>Now, let’s add the newly copied text file to the repository:</p>
<pre class="source-code">
git add 0EA27ICBLF_1607.txt
git commit -m "adding big file"
git lfs ls-files</pre> <p>We covered the first two Git commands extensively in <a href="B19644_05.xhtml#_idTextAnchor112"><em class="italic">Chapter 5</em></a>. The last command will list all files being tracked by LFS in this repository. You can see my output in <em class="italic">Figure 14</em><em class="italic">.8</em>:</p>
<div><div><img alt="Figure 14.8: I can see that my text file is being tracked by LFS" src="img/B19644_14_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.8: I can see that my text file is being tracked by LFS</p>
<p>You now understand how to use Git LFS to track large files. If this were a real repository we were interested in <a id="_idIndexMarker1222"/>keeping, there’s one last thing we’d need to do. When we commanded Git to track our text files, a special file called <code>.gitattributes</code> was created on our behalf. We should add and commit that file:</p>
<pre class="source-code">
Git add .gitattributes
Git commit -m "Added .gitattributes to repo"</pre> <p>You’re all set! Let’s move on to our next formal step in the process of data analysis, which entails data cleansing and preprocessing.</p>
<h1 id="_idParaDest-347"><a id="_idTextAnchor356"/>Data cleansing and preprocessing</h1>
<p>As I mentioned earlier, we’ve been pretty lucky. Some of the data my team works with can be downright filthy. When we use<a id="_idIndexMarker1223"/> terms such as “dirty," “filthy,” and “cleansing” concerning data, what we’re talking about is addressing the format of the data, as well as the fitness of the data for processing. Data is only useful if it’s in a format we can work with. Structured data is what we always prefer.</p>
<p>Structured data refers to data that is split into identifiable fields. We’ve seen comma-separated and tab-separated text. Other examples of structured data include formats such as XML, JSON, Parquet, and HDF5. The first two, XML and JSON, are very common and have the advantage of being text formats. The latter two, Parquet and HDF5, are binary files and are specialized for storing larger datasets than would be comfortable when working with text. As we’ve seen, most tools, including PyCharm, buckle when they try to read very large text files. You<a id="_idIndexMarker1224"/> need tools specialized for working with large files if you want to peruse or edit them in place.</p>
<p>When I talk about dirty versus clean data, I’m looking for things such as missing or invalid field data. Recall our earlier data sample:</p>
<pre class="source-code">
BirthYear: 1952
Gender: Female
Parkinsons: True
Tremors: True
DiagnosisYear: 2000
Sided: Left
UPDRS: Don't know
Impact: Severe
Levadopa: True
DA: True
MAOB: False
Other: False</pre> <p>The <code>UPDRS</code> field is marked as unknown. This isn’t ideal. If the field is included, I’d like to see a value there. In this case, there’s no way to backfill it, but in a perfect world, that might be a candidate for an exercise in data cleansing.</p>
<h2 id="_idParaDest-348"><a id="_idTextAnchor357"/>A toxic data example peripherally involving ninjas</h2>
<p>The most relatable example I’ve ever <a id="_idIndexMarker1225"/>encountered with dirty – or in this case, toxic – data came from the corporate world rather than from a data science experiment. My company was consulting for a large aviation company, which is also a contractor for the US Department of Defense. I won’t be naming real names here because I am generally averse to government ninjas kicking in my door at 2 A.M., or worse, being flagged for a tax audit for what I’ve written here. So, we’ll keep this more or less theoretical.</p>
<p>The aviation company did business with lots of vendors, and when you do business with vendors at scale, it isn’t uncommon to see discounts applied to whatever you might be buying based on volume. If you or I go to Hammers R Us and buy a hammer, we might pay $12.95 for a hammer. But if the aviation company buys 5,000 hammers across many orders in a single quarter, they might get a discount of up to 60%. It is the aviation company’s job to track what they buy and from whom so that they can cash in on whatever bulk purchasing deals their company has negotiated with their suppliers.</p>
<p>When it’s time to run the discount reports, an accounting analyst might query a database filled with data entered by hundreds or even thousands of people working in the field on behalf of the aviation company. Since these operatives are human, their ability to enter clean, standardized data into a poorly designed system without any kind of validation is virtually nil. In this case, the software used for order entry allowed users to type the name of the company into a text field, which was never validated against any sort of approved vendors list.</p>
<p>One guy enters a purchase<a id="_idIndexMarker1226"/> with the vendor listed as “Hammers R Us.” Another enters it as "HRUS" (naturally that’s the stock symbol), and another as "H.R.U.S." Someone else misspells it as “Hammers Are Us” and yet another as “Hammers-R-Us.” Now, we have five different references to the same company, which dilutes our ability to figure out how much of a discount we can ask for. If there are 5 spellings and the purchase quantities are even across 5 orders, each order will only be for 1,000 hammers and our discount is only 20% instead of 60%. Our toxic data problem is costing the company serious money!</p>
<p>The aviation company hired my <a id="_idIndexMarker1227"/>company to do <strong class="bold">data cleansing</strong>. It was our job to clean all the data up and standardize all the references to Hammers R Us. The project was successful for us because all we had to do was charge the client a few dollars less than what they were losing, which was substantial. Then, we helped them fix their software to make it impossible to enter toxic data after that. It was a win for everyone! I even got a free hammer from Hammers R Us, at least in my version of the story that entails me not getting audited or visited by ninjas.</p>
<h2 id="_idParaDest-349"><a id="_idTextAnchor358"/>Exploratory analysis in PyCharm</h2>
<p>While data cleansing in a data science project isn’t usually financially profitable, it is a very necessary step. As you begin to examine your data for the first time, you will often hear this process referred to as <strong class="bold">exploratory data analysis</strong>, where we are exploring and analyzing the data at the same time. What <a id="_idIndexMarker1228"/>we’re doing though is taking stock to see what we can do with our data. It would be very difficult to perform a tabulation, such as computing sums, means, and standard deviations, without first making sure all our necessary data is both there and in a usable numerical format. We might also look for outliers. Maybe a hammer order was misentered and we have an order for a million hammers that was canceled via a separate transaction. These kinds of outliers would likely need to be removed before we begin our analysis in earnest.</p>
<p>In the case of our data, a few things are bothering me:</p>
<ul>
<li>The study says it examined 103 subjects; however, there are 277 user files in the <code>Archived users</code> folder. I suspect that not every user has matching collected data. We’ll need a way to check that each user in the <code>Archived users</code> folder has a related dataset in the <code>Tappy </code><code>Data</code> folder.</li>
<li>Our raw data is purely textual, which means when we import it into Python by reading the files, the data will be expressed as strings. This is not ideal for data analysis. I’d like numbers to be converted into number types, dates into date types, Booleans into Boolean, and so on.</li>
<li>The <code>Impact</code> column should be fully standardized to account for missing values in the data. Naturally, this applies to any other column where I can see or suspect the data might contain missing values.</li>
<li>We can convert some of the fields in the user datasets into a binary format to make analysis easier. Specific examples include <code>Parkinsons</code>, <code>Tremors</code>, <code>Levadopa</code>, <code>DA</code>, <code>MAOB</code>, and <code>Other</code>.</li>
<li>We can use a process called one-hot encoding to more easily process the fields labeled <code>Sided</code>, <code>UPDRS</code>, and <code>Impact</code>. I’ll go into detail on one-hot encoding once we’re ready to perform this process.</li>
</ul>
<p>This is just what I see at first glance. There may be other opportunities for cleansing that present themselves once we <a id="_idIndexMarker1229"/>get underway.</p>
<h3>Reading the data from text files</h3>
<p>Let’s look at what we need to do with<a id="_idIndexMarker1230"/> preprocessing our data. If you open the <code>data_clean.py</code> file, you’ll see our clean-up script, which uses the cell mode discussed in <a href="B19644_12.xhtml#_idTextAnchor298"><em class="italic">Chapter 12</em></a>. Our first cell handles our imports:</p>
<pre class="source-code">
import pandas as pd
import numpy as np
import os
import gc</pre> <p>If you’re following along with this chapter’s code, don’t forget to create a virtual environment using the <code>requirements.txt</code> file. Here we’re importing a few old friends. <code>numpy</code> and <code>pandas</code> are standard analysis libraries. The <code>os</code> package will be needed for working with the file directories, and the <code>gc</code> package allows us to control the <strong class="bold">garbage collection</strong> (<strong class="bold">GC</strong>) process. If you’ve<a id="_idIndexMarker1231"/> never heard of this before, it is because most programming languages, including Python, handle GC automatically. One common occurrence of GC happens when a variable, which will have memory allocated to store its value, goes out of scope and is no longer needed. In the C programming language, you would need to allocate that memory yourself before you could use the variable. When you were finished with the variable, you’d need to deallocate that memory “by hand.” If you didn’t, you’d be using more memory than you needed, and that’s the kind of thing that gets you uninvited to the Pi Day pizza party.</p>
<p>Most modern languages handle this allocation and deallocation automatically in a process called GC. However, there are times, especially when you are loading and manipulating large amounts of data, that it makes sense to take a more active role when the garbage gets taken out, which frees up memory for further exploits.</p>
<p>With our imports out of the way, let’s read some data with the following cell:</p>
<pre class="source-code">
#%% Read in data
user_file_list = os.listdir('data/Archived users/')
user_set_v1 = set(map(lambda x: x[5: 15], user_file_list)) # [5: 15] to return just the user IDs</pre> <p>The <code>os.listdir</code> method takes our <code>data/Archived users/</code> folder and gives us an iterable list of files from that folder. This is important because we need a list of the IDs for each user, which is contained in the filename.</p>
<p>We create a variable called <code>user_set_v1</code>, and we instantiate a set. In Python, a <code>set</code> is a built-in data type that represents an unordered collection of unique elements. This means that a set cannot contain duplicate values, and the order in which elements are stored is not guaranteed to be the same as the order in which they were added.</p>
<p>We fill this <code>set</code> with data using a <code>map</code> statement, which iterates over our list of files in the <code>Archived users</code> folder. For each iteration of the map, we use a lambda function to extract a portion of each filename in <code>user_file_list</code>. Specifically, it takes a substring from the 5th to the <a id="_idIndexMarker1232"/>15th character of each filename. This is intended to extract user IDs from the filenames. Next, we’ll need to do roughly the same thing to the <code>Tappy </code><code>Data</code> files:</p>
<pre class="source-code">
tappy_file_list = os.listdir('data/Tappy Data/')
user_set_v2 = set(map(lambda x: x[: 10], tappy_file_list)) # [: 10] to return just the user IDs</pre> <p>Now, we have two sets, one from the user files and one from the Tappy data files. We need to find the intersection between the sets.</p>
<p>In <strong class="bold">set theory</strong>, the term <em class="italic">intersection</em> refers to an operation that combines two sets to create a new set containing only the elements that are common to both of the original sets. The intersection of two sets, often denoted by the ∩ symbol, represents the overlap or shared elements between the sets.</p>
<p>Mathematically, if you have two sets, A and B, the intersection of A and B is a new set that contains all the elements that are both in set A and set B.</p>
<p>I know all you math geeks out there love your symbols, and I also know that your brains are wired to scan for patterns rather than word-for-word reading, so I’ll help you out. Symbolically, it is represented as A ∩ B = {x ∣ x ∈ A and x ∈ B}.</p>
<p>In the context of programming in Python, the <code>intersection()</code> method of <code>set</code> performs this mathematical operation. Given two sets, it returns a new set containing only the elements that exist in both sets.</p>
<p>For example, let’s say you have the following two sets:</p>
<ul>
<li>Set A = {1,2,3,4}</li>
<li>Set B = {3,4,5,6}</li>
</ul>
<p>The intersection of A and B would be A ∩ B = {3, 4} since 3 and 4 are in both sets. In our case, it is important to get the intersection because the study text stated it examined 103 subjects, yet there are 227 subjects listed in the <code>Archived users</code> folder. I could make a list, and then go through and visually compare the contents of the <code>Tappy Data</code> folder to make sure<a id="_idIndexMarker1233"/> everyone is accounted for, but that would be boring, time-consuming, and error-prone. I’ll just have Python do it for me:</p>
<pre class="source-code">
user_set = user_set_v1.intersection(user_set_v2)</pre> <p>Don’t you just love Python’s one-liners? Sure, there was some setup (hee hee, <code>intersection</code> method and we have our new set, which is all funk and no junk! Let’s see what we’ve got by printing out the length:</p>
<pre class="source-code">
print(len(user_set))</pre> <p>I’m going to run the first two cells using the green arrows indicated in <em class="italic">Figure 14</em><em class="italic">.9</em>:</p>
<div><div><img alt="Figure 14.9: I’m running the first two cells we’ve covered so far using the green arrows at the top of each cell" src="img/B19644_14_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.9: I’m running the first two cells we’ve covered so far using the green arrows at the top of each cell</p>
<p>The result of the run is<a id="_idIndexMarker1234"/> shown in <em class="italic">Figure 14</em><em class="italic">.10</em>:</p>
<div><div><img alt="Figure 14.10: I have a relatively clean list of users after our first steps of cleaning the data" src="img/B19644_14_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.10: I have a relatively clean list of users after our first steps of cleaning the data</p>
<p>We got 217 users with <a id="_idIndexMarker1235"/>correlated data between the two sets, so we’ve managed to eliminate 60 user files we aren’t going to use. The number doesn’t match the 103 subjects reported in the test, but that’s OK – the day is still young, and we might eliminate more later. Even if we don’t, there might be other reasons to eliminate properly matched data later on. Our new set can be used to iterate over data in either data folder since the filenames in both use the ID as a major part of the filename. This will be very useful in the next step in our data preparation process.</p>
<p>In <em class="italic">Figure 14</em><em class="italic">.10</em>, I’m clicking on the view link to see my list with the <strong class="bold">SciView</strong> panel. It isn’t particularly exciting since it’s just a list of IDs, but the ability to easily inspect as we work without performing <a id="_idIndexMarker1236"/>additional prints is very useful.</p>
<h3>Getting our data into a pandas DataFrame</h3>
<p>Our next cell contains code designed to<a id="_idIndexMarker1237"/> take our loaded dataset and pull that data into a pandas DataFrame. pandas is a library that allows for easy analysis of tabular data and even provides a lot of very useful methods for loading data directly into a DataFrame, which is a tabular structure within pandas. A DataFrame object is a lot like an in-memory spreadsheet without the editor. You can perform all kinds of calculations with minimal effort.</p>
<p>Let’s examine the code from the next cell:</p>
<pre class="source-code">
#%% Format into a Pandas dataframe</pre> <p>Don’t forget that <code>#%%</code> is a special formatting comment in PyCharm. It isn’t part of Python. We covered this back in <a href="B19644_13.xhtml#_idTextAnchor318"><em class="italic">Chapter 13</em></a>. These characters are used to split cells in our code, which allows us to use one script but operate step-wise from one cell to the next. At the end of the day, it is still a comment, so we should include some documentation to explain what is happening in the cell.</p>
<p>Next, we’ll create a function that reads the data from the files in the <code>Archived </code><code>users</code> folder:</p>
<pre class="source-code">
def read_user_file(file_name):
  f = open('data/Archived users/' + file_name)
  data = [line.split(': ')[1][:-1] for line in f.readlines()]
  f.close()
  return data</pre> <p>The function simply takes a filename as an argument and opens the file. It then reads the file line by line. For each line, we’re using the <code>split</code> string function to split the line into chunks as a list. This allows us to grab only the parts we need. As you may recall, a few lines of data for these files look like this:</p>
<pre class="source-code">
BirthYear: 1952
Gender: Female
Parkinsons: True</pre> <p>The separation between the field name and the data is a colon and a space (<code>: </code>). We’re using that as our splitter, so if you split <code>"BirthYear: 1952".split(': ')</code>, you’ll get back a list: <code>["BirthYear", "1952"]</code>. We don’t care about the field name right now, we care about the value. To get that, we grab <code>[1]</code>, which gives us <code>"1952"</code>, which is the value, but there is a newline character at the end of each line, and that was included in our split. The last thing we do, then, before moving on with the next iteration, is clear off the newline<a id="_idIndexMarker1238"/> character with the Python split operator, <code>[:-1]</code>, which effectively says “go to the end of the string,” as evidenced by the fact that the number is after the colon, “and slice off one character from the end,” as denoted by the negative number. Rather than using a loop, we’ve used list comprehension, which is an alternative way to iterate a list. These are generally more performant than a normal <code>for</code> loop. The result of the list comprehension is a new list that contains only the data we want.</p>
<p>The next few lines are setting us up for filling in a pandas DataFrame. First, we get a list of files in the <code>Archived </code><code>users</code> folder:</p>
<pre class="source-code">
files = os.listdir('data/Archived users/')</pre> <p>Next, we create a list of fields. We’ve already set up a function to rip the data out of the files without the field name. Ripping the names at the same time might add a lot of time since it is the same thing over and over; this is simply more efficient:</p>
<pre class="source-code">
columns = [
  'BirthYear', 'Gender', 'Parkinsons', 'Tremors', 'DiagnosisYear',
  'Sided', 'UPDRS', 'Impact', 'Levadopa', 'DA', 'MAOB', 'Other'
]</pre> <p>Next, we make an empty DataFrame as a starting point using our <code>columns</code> list. Think of this like making a new spreadsheet, and filling in the first row of your sheet with your column names:</p>
<pre class="source-code">
user_df = pd.DataFrame(columns=columns) # empty Data Frame for now</pre> <p>Next, let’s loop through <code>user_set</code>, which we created in the previous cell. Remember, this is the list of user IDs that have data in the <code>Tappy Data</code> folder. Recall that the structure of the filename for this file is the word <code>User</code> followed by an underscore followed by the user ID and appended with the <code>.txt</code> file extension:</p>
<pre class="source-code">
for user_id in user_set:
  temp_file_name = 'User_' + user_id + '.txt'</pre> <p>Next, we make sure that the file is there. It should be since we did our <code>set</code> operation earlier, but it is a good idea to check. If the file isn’t there, our analysis set will crash. This isn’t a big deal for a few hundred files, but it can be heartbreaking if you’re going through tens of thousands. Assuming the file is there, we read it into a variable called <code>temp_data</code> using the function we created earlier. Remember, that function returns a list of data values that look just like the cells in a row of a spreadsheet. Then, we insert that data into the DataFrame using the user ID as the index for the row:</p>
<pre class="source-code">
  if temp_file_name in files:
    temp_data = read_user_file(temp_file_name)
    user_df.loc[user_id] = temp_data</pre> <p>Naturally, we want to check, but<a id="_idIndexMarker1239"/> we don’t want every row – we just want the first few to make sure they are formatted as we expect:</p>
<pre class="source-code">
print(user_df.head())</pre> <p>When I run this cell, I get the following output:</p>
<div><div><img alt="Figure 14.11: My run of our latest cell shows we have a populated pandas DataFrame" src="img/B19644_14_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.11: My run of our latest cell shows we have a populated pandas DataFrame</p>
<p>Remember, you can view the DataFrame in <strong class="bold">SciView</strong> by clicking the <strong class="bold">View as DataFrame</strong> button indicated by the arrow in <em class="italic">Figure 14</em><em class="italic">.11</em>. Mine is shown in <em class="italic">Figure 14</em><em class="italic">.12</em>:</p>
<div><div><img alt="Figure 14.12: Viewing the DataFrame I created in the previous step is easy and colorful in PyCharm" src="img/B19644_14_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.12: Viewing the DataFrame I created in the previous step is easy and colorful in PyCharm</p>
<p>I can see from this that I still have<a id="_idIndexMarker1240"/> work to do. The diagnosis year is messy, as are a few of the other fields. Let’s keep chipping away at it.</p>
<h2 id="_idParaDest-350"><a id="_idTextAnchor359"/>Data cleansing</h2>
<p>Now that we can see our data in a tabular format, there are some ways we can improve the format of this data with the<a id="_idIndexMarker1241"/> express purpose of performing numerical analysis across any dimensions we might choose.</p>
<h3>Changing numeric data into actual numbers</h3>
<p>Our next cell contains a few lines <a id="_idIndexMarker1242"/>of code designed to convert numeric values into numeric types. Remember, everything is coming in as text and is treated like a string until you tell pandas otherwise. Here’s the code for the cell:</p>
<pre class="source-code">
#%% Change numeric data into appropriate format
# force some columns to have numeric data type
user_df['BirthYear'] = pd.to_numeric(user_df['BirthYear'], errors='coerce')
user_df['DiagnosisYear'] = pd.to_numeric(user_df['DiagnosisYear'], errors='coerce')</pre> <p>An application programmer would be tempted to process the data line by line and handle type conversions field by field. The neat thing about pandas is that once you have your data in a DataFrame, you can operate on entire rows and columns.</p>
<p>In this code, we’re doing just that. <code>BirthYear</code> and <code>DiagnosisYear</code> are being converted into numbers using the <code>pd.to_numeric</code> method. The second argument, <code>errors='coerce'</code>, will attempt to force a data conversion to a numeric type. If this is impossible, such as with a value of “<code>-------</code>” (a bunch of dashes), which we saw in the <code>NaN</code>, or “not a number.” While <code>NaN</code> isn’t computationally valuable, it does at least standardize all non-numeric values to just this one, which will make these rows easier to ignore should we choose.</p>
<p>The mention of <code>NaN</code> also indicates it’s time to bake some delicious bread in your mom’s tandoori oven. Some authors do Patreon, and I do bread, but it has to be your mom’s recipe. That means you have to call her and tell her you love her. Do it now, even if she doesn’t have a tandoori oven and can’t bake bread! I’ll wait.</p>
<p>While you were on the phone, I ran<a id="_idIndexMarker1243"/> the cell; my result is shown in <em class="italic">Figure 14</em><em class="italic">.13</em>:</p>
<div><div><img alt="Figure 14.13: The year fields are not actual numbers. Wherever there was invalid data, we now see a standardized value of nan" src="img/B19644_14_13.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.13: The year fields are not actual numbers. Wherever there was invalid data, we now see a standardized value of nan</p>
<h3>Binarizing data</h3>
<p>Any place where we can convert data that is essentially binary, we should. Within our data, gender is reported with two <a id="_idIndexMarker1244"/>possible values, male and female, representing a possibility of representing it in a binary format. Likewise, many of the fields are presented as binaries, as shown here:</p>
<pre class="source-code">
Parkinsons: True
Tremors: True
Levodopa: True
DA: True
MAOB: False
Other: False</pre> <p>In these cases, we just need to standardize the values as actual binaries, which may result in renaming or expanding our list of field names. Let’s look at the cell code:</p>
<pre class="source-code">
#%% "Binarize" true-false data
user_df = user_df.rename(index=str, columns={'Gender': 'Female'})
user_df['Female'] = user_df['Female'] == 'Female'
user_df['Female'] = user_df['Female'].astype(int)</pre> <p>In the preceding code, we<a id="_idIndexMarker1245"/> renamed the column in our DataFrame from <code>Gender</code> to <code>Female</code>. The second line changes the value in each row for the newly renamed column to the result of an expression comparing the current value versus the word <code>Female</code>. It either is or isn’t <code>Female</code>, so we get back a <code>True</code> or <code>False</code> value. The third line converts the Boolean type into an integer, making it more amenable to analysis.</p>
<p>Next, we’ll turn our attention to the previously listed columns and do the same conversion. This time, we’re checking for the word “True” in our expression. The value is either <code>True</code> or it isn’t, which results in a Boolean value:</p>
<pre class="source-code">
str_to_binary_columns = ['Parkinsons', 'Tremors', 'Levadopa', 'DA', 'MAOB', 'Other'] # columns to be converted to binary data
for column in str_to_binary_columns:
  user_df[column] = user_df[column] == 'True'
  user_df[column] = user_df[column].astype(int)</pre> <p>Running this code yields changes to our DataFrame, as shown in <em class="italic">Figure 14</em><em class="italic">.14</em>:</p>
<div><div><img alt="Figure 14.14: We’ve successfully binarized our fields" src="img/B19644_14_14.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.14: We’ve successfully binarized our fields</p>
<p>We can see that our fields are now binary numbers! This is going to make things easier later!</p>
<p>Let’s jump into the next cell since the<a id="_idIndexMarker1246"/> first part of the code is doing some more cleanup, similar to what we have done so far. In the first part of the cell, we are cleaning up the <code>Impact</code> field. We’re standardizing any value that isn’t <code>Mild</code>, <code>Medium</code>, or <code>Severe</code> as <code>None</code>:</p>
<pre class="source-code">
# prior processing for `Impact` column
user_df.loc[
  (user_df['Impact'] != 'Medium') &amp;
  (user_df['Impact'] != 'Mild') &amp;
  (user_df['Impact'] != 'Severe'), 'Impact'] = 'None'</pre> <p>Next, while staying in the same cell, we’re going to explore a powerful and popular technique that is used by an <a id="_idIndexMarker1247"/>ML algorithm when preparing data for analysis.</p>
<h3>One-hot encoding</h3>
<p>For some reason, when<a id="_idIndexMarker1248"/> I first heard the term <strong class="bold">one-hot encoding</strong>, I immediately thought of hot dogs and how I would love to <a id="_idIndexMarker1249"/>encode one with mustard and sweet relish on a nice steamed bun, or maybe the NaN y’all are doing to send me. For the record, I know that’s not how the bread is spelled, and I don’t care. The joke only works if I spell it incorrectly. I don’t know why I’m telling you that, but here we are.</p>
<p>One-hot coding is a technique that allows you to take data that isn’t inherently Boolean, and make it so. When I was in the market for a new Jeep Wrangler, there were only a few colors I considered:</p>
<ul>
<li>Firecracker Red</li>
<li>Ocean Blue Metallic</li>
<li>Mojito!</li>
<li>Hellayella</li>
</ul>
<p>There are more colors than that, but they are all boring variants of black, white, or gray. I can’t get an orange Jeep because people will think I went to Oklahoma State University, and we can’t have that. I can ignore those colors, leaving me with a list that will fit on the page. Now, let’s one-hot encode that list:</p>
<table class="No-Table-Style" id="table001-5">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Color_Firecracker_Red</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Color_Ocean_Blue_Metallic</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Color_Mojito</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Color_Hellayella</strong></p>
</td>
</tr>
</thead>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p>You can easily see how one-hot encoding works – it pivots the fields and then makes them binary. If you’re a relational database guru, you have probably just lost your lunch. Data scientists do things a little differently. In the one-hot encoded representation, each observation gets a “1” in the column corresponding to its category and a “0” in all other columns. This encoding ensures that the<a id="_idIndexMarker1250"/> categorical information is preserved in a way that ML algorithms can understand and use effectively. For the record, I went with <em class="italic">Hellayella</em> based on the idea that if I got my Jeep stuck somewhere inaccessible, such as the deserts of Big Bend National Park, or deep in the Piney Woods region of east Texas, the rescue helicopters would easily find my corpse.</p>
<p>One-hot encoding is commonly used for features such as categorical variables, which can’t be directly used as numerical inputs in many ML algorithms. It’s an important step in data preprocessing to convert such variables into a suitable format for training models.</p>
<p>Let’s go back to our code for the current cell. We’ve explained the first few lines, so let’s move on to setting up for one-hot encoding on several fields:</p>
<pre class="source-code">
to_dummy_column_indices = ['Sided', 'UPDRS', 'Impact'] # columns to be one-hot encoded</pre> <p>We’re going to encode these three columns. One of the columns under consideration is the <code>Impact</code> column, which we just standardized as a lead-in for this step. We’ll perform the one-hot encoding for all three columns here:</p>
<pre class="source-code">
for column in to_dummy_column_indices:
  user_df = pd.concat([
    user_df.iloc[:, : user_df.columns.get_loc(column)],
    pd.get_dummies(user_df[column], prefix=str(column)),
    user_df.iloc[:, user_df.columns.get_loc(column) + 1 :]
  ], axis=1)
print(user_df.head())</pre> <p>Within the loop, the code performs the following steps:</p>
<ol>
<li><code>user_df.iloc[:, : user_df.columns.get_loc(column)]</code>: Selects the columns to the left of the current column being processed. This preserves the columns before the <a id="_idIndexMarker1251"/>one being one-hot encoded.</li>
<li><code>pd.get_dummies(user_df[column], prefix=str(column))</code>: Applies one-hot encoding to the current column using the <code>pd.get_dummies()</code> method. It creates a DataFrame with binary columns representing the different categories in the column. The <code>prefix</code> parameter adds a prefix to the column names to indicate which original column they were derived from.</li>
<li><code>user_df.iloc[:, user_df.columns.get_loc(column) + 1 :]</code>: Selects the columns to the right of the current column being processed. This preserves the columns after the one being one-hot encoded.</li>
</ol>
<p>When fed into the <code>pd.concat</code> method, these steps effectively replace each of the three categorical columns with one-hot encoded binary columns while keeping the rest of the DataFrame intact. When you run the cell, you should see results like mine, as shown in <em class="italic">Figure 14</em><em class="italic">.15</em>:</p>
<div><div><img alt="Figure 14.15: I’ve scrolled to the right so that you can see the newly added one-hot encoded columns that were added" src="img/B19644_14_15.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.15: I’ve scrolled to the right so that you can see the newly added one-hot encoded columns that were added</p>
<p>One-hot encoding will have<a id="_idIndexMarker1252"/> added many new columns to the DataFrame, so you might need to scroll to the right to see them all.</p>
<h2 id="_idParaDest-351"><a id="_idTextAnchor360"/>Exploring the second dataset</h2>
<p>With our user data fairly well-cleaned<a id="_idIndexMarker1253"/> up and sitting in a pandas DataFrame, we are now ready to tackle the Tappy data. To keep things relatable, I’m going to arbitrarily pick one file from the <code>Tappy Data</code> set. Let’s look at the code in our next cell:</p>
<pre class="source-code">
#%% Explore the second dataset
file_name = '0EA27ICBLF_1607.txt'</pre> <p>As I said, I picked one arbitrary file to examine. We opened one of these files earlier and noted they were all in tab-separated format. pandas has a method that will easily read this file directly into a DataFrame. Despite the method being called <code>read_csv</code>, you get to specify a delimiter, which doesn’t have to be a comma. The method will read any kind of delimited file:</p>
<pre class="source-code">
df = pd.read_csv(
  'data/Tappy Data/' + file_name,
  delimiter = '\t',
  index_col = False,
  names = ['UserKey', 'Date', 'Timestamp', 'Hand', 'Hold time', 'Direction', 'Latency time', 'Flight time']
)</pre> <p>For our purposes, we don’t <a id="_idIndexMarker1254"/>need the <code>UserKey</code> field:</p>
<pre class="source-code">
df = df.drop('UserKey', axis=1)
print(df.head())</pre> <p>When we run this cell, we create a new DataFrame called <code>df</code>. Be sure to pick it from the console variables panel shown in <em class="italic">Figure 14</em><em class="italic">.16</em>:</p>
<div><div><img alt="Figure 14.16: Our new DataFrame can be viewed by clicking the View as DataFrame button" src="img/B19644_14_16.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.16: Our new DataFrame can be viewed by clicking the View as DataFrame button</p>
<h3>Formatting datetime data</h3>
<p>The next cell fixes our <code>datetime</code> data:</p>
<pre class="source-code">
#%% Format datetime data</pre> <p>This first line tries to force the <a id="_idIndexMarker1255"/>values in the <code>Date</code> column to be dates. If the coercion doesn’t work, we’ll see <code>NaT</code> (not a time), which is disappointing since there’s no food joke to be made. Next, we’ll do some more coercion on the <code>Hold time</code>, <code>Latency time</code>, and <code>Flight </code><code>time</code> fields:</p>
<pre class="source-code">
df['Date'] = pd.to_datetime(df['Date'], errors='coerce', format='%y%M%d').dt.date
# converting time data to numeric
for column in ['Hold time', 'Latency time', 'Flight time']:
  df[column] = pd.to_numeric(df[column], errors='coerce')</pre> <p>Any observations lacking time data should be dropped:</p>
<pre class="source-code">
df = df.dropna(axis=0)</pre> <p>Let’s print the result for inspection:</p>
<pre class="source-code">
print(df.head())</pre> <p>Let’s run it! My cell run results are shown in <em class="italic">Figure 14</em><em class="italic">.17</em>:</p>
<div><div><img alt="Figure 14.17: Our datetime data is now numeric and any observation with missing time data, being useless, has been dropped" src="img/B19644_14_17.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.17: Our datetime data is now numeric and any observation with missing time data, being useless, has been dropped</p>
<h3>Washing hands and fixing direction</h3>
<p>The next cell cleans up the<a id="_idIndexMarker1256"/> hand and direction columns:</p>
<pre class="source-code">
# cleaning data in Hand
df = df[
  (df['Hand'] == 'L') |
  (df['Hand'] == 'R') |
  (df['Hand'] == 'S')
]</pre> <p>This code uses a logical <code>OR</code> to filter out anything that doesn’t have a value of <code>L</code>, <code>R</code>, or <code>S</code>. Since it is presented as an <code>OR</code>, anything outside the three desirable possibilities will return as <code>false</code>, and be excluded.</p>
<p>Let’s do the same thing with direction, which has more possibilities:</p>
<pre class="source-code">
# cleaning data in Direction
df = df[
  (df['Direction'] == 'LL') |
  (df['Direction'] == 'LR') |
  (df['Direction'] == 'LS') |
  (df['Direction'] == 'RL') |
  (df['Direction'] == 'RR') |
  (df['Direction'] == 'RS') |
  (df['Direction'] == 'SL') |
  (df['Direction'] == 'SR') |
  (df['Direction'] == 'SS')
]</pre> <p>Of course, we’ll print the result:</p>
<pre class="source-code">
print(df.head())</pre> <p>Go ahead and run the cell. All rows <a id="_idIndexMarker1257"/>containing invalid data have been removed. This result isn’t as visual as most have been, so I don’t think we need a screenshot for this one.</p>
<h3>Summarizing data</h3>
<p>Our next cell provides an example of how to <a id="_idIndexMarker1258"/>summarize our data, which we have been working so hard to set up for analysis. We’re ready! Let’s try something simple. As usual, the first line of code in the cell just marks the beginning of the cell:</p>
<pre class="source-code">
#%% Group by direction (hand transition)</pre> <p>Recall that the data we have been working with so far is typing speed data for a specific subject at a given time. A subject (<code>User</code>) is simply a single data point within our first dataset, and we would like to combine the two datasets somehow, so we need a way to aggregate our current data into a single data point.</p>
<p>Since we are working with numerical data (typing time), we can take the average (mean) of the time data across different columns as a way to summarize the data of a given user. We can achieve this<a id="_idIndexMarker1259"/> with the <code>groupby()</code> function from pandas:</p>
<pre class="source-code">
 direction_grouped_df = df.groupby('Direction')[numeric_columns].mean()</pre> <p>Of course, we should print it:</p>
<pre class="source-code">
print(direction_grouped_df)</pre> <p>The result of the run is shown in <em class="italic">Figure 14</em><em class="italic">.18</em>. The code puts the results in a new DataFrame called <code>direction_group_df</code>, so be sure you select it as shown in the figure:</p>
<div><div><img alt="Figure 14.18: Hooray! We have our first calculated insight!" src="img/B19644_14_18.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.18: Hooray! We have our first calculated insight!</p>
<p>This is exciting! We have the<a id="_idIndexMarker1260"/> mechanics working, but now, we need to concentrate on making this work with many data files instead of just one.</p>
<h2 id="_idParaDest-352"><a id="_idTextAnchor361"/>Refactoring for scale</h2>
<p>Our exploration of the Tappy data has focused on one file to establish in an easily verifiable way that our code is working. We’ve<a id="_idIndexMarker1261"/> determined that it is, so now, we should refactor our code so that we can process thousands of files. To do this, we should consolidate some of our cells into a function. The code in the next cell is long but familiar since it is just all the code we’ve written so far combined into one function. If you’re an application developer, and you<a id="_idIndexMarker1262"/> understand the design principle known as the <strong class="bold">single responsibility principle</strong> (<strong class="bold">SRP</strong>), you know this is an antipattern. Remember, though, this isn’t application code. Nobody will run this beyond performing the analysis, so the rigors of SOLID principles that normally apply to software development are not observed<a id="_idIndexMarker1263"/> in data science work.</p>
<h3>Processing the Tappy data with one function</h3>
<p>Here’s the <a id="_idIndexMarker1264"/>function:</p>
<pre class="source-code">
#%% Combine into one function
def read_tappy(file_name):</pre> <p>Here, we’re reading in the CSV filename passed as an argument to our function. We enrich the data with hardcoded field names:</p>
<pre class="source-code">
  df = pd.read_csv(
    'data/Tappy Data/' + file_name,
    delimiter='\t',
    index_col=False,
    names=['UserKey', 'Date', 'Timestamp', 'Hand', 'Hold time',
        'Direction', 'Latency time', 'Flight time']
  )</pre> <p>We drop the unneeded column:</p>
<pre class="source-code">
  df = df.drop('UserKey', axis=1)</pre> <p>We fix the dates:</p>
<pre class="source-code">
  df['Date'] = pd.to_datetime(df['Date'], errors='coerce', format='%y%M%d').dt.date
  # Convert time data to numeric
  for column in ['Hold time', 'Latency time', 'Flight time']:
    df[column] = pd.to_numeric(df[column], errors='coerce')
  df = df.dropna(axis=0)</pre> <p>Always wash your hands by getting rid <a id="_idIndexMarker1265"/>of invalid values:</p>
<pre class="source-code">
  # Clean data in `Hand`
  df = df[
    (df['Hand'] == 'L') |
    (df['Hand'] == 'R') |
    (df['Hand'] == 'S')
    ]</pre> <p>Do the same with direction data values:</p>
<pre class="source-code">
  # Clean data in `Direction`
  df = df[
    (df['Direction'] == 'LL') |
    (df['Direction'] == 'LR') |
    (df['Direction'] == 'LS') |
    (df['Direction'] == 'RL') |
    (df['Direction'] == 'RR') |
    (df['Direction'] == 'RS') |
    (df['Direction'] == 'SL') |
    (df['Direction'] == 'SR') |
    (df['Direction'] == 'SS')
    ]</pre> <p>We’re doing our math! This is where the manual GC process comes in. It’s a good thing we washed our hands, right? In the following code, we’re doing our calculations. The results are being returned as a new DataFrame, so to save memory, we’re deleting the old DataFrames as we go. This frees up memory <a id="_idIndexMarker1266"/>since this kind of work is memory intensive:</p>
<pre class="source-code">
     direction_group_df = df.groupby('Direction')[numeric_columns][numeric_columns][numeric_columns] direction_group_df = df.groupby('Direction')[numeric_columns].mean()
  del df
  gc.collect()</pre> <p>With our new result, we re-index and then sort:</p>
<pre class="source-code">
  direction_group_df = direction_group_df.reindex(
    ['LL', 'LR', 'LS', 'RL', 'RR', 'RS', 'SL', 'SR', 'SS'])
  direction_group_df = direction_group_df.sort_index() # to ensure correct order of data</pre> <p>This line returns the flattened NumPy array, which contains the mean values of the grouped data. The <code>.values.flatten()</code> method<a id="_idIndexMarker1267"/> converts the DataFrame into a two-dimensional NumPy array and then flattens it into a one-dimensional array for ease of use:</p>
<pre class="source-code">
  return direction_group_df.values.flatten()</pre> <h3>Processing the users with a function</h3>
<p>Within the same cell is a<a id="_idIndexMarker1268"/> second function:</p>
<pre class="source-code">
def process_user(user_id, filenames):
  running_user_data = np.array([])</pre> <p>This line initializes an empty NumPy array named <code>running_user_data</code>. This array will be used to accumulate data as the function iterates through filenames, which is what the following block does:</p>
<pre class="source-code">
  for filename in filenames:
    if user_id in filename:
      running_user_data = np.append(running_user_data, read_tappy(filename))</pre> <p>This loop iterates through the list of filenames. If the provided user ID is found in the filename, it calls the <code>read_tappy()</code> function (which returns a flattened NumPy array of mean values) and appends its contents to the <code>running_user_data</code> array.</p>
<p>After iterating through the filenames and appending the data, the following line reshapes the <code>running_user_data</code> array into a two-dimensional array, with each row containing 27 columns. This flattening of time data allows for further analysis:</p>
<pre class="source-code">
  running_user_data = np.reshape(running_user_data, (-1, 27))</pre> <p>The last line calculates the mean values along the rows (<code>axis=0</code>) of the <code>running_user_data</code> array using <code>np.nanmean()</code>. The <code>np.nanmean()</code> function ignores <code>NaN</code> values while<a id="_idIndexMarker1269"/> calculating the mean:</p>
<pre class="source-code">
  return np.nanmean(running_user_data, axis=0)</pre> <p>To summarize, the <code>process_user</code> function processes data for a specific user by iterating through relevant filenames, aggregating the data using the <code>read_tappy</code> function, reshaping the data, and calculating the mean values while ignoring <code>NaN</code> values. The final result is an array of mean values for each column of the data.</p>
<h3>Processing all the data</h3>
<p>This one’s for all the marbles! The<a id="_idIndexMarker1270"/> following cell processes the data for all available users by aggregating and calculating mean values based on the Tappy data. First, there’s a little housekeeping. We’re going to ignore any warnings:</p>
<pre class="source-code">
#%% Run through all available data
import warnings
warnings.filterwarnings("ignore")</pre> <p>We’ll make one more trip through the <code>Tappy </code><code>Data</code> folder:</p>
<pre class="source-code">
filenames = os.listdir('data/Tappy Data/')</pre> <p>Next, we’ll make some column names for the final DataFrame:</p>
<pre class="source-code">
column_names = [first_hand + second_hand + '_' + time
        for first_hand in ['L', 'R', 'S']
        for second_hand in ['L', 'R', 'S']
        for time in ['Hold time', 'Latency time', 'Flight time']]
user_tappy_df = pd.DataFrame(columns=column_names)</pre> <p>Next, let’s loop through the user indexes and use our <code>process_user</code> function:</p>
<pre class="source-code">
for user_id in user_df.index:
  user_tappy_data = process_user(str(user_id), filenames)
  user_tappy_df.loc[user_id] = user_tappy_data</pre> <p>These next few lines do a little <a id="_idIndexMarker1271"/>interim cleaning by ensuring any NaN values are substituted with zeros, and any negative numeric data is also normalized to zero:</p>
<pre class="source-code">
user_tappy_df = user_tappy_df.fillna(0)
user_tappy_df[user_tappy_df &lt; 0] = 0</pre> <p>And then, we print like we’ve never printed before! OK, that’s not true – we’ve done this a lot:</p>
<pre class="source-code">
print(user_tappy_df.head())</pre> <h3>Saving the processed data</h3>
<p>The last code cell likely<a id="_idIndexMarker1272"/> doesn’t need much explanation:</p>
<pre class="source-code">
#%% Save processed data</pre> <p>First, we concatenate the two DataFrames together:</p>
<pre class="source-code">
combined_user_df = pd.concat([user_df, user_tappy_df], axis=1)
print(combined_user_df.head())</pre> <p>Finally, we save it to a CSV file:</p>
<pre class="source-code">
combined_user_df.to_csv('data/combined_user.csv')</pre> <p>This is generally a good practice in a given data pipeline. Saving the processed, cleaned version of a dataset can save data engineers a lot of effort if something goes wrong along the way. It also offers flexibility, if and when we want to change or extend our pipeline further.</p>
<p>I’ll open the CSV file in<a id="_idIndexMarker1273"/> PyCharm for one last look before we start doing the real analysis work. You can see mine in <em class="italic">Figure 14</em><em class="italic">.19</em>:</p>
<div><div><img alt="Figure 14.19: Our hard work has paid off! Our data is ready for analysis" src="img/B19644_14_19.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.19: Our hard work has paid off! Our data is ready for analysis</p>
<p>With that, we are ready to start exploring our dataset and searc<a id="_idTextAnchor362"/>hing for insights.</p>
<h1 id="_idParaDest-353"><a id="_idTextAnchor363"/>Data analysis and insights</h1>
<p>Remember what we <a id="_idIndexMarker1274"/>said about the importance of having a question in mind when starting to work on a data science project? This is especially true during this phase, where we explore our dataset and extract insights, which should revolve around our initial question – the connection between typing speed and whether a patient has PD or not.</p>
<p>Throughout this section, we will be working with the <code>EDA.ipynb</code> file, located in the <code>notebooks</code> folder of our current project. In the following subsections, we will be looking at the code included in this <code>notebooks</code> folder. Go ahead and open this Jupyter notebook in your PyCharm<a id="_idIndexMarker1275"/> editor, or, if you are following our discussions and entering your own code, create a new Jupyter notebook.</p>
<h2 id="_idParaDest-354"><a id="_idTextAnchor364"/>Starting the notebook and reading in our processed data</h2>
<p>Remember that when you <a id="_idIndexMarker1276"/>open a Jupyter notebook in Python, you can see the code, but Jupyter won’t run unless you click the <strong class="bold">Run</strong> button. You can see PyCharm ready for this in <em class="italic">Figure 14</em><em class="italic">.20</em>:</p>
<div><div><img alt="Figure 14.20: The notebook is open, I’ve clicked in the first cell (In 1), and I’ll now click the Run button indicated by the arrow" src="img/B19644_14_20.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.20: The notebook is open, I’ve clicked in the first cell (In 1), and I’ll now click the Run button indicated by the arrow</p>
<p>Once you click the <strong class="bold">Run</strong> button, a Jupyter server will start and run the first cell in the notebook, which handles our imports and reads in our cleaned dataset:</p>
<pre class="source-code">
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
combined_user_df = pd.read_csv('../data/combined_user.csv', index_col=0)
combined_user_df.head()</pre> <p>Since the last line has us printing<a id="_idIndexMarker1277"/> the first five lines of our output, you’ll see them appear below the code and next to a marker that says <strong class="bold">Out 2</strong>, as shown in <em class="italic">Figure 14</em><em class="italic">.21</em>:</p>
<div><div><img alt="Figure 14.21: The output from the head statement in In 2 is shown in Out 2 and is horizontally scrollable" src="img/B19644_14_21.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.21: The output from the head statement in In 2 is shown in Out 2 and is horizontally scrollable</p>
<p>Now that our cleaned data has been loaded up, we can move on to analysis techniques.</p>
<h1 id="_idParaDest-355"><a id="_idTextAnchor365"/>Using charts and graphs</h1>
<p>Visualization is normally the end goal for most of my work, so for me, this is a natural next step. I’m going to start by <a id="_idIndexMarker1278"/>creating a bar graph that will show me the distribution of the counts of unique values within the data. I think this might give us some insight into which factor would affect the <a id="_idIndexMarker1279"/>dependent variable in this study, which is whether a subject has early-onset PD. However, there’s still a problem. As shown in <em class="italic">Figure 14</em><em class="italic">.21</em>, there are still some holes in the data I will need to account for before I begin analysis in earnest.</p>
<p>What I’m going to do first is create a bar chart to visualize our missing data. The following code cell handles this:</p>
<pre class="source-code">
#%%
missing_data = combined_user_df.isnull().sum()
g = sns.barplot(x=missing_data.index, y=missing_data)
g.set_xticklabels(labels=missing_data.index, rotation=90)
plt.show()</pre> <p>Running this code produces the visualization shown in <em class="italic">Figure 14</em><em class="italic">.22</em>:</p>
<div><div><img alt="Figure 14.22: The missing data is visualized in the bar chart" src="img/B19644_14_22.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.22: The missing data is visualized in the bar chart</p>
<p>Thankfully, our chart is very sparse. There<a id="_idIndexMarker1280"/> is only a small amount of data that is missing, or incomplete. There are some missing values for <code>BirthYear</code> and <code>DiagnosisYear</code>. You can even see one in the preview shown in <em class="italic">Figure 14</em><em class="italic">.21</em>. Analyzing missing values is important, and we<a id="_idIndexMarker1281"/> will come back to the process of filling in these values later on. But for now, let’s continue with the visualization process.</p>
<p>A great feature in Matplotlib is subplots, which allow us to generate multiple visualizations side by side. In the following code cell, we are creating multiple visualizations with this feature to highlight potential differences between patients with and without Parkinson’s:</p>
<pre class="source-code">
#%%
f, ax = plt.subplots(2, 2, figsize=(20, 10))
sns.distplot(
combined_user_df.loc[combined_user_df['Parkinsons'] == 0,
'BirthYear'].dropna(axis=0),
kde_kws = {'label': "Without Parkinson's"},
ax = ax[0][0]
)
sns.distplot(
combined_user_df.loc[combined_user_df['Parkinsons'] == 1,
'BirthYear'].dropna(axis=0),
kde_kws = {'label': "With Parkinson's"},
ax = ax[0][1]
)
sns.countplot(x='Female', hue='Parkinsons', data=combined_user_df, ax=ax[1][0])
sns.countplot(x='Tremors', hue='Parkinsons', data=combined_user_df, ax=ax[1][1])
plt.show()</pre> <p>After<a id="_idIndexMarker1282"/> running this code cell, a visualization<a id="_idIndexMarker1283"/> will be generated, as shown in <em class="italic">Figure 14</em><em class="italic">.23</em>:</p>
<div><div><img alt="Figure 14.23: Four plots drawn together from the previous cell" src="img/B19644_14_23.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.23: Four plots drawn together from the previous cell</p>
<p>The top two visualizations represent the <a id="_idIndexMarker1284"/>distribution in the year of birth of people with (top right) and without (top left) Parkinson’s. We can see that these distributions roughly follow the normal <a id="_idIndexMarker1285"/>bell curve. If you were to encounter a distribution that is skewed or in a strange shape, it might be worth digging into that data further. Note that we can also apply the same visualization for the <code>DiagnosisYear</code> column.</p>
<p>In the bottom-left visualization, we have a bar chart representing the count of male patients (two bars on the left) and female patients (two bars on the right). Patients with Parkinson’s are counted with the orange bars, and patients without are counted with the blue bars. In this visualization, we can see that while there are more patients with the disease than the ones without, the breakdown across the two genders is roughly the same.</p>
<p>The bottom-right visualization, on the other hand, illustrates the breakdown between patients with tremors (two bars on the right) and those without tremors (two bars on the left). From this visualization, we can see that tremors are significantly more common in patients with Parkinson’s, which is quite intuitive and can serve as a sanity check for our analyses so far.</p>
<p>Next, we will move on to box plots. Specifically, we will use box plots to visualize the distributions of different time data (<code>Hold time</code>, <code>Latency time</code>, and <code>Flight time</code>) among patients with<a id="_idIndexMarker1286"/> and without Parkinson’s. Once again, we will use the subplots feature to <a id="_idIndexMarker1287"/>generate multiple visualizations at the same time:</p>
<pre class="source-code">
#%%
column_names = [first_hand + second_hand + '_' + time
for first_hand in ['L', 'R', 'S']
for second_hand in ['L', 'R', 'S']
for time in ['Hold time', 'Latency time', 'Flight time']]
f, ax = plt.subplots(3, 3, figsize=(10, 5))
plt.subplots_adjust(
right = 3,
top = 3
)
for i in range(9):
temp_columns = column_names[3 * i : 3 * i + 3]
stacked_df = combined_user_df[temp_columns].stack().reset_index()
stacked_df = stacked_df.rename(
columns={'level_0': 'index', 'level_1': 'Type', 0: 'Time'})
stacked_df = stacked_df.set_index('index')
for index in stacked_df.index:
stacked_df.loc[index, 'Parkinsons'] = combined_user_df.loc[index,
'Parkinsons']
sns.boxplot(x='Type', y='Time',
hue='Parkinsons',
data=stacked_df,
ax=ax[i // 3][i % 3]
).set_title(column_names[i * 3][: 2], fontsize=20)
plt.show()</pre> <p>In this code cell, each subplot will<a id="_idIndexMarker1288"/> visualize data of a specific direction type (<code>LL</code>, <code>LR</code>, <code>LS</code>, and so on) and will <a id="_idIndexMarker1289"/>contain different splits denoting patients with and without the disease. You should obtain the visualization shown in <em class="italic">Figure 14</em><em class="italic">.24</em>:</p>
<div><div><img alt="Figure 14.24: The plots from the previous run cell" src="img/B19644_14_24.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.24: The plots from the previous run cell</p>
<p>What we can gather from this <a id="_idIndexMarker1290"/>visualization is that, surprisingly, the distribution of typing speed among patients without Parkinson’s can span across higher values and have more variance than that <a id="_idIndexMarker1291"/>among patients with Parkinson’s, which might contradict the intuition some might have that patients with Parkinson’s take more time to press keystrokes.</p>
<p>Overall, bar charts, distribution plots, and box plots are some of the most common visualization techniques in data science tasks, mostly because they are both simple to understand and powerful enough to highlight important patterns in our datasets. In the next and final subsection on the topic of data analysis, we will consider more advanced techniques – namely, the correlation matrix between attributes and leveraging ML models.</p>
<h1 id="_idParaDest-356"><a id="_idTextAnchor366"/>Machine learning-based insights</h1>
<p>Unlike the previous analysis methods, the methods discussed in this subsection and other similar ones are based on more complex mathematical models and ML algorithms. Given the scope of this book, we will not be <a id="_idIndexMarker1292"/>going into the specific theoretical details for these models, but it’s still worth seeing some of them in action by applying them to our dataset.</p>
<p>First, let’s consider the feature correlation matrix for our dataset. As the name suggests, this model is a matrix (a 2D table) that contains the correlation between each pair of numerical attributes (or features) within our dataset. A correlation between two features is a real number between -1 and 1, indicating the magnitude and direction of the correlation. The higher the value, the more correlated the two features are.</p>
<p>To obtain the feature correlation matrix from a pandas DataFrame, we must call the <code>corr()</code> method, as shown here:</p>
<pre class="source-code">
corr_matrix = combined_user_df.corr()</pre> <p>We usually visualize a correlation matrix using a heat map, as implemented in the same code cell:</p>
<pre class="source-code">
f, ax = plt.subplots(1, 1, figsize=(15, 10))
sns.heatmap(corr_matrix)
plt.show()</pre> <p>This code will produce the visualization shown in <em class="italic">Figure 14</em><em class="italic">.25</em>:</p>
<div><div><img alt="Figure 14.25: A heatmap is ideal for visualizing correlation matrices" src="img/B19644_14_25.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.25: A heatmap is ideal for visualizing correlation matrices</p>
<p>Next, we will try applying an ML model to our dataset. Contrary to popular belief, in many data science projects, we don’t take advantage of ML models for predictive tasks, where we train our models to be able to <a id="_idIndexMarker1293"/>predict future data. Instead, we feed our dataset to a specific model so that we can extract more insights from that current dataset.</p>
<p>Here, we are using the linear <strong class="bold">support vector classifier</strong> (<strong class="bold">SVC</strong>) model from scikit-learn to analyze the <a id="_idIndexMarker1294"/>data we have and return the feature importance list:</p>
<pre class="source-code">
#%%
from sklearn.svm import LinearSVC
combined_user_df['BirthYear'].fillna(combined_user_df['BirthYear'].mode(dropna=True)[0], inplace=True)
combined_user_df['DiagnosisYear'].fillna(combined_user_df['DiagnosisYear'].mode(dropna=True)[0], inplace=True)
X_train = combined_user_df.drop(['Parkinsons'], axis=1)
y_train = combined_user_df['Parkinsons']
clf = LinearSVC()
clf.fit(X_train, y_train)
nfeatures = 10
coef = clf.coef_.ravel()
top_positive_coefs = np.argsort(coef)[-nfeatures :]
top_negative_coefs = np.argsort(coef)[: nfeatures]
top_coefs = np.hstack([top_negative_coefs, top_positive_coefs])</pre> <p>Note that before we feed the data we <a id="_idIndexMarker1295"/>have to the ML model, we need to fill in the missing values we have in the two columns we identified earlier – <code>BirthYear</code> and <code>DiagnosisYear</code>. Most ML models cannot handle missing values very well, and it is up to the data engineers to choose how these values should be filled.</p>
<p>Here, we are using the <code>coef_</code> attribute of the model afterward.</p>
<p>This attribute contains the feature importance list, which is visualized by the last section of the code:</p>
<pre class="source-code">
plt.figure(figsize=(15, 5))
colors = ['red' if c &lt; 0 else 'blue' for c in coef[top_coefs]]
plt.bar(np.arange(2 * nfeatures), coef[top_coefs], color=colors)
feature_names = np.array(X_train.columns)
# Make sure the number of tick locations matches the number of tick labels.
plt.xticks(np.arange(0, 2 * nfeatures), feature_names[top_coefs], rotation=60, ha='right')
plt.show()</pre> <p>Running this code produces the<a id="_idIndexMarker1296"/> visualization shown in <em class="italic">Figure 14</em><em class="italic">.26</em>:</p>
<div><div><img alt="Figure 14.26: A graph of the feature important list identifies features used extensively while training an ML model" src="img/B19644_14_26.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.26: A graph of the feature important list identifies features used extensively while training an ML model</p>
<p>From the feature importance list, we can identify any features that were used extensively by the ML model while training. A<a id="_idIndexMarker1297"/> feature with a very high importance value could be correlated with the target attribute (whether someone has Parkinson’s or not) in some interesting way. For example, we can see that <code>Tremors</code> (which we know is quite correlated to our target attribute) is the third most important feature of our current ML model.</p>
<p>That’s our last discussion point regarding analyzing our dataset. In the last section of this chapter, we will have a brief discussion on deciding how to write a script<a id="_idTextAnchor367"/> for a Python data science project.</p>
<h1 id="_idParaDest-357"><a id="_idTextAnchor368"/>Scripts versus notebooks in data science</h1>
<p>In the preceding data science pipeline, there are two main sections: data cleaning, where we remove inconsistent <a id="_idIndexMarker1298"/>data, fill in missing data, and appropriately <a id="_idIndexMarker1299"/>encode the attributes, and data analysis, where we <a id="_idIndexMarker1300"/>generate visualizations and insights from our cleaned dataset.</p>
<p>The data cleaning process was implemented by a Python script while the data analysis process was done with a Jupyter notebook. In general, deciding whether a Python program should be done in a script or a notebook is quite an important, yet often overlooked, aspect while working on a data science project.</p>
<p>As we discussed in the<a id="_idIndexMarker1301"/> previous chapter, Jupyter notebooks are perfect for iterative <a id="_idIndexMarker1302"/>development processes, where we can transform and manipulate our data as we go. A Python script, on the other hand, offers no such dynamism. We need to enter all of the code necessary in the script and run it as a complete program.</p>
<p>However, as illustrated in the <em class="italic">Data cleansing and preprocessing</em> section, PyCharm allows us to divide a traditional Python script into separate code cells and inspect the data we have as we go using the <strong class="bold">SciView</strong> panel. The dynamism offered by Jupyter notebooks can also be found within PyCharm.</p>
<p>Now, another core difference between regular Python scripts and Jupyter notebooks is the fact that printed output and visualizations are included inside a notebook, together with the code cells that generated them. While looking at this from the perspective of data scientists, we can see that this feature is considerably useful when making reports and presentations.</p>
<p>Say you are tasked with finding actionable insights from a dataset in a company project, and you need to present your final findings, as well as how you came across them with your team. A Jupyter notebook can effectively serve as the main platform for your presentation. Not only will people be able to see which specific commands were used to process and manipulate the original data but you will also be able to include Markdown texts to further explain any subtle discussion points.</p>
<p>Regular Python scripts can simply be used for low-level tasks where the general workflow has already been agreed upon, and you will not need to present it to anyone else. In our current example, I chose to clean the dataset using a Python script as most of the cleaning and formatting changes we applied to the dataset don’t generate any actionable insights that can address our initial question. I only used a notebook for data analysis tasks, where there were<a id="_idIndexMarker1303"/> many visualizations and insights worthy of further discussion.</p>
<p>Overall, the decision to use either a traditional Python script or a Jupyter notebook solely depends on your tasks and purposes. We simply need to remember that, for whichever tool we would like to use, PyCharm offers incredible<a id="_idTextAnchor369"/> <a id="_idTextAnchor370"/>support that can streamline our workflow.</p>
<h1 id="_idParaDest-358"><a id="_idTextAnchor371"/>Summary</h1>
<p>In this chapter, we walked through the hands-on process of working on a data science pipeline. First, we discussed the importance of having version control for not just our code and project-related files but also our datasets; we then learned how to use Git LFS to apply version control to large files and datasets.</p>
<p>Next, we looked at various data cleaning and preprocessing techniques that are specific to the example dataset. Using the <strong class="bold">SciView</strong> panel in PyCharm, we can dynamically inspect the current state of our data and variables and see how they change after each command.</p>
<p>Finally, we considered several techniques to generate visualizations and extract insights from our dataset. Using the Jupyter editor in PyCharm, we were able to avoid working with a Jupyter server and work on our notebook entirely within PyCharm. Having walked through this process, you are now ready to tackle real-life data science problems and projects using the same tools and functionalities that we have discussed so far.</p>
<p>So, we have finished our discussion on using PyCharm in the context of scientific computing and data science. In the next chapter, we will finally consider a topic that we have mentioned multiple times throug<a id="_idTextAnchor372"/>h<a id="_idTextAnchor373"/> our previous chapters – PyCharm plugins.</p>
<h1 id="_idParaDest-359"><a id="_idTextAnchor374"/>Questions</h1>
<p>Answer the following questions to test your knowledge of this chapter:</p>
<ol>
<li>What are some of the main ways of collecting datasets for a data science project?</li>
<li>Can Git LFS be used with Git? If so, what is the overall process?</li>
<li>Which type of attribute can have its missing values filled out with the mean? What about the mode?</li>
<li>What problem does one-hot encoding address? What problem can arise from using one-hot encoding?</li>
<li>Which type of attribute can benefit from bar charts? What about distribution plots?</li>
<li>Why is it important to consider the feature correlation matrix for a dataset?</li>
<li>Aside from predictive tasks, what can we use ML<a id="_idTextAnchor375"/> <a id="_idTextAnchor376"/>models for (like we did in this chapter)?</li>
</ol>
<h1 id="_idParaDest-360"><a id="_idTextAnchor377"/>Further reading</h1>
<p>Be sure to check out the companion website for this book at <a href="https://www.pycharm-book.com">https://www.pycharm-book.com</a>.</p>
<p>More information can be found in the following articles and reading materials:</p>
<ul>
<li>Adams, W. R. (2017). <em class="italic">High-accuracy detection of early Parkinson’s Disease using multiple characteristics of finger movement while typing</em>. PloS one, <em class="italic">12</em>(11), e0188226.</li>
<li>The <em class="italic">Tappy Keystroke Data with Parkinson’s Patients</em> data, uploaded by Patrick DeKelly: <a href="https://www.kaggle.com/valkling/tappy-keystroke-data-with-parkinsons-patients">https://www.kaggle.com/valkling/tappy-keystroke-data-with-parkinsons-patients</a>.</li>
<li><em class="italic">Building a Data Pipeline from Scratch</em>, by Alan Marazzi: <a href="https://medium.com/the-data-experience/building-a-data-pipeline-from-scratch-32b712cfb1db">https://medium.com/the-data-experience/building-a-data-pipeline-from-scratch-32b712cfb1db</a>.</li>
<li><em class="italic">A Business Perspective to Designing an Enterprise-Level Data Science Pipeline</em>, by Vikram Reddy: <a href="https://www.datascience.com/blog/designing-an-enterprise-level-data-science-pipeline">https://www.datascience.com/blog/designing-an-enterprise-level-data-science-pipeline</a>.</li>
<li><em class="italic">Data Science for Startups: Data Pipelines</em>, by Ben Weber: <a href="https://towardsdatascience.com/data-science-for-startups-data-pipelines-786f6746a59a">https://towardsdatascience.com/data-science-for-startups-data-pipelines-786f6746a59a</a>.</li>
<li>Documentation for the pandas library: <a href="https://pandas.pydata.org/pandas-docs/stable/">https://pandas.pydata.org/pandas-docs/stable/</a>.</li>
</ul>
</div>


<div><h1 id="_idParaDest-361" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor378"/>Part 5: Plugins and Conclusion</h1>
<p>This part will introduce readers to the concept of PyCharm plugins and walk through the process of downloading plugins and adding them to their PyCharm environment. It will also go into details regarding the most popular plugins and how they can optimize a programmer’s productivity even further. We’ll also gloss over important topics discussed in previous chapters of the book and offers a comprehensive view on PyCharm’s most popular features.</p>
<p>This part has the following chapters:</p>
<ul>
<li><a href="B19644_15.xhtml#_idTextAnchor379"><em class="italic">Chapter 15</em></a>, <em class="italic">More Possibilities with PyCharm Plugins</em></li>
<li><a href="B19644_16.xhtml#_idTextAnchor401"><em class="italic">Chapter 16</em></a>, <em class="italic">Future Developments</em></li>
</ul>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
</body></html>