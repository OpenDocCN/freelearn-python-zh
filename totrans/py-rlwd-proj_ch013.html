<html xml:lang="en-US" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<meta charset="utf-8"/>
<meta content="pandoc" name="generator"/>
<title>ch013.xhtml</title>

<!-- kobo-style -->
<style id="koboSpanStyle" type="text/css" xmlns="http://www.w3.org/1999/xhtml">.koboSpan { -webkit-text-combine: inherit; }</style>
</head>
<body epub:type="bodymatter">

<h1 data-number="13">Chapter 9<br/>
Project 3.1: Data Cleaning Base Application</h1>
<p>Data validation, cleaning, converting, and standardizing are steps required to transform raw data acquired from source applications into something that can be used for analytical purposes. Since we started using a small data set of very clean data, we may need to improvise a bit to create some ”dirty” raw data. A good alternative is to search for more complicated, raw data.</p>
<p>This chapter will guide you through the design of a data cleaning application, separate from the raw data acquisition. Many details of cleaning, converting, and standardizing will be left for subsequent projects. This initial project creates a foundation that will be extended by adding features. The idea is to prepare for the goal of a complete data pipeline that starts with acquisition and passes the data through a separate cleaning stage. We want to exploit the Linux principle of having applications connected by a shared buffer, often referred to as a shell pipeline.</p>
<p>This chapter will cover a number of skills related to the design of data validation and cleaning applications:</p>
<ul>
<li><p>CLI architecture and how to design a pipeline of processes</p></li>
<li><p>The core concepts of validating, cleaning, converting, and standardizing raw data</p></li>
</ul>
<p>We won’t address all the aspects of converting and standardizing data in this chapter. Projects in <a href="ch014.xhtml#x1-22900010"><em>Chapter</em><em> 10</em></a>, <a href="ch014.xhtml#x1-22900010"><em>Data Cleaning Features</em></a> will expand on many conversion topics. The project in <a href="ch016.xhtml#x1-27600012"><em>Chapter</em><em> 12</em></a>, <a href="ch016.xhtml#x1-27600012"><em>Project 3.8: Integrated Data</em> <em>Acquisition Web Service</em></a> will address the integrated pipeline idea. For now, we want to build an adaptable base application that can be extended to add features.</p>
<p>We’ll start with a description of an idealized data cleaning application. </p>

<h2 data-number="13.1">9.1  Description</h2>
<p>We need to build a data validating, cleaning, and standardizing application. A data inspection notebook is a handy starting point for this design work. The goal is a fully-automated application to reflect the lessons learned from inspecting the data.</p>
<p>A data preparation pipeline has the following conceptual tasks:</p>
<ul>
<li><p>Validate the acquired source text to be sure it’s usable and to mark invalid data for remediation.</p></li>
<li><p>Clean any invalid raw data where necessary; this expands the available data in those cases where sensible cleaning can be defined.</p></li>
<li><p>Convert the validated and cleaned source data from text (or bytes) to usable Python objects.</p></li>
<li><p>Where necessary, standardize the code or ranges of source data. The requirements here vary with the problem domain.</p></li>
</ul>
<p>The goal is to create clean, standardized data for subsequent analysis. Surprises occur all the time. There are several sources:</p>
<ul>
<li><p>Technical problems with file formats of the upstream software. The intent of the acquisition program is to isolate physical format issues.</p></li>
<li><p>Data representation problems with the source data. The intent of this project is to isolate the validity and standardization of the values.</p></li>
</ul>
<p>Once cleaned, the data itself may still contain surprising relationships, trends, or distributions. This is discovered with later projects that create analytic notebooks and reports. Sometimes a surprise comes from finding the <em>Null Hypothesis </em>is true and the data only shows insignificant random variation.</p>
<p>In many practical cases, the first three steps — validate, clean, and convert — are often combined into a single function call. For example, when dealing with numeric values, the <code>int()</code> or <code>float()</code> functions will validate and convert a value, raising an exception for invalid numbers.</p>
<p>In a few edge cases, these steps need to be considered in isolation – often because there’s a tangled interaction between validation and cleaning. For example, some data is plagued by dropping the leading zeros from US postal codes. This can be a tangled problem where the data is superficially invalid but can be reliably cleaned before attempting validation. In this case, validating the postal code against an official list of codes comes after cleaning, not before. Since the data will remain as text, there’s no actual conversion step after the clean-and-validate composite step. </p>

<h3 data-number="13.1.1">9.1.1  User experience</h3>
<p>The overall <strong>User Experience </strong>(<strong>UX</strong>) will be two command-line applications. The first application will acquire the raw data, and the second will clean the data. Each has options to fine-tune the <code>acquire</code> and <code>cleanse</code> steps.</p>
<p>There are several variations on the <code>acquire</code> command, shown in earlier chapters. Most notably, <a href="ch007.xhtml#x1-560003"><em>Chapter</em><em> 3</em></a>, <a href="ch007.xhtml#x1-560003"><em>Project 1.1: Data Acquisition Base Application</em></a>, <a href="ch008.xhtml#x1-780004"><em>Chapter</em><em> 4</em></a>, <a href="ch008.xhtml#x1-780004"><em>Data Acquisition Features: Web APIs and Scraping</em></a>, and <a href="ch009.xhtml#x1-1140005"><em>Chapter</em><em> 5</em></a>, <a href="ch009.xhtml#x1-1140005"><em>Data Acquisition Features: SQL Database</em></a>.</p>
<p>For the <strong>clean </strong>application, the expected command-line should like something like this:</p>
<div><div><pre class="console">% python src/clean.py -o analysis -i quartet/Series_1.ndjson</pre>
</div>
</div>
<p>The <code>-o</code><code> analysis</code> specifies a directory into which the resulting clean data is written.</p>
<p>The <code>-i</code><code> quartet/Series_1.ndjson</code> specifies the path to the source data file. This is a file written by the acquisition application.</p>
<p>Note that we’re not using a positional argument to name the input file. The use of a positional argument for a filename is a common provision in many — but not all — Linux commands. The reason to avoid positional arguments is to make it easier to adapt this to become part of a pipeline of processing stages.</p>
<p>Specifically, we’d like the following to work, also:</p>
<div><div><pre class="console">% python src/acquire.py -s Series_1Pair --csv source.csv | \
    python src/clean.py -o analysis/Series_1.ndjson</pre>
</div>
</div>
<p>This shell line has two commands, one to do the raw data acquisition, and the other to perform the validation and cleaning. The acquisition command uses the <code>-s</code><code> Series_1Pair</code> argument to name a specific series extraction class. This class will be used to create a single series as output. The <code>--csv</code><code> source.csv</code> argument names the input file to process. Other options could name RESTful APIs or provide a database connection string.</p>
<p>The second command reads the output from the first command and writes this to a file. The file is named by the <code>-o</code> argument value in the second command.</p>
<p>This pipeline concept, made available with the shell’s <code>|</code> operator, means these two processes will run concurrently. This means data is passed from one process to the other as it becomes available. For very large source files, cleaning data as it’s being acquired can reduce processing time dramatically.</p>
<p>In <a href="ch014.xhtml#x1-2510005"><em>Project 3.6: Integration to create an acquisition pipeline</em></a> we’ll expand on this design to include some ideas for concurrent processing.</p>
<p>Now that we’ve seen an overview of the application’s purpose, let’s take a look at the source data. </p>


<h3 data-number="13.1.2">9.1.2  Source data</h3>
<p>The earlier projects produced source data in an approximately consistent format. These projects focused on acquiring data that is text. The individual samples were transformed into small JSON-friendly dictionaries, using the NDJSON format. This can simplify the validation and cleaning operation.</p>
<p>The NDJSON file format is described at <a class="url" href="http://ndjson.org">http://ndjson.org</a> and <a class="url" href="https://jsonlines.org">https://jsonlines.org</a>.</p>
<p>There are two design principles behind the <strong>acquire </strong>application:</p>
<ul>
<li><p>Preserve the original source data as much as possible.</p></li>
<li><p>Perform the fewest text transformations needed during acquisition.</p></li>
</ul>
<p>Preserving the source data makes it slightly easier to locate problems when there are unexpected changes to source applications. Minimizing the text transformations, similarly, keeps the data closer to the source. Moving from a variety of representations to a single representation simplifies the data cleaning and transformation steps.</p>
<p>All of the data acquisition projects involve some kind of textual transformation from a source representation to ND JSON.</p>
<div><div><table class="tabular" id="TBL-2">
<tbody>
<tr class="odd hline">
<td><hr/>
</td>
<td><hr/>
</td>
<td><hr/>
</td>
<td><hr/>
</td>
</tr>
<tr class="even" id="TBL-2-1-" style="vertical-align:baseline;">
<td class="td11" id="TBL-2-1-1" style="text-align: center; white-space: nowrap;"><strong>Chapter </strong></td>
<td class="td11" id="TBL-2-1-2" style="text-align: center; white-space: nowrap;"><strong>Section </strong></td>
<td class="td11" id="TBL-2-1-3" style="text-align: center; white-space: nowrap;"><strong>Source </strong></td>
<td/>
</tr>
<tr class="odd hline">
<td><hr/>
</td>
<td><hr/>
</td>
<td><hr/>
</td>
<td><hr/>
</td>
</tr>
<tr class="even" id="TBL-2-2-" style="vertical-align:baseline;">
<td class="td11" id="TBL-2-2-1" style="text-align: center; white-space: nowrap;"><a href="ch007.xhtml#x1-560003">3</a></td>
<td class="td11" id="TBL-2-2-2" style="text-align: center; white-space: nowrap;"><a href="ch007.xhtml#x1-560003"><em>Chapter</em><em> 3</em></a>, <a href="ch007.xhtml#x1-560003"><em>Project 1.1: Data Acquisition Base Application</em></a></td>
<td class="td11" id="TBL-2-2-3" style="text-align: center; white-space: nowrap;">CSV parsing</td>
<td/>
</tr>
<tr class="odd" id="TBL-2-3-" style="vertical-align:baseline;">
<td class="td11" id="TBL-2-3-1" style="text-align: center; white-space: nowrap;"><a href="ch008.xhtml#x1-780004">4</a></td>
<td class="td11" id="TBL-2-3-2" style="text-align: center; white-space: nowrap;"><a href="ch008.xhtml#x1-790001"><em>Project 1.2: Acquire data from a web service</em></a></td>
<td class="td11" id="TBL-2-3-3" style="text-align: center; white-space: nowrap;">Zipped CSV or JSON</td>
<td/>
</tr>
<tr class="even" id="TBL-2-4-" style="vertical-align:baseline;">
<td class="td11" id="TBL-2-4-1" style="text-align: center; white-space: nowrap;"><a href="ch008.xhtml#x1-780004">4</a></td>
<td class="td11" id="TBL-2-4-2" style="text-align: center; white-space: nowrap;"><a href="ch008.xhtml#x1-970002"><em>Project 1.3: Scrape data from a web page</em></a></td>
<td class="td11" id="TBL-2-4-3" style="text-align: center; white-space: nowrap;">HTML</td>
<td/>
</tr>
<tr class="odd" id="TBL-2-5-" style="vertical-align:baseline;">
<td class="td11" id="TBL-2-5-1" style="text-align: center; white-space: nowrap;"><a href="ch009.xhtml#x1-1140005">5</a></td>
<td class="td11" id="TBL-2-5-2" style="text-align: center; white-space: nowrap;"><a href="ch009.xhtml#x1-1260002"><em>Project 1.5: Acquire data from a SQL extract</em></a></td>
<td class="td11" id="TBL-2-5-3" style="text-align: center; white-space: nowrap;">SQL Extract</td>
<td/>
</tr>
<tr class="even hline">
<td><hr/>
</td>
<td><hr/>
</td>
<td><hr/>
</td>
<td><hr/>
</td>
</tr>
<tr class="odd" id="TBL-2-6-" style="vertical-align:baseline;">
<td class="td11" id="TBL-2-6-1" style="text-align: center; white-space: nowrap;"/>
<td/>
<td/>
<td/>
</tr>
</tbody>
</table>
</div>
</div>
<p>In some cases — i.e., extracting HTML — the textual changes to peel the markup away from the data is profound. The SQL database extract involves undoing the database’s internal representation of numbers or dates and writing the values as text. In some cases, the text transformations are minor. </p>


<h3 data-number="13.1.3">9.1.3  Result data</h3>
<p>The cleaned output files will be ND JSON; similar to the raw input files. We’ll address this output file format in detail in <a href="ch015.xhtml#x1-26400011"><em>Chapter</em><em> 11</em></a>, <a href="ch015.xhtml#x1-26400011"><em>Project 3.7: Interim Data</em> <em>Persistence</em></a>. For this project, it’s easiest to stick with writing the JSON representation of a Pydantic dataclass.</p>
<p>For Python’s native <code>dataclasses</code>, the <code>dataclasses.asdict()</code> function will produce a dictionary from a dataclass instance. The <code>json.dumps()</code> function will convert this to text in JSON syntax.</p>
<p>For Pydantic dataclasses, however, the <code>asdict()</code> function can’t be used. There’s no built-in method for emitting the JSON representation of a <code>pydantic</code> dataclass instance.</p>
<p>For version 1 of <strong>Pydantic</strong>, a slight change is required to write ND JSON. An expression like the following will emit a JSON serialization of a <code>pydantic</code> dataclass:</p>
<div><div><pre class="source-code">import json
from pydantic.json import pydantic_encoder
from typing import TextIO, Iterable

import analysis_model

def persist_samples(
        target_file: TextIO,
        analysis_samples: Iterable[analysis_model.SeriesSample | None]
) -&gt; int:
    count = 0
    for result in filter(None, analysis_samples):
        target_file.write(json.dumps(result, default=pydantic_encoder))
        target_file.write("\n")
        count += 1
    return count</pre>
</div>
</div>
<p>This central feature is the <code>default=pydantic_encoder</code> argument value for the <code>json.dumps()</code> function. This will handle the proper decoding of the dataclass structure into JSON notation.</p>
<p>For version 2 of <strong>pydantic</strong>, there will be a slightly different approach. This makes use of a <code>RootModel[classname](object)</code> construct to extract the root model for a given class from an object. In this case, <code>RootModel[SeriesSample](result).model_dump()</code> will create a root model that can emit a nested dictionary structure. No special <code>pydantic_encoder</code> will be required for version 2.</p>
<p>Now that we’ve looked at the inputs and outputs, we can survey the processing concepts. Additional processing details will wait for later projects. </p>


<h3 data-number="13.1.4">9.1.4  Conversions and processing</h3>
<p>For this project, we’re trying to minimize the processing complications. In the next chapter, <a href="ch014.xhtml#x1-22900010"><em>Chapter</em><em> 10</em></a>, <a href="ch014.xhtml#x1-22900010"><em>Data Cleaning Features</em></a>, we’ll look at a number of additional processing requirements that will add complications. As a teaser for the projects in the next chapter, we’ll describe some of the kinds of field-level validation, cleaning, and conversion that may be required.</p>
<p>One example we’ve focused on, Anscombe’s Quartet data, needs to be converted to a series of floating-point values. While this is painfully obvious, we’ve held off on the conversion from the text to the Python <code>float</code> object to illustrate the more general principle of separating the complications of acquiring data from analyzing the data. The output from this application will have each resulting ND JSON document with <code>float</code> values instead of <code>string</code> values.</p>
<p>The distinction in the JSON documents will be tiny: the use of <code>"</code> for the raw-data strings. This will be omitted for the <code>float</code> values.</p>
<p>This tiny detail is important because every data set will have distinct conversion requirements. The data inspection notebook will reveal data domains like text, integers, date-time stamps, durations, and a mixture of more specialized domains. It’s essential to examine the data before trusting any schema definition or documentation about the data.</p>
<p>We’ll look at three common kinds of complications:</p>
<ul>
<li><p>Fields that must be decomposed.</p></li>
<li><p>Fields which must be composed.</p></li>
<li><p>Unions of sample types in a single collection.</p></li>
<li><p>”Opaque” codes used to replace particularly sensitive information.</p></li>
</ul>
<p>One complication is when multiple source values are collapsed into a single field. This single source value will need to be decomposed into multiple values for analysis. With the very clean data sets available from Kaggle, a need for decomposition is unusual. Enterprise data sets, on the other hand, will often have fields that are not properly decomposed into atomic values, reflecting optimizations or legacy processing requirements. For example, a product ID code may include a line of business and a year of introduction as part of the code. For example, a boat’s hull ID number might include ”421880182,” meaning it’s a 42-foot hull, serial number 188, completed in January 1982. Three disparate items were all coded as digits. For analytical purposes, it may be necessary to separate the items that comprise the coded value. In other cases, several source fields will need to be combined. An example data set where a timestamp is decomposed into three separate fields can be found when looking at tidal data.</p>
<p>See <a class="url" href="https://tidesandcurrents.noaa.gov/tide_predictions.html">https://tidesandcurrents.noaa.gov/tide_predictions.html</a> for Tide Predictions around the US. This site supports downloads in a variety of formats, as well as RESTful API requests for tide predictions.</p>
<p>Each of the tidal events in an annual tide table has a timestamp. The timestamp is decomposed into three separate fields: the date, the day of the week, and the local time. The day of the week is helpful, but it is entirely derived from the date. The date and time need to be combined into a single datetime value to make this data useful. It’s common to use <code>datetime.combine(date,</code><code> time)</code> to merge separate date and time values into a single value.</p>
<p>Sometimes a data set will have records of a variety of subtypes merged into a single collection. The various types are often discriminated from each other by the values of a field. A financial application might include a mixture of invoices and payments; many fields overlap, but the meanings of these two transaction types are dramatically different. A single field with a code value of ”I” or ”P” may be the only way to distinguish the types of business records represented.</p>
<p>When multiple subtypes are present, the collection can be called a <em>discriminated</em> <em>union </em>of subtypes; sometimes simply called a <strong>union</strong>. The discriminator and the subtypes suggest a class hierachy is required to describe the variety of sample types. A common base class is needed to describe the common fields, including the discriminator. Each subclass has a distinct definition for the fields unique to the subclass.</p>
<p>One additional complication stems from data sources with ”opaque” data. These are string fields that can be used for equality comparison, but nothing else. These values are often the result of a data analysis approach called <strong>masking</strong>, <strong>deidentification</strong>, or <strong>pseudonymization</strong>. This is sometimes also called ”tokenizing” because an opaque token has replaced the sensitive data. In banking, for example, it’s common for analytical data to have account numbers or payment card numbers transformed into opaque values. These can be used to aggregate behavior, but cannot be used to identify an individual account holder or payment card. These fields must be treated as strings, and no other processing can be done.</p>
<p>For now, we’ll defer the implementation details of these complications to a later chapter. The ideas should inform design decisions for the initial, foundational application.</p>
<p>In addition to clean valid data, the application needs to produce information about the invalid data. Next, we’ll look at the logs and error reports. </p>


<h3 data-number="13.1.5">9.1.5  Error reports</h3>
<p>The central feature of this application is to output files with valid, useful data for analytic purposes. We’ve left off some details of what happens when an acquired document isn’t actually usable.</p>
<p>Here are a number of choices related to the observability of invalid data:</p>
<ul>
<li><p>Raise an overall exception and stop. This is appropriate when working with carefully-curated data sets like the Anscombe Quartet.</p></li>
<li><p>Make all of the bad data observable, either through the log or by writing bad data to a separate rejected samples file.</p></li>
<li><p>Silently reject the bad data. This is often used with large data sources where there is no curation or quality control over the source.</p></li>
</ul>
<p>In all cases, the summary counts of acquired data, usable analytic data, and cleaned, and rejected data are essential. It’s imperative to be sure the number of raw records read is accounted for, and the provenance of cleaned and rejected data is clear. The summary counts, in many cases, are the primary way to observe changes in data sources. A non-zero error count, may be so important that it’s used as the final exit status code for the cleaning application.</p>
<p>In addition to the observability of bad data, we may be able to clean the source data. There are several choices here, also:</p>
<ul>
<li><p>Log the details of each object where cleaning is done. This is often used with data coming from a spreadsheet where the unexpected data may be rows that need to be corrected manually.</p></li>
<li><p>Count the number of items cleaned without the supporting details. This is often used with large data sources where changes are frequent.</p></li>
<li><p>Quietly clean the bad data as an expected, normal operational step. This is often used when raw data comes directly from measurement devices in unreliable environments, perhaps in space, or at the bottom of the ocean.</p></li>
</ul>
<p>Further, each field may have distinct rules for whether or not cleaning bad data is a significant concern or a common, expected operation. The intersection of observability and automated cleaning has a large number of alternatives.</p>
<p>The solutions to data cleaning and standardization are often a matter of deep, ongoing conversations with users. Each data acquisition pipeline is unique with regard to error reporting and data cleaning.</p>
<p>It’s sometimes necessary to have a command-line option to choose between logging each error or simply summarizing the number of errors. Additionally, the application might return a non-zero exit code when any bad records are found; this permits a parent application (e.g., a shell script) to stop processing in the presence of errors.</p>
<p>We’ve looked at the overall processing, the source files, the result files, and some of the error-reporting alternatives that might be used. In the next section, we’ll look at some design approaches we can use to implement this application. </p>



<h2 data-number="13.2">9.2  Approach</h2>
<p>We’ll take some guidance from the C4 model ( <a class="url" href="https://c4model.com">https://c4model.com</a>) when looking at our approach.</p>
<ul>
<li><p><strong>Context</strong>: For this project, the context diagram has expanded to three use cases: acquire, inspect, and clean.</p></li>
<li><p><strong>Containers</strong>: There’s one container for the various applications: the user’s personal computer.</p></li>
<li><p><strong>Components</strong>: There are two significantly different collections of software components: the acquisition program and the cleaning program.</p></li>
<li><p><strong>Code</strong>: We’ll touch on this to provide some suggested directions.</p></li>
</ul>
<p>A context diagram for this application is shown in <a href="#9.1"><em>Figure 9.1</em></a>.</p>
<figure class="IMG---Figure">
<img alt="Figure 9.1: Context Diagram " src="img/file40.jpg"/>
<figcaption class="IMG---Caption">Figure 9.1: Context Diagram </figcaption>
</figure>
<p>A component diagram for the conversion application isn’t going to be as complicated as the component diagrams for acquisition applications. One reason for this is there are no choices for reading, extracting, or downloading raw data files. The source files are the ND JSON files created by the acquisition application.</p>
<p>The second reason the conversion programs tend to be simpler is they often rely on built-in Python-type definitions, and packages like <code>pydantic</code> to provide the needed conversion processing. The complications of parsing HTML or XML sources were isolated in the acquisition layer, permitting this application to focus on the problem domain data types and relationships.</p>
<p>The components for this application are shown in <a href="#9.2"><em>Figure 9.2</em></a>.</p>
<figure class="IMG---Figure">
<img alt="Figure 9.2: Component Diagram " src="img/file41.jpg"/>
<figcaption class="IMG---Caption">Figure 9.2: Component Diagram </figcaption>
</figure>
<p>Note that we’ve used a dotted ”depends on” arrow. This does not show the data flow from acquire to clean. It shows how the clean application depends on the acquire application’s output.</p>
<p>The design for the <strong>clean </strong>application often involves an almost purely functional design. Class definitions — of course — can be used. Classes don’t seem to be helpful when the application processing involves stateless, immutable objects.</p>
<p>In rare cases, a cleaning application will be required to perform dramatic reorganizations of data. It may be necessary to accumulate details from a variety of transactions, updating the state of a composite object. For example, there may be multiple payments for an invoice that must be combined for reconciliation purposes. In this kind of application, associating payments and invoices may require working through sophisticated matching rules.</p>
<p>Note that the <strong>clean </strong>application and the <strong>acquire </strong>application will both share a common set of dataclasses. These classes represent the source data, the output from the <strong>acquire </strong>application. They also define the input to the <strong>clean</strong> application. A separate set of dataclasses represent the working values used for later analysis applications.</p>
<p>Our goal is to create three modules:</p>
<ul>
<li><p><code>clean.py</code>: The main application.</p></li>
<li><p><code>analytical_model.py</code>: A module with dataclass definitions for the pure-Python objects that we’ll be working with. These classes will — generally — be created from JSON-friendly dictionaries with string values.</p></li>
<li><p><code>conversions.py</code>: A module with any specialized validation, cleaning, and conversion functions.</p></li>
</ul>
<p>If needed, any application-specific conversion functions may be required to transform source values to ”clean,” usable Python objects. If this can’t be done, the function can instead raise <code>ValueError</code> exceptions for invalid data, following the established pattern for functions like Python’s built-in <code>float()</code> function. Additionally, <code>TypeError</code> exceptions may be helpful when the object — as a whole — is invalid. In some cases, the <code>assert</code> statement is used, and an <code>AssertionError</code> may be raised to indicate invalid data.</p>
<p>For this baseline application, we’ll stick to the simpler and more common design pattern. We’ll look at individual functions that combine validation and cleaning. </p>

<h3 data-number="13.2.1">9.2.1  Model module refactoring</h3>
<p>We appear to have two distinct models: the ”as-acquired” model with text fields, and the ”to-be-analyzed” model with proper Python types, like <code>float</code> and <code>int</code>. The presence of multiple variations on the model means we either need a lot of distinct class names, or two distinct modules as namespaces to keep the classes organized.</p>
<p>The cleaning application is the only application where the acquire and analysis models are <strong>both </strong>used. All other applications either acquire raw data or work with clean analysis data.</p>
<p>The previous examples had a single <code>model.py</code> module with dataclasses for the acquired data. At this point, it has become more clear that this was not a great long-term decision. Because there are two distinct variations on the data model, the generic <code>model</code> module name needs to be refactored. To distinguish the acquired data model from the analytic data model, a prefix should be adequate: the module names can be <code>acquire_model</code> and <code>analysis_model</code>.</p>
<p>(The English parts of speech don’t match exactly. We’d rather not have to type ”acquisition_model”. The slightly shorter name seems easier to work with and clear enough.)</p>
<p>Within these two model files, the class names can be the same. We might have names <code>acquire_model.SeriesSample</code> and <code>analysis_model.SeriesSample</code> as distinct classes.</p>
<p>To an extent, we can sometimes copy the acquired model module to create the analysis model module. We’d need to change <code>from</code><code> dataclasses</code><code> import</code><code> dataclass</code> to the <strong>Pydantic </strong>version, <code>from</code><code> pydantic</code><code> import</code><code> dataclass</code>. This is a very small change, which makes it easy to start with. In some older versions of <strong>Pydantic </strong>and <strong>mypy</strong>, the <strong>Pydantic </strong>version of <code>dataclass</code> doesn’t expose the attribute types in a way that is transparent to the <strong>mypy</strong> tool.</p>
<p>In many cases, it can work out well to import <code>BaseModel</code> and use this as the parent class for the analytic models. Using the <code>pydantic.BaseModel</code> parent class often has a better coexistence with the <strong>mypy </strong>tool. This requires a larger change when upgrading from dataclasses to leverage the <strong>pydantic </strong>package. Since it’s beneficial when using the <strong>mypy </strong>tool, it’s the path we recommend following.</p>
<p>This <strong>Pydantic </strong>version of <code>dataclass</code> introduces a separate validator method that will be used (automatically) to process fields. For simple class definitions with a relatively clear mapping from the acquire class to the analysis class, a small change is required to the class definition.</p>
<p>One common design pattern for this new analysis model class is shown in the following example for <strong>Pydantic </strong>version 1:</p>
<div><div><pre class="source-code">from pydantic import validator, BaseModel, Field

class SeriesSample(BaseModel):
    """
    An individual sample value.
    """
    x: float = Field(title="The x attribute", ge=0.0)
    y: float = Field(title="The y attribute", ge=0.0)

    @validator(’x’, ’y’, pre=True)
    def clean_value(cls, value: str | float) -&gt; str:
        match value:
            case str():
                for char in "\N{ZERO WIDTH SPACE}":
                    value = value.replace(char, "")
                return value
            case float():
                return value</pre>
</div>
</div>
<p>This design defines a class-level method, <code>clean_value()</code>, to handle cleaning the source data when it’s a string. The validator has the <code>@validator()</code> decorator to provide the attribute names to which this function applies, as well as the specific stage in the sequence of operations. In this case, <code>pre=True</code> means this validation applies <strong>before </strong>the individual fields are validated and converted to useful types.</p>
<p>This will be replaced by a number of much more flexible alternatives in <strong>Pydantic </strong>version 2. The newer release will step away from the <code>pre=True</code> syntax used to assure this is done prior to the built-in handler accessing the field.</p>
<p>The Pydantic 2 release will introduce a radically new approach using annotations to specify validation rules. It will also retain a decorator that’s very similar to the old version 1 validation.</p>
<p>One migration path is to replace <code>validator</code> with <code>field_validator</code>. This will require changing the <code>pre=True</code> or <code>post=True</code> with a more universal <code>mode=’before’</code> or <code>mode=’after’</code>. This new approach permits writing field validators that “wrap” the conversion handler with both before and after processing.</p>
<p>To use <strong>Pydantic </strong>version two, use <code>@field_validator(’x’,</code><code> ’y’,</code><code> mode=’before’)</code> to replace the <code>@validator</code> decorator in the example. The <code>import</code> must also change to reflect the new name of the decorator.</p>
<p>This validator function handles the case where the string version of source data can include Unicode <code>U+200B</code>, a special character called the zero-width space. In Python, we can use <code>"\N{ZERO</code><code> WIDTH</code><code> SPACE}"</code> to make this character visible. While lengthy, this name seems better than the obscure <code>"\u200b"</code>.</p>
<p>(See <a class="url" href="https://www.fileformat.info/info/unicode/char/200b/index.htm">https://www.fileformat.info/info/unicode/char/200b/index.htm</a> for details of this character.)</p>
<p>When a function works in the <code>pre=True</code> or <code>mode=’before’</code> phase, then <strong>pydantic </strong>will automatically apply the final conversion function to complete the essential work of validation and conversion. This additional validator function can be designed, then, to focus narrowly only on cleaning the raw data.</p>
<p>The idea of a validator function must reflect two separate use cases for this class:</p>
<ol>
<li><div><p>Cleaning and converting acquired data, generally strings, to more useful analytical data types.</p>
</div></li>
<li><div><p>Loading already cleaned analytical data, where type conversion is not required.</p>
</div></li>
</ol>
<p>Our primary interest at this time is in the first use case, cleaning and conversion. Later, starting in chapter <a href="ch017.xhtml#x1-29700013"><em>Chapter</em><em> 13</em></a>, <a href="ch017.xhtml#x1-29700013"><em>Project 4.1: Visual Analysis Techniques</em></a> we’ll switch over to the second case, loading clean data.</p>
<p>These two use cases are reflected in the type hint for the validator function. The parameter is defined as <code>value:</code><code> str</code><code> |</code><code> float</code>. The first use case, conversion, expects a value of type <code>str</code>. The second use case, loading cleaned data, expects a cleaned value of type <code>float</code>. This kind of type of union is helpful with validator functions.</p>
<p>Instances of the analytic model will be built from <code>acquire_model</code> objects. Because the acquired model uses <code>dataclasses</code>, we can leverage the <code>dataclasses.asdict()</code> function to transform a source object into a dictionary. This can be used to perform Pydantic validation and conversion to create the analytic model objects.</p>
<p>We can add the following method in the dataclass definition:</p>
<div><div><pre class="source-code">    @classmethod
    def from_acquire_dataclass(
            cls,
            acquired: acquire_model.SeriesSample
    ) -&gt; "SeriesSample":
        return SeriesSample(**asdict(acquired))</pre>
</div>
</div>
<p>This method extracts a dictionary from the acquired data model’s version of the <code>SeriesSample</code> class and uses it to create an instance of the analytic model’s variation of this class. This method pushes all of the validation and conversion work to the <strong>Pydantic </strong>declarations. This method also requires <code>from</code><code> dataclasses</code><code> import</code><code> asdict</code> to introduce the needed <code>asdict()</code> function.</p>
<p>In cases where the field names don’t match, or some other transformation is required, a more complicated dictionary builder can replace the <code>asdict(acquired)</code> processing. We’ll see examples of this in <a href="ch014.xhtml#x1-22900010"><em>Chapter</em><em> 10</em></a>, <a href="ch014.xhtml#x1-22900010"><em>Data</em> <em>Cleaning Features</em></a>, where acquired fields need to be combined before they can be converted.</p>
<p>We’ll revisit some aspects of this design decision in <a href="ch015.xhtml#x1-26400011"><em>Chapter</em><em> 11</em></a>, <a href="ch015.xhtml#x1-26400011"><em>Project 3.7:</em> <em>Interim Data Persistence</em></a>. First, however, we’ll look at <strong>pydantic </strong>version 2 validation, which offers a somewhat more explict path to validation functions. </p>


<h3 data-number="13.2.2">9.2.2  Pydantic V2 validation</h3>
<p>While <strong>pydantic </strong>version 2 will offer a <code>@field_validator</code> decorator that’s very similar to the legacy <code>@validator</code> decorator, this approach suffers from an irksome problem. It can be confusing to have the decorator listing the fields to which the validation rule applies. Some confusion can arise because of the separation between the field definition and the function that validates the values for the field. In our example class, the validator applies to the <code>x</code> and <code>y</code> fields, a detail that might be difficult to spot when first looking at the class.</p>
<p>The newer design pattern for the analysis model class is shown in the following example for Pydantic version 2:</p>
<div><div><pre class="source-code">from pydantic import BaseModel
from pydantic.functional_validators import field_validator, BeforeValidator

from typing import Annotated

def clean_value(value: str | float) -&gt; str | float:
    match value:
        case str():
            for char in "\N{ZERO WIDTH SPACE}":
                value = value.replace(char, "")
            return value
        case float():
            return value

class SeriesSample(BaseModel):
    x: Annotated[float, BeforeValidator(clean_value)]
    y: Annotated[float, BeforeValidator(clean_value)]</pre>
</div>
</div>
<p>We’ve omitted the <code>from_acquire_dataclass()</code> method definition, since it doesn’t change.</p>
<p>The cleaning function is defined outside the class, making it more easily reused in a complicated application where a number of rules may be widely reused in several models. The <code>Annotated[]</code> type hint combines the base type with a sequence of validator objects. In this example, the base type is <code>float</code> and the validator objects are <code>BeforeValidator</code> objects that contain the function to apply.</p>
<p>To reduce the obvious duplication, a <code>TypeAlias</code> can be used. For example,</p>
<div><div><pre class="source-code">from typing import Annotated, TypeAlias

CleanFloat: TypeAlias = Annotated[float, BeforeValidator(clean_value)]</pre>
</div>
</div>
<p>Using an alias permits the model to use the type hint <code>CleanFloat</code>. For example <code>x:</code><code> CleanFloat</code>.</p>
<p>Further, the <code>Annotated</code> hints are composable. An annotation can add features to a previously-defined annotation. This ability to build more sophisticated annotations on top of foundational annotations offers a great deal of promise for defining classes in a succinct and expressive fashion.</p>
<p>Now that we’ve seen how to implement a single validation, we need to consider the alternatives, and how many different kinds of validation functions an application might need. </p>


<h3 data-number="13.2.3">9.2.3  Validation function design</h3>
<p>The <code>pydantic</code> package offers a vast number of built-in conversions based entirely on annotations. While these can cover a large number of common cases, there are still some situations that require special validators, and perhaps even special type definitions.</p>
<p>In <a href="#x1-2130004"><em>Conversions and processing</em></a>, we considered some of the kinds of processing that might be required. These included the following kinds of conversions:</p>
<ul>
<li><p>Decomposing source fields into their atomic components.</p></li>
<li><p>Merging separated source fields to create proper value. This is common with dates and times, for example.</p></li>
<li><p>Multiple subentities may be present in a feed of samples. This can be called a discriminated union: the feed as a whole is a unique of disjoint types, and a discriminator value (or values) distinguishes the various subtypes.</p></li>
<li><p>A field may be a “token” used to deidentify something about the original source. For example, a replacement token for a driver’s license number may replace the real government-issued number to make the individual anonymous.</p></li>
</ul>
<p>Additionally, we may have observability considerations that lead us to write our own a unique validator that can write needed log entries or update counters showing how many times a particular validation found problems. This enhanced visibility can help pinpoint problems with data that is often irregular or suffers from poor quality control.</p>
<p>We’ll dive into these concepts more deeply in <a href="ch014.xhtml#x1-22900010"><em>Chapter</em><em> 10</em></a>, <a href="ch014.xhtml#x1-22900010"><em>Data Cleaning</em> <em>Features</em></a>. In <a href="ch014.xhtml#x1-22900010"><em>Chapter</em><em> 10</em></a>, <a href="ch014.xhtml#x1-22900010"><em>Data Cleaning Features</em></a>, we’ll also look at features for handling primary and foreign keys. For now, we’ll focus on the built-in type conversion functions that are part of Python’s built-in functions, and the standard library. But we need to recognize that there are going to be extensions and exceptions.</p>
<p>We’ll look at the overall design approach in the next section. </p>


<h3 data-number="13.2.4">9.2.4  Incremental design</h3>
<p>The design of the cleaning application is difficult to finalize without detailed knowledge of the source data. This means the cleaning application depends on lessons learned by making a data inspection notebook. One idealized workflow begins with “understand the requirements” and proceeds to “write the code,” treating these two activities as separate, isolated steps. This conceptual workflow is a bit of a fallacy. It’s often difficult to understand the requirements without a detailed examination of the actual source data to reveal the quirks and oddities that are present. The examination of the data often leads to the first drafts of data validation functions. In this case, the requirements will take the form of draft versions of the code, not a carefully-crafted document.</p>
<p>This leads to a kind of back-and-forth between <em>ad-hoc </em>inspection and a formal data cleaning application. This iterative work often leads to a module of functions to handle the problem domain’s data. This module can be shared by inspection notebooks as well as automated applications. Proper engineering follows the <strong>DRY </strong>(<strong>Don’t Repeat Yourself</strong>) principle: code should not be copied and pasted between modules. It should be put into a shared module so it can be reused properly.</p>
<p>In some cases, two data cleaning functions will be similar. Finding this suggests some kind of decomposition is appropriate to separate the common parts from the unique parts. The redesign and refactoring are made easier by having a suite of unit tests to confirm that no old functionality was broken when the functions were transformed to remove duplicated code.</p>
<p>The work of creating cleaning applications is iterative and incremental. Rare special cases are — well — rare, and won’t show up until well after the processing pipeline seems finished. The unexpected arrival special case data is something like birders seeing a bird outside its expected habitat. It helps to think of a data inspection notebook like a bird watcher’s immense spotting scope, used to look closely at one unexpected, rare bird, often in a flock of birds with similar feeding and roosting preferences. The presence of the rare bird becomes a new datapoint for ornithologists (and amateur enthusiasts). In the case of unexpected data, the inspection notebook’s lessons become a new code for the conversions module.</p>
<p>The overall main module in the data cleaning application will implement the <strong>command-line interface </strong>(<strong>CLI</strong>). We’ll look at this in the next section. </p>


<h3 data-number="13.2.5">9.2.5  CLI application</h3>
<p>The UX for this application suggests that it operates in the following distinct contexts:</p>
<ul>
<li><p>As a standalone application. The user runs the <code>src/acquire.py</code> program. Then, the user runs the <code>src/clean.py</code> program.</p></li>
<li><p>As a stage in a processing pipeline. The user runs a shell command that pipes the output from the <code>src/acquire.py</code> program into the <code>src/clean.py</code> program. This is the subject of <a href="ch014.xhtml#x1-2510005"><em>Project 3.6: Integration</em> <em>to create an acquisition pipeline</em></a>.</p></li>
</ul>
<p>This leads to the following two runtime contexts:</p>
<ul>
<li><p>When the application is provided an input path, it’s being used as a stand-alone application.</p></li>
<li><p>When no input path is provided, the application reads from <code>sys.stdin</code>.</p></li>
</ul>
<p>A similar analysis can apply to the <strong>acquire </strong>application. If an output path is provided, the application creates and writes the named file. If no output path is provided, the application writes to <code>sys.stdout</code>.</p>
<p>One essential consequence of this is all logging <strong>must </strong>be written to <code>sys.stderr</code>.</p>
<div><div><p>Use <strong>stdin </strong>and <strong>stdout </strong>exclusively for application data, nothing else.</p>
<p>Use a consistent, easy-to-parse text format like ND JSON for application data.</p>
<p>Use <strong>stderr </strong>as the destination for all control and error messages.</p>
<p>This means <code>print()</code> may require the <code>file=sys.stderr</code> to direct debugging output to <strong>stderr</strong>. Or, avoid simple <code>print()</code> and use <code>logger.debug()</code> instead.</p>
</div>
</div>
<p>For this project, the stand-alone option is all that’s needed. However, it’s important to understand the alternatives that will be added in later projects. See <a href="ch014.xhtml#x1-2510005"><em>Project 3.6: Integration to create an acquisition pipeline</em></a> for this more tightly-integrated alternative.</p>

<h4 class="likesubsubsectionHead" data-number="13.2.5.1">Redirecting stdout</h4>
<p>Python provides a handy tool for managing the choice between ”write to an open file” and ”write to <strong>stdout</strong>”. It involves the following essential design principle.</p>
<div><div><p>Always provide file-like objects to functions and methods processing data.</p>
</div>
</div>
<p>This suggests a data-cleaning function like the following:</p>
<div><div><pre class="source-code">from typing import TextIO

def clean_all(acquire_file: TextIO, analysis_file: TextIO) -&gt; None:
    ...</pre>
</div>
</div>
<p>This function can use <code>json.loads()</code> to parse each document from the <code>acquire_file</code>. It uses <code>json.dumps()</code> to save each document to the <code>analysis_file</code> to be used for later analytics.</p>
<p>The overall application can then make a choice among four possible ways to use this <code>clean_all()</code> function:</p>
<ul>
<li><p><strong>Stand-alone</strong>: This means <code>with</code> statements manage the open files created from the <code>Path</code> names provided as argument values.</p></li>
<li><p><strong>Head</strong> <strong>of a pipeline</strong>: A <code>with</code> statement can manage an open file passed to <code>acquire_file</code>. The value of <code>analysis_file</code> is <code>sys.stdout</code>.</p></li>
<li><p><strong>Tail of a pipeline</strong>: The acquired input file is <code>sys.stdin</code>. A <code>with</code> statement manages an open file (in write mode) for the <code>analysis_file</code>.</p></li>
<li><p><strong>Middle of</strong> <strong>a pipeline</strong>: The <code>acquire_file</code> is <code>sys.stdin</code>; the <code>analysis_file</code> is <code>sys.stdout</code>.</p></li>
</ul>
<p>Now that we’ve looked at a number of technical approaches, we’ll turn to the list of deliverables for this project in the next section. </p>




<h2 data-number="13.3">9.3  Deliverables</h2>
<p>This project has the following deliverables:</p>
<ul>
<li><p>Documentation in the <code>docs</code> folder.</p></li>
<li><p>Acceptance tests in the <code>tests/features</code> and <code>tests/steps</code> folders.</p></li>
<li><p>Unit tests for the application modules in the <code>tests</code> folder.</p></li>
<li><p>Application to clean some acquired data and apply simple conversions to a few fields. Later projects will add more complex validation rules.</p></li>
</ul>
<p>We’ll look at a few of these deliverables in a little more detail.</p>
<p>When starting a new kind of application, it often makes sense to start with acceptance tests. Later, when adding features, the new acceptance tests may be less important than new unit tests for the features. We’ll start by looking at a new scenario for this new application. </p>

<h3 data-number="13.3.1">9.3.1  Acceptance tests</h3>
<p>As we noted in <a href="ch008.xhtml#x1-780004"><em>Chapter</em><em> 4</em></a>, <a href="ch008.xhtml#x1-780004"><em>Data Acquisition Features: Web APIs and Scraping</em></a>, we can provide a large block of text as part of a Gherkin scenario. This can be the contents of an input file. We can consider something like the following scenario.</p>
<div><div><pre class="source-code">Scenario: Valid file is recognized.
    Given a file "example_1.ndjson" with the following content
        """
        {"x": "1.2", "y": "3.4"}
        {"x": "five", "z": null}
        """
    When the clean tool is run
    Then the output shows 1 good records
    And the output shows 1 faulty records</pre>
</div>
</div>
<p>This kind of scenario lets us define source documents with valid data. We can also define source documents with invalid data.</p>
<p>We can use the <code>Then</code> steps to confirm additional details of the output. For example, if we’ve decided to make all of the cleaning operations visible, the test scenario can confirm the output contains all of the cleanup operations that were applied.</p>
<p>The variety of bad data examples and the number of combinations of good and bad data suggest there can be a lot of scenarios for this kind of application. Each time new data shows up that is acquired, but cannot be cleaned, new examples will be added to these acceptance test cases.</p>
<p>It can, in some cases, be very helpful to publish the scenarios widely so all of the stakeholders can understand the data cleaning operations. The Gherkin language is designed to make it possible for people with limited technical skills to contribute to the test cases.</p>
<p>We also need scenarios to run the application from the command-line. The <code>When</code> step definition for these scenarios will be <code>subprocess.run()</code> to invoke the <strong>clean </strong>application, or to invoke a shell command that includes the <strong>clean</strong> application. </p>


<h3 data-number="13.3.2">9.3.2  Unit tests for the model features</h3>
<p>It’s important to have automated unit tests for the model definition classes.</p>
<p>It’s also important to <strong>not </strong>test the <code>pydantic</code> components. We don’t, for example, need to test the ordinary string-to-float conversions the <code>pydantic</code> module already does; we can trust this works perfectly.</p>
<p>We <strong>must </strong>test the validator functions we’ve written. This means providing test cases to exercise the various features of the validators. Additionally, any overall <code>from_acquire_dataclass()</code> method needs to have test cases.</p>
<p>Each of these test scenarios works with a given acquired document with the raw data. When the <code>from_acquire_dataclass()</code> method is evaluated, then there may be an exception or a resulting analytic model document is created.</p>
<p>The exception testing can make use of the <code>pytest.raises()</code> context manager. The test is written using a <code>with</code> statement to capture the exception.</p>
<p>See <a class="url" href="https://docs.pytest.org/en/7.2.x/how-to/assert.html#assertions-about-expected-exceptions">https://docs.pytest.org/en/7.2.x/how-to/assert.html#assertions-about-expected-exceptions</a> for examples.</p>
<p>Of course, we also need to test the processing that’s being done. By design, there isn’t very much processing involved in this kind of application. The bulk of the processing can be only a few lines of code to consume the raw model objects and produce the analytical objects. Most of the work will be delegated to modules like <code>json</code> and <code>pydantic</code>. </p>


<h3 data-number="13.3.3">9.3.3  Application to clean data and create an NDJSON interim file</h3>
<p>Now that we have acceptance and unit test suites, we’ll need to create the <code>clean</code> application. Initially, we can create a place-holder application, just to see the test suite fail. Then we can fill in the various pieces until the application – as a whole – works.</p>
<p>Flexibility is paramount in this application. In the next chapter, <a href="ch014.xhtml#x1-22900010"><em>Chapter</em><em> 10</em></a>, <a href="ch014.xhtml#x1-22900010"><em>Data Cleaning Features</em></a>, we will introduce a large number of data validation scenarios. In <a href="ch015.xhtml#x1-26400011"><em>Chapter</em><em> 11</em></a>, <a href="ch015.xhtml#x1-26400011"><em>Project 3.7: Interim Data Persistence</em></a> we’ll revisit the idea of saving the cleaned data. For now, it’s imperative to create clean data; later, we can consider what format might be best. </p>



<h2 data-number="13.4">9.4  Summary</h2>
<p>This chapter has covered a number of aspects of data validation and cleaning applications:</p>
<ul>
<li><p>CLI architecture and how to design a simple pipeline of processes.</p></li>
<li><p>The core concepts of validating, cleaning, converting, and standardizing raw data.</p></li>
</ul>
<p>In the next chapter, we’ll dive more deeply into a number of data cleaning and standardizing features. Those projects will all build on this base application framework. After those projects, the next two chapters will look a little more closely at the analytical data persistence choices, and provide an integrated web service for providing cleaned data to other stakeholders. </p>


<h2 data-number="13.5">9.5  Extras</h2>
<p>Here are some ideas for you to add to this project. </p>

<h3 data-number="13.5.1">9.5.1  Create an output file with rejected samples</h3>
<p>In <a href="#x1-2140005"><em>Error reports</em></a> we suggested there are times when it’s appropriate to create a file of rejected samples. For the examples in this book — many of which are drawn from well-curated, carefully managed data sets — it can feel a bit odd to design an application that will reject data.</p>
<p>For enterprise applications, data rejection is a common need.</p>
<p>It can help to look at a data set like this: <a class="url" href="https://datahub.io/core/co2-ppm">https://datahub.io/core/co2-ppm</a>. This contains data same with measurements of CO2 levels measures with units of ppm, parts per million.</p>
<p>This has some samples with an invalid number of days in the month. It has some samples where a monthly CO2 level wasn’t recorded.</p>
<p>It can be insightful to use a rejection file to divide this data set into clearly usable records, and records that are not as clearly usable.</p>
<p>The output will <strong>not </strong>reflect the analysis model. These objects will reflect the acquire model; they are the items that would not convert properly from the acquired structure to the desired analysis structure. </p>



</body>
</html>
