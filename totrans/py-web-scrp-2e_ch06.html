<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Interacting with Forms</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Interacting with Forms</h1>
            </header>

            <article>
                
<p>In earlier chapters, we downloaded static web pages that return the same content. In this chapter, we will interact with web pages which&#160;depend on user input and state to return relevant content. This chapter will cover the following topics:</p>
<ul>
<li>Sending a <kbd>POST</kbd> request to submit a form</li>
<li>Using cookies and sessions to log in to a website</li>
<li>Using Selenium for form submissions</li>
</ul>
<p>To interact with these forms, you'll need a user account to log in to the website. You can register an account manually at <a href="http://example.webscraping.com/user/register" target="_blank"><span class="URLPACKT">http://example.webscraping.com/user/register</span></a>. Unfortunately, we can't yet automate the registration form until the next chapter, which deals with <kbd>CAPTCHA</kbd> images.</p>
<div class="packt_infobox"><span class="packt_screen">Form methods<br/></span>HTML forms define two methods for submitting data to the server-<kbd>GET</kbd> and <kbd>POST</kbd>. With the <kbd>GET</kbd> method, data such as&#160;<kbd>?name1=value1&amp;name2=value2</kbd> is appended to the URL, which is known as a "query string". The browser sets a limit on the URL length, so this is only useful for small amounts of data. Additionally, this method is generally intended to only retrieve data from the server and not make changes to it, but sometimes this intention is ignored. With <kbd>POST</kbd> requests, the data is sent in the request body, not the URL. Sensitive data should only be sent in a <kbd>POST</kbd> request to avoid exposing it in the URL. How the <kbd>POST</kbd> data is represented in the body depends on the encoding type.<br/>
Servers can also support other HTTP methods, such as <kbd>PUT</kbd> and <kbd>DELETE</kbd>, however, these are not supported in standard&#160;HTML forms.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">The Login form</h1>
            </header>

            <article>
                
<p>The first form we'll automate is the <span class="packt_screen">Login</span> form, which is available at <a href="http://example.webscraping.com/user/login" target="_blank"><span class="URLPACKT">http://example.webscraping.com/user/login</span></a>. To understand the form, we can use&#160;our browser development tools. With the full version of Firebug or Chrome Developer Tools, it is possible to simply submit the form and check what data was transmitted in the Network tab (similar to how we did in <a href="py-web-scrp-2e_ch05.html" target="_blank">Chapter 5</a>, <em>Dynamic Content</em>). However, we can also see information about the form if we use "Inspect Element" features:</p>
<div class="CDPAlignCenter CDPAlign"><img height="508" width="470" class="image-border" src="images/post_image.png"/></div>
<p>The important parts regarding how to send the form are the <kbd>action</kbd>, <kbd>enctype</kbd>, and <kbd>method</kbd> attributes of the <kbd>form</kbd> tag, and the two <kbd>input</kbd> fields (in the above image we have expanded the "password" field). The <kbd>action</kbd> attribute sets the HTTP location where the form data will be submitted, in this case, <kbd>#</kbd>, which represents the current URL. The <kbd>enctype</kbd> attribute (or encoding type) sets the encoding used for the submitted data, in this case, <kbd>application/x-www-form-urlencoded</kbd>. The <kbd>method</kbd> attribute is set to <kbd>post</kbd> to submit form data with a <kbd>POST</kbd> method&#160;in the message body to the server. For each&#160;<kbd>input</kbd> tags, the important attribute is <kbd>name</kbd>, which sets the name of the field when the <kbd>POST</kbd>&#160;data is submitted to the server.</p>
<div class="packt_infobox"><span class="packt_screen">Form encoding<br/></span>When a form uses the <kbd>POST</kbd> method, there are two useful choices for how the form data is encoded before being submitted to the server. The default is <kbd>application/x-www-form-urlencoded</kbd>, which specifies all non-alphanumeric characters must be converted to ASCII Hex values. However, this is inefficient for forms which&#160;contain a large amount of non-alphanumeric data, such as a binary file upload, so <kbd>multipart/form-data</kbd> encoding was defined. Here, the input is not encoded but sent as multiple parts using the MIME protocol, which is the same standard used for e-mail.<br/>
The official details of this standard can be viewed at <span class="URLPACKT"><a href="http://www.w3.org/TR/html5/forms.html" target="_blank">http://www.w3.org/TR/html5/forms.html</a>#selecting-a-form-submission-encoding</span>.</div>
<p>When regular users open this web page in their browser, they will enter their e-mail and password, and click on the <span class="packt_screen">Login</span> button to submit their details to the server. Then, if the login process on the server is successful, they will be redirected to the home page; otherwise, they will return to the <span class="packt_screen">Login</span> page to try again. Here is an initial attempt to automate this process:</p>
<pre><strong>&gt;&gt;&gt; from urllib.parse import urlencode</strong><br/><strong>&gt;&gt;&gt; from urllib.request import Request, urlopen</strong><br/><strong>&gt;&gt;&gt; LOGIN_URL = 'http://example.webscraping.com/user/login'</strong><br/><strong>&gt;&gt;&gt; LOGIN_EMAIL = 'example@webscraping.com'</strong><br/><strong>&gt;&gt;&gt; LOGIN_PASSWORD = 'example'</strong><br/><strong>&gt;&gt;&gt; data = {'email': LOGIN_EMAIL, 'password': LOGIN_PASSWORD}</strong><br/><strong>&gt;&gt;&gt; encoded_data = urlencode(data)</strong><br/><strong>&gt;&gt;&gt; request = Request(LOGIN_URL, encoded_data.encode('utf-8'))</strong><br/><strong>&gt;&gt;&gt; response = urlopen(request)</strong><br/><strong>&gt;&gt;&gt; print(response.geturl())</strong><br/><strong> 'http://example.webscraping.com/user/login'</strong>
</pre>
<p>This example sets the e-mail and password fields, encodes them with <kbd>urlencode</kbd>, and submits them to the server. When the final print statement is executed, it will output the URL of the <span class="packt_screen">Login</span> page, which means the login process has failed. You will notice we must also encode the already encoded data as bytes so&#160;<kbd>urllib</kbd> will accept it.&#160;</p>
<p>We can write the same process using <kbd>requests</kbd> in fewer lines:</p>
<pre><strong>&gt;&gt;&gt; import requests</strong><br/><strong>&gt;&gt;&gt; response = requests.post(LOGIN_URL, data)</strong><br/><strong>&gt;&gt;&gt; print(response.url)</strong><br/><strong> 'http://example.webscraping.com/user/login'</strong>
</pre>
<p>The&#160;<kbd>requests</kbd> library allows us to explicitly post data, and will do the encoding internally. Unfortunately, this code still fails to log in.</p>
<p>The&#160;<span class="packt_screen">Login</span> form is particularly strict and requires some additional fields to be submitted along with the e-mail and password. These additional fields can be found at the bottom of the previous screenshot, but are set to <kbd>hidden</kbd> and so they aren't displayed in the browser. To access these hidden fields, here is a function using the <kbd>lxml</kbd> library covered in <span class="ChapterrefPACKT">Chapter 2</span>, <em>Scraping the Data</em>, to extract all the <kbd>input</kbd> tag details in a form:</p>
<pre>from lxml.html import fromstring<br/><br/>def parse_form(html): <br/>    tree = fromstring(html) <br/>    data = {} <br/>    for e in tree.cssselect('form input'): <br/>        if e.get('name'): <br/>            data[e.get('name')] = e.get('value') <br/>    return data 
</pre>
<p>The function in the preceding code uses <kbd>lxml</kbd> CSS selectors to iterate&#160;over all&#160;<kbd>input</kbd> tags in a form and return their <kbd>name</kbd> and <kbd>value</kbd> attributes in a dictionary. Here is the result when the code is run on the <span class="packt_screen">Login</span> page:</p>
<pre><strong>&gt;&gt;&gt; html = requests.get(LOGIN_URL)</strong><br/><strong>&gt;&gt;&gt; form = parse_form(html.content) </strong><br/><strong>&gt;&gt;&gt; print(form) </strong><br/><strong>{'_formkey': 'a3cf2b3b-4f24-4236-a9f1-8a51159dda6d',</strong><br/><strong> '_formname': 'login',</strong><br/><strong> '_next': '/',</strong><br/><strong> 'email': '',</strong><br/><strong> 'password': '',</strong><br/><strong> 'remember_me': 'on'}</strong>
</pre>
<p>The <kbd>_formkey</kbd> attribute is the crucial piece; it contains&#160;a unique ID used by the server to prevent multiple form submissions. Each time the web page is loaded, a different ID is used, and the server can tell whether a form with a given ID has already been submitted. Here is an updated version of the login process&#160;which&#160;submits <kbd>_formkey</kbd> and other hidden values:</p>
<pre><strong>&gt;&gt;&gt; html = requests.get(LOGIN_URL)</strong><br/><strong>&gt;&gt;&gt; data = parse_form(html.content) </strong><br/><strong>&gt;&gt;&gt; data['email'] = LOGIN_EMAIL </strong><br/><strong>&gt;&gt;&gt; data['password'] = LOGIN_PASSWORD </strong><br/><strong>&gt;&gt;&gt; response = requests.post(LOGIN_URL, data) </strong><br/><strong>&gt;&gt;&gt; response.url </strong><br/><strong>'http://example.webscraping.com/user/login'</strong>
</pre>
<p>Unfortunately, this version doesn't work either, because&#160;the login URL was again returned.&#160;We are missing another essential&#160;component--browser cookies. When a regular user loads the <span class="packt_screen">Login</span> form, this <kbd>_formkey</kbd> value is&#160;stored in a cookie, which is compared to the <kbd>_formkey</kbd> value in the submitted <span class="packt_screen">Login</span> form data.&#160;We can take a look at the cookies and their values via our&#160;<kbd>response</kbd> object:</p>
<pre><strong>&gt;&gt;&gt; response.cookies.keys()</strong><br/><strong>['session_data_places', 'session_id_places']</strong><br/><strong>&gt;&gt;&gt; response.cookies.values()</strong><br/><strong>['"8bfbd84231e6d4dfe98fd4fa2b139e7f:N-almnUQ0oZtHRItjUOncTrmC30PeJpDgmAqXZEwLtR1RvKyFWBMeDnYQAIbWhKmnqVp-deo5Xbh41g87MgYB-oOpLysB8zyQci2FhhgU-YFA77ZbT0hD3o0NQ7aN_BaFVrHS4DYSh297eTYHIhNagDjFRS4Nny_8KaAFdcOV3a3jw_pVnpOg2Q95n2VvVqd1gug5pmjBjCNofpAGver3buIMxKsDV4y3TiFO97t2bSFKgghayz2z9jn_iOox2yn8Ol5nBw7mhVEndlx62jrVCAVWJBMLjamuDG01XFNFgMwwZBkLvYaZGMRbrls_cQh"',</strong><br/><strong> 'True']</strong>
</pre>
<p>You can also see via your Python interpreter that the&#160;<kbd>response.cookies</kbd> is a special object type, called a cookie jar. This object can also be passed to new requests. Let's retry our submission with cookies:</p>
<pre><strong>&gt;&gt;&gt; second_response = requests.post(LOGIN_URL, data, cookies=html.cookies)</strong><br/><strong>&gt;&gt;&gt; second_response.url</strong><br/><strong>'http://example.webscraping.com/'</strong>
</pre>
<div class="packt_infobox"><span class="packt_screen">What are cookies?<br/></span>Cookies are small amounts of data sent by a website in the HTTP <kbd>response</kbd> headers, which look like this: <kbd>Set-Cookie: session_id=example</kbd>;. The web browser will store them, and then include them in the headers of subsequent requests to that website. This allows a website to identify and track users.</div>
<p>Success! The submitted form values have been accepted and the <kbd>response</kbd> URL is the home page. Note that we needed to use the cookies which properly align with our form data from our initial request (which we have stored in the <kbd>html</kbd> variable). This snippet and the other login examples in this chapter are available for download at <a href="https://github.com/kjam/wswp/tree/master/code/chp6">https://github.com/kjam/wswp/tree/master/code/chp6</a>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Loading cookies from the web browser</h1>
            </header>

            <article>
                
<p>Working out how to submit the login details expected by a&#160;server can be quite complex, as demonstrated by the previous example. Fortunately, there's a workaround for difficult websites--we can log in to the website manually using a web browser, and have our Python script load and reuse the cookies to be automatically logged in.</p>
<p>Some&#160;web browsers store their cookies in different formats, but Firefox and Chrome use an easy-to-access format we can parse with Python:&#160;a <kbd>sqlite</kbd> database. &#160;</p>
<div class="packt_infobox"><a href="https://www.sqlite.org/">SQLite</a> is a very popular open-source SQL database. It can be easily installed on many platforms and comes pre-installed on Mac OSX. To download and install it on your operating system, check <a href="https://www.sqlite.org/download.html">the Download page</a> or simply search for your operating system instructions.&#160;</div>
<p>To take a look at your cookies, you can (if installed) run the <kbd>sqlite3</kbd> command and then the path to your cookie file (shown below is an example for Chrome):</p>
<pre><strong>$ sqlite3 [path_to_your_chrome_browser]/Default/Cookies</strong><br/><strong>SQLite version 3.13.0 2016-05-18 10:57:30</strong><br/><strong>Enter ".help" for usage hints.</strong><br/><strong>sqlite&gt; .tables</strong><br/><strong>cookies meta</strong>
</pre>
<p>You will need to first find the path to your browser's configuration files which can either be done by searching your filesystem or simply searching the web for your browser and operating system. To see table schema in SQLite, you can use <kbd>.schema</kbd> and select syntax functions similarly to other SQL databases.&#160;</p>
<p>In addition to storing cookies in a&#160;<kbd>sqlite</kbd> database, some browsers (such as&#160;Firefox) store sessions directly in a JSON file, which can be easily parsed using Python. There are also numerous browser extensions, such as SessionBuddy which&#160;can export your sessions into JSON files. For the login, we only need to find the proper sessions, which are stored in this structure:</p>
<pre>{"windows": [... <br/>  "cookies": [ <br/>    {"host":"example.webscraping.com", <br/>     "value":"514315085594624:e5e9a0db-5b1f-4c66-a864", <br/>     "path":"/", <br/>     "name":"session_id_places"} <br/>  ...] <br/>]} 
</pre>
<p>Here is a function that can be used to parse Firefox&#160;sessions into a Python dictionary, which we can then feed to the&#160;<kbd>requests</kbd> library:</p>
<pre>def load_ff_sessions(session_filename): <br/>    cookies = {}<br/>    if os.path.exists(session_filename): <br/>        json_data = json.loads(open(session_filename, 'rb').read()) <br/>        for window in json_data.get('windows', []): <br/>            for cookie in window.get('cookies', []): <br/>                cookies[cookie.get('name')] = cookie.get('value')<br/>    else: <br/>        print('Session filename does not exist:', session_filename)<br/>    return cookies
</pre>
<p>One complexity is that the location of the Firefox sessions file will vary, depending on the operating system. On Linux, it should be located at this path:</p>
<pre>~/.mozilla/firefox/*.default/sessionstore.js 
</pre>
<p>In OS X, it should be located at:</p>
<pre>~/Library/Application Support/Firefox/Profiles/*.default/ <br/>   sessionstore.js 
</pre>
<p>Also, for Windows Vista and above, it should be located at:</p>
<pre>%APPDATA%/Roaming/Mozilla/Firefox/Profiles/*.default/sessionstore.js 
</pre>
<p>Here is a helper function to return the path to the session file:</p>
<pre>import os, glob <br/>def find_ff_sessions(): <br/>    paths = [ <br/>        '~/.mozilla/firefox/*.default', <br/>        '~/Library/Application Support/Firefox/Profiles/*.default', <br/>        '%APPDATA%/Roaming/Mozilla/Firefox/Profiles/*.default' <br/>    ] <br/>    for path in paths: <br/>        filename = os.path.join(path, 'sessionstore.js') <br/>        matches = glob.glob(os.path.expanduser(filename)) <br/>        if matches: m<br/>            return matches[0] 
</pre>
<p>Note that the <kbd>glob</kbd> module used here will return all matching files for the given path. Now here is an updated snippet using the browser cookies to log in:</p>
<pre><strong>    &gt;&gt;&gt; session_filename = find_ff_sessions() </strong><br/><strong>    &gt;&gt;&gt; cookies = load_ff_sessions(session_filename) </strong><br/><strong>    &gt;&gt;&gt; url = 'http://example.webscraping.com' </strong><br/><strong>    &gt;&gt;&gt; html = requests.get(url, cookies=cookies)</strong>
</pre>
<p>To check whether the session was loaded successfully, we cannot rely on the login redirect this time. Instead, we will scrape the resulting HTML to check whether the logged in user label exists. If the result here is <kbd>Login</kbd>, the sessions have failed to load correctly. If this is the case, make sure you are already logged in to the example website using your Firefox browser.&#160;We can inspect the&#160;<kbd>User</kbd> label for the site using our browser tools:</p>
<div class="CDPAlignCenter CDPAlign"><img height="467" width="449" class="image-border" src="images/user_nav.png"/></div>
<p>The browser tools&#160;show this label is located within a <kbd>&lt;ul&gt;</kbd> tag of ID "navbar", which can easily be extracted with the <kbd>lxml</kbd> library used in <a href="py-web-scrp-2e_ch02.html" target="_blank"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Scraping the Data</em>:</p>
<pre><strong>&gt;&gt;&gt; tree = fromstring(html.content) </strong><br/><strong>&gt;&gt;&gt; tree.cssselect('ul#navbar li a')[0].text_content() </strong><br/><strong>'Welcome Test account'</strong> 
</pre>
<p>The code in this section was quite complex and only supports loading sessions from the Firefox browser. There are numerous browser add-ons and extensions that support saving your sessions in JSON files, so you can explore these&#160;as an option if you need session data for login.</p>
<p>In the next section, we will&#160;take a look at the&#160;<kbd>requests</kbd> library advanced usage for sessions&#160;<a href="http://docs.python-requests.org/en/master/user/advanced/#session-objects" target="_blank">http://docs.python-requests.org/en/master/user/advanced/#session-objects</a>, which allows you utilize browser sessions easily when scraping with Python.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Extending the login script to update content</h1>
            </header>

            <article>
                
<p>Now that we can login via a script, we can extend this script by adding code to update the website country data. The code used in this section is available at <a href="https://github.com/kjam/wswp/blob/master/code/chp6/edit.py">https://github.com/kjam/wswp/blob/master/code/chp6/edit.py</a> and <a href="https://github.com/kjam/wswp/blob/master/code/chp6/login.py">https://github.com/kjam/wswp/blob/master/code/chp6/login.py</a>.</p>
<p>You may have already noticed an <span class="packt_screen">Edit</span> link at the bottom of each country:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="419" width="313" class="image-border" src="images/4364OS_06_03.png"/></div>
<p>When logged in, clicking this link leads to another page where each property of a country can be edited:</p>
<div class="CDPAlignCenter CDPAlign"><img height="407" width="279" class="image-border" src="images/4364OS_06_04.png"/></div>
<p>We will make a script to increase the population of a country by one person every time it's run. The first step is to rewrite our <kbd>login</kbd> function to utilize <kbd>Session</kbd> objects. This will make our code cleaner and allow us to remain logged into our current session. The new code is as follows:</p>
<pre>def login(session=None):<br/>    """ Login to example website.<br/>        params:<br/>            session: request lib session object or None<br/>        returns tuple(response, session)<br/>    """<br/>    if session is None:<br/>        html = requests.get(LOGIN_URL)<br/>    else:<br/>        html = session.get(LOGIN_URL)<br/>    data = parse_form(html.content)<br/>    data['email'] = LOGIN_EMAIL<br/>    data['password'] = LOGIN_PASSWORD<br/>    if session is None:<br/>        response = requests.post(LOGIN_URL, data, cookies=html.cookies)<br/>    else:<br/>        response = session.post(LOGIN_URL, data)<br/>    assert 'login' not in response.url<br/>    return response, session
</pre>
<p>Now our login form can work with or without sessions. By default, it doesn't use sessions and expects the user to utilize the cookies to stay logged in. This can be problematic for some forms, however, so adding the session functionality is useful when extending our login function. Next, we need to extract&#160;the current values of the country by reusing the <kbd>parse_form()</kbd> function:</p>
<pre><strong>&gt;&gt;&gt; from chp6.login import login, parse_form </strong><br/><strong>&gt;&gt;&gt; session = requests.Session()</strong><br/><strong>&gt;&gt;&gt; COUNTRY_URL = 'http://example.webscraping.com/edit/United-Kingdom-239' </strong><br/><strong>&gt;&gt;&gt; response, session = login(session=session)</strong><br/><strong>&gt;&gt;&gt; country_html = session.get(COUNTRY_URL)</strong><br/><strong>&gt;&gt;&gt; data = parse_form(country_html.content) </strong><br/><strong>&gt;&gt;&gt; data</strong><br/><strong>{'_formkey': 'd9772d57-7bd7-4572-afbd-b1447bf3e5bd',</strong><br/><strong> '_formname': 'places/2575175',</strong><br/><strong> 'area': '244820.00',</strong><br/><strong> 'capital': 'London',</strong><br/><strong> 'continent': 'EU',</strong><br/><strong> 'country': 'United Kingdom',</strong><br/><strong> 'currency_code': 'GBP',</strong><br/><strong> 'currency_name': 'Pound',</strong><br/><strong> 'id': '2575175',</strong><br/><strong> 'iso': 'GB',</strong><br/><strong> 'languages': 'en-GB,cy-GB,gd',</strong><br/><strong> 'neighbours': 'IE',</strong><br/><strong> 'phone': '44',</strong><br/><strong> 'population': '62348448',</strong><br/><strong> 'postal_code_format': '@# #@@|@## #@@|@@# #@@|@@## #@@|@#@ #@@|@@#@ #@@|GIR0AA',</strong><br/><strong> 'postal_code_regex': '^(([A-Z]d{2}[A-Z]{2})|([A-Z]d{3}[A-Z]{2})|([A-Z]{2}d{2}[A-Z]{2})|([A-Z]{2}d{3}[A-Z]{2})|([A-Z]erd[A-Z]d[A-Z]{2})|([A-Z]{2}d[A-Z]d[A-Z]{2})|(GIR0AA))$',</strong><br/><strong> 'tld': '.uk'}</strong>
</pre>
<p>Now we can increase the population by one and submit the updated version to the server:</p>
<pre><strong>&gt;&gt;&gt; data['population'] = int(data['population']) + 1 </strong><br/><strong>&gt;&gt;&gt; response = session.post(COUNTRY_URL, data)</strong> 
</pre>
<p>When we return to the country page, we can verify that the population has increased to&#160;<span>62,348,449</span>:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="305" width="442" class="image-border" src="images/population_increase.png"/></div>
<p>Feel free to test and modify the other fields as well--the database is restored to the original country data each hour to keep the data sane. &#160;There is code for modifying the currency field in <a href="https://github.com/kjam/wswp/blob/master/code/chp6/edit.py">the edit script</a> to use as another example. You can also play around with modifying other countries.</p>
<p>Note that the example covered here is not strictly web scraping, but falls under the wider scope of online bots. The form techniques we used can also be applied to interacting with complex forms to access data you want to scrape. Make sure you use your new automated form powers for good and not for spam or malicious content bots!</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Automating forms with Selenium</h1>
            </header>

            <article>
                
<p>The examples built so far work, but each form requires a fair amount of work and testing. This effort can be minimized by using Selenium as we did in <a href="py-web-scrp-2e_ch05.html" target="_blank">Chapter 5</a>, <em>Dynamic Content</em>. Because it is a browser-based solution, Selenium can mock&#160;many user interactions including clicks, scrolling and typing. If you are using it with a headless browser like PhantomJS, you will also be able to parallelize and scale your processes because it has less overhead than running a full browser.</p>
<div class="packt_infobox">Using a complete browser can also be a good&#160;solution for "humanizing" your interactions, particularly if you are using a well-known browser or other browser-like headers which can set you apart from other more robot-like identifiers.</div>
<p>Rewriting our login and editing scripts to use Selenium is fairly straightforward, but we must first investigate the page to pick out the CSS or XPath identifiers to&#160;use. Doing so with our browser tools, we notice the login form has easy-to-identify CSS IDs for the login form and the country edit form. Now we can rewrite the login and edit using Selenium.</p>
<p>First, let's write a few methods for getting a driver and logging in:</p>
<pre>from selenium import webdriver<br/>from selenium.webdriver.common.keys import Keys<br/>from selenium.webdriver.common.by import By<br/>from selenium.webdriver.support.ui import WebDriverWait<br/>from selenium.webdriver.support import expected_conditions as EC<br/><br/><br/>def get_driver():<br/>    try:<br/>        return webdriver.PhantomJS()<br/>    except Exception:<br/>        return webdriver.Firefox()<br/><br/><br/>def login(driver):<br/>    driver.get(LOGIN_URL)<br/>    driver.find_element_by_id('auth_user_email').send_keys(LOGIN_EMAIL)<br/>    driver.find_element_by_id('auth_user_password').send_keys(<br/>        LOGIN_PASSWORD + Keys.RETURN)<br/>    pg_loaded = WebDriverWait(driver, 10).until(<br/>        EC.presence_of_element_located((By.ID, "results")))<br/>    assert 'login' not in driver.current_url
</pre>
<p>Here the <kbd>get_driver</kbd> function&#160;first attemps to get a PhantomJS driver, since it is faster and easier to install on servers. If that fails, we use Firefox. The <kbd>login</kbd> function uses a&#160;<kbd>driver</kbd> object passed as the argument, and uses the browser driver to login by first loading the page, then using the driver's&#160;<kbd>send_keys</kbd> method to type into the identified input elements. The&#160;<kbd>Keys.RETURN</kbd> sends the signal for a Return key, which on many forms will be mapped to submit the form.&#160;</p>
<p>We are also utilizing the Selenium explicit waits (<kbd>WebDriverWait</kbd> and&#160;<kbd>EC</kbd> for ExpectedConditions),&#160;which allow us to tell the browser to wait until a particular element or condition is met. In this case, we know that the homepage when logged in shows an element with the CSS ID <kbd>"results"</kbd>. The&#160;<kbd>WebDriverWait</kbd> object will wait 10 seconds for the element to load before raising an Exception. We can easily toggle this wait, or use other expected conditions to match how the page we are currently loading behaves.</p>
<div class="packt_infobox">To read more about Selenium explicit waits, I recommend looking at the Python bindings documentation:&#160;<a href="http://selenium-python.readthedocs.io/waits.html" target="_blank">http://selenium-python.readthedocs.io/waits.html</a>. Explicit waits are preferred to implicit waits as you are telling Selenium exactly what to wait for and can ensure the part of the page you want to interact with has been loaded.</div>
<p>Now that we can get a webdriver and login to the site, we want to interact with the form and change the population:</p>
<pre>def add_population(driver):<br/>    driver.get(COUNTRY_URL)<br/>    population = driver.find_element_by_id('places_population')<br/>    new_population = int(population.get_attribute('value')) + 1<br/>    population.clear()<br/>    population.send_keys(new_population)<br/>    driver.find_element_by_xpath('//input[@type="submit"]').click()<br/>    pg_loaded = WebDriverWait(driver, 10).until(<br/>        EC.presence_of_element_located((By.ID, "places_population__row")))<br/>    test_population = int(driver.find_element_by_css_selector(<br/>        '#places_population__row .w2p_fw').text.replace(',', ''))<br/>    assert test_population == new_population
</pre>
<p>The only new Selenium feature used is the&#160;<kbd>clear</kbd> method to clear the input value for the form (rather than appending it to the end of the field). We also use the element's&#160;<kbd>get_attribute</kbd> method to retrieve particular attributes from a HTML elements on the page. Because we are dealing with HTML&#160;<kbd>input</kbd> elements, we need to grab the&#160;<kbd>value</kbd> attribute, rather than checking the text attribute.</p>
<p>Now we have all of our methods for using Selenium to add one to the population, so we can run this script like so:</p>
<pre><strong> &gt;&gt;&gt; driver = get_driver()</strong><br/><strong> &gt;&gt;&gt; login(driver)</strong><br/><strong> &gt;&gt;&gt; add_population(driver)</strong><br/><strong> &gt;&gt;&gt; driver.quit()</strong>
</pre>
<p>Since our <kbd>assert</kbd> statement passed, we know we have updated the population using this simple script.&#160;</p>
<p>There are many more ways to use Selenium to interact with forms, and I encourage you to experiment further by reading the documentation. Selenium can be especially helpful for debugging problematic websites because of the ability to use&#160;<kbd>save_screenshot</kbd> to see what the browser has loaded.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">"Humanizing" methods for Web Scraping</h1>
            </header>

            <article>
                
<p>There are sites which detect web scrapers via particular behaviors. In <a href="py-web-scrp-2e_ch05.html" target="_blank">Chapter 5</a>, <em>Dynamic Content</em>,&#160;we covered how to avoid honeypots by avoiding clicking on hidden links. Here are a few other tips for appearing more like a human while scraping content online.</p>
<ul>
<li><strong>Utilize Headers</strong>: Most of the scraping libraries we have covered can alter the headers of your requests, allowing you to modify things like&#160;<kbd>User-Agent</kbd>,&#160;<kbd>Referrer</kbd>,&#160;<kbd>Host</kbd>, and&#160;<kbd>Connection</kbd>. Also, when utilizing browser-based scrapers like Selenium, your scraper will look&#160;like a normal browser with normal headers. You can always take a look at what headers your browser is using by opening&#160;your browser tools and viewing one of the recent requests in the Network tab. This might give you a good idea of what headers the site is expecting.</li>
<li><strong>Add Delays:</strong> Some scraper detection techniques use timing to determine if a form is filled out too quickly or links are clicked too soon after page load. To appear more "human-like", add reasonable delays when interacting with forms or use <kbd>sleep</kbd> to add&#160;delays between requests. This is also the polite way to scrape a site so as to not overload the server.</li>
<li><strong>Use Sessions and Cookies:</strong> As we have covered in this chapter, using sessions and cookies will help your scraper navigate the site easier and allow you to appear more like a normal browser. By saving sessions and cookies locally, you can pick up sessions where you left off and resume scraping with saved data.</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>Interacting with forms is a necessary skill when scraping web pages. This chapter covered two&#160;approaches: first, analyzing the form to generate the expected <kbd>POST</kbd> request manually and utilizing browser sessions and cookies to stay logged in. Then, we were able to replicate those interactions using Selenium. We also covered some tips&#160;to follow when "humanizing" your scrapers.</p>
<p>In the following chapter, we will expand our form skillset and learn how to submit forms that require passing <kbd>CAPTCHA</kbd>&#160;image solving.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>
</body>
</html>