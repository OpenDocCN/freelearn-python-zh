<html><head></head><body>
<div><div><div><h1 id="_idParaDest-131"><em class="italic"><a id="_idTextAnchor122"/>Chapter 7</em>: Implementing Concurrency</h1>
			<p>So far, we have explored how to measure and improve the performance of programs by reducing the number of operations performed by the <strong class="bold">central processing unit</strong> (<strong class="bold">CPU</strong>) through clever algorithms and more efficient machine code. In this chapter, we will shift our focus to programs where most of the time is spent waiting for resources that are much slower than the CPU, such as persistent storage and network resources.</p>
			<p><strong class="bold">Asynchronous programming</strong> is a programming paradigm that helps to deal with slow and unpredictable resources (such as users) and is widely used to build responsive services and <strong class="bold">user interfaces</strong> (<strong class="bold">UIs</strong>). In this chapter, we will show you how to program asynchronously in Python using techniques such as coroutines and reactive programming. As we will see, the successful application of these techniques will allow us to speed up our programs without the use of specialized data structures or algorithms.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Asynchronous programming</li>
				<li>The <code>asyncio</code> framework</li>
				<li>Reactive programming</li>
			</ul>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor123"/>Technical requirements</h1>
			<p>The code for this chapter can be found at <a href="https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter07">https://github.com/PacktPublishing/Advanced-Python-Programming-Second-Edition/tree/main/Chapter07</a>.</p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor124"/>Asynchronous programming</h1>
			<p>Asynchronous programming<a id="_idIndexMarker478"/> is a way of dealing with slow and unpredictable resources. Rather than waiting idly for resources to become available, asynchronous programs can handle multiple resources concurrently and efficiently. Programming in an asynchronous way can be challenging because it is necessary to deal with external requests that can arrive in any order, may take a variable amount of time, or may fail unpredictably. In this section, we will introduce the topic by explaining<a id="_idIndexMarker479"/> the main concepts and terminology as well as by giving an idea of how asynchronous programs work.</p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor125"/>Waiting for input/output</h2>
			<p>A modern <a id="_idIndexMarker480"/>computer employs different kinds of memory to store data and perform operations. In general, a computer possesses a combination of expensive memory that is capable of operating efficiently and cheaply and more abundant memory that is slower and is used to store a larger amount of data.</p>
			<p>The memory hierarchy is shown in the following diagram:</p>
			<div><div><img src="img/Figure_7.1_B17499.jpg" alt="Figure 7.1 – Illustration of the memory hierarchy " width="733" height="132"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – Illustration of the memory hierarchy</p>
			<p>At the top of the<a id="_idIndexMarker481"/> memory hierarchy, are the CPU <strong class="bold">registers</strong>. Those <a id="_idIndexMarker482"/>are integrated with the CPU and are used<a id="_idIndexMarker483"/> to store and execute machine instructions. Accessing data in a register generally takes one clock cycle. This means that if the CPU operates at 3 <strong class="bold">gigahertz</strong> (<strong class="bold">GHz</strong>), the time it takes to access one element in a CPU register is in the order of 0.3 nanoseconds.</p>
			<p>At the layer just below the registers, you<a id="_idIndexMarker484"/> can find the CPU <strong class="bold">cache</strong>, which <a id="_idIndexMarker485"/>comprise multiple levels and is integrated with the processor. The cache operates at a slightly slower speed than the registers but within the <a id="_idIndexMarker486"/>same <strong class="bold">order of magnitude</strong> (<strong class="bold">OOM</strong>).</p>
			<p>The next item in the hierarchy is the main memory (<strong class="bold">random-access memory</strong>, or <strong class="bold">RAM</strong>), which <a id="_idIndexMarker487"/>holds <a id="_idIndexMarker488"/>much more data but is slower than the cache. Fetching an item from memory can take a few hundred clock cycles.</p>
			<p>At the bottom layer, you can find<a id="_idIndexMarker489"/> persistent <strong class="bold">storage</strong>, such as<a id="_idIndexMarker490"/> rotating<a id="_idIndexMarker491"/> disks (<strong class="bold">hard disk drives</strong> (<strong class="bold">HDDs</strong>)) and <strong class="bold">solid-state drives</strong> (<strong class="bold">SSDs</strong>). These <a id="_idIndexMarker492"/>devices hold the most data and are OOMs slower than the main memory. An HDD may take a few milliseconds to seek and retrieve an item, while an SSD<a id="_idIndexMarker493"/> is substantially faster and takes only a fraction of a millisecond.</p>
			<p>To put<a id="_idIndexMarker494"/> the relative speed of each memory type into perspective, if you were to have the CPU with a clock speed of about 1 second, a register access would be equivalent to picking up a pen from a table. A cache access would be equivalent to picking up a book from a shelf. Moving higher up the hierarchy, a RAM access would be equivalent to loading up the laundry (about 20 times slower than the cache). When we move to persistent storage, things are quite different. Retrieving an element from an SSD will be equivalent to going on a 4-day road trip while retrieving an element from an HDD can take up to 6 months! The duration can stretch even further if we move on to access resources over the network.</p>
			<p>Overall, accessing data from storage and other <strong class="bold">input/output</strong> (<strong class="bold">I/O</strong>) devices is much slower compared to the CPU; therefore, it is very important to handle those resources so that the CPU is never stuck waiting aimlessly. This can be accomplished by carefully designed software capable of managing multiple ongoing requests at the same time. This is the idea of concurrency or concurrent programming.</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor126"/>Concurrency</h2>
			<p><strong class="bold">Concurrency</strong> is a <a id="_idIndexMarker495"/>way to implement a system that can deal with multiple requests at the same time. The idea is that we can move on and start handling other resources while we wait for a resource to become available. Concurrency works<a id="_idIndexMarker496"/> by splitting a task into smaller subtasks that can be executed out of order so that multiple tasks can be partially advanced without waiting for the previous tasks to finish.</p>
			<p>As a first example, we will <a id="_idIndexMarker497"/>describe how to implement concurrent access to a slow network resource; the code for this example is included in <code>Chapter07/example1.py</code>. Let's say we have a web service that takes the square of a number, and the time between our request and the response will be approximately 1 second. We can implement the <code>network_request</code> function that takes a number and returns a dictionary that contains information about the success of the operation and the result. We can simulate such services using the <code>time.sleep</code> function, as follows:</p>
			<pre>    import time
    def network_request(number):
        time.sleep(1.0)
        return {"success": True, "result": number ** 2}</pre>
			<p>We will<a id="_idIndexMarker498"/> also write some additional code that <a id="_idIndexMarker499"/>performs the request, verifies that the request was successful, and prints the result. In the following code snippet, we define the <code>fetch_square</code> function and use it to calculate the square of the number 2 using a call to <code>network_request</code>:</p>
			<pre>    def fetch_square(number):
        response = network_request(number)
        if response["success"]:
            print("Result is: \
              {}".format(response["result"]))
    fetch_square(2)
    # Output:
    # Result is: 4</pre>
			<p>Fetching a number from the network will take 1 second because of the slow network. What if we want to calculate the square of multiple numbers? We can call <code>fetch_square</code>, which will start a network request as soon as the previous one is done. Its use is illustrated in the following code snippet:</p>
			<pre>    fetch_square(2)
    fetch_square(3)
    fetch_square(4)
    # Output:
    # Result is: 4
    # Result is: 9
    # Result is: 16</pre>
			<p>This code as-is will take roughly 3 seconds to run, but it's not the best we can do. Notice that the calculation of the square of 2 is independent of that of the square of 3, and both are in<a id="_idIndexMarker500"/> turn independent of calculating the square of 4. As such, waiting for a previous result to finish before moving on to the next number is unnecessary, if we can technically submit multiple requests and wait for them at the same time.</p>
			<p>In the<a id="_idIndexMarker501"/> following diagram, the three tasks are represented as boxes. The time spent by the CPU processing and submitting the request is in orange, while the waiting times are in blue. You can see how most of the time is spent waiting for the resources while our machine sits idle without doing anything else:</p>
			<div><div><img src="img/Figure_7.2_B17499.jpg" alt="Figure 7.2 – Illustration of the execution time of independent calculations " width="990" height="146"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – Illustration of the execution time of independent calculations</p>
			<p>Ideally, we would like to start another new task while we are waiting for the already submitted tasks to finish. In the following screenshot, you can see that as soon as we submit our request in <code>fetch_square(2)</code>, we can start preparing for <code>fetch_square(3)</code>, and so on. This allows us to reduce the CPU waiting time and to start processing the results as soon as they become available:</p>
			<div><div><img src="img/Figure_7.3_B17499.jpg" alt="Figure 7.3 – A more efficient way of performing independent calculations " width="973" height="234"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – A more efficient way of performing independent calculations</p>
			<p>Again, this strategy is made possible by the fact that the three requests are completely independent, and we don't need to wait for the completion of a previous task to start the next one. Also, note<a id="_idIndexMarker502"/> how a single CPU can comfortably handle this scenario. While distributing the <a id="_idIndexMarker503"/>work on multiple CPUs can further speed up the execution, if the waiting time is large compared to the processing times, the speedup will be minimal.</p>
			<p>To implement concurrency, it is necessary to think about our programs and their design differently; in the following sections, we'll demonstrate techniques and best practices to implement robust concurrent applications, starting with a new concept: callbacks.</p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor127"/>Callbacks</h2>
			<p>The code <a id="_idIndexMarker504"/>we have seen so far blocks the execution of the program until the resource is available. The call responsible for the waiting is <code>time.sleep</code>. To make the code start working on other tasks, we need to find a way to avoid blocking the program flow so that the rest of the program can move on to these other tasks.</p>
			<p>One of the simplest ways to<a id="_idIndexMarker505"/> accomplish this behavior is through callbacks. The strategy is quite similar to what we do when we request a cab. Imagine that you are at a restaurant and you've had a few drinks. It's raining outside, and you'd rather not take the bus; therefore, you request a taxi and ask them to call when they're outside so that you can come out and you don't have to wait in the rain. What you did, in this case, is request a taxi (that is, the slow resource), but instead of waiting outside until the taxi arrives, you provide your number and instructions (callback) so that you can come outside when they're ready and go home.</p>
			<p>We will now show how this <a id="_idIndexMarker506"/>mechanism can work in code. We will compare the blocking code of <code>time.sleep</code> with the equivalent non-blocking code of <code>threading.Timer</code>.</p>
			<p>For this example, which is implemented in <code>example2.py</code>, we will write a function, <code>wait_and_print</code>, that will block the program execution for 1 second and then print a message, as follows:</p>
			<pre>    def wait_and_print(msg):
        time.sleep(1.0)
        print(msg)</pre>
			<p>If we want to write the same function in a non-blocking way, we can use the <code>threading.Timer</code> class. We can initialize a <code>threading.Timer</code> instance by passing the amount of time we want to wait and a <code>Timer.start</code> method to activate the timer:</p>
			<pre>    import threading
    def wait_and_print_async(msg):
        def callback():
            print(msg)
        timer = threading.Timer(1.0, callback)
        timer.start()</pre>
			<p>An<a id="_idIndexMarker508"/> important feature of the <code>wait_and_print_async</code> function is that none of the statements is blocking the execution flow of the program.</p>
			<p class="callout-heading">How Is threading.Timer Capable of Waiting without Blocking?</p>
			<p class="callout">The strategy used <a id="_idIndexMarker509"/>by <code>threading.Timer</code> involves starting a new thread that can execute code in parallel. If this is confusing, don't worry, as we will explore threading and parallel programming in detail in the following chapters.</p>
			<p>This technique of registering callbacks for execution in response to certain events is commonly called the <em class="italic">Hollywood principle</em>. This is because, after auditioning for a movie or TV role at Hollywood, you may be told <em class="italic">Don't call us, we'll call you</em>, meaning that they won't tell you if they chose you for the role immediately, but they'll call you if they do.</p>
			<p>To highlight the difference between the blocking and non-blocking versions of <code>wait_and_print</code>, we can test and compare the execution of the two versions. In the output comments, the <a id="_idIndexMarker510"/>waiting periods are indicated by <code>&lt;wait...&gt;</code>, as illustrated in the following code snippet:</p>
			<pre>    # Synchronous
    wait_and_print("First call")
    wait_and_print("Second call")
    print("After call")
    # Output:
    # &lt;wait...&gt;
    # First call  
    # &lt;wait...&gt;
    # Second call
    # After call
    # Asynchronous
    wait_and_print_async("First call async")
    wait_and_print_async("Second call async")
    print("After submission")
    # Output:
    # After submission 
    # &lt;wait...&gt;
    # First call
    # Second call</pre>
			<p>The<a id="_idIndexMarker511"/> synchronous version behaves in a very familiar, expected way. The code waits for a second, prints <code>First call</code>, waits for another second, and then prints <code>Second call</code> and <code>After call</code> messages.</p>
			<p>In the asynchronous version, <code>wait_and_print_async</code> <em class="italic">submits</em> (rather than <em class="italic">executes</em>) those calls and moves on <em class="italic">immediately</em>. You can see this mechanism in action by noticing that the <code>"After submission"</code> message is printed immediately.</p>
			<p>With this in mind, we can explore a slightly more complex situation by rewriting our <code>network_request</code> function <a id="_idIndexMarker512"/>using callbacks. In <code>example3.py</code>, we define a <code>network_request_async</code> function, as follows:</p>
			<pre>    def network_request_async(number, on_done):
        def timer_done():
            on_done({"success": True, \
                     "result": number ** 2})
        timer = threading.Timer(1.0, timer_done)
        timer.start()</pre>
			<p>The biggest difference between <code>network_request_async</code> and its blocking counterpart is that <code>network_request_async</code> <em class="italic">doesn't return anything</em>. This is because we are merely submitting the request when <code>network_request_async</code> is called, but the value is available only when the request is completed.</p>
			<p>If we<a id="_idIndexMarker513"/> can't return anything, how do we pass the result of the request? Rather than returning the value, we will pass the result as an argument to the <code>on_done</code> callback. The rest of the function consists of submitting a callback (called <code>timer_done</code>) to the <code>threading.Timer</code> class that will call <code>on_done</code> when it's ready.</p>
			<p>The usage of <code>network_request_async</code> is quite similar to <code>threading.Timer</code>; all we have to do is pass the number we want to square and a callback that will receive the result <em class="italic">when it's ready</em>. This is demonstrated in the following code snippet:</p>
			<pre>    def on_done(result):
        print(result)
    network_request_async(2, on_done)</pre>
			<p>Now, if we submit multiple network requests, we note that the calls get executed concurrently <a id="_idIndexMarker514"/>and do not block the code, as illustrated in the following code snippet:</p>
			<pre>    network_request_async(2, on_done)
    network_request_async(3, on_done)
    network_request_async(4, on_done)
    print("After submission")</pre>
			<p>In order to use <code>network_request_async</code> in <code>fetch_square</code>, we need to adapt the code to take advantage of asynchronous constructs. In the following code snippet, we modify <code>fetch_square</code> by defining and passing the <code>on_done</code> callback to <code>network_request_async</code>:</p>
			<pre>    def fetch_square(number):
        def on_done(response):
            if response["success"]:
                print("Result is: \
                  {}".format(response["result"]))
        network_request_async(number, on_done)</pre>
			<p>You<a id="_idIndexMarker515"/> may have noted that the asynchronous code is significantly more convoluted than its synchronous counterpart. This is due to the fact that we are required to write and pass a callback every time we need to retrieve a certain result, causing the code to become nested and hard to follow.</p>
			<p>Fortunately, a concept that is essential to concurrent programming that we are examining next, futures, will help simplify matters.</p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor128"/>Futures</h2>
			<p>Futures<a id="_idIndexMarker516"/> are a <a id="_idIndexMarker517"/>more convenient pattern that can be used to keep track of the results of asynchronous calls. In the preceding code snippet, we saw that rather than returning values, we accept callbacks and pass the results when they are ready. It is interesting to note that, so far, there is no easy way to track the status of the resource.</p>
			<p>A <code>concurrent.futures.Future</code> class. A <code>Future</code> instance can be created by calling its constructor with no arguments, as in the following IPython snippet:</p>
			<pre>    from concurrent.futures import Future
    fut = Future()
    # Result:
    # &lt;Future at 0x7f03e41599e8 state=pending&gt;</pre>
			<p>A future represents a value that is not yet available. You can see that its string representation reports the current status of the result, which, in our case, is still pending. In order to make a result available, we can use the <code>fut.set_result</code> method, as follows:</p>
			<pre>    fut.set_result("Hello")
    # Result:
    # &lt;Future at 0x7f03e41599e8 state=finished returned 
      str&gt;
    fut.result()
    # Result:
    # "Hello"</pre>
			<p>You can <a id="_idIndexMarker519"/>see that once we set the result, the <code>Future</code> instance will report that the task is finished and can be accessed using the <code>fut.result</code> method. It is also possible to subscribe a callback to a future so that, as soon as the result is available, the callback is executed. To attach a callback, it is sufficient to pass a function to the <code>fut.add_done_callback</code> method. When the task is completed, the function will be called with the <code>Future</code> instance as its first argument and the result can be retrieved using the <code>future.result()</code> method, as illustrated in the following code snippet:</p>
			<pre>    fut = Future()
    fut.add_done_callback(lambda future: 
      print(future.result(), flush=True))
    fut.set_result("Hello")
    # Output:
    # Hello</pre>
			<p>To understand how futures<a id="_idIndexMarker520"/> can be used in practice, we will adapt the <code>network_request_async</code> function to use futures in <code>example4.py</code>. The idea is that this time, instead of returning nothing, we return a <code>Future</code> instance that will keep track of the result for us. Note the following two things:</p>
			<ul>
				<li>We don't need to accept an <code>on_done callback</code> as callbacks can be connected later using the <code>fut.add_done_callback</code> method. Also, we pass the generic <code>fut.set_result</code> method as the callback for <code>threading.Timer</code>.</li>
				<li>This time, we<a id="_idIndexMarker521"/> are able to return a value, thus making the code a bit more similar to the blocking version we saw in the preceding section, as illustrated here:<pre>    from concurrent.futures import Future
    def network_request_async(number):
        future = Future()
        result = {"success": True, "result": number  \
          ** 2}
        timer = threading.Timer(1.0, lambda:  \
          future.set_result(result))
        timer.start()
        return future
    fut = network_request_async(2)</pre><p class="callout-heading">Note</p><p class="callout">Even though we instantiate and manage futures directly in these examples, in practical applications, futures are handled by frameworks. </p></li>
			</ul>
			<p>If you <a id="_idIndexMarker522"/>execute the preceding code, nothing will happen as the code only consists of preparing and returning a <code>Future</code> instance. To enable further operation of the future results, we need to use the <code>fut.add_done_callback</code> method. In the following code snippet, we adapt the <code>fetch_square</code> function to use futures:</p>
			<pre>    def fetch_square(number):
        fut = network_request_async(number)
        def on_done_future(future):
            response = future.result()
            if response["success"]:
                print("Result is: \
                  {}".format(response["result"]))
        
        fut.add_done_callback(on_done_future)</pre>
			<p>As you <a id="_idIndexMarker523"/>can see, the code still looks quite similar to the callback version, but when <code>fetch_square</code> is called, the corresponding future will be processed, and the result string will be printed out.</p>
			<p>Overall, futures are a different and slightly more convenient way of working with callbacks. Futures are also advantageous in the sense that they can keep track of the resource status, cancel (unschedule) scheduled tasks, and handle exceptions more naturally.</p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor129"/>Event loops</h2>
			<p>So far, we<a id="_idIndexMarker524"/> have implemented parallelism using <strong class="bold">operating system</strong> (<strong class="bold">OS</strong>) threads. However, in many asynchronous frameworks, the coordination of concurrent tasks is <a id="_idIndexMarker525"/>managed by an <strong class="bold">event loop</strong>.</p>
			<p>The idea <a id="_idIndexMarker526"/>behind an event loop is to continuously monitor the status of the various resources (for example, network connections and database queries) and trigger the execution of callbacks when specific events take place (for example, when a resource is ready or when a timer expires).</p>
			<p class="callout-heading">Why Not Just Stick to Threading?</p>
			<p class="callout">Events loops are sometimes preferred as every unit of execution never runs at the same time as another, and this can simplify dealing with shared variables, data structures, and resources.</p>
			<p>As the first example in <code>example5.py</code>, we will implement a thread-free version of <code>threading.Timer</code>. We can define a <code>Timer</code> class that will take a timeout and implement the <code>Timer.done</code> method, which returns <code>True</code> if the timer has expired. The code is illustrated in the following snippet:</p>
			<pre>    class Timer:
    
        def __init__(self, timeout):
            self.timeout = timeout
            self.start = time.time()
    
        def done(self):
            return time.time() - self.start &gt; self.timeout</pre>
			<p>To determine whether the timer has expired, we can write a loop that continuously checks the timer status by calling the <code>Timer.done</code> method. When the timer expires, we can print a message and exit the cycle. The code is illustrated in the following snippet:</p>
			<pre>    timer = Timer(1.0)
  
    while True:
        if timer.done():
            print("Timer is done!")
            break</pre>
			<p>By<a id="_idIndexMarker527"/> implementing the timer in this way, the flow of execution is never blocked, and we can, in principle, do other work inside the <code>while</code> loop.</p>
			<p class="callout-heading">Busy-Waiting</p>
			<p class="callout">Waiting for events to happen by continuously polling <a id="_idIndexMarker528"/>using a loop is commonly termed <strong class="bold">busy-waiting</strong>.</p>
			<p>Ideally, we <a id="_idIndexMarker529"/>would like to attach a custom function that executes when the timer goes off, just as we did in <code>threading.Timer</code>. To do this, we can implement a <code>Timer.on_timer_done</code> method that will accept a callback to be executed when the timer goes off in <code>example6.py</code>, as follows:</p>
			<pre>    class Timer:
       # ... previous code 
       def on_timer_done(self, callback):
            self.callback = callback</pre>
			<p>Note that <code>on_timer_done</code> merely stores a reference to the callback. The entity that monitors the event and executes the callback is the loop. This concept is demonstrated as follows. Rather than using the <code>print</code> function, the loop will call <code>timer.callback</code> when appropriate:</p>
			<pre>    timer = Timer(1.0)
    timer.on_timer_done(lambda: print("Timer is done!"))
    while True:
        if timer.done():
            <strong class="bold">timer.callback()</strong>
            break</pre>
			<p>As you can see, an asynchronous framework is starting to take place. All we did outside the loop was define the timer and the callback, while the loop took care of monitoring the timer and executing the associated callback. We can further extend our code by implementing support for multiple timers.</p>
			<p>A <a id="_idIndexMarker530"/>natural way to implement multiple timers is to add a few <code>Timer</code> instances to a list and modify our event loop to periodically check all the timers and dispatch the callbacks when required. In the following code snippet, we define two timers and attach a callback<a id="_idIndexMarker531"/> to each of them. Those timers are added to a list, <code>timers</code>, that is continuously monitored by our event loop. As soon as a timer is done, we execute the callback and remove the event from the list:</p>
			<pre>    timers = []
    timer1 = Timer(1.0)
    timer1.on_timer_done(lambda: print("First timer is  \
      done!"))
    timer2 = Timer(2.0)
    timer2.on_timer_done(lambda: print("Second timer is  \
      done!"))
    timers.append(timer1)
    timers.append(timer2)
    while True:
        for timer in timers:
            if timer.done():
                timer.callback()
                timers.remove(timer)
        # If no more timers are left, we exit the loop 
        if len(timers) == 0:
            break</pre>
			<p>The <a id="_idIndexMarker532"/>main restriction of an event loop is, since the flow of execution is managed by a continuously running loop, that it <code>time.sleep</code>) inside the loop, you can imagine how the event monitoring and callback dispatching will stop until the blocking call is done.</p>
			<p>To <a id="_idIndexMarker533"/>avoid this, rather than using a blocking call such as <code>time.sleep</code>, we let the event loop detect and execute the callback when the resource is ready. By not blocking the execution flow, the event loop is free to monitor multiple resources in a concurrent way.</p>
			<p class="callout-heading">How Is the Event Loop Notified of Events?</p>
			<p class="callout">Events notification is <a id="_idIndexMarker534"/>usually implemented through OS calls (such as the <code>select</code> Unix tool) that will resume the execution of the program whenever an event is ready (in contrast to busy-waiting).</p>
			<p>The Python standard libraries include a very convenient event loop-based concurrency framework, <code>asyncio</code>, which will be the topic of the next section.</p>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor130"/>The asyncio framework</h1>
			<p>At this point, we<a id="_idIndexMarker535"/> have seen how concurrency works and how to use callbacks and futures. We can now move on and learn how to use the <code>asyncio</code> package, which has been present in the standard Python library since version 3.4. We will also explore the <code>async</code>/<code>await</code> syntax to deal with asynchronous programming in a very natural way.</p>
			<p>As a first example, we will see how to retrieve and execute a simple callback using <code>asyncio</code>. The <code>asyncio</code> loop can be retrieved by calling the <code>asyncio.get_event_loop()</code> function. We can schedule a callback for execution using <code>loop.call_later</code>, which takes a delay in seconds and a callback. We can also use the <code>loop.stop</code> method to halt the loop and exit the program. To start processing the scheduled call, it is necessary to start the loop, which can be done using <code>loop.run_forever</code>. The following example in <code>example7.py</code> demonstrates the usage of these basic methods by scheduling a callback that will print a message and halt the loop:</p>
			<pre>    import asyncio
    loop = asyncio.get_event_loop()
    def callback():
        print("Hello, asyncio")
        loop.stop()
    loop.call_later(1.0, callback)
    loop.run_forever()</pre>
			<p>This code schedules a callback that will print out a message and then halt the loop. One of the main problems with callbacks is that they require you to break the program execution into small functions that will be invoked when a certain event takes place. As we saw in the earlier sections, callbacks can quickly become cumbersome. In the next section, we will see how to work with coroutines to, as with futures, simplify many aspects of concurrent programming.</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor131"/>Coroutines</h2>
			<p><strong class="bold">Coroutines</strong> are <a id="_idIndexMarker536"/>another, perhaps more natural, way<a id="_idIndexMarker537"/> to break up the program execution into chunks. They allow the programmer to write code that resembles synchronous code but will execute asynchronously. You may think of a coroutine as a function that can be stopped and resumed. A basic example of coroutines is generators.</p>
			<p>Generators <a id="_idIndexMarker538"/>can be defined in Python using the <code>yield</code> statement inside a function. In the following code example in <code>example8.py</code>, we implement the <code>range_generator</code> function, which produces and returns values from <code>0</code> to <code>n</code>. We also add a <code>print</code> statement to log the internal state of the generator:</p>
			<pre>    def range_generator(n):
        i = 0
        while i &lt; n:
            print("Generating value {}".format(i))
            yield i
            i += 1</pre>
			<p>When we call the <code>range_generator</code> function, the code is not executed immediately. Note that nothing is printed to output when the following snippet is executed. Instead, a <code>generator</code> object is returned:</p>
			<pre>    generator = range_generator(3)
    generator
    # Result:
    # &lt;generator object range_generator at 0x7f03e418ba40&gt;</pre>
			<p>In order to start pulling values from a generator, it is necessary to use the <code>next</code> function, as follows:</p>
			<pre>    next(generator)
    # Output:
    # Generating value 0
    next(generator)
    # Output:
    # Generating value 1</pre>
			<p>Note that <a id="_idIndexMarker539"/>every time we invoke <code>next</code>, the code runs until it encounters the next <code>yield</code> statement, and it is necessary to issue another <code>next</code> statement to resume the generator execution. You can think of a <code>yield</code> statement as a <a id="_idIndexMarker540"/>breakpoint where we can stop and resume execution (while also maintaining the internal state of the generator). This ability to stop and resume execution can be leveraged by the event loop to allow for and implement concurrency.</p>
			<p>It is also possible to <em class="italic">inject</em> (rather than <em class="italic">extract</em>) values in the generator through the <code>yield</code> statement. In the following code example in <code>example9.py</code>, we declare a <code>parrot</code> function that will repeat each message that we send: </p>
			<pre>    def parrot():
        while True:
            message = yield
            print("Parrot says: {}".format(message))
    generator = parrot()
    generator.send(None)
    generator.send("Hello")
    generator.send("World")</pre>
			<p>To allow a generator to receive a value, you can assign <code>yield</code> to a variable (in our case, it is <code>message = yield</code>). To insert values in the generator, we can use the <code>send</code> method. In the Python world, a generator that can also receive <a id="_idIndexMarker541"/>values is<a id="_idIndexMarker542"/> called a <strong class="bold">generator-based coroutine</strong>.</p>
			<p>Note that we also need to issue a <code>generator.send(None)</code> request before we can start sending messages; this is to bootstrap the function execution and bring us to the first <code>yield</code> statement. Also, note that there is an infinite loop inside <code>parrot</code>; if we implement this without using generators, we will get stuck running the loop forever!</p>
			<p>With <a id="_idIndexMarker543"/>this in mind, you can<a id="_idIndexMarker544"/> imagine how an event loop can partially progress several of these generators without blocking the execution of the whole program, as well as how a generator can be advanced only when some resource is ready, therefore eliminating the need for a callback.</p>
			<p>It is possible to implement coroutines in <code>asyncio</code> using the <code>yield</code> statement. However, Python supports the definition of powerful coroutines using a more intuitive syntax. To define a coroutine with <code>asyncio</code>, you can use the <code>async def</code> statement, as follows: </p>
			<pre>    async def hello():
        print("Hello, async!")
    coro = hello()
    coro
    # Output:
    # &lt;coroutine object hello at 0x7f314846bd58&gt;</pre>
			<p>As you can see, if we call the <code>hello</code> function, the function body is not executed immediately, but a <code>coroutine object</code> instance is returned. The <code>asyncio</code> coroutines do not support <code>next</code>, but they can be easily run in the <code>asyncio</code> event loop using the <code>run_until_complete</code> method, as follows:</p>
			<pre>    import asyncio
    loop = asyncio.get_event_loop()
    loop.run_until_complete(coro)</pre>
			<p class="callout-heading">Native Coroutines</p>
			<p class="callout">Coroutines defined with the <code>async def</code> statement <a id="_idIndexMarker545"/>are also <a id="_idIndexMarker546"/>called <em class="italic">native coroutines</em>.</p>
			<p>The <code>asyncio</code> module <a id="_idIndexMarker547"/>provides resources (called <code>await</code> syntax. For example, in <code>example10.py</code>, if we want to wait for a certain time and then execute a statement, we can use the <code>asyncio.sleep</code> function, as follows:</p>
			<pre>    async def wait_and_print(msg):
        await asyncio.sleep(1)
        print("Message: ", msg)
    loop = asyncio.get_event_loop()
    loop.run_until_complete(wait_and_print("Hello"))</pre>
			<p>The result is beautiful, clean code. We are writing perfectly functional asynchronous code without all the ugliness of callbacks!</p>
			<p class="callout-heading">Breakpoints for Event Loops</p>
			<p class="callout">You may have <a id="_idIndexMarker550"/>noted how <code>await</code> provides a breakpoint for the event loop so that, as it waits for the resource, the event loop can move on and concurrently manage other coroutines.</p>
			<p>Even better, coroutines are also <code>awaitable</code>, and we can use the <code>await</code> statement to chain coroutines asynchronously. In the following example, we rewrite the <code>network_request</code> function, which we defined earlier, by replacing the call to <code>time.sleep</code> with <code>asyncio.sleep</code>:</p>
			<pre>    async def network_request(number):
         <strong class="bold">await asyncio.sleep(1.0)</strong>
         return {"success": True, "result": number ** 2}</pre>
			<p>We can follow up by reimplementing <code>fetch_square</code>, as illustrated in the following code snippet. As you can see, we await <code>network_request</code> directly without needing additional futures or callbacks:</p>
			<pre>    async def fetch_square(number):
         response = await network_request(number)
         if response["success"]:
             print("Result is: \
              {}".format(response["result"]))</pre>
			<p>The coroutines can be executed individually using <code>loop.run_until_complete</code>, as follows:</p>
			<pre>    loop.run_until_complete(fetch_square(2))
    loop.run_until_complete(fetch_square(3))
    loop.run_until_complete(fetch_square(4))</pre>
			<p>Running<a id="_idIndexMarker551"/> tasks using <code>run_until_complete</code> serves as a <a id="_idIndexMarker552"/>demonstration for testing and debugging. However, our program will be started with <code>loop.run_forever</code> most of the time, and we will need to submit our tasks while the loop is already running.</p>
			<p><code>asyncio</code> provides<a id="_idIndexMarker553"/> the <code>ensure_future</code> function, which schedules coroutines (as well as futures) for execution. <code>ensure_future</code> can be used by simply passing the coroutine we want to schedule. The following code snippet in <code>example11.py</code> will schedule multiple calls to <code>fetch_square</code> that will be executed concurrently:</p>
			<pre>    asyncio.ensure_future(fetch_square(2))
    asyncio.ensure_future(fetch_square(3))
    asyncio.ensure_future(fetch_square(4))
    loop.run_forever()
    # Hit Ctrl-C to stop the loop</pre>
			<p>As a bonus, when passing a coroutine, the <code>asyncio.ensure_future</code> function will return a <code>Task</code> instance (which is a subclass of <code>Future</code>) so that we can take advantage of the <code>await</code> syntax <a id="_idIndexMarker554"/>without<a id="_idIndexMarker555"/> having to give up the resource-tracking capabilities of regular futures.</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor132"/>Converting blocking code into non-blocking code</h2>
			<p>While <code>asyncio</code> supports<a id="_idIndexMarker556"/> connecting to resources in an asynchronous way, it is required to use blocking <a id="_idIndexMarker557"/>calls in certain cases. This happens, for example, when third-party <code>asyncio</code>. Here are the steps:</p>
			<ol>
				<li>An<a id="_idIndexMarker558"/> effective strategy for dealing with blocking code is to run it in a separate thread. Threads are implemented at the OS level and allow parallel execution of blocking code. For this purpose, Python provides the <code>Executor</code> interface designed to run tasks in a separate thread and to monitor their progress using futures.</li>
				<li>You can initialize a <code>ThreadPoolExecutor</code> instance by importing it from the <code>concurrent.futures</code> module. The executor will spawn a collection of threads (called workers) that will wait to execute whichever task we throw at them. Once a function is submitted, the executor will take care of dispatching its execution to an available worker thread and keep track of the result. The <code>max_workers</code> argument can be used to select the number of threads.</li>
			</ol>
			<p>Note that the executor will not destroy a thread once a task is completed. By doing so, it reduces the cost associated with the creation and destruction of threads. </p>
			<ol>
				<li value="3">In the following code example in <code>example12.py</code>, we create a <code>ThreadPoolExecutor</code> instance with three workers, and we submit a <code>wait_and_return</code> function that will block the program execution for 1 second and<a id="_idIndexMarker559"/> return a message string. We then use the <code>submit</code> method to schedule its execution:<pre>    import time
    from concurrent.futures import ThreadPoolExecutor
    executor = ThreadPoolExecutor(max_workers=3)
    def wait_and_return(msg):
        time.sleep(1)
        return msg
    print(executor.submit(wait_and_return, "Hello. \
      executor"))
    # Result:
    # &lt;Future at 0x7ff616ff6748 state=running&gt;</pre></li>
				<li>The <code>executor.submit</code> method<a id="_idIndexMarker560"/> immediately schedules the function and returns a future. It is possible to manage the execution of tasks in <code>asyncio</code> using the <code>loop.run_in_executor</code> method, which works quite similarly to <code>executor.submit</code>. The code is illustrated in the following snippet:<pre>    fut = loop.run_in_executor(executor, \
      wait_and_return, "Hello, asyncio executor")
    # &lt;Future pending ...more info...&gt;</pre></li>
				<li>The <code>run_in_executor</code> method will also return an <code>asyncio.Future</code> instance that can be awaited from other code, the main difference being that the future will not be run until we start the loop. We can run and obtain the response using <code>loop.run_until_complete</code>, as illustrated in the following snippet:<pre>    loop.run_until_complete(fut)
    # Result:
    # 'Hello, executor'</pre></li>
				<li>As <a id="_idIndexMarker561"/>a practical example, we <a id="_idIndexMarker562"/>can use this technique to implement concurrent fetching of several web pages. To do this, we will import the popular (blocking) <code>requests</code> library and run the <code>requests.get</code> function in the executor in <code>example13.py</code>, as follows:<pre>    import requests
    async def fetch_urls(urls):
        responses = []
        for url in urls:
            responses.append(await \
              loop.run_in_executor(executor, \
                requests.get, url))
        return responses
    responses = loop.run_until_complete(
      fetch_urls(
        [
      'http://www.google.com', 
      'http://www.example.com',
      'http://www.facebook.com'
        ]
      )
    )
    print(response)
    # Result
    # [&lt;Response [200]&gt;, &lt;Response [200]&gt;, &lt;Response 
      [200]&gt;] </pre></li>
				<li>This <a id="_idIndexMarker563"/>version of <code>fetch_url</code> will <a id="_idIndexMarker564"/>not block the execution and allow other coroutines in <code>asyncio</code> to run; however, it is not optimal as the function will not fetch a <code>asyncio.ensure_future</code> or employ the <code>asyncio.gather</code> convenience function that will submit all the coroutines at once and gather the results as they come. The usage of <code>asyncio.gather</code> is demonstrated in <code>example14.py</code>, as follows:<pre>    def fetch_urls(urls):
        return asyncio.gather(
          *[loop.run_in_executor(executor, \
             requests.get, url) for url in urls])</pre><p class="callout-heading">Upper Bound for the Number of Threads</p><p class="callout">The number of URLs you can fetch in parallel with this method will be dependent on the number of <a id="_idIndexMarker566"/>worker threads you have. To avoid this limitation, you should use a natively non-blocking library, such as <code>aiohttp</code>.</p></li>
			</ol>
			<p>So far, we have seen how to work with concurrent programs in Python using core concepts such as callbacks, futures, and coroutines. For the remaining portion of this chapter, we will discuss a more streamlined programming paradigm for implementing concurrency.</p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor133"/>Reactive programming</h1>
			<p><strong class="bold">Reactive programming</strong> is a <a id="_idIndexMarker567"/>paradigm that aims at building better concurrent systems. Reactive applications are designed to comply with the following requirements exemplified by the reactive manifesto:</p>
			<ul>
				<li><strong class="bold">Responsive</strong>: The<a id="_idIndexMarker568"/> system responds immediately to the user.</li>
				<li><strong class="bold">Elastic</strong>: The <a id="_idIndexMarker569"/>system is capable of handling different levels of load and can adapt to accommodate increasing demands.</li>
				<li><strong class="bold">Resilient</strong>: The <a id="_idIndexMarker570"/>system deals with failure gracefully. This is achieved by modularity<a id="_idIndexMarker571"/> and avoiding having a <strong class="bold">single point of failure</strong> (<strong class="bold">SPOF</strong>).</li>
				<li><strong class="bold">Message-driven</strong>: The <a id="_idIndexMarker572"/>system should not block and take advantage of events and messages. A message-driven application helps achieve all the previous requirements.</li>
			</ul>
			<p>The requirements for reactive systems are quite reasonable but abstract, which leads us to a natural question: how exactly does reactive programming work? In this section, we will learn about the principles of <a id="_idIndexMarker573"/>reactive programming using the <strong class="bold">Reactive Extensions for Python</strong> (<strong class="bold">RxPY</strong>) library.</p>
			<p class="callout-heading">Additional Information</p>
			<p class="callout">The <code>RxPY</code> library <a id="_idIndexMarker574"/>is part of<a id="_idIndexMarker575"/> ReactiveX (<a href="http://reactivex.io/">http://reactivex.io/</a>), which is a project that implements reactive programming tools for a large variety of languages.</p>
			<p class="callout">To install the library, simply run <code>pip install rx</code>.</p>
			<p class="callout">Note that the following code uses <code>RxPY</code> v3, the syntax of which is quite different from <code>RxPY</code> v1. If you are familiar with <code>RxPY</code> v1 and the discussion from this book's previous version, watch out for changes in syntax!</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor134"/>Observables</h2>
			<p>As the <a id="_idIndexMarker576"/>name implies, the main idea of reactive programming is<a id="_idIndexMarker577"/> to <em class="italic">react</em> to events. In the preceding section, we saw some examples of this idea with callbacks; you subscribe to them and the callback is executed as soon as the event takes place.</p>
			<p>In reactive programming, this idea is expanded if we think of events as streams of data. This can be exemplified by showing examples of such streams in <code>RxPY</code>. A data stream can be created from an iterator using the <code>from_iterable</code> method in IPython, as follows: </p>
			<pre>    from rx import from_iterable
    obs = from_iterable(range(4))</pre>
			<p>In order to receive data from <code>obs</code>, we can use the <code>Observable.subscribe</code> method, which will execute the function we pass for each value that the data source emits. This method is shown in the following code snippet:</p>
			<pre>    obs.subscribe(print)
    # Output:
    # 0
    # 1
    # 2
    # 3</pre>
			<p>You <a id="_idIndexMarker578"/>may have noticed that observables are ordered collections of items just like lists or, more generally, iterators. This is not a coincidence.</p>
			<p>The term <em class="italic">observable</em> comes <a id="_idIndexMarker579"/>from the combination of observer and iterable. An <em class="italic">observer</em> is an <a id="_idIndexMarker580"/>object that reacts to changes of the variable it observes, while an <em class="italic">iterable</em> is an<a id="_idIndexMarker581"/> object that can produce and keep track of an iterator.</p>
			<p>In Python, iterators are objects that define the <code>__next__</code> method, and whose elements can be extracted by calling <code>next</code>. An iterator can generally be obtained by a collection using <code>iter</code>; then, we can extract elements using <code>next</code> or a <code>for</code> loop. Once an element is consumed from the iterator, we can't go back. We can demonstrate its usage by creating an iterator from a list, as follows:</p>
			<pre>    collection = list([1, 2, 3, 4, 5])
    iterator = iter(collection)
    print("Next")
    print(next(iterator))
    print(next(iterator))
    print("For loop")
    for i in iterator:
         print(i)
    # Output:
    # Next
    # 1
    # 2
    # For loop
    # 3
    # 4
    # 5</pre>
			<p>You<a id="_idIndexMarker582"/> can see how, every time we call <code>next</code> or we iterate, the iterator <a id="_idIndexMarker583"/>produces a value and advances. In a sense, we are <em class="italic">pulling</em> results from the iterator.</p>
			<p class="callout-heading">Iterators versus Generators</p>
			<p class="callout">Iterators <a id="_idIndexMarker584"/>sound a lot like generators; however, they are more general. In Python, generators are returned by functions that use <code>yield</code> expressions. As<a id="_idIndexMarker585"/> we saw, generators support <code>next</code>; therefore, they are a special class of iterators.</p>
			<p>Now, you can appreciate the contrast between an iterator and an observable. An observable <em class="italic">pushes</em> a stream of data to us whenever it's ready, but that's not everything. An observable can also tell us when there is an error and where there is no more data. In fact, it is possible to register further callbacks to the <code>Observable.subscribe</code> method.</p>
			<p>In the following example in IPython, we create an observable and register callbacks to be called using <code>on_next</code> whenever the next item is available and using the <code>on_completed</code> argument when there is no more data:</p>
			<pre>    obs = from_iterable(range(4))
    obs.subscribe(on_next=lambda x: print("Next item:", x),
                  on_completed=lambda: print("No more  \
                    data"))
    # Output:
    # Next element: 0
    # Next element: 1
    # Next element: 2
    # Next element: 3
    # No more data</pre>
			<p>With that said, the <a id="_idIndexMarker586"/>similarity between observables and iterators is more important, because we can use the same techniques that can be used with iterators to handle streams of events.</p>
			<p><code>RxPy</code> provides<a id="_idIndexMarker587"/> operators that can be used to create, transform, filter, and group observables. The power of reactive programming lies in the fact that those operations return other observables that can be conveniently chained and composed together. For a quick demonstration, we will examine the usage of the <code>take</code> operator next.</p>
			<p>Given an observable, <code>take</code> will return a new observable that will stop after <code>n</code> items. Its usage is straightforward, as we can see here:</p>
			<pre>    from rx.operators import take
    
    op = take(4)
    obs = from_iterable(range(1000))
    op(obs).subscribe(print)
    # Output:
    # 0
    # 1
    # 2
    # 3</pre>
			<p>The collection of operations implemented in <code>RxPy</code> is varied and rich and can be used to build complex applications using these operators as building blocks.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor135"/>Useful operators</h2>
			<p>In this <a id="_idIndexMarker588"/>subsection, we will explore operators that transform the elements of a source observable in some way. The most prominent member of this family of<a id="_idIndexMarker589"/> operators is the familiar <code>map</code> operator, which<a id="_idIndexMarker590"/> emits the elements of the source observable after applying a function to them.</p>
			<p>For example, we may use <code>map</code> to calculate the square of a sequence of numbers, as follows:</p>
			<pre>    from rx.operators import map
    map(lambda x: x**2)(from_iterable(range(4))). \
        subscribe(print)
    # Output:
    # 0
    # 1
    # 4
    # 9</pre>
			<p>Operators can be represented with marble diagrams that help us better understand how the operator works, especially when taking into account the fact that elements can be emitted over a region of time. In a marble diagram, a data stream (in our case, an observable) is represented by a solid line. A circle (or another shape) identifies a value emitted by the observable, an <em class="italic">X</em> symbol represents an error, and a vertical line represents the end of the stream.</p>
			<p>Here, we can see a marble diagram of <code>map</code>:</p>
			<div><div><img src="img/Figure_7.4_B17499.jpg" alt="Figure 7.4 – Marble diagram illustrating the procedure of squaring numbers " width="857" height="201"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4 – Marble diagram illustrating the procedure of squaring numbers</p>
			<p>The <a id="_idIndexMarker591"/>source <a id="_idIndexMarker592"/>observable is placed at the top of the diagram, the transformation is placed in the middle, and the resulting observable is placed at the bottom.</p>
			<p>Another example of a transformation is <code>group_by</code>, which sorts items into groups based on a key. The <code>group_by</code> operator <a id="_idIndexMarker593"/>takes <a id="_idIndexMarker594"/>a function that extracts a key when given an element and produces an observable for each key with the elements associated with it.</p>
			<p>The <code>group_by</code> operation can be expressed more clearly using a marble diagram. In the following diagram, you can see how <code>group_by</code> emits two observables. Additionally, the items are dynamically sorted into groups <em class="italic">as soon as they are emitted</em>:</p>
			<div><div><img src="img/Figure_7.5_B17499.jpg" alt="Figure 7.5 – Marble diagram illustrating grouping " width="1000" height="292"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5 – Marble diagram illustrating grouping</p>
			<p>We can further understand how <code>group_by</code> works with a simple example. Let's say that we want to group a number according to the fact that it's even or odd. We can implement this<a id="_idIndexMarker595"/> using <code>group_by</code> by passing the <code>lambda x: x % 2</code> expression as a key function, which will return <code>0</code> if the number is even and <code>1</code> if the number is odd, as follows:</p>
			<pre>    from rx.operators import group_by
    obs = group_by(lambda x: x %  \
      2)(from_iterable(range(4)))</pre>
			<p>At this point, if we <a id="_idIndexMarker596"/>subscribe and print the content of <code>obs</code>, two observables are actually printed, as illustrated in the following code snippet:</p>
			<pre>    obs.subscribe(print)
    # &lt;rx.linq.groupedobservable.GroupedObservable object 
      at 0x7f0fba51f9e8&gt;
    # &lt;rx.linq.groupedobservable.GroupedObservable object 
      at 0x7f0fba51fa58&gt;</pre>
			<p>You can determine the group key using the <code>key</code> attribute. To extract all the even numbers, we can take the first observable (corresponding to a key equal to 0) and subscribe to it. In the following code snippet, we show how this works:</p>
			<pre>    obs.subscribe(lambda x: print("group key: ", x.key))
    # Output:
    # group key:  0
    # group key:  1
    take(1)(obs).subscribe(lambda x: x.subscribe(print))
    # Output:
    # 0
    # 2</pre>
			<p>With <code>group_by</code>, we introduced an observable that emits other observables. This turns out to be quite a common pattern in reactive programming, and there are functions that allow you to combine different observables.</p>
			<p>A useful tool for<a id="_idIndexMarker597"/> combining observables is <code>merge_all</code> which<a id="_idIndexMarker598"/> takes multiple observables and produces a single observable that contains the element<a id="_idIndexMarker599"/> of the two observables in the order they are emitted. This is better illustrated using a marble diagram, as follows:</p>
			<div><div><img src="img/Figure_7.6_B17499.jpg" alt="Figure 7.6 – Marble diagram illustrating merging " width="972" height="213"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6 – Marble diagram illustrating merging</p>
			<p>To demonstrate its usage, we can apply the operation to the observable of observables returned by <code>group_by</code>, as follows:</p>
			<pre>    from rx.operators import merge_all
    merge_all()(obs).subscribe(print)
    # Output
    # 0
    # 1
    # 2
    # 3</pre>
			<p>With <code>merge_all</code>, the items are returned in the same order as they were initially (remember that <code>group_by</code> emits elements in the two groups as they come).</p>
			<p class="callout-heading">Tip</p>
			<p class="callout"><code>RxPy</code> also provides the <code>merge</code> operation, which can be used to combine individual observables.</p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor136"/>Hot and cold observables</h2>
			<p>In the <a id="_idIndexMarker600"/>preceding section, we learned how to create an observable using the <code>from_iterable</code> method. <code>RxPy</code> provides many other tools to create more interesting event sources.</p>
			<p><code>interval</code> takes<a id="_idIndexMarker601"/> a time interval in seconds, <code>period</code>, and creates an observable that emits a value every time the period has passed. The following code can be used to define an observable, <code>obs</code>, that will emit a number, starting from zero, every second. We use the <code>take</code> operator to limit the timer to four events:</p>
			<pre>    from rx import interval
    from rx.operators import take
    
    take(4)(interval(1)).subscribe(print)
    # Output:
    # 0
    # 1
    # 2
    # 3</pre>
			<p>A very important fact about <code>interval</code> is that the timer doesn't start until we subscribe. We can observe this by printing both the index and the delay from when the timer starts definition using <code>time.time()</code>, as follows:</p>
			<pre>    import time
    start = time.time()
    obs = map(lambda a: (a, time.time() - \
      start))(interval(1))
    # Let's wait 2 seconds before starting the subscription
    time.sleep(2)
    take(4)(obs).subscribe(print)
    # Output:
    # (0, 3.003735303878784)
    # (1, 4.004871129989624)
    # (2, 5.005947589874268)
    # (3, 6.00749135017395)</pre>
			<p>As you can see, the first element (corresponding to a <code>0</code> index) is produced after 3 seconds, which means that the timer started when we issue the <code>subscribe(print)</code> method.</p>
			<p>Observables such as <code>interval</code> are called <em class="italic">lazy</em> because they start producing values only when requested (think of them as vending machines that won't dispense food unless we press the button). In Rx jargon, these kinds of observables<a id="_idIndexMarker602"/> are called <strong class="bold">cold</strong>. A property of cold observables is that, if we attach two subscribers, the interval timer will be started multiple<a id="_idIndexMarker603"/> times. This is quite evident from the following example. Here, we add a new subscription 0.5 seconds after the first, and you can see how the output of the two subscriptions comes at different times:</p>
			<pre>    start = time.time()
    obs = map(lambda a: (a, time.time() - \
      start))(interval(1))
    # Let's wait 2 seconds before starting the subscription
    time.sleep(2)
    take(4)(obs).subscribe(lambda x: print("First \
      subscriber: {}".format(x)))
    time.sleep(0.5)
    take(4)(obs).subscribe(lambda x: print("Second  \
      subscriber: {}".format(x)))
    # Output:
    # First subscriber: (0, 3.0036110877990723)
    # Second subscriber: (0, 3.5052847862243652)
    # First subscriber: (1, 4.004414081573486)
    # Second subscriber: (1, 4.506155252456665)
    # First subscriber: (2, 5.005316972732544)
    # Second subscriber: (2, 5.506817102432251)
    # First subscriber: (3, 6.0062034130096436)
    # Second subscriber: (3, 6.508296489715576)</pre>
			<p>Sometimes, we<a id="_idIndexMarker604"/> may not want this behavior as we may want multiple subscribers to subscribe to the same data source. To make the observable produce the same data, we can delay the data production and ensure that all the subscribers will get the same data using the <code>publish</code> method.</p>
			<p><code>publish</code> will transform our observable into <code>ConnectableObservable</code>, which won't start pushing data immediately, but only when we call the <code>connect</code> method. The usage of <code>publish</code> and <code>connect</code> is demonstrated in the following code snippet:</p>
			<pre>    from rx.operators import publish
    start = time.time()
    obs = <strong class="bold">publish()</strong>(map(lambda a: (a, time.time() - \
    start))(interval(1)))
    take(4)(obs).subscribe(lambda x: print("First \
      subscriber: {}".format(x)))
<strong class="bold">    obs.connect() # Data production starts here</strong>
    time.sleep(2)
    take(4)(obs).subscribe(lambda x: print("Second \
      subscriber: {}".format(x)))
    # Output:
    # First subscriber: (0, 1.0016899108886719)
    # First subscriber: (1, 2.0027990341186523)
    # First subscriber: (2, 3.003532648086548)
    # Second subscriber: (2, 3.003532648086548)
    # First subscriber: (3, 4.004265308380127)
    # Second subscriber: (3, 4.004265308380127)
    # Second subscriber: (4, 5.005320310592651)
    # Second subscriber: (5, 6.005795240402222)</pre>
			<p>In this example, you can see how we first issue <code>publish</code>, then we subscribe the first subscriber, and finally, we issue <code>connect</code>. When <code>connect</code> is issued, the timer will start producing<a id="_idIndexMarker605"/> data. The second subscriber joins the party late and, in fact, won't receive the first two messages but will start receiving data<a id="_idIndexMarker606"/> from the third, and so on. Note that, this time around, the subscribers share the exact same data. This kind of data source, where data is produced independently of the subscribers, is called <strong class="bold">hot</strong>.</p>
			<p>Similar to <code>publish</code>, you can use the <code>replay</code> method that will produce the data <em class="italic">from the beginning</em> for each new subscriber. This is illustrated in the following example, which is identical to the preceding one except that we replaced <code>publish</code> with <code>replay</code>:</p>
			<pre>    from rx.operators import replay
    start = time.time()
    obs = <strong class="bold">replay()</strong>(map(lambda a: (a, time.time() - start) \
      ( interval(1)))
    take(4)(obs).subscribe(lambda x: print("First  \
     subscriber: {}".format(x)))
<strong class="bold">    obs.connect()</strong>
    time.sleep(2)
    take(4)(obs).subscribe(lambda x: print("Second  \
      subscriber: {}".format(x)))
    First subscriber: (0, 1.0008857250213623)
    First subscriber: (1, 2.0019824504852295)
<strong class="bold">    </strong><strong class="bold">Second subscriber: (0, 1.0008857250213623)</strong>
<strong class="bold">    Second subscriber: (1, 2.0019824504852295)</strong>
    First subscriber: (2, 3.0030810832977295)
    Second subscriber: (2, 3.0030810832977295)
    First subscriber: (3, 4.004604816436768)
    Second subscriber: (3, 4.004604816436768)</pre>
			<p>Notice<a id="_idIndexMarker607"/> that even though the second subscriber arrives late to the party, it is still given all the items that have been given out so far.</p>
			<p>Another way of creating <a id="_idIndexMarker608"/>hot observables is through the <code>Subject</code> class. <code>Subject</code> is interesting because it's capable of both receiving and pushing data, and thus it can be used to manually <em class="italic">push</em> items to an observable. Using <code>Subject</code> is very intuitive; in the following code snippet, we create a <code>Subject</code> instance and subscribe to it. Later, we push values to it using the <code>on_next</code> method; as soon as we do that, the subscriber is called:</p>
			<pre>    from rx.subject import Subject
    s = Subject()
    s.subscribe(lambda x: print("Subject emitted value: \
      {}".format(x)))
    s.on_next(1)
    # Subject emitted value: 1
    s.on_next(2)
    # Subject emitted value: 2</pre>
			<p>Note that <code>Subject</code> is another<a id="_idIndexMarker609"/> example of a hot observable.</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor137"/>Building a CPU monitor</h2>
			<p>Now that we <a id="_idIndexMarker610"/>have a grasp of the main<a id="_idIndexMarker611"/> reactive programming concepts, we can implement an example application: a monitor that will give us real-time information about our CPU usage and that can detect spikes.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The complete code for the CPU monitor can be found in the <code>cpu_monitor.py</code> file.</p>
			<p>As a first step, let's implement a data source. We will use the <code>psutil</code> module that provides a function, <code>psutil.cpu_percent</code>, that returns the latest available CPU usage as a percentage (and doesn't block). The code is illustrated in the following snippet:</p>
			<pre>    import psutil
    psutil.cpu_percent()
    # Result: 9.7</pre>
			<p>Since we are developing a monitor, we would like to sample this information over a few time intervals. To accomplish, this we can use the familiar <code>interval</code> observable, followed by <code>map</code>, just as we did in the previous section. Also, we would like to make this observable <em class="italic">hot</em> as, for this application, all subscribers should receive a single source of data; to make <code>interval</code> hot, we can use the <code>publish</code> and <code>connect</code> methods. The full code for the creation of a <code>cpu_data</code> observable is shown here:</p>
			<pre>    cpu_data = publish()(map(lambda x: \
      psutil.cpu_percent())(interval(0.1)))
    cpu_data.connect() # Start producing data</pre>
			<p>We can test our monitor by printing a sample of four items, as follows:</p>
			<pre>    take(4)(cpu_data).subscribe(print)
    # Output:
    # 12.5
    # 5.6
    # 4.5
    # 9.6</pre>
			<p>Now that<a id="_idIndexMarker612"/> our main data source is in place, we can implement a monitor visualization using <code>matplotlib</code>. The idea is to create a plot<a id="_idIndexMarker613"/> that contains a fixed number of measurements and, as new data arrives, we include the newest measurement and remove the oldest one. This is commonly referred to<a id="_idIndexMarker614"/> as a <em class="italic">moving window</em> and is better understood with an illustration. In the following diagram, our <code>cpu_data</code> stream is represented as a list of numbers. The first plot is produced as soon as we have the first four numbers and, each time a new number arrives, we shift the window by one position and update the plot:</p>
			<div><div><img src="img/Figure_7.7_B17499.jpg" alt="Figure 7.7 – Illustration of a moving window " width="896" height="314"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7 – Illustration of a moving window</p>
			<p>To implement this algorithm, we can write a function called <code>monitor_cpu</code> that will create and update our plotting window. The function will do the following things:</p>
			<ul>
				<li>Initialize an empty plot and set up the correct plot limits.</li>
				<li>Transform our <code>cpu_data</code> observable to return a moving window over the data. This can be accomplished using the <code>buffer_with_count</code> operator, which will take the number of points in our window, <code>npoints</code>, as parameters and the shift as <code>1</code>.</li>
				<li>Subscribe to this new data stream and update the plot with the incoming data.</li>
			</ul>
			<p>The <a id="_idIndexMarker615"/>complete code for the function is implemented in <code>cpu_monitor.py</code> and shown here and, as you can see, is extremely<a id="_idIndexMarker616"/> compact. Go ahead and install <code>matplotlib</code> if you don't have it in your environment already, using <code>pip install matplotlib</code>.</p>
			<p>Take some time to run the function and play with the parameters. You can view the code here:</p>
			<pre>    from rx.operators import buffer_with_count
    import numpy as np
    import pylab as plt
    def monitor_cpu(npoints):
        lines, = plt.plot([], [])
        plt.xlim(0, npoints) 
        plt.ylim(0, 100) # 0 to 100 percent
        <strong class="bold">cpu_data_window = buffer_with_count(npoints, \ </strong>
<strong class="bold">          1)(cpu_data)</strong>
        def update_plot(cpu_readings):
            lines.set_xdata(np.arange(len(cpu_readings)))
            lines.set_ydata(np.array(cpu_readings))
            plt.draw()
        <strong class="bold">cpu_data_window.subscribe(update_plot)</strong>
        plt.show()</pre>
			<p>Another<a id="_idIndexMarker617"/> feature we may want to develop is an alert that triggers when the CPU has been high for a certain amount of time, as <a id="_idIndexMarker618"/>this may indicate that some of the processes in our machine are working very hard. This can be accomplished by combining <code>buffer_with_count</code> and <code>map</code>. We can take the CPU stream and a window, and test whether all items have a value higher than 20% usage (in a quad-core CPU, that corresponds to about one processor working at 100%) in the <code>map</code> function. If all the points in the window have a higher-than-20% usage, we display a warning in our plot window.</p>
			<p>The implementation of the new observable can be written as follows and will produce an observable that emits <code>True</code> if the CPU has high usage, and <code>False</code> otherwise:</p>
			<pre>    alertpoints = 4    
    high_cpu = map(lambda readings: all(r &gt; 20 for r in \
      readings))(
        buffer_with_count(alertpoints, 1)(cpu_data)
    )</pre>
			<p>Now that the <code>high_cpu</code> observable is ready, we can create a <code>matplotlib</code> label and subscribe to it for updates, as follows:</p>
			<pre>    label = plt.text(1, 1, "normal")
    def update_warning(is_high):
        if is_high:
            label.set_text("high")
        else:
            label.set_text("normal")
    high_cpu.subscribe(update_warning)</pre>
			<p>Run the program from your terminal, and an interactive window will open to display your CPU usage<a id="_idIndexMarker619"/> data in real time. Here is a <a id="_idIndexMarker620"/>screenshot showing the output of our program:</p>
			<div><div><img src="img/Figure_7.8_B17499.jpg" alt="Figure 7.8 – Monitoring CPU usage " width="1147" height="1015"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.8 – Monitoring CPU usage</p>
			<p>Here, the blue curve denotes the CPU usage, which stays below the normal threshold.</p>
			<p>While this example is relatively simple, it possesses the core components of a reactive program, and more complex applications that fully utilize the power of <code>RxPY</code> may be built using this as a blueprint.</p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor138"/>Summary</h1>
			<p>Asynchronous programming is useful when our code deals with slow and unpredictable resources, such as I/O devices and networks. In this chapter, we explored the fundamental concepts of concurrency and asynchronous programming and how to write concurrent code with the <code>asyncio</code> and <code>RxPy</code> libraries.</p>
			<p><code>asyncio</code> coroutines are an excellent choice when dealing with multiple, interconnected resources as they greatly simplify the code logic by cleverly avoiding callbacks. Reactive programming is also very good in these situations, but it truly shines when dealing with streams of data that are common in real-time applications and UIs.</p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor139"/>Questions</h1>
			<ol>
				<li value="1">How does asynchronous programming help programs run at higher speed?</li>
				<li>What are the main differences between callbacks and futures in asynchronous programming?</li>
				<li>What are the core characteristics/requirements of a reactive application?</li>
			</ol>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor140"/>Further reading</h1>
			<ul>
				<li>Migrating to <code>RxPY</code> v3: <a href="https://rxpy.readthedocs.io/en/latests/migration.html">https://rxpy.readthedocs.io/en/latests/migration.html</a></li>
			</ul>
		</div>
	</div>
</div>
</body></html>